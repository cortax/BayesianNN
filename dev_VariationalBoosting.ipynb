{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Inference import BBVI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Inference.BBVI import VariationalNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = 0.1\n",
    "data = torch.load('Data/foong_data.pt')\n",
    "x_data = data[0].to(device)\n",
    "y_data = data[1].to(device)\n",
    "y_data = y_data.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureVariationalNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_width, nb_layers, device=None):\n",
    "        super(MixtureVariationalNetwork, self).__init__()\n",
    "        \n",
    "        self.layer_width = layer_width\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.nb_layers = nb_layers\n",
    "        self.device = device\n",
    "        \n",
    "        self.components = []\n",
    "        self.pi = torch.tensor([])\n",
    "        \n",
    "    def add_component(self, component, proportion):\n",
    "        #todo check compatibility with other components\n",
    "        self.components.append(component)\n",
    "        self.pi = torch.cat((self.pi*(1-proportion), proportion.unsqueeze(0)))\n",
    "        \n",
    "    def sample_parameters(self, M=1):\n",
    "        D = torch.distributions.multinomial.Multinomial(M, self.pi)\n",
    "        m = D.sample()\n",
    "        S = []\n",
    "        for j in range(len(self.pi)):\n",
    "            S.append(self.components[j].sample_parameters(int(m[j])))\n",
    "        return ([torch.cat([c[0][k] for c in S]) for k in range(self.nb_layers)], [torch.cat([c[1][k] for c in S]) for k in range(self.nb_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for j in range(len(self.components)):\n",
    "            outs.append(self.components[j].forward(x))\n",
    "        return torch.cat(outs, dim=0)\n",
    "        \n",
    "    def q_log_pdf(self, layered_w_samples, layered_bias_samples):\n",
    "        log_q = [c.q_log_pdf(layered_w_samples, layered_bias_samples) for c in self.components]\n",
    "        return torch.logsumexp(torch.stack(log_q) + torch.log(self.pi).unsqueeze(0).t(), dim=0)\n",
    "\n",
    "    def prior_log_pdf(self, layered_w_samples, layered_bias_samples):\n",
    "        log_prior = [c.prior_log_pdf(layered_w_samples, layered_bias_samples) for c in self.components]\n",
    "        return torch.logsumexp(torch.stack(log_prior) + torch.log(self.pi).unsqueeze(0).t(), dim=0)\n",
    "    \n",
    "    def compute_elbo(self, x_data, y_data, n_samples_ELBO, sigma_noise, device, new_component=None, new_proportion=None):\n",
    "    \n",
    "        # sample X^(c)\n",
    "        (layered_w_samples_XC, layered_bias_samples_XC) = self.sample_parameters(n_samples_ELBO)\n",
    "\n",
    "        LP_XC = self.prior_log_pdf(layered_w_samples_XC, layered_bias_samples_XC)\n",
    "        y_pred_XC = self.forward(x_data)\n",
    "        LL_XC = self._log_norm(y_pred_XC, y_data, torch.tensor(sigma_noise).to(device))\n",
    "        posterior_XC = torch.sum(LP_XC.unsqueeze(-1).unsqueeze(-1) + LL_XC, dim=[1,2])\n",
    "\n",
    "        qC_log_XC = self.q_log_pdf(layered_w_samples_XC, layered_bias_samples_XC)\n",
    "        \n",
    "        if new_component is None:\n",
    "            return torch.mean(qC_log_XC - posterior_XC)\n",
    "        \n",
    "        qN_log_XC = new_component.q_log_pdf(layered_w_samples_XC, layered_bias_samples_XC)\n",
    "        qCN_log_XC = torch.logsumexp(torch.stack([torch.log(torch.tensor(1.0)-new_proportion) + qC_log_XC, torch.log(new_proportion) + qN_log_XC],dim=0),dim=0)\n",
    "        \n",
    "        # sample X_(c+1)\n",
    "        (layered_w_samples_XN, layered_bias_samples_XN) = new_component.sample_parameters(n_samples_ELBO)\n",
    "        \n",
    "        LP_XN = new_component.prior_log_pdf(layered_w_samples_XN, layered_bias_samples_XN)\n",
    "        y_pred_XN = new_component.forward(x_data)\n",
    "        LL_XN = new_component._log_norm(y_pred_XN, y_data, torch.tensor(sigma_noise).to(device))\n",
    "        posterior_XN = torch.sum(LP_XN.unsqueeze(-1).unsqueeze(-1) + LL_XN, dim=[1,2])\n",
    "\n",
    "        qC_log_XN = self.q_log_pdf(layered_w_samples_XN, layered_bias_samples_XN)\n",
    "        \n",
    "        qN_log_XN = new_component.q_log_pdf(layered_w_samples_XN, layered_bias_samples_XN)\n",
    "        qCN_log_XN = torch.logsumexp(torch.stack([torch.log(torch.tensor(1.0)-new_proportion) + qC_log_XN, torch.log(new_proportion) + qN_log_XN],dim=0),dim=0)\n",
    "        \n",
    "        L = (torch.tensor(1.0)-new_proportion) * qCN_log_XC.mean() + new_proportion * qCN_log_XN.mean() \n",
    "        \n",
    "        return L\n",
    "    \n",
    "    def requires_grad_rhos(self, v = False):\n",
    "        for k in range(len(self.components)):\n",
    "            self.components[k].requires_grad_rhos(v)\n",
    "        \n",
    "    def requires_grad_mus(self, v = False):\n",
    "        for k in range(len(self.components)):\n",
    "            self.components[k].requires_grad_mus(v)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.components[k].forward(x_data) for k in range(len(self.components))],dim=0)\n",
    "\n",
    "    def _log_norm(self, x, mu, std):\n",
    "        return -0.5 * torch.log(2*np.pi*std**2) -(0.5 * (1/(std**2))* (x-mu)**2)\n",
    "    \n",
    "    #def KL_log_pdf(self):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = VariationalNetwork(1, 1, 10, 5, device=None)\n",
    "c2 = VariationalNetwork(1, 1, 10, 5, device=None)\n",
    "c3 = VariationalNetwork(1, 1, 10, 5, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = MixtureVariationalNetwork(1, 1, 10, 5, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.add_component(c1, torch.tensor(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.add_component(c2, torch.tensor(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.add_component(c3, torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_component = VariationalNetwork(1, 1, 10, 5, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix.components[2].linear1.q_weight_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.requires_grad_rhos(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(.1, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a) == torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtureVariationalNetwork()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_component.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at /opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/THC/THCGeneral.cpp:50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-864e65c014cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0melbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_component\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_component\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_proportion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-7bf204d466bd>\u001b[0m in \u001b[0;36mcompute_elbo\u001b[0;34m(self, x_data, y_data, n_samples_ELBO, sigma_noise, device, new_component, new_proportion)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mLP_XC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior_log_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayered_w_samples_XC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayered_bias_samples_XC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0my_pred_XC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mLL_XC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_XC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mposterior_XC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLP_XC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mLL_XC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m    178\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0m_cudart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudaGetErrorName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at /opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/THC/THCGeneral.cpp:50"
     ]
    }
   ],
   "source": [
    "elbo = mix.compute_elbo(x_data, y_data, 100, .1, new_component=new_component, new_proportion=a, device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = mix\n",
    "n_samples_ELBO = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "(layered_w_samples_XC, layered_bias_samples_XC) = self.sample_parameters(n_samples_ELBO)\n",
    "\n",
    "LP_XC = self.prior_log_pdf(layered_w_samples_XC, layered_bias_samples_XC)\n",
    "y_pred_XC = self.forward(x_data)\n",
    "LL_XC = self._log_norm(y_pred_XC, y_data, torch.tensor(sigma_noise).to(device))\n",
    "posterior_XC = torch.sum(LP_XC.unsqueeze(-1).unsqueeze(-1) + LL_XC, dim=[1,2])\n",
    "\n",
    "qC_log_XC = self.q_log_pdf(layered_w_samples_XC, layered_bias_samples_XC)\n",
    "\n",
    "qN_log_XC = new_component.q_log_pdf(layered_w_samples_XC, layered_bias_samples_XC)\n",
    "qCN_log_XC = torch.logsumexp(torch.stack([torch.log(torch.tensor(1.0)-new_proportion) + qC_log_XC, torch.log(new_proportion) + qN_log_XC],dim=0),dim=0)\n",
    "\n",
    "# sample X_(c+1)\n",
    "(layered_w_samples_XN, layered_bias_samples_XN) = new_component.sample_parameters(n_samples_ELBO)\n",
    "\n",
    "LP_XN = new_component.prior_log_pdf(layered_w_samples_XN, layered_bias_samples_XN)\n",
    "y_pred_XN = new_component.forward(x_data)\n",
    "LL_XN = new_component._log_norm(y_pred_XN, y_data, torch.tensor(sigma_noise).to(device))\n",
    "posterior_XN = torch.sum(LP_XN.unsqueeze(-1).unsqueeze(-1) + LL_XN, dim=[1,2])\n",
    "\n",
    "qC_log_XN = self.q_log_pdf(layered_w_samples_XN, layered_bias_samples_XN)\n",
    "\n",
    "qN_log_XN = new_component.q_log_pdf(layered_w_samples_XN, layered_bias_samples_XN)\n",
    "qCN_log_XN = torch.logsumexp(torch.stack([torch.log(torch.tensor(1.0)-new_proportion) + qC_log_XN, torch.log(new_proportion) + qN_log_XN],dim=0),dim=0)\n",
    "\n",
    "L = (torch.tensor(1.0)-new_proportion) * qCN_log_XC.sum() + new_proportion * qCN_log_XN.sum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2169],\n",
       "        [ 0.6991],\n",
       "        [-3.8079],\n",
       "        [-1.1187],\n",
       "        [-0.4202],\n",
       "        [ 4.6133],\n",
       "        [-6.4457],\n",
       "        [-0.7107],\n",
       "        [-5.0732],\n",
       "        [-6.7637]], requires_grad=True)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.components[0].linear1.q_weight_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, nComponents, tolerance, input_size, output_size, layer_width, nb_layers, device=None):\n",
    "        super(BoostingModel, self).__init__()\n",
    "        \n",
    "        self.H = layer_width\n",
    "        self.fixed_components = []\n",
    "        self.fixed_mixture_probas = torch.tensor([1.])\n",
    "        self.learnable_proba = None\n",
    "        self.learnable_component = VariationalNetwork(input_size, output_size, layer_width, nb_layers, device)\n",
    "        self.nComponents = nComponents\n",
    "        self.current_nComponents = 1\n",
    "        self.current_component = self.learnable_component\n",
    "        self.current_component_index = None\n",
    "        self.current_hyper_parameters = {'weight_mus': [layer.q_weight_mu for layer in self.current_component.registered_layers],\n",
    "                                   'weight_rhos': [layer.q_weight_rho for layer in self.current_component.registered_layers],\n",
    "                                   'bias_mus' : [layer.q_bias_mu for layer in self.current_component.registered_layers],\n",
    "                                   'bias_rhos': [layer.q_bias_rho for layer in self.current_component.registered_layers]}\n",
    "        \n",
    "        self.current_sampled_parameters = {'weight': [layer.weight_sample for layer in self.current_component.registered_layers],\n",
    "                                          'bias': [layer.bias_sample for layer in self.current_component.registered_layers]}\n",
    "        \n",
    "        self.tolerance = tolerance\n",
    "        self.potential_starting_points = [(0, torch.tensor(np.inf)) for i in range(self.tolerance)]\n",
    "        self.device = device\n",
    "        \n",
    "    def refresh_current_hyper_parameters(self):\n",
    "        self.current_hyper_parameters = {'weight_mus': [layer.q_weight_mu for layer in self.current_component.registered_layers],\n",
    "                                   'weight_rhos': [layer.q_weight_rho for layer in self.current_component.registered_layers],\n",
    "                                   'bias_mus' : [layer.q_bias_mu for layer in self.current_component.registered_layers],\n",
    "                                   'bias_rhos': [layer.q_bias_rho for layer in self.current_component.registered_layers]}\n",
    "    \n",
    "    def refresh_current_sampled_parameters(self):\n",
    "        self.current_sampled_parameters = {'weight': [layer.weight_sample for layer in self.current_component.registered_layers],\n",
    "                                          'bias': [layer.bias_sample for layer in self.current_component.registered_layers]}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.current_component(x)\n",
    "        return out\n",
    "    \n",
    "    def resample_parameters_in_eval(self):\n",
    "        self.sample_component(last = True)\n",
    "        self.sample_parameters()\n",
    "        self.refresh_current_sampled_parameters()\n",
    "        \n",
    "    def resample_parameters_in_train(self):\n",
    "        self.sample_component(last = False)\n",
    "        self.sample_parameters()\n",
    "        self.refresh_current_sampled_parameters()\n",
    "    \n",
    "    def sample_component(self, last = False):\n",
    "        \n",
    "        if len(self.fixed_components) == 0:\n",
    "            self.current_component = self.current_component\n",
    "        \n",
    "        elif not last:\n",
    "            self.current_component = np.random.choice(self.fixed_components, p = self.fixed_mixture_probas.data.numpy())\n",
    "        else:\n",
    "            pi = sigmoid(self.learnable_proba.detach())\n",
    "            if uniform.sample()< pi:\n",
    "                self.current_component = self.learnable_component\n",
    "            else:\n",
    "                self.sample_component(last = False)\n",
    "                \n",
    "    def sample_parameters(self):\n",
    "        self.current_component.resample_parameters()\n",
    "        \n",
    "    def mixture_log_pdf(self):\n",
    "        pi = sigmoid(self.learnable_proba)\n",
    "        probs = torch.cat((self.fixed_mixture_probas*(1-pi), pi.unsqueeze(0)))\n",
    "        #print(probs)\n",
    "        log_q = []\n",
    "        for i, component in enumerate(self.fixed_components):\n",
    "            #component.set_sampled_parameters(self.current_parameters['weight_mus'],\n",
    "                                     #self.current_parameters['weight_rhos'],\n",
    "                                     #self.current_parameters['bias_mus'],\n",
    "                                     #self.current_parameters['bias_rhos'])\n",
    "            component.set_sampled_parameters(self.current_sampled_parameters['weight'],\n",
    "                                            self.current_sampled_parameters['bias'])\n",
    "            log_q.append(component.q_log_pdf())\n",
    "        #self.learnable_component.set_parameters(self.current_parameters['weight_mus'],\n",
    "                                     #self.current_parameters['weight_rhos'],\n",
    "                                     #self.current_parameters['bias_mus'],\n",
    "                                     #self.current_parameters['bias_rhos'])\n",
    "        self.learnable_component.set_sampled_parameters(self.current_sampled_parameters['weight'],\n",
    "                                                       self.current_sampled_parameters['bias'])\n",
    "        log_q.append(self.learnable_component.q_log_pdf())\n",
    "        log_q = torch.stack(log_q)\n",
    "        #signs = log_q.detach().sign()\n",
    "        log_q = log_q.add(torch.log(probs))\n",
    "        log_q = torch.logsumexp(log_q, dim = 0)\n",
    "        \"\"\"investigate gradient of max\"\"\"\n",
    "        #maximum = log_q.max()\n",
    "        #log_q = log_q - maximum\n",
    "        #log_q = torch.exp(log_q)\n",
    "        #log_q = log_q*probs\n",
    "        #log_q = torch.log(torch.sum(log_q)) + maximum\n",
    "        return(log_q)\n",
    "    \n",
    "    def compute_mixture_elbo(self, x_data, y_data, sample_size):\n",
    "        if type(self.learnable_proba) != type(None):\n",
    "            #print(\"1\")\n",
    "            \n",
    "            pi_new = sigmoid(self.learnable_proba)\n",
    "        \n",
    "            \"\"\"Compute expectancy w.r.t old mixture\"\"\"\n",
    "            L_old_mixture = []\n",
    "            for _ in range(sample_size):\n",
    "                self.resample_parameters_in_train()\n",
    "                LQ = self.mixture_log_pdf()\n",
    "             #   print(LQ.grad_fn)\n",
    "                y_pred = self.forward(x_data)\n",
    "                LL = log_norm(y_data, y_pred.t(), torch.tensor(sigma_noise)).sum()\n",
    "                LP = self.current_component.prior_log_pdf()\n",
    "                L_old_mixture.append(LQ - LL - LP)\n",
    "            L_old_mixture = torch.stack(L_old_mixture)\n",
    "            #print(L_old_mixture.grad_fn)\n",
    "            L_old_mixture = torch.mean(L_old_mixture)\n",
    "            #print(L_old_mixture < self.potential_starting_points[-1][1])\n",
    "            if L_old_mixture < self.potential_starting_points[-1][1]:\n",
    "                self.potential_starting_points[-1] = (self.current_sampled_parameters, L_old_mixture.detach().data)\n",
    "                self.potential_starting_points = sorted(self.potential_starting_points, key = lambda x: x[1])\n",
    "            \n",
    "            #print(L_old_mixture.grad_fn)\n",
    "            #print(L_old_mixture)\n",
    "\n",
    "            \"\"\"Compute expectancy w.r.t new component\"\"\"\n",
    "            L_new_component = []\n",
    "            for _ in range(sample_size):\n",
    "                self.current_component = self.learnable_component\n",
    "                self.sample_parameters()\n",
    "                self.refresh_current_sampled_parameters()\n",
    "                LQ = self.mixture_log_pdf()\n",
    "                self.sample_component(last = True)\n",
    "                y_pred = self.forward(x_data)\n",
    "                LL = log_norm(y_data, y_pred.t(), torch.tensor(sigma_noise)).sum()\n",
    "                LP = self.current_component.prior_log_pdf()\n",
    "                L_new_component.append(LQ - LL - LP)\n",
    "            L_new_component = torch.stack(L_new_component)\n",
    "            L_new_component = torch.mean(L_new_component)\n",
    "            #print(L_new_component.grad_fn)\n",
    "            L = (1-pi_new)*L_old_mixture + pi_new*L_new_component\n",
    "            #\n",
    "            return L\n",
    "        else:\n",
    "            #print(\"2\")\n",
    "            L = self.learnable_component.compute_elbo(x_data, y_data, sample_size, sigma_noise, self.device)\n",
    "            #print(L)\n",
    "            if L.detach() < self.potential_starting_points[-1][1]:\n",
    "                self.potential_starting_points[-1] = (self.current_sampled_parameters, L.detach().data)\n",
    "                self.potential_starting_points = sorted(self.potential_starting_points, key = lambda x: x[1])\n",
    "                #print(self.potential_starting_points)\n",
    "                #self.potential_starting_points = self.potential_starting_points[1:]\n",
    "            return(L)\n",
    "    \n",
    "    \n",
    "    def new_component(self, losses, epsilon, new_pi):\n",
    "        #print('std losses',torch.std(losses))\n",
    "        \n",
    "        if epsilon and len(self.fixed_components)+1<self.nComponents:\n",
    "        #if torch.std(losses) < epsilon and len(self.components)<self.nComponents:\n",
    "            self.refresh_current_hyper_parameters()\n",
    "            print(\"WE GOT THERE !!!!\")\n",
    "            self.learnable_component.lock_means()\n",
    "            self.learnable_component.lock_rhos()\n",
    "            self.fixed_components.append(self.learnable_component)\n",
    "            self.current_component = self.learnable_component\n",
    "            self.refresh_current_hyper_parameters()\n",
    "            self.learnable_component = RegressionModel(self.H)\n",
    "            #self.learnable_component.set_hyper_parameters(torch.mean(torch.stack([c[0]['weight'] for c in self.potential_starting_points])),\n",
    "            #                         torch.tensor(,\n",
    "            #                         torch.mean(torch.stack([c[0]['bias_mus'] for c in self.potential_starting_points])),\n",
    "            #                         torch.mean(torch.stack([c[0]['bias_rhos'] for c in self.potential_starting_points])))\n",
    "            #self.learnable_component.lock_means()\n",
    "            #self.learnable_component.lock_rhos()\n",
    "            if type(self.learnable_proba) != type(None):\n",
    "                self.learnable_proba.detach_()\n",
    "                pi = sigmoid(self.learnable_proba)\n",
    "                self.fixed_mixture_probas = torch.cat((self.fixed_mixture_probas*(1 - pi), pi.unsqueeze(0)))\n",
    "            self.learnable_proba = torch.tensor(float(new_pi), requires_grad = True)\n",
    "            print('NEW COMPONENT OK')\n",
    "            return 1\n",
    "#torch.std(losses) < epsilon\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nComponents = 3\n",
    "tolerance = 10\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "layer_width = 20\n",
    "nb_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BoostingModel(nComponents, tolerance, input_size, output_size, layer_width, nb_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.95,verbose=True)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "num_epoch = 300\n",
    "num_iterations = 100\n",
    "liveloss = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ELBO_samples = 10\n",
    "loss = model.compute_mixture_elbo(x_data, y_data, n_ELBO_samples)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M = int(.005/learning_rate)+1\n",
    "weights = []\n",
    "n_ELBO_samples=5\n",
    "j = 0\n",
    "std_steps = 100\n",
    "stop = False\n",
    "while stop == False:\n",
    "#for j in range(num_epoch):\n",
    "    logs = {}\n",
    "    losses = [None] * num_iterations\n",
    "    \n",
    "    for k in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        #if new_comp_this_epoch:\n",
    "            #print('before grad', model.learnable_proba)\n",
    "        loss = model.compute_mixture_elbo(x_data, y_data, n_ELBO_samples)\n",
    "        losses[k] = loss.detach()\n",
    "        loss.backward()\n",
    "        #if new_comp_this_epoch:\n",
    "        #    print('after grad', model.current_proba_parameter)\n",
    "        #print(model.components[0].linear1.weight_sample.grad)\n",
    "        gradients = torch.sum(model.learnable_component.linear1.q_weight_mu.detach()**2)\n",
    "        weights.append(gradients)\n",
    "        \n",
    "        optimizer.step()\n",
    "        model.refresh_current_hyper_parameters()\n",
    "        #print(model.current_proba_parameter)\n",
    "    if j>std_steps and model.current_nComponents < model.nComponents:\n",
    "#        new_comp = model.new_component(None, epsilon = (optimizer.param_groups[0]['lr'] != learning_rate), new_pi = np.log(1./(4*model.nComponents+3)))\n",
    "        new_comp = model.new_component(None, epsilon = (torch.std(torch.stack(weights[-std_steps:])) < .1), new_pi = 0.)#np.log(1./(4*model.nComponents+3)))\n",
    "\n",
    "        #print('STOP', stop)\n",
    "        if new_comp:\n",
    "            model.current_nComponents += 1\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "            optimizer.add_param_group({\"params\": model.learnable_proba, 'lr': .005})\n",
    "            #break\n",
    "            #M+=10\n",
    "    #if model.nComponents ==3 and (optimizer.param_groups[0]['lr'] != learning_rate):\n",
    "    #    model.learnable_component.lock_means()\n",
    "    #    model.learnable_component.lock_rhos()\n",
    "    #    model.fixed_components.append(model.learnable_component)\n",
    "    #    pi = model.learnable_proba.detach()\n",
    "    #    model.fixed_mixture_probas = torch.cat(((1-pi)*model.fixed_mixture_probas, pi.unsqueeze(0)))\n",
    "    #    break\n",
    "\n",
    "        #print(model.current_proba_parameter)\n",
    "    #print(model.mixture_probas)\n",
    "    #print(torch.std(torch.stack(losses[k-4:k])))\n",
    "    def f(x):\n",
    "        if type(x) == type(None):\n",
    "            return(0.)\n",
    "        else:\n",
    "            return(sigmoid(x))\n",
    "    #f = lambda x: {type(None): 1}.get(type(x), sigmoid(x))\n",
    "    logs['expected_loss'] = torch.stack(losses).mean().detach().clone().numpy()\n",
    "    logs['learning rate'] = optimizer.param_groups[0]['lr']\n",
    "    logs['ncomponents'] = len(model.fixed_components) + 1\n",
    "    logs['current_proba'] = f(model.learnable_proba)\n",
    "    logs['gradients_weights'] = gradients\n",
    "    #iilogs['proba_gradients'] = prob_grad\n",
    "    #if type(model.current_proba_parameter) != type(None):\n",
    "    #    logs['current_proba'] = model.current_proba_parameter.detach().data\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    #M = int(.005/lr)+1\n",
    "    #M=5\n",
    "    #print(M)\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    "    if j > std_steps:\n",
    "         weights = weights[-std_steps:]\n",
    "    #print('epoch', j, 'num_components', len(model.components), 'stop', stop)\n",
    "    \n",
    "    scheduler.step(logs['expected_loss'])\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
