{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Inference import BBVI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Inference.BBVI import VariationalNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = 0.1\n",
    "(x_data, y_data) = torch.load('Data/foong_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureVariationalNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_width, nb_layers, device=None):\n",
    "        super(MixtureVariationalNetwork, self).__init__()\n",
    "        \n",
    "        self.layer_width = layer_width\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.nb_layers = nb_layers\n",
    "        self.device = device\n",
    "        \n",
    "        self.components = []\n",
    "        self.pi = torch.tensor([])\n",
    "        \n",
    "    def add_component(self, component, proportion):\n",
    "        #todo check compatibility with other components\n",
    "        self.components.append(component)\n",
    "        self.pi = torch.cat((self.pi*(1-proportion), proportion.unsqueeze(0)))\n",
    "        \n",
    "    def sample_parameters(self, M=1):\n",
    "        D = torch.distributions.multinomial.Multinomial(M, self.pi)\n",
    "        m = D.sample()\n",
    "        \n",
    "        \n",
    "        \n",
    "        for j in range(len(self.components)):\n",
    "            self.components[j].resample_parameters(int(m[j]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for j in range(len(self.components)):\n",
    "            outs.append(self.components[j].forward(x))\n",
    "        return torch.cat(outs, dim=0)\n",
    "        \n",
    "\n",
    "    #def compute_elbo(self, x_data, y_data, n_samples_ELBO, sigma_noise, device, new_component=None, new_proportion=None):\n",
    "        \n",
    "    def q_log_pdf(self, new_component=None, new_proportion=None):\n",
    "        pi = sigmoid(self.learnable_proba)\n",
    "        probs = torch.cat((self.fixed_mixture_probas*(1-pi), pi.unsqueeze(0)))\n",
    "        #print(probs)\n",
    "        log_q = []\n",
    "        for i, component in enumerate(self.fixed_components):\n",
    "            #component.set_sampled_parameters(self.current_parameters['weight_mus'],\n",
    "                                     #self.current_parameters['weight_rhos'],\n",
    "                                     #self.current_parameters['bias_mus'],\n",
    "                                     #self.current_parameters['bias_rhos'])\n",
    "            component.set_sampled_parameters(self.current_sampled_parameters['weight'],\n",
    "                                            self.current_sampled_parameters['bias'])\n",
    "            log_q.append(component.q_log_pdf())\n",
    "        #self.learnable_component.set_parameters(self.current_parameters['weight_mus'],\n",
    "                                     #self.current_parameters['weight_rhos'],\n",
    "                                     #self.current_parameters['bias_mus'],\n",
    "                                     #self.current_parameters['bias_rhos'])\n",
    "        self.learnable_component.set_sampled_parameters(self.current_sampled_parameters['weight'],\n",
    "                                                       self.current_sampled_parameters['bias'])\n",
    "        log_q.append(self.learnable_component.q_log_pdf())\n",
    "        log_q = torch.stack(log_q)\n",
    "        #signs = log_q.detach().sign()\n",
    "        log_q = log_q.add(torch.log(probs))\n",
    "        log_q = torch.logsumexp(log_q, dim = 0)\n",
    "        \"\"\"investigate gradient of max\"\"\"\n",
    "        #maximum = log_q.max()\n",
    "        #log_q = log_q - maximum\n",
    "        #log_q = torch.exp(log_q)\n",
    "        #log_q = log_q*probs\n",
    "        #log_q = torch.log(torch.sum(log_q)) + maximum\n",
    "        return(log_q)\n",
    "\n",
    "    #def prior_log_pdf(self, new_component=None, new_proportion=None):\n",
    "       \n",
    "    \n",
    "        \n",
    "    \n",
    "    #def forward(self, x):\n",
    "    \n",
    "    #def KL_log_pdf(self):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = VariationalNetwork(1, 1, 10, 2, device=None)\n",
    "c2 = VariationalNetwork(1, 1, 10, 2, device=None)\n",
    "c3 = VariationalNetwork(1, 1, 10, 2, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = MixtureVariationalNetwork(1, 1, 10, 2, device=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.add_component(c1, torch.tensor(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.add_component(c2, torch.tensor(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.add_component(c3, torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.components[0].linear1.weight_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.resample_component(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = mix.forward(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, nComponents, tolerance, input_size, output_size, layer_width, nb_layers, device=None):\n",
    "        super(BoostingModel, self).__init__()\n",
    "        \n",
    "        self.H = layer_width\n",
    "        self.fixed_components = []\n",
    "        self.fixed_mixture_probas = torch.tensor([1.])\n",
    "        self.learnable_proba = None\n",
    "        self.learnable_component = VariationalNetwork(input_size, output_size, layer_width, nb_layers, device)\n",
    "        self.nComponents = nComponents\n",
    "        self.current_nComponents = 1\n",
    "        self.current_component = self.learnable_component\n",
    "        self.current_component_index = None\n",
    "        self.current_hyper_parameters = {'weight_mus': [layer.q_weight_mu for layer in self.current_component.registered_layers],\n",
    "                                   'weight_rhos': [layer.q_weight_rho for layer in self.current_component.registered_layers],\n",
    "                                   'bias_mus' : [layer.q_bias_mu for layer in self.current_component.registered_layers],\n",
    "                                   'bias_rhos': [layer.q_bias_rho for layer in self.current_component.registered_layers]}\n",
    "        \n",
    "        self.current_sampled_parameters = {'weight': [layer.weight_sample for layer in self.current_component.registered_layers],\n",
    "                                          'bias': [layer.bias_sample for layer in self.current_component.registered_layers]}\n",
    "        \n",
    "        self.tolerance = tolerance\n",
    "        self.potential_starting_points = [(0, torch.tensor(np.inf)) for i in range(self.tolerance)]\n",
    "        self.device = device\n",
    "        \n",
    "    def refresh_current_hyper_parameters(self):\n",
    "        self.current_hyper_parameters = {'weight_mus': [layer.q_weight_mu for layer in self.current_component.registered_layers],\n",
    "                                   'weight_rhos': [layer.q_weight_rho for layer in self.current_component.registered_layers],\n",
    "                                   'bias_mus' : [layer.q_bias_mu for layer in self.current_component.registered_layers],\n",
    "                                   'bias_rhos': [layer.q_bias_rho for layer in self.current_component.registered_layers]}\n",
    "    \n",
    "    def refresh_current_sampled_parameters(self):\n",
    "        self.current_sampled_parameters = {'weight': [layer.weight_sample for layer in self.current_component.registered_layers],\n",
    "                                          'bias': [layer.bias_sample for layer in self.current_component.registered_layers]}\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.current_component(x)\n",
    "        return out\n",
    "    \n",
    "    def resample_parameters_in_eval(self):\n",
    "        self.sample_component(last = True)\n",
    "        self.sample_parameters()\n",
    "        self.refresh_current_sampled_parameters()\n",
    "        \n",
    "    def resample_parameters_in_train(self):\n",
    "        self.sample_component(last = False)\n",
    "        self.sample_parameters()\n",
    "        self.refresh_current_sampled_parameters()\n",
    "    \n",
    "    def sample_component(self, last = False):\n",
    "        \n",
    "        if len(self.fixed_components) == 0:\n",
    "            self.current_component = self.current_component\n",
    "        \n",
    "        elif not last:\n",
    "            self.current_component = np.random.choice(self.fixed_components, p = self.fixed_mixture_probas.data.numpy())\n",
    "        else:\n",
    "            pi = sigmoid(self.learnable_proba.detach())\n",
    "            if uniform.sample()< pi:\n",
    "                self.current_component = self.learnable_component\n",
    "            else:\n",
    "                self.sample_component(last = False)\n",
    "                \n",
    "    def sample_parameters(self):\n",
    "        self.current_component.resample_parameters()\n",
    "        \n",
    "    def mixture_log_pdf(self):\n",
    "        pi = sigmoid(self.learnable_proba)\n",
    "        probs = torch.cat((self.fixed_mixture_probas*(1-pi), pi.unsqueeze(0)))\n",
    "        #print(probs)\n",
    "        log_q = []\n",
    "        for i, component in enumerate(self.fixed_components):\n",
    "            #component.set_sampled_parameters(self.current_parameters['weight_mus'],\n",
    "                                     #self.current_parameters['weight_rhos'],\n",
    "                                     #self.current_parameters['bias_mus'],\n",
    "                                     #self.current_parameters['bias_rhos'])\n",
    "            component.set_sampled_parameters(self.current_sampled_parameters['weight'],\n",
    "                                            self.current_sampled_parameters['bias'])\n",
    "            log_q.append(component.q_log_pdf())\n",
    "        #self.learnable_component.set_parameters(self.current_parameters['weight_mus'],\n",
    "                                     #self.current_parameters['weight_rhos'],\n",
    "                                     #self.current_parameters['bias_mus'],\n",
    "                                     #self.current_parameters['bias_rhos'])\n",
    "        self.learnable_component.set_sampled_parameters(self.current_sampled_parameters['weight'],\n",
    "                                                       self.current_sampled_parameters['bias'])\n",
    "        log_q.append(self.learnable_component.q_log_pdf())\n",
    "        log_q = torch.stack(log_q)\n",
    "        #signs = log_q.detach().sign()\n",
    "        log_q = log_q.add(torch.log(probs))\n",
    "        log_q = torch.logsumexp(log_q, dim = 0)\n",
    "        \"\"\"investigate gradient of max\"\"\"\n",
    "        #maximum = log_q.max()\n",
    "        #log_q = log_q - maximum\n",
    "        #log_q = torch.exp(log_q)\n",
    "        #log_q = log_q*probs\n",
    "        #log_q = torch.log(torch.sum(log_q)) + maximum\n",
    "        return(log_q)\n",
    "    \n",
    "    def compute_mixture_elbo(self, x_data, y_data, sample_size):\n",
    "        if type(self.learnable_proba) != type(None):\n",
    "            #print(\"1\")\n",
    "            \n",
    "            pi_new = sigmoid(self.learnable_proba)\n",
    "        \n",
    "            \"\"\"Compute expectancy w.r.t old mixture\"\"\"\n",
    "            L_old_mixture = []\n",
    "            for _ in range(sample_size):\n",
    "                self.resample_parameters_in_train()\n",
    "                LQ = self.mixture_log_pdf()\n",
    "             #   print(LQ.grad_fn)\n",
    "                y_pred = self.forward(x_data)\n",
    "                LL = log_norm(y_data, y_pred.t(), torch.tensor(sigma_noise)).sum()\n",
    "                LP = self.current_component.prior_log_pdf()\n",
    "                L_old_mixture.append(LQ - LL - LP)\n",
    "            L_old_mixture = torch.stack(L_old_mixture)\n",
    "            #print(L_old_mixture.grad_fn)\n",
    "            L_old_mixture = torch.mean(L_old_mixture)\n",
    "            #print(L_old_mixture < self.potential_starting_points[-1][1])\n",
    "            if L_old_mixture < self.potential_starting_points[-1][1]:\n",
    "                self.potential_starting_points[-1] = (self.current_sampled_parameters, L_old_mixture.detach().data)\n",
    "                self.potential_starting_points = sorted(self.potential_starting_points, key = lambda x: x[1])\n",
    "            \n",
    "            #print(L_old_mixture.grad_fn)\n",
    "            #print(L_old_mixture)\n",
    "\n",
    "            \"\"\"Compute expectancy w.r.t new component\"\"\"\n",
    "            L_new_component = []\n",
    "            for _ in range(sample_size):\n",
    "                self.current_component = self.learnable_component\n",
    "                self.sample_parameters()\n",
    "                self.refresh_current_sampled_parameters()\n",
    "                LQ = self.mixture_log_pdf()\n",
    "                self.sample_component(last = True)\n",
    "                y_pred = self.forward(x_data)\n",
    "                LL = log_norm(y_data, y_pred.t(), torch.tensor(sigma_noise)).sum()\n",
    "                LP = self.current_component.prior_log_pdf()\n",
    "                L_new_component.append(LQ - LL - LP)\n",
    "            L_new_component = torch.stack(L_new_component)\n",
    "            L_new_component = torch.mean(L_new_component)\n",
    "            #print(L_new_component.grad_fn)\n",
    "            L = (1-pi_new)*L_old_mixture + pi_new*L_new_component\n",
    "            #\n",
    "            return L\n",
    "        else:\n",
    "            #print(\"2\")\n",
    "            L = self.learnable_component.compute_elbo(x_data, y_data, sample_size, sigma_noise, self.device)\n",
    "            #print(L)\n",
    "            if L.detach() < self.potential_starting_points[-1][1]:\n",
    "                self.potential_starting_points[-1] = (self.current_sampled_parameters, L.detach().data)\n",
    "                self.potential_starting_points = sorted(self.potential_starting_points, key = lambda x: x[1])\n",
    "                #print(self.potential_starting_points)\n",
    "                #self.potential_starting_points = self.potential_starting_points[1:]\n",
    "            return(L)\n",
    "    \n",
    "    \n",
    "    def new_component(self, losses, epsilon, new_pi):\n",
    "        #print('std losses',torch.std(losses))\n",
    "        \n",
    "        if epsilon and len(self.fixed_components)+1<self.nComponents:\n",
    "        #if torch.std(losses) < epsilon and len(self.components)<self.nComponents:\n",
    "            self.refresh_current_hyper_parameters()\n",
    "            print(\"WE GOT THERE !!!!\")\n",
    "            self.learnable_component.lock_means()\n",
    "            self.learnable_component.lock_rhos()\n",
    "            self.fixed_components.append(self.learnable_component)\n",
    "            self.current_component = self.learnable_component\n",
    "            self.refresh_current_hyper_parameters()\n",
    "            self.learnable_component = RegressionModel(self.H)\n",
    "            #self.learnable_component.set_hyper_parameters(torch.mean(torch.stack([c[0]['weight'] for c in self.potential_starting_points])),\n",
    "            #                         torch.tensor(,\n",
    "            #                         torch.mean(torch.stack([c[0]['bias_mus'] for c in self.potential_starting_points])),\n",
    "            #                         torch.mean(torch.stack([c[0]['bias_rhos'] for c in self.potential_starting_points])))\n",
    "            #self.learnable_component.lock_means()\n",
    "            #self.learnable_component.lock_rhos()\n",
    "            if type(self.learnable_proba) != type(None):\n",
    "                self.learnable_proba.detach_()\n",
    "                pi = sigmoid(self.learnable_proba)\n",
    "                self.fixed_mixture_probas = torch.cat((self.fixed_mixture_probas*(1 - pi), pi.unsqueeze(0)))\n",
    "            self.learnable_proba = torch.tensor(float(new_pi), requires_grad = True)\n",
    "            print('NEW COMPONENT OK')\n",
    "            return 1\n",
    "#torch.std(losses) < epsilon\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nComponents = 3\n",
    "tolerance = 10\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "layer_width = 20\n",
    "nb_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BoostingModel(nComponents, tolerance, input_size, output_size, layer_width, nb_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, factor=0.95,verbose=True)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "num_epoch = 300\n",
    "num_iterations = 100\n",
    "liveloss = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ELBO_samples = 10\n",
    "loss = model.compute_mixture_elbo(x_data, y_data, n_ELBO_samples)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M = int(.005/learning_rate)+1\n",
    "weights = []\n",
    "n_ELBO_samples=5\n",
    "j = 0\n",
    "std_steps = 100\n",
    "stop = False\n",
    "while stop == False:\n",
    "#for j in range(num_epoch):\n",
    "    logs = {}\n",
    "    losses = [None] * num_iterations\n",
    "    \n",
    "    for k in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        #if new_comp_this_epoch:\n",
    "            #print('before grad', model.learnable_proba)\n",
    "        loss = model.compute_mixture_elbo(x_data, y_data, n_ELBO_samples)\n",
    "        losses[k] = loss.detach()\n",
    "        loss.backward()\n",
    "        #if new_comp_this_epoch:\n",
    "        #    print('after grad', model.current_proba_parameter)\n",
    "        #print(model.components[0].linear1.weight_sample.grad)\n",
    "        gradients = torch.sum(model.learnable_component.linear1.q_weight_mu.detach()**2)\n",
    "        weights.append(gradients)\n",
    "        \n",
    "        optimizer.step()\n",
    "        model.refresh_current_hyper_parameters()\n",
    "        #print(model.current_proba_parameter)\n",
    "    if j>std_steps and model.current_nComponents < model.nComponents:\n",
    "#        new_comp = model.new_component(None, epsilon = (optimizer.param_groups[0]['lr'] != learning_rate), new_pi = np.log(1./(4*model.nComponents+3)))\n",
    "        new_comp = model.new_component(None, epsilon = (torch.std(torch.stack(weights[-std_steps:])) < .1), new_pi = 0.)#np.log(1./(4*model.nComponents+3)))\n",
    "\n",
    "        #print('STOP', stop)\n",
    "        if new_comp:\n",
    "            model.current_nComponents += 1\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)\n",
    "            optimizer.add_param_group({\"params\": model.learnable_proba, 'lr': .005})\n",
    "            #break\n",
    "            #M+=10\n",
    "    #if model.nComponents ==3 and (optimizer.param_groups[0]['lr'] != learning_rate):\n",
    "    #    model.learnable_component.lock_means()\n",
    "    #    model.learnable_component.lock_rhos()\n",
    "    #    model.fixed_components.append(model.learnable_component)\n",
    "    #    pi = model.learnable_proba.detach()\n",
    "    #    model.fixed_mixture_probas = torch.cat(((1-pi)*model.fixed_mixture_probas, pi.unsqueeze(0)))\n",
    "    #    break\n",
    "\n",
    "        #print(model.current_proba_parameter)\n",
    "    #print(model.mixture_probas)\n",
    "    #print(torch.std(torch.stack(losses[k-4:k])))\n",
    "    def f(x):\n",
    "        if type(x) == type(None):\n",
    "            return(0.)\n",
    "        else:\n",
    "            return(sigmoid(x))\n",
    "    #f = lambda x: {type(None): 1}.get(type(x), sigmoid(x))\n",
    "    logs['expected_loss'] = torch.stack(losses).mean().detach().clone().numpy()\n",
    "    logs['learning rate'] = optimizer.param_groups[0]['lr']\n",
    "    logs['ncomponents'] = len(model.fixed_components) + 1\n",
    "    logs['current_proba'] = f(model.learnable_proba)\n",
    "    logs['gradients_weights'] = gradients\n",
    "    #iilogs['proba_gradients'] = prob_grad\n",
    "    #if type(model.current_proba_parameter) != type(None):\n",
    "    #    logs['current_proba'] = model.current_proba_parameter.detach().data\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    #M = int(.005/lr)+1\n",
    "    #M=5\n",
    "    #print(M)\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    "    if j > std_steps:\n",
    "         weights = weights[-std_steps:]\n",
    "    #print('epoch', j, 'num_components', len(model.components), 'stop', stop)\n",
    "    \n",
    "    scheduler.step(logs['expected_loss'])\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
