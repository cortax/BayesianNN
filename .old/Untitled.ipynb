{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from Tools.NNtools import *\n",
    "import tempfile\n",
    "import mlflow\n",
    "import Experiments.Foong_L1W50.setup as exp\n",
    "from Inference.Variational import MeanFieldVariationalDistribution\n",
    "import argparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter=100000\n",
    "learning_rate=0.01\n",
    "min_lr=0.0005\n",
    "n_ELBO_samples=1000 \n",
    "patience=100\n",
    "lr_decay=0.9\n",
    "init_std=0.01\n",
    "optimize=0\n",
    "expansion=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = exp.get_training_data(device)\n",
    "x_validation, y_validation = exp.get_validation_data(device)\n",
    "x_test, y_test = exp.get_test_data(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior = exp.get_logprior_fn(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "logposterior = exp.get_logposterior_fn(device)\n",
    "x_train, y_train = exp.get_training_data(device)\n",
    "x_validation, y_validation = exp.get_validation_data(device)\n",
    "x_test, y_test = exp.get_test_data(device)\n",
    "logtarget = lambda theta: logposterior(theta, x_train, y_train, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(1,50).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, nblayers, layerwidth):\n",
    "        super(MLP, self).__init__()\n",
    "        L = [1] + [layerwidth]*nblayers + [1]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for k in range(len(L)-1):\n",
    "            self.layers.append(nn.Linear(L[k], L[k+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for j in range(len(self.layers)-1):\n",
    "            x = torch.tanh(self.layers[j](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = torch.tensor(init_std)\n",
    "theta = torch.nn.Parameter(torch.empty([1, exp.param_count], device=device).normal_(std=std), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100000], Training Loss: -20.643386840820312, Learning Rate: 0.05\n",
      "Epoch [1/100000], Training Loss: 480.46600341796875, Learning Rate: 0.05\n",
      "Epoch [2/100000], Training Loss: 601.5391845703125, Learning Rate: 0.05\n",
      "Epoch [3/100000], Training Loss: 182.64456176757812, Learning Rate: 0.05\n",
      "Epoch [4/100000], Training Loss: 37.86041259765625, Learning Rate: 0.05\n",
      "Epoch [5/100000], Training Loss: 236.24935913085938, Learning Rate: 0.05\n",
      "Epoch [6/100000], Training Loss: 128.4328155517578, Learning Rate: 0.05\n",
      "Epoch [7/100000], Training Loss: -2.4320220947265625, Learning Rate: 0.05\n",
      "Epoch [8/100000], Training Loss: 57.212867736816406, Learning Rate: 0.05\n",
      "Epoch [9/100000], Training Loss: 149.36746215820312, Learning Rate: 0.05\n",
      "Epoch [10/100000], Training Loss: 97.75018310546875, Learning Rate: 0.05\n",
      "Epoch [11/100000], Training Loss: -1.681396484375, Learning Rate: 0.05\n",
      "Epoch [12/100000], Training Loss: -5.9177703857421875, Learning Rate: 0.05\n",
      "Epoch [13/100000], Training Loss: 62.38002014160156, Learning Rate: 0.05\n",
      "Epoch [14/100000], Training Loss: 78.9662857055664, Learning Rate: 0.05\n",
      "Epoch [15/100000], Training Loss: 24.685379028320312, Learning Rate: 0.05\n",
      "Epoch [16/100000], Training Loss: -17.934494018554688, Learning Rate: 0.05\n",
      "Epoch [17/100000], Training Loss: -1.3894805908203125, Learning Rate: 0.05\n",
      "Epoch [18/100000], Training Loss: 35.27674865722656, Learning Rate: 0.05\n",
      "Epoch [19/100000], Training Loss: 33.60662078857422, Learning Rate: 0.05\n",
      "Epoch [20/100000], Training Loss: -1.429840087890625, Learning Rate: 0.05\n",
      "Epoch [21/100000], Training Loss: -20.074462890625, Learning Rate: 0.05\n",
      "Epoch [22/100000], Training Loss: -4.066925048828125, Learning Rate: 0.05\n",
      "Epoch [23/100000], Training Loss: 15.475509643554688, Learning Rate: 0.05\n",
      "Epoch [24/100000], Training Loss: 9.37396240234375, Learning Rate: 0.05\n",
      "Epoch [25/100000], Training Loss: -11.736434936523438, Learning Rate: 0.05\n",
      "Epoch [26/100000], Training Loss: -19.901641845703125, Learning Rate: 0.05\n",
      "Epoch [27/100000], Training Loss: -9.24945068359375, Learning Rate: 0.05\n",
      "Epoch [28/100000], Training Loss: 1.152984619140625, Learning Rate: 0.05\n",
      "Epoch [29/100000], Training Loss: -3.49932861328125, Learning Rate: 0.05\n",
      "Epoch [30/100000], Training Loss: -15.968978881835938, Learning Rate: 0.05\n",
      "Epoch [31/100000], Training Loss: -20.145034790039062, Learning Rate: 0.05\n",
      "Epoch [32/100000], Training Loss: -13.09893798828125, Learning Rate: 0.05\n",
      "Epoch [33/100000], Training Loss: -7.288970947265625, Learning Rate: 0.05\n",
      "Epoch [34/100000], Training Loss: -11.199020385742188, Learning Rate: 0.05\n",
      "Epoch [35/100000], Training Loss: -18.534576416015625, Learning Rate: 0.05\n",
      "Epoch [36/100000], Training Loss: -19.927169799804688, Learning Rate: 0.05\n",
      "Epoch [37/100000], Training Loss: -15.472427368164062, Learning Rate: 0.05\n",
      "Epoch [38/100000], Training Loss: -12.581451416015625, Learning Rate: 0.05\n",
      "Epoch [39/100000], Training Loss: -15.482833862304688, Learning Rate: 0.05\n",
      "Epoch [40/100000], Training Loss: -19.927703857421875, Learning Rate: 0.05\n",
      "Epoch [41/100000], Training Loss: -20.009048461914062, Learning Rate: 0.05\n",
      "Epoch [42/100000], Training Loss: -16.86529541015625, Learning Rate: 0.05\n",
      "Epoch [43/100000], Training Loss: -15.867218017578125, Learning Rate: 0.05\n",
      "Epoch [44/100000], Training Loss: -18.28106689453125, Learning Rate: 0.05\n",
      "Epoch [45/100000], Training Loss: -20.5086669921875, Learning Rate: 0.05\n",
      "Epoch [46/100000], Training Loss: -19.84588623046875, Learning Rate: 0.05\n",
      "Epoch [47/100000], Training Loss: -17.926544189453125, Learning Rate: 0.05\n",
      "Epoch [48/100000], Training Loss: -17.939468383789062, Learning Rate: 0.05\n",
      "Epoch [49/100000], Training Loss: -19.794815063476562, Learning Rate: 0.05\n",
      "Epoch [50/100000], Training Loss: -20.653839111328125, Learning Rate: 0.05\n",
      "Epoch [51/100000], Training Loss: -19.653472900390625, Learning Rate: 0.05\n",
      "Epoch [52/100000], Training Loss: -18.774917602539062, Learning Rate: 0.05\n",
      "Epoch [53/100000], Training Loss: -19.388412475585938, Learning Rate: 0.05\n",
      "Epoch [54/100000], Training Loss: -20.49249267578125, Learning Rate: 0.05\n",
      "Epoch [55/100000], Training Loss: -20.512893676757812, Learning Rate: 0.05\n",
      "Epoch [56/100000], Training Loss: -19.696868896484375, Learning Rate: 0.05\n",
      "Epoch [57/100000], Training Loss: -19.553298950195312, Learning Rate: 0.05\n",
      "Epoch [58/100000], Training Loss: -20.282363891601562, Learning Rate: 0.05\n",
      "Epoch [59/100000], Training Loss: -20.694427490234375, Learning Rate: 0.05\n",
      "Epoch [60/100000], Training Loss: -20.324996948242188, Learning Rate: 0.05\n",
      "Epoch [61/100000], Training Loss: -19.94775390625, Learning Rate: 0.05\n",
      "Epoch [62/100000], Training Loss: -20.209426879882812, Learning Rate: 0.05\n",
      "Epoch [63/100000], Training Loss: -20.6685791015625, Learning Rate: 0.05\n",
      "Epoch [64/100000], Training Loss: -20.613357543945312, Learning Rate: 0.05\n",
      "Epoch [65/100000], Training Loss: -20.2755126953125, Learning Rate: 0.05\n",
      "Epoch [66/100000], Training Loss: -20.295181274414062, Learning Rate: 0.05\n",
      "Epoch [67/100000], Training Loss: -20.607589721679688, Learning Rate: 0.05\n",
      "Epoch [68/100000], Training Loss: -20.706390380859375, Learning Rate: 0.05\n",
      "Epoch [69/100000], Training Loss: -20.505950927734375, Learning Rate: 0.05\n",
      "Epoch [70/100000], Training Loss: -20.411453247070312, Learning Rate: 0.05\n",
      "Epoch [71/100000], Training Loss: -20.596603393554688, Learning Rate: 0.05\n",
      "Epoch [72/100000], Training Loss: -20.730712890625, Learning Rate: 0.05\n",
      "Epoch [73/100000], Training Loss: -20.627166748046875, Learning Rate: 0.05\n",
      "Epoch [74/100000], Training Loss: -20.52838134765625, Learning Rate: 0.05\n",
      "Epoch [75/100000], Training Loss: -20.614608764648438, Learning Rate: 0.05\n",
      "Epoch [76/100000], Training Loss: -20.731536865234375, Learning Rate: 0.05\n",
      "Epoch [77/100000], Training Loss: -20.694290161132812, Learning Rate: 0.05\n",
      "Epoch [78/100000], Training Loss: -20.607803344726562, Learning Rate: 0.05\n",
      "Epoch [79/100000], Training Loss: -20.647369384765625, Learning Rate: 0.05\n",
      "Epoch [80/100000], Training Loss: -20.733810424804688, Learning Rate: 0.05\n",
      "Epoch [81/100000], Training Loss: -20.72357177734375, Learning Rate: 0.05\n",
      "Epoch [82/100000], Training Loss: -20.665008544921875, Learning Rate: 0.05\n",
      "Epoch [83/100000], Training Loss: -20.677841186523438, Learning Rate: 0.05\n",
      "Epoch [84/100000], Training Loss: -20.739486694335938, Learning Rate: 0.05\n",
      "Epoch [85/100000], Training Loss: -20.741302490234375, Learning Rate: 0.05\n",
      "Epoch [86/100000], Training Loss: -20.699981689453125, Learning Rate: 0.05\n",
      "Epoch [87/100000], Training Loss: -20.70635986328125, Learning Rate: 0.05\n",
      "Epoch [88/100000], Training Loss: -20.746978759765625, Learning Rate: 0.05\n",
      "Epoch [89/100000], Training Loss: -20.751358032226562, Learning Rate: 0.05\n",
      "Epoch [90/100000], Training Loss: -20.7244873046875, Learning Rate: 0.05\n",
      "Epoch [91/100000], Training Loss: -20.726913452148438, Learning Rate: 0.05\n",
      "Epoch [92/100000], Training Loss: -20.755661010742188, Learning Rate: 0.05\n",
      "Epoch [93/100000], Training Loss: -20.758193969726562, Learning Rate: 0.05\n",
      "Epoch [94/100000], Training Loss: -20.740707397460938, Learning Rate: 0.05\n",
      "Epoch [95/100000], Training Loss: -20.743865966796875, Learning Rate: 0.05\n",
      "Epoch [96/100000], Training Loss: -20.762893676757812, Learning Rate: 0.05\n",
      "Epoch [97/100000], Training Loss: -20.764617919921875, Learning Rate: 0.05\n",
      "Epoch [98/100000], Training Loss: -20.75299072265625, Learning Rate: 0.05\n",
      "Epoch [99/100000], Training Loss: -20.757110595703125, Learning Rate: 0.05\n",
      "Epoch [100/100000], Training Loss: -20.770355224609375, Learning Rate: 0.05\n",
      "Epoch [101/100000], Training Loss: -20.770050048828125, Learning Rate: 0.05\n",
      "Epoch [102/100000], Training Loss: -20.763351440429688, Learning Rate: 0.05\n",
      "Epoch [103/100000], Training Loss: -20.767730712890625, Learning Rate: 0.05\n",
      "Epoch [104/100000], Training Loss: -20.776718139648438, Learning Rate: 0.05\n",
      "Epoch [105/100000], Training Loss: -20.775527954101562, Learning Rate: 0.05\n",
      "Epoch [106/100000], Training Loss: -20.77178955078125, Learning Rate: 0.05\n",
      "Epoch [107/100000], Training Loss: -20.776641845703125, Learning Rate: 0.05\n",
      "Epoch [108/100000], Training Loss: -20.78216552734375, Learning Rate: 0.05\n",
      "Epoch [109/100000], Training Loss: -20.780868530273438, Learning Rate: 0.05\n",
      "Epoch [110/100000], Training Loss: -20.779510498046875, Learning Rate: 0.05\n",
      "Epoch [111/100000], Training Loss: -20.784027099609375, Learning Rate: 0.05\n",
      "Epoch [112/100000], Training Loss: -20.787445068359375, Learning Rate: 0.05\n",
      "Epoch [113/100000], Training Loss: -20.786178588867188, Learning Rate: 0.05\n",
      "Epoch [114/100000], Training Loss: -20.786666870117188, Learning Rate: 0.05\n",
      "Epoch [115/100000], Training Loss: -20.790481567382812, Learning Rate: 0.05\n",
      "Epoch [116/100000], Training Loss: -20.792327880859375, Learning Rate: 0.05\n",
      "Epoch [117/100000], Training Loss: -20.791702270507812, Learning Rate: 0.05\n",
      "Epoch [118/100000], Training Loss: -20.793197631835938, Learning Rate: 0.05\n",
      "Epoch [119/100000], Training Loss: -20.796249389648438, Learning Rate: 0.05\n",
      "Epoch [120/100000], Training Loss: -20.797149658203125, Learning Rate: 0.05\n",
      "Epoch [121/100000], Training Loss: -20.797317504882812, Learning Rate: 0.05\n",
      "Epoch [122/100000], Training Loss: -20.79931640625, Learning Rate: 0.05\n",
      "Epoch [123/100000], Training Loss: -20.801528930664062, Learning Rate: 0.05\n",
      "Epoch [124/100000], Training Loss: -20.802093505859375, Learning Rate: 0.05\n",
      "Epoch [125/100000], Training Loss: -20.80291748046875, Learning Rate: 0.05\n",
      "Epoch [126/100000], Training Loss: -20.804962158203125, Learning Rate: 0.05\n",
      "Epoch [127/100000], Training Loss: -20.80645751953125, Learning Rate: 0.05\n",
      "Epoch [128/100000], Training Loss: -20.807159423828125, Learning Rate: 0.05\n",
      "Epoch [129/100000], Training Loss: -20.808441162109375, Learning Rate: 0.05\n",
      "Epoch [130/100000], Training Loss: -20.81024169921875, Learning Rate: 0.05\n",
      "Epoch [131/100000], Training Loss: -20.811355590820312, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [132/100000], Training Loss: -20.812240600585938, Learning Rate: 0.05\n",
      "Epoch [133/100000], Training Loss: -20.813796997070312, Learning Rate: 0.05\n",
      "Epoch [134/100000], Training Loss: -20.815277099609375, Learning Rate: 0.05\n",
      "Epoch [135/100000], Training Loss: -20.816238403320312, Learning Rate: 0.05\n",
      "Epoch [136/100000], Training Loss: -20.817398071289062, Learning Rate: 0.05\n",
      "Epoch [137/100000], Training Loss: -20.818939208984375, Learning Rate: 0.05\n",
      "Epoch [138/100000], Training Loss: -20.82012939453125, Learning Rate: 0.05\n",
      "Epoch [139/100000], Training Loss: -20.8211669921875, Learning Rate: 0.05\n",
      "Epoch [140/100000], Training Loss: -20.822494506835938, Learning Rate: 0.05\n",
      "Epoch [141/100000], Training Loss: -20.823898315429688, Learning Rate: 0.05\n",
      "Epoch [142/100000], Training Loss: -20.824981689453125, Learning Rate: 0.05\n",
      "Epoch [143/100000], Training Loss: -20.826141357421875, Learning Rate: 0.05\n",
      "Epoch [144/100000], Training Loss: -20.8275146484375, Learning Rate: 0.05\n",
      "Epoch [145/100000], Training Loss: -20.828689575195312, Learning Rate: 0.05\n",
      "Epoch [146/100000], Training Loss: -20.829849243164062, Learning Rate: 0.05\n",
      "Epoch [147/100000], Training Loss: -20.831100463867188, Learning Rate: 0.05\n",
      "Epoch [148/100000], Training Loss: -20.83233642578125, Learning Rate: 0.05\n",
      "Epoch [149/100000], Training Loss: -20.833511352539062, Learning Rate: 0.05\n",
      "Epoch [150/100000], Training Loss: -20.83465576171875, Learning Rate: 0.05\n",
      "Epoch [151/100000], Training Loss: -20.8359375, Learning Rate: 0.05\n",
      "Epoch [152/100000], Training Loss: -20.837142944335938, Learning Rate: 0.05\n",
      "Epoch [153/100000], Training Loss: -20.838272094726562, Learning Rate: 0.05\n",
      "Epoch [154/100000], Training Loss: -20.839492797851562, Learning Rate: 0.05\n",
      "Epoch [155/100000], Training Loss: -20.840682983398438, Learning Rate: 0.05\n",
      "Epoch [156/100000], Training Loss: -20.841827392578125, Learning Rate: 0.05\n",
      "Epoch [157/100000], Training Loss: -20.84295654296875, Learning Rate: 0.05\n",
      "Epoch [158/100000], Training Loss: -20.844161987304688, Learning Rate: 0.05\n",
      "Epoch [159/100000], Training Loss: -20.8453369140625, Learning Rate: 0.05\n",
      "Epoch [160/100000], Training Loss: -20.846527099609375, Learning Rate: 0.05\n",
      "Epoch [161/100000], Training Loss: -20.847686767578125, Learning Rate: 0.05\n",
      "Epoch [162/100000], Training Loss: -20.848831176757812, Learning Rate: 0.05\n",
      "Epoch [163/100000], Training Loss: -20.849945068359375, Learning Rate: 0.05\n",
      "Epoch [164/100000], Training Loss: -20.85107421875, Learning Rate: 0.05\n",
      "Epoch [165/100000], Training Loss: -20.852294921875, Learning Rate: 0.05\n",
      "Epoch [166/100000], Training Loss: -20.853408813476562, Learning Rate: 0.05\n",
      "Epoch [167/100000], Training Loss: -20.854568481445312, Learning Rate: 0.05\n",
      "Epoch [168/100000], Training Loss: -20.855697631835938, Learning Rate: 0.05\n",
      "Epoch [169/100000], Training Loss: -20.856796264648438, Learning Rate: 0.05\n",
      "Epoch [170/100000], Training Loss: -20.85791015625, Learning Rate: 0.05\n",
      "Epoch [171/100000], Training Loss: -20.859039306640625, Learning Rate: 0.05\n",
      "Epoch [172/100000], Training Loss: -20.860198974609375, Learning Rate: 0.05\n",
      "Epoch [173/100000], Training Loss: -20.861328125, Learning Rate: 0.05\n",
      "Epoch [174/100000], Training Loss: -20.862411499023438, Learning Rate: 0.05\n",
      "Epoch [175/100000], Training Loss: -20.863525390625, Learning Rate: 0.05\n",
      "Epoch [176/100000], Training Loss: -20.8646240234375, Learning Rate: 0.05\n",
      "Epoch [177/100000], Training Loss: -20.865753173828125, Learning Rate: 0.05\n",
      "Epoch [178/100000], Training Loss: -20.866836547851562, Learning Rate: 0.05\n",
      "Epoch [179/100000], Training Loss: -20.867904663085938, Learning Rate: 0.05\n",
      "Epoch [180/100000], Training Loss: -20.8690185546875, Learning Rate: 0.05\n",
      "Epoch [181/100000], Training Loss: -20.870071411132812, Learning Rate: 0.05\n",
      "Epoch [182/100000], Training Loss: -20.871185302734375, Learning Rate: 0.05\n",
      "Epoch [183/100000], Training Loss: -20.872268676757812, Learning Rate: 0.05\n",
      "Epoch [184/100000], Training Loss: -20.873336791992188, Learning Rate: 0.05\n",
      "Epoch [185/100000], Training Loss: -20.874404907226562, Learning Rate: 0.05\n",
      "Epoch [186/100000], Training Loss: -20.87548828125, Learning Rate: 0.05\n",
      "Epoch [187/100000], Training Loss: -20.876510620117188, Learning Rate: 0.05\n",
      "Epoch [188/100000], Training Loss: -20.8775634765625, Learning Rate: 0.05\n",
      "Epoch [189/100000], Training Loss: -20.878616333007812, Learning Rate: 0.05\n",
      "Epoch [190/100000], Training Loss: -20.879714965820312, Learning Rate: 0.05\n",
      "Epoch [191/100000], Training Loss: -20.880722045898438, Learning Rate: 0.05\n",
      "Epoch [192/100000], Training Loss: -20.881805419921875, Learning Rate: 0.05\n",
      "Epoch [193/100000], Training Loss: -20.882797241210938, Learning Rate: 0.05\n",
      "Epoch [194/100000], Training Loss: -20.88385009765625, Learning Rate: 0.05\n",
      "Epoch [195/100000], Training Loss: -20.884902954101562, Learning Rate: 0.05\n",
      "Epoch [196/100000], Training Loss: -20.885894775390625, Learning Rate: 0.05\n",
      "Epoch [197/100000], Training Loss: -20.88690185546875, Learning Rate: 0.05\n",
      "Epoch [198/100000], Training Loss: -20.887985229492188, Learning Rate: 0.05\n",
      "Epoch [199/100000], Training Loss: -20.888946533203125, Learning Rate: 0.05\n",
      "Epoch [200/100000], Training Loss: -20.889999389648438, Learning Rate: 0.05\n",
      "Epoch [201/100000], Training Loss: -20.890975952148438, Learning Rate: 0.05\n",
      "Epoch [202/100000], Training Loss: -20.891998291015625, Learning Rate: 0.05\n",
      "Epoch [203/100000], Training Loss: -20.892990112304688, Learning Rate: 0.05\n",
      "Epoch [204/100000], Training Loss: -20.893966674804688, Learning Rate: 0.05\n",
      "Epoch [205/100000], Training Loss: -20.894989013671875, Learning Rate: 0.05\n",
      "Epoch [206/100000], Training Loss: -20.895919799804688, Learning Rate: 0.05\n",
      "Epoch [207/100000], Training Loss: -20.89691162109375, Learning Rate: 0.05\n",
      "Epoch [208/100000], Training Loss: -20.897903442382812, Learning Rate: 0.05\n",
      "Epoch [209/100000], Training Loss: -20.898849487304688, Learning Rate: 0.05\n",
      "Epoch [210/100000], Training Loss: -20.89984130859375, Learning Rate: 0.05\n",
      "Epoch [211/100000], Training Loss: -20.90081787109375, Learning Rate: 0.05\n",
      "Epoch [212/100000], Training Loss: -20.9017333984375, Learning Rate: 0.05\n",
      "Epoch [213/100000], Training Loss: -20.9027099609375, Learning Rate: 0.05\n",
      "Epoch [214/100000], Training Loss: -20.903717041015625, Learning Rate: 0.05\n",
      "Epoch [215/100000], Training Loss: -20.904632568359375, Learning Rate: 0.05\n",
      "Epoch [216/100000], Training Loss: -20.90557861328125, Learning Rate: 0.05\n",
      "Epoch [217/100000], Training Loss: -20.906524658203125, Learning Rate: 0.05\n",
      "Epoch [218/100000], Training Loss: -20.907424926757812, Learning Rate: 0.05\n",
      "Epoch [219/100000], Training Loss: -20.908401489257812, Learning Rate: 0.05\n",
      "Epoch [220/100000], Training Loss: -20.909286499023438, Learning Rate: 0.05\n",
      "Epoch [221/100000], Training Loss: -20.910232543945312, Learning Rate: 0.05\n",
      "Epoch [222/100000], Training Loss: -20.9111328125, Learning Rate: 0.05\n",
      "Epoch [223/100000], Training Loss: -20.912078857421875, Learning Rate: 0.05\n",
      "Epoch [224/100000], Training Loss: -20.913009643554688, Learning Rate: 0.05\n",
      "Epoch [225/100000], Training Loss: -20.9139404296875, Learning Rate: 0.05\n",
      "Epoch [226/100000], Training Loss: -20.914810180664062, Learning Rate: 0.05\n",
      "Epoch [227/100000], Training Loss: -20.91571044921875, Learning Rate: 0.05\n",
      "Epoch [228/100000], Training Loss: -20.9166259765625, Learning Rate: 0.05\n",
      "Epoch [229/100000], Training Loss: -20.917510986328125, Learning Rate: 0.05\n",
      "Epoch [230/100000], Training Loss: -20.918350219726562, Learning Rate: 0.05\n",
      "Epoch [231/100000], Training Loss: -20.91925048828125, Learning Rate: 0.05\n",
      "Epoch [232/100000], Training Loss: -20.920196533203125, Learning Rate: 0.05\n",
      "Epoch [233/100000], Training Loss: -20.921035766601562, Learning Rate: 0.05\n",
      "Epoch [234/100000], Training Loss: -20.921951293945312, Learning Rate: 0.05\n",
      "Epoch [235/100000], Training Loss: -20.922805786132812, Learning Rate: 0.05\n",
      "Epoch [236/100000], Training Loss: -20.923614501953125, Learning Rate: 0.05\n",
      "Epoch [237/100000], Training Loss: -20.924484252929688, Learning Rate: 0.05\n",
      "Epoch [238/100000], Training Loss: -20.925369262695312, Learning Rate: 0.05\n",
      "Epoch [239/100000], Training Loss: -20.926223754882812, Learning Rate: 0.05\n",
      "Epoch [240/100000], Training Loss: -20.927093505859375, Learning Rate: 0.05\n",
      "Epoch [241/100000], Training Loss: -20.92791748046875, Learning Rate: 0.05\n",
      "Epoch [242/100000], Training Loss: -20.92877197265625, Learning Rate: 0.05\n",
      "Epoch [243/100000], Training Loss: -20.929641723632812, Learning Rate: 0.05\n",
      "Epoch [244/100000], Training Loss: -20.930450439453125, Learning Rate: 0.05\n",
      "Epoch [245/100000], Training Loss: -20.9312744140625, Learning Rate: 0.05\n",
      "Epoch [246/100000], Training Loss: -20.932098388671875, Learning Rate: 0.05\n",
      "Epoch [247/100000], Training Loss: -20.932952880859375, Learning Rate: 0.05\n",
      "Epoch [248/100000], Training Loss: -20.9337158203125, Learning Rate: 0.05\n",
      "Epoch [249/100000], Training Loss: -20.934585571289062, Learning Rate: 0.05\n",
      "Epoch [250/100000], Training Loss: -20.935409545898438, Learning Rate: 0.05\n",
      "Epoch [251/100000], Training Loss: -20.936187744140625, Learning Rate: 0.05\n",
      "Epoch [252/100000], Training Loss: -20.937026977539062, Learning Rate: 0.05\n",
      "Epoch [253/100000], Training Loss: -20.937835693359375, Learning Rate: 0.05\n",
      "Epoch [254/100000], Training Loss: -20.938613891601562, Learning Rate: 0.05\n",
      "Epoch [255/100000], Training Loss: -20.939422607421875, Learning Rate: 0.05\n",
      "Epoch [256/100000], Training Loss: -20.940216064453125, Learning Rate: 0.05\n",
      "Epoch [257/100000], Training Loss: -20.941009521484375, Learning Rate: 0.05\n",
      "Epoch [258/100000], Training Loss: -20.941818237304688, Learning Rate: 0.05\n",
      "Epoch [259/100000], Training Loss: -20.942581176757812, Learning Rate: 0.05\n",
      "Epoch [260/100000], Training Loss: -20.943359375, Learning Rate: 0.05\n",
      "Epoch [261/100000], Training Loss: -20.944137573242188, Learning Rate: 0.05\n",
      "Epoch [262/100000], Training Loss: -20.944900512695312, Learning Rate: 0.05\n",
      "Epoch [263/100000], Training Loss: -20.945709228515625, Learning Rate: 0.05\n",
      "Epoch [264/100000], Training Loss: -20.946441650390625, Learning Rate: 0.05\n",
      "Epoch [265/100000], Training Loss: -20.947235107421875, Learning Rate: 0.05\n",
      "Epoch [266/100000], Training Loss: -20.947967529296875, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [267/100000], Training Loss: -20.948745727539062, Learning Rate: 0.05\n",
      "Epoch [268/100000], Training Loss: -20.949493408203125, Learning Rate: 0.05\n",
      "Epoch [269/100000], Training Loss: -20.950210571289062, Learning Rate: 0.05\n",
      "Epoch [270/100000], Training Loss: -20.951004028320312, Learning Rate: 0.05\n",
      "Epoch [271/100000], Training Loss: -20.951766967773438, Learning Rate: 0.05\n",
      "Epoch [272/100000], Training Loss: -20.952484130859375, Learning Rate: 0.05\n",
      "Epoch [273/100000], Training Loss: -20.953201293945312, Learning Rate: 0.05\n",
      "Epoch [274/100000], Training Loss: -20.953933715820312, Learning Rate: 0.05\n",
      "Epoch [275/100000], Training Loss: -20.9547119140625, Learning Rate: 0.05\n",
      "Epoch [276/100000], Training Loss: -20.9554443359375, Learning Rate: 0.05\n",
      "Epoch [277/100000], Training Loss: -20.956161499023438, Learning Rate: 0.05\n",
      "Epoch [278/100000], Training Loss: -20.956863403320312, Learning Rate: 0.05\n",
      "Epoch [279/100000], Training Loss: -20.95758056640625, Learning Rate: 0.05\n",
      "Epoch [280/100000], Training Loss: -20.958282470703125, Learning Rate: 0.05\n",
      "Epoch [281/100000], Training Loss: -20.959060668945312, Learning Rate: 0.05\n",
      "Epoch [282/100000], Training Loss: -20.959762573242188, Learning Rate: 0.05\n",
      "Epoch [283/100000], Training Loss: -20.960494995117188, Learning Rate: 0.05\n",
      "Epoch [284/100000], Training Loss: -20.961166381835938, Learning Rate: 0.05\n",
      "Epoch [285/100000], Training Loss: -20.961822509765625, Learning Rate: 0.05\n",
      "Epoch [286/100000], Training Loss: -20.96258544921875, Learning Rate: 0.05\n",
      "Epoch [287/100000], Training Loss: -20.963241577148438, Learning Rate: 0.05\n",
      "Epoch [288/100000], Training Loss: -20.963958740234375, Learning Rate: 0.05\n",
      "Epoch [289/100000], Training Loss: -20.964630126953125, Learning Rate: 0.05\n",
      "Epoch [290/100000], Training Loss: -20.965347290039062, Learning Rate: 0.05\n",
      "Epoch [291/100000], Training Loss: -20.966033935546875, Learning Rate: 0.05\n",
      "Epoch [292/100000], Training Loss: -20.966690063476562, Learning Rate: 0.05\n",
      "Epoch [293/100000], Training Loss: -20.967376708984375, Learning Rate: 0.05\n",
      "Epoch [294/100000], Training Loss: -20.96807861328125, Learning Rate: 0.05\n",
      "Epoch [295/100000], Training Loss: -20.968719482421875, Learning Rate: 0.05\n",
      "Epoch [296/100000], Training Loss: -20.969451904296875, Learning Rate: 0.05\n",
      "Epoch [297/100000], Training Loss: -20.970077514648438, Learning Rate: 0.05\n",
      "Epoch [298/100000], Training Loss: -20.970733642578125, Learning Rate: 0.05\n",
      "Epoch [299/100000], Training Loss: -20.971435546875, Learning Rate: 0.05\n",
      "Epoch [300/100000], Training Loss: -20.972076416015625, Learning Rate: 0.05\n",
      "Epoch [301/100000], Training Loss: -20.972732543945312, Learning Rate: 0.05\n",
      "Epoch [302/100000], Training Loss: -20.973419189453125, Learning Rate: 0.05\n",
      "Epoch [303/100000], Training Loss: -20.974090576171875, Learning Rate: 0.05\n",
      "Epoch [304/100000], Training Loss: -20.974716186523438, Learning Rate: 0.05\n",
      "Epoch [305/100000], Training Loss: -20.975357055664062, Learning Rate: 0.05\n",
      "Epoch [306/100000], Training Loss: -20.975997924804688, Learning Rate: 0.05\n",
      "Epoch [307/100000], Training Loss: -20.976638793945312, Learning Rate: 0.05\n",
      "Epoch [308/100000], Training Loss: -20.977325439453125, Learning Rate: 0.05\n",
      "Epoch [309/100000], Training Loss: -20.977920532226562, Learning Rate: 0.05\n",
      "Epoch [310/100000], Training Loss: -20.978607177734375, Learning Rate: 0.05\n",
      "Epoch [311/100000], Training Loss: -20.979217529296875, Learning Rate: 0.05\n",
      "Epoch [312/100000], Training Loss: -20.979873657226562, Learning Rate: 0.05\n",
      "Epoch [313/100000], Training Loss: -20.980499267578125, Learning Rate: 0.05\n",
      "Epoch [314/100000], Training Loss: -20.981109619140625, Learning Rate: 0.05\n",
      "Epoch [315/100000], Training Loss: -20.98175048828125, Learning Rate: 0.05\n",
      "Epoch [316/100000], Training Loss: -20.982391357421875, Learning Rate: 0.05\n",
      "Epoch [317/100000], Training Loss: -20.983001708984375, Learning Rate: 0.05\n",
      "Epoch [318/100000], Training Loss: -20.983627319335938, Learning Rate: 0.05\n",
      "Epoch [319/100000], Training Loss: -20.984207153320312, Learning Rate: 0.05\n",
      "Epoch [320/100000], Training Loss: -20.98486328125, Learning Rate: 0.05\n",
      "Epoch [321/100000], Training Loss: -20.985504150390625, Learning Rate: 0.05\n",
      "Epoch [322/100000], Training Loss: -20.986083984375, Learning Rate: 0.05\n",
      "Epoch [323/100000], Training Loss: -20.986724853515625, Learning Rate: 0.05\n",
      "Epoch [324/100000], Training Loss: -20.987335205078125, Learning Rate: 0.05\n",
      "Epoch [325/100000], Training Loss: -20.9879150390625, Learning Rate: 0.05\n",
      "Epoch [326/100000], Training Loss: -20.988540649414062, Learning Rate: 0.05\n",
      "Epoch [327/100000], Training Loss: -20.9891357421875, Learning Rate: 0.05\n",
      "Epoch [328/100000], Training Loss: -20.989715576171875, Learning Rate: 0.05\n",
      "Epoch [329/100000], Training Loss: -20.990325927734375, Learning Rate: 0.05\n",
      "Epoch [330/100000], Training Loss: -20.990951538085938, Learning Rate: 0.05\n",
      "Epoch [331/100000], Training Loss: -20.991500854492188, Learning Rate: 0.05\n",
      "Epoch [332/100000], Training Loss: -20.992141723632812, Learning Rate: 0.05\n",
      "Epoch [333/100000], Training Loss: -20.99273681640625, Learning Rate: 0.05\n",
      "Epoch [334/100000], Training Loss: -20.993316650390625, Learning Rate: 0.05\n",
      "Epoch [335/100000], Training Loss: -20.993865966796875, Learning Rate: 0.05\n",
      "Epoch [336/100000], Training Loss: -20.994476318359375, Learning Rate: 0.05\n",
      "Epoch [337/100000], Training Loss: -20.995025634765625, Learning Rate: 0.05\n",
      "Epoch [338/100000], Training Loss: -20.99566650390625, Learning Rate: 0.05\n",
      "Epoch [339/100000], Training Loss: -20.996231079101562, Learning Rate: 0.05\n",
      "Epoch [340/100000], Training Loss: -20.996795654296875, Learning Rate: 0.05\n",
      "Epoch [341/100000], Training Loss: -20.997390747070312, Learning Rate: 0.05\n",
      "Epoch [342/100000], Training Loss: -20.997955322265625, Learning Rate: 0.05\n",
      "Epoch [343/100000], Training Loss: -20.99853515625, Learning Rate: 0.05\n",
      "Epoch [344/100000], Training Loss: -20.999114990234375, Learning Rate: 0.05\n",
      "Epoch [345/100000], Training Loss: -20.99969482421875, Learning Rate: 0.05\n",
      "Epoch [346/100000], Training Loss: -21.000244140625, Learning Rate: 0.05\n",
      "Epoch [347/100000], Training Loss: -21.000808715820312, Learning Rate: 0.05\n",
      "Epoch [348/100000], Training Loss: -21.00140380859375, Learning Rate: 0.05\n",
      "Epoch [349/100000], Training Loss: -21.001968383789062, Learning Rate: 0.05\n",
      "Epoch [350/100000], Training Loss: -21.002517700195312, Learning Rate: 0.05\n",
      "Epoch [351/100000], Training Loss: -21.003082275390625, Learning Rate: 0.05\n",
      "Epoch [352/100000], Training Loss: -21.003646850585938, Learning Rate: 0.05\n",
      "Epoch [353/100000], Training Loss: -21.004196166992188, Learning Rate: 0.05\n",
      "Epoch [354/100000], Training Loss: -21.0047607421875, Learning Rate: 0.05\n",
      "Epoch [355/100000], Training Loss: -21.005340576171875, Learning Rate: 0.05\n",
      "Epoch [356/100000], Training Loss: -21.005905151367188, Learning Rate: 0.05\n",
      "Epoch [357/100000], Training Loss: -21.006423950195312, Learning Rate: 0.05\n",
      "Epoch [358/100000], Training Loss: -21.007003784179688, Learning Rate: 0.05\n",
      "Epoch [359/100000], Training Loss: -21.007522583007812, Learning Rate: 0.05\n",
      "Epoch [360/100000], Training Loss: -21.008102416992188, Learning Rate: 0.05\n",
      "Epoch [361/100000], Training Loss: -21.008636474609375, Learning Rate: 0.05\n",
      "Epoch [362/100000], Training Loss: -21.009170532226562, Learning Rate: 0.05\n",
      "Epoch [363/100000], Training Loss: -21.009719848632812, Learning Rate: 0.05\n",
      "Epoch [364/100000], Training Loss: -21.010284423828125, Learning Rate: 0.05\n",
      "Epoch [365/100000], Training Loss: -21.01080322265625, Learning Rate: 0.05\n",
      "Epoch [366/100000], Training Loss: -21.011322021484375, Learning Rate: 0.05\n",
      "Epoch [367/100000], Training Loss: -21.01190185546875, Learning Rate: 0.05\n",
      "Epoch [368/100000], Training Loss: -21.012435913085938, Learning Rate: 0.05\n",
      "Epoch [369/100000], Training Loss: -21.012908935546875, Learning Rate: 0.05\n",
      "Epoch [370/100000], Training Loss: -21.01348876953125, Learning Rate: 0.05\n",
      "Epoch [371/100000], Training Loss: -21.014007568359375, Learning Rate: 0.05\n",
      "Epoch [372/100000], Training Loss: -21.014556884765625, Learning Rate: 0.05\n",
      "Epoch [373/100000], Training Loss: -21.01507568359375, Learning Rate: 0.05\n",
      "Epoch [374/100000], Training Loss: -21.015609741210938, Learning Rate: 0.05\n",
      "Epoch [375/100000], Training Loss: -21.016159057617188, Learning Rate: 0.05\n",
      "Epoch [376/100000], Training Loss: -21.016677856445312, Learning Rate: 0.05\n",
      "Epoch [377/100000], Training Loss: -21.017166137695312, Learning Rate: 0.05\n",
      "Epoch [378/100000], Training Loss: -21.017715454101562, Learning Rate: 0.05\n",
      "Epoch [379/100000], Training Loss: -21.01824951171875, Learning Rate: 0.05\n",
      "Epoch [380/100000], Training Loss: -21.018798828125, Learning Rate: 0.05\n",
      "Epoch [381/100000], Training Loss: -21.019317626953125, Learning Rate: 0.05\n",
      "Epoch [382/100000], Training Loss: -21.019805908203125, Learning Rate: 0.05\n",
      "Epoch [383/100000], Training Loss: -21.020339965820312, Learning Rate: 0.05\n",
      "Epoch [384/100000], Training Loss: -21.020904541015625, Learning Rate: 0.05\n",
      "Epoch [385/100000], Training Loss: -21.021392822265625, Learning Rate: 0.05\n",
      "Epoch [386/100000], Training Loss: -21.02191162109375, Learning Rate: 0.05\n",
      "Epoch [387/100000], Training Loss: -21.022415161132812, Learning Rate: 0.05\n",
      "Epoch [388/100000], Training Loss: -21.022933959960938, Learning Rate: 0.05\n",
      "Epoch [389/100000], Training Loss: -21.0234375, Learning Rate: 0.05\n",
      "Epoch [390/100000], Training Loss: -21.023971557617188, Learning Rate: 0.05\n",
      "Epoch [391/100000], Training Loss: -21.02447509765625, Learning Rate: 0.05\n",
      "Epoch [392/100000], Training Loss: -21.02496337890625, Learning Rate: 0.05\n",
      "Epoch [393/100000], Training Loss: -21.025482177734375, Learning Rate: 0.05\n",
      "Epoch [394/100000], Training Loss: -21.026016235351562, Learning Rate: 0.05\n",
      "Epoch [395/100000], Training Loss: -21.026519775390625, Learning Rate: 0.05\n",
      "Epoch [396/100000], Training Loss: -21.02703857421875, Learning Rate: 0.05\n",
      "Epoch [397/100000], Training Loss: -21.02752685546875, Learning Rate: 0.05\n",
      "Epoch [398/100000], Training Loss: -21.02801513671875, Learning Rate: 0.05\n",
      "Epoch [399/100000], Training Loss: -21.028549194335938, Learning Rate: 0.05\n",
      "Epoch [400/100000], Training Loss: -21.029006958007812, Learning Rate: 0.05\n",
      "Epoch [401/100000], Training Loss: -21.029510498046875, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [402/100000], Training Loss: -21.030044555664062, Learning Rate: 0.05\n",
      "Epoch [403/100000], Training Loss: -21.030563354492188, Learning Rate: 0.05\n",
      "Epoch [404/100000], Training Loss: -21.031021118164062, Learning Rate: 0.05\n",
      "Epoch [405/100000], Training Loss: -21.031509399414062, Learning Rate: 0.05\n",
      "Epoch [406/100000], Training Loss: -21.03204345703125, Learning Rate: 0.05\n",
      "Epoch [407/100000], Training Loss: -21.032470703125, Learning Rate: 0.05\n",
      "Epoch [408/100000], Training Loss: -21.03302001953125, Learning Rate: 0.05\n",
      "Epoch [409/100000], Training Loss: -21.03350830078125, Learning Rate: 0.05\n",
      "Epoch [410/100000], Training Loss: -21.034027099609375, Learning Rate: 0.05\n",
      "Epoch [411/100000], Training Loss: -21.034515380859375, Learning Rate: 0.05\n",
      "Epoch [412/100000], Training Loss: -21.035018920898438, Learning Rate: 0.05\n",
      "Epoch [413/100000], Training Loss: -21.035507202148438, Learning Rate: 0.05\n",
      "Epoch [414/100000], Training Loss: -21.035980224609375, Learning Rate: 0.05\n",
      "Epoch [415/100000], Training Loss: -21.036468505859375, Learning Rate: 0.05\n",
      "Epoch [416/100000], Training Loss: -21.03692626953125, Learning Rate: 0.05\n",
      "Epoch [417/100000], Training Loss: -21.037429809570312, Learning Rate: 0.05\n",
      "Epoch [418/100000], Training Loss: -21.037948608398438, Learning Rate: 0.05\n",
      "Epoch [419/100000], Training Loss: -21.0384521484375, Learning Rate: 0.05\n",
      "Epoch [420/100000], Training Loss: -21.038925170898438, Learning Rate: 0.05\n",
      "Epoch [421/100000], Training Loss: -21.039398193359375, Learning Rate: 0.05\n",
      "Epoch [422/100000], Training Loss: -21.039886474609375, Learning Rate: 0.05\n",
      "Epoch [423/100000], Training Loss: -21.040390014648438, Learning Rate: 0.05\n",
      "Epoch [424/100000], Training Loss: -21.04083251953125, Learning Rate: 0.05\n",
      "Epoch [425/100000], Training Loss: -21.041336059570312, Learning Rate: 0.05\n",
      "Epoch [426/100000], Training Loss: -21.041839599609375, Learning Rate: 0.05\n",
      "Epoch [427/100000], Training Loss: -21.042266845703125, Learning Rate: 0.05\n",
      "Epoch [428/100000], Training Loss: -21.04278564453125, Learning Rate: 0.05\n",
      "Epoch [429/100000], Training Loss: -21.043243408203125, Learning Rate: 0.05\n",
      "Epoch [430/100000], Training Loss: -21.043731689453125, Learning Rate: 0.05\n",
      "Epoch [431/100000], Training Loss: -21.044235229492188, Learning Rate: 0.05\n",
      "Epoch [432/100000], Training Loss: -21.044692993164062, Learning Rate: 0.05\n",
      "Epoch [433/100000], Training Loss: -21.045166015625, Learning Rate: 0.05\n",
      "Epoch [434/100000], Training Loss: -21.045623779296875, Learning Rate: 0.05\n",
      "Epoch [435/100000], Training Loss: -21.046142578125, Learning Rate: 0.05\n",
      "Epoch [436/100000], Training Loss: -21.04656982421875, Learning Rate: 0.05\n",
      "Epoch [437/100000], Training Loss: -21.047073364257812, Learning Rate: 0.05\n",
      "Epoch [438/100000], Training Loss: -21.047592163085938, Learning Rate: 0.05\n",
      "Epoch [439/100000], Training Loss: -21.04803466796875, Learning Rate: 0.05\n",
      "Epoch [440/100000], Training Loss: -21.048507690429688, Learning Rate: 0.05\n",
      "Epoch [441/100000], Training Loss: -21.048980712890625, Learning Rate: 0.05\n",
      "Epoch [442/100000], Training Loss: -21.0494384765625, Learning Rate: 0.05\n",
      "Epoch [443/100000], Training Loss: -21.0499267578125, Learning Rate: 0.05\n",
      "Epoch [444/100000], Training Loss: -21.050384521484375, Learning Rate: 0.05\n",
      "Epoch [445/100000], Training Loss: -21.05084228515625, Learning Rate: 0.05\n",
      "Epoch [446/100000], Training Loss: -21.051315307617188, Learning Rate: 0.05\n",
      "Epoch [447/100000], Training Loss: -21.05181884765625, Learning Rate: 0.05\n",
      "Epoch [448/100000], Training Loss: -21.052276611328125, Learning Rate: 0.05\n",
      "Epoch [449/100000], Training Loss: -21.052719116210938, Learning Rate: 0.05\n",
      "Epoch [450/100000], Training Loss: -21.053192138671875, Learning Rate: 0.05\n",
      "Epoch [451/100000], Training Loss: -21.05364990234375, Learning Rate: 0.05\n",
      "Epoch [452/100000], Training Loss: -21.054122924804688, Learning Rate: 0.05\n",
      "Epoch [453/100000], Training Loss: -21.0545654296875, Learning Rate: 0.05\n",
      "Epoch [454/100000], Training Loss: -21.055023193359375, Learning Rate: 0.05\n",
      "Epoch [455/100000], Training Loss: -21.055511474609375, Learning Rate: 0.05\n",
      "Epoch [456/100000], Training Loss: -21.055984497070312, Learning Rate: 0.05\n",
      "Epoch [457/100000], Training Loss: -21.05645751953125, Learning Rate: 0.05\n",
      "Epoch [458/100000], Training Loss: -21.056884765625, Learning Rate: 0.05\n",
      "Epoch [459/100000], Training Loss: -21.057373046875, Learning Rate: 0.05\n",
      "Epoch [460/100000], Training Loss: -21.057815551757812, Learning Rate: 0.05\n",
      "Epoch [461/100000], Training Loss: -21.058258056640625, Learning Rate: 0.05\n",
      "Epoch [462/100000], Training Loss: -21.058746337890625, Learning Rate: 0.05\n",
      "Epoch [463/100000], Training Loss: -21.059219360351562, Learning Rate: 0.05\n",
      "Epoch [464/100000], Training Loss: -21.0596923828125, Learning Rate: 0.05\n",
      "Epoch [465/100000], Training Loss: -21.060134887695312, Learning Rate: 0.05\n",
      "Epoch [466/100000], Training Loss: -21.060546875, Learning Rate: 0.05\n",
      "Epoch [467/100000], Training Loss: -21.061080932617188, Learning Rate: 0.05\n",
      "Epoch [468/100000], Training Loss: -21.061508178710938, Learning Rate: 0.05\n",
      "Epoch [469/100000], Training Loss: -21.06195068359375, Learning Rate: 0.05\n",
      "Epoch [470/100000], Training Loss: -21.062408447265625, Learning Rate: 0.05\n",
      "Epoch [471/100000], Training Loss: -21.062850952148438, Learning Rate: 0.05\n",
      "Epoch [472/100000], Training Loss: -21.063323974609375, Learning Rate: 0.05\n",
      "Epoch [473/100000], Training Loss: -21.063751220703125, Learning Rate: 0.05\n",
      "Epoch [474/100000], Training Loss: -21.064254760742188, Learning Rate: 0.05\n",
      "Epoch [475/100000], Training Loss: -21.064712524414062, Learning Rate: 0.05\n",
      "Epoch [476/100000], Training Loss: -21.065170288085938, Learning Rate: 0.05\n",
      "Epoch [477/100000], Training Loss: -21.06561279296875, Learning Rate: 0.05\n",
      "Epoch [478/100000], Training Loss: -21.066070556640625, Learning Rate: 0.05\n",
      "Epoch [479/100000], Training Loss: -21.066497802734375, Learning Rate: 0.05\n",
      "Epoch [480/100000], Training Loss: -21.066970825195312, Learning Rate: 0.05\n",
      "Epoch [481/100000], Training Loss: -21.067428588867188, Learning Rate: 0.05\n",
      "Epoch [482/100000], Training Loss: -21.06787109375, Learning Rate: 0.05\n",
      "Epoch [483/100000], Training Loss: -21.068344116210938, Learning Rate: 0.05\n",
      "Epoch [484/100000], Training Loss: -21.0687255859375, Learning Rate: 0.05\n",
      "Epoch [485/100000], Training Loss: -21.0692138671875, Learning Rate: 0.05\n",
      "Epoch [486/100000], Training Loss: -21.069686889648438, Learning Rate: 0.05\n",
      "Epoch [487/100000], Training Loss: -21.070098876953125, Learning Rate: 0.05\n",
      "Epoch [488/100000], Training Loss: -21.070556640625, Learning Rate: 0.05\n",
      "Epoch [489/100000], Training Loss: -21.071014404296875, Learning Rate: 0.05\n",
      "Epoch [490/100000], Training Loss: -21.071502685546875, Learning Rate: 0.05\n",
      "Epoch [491/100000], Training Loss: -21.0718994140625, Learning Rate: 0.05\n",
      "Epoch [492/100000], Training Loss: -21.072357177734375, Learning Rate: 0.05\n",
      "Epoch [493/100000], Training Loss: -21.072830200195312, Learning Rate: 0.05\n",
      "Epoch [494/100000], Training Loss: -21.073287963867188, Learning Rate: 0.05\n",
      "Epoch [495/100000], Training Loss: -21.073684692382812, Learning Rate: 0.05\n",
      "Epoch [496/100000], Training Loss: -21.074127197265625, Learning Rate: 0.05\n",
      "Epoch [497/100000], Training Loss: -21.074554443359375, Learning Rate: 0.05\n",
      "Epoch [498/100000], Training Loss: -21.07501220703125, Learning Rate: 0.05\n",
      "Epoch [499/100000], Training Loss: -21.075485229492188, Learning Rate: 0.05\n",
      "Epoch [500/100000], Training Loss: -21.075897216796875, Learning Rate: 0.05\n",
      "Epoch [501/100000], Training Loss: -21.076339721679688, Learning Rate: 0.05\n",
      "Epoch [502/100000], Training Loss: -21.076812744140625, Learning Rate: 0.05\n",
      "Epoch [503/100000], Training Loss: -21.077239990234375, Learning Rate: 0.05\n",
      "Epoch [504/100000], Training Loss: -21.07763671875, Learning Rate: 0.05\n",
      "Epoch [505/100000], Training Loss: -21.078125, Learning Rate: 0.05\n",
      "Epoch [506/100000], Training Loss: -21.078598022460938, Learning Rate: 0.05\n",
      "Epoch [507/100000], Training Loss: -21.079010009765625, Learning Rate: 0.05\n",
      "Epoch [508/100000], Training Loss: -21.079452514648438, Learning Rate: 0.05\n",
      "Epoch [509/100000], Training Loss: -21.079925537109375, Learning Rate: 0.05\n",
      "Epoch [510/100000], Training Loss: -21.080322265625, Learning Rate: 0.05\n",
      "Epoch [511/100000], Training Loss: -21.080780029296875, Learning Rate: 0.05\n",
      "Epoch [512/100000], Training Loss: -21.081207275390625, Learning Rate: 0.05\n",
      "Epoch [513/100000], Training Loss: -21.081649780273438, Learning Rate: 0.05\n",
      "Epoch [514/100000], Training Loss: -21.082107543945312, Learning Rate: 0.05\n",
      "Epoch [515/100000], Training Loss: -21.082504272460938, Learning Rate: 0.05\n",
      "Epoch [516/100000], Training Loss: -21.082977294921875, Learning Rate: 0.05\n",
      "Epoch [517/100000], Training Loss: -21.083404541015625, Learning Rate: 0.05\n",
      "Epoch [518/100000], Training Loss: -21.083816528320312, Learning Rate: 0.05\n",
      "Epoch [519/100000], Training Loss: -21.084259033203125, Learning Rate: 0.05\n",
      "Epoch [520/100000], Training Loss: -21.084716796875, Learning Rate: 0.05\n",
      "Epoch [521/100000], Training Loss: -21.085159301757812, Learning Rate: 0.05\n",
      "Epoch [522/100000], Training Loss: -21.085556030273438, Learning Rate: 0.05\n",
      "Epoch [523/100000], Training Loss: -21.086013793945312, Learning Rate: 0.05\n",
      "Epoch [524/100000], Training Loss: -21.086532592773438, Learning Rate: 0.05\n",
      "Epoch [525/100000], Training Loss: -21.08685302734375, Learning Rate: 0.05\n",
      "Epoch [526/100000], Training Loss: -21.087326049804688, Learning Rate: 0.05\n",
      "Epoch [527/100000], Training Loss: -21.0877685546875, Learning Rate: 0.05\n",
      "Epoch [528/100000], Training Loss: -21.08819580078125, Learning Rate: 0.05\n",
      "Epoch [529/100000], Training Loss: -21.088653564453125, Learning Rate: 0.05\n",
      "Epoch [530/100000], Training Loss: -21.089035034179688, Learning Rate: 0.05\n",
      "Epoch [531/100000], Training Loss: -21.089508056640625, Learning Rate: 0.05\n",
      "Epoch [532/100000], Training Loss: -21.089920043945312, Learning Rate: 0.05\n",
      "Epoch [533/100000], Training Loss: -21.090362548828125, Learning Rate: 0.05\n",
      "Epoch [534/100000], Training Loss: -21.090789794921875, Learning Rate: 0.05\n",
      "Epoch [535/100000], Training Loss: -21.091232299804688, Learning Rate: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [536/100000], Training Loss: -21.0916748046875, Learning Rate: 0.05\n",
      "Epoch [537/100000], Training Loss: -21.092071533203125, Learning Rate: 0.05\n",
      "Epoch [538/100000], Training Loss: -21.092498779296875, Learning Rate: 0.05\n",
      "Epoch [539/100000], Training Loss: -21.092880249023438, Learning Rate: 0.05\n",
      "Epoch [540/100000], Training Loss: -21.093368530273438, Learning Rate: 0.05\n",
      "Epoch [541/100000], Training Loss: -21.093795776367188, Learning Rate: 0.05\n",
      "Epoch [542/100000], Training Loss: -21.09423828125, Learning Rate: 0.05\n",
      "Epoch [543/100000], Training Loss: -21.094680786132812, Learning Rate: 0.05\n",
      "Epoch [544/100000], Training Loss: -21.095108032226562, Learning Rate: 0.05\n",
      "Epoch [545/100000], Training Loss: -21.095489501953125, Learning Rate: 0.05\n",
      "Epoch [546/100000], Training Loss: -21.095947265625, Learning Rate: 0.05\n",
      "Epoch [547/100000], Training Loss: -21.096389770507812, Learning Rate: 0.05\n",
      "Epoch [548/100000], Training Loss: -21.0968017578125, Learning Rate: 0.05\n",
      "Epoch [549/100000], Training Loss: -21.09722900390625, Learning Rate: 0.05\n",
      "Epoch [550/100000], Training Loss: -21.097625732421875, Learning Rate: 0.05\n",
      "Epoch [551/100000], Training Loss: -21.098052978515625, Learning Rate: 0.05\n",
      "Epoch [552/100000], Training Loss: -21.0985107421875, Learning Rate: 0.05\n",
      "Epoch [553/100000], Training Loss: -21.098907470703125, Learning Rate: 0.05\n",
      "Epoch [554/100000], Training Loss: -21.099349975585938, Learning Rate: 0.05\n",
      "Epoch [555/100000], Training Loss: -21.099746704101562, Learning Rate: 0.05\n",
      "Epoch [556/100000], Training Loss: -21.100082397460938, Learning Rate: 0.05\n",
      "Epoch [557/100000], Training Loss: -21.100372314453125, Learning Rate: 0.05\n",
      "Epoch [558/100000], Training Loss: -21.100448608398438, Learning Rate: 0.05\n",
      "Epoch [559/100000], Training Loss: -21.100006103515625, Learning Rate: 0.05\n",
      "Epoch [560/100000], Training Loss: -21.098602294921875, Learning Rate: 0.05\n",
      "Epoch [561/100000], Training Loss: -21.094635009765625, Learning Rate: 0.05\n",
      "Epoch [562/100000], Training Loss: -21.084442138671875, Learning Rate: 0.05\n",
      "Epoch [563/100000], Training Loss: -21.05877685546875, Learning Rate: 0.05\n",
      "Epoch [564/100000], Training Loss: -20.994720458984375, Learning Rate: 0.05\n",
      "Epoch [565/100000], Training Loss: -20.829345703125, Learning Rate: 0.05\n",
      "Epoch [566/100000], Training Loss: -20.41485595703125, Learning Rate: 0.05\n",
      "Epoch [567/100000], Training Loss: -19.30450439453125, Learning Rate: 0.05\n",
      "Epoch [568/100000], Training Loss: -16.643829345703125, Learning Rate: 0.05\n",
      "Epoch [569/100000], Training Loss: -9.581024169921875, Learning Rate: 0.05\n",
      "Epoch [570/100000], Training Loss: 2.333984375, Learning Rate: 0.05\n",
      "Epoch [571/100000], Training Loss: 23.127349853515625, Learning Rate: 0.05\n",
      "Epoch [572/100000], Training Loss: 12.69036865234375, Learning Rate: 0.05\n",
      "Epoch [573/100000], Training Loss: -12.930923461914062, Learning Rate: 0.05\n",
      "Epoch [574/100000], Training Loss: -18.399246215820312, Learning Rate: 0.05\n",
      "Epoch [575/100000], Training Loss: -2.87786865234375, Learning Rate: 0.05\n",
      "Epoch [576/100000], Training Loss: -13.343765258789062, Learning Rate: 0.05\n",
      "Epoch [577/100000], Training Loss: -19.02923583984375, Learning Rate: 0.05\n",
      "Epoch [578/100000], Training Loss: -8.207901000976562, Learning Rate: 0.05\n",
      "Epoch [579/100000], Training Loss: -18.994171142578125, Learning Rate: 0.05\n",
      "Epoch [580/100000], Training Loss: -16.2987060546875, Learning Rate: 0.05\n",
      "Epoch [581/100000], Training Loss: -13.956527709960938, Learning Rate: 0.05\n",
      "Epoch [582/100000], Training Loss: -20.915908813476562, Learning Rate: 0.05\n",
      "Epoch [583/100000], Training Loss: -14.76531982421875, Learning Rate: 0.05\n",
      "Epoch [584/100000], Training Loss: -19.613632202148438, Learning Rate: 0.05\n",
      "Epoch [585/100000], Training Loss: -18.410202026367188, Learning Rate: 0.05\n",
      "Epoch [586/100000], Training Loss: -17.216140747070312, Learning Rate: 0.05\n",
      "Epoch [587/100000], Training Loss: -20.796401977539062, Learning Rate: 0.05\n",
      "Epoch [588/100000], Training Loss: -17.068023681640625, Learning Rate: 0.05\n",
      "Epoch [589/100000], Training Loss: -20.449859619140625, Learning Rate: 0.05\n",
      "Epoch [590/100000], Training Loss: -19.045425415039062, Learning Rate: 0.05\n",
      "Epoch [591/100000], Training Loss: -19.161758422851562, Learning Rate: 0.05\n",
      "Epoch [592/100000], Training Loss: -20.692337036132812, Learning Rate: 0.05\n",
      "Epoch [593/100000], Training Loss: -18.803802490234375, Learning Rate: 0.05\n",
      "Epoch [594/100000], Training Loss: -20.954833984375, Learning Rate: 0.05\n",
      "Epoch [595/100000], Training Loss: -19.512420654296875, Learning Rate: 0.05\n",
      "Epoch [596/100000], Training Loss: -20.319717407226562, Learning Rate: 0.05\n",
      "Epoch [597/100000], Training Loss: -20.522537231445312, Learning Rate: 0.05\n",
      "Epoch [598/100000], Training Loss: -19.813949584960938, Learning Rate: 0.05\n",
      "Epoch [599/100000], Training Loss: -20.988128662109375, Learning Rate: 0.05\n",
      "Epoch [600/100000], Training Loss: -19.965286254882812, Learning Rate: 0.05\n",
      "Epoch [601/100000], Training Loss: -20.821426391601562, Learning Rate: 0.05\n",
      "Epoch [602/100000], Training Loss: -20.501571655273438, Learning Rate: 0.05\n",
      "Epoch [603/100000], Training Loss: -20.47308349609375, Learning Rate: 0.05\n",
      "Epoch [604/100000], Training Loss: -20.93206787109375, Learning Rate: 0.05\n",
      "Epoch [605/100000], Training Loss: -20.376113891601562, Learning Rate: 0.05\n",
      "Epoch [606/100000], Training Loss: -20.990509033203125, Learning Rate: 0.05\n",
      "Epoch [607/100000], Training Loss: -20.600982666015625, Learning Rate: 0.05\n",
      "Epoch [608/100000], Training Loss: -20.8062744140625, Learning Rate: 0.05\n",
      "Epoch [609/100000], Training Loss: -20.89111328125, Learning Rate: 0.05\n",
      "Epoch [610/100000], Training Loss: -20.668197631835938, Learning Rate: 0.05\n",
      "Epoch [611/100000], Training Loss: -21.011184692382812, Learning Rate: 0.05\n",
      "Epoch [612/100000], Training Loss: -20.72161865234375, Learning Rate: 0.05\n",
      "Epoch [613/100000], Training Loss: -20.94317626953125, Learning Rate: 0.05\n",
      "Epoch [614/100000], Training Loss: -20.890121459960938, Learning Rate: 0.05\n",
      "Epoch [615/100000], Training Loss: -20.84039306640625, Learning Rate: 0.05\n",
      "Epoch [616/100000], Training Loss: -21.005752563476562, Learning Rate: 0.05\n",
      "Epoch [617/100000], Training Loss: -20.83489990234375, Learning Rate: 0.05\n",
      "Epoch [618/100000], Training Loss: -20.999191284179688, Learning Rate: 0.05\n",
      "Epoch [619/100000], Training Loss: -20.919464111328125, Learning Rate: 0.05\n",
      "Epoch [620/100000], Training Loss: -20.934768676757812, Learning Rate: 0.05\n",
      "Epoch [621/100000], Training Loss: -21.000579833984375, Learning Rate: 0.05\n",
      "Epoch [622/100000], Training Loss: -20.909271240234375, Learning Rate: 0.05\n",
      "Epoch [623/100000], Training Loss: -21.013519287109375, Learning Rate: 0.05\n",
      "Epoch [624/100000], Training Loss: -20.948410034179688, Learning Rate: 0.05\n",
      "Epoch [625/100000], Training Loss: -20.9788818359375, Learning Rate: 0.05\n",
      "Epoch [626/100000], Training Loss: -21.001998901367188, Learning Rate: 0.05\n",
      "Epoch [627/100000], Training Loss: -20.957122802734375, Learning Rate: 0.05\n",
      "Epoch [628/100000], Training Loss: -21.019088745117188, Learning Rate: 0.05\n",
      "Epoch [629/100000], Training Loss: -20.974960327148438, Learning Rate: 0.05\n",
      "Epoch [630/100000], Training Loss: -21.001129150390625, Learning Rate: 0.05\n",
      "Epoch [631/100000], Training Loss: -21.008071899414062, Learning Rate: 0.05\n",
      "Epoch [632/100000], Training Loss: -20.9852294921875, Learning Rate: 0.05\n",
      "Epoch [633/100000], Training Loss: -21.02130126953125, Learning Rate: 0.05\n",
      "Epoch [634/100000], Training Loss: -20.993728637695312, Learning Rate: 0.05\n",
      "Epoch [635/100000], Training Loss: -21.011367797851562, Learning Rate: 0.05\n",
      "Epoch [636/100000], Training Loss: -21.0137939453125, Learning Rate: 0.05\n",
      "Epoch [637/100000], Training Loss: -21.001220703125, Learning Rate: 0.05\n",
      "Epoch [638/100000], Training Loss: -21.022720336914062, Learning Rate: 0.05\n",
      "Epoch [639/100000], Training Loss: -21.00653076171875, Learning Rate: 0.05\n",
      "Epoch [640/100000], Training Loss: -21.01715087890625, Learning Rate: 0.05\n",
      "Epoch [641/100000], Training Loss: -21.019332885742188, Learning Rate: 0.05\n",
      "Epoch [642/100000], Training Loss: -21.011550903320312, Learning Rate: 0.05\n",
      "Epoch [643/100000], Training Loss: -21.024658203125, Learning Rate: 0.05\n",
      "Epoch [644/100000], Training Loss: -21.015426635742188, Learning Rate: 0.05\n",
      "Epoch [645/100000], Training Loss: -21.0208740234375, Learning Rate: 0.05\n",
      "Epoch [646/100000], Training Loss: -21.023330688476562, Learning Rate: 0.05\n",
      "Epoch [647/100000], Training Loss: -21.017959594726562, Learning Rate: 0.05\n",
      "Epoch [648/100000], Training Loss: -21.026077270507812, Learning Rate: 0.05\n",
      "Epoch [649/100000], Training Loss: -21.021377563476562, Learning Rate: 0.05\n",
      "Epoch [650/100000], Training Loss: -21.023635864257812, Learning Rate: 0.05\n",
      "Epoch [651/100000], Training Loss: -21.026397705078125, Learning Rate: 0.05\n",
      "Epoch [652/100000], Training Loss: -21.022720336914062, Learning Rate: 0.05\n",
      "Epoch [653/100000], Training Loss: -21.027587890625, Learning Rate: 0.05\n",
      "Epoch [654/100000], Training Loss: -21.025711059570312, Learning Rate: 0.05\n",
      "Epoch [655/100000], Training Loss: -21.026138305664062, Learning Rate: 0.05\n",
      "Epoch [656/100000], Training Loss: -21.028793334960938, Learning Rate: 0.05\n",
      "Epoch [657/100000], Training Loss: -21.026382446289062, Learning Rate: 0.05\n",
      "Epoch [658/100000], Training Loss: -21.02899169921875, Learning Rate: 0.05\n",
      "Epoch [659/100000], Training Loss: -21.028854370117188, Learning Rate: 0.05\n",
      "Epoch [660/100000], Training Loss: -21.02838134765625, Learning Rate: 0.05\n",
      "Epoch [661/100000], Training Loss: -21.030563354492188, Learning Rate: 0.05\n",
      "Epoch [662/100000], Training Loss: -21.029632568359375, Learning Rate: 0.045000000000000005\n",
      "Epoch [663/100000], Training Loss: -21.030960083007812, Learning Rate: 0.045000000000000005\n",
      "Epoch [664/100000], Training Loss: -21.030838012695312, Learning Rate: 0.045000000000000005\n",
      "Epoch [665/100000], Training Loss: -21.031204223632812, Learning Rate: 0.045000000000000005\n",
      "Epoch [666/100000], Training Loss: -21.031875610351562, Learning Rate: 0.045000000000000005\n",
      "Epoch [667/100000], Training Loss: -21.031631469726562, Learning Rate: 0.045000000000000005\n",
      "Epoch [668/100000], Training Loss: -21.03265380859375, Learning Rate: 0.045000000000000005\n",
      "Epoch [669/100000], Training Loss: -21.032379150390625, Learning Rate: 0.045000000000000005\n",
      "Epoch [670/100000], Training Loss: -21.033096313476562, Learning Rate: 0.045000000000000005\n",
      "Epoch [671/100000], Training Loss: -21.033279418945312, Learning Rate: 0.045000000000000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [672/100000], Training Loss: -21.033493041992188, Learning Rate: 0.045000000000000005\n",
      "Epoch [673/100000], Training Loss: -21.0340576171875, Learning Rate: 0.045000000000000005\n",
      "Epoch [674/100000], Training Loss: -21.0340576171875, Learning Rate: 0.045000000000000005\n",
      "Epoch [675/100000], Training Loss: -21.03472900390625, Learning Rate: 0.045000000000000005\n",
      "Epoch [676/100000], Training Loss: -21.034744262695312, Learning Rate: 0.045000000000000005\n",
      "Epoch [677/100000], Training Loss: -21.035186767578125, Learning Rate: 0.045000000000000005\n",
      "Epoch [678/100000], Training Loss: -21.0355224609375, Learning Rate: 0.045000000000000005\n",
      "Epoch [679/100000], Training Loss: -21.035720825195312, Learning Rate: 0.045000000000000005\n",
      "Epoch [680/100000], Training Loss: -21.036163330078125, Learning Rate: 0.045000000000000005\n",
      "Epoch [681/100000], Training Loss: -21.036331176757812, Learning Rate: 0.045000000000000005\n",
      "Epoch [682/100000], Training Loss: -21.036727905273438, Learning Rate: 0.045000000000000005\n",
      "Epoch [683/100000], Training Loss: -21.036956787109375, Learning Rate: 0.045000000000000005\n",
      "Epoch [684/100000], Training Loss: -21.0372314453125, Learning Rate: 0.045000000000000005\n",
      "Epoch [685/100000], Training Loss: -21.037628173828125, Learning Rate: 0.045000000000000005\n",
      "Epoch [686/100000], Training Loss: -21.037826538085938, Learning Rate: 0.045000000000000005\n",
      "Epoch [687/100000], Training Loss: -21.038238525390625, Learning Rate: 0.045000000000000005\n",
      "Epoch [688/100000], Training Loss: -21.038421630859375, Learning Rate: 0.045000000000000005\n",
      "Epoch [689/100000], Training Loss: -21.038772583007812, Learning Rate: 0.045000000000000005\n",
      "Epoch [690/100000], Training Loss: -21.039077758789062, Learning Rate: 0.045000000000000005\n",
      "Epoch [691/100000], Training Loss: -21.039321899414062, Learning Rate: 0.045000000000000005\n",
      "Epoch [692/100000], Training Loss: -21.0396728515625, Learning Rate: 0.045000000000000005\n",
      "Epoch [693/100000], Training Loss: -21.039932250976562, Learning Rate: 0.045000000000000005\n",
      "Epoch [694/100000], Training Loss: -21.04022216796875, Learning Rate: 0.045000000000000005\n",
      "Epoch [695/100000], Training Loss: -21.040542602539062, Learning Rate: 0.045000000000000005\n",
      "Epoch [696/100000], Training Loss: -21.040817260742188, Learning Rate: 0.045000000000000005\n",
      "Epoch [697/100000], Training Loss: -21.041122436523438, Learning Rate: 0.045000000000000005\n",
      "Epoch [698/100000], Training Loss: -21.041412353515625, Learning Rate: 0.045000000000000005\n",
      "Epoch [699/100000], Training Loss: -21.04168701171875, Learning Rate: 0.045000000000000005\n",
      "Epoch [700/100000], Training Loss: -21.0419921875, Learning Rate: 0.045000000000000005\n",
      "Epoch [701/100000], Training Loss: -21.042236328125, Learning Rate: 0.045000000000000005\n",
      "Epoch [702/100000], Training Loss: -21.042572021484375, Learning Rate: 0.045000000000000005\n",
      "Epoch [703/100000], Training Loss: -21.042800903320312, Learning Rate: 0.045000000000000005\n",
      "Epoch [704/100000], Training Loss: -21.043136596679688, Learning Rate: 0.045000000000000005\n",
      "Epoch [705/100000], Training Loss: -21.043411254882812, Learning Rate: 0.045000000000000005\n",
      "Epoch [706/100000], Training Loss: -21.043701171875, Learning Rate: 0.045000000000000005\n",
      "Epoch [707/100000], Training Loss: -21.04400634765625, Learning Rate: 0.045000000000000005\n",
      "Epoch [708/100000], Training Loss: -21.044296264648438, Learning Rate: 0.045000000000000005\n",
      "Epoch [709/100000], Training Loss: -21.044586181640625, Learning Rate: 0.045000000000000005\n",
      "Epoch [710/100000], Training Loss: -21.044830322265625, Learning Rate: 0.045000000000000005\n",
      "Epoch [711/100000], Training Loss: -21.045166015625, Learning Rate: 0.045000000000000005\n",
      "Epoch [712/100000], Training Loss: -21.045455932617188, Learning Rate: 0.045000000000000005\n",
      "Epoch [713/100000], Training Loss: -21.045745849609375, Learning Rate: 0.045000000000000005\n",
      "Epoch [714/100000], Training Loss: -21.045974731445312, Learning Rate: 0.045000000000000005\n",
      "Epoch [715/100000], Training Loss: -21.04632568359375, Learning Rate: 0.045000000000000005\n",
      "Epoch [716/100000], Training Loss: -21.046615600585938, Learning Rate: 0.045000000000000005\n",
      "Epoch [717/100000], Training Loss: -21.046844482421875, Learning Rate: 0.045000000000000005\n",
      "Epoch [718/100000], Training Loss: -21.047119140625, Learning Rate: 0.045000000000000005\n",
      "Epoch [719/100000], Training Loss: -21.047454833984375, Learning Rate: 0.045000000000000005\n",
      "Epoch [720/100000], Training Loss: -21.047744750976562, Learning Rate: 0.045000000000000005\n",
      "Epoch [721/100000], Training Loss: -21.048049926757812, Learning Rate: 0.045000000000000005\n",
      "Epoch [722/100000], Training Loss: -21.048294067382812, Learning Rate: 0.045000000000000005\n",
      "Epoch [723/100000], Training Loss: -21.048599243164062, Learning Rate: 0.045000000000000005\n",
      "Epoch [724/100000], Training Loss: -21.048873901367188, Learning Rate: 0.045000000000000005\n",
      "Epoch [725/100000], Training Loss: -21.049179077148438, Learning Rate: 0.045000000000000005\n",
      "Epoch [726/100000], Training Loss: -21.049453735351562, Learning Rate: 0.045000000000000005\n",
      "Epoch [727/100000], Training Loss: -21.049728393554688, Learning Rate: 0.045000000000000005\n",
      "Epoch [728/100000], Training Loss: -21.050033569335938, Learning Rate: 0.045000000000000005\n",
      "Epoch [729/100000], Training Loss: -21.05035400390625, Learning Rate: 0.045000000000000005\n",
      "Epoch [730/100000], Training Loss: -21.05059814453125, Learning Rate: 0.045000000000000005\n",
      "Epoch [731/100000], Training Loss: -21.050872802734375, Learning Rate: 0.045000000000000005\n",
      "Epoch [732/100000], Training Loss: -21.051177978515625, Learning Rate: 0.045000000000000005\n",
      "Epoch [733/100000], Training Loss: -21.051422119140625, Learning Rate: 0.045000000000000005\n",
      "Epoch [734/100000], Training Loss: -21.051727294921875, Learning Rate: 0.045000000000000005\n",
      "Epoch [735/100000], Training Loss: -21.052017211914062, Learning Rate: 0.045000000000000005\n",
      "Epoch [736/100000], Training Loss: -21.052291870117188, Learning Rate: 0.045000000000000005\n",
      "Epoch [737/100000], Training Loss: -21.05255126953125, Learning Rate: 0.045000000000000005\n",
      "Epoch [738/100000], Training Loss: -21.0528564453125, Learning Rate: 0.045000000000000005\n",
      "Epoch [739/100000], Training Loss: -21.053131103515625, Learning Rate: 0.045000000000000005\n",
      "Epoch [740/100000], Training Loss: -21.053482055664062, Learning Rate: 0.045000000000000005\n",
      "Epoch [741/100000], Training Loss: -21.053726196289062, Learning Rate: 0.045000000000000005\n",
      "Epoch [742/100000], Training Loss: -21.054031372070312, Learning Rate: 0.045000000000000005\n",
      "Epoch [743/100000], Training Loss: -21.054275512695312, Learning Rate: 0.045000000000000005\n",
      "Epoch [744/100000], Training Loss: -21.054595947265625, Learning Rate: 0.045000000000000005\n",
      "Epoch [745/100000], Training Loss: -21.05487060546875, Learning Rate: 0.045000000000000005\n",
      "Epoch [746/100000], Training Loss: -21.055130004882812, Learning Rate: 0.045000000000000005\n",
      "Epoch [747/100000], Training Loss: -21.055419921875, Learning Rate: 0.045000000000000005\n",
      "Epoch [748/100000], Training Loss: -21.055770874023438, Learning Rate: 0.045000000000000005\n",
      "Epoch [749/100000], Training Loss: -21.055999755859375, Learning Rate: 0.045000000000000005\n",
      "Epoch [750/100000], Training Loss: -21.056289672851562, Learning Rate: 0.045000000000000005\n",
      "Epoch [751/100000], Training Loss: -21.056625366210938, Learning Rate: 0.045000000000000005\n",
      "Epoch [752/100000], Training Loss: -21.056869506835938, Learning Rate: 0.045000000000000005\n",
      "Epoch [753/100000], Training Loss: -21.057144165039062, Learning Rate: 0.045000000000000005\n",
      "Epoch [754/100000], Training Loss: -21.057449340820312, Learning Rate: 0.045000000000000005\n",
      "Epoch [755/100000], Training Loss: -21.0577392578125, Learning Rate: 0.045000000000000005\n",
      "Epoch [756/100000], Training Loss: -21.0579833984375, Learning Rate: 0.045000000000000005\n",
      "Epoch [757/100000], Training Loss: -21.058303833007812, Learning Rate: 0.045000000000000005\n",
      "Epoch [758/100000], Training Loss: -21.05859375, Learning Rate: 0.045000000000000005\n",
      "Epoch [759/100000], Training Loss: -21.058868408203125, Learning Rate: 0.045000000000000005\n",
      "Epoch [760/100000], Training Loss: -21.059158325195312, Learning Rate: 0.045000000000000005\n",
      "Epoch [761/100000], Training Loss: -21.0594482421875, Learning Rate: 0.045000000000000005\n",
      "Epoch [762/100000], Training Loss: -21.0596923828125, Learning Rate: 0.045000000000000005\n",
      "Epoch [763/100000], Training Loss: -21.059951782226562, Learning Rate: 0.04050000000000001\n",
      "Epoch [764/100000], Training Loss: -21.060195922851562, Learning Rate: 0.04050000000000001\n",
      "Epoch [765/100000], Training Loss: -21.060501098632812, Learning Rate: 0.04050000000000001\n",
      "Epoch [766/100000], Training Loss: -21.060745239257812, Learning Rate: 0.04050000000000001\n",
      "Epoch [767/100000], Training Loss: -21.06097412109375, Learning Rate: 0.04050000000000001\n",
      "Epoch [768/100000], Training Loss: -21.061264038085938, Learning Rate: 0.04050000000000001\n",
      "Epoch [769/100000], Training Loss: -21.061553955078125, Learning Rate: 0.04050000000000001\n",
      "Epoch [770/100000], Training Loss: -21.061737060546875, Learning Rate: 0.04050000000000001\n",
      "Epoch [771/100000], Training Loss: -21.062057495117188, Learning Rate: 0.04050000000000001\n",
      "Epoch [772/100000], Training Loss: -21.062271118164062, Learning Rate: 0.04050000000000001\n",
      "Epoch [773/100000], Training Loss: -21.06256103515625, Learning Rate: 0.04050000000000001\n",
      "Epoch [774/100000], Training Loss: -21.062789916992188, Learning Rate: 0.04050000000000001\n",
      "Epoch [775/100000], Training Loss: -21.063079833984375, Learning Rate: 0.04050000000000001\n",
      "Epoch [776/100000], Training Loss: -21.06329345703125, Learning Rate: 0.04050000000000001\n",
      "Epoch [777/100000], Training Loss: -21.063583374023438, Learning Rate: 0.04050000000000001\n",
      "Epoch [778/100000], Training Loss: -21.063827514648438, Learning Rate: 0.04050000000000001\n",
      "Epoch [779/100000], Training Loss: -21.064071655273438, Learning Rate: 0.04050000000000001\n",
      "Epoch [780/100000], Training Loss: -21.064346313476562, Learning Rate: 0.04050000000000001\n",
      "Epoch [781/100000], Training Loss: -21.064651489257812, Learning Rate: 0.04050000000000001\n",
      "Epoch [782/100000], Training Loss: -21.064849853515625, Learning Rate: 0.04050000000000001\n",
      "Epoch [783/100000], Training Loss: -21.065093994140625, Learning Rate: 0.04050000000000001\n",
      "Epoch [784/100000], Training Loss: -21.065383911132812, Learning Rate: 0.04050000000000001\n",
      "Epoch [785/100000], Training Loss: -21.065582275390625, Learning Rate: 0.04050000000000001\n",
      "Epoch [786/100000], Training Loss: -21.065872192382812, Learning Rate: 0.04050000000000001\n",
      "Epoch [787/100000], Training Loss: -21.066162109375, Learning Rate: 0.04050000000000001\n",
      "Epoch [788/100000], Training Loss: -21.06640625, Learning Rate: 0.04050000000000001\n",
      "Epoch [789/100000], Training Loss: -21.066665649414062, Learning Rate: 0.04050000000000001\n",
      "Epoch [790/100000], Training Loss: -21.066925048828125, Learning Rate: 0.04050000000000001\n",
      "Epoch [791/100000], Training Loss: -21.067184448242188, Learning Rate: 0.04050000000000001\n",
      "Epoch [792/100000], Training Loss: -21.06744384765625, Learning Rate: 0.04050000000000001\n",
      "Epoch [793/100000], Training Loss: -21.067672729492188, Learning Rate: 0.04050000000000001\n",
      "Epoch [794/100000], Training Loss: -21.06793212890625, Learning Rate: 0.04050000000000001\n",
      "Epoch [795/100000], Training Loss: -21.068206787109375, Learning Rate: 0.04050000000000001\n",
      "Epoch [796/100000], Training Loss: -21.068450927734375, Learning Rate: 0.04050000000000001\n",
      "Epoch [797/100000], Training Loss: -21.068710327148438, Learning Rate: 0.04050000000000001\n",
      "Epoch [798/100000], Training Loss: -21.068984985351562, Learning Rate: 0.04050000000000001\n",
      "Epoch [799/100000], Training Loss: -21.069244384765625, Learning Rate: 0.04050000000000001\n",
      "Epoch [800/100000], Training Loss: -21.069503784179688, Learning Rate: 0.04050000000000001\n",
      "Epoch [801/100000], Training Loss: -21.069732666015625, Learning Rate: 0.04050000000000001\n",
      "Epoch [802/100000], Training Loss: -21.070037841796875, Learning Rate: 0.04050000000000001\n",
      "Epoch [803/100000], Training Loss: -21.070266723632812, Learning Rate: 0.04050000000000001\n",
      "Epoch [804/100000], Training Loss: -21.070556640625, Learning Rate: 0.04050000000000001\n",
      "Epoch [805/100000], Training Loss: -21.070831298828125, Learning Rate: 0.04050000000000001\n",
      "Epoch [806/100000], Training Loss: -21.071044921875, Learning Rate: 0.04050000000000001\n",
      "Epoch [807/100000], Training Loss: -21.071319580078125, Learning Rate: 0.04050000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [808/100000], Training Loss: -21.071578979492188, Learning Rate: 0.04050000000000001\n",
      "Epoch [809/100000], Training Loss: -21.071868896484375, Learning Rate: 0.04050000000000001\n",
      "Epoch [810/100000], Training Loss: -21.072097778320312, Learning Rate: 0.04050000000000001\n",
      "Epoch [811/100000], Training Loss: -21.072357177734375, Learning Rate: 0.04050000000000001\n",
      "Epoch [812/100000], Training Loss: -21.0726318359375, Learning Rate: 0.04050000000000001\n",
      "Epoch [813/100000], Training Loss: -21.0728759765625, Learning Rate: 0.04050000000000001\n",
      "Epoch [814/100000], Training Loss: -21.073135375976562, Learning Rate: 0.04050000000000001\n",
      "Epoch [815/100000], Training Loss: -21.073379516601562, Learning Rate: 0.04050000000000001\n",
      "Epoch [816/100000], Training Loss: -21.073638916015625, Learning Rate: 0.04050000000000001\n",
      "Epoch [817/100000], Training Loss: -21.073944091796875, Learning Rate: 0.04050000000000001\n",
      "Epoch [818/100000], Training Loss: -21.074188232421875, Learning Rate: 0.04050000000000001\n",
      "Epoch [819/100000], Training Loss: -21.074417114257812, Learning Rate: 0.04050000000000001\n",
      "Epoch [820/100000], Training Loss: -21.074722290039062, Learning Rate: 0.04050000000000001\n",
      "Epoch [821/100000], Training Loss: -21.074951171875, Learning Rate: 0.04050000000000001\n",
      "Epoch [822/100000], Training Loss: -21.075225830078125, Learning Rate: 0.04050000000000001\n",
      "Epoch [823/100000], Training Loss: -21.07550048828125, Learning Rate: 0.04050000000000001\n",
      "Epoch [824/100000], Training Loss: -21.075775146484375, Learning Rate: 0.04050000000000001\n",
      "Epoch [825/100000], Training Loss: -21.0760498046875, Learning Rate: 0.04050000000000001\n",
      "Epoch [826/100000], Training Loss: -21.0762939453125, Learning Rate: 0.04050000000000001\n",
      "Epoch [827/100000], Training Loss: -21.076522827148438, Learning Rate: 0.04050000000000001\n",
      "Epoch [828/100000], Training Loss: -21.076858520507812, Learning Rate: 0.04050000000000001\n",
      "Epoch [829/100000], Training Loss: -21.077072143554688, Learning Rate: 0.04050000000000001\n",
      "Epoch [830/100000], Training Loss: -21.077316284179688, Learning Rate: 0.04050000000000001\n",
      "Epoch [831/100000], Training Loss: -21.07757568359375, Learning Rate: 0.04050000000000001\n",
      "Epoch [832/100000], Training Loss: -21.077850341796875, Learning Rate: 0.04050000000000001\n",
      "Epoch [833/100000], Training Loss: -21.078109741210938, Learning Rate: 0.04050000000000001\n",
      "Epoch [834/100000], Training Loss: -21.078384399414062, Learning Rate: 0.04050000000000001\n",
      "Epoch [835/100000], Training Loss: -21.07861328125, Learning Rate: 0.04050000000000001\n",
      "Epoch [836/100000], Training Loss: -21.078887939453125, Learning Rate: 0.04050000000000001\n",
      "Epoch [837/100000], Training Loss: -21.079147338867188, Learning Rate: 0.04050000000000001\n",
      "Epoch [838/100000], Training Loss: -21.079452514648438, Learning Rate: 0.04050000000000001\n",
      "Epoch [839/100000], Training Loss: -21.079681396484375, Learning Rate: 0.04050000000000001\n",
      "Epoch [840/100000], Training Loss: -21.079971313476562, Learning Rate: 0.04050000000000001\n",
      "Epoch [841/100000], Training Loss: -21.080230712890625, Learning Rate: 0.04050000000000001\n",
      "Epoch [842/100000], Training Loss: -21.080474853515625, Learning Rate: 0.04050000000000001\n",
      "Epoch [843/100000], Training Loss: -21.08074951171875, Learning Rate: 0.04050000000000001\n",
      "Epoch [844/100000], Training Loss: -21.081024169921875, Learning Rate: 0.04050000000000001\n",
      "Epoch [845/100000], Training Loss: -21.081268310546875, Learning Rate: 0.04050000000000001\n",
      "Epoch [846/100000], Training Loss: -21.081558227539062, Learning Rate: 0.04050000000000001\n",
      "Epoch [847/100000], Training Loss: -21.081771850585938, Learning Rate: 0.04050000000000001\n",
      "Epoch [848/100000], Training Loss: -21.082061767578125, Learning Rate: 0.04050000000000001\n",
      "Epoch [849/100000], Training Loss: -21.082321166992188, Learning Rate: 0.04050000000000001\n",
      "Epoch [850/100000], Training Loss: -21.082611083984375, Learning Rate: 0.04050000000000001\n",
      "Epoch [851/100000], Training Loss: -21.082901000976562, Learning Rate: 0.04050000000000001\n",
      "Epoch [852/100000], Training Loss: -21.083145141601562, Learning Rate: 0.04050000000000001\n",
      "Epoch [853/100000], Training Loss: -21.083419799804688, Learning Rate: 0.04050000000000001\n",
      "Epoch [854/100000], Training Loss: -21.083663940429688, Learning Rate: 0.04050000000000001\n",
      "Epoch [855/100000], Training Loss: -21.083892822265625, Learning Rate: 0.04050000000000001\n",
      "Epoch [856/100000], Training Loss: -21.08416748046875, Learning Rate: 0.04050000000000001\n",
      "Epoch [857/100000], Training Loss: -21.084442138671875, Learning Rate: 0.04050000000000001\n",
      "Epoch [858/100000], Training Loss: -21.084747314453125, Learning Rate: 0.04050000000000001\n",
      "Epoch [859/100000], Training Loss: -21.085006713867188, Learning Rate: 0.04050000000000001\n",
      "Epoch [860/100000], Training Loss: -21.08526611328125, Learning Rate: 0.04050000000000001\n",
      "Epoch [861/100000], Training Loss: -21.085525512695312, Learning Rate: 0.04050000000000001\n",
      "Epoch [862/100000], Training Loss: -21.085784912109375, Learning Rate: 0.04050000000000001\n",
      "Epoch [863/100000], Training Loss: -21.0860595703125, Learning Rate: 0.04050000000000001\n",
      "Epoch [864/100000], Training Loss: -21.0863037109375, Learning Rate: 0.03645000000000001\n",
      "Epoch [865/100000], Training Loss: -21.086517333984375, Learning Rate: 0.03645000000000001\n",
      "Epoch [866/100000], Training Loss: -21.0867919921875, Learning Rate: 0.03645000000000001\n",
      "Epoch [867/100000], Training Loss: -21.087051391601562, Learning Rate: 0.03645000000000001\n",
      "Epoch [868/100000], Training Loss: -21.087203979492188, Learning Rate: 0.03645000000000001\n",
      "Epoch [869/100000], Training Loss: -21.087493896484375, Learning Rate: 0.03645000000000001\n",
      "Epoch [870/100000], Training Loss: -21.0877685546875, Learning Rate: 0.03645000000000001\n",
      "Epoch [871/100000], Training Loss: -21.0880126953125, Learning Rate: 0.03645000000000001\n",
      "Epoch [872/100000], Training Loss: -21.088226318359375, Learning Rate: 0.03645000000000001\n",
      "Epoch [873/100000], Training Loss: -21.088455200195312, Learning Rate: 0.03645000000000001\n",
      "Epoch [874/100000], Training Loss: -21.088699340820312, Learning Rate: 0.03645000000000001\n",
      "Epoch [875/100000], Training Loss: -21.088973999023438, Learning Rate: 0.03645000000000001\n",
      "Epoch [876/100000], Training Loss: -21.089202880859375, Learning Rate: 0.03645000000000001\n",
      "Epoch [877/100000], Training Loss: -21.0894775390625, Learning Rate: 0.03645000000000001\n",
      "Epoch [878/100000], Training Loss: -21.0897216796875, Learning Rate: 0.03645000000000001\n",
      "Epoch [879/100000], Training Loss: -21.08990478515625, Learning Rate: 0.03645000000000001\n",
      "Epoch [880/100000], Training Loss: -21.090194702148438, Learning Rate: 0.03645000000000001\n",
      "Epoch [881/100000], Training Loss: -21.09039306640625, Learning Rate: 0.03645000000000001\n",
      "Epoch [882/100000], Training Loss: -21.090652465820312, Learning Rate: 0.03645000000000001\n",
      "Epoch [883/100000], Training Loss: -21.09088134765625, Learning Rate: 0.03645000000000001\n",
      "Epoch [884/100000], Training Loss: -21.09112548828125, Learning Rate: 0.03645000000000001\n",
      "Epoch [885/100000], Training Loss: -21.091354370117188, Learning Rate: 0.03645000000000001\n",
      "Epoch [886/100000], Training Loss: -21.091629028320312, Learning Rate: 0.03645000000000001\n",
      "Epoch [887/100000], Training Loss: -21.09185791015625, Learning Rate: 0.03645000000000001\n",
      "Epoch [888/100000], Training Loss: -21.092117309570312, Learning Rate: 0.03645000000000001\n",
      "Epoch [889/100000], Training Loss: -21.092361450195312, Learning Rate: 0.03645000000000001\n",
      "Epoch [890/100000], Training Loss: -21.092605590820312, Learning Rate: 0.03645000000000001\n",
      "Epoch [891/100000], Training Loss: -21.09283447265625, Learning Rate: 0.03645000000000001\n",
      "Epoch [892/100000], Training Loss: -21.093093872070312, Learning Rate: 0.03645000000000001\n",
      "Epoch [893/100000], Training Loss: -21.093353271484375, Learning Rate: 0.03645000000000001\n",
      "Epoch [894/100000], Training Loss: -21.093582153320312, Learning Rate: 0.03645000000000001\n",
      "Epoch [895/100000], Training Loss: -21.093795776367188, Learning Rate: 0.03645000000000001\n",
      "Epoch [896/100000], Training Loss: -21.09405517578125, Learning Rate: 0.03645000000000001\n",
      "Epoch [897/100000], Training Loss: -21.094345092773438, Learning Rate: 0.03645000000000001\n",
      "Epoch [898/100000], Training Loss: -21.09454345703125, Learning Rate: 0.03645000000000001\n",
      "Epoch [899/100000], Training Loss: -21.09478759765625, Learning Rate: 0.03645000000000001\n",
      "Epoch [900/100000], Training Loss: -21.095046997070312, Learning Rate: 0.03645000000000001\n",
      "Epoch [901/100000], Training Loss: -21.095291137695312, Learning Rate: 0.03645000000000001\n",
      "Epoch [902/100000], Training Loss: -21.095535278320312, Learning Rate: 0.03645000000000001\n",
      "Epoch [903/100000], Training Loss: -21.095733642578125, Learning Rate: 0.03645000000000001\n",
      "Epoch [904/100000], Training Loss: -21.09600830078125, Learning Rate: 0.03645000000000001\n",
      "Epoch [905/100000], Training Loss: -21.096298217773438, Learning Rate: 0.03645000000000001\n",
      "Epoch [906/100000], Training Loss: -21.096542358398438, Learning Rate: 0.03645000000000001\n",
      "Epoch [907/100000], Training Loss: -21.096755981445312, Learning Rate: 0.03645000000000001\n",
      "Epoch [908/100000], Training Loss: -21.097030639648438, Learning Rate: 0.03645000000000001\n",
      "Epoch [909/100000], Training Loss: -21.0972900390625, Learning Rate: 0.03645000000000001\n",
      "Epoch [910/100000], Training Loss: -21.09747314453125, Learning Rate: 0.03645000000000001\n",
      "Epoch [911/100000], Training Loss: -21.0977783203125, Learning Rate: 0.03645000000000001\n",
      "Epoch [912/100000], Training Loss: -21.097991943359375, Learning Rate: 0.03645000000000001\n",
      "Epoch [913/100000], Training Loss: -21.098236083984375, Learning Rate: 0.03645000000000001\n",
      "Epoch [914/100000], Training Loss: -21.098480224609375, Learning Rate: 0.03645000000000001\n",
      "Epoch [915/100000], Training Loss: -21.098724365234375, Learning Rate: 0.03645000000000001\n",
      "Epoch [916/100000], Training Loss: -21.0989990234375, Learning Rate: 0.03645000000000001\n",
      "Epoch [917/100000], Training Loss: -21.0992431640625, Learning Rate: 0.03645000000000001\n",
      "Epoch [918/100000], Training Loss: -21.0994873046875, Learning Rate: 0.03645000000000001\n",
      "Epoch [919/100000], Training Loss: -21.099761962890625, Learning Rate: 0.03645000000000001\n",
      "Epoch [920/100000], Training Loss: -21.099990844726562, Learning Rate: 0.03645000000000001\n",
      "Epoch [921/100000], Training Loss: -21.100250244140625, Learning Rate: 0.03645000000000001\n",
      "Epoch [922/100000], Training Loss: -21.100479125976562, Learning Rate: 0.03645000000000001\n",
      "Epoch [923/100000], Training Loss: -21.100738525390625, Learning Rate: 0.03645000000000001\n",
      "Epoch [924/100000], Training Loss: -21.100967407226562, Learning Rate: 0.03645000000000001\n",
      "Epoch [925/100000], Training Loss: -21.101211547851562, Learning Rate: 0.03645000000000001\n",
      "Epoch [926/100000], Training Loss: -21.101486206054688, Learning Rate: 0.03645000000000001\n",
      "Epoch [927/100000], Training Loss: -21.101730346679688, Learning Rate: 0.03645000000000001\n",
      "Epoch [928/100000], Training Loss: -21.10198974609375, Learning Rate: 0.03645000000000001\n",
      "Epoch [929/100000], Training Loss: -21.102264404296875, Learning Rate: 0.03645000000000001\n",
      "Epoch [930/100000], Training Loss: -21.102462768554688, Learning Rate: 0.03645000000000001\n",
      "Epoch [931/100000], Training Loss: -21.10272216796875, Learning Rate: 0.03645000000000001\n",
      "Epoch [932/100000], Training Loss: -21.10296630859375, Learning Rate: 0.03645000000000001\n",
      "Epoch [933/100000], Training Loss: -21.103240966796875, Learning Rate: 0.03645000000000001\n",
      "Epoch [934/100000], Training Loss: -21.103469848632812, Learning Rate: 0.03645000000000001\n",
      "Epoch [935/100000], Training Loss: -21.103759765625, Learning Rate: 0.03645000000000001\n",
      "Epoch [936/100000], Training Loss: -21.103988647460938, Learning Rate: 0.03645000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [937/100000], Training Loss: -21.104248046875, Learning Rate: 0.03645000000000001\n",
      "Epoch [938/100000], Training Loss: -21.1044921875, Learning Rate: 0.03645000000000001\n",
      "Epoch [939/100000], Training Loss: -21.104736328125, Learning Rate: 0.03645000000000001\n",
      "Epoch [940/100000], Training Loss: -21.10498046875, Learning Rate: 0.03645000000000001\n",
      "Epoch [941/100000], Training Loss: -21.105270385742188, Learning Rate: 0.03645000000000001\n",
      "Epoch [942/100000], Training Loss: -21.105514526367188, Learning Rate: 0.03645000000000001\n",
      "Epoch [943/100000], Training Loss: -21.105743408203125, Learning Rate: 0.03645000000000001\n",
      "Epoch [944/100000], Training Loss: -21.10601806640625, Learning Rate: 0.03645000000000001\n",
      "Epoch [945/100000], Training Loss: -21.106246948242188, Learning Rate: 0.03645000000000001\n",
      "Epoch [946/100000], Training Loss: -21.106491088867188, Learning Rate: 0.03645000000000001\n",
      "Epoch [947/100000], Training Loss: -21.106796264648438, Learning Rate: 0.03645000000000001\n",
      "Epoch [948/100000], Training Loss: -21.107025146484375, Learning Rate: 0.03645000000000001\n",
      "Epoch [949/100000], Training Loss: -21.107284545898438, Learning Rate: 0.03645000000000001\n",
      "Epoch [950/100000], Training Loss: -21.10748291015625, Learning Rate: 0.03645000000000001\n",
      "Epoch [951/100000], Training Loss: -21.107772827148438, Learning Rate: 0.03645000000000001\n",
      "Epoch [952/100000], Training Loss: -21.108016967773438, Learning Rate: 0.03645000000000001\n",
      "Epoch [953/100000], Training Loss: -21.1082763671875, Learning Rate: 0.03645000000000001\n",
      "Epoch [954/100000], Training Loss: -21.108551025390625, Learning Rate: 0.03645000000000001\n",
      "Epoch [955/100000], Training Loss: -21.108779907226562, Learning Rate: 0.03645000000000001\n",
      "Epoch [956/100000], Training Loss: -21.109024047851562, Learning Rate: 0.03645000000000001\n",
      "Epoch [957/100000], Training Loss: -21.109283447265625, Learning Rate: 0.03645000000000001\n",
      "Epoch [958/100000], Training Loss: -21.10955810546875, Learning Rate: 0.03645000000000001\n",
      "Epoch [959/100000], Training Loss: -21.10980224609375, Learning Rate: 0.03645000000000001\n",
      "Epoch [960/100000], Training Loss: -21.110061645507812, Learning Rate: 0.03645000000000001\n",
      "Epoch [961/100000], Training Loss: -21.110305786132812, Learning Rate: 0.03645000000000001\n",
      "Epoch [962/100000], Training Loss: -21.110549926757812, Learning Rate: 0.03645000000000001\n",
      "Epoch [963/100000], Training Loss: -21.11077880859375, Learning Rate: 0.03645000000000001\n",
      "Epoch [964/100000], Training Loss: -21.111114501953125, Learning Rate: 0.03645000000000001\n",
      "Epoch [965/100000], Training Loss: -21.111328125, Learning Rate: 0.03645000000000001\n",
      "Epoch [966/100000], Training Loss: -21.111572265625, Learning Rate: 0.03645000000000001\n",
      "Epoch [967/100000], Training Loss: -21.111846923828125, Learning Rate: 0.03645000000000001\n",
      "Epoch [968/100000], Training Loss: -21.112091064453125, Learning Rate: 0.03645000000000001\n",
      "Epoch [969/100000], Training Loss: -21.11236572265625, Learning Rate: 0.03645000000000001\n",
      "Epoch [970/100000], Training Loss: -21.112579345703125, Learning Rate: 0.03645000000000001\n",
      "Epoch [971/100000], Training Loss: -21.112884521484375, Learning Rate: 0.03645000000000001\n",
      "Epoch [972/100000], Training Loss: -21.113113403320312, Learning Rate: 0.03645000000000001\n",
      "Epoch [973/100000], Training Loss: -21.113357543945312, Learning Rate: 0.03645000000000001\n",
      "Epoch [974/100000], Training Loss: -21.113632202148438, Learning Rate: 0.03645000000000001\n",
      "Epoch [975/100000], Training Loss: -21.1138916015625, Learning Rate: 0.03645000000000001\n",
      "Epoch [976/100000], Training Loss: -21.1141357421875, Learning Rate: 0.03645000000000001\n",
      "Epoch [977/100000], Training Loss: -21.114410400390625, Learning Rate: 0.03645000000000001\n",
      "Epoch [978/100000], Training Loss: -21.114654541015625, Learning Rate: 0.03645000000000001\n",
      "Epoch [979/100000], Training Loss: -21.114913940429688, Learning Rate: 0.03645000000000001\n",
      "Epoch [980/100000], Training Loss: -21.115188598632812, Learning Rate: 0.03645000000000001\n",
      "Epoch [981/100000], Training Loss: -21.11541748046875, Learning Rate: 0.03645000000000001\n",
      "Epoch [982/100000], Training Loss: -21.115707397460938, Learning Rate: 0.03645000000000001\n",
      "Epoch [983/100000], Training Loss: -21.115966796875, Learning Rate: 0.03645000000000001\n",
      "Epoch [984/100000], Training Loss: -21.116226196289062, Learning Rate: 0.03645000000000001\n",
      "Epoch [985/100000], Training Loss: -21.116455078125, Learning Rate: 0.03645000000000001\n",
      "Epoch [986/100000], Training Loss: -21.11669921875, Learning Rate: 0.03645000000000001\n",
      "Epoch [987/100000], Training Loss: -21.117019653320312, Learning Rate: 0.03645000000000001\n",
      "Epoch [988/100000], Training Loss: -21.117233276367188, Learning Rate: 0.03645000000000001\n",
      "Epoch [989/100000], Training Loss: -21.117538452148438, Learning Rate: 0.03645000000000001\n",
      "Epoch [990/100000], Training Loss: -21.117767333984375, Learning Rate: 0.03645000000000001\n",
      "Epoch [991/100000], Training Loss: -21.118011474609375, Learning Rate: 0.03645000000000001\n",
      "Epoch [992/100000], Training Loss: -21.118301391601562, Learning Rate: 0.03645000000000001\n",
      "Epoch [993/100000], Training Loss: -21.118560791015625, Learning Rate: 0.03645000000000001\n",
      "Epoch [994/100000], Training Loss: -21.1187744140625, Learning Rate: 0.03645000000000001\n",
      "Epoch [995/100000], Training Loss: -21.119094848632812, Learning Rate: 0.03645000000000001\n",
      "Epoch [996/100000], Training Loss: -21.119308471679688, Learning Rate: 0.03645000000000001\n",
      "Epoch [997/100000], Training Loss: -21.119583129882812, Learning Rate: 0.03645000000000001\n",
      "Epoch [998/100000], Training Loss: -21.11981201171875, Learning Rate: 0.03645000000000001\n",
      "Epoch [999/100000], Training Loss: -21.120132446289062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1000/100000], Training Loss: -21.120391845703125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1001/100000], Training Loss: -21.120635986328125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1002/100000], Training Loss: -21.120941162109375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1003/100000], Training Loss: -21.121109008789062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1004/100000], Training Loss: -21.121353149414062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1005/100000], Training Loss: -21.121627807617188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1006/100000], Training Loss: -21.121978759765625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1007/100000], Training Loss: -21.122177124023438, Learning Rate: 0.03645000000000001\n",
      "Epoch [1008/100000], Training Loss: -21.122406005859375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1009/100000], Training Loss: -21.12274169921875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1010/100000], Training Loss: -21.1229248046875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1011/100000], Training Loss: -21.123199462890625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1012/100000], Training Loss: -21.12347412109375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1013/100000], Training Loss: -21.123748779296875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1014/100000], Training Loss: -21.124008178710938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1015/100000], Training Loss: -21.124282836914062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1016/100000], Training Loss: -21.12457275390625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1017/100000], Training Loss: -21.124801635742188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1018/100000], Training Loss: -21.125045776367188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1019/100000], Training Loss: -21.125320434570312, Learning Rate: 0.03645000000000001\n",
      "Epoch [1020/100000], Training Loss: -21.125595092773438, Learning Rate: 0.03645000000000001\n",
      "Epoch [1021/100000], Training Loss: -21.1258544921875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1022/100000], Training Loss: -21.126113891601562, Learning Rate: 0.03645000000000001\n",
      "Epoch [1023/100000], Training Loss: -21.12640380859375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1024/100000], Training Loss: -21.12664794921875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1025/100000], Training Loss: -21.126922607421875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1026/100000], Training Loss: -21.127166748046875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1027/100000], Training Loss: -21.127456665039062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1028/100000], Training Loss: -21.127731323242188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1029/100000], Training Loss: -21.127960205078125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1030/100000], Training Loss: -21.128204345703125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1031/100000], Training Loss: -21.128448486328125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1032/100000], Training Loss: -21.128692626953125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1033/100000], Training Loss: -21.129013061523438, Learning Rate: 0.03645000000000001\n",
      "Epoch [1034/100000], Training Loss: -21.129226684570312, Learning Rate: 0.03645000000000001\n",
      "Epoch [1035/100000], Training Loss: -21.129562377929688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1036/100000], Training Loss: -21.129791259765625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1037/100000], Training Loss: -21.130081176757812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1038/100000], Training Loss: -21.13037109375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1039/100000], Training Loss: -21.130584716796875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1040/100000], Training Loss: -21.130859375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1041/100000], Training Loss: -21.131134033203125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1042/100000], Training Loss: -21.131393432617188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1043/100000], Training Loss: -21.131683349609375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1044/100000], Training Loss: -21.13189697265625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1045/100000], Training Loss: -21.1322021484375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1046/100000], Training Loss: -21.1324462890625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1047/100000], Training Loss: -21.132705688476562, Learning Rate: 0.03645000000000001\n",
      "Epoch [1048/100000], Training Loss: -21.1329345703125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1049/100000], Training Loss: -21.133255004882812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1050/100000], Training Loss: -21.133529663085938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1051/100000], Training Loss: -21.133743286132812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1052/100000], Training Loss: -21.134048461914062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1053/100000], Training Loss: -21.134323120117188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1054/100000], Training Loss: -21.1346435546875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1055/100000], Training Loss: -21.134872436523438, Learning Rate: 0.03645000000000001\n",
      "Epoch [1056/100000], Training Loss: -21.135101318359375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1057/100000], Training Loss: -21.135406494140625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1058/100000], Training Loss: -21.135665893554688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1059/100000], Training Loss: -21.13592529296875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1060/100000], Training Loss: -21.136215209960938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1061/100000], Training Loss: -21.136505126953125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1062/100000], Training Loss: -21.136749267578125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1063/100000], Training Loss: -21.136962890625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1064/100000], Training Loss: -21.137222290039062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1065/100000], Training Loss: -21.137527465820312, Learning Rate: 0.03645000000000001\n",
      "Epoch [1066/100000], Training Loss: -21.137802124023438, Learning Rate: 0.03645000000000001\n",
      "Epoch [1067/100000], Training Loss: -21.1380615234375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1068/100000], Training Loss: -21.13836669921875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1069/100000], Training Loss: -21.138626098632812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1070/100000], Training Loss: -21.138870239257812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1071/100000], Training Loss: -21.139144897460938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1072/100000], Training Loss: -21.139373779296875, Learning Rate: 0.03645000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1073/100000], Training Loss: -21.139678955078125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1074/100000], Training Loss: -21.139938354492188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1075/100000], Training Loss: -21.140182495117188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1076/100000], Training Loss: -21.140457153320312, Learning Rate: 0.03645000000000001\n",
      "Epoch [1077/100000], Training Loss: -21.1407470703125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1078/100000], Training Loss: -21.141006469726562, Learning Rate: 0.03645000000000001\n",
      "Epoch [1079/100000], Training Loss: -21.141281127929688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1080/100000], Training Loss: -21.141571044921875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1081/100000], Training Loss: -21.141799926757812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1082/100000], Training Loss: -21.142105102539062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1083/100000], Training Loss: -21.142333984375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1084/100000], Training Loss: -21.142623901367188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1085/100000], Training Loss: -21.142868041992188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1086/100000], Training Loss: -21.143173217773438, Learning Rate: 0.03645000000000001\n",
      "Epoch [1087/100000], Training Loss: -21.143402099609375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1088/100000], Training Loss: -21.1436767578125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1089/100000], Training Loss: -21.143966674804688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1090/100000], Training Loss: -21.144241333007812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1091/100000], Training Loss: -21.144500732421875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1092/100000], Training Loss: -21.144775390625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1093/100000], Training Loss: -21.145050048828125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1094/100000], Training Loss: -21.145309448242188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1095/100000], Training Loss: -21.145523071289062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1096/100000], Training Loss: -21.145843505859375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1097/100000], Training Loss: -21.146163940429688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1098/100000], Training Loss: -21.146377563476562, Learning Rate: 0.03645000000000001\n",
      "Epoch [1099/100000], Training Loss: -21.14666748046875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1100/100000], Training Loss: -21.146926879882812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1101/100000], Training Loss: -21.147186279296875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1102/100000], Training Loss: -21.147476196289062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1103/100000], Training Loss: -21.147720336914062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1104/100000], Training Loss: -21.14801025390625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1105/100000], Training Loss: -21.148269653320312, Learning Rate: 0.03645000000000001\n",
      "Epoch [1106/100000], Training Loss: -21.148513793945312, Learning Rate: 0.03645000000000001\n",
      "Epoch [1107/100000], Training Loss: -21.1488037109375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1108/100000], Training Loss: -21.149093627929688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1109/100000], Training Loss: -21.14935302734375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1110/100000], Training Loss: -21.149612426757812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1111/100000], Training Loss: -21.14990234375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1112/100000], Training Loss: -21.150177001953125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1113/100000], Training Loss: -21.15045166015625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1114/100000], Training Loss: -21.150726318359375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1115/100000], Training Loss: -21.151046752929688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1116/100000], Training Loss: -21.151229858398438, Learning Rate: 0.03645000000000001\n",
      "Epoch [1117/100000], Training Loss: -21.15155029296875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1118/100000], Training Loss: -21.151809692382812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1119/100000], Training Loss: -21.152069091796875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1120/100000], Training Loss: -21.152328491210938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1121/100000], Training Loss: -21.152603149414062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1122/100000], Training Loss: -21.152816772460938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1123/100000], Training Loss: -21.153091430664062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1124/100000], Training Loss: -21.153289794921875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1125/100000], Training Loss: -21.15338134765625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1126/100000], Training Loss: -21.153350830078125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1127/100000], Training Loss: -21.153030395507812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1128/100000], Training Loss: -21.152084350585938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1129/100000], Training Loss: -21.149826049804688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1130/100000], Training Loss: -21.144546508789062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1131/100000], Training Loss: -21.132781982421875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1132/100000], Training Loss: -21.106185913085938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1133/100000], Training Loss: -21.0467529296875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1134/100000], Training Loss: -20.907989501953125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1135/100000], Training Loss: -20.596847534179688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1136/100000], Training Loss: -19.843994140625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1137/100000], Training Loss: -18.228485107421875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1138/100000], Training Loss: -14.311065673828125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1139/100000], Training Loss: -7.885498046875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1140/100000], Training Loss: 4.170379638671875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1141/100000], Training Loss: 4.9932861328125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1142/100000], Training Loss: -4.6522369384765625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1143/100000], Training Loss: -20.298065185546875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1144/100000], Training Loss: -14.212631225585938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1145/100000], Training Loss: -7.921417236328125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1146/100000], Training Loss: -19.59063720703125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1147/100000], Training Loss: -16.238754272460938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1148/100000], Training Loss: -12.874923706054688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1149/100000], Training Loss: -20.985946655273438, Learning Rate: 0.03645000000000001\n",
      "Epoch [1150/100000], Training Loss: -15.284332275390625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1151/100000], Training Loss: -17.362503051757812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1152/100000], Training Loss: -20.361984252929688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1153/100000], Training Loss: -15.7569580078125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1154/100000], Training Loss: -20.513458251953125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1155/100000], Training Loss: -18.510116577148438, Learning Rate: 0.03645000000000001\n",
      "Epoch [1156/100000], Training Loss: -18.337066650390625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1157/100000], Training Loss: -20.854766845703125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1158/100000], Training Loss: -18.079620361328125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1159/100000], Training Loss: -20.619857788085938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1160/100000], Training Loss: -19.722030639648438, Learning Rate: 0.03645000000000001\n",
      "Epoch [1161/100000], Training Loss: -19.401824951171875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1162/100000], Training Loss: -21.013092041015625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1163/100000], Training Loss: -19.309463500976562, Learning Rate: 0.03645000000000001\n",
      "Epoch [1164/100000], Training Loss: -20.749984741210938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1165/100000], Training Loss: -20.36004638671875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1166/100000], Training Loss: -20.0064697265625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1167/100000], Training Loss: -21.056594848632812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1168/100000], Training Loss: -20.059112548828125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1169/100000], Training Loss: -20.829544067382812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1170/100000], Training Loss: -20.714340209960938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1171/100000], Training Loss: -20.410369873046875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1172/100000], Training Loss: -21.082595825195312, Learning Rate: 0.03645000000000001\n",
      "Epoch [1173/100000], Training Loss: -20.50140380859375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1174/100000], Training Loss: -20.891525268554688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1175/100000], Training Loss: -20.917633056640625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1176/100000], Training Loss: -20.655532836914062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1177/100000], Training Loss: -21.084686279296875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1178/100000], Training Loss: -20.768692016601562, Learning Rate: 0.03645000000000001\n",
      "Epoch [1179/100000], Training Loss: -20.929168701171875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1180/100000], Training Loss: -21.019866943359375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1181/100000], Training Loss: -20.816574096679688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1182/100000], Training Loss: -21.076812744140625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1183/100000], Training Loss: -20.924758911132812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1184/100000], Training Loss: -20.96624755859375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1185/100000], Training Loss: -21.070266723632812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1186/100000], Training Loss: -20.925643920898438, Learning Rate: 0.03645000000000001\n",
      "Epoch [1187/100000], Training Loss: -21.070480346679688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1188/100000], Training Loss: -21.01409912109375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1189/100000], Training Loss: -20.997085571289062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1190/100000], Training Loss: -21.086944580078125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1191/100000], Training Loss: -20.996337890625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1192/100000], Training Loss: -21.06500244140625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1193/100000], Training Loss: -21.060714721679688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1194/100000], Training Loss: -21.025421142578125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1195/100000], Training Loss: -21.090606689453125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1196/100000], Training Loss: -21.042984008789062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1197/100000], Training Loss: -21.065521240234375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1198/100000], Training Loss: -21.083816528320312, Learning Rate: 0.03645000000000001\n",
      "Epoch [1199/100000], Training Loss: -21.049560546875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1200/100000], Training Loss: -21.088470458984375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1201/100000], Training Loss: -21.071182250976562, Learning Rate: 0.03645000000000001\n",
      "Epoch [1202/100000], Training Loss: -21.068893432617188, Learning Rate: 0.03645000000000001\n",
      "Epoch [1203/100000], Training Loss: -21.091827392578125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1204/100000], Training Loss: -21.06866455078125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1205/100000], Training Loss: -21.085861206054688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1206/100000], Training Loss: -21.086700439453125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1207/100000], Training Loss: -21.07598876953125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1208/100000], Training Loss: -21.093429565429688, Learning Rate: 0.03645000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1209/100000], Training Loss: -21.082931518554688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1210/100000], Training Loss: -21.085708618164062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1211/100000], Training Loss: -21.093521118164062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1212/100000], Training Loss: -21.083755493164062, Learning Rate: 0.03645000000000001\n",
      "Epoch [1213/100000], Training Loss: -21.09246826171875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1214/100000], Training Loss: -21.091461181640625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1215/100000], Training Loss: -21.087799072265625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1216/100000], Training Loss: -21.095352172851562, Learning Rate: 0.03645000000000001\n",
      "Epoch [1217/100000], Training Loss: -21.090545654296875, Learning Rate: 0.03645000000000001\n",
      "Epoch [1218/100000], Training Loss: -21.092315673828125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1219/100000], Training Loss: -21.095687866210938, Learning Rate: 0.03645000000000001\n",
      "Epoch [1220/100000], Training Loss: -21.09161376953125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1221/100000], Training Loss: -21.095535278320312, Learning Rate: 0.03645000000000001\n",
      "Epoch [1222/100000], Training Loss: -21.0953369140625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1223/100000], Training Loss: -21.093673706054688, Learning Rate: 0.03645000000000001\n",
      "Epoch [1224/100000], Training Loss: -21.097122192382812, Learning Rate: 0.03645000000000001\n",
      "Epoch [1225/100000], Training Loss: -21.095306396484375, Learning Rate: 0.03645000000000001\n",
      "Epoch [1226/100000], Training Loss: -21.095840454101562, Learning Rate: 0.03645000000000001\n",
      "Epoch [1227/100000], Training Loss: -21.09771728515625, Learning Rate: 0.03645000000000001\n",
      "Epoch [1228/100000], Training Loss: -21.09600830078125, Learning Rate: 0.03645000000000001\n",
      "Epoch [1229/100000], Training Loss: -21.097549438476562, Learning Rate: 0.03645000000000001\n",
      "Epoch [1230/100000], Training Loss: -21.097991943359375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1231/100000], Training Loss: -21.09747314453125, Learning Rate: 0.03280500000000001\n",
      "Epoch [1232/100000], Training Loss: -21.098831176757812, Learning Rate: 0.03280500000000001\n",
      "Epoch [1233/100000], Training Loss: -21.0980224609375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1234/100000], Training Loss: -21.098968505859375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1235/100000], Training Loss: -21.098922729492188, Learning Rate: 0.03280500000000001\n",
      "Epoch [1236/100000], Training Loss: -21.0989990234375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1237/100000], Training Loss: -21.099700927734375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1238/100000], Training Loss: -21.099319458007812, Learning Rate: 0.03280500000000001\n",
      "Epoch [1239/100000], Training Loss: -21.100067138671875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1240/100000], Training Loss: -21.0999755859375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1241/100000], Training Loss: -21.100234985351562, Learning Rate: 0.03280500000000001\n",
      "Epoch [1242/100000], Training Loss: -21.100601196289062, Learning Rate: 0.03280500000000001\n",
      "Epoch [1243/100000], Training Loss: -21.10052490234375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1244/100000], Training Loss: -21.10107421875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1245/100000], Training Loss: -21.100982666015625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1246/100000], Training Loss: -21.101287841796875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1247/100000], Training Loss: -21.101547241210938, Learning Rate: 0.03280500000000001\n",
      "Epoch [1248/100000], Training Loss: -21.1015625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1249/100000], Training Loss: -21.102005004882812, Learning Rate: 0.03280500000000001\n",
      "Epoch [1250/100000], Training Loss: -21.10198974609375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1251/100000], Training Loss: -21.102279663085938, Learning Rate: 0.03280500000000001\n",
      "Epoch [1252/100000], Training Loss: -21.102508544921875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1253/100000], Training Loss: -21.102645874023438, Learning Rate: 0.03280500000000001\n",
      "Epoch [1254/100000], Training Loss: -21.102920532226562, Learning Rate: 0.03280500000000001\n",
      "Epoch [1255/100000], Training Loss: -21.102951049804688, Learning Rate: 0.03280500000000001\n",
      "Epoch [1256/100000], Training Loss: -21.103225708007812, Learning Rate: 0.03280500000000001\n",
      "Epoch [1257/100000], Training Loss: -21.103439331054688, Learning Rate: 0.03280500000000001\n",
      "Epoch [1258/100000], Training Loss: -21.103561401367188, Learning Rate: 0.03280500000000001\n",
      "Epoch [1259/100000], Training Loss: -21.103805541992188, Learning Rate: 0.03280500000000001\n",
      "Epoch [1260/100000], Training Loss: -21.10394287109375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1261/100000], Training Loss: -21.104156494140625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1262/100000], Training Loss: -21.1043701171875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1263/100000], Training Loss: -21.10455322265625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1264/100000], Training Loss: -21.104736328125, Learning Rate: 0.03280500000000001\n",
      "Epoch [1265/100000], Training Loss: -21.1048583984375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1266/100000], Training Loss: -21.105072021484375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1267/100000], Training Loss: -21.105239868164062, Learning Rate: 0.03280500000000001\n",
      "Epoch [1268/100000], Training Loss: -21.105422973632812, Learning Rate: 0.03280500000000001\n",
      "Epoch [1269/100000], Training Loss: -21.105636596679688, Learning Rate: 0.03280500000000001\n",
      "Epoch [1270/100000], Training Loss: -21.105789184570312, Learning Rate: 0.03280500000000001\n",
      "Epoch [1271/100000], Training Loss: -21.105987548828125, Learning Rate: 0.03280500000000001\n",
      "Epoch [1272/100000], Training Loss: -21.106170654296875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1273/100000], Training Loss: -21.106338500976562, Learning Rate: 0.03280500000000001\n",
      "Epoch [1274/100000], Training Loss: -21.106552124023438, Learning Rate: 0.03280500000000001\n",
      "Epoch [1275/100000], Training Loss: -21.106674194335938, Learning Rate: 0.03280500000000001\n",
      "Epoch [1276/100000], Training Loss: -21.106903076171875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1277/100000], Training Loss: -21.107070922851562, Learning Rate: 0.03280500000000001\n",
      "Epoch [1278/100000], Training Loss: -21.10723876953125, Learning Rate: 0.03280500000000001\n",
      "Epoch [1279/100000], Training Loss: -21.107437133789062, Learning Rate: 0.03280500000000001\n",
      "Epoch [1280/100000], Training Loss: -21.107635498046875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1281/100000], Training Loss: -21.107818603515625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1282/100000], Training Loss: -21.10797119140625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1283/100000], Training Loss: -21.108169555664062, Learning Rate: 0.03280500000000001\n",
      "Epoch [1284/100000], Training Loss: -21.10833740234375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1285/100000], Training Loss: -21.1085205078125, Learning Rate: 0.03280500000000001\n",
      "Epoch [1286/100000], Training Loss: -21.108673095703125, Learning Rate: 0.03280500000000001\n",
      "Epoch [1287/100000], Training Loss: -21.10888671875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1288/100000], Training Loss: -21.109039306640625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1289/100000], Training Loss: -21.109222412109375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1290/100000], Training Loss: -21.10943603515625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1291/100000], Training Loss: -21.109588623046875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1292/100000], Training Loss: -21.109771728515625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1293/100000], Training Loss: -21.10992431640625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1294/100000], Training Loss: -21.110122680664062, Learning Rate: 0.03280500000000001\n",
      "Epoch [1295/100000], Training Loss: -21.110336303710938, Learning Rate: 0.03280500000000001\n",
      "Epoch [1296/100000], Training Loss: -21.11041259765625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1297/100000], Training Loss: -21.11065673828125, Learning Rate: 0.03280500000000001\n",
      "Epoch [1298/100000], Training Loss: -21.110824584960938, Learning Rate: 0.03280500000000001\n",
      "Epoch [1299/100000], Training Loss: -21.111053466796875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1300/100000], Training Loss: -21.111190795898438, Learning Rate: 0.03280500000000001\n",
      "Epoch [1301/100000], Training Loss: -21.11138916015625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1302/100000], Training Loss: -21.111541748046875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1303/100000], Training Loss: -21.111709594726562, Learning Rate: 0.03280500000000001\n",
      "Epoch [1304/100000], Training Loss: -21.111892700195312, Learning Rate: 0.03280500000000001\n",
      "Epoch [1305/100000], Training Loss: -21.112075805664062, Learning Rate: 0.03280500000000001\n",
      "Epoch [1306/100000], Training Loss: -21.112274169921875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1307/100000], Training Loss: -21.112396240234375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1308/100000], Training Loss: -21.112625122070312, Learning Rate: 0.03280500000000001\n",
      "Epoch [1309/100000], Training Loss: -21.112838745117188, Learning Rate: 0.03280500000000001\n",
      "Epoch [1310/100000], Training Loss: -21.113006591796875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1311/100000], Training Loss: -21.1131591796875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1312/100000], Training Loss: -21.113327026367188, Learning Rate: 0.03280500000000001\n",
      "Epoch [1313/100000], Training Loss: -21.113510131835938, Learning Rate: 0.03280500000000001\n",
      "Epoch [1314/100000], Training Loss: -21.11370849609375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1315/100000], Training Loss: -21.113922119140625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1316/100000], Training Loss: -21.114120483398438, Learning Rate: 0.03280500000000001\n",
      "Epoch [1317/100000], Training Loss: -21.114227294921875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1318/100000], Training Loss: -21.114395141601562, Learning Rate: 0.03280500000000001\n",
      "Epoch [1319/100000], Training Loss: -21.114578247070312, Learning Rate: 0.03280500000000001\n",
      "Epoch [1320/100000], Training Loss: -21.114761352539062, Learning Rate: 0.03280500000000001\n",
      "Epoch [1321/100000], Training Loss: -21.114913940429688, Learning Rate: 0.03280500000000001\n",
      "Epoch [1322/100000], Training Loss: -21.115097045898438, Learning Rate: 0.03280500000000001\n",
      "Epoch [1323/100000], Training Loss: -21.115341186523438, Learning Rate: 0.03280500000000001\n",
      "Epoch [1324/100000], Training Loss: -21.115509033203125, Learning Rate: 0.03280500000000001\n",
      "Epoch [1325/100000], Training Loss: -21.115631103515625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1326/100000], Training Loss: -21.115814208984375, Learning Rate: 0.03280500000000001\n",
      "Epoch [1327/100000], Training Loss: -21.116012573242188, Learning Rate: 0.03280500000000001\n",
      "Epoch [1328/100000], Training Loss: -21.116180419921875, Learning Rate: 0.03280500000000001\n",
      "Epoch [1329/100000], Training Loss: -21.116363525390625, Learning Rate: 0.03280500000000001\n",
      "Epoch [1330/100000], Training Loss: -21.116561889648438, Learning Rate: 0.03280500000000001\n",
      "Epoch [1331/100000], Training Loss: -21.116668701171875, Learning Rate: 0.02952450000000001\n",
      "Epoch [1332/100000], Training Loss: -21.11688232421875, Learning Rate: 0.02952450000000001\n",
      "Epoch [1333/100000], Training Loss: -21.117019653320312, Learning Rate: 0.02952450000000001\n",
      "Epoch [1334/100000], Training Loss: -21.117218017578125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1335/100000], Training Loss: -21.117355346679688, Learning Rate: 0.02952450000000001\n",
      "Epoch [1336/100000], Training Loss: -21.117523193359375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1337/100000], Training Loss: -21.117706298828125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1338/100000], Training Loss: -21.117843627929688, Learning Rate: 0.02952450000000001\n",
      "Epoch [1339/100000], Training Loss: -21.118026733398438, Learning Rate: 0.02952450000000001\n",
      "Epoch [1340/100000], Training Loss: -21.1181640625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1341/100000], Training Loss: -21.118316650390625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1342/100000], Training Loss: -21.118484497070312, Learning Rate: 0.02952450000000001\n",
      "Epoch [1343/100000], Training Loss: -21.118637084960938, Learning Rate: 0.02952450000000001\n",
      "Epoch [1344/100000], Training Loss: -21.118804931640625, Learning Rate: 0.02952450000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1345/100000], Training Loss: -21.118927001953125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1346/100000], Training Loss: -21.119140625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1347/100000], Training Loss: -21.119308471679688, Learning Rate: 0.02952450000000001\n",
      "Epoch [1348/100000], Training Loss: -21.119461059570312, Learning Rate: 0.02952450000000001\n",
      "Epoch [1349/100000], Training Loss: -21.119583129882812, Learning Rate: 0.02952450000000001\n",
      "Epoch [1350/100000], Training Loss: -21.119781494140625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1351/100000], Training Loss: -21.119949340820312, Learning Rate: 0.02952450000000001\n",
      "Epoch [1352/100000], Training Loss: -21.120101928710938, Learning Rate: 0.02952450000000001\n",
      "Epoch [1353/100000], Training Loss: -21.120223999023438, Learning Rate: 0.02952450000000001\n",
      "Epoch [1354/100000], Training Loss: -21.120437622070312, Learning Rate: 0.02952450000000001\n",
      "Epoch [1355/100000], Training Loss: -21.120635986328125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1356/100000], Training Loss: -21.120803833007812, Learning Rate: 0.02952450000000001\n",
      "Epoch [1357/100000], Training Loss: -21.120941162109375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1358/100000], Training Loss: -21.121063232421875, Learning Rate: 0.02952450000000001\n",
      "Epoch [1359/100000], Training Loss: -21.121231079101562, Learning Rate: 0.02952450000000001\n",
      "Epoch [1360/100000], Training Loss: -21.121368408203125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1361/100000], Training Loss: -21.121551513671875, Learning Rate: 0.02952450000000001\n",
      "Epoch [1362/100000], Training Loss: -21.121749877929688, Learning Rate: 0.02952450000000001\n",
      "Epoch [1363/100000], Training Loss: -21.121902465820312, Learning Rate: 0.02952450000000001\n",
      "Epoch [1364/100000], Training Loss: -21.122039794921875, Learning Rate: 0.02952450000000001\n",
      "Epoch [1365/100000], Training Loss: -21.122222900390625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1366/100000], Training Loss: -21.12237548828125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1367/100000], Training Loss: -21.122543334960938, Learning Rate: 0.02952450000000001\n",
      "Epoch [1368/100000], Training Loss: -21.122711181640625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1369/100000], Training Loss: -21.122894287109375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1370/100000], Training Loss: -21.123046875, Learning Rate: 0.02952450000000001\n",
      "Epoch [1371/100000], Training Loss: -21.123184204101562, Learning Rate: 0.02952450000000001\n",
      "Epoch [1372/100000], Training Loss: -21.12335205078125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1373/100000], Training Loss: -21.12353515625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1374/100000], Training Loss: -21.123687744140625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1375/100000], Training Loss: -21.123870849609375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1376/100000], Training Loss: -21.123977661132812, Learning Rate: 0.02952450000000001\n",
      "Epoch [1377/100000], Training Loss: -21.124176025390625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1378/100000], Training Loss: -21.124359130859375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1379/100000], Training Loss: -21.124542236328125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1380/100000], Training Loss: -21.12469482421875, Learning Rate: 0.02952450000000001\n",
      "Epoch [1381/100000], Training Loss: -21.124847412109375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1382/100000], Training Loss: -21.125030517578125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1383/100000], Training Loss: -21.125198364257812, Learning Rate: 0.02952450000000001\n",
      "Epoch [1384/100000], Training Loss: -21.125335693359375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1385/100000], Training Loss: -21.125518798828125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1386/100000], Training Loss: -21.125686645507812, Learning Rate: 0.02952450000000001\n",
      "Epoch [1387/100000], Training Loss: -21.125823974609375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1388/100000], Training Loss: -21.125991821289062, Learning Rate: 0.02952450000000001\n",
      "Epoch [1389/100000], Training Loss: -21.126220703125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1390/100000], Training Loss: -21.126358032226562, Learning Rate: 0.02952450000000001\n",
      "Epoch [1391/100000], Training Loss: -21.12652587890625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1392/100000], Training Loss: -21.126693725585938, Learning Rate: 0.02952450000000001\n",
      "Epoch [1393/100000], Training Loss: -21.126800537109375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1394/100000], Training Loss: -21.126998901367188, Learning Rate: 0.02952450000000001\n",
      "Epoch [1395/100000], Training Loss: -21.127212524414062, Learning Rate: 0.02952450000000001\n",
      "Epoch [1396/100000], Training Loss: -21.127365112304688, Learning Rate: 0.02952450000000001\n",
      "Epoch [1397/100000], Training Loss: -21.12750244140625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1398/100000], Training Loss: -21.127700805664062, Learning Rate: 0.02952450000000001\n",
      "Epoch [1399/100000], Training Loss: -21.127838134765625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1400/100000], Training Loss: -21.128021240234375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1401/100000], Training Loss: -21.128173828125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1402/100000], Training Loss: -21.128341674804688, Learning Rate: 0.02952450000000001\n",
      "Epoch [1403/100000], Training Loss: -21.128509521484375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1404/100000], Training Loss: -21.128692626953125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1405/100000], Training Loss: -21.128860473632812, Learning Rate: 0.02952450000000001\n",
      "Epoch [1406/100000], Training Loss: -21.129043579101562, Learning Rate: 0.02952450000000001\n",
      "Epoch [1407/100000], Training Loss: -21.129165649414062, Learning Rate: 0.02952450000000001\n",
      "Epoch [1408/100000], Training Loss: -21.12939453125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1409/100000], Training Loss: -21.129531860351562, Learning Rate: 0.02952450000000001\n",
      "Epoch [1410/100000], Training Loss: -21.12969970703125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1411/100000], Training Loss: -21.129852294921875, Learning Rate: 0.02952450000000001\n",
      "Epoch [1412/100000], Training Loss: -21.130050659179688, Learning Rate: 0.02952450000000001\n",
      "Epoch [1413/100000], Training Loss: -21.130233764648438, Learning Rate: 0.02952450000000001\n",
      "Epoch [1414/100000], Training Loss: -21.130401611328125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1415/100000], Training Loss: -21.130523681640625, Learning Rate: 0.02952450000000001\n",
      "Epoch [1416/100000], Training Loss: -21.130706787109375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1417/100000], Training Loss: -21.130889892578125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1418/100000], Training Loss: -21.13104248046875, Learning Rate: 0.02952450000000001\n",
      "Epoch [1419/100000], Training Loss: -21.1312255859375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1420/100000], Training Loss: -21.131423950195312, Learning Rate: 0.02952450000000001\n",
      "Epoch [1421/100000], Training Loss: -21.13153076171875, Learning Rate: 0.02952450000000001\n",
      "Epoch [1422/100000], Training Loss: -21.1317138671875, Learning Rate: 0.02952450000000001\n",
      "Epoch [1423/100000], Training Loss: -21.131912231445312, Learning Rate: 0.02952450000000001\n",
      "Epoch [1424/100000], Training Loss: -21.132064819335938, Learning Rate: 0.02952450000000001\n",
      "Epoch [1425/100000], Training Loss: -21.13226318359375, Learning Rate: 0.02952450000000001\n",
      "Epoch [1426/100000], Training Loss: -21.132400512695312, Learning Rate: 0.02952450000000001\n",
      "Epoch [1427/100000], Training Loss: -21.132583618164062, Learning Rate: 0.02952450000000001\n",
      "Epoch [1428/100000], Training Loss: -21.132766723632812, Learning Rate: 0.02952450000000001\n",
      "Epoch [1429/100000], Training Loss: -21.1329345703125, Learning Rate: 0.02952450000000001\n",
      "Epoch [1430/100000], Training Loss: -21.133071899414062, Learning Rate: 0.02952450000000001\n",
      "Epoch [1431/100000], Training Loss: -21.133255004882812, Learning Rate: 0.02952450000000001\n",
      "Epoch [1432/100000], Training Loss: -21.1334228515625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1433/100000], Training Loss: -21.13360595703125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1434/100000], Training Loss: -21.13372802734375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1435/100000], Training Loss: -21.133880615234375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1436/100000], Training Loss: -21.134048461914062, Learning Rate: 0.02657205000000001\n",
      "Epoch [1437/100000], Training Loss: -21.13421630859375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1438/100000], Training Loss: -21.134384155273438, Learning Rate: 0.02657205000000001\n",
      "Epoch [1439/100000], Training Loss: -21.134536743164062, Learning Rate: 0.02657205000000001\n",
      "Epoch [1440/100000], Training Loss: -21.134689331054688, Learning Rate: 0.02657205000000001\n",
      "Epoch [1441/100000], Training Loss: -21.134857177734375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1442/100000], Training Loss: -21.13494873046875, Learning Rate: 0.02657205000000001\n",
      "Epoch [1443/100000], Training Loss: -21.135177612304688, Learning Rate: 0.02657205000000001\n",
      "Epoch [1444/100000], Training Loss: -21.135330200195312, Learning Rate: 0.02657205000000001\n",
      "Epoch [1445/100000], Training Loss: -21.135467529296875, Learning Rate: 0.02657205000000001\n",
      "Epoch [1446/100000], Training Loss: -21.135574340820312, Learning Rate: 0.02657205000000001\n",
      "Epoch [1447/100000], Training Loss: -21.135787963867188, Learning Rate: 0.02657205000000001\n",
      "Epoch [1448/100000], Training Loss: -21.135910034179688, Learning Rate: 0.02657205000000001\n",
      "Epoch [1449/100000], Training Loss: -21.136093139648438, Learning Rate: 0.02657205000000001\n",
      "Epoch [1450/100000], Training Loss: -21.136260986328125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1451/100000], Training Loss: -21.136398315429688, Learning Rate: 0.02657205000000001\n",
      "Epoch [1452/100000], Training Loss: -21.136566162109375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1453/100000], Training Loss: -21.13671875, Learning Rate: 0.02657205000000001\n",
      "Epoch [1454/100000], Training Loss: -21.136856079101562, Learning Rate: 0.02657205000000001\n",
      "Epoch [1455/100000], Training Loss: -21.13702392578125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1456/100000], Training Loss: -21.13720703125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1457/100000], Training Loss: -21.137344360351562, Learning Rate: 0.02657205000000001\n",
      "Epoch [1458/100000], Training Loss: -21.13751220703125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1459/100000], Training Loss: -21.137664794921875, Learning Rate: 0.02657205000000001\n",
      "Epoch [1460/100000], Training Loss: -21.137863159179688, Learning Rate: 0.02657205000000001\n",
      "Epoch [1461/100000], Training Loss: -21.13800048828125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1462/100000], Training Loss: -21.138137817382812, Learning Rate: 0.02657205000000001\n",
      "Epoch [1463/100000], Training Loss: -21.138336181640625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1464/100000], Training Loss: -21.138427734375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1465/100000], Training Loss: -21.138656616210938, Learning Rate: 0.02657205000000001\n",
      "Epoch [1466/100000], Training Loss: -21.138748168945312, Learning Rate: 0.02657205000000001\n",
      "Epoch [1467/100000], Training Loss: -21.138946533203125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1468/100000], Training Loss: -21.139114379882812, Learning Rate: 0.02657205000000001\n",
      "Epoch [1469/100000], Training Loss: -21.13922119140625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1470/100000], Training Loss: -21.139404296875, Learning Rate: 0.02657205000000001\n",
      "Epoch [1471/100000], Training Loss: -21.139602661132812, Learning Rate: 0.02657205000000001\n",
      "Epoch [1472/100000], Training Loss: -21.1397705078125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1473/100000], Training Loss: -21.139923095703125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1474/100000], Training Loss: -21.140045166015625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1475/100000], Training Loss: -21.1402587890625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1476/100000], Training Loss: -21.140380859375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1477/100000], Training Loss: -21.140548706054688, Learning Rate: 0.02657205000000001\n",
      "Epoch [1478/100000], Training Loss: -21.14068603515625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1479/100000], Training Loss: -21.140899658203125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1480/100000], Training Loss: -21.141036987304688, Learning Rate: 0.02657205000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1481/100000], Training Loss: -21.141143798828125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1482/100000], Training Loss: -21.141357421875, Learning Rate: 0.02657205000000001\n",
      "Epoch [1483/100000], Training Loss: -21.14154052734375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1484/100000], Training Loss: -21.141693115234375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1485/100000], Training Loss: -21.141830444335938, Learning Rate: 0.02657205000000001\n",
      "Epoch [1486/100000], Training Loss: -21.141983032226562, Learning Rate: 0.02657205000000001\n",
      "Epoch [1487/100000], Training Loss: -21.142166137695312, Learning Rate: 0.02657205000000001\n",
      "Epoch [1488/100000], Training Loss: -21.142303466796875, Learning Rate: 0.02657205000000001\n",
      "Epoch [1489/100000], Training Loss: -21.14251708984375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1490/100000], Training Loss: -21.142654418945312, Learning Rate: 0.02657205000000001\n",
      "Epoch [1491/100000], Training Loss: -21.142807006835938, Learning Rate: 0.02657205000000001\n",
      "Epoch [1492/100000], Training Loss: -21.142959594726562, Learning Rate: 0.02657205000000001\n",
      "Epoch [1493/100000], Training Loss: -21.143157958984375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1494/100000], Training Loss: -21.143295288085938, Learning Rate: 0.02657205000000001\n",
      "Epoch [1495/100000], Training Loss: -21.14349365234375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1496/100000], Training Loss: -21.14361572265625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1497/100000], Training Loss: -21.143814086914062, Learning Rate: 0.02657205000000001\n",
      "Epoch [1498/100000], Training Loss: -21.143951416015625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1499/100000], Training Loss: -21.144149780273438, Learning Rate: 0.02657205000000001\n",
      "Epoch [1500/100000], Training Loss: -21.144271850585938, Learning Rate: 0.02657205000000001\n",
      "Epoch [1501/100000], Training Loss: -21.144454956054688, Learning Rate: 0.02657205000000001\n",
      "Epoch [1502/100000], Training Loss: -21.1446533203125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1503/100000], Training Loss: -21.144775390625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1504/100000], Training Loss: -21.144927978515625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1505/100000], Training Loss: -21.145126342773438, Learning Rate: 0.02657205000000001\n",
      "Epoch [1506/100000], Training Loss: -21.145248413085938, Learning Rate: 0.02657205000000001\n",
      "Epoch [1507/100000], Training Loss: -21.145431518554688, Learning Rate: 0.02657205000000001\n",
      "Epoch [1508/100000], Training Loss: -21.145584106445312, Learning Rate: 0.02657205000000001\n",
      "Epoch [1509/100000], Training Loss: -21.145782470703125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1510/100000], Training Loss: -21.14593505859375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1511/100000], Training Loss: -21.1461181640625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1512/100000], Training Loss: -21.146209716796875, Learning Rate: 0.02657205000000001\n",
      "Epoch [1513/100000], Training Loss: -21.146392822265625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1514/100000], Training Loss: -21.14654541015625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1515/100000], Training Loss: -21.146759033203125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1516/100000], Training Loss: -21.146957397460938, Learning Rate: 0.02657205000000001\n",
      "Epoch [1517/100000], Training Loss: -21.147079467773438, Learning Rate: 0.02657205000000001\n",
      "Epoch [1518/100000], Training Loss: -21.147247314453125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1519/100000], Training Loss: -21.147430419921875, Learning Rate: 0.02657205000000001\n",
      "Epoch [1520/100000], Training Loss: -21.1475830078125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1521/100000], Training Loss: -21.147735595703125, Learning Rate: 0.02657205000000001\n",
      "Epoch [1522/100000], Training Loss: -21.14788818359375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1523/100000], Training Loss: -21.148086547851562, Learning Rate: 0.02657205000000001\n",
      "Epoch [1524/100000], Training Loss: -21.148284912109375, Learning Rate: 0.02657205000000001\n",
      "Epoch [1525/100000], Training Loss: -21.148406982421875, Learning Rate: 0.02657205000000001\n",
      "Epoch [1526/100000], Training Loss: -21.148635864257812, Learning Rate: 0.02657205000000001\n",
      "Epoch [1527/100000], Training Loss: -21.148757934570312, Learning Rate: 0.02657205000000001\n",
      "Epoch [1528/100000], Training Loss: -21.148941040039062, Learning Rate: 0.02657205000000001\n",
      "Epoch [1529/100000], Training Loss: -21.149093627929688, Learning Rate: 0.02657205000000001\n",
      "Epoch [1530/100000], Training Loss: -21.149246215820312, Learning Rate: 0.02657205000000001\n",
      "Epoch [1531/100000], Training Loss: -21.14947509765625, Learning Rate: 0.02657205000000001\n",
      "Epoch [1532/100000], Training Loss: -21.149627685546875, Learning Rate: 0.02657205000000001\n",
      "Epoch [1533/100000], Training Loss: -21.149765014648438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1534/100000], Training Loss: -21.149917602539062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1535/100000], Training Loss: -21.150070190429688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1536/100000], Training Loss: -21.15020751953125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1537/100000], Training Loss: -21.150405883789062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1538/100000], Training Loss: -21.150497436523438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1539/100000], Training Loss: -21.150665283203125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1540/100000], Training Loss: -21.150833129882812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1541/100000], Training Loss: -21.151016235351562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1542/100000], Training Loss: -21.151153564453125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1543/100000], Training Loss: -21.151290893554688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1544/100000], Training Loss: -21.151458740234375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1545/100000], Training Loss: -21.151580810546875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1546/100000], Training Loss: -21.151702880859375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1547/100000], Training Loss: -21.15185546875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1548/100000], Training Loss: -21.152053833007812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1549/100000], Training Loss: -21.152175903320312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1550/100000], Training Loss: -21.152374267578125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1551/100000], Training Loss: -21.15252685546875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1552/100000], Training Loss: -21.152694702148438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1553/100000], Training Loss: -21.152801513671875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1554/100000], Training Loss: -21.152984619140625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1555/100000], Training Loss: -21.15313720703125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1556/100000], Training Loss: -21.153274536132812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1557/100000], Training Loss: -21.153427124023438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1558/100000], Training Loss: -21.153594970703125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1559/100000], Training Loss: -21.15374755859375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1560/100000], Training Loss: -21.153915405273438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1561/100000], Training Loss: -21.154067993164062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1562/100000], Training Loss: -21.154190063476562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1563/100000], Training Loss: -21.154327392578125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1564/100000], Training Loss: -21.154525756835938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1565/100000], Training Loss: -21.154678344726562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1566/100000], Training Loss: -21.154815673828125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1567/100000], Training Loss: -21.154998779296875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1568/100000], Training Loss: -21.155166625976562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1569/100000], Training Loss: -21.155303955078125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1570/100000], Training Loss: -21.155426025390625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1571/100000], Training Loss: -21.155609130859375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1572/100000], Training Loss: -21.15576171875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1573/100000], Training Loss: -21.15594482421875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1574/100000], Training Loss: -21.156112670898438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1575/100000], Training Loss: -21.156234741210938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1576/100000], Training Loss: -21.156402587890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1577/100000], Training Loss: -21.156600952148438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1578/100000], Training Loss: -21.156723022460938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1579/100000], Training Loss: -21.156936645507812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1580/100000], Training Loss: -21.157012939453125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1581/100000], Training Loss: -21.15716552734375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1582/100000], Training Loss: -21.1573486328125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1583/100000], Training Loss: -21.157546997070312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1584/100000], Training Loss: -21.157638549804688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1585/100000], Training Loss: -21.1578369140625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1586/100000], Training Loss: -21.157989501953125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1587/100000], Training Loss: -21.158126831054688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1588/100000], Training Loss: -21.158309936523438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1589/100000], Training Loss: -21.15850830078125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1590/100000], Training Loss: -21.158615112304688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1591/100000], Training Loss: -21.15875244140625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1592/100000], Training Loss: -21.158935546875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1593/100000], Training Loss: -21.15911865234375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1594/100000], Training Loss: -21.1593017578125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1595/100000], Training Loss: -21.159439086914062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1596/100000], Training Loss: -21.159561157226562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1597/100000], Training Loss: -21.159774780273438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1598/100000], Training Loss: -21.159957885742188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1599/100000], Training Loss: -21.1600341796875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1600/100000], Training Loss: -21.160232543945312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1601/100000], Training Loss: -21.160324096679688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1602/100000], Training Loss: -21.160507202148438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1603/100000], Training Loss: -21.160720825195312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1604/100000], Training Loss: -21.160842895507812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1605/100000], Training Loss: -21.161026000976562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1606/100000], Training Loss: -21.16119384765625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1607/100000], Training Loss: -21.161331176757812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1608/100000], Training Loss: -21.1614990234375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1609/100000], Training Loss: -21.16168212890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1610/100000], Training Loss: -21.16180419921875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1611/100000], Training Loss: -21.161972045898438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1612/100000], Training Loss: -21.162109375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1613/100000], Training Loss: -21.162261962890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1614/100000], Training Loss: -21.162460327148438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1615/100000], Training Loss: -21.162582397460938, Learning Rate: 0.02391484500000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1616/100000], Training Loss: -21.162811279296875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1617/100000], Training Loss: -21.1629638671875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1618/100000], Training Loss: -21.1630859375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1619/100000], Training Loss: -21.163284301757812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1620/100000], Training Loss: -21.163436889648438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1621/100000], Training Loss: -21.163604736328125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1622/100000], Training Loss: -21.163742065429688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1623/100000], Training Loss: -21.163909912109375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1624/100000], Training Loss: -21.1640625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1625/100000], Training Loss: -21.164230346679688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1626/100000], Training Loss: -21.164398193359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1627/100000], Training Loss: -21.16455078125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1628/100000], Training Loss: -21.164688110351562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1629/100000], Training Loss: -21.164871215820312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1630/100000], Training Loss: -21.165069580078125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1631/100000], Training Loss: -21.165206909179688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1632/100000], Training Loss: -21.165374755859375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1633/100000], Training Loss: -21.16552734375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1634/100000], Training Loss: -21.16571044921875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1635/100000], Training Loss: -21.165847778320312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1636/100000], Training Loss: -21.166046142578125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1637/100000], Training Loss: -21.16619873046875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1638/100000], Training Loss: -21.166351318359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1639/100000], Training Loss: -21.166534423828125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1640/100000], Training Loss: -21.166702270507812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1641/100000], Training Loss: -21.1668701171875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1642/100000], Training Loss: -21.167007446289062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1643/100000], Training Loss: -21.167205810546875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1644/100000], Training Loss: -21.167373657226562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1645/100000], Training Loss: -21.167495727539062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1646/100000], Training Loss: -21.167709350585938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1647/100000], Training Loss: -21.167861938476562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1648/100000], Training Loss: -21.167999267578125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1649/100000], Training Loss: -21.16815185546875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1650/100000], Training Loss: -21.1683349609375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1651/100000], Training Loss: -21.168487548828125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1652/100000], Training Loss: -21.168624877929688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1653/100000], Training Loss: -21.168807983398438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1654/100000], Training Loss: -21.168991088867188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1655/100000], Training Loss: -21.169158935546875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1656/100000], Training Loss: -21.169357299804688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1657/100000], Training Loss: -21.16949462890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1658/100000], Training Loss: -21.169631958007812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1659/100000], Training Loss: -21.169830322265625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1660/100000], Training Loss: -21.169967651367188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1661/100000], Training Loss: -21.170166015625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1662/100000], Training Loss: -21.170318603515625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1663/100000], Training Loss: -21.170516967773438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1664/100000], Training Loss: -21.170623779296875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1665/100000], Training Loss: -21.170822143554688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1666/100000], Training Loss: -21.171005249023438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1667/100000], Training Loss: -21.171127319335938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1668/100000], Training Loss: -21.171295166015625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1669/100000], Training Loss: -21.171493530273438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1670/100000], Training Loss: -21.171630859375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1671/100000], Training Loss: -21.17181396484375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1672/100000], Training Loss: -21.1719970703125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1673/100000], Training Loss: -21.172149658203125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1674/100000], Training Loss: -21.172286987304688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1675/100000], Training Loss: -21.1724853515625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1676/100000], Training Loss: -21.17266845703125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1677/100000], Training Loss: -21.172805786132812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1678/100000], Training Loss: -21.1729736328125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1679/100000], Training Loss: -21.173126220703125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1680/100000], Training Loss: -21.173309326171875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1681/100000], Training Loss: -21.173507690429688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1682/100000], Training Loss: -21.17364501953125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1683/100000], Training Loss: -21.173828125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1684/100000], Training Loss: -21.173995971679688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1685/100000], Training Loss: -21.1741943359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1686/100000], Training Loss: -21.174301147460938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1687/100000], Training Loss: -21.17449951171875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1688/100000], Training Loss: -21.174667358398438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1689/100000], Training Loss: -21.174835205078125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1690/100000], Training Loss: -21.17498779296875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1691/100000], Training Loss: -21.1751708984375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1692/100000], Training Loss: -21.175323486328125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1693/100000], Training Loss: -21.17547607421875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1694/100000], Training Loss: -21.1756591796875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1695/100000], Training Loss: -21.175872802734375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1696/100000], Training Loss: -21.176010131835938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1697/100000], Training Loss: -21.1761474609375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1698/100000], Training Loss: -21.176376342773438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1699/100000], Training Loss: -21.176498413085938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1700/100000], Training Loss: -21.176666259765625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1701/100000], Training Loss: -21.176834106445312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1702/100000], Training Loss: -21.177017211914062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1703/100000], Training Loss: -21.177169799804688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1704/100000], Training Loss: -21.177291870117188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1705/100000], Training Loss: -21.17755126953125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1706/100000], Training Loss: -21.17767333984375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1707/100000], Training Loss: -21.1778564453125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1708/100000], Training Loss: -21.177993774414062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1709/100000], Training Loss: -21.178192138671875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1710/100000], Training Loss: -21.1783447265625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1711/100000], Training Loss: -21.178512573242188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1712/100000], Training Loss: -21.178695678710938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1713/100000], Training Loss: -21.17889404296875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1714/100000], Training Loss: -21.179031372070312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1715/100000], Training Loss: -21.179214477539062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1716/100000], Training Loss: -21.179412841796875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1717/100000], Training Loss: -21.179580688476562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1718/100000], Training Loss: -21.179763793945312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1719/100000], Training Loss: -21.179931640625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1720/100000], Training Loss: -21.18011474609375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1721/100000], Training Loss: -21.180252075195312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1722/100000], Training Loss: -21.180389404296875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1723/100000], Training Loss: -21.180587768554688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1724/100000], Training Loss: -21.180770874023438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1725/100000], Training Loss: -21.180923461914062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1726/100000], Training Loss: -21.181121826171875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1727/100000], Training Loss: -21.1812744140625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1728/100000], Training Loss: -21.181472778320312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1729/100000], Training Loss: -21.181655883789062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1730/100000], Training Loss: -21.181793212890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1731/100000], Training Loss: -21.18194580078125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1732/100000], Training Loss: -21.182098388671875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1733/100000], Training Loss: -21.182296752929688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1734/100000], Training Loss: -21.182464599609375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1735/100000], Training Loss: -21.182647705078125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1736/100000], Training Loss: -21.182815551757812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1737/100000], Training Loss: -21.182968139648438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1738/100000], Training Loss: -21.183135986328125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1739/100000], Training Loss: -21.183364868164062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1740/100000], Training Loss: -21.1834716796875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1741/100000], Training Loss: -21.183670043945312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1742/100000], Training Loss: -21.183883666992188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1743/100000], Training Loss: -21.183975219726562, Learning Rate: 0.02391484500000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1744/100000], Training Loss: -21.1842041015625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1745/100000], Training Loss: -21.184356689453125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1746/100000], Training Loss: -21.184555053710938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1747/100000], Training Loss: -21.184722900390625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1748/100000], Training Loss: -21.18487548828125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1749/100000], Training Loss: -21.185028076171875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1750/100000], Training Loss: -21.18524169921875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1751/100000], Training Loss: -21.185379028320312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1752/100000], Training Loss: -21.185562133789062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1753/100000], Training Loss: -21.185745239257812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1754/100000], Training Loss: -21.1859130859375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1755/100000], Training Loss: -21.186080932617188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1756/100000], Training Loss: -21.186264038085938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1757/100000], Training Loss: -21.1864013671875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1758/100000], Training Loss: -21.186599731445312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1759/100000], Training Loss: -21.186782836914062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1760/100000], Training Loss: -21.186935424804688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1761/100000], Training Loss: -21.187118530273438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1762/100000], Training Loss: -21.18731689453125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1763/100000], Training Loss: -21.187423706054688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1764/100000], Training Loss: -21.187652587890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1765/100000], Training Loss: -21.187835693359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1766/100000], Training Loss: -21.18798828125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1767/100000], Training Loss: -21.188156127929688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1768/100000], Training Loss: -21.188323974609375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1769/100000], Training Loss: -21.188491821289062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1770/100000], Training Loss: -21.188674926757812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1771/100000], Training Loss: -21.188858032226562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1772/100000], Training Loss: -21.18902587890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1773/100000], Training Loss: -21.189193725585938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1774/100000], Training Loss: -21.1893310546875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1775/100000], Training Loss: -21.189544677734375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1776/100000], Training Loss: -21.189697265625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1777/100000], Training Loss: -21.189834594726562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1778/100000], Training Loss: -21.190048217773438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1779/100000], Training Loss: -21.19024658203125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1780/100000], Training Loss: -21.190338134765625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1781/100000], Training Loss: -21.1905517578125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1782/100000], Training Loss: -21.190704345703125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1783/100000], Training Loss: -21.19091796875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1784/100000], Training Loss: -21.19110107421875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1785/100000], Training Loss: -21.1912841796875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1786/100000], Training Loss: -21.19146728515625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1787/100000], Training Loss: -21.191619873046875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1788/100000], Training Loss: -21.191802978515625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1789/100000], Training Loss: -21.191986083984375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1790/100000], Training Loss: -21.192092895507812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1791/100000], Training Loss: -21.1922607421875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1792/100000], Training Loss: -21.192459106445312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1793/100000], Training Loss: -21.192626953125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1794/100000], Training Loss: -21.19281005859375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1795/100000], Training Loss: -21.19293212890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1796/100000], Training Loss: -21.19317626953125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1797/100000], Training Loss: -21.193359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1798/100000], Training Loss: -21.19354248046875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1799/100000], Training Loss: -21.19366455078125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1800/100000], Training Loss: -21.193862915039062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1801/100000], Training Loss: -21.19403076171875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1802/100000], Training Loss: -21.194229125976562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1803/100000], Training Loss: -21.19439697265625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1804/100000], Training Loss: -21.194549560546875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1805/100000], Training Loss: -21.194732666015625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1806/100000], Training Loss: -21.194900512695312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1807/100000], Training Loss: -21.195098876953125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1808/100000], Training Loss: -21.195266723632812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1809/100000], Training Loss: -21.195449829101562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1810/100000], Training Loss: -21.195632934570312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1811/100000], Training Loss: -21.195816040039062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1812/100000], Training Loss: -21.19598388671875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1813/100000], Training Loss: -21.196136474609375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1814/100000], Training Loss: -21.196304321289062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1815/100000], Training Loss: -21.196487426757812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1816/100000], Training Loss: -21.196670532226562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1817/100000], Training Loss: -21.196823120117188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1818/100000], Training Loss: -21.197036743164062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1819/100000], Training Loss: -21.197189331054688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1820/100000], Training Loss: -21.197296142578125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1821/100000], Training Loss: -21.197509765625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1822/100000], Training Loss: -21.197708129882812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1823/100000], Training Loss: -21.197891235351562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1824/100000], Training Loss: -21.198074340820312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1825/100000], Training Loss: -21.198211669921875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1826/100000], Training Loss: -21.198410034179688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1827/100000], Training Loss: -21.1986083984375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1828/100000], Training Loss: -21.198745727539062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1829/100000], Training Loss: -21.19891357421875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1830/100000], Training Loss: -21.199081420898438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1831/100000], Training Loss: -21.199234008789062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1832/100000], Training Loss: -21.199432373046875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1833/100000], Training Loss: -21.199615478515625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1834/100000], Training Loss: -21.19976806640625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1835/100000], Training Loss: -21.199951171875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1836/100000], Training Loss: -21.200103759765625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1837/100000], Training Loss: -21.20025634765625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1838/100000], Training Loss: -21.200363159179688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1839/100000], Training Loss: -21.200546264648438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1840/100000], Training Loss: -21.200546264648438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1841/100000], Training Loss: -21.200332641601562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1842/100000], Training Loss: -21.19989013671875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1843/100000], Training Loss: -21.198684692382812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1844/100000], Training Loss: -21.195907592773438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1845/100000], Training Loss: -21.189697265625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1846/100000], Training Loss: -21.176055908203125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1847/100000], Training Loss: -21.145217895507812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1848/100000], Training Loss: -21.077285766601562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1849/100000], Training Loss: -20.920074462890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1850/100000], Training Loss: -20.576126098632812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1851/100000], Training Loss: -19.760971069335938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1852/100000], Training Loss: -18.12652587890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1853/100000], Training Loss: -14.453414916992188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1854/100000], Training Loss: -9.77679443359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1855/100000], Training Loss: -3.5892486572265625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1856/100000], Training Loss: -8.262313842773438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1857/100000], Training Loss: -17.376800537109375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1858/100000], Training Loss: -20.8602294921875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1859/100000], Training Loss: -15.217254638671875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1860/100000], Training Loss: -14.582046508789062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1861/100000], Training Loss: -20.714035034179688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1862/100000], Training Loss: -18.52252197265625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1863/100000], Training Loss: -16.188735961914062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1864/100000], Training Loss: -20.73077392578125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1865/100000], Training Loss: -19.166366577148438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1866/100000], Training Loss: -17.796157836914062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1867/100000], Training Loss: -21.075912475585938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1868/100000], Training Loss: -19.088592529296875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1869/100000], Training Loss: -19.1978759765625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1870/100000], Training Loss: -21.084548950195312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1871/100000], Training Loss: -19.10235595703125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1872/100000], Training Loss: -20.40313720703125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1873/100000], Training Loss: -20.682662963867188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1874/100000], Training Loss: -19.624038696289062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1875/100000], Training Loss: -21.079940795898438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1876/100000], Training Loss: -20.264984130859375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1877/100000], Training Loss: -20.47265625, Learning Rate: 0.02391484500000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1878/100000], Training Loss: -21.044235229492188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1879/100000], Training Loss: -20.279541015625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1880/100000], Training Loss: -21.05902099609375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1881/100000], Training Loss: -20.705078125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1882/100000], Training Loss: -20.703964233398438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1883/100000], Training Loss: -21.101760864257812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1884/100000], Training Loss: -20.632095336914062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1885/100000], Training Loss: -21.080810546875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1886/100000], Training Loss: -20.899307250976562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1887/100000], Training Loss: -20.881423950195312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1888/100000], Training Loss: -21.1251220703125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1889/100000], Training Loss: -20.849777221679688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1890/100000], Training Loss: -21.111129760742188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1891/100000], Training Loss: -21.001419067382812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1892/100000], Training Loss: -20.992919921875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1893/100000], Training Loss: -21.13287353515625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1894/100000], Training Loss: -20.969589233398438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1895/100000], Training Loss: -21.125808715820312, Learning Rate: 0.02391484500000001\n",
      "Epoch [1896/100000], Training Loss: -21.059478759765625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1897/100000], Training Loss: -21.056610107421875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1898/100000], Training Loss: -21.139007568359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1899/100000], Training Loss: -21.04345703125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1900/100000], Training Loss: -21.135879516601562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1901/100000], Training Loss: -21.096160888671875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1902/100000], Training Loss: -21.094436645507812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1903/100000], Training Loss: -21.142990112304688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1904/100000], Training Loss: -21.085952758789062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1905/100000], Training Loss: -21.140380859375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1906/100000], Training Loss: -21.117889404296875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1907/100000], Training Loss: -21.115478515625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1908/100000], Training Loss: -21.14569091796875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1909/100000], Training Loss: -21.11175537109375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1910/100000], Training Loss: -21.143264770507812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1911/100000], Training Loss: -21.1318359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1912/100000], Training Loss: -21.128402709960938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1913/100000], Training Loss: -21.147903442382812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1914/100000], Training Loss: -21.127456665039062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1915/100000], Training Loss: -21.145065307617188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1916/100000], Training Loss: -21.140365600585938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1917/100000], Training Loss: -21.13623046875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1918/100000], Training Loss: -21.14910888671875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1919/100000], Training Loss: -21.137100219726562, Learning Rate: 0.02391484500000001\n",
      "Epoch [1920/100000], Training Loss: -21.146408081054688, Learning Rate: 0.02391484500000001\n",
      "Epoch [1921/100000], Training Loss: -21.145553588867188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1922/100000], Training Loss: -21.141571044921875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1923/100000], Training Loss: -21.15008544921875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1924/100000], Training Loss: -21.143356323242188, Learning Rate: 0.02391484500000001\n",
      "Epoch [1925/100000], Training Loss: -21.147705078125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1926/100000], Training Loss: -21.148773193359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1927/100000], Training Loss: -21.145294189453125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1928/100000], Training Loss: -21.150726318359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1929/100000], Training Loss: -21.147323608398438, Learning Rate: 0.02391484500000001\n",
      "Epoch [1930/100000], Training Loss: -21.14886474609375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1931/100000], Training Loss: -21.15069580078125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1932/100000], Training Loss: -21.1480712890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1933/100000], Training Loss: -21.151321411132812, Learning Rate: 0.02391484500000001\n",
      "Epoch [1934/100000], Training Loss: -21.14996337890625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1935/100000], Training Loss: -21.15008544921875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1936/100000], Training Loss: -21.151870727539062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1937/100000], Training Loss: -21.150161743164062, Learning Rate: 0.02391484500000001\n",
      "Epoch [1938/100000], Training Loss: -21.151840209960938, Learning Rate: 0.02391484500000001\n",
      "Epoch [1939/100000], Training Loss: -21.151702880859375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1940/100000], Training Loss: -21.1512451171875, Learning Rate: 0.02391484500000001\n",
      "Epoch [1941/100000], Training Loss: -21.152679443359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1942/100000], Training Loss: -21.15179443359375, Learning Rate: 0.02391484500000001\n",
      "Epoch [1943/100000], Training Loss: -21.1524658203125, Learning Rate: 0.02391484500000001\n",
      "Epoch [1944/100000], Training Loss: -21.15289306640625, Learning Rate: 0.02391484500000001\n",
      "Epoch [1945/100000], Training Loss: -21.152587890625, Learning Rate: 0.021523360500000012\n",
      "Epoch [1946/100000], Training Loss: -21.153305053710938, Learning Rate: 0.021523360500000012\n",
      "Epoch [1947/100000], Training Loss: -21.15289306640625, Learning Rate: 0.021523360500000012\n",
      "Epoch [1948/100000], Training Loss: -21.153488159179688, Learning Rate: 0.021523360500000012\n",
      "Epoch [1949/100000], Training Loss: -21.153396606445312, Learning Rate: 0.021523360500000012\n",
      "Epoch [1950/100000], Training Loss: -21.153610229492188, Learning Rate: 0.021523360500000012\n",
      "Epoch [1951/100000], Training Loss: -21.153839111328125, Learning Rate: 0.021523360500000012\n",
      "Epoch [1952/100000], Training Loss: -21.153778076171875, Learning Rate: 0.021523360500000012\n",
      "Epoch [1953/100000], Training Loss: -21.154190063476562, Learning Rate: 0.021523360500000012\n",
      "Epoch [1954/100000], Training Loss: -21.154037475585938, Learning Rate: 0.021523360500000012\n",
      "Epoch [1955/100000], Training Loss: -21.154373168945312, Learning Rate: 0.021523360500000012\n",
      "Epoch [1956/100000], Training Loss: -21.154434204101562, Learning Rate: 0.021523360500000012\n",
      "Epoch [1957/100000], Training Loss: -21.154541015625, Learning Rate: 0.021523360500000012\n",
      "Epoch [1958/100000], Training Loss: -21.15478515625, Learning Rate: 0.021523360500000012\n",
      "Epoch [1959/100000], Training Loss: -21.154800415039062, Learning Rate: 0.021523360500000012\n",
      "Epoch [1960/100000], Training Loss: -21.15509033203125, Learning Rate: 0.021523360500000012\n",
      "Epoch [1961/100000], Training Loss: -21.155044555664062, Learning Rate: 0.021523360500000012\n",
      "Epoch [1962/100000], Training Loss: -21.155258178710938, Learning Rate: 0.021523360500000012\n",
      "Epoch [1963/100000], Training Loss: -21.15533447265625, Learning Rate: 0.021523360500000012\n",
      "Epoch [1964/100000], Training Loss: -21.155471801757812, Learning Rate: 0.021523360500000012\n",
      "Epoch [1965/100000], Training Loss: -21.155654907226562, Learning Rate: 0.021523360500000012\n",
      "Epoch [1966/100000], Training Loss: -21.15570068359375, Learning Rate: 0.021523360500000012\n",
      "Epoch [1967/100000], Training Loss: -21.1558837890625, Learning Rate: 0.021523360500000012\n",
      "Epoch [1968/100000], Training Loss: -21.155975341796875, Learning Rate: 0.021523360500000012\n",
      "Epoch [1969/100000], Training Loss: -21.156082153320312, Learning Rate: 0.021523360500000012\n",
      "Epoch [1970/100000], Training Loss: -21.156234741210938, Learning Rate: 0.021523360500000012\n",
      "Epoch [1971/100000], Training Loss: -21.15631103515625, Learning Rate: 0.021523360500000012\n",
      "Epoch [1972/100000], Training Loss: -21.156478881835938, Learning Rate: 0.021523360500000012\n",
      "Epoch [1973/100000], Training Loss: -21.15655517578125, Learning Rate: 0.021523360500000012\n",
      "Epoch [1974/100000], Training Loss: -21.15673828125, Learning Rate: 0.021523360500000012\n",
      "Epoch [1975/100000], Training Loss: -21.156845092773438, Learning Rate: 0.021523360500000012\n",
      "Epoch [1976/100000], Training Loss: -21.156890869140625, Learning Rate: 0.021523360500000012\n",
      "Epoch [1977/100000], Training Loss: -21.157073974609375, Learning Rate: 0.021523360500000012\n",
      "Epoch [1978/100000], Training Loss: -21.157196044921875, Learning Rate: 0.021523360500000012\n",
      "Epoch [1979/100000], Training Loss: -21.157333374023438, Learning Rate: 0.021523360500000012\n",
      "Epoch [1980/100000], Training Loss: -21.157379150390625, Learning Rate: 0.021523360500000012\n",
      "Epoch [1981/100000], Training Loss: -21.157501220703125, Learning Rate: 0.021523360500000012\n",
      "Epoch [1982/100000], Training Loss: -21.157669067382812, Learning Rate: 0.021523360500000012\n",
      "Epoch [1983/100000], Training Loss: -21.157806396484375, Learning Rate: 0.021523360500000012\n",
      "Epoch [1984/100000], Training Loss: -21.157882690429688, Learning Rate: 0.021523360500000012\n",
      "Epoch [1985/100000], Training Loss: -21.158004760742188, Learning Rate: 0.021523360500000012\n",
      "Epoch [1986/100000], Training Loss: -21.158126831054688, Learning Rate: 0.021523360500000012\n",
      "Epoch [1987/100000], Training Loss: -21.158279418945312, Learning Rate: 0.021523360500000012\n",
      "Epoch [1988/100000], Training Loss: -21.158355712890625, Learning Rate: 0.021523360500000012\n",
      "Epoch [1989/100000], Training Loss: -21.158477783203125, Learning Rate: 0.021523360500000012\n",
      "Epoch [1990/100000], Training Loss: -21.158584594726562, Learning Rate: 0.021523360500000012\n",
      "Epoch [1991/100000], Training Loss: -21.15869140625, Learning Rate: 0.021523360500000012\n",
      "Epoch [1992/100000], Training Loss: -21.158828735351562, Learning Rate: 0.021523360500000012\n",
      "Epoch [1993/100000], Training Loss: -21.158920288085938, Learning Rate: 0.021523360500000012\n",
      "Epoch [1994/100000], Training Loss: -21.159027099609375, Learning Rate: 0.021523360500000012\n",
      "Epoch [1995/100000], Training Loss: -21.159149169921875, Learning Rate: 0.021523360500000012\n",
      "Epoch [1996/100000], Training Loss: -21.1593017578125, Learning Rate: 0.021523360500000012\n",
      "Epoch [1997/100000], Training Loss: -21.159347534179688, Learning Rate: 0.021523360500000012\n",
      "Epoch [1998/100000], Training Loss: -21.159530639648438, Learning Rate: 0.021523360500000012\n",
      "Epoch [1999/100000], Training Loss: -21.159637451171875, Learning Rate: 0.021523360500000012\n",
      "Epoch [2000/100000], Training Loss: -21.15972900390625, Learning Rate: 0.021523360500000012\n",
      "Epoch [2001/100000], Training Loss: -21.159866333007812, Learning Rate: 0.021523360500000012\n",
      "Epoch [2002/100000], Training Loss: -21.159957885742188, Learning Rate: 0.021523360500000012\n",
      "Epoch [2003/100000], Training Loss: -21.16009521484375, Learning Rate: 0.021523360500000012\n",
      "Epoch [2004/100000], Training Loss: -21.160232543945312, Learning Rate: 0.021523360500000012\n",
      "Epoch [2005/100000], Training Loss: -21.160324096679688, Learning Rate: 0.021523360500000012\n",
      "Epoch [2006/100000], Training Loss: -21.160415649414062, Learning Rate: 0.021523360500000012\n",
      "Epoch [2007/100000], Training Loss: -21.160537719726562, Learning Rate: 0.021523360500000012\n",
      "Epoch [2008/100000], Training Loss: -21.160614013671875, Learning Rate: 0.021523360500000012\n",
      "Epoch [2009/100000], Training Loss: -21.160751342773438, Learning Rate: 0.021523360500000012\n",
      "Epoch [2010/100000], Training Loss: -21.160858154296875, Learning Rate: 0.021523360500000012\n",
      "Epoch [2011/100000], Training Loss: -21.1610107421875, Learning Rate: 0.021523360500000012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2012/100000], Training Loss: -21.1611328125, Learning Rate: 0.021523360500000012\n",
      "Epoch [2013/100000], Training Loss: -21.161209106445312, Learning Rate: 0.021523360500000012\n",
      "Epoch [2014/100000], Training Loss: -21.161331176757812, Learning Rate: 0.021523360500000012\n",
      "Epoch [2015/100000], Training Loss: -21.16143798828125, Learning Rate: 0.021523360500000012\n",
      "Epoch [2016/100000], Training Loss: -21.161575317382812, Learning Rate: 0.021523360500000012\n",
      "Epoch [2017/100000], Training Loss: -21.161697387695312, Learning Rate: 0.021523360500000012\n",
      "Epoch [2018/100000], Training Loss: -21.16180419921875, Learning Rate: 0.021523360500000012\n",
      "Epoch [2019/100000], Training Loss: -21.161865234375, Learning Rate: 0.021523360500000012\n",
      "Epoch [2020/100000], Training Loss: -21.161972045898438, Learning Rate: 0.021523360500000012\n",
      "Epoch [2021/100000], Training Loss: -21.162109375, Learning Rate: 0.021523360500000012\n",
      "Epoch [2022/100000], Training Loss: -21.162277221679688, Learning Rate: 0.021523360500000012\n",
      "Epoch [2023/100000], Training Loss: -21.162353515625, Learning Rate: 0.021523360500000012\n",
      "Epoch [2024/100000], Training Loss: -21.16241455078125, Learning Rate: 0.021523360500000012\n",
      "Epoch [2025/100000], Training Loss: -21.162551879882812, Learning Rate: 0.021523360500000012\n",
      "Epoch [2026/100000], Training Loss: -21.162689208984375, Learning Rate: 0.021523360500000012\n",
      "Epoch [2027/100000], Training Loss: -21.162811279296875, Learning Rate: 0.021523360500000012\n",
      "Epoch [2028/100000], Training Loss: -21.162872314453125, Learning Rate: 0.021523360500000012\n",
      "Epoch [2029/100000], Training Loss: -21.16302490234375, Learning Rate: 0.021523360500000012\n",
      "Epoch [2030/100000], Training Loss: -21.1632080078125, Learning Rate: 0.021523360500000012\n",
      "Epoch [2031/100000], Training Loss: -21.163238525390625, Learning Rate: 0.021523360500000012\n",
      "Epoch [2032/100000], Training Loss: -21.163375854492188, Learning Rate: 0.021523360500000012\n",
      "Epoch [2033/100000], Training Loss: -21.163436889648438, Learning Rate: 0.021523360500000012\n",
      "Epoch [2034/100000], Training Loss: -21.163604736328125, Learning Rate: 0.021523360500000012\n",
      "Epoch [2035/100000], Training Loss: -21.163742065429688, Learning Rate: 0.021523360500000012\n",
      "Epoch [2036/100000], Training Loss: -21.163803100585938, Learning Rate: 0.021523360500000012\n",
      "Epoch [2037/100000], Training Loss: -21.16387939453125, Learning Rate: 0.021523360500000012\n",
      "Epoch [2038/100000], Training Loss: -21.164016723632812, Learning Rate: 0.021523360500000012\n",
      "Epoch [2039/100000], Training Loss: -21.164108276367188, Learning Rate: 0.021523360500000012\n",
      "Epoch [2040/100000], Training Loss: -21.164199829101562, Learning Rate: 0.021523360500000012\n",
      "Epoch [2041/100000], Training Loss: -21.16436767578125, Learning Rate: 0.021523360500000012\n",
      "Epoch [2042/100000], Training Loss: -21.164459228515625, Learning Rate: 0.021523360500000012\n",
      "Epoch [2043/100000], Training Loss: -21.164581298828125, Learning Rate: 0.021523360500000012\n",
      "Epoch [2044/100000], Training Loss: -21.164703369140625, Learning Rate: 0.021523360500000012\n",
      "Epoch [2045/100000], Training Loss: -21.164794921875, Learning Rate: 0.021523360500000012\n",
      "Epoch [2046/100000], Training Loss: -21.164871215820312, Learning Rate: 0.01937102445000001\n",
      "Epoch [2047/100000], Training Loss: -21.16497802734375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2048/100000], Training Loss: -21.16510009765625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2049/100000], Training Loss: -21.165206909179688, Learning Rate: 0.01937102445000001\n",
      "Epoch [2050/100000], Training Loss: -21.165313720703125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2051/100000], Training Loss: -21.165420532226562, Learning Rate: 0.01937102445000001\n",
      "Epoch [2052/100000], Training Loss: -21.165496826171875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2053/100000], Training Loss: -21.165618896484375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2054/100000], Training Loss: -21.165679931640625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2055/100000], Training Loss: -21.165802001953125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2056/100000], Training Loss: -21.165924072265625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2057/100000], Training Loss: -21.165985107421875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2058/100000], Training Loss: -21.166091918945312, Learning Rate: 0.01937102445000001\n",
      "Epoch [2059/100000], Training Loss: -21.166244506835938, Learning Rate: 0.01937102445000001\n",
      "Epoch [2060/100000], Training Loss: -21.166305541992188, Learning Rate: 0.01937102445000001\n",
      "Epoch [2061/100000], Training Loss: -21.166412353515625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2062/100000], Training Loss: -21.16650390625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2063/100000], Training Loss: -21.166595458984375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2064/100000], Training Loss: -21.166732788085938, Learning Rate: 0.01937102445000001\n",
      "Epoch [2065/100000], Training Loss: -21.166778564453125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2066/100000], Training Loss: -21.1668701171875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2067/100000], Training Loss: -21.167022705078125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2068/100000], Training Loss: -21.167068481445312, Learning Rate: 0.01937102445000001\n",
      "Epoch [2069/100000], Training Loss: -21.167236328125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2070/100000], Training Loss: -21.16729736328125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2071/100000], Training Loss: -21.167388916015625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2072/100000], Training Loss: -21.167526245117188, Learning Rate: 0.01937102445000001\n",
      "Epoch [2073/100000], Training Loss: -21.167648315429688, Learning Rate: 0.01937102445000001\n",
      "Epoch [2074/100000], Training Loss: -21.167724609375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2075/100000], Training Loss: -21.167816162109375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2076/100000], Training Loss: -21.16790771484375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2077/100000], Training Loss: -21.16802978515625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2078/100000], Training Loss: -21.168136596679688, Learning Rate: 0.01937102445000001\n",
      "Epoch [2079/100000], Training Loss: -21.168243408203125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2080/100000], Training Loss: -21.1683349609375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2081/100000], Training Loss: -21.168426513671875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2082/100000], Training Loss: -21.16851806640625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2083/100000], Training Loss: -21.168594360351562, Learning Rate: 0.01937102445000001\n",
      "Epoch [2084/100000], Training Loss: -21.168716430664062, Learning Rate: 0.01937102445000001\n",
      "Epoch [2085/100000], Training Loss: -21.1688232421875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2086/100000], Training Loss: -21.168914794921875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2087/100000], Training Loss: -21.169036865234375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2088/100000], Training Loss: -21.16912841796875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2089/100000], Training Loss: -21.169235229492188, Learning Rate: 0.01937102445000001\n",
      "Epoch [2090/100000], Training Loss: -21.169296264648438, Learning Rate: 0.01937102445000001\n",
      "Epoch [2091/100000], Training Loss: -21.169479370117188, Learning Rate: 0.01937102445000001\n",
      "Epoch [2092/100000], Training Loss: -21.169540405273438, Learning Rate: 0.01937102445000001\n",
      "Epoch [2093/100000], Training Loss: -21.169631958007812, Learning Rate: 0.01937102445000001\n",
      "Epoch [2094/100000], Training Loss: -21.169769287109375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2095/100000], Training Loss: -21.169815063476562, Learning Rate: 0.01937102445000001\n",
      "Epoch [2096/100000], Training Loss: -21.169906616210938, Learning Rate: 0.01937102445000001\n",
      "Epoch [2097/100000], Training Loss: -21.170028686523438, Learning Rate: 0.01937102445000001\n",
      "Epoch [2098/100000], Training Loss: -21.170120239257812, Learning Rate: 0.01937102445000001\n",
      "Epoch [2099/100000], Training Loss: -21.170211791992188, Learning Rate: 0.01937102445000001\n",
      "Epoch [2100/100000], Training Loss: -21.170333862304688, Learning Rate: 0.01937102445000001\n",
      "Epoch [2101/100000], Training Loss: -21.170440673828125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2102/100000], Training Loss: -21.170562744140625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2103/100000], Training Loss: -21.170684814453125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2104/100000], Training Loss: -21.170745849609375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2105/100000], Training Loss: -21.170867919921875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2106/100000], Training Loss: -21.170944213867188, Learning Rate: 0.01937102445000001\n",
      "Epoch [2107/100000], Training Loss: -21.171096801757812, Learning Rate: 0.01937102445000001\n",
      "Epoch [2108/100000], Training Loss: -21.171157836914062, Learning Rate: 0.01937102445000001\n",
      "Epoch [2109/100000], Training Loss: -21.171295166015625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2110/100000], Training Loss: -21.17138671875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2111/100000], Training Loss: -21.171463012695312, Learning Rate: 0.01937102445000001\n",
      "Epoch [2112/100000], Training Loss: -21.171585083007812, Learning Rate: 0.01937102445000001\n",
      "Epoch [2113/100000], Training Loss: -21.17169189453125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2114/100000], Training Loss: -21.171783447265625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2115/100000], Training Loss: -21.171859741210938, Learning Rate: 0.01937102445000001\n",
      "Epoch [2116/100000], Training Loss: -21.171981811523438, Learning Rate: 0.01937102445000001\n",
      "Epoch [2117/100000], Training Loss: -21.172119140625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2118/100000], Training Loss: -21.172195434570312, Learning Rate: 0.01937102445000001\n",
      "Epoch [2119/100000], Training Loss: -21.17230224609375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2120/100000], Training Loss: -21.172393798828125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2121/100000], Training Loss: -21.172515869140625, Learning Rate: 0.01937102445000001\n",
      "Epoch [2122/100000], Training Loss: -21.172622680664062, Learning Rate: 0.01937102445000001\n",
      "Epoch [2123/100000], Training Loss: -21.172714233398438, Learning Rate: 0.01937102445000001\n",
      "Epoch [2124/100000], Training Loss: -21.172821044921875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2125/100000], Training Loss: -21.172943115234375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2126/100000], Training Loss: -21.173019409179688, Learning Rate: 0.01937102445000001\n",
      "Epoch [2127/100000], Training Loss: -21.173126220703125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2128/100000], Training Loss: -21.1732177734375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2129/100000], Training Loss: -21.173324584960938, Learning Rate: 0.01937102445000001\n",
      "Epoch [2130/100000], Training Loss: -21.173431396484375, Learning Rate: 0.01937102445000001\n",
      "Epoch [2131/100000], Training Loss: -21.173538208007812, Learning Rate: 0.01937102445000001\n",
      "Epoch [2132/100000], Training Loss: -21.173660278320312, Learning Rate: 0.01937102445000001\n",
      "Epoch [2133/100000], Training Loss: -21.173721313476562, Learning Rate: 0.01937102445000001\n",
      "Epoch [2134/100000], Training Loss: -21.173843383789062, Learning Rate: 0.01937102445000001\n",
      "Epoch [2135/100000], Training Loss: -21.173965454101562, Learning Rate: 0.01937102445000001\n",
      "Epoch [2136/100000], Training Loss: -21.174057006835938, Learning Rate: 0.01937102445000001\n",
      "Epoch [2137/100000], Training Loss: -21.174179077148438, Learning Rate: 0.01937102445000001\n",
      "Epoch [2138/100000], Training Loss: -21.174285888671875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2139/100000], Training Loss: -21.174362182617188, Learning Rate: 0.01937102445000001\n",
      "Epoch [2140/100000], Training Loss: -21.174453735351562, Learning Rate: 0.01937102445000001\n",
      "Epoch [2141/100000], Training Loss: -21.174514770507812, Learning Rate: 0.01937102445000001\n",
      "Epoch [2142/100000], Training Loss: -21.1746826171875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2143/100000], Training Loss: -21.174774169921875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2144/100000], Training Loss: -21.1749267578125, Learning Rate: 0.01937102445000001\n",
      "Epoch [2145/100000], Training Loss: -21.175018310546875, Learning Rate: 0.01937102445000001\n",
      "Epoch [2146/100000], Training Loss: -21.175094604492188, Learning Rate: 0.01937102445000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2147/100000], Training Loss: -21.175247192382812, Learning Rate: 0.01743392200500001\n",
      "Epoch [2148/100000], Training Loss: -21.17529296875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2149/100000], Training Loss: -21.17535400390625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2150/100000], Training Loss: -21.175460815429688, Learning Rate: 0.01743392200500001\n",
      "Epoch [2151/100000], Training Loss: -21.17559814453125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2152/100000], Training Loss: -21.175689697265625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2153/100000], Training Loss: -21.17578125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2154/100000], Training Loss: -21.175857543945312, Learning Rate: 0.01743392200500001\n",
      "Epoch [2155/100000], Training Loss: -21.175979614257812, Learning Rate: 0.01743392200500001\n",
      "Epoch [2156/100000], Training Loss: -21.176055908203125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2157/100000], Training Loss: -21.176177978515625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2158/100000], Training Loss: -21.17626953125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2159/100000], Training Loss: -21.176345825195312, Learning Rate: 0.01743392200500001\n",
      "Epoch [2160/100000], Training Loss: -21.176437377929688, Learning Rate: 0.01743392200500001\n",
      "Epoch [2161/100000], Training Loss: -21.176559448242188, Learning Rate: 0.01743392200500001\n",
      "Epoch [2162/100000], Training Loss: -21.1766357421875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2163/100000], Training Loss: -21.176712036132812, Learning Rate: 0.01743392200500001\n",
      "Epoch [2164/100000], Training Loss: -21.176834106445312, Learning Rate: 0.01743392200500001\n",
      "Epoch [2165/100000], Training Loss: -21.17694091796875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2166/100000], Training Loss: -21.177017211914062, Learning Rate: 0.01743392200500001\n",
      "Epoch [2167/100000], Training Loss: -21.1771240234375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2168/100000], Training Loss: -21.177215576171875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2169/100000], Training Loss: -21.17730712890625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2170/100000], Training Loss: -21.177413940429688, Learning Rate: 0.01743392200500001\n",
      "Epoch [2171/100000], Training Loss: -21.177474975585938, Learning Rate: 0.01743392200500001\n",
      "Epoch [2172/100000], Training Loss: -21.177581787109375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2173/100000], Training Loss: -21.177688598632812, Learning Rate: 0.01743392200500001\n",
      "Epoch [2174/100000], Training Loss: -21.177780151367188, Learning Rate: 0.01743392200500001\n",
      "Epoch [2175/100000], Training Loss: -21.177871704101562, Learning Rate: 0.01743392200500001\n",
      "Epoch [2176/100000], Training Loss: -21.177993774414062, Learning Rate: 0.01743392200500001\n",
      "Epoch [2177/100000], Training Loss: -21.178070068359375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2178/100000], Training Loss: -21.178192138671875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2179/100000], Training Loss: -21.178268432617188, Learning Rate: 0.01743392200500001\n",
      "Epoch [2180/100000], Training Loss: -21.1783447265625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2181/100000], Training Loss: -21.178466796875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2182/100000], Training Loss: -21.17852783203125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2183/100000], Training Loss: -21.178634643554688, Learning Rate: 0.01743392200500001\n",
      "Epoch [2184/100000], Training Loss: -21.178726196289062, Learning Rate: 0.01743392200500001\n",
      "Epoch [2185/100000], Training Loss: -21.1788330078125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2186/100000], Training Loss: -21.178985595703125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2187/100000], Training Loss: -21.179046630859375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2188/100000], Training Loss: -21.179107666015625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2189/100000], Training Loss: -21.179229736328125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2190/100000], Training Loss: -21.179290771484375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2191/100000], Training Loss: -21.179428100585938, Learning Rate: 0.01743392200500001\n",
      "Epoch [2192/100000], Training Loss: -21.179519653320312, Learning Rate: 0.01743392200500001\n",
      "Epoch [2193/100000], Training Loss: -21.179595947265625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2194/100000], Training Loss: -21.1796875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2195/100000], Training Loss: -21.1798095703125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2196/100000], Training Loss: -21.179931640625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2197/100000], Training Loss: -21.180038452148438, Learning Rate: 0.01743392200500001\n",
      "Epoch [2198/100000], Training Loss: -21.18011474609375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2199/100000], Training Loss: -21.180206298828125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2200/100000], Training Loss: -21.1802978515625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2201/100000], Training Loss: -21.180389404296875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2202/100000], Training Loss: -21.180511474609375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2203/100000], Training Loss: -21.18060302734375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2204/100000], Training Loss: -21.180709838867188, Learning Rate: 0.01743392200500001\n",
      "Epoch [2205/100000], Training Loss: -21.180801391601562, Learning Rate: 0.01743392200500001\n",
      "Epoch [2206/100000], Training Loss: -21.180908203125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2207/100000], Training Loss: -21.18096923828125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2208/100000], Training Loss: -21.181076049804688, Learning Rate: 0.01743392200500001\n",
      "Epoch [2209/100000], Training Loss: -21.181182861328125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2210/100000], Training Loss: -21.1812744140625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2211/100000], Training Loss: -21.181411743164062, Learning Rate: 0.01743392200500001\n",
      "Epoch [2212/100000], Training Loss: -21.1815185546875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2213/100000], Training Loss: -21.18157958984375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2214/100000], Training Loss: -21.181671142578125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2215/100000], Training Loss: -21.1817626953125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2216/100000], Training Loss: -21.181884765625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2217/100000], Training Loss: -21.18194580078125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2218/100000], Training Loss: -21.182083129882812, Learning Rate: 0.01743392200500001\n",
      "Epoch [2219/100000], Training Loss: -21.182174682617188, Learning Rate: 0.01743392200500001\n",
      "Epoch [2220/100000], Training Loss: -21.182281494140625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2221/100000], Training Loss: -21.182327270507812, Learning Rate: 0.01743392200500001\n",
      "Epoch [2222/100000], Training Loss: -21.18243408203125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2223/100000], Training Loss: -21.182571411132812, Learning Rate: 0.01743392200500001\n",
      "Epoch [2224/100000], Training Loss: -21.18267822265625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2225/100000], Training Loss: -21.18280029296875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2226/100000], Training Loss: -21.182891845703125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2227/100000], Training Loss: -21.182952880859375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2228/100000], Training Loss: -21.18304443359375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2229/100000], Training Loss: -21.18316650390625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2230/100000], Training Loss: -21.183258056640625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2231/100000], Training Loss: -21.183395385742188, Learning Rate: 0.01743392200500001\n",
      "Epoch [2232/100000], Training Loss: -21.183502197265625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2233/100000], Training Loss: -21.183578491210938, Learning Rate: 0.01743392200500001\n",
      "Epoch [2234/100000], Training Loss: -21.183685302734375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2235/100000], Training Loss: -21.183807373046875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2236/100000], Training Loss: -21.18389892578125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2237/100000], Training Loss: -21.183990478515625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2238/100000], Training Loss: -21.184112548828125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2239/100000], Training Loss: -21.184173583984375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2240/100000], Training Loss: -21.184295654296875, Learning Rate: 0.01743392200500001\n",
      "Epoch [2241/100000], Training Loss: -21.184417724609375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2242/100000], Training Loss: -21.184524536132812, Learning Rate: 0.01743392200500001\n",
      "Epoch [2243/100000], Training Loss: -21.1845703125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2244/100000], Training Loss: -21.184707641601562, Learning Rate: 0.01743392200500001\n",
      "Epoch [2245/100000], Training Loss: -21.184814453125, Learning Rate: 0.01743392200500001\n",
      "Epoch [2246/100000], Training Loss: -21.184906005859375, Learning Rate: 0.01743392200500001\n",
      "Epoch [2247/100000], Training Loss: -21.184967041015625, Learning Rate: 0.01743392200500001\n",
      "Epoch [2248/100000], Training Loss: -21.185073852539062, Learning Rate: 0.015690529804500006\n",
      "Epoch [2249/100000], Training Loss: -21.1851806640625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2250/100000], Training Loss: -21.185302734375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2251/100000], Training Loss: -21.185394287109375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2252/100000], Training Loss: -21.18548583984375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2253/100000], Training Loss: -21.18560791015625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2254/100000], Training Loss: -21.185638427734375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2255/100000], Training Loss: -21.18572998046875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2256/100000], Training Loss: -21.18585205078125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2257/100000], Training Loss: -21.185928344726562, Learning Rate: 0.015690529804500006\n",
      "Epoch [2258/100000], Training Loss: -21.186004638671875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2259/100000], Training Loss: -21.18609619140625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2260/100000], Training Loss: -21.186187744140625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2261/100000], Training Loss: -21.186325073242188, Learning Rate: 0.015690529804500006\n",
      "Epoch [2262/100000], Training Loss: -21.1864013671875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2263/100000], Training Loss: -21.186492919921875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2264/100000], Training Loss: -21.186569213867188, Learning Rate: 0.015690529804500006\n",
      "Epoch [2265/100000], Training Loss: -21.186691284179688, Learning Rate: 0.015690529804500006\n",
      "Epoch [2266/100000], Training Loss: -21.186813354492188, Learning Rate: 0.015690529804500006\n",
      "Epoch [2267/100000], Training Loss: -21.186859130859375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2268/100000], Training Loss: -21.18695068359375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2269/100000], Training Loss: -21.187057495117188, Learning Rate: 0.015690529804500006\n",
      "Epoch [2270/100000], Training Loss: -21.1871337890625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2271/100000], Training Loss: -21.187210083007812, Learning Rate: 0.015690529804500006\n",
      "Epoch [2272/100000], Training Loss: -21.18731689453125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2273/100000], Training Loss: -21.187408447265625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2274/100000], Training Loss: -21.187515258789062, Learning Rate: 0.015690529804500006\n",
      "Epoch [2275/100000], Training Loss: -21.187606811523438, Learning Rate: 0.015690529804500006\n",
      "Epoch [2276/100000], Training Loss: -21.187667846679688, Learning Rate: 0.015690529804500006\n",
      "Epoch [2277/100000], Training Loss: -21.187789916992188, Learning Rate: 0.015690529804500006\n",
      "Epoch [2278/100000], Training Loss: -21.187881469726562, Learning Rate: 0.015690529804500006\n",
      "Epoch [2279/100000], Training Loss: -21.187973022460938, Learning Rate: 0.015690529804500006\n",
      "Epoch [2280/100000], Training Loss: -21.188095092773438, Learning Rate: 0.015690529804500006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2281/100000], Training Loss: -21.188186645507812, Learning Rate: 0.015690529804500006\n",
      "Epoch [2282/100000], Training Loss: -21.188262939453125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2283/100000], Training Loss: -21.18829345703125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2284/100000], Training Loss: -21.188446044921875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2285/100000], Training Loss: -21.18853759765625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2286/100000], Training Loss: -21.188644409179688, Learning Rate: 0.015690529804500006\n",
      "Epoch [2287/100000], Training Loss: -21.188690185546875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2288/100000], Training Loss: -21.188812255859375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2289/100000], Training Loss: -21.188888549804688, Learning Rate: 0.015690529804500006\n",
      "Epoch [2290/100000], Training Loss: -21.18896484375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2291/100000], Training Loss: -21.189117431640625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2292/100000], Training Loss: -21.189178466796875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2293/100000], Training Loss: -21.189315795898438, Learning Rate: 0.015690529804500006\n",
      "Epoch [2294/100000], Training Loss: -21.189346313476562, Learning Rate: 0.015690529804500006\n",
      "Epoch [2295/100000], Training Loss: -21.189498901367188, Learning Rate: 0.015690529804500006\n",
      "Epoch [2296/100000], Training Loss: -21.1895751953125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2297/100000], Training Loss: -21.189666748046875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2298/100000], Training Loss: -21.189804077148438, Learning Rate: 0.015690529804500006\n",
      "Epoch [2299/100000], Training Loss: -21.189849853515625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2300/100000], Training Loss: -21.189971923828125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2301/100000], Training Loss: -21.190017700195312, Learning Rate: 0.015690529804500006\n",
      "Epoch [2302/100000], Training Loss: -21.190170288085938, Learning Rate: 0.015690529804500006\n",
      "Epoch [2303/100000], Training Loss: -21.190216064453125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2304/100000], Training Loss: -21.190353393554688, Learning Rate: 0.015690529804500006\n",
      "Epoch [2305/100000], Training Loss: -21.190460205078125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2306/100000], Training Loss: -21.19049072265625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2307/100000], Training Loss: -21.1905517578125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2308/100000], Training Loss: -21.190719604492188, Learning Rate: 0.015690529804500006\n",
      "Epoch [2309/100000], Training Loss: -21.1907958984375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2310/100000], Training Loss: -21.190887451171875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2311/100000], Training Loss: -21.190994262695312, Learning Rate: 0.015690529804500006\n",
      "Epoch [2312/100000], Training Loss: -21.191085815429688, Learning Rate: 0.015690529804500006\n",
      "Epoch [2313/100000], Training Loss: -21.191192626953125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2314/100000], Training Loss: -21.191238403320312, Learning Rate: 0.015690529804500006\n",
      "Epoch [2315/100000], Training Loss: -21.191390991210938, Learning Rate: 0.015690529804500006\n",
      "Epoch [2316/100000], Training Loss: -21.19146728515625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2317/100000], Training Loss: -21.19158935546875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2318/100000], Training Loss: -21.191680908203125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2319/100000], Training Loss: -21.1917724609375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2320/100000], Training Loss: -21.191864013671875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2321/100000], Training Loss: -21.191986083984375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2322/100000], Training Loss: -21.192047119140625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2323/100000], Training Loss: -21.192169189453125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2324/100000], Training Loss: -21.192276000976562, Learning Rate: 0.015690529804500006\n",
      "Epoch [2325/100000], Training Loss: -21.192367553710938, Learning Rate: 0.015690529804500006\n",
      "Epoch [2326/100000], Training Loss: -21.192474365234375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2327/100000], Training Loss: -21.192535400390625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2328/100000], Training Loss: -21.192672729492188, Learning Rate: 0.015690529804500006\n",
      "Epoch [2329/100000], Training Loss: -21.1927490234375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2330/100000], Training Loss: -21.192840576171875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2331/100000], Training Loss: -21.192962646484375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2332/100000], Training Loss: -21.193038940429688, Learning Rate: 0.015690529804500006\n",
      "Epoch [2333/100000], Training Loss: -21.193145751953125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2334/100000], Training Loss: -21.193191528320312, Learning Rate: 0.015690529804500006\n",
      "Epoch [2335/100000], Training Loss: -21.193313598632812, Learning Rate: 0.015690529804500006\n",
      "Epoch [2336/100000], Training Loss: -21.193466186523438, Learning Rate: 0.015690529804500006\n",
      "Epoch [2337/100000], Training Loss: -21.193572998046875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2338/100000], Training Loss: -21.19366455078125, Learning Rate: 0.015690529804500006\n",
      "Epoch [2339/100000], Training Loss: -21.193756103515625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2340/100000], Training Loss: -21.193817138671875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2341/100000], Training Loss: -21.193923950195312, Learning Rate: 0.015690529804500006\n",
      "Epoch [2342/100000], Training Loss: -21.194000244140625, Learning Rate: 0.015690529804500006\n",
      "Epoch [2343/100000], Training Loss: -21.194168090820312, Learning Rate: 0.015690529804500006\n",
      "Epoch [2344/100000], Training Loss: -21.194229125976562, Learning Rate: 0.015690529804500006\n",
      "Epoch [2345/100000], Training Loss: -21.194351196289062, Learning Rate: 0.015690529804500006\n",
      "Epoch [2346/100000], Training Loss: -21.194442749023438, Learning Rate: 0.015690529804500006\n",
      "Epoch [2347/100000], Training Loss: -21.194549560546875, Learning Rate: 0.015690529804500006\n",
      "Epoch [2348/100000], Training Loss: -21.194671630859375, Learning Rate: 0.015690529804500006\n",
      "Epoch [2349/100000], Training Loss: -21.194732666015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2350/100000], Training Loss: -21.194793701171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2351/100000], Training Loss: -21.194915771484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2352/100000], Training Loss: -21.194992065429688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2353/100000], Training Loss: -21.19512939453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2354/100000], Training Loss: -21.195159912109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2355/100000], Training Loss: -21.195220947265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2356/100000], Training Loss: -21.195388793945312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2357/100000], Training Loss: -21.195449829101562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2358/100000], Training Loss: -21.19549560546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2359/100000], Training Loss: -21.195632934570312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2360/100000], Training Loss: -21.1956787109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2361/100000], Training Loss: -21.19580078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2362/100000], Training Loss: -21.195892333984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2363/100000], Training Loss: -21.19598388671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2364/100000], Training Loss: -21.196090698242188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2365/100000], Training Loss: -21.196136474609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2366/100000], Training Loss: -21.196273803710938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2367/100000], Training Loss: -21.196319580078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2368/100000], Training Loss: -21.1964111328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2369/100000], Training Loss: -21.19647216796875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2370/100000], Training Loss: -21.19659423828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2371/100000], Training Loss: -21.1966552734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2372/100000], Training Loss: -21.19677734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2373/100000], Training Loss: -21.196868896484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2374/100000], Training Loss: -21.19696044921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2375/100000], Training Loss: -21.197067260742188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2376/100000], Training Loss: -21.197113037109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2377/100000], Training Loss: -21.197219848632812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2378/100000], Training Loss: -21.197372436523438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2379/100000], Training Loss: -21.197402954101562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2380/100000], Training Loss: -21.197479248046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2381/100000], Training Loss: -21.197616577148438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2382/100000], Training Loss: -21.197708129882812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2383/100000], Training Loss: -21.197799682617188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2384/100000], Training Loss: -21.1978759765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2385/100000], Training Loss: -21.197982788085938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2386/100000], Training Loss: -21.19805908203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2387/100000], Training Loss: -21.198150634765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2388/100000], Training Loss: -21.198257446289062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2389/100000], Training Loss: -21.198318481445312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2390/100000], Training Loss: -21.198333740234375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2391/100000], Training Loss: -21.198486328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2392/100000], Training Loss: -21.198638916015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2393/100000], Training Loss: -21.198699951171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2394/100000], Training Loss: -21.198822021484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2395/100000], Training Loss: -21.198883056640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2396/100000], Training Loss: -21.198974609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2397/100000], Training Loss: -21.199050903320312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2398/100000], Training Loss: -21.199142456054688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2399/100000], Training Loss: -21.199249267578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2400/100000], Training Loss: -21.199325561523438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2401/100000], Training Loss: -21.199386596679688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2402/100000], Training Loss: -21.19952392578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2403/100000], Training Loss: -21.199600219726562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2404/100000], Training Loss: -21.199676513671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2405/100000], Training Loss: -21.199798583984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2406/100000], Training Loss: -21.199844360351562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2407/100000], Training Loss: -21.199996948242188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2408/100000], Training Loss: -21.200088500976562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2409/100000], Training Loss: -21.200180053710938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2410/100000], Training Loss: -21.200271606445312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2411/100000], Training Loss: -21.200347900390625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2412/100000], Training Loss: -21.200469970703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2413/100000], Training Loss: -21.200515747070312, Learning Rate: 0.014121476824050006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2414/100000], Training Loss: -21.200607299804688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2415/100000], Training Loss: -21.200729370117188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2416/100000], Training Loss: -21.200790405273438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2417/100000], Training Loss: -21.200897216796875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2418/100000], Training Loss: -21.201004028320312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2419/100000], Training Loss: -21.201095581054688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2420/100000], Training Loss: -21.201171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2421/100000], Training Loss: -21.201278686523438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2422/100000], Training Loss: -21.20135498046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2423/100000], Training Loss: -21.201507568359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2424/100000], Training Loss: -21.20159912109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2425/100000], Training Loss: -21.20166015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2426/100000], Training Loss: -21.201812744140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2427/100000], Training Loss: -21.20184326171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2428/100000], Training Loss: -21.201934814453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2429/100000], Training Loss: -21.202041625976562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2430/100000], Training Loss: -21.202133178710938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2431/100000], Training Loss: -21.20220947265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2432/100000], Training Loss: -21.20233154296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2433/100000], Training Loss: -21.202438354492188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2434/100000], Training Loss: -21.202529907226562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2435/100000], Training Loss: -21.202606201171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2436/100000], Training Loss: -21.202713012695312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2437/100000], Training Loss: -21.20281982421875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2438/100000], Training Loss: -21.202880859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2439/100000], Training Loss: -21.2030029296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2440/100000], Training Loss: -21.203109741210938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2441/100000], Training Loss: -21.203201293945312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2442/100000], Training Loss: -21.203292846679688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2443/100000], Training Loss: -21.203399658203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2444/100000], Training Loss: -21.203445434570312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2445/100000], Training Loss: -21.203536987304688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2446/100000], Training Loss: -21.203643798828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2447/100000], Training Loss: -21.203750610351562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2448/100000], Training Loss: -21.203842163085938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2449/100000], Training Loss: -21.203903198242188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2450/100000], Training Loss: -21.20404052734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2451/100000], Training Loss: -21.204116821289062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2452/100000], Training Loss: -21.204208374023438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2453/100000], Training Loss: -21.204315185546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2454/100000], Training Loss: -21.20440673828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2455/100000], Training Loss: -21.20452880859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2456/100000], Training Loss: -21.204620361328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2457/100000], Training Loss: -21.204681396484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2458/100000], Training Loss: -21.204833984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2459/100000], Training Loss: -21.204879760742188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2460/100000], Training Loss: -21.205001831054688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2461/100000], Training Loss: -21.205108642578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2462/100000], Training Loss: -21.205215454101562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2463/100000], Training Loss: -21.205291748046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2464/100000], Training Loss: -21.205352783203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2465/100000], Training Loss: -21.205474853515625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2466/100000], Training Loss: -21.205551147460938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2467/100000], Training Loss: -21.205673217773438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2468/100000], Training Loss: -21.205764770507812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2469/100000], Training Loss: -21.205886840820312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2470/100000], Training Loss: -21.205978393554688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2471/100000], Training Loss: -21.206024169921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2472/100000], Training Loss: -21.206161499023438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2473/100000], Training Loss: -21.20623779296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2474/100000], Training Loss: -21.206314086914062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2475/100000], Training Loss: -21.20648193359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2476/100000], Training Loss: -21.206558227539062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2477/100000], Training Loss: -21.2066650390625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2478/100000], Training Loss: -21.206756591796875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2479/100000], Training Loss: -21.206817626953125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2480/100000], Training Loss: -21.206893920898438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2481/100000], Training Loss: -21.207015991210938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2482/100000], Training Loss: -21.207122802734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2483/100000], Training Loss: -21.2071533203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2484/100000], Training Loss: -21.207290649414062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2485/100000], Training Loss: -21.207412719726562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2486/100000], Training Loss: -21.20745849609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2487/100000], Training Loss: -21.20758056640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2488/100000], Training Loss: -21.207672119140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2489/100000], Training Loss: -21.207809448242188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2490/100000], Training Loss: -21.2078857421875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2491/100000], Training Loss: -21.20794677734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2492/100000], Training Loss: -21.208114624023438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2493/100000], Training Loss: -21.208206176757812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2494/100000], Training Loss: -21.20831298828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2495/100000], Training Loss: -21.208404541015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2496/100000], Training Loss: -21.208465576171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2497/100000], Training Loss: -21.208526611328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2498/100000], Training Loss: -21.208648681640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2499/100000], Training Loss: -21.208786010742188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2500/100000], Training Loss: -21.208892822265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2501/100000], Training Loss: -21.208984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2502/100000], Training Loss: -21.209075927734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2503/100000], Training Loss: -21.209136962890625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2504/100000], Training Loss: -21.209274291992188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2505/100000], Training Loss: -21.2093505859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2506/100000], Training Loss: -21.20947265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2507/100000], Training Loss: -21.209579467773438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2508/100000], Training Loss: -21.209640502929688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2509/100000], Training Loss: -21.209747314453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2510/100000], Training Loss: -21.209869384765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2511/100000], Training Loss: -21.2099609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2512/100000], Training Loss: -21.2100830078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2513/100000], Training Loss: -21.21014404296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2514/100000], Training Loss: -21.210205078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2515/100000], Training Loss: -21.210372924804688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2516/100000], Training Loss: -21.210464477539062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2517/100000], Training Loss: -21.210586547851562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2518/100000], Training Loss: -21.210647583007812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2519/100000], Training Loss: -21.210678100585938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2520/100000], Training Loss: -21.210906982421875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2521/100000], Training Loss: -21.210922241210938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2522/100000], Training Loss: -21.211044311523438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2523/100000], Training Loss: -21.211151123046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2524/100000], Training Loss: -21.21124267578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2525/100000], Training Loss: -21.211349487304688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2526/100000], Training Loss: -21.211471557617188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2527/100000], Training Loss: -21.2115478515625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2528/100000], Training Loss: -21.211669921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2529/100000], Training Loss: -21.211761474609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2530/100000], Training Loss: -21.211822509765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2531/100000], Training Loss: -21.211944580078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2532/100000], Training Loss: -21.212051391601562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2533/100000], Training Loss: -21.212158203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2534/100000], Training Loss: -21.212295532226562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2535/100000], Training Loss: -21.212371826171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2536/100000], Training Loss: -21.212448120117188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2537/100000], Training Loss: -21.212554931640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2538/100000], Training Loss: -21.212677001953125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2539/100000], Training Loss: -21.212738037109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2540/100000], Training Loss: -21.212844848632812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2541/100000], Training Loss: -21.212966918945312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2542/100000], Training Loss: -21.213043212890625, Learning Rate: 0.014121476824050006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2543/100000], Training Loss: -21.21319580078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2544/100000], Training Loss: -21.2132568359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2545/100000], Training Loss: -21.21337890625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2546/100000], Training Loss: -21.21343994140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2547/100000], Training Loss: -21.21356201171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2548/100000], Training Loss: -21.213653564453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2549/100000], Training Loss: -21.213760375976562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2550/100000], Training Loss: -21.213821411132812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2551/100000], Training Loss: -21.21392822265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2552/100000], Training Loss: -21.214065551757812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2553/100000], Training Loss: -21.214157104492188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2554/100000], Training Loss: -21.214263916015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2555/100000], Training Loss: -21.214340209960938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2556/100000], Training Loss: -21.214431762695312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2557/100000], Training Loss: -21.21453857421875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2558/100000], Training Loss: -21.214706420898438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2559/100000], Training Loss: -21.214736938476562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2560/100000], Training Loss: -21.214889526367188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2561/100000], Training Loss: -21.2149658203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2562/100000], Training Loss: -21.215057373046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2563/100000], Training Loss: -21.215194702148438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2564/100000], Training Loss: -21.215255737304688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2565/100000], Training Loss: -21.215377807617188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2566/100000], Training Loss: -21.215469360351562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2567/100000], Training Loss: -21.215560913085938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2568/100000], Training Loss: -21.215713500976562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2569/100000], Training Loss: -21.215774536132812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2570/100000], Training Loss: -21.21588134765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2571/100000], Training Loss: -21.215988159179688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2572/100000], Training Loss: -21.216094970703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2573/100000], Training Loss: -21.216201782226562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2574/100000], Training Loss: -21.216278076171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2575/100000], Training Loss: -21.216400146484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2576/100000], Training Loss: -21.21649169921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2577/100000], Training Loss: -21.216598510742188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2578/100000], Training Loss: -21.2166748046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2579/100000], Training Loss: -21.216766357421875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2580/100000], Training Loss: -21.216873168945312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2581/100000], Training Loss: -21.217010498046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2582/100000], Training Loss: -21.217132568359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2583/100000], Training Loss: -21.21722412109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2584/100000], Training Loss: -21.217315673828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2585/100000], Training Loss: -21.217437744140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2586/100000], Training Loss: -21.217498779296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2587/100000], Training Loss: -21.217636108398438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2588/100000], Training Loss: -21.217697143554688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2589/100000], Training Loss: -21.21783447265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2590/100000], Training Loss: -21.217941284179688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2591/100000], Training Loss: -21.218017578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2592/100000], Training Loss: -21.218109130859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2593/100000], Training Loss: -21.218231201171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2594/100000], Training Loss: -21.218368530273438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2595/100000], Training Loss: -21.21844482421875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2596/100000], Training Loss: -21.218521118164062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2597/100000], Training Loss: -21.218597412109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2598/100000], Training Loss: -21.218765258789062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2599/100000], Training Loss: -21.218841552734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2600/100000], Training Loss: -21.218948364257812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2601/100000], Training Loss: -21.219070434570312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2602/100000], Training Loss: -21.219146728515625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2603/100000], Training Loss: -21.219268798828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2604/100000], Training Loss: -21.219390869140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2605/100000], Training Loss: -21.219497680664062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2606/100000], Training Loss: -21.219573974609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2607/100000], Training Loss: -21.219680786132812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2608/100000], Training Loss: -21.21978759765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2609/100000], Training Loss: -21.2198486328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2610/100000], Training Loss: -21.219985961914062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2611/100000], Training Loss: -21.220077514648438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2612/100000], Training Loss: -21.22021484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2613/100000], Training Loss: -21.220306396484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2614/100000], Training Loss: -21.220413208007812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2615/100000], Training Loss: -21.220504760742188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2616/100000], Training Loss: -21.220596313476562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2617/100000], Training Loss: -21.220733642578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2618/100000], Training Loss: -21.220779418945312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2619/100000], Training Loss: -21.220870971679688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2620/100000], Training Loss: -21.2210693359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2621/100000], Training Loss: -21.221145629882812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2622/100000], Training Loss: -21.221221923828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2623/100000], Training Loss: -21.221343994140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2624/100000], Training Loss: -21.221405029296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2625/100000], Training Loss: -21.221527099609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2626/100000], Training Loss: -21.221710205078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2627/100000], Training Loss: -21.221771240234375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2628/100000], Training Loss: -21.221939086914062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2629/100000], Training Loss: -21.221923828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2630/100000], Training Loss: -21.222091674804688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2631/100000], Training Loss: -21.222152709960938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2632/100000], Training Loss: -21.222274780273438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2633/100000], Training Loss: -21.222396850585938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2634/100000], Training Loss: -21.222503662109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2635/100000], Training Loss: -21.222579956054688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2636/100000], Training Loss: -21.222702026367188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2637/100000], Training Loss: -21.222808837890625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2638/100000], Training Loss: -21.222900390625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2639/100000], Training Loss: -21.22296142578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2640/100000], Training Loss: -21.223114013671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2641/100000], Training Loss: -21.223220825195312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2642/100000], Training Loss: -21.22332763671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2643/100000], Training Loss: -21.22344970703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2644/100000], Training Loss: -21.223526000976562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2645/100000], Training Loss: -21.2236328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2646/100000], Training Loss: -21.22369384765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2647/100000], Training Loss: -21.223831176757812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2648/100000], Training Loss: -21.22393798828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2649/100000], Training Loss: -21.224075317382812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2650/100000], Training Loss: -21.224212646484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2651/100000], Training Loss: -21.224273681640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2652/100000], Training Loss: -21.224380493164062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2653/100000], Training Loss: -21.224472045898438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2654/100000], Training Loss: -21.224609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2655/100000], Training Loss: -21.2247314453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2656/100000], Training Loss: -21.224822998046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2657/100000], Training Loss: -21.224899291992188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2658/100000], Training Loss: -21.22503662109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2659/100000], Training Loss: -21.22509765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2660/100000], Training Loss: -21.2252197265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2661/100000], Training Loss: -21.225311279296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2662/100000], Training Loss: -21.225433349609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2663/100000], Training Loss: -21.225570678710938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2664/100000], Training Loss: -21.22564697265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2665/100000], Training Loss: -21.225723266601562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2666/100000], Training Loss: -21.225830078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2667/100000], Training Loss: -21.225936889648438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2668/100000], Training Loss: -21.226043701171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2669/100000], Training Loss: -21.226165771484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2670/100000], Training Loss: -21.226272583007812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2671/100000], Training Loss: -21.226409912109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2672/100000], Training Loss: -21.226516723632812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2673/100000], Training Loss: -21.226608276367188, Learning Rate: 0.014121476824050006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2674/100000], Training Loss: -21.2266845703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2675/100000], Training Loss: -21.226806640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2676/100000], Training Loss: -21.226898193359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2677/100000], Training Loss: -21.227035522460938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2678/100000], Training Loss: -21.227142333984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2679/100000], Training Loss: -21.227218627929688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2680/100000], Training Loss: -21.227325439453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2681/100000], Training Loss: -21.227462768554688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2682/100000], Training Loss: -21.227554321289062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2683/100000], Training Loss: -21.227645874023438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2684/100000], Training Loss: -21.227752685546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2685/100000], Training Loss: -21.227874755859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2686/100000], Training Loss: -21.227935791015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2687/100000], Training Loss: -21.22802734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2688/100000], Training Loss: -21.228179931640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2689/100000], Training Loss: -21.228271484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2690/100000], Training Loss: -21.2283935546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2691/100000], Training Loss: -21.228485107421875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2692/100000], Training Loss: -21.228607177734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2693/100000], Training Loss: -21.228729248046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2694/100000], Training Loss: -21.22882080078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2695/100000], Training Loss: -21.228897094726562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2696/100000], Training Loss: -21.229019165039062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2697/100000], Training Loss: -21.229110717773438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2698/100000], Training Loss: -21.229263305664062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2699/100000], Training Loss: -21.229324340820312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2700/100000], Training Loss: -21.229446411132812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2701/100000], Training Loss: -21.229583740234375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2702/100000], Training Loss: -21.229629516601562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2703/100000], Training Loss: -21.229782104492188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2704/100000], Training Loss: -21.229888916015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2705/100000], Training Loss: -21.22998046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2706/100000], Training Loss: -21.2301025390625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2707/100000], Training Loss: -21.230178833007812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2708/100000], Training Loss: -21.23028564453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2709/100000], Training Loss: -21.230422973632812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2710/100000], Training Loss: -21.230499267578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2711/100000], Training Loss: -21.23065185546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2712/100000], Training Loss: -21.230728149414062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2713/100000], Training Loss: -21.230850219726562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2714/100000], Training Loss: -21.23095703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2715/100000], Training Loss: -21.231063842773438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2716/100000], Training Loss: -21.231201171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2717/100000], Training Loss: -21.231292724609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2718/100000], Training Loss: -21.23138427734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2719/100000], Training Loss: -21.231475830078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2720/100000], Training Loss: -21.231597900390625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2721/100000], Training Loss: -21.231719970703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2722/100000], Training Loss: -21.231826782226562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2723/100000], Training Loss: -21.231903076171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2724/100000], Training Loss: -21.232009887695312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2725/100000], Training Loss: -21.232101440429688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2726/100000], Training Loss: -21.23223876953125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2727/100000], Training Loss: -21.23236083984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2728/100000], Training Loss: -21.232467651367188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2729/100000], Training Loss: -21.232589721679688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2730/100000], Training Loss: -21.232650756835938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2731/100000], Training Loss: -21.2327880859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2732/100000], Training Loss: -21.232894897460938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2733/100000], Training Loss: -21.233001708984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2734/100000], Training Loss: -21.233108520507812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2735/100000], Training Loss: -21.233245849609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2736/100000], Training Loss: -21.23333740234375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2737/100000], Training Loss: -21.233428955078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2738/100000], Training Loss: -21.2335205078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2739/100000], Training Loss: -21.233642578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2740/100000], Training Loss: -21.2337646484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2741/100000], Training Loss: -21.233871459960938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2742/100000], Training Loss: -21.23394775390625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2743/100000], Training Loss: -21.23406982421875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2744/100000], Training Loss: -21.23419189453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2745/100000], Training Loss: -21.234268188476562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2746/100000], Training Loss: -21.234390258789062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2747/100000], Training Loss: -21.234481811523438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2748/100000], Training Loss: -21.234619140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2749/100000], Training Loss: -21.234710693359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2750/100000], Training Loss: -21.234832763671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2751/100000], Training Loss: -21.234939575195312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2752/100000], Training Loss: -21.235031127929688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2753/100000], Training Loss: -21.235122680664062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2754/100000], Training Loss: -21.2352294921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2755/100000], Training Loss: -21.235382080078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2756/100000], Training Loss: -21.235458374023438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2757/100000], Training Loss: -21.235519409179688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2758/100000], Training Loss: -21.235687255859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2759/100000], Training Loss: -21.235809326171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2760/100000], Training Loss: -21.235870361328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2761/100000], Training Loss: -21.236007690429688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2762/100000], Training Loss: -21.236114501953125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2763/100000], Training Loss: -21.236190795898438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2764/100000], Training Loss: -21.236312866210938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2765/100000], Training Loss: -21.236465454101562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2766/100000], Training Loss: -21.236557006835938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2767/100000], Training Loss: -21.236648559570312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2768/100000], Training Loss: -21.23675537109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2769/100000], Training Loss: -21.236831665039062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2770/100000], Training Loss: -21.236953735351562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2771/100000], Training Loss: -21.237060546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2772/100000], Training Loss: -21.237213134765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2773/100000], Training Loss: -21.237319946289062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2774/100000], Training Loss: -21.237380981445312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2775/100000], Training Loss: -21.237533569335938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2776/100000], Training Loss: -21.237564086914062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2777/100000], Training Loss: -21.237762451171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2778/100000], Training Loss: -21.23785400390625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2779/100000], Training Loss: -21.237960815429688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2780/100000], Training Loss: -21.23809814453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2781/100000], Training Loss: -21.238174438476562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2782/100000], Training Loss: -21.238250732421875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2783/100000], Training Loss: -21.238388061523438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2784/100000], Training Loss: -21.238479614257812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2785/100000], Training Loss: -21.238616943359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2786/100000], Training Loss: -21.238723754882812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2787/100000], Training Loss: -21.238815307617188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2788/100000], Training Loss: -21.238937377929688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2789/100000], Training Loss: -21.239028930664062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2790/100000], Training Loss: -21.239181518554688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2791/100000], Training Loss: -21.2392578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2792/100000], Training Loss: -21.23931884765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2793/100000], Training Loss: -21.2393798828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2794/100000], Training Loss: -21.239517211914062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2795/100000], Training Loss: -21.23944091796875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2796/100000], Training Loss: -21.239517211914062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2797/100000], Training Loss: -21.239425659179688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2798/100000], Training Loss: -21.239105224609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2799/100000], Training Loss: -21.23846435546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2800/100000], Training Loss: -21.237197875976562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2801/100000], Training Loss: -21.23468017578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2802/100000], Training Loss: -21.229705810546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2803/100000], Training Loss: -21.2200927734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2804/100000], Training Loss: -21.200698852539062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2805/100000], Training Loss: -21.162841796875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2806/100000], Training Loss: -21.085479736328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2807/100000], Training Loss: -20.938125610351562, Learning Rate: 0.014121476824050006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2808/100000], Training Loss: -20.639404296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2809/100000], Training Loss: -20.136978149414062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2810/100000], Training Loss: -19.241226196289062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2811/100000], Training Loss: -18.31689453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2812/100000], Training Loss: -17.45233154296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2813/100000], Training Loss: -18.165863037109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2814/100000], Training Loss: -19.730300903320312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2815/100000], Training Loss: -21.111740112304688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2816/100000], Training Loss: -20.920852661132812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2817/100000], Training Loss: -19.861785888671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2818/100000], Training Loss: -19.72882080078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2819/100000], Training Loss: -20.617385864257812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2820/100000], Training Loss: -21.232635498046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2821/100000], Training Loss: -20.770477294921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2822/100000], Training Loss: -20.266204833984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2823/100000], Training Loss: -20.692901611328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2824/100000], Training Loss: -21.214111328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2825/100000], Training Loss: -21.00946044921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2826/100000], Training Loss: -20.658966064453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2827/100000], Training Loss: -20.868453979492188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2828/100000], Training Loss: -21.217987060546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2829/100000], Training Loss: -21.076705932617188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2830/100000], Training Loss: -20.845840454101562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2831/100000], Training Loss: -21.027969360351562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2832/100000], Training Loss: -21.22967529296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2833/100000], Training Loss: -21.098876953125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2834/100000], Training Loss: -20.986068725585938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2835/100000], Training Loss: -21.131500244140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2836/100000], Training Loss: -21.228805541992188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2837/100000], Training Loss: -21.116073608398438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2838/100000], Training Loss: -21.073104858398438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2839/100000], Training Loss: -21.190887451171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2840/100000], Training Loss: -21.219985961914062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2841/100000], Training Loss: -21.134506225585938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2842/100000], Training Loss: -21.140029907226562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2843/100000], Training Loss: -21.21942138671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2844/100000], Training Loss: -21.212799072265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2845/100000], Training Loss: -21.15911865234375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2846/100000], Training Loss: -21.18115234375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2847/100000], Training Loss: -21.229171752929688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2848/100000], Training Loss: -21.21002197265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2849/100000], Training Loss: -21.179916381835938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2850/100000], Training Loss: -21.205841064453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2851/100000], Training Loss: -21.231292724609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2852/100000], Training Loss: -21.211441040039062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2853/100000], Training Loss: -21.197723388671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2854/100000], Training Loss: -21.219100952148438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2855/100000], Training Loss: -21.23114013671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2856/100000], Training Loss: -21.215301513671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2857/100000], Training Loss: -21.209869384765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2858/100000], Training Loss: -21.225845336914062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2859/100000], Training Loss: -21.230758666992188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2860/100000], Training Loss: -21.219375610351562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2861/100000], Training Loss: -21.21826171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2862/100000], Training Loss: -21.229156494140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2863/100000], Training Loss: -21.230926513671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2864/100000], Training Loss: -21.223312377929688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2865/100000], Training Loss: -21.223541259765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2866/100000], Training Loss: -21.230865478515625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2867/100000], Training Loss: -21.231414794921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2868/100000], Training Loss: -21.226425170898438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2869/100000], Training Loss: -21.226959228515625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2870/100000], Training Loss: -21.231796264648438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2871/100000], Training Loss: -21.232070922851562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2872/100000], Training Loss: -21.228866577148438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2873/100000], Training Loss: -21.22918701171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2874/100000], Training Loss: -21.232437133789062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2875/100000], Training Loss: -21.232742309570312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2876/100000], Training Loss: -21.230697631835938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2877/100000], Training Loss: -21.230758666992188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2878/100000], Training Loss: -21.232925415039062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2879/100000], Training Loss: -21.233444213867188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2880/100000], Training Loss: -21.232101440429688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2881/100000], Training Loss: -21.23193359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2882/100000], Training Loss: -21.2333984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2883/100000], Training Loss: -21.233963012695312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2884/100000], Training Loss: -21.233200073242188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2885/100000], Training Loss: -21.23291015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2886/100000], Training Loss: -21.233749389648438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2887/100000], Training Loss: -21.234344482421875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2888/100000], Training Loss: -21.234024047851562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2889/100000], Training Loss: -21.233688354492188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2890/100000], Training Loss: -21.23419189453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2891/100000], Training Loss: -21.234771728515625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2892/100000], Training Loss: -21.234695434570312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2893/100000], Training Loss: -21.234481811523438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2894/100000], Training Loss: -21.234664916992188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2895/100000], Training Loss: -21.235137939453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2896/100000], Training Loss: -21.235244750976562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2897/100000], Training Loss: -21.235076904296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2898/100000], Training Loss: -21.235122680664062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2899/100000], Training Loss: -21.235488891601562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2900/100000], Training Loss: -21.23565673828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2901/100000], Training Loss: -21.235687255859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2902/100000], Training Loss: -21.235641479492188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2903/100000], Training Loss: -21.235794067382812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2904/100000], Training Loss: -21.23602294921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2905/100000], Training Loss: -21.236175537109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2906/100000], Training Loss: -21.236129760742188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2907/100000], Training Loss: -21.236251831054688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2908/100000], Training Loss: -21.236373901367188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2909/100000], Training Loss: -21.236557006835938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2910/100000], Training Loss: -21.236618041992188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2911/100000], Training Loss: -21.236679077148438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2912/100000], Training Loss: -21.236785888671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2913/100000], Training Loss: -21.2369384765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2914/100000], Training Loss: -21.237106323242188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2915/100000], Training Loss: -21.237167358398438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2916/100000], Training Loss: -21.237228393554688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2917/100000], Training Loss: -21.2373046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2918/100000], Training Loss: -21.23748779296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2919/100000], Training Loss: -21.237594604492188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2920/100000], Training Loss: -21.237640380859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2921/100000], Training Loss: -21.237701416015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2922/100000], Training Loss: -21.237838745117188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2923/100000], Training Loss: -21.238006591796875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2924/100000], Training Loss: -21.238037109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2925/100000], Training Loss: -21.238189697265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2926/100000], Training Loss: -21.23822021484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2927/100000], Training Loss: -21.2384033203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2928/100000], Training Loss: -21.238494873046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2929/100000], Training Loss: -21.238571166992188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2930/100000], Training Loss: -21.238677978515625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2931/100000], Training Loss: -21.238739013671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2932/100000], Training Loss: -21.23883056640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2933/100000], Training Loss: -21.238967895507812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2934/100000], Training Loss: -21.239089965820312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2935/100000], Training Loss: -21.239166259765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2936/100000], Training Loss: -21.239273071289062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2937/100000], Training Loss: -21.239395141601562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2938/100000], Training Loss: -21.239456176757812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2939/100000], Training Loss: -21.239532470703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2940/100000], Training Loss: -21.239669799804688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2941/100000], Training Loss: -21.23980712890625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2942/100000], Training Loss: -21.239883422851562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2943/100000], Training Loss: -21.240036010742188, Learning Rate: 0.014121476824050006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2944/100000], Training Loss: -21.240097045898438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2945/100000], Training Loss: -21.240188598632812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2946/100000], Training Loss: -21.240264892578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2947/100000], Training Loss: -21.240341186523438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2948/100000], Training Loss: -21.240463256835938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2949/100000], Training Loss: -21.240524291992188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2950/100000], Training Loss: -21.24066162109375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2951/100000], Training Loss: -21.24078369140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2952/100000], Training Loss: -21.2408447265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2953/100000], Training Loss: -21.240982055664062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2954/100000], Training Loss: -21.241058349609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2955/100000], Training Loss: -21.241180419921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2956/100000], Training Loss: -21.241241455078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2957/100000], Training Loss: -21.241363525390625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2958/100000], Training Loss: -21.241439819335938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2959/100000], Training Loss: -21.241546630859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2960/100000], Training Loss: -21.241683959960938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2961/100000], Training Loss: -21.241744995117188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2962/100000], Training Loss: -21.241851806640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2963/100000], Training Loss: -21.241912841796875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2964/100000], Training Loss: -21.242019653320312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2965/100000], Training Loss: -21.24212646484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2966/100000], Training Loss: -21.242263793945312, Learning Rate: 0.014121476824050006\n",
      "Epoch [2967/100000], Training Loss: -21.242355346679688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2968/100000], Training Loss: -21.242416381835938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2969/100000], Training Loss: -21.242568969726562, Learning Rate: 0.014121476824050006\n",
      "Epoch [2970/100000], Training Loss: -21.242599487304688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2971/100000], Training Loss: -21.24273681640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2972/100000], Training Loss: -21.242874145507812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2973/100000], Training Loss: -21.242935180664062, Learning Rate: 0.014121476824050006\n",
      "Epoch [2974/100000], Training Loss: -21.243026733398438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2975/100000], Training Loss: -21.24310302734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2976/100000], Training Loss: -21.243209838867188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2977/100000], Training Loss: -21.243270874023438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2978/100000], Training Loss: -21.243438720703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2979/100000], Training Loss: -21.243515014648438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2980/100000], Training Loss: -21.243606567382812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2981/100000], Training Loss: -21.243698120117188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2982/100000], Training Loss: -21.243850708007812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2983/100000], Training Loss: -21.243942260742188, Learning Rate: 0.014121476824050006\n",
      "Epoch [2984/100000], Training Loss: -21.2440185546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2985/100000], Training Loss: -21.244094848632812, Learning Rate: 0.014121476824050006\n",
      "Epoch [2986/100000], Training Loss: -21.244232177734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2987/100000], Training Loss: -21.24432373046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2988/100000], Training Loss: -21.244415283203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2989/100000], Training Loss: -21.2445068359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2990/100000], Training Loss: -21.244598388671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2991/100000], Training Loss: -21.244735717773438, Learning Rate: 0.014121476824050006\n",
      "Epoch [2992/100000], Training Loss: -21.244781494140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2993/100000], Training Loss: -21.244873046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [2994/100000], Training Loss: -21.245025634765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [2995/100000], Training Loss: -21.245101928710938, Learning Rate: 0.014121476824050006\n",
      "Epoch [2996/100000], Training Loss: -21.245208740234375, Learning Rate: 0.014121476824050006\n",
      "Epoch [2997/100000], Training Loss: -21.245285034179688, Learning Rate: 0.014121476824050006\n",
      "Epoch [2998/100000], Training Loss: -21.24542236328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [2999/100000], Training Loss: -21.245513916015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3000/100000], Training Loss: -21.245635986328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3001/100000], Training Loss: -21.245697021484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3002/100000], Training Loss: -21.24578857421875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3003/100000], Training Loss: -21.245880126953125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3004/100000], Training Loss: -21.246047973632812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3005/100000], Training Loss: -21.246078491210938, Learning Rate: 0.014121476824050006\n",
      "Epoch [3006/100000], Training Loss: -21.2462158203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3007/100000], Training Loss: -21.246292114257812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3008/100000], Training Loss: -21.246444702148438, Learning Rate: 0.014121476824050006\n",
      "Epoch [3009/100000], Training Loss: -21.2464599609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3010/100000], Training Loss: -21.246597290039062, Learning Rate: 0.014121476824050006\n",
      "Epoch [3011/100000], Training Loss: -21.2467041015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3012/100000], Training Loss: -21.246749877929688, Learning Rate: 0.014121476824050006\n",
      "Epoch [3013/100000], Training Loss: -21.24688720703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3014/100000], Training Loss: -21.247024536132812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3015/100000], Training Loss: -21.247100830078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3016/100000], Training Loss: -21.247207641601562, Learning Rate: 0.014121476824050006\n",
      "Epoch [3017/100000], Training Loss: -21.247268676757812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3018/100000], Training Loss: -21.247360229492188, Learning Rate: 0.014121476824050006\n",
      "Epoch [3019/100000], Training Loss: -21.247467041015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3020/100000], Training Loss: -21.24761962890625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3021/100000], Training Loss: -21.247695922851562, Learning Rate: 0.014121476824050006\n",
      "Epoch [3022/100000], Training Loss: -21.247802734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3023/100000], Training Loss: -21.247879028320312, Learning Rate: 0.014121476824050006\n",
      "Epoch [3024/100000], Training Loss: -21.247955322265625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3025/100000], Training Loss: -21.248031616210938, Learning Rate: 0.014121476824050006\n",
      "Epoch [3026/100000], Training Loss: -21.2481689453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3027/100000], Training Loss: -21.24822998046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3028/100000], Training Loss: -21.248382568359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3029/100000], Training Loss: -21.248489379882812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3030/100000], Training Loss: -21.248550415039062, Learning Rate: 0.014121476824050006\n",
      "Epoch [3031/100000], Training Loss: -21.248672485351562, Learning Rate: 0.014121476824050006\n",
      "Epoch [3032/100000], Training Loss: -21.248748779296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3033/100000], Training Loss: -21.248870849609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3034/100000], Training Loss: -21.248931884765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3035/100000], Training Loss: -21.2490234375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3036/100000], Training Loss: -21.2491455078125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3037/100000], Training Loss: -21.249221801757812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3038/100000], Training Loss: -21.249359130859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3039/100000], Training Loss: -21.249420166015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3040/100000], Training Loss: -21.24945068359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3041/100000], Training Loss: -21.249618530273438, Learning Rate: 0.014121476824050006\n",
      "Epoch [3042/100000], Training Loss: -21.249679565429688, Learning Rate: 0.014121476824050006\n",
      "Epoch [3043/100000], Training Loss: -21.249771118164062, Learning Rate: 0.014121476824050006\n",
      "Epoch [3044/100000], Training Loss: -21.249755859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3045/100000], Training Loss: -21.249786376953125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3046/100000], Training Loss: -21.249649047851562, Learning Rate: 0.014121476824050006\n",
      "Epoch [3047/100000], Training Loss: -21.2493896484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3048/100000], Training Loss: -21.248809814453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3049/100000], Training Loss: -21.247665405273438, Learning Rate: 0.014121476824050006\n",
      "Epoch [3050/100000], Training Loss: -21.245346069335938, Learning Rate: 0.014121476824050006\n",
      "Epoch [3051/100000], Training Loss: -21.241058349609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3052/100000], Training Loss: -21.232559204101562, Learning Rate: 0.014121476824050006\n",
      "Epoch [3053/100000], Training Loss: -21.216354370117188, Learning Rate: 0.014121476824050006\n",
      "Epoch [3054/100000], Training Loss: -21.184234619140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3055/100000], Training Loss: -21.122344970703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3056/100000], Training Loss: -20.997482299804688, Learning Rate: 0.014121476824050006\n",
      "Epoch [3057/100000], Training Loss: -20.760971069335938, Learning Rate: 0.014121476824050006\n",
      "Epoch [3058/100000], Training Loss: -20.283248901367188, Learning Rate: 0.014121476824050006\n",
      "Epoch [3059/100000], Training Loss: -19.464401245117188, Learning Rate: 0.014121476824050006\n",
      "Epoch [3060/100000], Training Loss: -17.96099853515625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3061/100000], Training Loss: -16.287612915039062, Learning Rate: 0.014121476824050006\n",
      "Epoch [3062/100000], Training Loss: -14.5904541015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3063/100000], Training Loss: -15.848098754882812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3064/100000], Training Loss: -18.798629760742188, Learning Rate: 0.014121476824050006\n",
      "Epoch [3065/100000], Training Loss: -21.149383544921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3066/100000], Training Loss: -20.305908203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3067/100000], Training Loss: -18.369720458984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3068/100000], Training Loss: -18.9599609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3069/100000], Training Loss: -20.850692749023438, Learning Rate: 0.014121476824050006\n",
      "Epoch [3070/100000], Training Loss: -20.97381591796875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3071/100000], Training Loss: -19.701263427734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3072/100000], Training Loss: -19.800765991210938, Learning Rate: 0.014121476824050006\n",
      "Epoch [3073/100000], Training Loss: -21.066421508789062, Learning Rate: 0.014121476824050006\n",
      "Epoch [3074/100000], Training Loss: -20.932769775390625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3075/100000], Training Loss: -20.098098754882812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3076/100000], Training Loss: -20.573867797851562, Learning Rate: 0.014121476824050006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3077/100000], Training Loss: -21.232803344726562, Learning Rate: 0.014121476824050006\n",
      "Epoch [3078/100000], Training Loss: -20.7928466796875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3079/100000], Training Loss: -20.52239990234375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3080/100000], Training Loss: -21.054458618164062, Learning Rate: 0.014121476824050006\n",
      "Epoch [3081/100000], Training Loss: -21.17041015625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3082/100000], Training Loss: -20.768264770507812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3083/100000], Training Loss: -20.897781372070312, Learning Rate: 0.014121476824050006\n",
      "Epoch [3084/100000], Training Loss: -21.231796264648438, Learning Rate: 0.014121476824050006\n",
      "Epoch [3085/100000], Training Loss: -21.028656005859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3086/100000], Training Loss: -20.896148681640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3087/100000], Training Loss: -21.168182373046875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3088/100000], Training Loss: -21.182525634765625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3089/100000], Training Loss: -20.992630004882812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3090/100000], Training Loss: -21.108489990234375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3091/100000], Training Loss: -21.232406616210938, Learning Rate: 0.014121476824050006\n",
      "Epoch [3092/100000], Training Loss: -21.105743408203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3093/100000], Training Loss: -21.094390869140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3094/100000], Training Loss: -21.22283935546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3095/100000], Training Loss: -21.18804931640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3096/100000], Training Loss: -21.119125366210938, Learning Rate: 0.014121476824050006\n",
      "Epoch [3097/100000], Training Loss: -21.196212768554688, Learning Rate: 0.014121476824050006\n",
      "Epoch [3098/100000], Training Loss: -21.227813720703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3099/100000], Training Loss: -21.161346435546875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3100/100000], Training Loss: -21.178863525390625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3101/100000], Training Loss: -21.23443603515625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3102/100000], Training Loss: -21.201507568359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3103/100000], Training Loss: -21.179962158203125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3104/100000], Training Loss: -21.224807739257812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3105/100000], Training Loss: -21.22735595703125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3106/100000], Training Loss: -21.195663452148438, Learning Rate: 0.014121476824050006\n",
      "Epoch [3107/100000], Training Loss: -21.213943481445312, Learning Rate: 0.014121476824050006\n",
      "Epoch [3108/100000], Training Loss: -21.236068725585938, Learning Rate: 0.014121476824050006\n",
      "Epoch [3109/100000], Training Loss: -21.21551513671875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3110/100000], Training Loss: -21.210830688476562, Learning Rate: 0.014121476824050006\n",
      "Epoch [3111/100000], Training Loss: -21.233078002929688, Learning Rate: 0.014121476824050006\n",
      "Epoch [3112/100000], Training Loss: -21.23040771484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3113/100000], Training Loss: -21.216445922851562, Learning Rate: 0.014121476824050006\n",
      "Epoch [3114/100000], Training Loss: -21.227142333984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3115/100000], Training Loss: -21.236358642578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3116/100000], Training Loss: -21.2259521484375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3117/100000], Training Loss: -21.224777221679688, Learning Rate: 0.014121476824050006\n",
      "Epoch [3118/100000], Training Loss: -21.235488891601562, Learning Rate: 0.014121476824050006\n",
      "Epoch [3119/100000], Training Loss: -21.23370361328125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3120/100000], Training Loss: -21.227249145507812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3121/100000], Training Loss: -21.232574462890625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3122/100000], Training Loss: -21.23699951171875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3123/100000], Training Loss: -21.23211669921875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3124/100000], Training Loss: -21.231430053710938, Learning Rate: 0.014121476824050006\n",
      "Epoch [3125/100000], Training Loss: -21.236663818359375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3126/100000], Training Loss: -21.236251831054688, Learning Rate: 0.014121476824050006\n",
      "Epoch [3127/100000], Training Loss: -21.23297119140625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3128/100000], Training Loss: -21.235198974609375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3129/100000], Training Loss: -21.23779296875, Learning Rate: 0.014121476824050006\n",
      "Epoch [3130/100000], Training Loss: -21.23577880859375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3131/100000], Training Loss: -21.23492431640625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3132/100000], Training Loss: -21.237396240234375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3133/100000], Training Loss: -21.237838745117188, Learning Rate: 0.014121476824050006\n",
      "Epoch [3134/100000], Training Loss: -21.236160278320312, Learning Rate: 0.014121476824050006\n",
      "Epoch [3135/100000], Training Loss: -21.236740112304688, Learning Rate: 0.014121476824050006\n",
      "Epoch [3136/100000], Training Loss: -21.238372802734375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3137/100000], Training Loss: -21.237869262695312, Learning Rate: 0.014121476824050006\n",
      "Epoch [3138/100000], Training Loss: -21.237106323242188, Learning Rate: 0.014121476824050006\n",
      "Epoch [3139/100000], Training Loss: -21.238067626953125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3140/100000], Training Loss: -21.23876953125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3141/100000], Training Loss: -21.238143920898438, Learning Rate: 0.014121476824050006\n",
      "Epoch [3142/100000], Training Loss: -21.238052368164062, Learning Rate: 0.014121476824050006\n",
      "Epoch [3143/100000], Training Loss: -21.238937377929688, Learning Rate: 0.014121476824050006\n",
      "Epoch [3144/100000], Training Loss: -21.239044189453125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3145/100000], Training Loss: -21.23858642578125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3146/100000], Training Loss: -21.238861083984375, Learning Rate: 0.014121476824050006\n",
      "Epoch [3147/100000], Training Loss: -21.239410400390625, Learning Rate: 0.014121476824050006\n",
      "Epoch [3148/100000], Training Loss: -21.2393798828125, Learning Rate: 0.014121476824050006\n",
      "Epoch [3149/100000], Training Loss: -21.239181518554688, Learning Rate: 0.014121476824050006\n",
      "Epoch [3150/100000], Training Loss: -21.239456176757812, Learning Rate: 0.014121476824050006\n",
      "Epoch [3151/100000], Training Loss: -21.239837646484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3152/100000], Training Loss: -21.239700317382812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3153/100000], Training Loss: -21.239761352539062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3154/100000], Training Loss: -21.240066528320312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3155/100000], Training Loss: -21.239944458007812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3156/100000], Training Loss: -21.240081787109375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3157/100000], Training Loss: -21.24029541015625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3158/100000], Training Loss: -21.240280151367188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3159/100000], Training Loss: -21.240371704101562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3160/100000], Training Loss: -21.240554809570312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3161/100000], Training Loss: -21.240585327148438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3162/100000], Training Loss: -21.2406005859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3163/100000], Training Loss: -21.240798950195312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3164/100000], Training Loss: -21.2408447265625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3165/100000], Training Loss: -21.240875244140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3166/100000], Training Loss: -21.241073608398438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3167/100000], Training Loss: -21.241134643554688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3168/100000], Training Loss: -21.241134643554688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3169/100000], Training Loss: -21.241287231445312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3170/100000], Training Loss: -21.24139404296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3171/100000], Training Loss: -21.241424560546875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3172/100000], Training Loss: -21.241546630859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3173/100000], Training Loss: -21.24163818359375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3174/100000], Training Loss: -21.241683959960938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3175/100000], Training Loss: -21.241806030273438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3176/100000], Training Loss: -21.241897583007812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3177/100000], Training Loss: -21.241958618164062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3178/100000], Training Loss: -21.242034912109375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3179/100000], Training Loss: -21.24212646484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3180/100000], Training Loss: -21.242202758789062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3181/100000], Training Loss: -21.2423095703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3182/100000], Training Loss: -21.242340087890625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3183/100000], Training Loss: -21.242446899414062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3184/100000], Training Loss: -21.24249267578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3185/100000], Training Loss: -21.242645263671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3186/100000], Training Loss: -21.24267578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3187/100000], Training Loss: -21.242767333984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3188/100000], Training Loss: -21.24285888671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3189/100000], Training Loss: -21.242950439453125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3190/100000], Training Loss: -21.243026733398438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3191/100000], Training Loss: -21.24310302734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3192/100000], Training Loss: -21.2431640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3193/100000], Training Loss: -21.243240356445312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3194/100000], Training Loss: -21.243331909179688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3195/100000], Training Loss: -21.243438720703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3196/100000], Training Loss: -21.243545532226562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3197/100000], Training Loss: -21.243606567382812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3198/100000], Training Loss: -21.243682861328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3199/100000], Training Loss: -21.243759155273438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3200/100000], Training Loss: -21.24383544921875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3201/100000], Training Loss: -21.243896484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3202/100000], Training Loss: -21.243988037109375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3203/100000], Training Loss: -21.244064331054688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3204/100000], Training Loss: -21.244140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3205/100000], Training Loss: -21.244232177734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3206/100000], Training Loss: -21.244277954101562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3207/100000], Training Loss: -21.244400024414062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3208/100000], Training Loss: -21.244476318359375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3209/100000], Training Loss: -21.244537353515625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3210/100000], Training Loss: -21.24462890625, Learning Rate: 0.012709329141645007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3211/100000], Training Loss: -21.244720458984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3212/100000], Training Loss: -21.2447509765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3213/100000], Training Loss: -21.24481201171875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3214/100000], Training Loss: -21.244964599609375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3215/100000], Training Loss: -21.245025634765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3216/100000], Training Loss: -21.245132446289062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3217/100000], Training Loss: -21.245208740234375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3218/100000], Training Loss: -21.24530029296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3219/100000], Training Loss: -21.245361328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3220/100000], Training Loss: -21.245452880859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3221/100000], Training Loss: -21.245498657226562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3222/100000], Training Loss: -21.245620727539062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3223/100000], Training Loss: -21.245681762695312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3224/100000], Training Loss: -21.245742797851562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3225/100000], Training Loss: -21.245819091796875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3226/100000], Training Loss: -21.245956420898438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3227/100000], Training Loss: -21.245986938476562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3228/100000], Training Loss: -21.246078491210938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3229/100000], Training Loss: -21.246200561523438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3230/100000], Training Loss: -21.246231079101562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3231/100000], Training Loss: -21.246307373046875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3232/100000], Training Loss: -21.246368408203125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3233/100000], Training Loss: -21.246490478515625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3234/100000], Training Loss: -21.24652099609375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3235/100000], Training Loss: -21.246627807617188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3236/100000], Training Loss: -21.246734619140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3237/100000], Training Loss: -21.246810913085938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3238/100000], Training Loss: -21.24688720703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3239/100000], Training Loss: -21.2469482421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3240/100000], Training Loss: -21.24700927734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3241/100000], Training Loss: -21.247100830078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3242/100000], Training Loss: -21.24713134765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3243/100000], Training Loss: -21.24725341796875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3244/100000], Training Loss: -21.247344970703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3245/100000], Training Loss: -21.247451782226562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3246/100000], Training Loss: -21.24749755859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3247/100000], Training Loss: -21.247589111328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3248/100000], Training Loss: -21.247665405273438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3249/100000], Training Loss: -21.24774169921875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3250/100000], Training Loss: -21.247817993164062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3251/100000], Training Loss: -21.247909545898438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3252/100000], Training Loss: -21.248001098632812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3253/100000], Training Loss: -21.248062133789062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3254/100000], Training Loss: -21.2481689453125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3255/100000], Training Loss: -21.248184204101562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3256/100000], Training Loss: -21.248291015625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3257/100000], Training Loss: -21.248397827148438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3258/100000], Training Loss: -21.248489379882812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3259/100000], Training Loss: -21.24853515625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3260/100000], Training Loss: -21.248611450195312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3261/100000], Training Loss: -21.248703002929688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3262/100000], Training Loss: -21.248779296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3263/100000], Training Loss: -21.248855590820312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3264/100000], Training Loss: -21.248931884765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3265/100000], Training Loss: -21.249038696289062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3266/100000], Training Loss: -21.24908447265625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3267/100000], Training Loss: -21.249191284179688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3268/100000], Training Loss: -21.249313354492188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3269/100000], Training Loss: -21.249374389648438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3270/100000], Training Loss: -21.249435424804688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3271/100000], Training Loss: -21.249496459960938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3272/100000], Training Loss: -21.249588012695312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3273/100000], Training Loss: -21.249664306640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3274/100000], Training Loss: -21.24969482421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3275/100000], Training Loss: -21.249832153320312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3276/100000], Training Loss: -21.24993896484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3277/100000], Training Loss: -21.249969482421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3278/100000], Training Loss: -21.250045776367188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3279/100000], Training Loss: -21.250198364257812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3280/100000], Training Loss: -21.250213623046875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3281/100000], Training Loss: -21.25030517578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3282/100000], Training Loss: -21.250396728515625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3283/100000], Training Loss: -21.250442504882812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3284/100000], Training Loss: -21.25054931640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3285/100000], Training Loss: -21.250625610351562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3286/100000], Training Loss: -21.250701904296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3287/100000], Training Loss: -21.25079345703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3288/100000], Training Loss: -21.2508544921875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3289/100000], Training Loss: -21.25091552734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3290/100000], Training Loss: -21.250991821289062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3291/100000], Training Loss: -21.251113891601562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3292/100000], Training Loss: -21.251205444335938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3293/100000], Training Loss: -21.251266479492188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3294/100000], Training Loss: -21.2513427734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3295/100000], Training Loss: -21.251388549804688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3296/100000], Training Loss: -21.251510620117188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3297/100000], Training Loss: -21.251510620117188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3298/100000], Training Loss: -21.2515869140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3299/100000], Training Loss: -21.251739501953125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3300/100000], Training Loss: -21.251785278320312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3301/100000], Training Loss: -21.25189208984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3302/100000], Training Loss: -21.251953125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3303/100000], Training Loss: -21.252044677734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3304/100000], Training Loss: -21.252151489257812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3305/100000], Training Loss: -21.252212524414062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3306/100000], Training Loss: -21.252273559570312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3307/100000], Training Loss: -21.252334594726562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3308/100000], Training Loss: -21.25244140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3309/100000], Training Loss: -21.25250244140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3310/100000], Training Loss: -21.2525634765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3311/100000], Training Loss: -21.252685546875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3312/100000], Training Loss: -21.252792358398438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3313/100000], Training Loss: -21.252822875976562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3314/100000], Training Loss: -21.252899169921875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3315/100000], Training Loss: -21.252975463867188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3316/100000], Training Loss: -21.253097534179688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3317/100000], Training Loss: -21.253158569335938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3318/100000], Training Loss: -21.253219604492188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3319/100000], Training Loss: -21.253280639648438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3320/100000], Training Loss: -21.253387451171875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3321/100000], Training Loss: -21.253448486328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3322/100000], Training Loss: -21.253570556640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3323/100000], Training Loss: -21.253631591796875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3324/100000], Training Loss: -21.253677368164062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3325/100000], Training Loss: -21.253738403320312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3326/100000], Training Loss: -21.253829956054688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3327/100000], Training Loss: -21.25390625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3328/100000], Training Loss: -21.253997802734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3329/100000], Training Loss: -21.254074096679688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3330/100000], Training Loss: -21.254119873046875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3331/100000], Training Loss: -21.254257202148438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3332/100000], Training Loss: -21.254364013671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3333/100000], Training Loss: -21.254425048828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3334/100000], Training Loss: -21.2545166015625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3335/100000], Training Loss: -21.254562377929688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3336/100000], Training Loss: -21.254623413085938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3337/100000], Training Loss: -21.25469970703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3338/100000], Training Loss: -21.254776000976562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3339/100000], Training Loss: -21.2548828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3340/100000], Training Loss: -21.254974365234375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3341/100000], Training Loss: -21.2550048828125, Learning Rate: 0.012709329141645007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3342/100000], Training Loss: -21.255142211914062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3343/100000], Training Loss: -21.25518798828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3344/100000], Training Loss: -21.255294799804688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3345/100000], Training Loss: -21.255355834960938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3346/100000], Training Loss: -21.255447387695312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3347/100000], Training Loss: -21.2554931640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3348/100000], Training Loss: -21.255569458007812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3349/100000], Training Loss: -21.255645751953125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3350/100000], Training Loss: -21.255722045898438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3351/100000], Training Loss: -21.25579833984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3352/100000], Training Loss: -21.255889892578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3353/100000], Training Loss: -21.256027221679688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3354/100000], Training Loss: -21.256072998046875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3355/100000], Training Loss: -21.25616455078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3356/100000], Training Loss: -21.25628662109375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3357/100000], Training Loss: -21.25628662109375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3358/100000], Training Loss: -21.256393432617188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3359/100000], Training Loss: -21.2564697265625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3360/100000], Training Loss: -21.256546020507812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3361/100000], Training Loss: -21.256622314453125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3362/100000], Training Loss: -21.2567138671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3363/100000], Training Loss: -21.256805419921875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3364/100000], Training Loss: -21.256881713867188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3365/100000], Training Loss: -21.256942749023438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3366/100000], Training Loss: -21.257064819335938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3367/100000], Training Loss: -21.257110595703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3368/100000], Training Loss: -21.2572021484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3369/100000], Training Loss: -21.257247924804688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3370/100000], Training Loss: -21.25732421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3371/100000], Training Loss: -21.257431030273438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3372/100000], Training Loss: -21.257476806640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3373/100000], Training Loss: -21.257568359375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3374/100000], Training Loss: -21.257659912109375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3375/100000], Training Loss: -21.257720947265625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3376/100000], Training Loss: -21.257781982421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3377/100000], Training Loss: -21.257919311523438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3378/100000], Training Loss: -21.257949829101562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3379/100000], Training Loss: -21.258071899414062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3380/100000], Training Loss: -21.258132934570312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3381/100000], Training Loss: -21.258193969726562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3382/100000], Training Loss: -21.258270263671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3383/100000], Training Loss: -21.258377075195312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3384/100000], Training Loss: -21.258438110351562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3385/100000], Training Loss: -21.258560180664062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3386/100000], Training Loss: -21.258590698242188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3387/100000], Training Loss: -21.258697509765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3388/100000], Training Loss: -21.258758544921875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3389/100000], Training Loss: -21.25885009765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3390/100000], Training Loss: -21.2589111328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3391/100000], Training Loss: -21.258987426757812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3392/100000], Training Loss: -21.259078979492188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3393/100000], Training Loss: -21.2591552734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3394/100000], Training Loss: -21.259246826171875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3395/100000], Training Loss: -21.25933837890625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3396/100000], Training Loss: -21.2593994140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3397/100000], Training Loss: -21.259475708007812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3398/100000], Training Loss: -21.259552001953125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3399/100000], Training Loss: -21.2596435546875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3400/100000], Training Loss: -21.25970458984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3401/100000], Training Loss: -21.259780883789062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3402/100000], Training Loss: -21.259902954101562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3403/100000], Training Loss: -21.259933471679688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3404/100000], Training Loss: -21.260040283203125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3405/100000], Training Loss: -21.260147094726562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3406/100000], Training Loss: -21.260208129882812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3407/100000], Training Loss: -21.26031494140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3408/100000], Training Loss: -21.2603759765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3409/100000], Training Loss: -21.260391235351562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3410/100000], Training Loss: -21.260513305664062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3411/100000], Training Loss: -21.260589599609375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3412/100000], Training Loss: -21.260665893554688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3413/100000], Training Loss: -21.260711669921875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3414/100000], Training Loss: -21.260833740234375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3415/100000], Training Loss: -21.260910034179688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3416/100000], Training Loss: -21.260940551757812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3417/100000], Training Loss: -21.261032104492188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3418/100000], Training Loss: -21.261077880859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3419/100000], Training Loss: -21.261123657226562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3420/100000], Training Loss: -21.261123657226562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3421/100000], Training Loss: -21.261077880859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3422/100000], Training Loss: -21.260848999023438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3423/100000], Training Loss: -21.2603759765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3424/100000], Training Loss: -21.259368896484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3425/100000], Training Loss: -21.257354736328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3426/100000], Training Loss: -21.253326416015625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3427/100000], Training Loss: -21.245285034179688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3428/100000], Training Loss: -21.229263305664062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3429/100000], Training Loss: -21.197006225585938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3430/100000], Training Loss: -21.133255004882812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3431/100000], Training Loss: -21.00872802734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3432/100000], Training Loss: -20.7784423828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3433/100000], Training Loss: -20.390487670898438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3434/100000], Training Loss: -19.86846923828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3435/100000], Training Loss: -19.424530029296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3436/100000], Training Loss: -19.504653930664062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3437/100000], Training Loss: -20.24359130859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3438/100000], Training Loss: -21.034271240234375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3439/100000], Training Loss: -21.134933471679688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3440/100000], Training Loss: -20.659439086914062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3441/100000], Training Loss: -20.424041748046875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3442/100000], Training Loss: -20.781143188476562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3443/100000], Training Loss: -21.214950561523438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3444/100000], Training Loss: -21.125381469726562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3445/100000], Training Loss: -20.786727905273438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3446/100000], Training Loss: -20.82049560546875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3447/100000], Training Loss: -21.139984130859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3448/100000], Training Loss: -21.22576904296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3449/100000], Training Loss: -21.027694702148438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3450/100000], Training Loss: -20.977081298828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3451/100000], Training Loss: -21.167694091796875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3452/100000], Training Loss: -21.243637084960938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3453/100000], Training Loss: -21.111175537109375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3454/100000], Training Loss: -21.06072998046875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3455/100000], Training Loss: -21.18670654296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3456/100000], Training Loss: -21.251602172851562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3457/100000], Training Loss: -21.172439575195312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3458/100000], Training Loss: -21.138565063476562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3459/100000], Training Loss: -21.217010498046875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3460/100000], Training Loss: -21.251617431640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3461/100000], Training Loss: -21.193862915039062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3462/100000], Training Loss: -21.174880981445312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3463/100000], Training Loss: -21.230239868164062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3464/100000], Training Loss: -21.254379272460938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3465/100000], Training Loss: -21.218353271484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3466/100000], Training Loss: -21.207870483398438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3467/100000], Training Loss: -21.243576049804688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3468/100000], Training Loss: -21.25518798828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3469/100000], Training Loss: -21.228958129882812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3470/100000], Training Loss: -21.222610473632812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3471/100000], Training Loss: -21.247283935546875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3472/100000], Training Loss: -21.256057739257812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3473/100000], Training Loss: -21.240280151367188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3474/100000], Training Loss: -21.236846923828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3475/100000], Training Loss: -21.252792358398438, Learning Rate: 0.012709329141645007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3476/100000], Training Loss: -21.258163452148438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3477/100000], Training Loss: -21.2469482421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3478/100000], Training Loss: -21.243438720703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3479/100000], Training Loss: -21.25347900390625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3480/100000], Training Loss: -21.25811767578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3481/100000], Training Loss: -21.251602172851562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3482/100000], Training Loss: -21.249053955078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3483/100000], Training Loss: -21.255722045898438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3484/100000], Training Loss: -21.259765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3485/100000], Training Loss: -21.255935668945312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3486/100000], Training Loss: -21.253173828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3487/100000], Training Loss: -21.256729125976562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3488/100000], Training Loss: -21.259872436523438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3489/100000], Training Loss: -21.25787353515625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3490/100000], Training Loss: -21.255462646484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3491/100000], Training Loss: -21.257247924804688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3492/100000], Training Loss: -21.260009765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3493/100000], Training Loss: -21.25958251953125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3494/100000], Training Loss: -21.25787353515625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3495/100000], Training Loss: -21.258468627929688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3496/100000], Training Loss: -21.260467529296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3497/100000], Training Loss: -21.26092529296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3498/100000], Training Loss: -21.259735107421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3499/100000], Training Loss: -21.259536743164062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3500/100000], Training Loss: -21.26068115234375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3501/100000], Training Loss: -21.261428833007812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3502/100000], Training Loss: -21.260894775390625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3503/100000], Training Loss: -21.260360717773438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3504/100000], Training Loss: -21.260848999023438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3505/100000], Training Loss: -21.261611938476562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3506/100000], Training Loss: -21.261642456054688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3507/100000], Training Loss: -21.26123046875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3508/100000], Training Loss: -21.26123046875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3509/100000], Training Loss: -21.261810302734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3510/100000], Training Loss: -21.262100219726562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3511/100000], Training Loss: -21.261947631835938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3512/100000], Training Loss: -21.261810302734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3513/100000], Training Loss: -21.262008666992188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3514/100000], Training Loss: -21.262298583984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3515/100000], Training Loss: -21.262374877929688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3516/100000], Training Loss: -21.262222290039062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3517/100000], Training Loss: -21.262100219726562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3518/100000], Training Loss: -21.26220703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3519/100000], Training Loss: -21.262237548828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3520/100000], Training Loss: -21.26202392578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3521/100000], Training Loss: -21.26153564453125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3522/100000], Training Loss: -21.261016845703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3523/100000], Training Loss: -21.260299682617188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3524/100000], Training Loss: -21.259048461914062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3525/100000], Training Loss: -21.25701904296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3526/100000], Training Loss: -21.25384521484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3527/100000], Training Loss: -21.2489013671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3528/100000], Training Loss: -21.241363525390625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3529/100000], Training Loss: -21.22906494140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3530/100000], Training Loss: -21.209716796875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3531/100000], Training Loss: -21.177810668945312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3532/100000], Training Loss: -21.127410888671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3533/100000], Training Loss: -21.043289184570312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3534/100000], Training Loss: -20.912673950195312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3535/100000], Training Loss: -20.695510864257812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3536/100000], Training Loss: -20.383895874023438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3537/100000], Training Loss: -19.900161743164062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3538/100000], Training Loss: -19.359756469726562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3539/100000], Training Loss: -18.73028564453125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3540/100000], Training Loss: -18.560684204101562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3541/100000], Training Loss: -18.848541259765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3542/100000], Training Loss: -19.882156372070312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3543/100000], Training Loss: -20.881210327148438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3544/100000], Training Loss: -21.258346557617188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3545/100000], Training Loss: -20.891983032226562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3546/100000], Training Loss: -20.30670166015625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3547/100000], Training Loss: -20.191116333007812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3548/100000], Training Loss: -20.6058349609375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3549/100000], Training Loss: -21.134963989257812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3550/100000], Training Loss: -21.229583740234375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3551/100000], Training Loss: -20.9202880859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3552/100000], Training Loss: -20.682998657226562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3553/100000], Training Loss: -20.803695678710938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3554/100000], Training Loss: -21.128189086914062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3555/100000], Training Loss: -21.2552490234375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3556/100000], Training Loss: -21.101058959960938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3557/100000], Training Loss: -20.937164306640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3558/100000], Training Loss: -20.986572265625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3559/100000], Training Loss: -21.176223754882812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3560/100000], Training Loss: -21.257095336914062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3561/100000], Training Loss: -21.1666259765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3562/100000], Training Loss: -21.06951904296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3563/100000], Training Loss: -21.1025390625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3564/100000], Training Loss: -21.214614868164062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3565/100000], Training Loss: -21.257171630859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3566/100000], Training Loss: -21.19989013671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3567/100000], Training Loss: -21.14508056640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3568/100000], Training Loss: -21.169464111328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3569/100000], Training Loss: -21.235809326171875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3570/100000], Training Loss: -21.25775146484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3571/100000], Training Loss: -21.222152709960938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3572/100000], Training Loss: -21.190750122070312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3573/100000], Training Loss: -21.206619262695312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3574/100000], Training Loss: -21.245925903320312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3575/100000], Training Loss: -21.258804321289062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3576/100000], Training Loss: -21.237945556640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3577/100000], Training Loss: -21.218826293945312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3578/100000], Training Loss: -21.227203369140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3579/100000], Training Loss: -21.250503540039062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3580/100000], Training Loss: -21.25982666015625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3581/100000], Training Loss: -21.248779296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3582/100000], Training Loss: -21.236404418945312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3583/100000], Training Loss: -21.2392578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3584/100000], Training Loss: -21.252700805664062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3585/100000], Training Loss: -21.260345458984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3586/100000], Training Loss: -21.255538940429688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3587/100000], Training Loss: -21.247482299804688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3588/100000], Training Loss: -21.247055053710938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3589/100000], Training Loss: -21.25421142578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3590/100000], Training Loss: -21.260299682617188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3591/100000], Training Loss: -21.2593994140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3592/100000], Training Loss: -21.254562377929688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3593/100000], Training Loss: -21.25244140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3594/100000], Training Loss: -21.2554931640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3595/100000], Training Loss: -21.259918212890625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3596/100000], Training Loss: -21.2611083984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3597/100000], Training Loss: -21.258956909179688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3598/100000], Training Loss: -21.25665283203125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3599/100000], Training Loss: -21.257080078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3600/100000], Training Loss: -21.2596435546875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3601/100000], Training Loss: -21.26153564453125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3602/100000], Training Loss: -21.261276245117188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3603/100000], Training Loss: -21.25982666015625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3604/100000], Training Loss: -21.259078979492188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3605/100000], Training Loss: -21.259872436523438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3606/100000], Training Loss: -21.261367797851562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3607/100000], Training Loss: -21.262161254882812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3608/100000], Training Loss: -21.261825561523438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3609/100000], Training Loss: -21.261032104492188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3610/100000], Training Loss: -21.260787963867188, Learning Rate: 0.012709329141645007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3611/100000], Training Loss: -21.261383056640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3612/100000], Training Loss: -21.262237548828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3613/100000], Training Loss: -21.262619018554688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3614/100000], Training Loss: -21.262481689453125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3615/100000], Training Loss: -21.262069702148438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3616/100000], Training Loss: -21.261993408203125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3617/100000], Training Loss: -21.262313842773438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3618/100000], Training Loss: -21.262802124023438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3619/100000], Training Loss: -21.263092041015625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3620/100000], Training Loss: -21.263107299804688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3621/100000], Training Loss: -21.262939453125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3622/100000], Training Loss: -21.262863159179688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3623/100000], Training Loss: -21.26300048828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3624/100000], Training Loss: -21.263320922851562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3625/100000], Training Loss: -21.2635498046875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3626/100000], Training Loss: -21.263641357421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3627/100000], Training Loss: -21.263702392578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3628/100000], Training Loss: -21.263626098632812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3629/100000], Training Loss: -21.263671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3630/100000], Training Loss: -21.2637939453125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3631/100000], Training Loss: -21.264007568359375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3632/100000], Training Loss: -21.264144897460938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3633/100000], Training Loss: -21.264251708984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3634/100000], Training Loss: -21.264251708984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3635/100000], Training Loss: -21.264251708984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3636/100000], Training Loss: -21.26434326171875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3637/100000], Training Loss: -21.264434814453125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3638/100000], Training Loss: -21.264572143554688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3639/100000], Training Loss: -21.264678955078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3640/100000], Training Loss: -21.264785766601562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3641/100000], Training Loss: -21.264801025390625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3642/100000], Training Loss: -21.264862060546875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3643/100000], Training Loss: -21.264984130859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3644/100000], Training Loss: -21.265090942382812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3645/100000], Training Loss: -21.265167236328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3646/100000], Training Loss: -21.265228271484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3647/100000], Training Loss: -21.265365600585938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3648/100000], Training Loss: -21.26544189453125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3649/100000], Training Loss: -21.265487670898438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3650/100000], Training Loss: -21.265609741210938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3651/100000], Training Loss: -21.265594482421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3652/100000], Training Loss: -21.26568603515625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3653/100000], Training Loss: -21.265762329101562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3654/100000], Training Loss: -21.265899658203125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3655/100000], Training Loss: -21.2659912109375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3656/100000], Training Loss: -21.266067504882812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3657/100000], Training Loss: -21.266143798828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3658/100000], Training Loss: -21.266189575195312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3659/100000], Training Loss: -21.266265869140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3660/100000], Training Loss: -21.266372680664062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3661/100000], Training Loss: -21.26641845703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3662/100000], Training Loss: -21.266494750976562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3663/100000], Training Loss: -21.266586303710938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3664/100000], Training Loss: -21.266677856445312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3665/100000], Training Loss: -21.266693115234375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3666/100000], Training Loss: -21.266876220703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3667/100000], Training Loss: -21.266921997070312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3668/100000], Training Loss: -21.266998291015625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3669/100000], Training Loss: -21.267074584960938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3670/100000], Training Loss: -21.267120361328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3671/100000], Training Loss: -21.267181396484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3672/100000], Training Loss: -21.267303466796875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3673/100000], Training Loss: -21.267349243164062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3674/100000], Training Loss: -21.267410278320312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3675/100000], Training Loss: -21.267562866210938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3676/100000], Training Loss: -21.267593383789062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3677/100000], Training Loss: -21.267715454101562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3678/100000], Training Loss: -21.267730712890625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3679/100000], Training Loss: -21.267837524414062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3680/100000], Training Loss: -21.2679443359375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3681/100000], Training Loss: -21.267974853515625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3682/100000], Training Loss: -21.26806640625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3683/100000], Training Loss: -21.268157958984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3684/100000], Training Loss: -21.268218994140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3685/100000], Training Loss: -21.268356323242188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3686/100000], Training Loss: -21.26837158203125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3687/100000], Training Loss: -21.268447875976562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3688/100000], Training Loss: -21.268478393554688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3689/100000], Training Loss: -21.268585205078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3690/100000], Training Loss: -21.268661499023438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3691/100000], Training Loss: -21.268707275390625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3692/100000], Training Loss: -21.268783569335938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3693/100000], Training Loss: -21.268875122070312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3694/100000], Training Loss: -21.268966674804688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3695/100000], Training Loss: -21.269073486328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3696/100000], Training Loss: -21.269149780273438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3697/100000], Training Loss: -21.26922607421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3698/100000], Training Loss: -21.269302368164062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3699/100000], Training Loss: -21.269363403320312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3700/100000], Training Loss: -21.269439697265625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3701/100000], Training Loss: -21.26953125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3702/100000], Training Loss: -21.269577026367188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3703/100000], Training Loss: -21.269683837890625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3704/100000], Training Loss: -21.26971435546875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3705/100000], Training Loss: -21.26983642578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3706/100000], Training Loss: -21.2698974609375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3707/100000], Training Loss: -21.26995849609375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3708/100000], Training Loss: -21.269989013671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3709/100000], Training Loss: -21.270050048828125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3710/100000], Training Loss: -21.270111083984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3711/100000], Training Loss: -21.270172119140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3712/100000], Training Loss: -21.270172119140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3713/100000], Training Loss: -21.270111083984375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3714/100000], Training Loss: -21.269973754882812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3715/100000], Training Loss: -21.26971435546875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3716/100000], Training Loss: -21.26922607421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3717/100000], Training Loss: -21.268280029296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3718/100000], Training Loss: -21.266647338867188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3719/100000], Training Loss: -21.263809204101562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3720/100000], Training Loss: -21.258560180664062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3721/100000], Training Loss: -21.249374389648438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3722/100000], Training Loss: -21.232711791992188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3723/100000], Training Loss: -21.20220947265625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3724/100000], Training Loss: -21.14727783203125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3725/100000], Training Loss: -21.044921875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3726/100000], Training Loss: -20.862823486328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3727/100000], Training Loss: -20.523513793945312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3728/100000], Training Loss: -19.95928955078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3729/100000], Training Loss: -18.978866577148438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3730/100000], Training Loss: -17.728286743164062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3731/100000], Training Loss: -16.207733154296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3732/100000], Training Loss: -15.97125244140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3733/100000], Training Loss: -17.201873779296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3734/100000], Training Loss: -19.865325927734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3735/100000], Training Loss: -21.237960815429688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3736/100000], Training Loss: -20.372528076171875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3737/100000], Training Loss: -19.033905029296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3738/100000], Training Loss: -19.208038330078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3739/100000], Training Loss: -20.717514038085938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3740/100000], Training Loss: -21.211318969726562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3741/100000], Training Loss: -20.313461303710938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3742/100000], Training Loss: -19.904205322265625, Learning Rate: 0.012709329141645007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3743/100000], Training Loss: -20.672515869140625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3744/100000], Training Loss: -21.258316040039062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3745/100000], Training Loss: -20.785186767578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3746/100000], Training Loss: -20.379776000976562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3747/100000], Training Loss: -20.86102294921875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3748/100000], Training Loss: -21.25201416015625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3749/100000], Training Loss: -20.934600830078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3750/100000], Training Loss: -20.712936401367188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3751/100000], Training Loss: -21.040206909179688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3752/100000], Training Loss: -21.249679565429688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3753/100000], Training Loss: -21.007308959960938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3754/100000], Training Loss: -20.905838012695312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3755/100000], Training Loss: -21.159194946289062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3756/100000], Training Loss: -21.241165161132812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3757/100000], Training Loss: -21.056930541992188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3758/100000], Training Loss: -21.046783447265625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3759/100000], Training Loss: -21.222335815429688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3760/100000], Training Loss: -21.23114013671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3761/100000], Training Loss: -21.1085205078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3762/100000], Training Loss: -21.13629150390625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3763/100000], Training Loss: -21.248947143554688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3764/100000], Training Loss: -21.224594116210938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3765/100000], Training Loss: -21.150588989257812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3766/100000], Training Loss: -21.193817138671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3767/100000], Training Loss: -21.258071899414062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3768/100000], Training Loss: -21.224624633789062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3769/100000], Training Loss: -21.18603515625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3770/100000], Training Loss: -21.22589111328125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3771/100000], Training Loss: -21.259979248046875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3772/100000], Training Loss: -21.229705810546875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3773/100000], Training Loss: -21.2109375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3774/100000], Training Loss: -21.24298095703125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3775/100000], Training Loss: -21.259780883789062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3776/100000], Training Loss: -21.236602783203125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3777/100000], Training Loss: -21.228836059570312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3778/100000], Training Loss: -21.251556396484375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3779/100000], Training Loss: -21.259353637695312, Learning Rate: 0.012709329141645007\n",
      "Epoch [3780/100000], Training Loss: -21.24322509765625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3781/100000], Training Loss: -21.240509033203125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3782/100000], Training Loss: -21.256027221679688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3783/100000], Training Loss: -21.259475708007812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3784/100000], Training Loss: -21.248641967773438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3785/100000], Training Loss: -21.248077392578125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3786/100000], Training Loss: -21.258514404296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3787/100000], Training Loss: -21.260162353515625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3788/100000], Training Loss: -21.252960205078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3789/100000], Training Loss: -21.252853393554688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3790/100000], Training Loss: -21.259780883789062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3791/100000], Training Loss: -21.26092529296875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3792/100000], Training Loss: -21.256317138671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3793/100000], Training Loss: -21.256011962890625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3794/100000], Training Loss: -21.260589599609375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3795/100000], Training Loss: -21.261627197265625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3796/100000], Training Loss: -21.258682250976562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3797/100000], Training Loss: -21.258270263671875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3798/100000], Training Loss: -21.261245727539062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3799/100000], Training Loss: -21.262252807617188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3800/100000], Training Loss: -21.260421752929688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3801/100000], Training Loss: -21.259857177734375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3802/100000], Training Loss: -21.261734008789062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3803/100000], Training Loss: -21.262725830078125, Learning Rate: 0.012709329141645007\n",
      "Epoch [3804/100000], Training Loss: -21.261688232421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3805/100000], Training Loss: -21.261077880859375, Learning Rate: 0.012709329141645007\n",
      "Epoch [3806/100000], Training Loss: -21.262222290039062, Learning Rate: 0.012709329141645007\n",
      "Epoch [3807/100000], Training Loss: -21.263168334960938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3808/100000], Training Loss: -21.262664794921875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3809/100000], Training Loss: -21.262069702148438, Learning Rate: 0.012709329141645007\n",
      "Epoch [3810/100000], Training Loss: -21.262664794921875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3811/100000], Training Loss: -21.263473510742188, Learning Rate: 0.012709329141645007\n",
      "Epoch [3812/100000], Training Loss: -21.263351440429688, Learning Rate: 0.012709329141645007\n",
      "Epoch [3813/100000], Training Loss: -21.262924194335938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3814/100000], Training Loss: -21.263168334960938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3815/100000], Training Loss: -21.263809204101562, Learning Rate: 0.012709329141645007\n",
      "Epoch [3816/100000], Training Loss: -21.263900756835938, Learning Rate: 0.012709329141645007\n",
      "Epoch [3817/100000], Training Loss: -21.263641357421875, Learning Rate: 0.012709329141645007\n",
      "Epoch [3818/100000], Training Loss: -21.263626098632812, Learning Rate: 0.012709329141645007\n",
      "Epoch [3819/100000], Training Loss: -21.264068603515625, Learning Rate: 0.012709329141645007\n",
      "Epoch [3820/100000], Training Loss: -21.2642822265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3821/100000], Training Loss: -21.264205932617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [3822/100000], Training Loss: -21.264251708984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3823/100000], Training Loss: -21.264511108398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3824/100000], Training Loss: -21.264495849609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3825/100000], Training Loss: -21.2645263671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3826/100000], Training Loss: -21.2647705078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3827/100000], Training Loss: -21.264816284179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [3828/100000], Training Loss: -21.2647705078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3829/100000], Training Loss: -21.26495361328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3830/100000], Training Loss: -21.26507568359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3831/100000], Training Loss: -21.2650146484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3832/100000], Training Loss: -21.265151977539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [3833/100000], Training Loss: -21.265289306640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3834/100000], Training Loss: -21.26531982421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3835/100000], Training Loss: -21.265350341796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3836/100000], Training Loss: -21.265533447265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3837/100000], Training Loss: -21.26556396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3838/100000], Training Loss: -21.265640258789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [3839/100000], Training Loss: -21.265731811523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3840/100000], Training Loss: -21.265853881835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [3841/100000], Training Loss: -21.265869140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3842/100000], Training Loss: -21.265945434570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3843/100000], Training Loss: -21.266006469726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [3844/100000], Training Loss: -21.266098022460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [3845/100000], Training Loss: -21.266159057617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [3846/100000], Training Loss: -21.266265869140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3847/100000], Training Loss: -21.266326904296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3848/100000], Training Loss: -21.266372680664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [3849/100000], Training Loss: -21.2664794921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3850/100000], Training Loss: -21.266571044921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3851/100000], Training Loss: -21.266616821289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [3852/100000], Training Loss: -21.266708374023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3853/100000], Training Loss: -21.266815185546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3854/100000], Training Loss: -21.266815185546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3855/100000], Training Loss: -21.26690673828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3856/100000], Training Loss: -21.266998291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3857/100000], Training Loss: -21.267044067382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [3858/100000], Training Loss: -21.26715087890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3859/100000], Training Loss: -21.267227172851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [3860/100000], Training Loss: -21.26727294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3861/100000], Training Loss: -21.267349243164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [3862/100000], Training Loss: -21.267410278320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3863/100000], Training Loss: -21.267440795898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3864/100000], Training Loss: -21.267562866210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [3865/100000], Training Loss: -21.267654418945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3866/100000], Training Loss: -21.2677001953125, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3867/100000], Training Loss: -21.26776123046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3868/100000], Training Loss: -21.267837524414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [3869/100000], Training Loss: -21.26788330078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3870/100000], Training Loss: -21.2679443359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3871/100000], Training Loss: -21.268035888671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3872/100000], Training Loss: -21.26812744140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3873/100000], Training Loss: -21.268173217773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3874/100000], Training Loss: -21.268295288085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [3875/100000], Training Loss: -21.26837158203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3876/100000], Training Loss: -21.268417358398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3877/100000], Training Loss: -21.26849365234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3878/100000], Training Loss: -21.268539428710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [3879/100000], Training Loss: -21.26861572265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3880/100000], Training Loss: -21.268692016601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [3881/100000], Training Loss: -21.26873779296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3882/100000], Training Loss: -21.268814086914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [3883/100000], Training Loss: -21.2689208984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3884/100000], Training Loss: -21.26898193359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3885/100000], Training Loss: -21.26904296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3886/100000], Training Loss: -21.269088745117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [3887/100000], Training Loss: -21.2691650390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3888/100000], Training Loss: -21.26922607421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3889/100000], Training Loss: -21.269287109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3890/100000], Training Loss: -21.269363403320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3891/100000], Training Loss: -21.26947021484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3892/100000], Training Loss: -21.269500732421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3893/100000], Training Loss: -21.269622802734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3894/100000], Training Loss: -21.269622802734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3895/100000], Training Loss: -21.26971435546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3896/100000], Training Loss: -21.269775390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3897/100000], Training Loss: -21.269805908203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3898/100000], Training Loss: -21.269882202148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3899/100000], Training Loss: -21.26995849609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3900/100000], Training Loss: -21.270034790039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [3901/100000], Training Loss: -21.270126342773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3902/100000], Training Loss: -21.270172119140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3903/100000], Training Loss: -21.270248413085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [3904/100000], Training Loss: -21.270370483398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3905/100000], Training Loss: -21.2703857421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3906/100000], Training Loss: -21.270477294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3907/100000], Training Loss: -21.270492553710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [3908/100000], Training Loss: -21.270614624023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3909/100000], Training Loss: -21.270614624023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3910/100000], Training Loss: -21.27069091796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3911/100000], Training Loss: -21.270782470703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3912/100000], Training Loss: -21.270843505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3913/100000], Training Loss: -21.2708740234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3914/100000], Training Loss: -21.27093505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3915/100000], Training Loss: -21.271041870117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [3916/100000], Training Loss: -21.271072387695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3917/100000], Training Loss: -21.271209716796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3918/100000], Training Loss: -21.271209716796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3919/100000], Training Loss: -21.271316528320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3920/100000], Training Loss: -21.271408081054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [3921/100000], Training Loss: -21.27142333984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3922/100000], Training Loss: -21.271484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3923/100000], Training Loss: -21.271530151367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [3924/100000], Training Loss: -21.271591186523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3925/100000], Training Loss: -21.271697998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3926/100000], Training Loss: -21.271774291992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [3927/100000], Training Loss: -21.27178955078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3928/100000], Training Loss: -21.27191162109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3929/100000], Training Loss: -21.271957397460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [3930/100000], Training Loss: -21.27203369140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3931/100000], Training Loss: -21.272109985351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [3932/100000], Training Loss: -21.272140502929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [3933/100000], Training Loss: -21.272247314453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3934/100000], Training Loss: -21.272293090820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3935/100000], Training Loss: -21.2723388671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3936/100000], Training Loss: -21.272430419921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3937/100000], Training Loss: -21.272445678710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [3938/100000], Training Loss: -21.272537231445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3939/100000], Training Loss: -21.272567749023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3940/100000], Training Loss: -21.272674560546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3941/100000], Training Loss: -21.272735595703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3942/100000], Training Loss: -21.272781372070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3943/100000], Training Loss: -21.27288818359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3944/100000], Training Loss: -21.272918701171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3945/100000], Training Loss: -21.272964477539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [3946/100000], Training Loss: -21.273056030273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3947/100000], Training Loss: -21.273117065429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [3948/100000], Training Loss: -21.273147583007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [3949/100000], Training Loss: -21.27325439453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3950/100000], Training Loss: -21.273345947265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3951/100000], Training Loss: -21.273345947265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3952/100000], Training Loss: -21.2734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3953/100000], Training Loss: -21.273513793945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3954/100000], Training Loss: -21.273590087890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3955/100000], Training Loss: -21.273666381835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [3956/100000], Training Loss: -21.273727416992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [3957/100000], Training Loss: -21.273773193359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3958/100000], Training Loss: -21.273880004882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [3959/100000], Training Loss: -21.273910522460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [3960/100000], Training Loss: -21.273971557617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [3961/100000], Training Loss: -21.2740478515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3962/100000], Training Loss: -21.274078369140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3963/100000], Training Loss: -21.274139404296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3964/100000], Training Loss: -21.274215698242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [3965/100000], Training Loss: -21.274246215820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3966/100000], Training Loss: -21.27435302734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3967/100000], Training Loss: -21.274429321289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [3968/100000], Training Loss: -21.274520874023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3969/100000], Training Loss: -21.2745361328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3970/100000], Training Loss: -21.274581909179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [3971/100000], Training Loss: -21.274658203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3972/100000], Training Loss: -21.274734497070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3973/100000], Training Loss: -21.274795532226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [3974/100000], Training Loss: -21.274871826171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3975/100000], Training Loss: -21.274932861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3976/100000], Training Loss: -21.274993896484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3977/100000], Training Loss: -21.275054931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3978/100000], Training Loss: -21.275100708007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [3979/100000], Training Loss: -21.275177001953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3980/100000], Training Loss: -21.275253295898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3981/100000], Training Loss: -21.275360107421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3982/100000], Training Loss: -21.27532958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3983/100000], Training Loss: -21.275405883789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [3984/100000], Training Loss: -21.275466918945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3985/100000], Training Loss: -21.27557373046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3986/100000], Training Loss: -21.275634765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3987/100000], Training Loss: -21.27569580078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3988/100000], Training Loss: -21.2757568359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3989/100000], Training Loss: -21.27581787109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [3990/100000], Training Loss: -21.275909423828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [3991/100000], Training Loss: -21.27593994140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3992/100000], Training Loss: -21.276031494140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [3993/100000], Training Loss: -21.27606201171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3994/100000], Training Loss: -21.276077270507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [3995/100000], Training Loss: -21.276199340820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [3996/100000], Training Loss: -21.276229858398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [3997/100000], Training Loss: -21.276336669921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3998/100000], Training Loss: -21.2763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [3999/100000], Training Loss: -21.276443481445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4000/100000], Training Loss: -21.276504516601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4001/100000], Training Loss: -21.276596069335938, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4002/100000], Training Loss: -21.276596069335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4003/100000], Training Loss: -21.276702880859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4004/100000], Training Loss: -21.276748657226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4005/100000], Training Loss: -21.276824951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4006/100000], Training Loss: -21.276885986328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4007/100000], Training Loss: -21.276931762695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4008/100000], Training Loss: -21.2769775390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4009/100000], Training Loss: -21.277053833007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4010/100000], Training Loss: -21.277130126953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4011/100000], Training Loss: -21.2772216796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4012/100000], Training Loss: -21.277252197265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4013/100000], Training Loss: -21.27734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4014/100000], Training Loss: -21.277435302734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4015/100000], Training Loss: -21.2774658203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4016/100000], Training Loss: -21.277511596679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4017/100000], Training Loss: -21.277618408203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4018/100000], Training Loss: -21.27764892578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4019/100000], Training Loss: -21.277740478515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4020/100000], Training Loss: -21.277740478515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4021/100000], Training Loss: -21.277847290039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4022/100000], Training Loss: -21.27789306640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4023/100000], Training Loss: -21.277999877929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4024/100000], Training Loss: -21.278060913085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4025/100000], Training Loss: -21.278091430664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4026/100000], Training Loss: -21.27813720703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4027/100000], Training Loss: -21.278244018554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4028/100000], Training Loss: -21.27825927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4029/100000], Training Loss: -21.278335571289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4030/100000], Training Loss: -21.278396606445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4031/100000], Training Loss: -21.278457641601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4032/100000], Training Loss: -21.278488159179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4033/100000], Training Loss: -21.278594970703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4034/100000], Training Loss: -21.278671264648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4035/100000], Training Loss: -21.278717041015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4036/100000], Training Loss: -21.278778076171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4037/100000], Training Loss: -21.278854370117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4038/100000], Training Loss: -21.278915405273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4039/100000], Training Loss: -21.279006958007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4040/100000], Training Loss: -21.279037475585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4041/100000], Training Loss: -21.27911376953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4042/100000], Training Loss: -21.279144287109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4043/100000], Training Loss: -21.279220581054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4044/100000], Training Loss: -21.279281616210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4045/100000], Training Loss: -21.279312133789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4046/100000], Training Loss: -21.2794189453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4047/100000], Training Loss: -21.279449462890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4048/100000], Training Loss: -21.27947998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4049/100000], Training Loss: -21.27947998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4050/100000], Training Loss: -21.279403686523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4051/100000], Training Loss: -21.279251098632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4052/100000], Training Loss: -21.278915405273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4053/100000], Training Loss: -21.278121948242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4054/100000], Training Loss: -21.276702880859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4055/100000], Training Loss: -21.273757934570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4056/100000], Training Loss: -21.26806640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4057/100000], Training Loss: -21.256744384765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4058/100000], Training Loss: -21.23443603515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4059/100000], Training Loss: -21.18994140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4060/100000], Training Loss: -21.102920532226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4061/100000], Training Loss: -20.930877685546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4062/100000], Training Loss: -20.614776611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4063/100000], Training Loss: -20.047500610351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4064/100000], Training Loss: -19.260177612304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4065/100000], Training Loss: -18.388763427734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4066/100000], Training Loss: -18.341827392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4067/100000], Training Loss: -19.255035400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4068/100000], Training Loss: -20.588897705078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4069/100000], Training Loss: -20.906356811523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4070/100000], Training Loss: -20.374359130859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4071/100000], Training Loss: -20.181137084960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4072/100000], Training Loss: -20.65924072265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4073/100000], Training Loss: -20.934600830078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4074/100000], Training Loss: -20.587081909179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4075/100000], Training Loss: -20.4847412109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4076/100000], Training Loss: -20.978378295898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4077/100000], Training Loss: -21.247604370117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4078/100000], Training Loss: -20.891433715820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4079/100000], Training Loss: -20.669281005859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4080/100000], Training Loss: -20.99761962890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4081/100000], Training Loss: -21.232025146484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4082/100000], Training Loss: -21.066513061523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4083/100000], Training Loss: -20.980178833007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4084/100000], Training Loss: -21.1317138671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4085/100000], Training Loss: -21.158233642578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4086/100000], Training Loss: -21.056838989257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4087/100000], Training Loss: -21.13232421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4088/100000], Training Loss: -21.259185791015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4089/100000], Training Loss: -21.18701171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4090/100000], Training Loss: -21.093551635742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4091/100000], Training Loss: -21.1773681640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4092/100000], Training Loss: -21.26507568359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4093/100000], Training Loss: -21.214752197265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4094/100000], Training Loss: -21.179473876953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4095/100000], Training Loss: -21.229080200195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4096/100000], Training Loss: -21.23931884765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4097/100000], Training Loss: -21.207534790039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4098/100000], Training Loss: -21.231292724609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4099/100000], Training Loss: -21.2703857421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4100/100000], Training Loss: -21.245758056640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4101/100000], Training Loss: -21.216827392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4102/100000], Training Loss: -21.245147705078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4103/100000], Training Loss: -21.271408081054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4104/100000], Training Loss: -21.2554931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4105/100000], Training Loss: -21.246002197265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4106/100000], Training Loss: -21.261306762695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4107/100000], Training Loss: -21.26348876953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4108/100000], Training Loss: -21.252731323242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4109/100000], Training Loss: -21.260604858398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4110/100000], Training Loss: -21.27447509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4111/100000], Training Loss: -21.26806640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4112/100000], Training Loss: -21.257553100585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4113/100000], Training Loss: -21.264266967773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4114/100000], Training Loss: -21.273513793945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4115/100000], Training Loss: -21.27032470703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4116/100000], Training Loss: -21.267074584960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4117/100000], Training Loss: -21.271514892578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4118/100000], Training Loss: -21.272979736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4119/100000], Training Loss: -21.268905639648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4120/100000], Training Loss: -21.269607543945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4121/100000], Training Loss: -21.275161743164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4122/100000], Training Loss: -21.275726318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4123/100000], Training Loss: -21.271987915039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4124/100000], Training Loss: -21.2718505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4125/100000], Training Loss: -21.27484130859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4126/100000], Training Loss: -21.27520751953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4127/100000], Training Loss: -21.273971557617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4128/100000], Training Loss: -21.275039672851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4129/100000], Training Loss: -21.27655029296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4130/100000], Training Loss: -21.275665283203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4131/100000], Training Loss: -21.274429321289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4132/100000], Training Loss: -21.275497436523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4133/100000], Training Loss: -21.277008056640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4134/100000], Training Loss: -21.276824951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4135/100000], Training Loss: -21.276123046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4136/100000], Training Loss: -21.27655029296875, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4137/100000], Training Loss: -21.27716064453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4138/100000], Training Loss: -21.276809692382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4139/100000], Training Loss: -21.276657104492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4140/100000], Training Loss: -21.277297973632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4141/100000], Training Loss: -21.277923583984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4142/100000], Training Loss: -21.277679443359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4143/100000], Training Loss: -21.277374267578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4144/100000], Training Loss: -21.277603149414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4145/100000], Training Loss: -21.277969360351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4146/100000], Training Loss: -21.277984619140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4147/100000], Training Loss: -21.277908325195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4148/100000], Training Loss: -21.278213500976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4149/100000], Training Loss: -21.278427124023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4150/100000], Training Loss: -21.278457641601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4151/100000], Training Loss: -21.278366088867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4152/100000], Training Loss: -21.278488159179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4153/100000], Training Loss: -21.278717041015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4154/100000], Training Loss: -21.278839111328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4155/100000], Training Loss: -21.278823852539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4156/100000], Training Loss: -21.278900146484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4157/100000], Training Loss: -21.279083251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4158/100000], Training Loss: -21.279220581054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4159/100000], Training Loss: -21.279129028320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4160/100000], Training Loss: -21.279205322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4161/100000], Training Loss: -21.279327392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4162/100000], Training Loss: -21.279449462890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4163/100000], Training Loss: -21.279525756835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4164/100000], Training Loss: -21.279571533203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4165/100000], Training Loss: -21.2796630859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4166/100000], Training Loss: -21.279800415039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4167/100000], Training Loss: -21.279815673828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4168/100000], Training Loss: -21.279861450195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4169/100000], Training Loss: -21.279983520507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4170/100000], Training Loss: -21.280105590820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4171/100000], Training Loss: -21.280166625976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4172/100000], Training Loss: -21.28021240234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4173/100000], Training Loss: -21.280288696289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4174/100000], Training Loss: -21.280364990234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4175/100000], Training Loss: -21.280441284179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4176/100000], Training Loss: -21.280517578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4177/100000], Training Loss: -21.280563354492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4178/100000], Training Loss: -21.280670166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4179/100000], Training Loss: -21.280746459960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4180/100000], Training Loss: -21.280776977539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4181/100000], Training Loss: -21.280853271484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4182/100000], Training Loss: -21.28094482421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4183/100000], Training Loss: -21.281021118164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4184/100000], Training Loss: -21.281082153320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4185/100000], Training Loss: -21.2811279296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4186/100000], Training Loss: -21.281219482421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4187/100000], Training Loss: -21.281280517578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4188/100000], Training Loss: -21.281341552734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4189/100000], Training Loss: -21.281463623046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4190/100000], Training Loss: -21.281524658203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4191/100000], Training Loss: -21.2816162109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4192/100000], Training Loss: -21.281631469726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4193/100000], Training Loss: -21.281707763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4194/100000], Training Loss: -21.28179931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4195/100000], Training Loss: -21.281845092773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4196/100000], Training Loss: -21.281890869140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4197/100000], Training Loss: -21.281997680664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4198/100000], Training Loss: -21.282073974609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4199/100000], Training Loss: -21.2821044921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4200/100000], Training Loss: -21.282241821289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4201/100000], Training Loss: -21.28228759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4202/100000], Training Loss: -21.282363891601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4203/100000], Training Loss: -21.282455444335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4204/100000], Training Loss: -21.282485961914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4205/100000], Training Loss: -21.282577514648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4206/100000], Training Loss: -21.282608032226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4207/100000], Training Loss: -21.282669067382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4208/100000], Training Loss: -21.282745361328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4209/100000], Training Loss: -21.282821655273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4210/100000], Training Loss: -21.28289794921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4211/100000], Training Loss: -21.282974243164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4212/100000], Training Loss: -21.28302001953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4213/100000], Training Loss: -21.283096313476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4214/100000], Training Loss: -21.283157348632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4215/100000], Training Loss: -21.283218383789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4216/100000], Training Loss: -21.283294677734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4217/100000], Training Loss: -21.283340454101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4218/100000], Training Loss: -21.28338623046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4219/100000], Training Loss: -21.283447265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4220/100000], Training Loss: -21.283477783203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4221/100000], Training Loss: -21.2835693359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4222/100000], Training Loss: -21.283615112304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4223/100000], Training Loss: -21.28369140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4224/100000], Training Loss: -21.28369140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4225/100000], Training Loss: -21.28369140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4226/100000], Training Loss: -21.28369140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4227/100000], Training Loss: -21.283599853515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4228/100000], Training Loss: -21.28338623046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4229/100000], Training Loss: -21.283004760742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4230/100000], Training Loss: -21.282440185546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4231/100000], Training Loss: -21.281463623046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4232/100000], Training Loss: -21.279769897460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4233/100000], Training Loss: -21.276901245117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4234/100000], Training Loss: -21.271942138671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4235/100000], Training Loss: -21.263473510742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4236/100000], Training Loss: -21.248825073242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4237/100000], Training Loss: -21.223678588867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4238/100000], Training Loss: -21.179214477539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4239/100000], Training Loss: -21.103179931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4240/100000], Training Loss: -20.967788696289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4241/100000], Training Loss: -20.744171142578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4242/100000], Training Loss: -20.357742309570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4243/100000], Training Loss: -19.799331665039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4244/100000], Training Loss: -18.977554321289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4245/100000], Training Loss: -18.26605224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4246/100000], Training Loss: -17.852935791015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4247/100000], Training Loss: -18.621261596679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4248/100000], Training Loss: -20.011825561523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4249/100000], Training Loss: -21.143783569335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4250/100000], Training Loss: -21.107513427734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4251/100000], Training Loss: -20.280960083007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4252/100000], Training Loss: -19.822555541992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4253/100000], Training Loss: -20.241928100585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4254/100000], Training Loss: -21.050628662109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4255/100000], Training Loss: -21.24139404296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4256/100000], Training Loss: -20.775482177734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4257/100000], Training Loss: -20.468536376953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4258/100000], Training Loss: -20.74761962890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4259/100000], Training Loss: -21.201553344726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4260/100000], Training Loss: -21.208831787109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4261/100000], Training Loss: -20.894363403320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4262/100000], Training Loss: -20.819793701171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4263/100000], Training Loss: -21.078689575195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4264/100000], Training Loss: -21.271148681640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4265/100000], Training Loss: -21.155502319335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4266/100000], Training Loss: -20.990509033203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4267/100000], Training Loss: -21.062332153320312, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4268/100000], Training Loss: -21.237152099609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4269/100000], Training Loss: -21.254364013671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4270/100000], Training Loss: -21.137527465820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4271/100000], Training Loss: -21.10577392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4272/100000], Training Loss: -21.208465576171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4273/100000], Training Loss: -21.274856567382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4274/100000], Training Loss: -21.22357177734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4275/100000], Training Loss: -21.16754150390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4276/100000], Training Loss: -21.202682495117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4277/100000], Training Loss: -21.267929077148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4278/100000], Training Loss: -21.265289306640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4279/100000], Training Loss: -21.218902587890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4280/100000], Training Loss: -21.214752197265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4281/100000], Training Loss: -21.256805419921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4282/100000], Training Loss: -21.278472900390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4283/100000], Training Loss: -21.255340576171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4284/100000], Training Loss: -21.234710693359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4285/100000], Training Loss: -21.251327514648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4286/100000], Training Loss: -21.27679443359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4287/100000], Training Loss: -21.274810791015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4288/100000], Training Loss: -21.256195068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4289/100000], Training Loss: -21.25390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4290/100000], Training Loss: -21.27081298828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4291/100000], Training Loss: -21.280670166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4292/100000], Training Loss: -21.27227783203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4293/100000], Training Loss: -21.262908935546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4294/100000], Training Loss: -21.267608642578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4295/100000], Training Loss: -21.278533935546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4296/100000], Training Loss: -21.280136108398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4297/100000], Training Loss: -21.272979736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4298/100000], Training Loss: -21.2698974609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4299/100000], Training Loss: -21.2752685546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4300/100000], Training Loss: -21.280899047851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4301/100000], Training Loss: -21.279693603515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4302/100000], Training Loss: -21.27532958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4303/100000], Training Loss: -21.275009155273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4304/100000], Training Loss: -21.278976440429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4305/100000], Training Loss: -21.281478881835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4306/100000], Training Loss: -21.27996826171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4307/100000], Training Loss: -21.277618408203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4308/100000], Training Loss: -21.278167724609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4309/100000], Training Loss: -21.280670166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4310/100000], Training Loss: -21.281784057617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4311/100000], Training Loss: -21.28057861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4312/100000], Training Loss: -21.279434204101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4313/100000], Training Loss: -21.280044555664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4314/100000], Training Loss: -21.28155517578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4315/100000], Training Loss: -21.282028198242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4316/100000], Training Loss: -21.28125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4317/100000], Training Loss: -21.280715942382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4318/100000], Training Loss: -21.281173706054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4319/100000], Training Loss: -21.282089233398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4320/100000], Training Loss: -21.282333374023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4321/100000], Training Loss: -21.281951904296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4322/100000], Training Loss: -21.281631469726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4323/100000], Training Loss: -21.281906127929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4324/100000], Training Loss: -21.282501220703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4325/100000], Training Loss: -21.282684326171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4326/100000], Training Loss: -21.28253173828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4327/100000], Training Loss: -21.28228759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4328/100000], Training Loss: -21.282424926757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4329/100000], Training Loss: -21.28277587890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4330/100000], Training Loss: -21.282958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4331/100000], Training Loss: -21.282943725585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4332/100000], Training Loss: -21.282852172851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4333/100000], Training Loss: -21.282821655273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4334/100000], Training Loss: -21.283065795898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4335/100000], Training Loss: -21.28326416015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4336/100000], Training Loss: -21.2833251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4337/100000], Training Loss: -21.283279418945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4338/100000], Training Loss: -21.28326416015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4339/100000], Training Loss: -21.28338623046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4340/100000], Training Loss: -21.283554077148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4341/100000], Training Loss: -21.28363037109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4342/100000], Training Loss: -21.283660888671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4343/100000], Training Loss: -21.283676147460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4344/100000], Training Loss: -21.283721923828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4345/100000], Training Loss: -21.283843994140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4346/100000], Training Loss: -21.283920288085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4347/100000], Training Loss: -21.284011840820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4348/100000], Training Loss: -21.284042358398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4349/100000], Training Loss: -21.284072875976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4350/100000], Training Loss: -21.28411865234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4351/100000], Training Loss: -21.284255981445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4352/100000], Training Loss: -21.28424072265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4353/100000], Training Loss: -21.284332275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4354/100000], Training Loss: -21.284408569335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4355/100000], Training Loss: -21.284469604492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4356/100000], Training Loss: -21.284500122070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4357/100000], Training Loss: -21.284561157226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4358/100000], Training Loss: -21.284683227539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4359/100000], Training Loss: -21.284698486328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4360/100000], Training Loss: -21.284774780273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4361/100000], Training Loss: -21.284774780273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4362/100000], Training Loss: -21.284866333007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4363/100000], Training Loss: -21.284942626953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4364/100000], Training Loss: -21.284988403320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4365/100000], Training Loss: -21.2850341796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4366/100000], Training Loss: -21.285079956054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4367/100000], Training Loss: -21.28515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4368/100000], Training Loss: -21.285263061523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4369/100000], Training Loss: -21.285324096679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4370/100000], Training Loss: -21.28533935546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4371/100000], Training Loss: -21.285369873046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4372/100000], Training Loss: -21.285446166992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4373/100000], Training Loss: -21.2855224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4374/100000], Training Loss: -21.28558349609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4375/100000], Training Loss: -21.285659790039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4376/100000], Training Loss: -21.285629272460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4377/100000], Training Loss: -21.2857666015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4378/100000], Training Loss: -21.285781860351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4379/100000], Training Loss: -21.285873413085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4380/100000], Training Loss: -21.285888671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4381/100000], Training Loss: -21.28594970703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4382/100000], Training Loss: -21.2860107421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4383/100000], Training Loss: -21.286102294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4384/100000], Training Loss: -21.286148071289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4385/100000], Training Loss: -21.2862548828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4386/100000], Training Loss: -21.2862548828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4387/100000], Training Loss: -21.286331176757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4388/100000], Training Loss: -21.286361694335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4389/100000], Training Loss: -21.286422729492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4390/100000], Training Loss: -21.286468505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4391/100000], Training Loss: -21.286544799804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4392/100000], Training Loss: -21.286590576171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4393/100000], Training Loss: -21.286651611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4394/100000], Training Loss: -21.286712646484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4395/100000], Training Loss: -21.286758422851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4396/100000], Training Loss: -21.286819458007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4397/100000], Training Loss: -21.286895751953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4398/100000], Training Loss: -21.286941528320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4399/100000], Training Loss: -21.287017822265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4400/100000], Training Loss: -21.28704833984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4401/100000], Training Loss: -21.287078857421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4402/100000], Training Loss: -21.287124633789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4403/100000], Training Loss: -21.287246704101562, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4404/100000], Training Loss: -21.287246704101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4405/100000], Training Loss: -21.287322998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4406/100000], Training Loss: -21.28741455078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4407/100000], Training Loss: -21.287506103515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4408/100000], Training Loss: -21.2874755859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4409/100000], Training Loss: -21.287521362304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4410/100000], Training Loss: -21.287643432617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4411/100000], Training Loss: -21.287673950195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4412/100000], Training Loss: -21.2877197265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4413/100000], Training Loss: -21.287811279296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4414/100000], Training Loss: -21.287841796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4415/100000], Training Loss: -21.287887573242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4416/100000], Training Loss: -21.287979125976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4417/100000], Training Loss: -21.288040161132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4418/100000], Training Loss: -21.288009643554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4419/100000], Training Loss: -21.288116455078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4420/100000], Training Loss: -21.288177490234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4421/100000], Training Loss: -21.2882080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4422/100000], Training Loss: -21.288284301757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4423/100000], Training Loss: -21.288330078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4424/100000], Training Loss: -21.288421630859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4425/100000], Training Loss: -21.2884521484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4426/100000], Training Loss: -21.288528442382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4427/100000], Training Loss: -21.288589477539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4428/100000], Training Loss: -21.28863525390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4429/100000], Training Loss: -21.288711547851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4430/100000], Training Loss: -21.28875732421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4431/100000], Training Loss: -21.288803100585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4432/100000], Training Loss: -21.288848876953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4433/100000], Training Loss: -21.288894653320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4434/100000], Training Loss: -21.288986206054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4435/100000], Training Loss: -21.2890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4436/100000], Training Loss: -21.2890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4437/100000], Training Loss: -21.28912353515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4438/100000], Training Loss: -21.289138793945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4439/100000], Training Loss: -21.289215087890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4440/100000], Training Loss: -21.28924560546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4441/100000], Training Loss: -21.289276123046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4442/100000], Training Loss: -21.289260864257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4443/100000], Training Loss: -21.289230346679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4444/100000], Training Loss: -21.2891845703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4445/100000], Training Loss: -21.28887939453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4446/100000], Training Loss: -21.288360595703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4447/100000], Training Loss: -21.287429809570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4448/100000], Training Loss: -21.285614013671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4449/100000], Training Loss: -21.282135009765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4450/100000], Training Loss: -21.27557373046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4451/100000], Training Loss: -21.26318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4452/100000], Training Loss: -21.239456176757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4453/100000], Training Loss: -21.194183349609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4454/100000], Training Loss: -21.10821533203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4455/100000], Training Loss: -20.947158813476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4456/100000], Training Loss: -20.655792236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4457/100000], Training Loss: -20.167007446289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4458/100000], Training Loss: -19.46563720703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4459/100000], Training Loss: -18.751373291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4460/100000], Training Loss: -18.57037353515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4461/100000], Training Loss: -19.33526611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4462/100000], Training Loss: -20.525619506835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4463/100000], Training Loss: -20.909576416015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4464/100000], Training Loss: -20.306961059570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4465/100000], Training Loss: -19.752456665039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4466/100000], Training Loss: -20.235153198242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4467/100000], Training Loss: -21.056564331054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4468/100000], Training Loss: -21.1439208984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4469/100000], Training Loss: -20.66754150390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4470/100000], Training Loss: -20.624374389648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4471/100000], Training Loss: -21.081924438476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4472/100000], Training Loss: -21.184707641601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4473/100000], Training Loss: -20.840240478515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4474/100000], Training Loss: -20.7708740234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4475/100000], Training Loss: -21.108169555664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4476/100000], Training Loss: -21.23358154296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4477/100000], Training Loss: -21.040557861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4478/100000], Training Loss: -21.018325805664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4479/100000], Training Loss: -21.222640991210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4480/100000], Training Loss: -21.240737915039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4481/100000], Training Loss: -21.078659057617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4482/100000], Training Loss: -21.090118408203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4483/100000], Training Loss: -21.234298706054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4484/100000], Training Loss: -21.235565185546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4485/100000], Training Loss: -21.151992797851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4486/100000], Training Loss: -21.19500732421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4487/100000], Training Loss: -21.280288696289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4488/100000], Training Loss: -21.245407104492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4489/100000], Training Loss: -21.18572998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4490/100000], Training Loss: -21.22418212890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4491/100000], Training Loss: -21.270309448242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4492/100000], Training Loss: -21.240386962890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4493/100000], Training Loss: -21.220352172851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4494/100000], Training Loss: -21.261505126953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4495/100000], Training Loss: -21.284469604492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4496/100000], Training Loss: -21.256271362304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4497/100000], Training Loss: -21.245742797851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4498/100000], Training Loss: -21.27154541015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4499/100000], Training Loss: -21.27703857421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4500/100000], Training Loss: -21.256744384765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4501/100000], Training Loss: -21.257598876953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4502/100000], Training Loss: -21.278579711914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4503/100000], Training Loss: -21.280838012695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4504/100000], Training Loss: -21.26885986328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4505/100000], Training Loss: -21.272186279296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4506/100000], Training Loss: -21.284271240234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4507/100000], Training Loss: -21.28179931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4508/100000], Training Loss: -21.272491455078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4509/100000], Training Loss: -21.275543212890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4510/100000], Training Loss: -21.283203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4511/100000], Training Loss: -21.281265258789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4512/100000], Training Loss: -21.276641845703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4513/100000], Training Loss: -21.28021240234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4514/100000], Training Loss: -21.285736083984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4515/100000], Training Loss: -21.284332275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4516/100000], Training Loss: -21.281265258789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4517/100000], Training Loss: -21.283447265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4518/100000], Training Loss: -21.286483764648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4519/100000], Training Loss: -21.285018920898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4520/100000], Training Loss: -21.282730102539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4521/100000], Training Loss: -21.284042358398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4522/100000], Training Loss: -21.286102294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4523/100000], Training Loss: -21.285400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4524/100000], Training Loss: -21.28411865234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4525/100000], Training Loss: -21.28509521484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4526/100000], Training Loss: -21.2867431640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4527/100000], Training Loss: -21.286544799804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4528/100000], Training Loss: -21.2857666015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4529/100000], Training Loss: -21.28631591796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4530/100000], Training Loss: -21.2874755859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4531/100000], Training Loss: -21.287490844726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4532/100000], Training Loss: -21.28692626953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4533/100000], Training Loss: -21.287063598632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4534/100000], Training Loss: -21.287826538085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4535/100000], Training Loss: -21.287948608398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4536/100000], Training Loss: -21.287506103515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4537/100000], Training Loss: -21.287490844726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4538/100000], Training Loss: -21.287933349609375, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4539/100000], Training Loss: -21.288131713867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4540/100000], Training Loss: -21.287887573242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4541/100000], Training Loss: -21.28778076171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4542/100000], Training Loss: -21.288101196289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4543/100000], Training Loss: -21.288299560546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4544/100000], Training Loss: -21.288177490234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4545/100000], Training Loss: -21.28802490234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4546/100000], Training Loss: -21.288055419921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4547/100000], Training Loss: -21.2882080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4548/100000], Training Loss: -21.288116455078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4549/100000], Training Loss: -21.287811279296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4550/100000], Training Loss: -21.287582397460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4551/100000], Training Loss: -21.287338256835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4552/100000], Training Loss: -21.286849975585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4553/100000], Training Loss: -21.286087036132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4554/100000], Training Loss: -21.284912109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4555/100000], Training Loss: -21.2833251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4556/100000], Training Loss: -21.280960083007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4557/100000], Training Loss: -21.27740478515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4558/100000], Training Loss: -21.271926879882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4559/100000], Training Loss: -21.2637939453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4560/100000], Training Loss: -21.251266479492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4561/100000], Training Loss: -21.232254028320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4562/100000], Training Loss: -21.202362060546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4563/100000], Training Loss: -21.157089233398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4564/100000], Training Loss: -21.08551025390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4565/100000], Training Loss: -20.979339599609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4566/100000], Training Loss: -20.813613891601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4567/100000], Training Loss: -20.585708618164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4568/100000], Training Loss: -20.256515502929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4569/100000], Training Loss: -19.8944091796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4570/100000], Training Loss: -19.497695922851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4571/100000], Training Loss: -19.346115112304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4572/100000], Training Loss: -19.470184326171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4573/100000], Training Loss: -20.057235717773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4574/100000], Training Loss: -20.7528076171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4575/100000], Training Loss: -21.224563598632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4576/100000], Training Loss: -21.2374267578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4577/100000], Training Loss: -20.912445068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4578/100000], Training Loss: -20.592849731445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4579/100000], Training Loss: -20.53961181640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4580/100000], Training Loss: -20.814376831054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4581/100000], Training Loss: -21.150177001953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4582/100000], Training Loss: -21.288436889648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4583/100000], Training Loss: -21.170333862304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4584/100000], Training Loss: -20.969772338867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4585/100000], Training Loss: -20.907516479492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4586/100000], Training Loss: -21.027801513671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4587/100000], Training Loss: -21.211441040039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4588/100000], Training Loss: -21.288055419921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4589/100000], Training Loss: -21.221038818359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4590/100000], Training Loss: -21.114364624023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4591/100000], Training Loss: -21.0859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4592/100000], Training Loss: -21.161941528320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4593/100000], Training Loss: -21.257369995117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4594/100000], Training Loss: -21.2869873046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4595/100000], Training Loss: -21.24224853515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4596/100000], Training Loss: -21.186553955078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4597/100000], Training Loss: -21.182113647460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4598/100000], Training Loss: -21.227996826171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4599/100000], Training Loss: -21.276931762695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4600/100000], Training Loss: -21.28607177734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4601/100000], Training Loss: -21.25848388671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4602/100000], Training Loss: -21.230758666992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4603/100000], Training Loss: -21.23162841796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4604/100000], Training Loss: -21.258270263671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4605/100000], Training Loss: -21.283554077148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4606/100000], Training Loss: -21.286956787109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4607/100000], Training Loss: -21.271759033203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4608/100000], Training Loss: -21.256973266601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4609/100000], Training Loss: -21.2576904296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4610/100000], Training Loss: -21.271591186523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4611/100000], Training Loss: -21.2855224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4612/100000], Training Loss: -21.288406372070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4613/100000], Training Loss: -21.280929565429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4614/100000], Training Loss: -21.272659301757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4615/100000], Training Loss: -21.271484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4616/100000], Training Loss: -21.278076171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4617/100000], Training Loss: -21.28594970703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4618/100000], Training Loss: -21.2891845703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4619/100000], Training Loss: -21.286422729492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4620/100000], Training Loss: -21.281692504882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4621/100000], Training Loss: -21.279693603515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4622/100000], Training Loss: -21.2818603515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4623/100000], Training Loss: -21.2861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4624/100000], Training Loss: -21.289154052734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4625/100000], Training Loss: -21.289093017578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4626/100000], Training Loss: -21.2869873046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4627/100000], Training Loss: -21.285003662109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4628/100000], Training Loss: -21.284881591796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4629/100000], Training Loss: -21.286514282226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4630/100000], Training Loss: -21.2886962890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4631/100000], Training Loss: -21.289871215820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4632/100000], Training Loss: -21.28955078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4633/100000], Training Loss: -21.288528442382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4634/100000], Training Loss: -21.287628173828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4635/100000], Training Loss: -21.287704467773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4636/100000], Training Loss: -21.288589477539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4637/100000], Training Loss: -21.28961181640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4638/100000], Training Loss: -21.29022216796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4639/100000], Training Loss: -21.2901611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4640/100000], Training Loss: -21.289718627929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4641/100000], Training Loss: -21.289291381835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4642/100000], Training Loss: -21.28924560546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4643/100000], Training Loss: -21.28961181640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4644/100000], Training Loss: -21.290145874023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4645/100000], Training Loss: -21.290603637695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4646/100000], Training Loss: -21.290725708007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4647/100000], Training Loss: -21.290679931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4648/100000], Training Loss: -21.290451049804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4649/100000], Training Loss: -21.290298461914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4650/100000], Training Loss: -21.29034423828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4651/100000], Training Loss: -21.29052734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4652/100000], Training Loss: -21.290786743164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4653/100000], Training Loss: -21.291061401367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4654/100000], Training Loss: -21.291122436523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4655/100000], Training Loss: -21.291183471679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4656/100000], Training Loss: -21.291152954101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4657/100000], Training Loss: -21.2911376953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4658/100000], Training Loss: -21.2911376953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4659/100000], Training Loss: -21.291244506835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4660/100000], Training Loss: -21.291397094726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4661/100000], Training Loss: -21.291488647460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4662/100000], Training Loss: -21.291610717773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4663/100000], Training Loss: -21.291671752929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4664/100000], Training Loss: -21.291732788085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4665/100000], Training Loss: -21.291748046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4666/100000], Training Loss: -21.291717529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4667/100000], Training Loss: -21.291778564453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4668/100000], Training Loss: -21.291824340820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4669/100000], Training Loss: -21.291946411132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4670/100000], Training Loss: -21.292007446289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4671/100000], Training Loss: -21.292083740234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4672/100000], Training Loss: -21.29217529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4673/100000], Training Loss: -21.292190551757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4674/100000], Training Loss: -21.29229736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4675/100000], Training Loss: -21.292327880859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4676/100000], Training Loss: -21.292373657226562, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4677/100000], Training Loss: -21.29241943359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4678/100000], Training Loss: -21.29248046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4679/100000], Training Loss: -21.292526245117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4680/100000], Training Loss: -21.2926025390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4681/100000], Training Loss: -21.292694091796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4682/100000], Training Loss: -21.292755126953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4683/100000], Training Loss: -21.292816162109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4684/100000], Training Loss: -21.292877197265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4685/100000], Training Loss: -21.29290771484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4686/100000], Training Loss: -21.29290771484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4687/100000], Training Loss: -21.293014526367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4688/100000], Training Loss: -21.293045043945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4689/100000], Training Loss: -21.293121337890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4690/100000], Training Loss: -21.293106079101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4691/100000], Training Loss: -21.293228149414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4692/100000], Training Loss: -21.293258666992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4693/100000], Training Loss: -21.293319702148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4694/100000], Training Loss: -21.293380737304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4695/100000], Training Loss: -21.29345703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4696/100000], Training Loss: -21.293502807617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4697/100000], Training Loss: -21.2935791015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4698/100000], Training Loss: -21.293609619140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4699/100000], Training Loss: -21.293670654296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4700/100000], Training Loss: -21.293701171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4701/100000], Training Loss: -21.29376220703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4702/100000], Training Loss: -21.293838500976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4703/100000], Training Loss: -21.293899536132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4704/100000], Training Loss: -21.293930053710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4705/100000], Training Loss: -21.29400634765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4706/100000], Training Loss: -21.294036865234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4707/100000], Training Loss: -21.294143676757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4708/100000], Training Loss: -21.294189453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4709/100000], Training Loss: -21.294219970703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4710/100000], Training Loss: -21.29425048828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4711/100000], Training Loss: -21.294326782226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4712/100000], Training Loss: -21.294418334960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4713/100000], Training Loss: -21.294448852539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4714/100000], Training Loss: -21.29449462890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4715/100000], Training Loss: -21.294540405273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4716/100000], Training Loss: -21.294586181640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4717/100000], Training Loss: -21.294647216796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4718/100000], Training Loss: -21.294708251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4719/100000], Training Loss: -21.294754028320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4720/100000], Training Loss: -21.294815063476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4721/100000], Training Loss: -21.294876098632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4722/100000], Training Loss: -21.294891357421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4723/100000], Training Loss: -21.294937133789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4724/100000], Training Loss: -21.294937133789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4725/100000], Training Loss: -21.295013427734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4726/100000], Training Loss: -21.29498291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4727/100000], Training Loss: -21.294906616210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4728/100000], Training Loss: -21.294830322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4729/100000], Training Loss: -21.294677734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4730/100000], Training Loss: -21.294387817382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4731/100000], Training Loss: -21.29376220703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4732/100000], Training Loss: -21.292831420898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4733/100000], Training Loss: -21.29107666015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4734/100000], Training Loss: -21.288131713867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4735/100000], Training Loss: -21.283004760742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4736/100000], Training Loss: -21.274063110351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4737/100000], Training Loss: -21.258529663085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4738/100000], Training Loss: -21.230880737304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4739/100000], Training Loss: -21.182510375976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4740/100000], Training Loss: -21.0955810546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4741/100000], Training Loss: -20.9456787109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4742/100000], Training Loss: -20.678436279296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4743/100000], Training Loss: -20.248062133789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4744/100000], Training Loss: -19.541305541992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4745/100000], Training Loss: -18.657211303710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4746/100000], Training Loss: -17.6514892578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4747/100000], Training Loss: -17.4241943359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4748/100000], Training Loss: -18.181625366210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4749/100000], Training Loss: -19.942520141601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4750/100000], Training Loss: -21.142669677734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4751/100000], Training Loss: -20.954696655273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4752/100000], Training Loss: -20.005783081054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4753/100000], Training Loss: -19.574874877929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4754/100000], Training Loss: -20.272598266601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4755/100000], Training Loss: -21.111328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4756/100000], Training Loss: -21.115570068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4757/100000], Training Loss: -20.534576416015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4758/100000], Training Loss: -20.338287353515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4759/100000], Training Loss: -20.84478759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4760/100000], Training Loss: -21.267181396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4761/100000], Training Loss: -21.081756591796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4762/100000], Training Loss: -20.727310180664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4763/100000], Training Loss: -20.798294067382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4764/100000], Training Loss: -21.173828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4765/100000], Training Loss: -21.2730712890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4766/100000], Training Loss: -21.040145874023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4767/100000], Training Loss: -20.929061889648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4768/100000], Training Loss: -21.11065673828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4769/100000], Training Loss: -21.28106689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4770/100000], Training Loss: -21.20343017578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4771/100000], Training Loss: -21.070419311523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4772/100000], Training Loss: -21.123764038085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4773/100000], Training Loss: -21.260467529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4774/100000], Training Loss: -21.26739501953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4775/100000], Training Loss: -21.172195434570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4776/100000], Training Loss: -21.156356811523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4777/100000], Training Loss: -21.244552612304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4778/100000], Training Loss: -21.290390014648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4779/100000], Training Loss: -21.2401123046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4780/100000], Training Loss: -21.19818115234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4781/100000], Training Loss: -21.23480224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4782/100000], Training Loss: -21.286483764648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4783/100000], Training Loss: -21.277389526367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4784/100000], Training Loss: -21.239456176757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4785/100000], Training Loss: -21.241714477539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4786/100000], Training Loss: -21.27691650390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4787/100000], Training Loss: -21.288925170898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4788/100000], Training Loss: -21.267669677734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4789/100000], Training Loss: -21.25543212890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4790/100000], Training Loss: -21.27276611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4791/100000], Training Loss: -21.290542602539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4792/100000], Training Loss: -21.284332275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4793/100000], Training Loss: -21.269927978515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4794/100000], Training Loss: -21.272186279296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4795/100000], Training Loss: -21.286666870117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4796/100000], Training Loss: -21.29150390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4797/100000], Training Loss: -21.282989501953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4798/100000], Training Loss: -21.277496337890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4799/100000], Training Loss: -21.28350830078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4800/100000], Training Loss: -21.29107666015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4801/100000], Training Loss: -21.289947509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4802/100000], Training Loss: -21.2843017578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4803/100000], Training Loss: -21.284149169921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4804/100000], Training Loss: -21.28936767578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4805/100000], Training Loss: -21.292236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4806/100000], Training Loss: -21.289703369140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4807/100000], Training Loss: -21.28680419921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4808/100000], Training Loss: -21.2882080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4809/100000], Training Loss: -21.2916259765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4810/100000], Training Loss: -21.29241943359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4811/100000], Training Loss: -21.290390014648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4812/100000], Training Loss: -21.289276123046875, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4813/100000], Training Loss: -21.2906494140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4814/100000], Training Loss: -21.292449951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4815/100000], Training Loss: -21.292434692382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4816/100000], Training Loss: -21.291213989257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4817/100000], Training Loss: -21.2908935546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4818/100000], Training Loss: -21.29205322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4819/100000], Training Loss: -21.292999267578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4820/100000], Training Loss: -21.292816162109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4821/100000], Training Loss: -21.292068481445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4822/100000], Training Loss: -21.291961669921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4823/100000], Training Loss: -21.292694091796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4824/100000], Training Loss: -21.29327392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4825/100000], Training Loss: -21.293121337890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4826/100000], Training Loss: -21.292770385742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4827/100000], Training Loss: -21.292724609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4828/100000], Training Loss: -21.293197631835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4829/100000], Training Loss: -21.293533325195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4830/100000], Training Loss: -21.293487548828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4831/100000], Training Loss: -21.293258666992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4832/100000], Training Loss: -21.29327392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4833/100000], Training Loss: -21.2935791015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4834/100000], Training Loss: -21.293792724609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4835/100000], Training Loss: -21.293807983398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4836/100000], Training Loss: -21.293655395507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4837/100000], Training Loss: -21.293716430664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4838/100000], Training Loss: -21.293914794921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4839/100000], Training Loss: -21.294052124023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4840/100000], Training Loss: -21.294113159179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4841/100000], Training Loss: -21.294082641601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4842/100000], Training Loss: -21.2940673828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4843/100000], Training Loss: -21.294143676757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4844/100000], Training Loss: -21.294326782226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4845/100000], Training Loss: -21.294387817382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4846/100000], Training Loss: -21.294403076171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4847/100000], Training Loss: -21.294403076171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4848/100000], Training Loss: -21.29443359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4849/100000], Training Loss: -21.294570922851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4850/100000], Training Loss: -21.294647216796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4851/100000], Training Loss: -21.294647216796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4852/100000], Training Loss: -21.294692993164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4853/100000], Training Loss: -21.294708251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4854/100000], Training Loss: -21.294769287109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4855/100000], Training Loss: -21.29486083984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4856/100000], Training Loss: -21.294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4857/100000], Training Loss: -21.29498291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4858/100000], Training Loss: -21.295013427734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4859/100000], Training Loss: -21.295028686523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4860/100000], Training Loss: -21.295120239257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4861/100000], Training Loss: -21.295181274414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4862/100000], Training Loss: -21.295242309570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4863/100000], Training Loss: -21.295272827148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4864/100000], Training Loss: -21.2952880859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4865/100000], Training Loss: -21.295379638671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4866/100000], Training Loss: -21.295425415039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4867/100000], Training Loss: -21.29547119140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4868/100000], Training Loss: -21.295516967773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4869/100000], Training Loss: -21.295578002929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4870/100000], Training Loss: -21.295623779296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4871/100000], Training Loss: -21.295684814453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4872/100000], Training Loss: -21.295745849609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4873/100000], Training Loss: -21.295761108398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4874/100000], Training Loss: -21.29583740234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4875/100000], Training Loss: -21.295852661132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4876/100000], Training Loss: -21.295913696289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4877/100000], Training Loss: -21.29595947265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4878/100000], Training Loss: -21.295989990234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4879/100000], Training Loss: -21.296096801757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4880/100000], Training Loss: -21.296096801757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4881/100000], Training Loss: -21.296173095703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4882/100000], Training Loss: -21.296218872070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4883/100000], Training Loss: -21.2962646484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4884/100000], Training Loss: -21.29632568359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4885/100000], Training Loss: -21.29638671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4886/100000], Training Loss: -21.296463012695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4887/100000], Training Loss: -21.296417236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4888/100000], Training Loss: -21.296478271484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4889/100000], Training Loss: -21.296539306640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4890/100000], Training Loss: -21.2965087890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4891/100000], Training Loss: -21.296539306640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4892/100000], Training Loss: -21.296554565429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4893/100000], Training Loss: -21.296524047851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4894/100000], Training Loss: -21.296417236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4895/100000], Training Loss: -21.296310424804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4896/100000], Training Loss: -21.296066284179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4897/100000], Training Loss: -21.295684814453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4898/100000], Training Loss: -21.29498291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4899/100000], Training Loss: -21.293792724609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4900/100000], Training Loss: -21.2918701171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4901/100000], Training Loss: -21.288619995117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4902/100000], Training Loss: -21.283172607421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4903/100000], Training Loss: -21.273971557617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4904/100000], Training Loss: -21.258392333984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4905/100000], Training Loss: -21.231857299804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4906/100000], Training Loss: -21.186660766601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4907/100000], Training Loss: -21.110565185546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4908/100000], Training Loss: -20.984161376953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4909/100000], Training Loss: -20.783584594726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4910/100000], Training Loss: -20.484573364257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4911/100000], Training Loss: -20.103973388671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4912/100000], Training Loss: -19.733001708984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4913/100000], Training Loss: -19.607650756835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4914/100000], Training Loss: -19.924224853515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4915/100000], Training Loss: -20.603515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4916/100000], Training Loss: -21.173858642578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4917/100000], Training Loss: -21.24261474609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4918/100000], Training Loss: -20.9052734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4919/100000], Training Loss: -20.60589599609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4920/100000], Training Loss: -20.6910400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4921/100000], Training Loss: -21.04412841796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4922/100000], Training Loss: -21.263839721679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [4923/100000], Training Loss: -21.160598754882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4924/100000], Training Loss: -20.952377319335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4925/100000], Training Loss: -20.947250366210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4926/100000], Training Loss: -21.14178466796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4927/100000], Training Loss: -21.281005859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4928/100000], Training Loss: -21.21575927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4929/100000], Training Loss: -21.083969116210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4930/100000], Training Loss: -21.089065551757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4931/100000], Training Loss: -21.216644287109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4932/100000], Training Loss: -21.295516967773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4933/100000], Training Loss: -21.244094848632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4934/100000], Training Loss: -21.165374755859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4935/100000], Training Loss: -21.178756713867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4936/100000], Training Loss: -21.256103515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4937/100000], Training Loss: -21.2911376953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4938/100000], Training Loss: -21.253936767578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4939/100000], Training Loss: -21.217041015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4940/100000], Training Loss: -21.237350463867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4941/100000], Training Loss: -21.28143310546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4942/100000], Training Loss: -21.28936767578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4943/100000], Training Loss: -21.261016845703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4944/100000], Training Loss: -21.245346069335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4945/100000], Training Loss: -21.265655517578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4946/100000], Training Loss: -21.292190551757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4947/100000], Training Loss: -21.291961669921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4948/100000], Training Loss: -21.272903442382812, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4949/100000], Training Loss: -21.265640258789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4950/100000], Training Loss: -21.279159545898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4951/100000], Training Loss: -21.293533325195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4952/100000], Training Loss: -21.29180908203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4953/100000], Training Loss: -21.2813720703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4954/100000], Training Loss: -21.279205322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4955/100000], Training Loss: -21.28802490234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4956/100000], Training Loss: -21.295425415039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4957/100000], Training Loss: -21.293060302734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4958/100000], Training Loss: -21.2864990234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4959/100000], Training Loss: -21.285675048828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4960/100000], Training Loss: -21.2913818359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4961/100000], Training Loss: -21.296218872070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [4962/100000], Training Loss: -21.295135498046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4963/100000], Training Loss: -21.291305541992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4964/100000], Training Loss: -21.2904052734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4965/100000], Training Loss: -21.293365478515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4966/100000], Training Loss: -21.296249389648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4967/100000], Training Loss: -21.295928955078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4968/100000], Training Loss: -21.293731689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4969/100000], Training Loss: -21.293106079101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4970/100000], Training Loss: -21.294784545898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4971/100000], Training Loss: -21.296783447265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4972/100000], Training Loss: -21.29693603515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4973/100000], Training Loss: -21.295700073242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4974/100000], Training Loss: -21.294937133789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4975/100000], Training Loss: -21.2955322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4976/100000], Training Loss: -21.2967529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4977/100000], Training Loss: -21.29730224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4978/100000], Training Loss: -21.296844482421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4979/100000], Training Loss: -21.296295166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4980/100000], Training Loss: -21.296417236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4981/100000], Training Loss: -21.297088623046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4982/100000], Training Loss: -21.297637939453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4983/100000], Training Loss: -21.297622680664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4984/100000], Training Loss: -21.2972412109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4985/100000], Training Loss: -21.29705810546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [4986/100000], Training Loss: -21.297348022460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [4987/100000], Training Loss: -21.29779052734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4988/100000], Training Loss: -21.297943115234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4989/100000], Training Loss: -21.297866821289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [4990/100000], Training Loss: -21.297714233398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [4991/100000], Training Loss: -21.29779052734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [4992/100000], Training Loss: -21.2979736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4993/100000], Training Loss: -21.298248291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4994/100000], Training Loss: -21.298370361328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [4995/100000], Training Loss: -21.298294067382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [4996/100000], Training Loss: -21.298233032226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [4997/100000], Training Loss: -21.298248291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [4998/100000], Training Loss: -21.298385620117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [4999/100000], Training Loss: -21.298583984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5000/100000], Training Loss: -21.298675537109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5001/100000], Training Loss: -21.29864501953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5002/100000], Training Loss: -21.298629760742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5003/100000], Training Loss: -21.298690795898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5004/100000], Training Loss: -21.298812866210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5005/100000], Training Loss: -21.298934936523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5006/100000], Training Loss: -21.298965454101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5007/100000], Training Loss: -21.29901123046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5008/100000], Training Loss: -21.299072265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5009/100000], Training Loss: -21.299118041992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5010/100000], Training Loss: -21.299148559570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5011/100000], Training Loss: -21.299209594726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5012/100000], Training Loss: -21.29931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5013/100000], Training Loss: -21.299362182617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5014/100000], Training Loss: -21.299407958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5015/100000], Training Loss: -21.2994384765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5016/100000], Training Loss: -21.299453735351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5017/100000], Training Loss: -21.299530029296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5018/100000], Training Loss: -21.299652099609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5019/100000], Training Loss: -21.2996826171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5020/100000], Training Loss: -21.29974365234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5021/100000], Training Loss: -21.299774169921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5022/100000], Training Loss: -21.299789428710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5023/100000], Training Loss: -21.29986572265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5024/100000], Training Loss: -21.299957275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5025/100000], Training Loss: -21.300003051757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5026/100000], Training Loss: -21.300033569335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5027/100000], Training Loss: -21.300094604492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5028/100000], Training Loss: -21.3001708984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5029/100000], Training Loss: -21.300201416015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5030/100000], Training Loss: -21.300216674804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5031/100000], Training Loss: -21.30029296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5032/100000], Training Loss: -21.30035400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5033/100000], Training Loss: -21.3004150390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5034/100000], Training Loss: -21.300445556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5035/100000], Training Loss: -21.30047607421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5036/100000], Training Loss: -21.300506591796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5037/100000], Training Loss: -21.300521850585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5038/100000], Training Loss: -21.300506591796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5039/100000], Training Loss: -21.30047607421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5040/100000], Training Loss: -21.300399780273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5041/100000], Training Loss: -21.30023193359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5042/100000], Training Loss: -21.2999267578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5043/100000], Training Loss: -21.299362182617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5044/100000], Training Loss: -21.298294067382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5045/100000], Training Loss: -21.296554565429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5046/100000], Training Loss: -21.29351806640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5047/100000], Training Loss: -21.288070678710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5048/100000], Training Loss: -21.278564453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5049/100000], Training Loss: -21.261688232421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5050/100000], Training Loss: -21.231903076171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5051/100000], Training Loss: -21.178298950195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5052/100000], Training Loss: -21.084030151367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5053/100000], Training Loss: -20.91363525390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5054/100000], Training Loss: -20.625152587890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5055/100000], Training Loss: -20.124252319335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5056/100000], Training Loss: -19.388565063476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5057/100000], Training Loss: -18.337249755859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5058/100000], Training Loss: -17.473190307617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5059/100000], Training Loss: -17.16522216796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5060/100000], Training Loss: -18.363677978515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5061/100000], Training Loss: -20.162872314453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5062/100000], Training Loss: -21.22991943359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5063/100000], Training Loss: -20.801483154296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5064/100000], Training Loss: -19.759048461914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5065/100000], Training Loss: -19.56072998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5066/100000], Training Loss: -20.376602172851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5067/100000], Training Loss: -21.179901123046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5068/100000], Training Loss: -21.0242919921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5069/100000], Training Loss: -20.402191162109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5070/100000], Training Loss: -20.367935180664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5071/100000], Training Loss: -20.930313110351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5072/100000], Training Loss: -21.2666015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5073/100000], Training Loss: -20.986129760742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5074/100000], Training Loss: -20.667449951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5075/100000], Training Loss: -20.86181640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5076/100000], Training Loss: -21.2314453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5077/100000], Training Loss: -21.23773193359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5078/100000], Training Loss: -20.9755859375, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5079/100000], Training Loss: -20.923675537109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5080/100000], Training Loss: -21.161529541015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5081/100000], Training Loss: -21.2958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5082/100000], Training Loss: -21.169387817382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5083/100000], Training Loss: -21.05316162109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5084/100000], Training Loss: -21.143142700195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5085/100000], Training Loss: -21.279830932617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5086/100000], Training Loss: -21.257186889648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5087/100000], Training Loss: -21.158355712890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5088/100000], Training Loss: -21.168838500976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5089/100000], Training Loss: -21.262863159179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5090/100000], Training Loss: -21.288681030273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5091/100000], Training Loss: -21.22808837890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5092/100000], Training Loss: -21.20111083984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5093/100000], Training Loss: -21.253265380859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5094/100000], Training Loss: -21.296295166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5095/100000], Training Loss: -21.27197265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5096/100000], Training Loss: -21.235931396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5097/100000], Training Loss: -21.2506103515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5098/100000], Training Loss: -21.289352416992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5099/100000], Training Loss: -21.293075561523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5100/100000], Training Loss: -21.267074584960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5101/100000], Training Loss: -21.26007080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5102/100000], Training Loss: -21.281692504882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5103/100000], Training Loss: -21.296585083007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5104/100000], Training Loss: -21.2862548828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5105/100000], Training Loss: -21.27362060546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5106/100000], Training Loss: -21.2803955078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5107/100000], Training Loss: -21.294647216796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5108/100000], Training Loss: -21.295303344726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5109/100000], Training Loss: -21.285354614257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5110/100000], Training Loss: -21.282852172851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5111/100000], Training Loss: -21.291519165039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5112/100000], Training Loss: -21.297866821289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5113/100000], Training Loss: -21.294219970703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5114/100000], Training Loss: -21.288604736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5115/100000], Training Loss: -21.290191650390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5116/100000], Training Loss: -21.296112060546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5117/100000], Training Loss: -21.297775268554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5118/100000], Training Loss: -21.29443359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5119/100000], Training Loss: -21.292373657226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5120/100000], Training Loss: -21.294769287109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5121/100000], Training Loss: -21.297836303710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5122/100000], Training Loss: -21.297576904296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5123/100000], Training Loss: -21.295333862304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5124/100000], Training Loss: -21.294967651367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5125/100000], Training Loss: -21.297042846679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5126/100000], Training Loss: -21.298538208007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5127/100000], Training Loss: -21.297836303710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5128/100000], Training Loss: -21.2965087890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5129/100000], Training Loss: -21.29669189453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5130/100000], Training Loss: -21.29803466796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5131/100000], Training Loss: -21.29876708984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5132/100000], Training Loss: -21.298263549804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5133/100000], Training Loss: -21.297576904296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5134/100000], Training Loss: -21.297882080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5135/100000], Training Loss: -21.298660278320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5136/100000], Training Loss: -21.298965454101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5137/100000], Training Loss: -21.298614501953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5138/100000], Training Loss: -21.298294067382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5139/100000], Training Loss: -21.29852294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5140/100000], Training Loss: -21.299072265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5141/100000], Training Loss: -21.299270629882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5142/100000], Training Loss: -21.299087524414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5143/100000], Training Loss: -21.298904418945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5144/100000], Training Loss: -21.299041748046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5145/100000], Training Loss: -21.299346923828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5146/100000], Training Loss: -21.299514770507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5147/100000], Training Loss: -21.29937744140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5148/100000], Training Loss: -21.299331665039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5149/100000], Training Loss: -21.299423217773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5150/100000], Training Loss: -21.299606323242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5151/100000], Training Loss: -21.299713134765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5152/100000], Training Loss: -21.299758911132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5153/100000], Training Loss: -21.299697875976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5154/100000], Training Loss: -21.299728393554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5155/100000], Training Loss: -21.299880981445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5156/100000], Training Loss: -21.29998779296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5157/100000], Training Loss: -21.300003051757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5158/100000], Training Loss: -21.300018310546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5159/100000], Training Loss: -21.300033569335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5160/100000], Training Loss: -21.30010986328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5161/100000], Training Loss: -21.300262451171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5162/100000], Training Loss: -21.300216674804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5163/100000], Training Loss: -21.300247192382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5164/100000], Training Loss: -21.300323486328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5165/100000], Training Loss: -21.300369262695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5166/100000], Training Loss: -21.300445556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5167/100000], Training Loss: -21.300552368164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5168/100000], Training Loss: -21.300552368164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5169/100000], Training Loss: -21.300582885742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5170/100000], Training Loss: -21.300613403320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5171/100000], Training Loss: -21.3006591796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5172/100000], Training Loss: -21.30072021484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5173/100000], Training Loss: -21.300811767578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5174/100000], Training Loss: -21.300811767578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5175/100000], Training Loss: -21.300827026367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5176/100000], Training Loss: -21.3009033203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5177/100000], Training Loss: -21.300933837890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5178/100000], Training Loss: -21.301010131835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5179/100000], Training Loss: -21.301040649414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5180/100000], Training Loss: -21.30108642578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5181/100000], Training Loss: -21.301132202148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5182/100000], Training Loss: -21.301177978515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5183/100000], Training Loss: -21.301239013671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5184/100000], Training Loss: -21.301254272460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5185/100000], Training Loss: -21.301361083984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5186/100000], Training Loss: -21.301361083984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5187/100000], Training Loss: -21.30145263671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5188/100000], Training Loss: -21.301483154296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5189/100000], Training Loss: -21.301467895507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5190/100000], Training Loss: -21.301559448242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5191/100000], Training Loss: -21.301589965820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5192/100000], Training Loss: -21.301651000976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5193/100000], Training Loss: -21.301666259765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5194/100000], Training Loss: -21.301742553710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5195/100000], Training Loss: -21.3017578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5196/100000], Training Loss: -21.30181884765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5197/100000], Training Loss: -21.301864624023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5198/100000], Training Loss: -21.3018798828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5199/100000], Training Loss: -21.30194091796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5200/100000], Training Loss: -21.302001953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5201/100000], Training Loss: -21.302032470703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5202/100000], Training Loss: -21.302078247070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5203/100000], Training Loss: -21.302108764648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5204/100000], Training Loss: -21.302169799804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5205/100000], Training Loss: -21.30224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5206/100000], Training Loss: -21.30224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5207/100000], Training Loss: -21.302322387695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5208/100000], Training Loss: -21.30230712890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5209/100000], Training Loss: -21.302383422851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5210/100000], Training Loss: -21.302398681640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5211/100000], Training Loss: -21.302413940429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5212/100000], Training Loss: -21.302474975585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5213/100000], Training Loss: -21.302474975585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5214/100000], Training Loss: -21.302505493164062, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5215/100000], Training Loss: -21.302474975585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5216/100000], Training Loss: -21.302459716796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5217/100000], Training Loss: -21.3023681640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5218/100000], Training Loss: -21.30218505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5219/100000], Training Loss: -21.301925659179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5220/100000], Training Loss: -21.301422119140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5221/100000], Training Loss: -21.300613403320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5222/100000], Training Loss: -21.299209594726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5223/100000], Training Loss: -21.29693603515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5224/100000], Training Loss: -21.29315185546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5225/100000], Training Loss: -21.286865234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5226/100000], Training Loss: -21.276168823242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5227/100000], Training Loss: -21.257980346679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5228/100000], Training Loss: -21.227249145507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5229/100000], Training Loss: -21.175033569335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5230/100000], Training Loss: -21.087615966796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5231/100000], Training Loss: -20.943588256835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5232/100000], Training Loss: -20.718002319335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5233/100000], Training Loss: -20.390029907226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5234/100000], Training Loss: -19.988800048828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5235/100000], Training Loss: -19.634002685546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5236/100000], Training Loss: -19.576919555664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5237/100000], Training Loss: -19.989959716796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5238/100000], Training Loss: -20.703216552734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5239/100000], Training Loss: -21.215377807617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5240/100000], Training Loss: -21.198287963867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5241/100000], Training Loss: -20.835662841796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5242/100000], Training Loss: -20.586090087890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5243/100000], Training Loss: -20.7335205078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5244/100000], Training Loss: -21.08917236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5245/100000], Training Loss: -21.255966186523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5246/100000], Training Loss: -21.116409301757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5247/100000], Training Loss: -20.928314208984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5248/100000], Training Loss: -20.968460083007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5249/100000], Training Loss: -21.176193237304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5250/100000], Training Loss: -21.2889404296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5251/100000], Training Loss: -21.19842529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5252/100000], Training Loss: -21.07037353515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5253/100000], Training Loss: -21.094924926757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5254/100000], Training Loss: -21.229827880859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5255/100000], Training Loss: -21.301010131835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5256/100000], Training Loss: -21.243484497070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5257/100000], Training Loss: -21.168426513671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5258/100000], Training Loss: -21.186614990234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5259/100000], Training Loss: -21.261871337890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5260/100000], Training Loss: -21.291778564453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5261/100000], Training Loss: -21.25408935546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5262/100000], Training Loss: -21.221343994140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5263/100000], Training Loss: -21.245040893554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5264/100000], Training Loss: -21.288986206054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5265/100000], Training Loss: -21.295166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5266/100000], Training Loss: -21.26531982421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5267/100000], Training Loss: -21.248748779296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5268/100000], Training Loss: -21.268646240234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5269/100000], Training Loss: -21.29644775390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5270/100000], Training Loss: -21.298431396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5271/100000], Training Loss: -21.280426025390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5272/100000], Training Loss: -21.272064208984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5273/100000], Training Loss: -21.283905029296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5274/100000], Training Loss: -21.2979736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5275/100000], Training Loss: -21.297134399414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5276/100000], Training Loss: -21.2869873046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5277/100000], Training Loss: -21.284210205078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5278/100000], Training Loss: -21.292861938476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5279/100000], Training Loss: -21.301361083984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5280/100000], Training Loss: -21.30010986328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5281/100000], Training Loss: -21.29327392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5282/100000], Training Loss: -21.290939331054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5283/100000], Training Loss: -21.295669555664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5284/100000], Training Loss: -21.301025390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5285/100000], Training Loss: -21.30108642578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5286/100000], Training Loss: -21.29766845703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5287/100000], Training Loss: -21.296295166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5288/100000], Training Loss: -21.298797607421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5289/100000], Training Loss: -21.301925659179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5290/100000], Training Loss: -21.302108764648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5291/100000], Training Loss: -21.299942016601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5292/100000], Training Loss: -21.29864501953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5293/100000], Training Loss: -21.299835205078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5294/100000], Training Loss: -21.301910400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5295/100000], Training Loss: -21.30279541015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5296/100000], Training Loss: -21.30194091796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5297/100000], Training Loss: -21.30096435546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5298/100000], Training Loss: -21.30120849609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5299/100000], Training Loss: -21.302352905273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5300/100000], Training Loss: -21.303070068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5301/100000], Training Loss: -21.302825927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5302/100000], Training Loss: -21.302093505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5303/100000], Training Loss: -21.301956176757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5304/100000], Training Loss: -21.30255126953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5305/100000], Training Loss: -21.30322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5306/100000], Training Loss: -21.303436279296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5307/100000], Training Loss: -21.303115844726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5308/100000], Training Loss: -21.302886962890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5309/100000], Training Loss: -21.303024291992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5310/100000], Training Loss: -21.303451538085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5311/100000], Training Loss: -21.3037109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5312/100000], Training Loss: -21.3037109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5313/100000], Training Loss: -21.303512573242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5314/100000], Training Loss: -21.303451538085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5315/100000], Training Loss: -21.303573608398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5316/100000], Training Loss: -21.303848266601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5317/100000], Training Loss: -21.304000854492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5318/100000], Training Loss: -21.30401611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5319/100000], Training Loss: -21.303924560546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5320/100000], Training Loss: -21.303939819335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5321/100000], Training Loss: -21.304061889648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5322/100000], Training Loss: -21.304244995117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5323/100000], Training Loss: -21.304351806640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5324/100000], Training Loss: -21.30438232421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5325/100000], Training Loss: -21.304336547851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5326/100000], Training Loss: -21.304397583007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5327/100000], Training Loss: -21.30450439453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5328/100000], Training Loss: -21.304580688476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5329/100000], Training Loss: -21.304656982421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5330/100000], Training Loss: -21.3046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5331/100000], Training Loss: -21.3046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5332/100000], Training Loss: -21.304702758789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5333/100000], Training Loss: -21.304779052734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5334/100000], Training Loss: -21.304840087890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5335/100000], Training Loss: -21.304916381835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5336/100000], Training Loss: -21.304977416992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5337/100000], Training Loss: -21.305007934570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5338/100000], Training Loss: -21.305023193359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5339/100000], Training Loss: -21.3050537109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5340/100000], Training Loss: -21.305130004882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5341/100000], Training Loss: -21.305160522460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5342/100000], Training Loss: -21.305191040039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5343/100000], Training Loss: -21.30523681640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5344/100000], Training Loss: -21.305252075195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5345/100000], Training Loss: -21.305267333984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5346/100000], Training Loss: -21.305252075195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5347/100000], Training Loss: -21.305206298828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5348/100000], Training Loss: -21.305160522460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5349/100000], Training Loss: -21.30499267578125, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5350/100000], Training Loss: -21.304794311523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5351/100000], Training Loss: -21.304458618164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5352/100000], Training Loss: -21.303970336914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5353/100000], Training Loss: -21.303115844726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5354/100000], Training Loss: -21.301895141601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5355/100000], Training Loss: -21.299880981445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5356/100000], Training Loss: -21.296768188476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5357/100000], Training Loss: -21.291915893554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5358/100000], Training Loss: -21.28411865234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5359/100000], Training Loss: -21.271713256835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5360/100000], Training Loss: -21.2515869140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5361/100000], Training Loss: -21.219436645507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5362/100000], Training Loss: -21.166488647460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5363/100000], Training Loss: -21.082473754882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5364/100000], Training Loss: -20.944442749023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5365/100000], Training Loss: -20.733642578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5366/100000], Training Loss: -20.4013671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5367/100000], Training Loss: -19.955352783203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5368/100000], Training Loss: -19.357757568359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5369/100000], Training Loss: -18.844207763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5370/100000], Training Loss: -18.538055419921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5371/100000], Training Loss: -18.938796997070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5372/100000], Training Loss: -19.836502075195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5373/100000], Training Loss: -20.848159790039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5374/100000], Training Loss: -21.285964965820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5375/100000], Training Loss: -21.013015747070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5376/100000], Training Loss: -20.460311889648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5377/100000], Training Loss: -20.185379028320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5378/100000], Training Loss: -20.477752685546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5379/100000], Training Loss: -20.996978759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5380/100000], Training Loss: -21.2696533203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5381/100000], Training Loss: -21.119918823242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5382/100000], Training Loss: -20.82318115234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5383/100000], Training Loss: -20.757720947265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5384/100000], Training Loss: -20.970504760742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5385/100000], Training Loss: -21.2200927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5386/100000], Training Loss: -21.253570556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5387/100000], Training Loss: -21.10479736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5388/100000], Training Loss: -21.000473022460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5389/100000], Training Loss: -21.065414428710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5390/100000], Training Loss: -21.21685791015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5391/100000], Training Loss: -21.280731201171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5392/100000], Training Loss: -21.216827392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5393/100000], Training Loss: -21.13726806640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5394/100000], Training Loss: -21.1473388671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5395/100000], Training Loss: -21.233245849609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5396/100000], Training Loss: -21.291519165039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5397/100000], Training Loss: -21.269439697265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5398/100000], Training Loss: -21.214508056640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5399/100000], Training Loss: -21.201629638671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5400/100000], Training Loss: -21.2462158203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5401/100000], Training Loss: -21.29364013671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5402/100000], Training Loss: -21.295928955078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5403/100000], Training Loss: -21.262908935546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5404/100000], Training Loss: -21.240936279296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5405/100000], Training Loss: -21.256439208984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5406/100000], Training Loss: -21.289093017578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5407/100000], Training Loss: -21.3040771484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5408/100000], Training Loss: -21.291213989257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5409/100000], Training Loss: -21.271774291992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5410/100000], Training Loss: -21.269699096679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5411/100000], Training Loss: -21.285614013671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5412/100000], Training Loss: -21.301437377929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5413/100000], Training Loss: -21.302490234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5414/100000], Training Loss: -21.292083740234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5415/100000], Training Loss: -21.2845458984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5416/100000], Training Loss: -21.287994384765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5417/100000], Training Loss: -21.297775268554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5418/100000], Training Loss: -21.303573608398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5419/100000], Training Loss: -21.301300048828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5420/100000], Training Loss: -21.295730590820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5421/100000], Training Loss: -21.293853759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5422/100000], Training Loss: -21.297378540039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5423/100000], Training Loss: -21.302337646484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5424/100000], Training Loss: -21.30401611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5425/100000], Training Loss: -21.301895141601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5426/100000], Training Loss: -21.2991943359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5427/100000], Training Loss: -21.299072265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5428/100000], Training Loss: -21.301559448242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5429/100000], Training Loss: -21.304153442382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5430/100000], Training Loss: -21.30462646484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5431/100000], Training Loss: -21.303176879882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5432/100000], Training Loss: -21.3017578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5433/100000], Training Loss: -21.30181884765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5434/100000], Training Loss: -21.303314208984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5435/100000], Training Loss: -21.3048095703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5436/100000], Training Loss: -21.305191040039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5437/100000], Training Loss: -21.304534912109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5438/100000], Training Loss: -21.3035888671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5439/100000], Training Loss: -21.303482055664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5440/100000], Training Loss: -21.30413818359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5441/100000], Training Loss: -21.305099487304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5442/100000], Training Loss: -21.305496215820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5443/100000], Training Loss: -21.305328369140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5444/100000], Training Loss: -21.30487060546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5445/100000], Training Loss: -21.304702758789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5446/100000], Training Loss: -21.304916381835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5447/100000], Training Loss: -21.305389404296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5448/100000], Training Loss: -21.305694580078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5449/100000], Training Loss: -21.305770874023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5450/100000], Training Loss: -21.305587768554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5451/100000], Training Loss: -21.305404663085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5452/100000], Training Loss: -21.30548095703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5453/100000], Training Loss: -21.305694580078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5454/100000], Training Loss: -21.305938720703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5455/100000], Training Loss: -21.306060791015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5456/100000], Training Loss: -21.306060791015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5457/100000], Training Loss: -21.305984497070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5458/100000], Training Loss: -21.305938720703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5459/100000], Training Loss: -21.3060302734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5460/100000], Training Loss: -21.30615234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5461/100000], Training Loss: -21.306289672851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5462/100000], Training Loss: -21.306350708007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5463/100000], Training Loss: -21.306396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5464/100000], Training Loss: -21.306427001953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5465/100000], Training Loss: -21.306381225585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5466/100000], Training Loss: -21.306396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5467/100000], Training Loss: -21.306533813476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5468/100000], Training Loss: -21.306610107421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5469/100000], Training Loss: -21.306640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5470/100000], Training Loss: -21.306716918945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5471/100000], Training Loss: -21.3067626953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5472/100000], Training Loss: -21.30670166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5473/100000], Training Loss: -21.306793212890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5474/100000], Training Loss: -21.306854248046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5475/100000], Training Loss: -21.306976318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5476/100000], Training Loss: -21.306976318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5477/100000], Training Loss: -21.307037353515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5478/100000], Training Loss: -21.307037353515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5479/100000], Training Loss: -21.307098388671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5480/100000], Training Loss: -21.307098388671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5481/100000], Training Loss: -21.307144165039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5482/100000], Training Loss: -21.307159423828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5483/100000], Training Loss: -21.307281494140625, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5484/100000], Training Loss: -21.307281494140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5485/100000], Training Loss: -21.307373046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5486/100000], Training Loss: -21.307403564453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5487/100000], Training Loss: -21.307479858398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5488/100000], Training Loss: -21.307449340820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5489/100000], Training Loss: -21.307540893554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5490/100000], Training Loss: -21.307586669921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5491/100000], Training Loss: -21.307601928710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5492/100000], Training Loss: -21.307632446289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5493/100000], Training Loss: -21.307662963867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5494/100000], Training Loss: -21.307723999023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5495/100000], Training Loss: -21.30780029296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5496/100000], Training Loss: -21.307846069335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5497/100000], Training Loss: -21.307891845703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5498/100000], Training Loss: -21.307907104492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5499/100000], Training Loss: -21.30792236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5500/100000], Training Loss: -21.3079833984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5501/100000], Training Loss: -21.308013916015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5502/100000], Training Loss: -21.308059692382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5503/100000], Training Loss: -21.30804443359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5504/100000], Training Loss: -21.308074951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5505/100000], Training Loss: -21.308135986328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5506/100000], Training Loss: -21.30810546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5507/100000], Training Loss: -21.30816650390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5508/100000], Training Loss: -21.308120727539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5509/100000], Training Loss: -21.308074951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5510/100000], Training Loss: -21.307952880859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5511/100000], Training Loss: -21.307754516601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5512/100000], Training Loss: -21.3074951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5513/100000], Training Loss: -21.306976318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5514/100000], Training Loss: -21.306228637695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5515/100000], Training Loss: -21.304946899414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5516/100000], Training Loss: -21.303009033203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5517/100000], Training Loss: -21.299835205078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5518/100000], Training Loss: -21.294677734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5519/100000], Training Loss: -21.2862548828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5520/100000], Training Loss: -21.27252197265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5521/100000], Training Loss: -21.24993896484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5522/100000], Training Loss: -21.212799072265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5523/100000], Training Loss: -21.15191650390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5524/100000], Training Loss: -21.053619384765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5525/100000], Training Loss: -20.898788452148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5526/100000], Training Loss: -20.667449951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5527/100000], Training Loss: -20.35247802734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5528/100000], Training Loss: -19.994857788085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5529/100000], Training Loss: -19.722320556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5530/100000], Training Loss: -19.731369018554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5531/100000], Training Loss: -20.134735107421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5532/100000], Training Loss: -20.73419189453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5533/100000], Training Loss: -21.140411376953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5534/100000], Training Loss: -21.1158447265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5535/100000], Training Loss: -20.822906494140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5536/100000], Training Loss: -20.618728637695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5537/100000], Training Loss: -20.756195068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5538/100000], Training Loss: -21.092147827148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5539/100000], Training Loss: -21.301727294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5540/100000], Training Loss: -21.219467163085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5541/100000], Training Loss: -21.0101318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5542/100000], Training Loss: -20.944610595703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5543/100000], Training Loss: -21.079727172851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5544/100000], Training Loss: -21.24444580078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5545/100000], Training Loss: -21.2576904296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5546/100000], Training Loss: -21.152877807617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5547/100000], Training Loss: -21.096221923828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5548/100000], Training Loss: -21.16796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5549/100000], Training Loss: -21.2779541015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5550/100000], Training Loss: -21.302658081054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5551/100000], Training Loss: -21.240097045898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5552/100000], Training Loss: -21.190261840820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5553/100000], Training Loss: -21.216629028320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5554/100000], Training Loss: -21.276779174804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5555/100000], Training Loss: -21.29498291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5556/100000], Training Loss: -21.26123046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5557/100000], Training Loss: -21.231979370117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5558/100000], Training Loss: -21.247543334960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5559/100000], Training Loss: -21.287506103515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5560/100000], Training Loss: -21.304733276367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5561/100000], Training Loss: -21.2884521484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5562/100000], Training Loss: -21.269271850585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5563/100000], Training Loss: -21.274429321289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5564/100000], Training Loss: -21.295822143554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5565/100000], Training Loss: -21.306594848632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5566/100000], Training Loss: -21.29736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5567/100000], Training Loss: -21.283721923828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5568/100000], Training Loss: -21.283462524414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5569/100000], Training Loss: -21.295272827148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5570/100000], Training Loss: -21.3040771484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5571/100000], Training Loss: -21.301406860351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5572/100000], Training Loss: -21.293777465820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5573/100000], Training Loss: -21.292160034179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5574/100000], Training Loss: -21.2987060546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5575/100000], Training Loss: -21.305618286132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5576/100000], Training Loss: -21.306259155273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5577/100000], Training Loss: -21.30224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5578/100000], Training Loss: -21.299835205078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5579/100000], Training Loss: -21.302169799804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5580/100000], Training Loss: -21.3065185546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5581/100000], Training Loss: -21.3082275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5582/100000], Training Loss: -21.30657958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5583/100000], Training Loss: -21.304214477539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5584/100000], Training Loss: -21.304183959960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5585/100000], Training Loss: -21.306411743164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5586/100000], Training Loss: -21.308303833007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5587/100000], Training Loss: -21.308181762695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5588/100000], Training Loss: -21.306686401367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5589/100000], Training Loss: -21.305801391601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5590/100000], Training Loss: -21.30633544921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5591/100000], Training Loss: -21.307586669921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5592/100000], Training Loss: -21.308212280273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5593/100000], Training Loss: -21.307723999023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5594/100000], Training Loss: -21.306747436523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5595/100000], Training Loss: -21.306243896484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5596/100000], Training Loss: -21.306411743164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5597/100000], Training Loss: -21.306640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5598/100000], Training Loss: -21.306289672851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5599/100000], Training Loss: -21.305099487304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5600/100000], Training Loss: -21.303390502929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5601/100000], Training Loss: -21.301437377929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5602/100000], Training Loss: -21.299072265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5603/100000], Training Loss: -21.295562744140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5604/100000], Training Loss: -21.2900390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5605/100000], Training Loss: -21.281524658203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5606/100000], Training Loss: -21.2686767578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5607/100000], Training Loss: -21.249893188476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5608/100000], Training Loss: -21.221435546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5609/100000], Training Loss: -21.179229736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5610/100000], Training Loss: -21.114089965820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5611/100000], Training Loss: -21.018997192382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5612/100000], Training Loss: -20.87457275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5613/100000], Training Loss: -20.676803588867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5614/100000], Training Loss: -20.395736694335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5615/100000], Training Loss: -20.072433471679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5616/100000], Training Loss: -19.702102661132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5617/100000], Training Loss: -19.476791381835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5618/100000], Training Loss: -19.44451904296875, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5619/100000], Training Loss: -19.821441650390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5620/100000], Training Loss: -20.417694091796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5621/100000], Training Loss: -21.015106201171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5622/100000], Training Loss: -21.298507690429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5623/100000], Training Loss: -21.198745727539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5624/100000], Training Loss: -20.88653564453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5625/100000], Training Loss: -20.62548828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5626/100000], Training Loss: -20.628463745117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5627/100000], Training Loss: -20.862716674804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5628/100000], Training Loss: -21.160110473632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5629/100000], Training Loss: -21.304046630859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5630/100000], Training Loss: -21.236801147460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5631/100000], Training Loss: -21.069580078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5632/100000], Training Loss: -20.964111328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5633/100000], Training Loss: -21.012649536132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5634/100000], Training Loss: -21.157958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5635/100000], Training Loss: -21.282257080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5636/100000], Training Loss: -21.299407958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5637/100000], Training Loss: -21.226470947265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5638/100000], Training Loss: -21.149307250976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5639/100000], Training Loss: -21.137802124023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5640/100000], Training Loss: -21.198699951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5641/100000], Training Loss: -21.274154663085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5642/100000], Training Loss: -21.307235717773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5643/100000], Training Loss: -21.284576416015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5644/100000], Training Loss: -21.239791870117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5645/100000], Training Loss: -21.217666625976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5646/100000], Training Loss: -21.234832763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5647/100000], Training Loss: -21.274444580078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5648/100000], Training Loss: -21.30377197265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5649/100000], Training Loss: -21.304794311523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5650/100000], Training Loss: -21.284698486328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5651/100000], Training Loss: -21.26519775390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5652/100000], Training Loss: -21.263412475585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5653/100000], Training Loss: -21.278839111328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5654/100000], Training Loss: -21.29840087890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5655/100000], Training Loss: -21.308212280273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5656/100000], Training Loss: -21.304061889648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5657/100000], Training Loss: -21.292999267578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5658/100000], Training Loss: -21.285385131835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5659/100000], Training Loss: -21.28704833984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5660/100000], Training Loss: -21.295806884765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5661/100000], Training Loss: -21.305068969726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5662/100000], Training Loss: -21.308807373046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5663/100000], Training Loss: -21.306228637695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5664/100000], Training Loss: -21.300872802734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5665/100000], Training Loss: -21.29730224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5666/100000], Training Loss: -21.298110961914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5667/100000], Training Loss: -21.302322387695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5668/100000], Training Loss: -21.306854248046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5669/100000], Training Loss: -21.309097290039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5670/100000], Training Loss: -21.308380126953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5671/100000], Training Loss: -21.305984497070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5672/100000], Training Loss: -21.303909301757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5673/100000], Training Loss: -21.303665161132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5674/100000], Training Loss: -21.305206298828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5675/100000], Training Loss: -21.307418823242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5676/100000], Training Loss: -21.309127807617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5677/100000], Training Loss: -21.30950927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5678/100000], Training Loss: -21.308792114257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5679/100000], Training Loss: -21.307708740234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5680/100000], Training Loss: -21.3070068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5681/100000], Training Loss: -21.30712890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5682/100000], Training Loss: -21.307968139648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5683/100000], Training Loss: -21.308914184570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5684/100000], Training Loss: -21.3096923828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5685/100000], Training Loss: -21.309890747070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5686/100000], Training Loss: -21.30963134765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5687/100000], Training Loss: -21.309188842773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5688/100000], Training Loss: -21.308837890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5689/100000], Training Loss: -21.308853149414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5690/100000], Training Loss: -21.309127807617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5691/100000], Training Loss: -21.309555053710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5692/100000], Training Loss: -21.309967041015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5693/100000], Training Loss: -21.310226440429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5694/100000], Training Loss: -21.310272216796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5695/100000], Training Loss: -21.310211181640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5696/100000], Training Loss: -21.310012817382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5697/100000], Training Loss: -21.309890747070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5698/100000], Training Loss: -21.309921264648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5699/100000], Training Loss: -21.310043334960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5700/100000], Training Loss: -21.31024169921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5701/100000], Training Loss: -21.310455322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5702/100000], Training Loss: -21.310577392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5703/100000], Training Loss: -21.3106689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5704/100000], Training Loss: -21.310699462890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5705/100000], Training Loss: -21.3106689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5706/100000], Training Loss: -21.310653686523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5707/100000], Training Loss: -21.310638427734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5708/100000], Training Loss: -21.310684204101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5709/100000], Training Loss: -21.31072998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5710/100000], Training Loss: -21.310821533203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5711/100000], Training Loss: -21.3109130859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5712/100000], Training Loss: -21.311004638671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5713/100000], Training Loss: -21.311111450195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5714/100000], Training Loss: -21.311111450195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5715/100000], Training Loss: -21.311187744140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5716/100000], Training Loss: -21.311233520507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5717/100000], Training Loss: -21.311233520507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5718/100000], Training Loss: -21.311248779296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5719/100000], Training Loss: -21.311294555664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5720/100000], Training Loss: -21.311370849609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5721/100000], Training Loss: -21.311355590820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5722/100000], Training Loss: -21.3114013671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5723/100000], Training Loss: -21.311431884765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5724/100000], Training Loss: -21.3115234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5725/100000], Training Loss: -21.31158447265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5726/100000], Training Loss: -21.311614990234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5727/100000], Training Loss: -21.311676025390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5728/100000], Training Loss: -21.311737060546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5729/100000], Training Loss: -21.311767578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5730/100000], Training Loss: -21.311813354492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5731/100000], Training Loss: -21.311859130859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5732/100000], Training Loss: -21.311874389648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5733/100000], Training Loss: -21.311920166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5734/100000], Training Loss: -21.311996459960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5735/100000], Training Loss: -21.311996459960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5736/100000], Training Loss: -21.31201171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5737/100000], Training Loss: -21.312088012695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5738/100000], Training Loss: -21.31207275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5739/100000], Training Loss: -21.3121337890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5740/100000], Training Loss: -21.31219482421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5741/100000], Training Loss: -21.312225341796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5742/100000], Training Loss: -21.312240600585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5743/100000], Training Loss: -21.312301635742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5744/100000], Training Loss: -21.31231689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5745/100000], Training Loss: -21.312408447265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5746/100000], Training Loss: -21.312347412109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5747/100000], Training Loss: -21.312393188476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5748/100000], Training Loss: -21.312484741210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5749/100000], Training Loss: -21.312469482421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5750/100000], Training Loss: -21.3125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5751/100000], Training Loss: -21.312484741210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5752/100000], Training Loss: -21.312545776367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5753/100000], Training Loss: -21.31243896484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5754/100000], Training Loss: -21.312362670898438, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5755/100000], Training Loss: -21.312393188476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5756/100000], Training Loss: -21.312179565429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5757/100000], Training Loss: -21.311965942382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5758/100000], Training Loss: -21.311614990234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5759/100000], Training Loss: -21.311111450195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5760/100000], Training Loss: -21.310348510742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5761/100000], Training Loss: -21.30914306640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5762/100000], Training Loss: -21.307220458984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5763/100000], Training Loss: -21.304290771484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5764/100000], Training Loss: -21.29974365234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5765/100000], Training Loss: -21.292495727539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5766/100000], Training Loss: -21.281021118164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5767/100000], Training Loss: -21.262939453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5768/100000], Training Loss: -21.233551025390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5769/100000], Training Loss: -21.186874389648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5770/100000], Training Loss: -21.110595703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5771/100000], Training Loss: -20.990753173828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5772/100000], Training Loss: -20.796539306640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5773/100000], Training Loss: -20.507308959960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5774/100000], Training Loss: -20.066497802734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5775/100000], Training Loss: -19.513015747070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5776/100000], Training Loss: -18.841384887695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5777/100000], Training Loss: -18.410003662109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5778/100000], Training Loss: -18.3782958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5779/100000], Training Loss: -19.163436889648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5780/100000], Training Loss: -20.295852661132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5781/100000], Training Loss: -21.165435791015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5782/100000], Training Loss: -21.241806030273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5783/100000], Training Loss: -20.69451904296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5784/100000], Training Loss: -20.169570922851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5785/100000], Training Loss: -20.159957885742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5786/100000], Training Loss: -20.696273803710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5787/100000], Training Loss: -21.212554931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5788/100000], Training Loss: -21.27020263671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5789/100000], Training Loss: -20.945327758789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5790/100000], Training Loss: -20.678192138671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5791/100000], Training Loss: -20.78668212890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5792/100000], Training Loss: -21.113784790039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5793/100000], Training Loss: -21.30584716796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5794/100000], Training Loss: -21.20587158203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5795/100000], Training Loss: -21.0045166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5796/100000], Training Loss: -20.9713134765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5797/100000], Training Loss: -21.129714965820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5798/100000], Training Loss: -21.288818359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5799/100000], Training Loss: -21.284286499023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5800/100000], Training Loss: -21.167694091796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5801/100000], Training Loss: -21.107666015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5802/100000], Training Loss: -21.172866821289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5803/100000], Training Loss: -21.27911376953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5804/100000], Training Loss: -21.307083129882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5805/100000], Training Loss: -21.249176025390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5806/100000], Training Loss: -21.19586181640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5807/100000], Training Loss: -21.21319580078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5808/100000], Training Loss: -21.275863647460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5809/100000], Training Loss: -21.310272216796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5810/100000], Training Loss: -21.288787841796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5811/100000], Training Loss: -21.251052856445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5812/100000], Training Loss: -21.246139526367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5813/100000], Training Loss: -21.277603149414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5814/100000], Training Loss: -21.3070068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5815/100000], Training Loss: -21.305709838867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5816/100000], Training Loss: -21.28387451171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5817/100000], Training Loss: -21.271575927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5818/100000], Training Loss: -21.282760620117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5819/100000], Training Loss: -21.302886962890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5820/100000], Training Loss: -21.310699462890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5821/100000], Training Loss: -21.301864624023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5822/100000], Training Loss: -21.290374755859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5823/100000], Training Loss: -21.290115356445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5824/100000], Training Loss: -21.300430297851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5825/100000], Training Loss: -21.309768676757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5826/100000], Training Loss: -21.309677124023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5827/100000], Training Loss: -21.302963256835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5828/100000], Training Loss: -21.2984619140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5829/100000], Training Loss: -21.301025390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5830/100000], Training Loss: -21.307357788085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5831/100000], Training Loss: -21.31109619140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5832/100000], Training Loss: -21.3095703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5833/100000], Training Loss: -21.305801391601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5834/100000], Training Loss: -21.304244995117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5835/100000], Training Loss: -21.306427001953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5836/100000], Training Loss: -21.30987548828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5837/100000], Training Loss: -21.311447143554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5838/100000], Training Loss: -21.310348510742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5839/100000], Training Loss: -21.308319091796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5840/100000], Training Loss: -21.307693481445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5841/100000], Training Loss: -21.309005737304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5842/100000], Training Loss: -21.310836791992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5843/100000], Training Loss: -21.311660766601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5844/100000], Training Loss: -21.311126708984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5845/100000], Training Loss: -21.310089111328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5846/100000], Training Loss: -21.309707641601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5847/100000], Training Loss: -21.31036376953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5848/100000], Training Loss: -21.31134033203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5849/100000], Training Loss: -21.311904907226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5850/100000], Training Loss: -21.311798095703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5851/100000], Training Loss: -21.311279296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5852/100000], Training Loss: -21.310943603515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5853/100000], Training Loss: -21.3111572265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5854/100000], Training Loss: -21.31170654296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5855/100000], Training Loss: -21.31219482421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5856/100000], Training Loss: -21.312240600585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5857/100000], Training Loss: -21.312042236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5858/100000], Training Loss: -21.31182861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5859/100000], Training Loss: -21.311767578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5860/100000], Training Loss: -21.31207275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5861/100000], Training Loss: -21.312301635742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5862/100000], Training Loss: -21.312515258789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5863/100000], Training Loss: -21.312545776367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5864/100000], Training Loss: -21.31243896484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5865/100000], Training Loss: -21.3123779296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5866/100000], Training Loss: -21.31243896484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5867/100000], Training Loss: -21.312530517578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5868/100000], Training Loss: -21.312728881835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5869/100000], Training Loss: -21.312835693359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5870/100000], Training Loss: -21.312835693359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5871/100000], Training Loss: -21.312820434570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5872/100000], Training Loss: -21.312774658203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5873/100000], Training Loss: -21.312835693359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5874/100000], Training Loss: -21.312896728515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5875/100000], Training Loss: -21.313003540039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5876/100000], Training Loss: -21.313079833984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5877/100000], Training Loss: -21.313125610351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5878/100000], Training Loss: -21.313156127929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5879/100000], Training Loss: -21.313186645507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5880/100000], Training Loss: -21.313232421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5881/100000], Training Loss: -21.313262939453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5882/100000], Training Loss: -21.313339233398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5883/100000], Training Loss: -21.313385009765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5884/100000], Training Loss: -21.313446044921875, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5885/100000], Training Loss: -21.313507080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5886/100000], Training Loss: -21.313507080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5887/100000], Training Loss: -21.313583374023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5888/100000], Training Loss: -21.313583374023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5889/100000], Training Loss: -21.313613891601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5890/100000], Training Loss: -21.31365966796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5891/100000], Training Loss: -21.313720703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5892/100000], Training Loss: -21.313766479492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5893/100000], Training Loss: -21.313796997070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5894/100000], Training Loss: -21.3138427734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5895/100000], Training Loss: -21.313858032226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5896/100000], Training Loss: -21.313796997070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5897/100000], Training Loss: -21.313796997070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5898/100000], Training Loss: -21.313812255859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5899/100000], Training Loss: -21.313735961914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5900/100000], Training Loss: -21.313644409179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5901/100000], Training Loss: -21.3134765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5902/100000], Training Loss: -21.313140869140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5903/100000], Training Loss: -21.3126220703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5904/100000], Training Loss: -21.31170654296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5905/100000], Training Loss: -21.310211181640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5906/100000], Training Loss: -21.3077392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5907/100000], Training Loss: -21.303512573242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5908/100000], Training Loss: -21.2965087890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5909/100000], Training Loss: -21.284698486328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5910/100000], Training Loss: -21.264450073242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5911/100000], Training Loss: -21.230392456054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5912/100000], Training Loss: -21.17169189453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5913/100000], Training Loss: -21.073959350585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5914/100000], Training Loss: -20.907989501953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5915/100000], Training Loss: -20.64935302734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5916/100000], Training Loss: -20.245315551757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5917/100000], Training Loss: -19.741561889648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5918/100000], Training Loss: -19.17315673828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5919/100000], Training Loss: -18.943817138671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5920/100000], Training Loss: -19.178359985351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5921/100000], Training Loss: -20.020660400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5922/100000], Training Loss: -20.8074951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5923/100000], Training Loss: -21.078353881835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5924/100000], Training Loss: -20.837936401367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5925/100000], Training Loss: -20.55035400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5926/100000], Training Loss: -20.60101318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5927/100000], Training Loss: -20.865859985351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5928/100000], Training Loss: -21.010055541992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5929/100000], Training Loss: -20.9090576171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5930/100000], Training Loss: -20.82379150390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5931/100000], Training Loss: -20.964309692382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5932/100000], Training Loss: -21.198150634765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5933/100000], Training Loss: -21.238494873046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5934/100000], Training Loss: -21.061172485351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [5935/100000], Training Loss: -20.935760498046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5936/100000], Training Loss: -21.038330078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5937/100000], Training Loss: -21.241043090820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5938/100000], Training Loss: -21.303817749023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5939/100000], Training Loss: -21.20513916015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5940/100000], Training Loss: -21.121734619140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5941/100000], Training Loss: -21.1595458984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5942/100000], Training Loss: -21.241989135742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5943/100000], Training Loss: -21.2587890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5944/100000], Training Loss: -21.226654052734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5945/100000], Training Loss: -21.226608276367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5946/100000], Training Loss: -21.26898193359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5947/100000], Training Loss: -21.288436889648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5948/100000], Training Loss: -21.258026123046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5949/100000], Training Loss: -21.229949951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5950/100000], Training Loss: -21.250701904296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5951/100000], Training Loss: -21.29669189453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5952/100000], Training Loss: -21.311721801757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5953/100000], Training Loss: -21.28778076171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5954/100000], Training Loss: -21.26611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5955/100000], Training Loss: -21.27398681640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5956/100000], Training Loss: -21.295150756835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5957/100000], Training Loss: -21.3017578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5958/100000], Training Loss: -21.29425048828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5959/100000], Training Loss: -21.291946411132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5960/100000], Training Loss: -21.3006591796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5961/100000], Training Loss: -21.307205200195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5962/100000], Training Loss: -21.30218505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5963/100000], Training Loss: -21.29412841796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5964/100000], Training Loss: -21.295654296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5965/100000], Training Loss: -21.305740356445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5966/100000], Training Loss: -21.312698364257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [5967/100000], Training Loss: -21.310226440429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5968/100000], Training Loss: -21.3043212890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5969/100000], Training Loss: -21.3031005859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5970/100000], Training Loss: -21.306930541992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [5971/100000], Training Loss: -21.309890747070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5972/100000], Training Loss: -21.309295654296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5973/100000], Training Loss: -21.30792236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5974/100000], Training Loss: -21.309158325195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5975/100000], Training Loss: -21.311843872070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [5976/100000], Training Loss: -21.312728881835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5977/100000], Training Loss: -21.311004638671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5978/100000], Training Loss: -21.309097290039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [5979/100000], Training Loss: -21.309478759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5980/100000], Training Loss: -21.3115234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5981/100000], Training Loss: -21.313095092773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5982/100000], Training Loss: -21.31298828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5983/100000], Training Loss: -21.312164306640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5984/100000], Training Loss: -21.312042236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5985/100000], Training Loss: -21.312667846679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5986/100000], Training Loss: -21.313156127929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5987/100000], Training Loss: -21.312896728515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5988/100000], Training Loss: -21.312423706054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5989/100000], Training Loss: -21.312484741210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [5990/100000], Training Loss: -21.313201904296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5991/100000], Training Loss: -21.313888549804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [5992/100000], Training Loss: -21.313995361328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [5993/100000], Training Loss: -21.313690185546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5994/100000], Training Loss: -21.313446044921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [5995/100000], Training Loss: -21.31353759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [5996/100000], Training Loss: -21.313827514648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [5997/100000], Training Loss: -21.31396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5998/100000], Training Loss: -21.31396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [5999/100000], Training Loss: -21.313888549804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6000/100000], Training Loss: -21.313980102539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6001/100000], Training Loss: -21.314254760742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6002/100000], Training Loss: -21.314453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6003/100000], Training Loss: -21.314498901367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6004/100000], Training Loss: -21.314468383789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6005/100000], Training Loss: -21.314407348632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6006/100000], Training Loss: -21.314468383789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6007/100000], Training Loss: -21.3145751953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6008/100000], Training Loss: -21.314666748046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6009/100000], Training Loss: -21.31463623046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6010/100000], Training Loss: -21.31463623046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6011/100000], Training Loss: -21.314666748046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6012/100000], Training Loss: -21.314773559570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6013/100000], Training Loss: -21.314895629882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6014/100000], Training Loss: -21.314971923828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6015/100000], Training Loss: -21.314987182617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6016/100000], Training Loss: -21.31500244140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6017/100000], Training Loss: -21.315032958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6018/100000], Training Loss: -21.31512451171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6019/100000], Training Loss: -21.315109252929688, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6020/100000], Training Loss: -21.315216064453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6021/100000], Training Loss: -21.315231323242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6022/100000], Training Loss: -21.315277099609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6023/100000], Training Loss: -21.315292358398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6024/100000], Training Loss: -21.31536865234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6025/100000], Training Loss: -21.315399169921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6026/100000], Training Loss: -21.31549072265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6027/100000], Training Loss: -21.31549072265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6028/100000], Training Loss: -21.315536499023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6029/100000], Training Loss: -21.315597534179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6030/100000], Training Loss: -21.315597534179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6031/100000], Training Loss: -21.315643310546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6032/100000], Training Loss: -21.315750122070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6033/100000], Training Loss: -21.315765380859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6034/100000], Training Loss: -21.3157958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6035/100000], Training Loss: -21.3157958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6036/100000], Training Loss: -21.315826416015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6037/100000], Training Loss: -21.315872192382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6038/100000], Training Loss: -21.315948486328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6039/100000], Training Loss: -21.315994262695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6040/100000], Training Loss: -21.31597900390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6041/100000], Training Loss: -21.3160400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6042/100000], Training Loss: -21.316070556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6043/100000], Training Loss: -21.316131591796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6044/100000], Training Loss: -21.316162109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6045/100000], Training Loss: -21.316162109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6046/100000], Training Loss: -21.316238403320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6047/100000], Training Loss: -21.316238403320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6048/100000], Training Loss: -21.31622314453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6049/100000], Training Loss: -21.31622314453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6050/100000], Training Loss: -21.316192626953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6051/100000], Training Loss: -21.31610107421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6052/100000], Training Loss: -21.31597900390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6053/100000], Training Loss: -21.315780639648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6054/100000], Training Loss: -21.315444946289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6055/100000], Training Loss: -21.315017700195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6056/100000], Training Loss: -21.31427001953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6057/100000], Training Loss: -21.313125610351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6058/100000], Training Loss: -21.311309814453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6059/100000], Training Loss: -21.3084716796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6060/100000], Training Loss: -21.303939819335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6061/100000], Training Loss: -21.2967529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6062/100000], Training Loss: -21.285247802734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6063/100000], Training Loss: -21.26678466796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6064/100000], Training Loss: -21.236587524414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6065/100000], Training Loss: -21.188140869140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6066/100000], Training Loss: -21.108291625976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6067/100000], Training Loss: -20.982192993164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6068/100000], Training Loss: -20.776962280273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6069/100000], Training Loss: -20.472793579101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6070/100000], Training Loss: -20.012832641601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6071/100000], Training Loss: -19.455596923828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6072/100000], Training Loss: -18.816314697265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6073/100000], Training Loss: -18.501327514648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6074/100000], Training Loss: -18.65545654296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6075/100000], Training Loss: -19.56646728515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6076/100000], Training Loss: -20.644317626953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6077/100000], Training Loss: -21.268142700195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6078/100000], Training Loss: -21.120635986328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6079/100000], Training Loss: -20.538665771484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6080/100000], Training Loss: -20.16741943359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6081/100000], Training Loss: -20.331085205078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6082/100000], Training Loss: -20.883087158203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6083/100000], Training Loss: -21.261947631835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6084/100000], Training Loss: -21.187423706054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6085/100000], Training Loss: -20.871307373046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6086/100000], Training Loss: -20.715560913085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6087/100000], Training Loss: -20.899505615234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6088/100000], Training Loss: -21.187255859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6089/100000], Training Loss: -21.284286499023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6090/100000], Training Loss: -21.149200439453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6091/100000], Training Loss: -20.998703002929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6092/100000], Training Loss: -21.029541015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6093/100000], Training Loss: -21.189834594726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6094/100000], Training Loss: -21.296066284179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6095/100000], Training Loss: -21.253173828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6096/100000], Training Loss: -21.151214599609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6097/100000], Training Loss: -21.1317138671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6098/100000], Training Loss: -21.21282958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6099/100000], Training Loss: -21.297576904296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6100/100000], Training Loss: -21.298080444335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6101/100000], Training Loss: -21.237014770507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6102/100000], Training Loss: -21.2020263671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6103/100000], Training Loss: -21.233810424804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6104/100000], Training Loss: -21.29278564453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6105/100000], Training Loss: -21.314056396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6106/100000], Training Loss: -21.286270141601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6107/100000], Training Loss: -21.253265380859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6108/100000], Training Loss: -21.255477905273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6109/100000], Training Loss: -21.28790283203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6110/100000], Training Loss: -21.313095092773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6111/100000], Training Loss: -21.308685302734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6112/100000], Training Loss: -21.287872314453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6113/100000], Training Loss: -21.278106689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6114/100000], Training Loss: -21.289718627929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6115/100000], Training Loss: -21.30792236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6116/100000], Training Loss: -21.314163208007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6117/100000], Training Loss: -21.305984497070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6118/100000], Training Loss: -21.296218872070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6119/100000], Training Loss: -21.296646118164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6120/100000], Training Loss: -21.305877685546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6121/100000], Training Loss: -21.31365966796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6122/100000], Training Loss: -21.313262939453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6123/100000], Training Loss: -21.30743408203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6124/100000], Training Loss: -21.303939819335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6125/100000], Training Loss: -21.30657958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6126/100000], Training Loss: -21.312255859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6127/100000], Training Loss: -21.315353393554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6128/100000], Training Loss: -21.313720703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6129/100000], Training Loss: -21.310226440429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6130/100000], Training Loss: -21.308990478515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6131/100000], Training Loss: -21.311080932617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6132/100000], Training Loss: -21.314361572265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6133/100000], Training Loss: -21.315826416015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6134/100000], Training Loss: -21.314773559570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6135/100000], Training Loss: -21.312850952148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6136/100000], Training Loss: -21.312271118164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6137/100000], Training Loss: -21.313385009765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6138/100000], Training Loss: -21.315139770507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6139/100000], Training Loss: -21.315963745117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6140/100000], Training Loss: -21.315521240234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6141/100000], Training Loss: -21.314605712890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6142/100000], Training Loss: -21.314239501953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6143/100000], Training Loss: -21.31475830078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6144/100000], Training Loss: -21.315597534179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6145/100000], Training Loss: -21.316177368164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6146/100000], Training Loss: -21.3160400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6147/100000], Training Loss: -21.31561279296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6148/100000], Training Loss: -21.315322875976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6149/100000], Training Loss: -21.315505981445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6150/100000], Training Loss: -21.315963745117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6151/100000], Training Loss: -21.31640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6152/100000], Training Loss: -21.316513061523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6153/100000], Training Loss: -21.3162841796875, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6154/100000], Training Loss: -21.316085815429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6155/100000], Training Loss: -21.316070556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6156/100000], Training Loss: -21.316314697265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6157/100000], Training Loss: -21.316558837890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6158/100000], Training Loss: -21.316741943359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6159/100000], Training Loss: -21.3167724609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6160/100000], Training Loss: -21.316619873046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6161/100000], Training Loss: -21.316604614257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6162/100000], Training Loss: -21.316635131835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6163/100000], Training Loss: -21.31671142578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6164/100000], Training Loss: -21.316864013671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6165/100000], Training Loss: -21.317001342773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6166/100000], Training Loss: -21.3170166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6167/100000], Training Loss: -21.31695556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6168/100000], Training Loss: -21.316925048828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6169/100000], Training Loss: -21.3170166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6170/100000], Training Loss: -21.31707763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6171/100000], Training Loss: -21.317184448242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6172/100000], Training Loss: -21.317245483398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6173/100000], Training Loss: -21.317291259765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6174/100000], Training Loss: -21.317291259765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6175/100000], Training Loss: -21.317291259765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6176/100000], Training Loss: -21.31732177734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6177/100000], Training Loss: -21.3173828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6178/100000], Training Loss: -21.317413330078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6179/100000], Training Loss: -21.317520141601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6180/100000], Training Loss: -21.317550659179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6181/100000], Training Loss: -21.317535400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6182/100000], Training Loss: -21.317550659179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6183/100000], Training Loss: -21.317626953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6184/100000], Training Loss: -21.317642211914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6185/100000], Training Loss: -21.31768798828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6186/100000], Training Loss: -21.317703247070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6187/100000], Training Loss: -21.317764282226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6188/100000], Training Loss: -21.317840576171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6189/100000], Training Loss: -21.317855834960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6190/100000], Training Loss: -21.317886352539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6191/100000], Training Loss: -21.317901611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6192/100000], Training Loss: -21.317947387695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6193/100000], Training Loss: -21.317977905273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6194/100000], Training Loss: -21.31805419921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6195/100000], Training Loss: -21.31805419921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6196/100000], Training Loss: -21.318084716796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6197/100000], Training Loss: -21.318099975585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6198/100000], Training Loss: -21.31817626953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6199/100000], Training Loss: -21.318222045898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6200/100000], Training Loss: -21.318206787109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6201/100000], Training Loss: -21.3182373046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6202/100000], Training Loss: -21.318328857421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6203/100000], Training Loss: -21.318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6204/100000], Training Loss: -21.318389892578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6205/100000], Training Loss: -21.318389892578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6206/100000], Training Loss: -21.318435668945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6207/100000], Training Loss: -21.31842041015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6208/100000], Training Loss: -21.31842041015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6209/100000], Training Loss: -21.3184814453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6210/100000], Training Loss: -21.318496704101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6211/100000], Training Loss: -21.3184814453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6212/100000], Training Loss: -21.318466186523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6213/100000], Training Loss: -21.318466186523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6214/100000], Training Loss: -21.318374633789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6215/100000], Training Loss: -21.318252563476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6216/100000], Training Loss: -21.31805419921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6217/100000], Training Loss: -21.317672729492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6218/100000], Training Loss: -21.31707763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6219/100000], Training Loss: -21.316177368164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6220/100000], Training Loss: -21.314727783203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6221/100000], Training Loss: -21.312347412109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6222/100000], Training Loss: -21.308425903320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6223/100000], Training Loss: -21.302032470703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6224/100000], Training Loss: -21.291549682617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6225/100000], Training Loss: -21.274276733398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6226/100000], Training Loss: -21.245651245117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6227/100000], Training Loss: -21.198257446289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6228/100000], Training Loss: -21.120574951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6229/100000], Training Loss: -20.995101928710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6230/100000], Training Loss: -20.799911499023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6231/100000], Training Loss: -20.515029907226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6232/100000], Training Loss: -20.147415161132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6233/100000], Training Loss: -19.774185180664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6234/100000], Training Loss: -19.574417114257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6235/100000], Training Loss: -19.760513305664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6236/100000], Training Loss: -20.324920654296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6237/100000], Training Loss: -20.941314697265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6238/100000], Training Loss: -21.187271118164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6239/100000], Training Loss: -20.9971923828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6240/100000], Training Loss: -20.67718505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6241/100000], Training Loss: -20.621932983398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6242/100000], Training Loss: -20.896743774414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6243/100000], Training Loss: -21.230224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6244/100000], Training Loss: -21.29571533203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6245/100000], Training Loss: -21.097640991210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6246/100000], Training Loss: -20.920181274414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6247/100000], Training Loss: -20.96807861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6248/100000], Training Loss: -21.1710205078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6249/100000], Training Loss: -21.284332275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6250/100000], Training Loss: -21.2193603515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6251/100000], Training Loss: -21.112838745117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6252/100000], Training Loss: -21.12445068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6253/100000], Training Loss: -21.239761352539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6254/100000], Training Loss: -21.314865112304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6255/100000], Training Loss: -21.276229858398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6256/100000], Training Loss: -21.198928833007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6257/100000], Training Loss: -21.189727783203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6258/100000], Training Loss: -21.252166748046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6259/100000], Training Loss: -21.302398681640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6260/100000], Training Loss: -21.2884521484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6261/100000], Training Loss: -21.248611450195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6262/100000], Training Loss: -21.245376586914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6263/100000], Training Loss: -21.284378051757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6264/100000], Training Loss: -21.316070556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6265/100000], Training Loss: -21.308120727539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6266/100000], Training Loss: -21.281051635742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6267/100000], Training Loss: -21.27362060546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6268/100000], Training Loss: -21.29278564453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6269/100000], Training Loss: -21.311416625976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6270/100000], Training Loss: -21.308731079101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6271/100000], Training Loss: -21.293701171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6272/100000], Training Loss: -21.288955688476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6273/100000], Training Loss: -21.300521850585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6274/100000], Training Loss: -21.314102172851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6275/100000], Training Loss: -21.315536499023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6276/100000], Training Loss: -21.3076171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6277/100000], Training Loss: -21.303070068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6278/100000], Training Loss: -21.307937622070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6279/100000], Training Loss: -21.315826416015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6280/100000], Training Loss: -21.317718505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6281/100000], Training Loss: -21.31317138671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6282/100000], Training Loss: -21.308868408203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6283/100000], Training Loss: -21.309951782226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6284/100000], Training Loss: -21.314453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6285/100000], Training Loss: -21.316940307617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6286/100000], Training Loss: -21.315383911132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6287/100000], Training Loss: -21.31256103515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6288/100000], Training Loss: -21.312210083007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6289/100000], Training Loss: -21.3145751953125, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6290/100000], Training Loss: -21.317001342773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6291/100000], Training Loss: -21.317230224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6292/100000], Training Loss: -21.315811157226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6293/100000], Training Loss: -21.3148193359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6294/100000], Training Loss: -21.31561279296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6295/100000], Training Loss: -21.317214965820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6296/100000], Training Loss: -21.318084716796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6297/100000], Training Loss: -21.317718505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6298/100000], Training Loss: -21.31689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6299/100000], Training Loss: -21.316696166992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6300/100000], Training Loss: -21.31732177734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6301/100000], Training Loss: -21.318115234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6302/100000], Training Loss: -21.318267822265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6303/100000], Training Loss: -21.317855834960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6304/100000], Training Loss: -21.317352294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6305/100000], Training Loss: -21.31719970703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6306/100000], Training Loss: -21.317337036132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6307/100000], Training Loss: -21.317459106445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6308/100000], Training Loss: -21.317047119140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6309/100000], Training Loss: -21.31610107421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6310/100000], Training Loss: -21.314971923828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6311/100000], Training Loss: -21.313751220703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6312/100000], Training Loss: -21.312149047851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6313/100000], Training Loss: -21.30975341796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6314/100000], Training Loss: -21.305984497070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6315/100000], Training Loss: -21.300277709960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6316/100000], Training Loss: -21.291946411132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6317/100000], Training Loss: -21.2799072265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6318/100000], Training Loss: -21.261825561523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6319/100000], Training Loss: -21.235137939453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6320/100000], Training Loss: -21.194305419921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6321/100000], Training Loss: -21.134246826171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6322/100000], Training Loss: -21.042648315429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6323/100000], Training Loss: -20.912185668945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6324/100000], Training Loss: -20.71923828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6325/100000], Training Loss: -20.46826171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6326/100000], Training Loss: -20.133834838867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6327/100000], Training Loss: -19.799331665039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6328/100000], Training Loss: -19.489349365234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6329/100000], Training Loss: -19.44390869140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6330/100000], Training Loss: -19.674667358398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6331/100000], Training Loss: -20.253158569335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6332/100000], Training Loss: -20.871978759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6333/100000], Training Loss: -21.263992309570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6334/100000], Training Loss: -21.275192260742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6335/100000], Training Loss: -21.006393432617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6336/100000], Training Loss: -20.71160888671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6337/100000], Training Loss: -20.602615356445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6338/100000], Training Loss: -20.768814086914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6339/100000], Training Loss: -21.0606689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6340/100000], Training Loss: -21.279434204101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6341/100000], Training Loss: -21.297164916992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6342/100000], Training Loss: -21.158340454101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6343/100000], Training Loss: -21.014144897460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6344/100000], Training Loss: -20.985366821289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6345/100000], Training Loss: -21.09259033203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6346/100000], Training Loss: -21.23712158203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6347/100000], Training Loss: -21.313705444335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6348/100000], Training Loss: -21.2855224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6349/100000], Training Loss: -21.203460693359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6350/100000], Training Loss: -21.150711059570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6351/100000], Training Loss: -21.16851806640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6352/100000], Training Loss: -21.238540649414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6353/100000], Training Loss: -21.301300048828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6354/100000], Training Loss: -21.31536865234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6355/100000], Training Loss: -21.284011840820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6356/100000], Training Loss: -21.244247436523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6357/100000], Training Loss: -21.232437133789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6358/100000], Training Loss: -21.255218505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6359/100000], Training Loss: -21.29254150390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6360/100000], Training Loss: -21.316055297851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6361/100000], Training Loss: -21.313446044921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6362/100000], Training Loss: -21.293899536132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6363/100000], Training Loss: -21.277069091796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6364/100000], Training Loss: -21.276687622070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6365/100000], Training Loss: -21.291412353515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6366/100000], Training Loss: -21.309371948242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6367/100000], Training Loss: -21.318466186523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6368/100000], Training Loss: -21.315093994140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6369/100000], Training Loss: -21.305099487304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6370/100000], Training Loss: -21.29754638671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6371/100000], Training Loss: -21.298126220703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6372/100000], Training Loss: -21.305587768554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6373/100000], Training Loss: -21.314407348632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6374/100000], Training Loss: -21.318939208984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6375/100000], Training Loss: -21.317672729492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6376/100000], Training Loss: -21.313064575195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6377/100000], Training Loss: -21.309066772460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6378/100000], Training Loss: -21.30859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6379/100000], Training Loss: -21.3116455078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6380/100000], Training Loss: -21.315948486328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6381/100000], Training Loss: -21.318923950195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6382/100000], Training Loss: -21.3193359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6383/100000], Training Loss: -21.317596435546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6384/100000], Training Loss: -21.315383911132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6385/100000], Training Loss: -21.314315795898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6386/100000], Training Loss: -21.31494140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6387/100000], Training Loss: -21.316665649414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6388/100000], Training Loss: -21.318572998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6389/100000], Training Loss: -21.319610595703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6390/100000], Training Loss: -21.319595336914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6391/100000], Training Loss: -21.31878662109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6392/100000], Training Loss: -21.317901611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6393/100000], Training Loss: -21.317459106445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6394/100000], Training Loss: -21.317718505859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6395/100000], Training Loss: -21.318496704101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6396/100000], Training Loss: -21.3193359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6397/100000], Training Loss: -21.31988525390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6398/100000], Training Loss: -21.320037841796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6399/100000], Training Loss: -21.319839477539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6400/100000], Training Loss: -21.319488525390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6401/100000], Training Loss: -21.319198608398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6402/100000], Training Loss: -21.319137573242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6403/100000], Training Loss: -21.319351196289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6404/100000], Training Loss: -21.3197021484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6405/100000], Training Loss: -21.320068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6406/100000], Training Loss: -21.320266723632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6407/100000], Training Loss: -21.32037353515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6408/100000], Training Loss: -21.320327758789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6409/100000], Training Loss: -21.320236206054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6410/100000], Training Loss: -21.32012939453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6411/100000], Training Loss: -21.320083618164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6412/100000], Training Loss: -21.320175170898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6413/100000], Training Loss: -21.320281982421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6414/100000], Training Loss: -21.320388793945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6415/100000], Training Loss: -21.320526123046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6416/100000], Training Loss: -21.320648193359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6417/100000], Training Loss: -21.320724487304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6418/100000], Training Loss: -21.320755004882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6419/100000], Training Loss: -21.320770263671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6420/100000], Training Loss: -21.320724487304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6421/100000], Training Loss: -21.32073974609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6422/100000], Training Loss: -21.32073974609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6423/100000], Training Loss: -21.320770263671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6424/100000], Training Loss: -21.320816040039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6425/100000], Training Loss: -21.320938110351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6426/100000], Training Loss: -21.32098388671875, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6427/100000], Training Loss: -21.321060180664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6428/100000], Training Loss: -21.321121215820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6429/100000], Training Loss: -21.321151733398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6430/100000], Training Loss: -21.321212768554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6431/100000], Training Loss: -21.321212768554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6432/100000], Training Loss: -21.321243286132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6433/100000], Training Loss: -21.32122802734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6434/100000], Training Loss: -21.321319580078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6435/100000], Training Loss: -21.3212890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6436/100000], Training Loss: -21.321334838867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6437/100000], Training Loss: -21.321380615234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6438/100000], Training Loss: -21.321426391601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6439/100000], Training Loss: -21.321441650390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6440/100000], Training Loss: -21.321517944335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6441/100000], Training Loss: -21.321517944335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6442/100000], Training Loss: -21.321609497070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6443/100000], Training Loss: -21.321624755859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6444/100000], Training Loss: -21.321670532226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6445/100000], Training Loss: -21.321670532226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6446/100000], Training Loss: -21.321762084960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6447/100000], Training Loss: -21.32177734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6448/100000], Training Loss: -21.321807861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6449/100000], Training Loss: -21.321807861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6450/100000], Training Loss: -21.321868896484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6451/100000], Training Loss: -21.3218994140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6452/100000], Training Loss: -21.321945190429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6453/100000], Training Loss: -21.321929931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6454/100000], Training Loss: -21.322021484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6455/100000], Training Loss: -21.322052001953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6456/100000], Training Loss: -21.32208251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6457/100000], Training Loss: -21.322128295898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6458/100000], Training Loss: -21.322113037109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6459/100000], Training Loss: -21.322174072265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6460/100000], Training Loss: -21.322219848632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6461/100000], Training Loss: -21.322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6462/100000], Training Loss: -21.32232666015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6463/100000], Training Loss: -21.322341918945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6464/100000], Training Loss: -21.3223876953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6465/100000], Training Loss: -21.322372436523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6466/100000], Training Loss: -21.322433471679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6467/100000], Training Loss: -21.32244873046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6468/100000], Training Loss: -21.322479248046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6469/100000], Training Loss: -21.322509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6470/100000], Training Loss: -21.322540283203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6471/100000], Training Loss: -21.322616577148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6472/100000], Training Loss: -21.322647094726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6473/100000], Training Loss: -21.322616577148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6474/100000], Training Loss: -21.3226318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6475/100000], Training Loss: -21.322662353515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6476/100000], Training Loss: -21.322677612304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6477/100000], Training Loss: -21.322616577148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6478/100000], Training Loss: -21.3226318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6479/100000], Training Loss: -21.322509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6480/100000], Training Loss: -21.322311401367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6481/100000], Training Loss: -21.322006225585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6482/100000], Training Loss: -21.321502685546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6483/100000], Training Loss: -21.320632934570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6484/100000], Training Loss: -21.319137573242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6485/100000], Training Loss: -21.316558837890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6486/100000], Training Loss: -21.312240600585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6487/100000], Training Loss: -21.304794311523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6488/100000], Training Loss: -21.291961669921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6489/100000], Training Loss: -21.269744873046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6490/100000], Training Loss: -21.230850219726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6491/100000], Training Loss: -21.1636962890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6492/100000], Training Loss: -21.04486083984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6493/100000], Training Loss: -20.84320068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6494/100000], Training Loss: -20.493072509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6495/100000], Training Loss: -19.942367553710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6496/100000], Training Loss: -19.07861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6497/100000], Training Loss: -18.048782348632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6498/100000], Training Loss: -17.004852294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6499/100000], Training Loss: -16.9442138671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6500/100000], Training Loss: -18.04742431640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6501/100000], Training Loss: -20.054702758789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6502/100000], Training Loss: -21.265029907226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6503/100000], Training Loss: -20.909225463867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6504/100000], Training Loss: -19.793731689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6505/100000], Training Loss: -19.320968627929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6506/100000], Training Loss: -20.1197509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6507/100000], Training Loss: -21.128005981445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6508/100000], Training Loss: -21.222900390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6509/100000], Training Loss: -20.559005737304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6510/100000], Training Loss: -20.202880859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6511/100000], Training Loss: -20.673309326171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6512/100000], Training Loss: -21.250152587890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6513/100000], Training Loss: -21.20721435546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6514/100000], Training Loss: -20.785110473632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6515/100000], Training Loss: -20.693695068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6516/100000], Training Loss: -21.0614013671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6517/100000], Training Loss: -21.315780639648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6518/100000], Training Loss: -21.156585693359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6519/100000], Training Loss: -20.931976318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6520/100000], Training Loss: -21.011703491210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6521/100000], Training Loss: -21.256576538085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6522/100000], Training Loss: -21.300247192382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6523/100000], Training Loss: -21.1412353515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6524/100000], Training Loss: -21.075927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6525/100000], Training Loss: -21.201400756835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6526/100000], Training Loss: -21.315887451171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6527/100000], Training Loss: -21.267440795898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6528/100000], Training Loss: -21.172683715820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6529/100000], Training Loss: -21.194183349609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6530/100000], Training Loss: -21.289077758789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6531/100000], Training Loss: -21.314773559570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6532/100000], Training Loss: -21.255813598632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6533/100000], Training Loss: -21.22314453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6534/100000], Training Loss: -21.267654418945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6535/100000], Training Loss: -21.316238403320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6536/100000], Training Loss: -21.303619384765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6537/100000], Training Loss: -21.265304565429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6538/100000], Training Loss: -21.2664794921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6539/100000], Training Loss: -21.302780151367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6540/100000], Training Loss: -21.319183349609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6541/100000], Training Loss: -21.29962158203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6542/100000], Training Loss: -21.282257080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6543/100000], Training Loss: -21.294097900390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6544/100000], Training Loss: -21.315658569335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6545/100000], Training Loss: -21.316879272460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6546/100000], Training Loss: -21.30206298828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6547/100000], Training Loss: -21.297042846679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6548/100000], Training Loss: -21.308578491210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6549/100000], Training Loss: -21.319183349609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6550/100000], Training Loss: -21.315704345703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6551/100000], Training Loss: -21.306915283203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6552/100000], Training Loss: -21.3070068359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6553/100000], Training Loss: -21.31524658203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6554/100000], Training Loss: -21.319854736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6555/100000], Training Loss: -21.316131591796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6556/100000], Training Loss: -21.311447143554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6557/100000], Training Loss: -21.31292724609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6558/100000], Training Loss: -21.31805419921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6559/100000], Training Loss: -21.319961547851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6560/100000], Training Loss: -21.317214965820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6561/100000], Training Loss: -21.314788818359375, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6562/100000], Training Loss: -21.316253662109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6563/100000], Training Loss: -21.319320678710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6564/100000], Training Loss: -21.320114135742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6565/100000], Training Loss: -21.31829833984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6566/100000], Training Loss: -21.317108154296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6567/100000], Training Loss: -21.318099975585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6568/100000], Training Loss: -21.3199462890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6569/100000], Training Loss: -21.320358276367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6570/100000], Training Loss: -21.31927490234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6571/100000], Training Loss: -21.318603515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6572/100000], Training Loss: -21.319137573242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6573/100000], Training Loss: -21.320281982421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6574/100000], Training Loss: -21.320602416992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6575/100000], Training Loss: -21.32000732421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6576/100000], Training Loss: -21.319580078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6577/100000], Training Loss: -21.319839477539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6578/100000], Training Loss: -21.320556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6579/100000], Training Loss: -21.320816040039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6580/100000], Training Loss: -21.320526123046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6581/100000], Training Loss: -21.320236206054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6582/100000], Training Loss: -21.320343017578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6583/100000], Training Loss: -21.320755004882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6584/100000], Training Loss: -21.320938110351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6585/100000], Training Loss: -21.3209228515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6586/100000], Training Loss: -21.320709228515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6587/100000], Training Loss: -21.320755004882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6588/100000], Training Loss: -21.3209228515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6589/100000], Training Loss: -21.321197509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6590/100000], Training Loss: -21.321197509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6591/100000], Training Loss: -21.321136474609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6592/100000], Training Loss: -21.321044921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6593/100000], Training Loss: -21.321197509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6594/100000], Training Loss: -21.3212890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6595/100000], Training Loss: -21.321441650390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6596/100000], Training Loss: -21.321395874023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6597/100000], Training Loss: -21.321365356445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6598/100000], Training Loss: -21.321426391601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6599/100000], Training Loss: -21.321533203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6600/100000], Training Loss: -21.321624755859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6601/100000], Training Loss: -21.321670532226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6602/100000], Training Loss: -21.321640014648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6603/100000], Training Loss: -21.3216552734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6604/100000], Training Loss: -21.32171630859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6605/100000], Training Loss: -21.321792602539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6606/100000], Training Loss: -21.321853637695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6607/100000], Training Loss: -21.321884155273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6608/100000], Training Loss: -21.321868896484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6609/100000], Training Loss: -21.321929931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6610/100000], Training Loss: -21.321975708007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6611/100000], Training Loss: -21.322021484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6612/100000], Training Loss: -21.32208251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6613/100000], Training Loss: -21.322067260742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6614/100000], Training Loss: -21.322128295898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6615/100000], Training Loss: -21.3221435546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6616/100000], Training Loss: -21.322158813476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6617/100000], Training Loss: -21.322250366210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6618/100000], Training Loss: -21.322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6619/100000], Training Loss: -21.322341918945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6620/100000], Training Loss: -21.322311401367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6621/100000], Training Loss: -21.322402954101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6622/100000], Training Loss: -21.3223876953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6623/100000], Training Loss: -21.322433471679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6624/100000], Training Loss: -21.322479248046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6625/100000], Training Loss: -21.322494506835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6626/100000], Training Loss: -21.32257080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6627/100000], Training Loss: -21.322555541992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6628/100000], Training Loss: -21.3226318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6629/100000], Training Loss: -21.322616577148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6630/100000], Training Loss: -21.32269287109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6631/100000], Training Loss: -21.32269287109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6632/100000], Training Loss: -21.32275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6633/100000], Training Loss: -21.322799682617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6634/100000], Training Loss: -21.322830200195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6635/100000], Training Loss: -21.322891235351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6636/100000], Training Loss: -21.322906494140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6637/100000], Training Loss: -21.322998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6638/100000], Training Loss: -21.322967529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6639/100000], Training Loss: -21.322967529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6640/100000], Training Loss: -21.323028564453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6641/100000], Training Loss: -21.32305908203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6642/100000], Training Loss: -21.323150634765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6643/100000], Training Loss: -21.32318115234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6644/100000], Training Loss: -21.323150634765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6645/100000], Training Loss: -21.323226928710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6646/100000], Training Loss: -21.323226928710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6647/100000], Training Loss: -21.323287963867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6648/100000], Training Loss: -21.32330322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6649/100000], Training Loss: -21.3233642578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6650/100000], Training Loss: -21.323394775390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6651/100000], Training Loss: -21.32342529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6652/100000], Training Loss: -21.323471069335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6653/100000], Training Loss: -21.32342529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6654/100000], Training Loss: -21.323486328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6655/100000], Training Loss: -21.32354736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6656/100000], Training Loss: -21.323532104492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6657/100000], Training Loss: -21.32354736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6658/100000], Training Loss: -21.323577880859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6659/100000], Training Loss: -21.323532104492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6660/100000], Training Loss: -21.323455810546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6661/100000], Training Loss: -21.323272705078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6662/100000], Training Loss: -21.322998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6663/100000], Training Loss: -21.322525024414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6664/100000], Training Loss: -21.321563720703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6665/100000], Training Loss: -21.319931030273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6666/100000], Training Loss: -21.317047119140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6667/100000], Training Loss: -21.311874389648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6668/100000], Training Loss: -21.3026123046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6669/100000], Training Loss: -21.28582763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6670/100000], Training Loss: -21.2557373046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6671/100000], Training Loss: -21.200927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6672/100000], Training Loss: -21.10345458984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6673/100000], Training Loss: -20.927871704101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6674/100000], Training Loss: -20.632919311523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6675/100000], Training Loss: -20.143722534179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6676/100000], Training Loss: -19.483352661132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6677/100000], Training Loss: -18.712875366210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6678/100000], Training Loss: -18.400177001953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6679/100000], Training Loss: -18.816497802734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6680/100000], Training Loss: -20.035980224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6681/100000], Training Loss: -20.977035522460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6682/100000], Training Loss: -21.016143798828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6683/100000], Training Loss: -20.49322509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6684/100000], Training Loss: -20.234542846679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6685/100000], Training Loss: -20.5672607421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6686/100000], Training Loss: -20.937210083007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6687/100000], Training Loss: -20.90875244140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6688/100000], Training Loss: -20.718246459960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6689/100000], Training Loss: -20.828262329101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6690/100000], Training Loss: -21.13623046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6691/100000], Training Loss: -21.179931640625, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6692/100000], Training Loss: -20.93304443359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6693/100000], Training Loss: -20.8310546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6694/100000], Training Loss: -21.073318481445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6695/100000], Training Loss: -21.3099365234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6696/100000], Training Loss: -21.24066162109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6697/100000], Training Loss: -21.051132202148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6698/100000], Training Loss: -21.053482055664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6699/100000], Training Loss: -21.212738037109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6700/100000], Training Loss: -21.277099609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6701/100000], Training Loss: -21.212081909179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6702/100000], Training Loss: -21.183990478515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6703/100000], Training Loss: -21.242462158203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6704/100000], Training Loss: -21.271713256835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6705/100000], Training Loss: -21.229507446289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6706/100000], Training Loss: -21.215087890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6707/100000], Training Loss: -21.272750854492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6708/100000], Training Loss: -21.317367553710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6709/100000], Training Loss: -21.286102294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6710/100000], Training Loss: -21.240951538085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6711/100000], Training Loss: -21.25689697265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6712/100000], Training Loss: -21.303955078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6713/100000], Training Loss: -21.314605712890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6714/100000], Training Loss: -21.290725708007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6715/100000], Training Loss: -21.282241821289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6716/100000], Training Loss: -21.299148559570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6717/100000], Training Loss: -21.30828857421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6718/100000], Training Loss: -21.299301147460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6719/100000], Training Loss: -21.29638671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6720/100000], Training Loss: -21.310134887695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6721/100000], Training Loss: -21.319717407226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6722/100000], Training Loss: -21.31134033203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6723/100000], Training Loss: -21.300643920898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6724/100000], Training Loss: -21.305068969726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6725/100000], Training Loss: -21.317489624023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6726/100000], Training Loss: -21.320846557617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6727/100000], Training Loss: -21.314697265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6728/100000], Training Loss: -21.311492919921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6729/100000], Training Loss: -21.315322875976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6730/100000], Training Loss: -21.3187255859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6731/100000], Training Loss: -21.317138671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6732/100000], Training Loss: -21.3155517578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6733/100000], Training Loss: -21.318267822265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6734/100000], Training Loss: -21.3216552734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6735/100000], Training Loss: -21.320892333984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6736/100000], Training Loss: -21.31781005859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6737/100000], Training Loss: -21.317230224609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6738/100000], Training Loss: -21.319915771484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6739/100000], Training Loss: -21.322158813476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6740/100000], Training Loss: -21.321624755859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6741/100000], Training Loss: -21.320343017578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6742/100000], Training Loss: -21.320541381835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6743/100000], Training Loss: -21.321640014648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6744/100000], Training Loss: -21.321807861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6745/100000], Training Loss: -21.321182250976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6746/100000], Training Loss: -21.321090698242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6747/100000], Training Loss: -21.32208251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6748/100000], Training Loss: -21.322860717773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6749/100000], Training Loss: -21.322647094726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6750/100000], Training Loss: -21.321945190429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6751/100000], Training Loss: -21.321807861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6752/100000], Training Loss: -21.322402954101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6753/100000], Training Loss: -21.3228759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6754/100000], Training Loss: -21.322830200195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6755/100000], Training Loss: -21.32269287109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6756/100000], Training Loss: -21.322784423828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6757/100000], Training Loss: -21.323104858398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6758/100000], Training Loss: -21.32318115234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6759/100000], Training Loss: -21.322998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6760/100000], Training Loss: -21.3228759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6761/100000], Training Loss: -21.32305908203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6762/100000], Training Loss: -21.323348999023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6763/100000], Training Loss: -21.323486328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6764/100000], Training Loss: -21.323455810546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6765/100000], Training Loss: -21.323379516601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6766/100000], Training Loss: -21.32342529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6767/100000], Training Loss: -21.323562622070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6768/100000], Training Loss: -21.323623657226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6769/100000], Training Loss: -21.323577880859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6770/100000], Training Loss: -21.323623657226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6771/100000], Training Loss: -21.323699951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6772/100000], Training Loss: -21.323822021484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6773/100000], Training Loss: -21.323867797851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6774/100000], Training Loss: -21.323898315429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6775/100000], Training Loss: -21.323867797851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6776/100000], Training Loss: -21.323883056640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6777/100000], Training Loss: -21.323959350585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6778/100000], Training Loss: -21.324050903320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6779/100000], Training Loss: -21.324050903320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6780/100000], Training Loss: -21.324066162109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6781/100000], Training Loss: -21.324081420898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6782/100000], Training Loss: -21.32421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6783/100000], Training Loss: -21.324234008789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6784/100000], Training Loss: -21.324234008789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6785/100000], Training Loss: -21.32427978515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6786/100000], Training Loss: -21.324325561523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6787/100000], Training Loss: -21.3243408203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6788/100000], Training Loss: -21.324432373046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6789/100000], Training Loss: -21.32440185546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6790/100000], Training Loss: -21.324447631835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6791/100000], Training Loss: -21.324493408203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6792/100000], Training Loss: -21.324539184570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6793/100000], Training Loss: -21.324615478515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6794/100000], Training Loss: -21.324615478515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6795/100000], Training Loss: -21.32464599609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6796/100000], Training Loss: -21.324676513671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6797/100000], Training Loss: -21.324722290039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6798/100000], Training Loss: -21.32476806640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6799/100000], Training Loss: -21.324798583984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6800/100000], Training Loss: -21.324813842773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6801/100000], Training Loss: -21.324859619140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6802/100000], Training Loss: -21.324905395507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6803/100000], Training Loss: -21.324920654296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6804/100000], Training Loss: -21.324951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6805/100000], Training Loss: -21.324981689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6806/100000], Training Loss: -21.325027465820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6807/100000], Training Loss: -21.3250732421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6808/100000], Training Loss: -21.325088500976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6809/100000], Training Loss: -21.325119018554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6810/100000], Training Loss: -21.32513427734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6811/100000], Training Loss: -21.325225830078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6812/100000], Training Loss: -21.325271606445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6813/100000], Training Loss: -21.325271606445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6814/100000], Training Loss: -21.32525634765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6815/100000], Training Loss: -21.325332641601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6816/100000], Training Loss: -21.325408935546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6817/100000], Training Loss: -21.325408935546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6818/100000], Training Loss: -21.325454711914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6819/100000], Training Loss: -21.325469970703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6820/100000], Training Loss: -21.325515747070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6821/100000], Training Loss: -21.325531005859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6822/100000], Training Loss: -21.325546264648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6823/100000], Training Loss: -21.325576782226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6824/100000], Training Loss: -21.325653076171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6825/100000], Training Loss: -21.32568359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6826/100000], Training Loss: -21.32568359375, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6827/100000], Training Loss: -21.325759887695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6828/100000], Training Loss: -21.325790405273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6829/100000], Training Loss: -21.3258056640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6830/100000], Training Loss: -21.32586669921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6831/100000], Training Loss: -21.325881958007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6832/100000], Training Loss: -21.325897216796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6833/100000], Training Loss: -21.325973510742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6834/100000], Training Loss: -21.326004028320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6835/100000], Training Loss: -21.326004028320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6836/100000], Training Loss: -21.325958251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6837/100000], Training Loss: -21.326065063476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6838/100000], Training Loss: -21.3260498046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6839/100000], Training Loss: -21.326095581054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6840/100000], Training Loss: -21.326019287109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6841/100000], Training Loss: -21.326080322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6842/100000], Training Loss: -21.326004028320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6843/100000], Training Loss: -21.325927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6844/100000], Training Loss: -21.325759887695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6845/100000], Training Loss: -21.325485229492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6846/100000], Training Loss: -21.325088500976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6847/100000], Training Loss: -21.324417114257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6848/100000], Training Loss: -21.323226928710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6849/100000], Training Loss: -21.32135009765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6850/100000], Training Loss: -21.318191528320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6851/100000], Training Loss: -21.31304931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6852/100000], Training Loss: -21.304351806640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6853/100000], Training Loss: -21.28985595703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6854/100000], Training Loss: -21.265304565429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6855/100000], Training Loss: -21.224151611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6856/100000], Training Loss: -21.1534423828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6857/100000], Training Loss: -21.0361328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6858/100000], Training Loss: -20.836257934570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6859/100000], Training Loss: -20.520477294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6860/100000], Training Loss: -20.013214111328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6861/100000], Training Loss: -19.336166381835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6862/100000], Training Loss: -18.47705078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6863/100000], Training Loss: -17.903594970703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6864/100000], Training Loss: -17.886688232421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6865/100000], Training Loss: -18.964920043945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6866/100000], Training Loss: -20.4041748046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6867/100000], Training Loss: -21.270477294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6868/100000], Training Loss: -21.037796020507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6869/100000], Training Loss: -20.22479248046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6870/100000], Training Loss: -19.8182373046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6871/100000], Training Loss: -20.2080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6872/100000], Training Loss: -20.98370361328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6873/100000], Training Loss: -21.29351806640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6874/100000], Training Loss: -20.963119506835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6875/100000], Training Loss: -20.563003540039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6876/100000], Training Loss: -20.614837646484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6877/100000], Training Loss: -21.04132080078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6878/100000], Training Loss: -21.293899536132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6879/100000], Training Loss: -21.14459228515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6880/100000], Training Loss: -20.898696899414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6881/100000], Training Loss: -20.912750244140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6882/100000], Training Loss: -21.160140991210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6883/100000], Training Loss: -21.306182861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6884/100000], Training Loss: -21.2095947265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6885/100000], Training Loss: -21.064407348632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6886/100000], Training Loss: -21.086318969726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6887/100000], Training Loss: -21.240264892578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6888/100000], Training Loss: -21.318099975585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6889/100000], Training Loss: -21.248214721679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6890/100000], Training Loss: -21.161773681640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6891/100000], Training Loss: -21.184722900390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6892/100000], Training Loss: -21.28125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6893/100000], Training Loss: -21.32330322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6894/100000], Training Loss: -21.276214599609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6895/100000], Training Loss: -21.2247314453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6896/100000], Training Loss: -21.2415771484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6897/100000], Training Loss: -21.299957275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6898/100000], Training Loss: -21.323867797851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6899/100000], Training Loss: -21.295486450195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6900/100000], Training Loss: -21.26544189453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6901/100000], Training Loss: -21.275497436523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6902/100000], Training Loss: -21.309219360351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6903/100000], Training Loss: -21.32330322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6904/100000], Training Loss: -21.307373046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6905/100000], Training Loss: -21.290115356445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6906/100000], Training Loss: -21.2955322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6907/100000], Training Loss: -21.314910888671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6908/100000], Training Loss: -21.323455810546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6909/100000], Training Loss: -21.314682006835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6910/100000], Training Loss: -21.304351806640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6911/100000], Training Loss: -21.306915283203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6912/100000], Training Loss: -21.31829833984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6913/100000], Training Loss: -21.324310302734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6914/100000], Training Loss: -21.31976318359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6915/100000], Training Loss: -21.312957763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6916/100000], Training Loss: -21.313232421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6917/100000], Training Loss: -21.319869995117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6918/100000], Training Loss: -21.324676513671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6919/100000], Training Loss: -21.323104858398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6920/100000], Training Loss: -21.3187255859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6921/100000], Training Loss: -21.317596435546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6922/100000], Training Loss: -21.320816040039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6923/100000], Training Loss: -21.324386596679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6924/100000], Training Loss: -21.324661254882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6925/100000], Training Loss: -21.32232666015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6926/100000], Training Loss: -21.320831298828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6927/100000], Training Loss: -21.32196044921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6928/100000], Training Loss: -21.324142456054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6929/100000], Training Loss: -21.3250732421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6930/100000], Training Loss: -21.32421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6931/100000], Training Loss: -21.323028564453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6932/100000], Training Loss: -21.323089599609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6933/100000], Training Loss: -21.324249267578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6934/100000], Training Loss: -21.325241088867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6935/100000], Training Loss: -21.325164794921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6936/100000], Training Loss: -21.324417114257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6937/100000], Training Loss: -21.324066162109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6938/100000], Training Loss: -21.324447631835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6939/100000], Training Loss: -21.3251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6940/100000], Training Loss: -21.3255615234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6941/100000], Training Loss: -21.325286865234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6942/100000], Training Loss: -21.324920654296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6943/100000], Training Loss: -21.324874877929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6944/100000], Training Loss: -21.325180053710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6945/100000], Training Loss: -21.325592041015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6946/100000], Training Loss: -21.325714111328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6947/100000], Training Loss: -21.325546264648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6948/100000], Training Loss: -21.32537841796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6949/100000], Training Loss: -21.325393676757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6950/100000], Training Loss: -21.325592041015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6951/100000], Training Loss: -21.325836181640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6952/100000], Training Loss: -21.325927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6953/100000], Training Loss: -21.325790405273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6954/100000], Training Loss: -21.325790405273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6955/100000], Training Loss: -21.3258056640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6956/100000], Training Loss: -21.325958251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6957/100000], Training Loss: -21.3260498046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6958/100000], Training Loss: -21.326065063476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6959/100000], Training Loss: -21.326080322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6960/100000], Training Loss: -21.326080322265625, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6961/100000], Training Loss: -21.326065063476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6962/100000], Training Loss: -21.326171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6963/100000], Training Loss: -21.326187133789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6964/100000], Training Loss: -21.326263427734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6965/100000], Training Loss: -21.326309204101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6966/100000], Training Loss: -21.3262939453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6967/100000], Training Loss: -21.326309204101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6968/100000], Training Loss: -21.32635498046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6969/100000], Training Loss: -21.326431274414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [6970/100000], Training Loss: -21.326461791992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6971/100000], Training Loss: -21.32647705078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6972/100000], Training Loss: -21.326522827148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6973/100000], Training Loss: -21.326522827148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [6974/100000], Training Loss: -21.326583862304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [6975/100000], Training Loss: -21.32659912109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6976/100000], Training Loss: -21.32666015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6977/100000], Training Loss: -21.32666015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6978/100000], Training Loss: -21.32672119140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6979/100000], Training Loss: -21.326705932617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6980/100000], Training Loss: -21.326736450195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6981/100000], Training Loss: -21.326797485351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6982/100000], Training Loss: -21.32684326171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6983/100000], Training Loss: -21.32684326171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6984/100000], Training Loss: -21.326873779296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6985/100000], Training Loss: -21.326950073242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6986/100000], Training Loss: -21.326934814453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6987/100000], Training Loss: -21.32696533203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [6988/100000], Training Loss: -21.326995849609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6989/100000], Training Loss: -21.3270263671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [6990/100000], Training Loss: -21.327056884765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6991/100000], Training Loss: -21.327102661132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [6992/100000], Training Loss: -21.3271484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6993/100000], Training Loss: -21.327133178710938, Learning Rate: 0.011438396227480507\n",
      "Epoch [6994/100000], Training Loss: -21.3271484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [6995/100000], Training Loss: -21.327194213867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [6996/100000], Training Loss: -21.327224731445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [6997/100000], Training Loss: -21.327301025390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [6998/100000], Training Loss: -21.327285766601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [6999/100000], Training Loss: -21.327346801757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7000/100000], Training Loss: -21.327392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7001/100000], Training Loss: -21.327377319335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7002/100000], Training Loss: -21.327423095703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7003/100000], Training Loss: -21.327438354492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7004/100000], Training Loss: -21.327499389648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7005/100000], Training Loss: -21.327529907226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7006/100000], Training Loss: -21.32757568359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7007/100000], Training Loss: -21.327545166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7008/100000], Training Loss: -21.327545166015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7009/100000], Training Loss: -21.32763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7010/100000], Training Loss: -21.32769775390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7011/100000], Training Loss: -21.32763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7012/100000], Training Loss: -21.327728271484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7013/100000], Training Loss: -21.327743530273438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7014/100000], Training Loss: -21.327713012695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7015/100000], Training Loss: -21.327774047851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7016/100000], Training Loss: -21.32781982421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7017/100000], Training Loss: -21.327850341796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7018/100000], Training Loss: -21.327865600585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7019/100000], Training Loss: -21.327926635742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7020/100000], Training Loss: -21.32794189453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7021/100000], Training Loss: -21.327957153320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7022/100000], Training Loss: -21.327987670898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7023/100000], Training Loss: -21.328018188476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7024/100000], Training Loss: -21.32806396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7025/100000], Training Loss: -21.328094482421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7026/100000], Training Loss: -21.328109741210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7027/100000], Training Loss: -21.328109741210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7028/100000], Training Loss: -21.328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7029/100000], Training Loss: -21.328140258789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7030/100000], Training Loss: -21.328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7031/100000], Training Loss: -21.32806396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7032/100000], Training Loss: -21.3280029296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7033/100000], Training Loss: -21.327896118164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7034/100000], Training Loss: -21.327651977539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7035/100000], Training Loss: -21.3272705078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7036/100000], Training Loss: -21.326675415039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7037/100000], Training Loss: -21.325592041015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7038/100000], Training Loss: -21.323822021484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7039/100000], Training Loss: -21.32073974609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7040/100000], Training Loss: -21.315536499023438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7041/100000], Training Loss: -21.306533813476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7042/100000], Training Loss: -21.291107177734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7043/100000], Training Loss: -21.264297485351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7044/100000], Training Loss: -21.21795654296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7045/100000], Training Loss: -21.138076782226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7046/100000], Training Loss: -21.002532958984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7047/100000], Training Loss: -20.779251098632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7048/100000], Training Loss: -20.433609008789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7049/100000], Training Loss: -19.95538330078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7050/100000], Training Loss: -19.4322509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7051/100000], Training Loss: -19.108917236328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7052/100000], Training Loss: -19.325210571289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7053/100000], Training Loss: -20.085052490234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7054/100000], Training Loss: -20.8897705078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7055/100000], Training Loss: -21.1231689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7056/100000], Training Loss: -20.787582397460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7057/100000], Training Loss: -20.421035766601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7058/100000], Training Loss: -20.539443969726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7059/100000], Training Loss: -21.0123291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7060/100000], Training Loss: -21.298049926757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7061/100000], Training Loss: -21.12188720703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7062/100000], Training Loss: -20.802291870117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7063/100000], Training Loss: -20.794708251953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7064/100000], Training Loss: -21.0799560546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7065/100000], Training Loss: -21.285888671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7066/100000], Training Loss: -21.203277587890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7067/100000], Training Loss: -21.0457763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7068/100000], Training Loss: -21.0789794921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7069/100000], Training Loss: -21.245681762695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7070/100000], Training Loss: -21.304641723632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7071/100000], Training Loss: -21.201171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7072/100000], Training Loss: -21.118865966796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7073/100000], Training Loss: -21.182113647460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7074/100000], Training Loss: -21.293716430664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7075/100000], Training Loss: -21.3074951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7076/100000], Training Loss: -21.24224853515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7077/100000], Training Loss: -21.220245361328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7078/100000], Training Loss: -21.274490356445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7079/100000], Training Loss: -21.31842041015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7080/100000], Training Loss: -21.294326782226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7081/100000], Training Loss: -21.252212524414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7082/100000], Training Loss: -21.25933837890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7083/100000], Training Loss: -21.303146362304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7084/100000], Training Loss: -21.322219848632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7085/100000], Training Loss: -21.302200317382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7086/100000], Training Loss: -21.285263061523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7087/100000], Training Loss: -21.299346923828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7088/100000], Training Loss: -21.32171630859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7089/100000], Training Loss: -21.3209228515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7090/100000], Training Loss: -21.303359985351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7091/100000], Training Loss: -21.29754638671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7092/100000], Training Loss: -21.310836791992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7093/100000], Training Loss: -21.32342529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7094/100000], Training Loss: -21.320770263671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7095/100000], Training Loss: -21.31207275390625, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7096/100000], Training Loss: -21.312744140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7097/100000], Training Loss: -21.322097778320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7098/100000], Training Loss: -21.327178955078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7099/100000], Training Loss: -21.322830200195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7100/100000], Training Loss: -21.31707763671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7101/100000], Training Loss: -21.318191528320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7102/100000], Training Loss: -21.323638916015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7103/100000], Training Loss: -21.325775146484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7104/100000], Training Loss: -21.322967529296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7105/100000], Training Loss: -21.320510864257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7106/100000], Training Loss: -21.322311401367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7107/100000], Training Loss: -21.325927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7108/100000], Training Loss: -21.327224731445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7109/100000], Training Loss: -21.325485229492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7110/100000], Training Loss: -21.324127197265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7111/100000], Training Loss: -21.325119018554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7112/100000], Training Loss: -21.327072143554688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7113/100000], Training Loss: -21.327529907226562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7114/100000], Training Loss: -21.326309204101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7115/100000], Training Loss: -21.32537841796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7116/100000], Training Loss: -21.325881958007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7117/100000], Training Loss: -21.3270263671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7118/100000], Training Loss: -21.327407836914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7119/100000], Training Loss: -21.32684326171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7120/100000], Training Loss: -21.326248168945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7121/100000], Training Loss: -21.3265380859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7122/100000], Training Loss: -21.32733154296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7123/100000], Training Loss: -21.3277587890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7124/100000], Training Loss: -21.3275146484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7125/100000], Training Loss: -21.327178955078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7126/100000], Training Loss: -21.32733154296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7127/100000], Training Loss: -21.327774047851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7128/100000], Training Loss: -21.328140258789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7129/100000], Training Loss: -21.32806396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7130/100000], Training Loss: -21.327880859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7131/100000], Training Loss: -21.327896118164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7132/100000], Training Loss: -21.328079223632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7133/100000], Training Loss: -21.328338623046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7134/100000], Training Loss: -21.328384399414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7135/100000], Training Loss: -21.328292846679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7136/100000], Training Loss: -21.328216552734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7137/100000], Training Loss: -21.328262329101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7138/100000], Training Loss: -21.328399658203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7139/100000], Training Loss: -21.3284912109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7140/100000], Training Loss: -21.328521728515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7141/100000], Training Loss: -21.328384399414062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7142/100000], Training Loss: -21.328338623046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7143/100000], Training Loss: -21.328353881835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7144/100000], Training Loss: -21.328399658203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7145/100000], Training Loss: -21.328262329101562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7146/100000], Training Loss: -21.328048706054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7147/100000], Training Loss: -21.327804565429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7148/100000], Training Loss: -21.327346801757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7149/100000], Training Loss: -21.32684326171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7150/100000], Training Loss: -21.325927734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7151/100000], Training Loss: -21.324569702148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7152/100000], Training Loss: -21.322494506835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7153/100000], Training Loss: -21.319366455078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7154/100000], Training Loss: -21.314605712890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7155/100000], Training Loss: -21.3072509765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7156/100000], Training Loss: -21.295867919921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7157/100000], Training Loss: -21.2779541015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7158/100000], Training Loss: -21.250228881835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7159/100000], Training Loss: -21.205902099609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7160/100000], Training Loss: -21.137481689453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7161/100000], Training Loss: -21.028457641601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7162/100000], Training Loss: -20.8646240234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7163/100000], Training Loss: -20.611419677734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7164/100000], Training Loss: -20.2655029296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7165/100000], Training Loss: -19.789337158203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7166/100000], Training Loss: -19.303848266601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7167/100000], Training Loss: -18.87322998046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7168/100000], Training Loss: -18.884506225585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7169/100000], Training Loss: -19.35491943359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7170/100000], Training Loss: -20.268203735351562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7171/100000], Training Loss: -21.058883666992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7172/100000], Training Loss: -21.322906494140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7173/100000], Training Loss: -21.032623291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7174/100000], Training Loss: -20.548248291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7175/100000], Training Loss: -20.330291748046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7176/100000], Training Loss: -20.5321044921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7177/100000], Training Loss: -20.989669799804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7178/100000], Training Loss: -21.297164916992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7179/100000], Training Loss: -21.25714111328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7180/100000], Training Loss: -21.004058837890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7181/100000], Training Loss: -20.827407836914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7182/100000], Training Loss: -20.906753540039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7183/100000], Training Loss: -21.1396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7184/100000], Training Loss: -21.308502197265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7185/100000], Training Loss: -21.285720825195312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7186/100000], Training Loss: -21.147140502929688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7187/100000], Training Loss: -21.062576293945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7188/100000], Training Loss: -21.113388061523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7189/100000], Training Loss: -21.241714477539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7190/100000], Training Loss: -21.320602416992188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7191/100000], Training Loss: -21.295501708984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7192/100000], Training Loss: -21.220046997070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7193/100000], Training Loss: -21.18310546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7194/100000], Training Loss: -21.221206665039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7195/100000], Training Loss: -21.28955078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7196/100000], Training Loss: -21.32464599609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7197/100000], Training Loss: -21.30517578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7198/100000], Training Loss: -21.264785766601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7199/100000], Training Loss: -21.249893188476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7200/100000], Training Loss: -21.272979736328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7201/100000], Training Loss: -21.309371948242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7202/100000], Training Loss: -21.326095581054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7203/100000], Training Loss: -21.314666748046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7204/100000], Training Loss: -21.29345703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7205/100000], Training Loss: -21.28564453125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7206/100000], Training Loss: -21.298065185546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7207/100000], Training Loss: -21.317352294921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7208/100000], Training Loss: -21.326980590820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7209/100000], Training Loss: -21.321792602539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7210/100000], Training Loss: -21.310455322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7211/100000], Training Loss: -21.305282592773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7212/100000], Training Loss: -21.31060791015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7213/100000], Training Loss: -21.320846557617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7214/100000], Training Loss: -21.32733154296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7215/100000], Training Loss: -21.326080322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7216/100000], Training Loss: -21.320327758789062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7217/100000], Training Loss: -21.31634521484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7218/100000], Training Loss: -21.317611694335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7219/100000], Training Loss: -21.322647094726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7220/100000], Training Loss: -21.327163696289062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7221/100000], Training Loss: -21.32806396484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7222/100000], Training Loss: -21.325714111328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7223/100000], Training Loss: -21.3228759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7224/100000], Training Loss: -21.322219848632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7225/100000], Training Loss: -21.3240966796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7226/100000], Training Loss: -21.326873779296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7227/100000], Training Loss: -21.328460693359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7228/100000], Training Loss: -21.328109741210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7229/100000], Training Loss: -21.326629638671875, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7230/100000], Training Loss: -21.32550048828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7231/100000], Training Loss: -21.325592041015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7232/100000], Training Loss: -21.32684326171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7233/100000], Training Loss: -21.32818603515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7234/100000], Training Loss: -21.328765869140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7235/100000], Training Loss: -21.328536987304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7236/100000], Training Loss: -21.327804565429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7237/100000], Training Loss: -21.327301025390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7238/100000], Training Loss: -21.327392578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7239/100000], Training Loss: -21.327987670898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7240/100000], Training Loss: -21.32867431640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7241/100000], Training Loss: -21.32904052734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7242/100000], Training Loss: -21.3289794921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7243/100000], Training Loss: -21.328659057617188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7244/100000], Training Loss: -21.328369140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7245/100000], Training Loss: -21.328353881835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7246/100000], Training Loss: -21.32861328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7247/100000], Training Loss: -21.328948974609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7248/100000], Training Loss: -21.329238891601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7249/100000], Training Loss: -21.329315185546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7250/100000], Training Loss: -21.329193115234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7251/100000], Training Loss: -21.329071044921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7252/100000], Training Loss: -21.329010009765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7253/100000], Training Loss: -21.329055786132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7254/100000], Training Loss: -21.329177856445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7255/100000], Training Loss: -21.329360961914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7256/100000], Training Loss: -21.329498291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7257/100000], Training Loss: -21.329574584960938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7258/100000], Training Loss: -21.32952880859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7259/100000], Training Loss: -21.329513549804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7260/100000], Training Loss: -21.329437255859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7261/100000], Training Loss: -21.329452514648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7262/100000], Training Loss: -21.329544067382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7263/100000], Training Loss: -21.329635620117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7264/100000], Training Loss: -21.329757690429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7265/100000], Training Loss: -21.329803466796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7266/100000], Training Loss: -21.329788208007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7267/100000], Training Loss: -21.329803466796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7268/100000], Training Loss: -21.329803466796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7269/100000], Training Loss: -21.329818725585938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7270/100000], Training Loss: -21.329849243164062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7271/100000], Training Loss: -21.329910278320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7272/100000], Training Loss: -21.329925537109375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7273/100000], Training Loss: -21.330001831054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7274/100000], Training Loss: -21.330001831054688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7275/100000], Training Loss: -21.330032348632812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7276/100000], Training Loss: -21.330123901367188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7277/100000], Training Loss: -21.330154418945312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7278/100000], Training Loss: -21.330108642578125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7279/100000], Training Loss: -21.330169677734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7280/100000], Training Loss: -21.330184936523438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7281/100000], Training Loss: -21.330230712890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7282/100000], Training Loss: -21.330245971679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7283/100000], Training Loss: -21.330307006835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7284/100000], Training Loss: -21.330307006835938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7285/100000], Training Loss: -21.330352783203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7286/100000], Training Loss: -21.330413818359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7287/100000], Training Loss: -21.330429077148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7288/100000], Training Loss: -21.330413818359375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7289/100000], Training Loss: -21.330429077148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7290/100000], Training Loss: -21.330459594726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7291/100000], Training Loss: -21.330520629882812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7292/100000], Training Loss: -21.330551147460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7293/100000], Training Loss: -21.33056640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7294/100000], Training Loss: -21.33062744140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7295/100000], Training Loss: -21.33062744140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7296/100000], Training Loss: -21.330673217773438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7297/100000], Training Loss: -21.330718994140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7298/100000], Training Loss: -21.330718994140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7299/100000], Training Loss: -21.33074951171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7300/100000], Training Loss: -21.330795288085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7301/100000], Training Loss: -21.330795288085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7302/100000], Training Loss: -21.330825805664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7303/100000], Training Loss: -21.33087158203125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7304/100000], Training Loss: -21.330902099609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7305/100000], Training Loss: -21.330947875976562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7306/100000], Training Loss: -21.331008911132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7307/100000], Training Loss: -21.331008911132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7308/100000], Training Loss: -21.33099365234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7309/100000], Training Loss: -21.3310546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7310/100000], Training Loss: -21.3310546875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7311/100000], Training Loss: -21.331100463867188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7312/100000], Training Loss: -21.331146240234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7313/100000], Training Loss: -21.331130981445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7314/100000], Training Loss: -21.331207275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7315/100000], Training Loss: -21.331222534179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7316/100000], Training Loss: -21.33123779296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7317/100000], Training Loss: -21.331283569335938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7318/100000], Training Loss: -21.331329345703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7319/100000], Training Loss: -21.331298828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7320/100000], Training Loss: -21.331329345703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7321/100000], Training Loss: -21.331390380859375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7322/100000], Training Loss: -21.3314208984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7323/100000], Training Loss: -21.331405639648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7324/100000], Training Loss: -21.331466674804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7325/100000], Training Loss: -21.331466674804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7326/100000], Training Loss: -21.331497192382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7327/100000], Training Loss: -21.33154296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7328/100000], Training Loss: -21.331558227539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7329/100000], Training Loss: -21.331588745117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7330/100000], Training Loss: -21.33160400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7331/100000], Training Loss: -21.3316650390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7332/100000], Training Loss: -21.3316650390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7333/100000], Training Loss: -21.331680297851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7334/100000], Training Loss: -21.331695556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7335/100000], Training Loss: -21.331741333007812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7336/100000], Training Loss: -21.331710815429688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7337/100000], Training Loss: -21.331695556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7338/100000], Training Loss: -21.331695556640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7339/100000], Training Loss: -21.331634521484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7340/100000], Training Loss: -21.331573486328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7341/100000], Training Loss: -21.331466674804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7342/100000], Training Loss: -21.33123779296875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7343/100000], Training Loss: -21.330917358398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7344/100000], Training Loss: -21.330322265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7345/100000], Training Loss: -21.329498291015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7346/100000], Training Loss: -21.328094482421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7347/100000], Training Loss: -21.3258056640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7348/100000], Training Loss: -21.322067260742188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7349/100000], Training Loss: -21.3160400390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7350/100000], Training Loss: -21.306076049804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7351/100000], Training Loss: -21.28948974609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7352/100000], Training Loss: -21.262115478515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7353/100000], Training Loss: -21.21600341796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7354/100000], Training Loss: -21.139801025390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7355/100000], Training Loss: -21.011001586914062, Learning Rate: 0.011438396227480507\n",
      "Epoch [7356/100000], Training Loss: -20.80291748046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7357/100000], Training Loss: -20.460372924804688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7358/100000], Training Loss: -19.953598022460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7359/100000], Training Loss: -19.211349487304688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7360/100000], Training Loss: -18.3970947265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7361/100000], Training Loss: -17.65771484375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7362/100000], Training Loss: -17.712554931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7363/100000], Training Loss: -18.618728637695312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7364/100000], Training Loss: -20.156692504882812, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7365/100000], Training Loss: -21.167831420898438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7366/100000], Training Loss: -21.076644897460938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7367/100000], Training Loss: -20.297393798828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7368/100000], Training Loss: -19.772537231445312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7369/100000], Training Loss: -20.117828369140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7370/100000], Training Loss: -20.883529663085938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7371/100000], Training Loss: -21.262603759765625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7372/100000], Training Loss: -20.972900390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7373/100000], Training Loss: -20.547561645507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7374/100000], Training Loss: -20.596099853515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7375/100000], Training Loss: -21.027542114257812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7376/100000], Training Loss: -21.29547119140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7377/100000], Training Loss: -21.132003784179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7378/100000], Training Loss: -20.865219116210938, Learning Rate: 0.011438396227480507\n",
      "Epoch [7379/100000], Training Loss: -20.903518676757812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7380/100000], Training Loss: -21.180068969726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7381/100000], Training Loss: -21.32391357421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7382/100000], Training Loss: -21.194503784179688, Learning Rate: 0.011438396227480507\n",
      "Epoch [7383/100000], Training Loss: -21.037002563476562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7384/100000], Training Loss: -21.09100341796875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7385/100000], Training Loss: -21.266143798828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7386/100000], Training Loss: -21.324905395507812, Learning Rate: 0.011438396227480507\n",
      "Epoch [7387/100000], Training Loss: -21.227371215820312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7388/100000], Training Loss: -21.149734497070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7389/100000], Training Loss: -21.207244873046875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7390/100000], Training Loss: -21.307022094726562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7391/100000], Training Loss: -21.316497802734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [7392/100000], Training Loss: -21.250518798828125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7393/100000], Training Loss: -21.222625732421875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7394/100000], Training Loss: -21.2723388671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7395/100000], Training Loss: -21.324050903320312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7396/100000], Training Loss: -21.313339233398438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7397/100000], Training Loss: -21.2720947265625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7398/100000], Training Loss: -21.266876220703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7399/100000], Training Loss: -21.303192138671875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7400/100000], Training Loss: -21.32891845703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7401/100000], Training Loss: -21.315582275390625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7402/100000], Training Loss: -21.291656494140625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7403/100000], Training Loss: -21.293701171875, Learning Rate: 0.011438396227480507\n",
      "Epoch [7404/100000], Training Loss: -21.316757202148438, Learning Rate: 0.011438396227480507\n",
      "Epoch [7405/100000], Training Loss: -21.328903198242188, Learning Rate: 0.011438396227480507\n",
      "Epoch [7406/100000], Training Loss: -21.318756103515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [7407/100000], Training Loss: -21.305984497070312, Learning Rate: 0.011438396227480507\n",
      "Epoch [7408/100000], Training Loss: -21.3095703125, Learning Rate: 0.011438396227480507\n",
      "Epoch [7409/100000], Training Loss: -21.323379516601562, Learning Rate: 0.011438396227480507\n",
      "Epoch [7410/100000], Training Loss: -21.328903198242188, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-efe52edc54f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mLL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mLL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(100000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    LP = torch.stack([exp._log_norm(torch.tensor(0.0), m, torch.tensor(1.0)).sum() for m in model.parameters()]).sum()\n",
    "    LL = exp._log_norm(model(x_train), y_train.unsqueeze(0).repeat(1, 1, 1), torch.tensor([0.1], device=device)).sum()\n",
    "    L = -LL - LP\n",
    "    L.backward()\n",
    "\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(L.detach().clone().cpu().numpy())\n",
    "\n",
    "\n",
    "    stats = 'Epoch [{}/{}], Training Loss: {}, Learning Rate: {}'.format(t, max_iter, L, lr)\n",
    "    print(stats)\n",
    "\n",
    "    if lr < min_lr:\n",
    "        break\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = [q.sample(1) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"1170\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "x_lin = torch.linspace(-2.0, 2.0).unsqueeze(1).to(device)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(11.7, 8.27)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-4, 4)\n",
    "plt.grid(True, which='major', linewidth=0.5)\n",
    "plt.title('Training set')\n",
    "for theta in ensemble:\n",
    "    y_pred = exp.mlp(x_lin, theta)\n",
    "    plt.plot(x_lin.detach().cpu().numpy(), y_pred.squeeze(0).detach().cpu().numpy(), alpha=1.0,\n",
    "             linewidth=1.0, color='black', zorder=80)\n",
    "    res = 5\n",
    "    for r in range(res):\n",
    "        mass = 1.0 - (r + 1) / res\n",
    "        plt.fill_between(x_lin.detach().cpu().numpy().squeeze(),\n",
    "                         y_pred.squeeze(0).detach().cpu().numpy().squeeze() - 3 * 0.1 * ((r + 1) / res),\n",
    "                         y_pred.squeeze(0).detach().cpu().numpy().squeeze() + 3 * 0.1 * ((r + 1) / res),\n",
    "                         alpha=0.2 * mass, color='lightblue', zorder=50)\n",
    "plt.scatter(x_train.cpu(), y_train.cpu(), c='red', zorder=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = MeanFieldVariationalDistribution(exp.param_count, sigma=0.0000001, device=device)\n",
    "q.mu = nn.Parameter(theta.detach().clone(), requires_grad=True)\n",
    "q.rho.requires_grad = True\n",
    "q.mu.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(q.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100000], Training Loss: 103.0980453491211, Learning Rate: 0.011438396227480507\n",
      "Epoch [1/100000], Training Loss: 103.58128356933594, Learning Rate: 0.011438396227480507\n",
      "Epoch [2/100000], Training Loss: 102.58480072021484, Learning Rate: 0.011438396227480507\n",
      "Epoch [3/100000], Training Loss: 103.0468521118164, Learning Rate: 0.011438396227480507\n",
      "Epoch [4/100000], Training Loss: 102.82642364501953, Learning Rate: 0.011438396227480507\n",
      "Epoch [5/100000], Training Loss: 104.3068618774414, Learning Rate: 0.011438396227480507\n",
      "Epoch [6/100000], Training Loss: 102.29850006103516, Learning Rate: 0.011438396227480507\n",
      "Epoch [7/100000], Training Loss: 102.53214263916016, Learning Rate: 0.011438396227480507\n",
      "Epoch [8/100000], Training Loss: 102.59877014160156, Learning Rate: 0.011438396227480507\n",
      "Epoch [9/100000], Training Loss: 103.66726684570312, Learning Rate: 0.011438396227480507\n",
      "Epoch [10/100000], Training Loss: 101.97938537597656, Learning Rate: 0.011438396227480507\n",
      "Epoch [11/100000], Training Loss: 104.52050018310547, Learning Rate: 0.011438396227480507\n",
      "Epoch [12/100000], Training Loss: 102.44677734375, Learning Rate: 0.011438396227480507\n",
      "Epoch [13/100000], Training Loss: 102.19554901123047, Learning Rate: 0.011438396227480507\n",
      "Epoch [14/100000], Training Loss: 102.12689208984375, Learning Rate: 0.011438396227480507\n",
      "Epoch [15/100000], Training Loss: 101.9794921875, Learning Rate: 0.011438396227480507\n",
      "Epoch [16/100000], Training Loss: 103.49370574951172, Learning Rate: 0.011438396227480507\n",
      "Epoch [17/100000], Training Loss: 102.1459732055664, Learning Rate: 0.011438396227480507\n",
      "Epoch [18/100000], Training Loss: 102.83089447021484, Learning Rate: 0.011438396227480507\n",
      "Epoch [19/100000], Training Loss: 104.16663360595703, Learning Rate: 0.011438396227480507\n",
      "Epoch [20/100000], Training Loss: 104.1761474609375, Learning Rate: 0.011438396227480507\n",
      "Epoch [21/100000], Training Loss: 101.67495727539062, Learning Rate: 0.011438396227480507\n",
      "Epoch [22/100000], Training Loss: 103.04452514648438, Learning Rate: 0.011438396227480507\n",
      "Epoch [23/100000], Training Loss: 102.11553955078125, Learning Rate: 0.011438396227480507\n",
      "Epoch [24/100000], Training Loss: 101.7894058227539, Learning Rate: 0.011438396227480507\n",
      "Epoch [25/100000], Training Loss: 102.49331665039062, Learning Rate: 0.011438396227480507\n",
      "Epoch [26/100000], Training Loss: 101.39495086669922, Learning Rate: 0.011438396227480507\n",
      "Epoch [27/100000], Training Loss: 101.99634552001953, Learning Rate: 0.011438396227480507\n",
      "Epoch [28/100000], Training Loss: 102.08309173583984, Learning Rate: 0.011438396227480507\n",
      "Epoch [29/100000], Training Loss: 103.39778137207031, Learning Rate: 0.011438396227480507\n",
      "Epoch [30/100000], Training Loss: 102.91144561767578, Learning Rate: 0.011438396227480507\n",
      "Epoch [31/100000], Training Loss: 103.66936492919922, Learning Rate: 0.011438396227480507\n",
      "Epoch [32/100000], Training Loss: 102.85216522216797, Learning Rate: 0.011438396227480507\n",
      "Epoch [33/100000], Training Loss: 102.7533187866211, Learning Rate: 0.011438396227480507\n",
      "Epoch [34/100000], Training Loss: 103.46186065673828, Learning Rate: 0.011438396227480507\n",
      "Epoch [35/100000], Training Loss: 103.48603057861328, Learning Rate: 0.011438396227480507\n",
      "Epoch [36/100000], Training Loss: 103.62150573730469, Learning Rate: 0.011438396227480507\n",
      "Epoch [37/100000], Training Loss: 103.1980972290039, Learning Rate: 0.011438396227480507\n",
      "Epoch [38/100000], Training Loss: 101.12090301513672, Learning Rate: 0.011438396227480507\n",
      "Epoch [39/100000], Training Loss: 101.56175994873047, Learning Rate: 0.011438396227480507\n",
      "Epoch [40/100000], Training Loss: 102.2265853881836, Learning Rate: 0.011438396227480507\n",
      "Epoch [41/100000], Training Loss: 103.13619232177734, Learning Rate: 0.011438396227480507\n",
      "Epoch [42/100000], Training Loss: 103.69658660888672, Learning Rate: 0.011438396227480507\n",
      "Epoch [43/100000], Training Loss: 102.57637786865234, Learning Rate: 0.011438396227480507\n",
      "Epoch [44/100000], Training Loss: 103.07793426513672, Learning Rate: 0.011438396227480507\n",
      "Epoch [45/100000], Training Loss: 104.56997680664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [46/100000], Training Loss: 102.1502456665039, Learning Rate: 0.011438396227480507\n",
      "Epoch [47/100000], Training Loss: 103.6036148071289, Learning Rate: 0.011438396227480507\n",
      "Epoch [48/100000], Training Loss: 101.12596130371094, Learning Rate: 0.011438396227480507\n",
      "Epoch [49/100000], Training Loss: 101.931640625, Learning Rate: 0.011438396227480507\n",
      "Epoch [50/100000], Training Loss: 101.86708068847656, Learning Rate: 0.011438396227480507\n",
      "Epoch [51/100000], Training Loss: 103.83250427246094, Learning Rate: 0.011438396227480507\n",
      "Epoch [52/100000], Training Loss: 100.73403930664062, Learning Rate: 0.011438396227480507\n",
      "Epoch [53/100000], Training Loss: 103.09231567382812, Learning Rate: 0.011438396227480507\n",
      "Epoch [54/100000], Training Loss: 103.91255950927734, Learning Rate: 0.011438396227480507\n",
      "Epoch [55/100000], Training Loss: 102.2006607055664, Learning Rate: 0.011438396227480507\n",
      "Epoch [56/100000], Training Loss: 101.63824462890625, Learning Rate: 0.011438396227480507\n",
      "Epoch [57/100000], Training Loss: 102.71821594238281, Learning Rate: 0.011438396227480507\n",
      "Epoch [58/100000], Training Loss: 102.07850646972656, Learning Rate: 0.011438396227480507\n",
      "Epoch [59/100000], Training Loss: 102.4455795288086, Learning Rate: 0.011438396227480507\n",
      "Epoch [60/100000], Training Loss: 100.23234558105469, Learning Rate: 0.011438396227480507\n",
      "Epoch [61/100000], Training Loss: 100.76594543457031, Learning Rate: 0.011438396227480507\n",
      "Epoch [62/100000], Training Loss: 101.41095733642578, Learning Rate: 0.011438396227480507\n",
      "Epoch [63/100000], Training Loss: 103.45462036132812, Learning Rate: 0.011438396227480507\n",
      "Epoch [64/100000], Training Loss: 103.29894256591797, Learning Rate: 0.011438396227480507\n",
      "Epoch [65/100000], Training Loss: 103.70072937011719, Learning Rate: 0.011438396227480507\n",
      "Epoch [66/100000], Training Loss: 101.85559844970703, Learning Rate: 0.011438396227480507\n",
      "Epoch [67/100000], Training Loss: 102.78692626953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [68/100000], Training Loss: 103.81256103515625, Learning Rate: 0.011438396227480507\n",
      "Epoch [69/100000], Training Loss: 102.34380340576172, Learning Rate: 0.011438396227480507\n",
      "Epoch [70/100000], Training Loss: 102.5908432006836, Learning Rate: 0.011438396227480507\n",
      "Epoch [71/100000], Training Loss: 102.0619888305664, Learning Rate: 0.011438396227480507\n",
      "Epoch [72/100000], Training Loss: 103.41864776611328, Learning Rate: 0.011438396227480507\n",
      "Epoch [73/100000], Training Loss: 102.989501953125, Learning Rate: 0.011438396227480507\n",
      "Epoch [74/100000], Training Loss: 101.3237533569336, Learning Rate: 0.011438396227480507\n",
      "Epoch [75/100000], Training Loss: 101.42473602294922, Learning Rate: 0.011438396227480507\n",
      "Epoch [76/100000], Training Loss: 102.2761459350586, Learning Rate: 0.011438396227480507\n",
      "Epoch [77/100000], Training Loss: 102.28650665283203, Learning Rate: 0.011438396227480507\n",
      "Epoch [78/100000], Training Loss: 102.20742797851562, Learning Rate: 0.011438396227480507\n",
      "Epoch [79/100000], Training Loss: 101.01350402832031, Learning Rate: 0.011438396227480507\n",
      "Epoch [80/100000], Training Loss: 102.85142517089844, Learning Rate: 0.011438396227480507\n",
      "Epoch [81/100000], Training Loss: 103.6022720336914, Learning Rate: 0.011438396227480507\n",
      "Epoch [82/100000], Training Loss: 101.81812286376953, Learning Rate: 0.011438396227480507\n",
      "Epoch [83/100000], Training Loss: 101.66278839111328, Learning Rate: 0.011438396227480507\n",
      "Epoch [84/100000], Training Loss: 102.13375854492188, Learning Rate: 0.011438396227480507\n",
      "Epoch [85/100000], Training Loss: 103.64569854736328, Learning Rate: 0.011438396227480507\n",
      "Epoch [86/100000], Training Loss: 103.0271987915039, Learning Rate: 0.011438396227480507\n",
      "Epoch [87/100000], Training Loss: 102.36148834228516, Learning Rate: 0.011438396227480507\n",
      "Epoch [88/100000], Training Loss: 102.3576431274414, Learning Rate: 0.011438396227480507\n",
      "Epoch [89/100000], Training Loss: 102.401611328125, Learning Rate: 0.011438396227480507\n",
      "Epoch [90/100000], Training Loss: 102.01458740234375, Learning Rate: 0.011438396227480507\n",
      "Epoch [91/100000], Training Loss: 104.91574096679688, Learning Rate: 0.011438396227480507\n",
      "Epoch [92/100000], Training Loss: 102.31588745117188, Learning Rate: 0.011438396227480507\n",
      "Epoch [93/100000], Training Loss: 103.45668029785156, Learning Rate: 0.011438396227480507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100000], Training Loss: 101.37226104736328, Learning Rate: 0.011438396227480507\n",
      "Epoch [95/100000], Training Loss: 102.10100555419922, Learning Rate: 0.011438396227480507\n",
      "Epoch [96/100000], Training Loss: 102.38953399658203, Learning Rate: 0.011438396227480507\n",
      "Epoch [97/100000], Training Loss: 101.04146575927734, Learning Rate: 0.011438396227480507\n",
      "Epoch [98/100000], Training Loss: 102.04541015625, Learning Rate: 0.011438396227480507\n",
      "Epoch [99/100000], Training Loss: 102.79669952392578, Learning Rate: 0.011438396227480507\n",
      "Epoch [100/100000], Training Loss: 100.95171356201172, Learning Rate: 0.011438396227480507\n",
      "Epoch [101/100000], Training Loss: 101.93506622314453, Learning Rate: 0.010294556604732457\n",
      "Epoch [102/100000], Training Loss: 102.269287109375, Learning Rate: 0.010294556604732457\n",
      "Epoch [103/100000], Training Loss: 103.19967651367188, Learning Rate: 0.010294556604732457\n",
      "Epoch [104/100000], Training Loss: 102.99616241455078, Learning Rate: 0.010294556604732457\n",
      "Epoch [105/100000], Training Loss: 102.01581573486328, Learning Rate: 0.010294556604732457\n",
      "Epoch [106/100000], Training Loss: 103.18121337890625, Learning Rate: 0.010294556604732457\n",
      "Epoch [107/100000], Training Loss: 102.80155181884766, Learning Rate: 0.010294556604732457\n",
      "Epoch [108/100000], Training Loss: 102.91755676269531, Learning Rate: 0.010294556604732457\n",
      "Epoch [109/100000], Training Loss: 101.88131713867188, Learning Rate: 0.010294556604732457\n",
      "Epoch [110/100000], Training Loss: 101.4634017944336, Learning Rate: 0.010294556604732457\n",
      "Epoch [111/100000], Training Loss: 103.3133773803711, Learning Rate: 0.010294556604732457\n",
      "Epoch [112/100000], Training Loss: 102.01546478271484, Learning Rate: 0.010294556604732457\n",
      "Epoch [113/100000], Training Loss: 102.2938232421875, Learning Rate: 0.010294556604732457\n",
      "Epoch [114/100000], Training Loss: 103.06983184814453, Learning Rate: 0.010294556604732457\n",
      "Epoch [115/100000], Training Loss: 100.38130950927734, Learning Rate: 0.010294556604732457\n",
      "Epoch [116/100000], Training Loss: 102.89176940917969, Learning Rate: 0.010294556604732457\n",
      "Epoch [117/100000], Training Loss: 100.92634582519531, Learning Rate: 0.010294556604732457\n",
      "Epoch [118/100000], Training Loss: 101.58462524414062, Learning Rate: 0.010294556604732457\n",
      "Epoch [119/100000], Training Loss: 103.40335845947266, Learning Rate: 0.010294556604732457\n",
      "Epoch [120/100000], Training Loss: 101.96881103515625, Learning Rate: 0.010294556604732457\n",
      "Epoch [121/100000], Training Loss: 101.59164428710938, Learning Rate: 0.010294556604732457\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-8ae134bfe0a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_ELBO_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mLQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mLP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogposterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLQ\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mLP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/BayesianNN/Experiments/Foong_L1W50/setup.py\u001b[0m in \u001b[0;36mlogposterior\u001b[0;34m(theta, x, y, sigma_noise)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mloglikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loglikelihood_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogposterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogprior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloglikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlogposterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/BayesianNN/Experiments/Foong_L1W50/setup.py\u001b[0m in \u001b[0;36mloglikelihood\u001b[0;34m(theta, x, y, sigma_noise)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloglikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_log_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msigma_noise\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloglikelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_loss = []\n",
    "for t in range(max_iter - 1):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = q.sample(n_ELBO_samples)\n",
    "    LQ = q.log_prob(z)\n",
    "    LP = logposterior(z, x_train, y_train, sigma_noise=0.1)\n",
    "    L = (LQ - LP).mean()\n",
    "\n",
    "    L.backward()\n",
    "\n",
    "    training_loss.append(L.detach().clone().cpu().numpy())\n",
    "\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(L.detach().clone().cpu().numpy())\n",
    "    if lr < min_lr:\n",
    "        break\n",
    "        \n",
    "    stats = 'Epoch [{}/{}], Training Loss: {}, Learning Rate: {}'.format(t, max_iter, L, lr)\n",
    "    print(stats)\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
