{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/ColCarroll/minimc.git\n",
    "\"\"\"\n",
    "\n",
    "def leapfrog(q, p, dVdq, potential, path_len, step_size):\n",
    "    \"\"\"Leapfrog integrator for Hamiltonian Monte Carlo.\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : np.floatX\n",
    "        Initial position\n",
    "    p : np.floatX\n",
    "        Initial momentum\n",
    "    dVdq : np.floatX\n",
    "        Gradient of the potential at the initial coordinates\n",
    "    potential : callable\n",
    "        Value and gradient of the potential\n",
    "    path_len : float\n",
    "        How long to integrate for\n",
    "    step_size : float\n",
    "        How long each integration step should be\n",
    "    Returns\n",
    "    -------\n",
    "    q, p : np.floatX, np.floatX\n",
    "        New position and momentum\n",
    "    \"\"\"\n",
    "    q, p = np.copy(q), np.copy(p)\n",
    "\n",
    "    p -= step_size * dVdq / 2  # half step\n",
    "    for _ in np.arange(path_len):#np.arange(np.round(path_len / step_size) - 1):\n",
    "        q += step_size * p  # whole step\n",
    "        V, dVdq = potential(q)\n",
    "        p -= step_size * dVdq  # whole step\n",
    "    q += step_size * p  # whole step\n",
    "    V, dVdq = potential(q)\n",
    "    p -= step_size * dVdq / 2  # half step\n",
    "\n",
    "    # momentum flip at end\n",
    "    return q, -p, V, dVdq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/ColCarroll/minimc.git\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def hamiltonian_monte_carlo(\n",
    "    n_samples,\n",
    "    potential,\n",
    "    initial_position,\n",
    "    initial_potential=None,\n",
    "    initial_potential_grad=None,\n",
    "    tune=500,\n",
    "    path_len=100,\n",
    "    initial_step_size=0.1,\n",
    "    integrator=leapfrog,\n",
    "    max_energy_change=1000.0,\n",
    "):\n",
    "    \"\"\"Run Hamiltonian Monte Carlo sampling.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples to return\n",
    "    negative_log_prob : callable\n",
    "        The negative log probability to sample from\n",
    "    initial_position : np.array\n",
    "        A place to start sampling from.\n",
    "    tune: int\n",
    "        Number of iterations to run tuning\n",
    "    path_len : float\n",
    "        How long each integration path is. Smaller is faster and more correlated.\n",
    "    initial_step_size : float\n",
    "        How long each integration step is. This will be tuned automatically.\n",
    "    max_energy_change : float\n",
    "        The largest tolerable integration error. Transitions with energy changes\n",
    "        larger than this will be declared divergences.\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        Array of length `n_samples`.\n",
    "    \"\"\"\n",
    "    initial_position = np.array(initial_position)\n",
    "    if initial_potential is None or initial_potential_grad is None:\n",
    "        initial_potential, initial_potential_grad = potential(initial_position)\n",
    "\n",
    "    # collect all our samples in a list\n",
    "    samples = [initial_position]\n",
    "\n",
    "    # Keep a single object for momentum resamplingtqdm\n",
    "    momentum = st.norm(0, 1)\n",
    "\n",
    "    step_size = initial_step_size\n",
    "    step_size_tuning = DualAveragingStepSize(step_size)\n",
    "    \n",
    "    # If initial_position is a 10d vector and n_samples is 100, we want 100 x 10 momentum draws\n",
    "    # we can do this in one call to np.random.normal, and iterate over rows\n",
    "    size = (n_samples + tune,) + initial_position.shape[:1]\n",
    "    for idx, p0 in tqdm(enumerate(momentum.rvs(size=size)), total=size[0]):\n",
    "        # Integrate over our path to get a new position and momentum\n",
    "        q_new, p_new, final_V, final_dVdq = integrator(\n",
    "            samples[-1],\n",
    "            p0,\n",
    "            initial_potential_grad,\n",
    "            potential,\n",
    "            path_len=2\n",
    "            * np.random.rand()\n",
    "            * path_len,  # We jitter the path length a bit\n",
    "            step_size=step_size,\n",
    "        )\n",
    "\n",
    "        \n",
    "        start_log_p = np.sum(momentum.logpdf(p0)) - initial_potential\n",
    "        new_log_p = np.sum(momentum.logpdf(p_new)) - final_V\n",
    "        energy_change = new_log_p - start_log_p\n",
    "\n",
    "        # Set accept prob to 0.0 if energy_change is `NaN` which may be\n",
    "        # the case for a diverging trajectory when using a large step size.\n",
    "        if np.isnan(np.exp(energy_change)) or np.isnan(p_new).sum():\n",
    "            p_accept = 0.\n",
    "        else:\n",
    "            p_accept = min(1, np.exp(energy_change))\n",
    "\n",
    "        # Check Metropolis acceptance criterion\n",
    "        if np.random.rand() < p_accept:\n",
    "            samples.append(q_new)\n",
    "            initial_potential = final_V\n",
    "            initial_potential_grad = final_dVdq\n",
    "        else:\n",
    "            samples.append(np.copy(samples[-1]))\n",
    "        \n",
    "        if idx < tune - 1:\n",
    "            step_size, _ = step_size_tuning.update(p_accept)\n",
    "        elif idx == tune - 1:\n",
    "            _, step_size = step_size_tuning.update(p_accept)\n",
    "        \n",
    "    return np.array(samples[1 + tune :])\n",
    "\n",
    "\n",
    "class DualAveragingStepSize:\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_step_size,\n",
    "        target_accept=0.8,\n",
    "        gamma=0.05,\n",
    "        t0=10.0,\n",
    "        kappa=0.75,\n",
    "    ):\n",
    "        \"\"\"Tune the step size to achieve a desired target acceptance.\n",
    "        Uses stochastic approximation of Robbins and Monro (1951), described in\n",
    "        Hoffman and Gelman (2013), section 3.2.1, and using those default values.\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_step_size: float > 0\n",
    "            Used to set a reasonable value for the stochastic step to drift towards\n",
    "        target_accept: float in (0, 1)\n",
    "            Will try to find a step size that accepts this percent of proposals\n",
    "        gamma: float\n",
    "            How quickly the stochastic step size reverts to a value mu\n",
    "        t0: float > 0\n",
    "            Larger values stabilize step size exploration early, while perhaps slowing\n",
    "            convergence\n",
    "        kappa: float in (0.5, 1]\n",
    "            The smaller kappa is, the faster we forget earlier step size iterates\n",
    "        \"\"\"\n",
    "        self.mu = np.log(10 * initial_step_size)\n",
    "        self.target_accept = target_accept\n",
    "        self.gamma = gamma\n",
    "        self.t = t0\n",
    "        self.kappa = kappa\n",
    "        self.error_sum = 0\n",
    "        self.log_averaged_step = 0\n",
    "\n",
    "    def update(self, p_accept):\n",
    "        \"\"\"Propose a new step size.\n",
    "        This method returns both a stochastic step size and a dual-averaged\n",
    "        step size. While tuning, the HMC algorithm should use the stochastic\n",
    "        step size and call `update` every loop. After tuning, HMC should use\n",
    "        the dual-averaged step size for sampling.\n",
    "        Parameters\n",
    "        ----------\n",
    "        p_accept: float\n",
    "            The probability of the previous HMC proposal being accepted\n",
    "        Returns\n",
    "        -------\n",
    "        float, float\n",
    "            A stochastic step size, and a dual-averaged step size\n",
    "        \"\"\"\n",
    "        self.error_sum += self.target_accept - p_accept\n",
    "        log_step = self.mu - self.error_sum / (np.sqrt(self.t) * self.gamma)\n",
    "        eta = self.t ** -self.kappa\n",
    "        self.log_averaged_step = (\n",
    "            eta * log_step + (1 - eta) * self.log_averaged_step\n",
    "        )\n",
    "        self.t += 1\n",
    "        return np.exp(log_step), np.exp(self.log_averaged_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device ='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools import logmvn01pdf\n",
    "def potential(x):\n",
    "    theta=torch.Tensor(x).requires_grad_(True).float()\n",
    "    #print(x)\n",
    "    lp=logmvn01pdf(theta.unsqueeze(0),'cpu')\n",
    "    lp.backward()\n",
    "    #print(x)\n",
    "    return -lp.detach().numpy(), -theta.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d344f7dd031408685a6fd9d8d881360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.534726115895444\n",
      "0.026774495729971513\n",
      "0.099112764584176\n",
      "0.32756606320548126\n",
      "0.9935978993488533\n",
      "2.791532306013573\n",
      "0.04948885091212659\n",
      "0.1426748280204074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ift/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: overflow encountered in multiply\n",
      "/Users/ift/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3869279639852953\n",
      "0.9934924692942829\n",
      "0.9857377379839116\n",
      "2.360451252644084\n",
      "0.0763746049145462\n",
      "0.18609263469040602\n",
      "0.43480763162505115\n",
      "0.9099519541372365\n",
      "1.9975741852720514\n",
      "0.09070018172725469\n",
      "0.2012492513338335\n",
      "0.43468382292407126\n",
      "0.9115401954368447\n",
      "1.872562343889791\n",
      "3.7604364665145793\n",
      "0.22742595648575173\n",
      "0.4530839094257336\n",
      "0.9010785061316892\n",
      "1.7576200843130125\n",
      "1.1833620351643672\n",
      "2.252626751982855\n",
      "0.17197045502784605\n",
      "0.3309353068472925\n",
      "0.6247205117710963\n",
      "1.1646223080008522\n",
      "1.4710461741177439\n",
      "2.676701480246688\n",
      "0.24376387973338137\n",
      "0.4132405879234686\n",
      "0.7476551229864113\n",
      "1.335869041531436\n",
      "2.358534539054263\n",
      "0.24333286143314253\n",
      "0.43201791760545233\n",
      "0.7584497251156164\n",
      "1.2244632642000908\n",
      "2.106335829570718\n",
      "0.2418849610859379\n",
      "0.41809725468715714\n",
      "0.6990827422210567\n",
      "1.1857068156493844\n",
      "1.9930078680318073\n",
      "0.2511592353947238\n",
      "0.4159507790327412\n",
      "0.696223199711764\n",
      "1.1328131919905038\n",
      "1.8436461843245902\n",
      "0.27656117684346326\n",
      "0.45658219114305393\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-93b69ae30415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m samples = hamiltonian_monte_carlo(5000, potential,\n\u001b[0;32m----> 3\u001b[0;31m                                   \u001b[0minitial_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#                                  initial_step_size=0.0001,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#                                  tune=,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-decbb528f59d>\u001b[0m in \u001b[0;36mhamiltonian_monte_carlo\u001b[0;34m(n_samples, potential, initial_position, initial_potential, initial_potential_grad, tune, path_len, initial_step_size, integrator, max_energy_change)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;34m*\u001b[0m \u001b[0mpath_len\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# We jitter the path length a bit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         )\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-299dec9af742>\u001b[0m in \u001b[0;36mleapfrog\u001b[0;34m(q, p, dVdq, potential, path_len, step_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#np.arange(np.round(path_len / step_size) - 1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m  \u001b[0;31m# whole step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdVdq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpotential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdVdq\u001b[0m  \u001b[0;31m# whole step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m  \u001b[0;31m# whole step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-0014d3fc3d67>\u001b[0m in \u001b[0;36mpotential\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogmvn01pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "samples = hamiltonian_monte_carlo(5000, potential,\n",
    "                                  initial_position=np.zeros(1), \n",
    "#                                  initial_step_size=0.0001,\n",
    "#                                  tune=,\n",
    "#                                 path_len=10,\n",
    "                                 )\n",
    "sns.distplot(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f829213cd90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns                                 \n",
    "sns.distplot(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Experiments.foong import Setup\n",
    "layerwidth=50\n",
    "nblayers=1\n",
    "setup=Setup(device,layerwidth=layerwidth,nblayers=nblayers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target density #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_count=setup.param_count\n",
    "logposterior=setup.logposterior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 219838.1875, Learning Rate: 0.01\n",
      "Epoch [2/10000], Loss: 189238.875, Learning Rate: 0.01\n",
      "Epoch [3/10000], Loss: 161335.546875, Learning Rate: 0.01\n",
      "Epoch [4/10000], Loss: 136111.078125, Learning Rate: 0.01\n",
      "Epoch [5/10000], Loss: 113538.8671875, Learning Rate: 0.01\n",
      "Epoch [6/10000], Loss: 93570.1171875, Learning Rate: 0.01\n",
      "Epoch [7/10000], Loss: 76126.09375, Learning Rate: 0.01\n",
      "Epoch [8/10000], Loss: 61096.75390625, Learning Rate: 0.01\n",
      "Epoch [9/10000], Loss: 48341.31640625, Learning Rate: 0.01\n",
      "Epoch [10/10000], Loss: 37691.25390625, Learning Rate: 0.01\n",
      "Epoch [11/10000], Loss: 28955.0, Learning Rate: 0.01\n",
      "Epoch [12/10000], Loss: 21924.310546875, Learning Rate: 0.01\n",
      "Epoch [13/10000], Loss: 16382.11328125, Learning Rate: 0.01\n",
      "Epoch [14/10000], Loss: 12111.4482421875, Learning Rate: 0.01\n",
      "Epoch [15/10000], Loss: 8904.541015625, Learning Rate: 0.01\n",
      "Epoch [16/10000], Loss: 6570.4697265625, Learning Rate: 0.01\n",
      "Epoch [17/10000], Loss: 4940.4296875, Learning Rate: 0.01\n",
      "Epoch [18/10000], Loss: 3870.137939453125, Learning Rate: 0.01\n",
      "Epoch [19/10000], Loss: 3239.638671875, Learning Rate: 0.01\n",
      "Epoch [20/10000], Loss: 2951.08642578125, Learning Rate: 0.01\n",
      "Epoch [21/10000], Loss: 2925.150146484375, Learning Rate: 0.01\n",
      "Epoch [22/10000], Loss: 3096.8056640625, Learning Rate: 0.01\n",
      "Epoch [23/10000], Loss: 3411.191650390625, Learning Rate: 0.01\n",
      "Epoch [24/10000], Loss: 3820.07861328125, Learning Rate: 0.01\n",
      "Epoch [25/10000], Loss: 4279.3203125, Learning Rate: 0.01\n",
      "Epoch [26/10000], Loss: 4747.49169921875, Learning Rate: 0.01\n",
      "Epoch [27/10000], Loss: 5185.7109375, Learning Rate: 0.01\n",
      "Epoch [28/10000], Loss: 5558.55224609375, Learning Rate: 0.01\n",
      "Epoch [29/10000], Loss: 5835.697265625, Learning Rate: 0.01\n",
      "Epoch [30/10000], Loss: 5993.9111328125, Learning Rate: 0.01\n",
      "Epoch [31/10000], Loss: 6018.82958984375, Learning Rate: 0.01\n",
      "Epoch [32/10000], Loss: 5906.12255859375, Learning Rate: 0.01\n",
      "Epoch [33/10000], Loss: 5661.67626953125, Learning Rate: 0.01\n",
      "Epoch [34/10000], Loss: 5300.81103515625, Learning Rate: 0.01\n",
      "Epoch [35/10000], Loss: 4846.54248046875, Learning Rate: 0.01\n",
      "Epoch [36/10000], Loss: 4327.208984375, Learning Rate: 0.01\n",
      "Epoch [37/10000], Loss: 3773.75927734375, Learning Rate: 0.01\n",
      "Epoch [38/10000], Loss: 3217.001220703125, Learning Rate: 0.01\n",
      "Epoch [39/10000], Loss: 2685.105224609375, Learning Rate: 0.01\n",
      "Epoch [40/10000], Loss: 2201.569580078125, Learning Rate: 0.01\n",
      "Epoch [41/10000], Loss: 1783.8023681640625, Learning Rate: 0.01\n",
      "Epoch [42/10000], Loss: 1442.3961181640625, Learning Rate: 0.01\n",
      "Epoch [43/10000], Loss: 1181.1060791015625, Learning Rate: 0.01\n",
      "Epoch [44/10000], Loss: 997.4664306640625, Learning Rate: 0.01\n",
      "Epoch [45/10000], Loss: 883.9207763671875, Learning Rate: 0.01\n",
      "Epoch [46/10000], Loss: 829.2993774414062, Learning Rate: 0.01\n",
      "Epoch [47/10000], Loss: 820.4339599609375, Learning Rate: 0.01\n",
      "Epoch [48/10000], Loss: 843.7111206054688, Learning Rate: 0.01\n",
      "Epoch [49/10000], Loss: 886.3969116210938, Learning Rate: 0.01\n",
      "Epoch [50/10000], Loss: 937.6002197265625, Learning Rate: 0.01\n",
      "Epoch [51/10000], Loss: 988.831787109375, Learning Rate: 0.01\n",
      "Epoch [52/10000], Loss: 1034.15869140625, Learning Rate: 0.01\n",
      "Epoch [53/10000], Loss: 1070.0289306640625, Learning Rate: 0.01\n",
      "Epoch [54/10000], Loss: 1094.8604736328125, Learning Rate: 0.01\n",
      "Epoch [55/10000], Loss: 1108.5130615234375, Learning Rate: 0.01\n",
      "Epoch [56/10000], Loss: 1111.7513427734375, Learning Rate: 0.01\n",
      "Epoch [57/10000], Loss: 1105.7843017578125, Learning Rate: 0.01\n",
      "Epoch [58/10000], Loss: 1091.9307861328125, Learning Rate: 0.01\n",
      "Epoch [59/10000], Loss: 1071.4345703125, Learning Rate: 0.01\n",
      "Epoch [60/10000], Loss: 1045.4102783203125, Learning Rate: 0.01\n",
      "Epoch [61/10000], Loss: 1014.8885498046875, Learning Rate: 0.01\n",
      "Epoch [62/10000], Loss: 980.9051513671875, Learning Rate: 0.01\n",
      "Epoch [63/10000], Loss: 944.59619140625, Learning Rate: 0.01\n",
      "Epoch [64/10000], Loss: 907.2452392578125, Learning Rate: 0.01\n",
      "Epoch [65/10000], Loss: 870.2783203125, Learning Rate: 0.01\n",
      "Epoch [66/10000], Loss: 835.1849365234375, Learning Rate: 0.01\n",
      "Epoch [67/10000], Loss: 803.395751953125, Learning Rate: 0.01\n",
      "Epoch [68/10000], Loss: 776.133544921875, Learning Rate: 0.01\n",
      "Epoch [69/10000], Loss: 754.27001953125, Learning Rate: 0.01\n",
      "Epoch [70/10000], Loss: 738.2251586914062, Learning Rate: 0.01\n",
      "Epoch [71/10000], Loss: 727.920654296875, Learning Rate: 0.01\n",
      "Epoch [72/10000], Loss: 722.8025512695312, Learning Rate: 0.01\n",
      "Epoch [73/10000], Loss: 721.92919921875, Learning Rate: 0.01\n",
      "Epoch [74/10000], Loss: 724.1048583984375, Learning Rate: 0.01\n",
      "Epoch [75/10000], Loss: 728.03662109375, Learning Rate: 0.01\n",
      "Epoch [76/10000], Loss: 732.4920043945312, Learning Rate: 0.01\n",
      "Epoch [77/10000], Loss: 736.4302978515625, Learning Rate: 0.01\n",
      "Epoch [78/10000], Loss: 739.0877075195312, Learning Rate: 0.01\n",
      "Epoch [79/10000], Loss: 740.0193481445312, Learning Rate: 0.01\n",
      "Epoch [80/10000], Loss: 739.0865478515625, Learning Rate: 0.01\n",
      "Epoch [81/10000], Loss: 736.402587890625, Learning Rate: 0.01\n",
      "Epoch [82/10000], Loss: 732.2628173828125, Learning Rate: 0.01\n",
      "Epoch [83/10000], Loss: 727.05859375, Learning Rate: 0.01\n",
      "Epoch [84/10000], Loss: 721.2010498046875, Learning Rate: 0.01\n",
      "Epoch [85/10000], Loss: 715.0625, Learning Rate: 0.01\n",
      "Epoch [86/10000], Loss: 708.9404907226562, Learning Rate: 0.01\n",
      "Epoch [87/10000], Loss: 703.0457763671875, Learning Rate: 0.01\n",
      "Epoch [88/10000], Loss: 697.5073852539062, Learning Rate: 0.01\n",
      "Epoch [89/10000], Loss: 692.3909912109375, Learning Rate: 0.01\n",
      "Epoch [90/10000], Loss: 687.7203369140625, Learning Rate: 0.01\n",
      "Epoch [91/10000], Loss: 683.4964599609375, Learning Rate: 0.01\n",
      "Epoch [92/10000], Loss: 679.7108154296875, Learning Rate: 0.01\n",
      "Epoch [93/10000], Loss: 676.350830078125, Learning Rate: 0.01\n",
      "Epoch [94/10000], Loss: 673.3969116210938, Learning Rate: 0.01\n",
      "Epoch [95/10000], Loss: 670.8190307617188, Learning Rate: 0.01\n",
      "Epoch [96/10000], Loss: 668.5689086914062, Learning Rate: 0.01\n",
      "Epoch [97/10000], Loss: 666.5794677734375, Learning Rate: 0.01\n",
      "Epoch [98/10000], Loss: 664.7659912109375, Learning Rate: 0.01\n",
      "Epoch [99/10000], Loss: 663.0323486328125, Learning Rate: 0.01\n",
      "Epoch [100/10000], Loss: 661.2817993164062, Learning Rate: 0.01\n",
      "Epoch [101/10000], Loss: 659.428955078125, Learning Rate: 0.01\n",
      "Epoch [102/10000], Loss: 657.4099731445312, Learning Rate: 0.01\n",
      "Epoch [103/10000], Loss: 655.188720703125, Learning Rate: 0.01\n",
      "Epoch [104/10000], Loss: 652.7611694335938, Learning Rate: 0.01\n",
      "Epoch [105/10000], Loss: 650.1506958007812, Learning Rate: 0.01\n",
      "Epoch [106/10000], Loss: 647.4019775390625, Learning Rate: 0.01\n",
      "Epoch [107/10000], Loss: 644.5714111328125, Learning Rate: 0.01\n",
      "Epoch [108/10000], Loss: 641.716796875, Learning Rate: 0.01\n",
      "Epoch [109/10000], Loss: 638.8892822265625, Learning Rate: 0.01\n",
      "Epoch [110/10000], Loss: 636.126220703125, Learning Rate: 0.01\n",
      "Epoch [111/10000], Loss: 633.4501953125, Learning Rate: 0.01\n",
      "Epoch [112/10000], Loss: 630.8673706054688, Learning Rate: 0.01\n",
      "Epoch [113/10000], Loss: 628.3726806640625, Learning Rate: 0.01\n",
      "Epoch [114/10000], Loss: 625.9517822265625, Learning Rate: 0.01\n",
      "Epoch [115/10000], Loss: 623.5872192382812, Learning Rate: 0.01\n",
      "Epoch [116/10000], Loss: 621.2615356445312, Learning Rate: 0.01\n",
      "Epoch [117/10000], Loss: 618.9595947265625, Learning Rate: 0.01\n",
      "Epoch [118/10000], Loss: 616.6693115234375, Learning Rate: 0.01\n",
      "Epoch [119/10000], Loss: 614.382568359375, Learning Rate: 0.01\n",
      "Epoch [120/10000], Loss: 612.0938720703125, Learning Rate: 0.01\n",
      "Epoch [121/10000], Loss: 609.7987060546875, Learning Rate: 0.01\n",
      "Epoch [122/10000], Loss: 607.4942016601562, Learning Rate: 0.01\n",
      "Epoch [123/10000], Loss: 605.1775512695312, Learning Rate: 0.01\n",
      "Epoch [124/10000], Loss: 602.846923828125, Learning Rate: 0.01\n",
      "Epoch [125/10000], Loss: 600.50048828125, Learning Rate: 0.01\n",
      "Epoch [126/10000], Loss: 598.1398315429688, Learning Rate: 0.01\n",
      "Epoch [127/10000], Loss: 595.766845703125, Learning Rate: 0.01\n",
      "Epoch [128/10000], Loss: 593.3854370117188, Learning Rate: 0.01\n",
      "Epoch [129/10000], Loss: 591.0013427734375, Learning Rate: 0.01\n",
      "Epoch [130/10000], Loss: 588.6203002929688, Learning Rate: 0.01\n",
      "Epoch [131/10000], Loss: 586.2471923828125, Learning Rate: 0.01\n",
      "Epoch [132/10000], Loss: 583.8870849609375, Learning Rate: 0.01\n",
      "Epoch [133/10000], Loss: 581.541748046875, Learning Rate: 0.01\n",
      "Epoch [134/10000], Loss: 579.21142578125, Learning Rate: 0.01\n",
      "Epoch [135/10000], Loss: 576.8953857421875, Learning Rate: 0.01\n",
      "Epoch [136/10000], Loss: 574.5904541015625, Learning Rate: 0.01\n",
      "Epoch [137/10000], Loss: 572.2935180664062, Learning Rate: 0.01\n",
      "Epoch [138/10000], Loss: 570.0012817382812, Learning Rate: 0.01\n",
      "Epoch [139/10000], Loss: 567.711181640625, Learning Rate: 0.01\n",
      "Epoch [140/10000], Loss: 565.42138671875, Learning Rate: 0.01\n",
      "Epoch [141/10000], Loss: 563.131103515625, Learning Rate: 0.01\n",
      "Epoch [142/10000], Loss: 560.840087890625, Learning Rate: 0.01\n",
      "Epoch [143/10000], Loss: 558.5488891601562, Learning Rate: 0.01\n",
      "Epoch [144/10000], Loss: 556.2583618164062, Learning Rate: 0.01\n",
      "Epoch [145/10000], Loss: 553.9690551757812, Learning Rate: 0.01\n",
      "Epoch [146/10000], Loss: 551.682373046875, Learning Rate: 0.01\n",
      "Epoch [147/10000], Loss: 549.3981323242188, Learning Rate: 0.01\n",
      "Epoch [148/10000], Loss: 547.1173706054688, Learning Rate: 0.01\n",
      "Epoch [149/10000], Loss: 544.8406982421875, Learning Rate: 0.01\n",
      "Epoch [150/10000], Loss: 542.568115234375, Learning Rate: 0.01\n",
      "Epoch [151/10000], Loss: 540.2996215820312, Learning Rate: 0.01\n",
      "Epoch [152/10000], Loss: 538.0361328125, Learning Rate: 0.01\n",
      "Epoch [153/10000], Loss: 535.7778930664062, Learning Rate: 0.01\n",
      "Epoch [154/10000], Loss: 533.5245971679688, Learning Rate: 0.01\n",
      "Epoch [155/10000], Loss: 531.2764892578125, Learning Rate: 0.01\n",
      "Epoch [156/10000], Loss: 529.033447265625, Learning Rate: 0.01\n",
      "Epoch [157/10000], Loss: 526.795166015625, Learning Rate: 0.01\n",
      "Epoch [158/10000], Loss: 524.5614013671875, Learning Rate: 0.01\n",
      "Epoch [159/10000], Loss: 522.3317260742188, Learning Rate: 0.01\n",
      "Epoch [160/10000], Loss: 520.1058959960938, Learning Rate: 0.01\n",
      "Epoch [161/10000], Loss: 517.884033203125, Learning Rate: 0.01\n",
      "Epoch [162/10000], Loss: 515.665771484375, Learning Rate: 0.01\n",
      "Epoch [163/10000], Loss: 513.4514770507812, Learning Rate: 0.01\n",
      "Epoch [164/10000], Loss: 511.2410583496094, Learning Rate: 0.01\n",
      "Epoch [165/10000], Loss: 509.0351867675781, Learning Rate: 0.01\n",
      "Epoch [166/10000], Loss: 506.833984375, Learning Rate: 0.01\n",
      "Epoch [167/10000], Loss: 504.6379089355469, Learning Rate: 0.01\n",
      "Epoch [168/10000], Loss: 502.4464111328125, Learning Rate: 0.01\n",
      "Epoch [169/10000], Loss: 500.2603759765625, Learning Rate: 0.01\n",
      "Epoch [170/10000], Loss: 498.0791320800781, Learning Rate: 0.01\n",
      "Epoch [171/10000], Loss: 495.9032287597656, Learning Rate: 0.01\n",
      "Epoch [172/10000], Loss: 493.7323303222656, Learning Rate: 0.01\n",
      "Epoch [173/10000], Loss: 491.56689453125, Learning Rate: 0.01\n",
      "Epoch [174/10000], Loss: 489.40667724609375, Learning Rate: 0.01\n",
      "Epoch [175/10000], Loss: 487.25152587890625, Learning Rate: 0.01\n",
      "Epoch [176/10000], Loss: 485.10174560546875, Learning Rate: 0.01\n",
      "Epoch [177/10000], Loss: 482.95721435546875, Learning Rate: 0.01\n",
      "Epoch [178/10000], Loss: 480.81787109375, Learning Rate: 0.01\n",
      "Epoch [179/10000], Loss: 478.68402099609375, Learning Rate: 0.01\n",
      "Epoch [180/10000], Loss: 476.5555114746094, Learning Rate: 0.01\n",
      "Epoch [181/10000], Loss: 474.43231201171875, Learning Rate: 0.01\n",
      "Epoch [182/10000], Loss: 472.31451416015625, Learning Rate: 0.01\n",
      "Epoch [183/10000], Loss: 470.2022399902344, Learning Rate: 0.01\n",
      "Epoch [184/10000], Loss: 468.09564208984375, Learning Rate: 0.01\n",
      "Epoch [185/10000], Loss: 465.99444580078125, Learning Rate: 0.01\n",
      "Epoch [186/10000], Loss: 463.89923095703125, Learning Rate: 0.01\n",
      "Epoch [187/10000], Loss: 461.8096923828125, Learning Rate: 0.01\n",
      "Epoch [188/10000], Loss: 459.72607421875, Learning Rate: 0.01\n",
      "Epoch [189/10000], Loss: 457.6481628417969, Learning Rate: 0.01\n",
      "Epoch [190/10000], Loss: 455.57635498046875, Learning Rate: 0.01\n",
      "Epoch [191/10000], Loss: 453.51043701171875, Learning Rate: 0.01\n",
      "Epoch [192/10000], Loss: 451.45013427734375, Learning Rate: 0.01\n",
      "Epoch [193/10000], Loss: 449.39630126953125, Learning Rate: 0.01\n",
      "Epoch [194/10000], Loss: 447.3484802246094, Learning Rate: 0.01\n",
      "Epoch [195/10000], Loss: 445.3065185546875, Learning Rate: 0.01\n",
      "Epoch [196/10000], Loss: 443.27069091796875, Learning Rate: 0.01\n",
      "Epoch [197/10000], Loss: 441.24127197265625, Learning Rate: 0.01\n",
      "Epoch [198/10000], Loss: 439.2176513671875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [199/10000], Loss: 437.20037841796875, Learning Rate: 0.01\n",
      "Epoch [200/10000], Loss: 435.18951416015625, Learning Rate: 0.01\n",
      "Epoch [201/10000], Loss: 433.184814453125, Learning Rate: 0.01\n",
      "Epoch [202/10000], Loss: 431.1864929199219, Learning Rate: 0.01\n",
      "Epoch [203/10000], Loss: 429.194580078125, Learning Rate: 0.01\n",
      "Epoch [204/10000], Loss: 427.20892333984375, Learning Rate: 0.01\n",
      "Epoch [205/10000], Loss: 425.2298583984375, Learning Rate: 0.01\n",
      "Epoch [206/10000], Loss: 423.25714111328125, Learning Rate: 0.01\n",
      "Epoch [207/10000], Loss: 421.291015625, Learning Rate: 0.01\n",
      "Epoch [208/10000], Loss: 419.33160400390625, Learning Rate: 0.01\n",
      "Epoch [209/10000], Loss: 417.3787536621094, Learning Rate: 0.01\n",
      "Epoch [210/10000], Loss: 415.43218994140625, Learning Rate: 0.01\n",
      "Epoch [211/10000], Loss: 413.49237060546875, Learning Rate: 0.01\n",
      "Epoch [212/10000], Loss: 411.5593566894531, Learning Rate: 0.01\n",
      "Epoch [213/10000], Loss: 409.63275146484375, Learning Rate: 0.01\n",
      "Epoch [214/10000], Loss: 407.7129821777344, Learning Rate: 0.01\n",
      "Epoch [215/10000], Loss: 405.8001708984375, Learning Rate: 0.01\n",
      "Epoch [216/10000], Loss: 403.89385986328125, Learning Rate: 0.01\n",
      "Epoch [217/10000], Loss: 401.99420166015625, Learning Rate: 0.01\n",
      "Epoch [218/10000], Loss: 400.10150146484375, Learning Rate: 0.01\n",
      "Epoch [219/10000], Loss: 398.2156982421875, Learning Rate: 0.01\n",
      "Epoch [220/10000], Loss: 396.33673095703125, Learning Rate: 0.01\n",
      "Epoch [221/10000], Loss: 394.46478271484375, Learning Rate: 0.01\n",
      "Epoch [222/10000], Loss: 392.599609375, Learning Rate: 0.01\n",
      "Epoch [223/10000], Loss: 390.741455078125, Learning Rate: 0.01\n",
      "Epoch [224/10000], Loss: 388.8902587890625, Learning Rate: 0.01\n",
      "Epoch [225/10000], Loss: 387.0458984375, Learning Rate: 0.01\n",
      "Epoch [226/10000], Loss: 385.20867919921875, Learning Rate: 0.01\n",
      "Epoch [227/10000], Loss: 383.3783264160156, Learning Rate: 0.01\n",
      "Epoch [228/10000], Loss: 381.5550842285156, Learning Rate: 0.01\n",
      "Epoch [229/10000], Loss: 379.73895263671875, Learning Rate: 0.01\n",
      "Epoch [230/10000], Loss: 377.9300537109375, Learning Rate: 0.01\n",
      "Epoch [231/10000], Loss: 376.1282653808594, Learning Rate: 0.01\n",
      "Epoch [232/10000], Loss: 374.33331298828125, Learning Rate: 0.01\n",
      "Epoch [233/10000], Loss: 372.54571533203125, Learning Rate: 0.01\n",
      "Epoch [234/10000], Loss: 370.76531982421875, Learning Rate: 0.01\n",
      "Epoch [235/10000], Loss: 368.99212646484375, Learning Rate: 0.01\n",
      "Epoch [236/10000], Loss: 367.22589111328125, Learning Rate: 0.01\n",
      "Epoch [237/10000], Loss: 365.46710205078125, Learning Rate: 0.01\n",
      "Epoch [238/10000], Loss: 363.71551513671875, Learning Rate: 0.01\n",
      "Epoch [239/10000], Loss: 361.97113037109375, Learning Rate: 0.01\n",
      "Epoch [240/10000], Loss: 360.2340393066406, Learning Rate: 0.01\n",
      "Epoch [241/10000], Loss: 358.5042724609375, Learning Rate: 0.01\n",
      "Epoch [242/10000], Loss: 356.78173828125, Learning Rate: 0.01\n",
      "Epoch [243/10000], Loss: 355.0667724609375, Learning Rate: 0.01\n",
      "Epoch [244/10000], Loss: 353.35882568359375, Learning Rate: 0.01\n",
      "Epoch [245/10000], Loss: 351.65838623046875, Learning Rate: 0.01\n",
      "Epoch [246/10000], Loss: 349.9653015136719, Learning Rate: 0.01\n",
      "Epoch [247/10000], Loss: 348.2796630859375, Learning Rate: 0.01\n",
      "Epoch [248/10000], Loss: 346.60125732421875, Learning Rate: 0.01\n",
      "Epoch [249/10000], Loss: 344.9302673339844, Learning Rate: 0.01\n",
      "Epoch [250/10000], Loss: 343.26666259765625, Learning Rate: 0.01\n",
      "Epoch [251/10000], Loss: 341.610595703125, Learning Rate: 0.01\n",
      "Epoch [252/10000], Loss: 339.9620056152344, Learning Rate: 0.01\n",
      "Epoch [253/10000], Loss: 338.3207702636719, Learning Rate: 0.01\n",
      "Epoch [254/10000], Loss: 336.68682861328125, Learning Rate: 0.01\n",
      "Epoch [255/10000], Loss: 335.0605163574219, Learning Rate: 0.01\n",
      "Epoch [256/10000], Loss: 333.4415588378906, Learning Rate: 0.01\n",
      "Epoch [257/10000], Loss: 331.8301696777344, Learning Rate: 0.01\n",
      "Epoch [258/10000], Loss: 330.2264404296875, Learning Rate: 0.01\n",
      "Epoch [259/10000], Loss: 328.6298522949219, Learning Rate: 0.01\n",
      "Epoch [260/10000], Loss: 327.04095458984375, Learning Rate: 0.01\n",
      "Epoch [261/10000], Loss: 325.4595947265625, Learning Rate: 0.01\n",
      "Epoch [262/10000], Loss: 323.8857116699219, Learning Rate: 0.01\n",
      "Epoch [263/10000], Loss: 322.31951904296875, Learning Rate: 0.01\n",
      "Epoch [264/10000], Loss: 320.760498046875, Learning Rate: 0.01\n",
      "Epoch [265/10000], Loss: 319.209228515625, Learning Rate: 0.01\n",
      "Epoch [266/10000], Loss: 317.6654052734375, Learning Rate: 0.01\n",
      "Epoch [267/10000], Loss: 316.1292419433594, Learning Rate: 0.01\n",
      "Epoch [268/10000], Loss: 314.6007080078125, Learning Rate: 0.01\n",
      "Epoch [269/10000], Loss: 313.0794982910156, Learning Rate: 0.01\n",
      "Epoch [270/10000], Loss: 311.5660095214844, Learning Rate: 0.01\n",
      "Epoch [271/10000], Loss: 310.06005859375, Learning Rate: 0.01\n",
      "Epoch [272/10000], Loss: 308.5616455078125, Learning Rate: 0.01\n",
      "Epoch [273/10000], Loss: 307.0706481933594, Learning Rate: 0.01\n",
      "Epoch [274/10000], Loss: 305.5874328613281, Learning Rate: 0.01\n",
      "Epoch [275/10000], Loss: 304.11163330078125, Learning Rate: 0.01\n",
      "Epoch [276/10000], Loss: 302.6433410644531, Learning Rate: 0.01\n",
      "Epoch [277/10000], Loss: 301.18292236328125, Learning Rate: 0.01\n",
      "Epoch [278/10000], Loss: 299.72998046875, Learning Rate: 0.01\n",
      "Epoch [279/10000], Loss: 298.2845458984375, Learning Rate: 0.01\n",
      "Epoch [280/10000], Loss: 296.84649658203125, Learning Rate: 0.01\n",
      "Epoch [281/10000], Loss: 295.4161682128906, Learning Rate: 0.01\n",
      "Epoch [282/10000], Loss: 293.99346923828125, Learning Rate: 0.01\n",
      "Epoch [283/10000], Loss: 292.578125, Learning Rate: 0.01\n",
      "Epoch [284/10000], Loss: 291.1703186035156, Learning Rate: 0.01\n",
      "Epoch [285/10000], Loss: 289.7703857421875, Learning Rate: 0.01\n",
      "Epoch [286/10000], Loss: 288.3776550292969, Learning Rate: 0.01\n",
      "Epoch [287/10000], Loss: 286.9927062988281, Learning Rate: 0.01\n",
      "Epoch [288/10000], Loss: 285.6150207519531, Learning Rate: 0.01\n",
      "Epoch [289/10000], Loss: 284.2451171875, Learning Rate: 0.01\n",
      "Epoch [290/10000], Loss: 282.88250732421875, Learning Rate: 0.01\n",
      "Epoch [291/10000], Loss: 281.5277099609375, Learning Rate: 0.01\n",
      "Epoch [292/10000], Loss: 280.18017578125, Learning Rate: 0.01\n",
      "Epoch [293/10000], Loss: 278.8402404785156, Learning Rate: 0.01\n",
      "Epoch [294/10000], Loss: 277.50762939453125, Learning Rate: 0.01\n",
      "Epoch [295/10000], Loss: 276.1827392578125, Learning Rate: 0.01\n",
      "Epoch [296/10000], Loss: 274.8652648925781, Learning Rate: 0.01\n",
      "Epoch [297/10000], Loss: 273.5552673339844, Learning Rate: 0.01\n",
      "Epoch [298/10000], Loss: 272.252685546875, Learning Rate: 0.01\n",
      "Epoch [299/10000], Loss: 270.9574890136719, Learning Rate: 0.01\n",
      "Epoch [300/10000], Loss: 269.6697692871094, Learning Rate: 0.01\n",
      "Epoch [301/10000], Loss: 268.38946533203125, Learning Rate: 0.01\n",
      "Epoch [302/10000], Loss: 267.1165771484375, Learning Rate: 0.01\n",
      "Epoch [303/10000], Loss: 265.85113525390625, Learning Rate: 0.01\n",
      "Epoch [304/10000], Loss: 264.59295654296875, Learning Rate: 0.01\n",
      "Epoch [305/10000], Loss: 263.34234619140625, Learning Rate: 0.01\n",
      "Epoch [306/10000], Loss: 262.0989990234375, Learning Rate: 0.01\n",
      "Epoch [307/10000], Loss: 260.8629150390625, Learning Rate: 0.01\n",
      "Epoch [308/10000], Loss: 259.63421630859375, Learning Rate: 0.01\n",
      "Epoch [309/10000], Loss: 258.41278076171875, Learning Rate: 0.01\n",
      "Epoch [310/10000], Loss: 257.1986999511719, Learning Rate: 0.01\n",
      "Epoch [311/10000], Loss: 255.99197387695312, Learning Rate: 0.01\n",
      "Epoch [312/10000], Loss: 254.7923583984375, Learning Rate: 0.01\n",
      "Epoch [313/10000], Loss: 253.60003662109375, Learning Rate: 0.01\n",
      "Epoch [314/10000], Loss: 252.4150390625, Learning Rate: 0.01\n",
      "Epoch [315/10000], Loss: 251.23716735839844, Learning Rate: 0.01\n",
      "Epoch [316/10000], Loss: 250.06655883789062, Learning Rate: 0.01\n",
      "Epoch [317/10000], Loss: 248.9029541015625, Learning Rate: 0.01\n",
      "Epoch [318/10000], Loss: 247.74667358398438, Learning Rate: 0.01\n",
      "Epoch [319/10000], Loss: 246.59732055664062, Learning Rate: 0.01\n",
      "Epoch [320/10000], Loss: 245.4554443359375, Learning Rate: 0.01\n",
      "Epoch [321/10000], Loss: 244.32032775878906, Learning Rate: 0.01\n",
      "Epoch [322/10000], Loss: 243.19241333007812, Learning Rate: 0.01\n",
      "Epoch [323/10000], Loss: 242.071533203125, Learning Rate: 0.01\n",
      "Epoch [324/10000], Loss: 240.95761108398438, Learning Rate: 0.01\n",
      "Epoch [325/10000], Loss: 239.8505859375, Learning Rate: 0.01\n",
      "Epoch [326/10000], Loss: 238.75083923339844, Learning Rate: 0.01\n",
      "Epoch [327/10000], Loss: 237.65773010253906, Learning Rate: 0.01\n",
      "Epoch [328/10000], Loss: 236.57183837890625, Learning Rate: 0.01\n",
      "Epoch [329/10000], Loss: 235.49258422851562, Learning Rate: 0.01\n",
      "Epoch [330/10000], Loss: 234.42039489746094, Learning Rate: 0.01\n",
      "Epoch [331/10000], Loss: 233.35496520996094, Learning Rate: 0.01\n",
      "Epoch [332/10000], Loss: 232.29635620117188, Learning Rate: 0.01\n",
      "Epoch [333/10000], Loss: 231.2445831298828, Learning Rate: 0.01\n",
      "Epoch [334/10000], Loss: 230.19949340820312, Learning Rate: 0.01\n",
      "Epoch [335/10000], Loss: 229.1612548828125, Learning Rate: 0.01\n",
      "Epoch [336/10000], Loss: 228.12969970703125, Learning Rate: 0.01\n",
      "Epoch [337/10000], Loss: 227.104736328125, Learning Rate: 0.01\n",
      "Epoch [338/10000], Loss: 226.08641052246094, Learning Rate: 0.01\n",
      "Epoch [339/10000], Loss: 225.07484436035156, Learning Rate: 0.01\n",
      "Epoch [340/10000], Loss: 224.06980895996094, Learning Rate: 0.01\n",
      "Epoch [341/10000], Loss: 223.07144165039062, Learning Rate: 0.01\n",
      "Epoch [342/10000], Loss: 222.0794677734375, Learning Rate: 0.01\n",
      "Epoch [343/10000], Loss: 221.09414672851562, Learning Rate: 0.01\n",
      "Epoch [344/10000], Loss: 220.11512756347656, Learning Rate: 0.01\n",
      "Epoch [345/10000], Loss: 219.14273071289062, Learning Rate: 0.01\n",
      "Epoch [346/10000], Loss: 218.1765899658203, Learning Rate: 0.01\n",
      "Epoch [347/10000], Loss: 217.2169189453125, Learning Rate: 0.01\n",
      "Epoch [348/10000], Loss: 216.26348876953125, Learning Rate: 0.01\n",
      "Epoch [349/10000], Loss: 215.31643676757812, Learning Rate: 0.01\n",
      "Epoch [350/10000], Loss: 214.37562561035156, Learning Rate: 0.01\n",
      "Epoch [351/10000], Loss: 213.44122314453125, Learning Rate: 0.01\n",
      "Epoch [352/10000], Loss: 212.51290893554688, Learning Rate: 0.01\n",
      "Epoch [353/10000], Loss: 211.5908203125, Learning Rate: 0.01\n",
      "Epoch [354/10000], Loss: 210.67494201660156, Learning Rate: 0.01\n",
      "Epoch [355/10000], Loss: 209.76502990722656, Learning Rate: 0.01\n",
      "Epoch [356/10000], Loss: 208.8612823486328, Learning Rate: 0.01\n",
      "Epoch [357/10000], Loss: 207.9634552001953, Learning Rate: 0.01\n",
      "Epoch [358/10000], Loss: 207.07176208496094, Learning Rate: 0.01\n",
      "Epoch [359/10000], Loss: 206.18601989746094, Learning Rate: 0.01\n",
      "Epoch [360/10000], Loss: 205.3064422607422, Learning Rate: 0.01\n",
      "Epoch [361/10000], Loss: 204.43246459960938, Learning Rate: 0.01\n",
      "Epoch [362/10000], Loss: 203.56434631347656, Learning Rate: 0.01\n",
      "Epoch [363/10000], Loss: 202.70223999023438, Learning Rate: 0.01\n",
      "Epoch [364/10000], Loss: 201.84600830078125, Learning Rate: 0.01\n",
      "Epoch [365/10000], Loss: 200.9954071044922, Learning Rate: 0.01\n",
      "Epoch [366/10000], Loss: 200.150634765625, Learning Rate: 0.01\n",
      "Epoch [367/10000], Loss: 199.3114776611328, Learning Rate: 0.01\n",
      "Epoch [368/10000], Loss: 198.47804260253906, Learning Rate: 0.01\n",
      "Epoch [369/10000], Loss: 197.6502227783203, Learning Rate: 0.01\n",
      "Epoch [370/10000], Loss: 196.828125, Learning Rate: 0.01\n",
      "Epoch [371/10000], Loss: 196.01150512695312, Learning Rate: 0.01\n",
      "Epoch [372/10000], Loss: 195.2005615234375, Learning Rate: 0.01\n",
      "Epoch [373/10000], Loss: 194.39505004882812, Learning Rate: 0.01\n",
      "Epoch [374/10000], Loss: 193.59506225585938, Learning Rate: 0.01\n",
      "Epoch [375/10000], Loss: 192.80038452148438, Learning Rate: 0.01\n",
      "Epoch [376/10000], Loss: 192.01124572753906, Learning Rate: 0.01\n",
      "Epoch [377/10000], Loss: 191.22755432128906, Learning Rate: 0.01\n",
      "Epoch [378/10000], Loss: 190.44912719726562, Learning Rate: 0.01\n",
      "Epoch [379/10000], Loss: 189.67613220214844, Learning Rate: 0.01\n",
      "Epoch [380/10000], Loss: 188.90846252441406, Learning Rate: 0.01\n",
      "Epoch [381/10000], Loss: 188.14593505859375, Learning Rate: 0.01\n",
      "Epoch [382/10000], Loss: 187.3887176513672, Learning Rate: 0.01\n",
      "Epoch [383/10000], Loss: 186.63681030273438, Learning Rate: 0.01\n",
      "Epoch [384/10000], Loss: 185.89004516601562, Learning Rate: 0.01\n",
      "Epoch [385/10000], Loss: 185.1486053466797, Learning Rate: 0.01\n",
      "Epoch [386/10000], Loss: 184.4121551513672, Learning Rate: 0.01\n",
      "Epoch [387/10000], Loss: 183.68087768554688, Learning Rate: 0.01\n",
      "Epoch [388/10000], Loss: 182.95484924316406, Learning Rate: 0.01\n",
      "Epoch [389/10000], Loss: 182.23388671875, Learning Rate: 0.01\n",
      "Epoch [390/10000], Loss: 181.5181121826172, Learning Rate: 0.01\n",
      "Epoch [391/10000], Loss: 180.80722045898438, Learning Rate: 0.01\n",
      "Epoch [392/10000], Loss: 180.10145568847656, Learning Rate: 0.01\n",
      "Epoch [393/10000], Loss: 179.40084838867188, Learning Rate: 0.01\n",
      "Epoch [394/10000], Loss: 178.70510864257812, Learning Rate: 0.01\n",
      "Epoch [395/10000], Loss: 178.01455688476562, Learning Rate: 0.01\n",
      "Epoch [396/10000], Loss: 177.32894897460938, Learning Rate: 0.01\n",
      "Epoch [397/10000], Loss: 176.6484375, Learning Rate: 0.01\n",
      "Epoch [398/10000], Loss: 175.97286987304688, Learning Rate: 0.01\n",
      "Epoch [399/10000], Loss: 175.30230712890625, Learning Rate: 0.01\n",
      "Epoch [400/10000], Loss: 174.63671875, Learning Rate: 0.01\n",
      "Epoch [401/10000], Loss: 173.9762725830078, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [402/10000], Loss: 173.3208465576172, Learning Rate: 0.01\n",
      "Epoch [403/10000], Loss: 172.6702880859375, Learning Rate: 0.01\n",
      "Epoch [404/10000], Loss: 172.0247802734375, Learning Rate: 0.01\n",
      "Epoch [405/10000], Loss: 171.3843231201172, Learning Rate: 0.01\n",
      "Epoch [406/10000], Loss: 170.7489013671875, Learning Rate: 0.01\n",
      "Epoch [407/10000], Loss: 170.11856079101562, Learning Rate: 0.01\n",
      "Epoch [408/10000], Loss: 169.49331665039062, Learning Rate: 0.01\n",
      "Epoch [409/10000], Loss: 168.87307739257812, Learning Rate: 0.01\n",
      "Epoch [410/10000], Loss: 168.25802612304688, Learning Rate: 0.01\n",
      "Epoch [411/10000], Loss: 167.64804077148438, Learning Rate: 0.01\n",
      "Epoch [412/10000], Loss: 167.04315185546875, Learning Rate: 0.01\n",
      "Epoch [413/10000], Loss: 166.4434814453125, Learning Rate: 0.01\n",
      "Epoch [414/10000], Loss: 165.84909057617188, Learning Rate: 0.01\n",
      "Epoch [415/10000], Loss: 165.25991821289062, Learning Rate: 0.01\n",
      "Epoch [416/10000], Loss: 164.67587280273438, Learning Rate: 0.01\n",
      "Epoch [417/10000], Loss: 164.0971221923828, Learning Rate: 0.01\n",
      "Epoch [418/10000], Loss: 163.5238494873047, Learning Rate: 0.01\n",
      "Epoch [419/10000], Loss: 162.95578002929688, Learning Rate: 0.01\n",
      "Epoch [420/10000], Loss: 162.393310546875, Learning Rate: 0.01\n",
      "Epoch [421/10000], Loss: 161.8360595703125, Learning Rate: 0.01\n",
      "Epoch [422/10000], Loss: 161.28424072265625, Learning Rate: 0.01\n",
      "Epoch [423/10000], Loss: 160.73826599121094, Learning Rate: 0.01\n",
      "Epoch [424/10000], Loss: 160.1976318359375, Learning Rate: 0.01\n",
      "Epoch [425/10000], Loss: 159.66261291503906, Learning Rate: 0.01\n",
      "Epoch [426/10000], Loss: 159.13336181640625, Learning Rate: 0.01\n",
      "Epoch [427/10000], Loss: 158.6096954345703, Learning Rate: 0.01\n",
      "Epoch [428/10000], Loss: 158.09173583984375, Learning Rate: 0.01\n",
      "Epoch [429/10000], Loss: 157.57958984375, Learning Rate: 0.01\n",
      "Epoch [430/10000], Loss: 157.0734405517578, Learning Rate: 0.01\n",
      "Epoch [431/10000], Loss: 156.5730438232422, Learning Rate: 0.01\n",
      "Epoch [432/10000], Loss: 156.07858276367188, Learning Rate: 0.01\n",
      "Epoch [433/10000], Loss: 155.59014892578125, Learning Rate: 0.01\n",
      "Epoch [434/10000], Loss: 155.1077423095703, Learning Rate: 0.01\n",
      "Epoch [435/10000], Loss: 154.63150024414062, Learning Rate: 0.01\n",
      "Epoch [436/10000], Loss: 154.1612548828125, Learning Rate: 0.01\n",
      "Epoch [437/10000], Loss: 153.6973876953125, Learning Rate: 0.01\n",
      "Epoch [438/10000], Loss: 153.23959350585938, Learning Rate: 0.01\n",
      "Epoch [439/10000], Loss: 152.78807067871094, Learning Rate: 0.01\n",
      "Epoch [440/10000], Loss: 152.34278869628906, Learning Rate: 0.01\n",
      "Epoch [441/10000], Loss: 151.90383911132812, Learning Rate: 0.01\n",
      "Epoch [442/10000], Loss: 151.4713897705078, Learning Rate: 0.01\n",
      "Epoch [443/10000], Loss: 151.04515075683594, Learning Rate: 0.01\n",
      "Epoch [444/10000], Loss: 150.62548828125, Learning Rate: 0.01\n",
      "Epoch [445/10000], Loss: 150.21229553222656, Learning Rate: 0.01\n",
      "Epoch [446/10000], Loss: 149.80555725097656, Learning Rate: 0.01\n",
      "Epoch [447/10000], Loss: 149.4053192138672, Learning Rate: 0.01\n",
      "Epoch [448/10000], Loss: 149.01156616210938, Learning Rate: 0.01\n",
      "Epoch [449/10000], Loss: 148.6244354248047, Learning Rate: 0.01\n",
      "Epoch [450/10000], Loss: 148.24365234375, Learning Rate: 0.01\n",
      "Epoch [451/10000], Loss: 147.86972045898438, Learning Rate: 0.01\n",
      "Epoch [452/10000], Loss: 147.5021209716797, Learning Rate: 0.01\n",
      "Epoch [453/10000], Loss: 147.1409454345703, Learning Rate: 0.01\n",
      "Epoch [454/10000], Loss: 146.78656005859375, Learning Rate: 0.01\n",
      "Epoch [455/10000], Loss: 146.43856811523438, Learning Rate: 0.01\n",
      "Epoch [456/10000], Loss: 146.09707641601562, Learning Rate: 0.01\n",
      "Epoch [457/10000], Loss: 145.7620849609375, Learning Rate: 0.01\n",
      "Epoch [458/10000], Loss: 145.43344116210938, Learning Rate: 0.01\n",
      "Epoch [459/10000], Loss: 145.1112060546875, Learning Rate: 0.01\n",
      "Epoch [460/10000], Loss: 144.79534912109375, Learning Rate: 0.01\n",
      "Epoch [461/10000], Loss: 144.48593139648438, Learning Rate: 0.01\n",
      "Epoch [462/10000], Loss: 144.1826629638672, Learning Rate: 0.01\n",
      "Epoch [463/10000], Loss: 143.88558959960938, Learning Rate: 0.01\n",
      "Epoch [464/10000], Loss: 143.59471130371094, Learning Rate: 0.01\n",
      "Epoch [465/10000], Loss: 143.3098907470703, Learning Rate: 0.01\n",
      "Epoch [466/10000], Loss: 143.03109741210938, Learning Rate: 0.01\n",
      "Epoch [467/10000], Loss: 142.75830078125, Learning Rate: 0.01\n",
      "Epoch [468/10000], Loss: 142.49127197265625, Learning Rate: 0.01\n",
      "Epoch [469/10000], Loss: 142.23016357421875, Learning Rate: 0.01\n",
      "Epoch [470/10000], Loss: 141.9747314453125, Learning Rate: 0.01\n",
      "Epoch [471/10000], Loss: 141.72494506835938, Learning Rate: 0.01\n",
      "Epoch [472/10000], Loss: 141.48057556152344, Learning Rate: 0.01\n",
      "Epoch [473/10000], Loss: 141.24171447753906, Learning Rate: 0.01\n",
      "Epoch [474/10000], Loss: 141.00808715820312, Learning Rate: 0.01\n",
      "Epoch [475/10000], Loss: 140.77981567382812, Learning Rate: 0.01\n",
      "Epoch [476/10000], Loss: 140.5565185546875, Learning Rate: 0.01\n",
      "Epoch [477/10000], Loss: 140.33840942382812, Learning Rate: 0.01\n",
      "Epoch [478/10000], Loss: 140.12509155273438, Learning Rate: 0.01\n",
      "Epoch [479/10000], Loss: 139.91661071777344, Learning Rate: 0.01\n",
      "Epoch [480/10000], Loss: 139.71286010742188, Learning Rate: 0.01\n",
      "Epoch [481/10000], Loss: 139.513671875, Learning Rate: 0.01\n",
      "Epoch [482/10000], Loss: 139.3190155029297, Learning Rate: 0.01\n",
      "Epoch [483/10000], Loss: 139.12863159179688, Learning Rate: 0.01\n",
      "Epoch [484/10000], Loss: 138.94253540039062, Learning Rate: 0.01\n",
      "Epoch [485/10000], Loss: 138.76077270507812, Learning Rate: 0.01\n",
      "Epoch [486/10000], Loss: 138.58273315429688, Learning Rate: 0.01\n",
      "Epoch [487/10000], Loss: 138.4088134765625, Learning Rate: 0.01\n",
      "Epoch [488/10000], Loss: 138.23870849609375, Learning Rate: 0.01\n",
      "Epoch [489/10000], Loss: 138.07223510742188, Learning Rate: 0.01\n",
      "Epoch [490/10000], Loss: 137.90951538085938, Learning Rate: 0.01\n",
      "Epoch [491/10000], Loss: 137.75018310546875, Learning Rate: 0.01\n",
      "Epoch [492/10000], Loss: 137.59423828125, Learning Rate: 0.01\n",
      "Epoch [493/10000], Loss: 137.44163513183594, Learning Rate: 0.01\n",
      "Epoch [494/10000], Loss: 137.29232788085938, Learning Rate: 0.01\n",
      "Epoch [495/10000], Loss: 137.14590454101562, Learning Rate: 0.01\n",
      "Epoch [496/10000], Loss: 137.00265502929688, Learning Rate: 0.01\n",
      "Epoch [497/10000], Loss: 136.86236572265625, Learning Rate: 0.01\n",
      "Epoch [498/10000], Loss: 136.72476196289062, Learning Rate: 0.01\n",
      "Epoch [499/10000], Loss: 136.58985900878906, Learning Rate: 0.01\n",
      "Epoch [500/10000], Loss: 136.45782470703125, Learning Rate: 0.01\n",
      "Epoch [501/10000], Loss: 136.3282470703125, Learning Rate: 0.01\n",
      "Epoch [502/10000], Loss: 136.2010040283203, Learning Rate: 0.01\n",
      "Epoch [503/10000], Loss: 136.076416015625, Learning Rate: 0.01\n",
      "Epoch [504/10000], Loss: 135.95404052734375, Learning Rate: 0.01\n",
      "Epoch [505/10000], Loss: 135.83389282226562, Learning Rate: 0.01\n",
      "Epoch [506/10000], Loss: 135.71592712402344, Learning Rate: 0.01\n",
      "Epoch [507/10000], Loss: 135.60012817382812, Learning Rate: 0.01\n",
      "Epoch [508/10000], Loss: 135.4864501953125, Learning Rate: 0.01\n",
      "Epoch [509/10000], Loss: 135.37472534179688, Learning Rate: 0.01\n",
      "Epoch [510/10000], Loss: 135.2650146484375, Learning Rate: 0.01\n",
      "Epoch [511/10000], Loss: 135.15689086914062, Learning Rate: 0.01\n",
      "Epoch [512/10000], Loss: 135.05087280273438, Learning Rate: 0.01\n",
      "Epoch [513/10000], Loss: 134.94644165039062, Learning Rate: 0.01\n",
      "Epoch [514/10000], Loss: 134.84375, Learning Rate: 0.01\n",
      "Epoch [515/10000], Loss: 134.74276733398438, Learning Rate: 0.01\n",
      "Epoch [516/10000], Loss: 134.64341735839844, Learning Rate: 0.01\n",
      "Epoch [517/10000], Loss: 134.54562377929688, Learning Rate: 0.01\n",
      "Epoch [518/10000], Loss: 134.44921875, Learning Rate: 0.01\n",
      "Epoch [519/10000], Loss: 134.35433959960938, Learning Rate: 0.01\n",
      "Epoch [520/10000], Loss: 134.26104736328125, Learning Rate: 0.01\n",
      "Epoch [521/10000], Loss: 134.16891479492188, Learning Rate: 0.01\n",
      "Epoch [522/10000], Loss: 134.07843017578125, Learning Rate: 0.01\n",
      "Epoch [523/10000], Loss: 133.98904418945312, Learning Rate: 0.01\n",
      "Epoch [524/10000], Loss: 133.9010009765625, Learning Rate: 0.01\n",
      "Epoch [525/10000], Loss: 133.814208984375, Learning Rate: 0.01\n",
      "Epoch [526/10000], Loss: 133.72857666015625, Learning Rate: 0.01\n",
      "Epoch [527/10000], Loss: 133.6441650390625, Learning Rate: 0.01\n",
      "Epoch [528/10000], Loss: 133.56089782714844, Learning Rate: 0.01\n",
      "Epoch [529/10000], Loss: 133.47894287109375, Learning Rate: 0.01\n",
      "Epoch [530/10000], Loss: 133.39796447753906, Learning Rate: 0.01\n",
      "Epoch [531/10000], Loss: 133.31802368164062, Learning Rate: 0.01\n",
      "Epoch [532/10000], Loss: 133.2390594482422, Learning Rate: 0.01\n",
      "Epoch [533/10000], Loss: 133.16119384765625, Learning Rate: 0.01\n",
      "Epoch [534/10000], Loss: 133.0843963623047, Learning Rate: 0.01\n",
      "Epoch [535/10000], Loss: 133.0085906982422, Learning Rate: 0.01\n",
      "Epoch [536/10000], Loss: 132.93365478515625, Learning Rate: 0.01\n",
      "Epoch [537/10000], Loss: 132.859619140625, Learning Rate: 0.01\n",
      "Epoch [538/10000], Loss: 132.7865753173828, Learning Rate: 0.01\n",
      "Epoch [539/10000], Loss: 132.7144012451172, Learning Rate: 0.01\n",
      "Epoch [540/10000], Loss: 132.64297485351562, Learning Rate: 0.01\n",
      "Epoch [541/10000], Loss: 132.57247924804688, Learning Rate: 0.01\n",
      "Epoch [542/10000], Loss: 132.5028533935547, Learning Rate: 0.01\n",
      "Epoch [543/10000], Loss: 132.43399047851562, Learning Rate: 0.01\n",
      "Epoch [544/10000], Loss: 132.36595153808594, Learning Rate: 0.01\n",
      "Epoch [545/10000], Loss: 132.29867553710938, Learning Rate: 0.01\n",
      "Epoch [546/10000], Loss: 132.23223876953125, Learning Rate: 0.01\n",
      "Epoch [547/10000], Loss: 132.16647338867188, Learning Rate: 0.01\n",
      "Epoch [548/10000], Loss: 132.1013641357422, Learning Rate: 0.01\n",
      "Epoch [549/10000], Loss: 132.0371856689453, Learning Rate: 0.01\n",
      "Epoch [550/10000], Loss: 131.9735107421875, Learning Rate: 0.01\n",
      "Epoch [551/10000], Loss: 131.91065979003906, Learning Rate: 0.01\n",
      "Epoch [552/10000], Loss: 131.8482666015625, Learning Rate: 0.01\n",
      "Epoch [553/10000], Loss: 131.7867431640625, Learning Rate: 0.01\n",
      "Epoch [554/10000], Loss: 131.725830078125, Learning Rate: 0.01\n",
      "Epoch [555/10000], Loss: 131.66543579101562, Learning Rate: 0.01\n",
      "Epoch [556/10000], Loss: 131.60568237304688, Learning Rate: 0.01\n",
      "Epoch [557/10000], Loss: 131.546630859375, Learning Rate: 0.01\n",
      "Epoch [558/10000], Loss: 131.48805236816406, Learning Rate: 0.01\n",
      "Epoch [559/10000], Loss: 131.43008422851562, Learning Rate: 0.01\n",
      "Epoch [560/10000], Loss: 131.37271118164062, Learning Rate: 0.01\n",
      "Epoch [561/10000], Loss: 131.3157958984375, Learning Rate: 0.01\n",
      "Epoch [562/10000], Loss: 131.25958251953125, Learning Rate: 0.01\n",
      "Epoch [563/10000], Loss: 131.20388793945312, Learning Rate: 0.01\n",
      "Epoch [564/10000], Loss: 131.1486358642578, Learning Rate: 0.01\n",
      "Epoch [565/10000], Loss: 131.09393310546875, Learning Rate: 0.01\n",
      "Epoch [566/10000], Loss: 131.0396270751953, Learning Rate: 0.01\n",
      "Epoch [567/10000], Loss: 130.9859161376953, Learning Rate: 0.01\n",
      "Epoch [568/10000], Loss: 130.93270874023438, Learning Rate: 0.01\n",
      "Epoch [569/10000], Loss: 130.87985229492188, Learning Rate: 0.01\n",
      "Epoch [570/10000], Loss: 130.82762145996094, Learning Rate: 0.01\n",
      "Epoch [571/10000], Loss: 130.7757110595703, Learning Rate: 0.01\n",
      "Epoch [572/10000], Loss: 130.7243194580078, Learning Rate: 0.01\n",
      "Epoch [573/10000], Loss: 130.67335510253906, Learning Rate: 0.01\n",
      "Epoch [574/10000], Loss: 130.62277221679688, Learning Rate: 0.01\n",
      "Epoch [575/10000], Loss: 130.57252502441406, Learning Rate: 0.01\n",
      "Epoch [576/10000], Loss: 130.52285766601562, Learning Rate: 0.01\n",
      "Epoch [577/10000], Loss: 130.47341918945312, Learning Rate: 0.01\n",
      "Epoch [578/10000], Loss: 130.42445373535156, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [579/10000], Loss: 130.37596130371094, Learning Rate: 0.01\n",
      "Epoch [580/10000], Loss: 130.32769775390625, Learning Rate: 0.01\n",
      "Epoch [581/10000], Loss: 130.27993774414062, Learning Rate: 0.01\n",
      "Epoch [582/10000], Loss: 130.23239135742188, Learning Rate: 0.01\n",
      "Epoch [583/10000], Loss: 130.1852569580078, Learning Rate: 0.01\n",
      "Epoch [584/10000], Loss: 130.13848876953125, Learning Rate: 0.01\n",
      "Epoch [585/10000], Loss: 130.09205627441406, Learning Rate: 0.01\n",
      "Epoch [586/10000], Loss: 130.04592895507812, Learning Rate: 0.01\n",
      "Epoch [587/10000], Loss: 130.00009155273438, Learning Rate: 0.01\n",
      "Epoch [588/10000], Loss: 129.9547119140625, Learning Rate: 0.01\n",
      "Epoch [589/10000], Loss: 129.909423828125, Learning Rate: 0.01\n",
      "Epoch [590/10000], Loss: 129.86465454101562, Learning Rate: 0.01\n",
      "Epoch [591/10000], Loss: 129.82015991210938, Learning Rate: 0.01\n",
      "Epoch [592/10000], Loss: 129.7759246826172, Learning Rate: 0.01\n",
      "Epoch [593/10000], Loss: 129.73190307617188, Learning Rate: 0.01\n",
      "Epoch [594/10000], Loss: 129.68826293945312, Learning Rate: 0.01\n",
      "Epoch [595/10000], Loss: 129.64480590820312, Learning Rate: 0.01\n",
      "Epoch [596/10000], Loss: 129.6016387939453, Learning Rate: 0.01\n",
      "Epoch [597/10000], Loss: 129.558837890625, Learning Rate: 0.01\n",
      "Epoch [598/10000], Loss: 129.51614379882812, Learning Rate: 0.01\n",
      "Epoch [599/10000], Loss: 129.4737548828125, Learning Rate: 0.01\n",
      "Epoch [600/10000], Loss: 129.43162536621094, Learning Rate: 0.01\n",
      "Epoch [601/10000], Loss: 129.38980102539062, Learning Rate: 0.01\n",
      "Epoch [602/10000], Loss: 129.34815979003906, Learning Rate: 0.01\n",
      "Epoch [603/10000], Loss: 129.30673217773438, Learning Rate: 0.01\n",
      "Epoch [604/10000], Loss: 129.26556396484375, Learning Rate: 0.01\n",
      "Epoch [605/10000], Loss: 129.22467041015625, Learning Rate: 0.01\n",
      "Epoch [606/10000], Loss: 129.18382263183594, Learning Rate: 0.01\n",
      "Epoch [607/10000], Loss: 129.1433563232422, Learning Rate: 0.01\n",
      "Epoch [608/10000], Loss: 129.10311889648438, Learning Rate: 0.01\n",
      "Epoch [609/10000], Loss: 129.06292724609375, Learning Rate: 0.01\n",
      "Epoch [610/10000], Loss: 129.02313232421875, Learning Rate: 0.01\n",
      "Epoch [611/10000], Loss: 128.9834442138672, Learning Rate: 0.01\n",
      "Epoch [612/10000], Loss: 128.9439697265625, Learning Rate: 0.01\n",
      "Epoch [613/10000], Loss: 128.90457153320312, Learning Rate: 0.01\n",
      "Epoch [614/10000], Loss: 128.8653106689453, Learning Rate: 0.01\n",
      "Epoch [615/10000], Loss: 128.8264923095703, Learning Rate: 0.01\n",
      "Epoch [616/10000], Loss: 128.787841796875, Learning Rate: 0.01\n",
      "Epoch [617/10000], Loss: 128.7491455078125, Learning Rate: 0.01\n",
      "Epoch [618/10000], Loss: 128.71070861816406, Learning Rate: 0.01\n",
      "Epoch [619/10000], Loss: 128.67227172851562, Learning Rate: 0.01\n",
      "Epoch [620/10000], Loss: 128.6343994140625, Learning Rate: 0.01\n",
      "Epoch [621/10000], Loss: 128.59634399414062, Learning Rate: 0.01\n",
      "Epoch [622/10000], Loss: 128.5585479736328, Learning Rate: 0.01\n",
      "Epoch [623/10000], Loss: 128.52102661132812, Learning Rate: 0.01\n",
      "Epoch [624/10000], Loss: 128.48355102539062, Learning Rate: 0.01\n",
      "Epoch [625/10000], Loss: 128.4461669921875, Learning Rate: 0.01\n",
      "Epoch [626/10000], Loss: 128.40890502929688, Learning Rate: 0.01\n",
      "Epoch [627/10000], Loss: 128.37188720703125, Learning Rate: 0.01\n",
      "Epoch [628/10000], Loss: 128.3350372314453, Learning Rate: 0.01\n",
      "Epoch [629/10000], Loss: 128.29818725585938, Learning Rate: 0.01\n",
      "Epoch [630/10000], Loss: 128.26165771484375, Learning Rate: 0.01\n",
      "Epoch [631/10000], Loss: 128.22500610351562, Learning Rate: 0.01\n",
      "Epoch [632/10000], Loss: 128.1886749267578, Learning Rate: 0.01\n",
      "Epoch [633/10000], Loss: 128.15232849121094, Learning Rate: 0.01\n",
      "Epoch [634/10000], Loss: 128.11624145507812, Learning Rate: 0.01\n",
      "Epoch [635/10000], Loss: 128.0802001953125, Learning Rate: 0.01\n",
      "Epoch [636/10000], Loss: 128.04421997070312, Learning Rate: 0.01\n",
      "Epoch [637/10000], Loss: 128.00843811035156, Learning Rate: 0.01\n",
      "Epoch [638/10000], Loss: 127.97261810302734, Learning Rate: 0.01\n",
      "Epoch [639/10000], Loss: 127.93714904785156, Learning Rate: 0.01\n",
      "Epoch [640/10000], Loss: 127.90162658691406, Learning Rate: 0.01\n",
      "Epoch [641/10000], Loss: 127.86620330810547, Learning Rate: 0.01\n",
      "Epoch [642/10000], Loss: 127.83089447021484, Learning Rate: 0.01\n",
      "Epoch [643/10000], Loss: 127.7957763671875, Learning Rate: 0.01\n",
      "Epoch [644/10000], Loss: 127.76071166992188, Learning Rate: 0.01\n",
      "Epoch [645/10000], Loss: 127.7256088256836, Learning Rate: 0.01\n",
      "Epoch [646/10000], Loss: 127.69072723388672, Learning Rate: 0.01\n",
      "Epoch [647/10000], Loss: 127.65596008300781, Learning Rate: 0.01\n",
      "Epoch [648/10000], Loss: 127.62113952636719, Learning Rate: 0.01\n",
      "Epoch [649/10000], Loss: 127.58650207519531, Learning Rate: 0.01\n",
      "Epoch [650/10000], Loss: 127.5518798828125, Learning Rate: 0.01\n",
      "Epoch [651/10000], Loss: 127.51740264892578, Learning Rate: 0.01\n",
      "Epoch [652/10000], Loss: 127.48300170898438, Learning Rate: 0.01\n",
      "Epoch [653/10000], Loss: 127.44862365722656, Learning Rate: 0.01\n",
      "Epoch [654/10000], Loss: 127.41432189941406, Learning Rate: 0.01\n",
      "Epoch [655/10000], Loss: 127.38004302978516, Learning Rate: 0.01\n",
      "Epoch [656/10000], Loss: 127.34600067138672, Learning Rate: 0.01\n",
      "Epoch [657/10000], Loss: 127.31195831298828, Learning Rate: 0.01\n",
      "Epoch [658/10000], Loss: 127.2778549194336, Learning Rate: 0.01\n",
      "Epoch [659/10000], Loss: 127.24403381347656, Learning Rate: 0.01\n",
      "Epoch [660/10000], Loss: 127.2101058959961, Learning Rate: 0.01\n",
      "Epoch [661/10000], Loss: 127.17628479003906, Learning Rate: 0.01\n",
      "Epoch [662/10000], Loss: 127.1425552368164, Learning Rate: 0.01\n",
      "Epoch [663/10000], Loss: 127.10881805419922, Learning Rate: 0.01\n",
      "Epoch [664/10000], Loss: 127.07516479492188, Learning Rate: 0.01\n",
      "Epoch [665/10000], Loss: 127.04165649414062, Learning Rate: 0.01\n",
      "Epoch [666/10000], Loss: 127.00807189941406, Learning Rate: 0.01\n",
      "Epoch [667/10000], Loss: 126.97459411621094, Learning Rate: 0.01\n",
      "Epoch [668/10000], Loss: 126.94119262695312, Learning Rate: 0.01\n",
      "Epoch [669/10000], Loss: 126.90773010253906, Learning Rate: 0.01\n",
      "Epoch [670/10000], Loss: 126.87445068359375, Learning Rate: 0.01\n",
      "Epoch [671/10000], Loss: 126.841064453125, Learning Rate: 0.01\n",
      "Epoch [672/10000], Loss: 126.8078842163086, Learning Rate: 0.01\n",
      "Epoch [673/10000], Loss: 126.77462005615234, Learning Rate: 0.01\n",
      "Epoch [674/10000], Loss: 126.74151611328125, Learning Rate: 0.01\n",
      "Epoch [675/10000], Loss: 126.70838165283203, Learning Rate: 0.01\n",
      "Epoch [676/10000], Loss: 126.67525482177734, Learning Rate: 0.01\n",
      "Epoch [677/10000], Loss: 126.64220428466797, Learning Rate: 0.01\n",
      "Epoch [678/10000], Loss: 126.60914611816406, Learning Rate: 0.01\n",
      "Epoch [679/10000], Loss: 126.57620239257812, Learning Rate: 0.01\n",
      "Epoch [680/10000], Loss: 126.5432357788086, Learning Rate: 0.01\n",
      "Epoch [681/10000], Loss: 126.51031494140625, Learning Rate: 0.01\n",
      "Epoch [682/10000], Loss: 126.47743225097656, Learning Rate: 0.01\n",
      "Epoch [683/10000], Loss: 126.44454193115234, Learning Rate: 0.01\n",
      "Epoch [684/10000], Loss: 126.4117202758789, Learning Rate: 0.01\n",
      "Epoch [685/10000], Loss: 126.37895202636719, Learning Rate: 0.01\n",
      "Epoch [686/10000], Loss: 126.3462142944336, Learning Rate: 0.01\n",
      "Epoch [687/10000], Loss: 126.31348419189453, Learning Rate: 0.01\n",
      "Epoch [688/10000], Loss: 126.28068542480469, Learning Rate: 0.01\n",
      "Epoch [689/10000], Loss: 126.24793243408203, Learning Rate: 0.01\n",
      "Epoch [690/10000], Loss: 126.2152099609375, Learning Rate: 0.01\n",
      "Epoch [691/10000], Loss: 126.18258666992188, Learning Rate: 0.01\n",
      "Epoch [692/10000], Loss: 126.14996337890625, Learning Rate: 0.01\n",
      "Epoch [693/10000], Loss: 126.11736297607422, Learning Rate: 0.01\n",
      "Epoch [694/10000], Loss: 126.08467102050781, Learning Rate: 0.01\n",
      "Epoch [695/10000], Loss: 126.05213928222656, Learning Rate: 0.01\n",
      "Epoch [696/10000], Loss: 126.0195541381836, Learning Rate: 0.01\n",
      "Epoch [697/10000], Loss: 125.98700714111328, Learning Rate: 0.01\n",
      "Epoch [698/10000], Loss: 125.95442199707031, Learning Rate: 0.01\n",
      "Epoch [699/10000], Loss: 125.92184448242188, Learning Rate: 0.01\n",
      "Epoch [700/10000], Loss: 125.88932037353516, Learning Rate: 0.01\n",
      "Epoch [701/10000], Loss: 125.85684967041016, Learning Rate: 0.01\n",
      "Epoch [702/10000], Loss: 125.8243408203125, Learning Rate: 0.01\n",
      "Epoch [703/10000], Loss: 125.7918930053711, Learning Rate: 0.01\n",
      "Epoch [704/10000], Loss: 125.75936889648438, Learning Rate: 0.01\n",
      "Epoch [705/10000], Loss: 125.72691345214844, Learning Rate: 0.01\n",
      "Epoch [706/10000], Loss: 125.69438171386719, Learning Rate: 0.01\n",
      "Epoch [707/10000], Loss: 125.66188049316406, Learning Rate: 0.01\n",
      "Epoch [708/10000], Loss: 125.62947845458984, Learning Rate: 0.01\n",
      "Epoch [709/10000], Loss: 125.59703826904297, Learning Rate: 0.01\n",
      "Epoch [710/10000], Loss: 125.56468963623047, Learning Rate: 0.01\n",
      "Epoch [711/10000], Loss: 125.53226470947266, Learning Rate: 0.01\n",
      "Epoch [712/10000], Loss: 125.49982452392578, Learning Rate: 0.01\n",
      "Epoch [713/10000], Loss: 125.46737670898438, Learning Rate: 0.01\n",
      "Epoch [714/10000], Loss: 125.43494415283203, Learning Rate: 0.01\n",
      "Epoch [715/10000], Loss: 125.40248107910156, Learning Rate: 0.01\n",
      "Epoch [716/10000], Loss: 125.37010955810547, Learning Rate: 0.01\n",
      "Epoch [717/10000], Loss: 125.33768463134766, Learning Rate: 0.01\n",
      "Epoch [718/10000], Loss: 125.30528259277344, Learning Rate: 0.01\n",
      "Epoch [719/10000], Loss: 125.27288818359375, Learning Rate: 0.01\n",
      "Epoch [720/10000], Loss: 125.24044799804688, Learning Rate: 0.01\n",
      "Epoch [721/10000], Loss: 125.20807647705078, Learning Rate: 0.01\n",
      "Epoch [722/10000], Loss: 125.17558288574219, Learning Rate: 0.01\n",
      "Epoch [723/10000], Loss: 125.14311218261719, Learning Rate: 0.01\n",
      "Epoch [724/10000], Loss: 125.11076354980469, Learning Rate: 0.01\n",
      "Epoch [725/10000], Loss: 125.07838439941406, Learning Rate: 0.01\n",
      "Epoch [726/10000], Loss: 125.0458984375, Learning Rate: 0.01\n",
      "Epoch [727/10000], Loss: 125.01351928710938, Learning Rate: 0.01\n",
      "Epoch [728/10000], Loss: 124.98111724853516, Learning Rate: 0.01\n",
      "Epoch [729/10000], Loss: 124.94864654541016, Learning Rate: 0.01\n",
      "Epoch [730/10000], Loss: 124.91620635986328, Learning Rate: 0.01\n",
      "Epoch [731/10000], Loss: 124.88388061523438, Learning Rate: 0.01\n",
      "Epoch [732/10000], Loss: 124.85136413574219, Learning Rate: 0.01\n",
      "Epoch [733/10000], Loss: 124.81896209716797, Learning Rate: 0.01\n",
      "Epoch [734/10000], Loss: 124.78650665283203, Learning Rate: 0.01\n",
      "Epoch [735/10000], Loss: 124.75406646728516, Learning Rate: 0.01\n",
      "Epoch [736/10000], Loss: 124.72159576416016, Learning Rate: 0.01\n",
      "Epoch [737/10000], Loss: 124.68909454345703, Learning Rate: 0.01\n",
      "Epoch [738/10000], Loss: 124.65674591064453, Learning Rate: 0.01\n",
      "Epoch [739/10000], Loss: 124.62427520751953, Learning Rate: 0.01\n",
      "Epoch [740/10000], Loss: 124.59184265136719, Learning Rate: 0.01\n",
      "Epoch [741/10000], Loss: 124.55931091308594, Learning Rate: 0.01\n",
      "Epoch [742/10000], Loss: 124.52687072753906, Learning Rate: 0.01\n",
      "Epoch [743/10000], Loss: 124.49444580078125, Learning Rate: 0.01\n",
      "Epoch [744/10000], Loss: 124.4619140625, Learning Rate: 0.01\n",
      "Epoch [745/10000], Loss: 124.42948913574219, Learning Rate: 0.01\n",
      "Epoch [746/10000], Loss: 124.3969955444336, Learning Rate: 0.01\n",
      "Epoch [747/10000], Loss: 124.36453247070312, Learning Rate: 0.01\n",
      "Epoch [748/10000], Loss: 124.33208465576172, Learning Rate: 0.01\n",
      "Epoch [749/10000], Loss: 124.29962921142578, Learning Rate: 0.01\n",
      "Epoch [750/10000], Loss: 124.26713562011719, Learning Rate: 0.01\n",
      "Epoch [751/10000], Loss: 124.23456573486328, Learning Rate: 0.01\n",
      "Epoch [752/10000], Loss: 124.2021713256836, Learning Rate: 0.01\n",
      "Epoch [753/10000], Loss: 124.16972351074219, Learning Rate: 0.01\n",
      "Epoch [754/10000], Loss: 124.13714599609375, Learning Rate: 0.01\n",
      "Epoch [755/10000], Loss: 124.10474395751953, Learning Rate: 0.01\n",
      "Epoch [756/10000], Loss: 124.07219696044922, Learning Rate: 0.01\n",
      "Epoch [757/10000], Loss: 124.039794921875, Learning Rate: 0.01\n",
      "Epoch [758/10000], Loss: 124.00731658935547, Learning Rate: 0.01\n",
      "Epoch [759/10000], Loss: 123.97482299804688, Learning Rate: 0.01\n",
      "Epoch [760/10000], Loss: 123.94233703613281, Learning Rate: 0.01\n",
      "Epoch [761/10000], Loss: 123.90990447998047, Learning Rate: 0.01\n",
      "Epoch [762/10000], Loss: 123.87745666503906, Learning Rate: 0.01\n",
      "Epoch [763/10000], Loss: 123.84489440917969, Learning Rate: 0.01\n",
      "Epoch [764/10000], Loss: 123.81249237060547, Learning Rate: 0.01\n",
      "Epoch [765/10000], Loss: 123.78021240234375, Learning Rate: 0.01\n",
      "Epoch [766/10000], Loss: 123.74763488769531, Learning Rate: 0.01\n",
      "Epoch [767/10000], Loss: 123.71529388427734, Learning Rate: 0.01\n",
      "Epoch [768/10000], Loss: 123.68283081054688, Learning Rate: 0.01\n",
      "Epoch [769/10000], Loss: 123.65042114257812, Learning Rate: 0.01\n",
      "Epoch [770/10000], Loss: 123.61792755126953, Learning Rate: 0.01\n",
      "Epoch [771/10000], Loss: 123.5855941772461, Learning Rate: 0.01\n",
      "Epoch [772/10000], Loss: 123.55323028564453, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [773/10000], Loss: 123.52094268798828, Learning Rate: 0.01\n",
      "Epoch [774/10000], Loss: 123.4885025024414, Learning Rate: 0.01\n",
      "Epoch [775/10000], Loss: 123.4561996459961, Learning Rate: 0.01\n",
      "Epoch [776/10000], Loss: 123.42378997802734, Learning Rate: 0.01\n",
      "Epoch [777/10000], Loss: 123.39154052734375, Learning Rate: 0.01\n",
      "Epoch [778/10000], Loss: 123.3592529296875, Learning Rate: 0.01\n",
      "Epoch [779/10000], Loss: 123.3270492553711, Learning Rate: 0.01\n",
      "Epoch [780/10000], Loss: 123.29472351074219, Learning Rate: 0.01\n",
      "Epoch [781/10000], Loss: 123.2625732421875, Learning Rate: 0.01\n",
      "Epoch [782/10000], Loss: 123.23021697998047, Learning Rate: 0.01\n",
      "Epoch [783/10000], Loss: 123.19811248779297, Learning Rate: 0.01\n",
      "Epoch [784/10000], Loss: 123.1658706665039, Learning Rate: 0.01\n",
      "Epoch [785/10000], Loss: 123.13378143310547, Learning Rate: 0.01\n",
      "Epoch [786/10000], Loss: 123.10161590576172, Learning Rate: 0.01\n",
      "Epoch [787/10000], Loss: 123.06951904296875, Learning Rate: 0.01\n",
      "Epoch [788/10000], Loss: 123.03749084472656, Learning Rate: 0.01\n",
      "Epoch [789/10000], Loss: 123.00541687011719, Learning Rate: 0.01\n",
      "Epoch [790/10000], Loss: 122.97341918945312, Learning Rate: 0.01\n",
      "Epoch [791/10000], Loss: 122.94142150878906, Learning Rate: 0.01\n",
      "Epoch [792/10000], Loss: 122.90940856933594, Learning Rate: 0.01\n",
      "Epoch [793/10000], Loss: 122.87745666503906, Learning Rate: 0.01\n",
      "Epoch [794/10000], Loss: 122.84555053710938, Learning Rate: 0.01\n",
      "Epoch [795/10000], Loss: 122.81356811523438, Learning Rate: 0.01\n",
      "Epoch [796/10000], Loss: 122.78186798095703, Learning Rate: 0.01\n",
      "Epoch [797/10000], Loss: 122.75006103515625, Learning Rate: 0.01\n",
      "Epoch [798/10000], Loss: 122.71824645996094, Learning Rate: 0.01\n",
      "Epoch [799/10000], Loss: 122.68660736083984, Learning Rate: 0.01\n",
      "Epoch [800/10000], Loss: 122.65483093261719, Learning Rate: 0.01\n",
      "Epoch [801/10000], Loss: 122.62310028076172, Learning Rate: 0.01\n",
      "Epoch [802/10000], Loss: 122.59153747558594, Learning Rate: 0.01\n",
      "Epoch [803/10000], Loss: 122.55998229980469, Learning Rate: 0.01\n",
      "Epoch [804/10000], Loss: 122.52849578857422, Learning Rate: 0.01\n",
      "Epoch [805/10000], Loss: 122.49702453613281, Learning Rate: 0.01\n",
      "Epoch [806/10000], Loss: 122.46549224853516, Learning Rate: 0.01\n",
      "Epoch [807/10000], Loss: 122.4340591430664, Learning Rate: 0.01\n",
      "Epoch [808/10000], Loss: 122.4027328491211, Learning Rate: 0.01\n",
      "Epoch [809/10000], Loss: 122.37137603759766, Learning Rate: 0.01\n",
      "Epoch [810/10000], Loss: 122.34010314941406, Learning Rate: 0.01\n",
      "Epoch [811/10000], Loss: 122.30886840820312, Learning Rate: 0.01\n",
      "Epoch [812/10000], Loss: 122.2776870727539, Learning Rate: 0.01\n",
      "Epoch [813/10000], Loss: 122.24659729003906, Learning Rate: 0.01\n",
      "Epoch [814/10000], Loss: 122.21558380126953, Learning Rate: 0.01\n",
      "Epoch [815/10000], Loss: 122.18445587158203, Learning Rate: 0.01\n",
      "Epoch [816/10000], Loss: 122.15348815917969, Learning Rate: 0.01\n",
      "Epoch [817/10000], Loss: 122.12252807617188, Learning Rate: 0.01\n",
      "Epoch [818/10000], Loss: 122.09173583984375, Learning Rate: 0.01\n",
      "Epoch [819/10000], Loss: 122.06092834472656, Learning Rate: 0.01\n",
      "Epoch [820/10000], Loss: 122.03009796142578, Learning Rate: 0.01\n",
      "Epoch [821/10000], Loss: 121.99938201904297, Learning Rate: 0.01\n",
      "Epoch [822/10000], Loss: 121.96869659423828, Learning Rate: 0.01\n",
      "Epoch [823/10000], Loss: 121.93818664550781, Learning Rate: 0.01\n",
      "Epoch [824/10000], Loss: 121.90766906738281, Learning Rate: 0.01\n",
      "Epoch [825/10000], Loss: 121.87720489501953, Learning Rate: 0.01\n",
      "Epoch [826/10000], Loss: 121.84677124023438, Learning Rate: 0.01\n",
      "Epoch [827/10000], Loss: 121.81639099121094, Learning Rate: 0.01\n",
      "Epoch [828/10000], Loss: 121.78617095947266, Learning Rate: 0.01\n",
      "Epoch [829/10000], Loss: 121.75587463378906, Learning Rate: 0.01\n",
      "Epoch [830/10000], Loss: 121.72571563720703, Learning Rate: 0.01\n",
      "Epoch [831/10000], Loss: 121.69561004638672, Learning Rate: 0.01\n",
      "Epoch [832/10000], Loss: 121.66561889648438, Learning Rate: 0.01\n",
      "Epoch [833/10000], Loss: 121.63556671142578, Learning Rate: 0.01\n",
      "Epoch [834/10000], Loss: 121.60570526123047, Learning Rate: 0.01\n",
      "Epoch [835/10000], Loss: 121.57588195800781, Learning Rate: 0.01\n",
      "Epoch [836/10000], Loss: 121.5461654663086, Learning Rate: 0.01\n",
      "Epoch [837/10000], Loss: 121.51637268066406, Learning Rate: 0.01\n",
      "Epoch [838/10000], Loss: 121.4866714477539, Learning Rate: 0.01\n",
      "Epoch [839/10000], Loss: 121.45708465576172, Learning Rate: 0.01\n",
      "Epoch [840/10000], Loss: 121.42760467529297, Learning Rate: 0.01\n",
      "Epoch [841/10000], Loss: 121.39813995361328, Learning Rate: 0.01\n",
      "Epoch [842/10000], Loss: 121.36881256103516, Learning Rate: 0.01\n",
      "Epoch [843/10000], Loss: 121.33953857421875, Learning Rate: 0.01\n",
      "Epoch [844/10000], Loss: 121.31026458740234, Learning Rate: 0.01\n",
      "Epoch [845/10000], Loss: 121.28111267089844, Learning Rate: 0.01\n",
      "Epoch [846/10000], Loss: 121.25200653076172, Learning Rate: 0.01\n",
      "Epoch [847/10000], Loss: 121.2229232788086, Learning Rate: 0.01\n",
      "Epoch [848/10000], Loss: 121.1940689086914, Learning Rate: 0.01\n",
      "Epoch [849/10000], Loss: 121.16509246826172, Learning Rate: 0.01\n",
      "Epoch [850/10000], Loss: 121.13629150390625, Learning Rate: 0.01\n",
      "Epoch [851/10000], Loss: 121.10759735107422, Learning Rate: 0.01\n",
      "Epoch [852/10000], Loss: 121.07891082763672, Learning Rate: 0.01\n",
      "Epoch [853/10000], Loss: 121.0502700805664, Learning Rate: 0.01\n",
      "Epoch [854/10000], Loss: 121.02178192138672, Learning Rate: 0.01\n",
      "Epoch [855/10000], Loss: 120.99333953857422, Learning Rate: 0.01\n",
      "Epoch [856/10000], Loss: 120.96490478515625, Learning Rate: 0.01\n",
      "Epoch [857/10000], Loss: 120.93659210205078, Learning Rate: 0.01\n",
      "Epoch [858/10000], Loss: 120.90838623046875, Learning Rate: 0.01\n",
      "Epoch [859/10000], Loss: 120.88019561767578, Learning Rate: 0.01\n",
      "Epoch [860/10000], Loss: 120.85215759277344, Learning Rate: 0.01\n",
      "Epoch [861/10000], Loss: 120.82420349121094, Learning Rate: 0.01\n",
      "Epoch [862/10000], Loss: 120.79627227783203, Learning Rate: 0.01\n",
      "Epoch [863/10000], Loss: 120.7683334350586, Learning Rate: 0.01\n",
      "Epoch [864/10000], Loss: 120.74052429199219, Learning Rate: 0.01\n",
      "Epoch [865/10000], Loss: 120.7128677368164, Learning Rate: 0.01\n",
      "Epoch [866/10000], Loss: 120.68527221679688, Learning Rate: 0.01\n",
      "Epoch [867/10000], Loss: 120.65760803222656, Learning Rate: 0.01\n",
      "Epoch [868/10000], Loss: 120.63020324707031, Learning Rate: 0.01\n",
      "Epoch [869/10000], Loss: 120.6028060913086, Learning Rate: 0.01\n",
      "Epoch [870/10000], Loss: 120.57548522949219, Learning Rate: 0.01\n",
      "Epoch [871/10000], Loss: 120.5481948852539, Learning Rate: 0.01\n",
      "Epoch [872/10000], Loss: 120.52100372314453, Learning Rate: 0.01\n",
      "Epoch [873/10000], Loss: 120.49390411376953, Learning Rate: 0.01\n",
      "Epoch [874/10000], Loss: 120.4668960571289, Learning Rate: 0.01\n",
      "Epoch [875/10000], Loss: 120.43978881835938, Learning Rate: 0.01\n",
      "Epoch [876/10000], Loss: 120.41295623779297, Learning Rate: 0.01\n",
      "Epoch [877/10000], Loss: 120.38615417480469, Learning Rate: 0.01\n",
      "Epoch [878/10000], Loss: 120.35936737060547, Learning Rate: 0.01\n",
      "Epoch [879/10000], Loss: 120.3326644897461, Learning Rate: 0.01\n",
      "Epoch [880/10000], Loss: 120.30615997314453, Learning Rate: 0.01\n",
      "Epoch [881/10000], Loss: 120.27959442138672, Learning Rate: 0.01\n",
      "Epoch [882/10000], Loss: 120.25323486328125, Learning Rate: 0.01\n",
      "Epoch [883/10000], Loss: 120.22672271728516, Learning Rate: 0.01\n",
      "Epoch [884/10000], Loss: 120.20049285888672, Learning Rate: 0.01\n",
      "Epoch [885/10000], Loss: 120.17430877685547, Learning Rate: 0.01\n",
      "Epoch [886/10000], Loss: 120.1481704711914, Learning Rate: 0.01\n",
      "Epoch [887/10000], Loss: 120.12199401855469, Learning Rate: 0.01\n",
      "Epoch [888/10000], Loss: 120.0960464477539, Learning Rate: 0.01\n",
      "Epoch [889/10000], Loss: 120.070068359375, Learning Rate: 0.01\n",
      "Epoch [890/10000], Loss: 120.044189453125, Learning Rate: 0.01\n",
      "Epoch [891/10000], Loss: 120.01841735839844, Learning Rate: 0.01\n",
      "Epoch [892/10000], Loss: 119.99273681640625, Learning Rate: 0.01\n",
      "Epoch [893/10000], Loss: 119.96710968017578, Learning Rate: 0.01\n",
      "Epoch [894/10000], Loss: 119.94140625, Learning Rate: 0.01\n",
      "Epoch [895/10000], Loss: 119.91593170166016, Learning Rate: 0.01\n",
      "Epoch [896/10000], Loss: 119.89054107666016, Learning Rate: 0.01\n",
      "Epoch [897/10000], Loss: 119.86518096923828, Learning Rate: 0.01\n",
      "Epoch [898/10000], Loss: 119.83984375, Learning Rate: 0.01\n",
      "Epoch [899/10000], Loss: 119.81461334228516, Learning Rate: 0.01\n",
      "Epoch [900/10000], Loss: 119.78944396972656, Learning Rate: 0.01\n",
      "Epoch [901/10000], Loss: 119.76436614990234, Learning Rate: 0.01\n",
      "Epoch [902/10000], Loss: 119.73947143554688, Learning Rate: 0.01\n",
      "Epoch [903/10000], Loss: 119.71446228027344, Learning Rate: 0.01\n",
      "Epoch [904/10000], Loss: 119.6895751953125, Learning Rate: 0.01\n",
      "Epoch [905/10000], Loss: 119.6647720336914, Learning Rate: 0.01\n",
      "Epoch [906/10000], Loss: 119.63995361328125, Learning Rate: 0.01\n",
      "Epoch [907/10000], Loss: 119.61531066894531, Learning Rate: 0.01\n",
      "Epoch [908/10000], Loss: 119.59075927734375, Learning Rate: 0.01\n",
      "Epoch [909/10000], Loss: 119.56620788574219, Learning Rate: 0.01\n",
      "Epoch [910/10000], Loss: 119.54168701171875, Learning Rate: 0.01\n",
      "Epoch [911/10000], Loss: 119.51729583740234, Learning Rate: 0.01\n",
      "Epoch [912/10000], Loss: 119.49288177490234, Learning Rate: 0.01\n",
      "Epoch [913/10000], Loss: 119.46863555908203, Learning Rate: 0.01\n",
      "Epoch [914/10000], Loss: 119.4444351196289, Learning Rate: 0.01\n",
      "Epoch [915/10000], Loss: 119.42032623291016, Learning Rate: 0.01\n",
      "Epoch [916/10000], Loss: 119.3962173461914, Learning Rate: 0.01\n",
      "Epoch [917/10000], Loss: 119.37218475341797, Learning Rate: 0.01\n",
      "Epoch [918/10000], Loss: 119.3482894897461, Learning Rate: 0.01\n",
      "Epoch [919/10000], Loss: 119.32431030273438, Learning Rate: 0.01\n",
      "Epoch [920/10000], Loss: 119.30052185058594, Learning Rate: 0.01\n",
      "Epoch [921/10000], Loss: 119.27680206298828, Learning Rate: 0.01\n",
      "Epoch [922/10000], Loss: 119.25312042236328, Learning Rate: 0.01\n",
      "Epoch [923/10000], Loss: 119.22944641113281, Learning Rate: 0.01\n",
      "Epoch [924/10000], Loss: 119.20585632324219, Learning Rate: 0.01\n",
      "Epoch [925/10000], Loss: 119.18242645263672, Learning Rate: 0.01\n",
      "Epoch [926/10000], Loss: 119.1589584350586, Learning Rate: 0.01\n",
      "Epoch [927/10000], Loss: 119.13560485839844, Learning Rate: 0.01\n",
      "Epoch [928/10000], Loss: 119.11227416992188, Learning Rate: 0.01\n",
      "Epoch [929/10000], Loss: 119.08905029296875, Learning Rate: 0.01\n",
      "Epoch [930/10000], Loss: 119.06574249267578, Learning Rate: 0.01\n",
      "Epoch [931/10000], Loss: 119.04267883300781, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [932/10000], Loss: 119.01959228515625, Learning Rate: 0.01\n",
      "Epoch [933/10000], Loss: 118.99655151367188, Learning Rate: 0.01\n",
      "Epoch [934/10000], Loss: 118.97367858886719, Learning Rate: 0.01\n",
      "Epoch [935/10000], Loss: 118.95060729980469, Learning Rate: 0.01\n",
      "Epoch [936/10000], Loss: 118.92789459228516, Learning Rate: 0.01\n",
      "Epoch [937/10000], Loss: 118.90513610839844, Learning Rate: 0.01\n",
      "Epoch [938/10000], Loss: 118.88235473632812, Learning Rate: 0.01\n",
      "Epoch [939/10000], Loss: 118.85969543457031, Learning Rate: 0.01\n",
      "Epoch [940/10000], Loss: 118.83702850341797, Learning Rate: 0.01\n",
      "Epoch [941/10000], Loss: 118.81449890136719, Learning Rate: 0.01\n",
      "Epoch [942/10000], Loss: 118.79193878173828, Learning Rate: 0.01\n",
      "Epoch [943/10000], Loss: 118.7695541381836, Learning Rate: 0.01\n",
      "Epoch [944/10000], Loss: 118.74716186523438, Learning Rate: 0.01\n",
      "Epoch [945/10000], Loss: 118.72474670410156, Learning Rate: 0.01\n",
      "Epoch [946/10000], Loss: 118.70250701904297, Learning Rate: 0.01\n",
      "Epoch [947/10000], Loss: 118.6803207397461, Learning Rate: 0.01\n",
      "Epoch [948/10000], Loss: 118.65807342529297, Learning Rate: 0.01\n",
      "Epoch [949/10000], Loss: 118.63594818115234, Learning Rate: 0.01\n",
      "Epoch [950/10000], Loss: 118.6139144897461, Learning Rate: 0.01\n",
      "Epoch [951/10000], Loss: 118.59186553955078, Learning Rate: 0.01\n",
      "Epoch [952/10000], Loss: 118.5699462890625, Learning Rate: 0.01\n",
      "Epoch [953/10000], Loss: 118.54801177978516, Learning Rate: 0.01\n",
      "Epoch [954/10000], Loss: 118.52608489990234, Learning Rate: 0.01\n",
      "Epoch [955/10000], Loss: 118.50439453125, Learning Rate: 0.01\n",
      "Epoch [956/10000], Loss: 118.48271942138672, Learning Rate: 0.01\n",
      "Epoch [957/10000], Loss: 118.46089172363281, Learning Rate: 0.01\n",
      "Epoch [958/10000], Loss: 118.43927764892578, Learning Rate: 0.01\n",
      "Epoch [959/10000], Loss: 118.41766357421875, Learning Rate: 0.01\n",
      "Epoch [960/10000], Loss: 118.39615631103516, Learning Rate: 0.01\n",
      "Epoch [961/10000], Loss: 118.37471008300781, Learning Rate: 0.01\n",
      "Epoch [962/10000], Loss: 118.35323333740234, Learning Rate: 0.01\n",
      "Epoch [963/10000], Loss: 118.3319320678711, Learning Rate: 0.01\n",
      "Epoch [964/10000], Loss: 118.31048583984375, Learning Rate: 0.01\n",
      "Epoch [965/10000], Loss: 118.28924560546875, Learning Rate: 0.01\n",
      "Epoch [966/10000], Loss: 118.26787567138672, Learning Rate: 0.01\n",
      "Epoch [967/10000], Loss: 118.24674987792969, Learning Rate: 0.01\n",
      "Epoch [968/10000], Loss: 118.22550964355469, Learning Rate: 0.01\n",
      "Epoch [969/10000], Loss: 118.20448303222656, Learning Rate: 0.01\n",
      "Epoch [970/10000], Loss: 118.18338012695312, Learning Rate: 0.01\n",
      "Epoch [971/10000], Loss: 118.16240692138672, Learning Rate: 0.01\n",
      "Epoch [972/10000], Loss: 118.1413345336914, Learning Rate: 0.01\n",
      "Epoch [973/10000], Loss: 118.12049102783203, Learning Rate: 0.01\n",
      "Epoch [974/10000], Loss: 118.09959411621094, Learning Rate: 0.01\n",
      "Epoch [975/10000], Loss: 118.07878112792969, Learning Rate: 0.01\n",
      "Epoch [976/10000], Loss: 118.05802154541016, Learning Rate: 0.01\n",
      "Epoch [977/10000], Loss: 118.03717041015625, Learning Rate: 0.01\n",
      "Epoch [978/10000], Loss: 118.0165023803711, Learning Rate: 0.01\n",
      "Epoch [979/10000], Loss: 117.99589538574219, Learning Rate: 0.01\n",
      "Epoch [980/10000], Loss: 117.97534942626953, Learning Rate: 0.01\n",
      "Epoch [981/10000], Loss: 117.9547348022461, Learning Rate: 0.01\n",
      "Epoch [982/10000], Loss: 117.93417358398438, Learning Rate: 0.01\n",
      "Epoch [983/10000], Loss: 117.9137191772461, Learning Rate: 0.01\n",
      "Epoch [984/10000], Loss: 117.89335632324219, Learning Rate: 0.01\n",
      "Epoch [985/10000], Loss: 117.87290954589844, Learning Rate: 0.01\n",
      "Epoch [986/10000], Loss: 117.85258483886719, Learning Rate: 0.01\n",
      "Epoch [987/10000], Loss: 117.832275390625, Learning Rate: 0.01\n",
      "Epoch [988/10000], Loss: 117.8121337890625, Learning Rate: 0.01\n",
      "Epoch [989/10000], Loss: 117.79180145263672, Learning Rate: 0.01\n",
      "Epoch [990/10000], Loss: 117.77169036865234, Learning Rate: 0.01\n",
      "Epoch [991/10000], Loss: 117.75154876708984, Learning Rate: 0.01\n",
      "Epoch [992/10000], Loss: 117.73136901855469, Learning Rate: 0.01\n",
      "Epoch [993/10000], Loss: 117.71139526367188, Learning Rate: 0.01\n",
      "Epoch [994/10000], Loss: 117.69131469726562, Learning Rate: 0.01\n",
      "Epoch [995/10000], Loss: 117.6713638305664, Learning Rate: 0.01\n",
      "Epoch [996/10000], Loss: 117.65149688720703, Learning Rate: 0.01\n",
      "Epoch [997/10000], Loss: 117.63154602050781, Learning Rate: 0.01\n",
      "Epoch [998/10000], Loss: 117.61163330078125, Learning Rate: 0.01\n",
      "Epoch [999/10000], Loss: 117.5918960571289, Learning Rate: 0.01\n",
      "Epoch [1000/10000], Loss: 117.57203674316406, Learning Rate: 0.01\n",
      "Epoch [1001/10000], Loss: 117.55242919921875, Learning Rate: 0.01\n",
      "Epoch [1002/10000], Loss: 117.53264617919922, Learning Rate: 0.01\n",
      "Epoch [1003/10000], Loss: 117.5130386352539, Learning Rate: 0.01\n",
      "Epoch [1004/10000], Loss: 117.49333190917969, Learning Rate: 0.01\n",
      "Epoch [1005/10000], Loss: 117.47378540039062, Learning Rate: 0.01\n",
      "Epoch [1006/10000], Loss: 117.45417785644531, Learning Rate: 0.01\n",
      "Epoch [1007/10000], Loss: 117.4346694946289, Learning Rate: 0.01\n",
      "Epoch [1008/10000], Loss: 117.41519927978516, Learning Rate: 0.01\n",
      "Epoch [1009/10000], Loss: 117.3958511352539, Learning Rate: 0.01\n",
      "Epoch [1010/10000], Loss: 117.37647247314453, Learning Rate: 0.01\n",
      "Epoch [1011/10000], Loss: 117.35701751708984, Learning Rate: 0.01\n",
      "Epoch [1012/10000], Loss: 117.33782196044922, Learning Rate: 0.01\n",
      "Epoch [1013/10000], Loss: 117.31848907470703, Learning Rate: 0.01\n",
      "Epoch [1014/10000], Loss: 117.29915618896484, Learning Rate: 0.01\n",
      "Epoch [1015/10000], Loss: 117.27986145019531, Learning Rate: 0.01\n",
      "Epoch [1016/10000], Loss: 117.26079559326172, Learning Rate: 0.01\n",
      "Epoch [1017/10000], Loss: 117.24159240722656, Learning Rate: 0.01\n",
      "Epoch [1018/10000], Loss: 117.2225112915039, Learning Rate: 0.01\n",
      "Epoch [1019/10000], Loss: 117.20344543457031, Learning Rate: 0.01\n",
      "Epoch [1020/10000], Loss: 117.18436431884766, Learning Rate: 0.01\n",
      "Epoch [1021/10000], Loss: 117.16532135009766, Learning Rate: 0.01\n",
      "Epoch [1022/10000], Loss: 117.14636993408203, Learning Rate: 0.01\n",
      "Epoch [1023/10000], Loss: 117.12741088867188, Learning Rate: 0.01\n",
      "Epoch [1024/10000], Loss: 117.10852813720703, Learning Rate: 0.01\n",
      "Epoch [1025/10000], Loss: 117.0895004272461, Learning Rate: 0.01\n",
      "Epoch [1026/10000], Loss: 117.07080841064453, Learning Rate: 0.01\n",
      "Epoch [1027/10000], Loss: 117.05197143554688, Learning Rate: 0.01\n",
      "Epoch [1028/10000], Loss: 117.03317260742188, Learning Rate: 0.01\n",
      "Epoch [1029/10000], Loss: 117.01444244384766, Learning Rate: 0.01\n",
      "Epoch [1030/10000], Loss: 116.99578094482422, Learning Rate: 0.01\n",
      "Epoch [1031/10000], Loss: 116.9770736694336, Learning Rate: 0.01\n",
      "Epoch [1032/10000], Loss: 116.95841979980469, Learning Rate: 0.01\n",
      "Epoch [1033/10000], Loss: 116.93974304199219, Learning Rate: 0.01\n",
      "Epoch [1034/10000], Loss: 116.92124938964844, Learning Rate: 0.01\n",
      "Epoch [1035/10000], Loss: 116.9026107788086, Learning Rate: 0.01\n",
      "Epoch [1036/10000], Loss: 116.88406372070312, Learning Rate: 0.01\n",
      "Epoch [1037/10000], Loss: 116.8656005859375, Learning Rate: 0.01\n",
      "Epoch [1038/10000], Loss: 116.84713745117188, Learning Rate: 0.01\n",
      "Epoch [1039/10000], Loss: 116.82868194580078, Learning Rate: 0.01\n",
      "Epoch [1040/10000], Loss: 116.81021118164062, Learning Rate: 0.01\n",
      "Epoch [1041/10000], Loss: 116.7918701171875, Learning Rate: 0.01\n",
      "Epoch [1042/10000], Loss: 116.77359771728516, Learning Rate: 0.01\n",
      "Epoch [1043/10000], Loss: 116.75533294677734, Learning Rate: 0.01\n",
      "Epoch [1044/10000], Loss: 116.73699188232422, Learning Rate: 0.01\n",
      "Epoch [1045/10000], Loss: 116.71873474121094, Learning Rate: 0.01\n",
      "Epoch [1046/10000], Loss: 116.70052337646484, Learning Rate: 0.01\n",
      "Epoch [1047/10000], Loss: 116.68234252929688, Learning Rate: 0.01\n",
      "Epoch [1048/10000], Loss: 116.66416931152344, Learning Rate: 0.01\n",
      "Epoch [1049/10000], Loss: 116.64600372314453, Learning Rate: 0.01\n",
      "Epoch [1050/10000], Loss: 116.62791442871094, Learning Rate: 0.01\n",
      "Epoch [1051/10000], Loss: 116.6098403930664, Learning Rate: 0.01\n",
      "Epoch [1052/10000], Loss: 116.59181213378906, Learning Rate: 0.01\n",
      "Epoch [1053/10000], Loss: 116.5737533569336, Learning Rate: 0.01\n",
      "Epoch [1054/10000], Loss: 116.5558090209961, Learning Rate: 0.01\n",
      "Epoch [1055/10000], Loss: 116.53778839111328, Learning Rate: 0.01\n",
      "Epoch [1056/10000], Loss: 116.51982116699219, Learning Rate: 0.01\n",
      "Epoch [1057/10000], Loss: 116.50188446044922, Learning Rate: 0.01\n",
      "Epoch [1058/10000], Loss: 116.48405456542969, Learning Rate: 0.01\n",
      "Epoch [1059/10000], Loss: 116.46614837646484, Learning Rate: 0.01\n",
      "Epoch [1060/10000], Loss: 116.44833374023438, Learning Rate: 0.01\n",
      "Epoch [1061/10000], Loss: 116.43053436279297, Learning Rate: 0.01\n",
      "Epoch [1062/10000], Loss: 116.41275024414062, Learning Rate: 0.01\n",
      "Epoch [1063/10000], Loss: 116.39505767822266, Learning Rate: 0.01\n",
      "Epoch [1064/10000], Loss: 116.37730407714844, Learning Rate: 0.01\n",
      "Epoch [1065/10000], Loss: 116.35955047607422, Learning Rate: 0.01\n",
      "Epoch [1066/10000], Loss: 116.34201049804688, Learning Rate: 0.01\n",
      "Epoch [1067/10000], Loss: 116.32427215576172, Learning Rate: 0.01\n",
      "Epoch [1068/10000], Loss: 116.30665588378906, Learning Rate: 0.01\n",
      "Epoch [1069/10000], Loss: 116.28911590576172, Learning Rate: 0.01\n",
      "Epoch [1070/10000], Loss: 116.27143096923828, Learning Rate: 0.01\n",
      "Epoch [1071/10000], Loss: 116.25391387939453, Learning Rate: 0.01\n",
      "Epoch [1072/10000], Loss: 116.23637390136719, Learning Rate: 0.01\n",
      "Epoch [1073/10000], Loss: 116.21894073486328, Learning Rate: 0.01\n",
      "Epoch [1074/10000], Loss: 116.20143127441406, Learning Rate: 0.01\n",
      "Epoch [1075/10000], Loss: 116.18402099609375, Learning Rate: 0.01\n",
      "Epoch [1076/10000], Loss: 116.16655731201172, Learning Rate: 0.01\n",
      "Epoch [1077/10000], Loss: 116.14923858642578, Learning Rate: 0.01\n",
      "Epoch [1078/10000], Loss: 116.13177490234375, Learning Rate: 0.01\n",
      "Epoch [1079/10000], Loss: 116.11446380615234, Learning Rate: 0.01\n",
      "Epoch [1080/10000], Loss: 116.09716796875, Learning Rate: 0.01\n",
      "Epoch [1081/10000], Loss: 116.0798110961914, Learning Rate: 0.01\n",
      "Epoch [1082/10000], Loss: 116.06255340576172, Learning Rate: 0.01\n",
      "Epoch [1083/10000], Loss: 116.04531860351562, Learning Rate: 0.01\n",
      "Epoch [1084/10000], Loss: 116.02799224853516, Learning Rate: 0.01\n",
      "Epoch [1085/10000], Loss: 116.01093292236328, Learning Rate: 0.01\n",
      "Epoch [1086/10000], Loss: 115.99372100830078, Learning Rate: 0.01\n",
      "Epoch [1087/10000], Loss: 115.97659301757812, Learning Rate: 0.01\n",
      "Epoch [1088/10000], Loss: 115.95938110351562, Learning Rate: 0.01\n",
      "Epoch [1089/10000], Loss: 115.94230651855469, Learning Rate: 0.01\n",
      "Epoch [1090/10000], Loss: 115.92523193359375, Learning Rate: 0.01\n",
      "Epoch [1091/10000], Loss: 115.90812683105469, Learning Rate: 0.01\n",
      "Epoch [1092/10000], Loss: 115.89122772216797, Learning Rate: 0.01\n",
      "Epoch [1093/10000], Loss: 115.87398529052734, Learning Rate: 0.01\n",
      "Epoch [1094/10000], Loss: 115.85713958740234, Learning Rate: 0.01\n",
      "Epoch [1095/10000], Loss: 115.84007263183594, Learning Rate: 0.01\n",
      "Epoch [1096/10000], Loss: 115.82320404052734, Learning Rate: 0.01\n",
      "Epoch [1097/10000], Loss: 115.80618286132812, Learning Rate: 0.01\n",
      "Epoch [1098/10000], Loss: 115.78925323486328, Learning Rate: 0.01\n",
      "Epoch [1099/10000], Loss: 115.77241516113281, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1100/10000], Loss: 115.75556945800781, Learning Rate: 0.01\n",
      "Epoch [1101/10000], Loss: 115.73870849609375, Learning Rate: 0.01\n",
      "Epoch [1102/10000], Loss: 115.72191619873047, Learning Rate: 0.01\n",
      "Epoch [1103/10000], Loss: 115.70507049560547, Learning Rate: 0.01\n",
      "Epoch [1104/10000], Loss: 115.688232421875, Learning Rate: 0.01\n",
      "Epoch [1105/10000], Loss: 115.67156982421875, Learning Rate: 0.01\n",
      "Epoch [1106/10000], Loss: 115.65485382080078, Learning Rate: 0.01\n",
      "Epoch [1107/10000], Loss: 115.63811492919922, Learning Rate: 0.01\n",
      "Epoch [1108/10000], Loss: 115.62133026123047, Learning Rate: 0.01\n",
      "Epoch [1109/10000], Loss: 115.60459899902344, Learning Rate: 0.01\n",
      "Epoch [1110/10000], Loss: 115.58797454833984, Learning Rate: 0.01\n",
      "Epoch [1111/10000], Loss: 115.57136535644531, Learning Rate: 0.01\n",
      "Epoch [1112/10000], Loss: 115.55477905273438, Learning Rate: 0.01\n",
      "Epoch [1113/10000], Loss: 115.53816986083984, Learning Rate: 0.01\n",
      "Epoch [1114/10000], Loss: 115.52156829833984, Learning Rate: 0.01\n",
      "Epoch [1115/10000], Loss: 115.5050277709961, Learning Rate: 0.01\n",
      "Epoch [1116/10000], Loss: 115.48851013183594, Learning Rate: 0.01\n",
      "Epoch [1117/10000], Loss: 115.47201538085938, Learning Rate: 0.01\n",
      "Epoch [1118/10000], Loss: 115.45553588867188, Learning Rate: 0.01\n",
      "Epoch [1119/10000], Loss: 115.43897247314453, Learning Rate: 0.01\n",
      "Epoch [1120/10000], Loss: 115.42249298095703, Learning Rate: 0.01\n",
      "Epoch [1121/10000], Loss: 115.40605163574219, Learning Rate: 0.01\n",
      "Epoch [1122/10000], Loss: 115.38959503173828, Learning Rate: 0.01\n",
      "Epoch [1123/10000], Loss: 115.37323760986328, Learning Rate: 0.01\n",
      "Epoch [1124/10000], Loss: 115.35692596435547, Learning Rate: 0.01\n",
      "Epoch [1125/10000], Loss: 115.34044647216797, Learning Rate: 0.01\n",
      "Epoch [1126/10000], Loss: 115.32405853271484, Learning Rate: 0.01\n",
      "Epoch [1127/10000], Loss: 115.3078842163086, Learning Rate: 0.01\n",
      "Epoch [1128/10000], Loss: 115.29151916503906, Learning Rate: 0.01\n",
      "Epoch [1129/10000], Loss: 115.27523803710938, Learning Rate: 0.01\n",
      "Epoch [1130/10000], Loss: 115.2589111328125, Learning Rate: 0.01\n",
      "Epoch [1131/10000], Loss: 115.24271392822266, Learning Rate: 0.01\n",
      "Epoch [1132/10000], Loss: 115.22650146484375, Learning Rate: 0.01\n",
      "Epoch [1133/10000], Loss: 115.21025085449219, Learning Rate: 0.01\n",
      "Epoch [1134/10000], Loss: 115.19393920898438, Learning Rate: 0.01\n",
      "Epoch [1135/10000], Loss: 115.17779541015625, Learning Rate: 0.01\n",
      "Epoch [1136/10000], Loss: 115.1617431640625, Learning Rate: 0.01\n",
      "Epoch [1137/10000], Loss: 115.14556121826172, Learning Rate: 0.01\n",
      "Epoch [1138/10000], Loss: 115.12944793701172, Learning Rate: 0.01\n",
      "Epoch [1139/10000], Loss: 115.11328887939453, Learning Rate: 0.01\n",
      "Epoch [1140/10000], Loss: 115.09723663330078, Learning Rate: 0.01\n",
      "Epoch [1141/10000], Loss: 115.0811996459961, Learning Rate: 0.01\n",
      "Epoch [1142/10000], Loss: 115.06520080566406, Learning Rate: 0.01\n",
      "Epoch [1143/10000], Loss: 115.04905700683594, Learning Rate: 0.01\n",
      "Epoch [1144/10000], Loss: 115.0330581665039, Learning Rate: 0.01\n",
      "Epoch [1145/10000], Loss: 115.01701354980469, Learning Rate: 0.01\n",
      "Epoch [1146/10000], Loss: 115.00098419189453, Learning Rate: 0.01\n",
      "Epoch [1147/10000], Loss: 114.98499298095703, Learning Rate: 0.01\n",
      "Epoch [1148/10000], Loss: 114.9691162109375, Learning Rate: 0.01\n",
      "Epoch [1149/10000], Loss: 114.95318603515625, Learning Rate: 0.01\n",
      "Epoch [1150/10000], Loss: 114.93726348876953, Learning Rate: 0.01\n",
      "Epoch [1151/10000], Loss: 114.9212875366211, Learning Rate: 0.01\n",
      "Epoch [1152/10000], Loss: 114.90547943115234, Learning Rate: 0.01\n",
      "Epoch [1153/10000], Loss: 114.88957214355469, Learning Rate: 0.01\n",
      "Epoch [1154/10000], Loss: 114.8736801147461, Learning Rate: 0.01\n",
      "Epoch [1155/10000], Loss: 114.85785675048828, Learning Rate: 0.01\n",
      "Epoch [1156/10000], Loss: 114.84199523925781, Learning Rate: 0.01\n",
      "Epoch [1157/10000], Loss: 114.82626342773438, Learning Rate: 0.01\n",
      "Epoch [1158/10000], Loss: 114.81041717529297, Learning Rate: 0.01\n",
      "Epoch [1159/10000], Loss: 114.79458618164062, Learning Rate: 0.01\n",
      "Epoch [1160/10000], Loss: 114.77890014648438, Learning Rate: 0.01\n",
      "Epoch [1161/10000], Loss: 114.76306915283203, Learning Rate: 0.01\n",
      "Epoch [1162/10000], Loss: 114.74742126464844, Learning Rate: 0.01\n",
      "Epoch [1163/10000], Loss: 114.73165893554688, Learning Rate: 0.01\n",
      "Epoch [1164/10000], Loss: 114.71605682373047, Learning Rate: 0.01\n",
      "Epoch [1165/10000], Loss: 114.70037841796875, Learning Rate: 0.01\n",
      "Epoch [1166/10000], Loss: 114.6846694946289, Learning Rate: 0.01\n",
      "Epoch [1167/10000], Loss: 114.66899108886719, Learning Rate: 0.01\n",
      "Epoch [1168/10000], Loss: 114.65330505371094, Learning Rate: 0.01\n",
      "Epoch [1169/10000], Loss: 114.63765716552734, Learning Rate: 0.01\n",
      "Epoch [1170/10000], Loss: 114.62215423583984, Learning Rate: 0.01\n",
      "Epoch [1171/10000], Loss: 114.60649871826172, Learning Rate: 0.01\n",
      "Epoch [1172/10000], Loss: 114.59088897705078, Learning Rate: 0.01\n",
      "Epoch [1173/10000], Loss: 114.57538604736328, Learning Rate: 0.01\n",
      "Epoch [1174/10000], Loss: 114.55986785888672, Learning Rate: 0.01\n",
      "Epoch [1175/10000], Loss: 114.54430389404297, Learning Rate: 0.01\n",
      "Epoch [1176/10000], Loss: 114.52874755859375, Learning Rate: 0.01\n",
      "Epoch [1177/10000], Loss: 114.51329803466797, Learning Rate: 0.01\n",
      "Epoch [1178/10000], Loss: 114.49769592285156, Learning Rate: 0.01\n",
      "Epoch [1179/10000], Loss: 114.48231506347656, Learning Rate: 0.01\n",
      "Epoch [1180/10000], Loss: 114.46691131591797, Learning Rate: 0.01\n",
      "Epoch [1181/10000], Loss: 114.45130157470703, Learning Rate: 0.01\n",
      "Epoch [1182/10000], Loss: 114.43595886230469, Learning Rate: 0.01\n",
      "Epoch [1183/10000], Loss: 114.42047119140625, Learning Rate: 0.01\n",
      "Epoch [1184/10000], Loss: 114.40501403808594, Learning Rate: 0.01\n",
      "Epoch [1185/10000], Loss: 114.38965606689453, Learning Rate: 0.01\n",
      "Epoch [1186/10000], Loss: 114.37435150146484, Learning Rate: 0.01\n",
      "Epoch [1187/10000], Loss: 114.35901641845703, Learning Rate: 0.01\n",
      "Epoch [1188/10000], Loss: 114.34354400634766, Learning Rate: 0.01\n",
      "Epoch [1189/10000], Loss: 114.32830810546875, Learning Rate: 0.01\n",
      "Epoch [1190/10000], Loss: 114.3128433227539, Learning Rate: 0.01\n",
      "Epoch [1191/10000], Loss: 114.29756164550781, Learning Rate: 0.01\n",
      "Epoch [1192/10000], Loss: 114.28226470947266, Learning Rate: 0.01\n",
      "Epoch [1193/10000], Loss: 114.26703643798828, Learning Rate: 0.01\n",
      "Epoch [1194/10000], Loss: 114.25173950195312, Learning Rate: 0.01\n",
      "Epoch [1195/10000], Loss: 114.23640441894531, Learning Rate: 0.01\n",
      "Epoch [1196/10000], Loss: 114.22127532958984, Learning Rate: 0.01\n",
      "Epoch [1197/10000], Loss: 114.20602416992188, Learning Rate: 0.01\n",
      "Epoch [1198/10000], Loss: 114.19074249267578, Learning Rate: 0.01\n",
      "Epoch [1199/10000], Loss: 114.17559051513672, Learning Rate: 0.01\n",
      "Epoch [1200/10000], Loss: 114.16030883789062, Learning Rate: 0.01\n",
      "Epoch [1201/10000], Loss: 114.14515686035156, Learning Rate: 0.01\n",
      "Epoch [1202/10000], Loss: 114.1299819946289, Learning Rate: 0.01\n",
      "Epoch [1203/10000], Loss: 114.11486053466797, Learning Rate: 0.01\n",
      "Epoch [1204/10000], Loss: 114.09973907470703, Learning Rate: 0.01\n",
      "Epoch [1205/10000], Loss: 114.0845718383789, Learning Rate: 0.01\n",
      "Epoch [1206/10000], Loss: 114.06941223144531, Learning Rate: 0.01\n",
      "Epoch [1207/10000], Loss: 114.05426788330078, Learning Rate: 0.01\n",
      "Epoch [1208/10000], Loss: 114.03921508789062, Learning Rate: 0.01\n",
      "Epoch [1209/10000], Loss: 114.02421569824219, Learning Rate: 0.01\n",
      "Epoch [1210/10000], Loss: 114.00912475585938, Learning Rate: 0.01\n",
      "Epoch [1211/10000], Loss: 113.99403381347656, Learning Rate: 0.01\n",
      "Epoch [1212/10000], Loss: 113.97900390625, Learning Rate: 0.01\n",
      "Epoch [1213/10000], Loss: 113.96392059326172, Learning Rate: 0.01\n",
      "Epoch [1214/10000], Loss: 113.94894409179688, Learning Rate: 0.01\n",
      "Epoch [1215/10000], Loss: 113.93387603759766, Learning Rate: 0.01\n",
      "Epoch [1216/10000], Loss: 113.91889953613281, Learning Rate: 0.01\n",
      "Epoch [1217/10000], Loss: 113.90392303466797, Learning Rate: 0.01\n",
      "Epoch [1218/10000], Loss: 113.88894653320312, Learning Rate: 0.01\n",
      "Epoch [1219/10000], Loss: 113.87403869628906, Learning Rate: 0.01\n",
      "Epoch [1220/10000], Loss: 113.85899353027344, Learning Rate: 0.01\n",
      "Epoch [1221/10000], Loss: 113.84404754638672, Learning Rate: 0.01\n",
      "Epoch [1222/10000], Loss: 113.82918548583984, Learning Rate: 0.01\n",
      "Epoch [1223/10000], Loss: 113.81420135498047, Learning Rate: 0.01\n",
      "Epoch [1224/10000], Loss: 113.79938507080078, Learning Rate: 0.01\n",
      "Epoch [1225/10000], Loss: 113.78443908691406, Learning Rate: 0.01\n",
      "Epoch [1226/10000], Loss: 113.7695541381836, Learning Rate: 0.01\n",
      "Epoch [1227/10000], Loss: 113.75470733642578, Learning Rate: 0.01\n",
      "Epoch [1228/10000], Loss: 113.73977661132812, Learning Rate: 0.01\n",
      "Epoch [1229/10000], Loss: 113.72503662109375, Learning Rate: 0.01\n",
      "Epoch [1230/10000], Loss: 113.71013641357422, Learning Rate: 0.01\n",
      "Epoch [1231/10000], Loss: 113.69532775878906, Learning Rate: 0.01\n",
      "Epoch [1232/10000], Loss: 113.68045043945312, Learning Rate: 0.01\n",
      "Epoch [1233/10000], Loss: 113.66573333740234, Learning Rate: 0.01\n",
      "Epoch [1234/10000], Loss: 113.6508560180664, Learning Rate: 0.01\n",
      "Epoch [1235/10000], Loss: 113.63607025146484, Learning Rate: 0.01\n",
      "Epoch [1236/10000], Loss: 113.62136840820312, Learning Rate: 0.01\n",
      "Epoch [1237/10000], Loss: 113.60657501220703, Learning Rate: 0.01\n",
      "Epoch [1238/10000], Loss: 113.59182739257812, Learning Rate: 0.01\n",
      "Epoch [1239/10000], Loss: 113.57708740234375, Learning Rate: 0.01\n",
      "Epoch [1240/10000], Loss: 113.56234741210938, Learning Rate: 0.01\n",
      "Epoch [1241/10000], Loss: 113.547607421875, Learning Rate: 0.01\n",
      "Epoch [1242/10000], Loss: 113.53288269042969, Learning Rate: 0.01\n",
      "Epoch [1243/10000], Loss: 113.51822662353516, Learning Rate: 0.01\n",
      "Epoch [1244/10000], Loss: 113.5036392211914, Learning Rate: 0.01\n",
      "Epoch [1245/10000], Loss: 113.48890686035156, Learning Rate: 0.01\n",
      "Epoch [1246/10000], Loss: 113.47418212890625, Learning Rate: 0.01\n",
      "Epoch [1247/10000], Loss: 113.45946502685547, Learning Rate: 0.01\n",
      "Epoch [1248/10000], Loss: 113.44488525390625, Learning Rate: 0.01\n",
      "Epoch [1249/10000], Loss: 113.43023681640625, Learning Rate: 0.01\n",
      "Epoch [1250/10000], Loss: 113.41557312011719, Learning Rate: 0.01\n",
      "Epoch [1251/10000], Loss: 113.4010238647461, Learning Rate: 0.01\n",
      "Epoch [1252/10000], Loss: 113.38628387451172, Learning Rate: 0.01\n",
      "Epoch [1253/10000], Loss: 113.37169647216797, Learning Rate: 0.01\n",
      "Epoch [1254/10000], Loss: 113.357177734375, Learning Rate: 0.01\n",
      "Epoch [1255/10000], Loss: 113.34259033203125, Learning Rate: 0.01\n",
      "Epoch [1256/10000], Loss: 113.32806396484375, Learning Rate: 0.01\n",
      "Epoch [1257/10000], Loss: 113.31349182128906, Learning Rate: 0.01\n",
      "Epoch [1258/10000], Loss: 113.29881286621094, Learning Rate: 0.01\n",
      "Epoch [1259/10000], Loss: 113.28441619873047, Learning Rate: 0.01\n",
      "Epoch [1260/10000], Loss: 113.26981353759766, Learning Rate: 0.01\n",
      "Epoch [1261/10000], Loss: 113.25531005859375, Learning Rate: 0.01\n",
      "Epoch [1262/10000], Loss: 113.24080657958984, Learning Rate: 0.01\n",
      "Epoch [1263/10000], Loss: 113.22632598876953, Learning Rate: 0.01\n",
      "Epoch [1264/10000], Loss: 113.21179962158203, Learning Rate: 0.01\n",
      "Epoch [1265/10000], Loss: 113.19728088378906, Learning Rate: 0.01\n",
      "Epoch [1266/10000], Loss: 113.18291473388672, Learning Rate: 0.01\n",
      "Epoch [1267/10000], Loss: 113.16832733154297, Learning Rate: 0.01\n",
      "Epoch [1268/10000], Loss: 113.1539077758789, Learning Rate: 0.01\n",
      "Epoch [1269/10000], Loss: 113.13948059082031, Learning Rate: 0.01\n",
      "Epoch [1270/10000], Loss: 113.12506866455078, Learning Rate: 0.01\n",
      "Epoch [1271/10000], Loss: 113.11067199707031, Learning Rate: 0.01\n",
      "Epoch [1272/10000], Loss: 113.09618377685547, Learning Rate: 0.01\n",
      "Epoch [1273/10000], Loss: 113.08183288574219, Learning Rate: 0.01\n",
      "Epoch [1274/10000], Loss: 113.06735229492188, Learning Rate: 0.01\n",
      "Epoch [1275/10000], Loss: 113.0530014038086, Learning Rate: 0.01\n",
      "Epoch [1276/10000], Loss: 113.03864288330078, Learning Rate: 0.01\n",
      "Epoch [1277/10000], Loss: 113.02427673339844, Learning Rate: 0.01\n",
      "Epoch [1278/10000], Loss: 113.00990295410156, Learning Rate: 0.01\n",
      "Epoch [1279/10000], Loss: 112.99546813964844, Learning Rate: 0.01\n",
      "Epoch [1280/10000], Loss: 112.98106384277344, Learning Rate: 0.01\n",
      "Epoch [1281/10000], Loss: 112.96676635742188, Learning Rate: 0.01\n",
      "Epoch [1282/10000], Loss: 112.95247650146484, Learning Rate: 0.01\n",
      "Epoch [1283/10000], Loss: 112.9381103515625, Learning Rate: 0.01\n",
      "Epoch [1284/10000], Loss: 112.92383575439453, Learning Rate: 0.01\n",
      "Epoch [1285/10000], Loss: 112.90955352783203, Learning Rate: 0.01\n",
      "Epoch [1286/10000], Loss: 112.89514923095703, Learning Rate: 0.01\n",
      "Epoch [1287/10000], Loss: 112.88089752197266, Learning Rate: 0.01\n",
      "Epoch [1288/10000], Loss: 112.86671447753906, Learning Rate: 0.01\n",
      "Epoch [1289/10000], Loss: 112.85238647460938, Learning Rate: 0.01\n",
      "Epoch [1290/10000], Loss: 112.83805847167969, Learning Rate: 0.01\n",
      "Epoch [1291/10000], Loss: 112.8238296508789, Learning Rate: 0.01\n",
      "Epoch [1292/10000], Loss: 112.80962371826172, Learning Rate: 0.01\n",
      "Epoch [1293/10000], Loss: 112.79541778564453, Learning Rate: 0.01\n",
      "Epoch [1294/10000], Loss: 112.78109741210938, Learning Rate: 0.01\n",
      "Epoch [1295/10000], Loss: 112.76688385009766, Learning Rate: 0.01\n",
      "Epoch [1296/10000], Loss: 112.7525634765625, Learning Rate: 0.01\n",
      "Epoch [1297/10000], Loss: 112.73847198486328, Learning Rate: 0.01\n",
      "Epoch [1298/10000], Loss: 112.72430419921875, Learning Rate: 0.01\n",
      "Epoch [1299/10000], Loss: 112.71006774902344, Learning Rate: 0.01\n",
      "Epoch [1300/10000], Loss: 112.69588470458984, Learning Rate: 0.01\n",
      "Epoch [1301/10000], Loss: 112.68177032470703, Learning Rate: 0.01\n",
      "Epoch [1302/10000], Loss: 112.66756439208984, Learning Rate: 0.01\n",
      "Epoch [1303/10000], Loss: 112.6534423828125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1304/10000], Loss: 112.63921356201172, Learning Rate: 0.01\n",
      "Epoch [1305/10000], Loss: 112.62512969970703, Learning Rate: 0.01\n",
      "Epoch [1306/10000], Loss: 112.61094665527344, Learning Rate: 0.01\n",
      "Epoch [1307/10000], Loss: 112.59687042236328, Learning Rate: 0.01\n",
      "Epoch [1308/10000], Loss: 112.5827407836914, Learning Rate: 0.01\n",
      "Epoch [1309/10000], Loss: 112.568603515625, Learning Rate: 0.01\n",
      "Epoch [1310/10000], Loss: 112.55443572998047, Learning Rate: 0.01\n",
      "Epoch [1311/10000], Loss: 112.54043579101562, Learning Rate: 0.01\n",
      "Epoch [1312/10000], Loss: 112.5263671875, Learning Rate: 0.01\n",
      "Epoch [1313/10000], Loss: 112.51216888427734, Learning Rate: 0.01\n",
      "Epoch [1314/10000], Loss: 112.49818420410156, Learning Rate: 0.01\n",
      "Epoch [1315/10000], Loss: 112.48408508300781, Learning Rate: 0.01\n",
      "Epoch [1316/10000], Loss: 112.469970703125, Learning Rate: 0.01\n",
      "Epoch [1317/10000], Loss: 112.45600128173828, Learning Rate: 0.01\n",
      "Epoch [1318/10000], Loss: 112.44194793701172, Learning Rate: 0.01\n",
      "Epoch [1319/10000], Loss: 112.42781066894531, Learning Rate: 0.01\n",
      "Epoch [1320/10000], Loss: 112.4138412475586, Learning Rate: 0.01\n",
      "Epoch [1321/10000], Loss: 112.39983367919922, Learning Rate: 0.01\n",
      "Epoch [1322/10000], Loss: 112.3858413696289, Learning Rate: 0.01\n",
      "Epoch [1323/10000], Loss: 112.37176513671875, Learning Rate: 0.01\n",
      "Epoch [1324/10000], Loss: 112.35778045654297, Learning Rate: 0.01\n",
      "Epoch [1325/10000], Loss: 112.34371185302734, Learning Rate: 0.01\n",
      "Epoch [1326/10000], Loss: 112.32986450195312, Learning Rate: 0.01\n",
      "Epoch [1327/10000], Loss: 112.31590270996094, Learning Rate: 0.01\n",
      "Epoch [1328/10000], Loss: 112.3019027709961, Learning Rate: 0.01\n",
      "Epoch [1329/10000], Loss: 112.28787994384766, Learning Rate: 0.01\n",
      "Epoch [1330/10000], Loss: 112.27391815185547, Learning Rate: 0.01\n",
      "Epoch [1331/10000], Loss: 112.25994873046875, Learning Rate: 0.01\n",
      "Epoch [1332/10000], Loss: 112.24613189697266, Learning Rate: 0.01\n",
      "Epoch [1333/10000], Loss: 112.2320785522461, Learning Rate: 0.01\n",
      "Epoch [1334/10000], Loss: 112.21814727783203, Learning Rate: 0.01\n",
      "Epoch [1335/10000], Loss: 112.20420837402344, Learning Rate: 0.01\n",
      "Epoch [1336/10000], Loss: 112.19032287597656, Learning Rate: 0.01\n",
      "Epoch [1337/10000], Loss: 112.17633056640625, Learning Rate: 0.01\n",
      "Epoch [1338/10000], Loss: 112.1624526977539, Learning Rate: 0.01\n",
      "Epoch [1339/10000], Loss: 112.14857482910156, Learning Rate: 0.01\n",
      "Epoch [1340/10000], Loss: 112.13469696044922, Learning Rate: 0.01\n",
      "Epoch [1341/10000], Loss: 112.12088012695312, Learning Rate: 0.01\n",
      "Epoch [1342/10000], Loss: 112.10694885253906, Learning Rate: 0.01\n",
      "Epoch [1343/10000], Loss: 112.09307861328125, Learning Rate: 0.01\n",
      "Epoch [1344/10000], Loss: 112.07915496826172, Learning Rate: 0.01\n",
      "Epoch [1345/10000], Loss: 112.06541442871094, Learning Rate: 0.01\n",
      "Epoch [1346/10000], Loss: 112.05144500732422, Learning Rate: 0.01\n",
      "Epoch [1347/10000], Loss: 112.0376968383789, Learning Rate: 0.01\n",
      "Epoch [1348/10000], Loss: 112.02386474609375, Learning Rate: 0.01\n",
      "Epoch [1349/10000], Loss: 112.01006317138672, Learning Rate: 0.01\n",
      "Epoch [1350/10000], Loss: 111.99620056152344, Learning Rate: 0.01\n",
      "Epoch [1351/10000], Loss: 111.98237609863281, Learning Rate: 0.01\n",
      "Epoch [1352/10000], Loss: 111.96858215332031, Learning Rate: 0.01\n",
      "Epoch [1353/10000], Loss: 111.95475006103516, Learning Rate: 0.01\n",
      "Epoch [1354/10000], Loss: 111.94097900390625, Learning Rate: 0.01\n",
      "Epoch [1355/10000], Loss: 111.92719268798828, Learning Rate: 0.01\n",
      "Epoch [1356/10000], Loss: 111.91343688964844, Learning Rate: 0.01\n",
      "Epoch [1357/10000], Loss: 111.89955139160156, Learning Rate: 0.01\n",
      "Epoch [1358/10000], Loss: 111.88587188720703, Learning Rate: 0.01\n",
      "Epoch [1359/10000], Loss: 111.87210845947266, Learning Rate: 0.01\n",
      "Epoch [1360/10000], Loss: 111.8582534790039, Learning Rate: 0.01\n",
      "Epoch [1361/10000], Loss: 111.8445053100586, Learning Rate: 0.01\n",
      "Epoch [1362/10000], Loss: 111.83085632324219, Learning Rate: 0.01\n",
      "Epoch [1363/10000], Loss: 111.81709289550781, Learning Rate: 0.01\n",
      "Epoch [1364/10000], Loss: 111.8033447265625, Learning Rate: 0.01\n",
      "Epoch [1365/10000], Loss: 111.78960418701172, Learning Rate: 0.01\n",
      "Epoch [1366/10000], Loss: 111.77603149414062, Learning Rate: 0.01\n",
      "Epoch [1367/10000], Loss: 111.7622299194336, Learning Rate: 0.01\n",
      "Epoch [1368/10000], Loss: 111.7485122680664, Learning Rate: 0.01\n",
      "Epoch [1369/10000], Loss: 111.7347183227539, Learning Rate: 0.01\n",
      "Epoch [1370/10000], Loss: 111.72111511230469, Learning Rate: 0.01\n",
      "Epoch [1371/10000], Loss: 111.70732116699219, Learning Rate: 0.01\n",
      "Epoch [1372/10000], Loss: 111.6937026977539, Learning Rate: 0.01\n",
      "Epoch [1373/10000], Loss: 111.68003845214844, Learning Rate: 0.01\n",
      "Epoch [1374/10000], Loss: 111.66641235351562, Learning Rate: 0.01\n",
      "Epoch [1375/10000], Loss: 111.65264129638672, Learning Rate: 0.01\n",
      "Epoch [1376/10000], Loss: 111.63906860351562, Learning Rate: 0.01\n",
      "Epoch [1377/10000], Loss: 111.62537384033203, Learning Rate: 0.01\n",
      "Epoch [1378/10000], Loss: 111.61177825927734, Learning Rate: 0.01\n",
      "Epoch [1379/10000], Loss: 111.5980453491211, Learning Rate: 0.01\n",
      "Epoch [1380/10000], Loss: 111.58441162109375, Learning Rate: 0.01\n",
      "Epoch [1381/10000], Loss: 111.5708236694336, Learning Rate: 0.01\n",
      "Epoch [1382/10000], Loss: 111.55724334716797, Learning Rate: 0.01\n",
      "Epoch [1383/10000], Loss: 111.5435791015625, Learning Rate: 0.01\n",
      "Epoch [1384/10000], Loss: 111.52998352050781, Learning Rate: 0.01\n",
      "Epoch [1385/10000], Loss: 111.51641845703125, Learning Rate: 0.01\n",
      "Epoch [1386/10000], Loss: 111.50279235839844, Learning Rate: 0.01\n",
      "Epoch [1387/10000], Loss: 111.48919677734375, Learning Rate: 0.01\n",
      "Epoch [1388/10000], Loss: 111.47560119628906, Learning Rate: 0.01\n",
      "Epoch [1389/10000], Loss: 111.46208190917969, Learning Rate: 0.01\n",
      "Epoch [1390/10000], Loss: 111.44853973388672, Learning Rate: 0.01\n",
      "Epoch [1391/10000], Loss: 111.4349365234375, Learning Rate: 0.01\n",
      "Epoch [1392/10000], Loss: 111.4212875366211, Learning Rate: 0.01\n",
      "Epoch [1393/10000], Loss: 111.40786743164062, Learning Rate: 0.01\n",
      "Epoch [1394/10000], Loss: 111.39424896240234, Learning Rate: 0.01\n",
      "Epoch [1395/10000], Loss: 111.3807373046875, Learning Rate: 0.01\n",
      "Epoch [1396/10000], Loss: 111.3671646118164, Learning Rate: 0.01\n",
      "Epoch [1397/10000], Loss: 111.3536148071289, Learning Rate: 0.01\n",
      "Epoch [1398/10000], Loss: 111.34010314941406, Learning Rate: 0.01\n",
      "Epoch [1399/10000], Loss: 111.3265151977539, Learning Rate: 0.01\n",
      "Epoch [1400/10000], Loss: 111.31319427490234, Learning Rate: 0.01\n",
      "Epoch [1401/10000], Loss: 111.29954528808594, Learning Rate: 0.01\n",
      "Epoch [1402/10000], Loss: 111.2861099243164, Learning Rate: 0.01\n",
      "Epoch [1403/10000], Loss: 111.27259826660156, Learning Rate: 0.01\n",
      "Epoch [1404/10000], Loss: 111.25910186767578, Learning Rate: 0.01\n",
      "Epoch [1405/10000], Loss: 111.24561309814453, Learning Rate: 0.01\n",
      "Epoch [1406/10000], Loss: 111.232177734375, Learning Rate: 0.01\n",
      "Epoch [1407/10000], Loss: 111.21863555908203, Learning Rate: 0.01\n",
      "Epoch [1408/10000], Loss: 111.20515441894531, Learning Rate: 0.01\n",
      "Epoch [1409/10000], Loss: 111.1916732788086, Learning Rate: 0.01\n",
      "Epoch [1410/10000], Loss: 111.17831420898438, Learning Rate: 0.01\n",
      "Epoch [1411/10000], Loss: 111.16480255126953, Learning Rate: 0.01\n",
      "Epoch [1412/10000], Loss: 111.15145111083984, Learning Rate: 0.01\n",
      "Epoch [1413/10000], Loss: 111.13797760009766, Learning Rate: 0.01\n",
      "Epoch [1414/10000], Loss: 111.12446594238281, Learning Rate: 0.01\n",
      "Epoch [1415/10000], Loss: 111.1111068725586, Learning Rate: 0.01\n",
      "Epoch [1416/10000], Loss: 111.09770965576172, Learning Rate: 0.01\n",
      "Epoch [1417/10000], Loss: 111.08428955078125, Learning Rate: 0.01\n",
      "Epoch [1418/10000], Loss: 111.07086944580078, Learning Rate: 0.01\n",
      "Epoch [1419/10000], Loss: 111.05743408203125, Learning Rate: 0.01\n",
      "Epoch [1420/10000], Loss: 111.04407501220703, Learning Rate: 0.01\n",
      "Epoch [1421/10000], Loss: 111.03073120117188, Learning Rate: 0.01\n",
      "Epoch [1422/10000], Loss: 111.017333984375, Learning Rate: 0.01\n",
      "Epoch [1423/10000], Loss: 111.00386047363281, Learning Rate: 0.01\n",
      "Epoch [1424/10000], Loss: 110.99042510986328, Learning Rate: 0.01\n",
      "Epoch [1425/10000], Loss: 110.97713470458984, Learning Rate: 0.01\n",
      "Epoch [1426/10000], Loss: 110.96379852294922, Learning Rate: 0.01\n",
      "Epoch [1427/10000], Loss: 110.95050048828125, Learning Rate: 0.01\n",
      "Epoch [1428/10000], Loss: 110.93704223632812, Learning Rate: 0.01\n",
      "Epoch [1429/10000], Loss: 110.92376708984375, Learning Rate: 0.01\n",
      "Epoch [1430/10000], Loss: 110.91043853759766, Learning Rate: 0.01\n",
      "Epoch [1431/10000], Loss: 110.89704895019531, Learning Rate: 0.01\n",
      "Epoch [1432/10000], Loss: 110.88377380371094, Learning Rate: 0.01\n",
      "Epoch [1433/10000], Loss: 110.87042999267578, Learning Rate: 0.01\n",
      "Epoch [1434/10000], Loss: 110.85716247558594, Learning Rate: 0.01\n",
      "Epoch [1435/10000], Loss: 110.8438720703125, Learning Rate: 0.01\n",
      "Epoch [1436/10000], Loss: 110.83049011230469, Learning Rate: 0.01\n",
      "Epoch [1437/10000], Loss: 110.81710815429688, Learning Rate: 0.01\n",
      "Epoch [1438/10000], Loss: 110.80388641357422, Learning Rate: 0.01\n",
      "Epoch [1439/10000], Loss: 110.79061889648438, Learning Rate: 0.01\n",
      "Epoch [1440/10000], Loss: 110.77735137939453, Learning Rate: 0.01\n",
      "Epoch [1441/10000], Loss: 110.76410675048828, Learning Rate: 0.01\n",
      "Epoch [1442/10000], Loss: 110.7507553100586, Learning Rate: 0.01\n",
      "Epoch [1443/10000], Loss: 110.73746490478516, Learning Rate: 0.01\n",
      "Epoch [1444/10000], Loss: 110.72430419921875, Learning Rate: 0.01\n",
      "Epoch [1445/10000], Loss: 110.71104431152344, Learning Rate: 0.01\n",
      "Epoch [1446/10000], Loss: 110.69786834716797, Learning Rate: 0.01\n",
      "Epoch [1447/10000], Loss: 110.68450164794922, Learning Rate: 0.01\n",
      "Epoch [1448/10000], Loss: 110.67131805419922, Learning Rate: 0.01\n",
      "Epoch [1449/10000], Loss: 110.65807342529297, Learning Rate: 0.01\n",
      "Epoch [1450/10000], Loss: 110.64493560791016, Learning Rate: 0.01\n",
      "Epoch [1451/10000], Loss: 110.63157653808594, Learning Rate: 0.01\n",
      "Epoch [1452/10000], Loss: 110.61843872070312, Learning Rate: 0.01\n",
      "Epoch [1453/10000], Loss: 110.60525512695312, Learning Rate: 0.01\n",
      "Epoch [1454/10000], Loss: 110.5919418334961, Learning Rate: 0.01\n",
      "Epoch [1455/10000], Loss: 110.57877349853516, Learning Rate: 0.01\n",
      "Epoch [1456/10000], Loss: 110.56568908691406, Learning Rate: 0.01\n",
      "Epoch [1457/10000], Loss: 110.5524673461914, Learning Rate: 0.01\n",
      "Epoch [1458/10000], Loss: 110.53922271728516, Learning Rate: 0.01\n",
      "Epoch [1459/10000], Loss: 110.52613067626953, Learning Rate: 0.01\n",
      "Epoch [1460/10000], Loss: 110.51295471191406, Learning Rate: 0.01\n",
      "Epoch [1461/10000], Loss: 110.49977111816406, Learning Rate: 0.01\n",
      "Epoch [1462/10000], Loss: 110.48661804199219, Learning Rate: 0.01\n",
      "Epoch [1463/10000], Loss: 110.47351837158203, Learning Rate: 0.01\n",
      "Epoch [1464/10000], Loss: 110.46034240722656, Learning Rate: 0.01\n",
      "Epoch [1465/10000], Loss: 110.44722747802734, Learning Rate: 0.01\n",
      "Epoch [1466/10000], Loss: 110.43408203125, Learning Rate: 0.01\n",
      "Epoch [1467/10000], Loss: 110.42095184326172, Learning Rate: 0.01\n",
      "Epoch [1468/10000], Loss: 110.40785217285156, Learning Rate: 0.01\n",
      "Epoch [1469/10000], Loss: 110.39473724365234, Learning Rate: 0.01\n",
      "Epoch [1470/10000], Loss: 110.38165283203125, Learning Rate: 0.01\n",
      "Epoch [1471/10000], Loss: 110.36853790283203, Learning Rate: 0.01\n",
      "Epoch [1472/10000], Loss: 110.35537719726562, Learning Rate: 0.01\n",
      "Epoch [1473/10000], Loss: 110.34236907958984, Learning Rate: 0.01\n",
      "Epoch [1474/10000], Loss: 110.32929229736328, Learning Rate: 0.01\n",
      "Epoch [1475/10000], Loss: 110.31622314453125, Learning Rate: 0.01\n",
      "Epoch [1476/10000], Loss: 110.3031234741211, Learning Rate: 0.01\n",
      "Epoch [1477/10000], Loss: 110.29005432128906, Learning Rate: 0.01\n",
      "Epoch [1478/10000], Loss: 110.27700805664062, Learning Rate: 0.01\n",
      "Epoch [1479/10000], Loss: 110.26388549804688, Learning Rate: 0.01\n",
      "Epoch [1480/10000], Loss: 110.25093078613281, Learning Rate: 0.01\n",
      "Epoch [1481/10000], Loss: 110.23792266845703, Learning Rate: 0.01\n",
      "Epoch [1482/10000], Loss: 110.22486877441406, Learning Rate: 0.01\n",
      "Epoch [1483/10000], Loss: 110.21180725097656, Learning Rate: 0.01\n",
      "Epoch [1484/10000], Loss: 110.19874572753906, Learning Rate: 0.01\n",
      "Epoch [1485/10000], Loss: 110.18576049804688, Learning Rate: 0.01\n",
      "Epoch [1486/10000], Loss: 110.1727066040039, Learning Rate: 0.01\n",
      "Epoch [1487/10000], Loss: 110.1597671508789, Learning Rate: 0.01\n",
      "Epoch [1488/10000], Loss: 110.14669036865234, Learning Rate: 0.01\n",
      "Epoch [1489/10000], Loss: 110.13374328613281, Learning Rate: 0.01\n",
      "Epoch [1490/10000], Loss: 110.12076568603516, Learning Rate: 0.01\n",
      "Epoch [1491/10000], Loss: 110.10781860351562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1492/10000], Loss: 110.0948486328125, Learning Rate: 0.01\n",
      "Epoch [1493/10000], Loss: 110.08187866210938, Learning Rate: 0.01\n",
      "Epoch [1494/10000], Loss: 110.06890869140625, Learning Rate: 0.01\n",
      "Epoch [1495/10000], Loss: 110.05599975585938, Learning Rate: 0.01\n",
      "Epoch [1496/10000], Loss: 110.04304504394531, Learning Rate: 0.01\n",
      "Epoch [1497/10000], Loss: 110.03005981445312, Learning Rate: 0.01\n",
      "Epoch [1498/10000], Loss: 110.01709747314453, Learning Rate: 0.01\n",
      "Epoch [1499/10000], Loss: 110.0042724609375, Learning Rate: 0.01\n",
      "Epoch [1500/10000], Loss: 109.99124145507812, Learning Rate: 0.01\n",
      "Epoch [1501/10000], Loss: 109.97827911376953, Learning Rate: 0.01\n",
      "Epoch [1502/10000], Loss: 109.96542358398438, Learning Rate: 0.01\n",
      "Epoch [1503/10000], Loss: 109.95260620117188, Learning Rate: 0.01\n",
      "Epoch [1504/10000], Loss: 109.93966674804688, Learning Rate: 0.01\n",
      "Epoch [1505/10000], Loss: 109.92668151855469, Learning Rate: 0.01\n",
      "Epoch [1506/10000], Loss: 109.9138412475586, Learning Rate: 0.01\n",
      "Epoch [1507/10000], Loss: 109.90101623535156, Learning Rate: 0.01\n",
      "Epoch [1508/10000], Loss: 109.8882064819336, Learning Rate: 0.01\n",
      "Epoch [1509/10000], Loss: 109.8752670288086, Learning Rate: 0.01\n",
      "Epoch [1510/10000], Loss: 109.86235046386719, Learning Rate: 0.01\n",
      "Epoch [1511/10000], Loss: 109.84950256347656, Learning Rate: 0.01\n",
      "Epoch [1512/10000], Loss: 109.83670806884766, Learning Rate: 0.01\n",
      "Epoch [1513/10000], Loss: 109.8238525390625, Learning Rate: 0.01\n",
      "Epoch [1514/10000], Loss: 109.81099700927734, Learning Rate: 0.01\n",
      "Epoch [1515/10000], Loss: 109.7982177734375, Learning Rate: 0.01\n",
      "Epoch [1516/10000], Loss: 109.78536224365234, Learning Rate: 0.01\n",
      "Epoch [1517/10000], Loss: 109.77254486083984, Learning Rate: 0.01\n",
      "Epoch [1518/10000], Loss: 109.75971221923828, Learning Rate: 0.01\n",
      "Epoch [1519/10000], Loss: 109.74697875976562, Learning Rate: 0.01\n",
      "Epoch [1520/10000], Loss: 109.7341537475586, Learning Rate: 0.01\n",
      "Epoch [1521/10000], Loss: 109.72144317626953, Learning Rate: 0.01\n",
      "Epoch [1522/10000], Loss: 109.7085952758789, Learning Rate: 0.01\n",
      "Epoch [1523/10000], Loss: 109.6957778930664, Learning Rate: 0.01\n",
      "Epoch [1524/10000], Loss: 109.6829605102539, Learning Rate: 0.01\n",
      "Epoch [1525/10000], Loss: 109.67023468017578, Learning Rate: 0.01\n",
      "Epoch [1526/10000], Loss: 109.65745544433594, Learning Rate: 0.01\n",
      "Epoch [1527/10000], Loss: 109.64474487304688, Learning Rate: 0.01\n",
      "Epoch [1528/10000], Loss: 109.63198852539062, Learning Rate: 0.01\n",
      "Epoch [1529/10000], Loss: 109.6192626953125, Learning Rate: 0.01\n",
      "Epoch [1530/10000], Loss: 109.60655212402344, Learning Rate: 0.01\n",
      "Epoch [1531/10000], Loss: 109.5938720703125, Learning Rate: 0.01\n",
      "Epoch [1532/10000], Loss: 109.58110046386719, Learning Rate: 0.01\n",
      "Epoch [1533/10000], Loss: 109.56832122802734, Learning Rate: 0.01\n",
      "Epoch [1534/10000], Loss: 109.55570983886719, Learning Rate: 0.01\n",
      "Epoch [1535/10000], Loss: 109.54300689697266, Learning Rate: 0.01\n",
      "Epoch [1536/10000], Loss: 109.53025817871094, Learning Rate: 0.01\n",
      "Epoch [1537/10000], Loss: 109.51760864257812, Learning Rate: 0.01\n",
      "Epoch [1538/10000], Loss: 109.50492858886719, Learning Rate: 0.01\n",
      "Epoch [1539/10000], Loss: 109.49226379394531, Learning Rate: 0.01\n",
      "Epoch [1540/10000], Loss: 109.47957611083984, Learning Rate: 0.01\n",
      "Epoch [1541/10000], Loss: 109.46697998046875, Learning Rate: 0.01\n",
      "Epoch [1542/10000], Loss: 109.45428466796875, Learning Rate: 0.01\n",
      "Epoch [1543/10000], Loss: 109.44158935546875, Learning Rate: 0.01\n",
      "Epoch [1544/10000], Loss: 109.42900085449219, Learning Rate: 0.01\n",
      "Epoch [1545/10000], Loss: 109.41635131835938, Learning Rate: 0.01\n",
      "Epoch [1546/10000], Loss: 109.40377807617188, Learning Rate: 0.01\n",
      "Epoch [1547/10000], Loss: 109.39115142822266, Learning Rate: 0.01\n",
      "Epoch [1548/10000], Loss: 109.37846374511719, Learning Rate: 0.01\n",
      "Epoch [1549/10000], Loss: 109.36582946777344, Learning Rate: 0.01\n",
      "Epoch [1550/10000], Loss: 109.35330200195312, Learning Rate: 0.01\n",
      "Epoch [1551/10000], Loss: 109.34061431884766, Learning Rate: 0.01\n",
      "Epoch [1552/10000], Loss: 109.32809448242188, Learning Rate: 0.01\n",
      "Epoch [1553/10000], Loss: 109.31555938720703, Learning Rate: 0.01\n",
      "Epoch [1554/10000], Loss: 109.30291748046875, Learning Rate: 0.01\n",
      "Epoch [1555/10000], Loss: 109.29046630859375, Learning Rate: 0.01\n",
      "Epoch [1556/10000], Loss: 109.27779388427734, Learning Rate: 0.01\n",
      "Epoch [1557/10000], Loss: 109.26518249511719, Learning Rate: 0.01\n",
      "Epoch [1558/10000], Loss: 109.25271606445312, Learning Rate: 0.01\n",
      "Epoch [1559/10000], Loss: 109.24017333984375, Learning Rate: 0.01\n",
      "Epoch [1560/10000], Loss: 109.22765350341797, Learning Rate: 0.01\n",
      "Epoch [1561/10000], Loss: 109.21501159667969, Learning Rate: 0.01\n",
      "Epoch [1562/10000], Loss: 109.20256805419922, Learning Rate: 0.01\n",
      "Epoch [1563/10000], Loss: 109.19007110595703, Learning Rate: 0.01\n",
      "Epoch [1564/10000], Loss: 109.17752075195312, Learning Rate: 0.01\n",
      "Epoch [1565/10000], Loss: 109.16508483886719, Learning Rate: 0.01\n",
      "Epoch [1566/10000], Loss: 109.15264129638672, Learning Rate: 0.01\n",
      "Epoch [1567/10000], Loss: 109.14009857177734, Learning Rate: 0.01\n",
      "Epoch [1568/10000], Loss: 109.12763214111328, Learning Rate: 0.01\n",
      "Epoch [1569/10000], Loss: 109.11515045166016, Learning Rate: 0.01\n",
      "Epoch [1570/10000], Loss: 109.10270690917969, Learning Rate: 0.01\n",
      "Epoch [1571/10000], Loss: 109.0902099609375, Learning Rate: 0.01\n",
      "Epoch [1572/10000], Loss: 109.07772827148438, Learning Rate: 0.01\n",
      "Epoch [1573/10000], Loss: 109.06525421142578, Learning Rate: 0.01\n",
      "Epoch [1574/10000], Loss: 109.05293273925781, Learning Rate: 0.01\n",
      "Epoch [1575/10000], Loss: 109.0403823852539, Learning Rate: 0.01\n",
      "Epoch [1576/10000], Loss: 109.02804565429688, Learning Rate: 0.01\n",
      "Epoch [1577/10000], Loss: 109.01557922363281, Learning Rate: 0.01\n",
      "Epoch [1578/10000], Loss: 109.00322723388672, Learning Rate: 0.01\n",
      "Epoch [1579/10000], Loss: 108.99079895019531, Learning Rate: 0.01\n",
      "Epoch [1580/10000], Loss: 108.97840118408203, Learning Rate: 0.01\n",
      "Epoch [1581/10000], Loss: 108.96602630615234, Learning Rate: 0.01\n",
      "Epoch [1582/10000], Loss: 108.95360565185547, Learning Rate: 0.01\n",
      "Epoch [1583/10000], Loss: 108.94126892089844, Learning Rate: 0.01\n",
      "Epoch [1584/10000], Loss: 108.92887878417969, Learning Rate: 0.01\n",
      "Epoch [1585/10000], Loss: 108.91648864746094, Learning Rate: 0.01\n",
      "Epoch [1586/10000], Loss: 108.90415954589844, Learning Rate: 0.01\n",
      "Epoch [1587/10000], Loss: 108.8918228149414, Learning Rate: 0.01\n",
      "Epoch [1588/10000], Loss: 108.8794937133789, Learning Rate: 0.01\n",
      "Epoch [1589/10000], Loss: 108.86698913574219, Learning Rate: 0.01\n",
      "Epoch [1590/10000], Loss: 108.85476684570312, Learning Rate: 0.01\n",
      "Epoch [1591/10000], Loss: 108.8425064086914, Learning Rate: 0.01\n",
      "Epoch [1592/10000], Loss: 108.83025360107422, Learning Rate: 0.01\n",
      "Epoch [1593/10000], Loss: 108.8178482055664, Learning Rate: 0.01\n",
      "Epoch [1594/10000], Loss: 108.80551147460938, Learning Rate: 0.01\n",
      "Epoch [1595/10000], Loss: 108.7931900024414, Learning Rate: 0.01\n",
      "Epoch [1596/10000], Loss: 108.78099822998047, Learning Rate: 0.01\n",
      "Epoch [1597/10000], Loss: 108.7686767578125, Learning Rate: 0.01\n",
      "Epoch [1598/10000], Loss: 108.7564697265625, Learning Rate: 0.01\n",
      "Epoch [1599/10000], Loss: 108.74406433105469, Learning Rate: 0.01\n",
      "Epoch [1600/10000], Loss: 108.73194885253906, Learning Rate: 0.01\n",
      "Epoch [1601/10000], Loss: 108.71971893310547, Learning Rate: 0.01\n",
      "Epoch [1602/10000], Loss: 108.7072982788086, Learning Rate: 0.01\n",
      "Epoch [1603/10000], Loss: 108.6951904296875, Learning Rate: 0.01\n",
      "Epoch [1604/10000], Loss: 108.68292999267578, Learning Rate: 0.01\n",
      "Epoch [1605/10000], Loss: 108.67062377929688, Learning Rate: 0.01\n",
      "Epoch [1606/10000], Loss: 108.6584701538086, Learning Rate: 0.01\n",
      "Epoch [1607/10000], Loss: 108.64616394042969, Learning Rate: 0.01\n",
      "Epoch [1608/10000], Loss: 108.63400268554688, Learning Rate: 0.01\n",
      "Epoch [1609/10000], Loss: 108.62181854248047, Learning Rate: 0.01\n",
      "Epoch [1610/10000], Loss: 108.60960388183594, Learning Rate: 0.01\n",
      "Epoch [1611/10000], Loss: 108.59739685058594, Learning Rate: 0.01\n",
      "Epoch [1612/10000], Loss: 108.58528900146484, Learning Rate: 0.01\n",
      "Epoch [1613/10000], Loss: 108.57310485839844, Learning Rate: 0.01\n",
      "Epoch [1614/10000], Loss: 108.56089782714844, Learning Rate: 0.01\n",
      "Epoch [1615/10000], Loss: 108.5486831665039, Learning Rate: 0.01\n",
      "Epoch [1616/10000], Loss: 108.53658294677734, Learning Rate: 0.01\n",
      "Epoch [1617/10000], Loss: 108.52442932128906, Learning Rate: 0.01\n",
      "Epoch [1618/10000], Loss: 108.51219177246094, Learning Rate: 0.01\n",
      "Epoch [1619/10000], Loss: 108.50016021728516, Learning Rate: 0.01\n",
      "Epoch [1620/10000], Loss: 108.48802947998047, Learning Rate: 0.01\n",
      "Epoch [1621/10000], Loss: 108.47586822509766, Learning Rate: 0.01\n",
      "Epoch [1622/10000], Loss: 108.46377563476562, Learning Rate: 0.01\n",
      "Epoch [1623/10000], Loss: 108.45165252685547, Learning Rate: 0.01\n",
      "Epoch [1624/10000], Loss: 108.43959045410156, Learning Rate: 0.01\n",
      "Epoch [1625/10000], Loss: 108.42744445800781, Learning Rate: 0.01\n",
      "Epoch [1626/10000], Loss: 108.41535949707031, Learning Rate: 0.01\n",
      "Epoch [1627/10000], Loss: 108.4033203125, Learning Rate: 0.01\n",
      "Epoch [1628/10000], Loss: 108.39128875732422, Learning Rate: 0.01\n",
      "Epoch [1629/10000], Loss: 108.37909698486328, Learning Rate: 0.01\n",
      "Epoch [1630/10000], Loss: 108.36710357666016, Learning Rate: 0.01\n",
      "Epoch [1631/10000], Loss: 108.35499572753906, Learning Rate: 0.01\n",
      "Epoch [1632/10000], Loss: 108.34288787841797, Learning Rate: 0.01\n",
      "Epoch [1633/10000], Loss: 108.33094024658203, Learning Rate: 0.01\n",
      "Epoch [1634/10000], Loss: 108.31890106201172, Learning Rate: 0.01\n",
      "Epoch [1635/10000], Loss: 108.3067855834961, Learning Rate: 0.01\n",
      "Epoch [1636/10000], Loss: 108.29490661621094, Learning Rate: 0.01\n",
      "Epoch [1637/10000], Loss: 108.28289794921875, Learning Rate: 0.01\n",
      "Epoch [1638/10000], Loss: 108.27085876464844, Learning Rate: 0.01\n",
      "Epoch [1639/10000], Loss: 108.25884246826172, Learning Rate: 0.01\n",
      "Epoch [1640/10000], Loss: 108.2468490600586, Learning Rate: 0.01\n",
      "Epoch [1641/10000], Loss: 108.23484802246094, Learning Rate: 0.01\n",
      "Epoch [1642/10000], Loss: 108.22284698486328, Learning Rate: 0.01\n",
      "Epoch [1643/10000], Loss: 108.2107925415039, Learning Rate: 0.01\n",
      "Epoch [1644/10000], Loss: 108.19888305664062, Learning Rate: 0.01\n",
      "Epoch [1645/10000], Loss: 108.18694305419922, Learning Rate: 0.01\n",
      "Epoch [1646/10000], Loss: 108.17499542236328, Learning Rate: 0.01\n",
      "Epoch [1647/10000], Loss: 108.1629867553711, Learning Rate: 0.01\n",
      "Epoch [1648/10000], Loss: 108.15104675292969, Learning Rate: 0.01\n",
      "Epoch [1649/10000], Loss: 108.13919830322266, Learning Rate: 0.01\n",
      "Epoch [1650/10000], Loss: 108.12715148925781, Learning Rate: 0.01\n",
      "Epoch [1651/10000], Loss: 108.11520385742188, Learning Rate: 0.01\n",
      "Epoch [1652/10000], Loss: 108.10330963134766, Learning Rate: 0.01\n",
      "Epoch [1653/10000], Loss: 108.0914077758789, Learning Rate: 0.01\n",
      "Epoch [1654/10000], Loss: 108.07951354980469, Learning Rate: 0.01\n",
      "Epoch [1655/10000], Loss: 108.067626953125, Learning Rate: 0.01\n",
      "Epoch [1656/10000], Loss: 108.05574798583984, Learning Rate: 0.01\n",
      "Epoch [1657/10000], Loss: 108.0438461303711, Learning Rate: 0.01\n",
      "Epoch [1658/10000], Loss: 108.03194427490234, Learning Rate: 0.01\n",
      "Epoch [1659/10000], Loss: 108.0201187133789, Learning Rate: 0.01\n",
      "Epoch [1660/10000], Loss: 108.00816345214844, Learning Rate: 0.01\n",
      "Epoch [1661/10000], Loss: 107.9963150024414, Learning Rate: 0.01\n",
      "Epoch [1662/10000], Loss: 107.98444366455078, Learning Rate: 0.01\n",
      "Epoch [1663/10000], Loss: 107.97252655029297, Learning Rate: 0.01\n",
      "Epoch [1664/10000], Loss: 107.96077728271484, Learning Rate: 0.01\n",
      "Epoch [1665/10000], Loss: 107.948974609375, Learning Rate: 0.01\n",
      "Epoch [1666/10000], Loss: 107.93704223632812, Learning Rate: 0.01\n",
      "Epoch [1667/10000], Loss: 107.92518615722656, Learning Rate: 0.01\n",
      "Epoch [1668/10000], Loss: 107.91340637207031, Learning Rate: 0.01\n",
      "Epoch [1669/10000], Loss: 107.90169525146484, Learning Rate: 0.01\n",
      "Epoch [1670/10000], Loss: 107.88982391357422, Learning Rate: 0.01\n",
      "Epoch [1671/10000], Loss: 107.87799835205078, Learning Rate: 0.01\n",
      "Epoch [1672/10000], Loss: 107.86617279052734, Learning Rate: 0.01\n",
      "Epoch [1673/10000], Loss: 107.8543930053711, Learning Rate: 0.01\n",
      "Epoch [1674/10000], Loss: 107.84264373779297, Learning Rate: 0.01\n",
      "Epoch [1675/10000], Loss: 107.83087158203125, Learning Rate: 0.01\n",
      "Epoch [1676/10000], Loss: 107.819091796875, Learning Rate: 0.01\n",
      "Epoch [1677/10000], Loss: 107.8072509765625, Learning Rate: 0.01\n",
      "Epoch [1678/10000], Loss: 107.7955551147461, Learning Rate: 0.01\n",
      "Epoch [1679/10000], Loss: 107.78376770019531, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1680/10000], Loss: 107.77201843261719, Learning Rate: 0.01\n",
      "Epoch [1681/10000], Loss: 107.76024627685547, Learning Rate: 0.01\n",
      "Epoch [1682/10000], Loss: 107.74845123291016, Learning Rate: 0.01\n",
      "Epoch [1683/10000], Loss: 107.73682403564453, Learning Rate: 0.01\n",
      "Epoch [1684/10000], Loss: 107.72505950927734, Learning Rate: 0.01\n",
      "Epoch [1685/10000], Loss: 107.71339416503906, Learning Rate: 0.01\n",
      "Epoch [1686/10000], Loss: 107.70170593261719, Learning Rate: 0.01\n",
      "Epoch [1687/10000], Loss: 107.68997192382812, Learning Rate: 0.01\n",
      "Epoch [1688/10000], Loss: 107.67826843261719, Learning Rate: 0.01\n",
      "Epoch [1689/10000], Loss: 107.66654968261719, Learning Rate: 0.01\n",
      "Epoch [1690/10000], Loss: 107.65486145019531, Learning Rate: 0.01\n",
      "Epoch [1691/10000], Loss: 107.64312744140625, Learning Rate: 0.01\n",
      "Epoch [1692/10000], Loss: 107.63143157958984, Learning Rate: 0.01\n",
      "Epoch [1693/10000], Loss: 107.61979675292969, Learning Rate: 0.01\n",
      "Epoch [1694/10000], Loss: 107.6081314086914, Learning Rate: 0.01\n",
      "Epoch [1695/10000], Loss: 107.5964584350586, Learning Rate: 0.01\n",
      "Epoch [1696/10000], Loss: 107.58480072021484, Learning Rate: 0.01\n",
      "Epoch [1697/10000], Loss: 107.57313537597656, Learning Rate: 0.01\n",
      "Epoch [1698/10000], Loss: 107.56155395507812, Learning Rate: 0.01\n",
      "Epoch [1699/10000], Loss: 107.5498275756836, Learning Rate: 0.01\n",
      "Epoch [1700/10000], Loss: 107.5381851196289, Learning Rate: 0.01\n",
      "Epoch [1701/10000], Loss: 107.52659606933594, Learning Rate: 0.01\n",
      "Epoch [1702/10000], Loss: 107.51497650146484, Learning Rate: 0.01\n",
      "Epoch [1703/10000], Loss: 107.50331115722656, Learning Rate: 0.01\n",
      "Epoch [1704/10000], Loss: 107.49170684814453, Learning Rate: 0.01\n",
      "Epoch [1705/10000], Loss: 107.4800796508789, Learning Rate: 0.01\n",
      "Epoch [1706/10000], Loss: 107.468505859375, Learning Rate: 0.01\n",
      "Epoch [1707/10000], Loss: 107.45687866210938, Learning Rate: 0.01\n",
      "Epoch [1708/10000], Loss: 107.44532012939453, Learning Rate: 0.01\n",
      "Epoch [1709/10000], Loss: 107.43374633789062, Learning Rate: 0.01\n",
      "Epoch [1710/10000], Loss: 107.42218017578125, Learning Rate: 0.01\n",
      "Epoch [1711/10000], Loss: 107.41056060791016, Learning Rate: 0.01\n",
      "Epoch [1712/10000], Loss: 107.39899444580078, Learning Rate: 0.01\n",
      "Epoch [1713/10000], Loss: 107.3874282836914, Learning Rate: 0.01\n",
      "Epoch [1714/10000], Loss: 107.37580871582031, Learning Rate: 0.01\n",
      "Epoch [1715/10000], Loss: 107.36433410644531, Learning Rate: 0.01\n",
      "Epoch [1716/10000], Loss: 107.35276794433594, Learning Rate: 0.01\n",
      "Epoch [1717/10000], Loss: 107.34127044677734, Learning Rate: 0.01\n",
      "Epoch [1718/10000], Loss: 107.3297348022461, Learning Rate: 0.01\n",
      "Epoch [1719/10000], Loss: 107.31819152832031, Learning Rate: 0.01\n",
      "Epoch [1720/10000], Loss: 107.30665588378906, Learning Rate: 0.01\n",
      "Epoch [1721/10000], Loss: 107.2950668334961, Learning Rate: 0.01\n",
      "Epoch [1722/10000], Loss: 107.2835464477539, Learning Rate: 0.01\n",
      "Epoch [1723/10000], Loss: 107.27200317382812, Learning Rate: 0.01\n",
      "Epoch [1724/10000], Loss: 107.26056671142578, Learning Rate: 0.01\n",
      "Epoch [1725/10000], Loss: 107.24905395507812, Learning Rate: 0.01\n",
      "Epoch [1726/10000], Loss: 107.23755645751953, Learning Rate: 0.01\n",
      "Epoch [1727/10000], Loss: 107.22605895996094, Learning Rate: 0.01\n",
      "Epoch [1728/10000], Loss: 107.214599609375, Learning Rate: 0.01\n",
      "Epoch [1729/10000], Loss: 107.20307922363281, Learning Rate: 0.01\n",
      "Epoch [1730/10000], Loss: 107.1916732788086, Learning Rate: 0.01\n",
      "Epoch [1731/10000], Loss: 107.18012237548828, Learning Rate: 0.01\n",
      "Epoch [1732/10000], Loss: 107.16862487792969, Learning Rate: 0.01\n",
      "Epoch [1733/10000], Loss: 107.15715026855469, Learning Rate: 0.01\n",
      "Epoch [1734/10000], Loss: 107.14574432373047, Learning Rate: 0.01\n",
      "Epoch [1735/10000], Loss: 107.13433074951172, Learning Rate: 0.01\n",
      "Epoch [1736/10000], Loss: 107.12278747558594, Learning Rate: 0.01\n",
      "Epoch [1737/10000], Loss: 107.11137390136719, Learning Rate: 0.01\n",
      "Epoch [1738/10000], Loss: 107.09994506835938, Learning Rate: 0.01\n",
      "Epoch [1739/10000], Loss: 107.08855438232422, Learning Rate: 0.01\n",
      "Epoch [1740/10000], Loss: 107.07709503173828, Learning Rate: 0.01\n",
      "Epoch [1741/10000], Loss: 107.06571197509766, Learning Rate: 0.01\n",
      "Epoch [1742/10000], Loss: 107.05422973632812, Learning Rate: 0.01\n",
      "Epoch [1743/10000], Loss: 107.04287719726562, Learning Rate: 0.01\n",
      "Epoch [1744/10000], Loss: 107.03138732910156, Learning Rate: 0.01\n",
      "Epoch [1745/10000], Loss: 107.01997375488281, Learning Rate: 0.01\n",
      "Epoch [1746/10000], Loss: 107.0086669921875, Learning Rate: 0.01\n",
      "Epoch [1747/10000], Loss: 106.9971923828125, Learning Rate: 0.01\n",
      "Epoch [1748/10000], Loss: 106.98588562011719, Learning Rate: 0.01\n",
      "Epoch [1749/10000], Loss: 106.97441101074219, Learning Rate: 0.01\n",
      "Epoch [1750/10000], Loss: 106.96316528320312, Learning Rate: 0.01\n",
      "Epoch [1751/10000], Loss: 106.95171356201172, Learning Rate: 0.01\n",
      "Epoch [1752/10000], Loss: 106.94025421142578, Learning Rate: 0.01\n",
      "Epoch [1753/10000], Loss: 106.928955078125, Learning Rate: 0.01\n",
      "Epoch [1754/10000], Loss: 106.91764068603516, Learning Rate: 0.01\n",
      "Epoch [1755/10000], Loss: 106.90625, Learning Rate: 0.01\n",
      "Epoch [1756/10000], Loss: 106.89491271972656, Learning Rate: 0.01\n",
      "Epoch [1757/10000], Loss: 106.8835678100586, Learning Rate: 0.01\n",
      "Epoch [1758/10000], Loss: 106.87220001220703, Learning Rate: 0.01\n",
      "Epoch [1759/10000], Loss: 106.8608169555664, Learning Rate: 0.01\n",
      "Epoch [1760/10000], Loss: 106.8494873046875, Learning Rate: 0.01\n",
      "Epoch [1761/10000], Loss: 106.83821868896484, Learning Rate: 0.01\n",
      "Epoch [1762/10000], Loss: 106.82697296142578, Learning Rate: 0.01\n",
      "Epoch [1763/10000], Loss: 106.81558227539062, Learning Rate: 0.01\n",
      "Epoch [1764/10000], Loss: 106.80427551269531, Learning Rate: 0.01\n",
      "Epoch [1765/10000], Loss: 106.79296875, Learning Rate: 0.01\n",
      "Epoch [1766/10000], Loss: 106.78160095214844, Learning Rate: 0.01\n",
      "Epoch [1767/10000], Loss: 106.77029418945312, Learning Rate: 0.01\n",
      "Epoch [1768/10000], Loss: 106.75894927978516, Learning Rate: 0.01\n",
      "Epoch [1769/10000], Loss: 106.74774932861328, Learning Rate: 0.01\n",
      "Epoch [1770/10000], Loss: 106.73645782470703, Learning Rate: 0.01\n",
      "Epoch [1771/10000], Loss: 106.72514343261719, Learning Rate: 0.01\n",
      "Epoch [1772/10000], Loss: 106.71387481689453, Learning Rate: 0.01\n",
      "Epoch [1773/10000], Loss: 106.70258331298828, Learning Rate: 0.01\n",
      "Epoch [1774/10000], Loss: 106.69136810302734, Learning Rate: 0.01\n",
      "Epoch [1775/10000], Loss: 106.6800537109375, Learning Rate: 0.01\n",
      "Epoch [1776/10000], Loss: 106.66873168945312, Learning Rate: 0.01\n",
      "Epoch [1777/10000], Loss: 106.65745544433594, Learning Rate: 0.01\n",
      "Epoch [1778/10000], Loss: 106.64627838134766, Learning Rate: 0.01\n",
      "Epoch [1779/10000], Loss: 106.6350326538086, Learning Rate: 0.01\n",
      "Epoch [1780/10000], Loss: 106.62374114990234, Learning Rate: 0.01\n",
      "Epoch [1781/10000], Loss: 106.6125259399414, Learning Rate: 0.01\n",
      "Epoch [1782/10000], Loss: 106.60130310058594, Learning Rate: 0.01\n",
      "Epoch [1783/10000], Loss: 106.590087890625, Learning Rate: 0.01\n",
      "Epoch [1784/10000], Loss: 106.57884979248047, Learning Rate: 0.01\n",
      "Epoch [1785/10000], Loss: 106.56758117675781, Learning Rate: 0.01\n",
      "Epoch [1786/10000], Loss: 106.5563735961914, Learning Rate: 0.01\n",
      "Epoch [1787/10000], Loss: 106.54515075683594, Learning Rate: 0.01\n",
      "Epoch [1788/10000], Loss: 106.533935546875, Learning Rate: 0.01\n",
      "Epoch [1789/10000], Loss: 106.52273559570312, Learning Rate: 0.01\n",
      "Epoch [1790/10000], Loss: 106.51151275634766, Learning Rate: 0.01\n",
      "Epoch [1791/10000], Loss: 106.50032043457031, Learning Rate: 0.01\n",
      "Epoch [1792/10000], Loss: 106.48912811279297, Learning Rate: 0.01\n",
      "Epoch [1793/10000], Loss: 106.47798156738281, Learning Rate: 0.01\n",
      "Epoch [1794/10000], Loss: 106.46673583984375, Learning Rate: 0.01\n",
      "Epoch [1795/10000], Loss: 106.45557403564453, Learning Rate: 0.01\n",
      "Epoch [1796/10000], Loss: 106.44437408447266, Learning Rate: 0.01\n",
      "Epoch [1797/10000], Loss: 106.43328857421875, Learning Rate: 0.01\n",
      "Epoch [1798/10000], Loss: 106.42205810546875, Learning Rate: 0.01\n",
      "Epoch [1799/10000], Loss: 106.41092681884766, Learning Rate: 0.01\n",
      "Epoch [1800/10000], Loss: 106.39974212646484, Learning Rate: 0.01\n",
      "Epoch [1801/10000], Loss: 106.3885269165039, Learning Rate: 0.01\n",
      "Epoch [1802/10000], Loss: 106.3774185180664, Learning Rate: 0.01\n",
      "Epoch [1803/10000], Loss: 106.3662338256836, Learning Rate: 0.01\n",
      "Epoch [1804/10000], Loss: 106.35513305664062, Learning Rate: 0.01\n",
      "Epoch [1805/10000], Loss: 106.3439712524414, Learning Rate: 0.01\n",
      "Epoch [1806/10000], Loss: 106.33279418945312, Learning Rate: 0.01\n",
      "Epoch [1807/10000], Loss: 106.32170867919922, Learning Rate: 0.01\n",
      "Epoch [1808/10000], Loss: 106.31061553955078, Learning Rate: 0.01\n",
      "Epoch [1809/10000], Loss: 106.2994384765625, Learning Rate: 0.01\n",
      "Epoch [1810/10000], Loss: 106.2883071899414, Learning Rate: 0.01\n",
      "Epoch [1811/10000], Loss: 106.27716064453125, Learning Rate: 0.01\n",
      "Epoch [1812/10000], Loss: 106.26604461669922, Learning Rate: 0.01\n",
      "Epoch [1813/10000], Loss: 106.25499725341797, Learning Rate: 0.01\n",
      "Epoch [1814/10000], Loss: 106.24384307861328, Learning Rate: 0.01\n",
      "Epoch [1815/10000], Loss: 106.23279571533203, Learning Rate: 0.01\n",
      "Epoch [1816/10000], Loss: 106.22160339355469, Learning Rate: 0.01\n",
      "Epoch [1817/10000], Loss: 106.21057891845703, Learning Rate: 0.01\n",
      "Epoch [1818/10000], Loss: 106.199462890625, Learning Rate: 0.01\n",
      "Epoch [1819/10000], Loss: 106.18839263916016, Learning Rate: 0.01\n",
      "Epoch [1820/10000], Loss: 106.17720031738281, Learning Rate: 0.01\n",
      "Epoch [1821/10000], Loss: 106.1661605834961, Learning Rate: 0.01\n",
      "Epoch [1822/10000], Loss: 106.15513610839844, Learning Rate: 0.01\n",
      "Epoch [1823/10000], Loss: 106.14400482177734, Learning Rate: 0.01\n",
      "Epoch [1824/10000], Loss: 106.13294219970703, Learning Rate: 0.01\n",
      "Epoch [1825/10000], Loss: 106.12187957763672, Learning Rate: 0.01\n",
      "Epoch [1826/10000], Loss: 106.11091613769531, Learning Rate: 0.01\n",
      "Epoch [1827/10000], Loss: 106.09979248046875, Learning Rate: 0.01\n",
      "Epoch [1828/10000], Loss: 106.08877563476562, Learning Rate: 0.01\n",
      "Epoch [1829/10000], Loss: 106.07764434814453, Learning Rate: 0.01\n",
      "Epoch [1830/10000], Loss: 106.06661224365234, Learning Rate: 0.01\n",
      "Epoch [1831/10000], Loss: 106.05554962158203, Learning Rate: 0.01\n",
      "Epoch [1832/10000], Loss: 106.04448699951172, Learning Rate: 0.01\n",
      "Epoch [1833/10000], Loss: 106.03352355957031, Learning Rate: 0.01\n",
      "Epoch [1834/10000], Loss: 106.02259826660156, Learning Rate: 0.01\n",
      "Epoch [1835/10000], Loss: 106.01141357421875, Learning Rate: 0.01\n",
      "Epoch [1836/10000], Loss: 106.00040435791016, Learning Rate: 0.01\n",
      "Epoch [1837/10000], Loss: 105.98939514160156, Learning Rate: 0.01\n",
      "Epoch [1838/10000], Loss: 105.9782943725586, Learning Rate: 0.01\n",
      "Epoch [1839/10000], Loss: 105.96730041503906, Learning Rate: 0.01\n",
      "Epoch [1840/10000], Loss: 105.9562759399414, Learning Rate: 0.01\n",
      "Epoch [1841/10000], Loss: 105.94525146484375, Learning Rate: 0.01\n",
      "Epoch [1842/10000], Loss: 105.9343490600586, Learning Rate: 0.01\n",
      "Epoch [1843/10000], Loss: 105.92329406738281, Learning Rate: 0.01\n",
      "Epoch [1844/10000], Loss: 105.91236114501953, Learning Rate: 0.01\n",
      "Epoch [1845/10000], Loss: 105.90133666992188, Learning Rate: 0.01\n",
      "Epoch [1846/10000], Loss: 105.89032745361328, Learning Rate: 0.01\n",
      "Epoch [1847/10000], Loss: 105.8792953491211, Learning Rate: 0.01\n",
      "Epoch [1848/10000], Loss: 105.8684310913086, Learning Rate: 0.01\n",
      "Epoch [1849/10000], Loss: 105.85736846923828, Learning Rate: 0.01\n",
      "Epoch [1850/10000], Loss: 105.84632110595703, Learning Rate: 0.01\n",
      "Epoch [1851/10000], Loss: 105.83541870117188, Learning Rate: 0.01\n",
      "Epoch [1852/10000], Loss: 105.824462890625, Learning Rate: 0.01\n",
      "Epoch [1853/10000], Loss: 105.81350708007812, Learning Rate: 0.01\n",
      "Epoch [1854/10000], Loss: 105.80250549316406, Learning Rate: 0.01\n",
      "Epoch [1855/10000], Loss: 105.79158782958984, Learning Rate: 0.01\n",
      "Epoch [1856/10000], Loss: 105.78050994873047, Learning Rate: 0.01\n",
      "Epoch [1857/10000], Loss: 105.76959991455078, Learning Rate: 0.01\n",
      "Epoch [1858/10000], Loss: 105.75869750976562, Learning Rate: 0.01\n",
      "Epoch [1859/10000], Loss: 105.74771881103516, Learning Rate: 0.01\n",
      "Epoch [1860/10000], Loss: 105.73678588867188, Learning Rate: 0.01\n",
      "Epoch [1861/10000], Loss: 105.72583770751953, Learning Rate: 0.01\n",
      "Epoch [1862/10000], Loss: 105.71497344970703, Learning Rate: 0.01\n",
      "Epoch [1863/10000], Loss: 105.70394897460938, Learning Rate: 0.01\n",
      "Epoch [1864/10000], Loss: 105.69305419921875, Learning Rate: 0.01\n",
      "Epoch [1865/10000], Loss: 105.68209075927734, Learning Rate: 0.01\n",
      "Epoch [1866/10000], Loss: 105.67112731933594, Learning Rate: 0.01\n",
      "Epoch [1867/10000], Loss: 105.66032409667969, Learning Rate: 0.01\n",
      "Epoch [1868/10000], Loss: 105.64942169189453, Learning Rate: 0.01\n",
      "Epoch [1869/10000], Loss: 105.63843536376953, Learning Rate: 0.01\n",
      "Epoch [1870/10000], Loss: 105.6275634765625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1871/10000], Loss: 105.61666107177734, Learning Rate: 0.01\n",
      "Epoch [1872/10000], Loss: 105.60572814941406, Learning Rate: 0.01\n",
      "Epoch [1873/10000], Loss: 105.59481811523438, Learning Rate: 0.01\n",
      "Epoch [1874/10000], Loss: 105.5839614868164, Learning Rate: 0.01\n",
      "Epoch [1875/10000], Loss: 105.57296752929688, Learning Rate: 0.01\n",
      "Epoch [1876/10000], Loss: 105.56217956542969, Learning Rate: 0.01\n",
      "Epoch [1877/10000], Loss: 105.5512466430664, Learning Rate: 0.01\n",
      "Epoch [1878/10000], Loss: 105.54035949707031, Learning Rate: 0.01\n",
      "Epoch [1879/10000], Loss: 105.5294418334961, Learning Rate: 0.01\n",
      "Epoch [1880/10000], Loss: 105.51854705810547, Learning Rate: 0.01\n",
      "Epoch [1881/10000], Loss: 105.50768280029297, Learning Rate: 0.01\n",
      "Epoch [1882/10000], Loss: 105.4968490600586, Learning Rate: 0.01\n",
      "Epoch [1883/10000], Loss: 105.4859619140625, Learning Rate: 0.01\n",
      "Epoch [1884/10000], Loss: 105.47509765625, Learning Rate: 0.01\n",
      "Epoch [1885/10000], Loss: 105.46431732177734, Learning Rate: 0.01\n",
      "Epoch [1886/10000], Loss: 105.45346069335938, Learning Rate: 0.01\n",
      "Epoch [1887/10000], Loss: 105.44253540039062, Learning Rate: 0.01\n",
      "Epoch [1888/10000], Loss: 105.43170928955078, Learning Rate: 0.01\n",
      "Epoch [1889/10000], Loss: 105.42086029052734, Learning Rate: 0.01\n",
      "Epoch [1890/10000], Loss: 105.41000366210938, Learning Rate: 0.01\n",
      "Epoch [1891/10000], Loss: 105.39913940429688, Learning Rate: 0.01\n",
      "Epoch [1892/10000], Loss: 105.38825225830078, Learning Rate: 0.01\n",
      "Epoch [1893/10000], Loss: 105.37751007080078, Learning Rate: 0.01\n",
      "Epoch [1894/10000], Loss: 105.3666000366211, Learning Rate: 0.01\n",
      "Epoch [1895/10000], Loss: 105.3558120727539, Learning Rate: 0.01\n",
      "Epoch [1896/10000], Loss: 105.34496307373047, Learning Rate: 0.01\n",
      "Epoch [1897/10000], Loss: 105.33418273925781, Learning Rate: 0.01\n",
      "Epoch [1898/10000], Loss: 105.32325744628906, Learning Rate: 0.01\n",
      "Epoch [1899/10000], Loss: 105.31255340576172, Learning Rate: 0.01\n",
      "Epoch [1900/10000], Loss: 105.30162811279297, Learning Rate: 0.01\n",
      "Epoch [1901/10000], Loss: 105.29084014892578, Learning Rate: 0.01\n",
      "Epoch [1902/10000], Loss: 105.28014373779297, Learning Rate: 0.01\n",
      "Epoch [1903/10000], Loss: 105.269287109375, Learning Rate: 0.01\n",
      "Epoch [1904/10000], Loss: 105.25846862792969, Learning Rate: 0.01\n",
      "Epoch [1905/10000], Loss: 105.24761962890625, Learning Rate: 0.01\n",
      "Epoch [1906/10000], Loss: 105.23688507080078, Learning Rate: 0.01\n",
      "Epoch [1907/10000], Loss: 105.22608947753906, Learning Rate: 0.01\n",
      "Epoch [1908/10000], Loss: 105.21529388427734, Learning Rate: 0.01\n",
      "Epoch [1909/10000], Loss: 105.20452117919922, Learning Rate: 0.01\n",
      "Epoch [1910/10000], Loss: 105.19375610351562, Learning Rate: 0.01\n",
      "Epoch [1911/10000], Loss: 105.18294525146484, Learning Rate: 0.01\n",
      "Epoch [1912/10000], Loss: 105.1721420288086, Learning Rate: 0.01\n",
      "Epoch [1913/10000], Loss: 105.16142272949219, Learning Rate: 0.01\n",
      "Epoch [1914/10000], Loss: 105.15065002441406, Learning Rate: 0.01\n",
      "Epoch [1915/10000], Loss: 105.13987731933594, Learning Rate: 0.01\n",
      "Epoch [1916/10000], Loss: 105.12907409667969, Learning Rate: 0.01\n",
      "Epoch [1917/10000], Loss: 105.11832427978516, Learning Rate: 0.01\n",
      "Epoch [1918/10000], Loss: 105.10753631591797, Learning Rate: 0.01\n",
      "Epoch [1919/10000], Loss: 105.09675598144531, Learning Rate: 0.01\n",
      "Epoch [1920/10000], Loss: 105.0859603881836, Learning Rate: 0.01\n",
      "Epoch [1921/10000], Loss: 105.07530212402344, Learning Rate: 0.01\n",
      "Epoch [1922/10000], Loss: 105.06453704833984, Learning Rate: 0.01\n",
      "Epoch [1923/10000], Loss: 105.05369567871094, Learning Rate: 0.01\n",
      "Epoch [1924/10000], Loss: 105.04302215576172, Learning Rate: 0.01\n",
      "Epoch [1925/10000], Loss: 105.03228759765625, Learning Rate: 0.01\n",
      "Epoch [1926/10000], Loss: 105.02154541015625, Learning Rate: 0.01\n",
      "Epoch [1927/10000], Loss: 105.01078796386719, Learning Rate: 0.01\n",
      "Epoch [1928/10000], Loss: 105.00006103515625, Learning Rate: 0.01\n",
      "Epoch [1929/10000], Loss: 104.98938751220703, Learning Rate: 0.01\n",
      "Epoch [1930/10000], Loss: 104.97860717773438, Learning Rate: 0.01\n",
      "Epoch [1931/10000], Loss: 104.9678955078125, Learning Rate: 0.01\n",
      "Epoch [1932/10000], Loss: 104.95718383789062, Learning Rate: 0.01\n",
      "Epoch [1933/10000], Loss: 104.94649505615234, Learning Rate: 0.01\n",
      "Epoch [1934/10000], Loss: 104.93577575683594, Learning Rate: 0.01\n",
      "Epoch [1935/10000], Loss: 104.925048828125, Learning Rate: 0.01\n",
      "Epoch [1936/10000], Loss: 104.9142837524414, Learning Rate: 0.01\n",
      "Epoch [1937/10000], Loss: 104.90361785888672, Learning Rate: 0.01\n",
      "Epoch [1938/10000], Loss: 104.8929214477539, Learning Rate: 0.01\n",
      "Epoch [1939/10000], Loss: 104.88217163085938, Learning Rate: 0.01\n",
      "Epoch [1940/10000], Loss: 104.87152099609375, Learning Rate: 0.01\n",
      "Epoch [1941/10000], Loss: 104.86077880859375, Learning Rate: 0.01\n",
      "Epoch [1942/10000], Loss: 104.8501205444336, Learning Rate: 0.01\n",
      "Epoch [1943/10000], Loss: 104.83940887451172, Learning Rate: 0.01\n",
      "Epoch [1944/10000], Loss: 104.82882690429688, Learning Rate: 0.01\n",
      "Epoch [1945/10000], Loss: 104.81805419921875, Learning Rate: 0.01\n",
      "Epoch [1946/10000], Loss: 104.80735778808594, Learning Rate: 0.01\n",
      "Epoch [1947/10000], Loss: 104.79673767089844, Learning Rate: 0.01\n",
      "Epoch [1948/10000], Loss: 104.7859878540039, Learning Rate: 0.01\n",
      "Epoch [1949/10000], Loss: 104.775390625, Learning Rate: 0.01\n",
      "Epoch [1950/10000], Loss: 104.7646484375, Learning Rate: 0.01\n",
      "Epoch [1951/10000], Loss: 104.75394439697266, Learning Rate: 0.01\n",
      "Epoch [1952/10000], Loss: 104.74340057373047, Learning Rate: 0.01\n",
      "Epoch [1953/10000], Loss: 104.73272705078125, Learning Rate: 0.01\n",
      "Epoch [1954/10000], Loss: 104.72205352783203, Learning Rate: 0.01\n",
      "Epoch [1955/10000], Loss: 104.71139526367188, Learning Rate: 0.01\n",
      "Epoch [1956/10000], Loss: 104.70069122314453, Learning Rate: 0.01\n",
      "Epoch [1957/10000], Loss: 104.69010162353516, Learning Rate: 0.01\n",
      "Epoch [1958/10000], Loss: 104.67943572998047, Learning Rate: 0.01\n",
      "Epoch [1959/10000], Loss: 104.66879272460938, Learning Rate: 0.01\n",
      "Epoch [1960/10000], Loss: 104.65809631347656, Learning Rate: 0.01\n",
      "Epoch [1961/10000], Loss: 104.64753723144531, Learning Rate: 0.01\n",
      "Epoch [1962/10000], Loss: 104.63681030273438, Learning Rate: 0.01\n",
      "Epoch [1963/10000], Loss: 104.62622833251953, Learning Rate: 0.01\n",
      "Epoch [1964/10000], Loss: 104.61556243896484, Learning Rate: 0.01\n",
      "Epoch [1965/10000], Loss: 104.6049575805664, Learning Rate: 0.01\n",
      "Epoch [1966/10000], Loss: 104.59429931640625, Learning Rate: 0.01\n",
      "Epoch [1967/10000], Loss: 104.58370971679688, Learning Rate: 0.01\n",
      "Epoch [1968/10000], Loss: 104.57306671142578, Learning Rate: 0.01\n",
      "Epoch [1969/10000], Loss: 104.56243133544922, Learning Rate: 0.01\n",
      "Epoch [1970/10000], Loss: 104.55181121826172, Learning Rate: 0.01\n",
      "Epoch [1971/10000], Loss: 104.54119873046875, Learning Rate: 0.01\n",
      "Epoch [1972/10000], Loss: 104.53063201904297, Learning Rate: 0.01\n",
      "Epoch [1973/10000], Loss: 104.5200424194336, Learning Rate: 0.01\n",
      "Epoch [1974/10000], Loss: 104.50944519042969, Learning Rate: 0.01\n",
      "Epoch [1975/10000], Loss: 104.49872589111328, Learning Rate: 0.01\n",
      "Epoch [1976/10000], Loss: 104.48809814453125, Learning Rate: 0.01\n",
      "Epoch [1977/10000], Loss: 104.47760009765625, Learning Rate: 0.01\n",
      "Epoch [1978/10000], Loss: 104.4670181274414, Learning Rate: 0.01\n",
      "Epoch [1979/10000], Loss: 104.4563980102539, Learning Rate: 0.01\n",
      "Epoch [1980/10000], Loss: 104.44580841064453, Learning Rate: 0.01\n",
      "Epoch [1981/10000], Loss: 104.43513488769531, Learning Rate: 0.01\n",
      "Epoch [1982/10000], Loss: 104.42457580566406, Learning Rate: 0.01\n",
      "Epoch [1983/10000], Loss: 104.4139633178711, Learning Rate: 0.01\n",
      "Epoch [1984/10000], Loss: 104.40338134765625, Learning Rate: 0.01\n",
      "Epoch [1985/10000], Loss: 104.392822265625, Learning Rate: 0.01\n",
      "Epoch [1986/10000], Loss: 104.38236999511719, Learning Rate: 0.01\n",
      "Epoch [1987/10000], Loss: 104.37169647216797, Learning Rate: 0.01\n",
      "Epoch [1988/10000], Loss: 104.3612060546875, Learning Rate: 0.01\n",
      "Epoch [1989/10000], Loss: 104.35072326660156, Learning Rate: 0.01\n",
      "Epoch [1990/10000], Loss: 104.34004974365234, Learning Rate: 0.01\n",
      "Epoch [1991/10000], Loss: 104.32953643798828, Learning Rate: 0.01\n",
      "Epoch [1992/10000], Loss: 104.31890106201172, Learning Rate: 0.01\n",
      "Epoch [1993/10000], Loss: 104.3083267211914, Learning Rate: 0.01\n",
      "Epoch [1994/10000], Loss: 104.29776763916016, Learning Rate: 0.01\n",
      "Epoch [1995/10000], Loss: 104.28721618652344, Learning Rate: 0.01\n",
      "Epoch [1996/10000], Loss: 104.27670288085938, Learning Rate: 0.01\n",
      "Epoch [1997/10000], Loss: 104.26617431640625, Learning Rate: 0.01\n",
      "Epoch [1998/10000], Loss: 104.25553131103516, Learning Rate: 0.01\n",
      "Epoch [1999/10000], Loss: 104.24505615234375, Learning Rate: 0.01\n",
      "Epoch [2000/10000], Loss: 104.23450469970703, Learning Rate: 0.01\n",
      "Epoch [2001/10000], Loss: 104.22401428222656, Learning Rate: 0.01\n",
      "Epoch [2002/10000], Loss: 104.21351623535156, Learning Rate: 0.01\n",
      "Epoch [2003/10000], Loss: 104.20286560058594, Learning Rate: 0.01\n",
      "Epoch [2004/10000], Loss: 104.19237518310547, Learning Rate: 0.01\n",
      "Epoch [2005/10000], Loss: 104.18180847167969, Learning Rate: 0.01\n",
      "Epoch [2006/10000], Loss: 104.17127227783203, Learning Rate: 0.01\n",
      "Epoch [2007/10000], Loss: 104.16077423095703, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2008/10000], Loss: 104.15029907226562, Learning Rate: 0.01\n",
      "Epoch [2009/10000], Loss: 104.1397705078125, Learning Rate: 0.01\n",
      "Epoch [2010/10000], Loss: 104.12925720214844, Learning Rate: 0.01\n",
      "Epoch [2011/10000], Loss: 104.11876678466797, Learning Rate: 0.01\n",
      "Epoch [2012/10000], Loss: 104.10816192626953, Learning Rate: 0.01\n",
      "Epoch [2013/10000], Loss: 104.09770965576172, Learning Rate: 0.01\n",
      "Epoch [2014/10000], Loss: 104.08719635009766, Learning Rate: 0.01\n",
      "Epoch [2015/10000], Loss: 104.07675170898438, Learning Rate: 0.01\n",
      "Epoch [2016/10000], Loss: 104.06620025634766, Learning Rate: 0.01\n",
      "Epoch [2017/10000], Loss: 104.05573272705078, Learning Rate: 0.01\n",
      "Epoch [2018/10000], Loss: 104.04522705078125, Learning Rate: 0.01\n",
      "Epoch [2019/10000], Loss: 104.03472137451172, Learning Rate: 0.01\n",
      "Epoch [2020/10000], Loss: 104.0241470336914, Learning Rate: 0.01\n",
      "Epoch [2021/10000], Loss: 104.01373291015625, Learning Rate: 0.01\n",
      "Epoch [2022/10000], Loss: 104.00330352783203, Learning Rate: 0.01\n",
      "Epoch [2023/10000], Loss: 103.9927749633789, Learning Rate: 0.01\n",
      "Epoch [2024/10000], Loss: 103.9823226928711, Learning Rate: 0.01\n",
      "Epoch [2025/10000], Loss: 103.97177124023438, Learning Rate: 0.01\n",
      "Epoch [2026/10000], Loss: 103.9613265991211, Learning Rate: 0.01\n",
      "Epoch [2027/10000], Loss: 103.95091247558594, Learning Rate: 0.01\n",
      "Epoch [2028/10000], Loss: 103.94031524658203, Learning Rate: 0.01\n",
      "Epoch [2029/10000], Loss: 103.929931640625, Learning Rate: 0.01\n",
      "Epoch [2030/10000], Loss: 103.91947174072266, Learning Rate: 0.01\n",
      "Epoch [2031/10000], Loss: 103.90901184082031, Learning Rate: 0.01\n",
      "Epoch [2032/10000], Loss: 103.89854431152344, Learning Rate: 0.01\n",
      "Epoch [2033/10000], Loss: 103.88795471191406, Learning Rate: 0.01\n",
      "Epoch [2034/10000], Loss: 103.87763214111328, Learning Rate: 0.01\n",
      "Epoch [2035/10000], Loss: 103.86714172363281, Learning Rate: 0.01\n",
      "Epoch [2036/10000], Loss: 103.85662078857422, Learning Rate: 0.01\n",
      "Epoch [2037/10000], Loss: 103.84629821777344, Learning Rate: 0.01\n",
      "Epoch [2038/10000], Loss: 103.83580017089844, Learning Rate: 0.01\n",
      "Epoch [2039/10000], Loss: 103.82540130615234, Learning Rate: 0.01\n",
      "Epoch [2040/10000], Loss: 103.8149185180664, Learning Rate: 0.01\n",
      "Epoch [2041/10000], Loss: 103.8044204711914, Learning Rate: 0.01\n",
      "Epoch [2042/10000], Loss: 103.79402160644531, Learning Rate: 0.01\n",
      "Epoch [2043/10000], Loss: 103.78353118896484, Learning Rate: 0.01\n",
      "Epoch [2044/10000], Loss: 103.77308654785156, Learning Rate: 0.01\n",
      "Epoch [2045/10000], Loss: 103.76265716552734, Learning Rate: 0.01\n",
      "Epoch [2046/10000], Loss: 103.75233459472656, Learning Rate: 0.01\n",
      "Epoch [2047/10000], Loss: 103.74182891845703, Learning Rate: 0.01\n",
      "Epoch [2048/10000], Loss: 103.7314224243164, Learning Rate: 0.01\n",
      "Epoch [2049/10000], Loss: 103.72100830078125, Learning Rate: 0.01\n",
      "Epoch [2050/10000], Loss: 103.7107162475586, Learning Rate: 0.01\n",
      "Epoch [2051/10000], Loss: 103.70027160644531, Learning Rate: 0.01\n",
      "Epoch [2052/10000], Loss: 103.689697265625, Learning Rate: 0.01\n",
      "Epoch [2053/10000], Loss: 103.67937469482422, Learning Rate: 0.01\n",
      "Epoch [2054/10000], Loss: 103.6689682006836, Learning Rate: 0.01\n",
      "Epoch [2055/10000], Loss: 103.65850830078125, Learning Rate: 0.01\n",
      "Epoch [2056/10000], Loss: 103.64815521240234, Learning Rate: 0.01\n",
      "Epoch [2057/10000], Loss: 103.6378173828125, Learning Rate: 0.01\n",
      "Epoch [2058/10000], Loss: 103.6273422241211, Learning Rate: 0.01\n",
      "Epoch [2059/10000], Loss: 103.61692810058594, Learning Rate: 0.01\n",
      "Epoch [2060/10000], Loss: 103.6065902709961, Learning Rate: 0.01\n",
      "Epoch [2061/10000], Loss: 103.59612274169922, Learning Rate: 0.01\n",
      "Epoch [2062/10000], Loss: 103.585693359375, Learning Rate: 0.01\n",
      "Epoch [2063/10000], Loss: 103.57534790039062, Learning Rate: 0.01\n",
      "Epoch [2064/10000], Loss: 103.56488037109375, Learning Rate: 0.01\n",
      "Epoch [2065/10000], Loss: 103.55458068847656, Learning Rate: 0.01\n",
      "Epoch [2066/10000], Loss: 103.54425811767578, Learning Rate: 0.01\n",
      "Epoch [2067/10000], Loss: 103.53386688232422, Learning Rate: 0.01\n",
      "Epoch [2068/10000], Loss: 103.52337646484375, Learning Rate: 0.01\n",
      "Epoch [2069/10000], Loss: 103.51310729980469, Learning Rate: 0.01\n",
      "Epoch [2070/10000], Loss: 103.50264739990234, Learning Rate: 0.01\n",
      "Epoch [2071/10000], Loss: 103.49231719970703, Learning Rate: 0.01\n",
      "Epoch [2072/10000], Loss: 103.48197174072266, Learning Rate: 0.01\n",
      "Epoch [2073/10000], Loss: 103.47161102294922, Learning Rate: 0.01\n",
      "Epoch [2074/10000], Loss: 103.46118927001953, Learning Rate: 0.01\n",
      "Epoch [2075/10000], Loss: 103.4508285522461, Learning Rate: 0.01\n",
      "Epoch [2076/10000], Loss: 103.44049835205078, Learning Rate: 0.01\n",
      "Epoch [2077/10000], Loss: 103.4301528930664, Learning Rate: 0.01\n",
      "Epoch [2078/10000], Loss: 103.41971588134766, Learning Rate: 0.01\n",
      "Epoch [2079/10000], Loss: 103.4094467163086, Learning Rate: 0.01\n",
      "Epoch [2080/10000], Loss: 103.39900970458984, Learning Rate: 0.01\n",
      "Epoch [2081/10000], Loss: 103.38871765136719, Learning Rate: 0.01\n",
      "Epoch [2082/10000], Loss: 103.37835693359375, Learning Rate: 0.01\n",
      "Epoch [2083/10000], Loss: 103.36802673339844, Learning Rate: 0.01\n",
      "Epoch [2084/10000], Loss: 103.35757446289062, Learning Rate: 0.01\n",
      "Epoch [2085/10000], Loss: 103.34734344482422, Learning Rate: 0.01\n",
      "Epoch [2086/10000], Loss: 103.33702850341797, Learning Rate: 0.01\n",
      "Epoch [2087/10000], Loss: 103.3266372680664, Learning Rate: 0.01\n",
      "Epoch [2088/10000], Loss: 103.3163070678711, Learning Rate: 0.01\n",
      "Epoch [2089/10000], Loss: 103.30598449707031, Learning Rate: 0.01\n",
      "Epoch [2090/10000], Loss: 103.29564666748047, Learning Rate: 0.01\n",
      "Epoch [2091/10000], Loss: 103.28533172607422, Learning Rate: 0.01\n",
      "Epoch [2092/10000], Loss: 103.27500915527344, Learning Rate: 0.01\n",
      "Epoch [2093/10000], Loss: 103.26465606689453, Learning Rate: 0.01\n",
      "Epoch [2094/10000], Loss: 103.25434112548828, Learning Rate: 0.01\n",
      "Epoch [2095/10000], Loss: 103.24394989013672, Learning Rate: 0.01\n",
      "Epoch [2096/10000], Loss: 103.23363494873047, Learning Rate: 0.01\n",
      "Epoch [2097/10000], Loss: 103.22343444824219, Learning Rate: 0.01\n",
      "Epoch [2098/10000], Loss: 103.21311950683594, Learning Rate: 0.01\n",
      "Epoch [2099/10000], Loss: 103.2027816772461, Learning Rate: 0.01\n",
      "Epoch [2100/10000], Loss: 103.19245147705078, Learning Rate: 0.01\n",
      "Epoch [2101/10000], Loss: 103.18214416503906, Learning Rate: 0.01\n",
      "Epoch [2102/10000], Loss: 103.1718521118164, Learning Rate: 0.01\n",
      "Epoch [2103/10000], Loss: 103.16158294677734, Learning Rate: 0.01\n",
      "Epoch [2104/10000], Loss: 103.15126037597656, Learning Rate: 0.01\n",
      "Epoch [2105/10000], Loss: 103.14096069335938, Learning Rate: 0.01\n",
      "Epoch [2106/10000], Loss: 103.13056945800781, Learning Rate: 0.01\n",
      "Epoch [2107/10000], Loss: 103.12035369873047, Learning Rate: 0.01\n",
      "Epoch [2108/10000], Loss: 103.11006927490234, Learning Rate: 0.01\n",
      "Epoch [2109/10000], Loss: 103.09980010986328, Learning Rate: 0.01\n",
      "Epoch [2110/10000], Loss: 103.08950805664062, Learning Rate: 0.01\n",
      "Epoch [2111/10000], Loss: 103.07919311523438, Learning Rate: 0.01\n",
      "Epoch [2112/10000], Loss: 103.06893920898438, Learning Rate: 0.01\n",
      "Epoch [2113/10000], Loss: 103.05867767333984, Learning Rate: 0.01\n",
      "Epoch [2114/10000], Loss: 103.04833984375, Learning Rate: 0.01\n",
      "Epoch [2115/10000], Loss: 103.03807067871094, Learning Rate: 0.01\n",
      "Epoch [2116/10000], Loss: 103.02777099609375, Learning Rate: 0.01\n",
      "Epoch [2117/10000], Loss: 103.017578125, Learning Rate: 0.01\n",
      "Epoch [2118/10000], Loss: 103.00724792480469, Learning Rate: 0.01\n",
      "Epoch [2119/10000], Loss: 102.99693298339844, Learning Rate: 0.01\n",
      "Epoch [2120/10000], Loss: 102.9866714477539, Learning Rate: 0.01\n",
      "Epoch [2121/10000], Loss: 102.97650909423828, Learning Rate: 0.01\n",
      "Epoch [2122/10000], Loss: 102.96621704101562, Learning Rate: 0.01\n",
      "Epoch [2123/10000], Loss: 102.95594024658203, Learning Rate: 0.01\n",
      "Epoch [2124/10000], Loss: 102.94569396972656, Learning Rate: 0.01\n",
      "Epoch [2125/10000], Loss: 102.9353256225586, Learning Rate: 0.01\n",
      "Epoch [2126/10000], Loss: 102.92520141601562, Learning Rate: 0.01\n",
      "Epoch [2127/10000], Loss: 102.91487884521484, Learning Rate: 0.01\n",
      "Epoch [2128/10000], Loss: 102.90463256835938, Learning Rate: 0.01\n",
      "Epoch [2129/10000], Loss: 102.89435577392578, Learning Rate: 0.01\n",
      "Epoch [2130/10000], Loss: 102.88420104980469, Learning Rate: 0.01\n",
      "Epoch [2131/10000], Loss: 102.8738784790039, Learning Rate: 0.01\n",
      "Epoch [2132/10000], Loss: 102.86366271972656, Learning Rate: 0.01\n",
      "Epoch [2133/10000], Loss: 102.85346221923828, Learning Rate: 0.01\n",
      "Epoch [2134/10000], Loss: 102.84322357177734, Learning Rate: 0.01\n",
      "Epoch [2135/10000], Loss: 102.83295440673828, Learning Rate: 0.01\n",
      "Epoch [2136/10000], Loss: 102.82270812988281, Learning Rate: 0.01\n",
      "Epoch [2137/10000], Loss: 102.81243896484375, Learning Rate: 0.01\n",
      "Epoch [2138/10000], Loss: 102.8022232055664, Learning Rate: 0.01\n",
      "Epoch [2139/10000], Loss: 102.79203033447266, Learning Rate: 0.01\n",
      "Epoch [2140/10000], Loss: 102.7818374633789, Learning Rate: 0.01\n",
      "Epoch [2141/10000], Loss: 102.77154541015625, Learning Rate: 0.01\n",
      "Epoch [2142/10000], Loss: 102.76142883300781, Learning Rate: 0.01\n",
      "Epoch [2143/10000], Loss: 102.7511215209961, Learning Rate: 0.01\n",
      "Epoch [2144/10000], Loss: 102.74091339111328, Learning Rate: 0.01\n",
      "Epoch [2145/10000], Loss: 102.73072814941406, Learning Rate: 0.01\n",
      "Epoch [2146/10000], Loss: 102.7204818725586, Learning Rate: 0.01\n",
      "Epoch [2147/10000], Loss: 102.7103042602539, Learning Rate: 0.01\n",
      "Epoch [2148/10000], Loss: 102.70008850097656, Learning Rate: 0.01\n",
      "Epoch [2149/10000], Loss: 102.68987274169922, Learning Rate: 0.01\n",
      "Epoch [2150/10000], Loss: 102.67959594726562, Learning Rate: 0.01\n",
      "Epoch [2151/10000], Loss: 102.66938781738281, Learning Rate: 0.01\n",
      "Epoch [2152/10000], Loss: 102.65924072265625, Learning Rate: 0.01\n",
      "Epoch [2153/10000], Loss: 102.64904022216797, Learning Rate: 0.01\n",
      "Epoch [2154/10000], Loss: 102.63885498046875, Learning Rate: 0.01\n",
      "Epoch [2155/10000], Loss: 102.62861633300781, Learning Rate: 0.01\n",
      "Epoch [2156/10000], Loss: 102.6183853149414, Learning Rate: 0.01\n",
      "Epoch [2157/10000], Loss: 102.60831451416016, Learning Rate: 0.01\n",
      "Epoch [2158/10000], Loss: 102.59812927246094, Learning Rate: 0.01\n",
      "Epoch [2159/10000], Loss: 102.58787536621094, Learning Rate: 0.01\n",
      "Epoch [2160/10000], Loss: 102.57774353027344, Learning Rate: 0.01\n",
      "Epoch [2161/10000], Loss: 102.56755065917969, Learning Rate: 0.01\n",
      "Epoch [2162/10000], Loss: 102.55731201171875, Learning Rate: 0.01\n",
      "Epoch [2163/10000], Loss: 102.54715728759766, Learning Rate: 0.01\n",
      "Epoch [2164/10000], Loss: 102.53688049316406, Learning Rate: 0.01\n",
      "Epoch [2165/10000], Loss: 102.5268325805664, Learning Rate: 0.01\n",
      "Epoch [2166/10000], Loss: 102.51657104492188, Learning Rate: 0.01\n",
      "Epoch [2167/10000], Loss: 102.5064697265625, Learning Rate: 0.01\n",
      "Epoch [2168/10000], Loss: 102.49623107910156, Learning Rate: 0.01\n",
      "Epoch [2169/10000], Loss: 102.48607635498047, Learning Rate: 0.01\n",
      "Epoch [2170/10000], Loss: 102.47595977783203, Learning Rate: 0.01\n",
      "Epoch [2171/10000], Loss: 102.46575927734375, Learning Rate: 0.01\n",
      "Epoch [2172/10000], Loss: 102.4555892944336, Learning Rate: 0.01\n",
      "Epoch [2173/10000], Loss: 102.4454116821289, Learning Rate: 0.01\n",
      "Epoch [2174/10000], Loss: 102.43537139892578, Learning Rate: 0.01\n",
      "Epoch [2175/10000], Loss: 102.42510223388672, Learning Rate: 0.01\n",
      "Epoch [2176/10000], Loss: 102.41496276855469, Learning Rate: 0.01\n",
      "Epoch [2177/10000], Loss: 102.40480041503906, Learning Rate: 0.01\n",
      "Epoch [2178/10000], Loss: 102.3946304321289, Learning Rate: 0.01\n",
      "Epoch [2179/10000], Loss: 102.38452911376953, Learning Rate: 0.01\n",
      "Epoch [2180/10000], Loss: 102.3743667602539, Learning Rate: 0.01\n",
      "Epoch [2181/10000], Loss: 102.36419677734375, Learning Rate: 0.01\n",
      "Epoch [2182/10000], Loss: 102.35407257080078, Learning Rate: 0.01\n",
      "Epoch [2183/10000], Loss: 102.3439712524414, Learning Rate: 0.01\n",
      "Epoch [2184/10000], Loss: 102.33380889892578, Learning Rate: 0.01\n",
      "Epoch [2185/10000], Loss: 102.32366180419922, Learning Rate: 0.01\n",
      "Epoch [2186/10000], Loss: 102.31355285644531, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2187/10000], Loss: 102.30341339111328, Learning Rate: 0.01\n",
      "Epoch [2188/10000], Loss: 102.29328155517578, Learning Rate: 0.01\n",
      "Epoch [2189/10000], Loss: 102.28321075439453, Learning Rate: 0.01\n",
      "Epoch [2190/10000], Loss: 102.27301025390625, Learning Rate: 0.01\n",
      "Epoch [2191/10000], Loss: 102.26280975341797, Learning Rate: 0.01\n",
      "Epoch [2192/10000], Loss: 102.2527847290039, Learning Rate: 0.01\n",
      "Epoch [2193/10000], Loss: 102.24274444580078, Learning Rate: 0.01\n",
      "Epoch [2194/10000], Loss: 102.23255157470703, Learning Rate: 0.01\n",
      "Epoch [2195/10000], Loss: 102.22235107421875, Learning Rate: 0.01\n",
      "Epoch [2196/10000], Loss: 102.21228790283203, Learning Rate: 0.01\n",
      "Epoch [2197/10000], Loss: 102.20215606689453, Learning Rate: 0.01\n",
      "Epoch [2198/10000], Loss: 102.19208526611328, Learning Rate: 0.01\n",
      "Epoch [2199/10000], Loss: 102.18196868896484, Learning Rate: 0.01\n",
      "Epoch [2200/10000], Loss: 102.17192077636719, Learning Rate: 0.01\n",
      "Epoch [2201/10000], Loss: 102.1617202758789, Learning Rate: 0.01\n",
      "Epoch [2202/10000], Loss: 102.15172576904297, Learning Rate: 0.01\n",
      "Epoch [2203/10000], Loss: 102.14155578613281, Learning Rate: 0.01\n",
      "Epoch [2204/10000], Loss: 102.1313705444336, Learning Rate: 0.01\n",
      "Epoch [2205/10000], Loss: 102.12135314941406, Learning Rate: 0.01\n",
      "Epoch [2206/10000], Loss: 102.1112060546875, Learning Rate: 0.01\n",
      "Epoch [2207/10000], Loss: 102.10111236572266, Learning Rate: 0.01\n",
      "Epoch [2208/10000], Loss: 102.09100341796875, Learning Rate: 0.01\n",
      "Epoch [2209/10000], Loss: 102.08097076416016, Learning Rate: 0.01\n",
      "Epoch [2210/10000], Loss: 102.07083892822266, Learning Rate: 0.01\n",
      "Epoch [2211/10000], Loss: 102.0608139038086, Learning Rate: 0.01\n",
      "Epoch [2212/10000], Loss: 102.05070495605469, Learning Rate: 0.01\n",
      "Epoch [2213/10000], Loss: 102.04063415527344, Learning Rate: 0.01\n",
      "Epoch [2214/10000], Loss: 102.030517578125, Learning Rate: 0.01\n",
      "Epoch [2215/10000], Loss: 102.02043151855469, Learning Rate: 0.01\n",
      "Epoch [2216/10000], Loss: 102.0103759765625, Learning Rate: 0.01\n",
      "Epoch [2217/10000], Loss: 102.00028991699219, Learning Rate: 0.01\n",
      "Epoch [2218/10000], Loss: 101.99024200439453, Learning Rate: 0.01\n",
      "Epoch [2219/10000], Loss: 101.98007202148438, Learning Rate: 0.01\n",
      "Epoch [2220/10000], Loss: 101.96998596191406, Learning Rate: 0.01\n",
      "Epoch [2221/10000], Loss: 101.95997619628906, Learning Rate: 0.01\n",
      "Epoch [2222/10000], Loss: 101.94991302490234, Learning Rate: 0.01\n",
      "Epoch [2223/10000], Loss: 101.93984985351562, Learning Rate: 0.01\n",
      "Epoch [2224/10000], Loss: 101.92971801757812, Learning Rate: 0.01\n",
      "Epoch [2225/10000], Loss: 101.91973114013672, Learning Rate: 0.01\n",
      "Epoch [2226/10000], Loss: 101.90965270996094, Learning Rate: 0.01\n",
      "Epoch [2227/10000], Loss: 101.89959716796875, Learning Rate: 0.01\n",
      "Epoch [2228/10000], Loss: 101.88961791992188, Learning Rate: 0.01\n",
      "Epoch [2229/10000], Loss: 101.8794937133789, Learning Rate: 0.01\n",
      "Epoch [2230/10000], Loss: 101.86946105957031, Learning Rate: 0.01\n",
      "Epoch [2231/10000], Loss: 101.85940551757812, Learning Rate: 0.01\n",
      "Epoch [2232/10000], Loss: 101.84928894042969, Learning Rate: 0.01\n",
      "Epoch [2233/10000], Loss: 101.83934020996094, Learning Rate: 0.01\n",
      "Epoch [2234/10000], Loss: 101.82927703857422, Learning Rate: 0.01\n",
      "Epoch [2235/10000], Loss: 101.81920623779297, Learning Rate: 0.01\n",
      "Epoch [2236/10000], Loss: 101.80915069580078, Learning Rate: 0.01\n",
      "Epoch [2237/10000], Loss: 101.79910278320312, Learning Rate: 0.01\n",
      "Epoch [2238/10000], Loss: 101.78905487060547, Learning Rate: 0.01\n",
      "Epoch [2239/10000], Loss: 101.77901458740234, Learning Rate: 0.01\n",
      "Epoch [2240/10000], Loss: 101.76905059814453, Learning Rate: 0.01\n",
      "Epoch [2241/10000], Loss: 101.75906372070312, Learning Rate: 0.01\n",
      "Epoch [2242/10000], Loss: 101.74893188476562, Learning Rate: 0.01\n",
      "Epoch [2243/10000], Loss: 101.73894500732422, Learning Rate: 0.01\n",
      "Epoch [2244/10000], Loss: 101.72888946533203, Learning Rate: 0.01\n",
      "Epoch [2245/10000], Loss: 101.71888732910156, Learning Rate: 0.01\n",
      "Epoch [2246/10000], Loss: 101.70879364013672, Learning Rate: 0.01\n",
      "Epoch [2247/10000], Loss: 101.6988525390625, Learning Rate: 0.01\n",
      "Epoch [2248/10000], Loss: 101.68888092041016, Learning Rate: 0.01\n",
      "Epoch [2249/10000], Loss: 101.67875671386719, Learning Rate: 0.01\n",
      "Epoch [2250/10000], Loss: 101.6688232421875, Learning Rate: 0.01\n",
      "Epoch [2251/10000], Loss: 101.65874481201172, Learning Rate: 0.01\n",
      "Epoch [2252/10000], Loss: 101.64871978759766, Learning Rate: 0.01\n",
      "Epoch [2253/10000], Loss: 101.63873291015625, Learning Rate: 0.01\n",
      "Epoch [2254/10000], Loss: 101.62873077392578, Learning Rate: 0.01\n",
      "Epoch [2255/10000], Loss: 101.6187515258789, Learning Rate: 0.01\n",
      "Epoch [2256/10000], Loss: 101.60875701904297, Learning Rate: 0.01\n",
      "Epoch [2257/10000], Loss: 101.59871673583984, Learning Rate: 0.01\n",
      "Epoch [2258/10000], Loss: 101.58868408203125, Learning Rate: 0.01\n",
      "Epoch [2259/10000], Loss: 101.57867431640625, Learning Rate: 0.01\n",
      "Epoch [2260/10000], Loss: 101.56874084472656, Learning Rate: 0.01\n",
      "Epoch [2261/10000], Loss: 101.55873107910156, Learning Rate: 0.01\n",
      "Epoch [2262/10000], Loss: 101.54869842529297, Learning Rate: 0.01\n",
      "Epoch [2263/10000], Loss: 101.53876495361328, Learning Rate: 0.01\n",
      "Epoch [2264/10000], Loss: 101.52876281738281, Learning Rate: 0.01\n",
      "Epoch [2265/10000], Loss: 101.51870727539062, Learning Rate: 0.01\n",
      "Epoch [2266/10000], Loss: 101.50867462158203, Learning Rate: 0.01\n",
      "Epoch [2267/10000], Loss: 101.49876403808594, Learning Rate: 0.01\n",
      "Epoch [2268/10000], Loss: 101.48876953125, Learning Rate: 0.01\n",
      "Epoch [2269/10000], Loss: 101.4787826538086, Learning Rate: 0.01\n",
      "Epoch [2270/10000], Loss: 101.46878814697266, Learning Rate: 0.01\n",
      "Epoch [2271/10000], Loss: 101.4588623046875, Learning Rate: 0.01\n",
      "Epoch [2272/10000], Loss: 101.44882202148438, Learning Rate: 0.01\n",
      "Epoch [2273/10000], Loss: 101.4389419555664, Learning Rate: 0.01\n",
      "Epoch [2274/10000], Loss: 101.42899322509766, Learning Rate: 0.01\n",
      "Epoch [2275/10000], Loss: 101.4189224243164, Learning Rate: 0.01\n",
      "Epoch [2276/10000], Loss: 101.40894317626953, Learning Rate: 0.01\n",
      "Epoch [2277/10000], Loss: 101.39899444580078, Learning Rate: 0.01\n",
      "Epoch [2278/10000], Loss: 101.38897705078125, Learning Rate: 0.01\n",
      "Epoch [2279/10000], Loss: 101.37910461425781, Learning Rate: 0.01\n",
      "Epoch [2280/10000], Loss: 101.36910247802734, Learning Rate: 0.01\n",
      "Epoch [2281/10000], Loss: 101.35916137695312, Learning Rate: 0.01\n",
      "Epoch [2282/10000], Loss: 101.34917449951172, Learning Rate: 0.01\n",
      "Epoch [2283/10000], Loss: 101.33924865722656, Learning Rate: 0.01\n",
      "Epoch [2284/10000], Loss: 101.32928466796875, Learning Rate: 0.01\n",
      "Epoch [2285/10000], Loss: 101.31935119628906, Learning Rate: 0.01\n",
      "Epoch [2286/10000], Loss: 101.30937957763672, Learning Rate: 0.01\n",
      "Epoch [2287/10000], Loss: 101.2994613647461, Learning Rate: 0.01\n",
      "Epoch [2288/10000], Loss: 101.28939819335938, Learning Rate: 0.01\n",
      "Epoch [2289/10000], Loss: 101.27952575683594, Learning Rate: 0.01\n",
      "Epoch [2290/10000], Loss: 101.26953887939453, Learning Rate: 0.01\n",
      "Epoch [2291/10000], Loss: 101.25959777832031, Learning Rate: 0.01\n",
      "Epoch [2292/10000], Loss: 101.24961853027344, Learning Rate: 0.01\n",
      "Epoch [2293/10000], Loss: 101.23967742919922, Learning Rate: 0.01\n",
      "Epoch [2294/10000], Loss: 101.22976684570312, Learning Rate: 0.01\n",
      "Epoch [2295/10000], Loss: 101.21988677978516, Learning Rate: 0.01\n",
      "Epoch [2296/10000], Loss: 101.20991516113281, Learning Rate: 0.01\n",
      "Epoch [2297/10000], Loss: 101.19991302490234, Learning Rate: 0.01\n",
      "Epoch [2298/10000], Loss: 101.1900863647461, Learning Rate: 0.01\n",
      "Epoch [2299/10000], Loss: 101.18012237548828, Learning Rate: 0.01\n",
      "Epoch [2300/10000], Loss: 101.1701889038086, Learning Rate: 0.01\n",
      "Epoch [2301/10000], Loss: 101.16020202636719, Learning Rate: 0.01\n",
      "Epoch [2302/10000], Loss: 101.15029907226562, Learning Rate: 0.01\n",
      "Epoch [2303/10000], Loss: 101.14036560058594, Learning Rate: 0.01\n",
      "Epoch [2304/10000], Loss: 101.13053131103516, Learning Rate: 0.01\n",
      "Epoch [2305/10000], Loss: 101.12059783935547, Learning Rate: 0.01\n",
      "Epoch [2306/10000], Loss: 101.11065673828125, Learning Rate: 0.01\n",
      "Epoch [2307/10000], Loss: 101.1007308959961, Learning Rate: 0.01\n",
      "Epoch [2308/10000], Loss: 101.0908432006836, Learning Rate: 0.01\n",
      "Epoch [2309/10000], Loss: 101.08099365234375, Learning Rate: 0.01\n",
      "Epoch [2310/10000], Loss: 101.07098388671875, Learning Rate: 0.01\n",
      "Epoch [2311/10000], Loss: 101.0611343383789, Learning Rate: 0.01\n",
      "Epoch [2312/10000], Loss: 101.05116271972656, Learning Rate: 0.01\n",
      "Epoch [2313/10000], Loss: 101.04122161865234, Learning Rate: 0.01\n",
      "Epoch [2314/10000], Loss: 101.03136444091797, Learning Rate: 0.01\n",
      "Epoch [2315/10000], Loss: 101.02146911621094, Learning Rate: 0.01\n",
      "Epoch [2316/10000], Loss: 101.01155090332031, Learning Rate: 0.01\n",
      "Epoch [2317/10000], Loss: 101.00167846679688, Learning Rate: 0.01\n",
      "Epoch [2318/10000], Loss: 100.99177551269531, Learning Rate: 0.01\n",
      "Epoch [2319/10000], Loss: 100.9819107055664, Learning Rate: 0.01\n",
      "Epoch [2320/10000], Loss: 100.97200012207031, Learning Rate: 0.01\n",
      "Epoch [2321/10000], Loss: 100.96208953857422, Learning Rate: 0.01\n",
      "Epoch [2322/10000], Loss: 100.9522705078125, Learning Rate: 0.01\n",
      "Epoch [2323/10000], Loss: 100.94229125976562, Learning Rate: 0.01\n",
      "Epoch [2324/10000], Loss: 100.93244171142578, Learning Rate: 0.01\n",
      "Epoch [2325/10000], Loss: 100.92247009277344, Learning Rate: 0.01\n",
      "Epoch [2326/10000], Loss: 100.91265869140625, Learning Rate: 0.01\n",
      "Epoch [2327/10000], Loss: 100.90267181396484, Learning Rate: 0.01\n",
      "Epoch [2328/10000], Loss: 100.89286041259766, Learning Rate: 0.01\n",
      "Epoch [2329/10000], Loss: 100.883056640625, Learning Rate: 0.01\n",
      "Epoch [2330/10000], Loss: 100.87313079833984, Learning Rate: 0.01\n",
      "Epoch [2331/10000], Loss: 100.8631820678711, Learning Rate: 0.01\n",
      "Epoch [2332/10000], Loss: 100.85340881347656, Learning Rate: 0.01\n",
      "Epoch [2333/10000], Loss: 100.84347534179688, Learning Rate: 0.01\n",
      "Epoch [2334/10000], Loss: 100.83364868164062, Learning Rate: 0.01\n",
      "Epoch [2335/10000], Loss: 100.82379913330078, Learning Rate: 0.01\n",
      "Epoch [2336/10000], Loss: 100.81387329101562, Learning Rate: 0.01\n",
      "Epoch [2337/10000], Loss: 100.80399322509766, Learning Rate: 0.01\n",
      "Epoch [2338/10000], Loss: 100.79415130615234, Learning Rate: 0.01\n",
      "Epoch [2339/10000], Loss: 100.78429412841797, Learning Rate: 0.01\n",
      "Epoch [2340/10000], Loss: 100.77436828613281, Learning Rate: 0.01\n",
      "Epoch [2341/10000], Loss: 100.76454162597656, Learning Rate: 0.01\n",
      "Epoch [2342/10000], Loss: 100.75479125976562, Learning Rate: 0.01\n",
      "Epoch [2343/10000], Loss: 100.74490356445312, Learning Rate: 0.01\n",
      "Epoch [2344/10000], Loss: 100.73501586914062, Learning Rate: 0.01\n",
      "Epoch [2345/10000], Loss: 100.7252197265625, Learning Rate: 0.01\n",
      "Epoch [2346/10000], Loss: 100.71526336669922, Learning Rate: 0.01\n",
      "Epoch [2347/10000], Loss: 100.70549774169922, Learning Rate: 0.01\n",
      "Epoch [2348/10000], Loss: 100.6956558227539, Learning Rate: 0.01\n",
      "Epoch [2349/10000], Loss: 100.68579864501953, Learning Rate: 0.01\n",
      "Epoch [2350/10000], Loss: 100.67595672607422, Learning Rate: 0.01\n",
      "Epoch [2351/10000], Loss: 100.66610717773438, Learning Rate: 0.01\n",
      "Epoch [2352/10000], Loss: 100.65623474121094, Learning Rate: 0.01\n",
      "Epoch [2353/10000], Loss: 100.64637756347656, Learning Rate: 0.01\n",
      "Epoch [2354/10000], Loss: 100.63655853271484, Learning Rate: 0.01\n",
      "Epoch [2355/10000], Loss: 100.626708984375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2356/10000], Loss: 100.61687469482422, Learning Rate: 0.01\n",
      "Epoch [2357/10000], Loss: 100.6070785522461, Learning Rate: 0.01\n",
      "Epoch [2358/10000], Loss: 100.59720611572266, Learning Rate: 0.01\n",
      "Epoch [2359/10000], Loss: 100.5874252319336, Learning Rate: 0.01\n",
      "Epoch [2360/10000], Loss: 100.57759857177734, Learning Rate: 0.01\n",
      "Epoch [2361/10000], Loss: 100.56774139404297, Learning Rate: 0.01\n",
      "Epoch [2362/10000], Loss: 100.55796813964844, Learning Rate: 0.01\n",
      "Epoch [2363/10000], Loss: 100.54803466796875, Learning Rate: 0.01\n",
      "Epoch [2364/10000], Loss: 100.53829956054688, Learning Rate: 0.01\n",
      "Epoch [2365/10000], Loss: 100.52841186523438, Learning Rate: 0.01\n",
      "Epoch [2366/10000], Loss: 100.51863861083984, Learning Rate: 0.01\n",
      "Epoch [2367/10000], Loss: 100.50882720947266, Learning Rate: 0.01\n",
      "Epoch [2368/10000], Loss: 100.49898529052734, Learning Rate: 0.01\n",
      "Epoch [2369/10000], Loss: 100.48919677734375, Learning Rate: 0.01\n",
      "Epoch [2370/10000], Loss: 100.47941589355469, Learning Rate: 0.01\n",
      "Epoch [2371/10000], Loss: 100.46954345703125, Learning Rate: 0.01\n",
      "Epoch [2372/10000], Loss: 100.45974731445312, Learning Rate: 0.01\n",
      "Epoch [2373/10000], Loss: 100.44995880126953, Learning Rate: 0.01\n",
      "Epoch [2374/10000], Loss: 100.44010162353516, Learning Rate: 0.01\n",
      "Epoch [2375/10000], Loss: 100.43032836914062, Learning Rate: 0.01\n",
      "Epoch [2376/10000], Loss: 100.42057037353516, Learning Rate: 0.01\n",
      "Epoch [2377/10000], Loss: 100.4107437133789, Learning Rate: 0.01\n",
      "Epoch [2378/10000], Loss: 100.40090942382812, Learning Rate: 0.01\n",
      "Epoch [2379/10000], Loss: 100.39119720458984, Learning Rate: 0.01\n",
      "Epoch [2380/10000], Loss: 100.38134002685547, Learning Rate: 0.01\n",
      "Epoch [2381/10000], Loss: 100.37152862548828, Learning Rate: 0.01\n",
      "Epoch [2382/10000], Loss: 100.36180114746094, Learning Rate: 0.01\n",
      "Epoch [2383/10000], Loss: 100.3520278930664, Learning Rate: 0.01\n",
      "Epoch [2384/10000], Loss: 100.34214782714844, Learning Rate: 0.01\n",
      "Epoch [2385/10000], Loss: 100.33240509033203, Learning Rate: 0.01\n",
      "Epoch [2386/10000], Loss: 100.32255554199219, Learning Rate: 0.01\n",
      "Epoch [2387/10000], Loss: 100.31285095214844, Learning Rate: 0.01\n",
      "Epoch [2388/10000], Loss: 100.30297088623047, Learning Rate: 0.01\n",
      "Epoch [2389/10000], Loss: 100.29324340820312, Learning Rate: 0.01\n",
      "Epoch [2390/10000], Loss: 100.28350830078125, Learning Rate: 0.01\n",
      "Epoch [2391/10000], Loss: 100.27374267578125, Learning Rate: 0.01\n",
      "Epoch [2392/10000], Loss: 100.26394653320312, Learning Rate: 0.01\n",
      "Epoch [2393/10000], Loss: 100.25411224365234, Learning Rate: 0.01\n",
      "Epoch [2394/10000], Loss: 100.24447631835938, Learning Rate: 0.01\n",
      "Epoch [2395/10000], Loss: 100.23462677001953, Learning Rate: 0.01\n",
      "Epoch [2396/10000], Loss: 100.22486877441406, Learning Rate: 0.01\n",
      "Epoch [2397/10000], Loss: 100.21509552001953, Learning Rate: 0.01\n",
      "Epoch [2398/10000], Loss: 100.20531463623047, Learning Rate: 0.01\n",
      "Epoch [2399/10000], Loss: 100.19559478759766, Learning Rate: 0.01\n",
      "Epoch [2400/10000], Loss: 100.18575286865234, Learning Rate: 0.01\n",
      "Epoch [2401/10000], Loss: 100.17605590820312, Learning Rate: 0.01\n",
      "Epoch [2402/10000], Loss: 100.16631317138672, Learning Rate: 0.01\n",
      "Epoch [2403/10000], Loss: 100.1566162109375, Learning Rate: 0.01\n",
      "Epoch [2404/10000], Loss: 100.14681243896484, Learning Rate: 0.01\n",
      "Epoch [2405/10000], Loss: 100.13700866699219, Learning Rate: 0.01\n",
      "Epoch [2406/10000], Loss: 100.12725830078125, Learning Rate: 0.01\n",
      "Epoch [2407/10000], Loss: 100.11750030517578, Learning Rate: 0.01\n",
      "Epoch [2408/10000], Loss: 100.10773468017578, Learning Rate: 0.01\n",
      "Epoch [2409/10000], Loss: 100.09796905517578, Learning Rate: 0.01\n",
      "Epoch [2410/10000], Loss: 100.08831024169922, Learning Rate: 0.01\n",
      "Epoch [2411/10000], Loss: 100.07856750488281, Learning Rate: 0.01\n",
      "Epoch [2412/10000], Loss: 100.06878662109375, Learning Rate: 0.01\n",
      "Epoch [2413/10000], Loss: 100.05907440185547, Learning Rate: 0.01\n",
      "Epoch [2414/10000], Loss: 100.04924774169922, Learning Rate: 0.01\n",
      "Epoch [2415/10000], Loss: 100.03955078125, Learning Rate: 0.01\n",
      "Epoch [2416/10000], Loss: 100.02978515625, Learning Rate: 0.01\n",
      "Epoch [2417/10000], Loss: 100.02010345458984, Learning Rate: 0.01\n",
      "Epoch [2418/10000], Loss: 100.01033782958984, Learning Rate: 0.01\n",
      "Epoch [2419/10000], Loss: 100.00062561035156, Learning Rate: 0.01\n",
      "Epoch [2420/10000], Loss: 99.99091339111328, Learning Rate: 0.01\n",
      "Epoch [2421/10000], Loss: 99.98111724853516, Learning Rate: 0.01\n",
      "Epoch [2422/10000], Loss: 99.9714126586914, Learning Rate: 0.01\n",
      "Epoch [2423/10000], Loss: 99.96178436279297, Learning Rate: 0.01\n",
      "Epoch [2424/10000], Loss: 99.95201873779297, Learning Rate: 0.01\n",
      "Epoch [2425/10000], Loss: 99.9422607421875, Learning Rate: 0.01\n",
      "Epoch [2426/10000], Loss: 99.9325180053711, Learning Rate: 0.01\n",
      "Epoch [2427/10000], Loss: 99.92283630371094, Learning Rate: 0.01\n",
      "Epoch [2428/10000], Loss: 99.91309356689453, Learning Rate: 0.01\n",
      "Epoch [2429/10000], Loss: 99.90335845947266, Learning Rate: 0.01\n",
      "Epoch [2430/10000], Loss: 99.89356994628906, Learning Rate: 0.01\n",
      "Epoch [2431/10000], Loss: 99.88397979736328, Learning Rate: 0.01\n",
      "Epoch [2432/10000], Loss: 99.87423706054688, Learning Rate: 0.01\n",
      "Epoch [2433/10000], Loss: 99.86463928222656, Learning Rate: 0.01\n",
      "Epoch [2434/10000], Loss: 99.85479736328125, Learning Rate: 0.01\n",
      "Epoch [2435/10000], Loss: 99.84510040283203, Learning Rate: 0.01\n",
      "Epoch [2436/10000], Loss: 99.83540344238281, Learning Rate: 0.01\n",
      "Epoch [2437/10000], Loss: 99.82579803466797, Learning Rate: 0.01\n",
      "Epoch [2438/10000], Loss: 99.81594848632812, Learning Rate: 0.01\n",
      "Epoch [2439/10000], Loss: 99.80628204345703, Learning Rate: 0.01\n",
      "Epoch [2440/10000], Loss: 99.79658508300781, Learning Rate: 0.01\n",
      "Epoch [2441/10000], Loss: 99.78692626953125, Learning Rate: 0.01\n",
      "Epoch [2442/10000], Loss: 99.77725219726562, Learning Rate: 0.01\n",
      "Epoch [2443/10000], Loss: 99.76761627197266, Learning Rate: 0.01\n",
      "Epoch [2444/10000], Loss: 99.7578125, Learning Rate: 0.01\n",
      "Epoch [2445/10000], Loss: 99.74811553955078, Learning Rate: 0.01\n",
      "Epoch [2446/10000], Loss: 99.73845672607422, Learning Rate: 0.01\n",
      "Epoch [2447/10000], Loss: 99.72876739501953, Learning Rate: 0.01\n",
      "Epoch [2448/10000], Loss: 99.71910095214844, Learning Rate: 0.01\n",
      "Epoch [2449/10000], Loss: 99.70932006835938, Learning Rate: 0.01\n",
      "Epoch [2450/10000], Loss: 99.6997299194336, Learning Rate: 0.01\n",
      "Epoch [2451/10000], Loss: 99.6900634765625, Learning Rate: 0.01\n",
      "Epoch [2452/10000], Loss: 99.6803207397461, Learning Rate: 0.01\n",
      "Epoch [2453/10000], Loss: 99.670654296875, Learning Rate: 0.01\n",
      "Epoch [2454/10000], Loss: 99.66099548339844, Learning Rate: 0.01\n",
      "Epoch [2455/10000], Loss: 99.65135955810547, Learning Rate: 0.01\n",
      "Epoch [2456/10000], Loss: 99.64167785644531, Learning Rate: 0.01\n",
      "Epoch [2457/10000], Loss: 99.63199615478516, Learning Rate: 0.01\n",
      "Epoch [2458/10000], Loss: 99.62232971191406, Learning Rate: 0.01\n",
      "Epoch [2459/10000], Loss: 99.61258697509766, Learning Rate: 0.01\n",
      "Epoch [2460/10000], Loss: 99.60301971435547, Learning Rate: 0.01\n",
      "Epoch [2461/10000], Loss: 99.5932846069336, Learning Rate: 0.01\n",
      "Epoch [2462/10000], Loss: 99.58367156982422, Learning Rate: 0.01\n",
      "Epoch [2463/10000], Loss: 99.5739974975586, Learning Rate: 0.01\n",
      "Epoch [2464/10000], Loss: 99.56432342529297, Learning Rate: 0.01\n",
      "Epoch [2465/10000], Loss: 99.55463409423828, Learning Rate: 0.01\n",
      "Epoch [2466/10000], Loss: 99.5450439453125, Learning Rate: 0.01\n",
      "Epoch [2467/10000], Loss: 99.53528594970703, Learning Rate: 0.01\n",
      "Epoch [2468/10000], Loss: 99.52569580078125, Learning Rate: 0.01\n",
      "Epoch [2469/10000], Loss: 99.5159683227539, Learning Rate: 0.01\n",
      "Epoch [2470/10000], Loss: 99.50630950927734, Learning Rate: 0.01\n",
      "Epoch [2471/10000], Loss: 99.49671173095703, Learning Rate: 0.01\n",
      "Epoch [2472/10000], Loss: 99.48710632324219, Learning Rate: 0.01\n",
      "Epoch [2473/10000], Loss: 99.47747039794922, Learning Rate: 0.01\n",
      "Epoch [2474/10000], Loss: 99.46772766113281, Learning Rate: 0.01\n",
      "Epoch [2475/10000], Loss: 99.45809936523438, Learning Rate: 0.01\n",
      "Epoch [2476/10000], Loss: 99.44847106933594, Learning Rate: 0.01\n",
      "Epoch [2477/10000], Loss: 99.43888092041016, Learning Rate: 0.01\n",
      "Epoch [2478/10000], Loss: 99.42924499511719, Learning Rate: 0.01\n",
      "Epoch [2479/10000], Loss: 99.41958618164062, Learning Rate: 0.01\n",
      "Epoch [2480/10000], Loss: 99.40995788574219, Learning Rate: 0.01\n",
      "Epoch [2481/10000], Loss: 99.40038299560547, Learning Rate: 0.01\n",
      "Epoch [2482/10000], Loss: 99.39071655273438, Learning Rate: 0.01\n",
      "Epoch [2483/10000], Loss: 99.38104248046875, Learning Rate: 0.01\n",
      "Epoch [2484/10000], Loss: 99.37129974365234, Learning Rate: 0.01\n",
      "Epoch [2485/10000], Loss: 99.36182403564453, Learning Rate: 0.01\n",
      "Epoch [2486/10000], Loss: 99.35209655761719, Learning Rate: 0.01\n",
      "Epoch [2487/10000], Loss: 99.34252166748047, Learning Rate: 0.01\n",
      "Epoch [2488/10000], Loss: 99.33287811279297, Learning Rate: 0.01\n",
      "Epoch [2489/10000], Loss: 99.32334899902344, Learning Rate: 0.01\n",
      "Epoch [2490/10000], Loss: 99.3136215209961, Learning Rate: 0.01\n",
      "Epoch [2491/10000], Loss: 99.3040542602539, Learning Rate: 0.01\n",
      "Epoch [2492/10000], Loss: 99.29443359375, Learning Rate: 0.01\n",
      "Epoch [2493/10000], Loss: 99.2848129272461, Learning Rate: 0.01\n",
      "Epoch [2494/10000], Loss: 99.27527618408203, Learning Rate: 0.01\n",
      "Epoch [2495/10000], Loss: 99.26557922363281, Learning Rate: 0.01\n",
      "Epoch [2496/10000], Loss: 99.2559585571289, Learning Rate: 0.01\n",
      "Epoch [2497/10000], Loss: 99.24636840820312, Learning Rate: 0.01\n",
      "Epoch [2498/10000], Loss: 99.23673248291016, Learning Rate: 0.01\n",
      "Epoch [2499/10000], Loss: 99.22714233398438, Learning Rate: 0.01\n",
      "Epoch [2500/10000], Loss: 99.21748352050781, Learning Rate: 0.01\n",
      "Epoch [2501/10000], Loss: 99.20797729492188, Learning Rate: 0.01\n",
      "Epoch [2502/10000], Loss: 99.19837951660156, Learning Rate: 0.01\n",
      "Epoch [2503/10000], Loss: 99.1887435913086, Learning Rate: 0.01\n",
      "Epoch [2504/10000], Loss: 99.17915344238281, Learning Rate: 0.01\n",
      "Epoch [2505/10000], Loss: 99.16950988769531, Learning Rate: 0.01\n",
      "Epoch [2506/10000], Loss: 99.15996551513672, Learning Rate: 0.01\n",
      "Epoch [2507/10000], Loss: 99.1502685546875, Learning Rate: 0.01\n",
      "Epoch [2508/10000], Loss: 99.1407470703125, Learning Rate: 0.01\n",
      "Epoch [2509/10000], Loss: 99.13109588623047, Learning Rate: 0.01\n",
      "Epoch [2510/10000], Loss: 99.12153625488281, Learning Rate: 0.01\n",
      "Epoch [2511/10000], Loss: 99.11199951171875, Learning Rate: 0.01\n",
      "Epoch [2512/10000], Loss: 99.10237884521484, Learning Rate: 0.01\n",
      "Epoch [2513/10000], Loss: 99.09282684326172, Learning Rate: 0.01\n",
      "Epoch [2514/10000], Loss: 99.08324432373047, Learning Rate: 0.01\n",
      "Epoch [2515/10000], Loss: 99.07354736328125, Learning Rate: 0.01\n",
      "Epoch [2516/10000], Loss: 99.06396484375, Learning Rate: 0.01\n",
      "Epoch [2517/10000], Loss: 99.05442810058594, Learning Rate: 0.01\n",
      "Epoch [2518/10000], Loss: 99.0448226928711, Learning Rate: 0.01\n",
      "Epoch [2519/10000], Loss: 99.03533935546875, Learning Rate: 0.01\n",
      "Epoch [2520/10000], Loss: 99.02578735351562, Learning Rate: 0.01\n",
      "Epoch [2521/10000], Loss: 99.01615142822266, Learning Rate: 0.01\n",
      "Epoch [2522/10000], Loss: 99.00659942626953, Learning Rate: 0.01\n",
      "Epoch [2523/10000], Loss: 98.9970703125, Learning Rate: 0.01\n",
      "Epoch [2524/10000], Loss: 98.98744201660156, Learning Rate: 0.01\n",
      "Epoch [2525/10000], Loss: 98.9778823852539, Learning Rate: 0.01\n",
      "Epoch [2526/10000], Loss: 98.9682846069336, Learning Rate: 0.01\n",
      "Epoch [2527/10000], Loss: 98.95877838134766, Learning Rate: 0.01\n",
      "Epoch [2528/10000], Loss: 98.94918060302734, Learning Rate: 0.01\n",
      "Epoch [2529/10000], Loss: 98.93961334228516, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2530/10000], Loss: 98.92998504638672, Learning Rate: 0.01\n",
      "Epoch [2531/10000], Loss: 98.9205322265625, Learning Rate: 0.01\n",
      "Epoch [2532/10000], Loss: 98.91094970703125, Learning Rate: 0.01\n",
      "Epoch [2533/10000], Loss: 98.90141296386719, Learning Rate: 0.01\n",
      "Epoch [2534/10000], Loss: 98.891845703125, Learning Rate: 0.01\n",
      "Epoch [2535/10000], Loss: 98.88232421875, Learning Rate: 0.01\n",
      "Epoch [2536/10000], Loss: 98.87268829345703, Learning Rate: 0.01\n",
      "Epoch [2537/10000], Loss: 98.86320495605469, Learning Rate: 0.01\n",
      "Epoch [2538/10000], Loss: 98.8535385131836, Learning Rate: 0.01\n",
      "Epoch [2539/10000], Loss: 98.84415435791016, Learning Rate: 0.01\n",
      "Epoch [2540/10000], Loss: 98.83450317382812, Learning Rate: 0.01\n",
      "Epoch [2541/10000], Loss: 98.82499694824219, Learning Rate: 0.01\n",
      "Epoch [2542/10000], Loss: 98.81544494628906, Learning Rate: 0.01\n",
      "Epoch [2543/10000], Loss: 98.8058853149414, Learning Rate: 0.01\n",
      "Epoch [2544/10000], Loss: 98.79641723632812, Learning Rate: 0.01\n",
      "Epoch [2545/10000], Loss: 98.78678894042969, Learning Rate: 0.01\n",
      "Epoch [2546/10000], Loss: 98.7771987915039, Learning Rate: 0.01\n",
      "Epoch [2547/10000], Loss: 98.76781463623047, Learning Rate: 0.01\n",
      "Epoch [2548/10000], Loss: 98.75829315185547, Learning Rate: 0.01\n",
      "Epoch [2549/10000], Loss: 98.74872589111328, Learning Rate: 0.01\n",
      "Epoch [2550/10000], Loss: 98.73919677734375, Learning Rate: 0.01\n",
      "Epoch [2551/10000], Loss: 98.72966003417969, Learning Rate: 0.01\n",
      "Epoch [2552/10000], Loss: 98.7200927734375, Learning Rate: 0.01\n",
      "Epoch [2553/10000], Loss: 98.71057891845703, Learning Rate: 0.01\n",
      "Epoch [2554/10000], Loss: 98.70108032226562, Learning Rate: 0.01\n",
      "Epoch [2555/10000], Loss: 98.69154357910156, Learning Rate: 0.01\n",
      "Epoch [2556/10000], Loss: 98.6820068359375, Learning Rate: 0.01\n",
      "Epoch [2557/10000], Loss: 98.67252349853516, Learning Rate: 0.01\n",
      "Epoch [2558/10000], Loss: 98.66294860839844, Learning Rate: 0.01\n",
      "Epoch [2559/10000], Loss: 98.6534652709961, Learning Rate: 0.01\n",
      "Epoch [2560/10000], Loss: 98.64388275146484, Learning Rate: 0.01\n",
      "Epoch [2561/10000], Loss: 98.63440704345703, Learning Rate: 0.01\n",
      "Epoch [2562/10000], Loss: 98.62486267089844, Learning Rate: 0.01\n",
      "Epoch [2563/10000], Loss: 98.615478515625, Learning Rate: 0.01\n",
      "Epoch [2564/10000], Loss: 98.60595703125, Learning Rate: 0.01\n",
      "Epoch [2565/10000], Loss: 98.59632110595703, Learning Rate: 0.01\n",
      "Epoch [2566/10000], Loss: 98.5868148803711, Learning Rate: 0.01\n",
      "Epoch [2567/10000], Loss: 98.5774154663086, Learning Rate: 0.01\n",
      "Epoch [2568/10000], Loss: 98.56792449951172, Learning Rate: 0.01\n",
      "Epoch [2569/10000], Loss: 98.55838012695312, Learning Rate: 0.01\n",
      "Epoch [2570/10000], Loss: 98.54884338378906, Learning Rate: 0.01\n",
      "Epoch [2571/10000], Loss: 98.5392837524414, Learning Rate: 0.01\n",
      "Epoch [2572/10000], Loss: 98.52991485595703, Learning Rate: 0.01\n",
      "Epoch [2573/10000], Loss: 98.52037048339844, Learning Rate: 0.01\n",
      "Epoch [2574/10000], Loss: 98.51087951660156, Learning Rate: 0.01\n",
      "Epoch [2575/10000], Loss: 98.50139617919922, Learning Rate: 0.01\n",
      "Epoch [2576/10000], Loss: 98.49185180664062, Learning Rate: 0.01\n",
      "Epoch [2577/10000], Loss: 98.48237609863281, Learning Rate: 0.01\n",
      "Epoch [2578/10000], Loss: 98.47291564941406, Learning Rate: 0.01\n",
      "Epoch [2579/10000], Loss: 98.46342468261719, Learning Rate: 0.01\n",
      "Epoch [2580/10000], Loss: 98.45391845703125, Learning Rate: 0.01\n",
      "Epoch [2581/10000], Loss: 98.44444274902344, Learning Rate: 0.01\n",
      "Epoch [2582/10000], Loss: 98.43500518798828, Learning Rate: 0.01\n",
      "Epoch [2583/10000], Loss: 98.42547607421875, Learning Rate: 0.01\n",
      "Epoch [2584/10000], Loss: 98.41604614257812, Learning Rate: 0.01\n",
      "Epoch [2585/10000], Loss: 98.40650177001953, Learning Rate: 0.01\n",
      "Epoch [2586/10000], Loss: 98.39707946777344, Learning Rate: 0.01\n",
      "Epoch [2587/10000], Loss: 98.38763427734375, Learning Rate: 0.01\n",
      "Epoch [2588/10000], Loss: 98.37812805175781, Learning Rate: 0.01\n",
      "Epoch [2589/10000], Loss: 98.36859130859375, Learning Rate: 0.01\n",
      "Epoch [2590/10000], Loss: 98.35911560058594, Learning Rate: 0.01\n",
      "Epoch [2591/10000], Loss: 98.34967041015625, Learning Rate: 0.01\n",
      "Epoch [2592/10000], Loss: 98.34024047851562, Learning Rate: 0.01\n",
      "Epoch [2593/10000], Loss: 98.33071899414062, Learning Rate: 0.01\n",
      "Epoch [2594/10000], Loss: 98.32122802734375, Learning Rate: 0.01\n",
      "Epoch [2595/10000], Loss: 98.31183624267578, Learning Rate: 0.01\n",
      "Epoch [2596/10000], Loss: 98.3023452758789, Learning Rate: 0.01\n",
      "Epoch [2597/10000], Loss: 98.29280853271484, Learning Rate: 0.01\n",
      "Epoch [2598/10000], Loss: 98.28340148925781, Learning Rate: 0.01\n",
      "Epoch [2599/10000], Loss: 98.27399444580078, Learning Rate: 0.01\n",
      "Epoch [2600/10000], Loss: 98.264404296875, Learning Rate: 0.01\n",
      "Epoch [2601/10000], Loss: 98.255126953125, Learning Rate: 0.01\n",
      "Epoch [2602/10000], Loss: 98.24559783935547, Learning Rate: 0.01\n",
      "Epoch [2603/10000], Loss: 98.23611450195312, Learning Rate: 0.01\n",
      "Epoch [2604/10000], Loss: 98.22666931152344, Learning Rate: 0.01\n",
      "Epoch [2605/10000], Loss: 98.21730041503906, Learning Rate: 0.01\n",
      "Epoch [2606/10000], Loss: 98.20777130126953, Learning Rate: 0.01\n",
      "Epoch [2607/10000], Loss: 98.19837188720703, Learning Rate: 0.01\n",
      "Epoch [2608/10000], Loss: 98.1888656616211, Learning Rate: 0.01\n",
      "Epoch [2609/10000], Loss: 98.17945098876953, Learning Rate: 0.01\n",
      "Epoch [2610/10000], Loss: 98.16999816894531, Learning Rate: 0.01\n",
      "Epoch [2611/10000], Loss: 98.16050720214844, Learning Rate: 0.01\n",
      "Epoch [2612/10000], Loss: 98.151123046875, Learning Rate: 0.01\n",
      "Epoch [2613/10000], Loss: 98.14165496826172, Learning Rate: 0.01\n",
      "Epoch [2614/10000], Loss: 98.13227844238281, Learning Rate: 0.01\n",
      "Epoch [2615/10000], Loss: 98.12278747558594, Learning Rate: 0.01\n",
      "Epoch [2616/10000], Loss: 98.11329650878906, Learning Rate: 0.01\n",
      "Epoch [2617/10000], Loss: 98.10399627685547, Learning Rate: 0.01\n",
      "Epoch [2618/10000], Loss: 98.09454345703125, Learning Rate: 0.01\n",
      "Epoch [2619/10000], Loss: 98.0850830078125, Learning Rate: 0.01\n",
      "Epoch [2620/10000], Loss: 98.07569885253906, Learning Rate: 0.01\n",
      "Epoch [2621/10000], Loss: 98.0662841796875, Learning Rate: 0.01\n",
      "Epoch [2622/10000], Loss: 98.05675506591797, Learning Rate: 0.01\n",
      "Epoch [2623/10000], Loss: 98.0473861694336, Learning Rate: 0.01\n",
      "Epoch [2624/10000], Loss: 98.03789520263672, Learning Rate: 0.01\n",
      "Epoch [2625/10000], Loss: 98.0285415649414, Learning Rate: 0.01\n",
      "Epoch [2626/10000], Loss: 98.01919555664062, Learning Rate: 0.01\n",
      "Epoch [2627/10000], Loss: 98.00971221923828, Learning Rate: 0.01\n",
      "Epoch [2628/10000], Loss: 98.00025177001953, Learning Rate: 0.01\n",
      "Epoch [2629/10000], Loss: 97.99085235595703, Learning Rate: 0.01\n",
      "Epoch [2630/10000], Loss: 97.98147583007812, Learning Rate: 0.01\n",
      "Epoch [2631/10000], Loss: 97.97203826904297, Learning Rate: 0.01\n",
      "Epoch [2632/10000], Loss: 97.962646484375, Learning Rate: 0.01\n",
      "Epoch [2633/10000], Loss: 97.95314025878906, Learning Rate: 0.01\n",
      "Epoch [2634/10000], Loss: 97.94389343261719, Learning Rate: 0.01\n",
      "Epoch [2635/10000], Loss: 97.93437194824219, Learning Rate: 0.01\n",
      "Epoch [2636/10000], Loss: 97.92505645751953, Learning Rate: 0.01\n",
      "Epoch [2637/10000], Loss: 97.91554260253906, Learning Rate: 0.01\n",
      "Epoch [2638/10000], Loss: 97.9061279296875, Learning Rate: 0.01\n",
      "Epoch [2639/10000], Loss: 97.89686584472656, Learning Rate: 0.01\n",
      "Epoch [2640/10000], Loss: 97.88740539550781, Learning Rate: 0.01\n",
      "Epoch [2641/10000], Loss: 97.8779296875, Learning Rate: 0.01\n",
      "Epoch [2642/10000], Loss: 97.86858367919922, Learning Rate: 0.01\n",
      "Epoch [2643/10000], Loss: 97.85923767089844, Learning Rate: 0.01\n",
      "Epoch [2644/10000], Loss: 97.84977722167969, Learning Rate: 0.01\n",
      "Epoch [2645/10000], Loss: 97.84037780761719, Learning Rate: 0.01\n",
      "Epoch [2646/10000], Loss: 97.83096313476562, Learning Rate: 0.01\n",
      "Epoch [2647/10000], Loss: 97.8216323852539, Learning Rate: 0.01\n",
      "Epoch [2648/10000], Loss: 97.8122329711914, Learning Rate: 0.01\n",
      "Epoch [2649/10000], Loss: 97.80281066894531, Learning Rate: 0.01\n",
      "Epoch [2650/10000], Loss: 97.79346466064453, Learning Rate: 0.01\n",
      "Epoch [2651/10000], Loss: 97.78404998779297, Learning Rate: 0.01\n",
      "Epoch [2652/10000], Loss: 97.77470397949219, Learning Rate: 0.01\n",
      "Epoch [2653/10000], Loss: 97.76528930664062, Learning Rate: 0.01\n",
      "Epoch [2654/10000], Loss: 97.75593566894531, Learning Rate: 0.01\n",
      "Epoch [2655/10000], Loss: 97.7465591430664, Learning Rate: 0.01\n",
      "Epoch [2656/10000], Loss: 97.73715209960938, Learning Rate: 0.01\n",
      "Epoch [2657/10000], Loss: 97.7278060913086, Learning Rate: 0.01\n",
      "Epoch [2658/10000], Loss: 97.71849060058594, Learning Rate: 0.01\n",
      "Epoch [2659/10000], Loss: 97.70906066894531, Learning Rate: 0.01\n",
      "Epoch [2660/10000], Loss: 97.69974517822266, Learning Rate: 0.01\n",
      "Epoch [2661/10000], Loss: 97.69026184082031, Learning Rate: 0.01\n",
      "Epoch [2662/10000], Loss: 97.68093872070312, Learning Rate: 0.01\n",
      "Epoch [2663/10000], Loss: 97.67156982421875, Learning Rate: 0.01\n",
      "Epoch [2664/10000], Loss: 97.66215515136719, Learning Rate: 0.01\n",
      "Epoch [2665/10000], Loss: 97.6528091430664, Learning Rate: 0.01\n",
      "Epoch [2666/10000], Loss: 97.6434555053711, Learning Rate: 0.01\n",
      "Epoch [2667/10000], Loss: 97.63410186767578, Learning Rate: 0.01\n",
      "Epoch [2668/10000], Loss: 97.62474060058594, Learning Rate: 0.01\n",
      "Epoch [2669/10000], Loss: 97.6153564453125, Learning Rate: 0.01\n",
      "Epoch [2670/10000], Loss: 97.60601806640625, Learning Rate: 0.01\n",
      "Epoch [2671/10000], Loss: 97.59660339355469, Learning Rate: 0.01\n",
      "Epoch [2672/10000], Loss: 97.58737182617188, Learning Rate: 0.01\n",
      "Epoch [2673/10000], Loss: 97.57804107666016, Learning Rate: 0.01\n",
      "Epoch [2674/10000], Loss: 97.56859588623047, Learning Rate: 0.01\n",
      "Epoch [2675/10000], Loss: 97.55923461914062, Learning Rate: 0.01\n",
      "Epoch [2676/10000], Loss: 97.54994201660156, Learning Rate: 0.01\n",
      "Epoch [2677/10000], Loss: 97.54051971435547, Learning Rate: 0.01\n",
      "Epoch [2678/10000], Loss: 97.53121185302734, Learning Rate: 0.01\n",
      "Epoch [2679/10000], Loss: 97.52184295654297, Learning Rate: 0.01\n",
      "Epoch [2680/10000], Loss: 97.51251983642578, Learning Rate: 0.01\n",
      "Epoch [2681/10000], Loss: 97.50305938720703, Learning Rate: 0.01\n",
      "Epoch [2682/10000], Loss: 97.49381256103516, Learning Rate: 0.01\n",
      "Epoch [2683/10000], Loss: 97.48442077636719, Learning Rate: 0.01\n",
      "Epoch [2684/10000], Loss: 97.47512817382812, Learning Rate: 0.01\n",
      "Epoch [2685/10000], Loss: 97.46577453613281, Learning Rate: 0.01\n",
      "Epoch [2686/10000], Loss: 97.45648956298828, Learning Rate: 0.01\n",
      "Epoch [2687/10000], Loss: 97.44717407226562, Learning Rate: 0.01\n",
      "Epoch [2688/10000], Loss: 97.43780517578125, Learning Rate: 0.01\n",
      "Epoch [2689/10000], Loss: 97.42845153808594, Learning Rate: 0.01\n",
      "Epoch [2690/10000], Loss: 97.41914367675781, Learning Rate: 0.01\n",
      "Epoch [2691/10000], Loss: 97.40978240966797, Learning Rate: 0.01\n",
      "Epoch [2692/10000], Loss: 97.40049743652344, Learning Rate: 0.01\n",
      "Epoch [2693/10000], Loss: 97.39110565185547, Learning Rate: 0.01\n",
      "Epoch [2694/10000], Loss: 97.38186645507812, Learning Rate: 0.01\n",
      "Epoch [2695/10000], Loss: 97.37247467041016, Learning Rate: 0.01\n",
      "Epoch [2696/10000], Loss: 97.36323547363281, Learning Rate: 0.01\n",
      "Epoch [2697/10000], Loss: 97.35386657714844, Learning Rate: 0.01\n",
      "Epoch [2698/10000], Loss: 97.3445816040039, Learning Rate: 0.01\n",
      "Epoch [2699/10000], Loss: 97.33518981933594, Learning Rate: 0.01\n",
      "Epoch [2700/10000], Loss: 97.32588195800781, Learning Rate: 0.01\n",
      "Epoch [2701/10000], Loss: 97.31658935546875, Learning Rate: 0.01\n",
      "Epoch [2702/10000], Loss: 97.30719757080078, Learning Rate: 0.01\n",
      "Epoch [2703/10000], Loss: 97.29798126220703, Learning Rate: 0.01\n",
      "Epoch [2704/10000], Loss: 97.28873443603516, Learning Rate: 0.01\n",
      "Epoch [2705/10000], Loss: 97.27937316894531, Learning Rate: 0.01\n",
      "Epoch [2706/10000], Loss: 97.27008056640625, Learning Rate: 0.01\n",
      "Epoch [2707/10000], Loss: 97.2607421875, Learning Rate: 0.01\n",
      "Epoch [2708/10000], Loss: 97.2514877319336, Learning Rate: 0.01\n",
      "Epoch [2709/10000], Loss: 97.24201965332031, Learning Rate: 0.01\n",
      "Epoch [2710/10000], Loss: 97.23279571533203, Learning Rate: 0.01\n",
      "Epoch [2711/10000], Loss: 97.22347259521484, Learning Rate: 0.01\n",
      "Epoch [2712/10000], Loss: 97.21422576904297, Learning Rate: 0.01\n",
      "Epoch [2713/10000], Loss: 97.20484924316406, Learning Rate: 0.01\n",
      "Epoch [2714/10000], Loss: 97.1956787109375, Learning Rate: 0.01\n",
      "Epoch [2715/10000], Loss: 97.18630981445312, Learning Rate: 0.01\n",
      "Epoch [2716/10000], Loss: 97.17697143554688, Learning Rate: 0.01\n",
      "Epoch [2717/10000], Loss: 97.167724609375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2718/10000], Loss: 97.15841674804688, Learning Rate: 0.01\n",
      "Epoch [2719/10000], Loss: 97.14916229248047, Learning Rate: 0.01\n",
      "Epoch [2720/10000], Loss: 97.13984680175781, Learning Rate: 0.01\n",
      "Epoch [2721/10000], Loss: 97.13058471679688, Learning Rate: 0.01\n",
      "Epoch [2722/10000], Loss: 97.12127685546875, Learning Rate: 0.01\n",
      "Epoch [2723/10000], Loss: 97.11205291748047, Learning Rate: 0.01\n",
      "Epoch [2724/10000], Loss: 97.10271453857422, Learning Rate: 0.01\n",
      "Epoch [2725/10000], Loss: 97.09344482421875, Learning Rate: 0.01\n",
      "Epoch [2726/10000], Loss: 97.08419036865234, Learning Rate: 0.01\n",
      "Epoch [2727/10000], Loss: 97.07486724853516, Learning Rate: 0.01\n",
      "Epoch [2728/10000], Loss: 97.06553649902344, Learning Rate: 0.01\n",
      "Epoch [2729/10000], Loss: 97.05632781982422, Learning Rate: 0.01\n",
      "Epoch [2730/10000], Loss: 97.04695129394531, Learning Rate: 0.01\n",
      "Epoch [2731/10000], Loss: 97.03768920898438, Learning Rate: 0.01\n",
      "Epoch [2732/10000], Loss: 97.02845001220703, Learning Rate: 0.01\n",
      "Epoch [2733/10000], Loss: 97.01919555664062, Learning Rate: 0.01\n",
      "Epoch [2734/10000], Loss: 97.01001739501953, Learning Rate: 0.01\n",
      "Epoch [2735/10000], Loss: 97.00055694580078, Learning Rate: 0.01\n",
      "Epoch [2736/10000], Loss: 96.9913558959961, Learning Rate: 0.01\n",
      "Epoch [2737/10000], Loss: 96.98210144042969, Learning Rate: 0.01\n",
      "Epoch [2738/10000], Loss: 96.97283172607422, Learning Rate: 0.01\n",
      "Epoch [2739/10000], Loss: 96.96355438232422, Learning Rate: 0.01\n",
      "Epoch [2740/10000], Loss: 96.95436096191406, Learning Rate: 0.01\n",
      "Epoch [2741/10000], Loss: 96.945068359375, Learning Rate: 0.01\n",
      "Epoch [2742/10000], Loss: 96.93579864501953, Learning Rate: 0.01\n",
      "Epoch [2743/10000], Loss: 96.92654418945312, Learning Rate: 0.01\n",
      "Epoch [2744/10000], Loss: 96.91722869873047, Learning Rate: 0.01\n",
      "Epoch [2745/10000], Loss: 96.90802001953125, Learning Rate: 0.01\n",
      "Epoch [2746/10000], Loss: 96.89886474609375, Learning Rate: 0.01\n",
      "Epoch [2747/10000], Loss: 96.88957977294922, Learning Rate: 0.01\n",
      "Epoch [2748/10000], Loss: 96.8802490234375, Learning Rate: 0.01\n",
      "Epoch [2749/10000], Loss: 96.87098693847656, Learning Rate: 0.01\n",
      "Epoch [2750/10000], Loss: 96.86177062988281, Learning Rate: 0.01\n",
      "Epoch [2751/10000], Loss: 96.85250091552734, Learning Rate: 0.01\n",
      "Epoch [2752/10000], Loss: 96.84322357177734, Learning Rate: 0.01\n",
      "Epoch [2753/10000], Loss: 96.83403015136719, Learning Rate: 0.01\n",
      "Epoch [2754/10000], Loss: 96.82479858398438, Learning Rate: 0.01\n",
      "Epoch [2755/10000], Loss: 96.81556701660156, Learning Rate: 0.01\n",
      "Epoch [2756/10000], Loss: 96.8062973022461, Learning Rate: 0.01\n",
      "Epoch [2757/10000], Loss: 96.7970962524414, Learning Rate: 0.01\n",
      "Epoch [2758/10000], Loss: 96.78781127929688, Learning Rate: 0.01\n",
      "Epoch [2759/10000], Loss: 96.77857971191406, Learning Rate: 0.01\n",
      "Epoch [2760/10000], Loss: 96.76933288574219, Learning Rate: 0.01\n",
      "Epoch [2761/10000], Loss: 96.76007843017578, Learning Rate: 0.01\n",
      "Epoch [2762/10000], Loss: 96.75094604492188, Learning Rate: 0.01\n",
      "Epoch [2763/10000], Loss: 96.74166107177734, Learning Rate: 0.01\n",
      "Epoch [2764/10000], Loss: 96.73242950439453, Learning Rate: 0.01\n",
      "Epoch [2765/10000], Loss: 96.72319030761719, Learning Rate: 0.01\n",
      "Epoch [2766/10000], Loss: 96.71395874023438, Learning Rate: 0.01\n",
      "Epoch [2767/10000], Loss: 96.70471954345703, Learning Rate: 0.01\n",
      "Epoch [2768/10000], Loss: 96.69547271728516, Learning Rate: 0.01\n",
      "Epoch [2769/10000], Loss: 96.6862564086914, Learning Rate: 0.01\n",
      "Epoch [2770/10000], Loss: 96.67695617675781, Learning Rate: 0.01\n",
      "Epoch [2771/10000], Loss: 96.66778564453125, Learning Rate: 0.01\n",
      "Epoch [2772/10000], Loss: 96.65853118896484, Learning Rate: 0.01\n",
      "Epoch [2773/10000], Loss: 96.64942169189453, Learning Rate: 0.01\n",
      "Epoch [2774/10000], Loss: 96.64022827148438, Learning Rate: 0.01\n",
      "Epoch [2775/10000], Loss: 96.63096618652344, Learning Rate: 0.01\n",
      "Epoch [2776/10000], Loss: 96.62168884277344, Learning Rate: 0.01\n",
      "Epoch [2777/10000], Loss: 96.61253356933594, Learning Rate: 0.01\n",
      "Epoch [2778/10000], Loss: 96.60328674316406, Learning Rate: 0.01\n",
      "Epoch [2779/10000], Loss: 96.59400177001953, Learning Rate: 0.01\n",
      "Epoch [2780/10000], Loss: 96.5848617553711, Learning Rate: 0.01\n",
      "Epoch [2781/10000], Loss: 96.57559967041016, Learning Rate: 0.01\n",
      "Epoch [2782/10000], Loss: 96.56636047363281, Learning Rate: 0.01\n",
      "Epoch [2783/10000], Loss: 96.5572738647461, Learning Rate: 0.01\n",
      "Epoch [2784/10000], Loss: 96.5481185913086, Learning Rate: 0.01\n",
      "Epoch [2785/10000], Loss: 96.53875732421875, Learning Rate: 0.01\n",
      "Epoch [2786/10000], Loss: 96.52963256835938, Learning Rate: 0.01\n",
      "Epoch [2787/10000], Loss: 96.52044677734375, Learning Rate: 0.01\n",
      "Epoch [2788/10000], Loss: 96.51129913330078, Learning Rate: 0.01\n",
      "Epoch [2789/10000], Loss: 96.50198364257812, Learning Rate: 0.01\n",
      "Epoch [2790/10000], Loss: 96.4928207397461, Learning Rate: 0.01\n",
      "Epoch [2791/10000], Loss: 96.48365783691406, Learning Rate: 0.01\n",
      "Epoch [2792/10000], Loss: 96.4744873046875, Learning Rate: 0.01\n",
      "Epoch [2793/10000], Loss: 96.46528625488281, Learning Rate: 0.01\n",
      "Epoch [2794/10000], Loss: 96.45606231689453, Learning Rate: 0.01\n",
      "Epoch [2795/10000], Loss: 96.44686126708984, Learning Rate: 0.01\n",
      "Epoch [2796/10000], Loss: 96.43763732910156, Learning Rate: 0.01\n",
      "Epoch [2797/10000], Loss: 96.42852020263672, Learning Rate: 0.01\n",
      "Epoch [2798/10000], Loss: 96.4193344116211, Learning Rate: 0.01\n",
      "Epoch [2799/10000], Loss: 96.4101333618164, Learning Rate: 0.01\n",
      "Epoch [2800/10000], Loss: 96.4009780883789, Learning Rate: 0.01\n",
      "Epoch [2801/10000], Loss: 96.39173126220703, Learning Rate: 0.01\n",
      "Epoch [2802/10000], Loss: 96.38255310058594, Learning Rate: 0.01\n",
      "Epoch [2803/10000], Loss: 96.37342834472656, Learning Rate: 0.01\n",
      "Epoch [2804/10000], Loss: 96.36417388916016, Learning Rate: 0.01\n",
      "Epoch [2805/10000], Loss: 96.35503387451172, Learning Rate: 0.01\n",
      "Epoch [2806/10000], Loss: 96.34587097167969, Learning Rate: 0.01\n",
      "Epoch [2807/10000], Loss: 96.33665466308594, Learning Rate: 0.01\n",
      "Epoch [2808/10000], Loss: 96.32750701904297, Learning Rate: 0.01\n",
      "Epoch [2809/10000], Loss: 96.31832122802734, Learning Rate: 0.01\n",
      "Epoch [2810/10000], Loss: 96.30914306640625, Learning Rate: 0.01\n",
      "Epoch [2811/10000], Loss: 96.29999542236328, Learning Rate: 0.01\n",
      "Epoch [2812/10000], Loss: 96.29080963134766, Learning Rate: 0.01\n",
      "Epoch [2813/10000], Loss: 96.28166198730469, Learning Rate: 0.01\n",
      "Epoch [2814/10000], Loss: 96.27245330810547, Learning Rate: 0.01\n",
      "Epoch [2815/10000], Loss: 96.26333618164062, Learning Rate: 0.01\n",
      "Epoch [2816/10000], Loss: 96.25418090820312, Learning Rate: 0.01\n",
      "Epoch [2817/10000], Loss: 96.24494934082031, Learning Rate: 0.01\n",
      "Epoch [2818/10000], Loss: 96.23591613769531, Learning Rate: 0.01\n",
      "Epoch [2819/10000], Loss: 96.2267074584961, Learning Rate: 0.01\n",
      "Epoch [2820/10000], Loss: 96.21751403808594, Learning Rate: 0.01\n",
      "Epoch [2821/10000], Loss: 96.20836639404297, Learning Rate: 0.01\n",
      "Epoch [2822/10000], Loss: 96.19917297363281, Learning Rate: 0.01\n",
      "Epoch [2823/10000], Loss: 96.19004821777344, Learning Rate: 0.01\n",
      "Epoch [2824/10000], Loss: 96.18093872070312, Learning Rate: 0.01\n",
      "Epoch [2825/10000], Loss: 96.17178344726562, Learning Rate: 0.01\n",
      "Epoch [2826/10000], Loss: 96.1626205444336, Learning Rate: 0.01\n",
      "Epoch [2827/10000], Loss: 96.15343475341797, Learning Rate: 0.01\n",
      "Epoch [2828/10000], Loss: 96.1442642211914, Learning Rate: 0.01\n",
      "Epoch [2829/10000], Loss: 96.13507843017578, Learning Rate: 0.01\n",
      "Epoch [2830/10000], Loss: 96.1259994506836, Learning Rate: 0.01\n",
      "Epoch [2831/10000], Loss: 96.11682891845703, Learning Rate: 0.01\n",
      "Epoch [2832/10000], Loss: 96.1076889038086, Learning Rate: 0.01\n",
      "Epoch [2833/10000], Loss: 96.09857940673828, Learning Rate: 0.01\n",
      "Epoch [2834/10000], Loss: 96.08943939208984, Learning Rate: 0.01\n",
      "Epoch [2835/10000], Loss: 96.08030700683594, Learning Rate: 0.01\n",
      "Epoch [2836/10000], Loss: 96.07113647460938, Learning Rate: 0.01\n",
      "Epoch [2837/10000], Loss: 96.06197357177734, Learning Rate: 0.01\n",
      "Epoch [2838/10000], Loss: 96.05290222167969, Learning Rate: 0.01\n",
      "Epoch [2839/10000], Loss: 96.04366302490234, Learning Rate: 0.01\n",
      "Epoch [2840/10000], Loss: 96.03469848632812, Learning Rate: 0.01\n",
      "Epoch [2841/10000], Loss: 96.025390625, Learning Rate: 0.01\n",
      "Epoch [2842/10000], Loss: 96.01628875732422, Learning Rate: 0.01\n",
      "Epoch [2843/10000], Loss: 96.00718688964844, Learning Rate: 0.01\n",
      "Epoch [2844/10000], Loss: 95.9980697631836, Learning Rate: 0.01\n",
      "Epoch [2845/10000], Loss: 95.98896026611328, Learning Rate: 0.01\n",
      "Epoch [2846/10000], Loss: 95.97978210449219, Learning Rate: 0.01\n",
      "Epoch [2847/10000], Loss: 95.9706802368164, Learning Rate: 0.01\n",
      "Epoch [2848/10000], Loss: 95.96148681640625, Learning Rate: 0.01\n",
      "Epoch [2849/10000], Loss: 95.95244598388672, Learning Rate: 0.01\n",
      "Epoch [2850/10000], Loss: 95.94329833984375, Learning Rate: 0.01\n",
      "Epoch [2851/10000], Loss: 95.9342041015625, Learning Rate: 0.01\n",
      "Epoch [2852/10000], Loss: 95.92513275146484, Learning Rate: 0.01\n",
      "Epoch [2853/10000], Loss: 95.9159164428711, Learning Rate: 0.01\n",
      "Epoch [2854/10000], Loss: 95.90684509277344, Learning Rate: 0.01\n",
      "Epoch [2855/10000], Loss: 95.89775085449219, Learning Rate: 0.01\n",
      "Epoch [2856/10000], Loss: 95.88862609863281, Learning Rate: 0.01\n",
      "Epoch [2857/10000], Loss: 95.87944030761719, Learning Rate: 0.01\n",
      "Epoch [2858/10000], Loss: 95.87040710449219, Learning Rate: 0.01\n",
      "Epoch [2859/10000], Loss: 95.86125183105469, Learning Rate: 0.01\n",
      "Epoch [2860/10000], Loss: 95.8521499633789, Learning Rate: 0.01\n",
      "Epoch [2861/10000], Loss: 95.84303283691406, Learning Rate: 0.01\n",
      "Epoch [2862/10000], Loss: 95.833984375, Learning Rate: 0.01\n",
      "Epoch [2863/10000], Loss: 95.82476806640625, Learning Rate: 0.01\n",
      "Epoch [2864/10000], Loss: 95.81572723388672, Learning Rate: 0.01\n",
      "Epoch [2865/10000], Loss: 95.80661010742188, Learning Rate: 0.01\n",
      "Epoch [2866/10000], Loss: 95.79751586914062, Learning Rate: 0.01\n",
      "Epoch [2867/10000], Loss: 95.7884292602539, Learning Rate: 0.01\n",
      "Epoch [2868/10000], Loss: 95.77926635742188, Learning Rate: 0.01\n",
      "Epoch [2869/10000], Loss: 95.7701416015625, Learning Rate: 0.01\n",
      "Epoch [2870/10000], Loss: 95.76119232177734, Learning Rate: 0.01\n",
      "Epoch [2871/10000], Loss: 95.75200653076172, Learning Rate: 0.01\n",
      "Epoch [2872/10000], Loss: 95.74290466308594, Learning Rate: 0.01\n",
      "Epoch [2873/10000], Loss: 95.7337646484375, Learning Rate: 0.01\n",
      "Epoch [2874/10000], Loss: 95.72477722167969, Learning Rate: 0.01\n",
      "Epoch [2875/10000], Loss: 95.71562194824219, Learning Rate: 0.01\n",
      "Epoch [2876/10000], Loss: 95.70655059814453, Learning Rate: 0.01\n",
      "Epoch [2877/10000], Loss: 95.69747924804688, Learning Rate: 0.01\n",
      "Epoch [2878/10000], Loss: 95.68840789794922, Learning Rate: 0.01\n",
      "Epoch [2879/10000], Loss: 95.6792984008789, Learning Rate: 0.01\n",
      "Epoch [2880/10000], Loss: 95.670166015625, Learning Rate: 0.01\n",
      "Epoch [2881/10000], Loss: 95.66108703613281, Learning Rate: 0.01\n",
      "Epoch [2882/10000], Loss: 95.6520004272461, Learning Rate: 0.01\n",
      "Epoch [2883/10000], Loss: 95.64300537109375, Learning Rate: 0.01\n",
      "Epoch [2884/10000], Loss: 95.63389587402344, Learning Rate: 0.01\n",
      "Epoch [2885/10000], Loss: 95.62481689453125, Learning Rate: 0.01\n",
      "Epoch [2886/10000], Loss: 95.61572265625, Learning Rate: 0.01\n",
      "Epoch [2887/10000], Loss: 95.60663604736328, Learning Rate: 0.01\n",
      "Epoch [2888/10000], Loss: 95.59761047363281, Learning Rate: 0.01\n",
      "Epoch [2889/10000], Loss: 95.58844757080078, Learning Rate: 0.01\n",
      "Epoch [2890/10000], Loss: 95.57939147949219, Learning Rate: 0.01\n",
      "Epoch [2891/10000], Loss: 95.57035827636719, Learning Rate: 0.01\n",
      "Epoch [2892/10000], Loss: 95.56129455566406, Learning Rate: 0.01\n",
      "Epoch [2893/10000], Loss: 95.5522689819336, Learning Rate: 0.01\n",
      "Epoch [2894/10000], Loss: 95.54310607910156, Learning Rate: 0.01\n",
      "Epoch [2895/10000], Loss: 95.53409576416016, Learning Rate: 0.01\n",
      "Epoch [2896/10000], Loss: 95.52496337890625, Learning Rate: 0.01\n",
      "Epoch [2897/10000], Loss: 95.5159683227539, Learning Rate: 0.01\n",
      "Epoch [2898/10000], Loss: 95.50688934326172, Learning Rate: 0.01\n",
      "Epoch [2899/10000], Loss: 95.49784088134766, Learning Rate: 0.01\n",
      "Epoch [2900/10000], Loss: 95.48877716064453, Learning Rate: 0.01\n",
      "Epoch [2901/10000], Loss: 95.47964477539062, Learning Rate: 0.01\n",
      "Epoch [2902/10000], Loss: 95.47063446044922, Learning Rate: 0.01\n",
      "Epoch [2903/10000], Loss: 95.46162414550781, Learning Rate: 0.01\n",
      "Epoch [2904/10000], Loss: 95.45252990722656, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2905/10000], Loss: 95.44338989257812, Learning Rate: 0.01\n",
      "Epoch [2906/10000], Loss: 95.4344482421875, Learning Rate: 0.01\n",
      "Epoch [2907/10000], Loss: 95.42534637451172, Learning Rate: 0.01\n",
      "Epoch [2908/10000], Loss: 95.41634368896484, Learning Rate: 0.01\n",
      "Epoch [2909/10000], Loss: 95.40725708007812, Learning Rate: 0.01\n",
      "Epoch [2910/10000], Loss: 95.39820098876953, Learning Rate: 0.01\n",
      "Epoch [2911/10000], Loss: 95.38916015625, Learning Rate: 0.01\n",
      "Epoch [2912/10000], Loss: 95.38006591796875, Learning Rate: 0.01\n",
      "Epoch [2913/10000], Loss: 95.37116241455078, Learning Rate: 0.01\n",
      "Epoch [2914/10000], Loss: 95.36199188232422, Learning Rate: 0.01\n",
      "Epoch [2915/10000], Loss: 95.35295867919922, Learning Rate: 0.01\n",
      "Epoch [2916/10000], Loss: 95.34390258789062, Learning Rate: 0.01\n",
      "Epoch [2917/10000], Loss: 95.3349838256836, Learning Rate: 0.01\n",
      "Epoch [2918/10000], Loss: 95.32587432861328, Learning Rate: 0.01\n",
      "Epoch [2919/10000], Loss: 95.31688690185547, Learning Rate: 0.01\n",
      "Epoch [2920/10000], Loss: 95.30784606933594, Learning Rate: 0.01\n",
      "Epoch [2921/10000], Loss: 95.29883575439453, Learning Rate: 0.01\n",
      "Epoch [2922/10000], Loss: 95.2897720336914, Learning Rate: 0.01\n",
      "Epoch [2923/10000], Loss: 95.28068542480469, Learning Rate: 0.01\n",
      "Epoch [2924/10000], Loss: 95.27165222167969, Learning Rate: 0.01\n",
      "Epoch [2925/10000], Loss: 95.26256561279297, Learning Rate: 0.01\n",
      "Epoch [2926/10000], Loss: 95.25354766845703, Learning Rate: 0.01\n",
      "Epoch [2927/10000], Loss: 95.24461364746094, Learning Rate: 0.01\n",
      "Epoch [2928/10000], Loss: 95.23554229736328, Learning Rate: 0.01\n",
      "Epoch [2929/10000], Loss: 95.22648620605469, Learning Rate: 0.01\n",
      "Epoch [2930/10000], Loss: 95.21749114990234, Learning Rate: 0.01\n",
      "Epoch [2931/10000], Loss: 95.20834350585938, Learning Rate: 0.01\n",
      "Epoch [2932/10000], Loss: 95.19952392578125, Learning Rate: 0.01\n",
      "Epoch [2933/10000], Loss: 95.19042205810547, Learning Rate: 0.01\n",
      "Epoch [2934/10000], Loss: 95.181396484375, Learning Rate: 0.01\n",
      "Epoch [2935/10000], Loss: 95.17236328125, Learning Rate: 0.01\n",
      "Epoch [2936/10000], Loss: 95.16336822509766, Learning Rate: 0.01\n",
      "Epoch [2937/10000], Loss: 95.15435791015625, Learning Rate: 0.01\n",
      "Epoch [2938/10000], Loss: 95.14529418945312, Learning Rate: 0.01\n",
      "Epoch [2939/10000], Loss: 95.1363754272461, Learning Rate: 0.01\n",
      "Epoch [2940/10000], Loss: 95.1273193359375, Learning Rate: 0.01\n",
      "Epoch [2941/10000], Loss: 95.11833953857422, Learning Rate: 0.01\n",
      "Epoch [2942/10000], Loss: 95.10921478271484, Learning Rate: 0.01\n",
      "Epoch [2943/10000], Loss: 95.10018920898438, Learning Rate: 0.01\n",
      "Epoch [2944/10000], Loss: 95.09129333496094, Learning Rate: 0.01\n",
      "Epoch [2945/10000], Loss: 95.082275390625, Learning Rate: 0.01\n",
      "Epoch [2946/10000], Loss: 95.07325744628906, Learning Rate: 0.01\n",
      "Epoch [2947/10000], Loss: 95.06423950195312, Learning Rate: 0.01\n",
      "Epoch [2948/10000], Loss: 95.05517578125, Learning Rate: 0.01\n",
      "Epoch [2949/10000], Loss: 95.0462875366211, Learning Rate: 0.01\n",
      "Epoch [2950/10000], Loss: 95.03722381591797, Learning Rate: 0.01\n",
      "Epoch [2951/10000], Loss: 95.0281982421875, Learning Rate: 0.01\n",
      "Epoch [2952/10000], Loss: 95.0191879272461, Learning Rate: 0.01\n",
      "Epoch [2953/10000], Loss: 95.01020050048828, Learning Rate: 0.01\n",
      "Epoch [2954/10000], Loss: 95.0012435913086, Learning Rate: 0.01\n",
      "Epoch [2955/10000], Loss: 94.99227142333984, Learning Rate: 0.01\n",
      "Epoch [2956/10000], Loss: 94.98323822021484, Learning Rate: 0.01\n",
      "Epoch [2957/10000], Loss: 94.97419738769531, Learning Rate: 0.01\n",
      "Epoch [2958/10000], Loss: 94.96524047851562, Learning Rate: 0.01\n",
      "Epoch [2959/10000], Loss: 94.95621490478516, Learning Rate: 0.01\n",
      "Epoch [2960/10000], Loss: 94.94722747802734, Learning Rate: 0.01\n",
      "Epoch [2961/10000], Loss: 94.93827056884766, Learning Rate: 0.01\n",
      "Epoch [2962/10000], Loss: 94.92924499511719, Learning Rate: 0.01\n",
      "Epoch [2963/10000], Loss: 94.92023468017578, Learning Rate: 0.01\n",
      "Epoch [2964/10000], Loss: 94.91130828857422, Learning Rate: 0.01\n",
      "Epoch [2965/10000], Loss: 94.90231323242188, Learning Rate: 0.01\n",
      "Epoch [2966/10000], Loss: 94.89334869384766, Learning Rate: 0.01\n",
      "Epoch [2967/10000], Loss: 94.88430786132812, Learning Rate: 0.01\n",
      "Epoch [2968/10000], Loss: 94.87539672851562, Learning Rate: 0.01\n",
      "Epoch [2969/10000], Loss: 94.8663330078125, Learning Rate: 0.01\n",
      "Epoch [2970/10000], Loss: 94.85745239257812, Learning Rate: 0.01\n",
      "Epoch [2971/10000], Loss: 94.84843444824219, Learning Rate: 0.01\n",
      "Epoch [2972/10000], Loss: 94.8394775390625, Learning Rate: 0.01\n",
      "Epoch [2973/10000], Loss: 94.83047485351562, Learning Rate: 0.01\n",
      "Epoch [2974/10000], Loss: 94.82151794433594, Learning Rate: 0.01\n",
      "Epoch [2975/10000], Loss: 94.81256103515625, Learning Rate: 0.01\n",
      "Epoch [2976/10000], Loss: 94.80358123779297, Learning Rate: 0.01\n",
      "Epoch [2977/10000], Loss: 94.7945785522461, Learning Rate: 0.01\n",
      "Epoch [2978/10000], Loss: 94.78563690185547, Learning Rate: 0.01\n",
      "Epoch [2979/10000], Loss: 94.77664184570312, Learning Rate: 0.01\n",
      "Epoch [2980/10000], Loss: 94.76758575439453, Learning Rate: 0.01\n",
      "Epoch [2981/10000], Loss: 94.75878143310547, Learning Rate: 0.01\n",
      "Epoch [2982/10000], Loss: 94.74971008300781, Learning Rate: 0.01\n",
      "Epoch [2983/10000], Loss: 94.74081420898438, Learning Rate: 0.01\n",
      "Epoch [2984/10000], Loss: 94.7318115234375, Learning Rate: 0.01\n",
      "Epoch [2985/10000], Loss: 94.72282409667969, Learning Rate: 0.01\n",
      "Epoch [2986/10000], Loss: 94.71393585205078, Learning Rate: 0.01\n",
      "Epoch [2987/10000], Loss: 94.7048568725586, Learning Rate: 0.01\n",
      "Epoch [2988/10000], Loss: 94.6959228515625, Learning Rate: 0.01\n",
      "Epoch [2989/10000], Loss: 94.68697357177734, Learning Rate: 0.01\n",
      "Epoch [2990/10000], Loss: 94.67806243896484, Learning Rate: 0.01\n",
      "Epoch [2991/10000], Loss: 94.66905975341797, Learning Rate: 0.01\n",
      "Epoch [2992/10000], Loss: 94.66017150878906, Learning Rate: 0.01\n",
      "Epoch [2993/10000], Loss: 94.65119934082031, Learning Rate: 0.01\n",
      "Epoch [2994/10000], Loss: 94.6423110961914, Learning Rate: 0.01\n",
      "Epoch [2995/10000], Loss: 94.63335418701172, Learning Rate: 0.01\n",
      "Epoch [2996/10000], Loss: 94.62427520751953, Learning Rate: 0.01\n",
      "Epoch [2997/10000], Loss: 94.6153335571289, Learning Rate: 0.01\n",
      "Epoch [2998/10000], Loss: 94.60641479492188, Learning Rate: 0.01\n",
      "Epoch [2999/10000], Loss: 94.59748077392578, Learning Rate: 0.01\n",
      "Epoch [3000/10000], Loss: 94.58855438232422, Learning Rate: 0.01\n",
      "Epoch [3001/10000], Loss: 94.57962799072266, Learning Rate: 0.01\n",
      "Epoch [3002/10000], Loss: 94.5706787109375, Learning Rate: 0.01\n",
      "Epoch [3003/10000], Loss: 94.56173706054688, Learning Rate: 0.01\n",
      "Epoch [3004/10000], Loss: 94.55276489257812, Learning Rate: 0.01\n",
      "Epoch [3005/10000], Loss: 94.54389953613281, Learning Rate: 0.01\n",
      "Epoch [3006/10000], Loss: 94.53485107421875, Learning Rate: 0.01\n",
      "Epoch [3007/10000], Loss: 94.52603149414062, Learning Rate: 0.01\n",
      "Epoch [3008/10000], Loss: 94.51709747314453, Learning Rate: 0.01\n",
      "Epoch [3009/10000], Loss: 94.50818634033203, Learning Rate: 0.01\n",
      "Epoch [3010/10000], Loss: 94.49907684326172, Learning Rate: 0.01\n",
      "Epoch [3011/10000], Loss: 94.490234375, Learning Rate: 0.01\n",
      "Epoch [3012/10000], Loss: 94.48135375976562, Learning Rate: 0.01\n",
      "Epoch [3013/10000], Loss: 94.47238159179688, Learning Rate: 0.01\n",
      "Epoch [3014/10000], Loss: 94.46346282958984, Learning Rate: 0.01\n",
      "Epoch [3015/10000], Loss: 94.45446014404297, Learning Rate: 0.01\n",
      "Epoch [3016/10000], Loss: 94.44559478759766, Learning Rate: 0.01\n",
      "Epoch [3017/10000], Loss: 94.43665313720703, Learning Rate: 0.01\n",
      "Epoch [3018/10000], Loss: 94.42777252197266, Learning Rate: 0.01\n",
      "Epoch [3019/10000], Loss: 94.41886901855469, Learning Rate: 0.01\n",
      "Epoch [3020/10000], Loss: 94.40988159179688, Learning Rate: 0.01\n",
      "Epoch [3021/10000], Loss: 94.40094757080078, Learning Rate: 0.01\n",
      "Epoch [3022/10000], Loss: 94.3920669555664, Learning Rate: 0.01\n",
      "Epoch [3023/10000], Loss: 94.38314819335938, Learning Rate: 0.01\n",
      "Epoch [3024/10000], Loss: 94.37425231933594, Learning Rate: 0.01\n",
      "Epoch [3025/10000], Loss: 94.36530303955078, Learning Rate: 0.01\n",
      "Epoch [3026/10000], Loss: 94.35638427734375, Learning Rate: 0.01\n",
      "Epoch [3027/10000], Loss: 94.34748840332031, Learning Rate: 0.01\n",
      "Epoch [3028/10000], Loss: 94.33854675292969, Learning Rate: 0.01\n",
      "Epoch [3029/10000], Loss: 94.32962799072266, Learning Rate: 0.01\n",
      "Epoch [3030/10000], Loss: 94.32077026367188, Learning Rate: 0.01\n",
      "Epoch [3031/10000], Loss: 94.311767578125, Learning Rate: 0.01\n",
      "Epoch [3032/10000], Loss: 94.30294799804688, Learning Rate: 0.01\n",
      "Epoch [3033/10000], Loss: 94.29399108886719, Learning Rate: 0.01\n",
      "Epoch [3034/10000], Loss: 94.28511810302734, Learning Rate: 0.01\n",
      "Epoch [3035/10000], Loss: 94.27619171142578, Learning Rate: 0.01\n",
      "Epoch [3036/10000], Loss: 94.26728057861328, Learning Rate: 0.01\n",
      "Epoch [3037/10000], Loss: 94.25840759277344, Learning Rate: 0.01\n",
      "Epoch [3038/10000], Loss: 94.24948120117188, Learning Rate: 0.01\n",
      "Epoch [3039/10000], Loss: 94.24061584472656, Learning Rate: 0.01\n",
      "Epoch [3040/10000], Loss: 94.2316665649414, Learning Rate: 0.01\n",
      "Epoch [3041/10000], Loss: 94.22286224365234, Learning Rate: 0.01\n",
      "Epoch [3042/10000], Loss: 94.21378326416016, Learning Rate: 0.01\n",
      "Epoch [3043/10000], Loss: 94.20491027832031, Learning Rate: 0.01\n",
      "Epoch [3044/10000], Loss: 94.196044921875, Learning Rate: 0.01\n",
      "Epoch [3045/10000], Loss: 94.18718719482422, Learning Rate: 0.01\n",
      "Epoch [3046/10000], Loss: 94.17829895019531, Learning Rate: 0.01\n",
      "Epoch [3047/10000], Loss: 94.16938781738281, Learning Rate: 0.01\n",
      "Epoch [3048/10000], Loss: 94.1604995727539, Learning Rate: 0.01\n",
      "Epoch [3049/10000], Loss: 94.15165710449219, Learning Rate: 0.01\n",
      "Epoch [3050/10000], Loss: 94.14276885986328, Learning Rate: 0.01\n",
      "Epoch [3051/10000], Loss: 94.13385009765625, Learning Rate: 0.01\n",
      "Epoch [3052/10000], Loss: 94.1249008178711, Learning Rate: 0.01\n",
      "Epoch [3053/10000], Loss: 94.11608123779297, Learning Rate: 0.01\n",
      "Epoch [3054/10000], Loss: 94.1070785522461, Learning Rate: 0.01\n",
      "Epoch [3055/10000], Loss: 94.09823608398438, Learning Rate: 0.01\n",
      "Epoch [3056/10000], Loss: 94.08935546875, Learning Rate: 0.01\n",
      "Epoch [3057/10000], Loss: 94.08047485351562, Learning Rate: 0.01\n",
      "Epoch [3058/10000], Loss: 94.07158660888672, Learning Rate: 0.01\n",
      "Epoch [3059/10000], Loss: 94.0627670288086, Learning Rate: 0.01\n",
      "Epoch [3060/10000], Loss: 94.05389404296875, Learning Rate: 0.01\n",
      "Epoch [3061/10000], Loss: 94.04505920410156, Learning Rate: 0.01\n",
      "Epoch [3062/10000], Loss: 94.03611755371094, Learning Rate: 0.01\n",
      "Epoch [3063/10000], Loss: 94.02727508544922, Learning Rate: 0.01\n",
      "Epoch [3064/10000], Loss: 94.01827239990234, Learning Rate: 0.01\n",
      "Epoch [3065/10000], Loss: 94.0094985961914, Learning Rate: 0.01\n",
      "Epoch [3066/10000], Loss: 94.00060272216797, Learning Rate: 0.01\n",
      "Epoch [3067/10000], Loss: 93.9918212890625, Learning Rate: 0.01\n",
      "Epoch [3068/10000], Loss: 93.98294067382812, Learning Rate: 0.01\n",
      "Epoch [3069/10000], Loss: 93.97406768798828, Learning Rate: 0.01\n",
      "Epoch [3070/10000], Loss: 93.96512603759766, Learning Rate: 0.01\n",
      "Epoch [3071/10000], Loss: 93.95628356933594, Learning Rate: 0.01\n",
      "Epoch [3072/10000], Loss: 93.94740295410156, Learning Rate: 0.01\n",
      "Epoch [3073/10000], Loss: 93.93853759765625, Learning Rate: 0.01\n",
      "Epoch [3074/10000], Loss: 93.92967224121094, Learning Rate: 0.01\n",
      "Epoch [3075/10000], Loss: 93.92080688476562, Learning Rate: 0.01\n",
      "Epoch [3076/10000], Loss: 93.91194915771484, Learning Rate: 0.01\n",
      "Epoch [3077/10000], Loss: 93.903076171875, Learning Rate: 0.01\n",
      "Epoch [3078/10000], Loss: 93.89424896240234, Learning Rate: 0.01\n",
      "Epoch [3079/10000], Loss: 93.88536834716797, Learning Rate: 0.01\n",
      "Epoch [3080/10000], Loss: 93.87654876708984, Learning Rate: 0.01\n",
      "Epoch [3081/10000], Loss: 93.86761474609375, Learning Rate: 0.01\n",
      "Epoch [3082/10000], Loss: 93.85881805419922, Learning Rate: 0.01\n",
      "Epoch [3083/10000], Loss: 93.84991455078125, Learning Rate: 0.01\n",
      "Epoch [3084/10000], Loss: 93.84107971191406, Learning Rate: 0.01\n",
      "Epoch [3085/10000], Loss: 93.83223724365234, Learning Rate: 0.01\n",
      "Epoch [3086/10000], Loss: 93.82339477539062, Learning Rate: 0.01\n",
      "Epoch [3087/10000], Loss: 93.81454467773438, Learning Rate: 0.01\n",
      "Epoch [3088/10000], Loss: 93.80570983886719, Learning Rate: 0.01\n",
      "Epoch [3089/10000], Loss: 93.79679870605469, Learning Rate: 0.01\n",
      "Epoch [3090/10000], Loss: 93.78795623779297, Learning Rate: 0.01\n",
      "Epoch [3091/10000], Loss: 93.77912902832031, Learning Rate: 0.01\n",
      "Epoch [3092/10000], Loss: 93.770263671875, Learning Rate: 0.01\n",
      "Epoch [3093/10000], Loss: 93.76140594482422, Learning Rate: 0.01\n",
      "Epoch [3094/10000], Loss: 93.7526626586914, Learning Rate: 0.01\n",
      "Epoch [3095/10000], Loss: 93.7437744140625, Learning Rate: 0.01\n",
      "Epoch [3096/10000], Loss: 93.73493957519531, Learning Rate: 0.01\n",
      "Epoch [3097/10000], Loss: 93.72606658935547, Learning Rate: 0.01\n",
      "Epoch [3098/10000], Loss: 93.71717834472656, Learning Rate: 0.01\n",
      "Epoch [3099/10000], Loss: 93.70844268798828, Learning Rate: 0.01\n",
      "Epoch [3100/10000], Loss: 93.69951629638672, Learning Rate: 0.01\n",
      "Epoch [3101/10000], Loss: 93.69078826904297, Learning Rate: 0.01\n",
      "Epoch [3102/10000], Loss: 93.68190002441406, Learning Rate: 0.01\n",
      "Epoch [3103/10000], Loss: 93.6730728149414, Learning Rate: 0.01\n",
      "Epoch [3104/10000], Loss: 93.66425323486328, Learning Rate: 0.01\n",
      "Epoch [3105/10000], Loss: 93.6553955078125, Learning Rate: 0.01\n",
      "Epoch [3106/10000], Loss: 93.64665985107422, Learning Rate: 0.01\n",
      "Epoch [3107/10000], Loss: 93.6377182006836, Learning Rate: 0.01\n",
      "Epoch [3108/10000], Loss: 93.62899017333984, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3109/10000], Loss: 93.62008666992188, Learning Rate: 0.01\n",
      "Epoch [3110/10000], Loss: 93.61135864257812, Learning Rate: 0.01\n",
      "Epoch [3111/10000], Loss: 93.6024398803711, Learning Rate: 0.01\n",
      "Epoch [3112/10000], Loss: 93.59364318847656, Learning Rate: 0.01\n",
      "Epoch [3113/10000], Loss: 93.58483123779297, Learning Rate: 0.01\n",
      "Epoch [3114/10000], Loss: 93.57603454589844, Learning Rate: 0.01\n",
      "Epoch [3115/10000], Loss: 93.56720733642578, Learning Rate: 0.01\n",
      "Epoch [3116/10000], Loss: 93.5583267211914, Learning Rate: 0.01\n",
      "Epoch [3117/10000], Loss: 93.54946899414062, Learning Rate: 0.01\n",
      "Epoch [3118/10000], Loss: 93.54066467285156, Learning Rate: 0.01\n",
      "Epoch [3119/10000], Loss: 93.53192138671875, Learning Rate: 0.01\n",
      "Epoch [3120/10000], Loss: 93.52310180664062, Learning Rate: 0.01\n",
      "Epoch [3121/10000], Loss: 93.51423645019531, Learning Rate: 0.01\n",
      "Epoch [3122/10000], Loss: 93.5054702758789, Learning Rate: 0.01\n",
      "Epoch [3123/10000], Loss: 93.4966049194336, Learning Rate: 0.01\n",
      "Epoch [3124/10000], Loss: 93.48784637451172, Learning Rate: 0.01\n",
      "Epoch [3125/10000], Loss: 93.47900390625, Learning Rate: 0.01\n",
      "Epoch [3126/10000], Loss: 93.47026062011719, Learning Rate: 0.01\n",
      "Epoch [3127/10000], Loss: 93.46138000488281, Learning Rate: 0.01\n",
      "Epoch [3128/10000], Loss: 93.45256805419922, Learning Rate: 0.01\n",
      "Epoch [3129/10000], Loss: 93.44375610351562, Learning Rate: 0.01\n",
      "Epoch [3130/10000], Loss: 93.43501281738281, Learning Rate: 0.01\n",
      "Epoch [3131/10000], Loss: 93.42613983154297, Learning Rate: 0.01\n",
      "Epoch [3132/10000], Loss: 93.41736602783203, Learning Rate: 0.01\n",
      "Epoch [3133/10000], Loss: 93.40863037109375, Learning Rate: 0.01\n",
      "Epoch [3134/10000], Loss: 93.3997802734375, Learning Rate: 0.01\n",
      "Epoch [3135/10000], Loss: 93.39098358154297, Learning Rate: 0.01\n",
      "Epoch [3136/10000], Loss: 93.38214111328125, Learning Rate: 0.01\n",
      "Epoch [3137/10000], Loss: 93.37337493896484, Learning Rate: 0.01\n",
      "Epoch [3138/10000], Loss: 93.3646011352539, Learning Rate: 0.01\n",
      "Epoch [3139/10000], Loss: 93.35578155517578, Learning Rate: 0.01\n",
      "Epoch [3140/10000], Loss: 93.34705352783203, Learning Rate: 0.01\n",
      "Epoch [3141/10000], Loss: 93.33811950683594, Learning Rate: 0.01\n",
      "Epoch [3142/10000], Loss: 93.3294448852539, Learning Rate: 0.01\n",
      "Epoch [3143/10000], Loss: 93.32061767578125, Learning Rate: 0.01\n",
      "Epoch [3144/10000], Loss: 93.31175231933594, Learning Rate: 0.01\n",
      "Epoch [3145/10000], Loss: 93.30301666259766, Learning Rate: 0.01\n",
      "Epoch [3146/10000], Loss: 93.29425048828125, Learning Rate: 0.01\n",
      "Epoch [3147/10000], Loss: 93.28540802001953, Learning Rate: 0.01\n",
      "Epoch [3148/10000], Loss: 93.27666473388672, Learning Rate: 0.01\n",
      "Epoch [3149/10000], Loss: 93.26787567138672, Learning Rate: 0.01\n",
      "Epoch [3150/10000], Loss: 93.25908660888672, Learning Rate: 0.01\n",
      "Epoch [3151/10000], Loss: 93.25031280517578, Learning Rate: 0.01\n",
      "Epoch [3152/10000], Loss: 93.24150848388672, Learning Rate: 0.01\n",
      "Epoch [3153/10000], Loss: 93.2327880859375, Learning Rate: 0.01\n",
      "Epoch [3154/10000], Loss: 93.22394561767578, Learning Rate: 0.01\n",
      "Epoch [3155/10000], Loss: 93.21520233154297, Learning Rate: 0.01\n",
      "Epoch [3156/10000], Loss: 93.2063980102539, Learning Rate: 0.01\n",
      "Epoch [3157/10000], Loss: 93.19760131835938, Learning Rate: 0.01\n",
      "Epoch [3158/10000], Loss: 93.18888854980469, Learning Rate: 0.01\n",
      "Epoch [3159/10000], Loss: 93.1800308227539, Learning Rate: 0.01\n",
      "Epoch [3160/10000], Loss: 93.17130279541016, Learning Rate: 0.01\n",
      "Epoch [3161/10000], Loss: 93.1625747680664, Learning Rate: 0.01\n",
      "Epoch [3162/10000], Loss: 93.15371704101562, Learning Rate: 0.01\n",
      "Epoch [3163/10000], Loss: 93.14502716064453, Learning Rate: 0.01\n",
      "Epoch [3164/10000], Loss: 93.1362075805664, Learning Rate: 0.01\n",
      "Epoch [3165/10000], Loss: 93.1274642944336, Learning Rate: 0.01\n",
      "Epoch [3166/10000], Loss: 93.11863708496094, Learning Rate: 0.01\n",
      "Epoch [3167/10000], Loss: 93.10990142822266, Learning Rate: 0.01\n",
      "Epoch [3168/10000], Loss: 93.1010513305664, Learning Rate: 0.01\n",
      "Epoch [3169/10000], Loss: 93.09239959716797, Learning Rate: 0.01\n",
      "Epoch [3170/10000], Loss: 93.08358764648438, Learning Rate: 0.01\n",
      "Epoch [3171/10000], Loss: 93.07492065429688, Learning Rate: 0.01\n",
      "Epoch [3172/10000], Loss: 93.06603240966797, Learning Rate: 0.01\n",
      "Epoch [3173/10000], Loss: 93.05732727050781, Learning Rate: 0.01\n",
      "Epoch [3174/10000], Loss: 93.048583984375, Learning Rate: 0.01\n",
      "Epoch [3175/10000], Loss: 93.03975677490234, Learning Rate: 0.01\n",
      "Epoch [3176/10000], Loss: 93.03099822998047, Learning Rate: 0.01\n",
      "Epoch [3177/10000], Loss: 93.022216796875, Learning Rate: 0.01\n",
      "Epoch [3178/10000], Loss: 93.01355743408203, Learning Rate: 0.01\n",
      "Epoch [3179/10000], Loss: 93.00484466552734, Learning Rate: 0.01\n",
      "Epoch [3180/10000], Loss: 92.9959945678711, Learning Rate: 0.01\n",
      "Epoch [3181/10000], Loss: 92.98722839355469, Learning Rate: 0.01\n",
      "Epoch [3182/10000], Loss: 92.97850799560547, Learning Rate: 0.01\n",
      "Epoch [3183/10000], Loss: 92.96976470947266, Learning Rate: 0.01\n",
      "Epoch [3184/10000], Loss: 92.9609603881836, Learning Rate: 0.01\n",
      "Epoch [3185/10000], Loss: 92.95230102539062, Learning Rate: 0.01\n",
      "Epoch [3186/10000], Loss: 92.94352722167969, Learning Rate: 0.01\n",
      "Epoch [3187/10000], Loss: 92.93475341796875, Learning Rate: 0.01\n",
      "Epoch [3188/10000], Loss: 92.9260025024414, Learning Rate: 0.01\n",
      "Epoch [3189/10000], Loss: 92.91731262207031, Learning Rate: 0.01\n",
      "Epoch [3190/10000], Loss: 92.90847778320312, Learning Rate: 0.01\n",
      "Epoch [3191/10000], Loss: 92.89973449707031, Learning Rate: 0.01\n",
      "Epoch [3192/10000], Loss: 92.89103698730469, Learning Rate: 0.01\n",
      "Epoch [3193/10000], Loss: 92.88225555419922, Learning Rate: 0.01\n",
      "Epoch [3194/10000], Loss: 92.8735122680664, Learning Rate: 0.01\n",
      "Epoch [3195/10000], Loss: 92.86483764648438, Learning Rate: 0.01\n",
      "Epoch [3196/10000], Loss: 92.85613250732422, Learning Rate: 0.01\n",
      "Epoch [3197/10000], Loss: 92.84732818603516, Learning Rate: 0.01\n",
      "Epoch [3198/10000], Loss: 92.83863067626953, Learning Rate: 0.01\n",
      "Epoch [3199/10000], Loss: 92.82980346679688, Learning Rate: 0.01\n",
      "Epoch [3200/10000], Loss: 92.8211669921875, Learning Rate: 0.01\n",
      "Epoch [3201/10000], Loss: 92.8124008178711, Learning Rate: 0.01\n",
      "Epoch [3202/10000], Loss: 92.80374145507812, Learning Rate: 0.01\n",
      "Epoch [3203/10000], Loss: 92.79493713378906, Learning Rate: 0.01\n",
      "Epoch [3204/10000], Loss: 92.78620910644531, Learning Rate: 0.01\n",
      "Epoch [3205/10000], Loss: 92.7774429321289, Learning Rate: 0.01\n",
      "Epoch [3206/10000], Loss: 92.76874542236328, Learning Rate: 0.01\n",
      "Epoch [3207/10000], Loss: 92.7599868774414, Learning Rate: 0.01\n",
      "Epoch [3208/10000], Loss: 92.7513427734375, Learning Rate: 0.01\n",
      "Epoch [3209/10000], Loss: 92.74258422851562, Learning Rate: 0.01\n",
      "Epoch [3210/10000], Loss: 92.73388671875, Learning Rate: 0.01\n",
      "Epoch [3211/10000], Loss: 92.72513580322266, Learning Rate: 0.01\n",
      "Epoch [3212/10000], Loss: 92.71646118164062, Learning Rate: 0.01\n",
      "Epoch [3213/10000], Loss: 92.70772552490234, Learning Rate: 0.01\n",
      "Epoch [3214/10000], Loss: 92.69902801513672, Learning Rate: 0.01\n",
      "Epoch [3215/10000], Loss: 92.69026184082031, Learning Rate: 0.01\n",
      "Epoch [3216/10000], Loss: 92.68146514892578, Learning Rate: 0.01\n",
      "Epoch [3217/10000], Loss: 92.67280578613281, Learning Rate: 0.01\n",
      "Epoch [3218/10000], Loss: 92.66409301757812, Learning Rate: 0.01\n",
      "Epoch [3219/10000], Loss: 92.65534973144531, Learning Rate: 0.01\n",
      "Epoch [3220/10000], Loss: 92.6466293334961, Learning Rate: 0.01\n",
      "Epoch [3221/10000], Loss: 92.6380386352539, Learning Rate: 0.01\n",
      "Epoch [3222/10000], Loss: 92.62921142578125, Learning Rate: 0.01\n",
      "Epoch [3223/10000], Loss: 92.62052917480469, Learning Rate: 0.01\n",
      "Epoch [3224/10000], Loss: 92.61188507080078, Learning Rate: 0.01\n",
      "Epoch [3225/10000], Loss: 92.60314178466797, Learning Rate: 0.01\n",
      "Epoch [3226/10000], Loss: 92.59442138671875, Learning Rate: 0.01\n",
      "Epoch [3227/10000], Loss: 92.5856704711914, Learning Rate: 0.01\n",
      "Epoch [3228/10000], Loss: 92.57698822021484, Learning Rate: 0.01\n",
      "Epoch [3229/10000], Loss: 92.56830596923828, Learning Rate: 0.01\n",
      "Epoch [3230/10000], Loss: 92.5595703125, Learning Rate: 0.01\n",
      "Epoch [3231/10000], Loss: 92.55085754394531, Learning Rate: 0.01\n",
      "Epoch [3232/10000], Loss: 92.54222106933594, Learning Rate: 0.01\n",
      "Epoch [3233/10000], Loss: 92.53350067138672, Learning Rate: 0.01\n",
      "Epoch [3234/10000], Loss: 92.52472686767578, Learning Rate: 0.01\n",
      "Epoch [3235/10000], Loss: 92.5160140991211, Learning Rate: 0.01\n",
      "Epoch [3236/10000], Loss: 92.50735473632812, Learning Rate: 0.01\n",
      "Epoch [3237/10000], Loss: 92.49874114990234, Learning Rate: 0.01\n",
      "Epoch [3238/10000], Loss: 92.48998260498047, Learning Rate: 0.01\n",
      "Epoch [3239/10000], Loss: 92.48126983642578, Learning Rate: 0.01\n",
      "Epoch [3240/10000], Loss: 92.47251892089844, Learning Rate: 0.01\n",
      "Epoch [3241/10000], Loss: 92.46385955810547, Learning Rate: 0.01\n",
      "Epoch [3242/10000], Loss: 92.45523834228516, Learning Rate: 0.01\n",
      "Epoch [3243/10000], Loss: 92.44654846191406, Learning Rate: 0.01\n",
      "Epoch [3244/10000], Loss: 92.4378662109375, Learning Rate: 0.01\n",
      "Epoch [3245/10000], Loss: 92.42918395996094, Learning Rate: 0.01\n",
      "Epoch [3246/10000], Loss: 92.42040252685547, Learning Rate: 0.01\n",
      "Epoch [3247/10000], Loss: 92.41181945800781, Learning Rate: 0.01\n",
      "Epoch [3248/10000], Loss: 92.40308380126953, Learning Rate: 0.01\n",
      "Epoch [3249/10000], Loss: 92.39440155029297, Learning Rate: 0.01\n",
      "Epoch [3250/10000], Loss: 92.38567352294922, Learning Rate: 0.01\n",
      "Epoch [3251/10000], Loss: 92.37699127197266, Learning Rate: 0.01\n",
      "Epoch [3252/10000], Loss: 92.36839294433594, Learning Rate: 0.01\n",
      "Epoch [3253/10000], Loss: 92.35961151123047, Learning Rate: 0.01\n",
      "Epoch [3254/10000], Loss: 92.35100555419922, Learning Rate: 0.01\n",
      "Epoch [3255/10000], Loss: 92.3422622680664, Learning Rate: 0.01\n",
      "Epoch [3256/10000], Loss: 92.33362579345703, Learning Rate: 0.01\n",
      "Epoch [3257/10000], Loss: 92.324951171875, Learning Rate: 0.01\n",
      "Epoch [3258/10000], Loss: 92.3162841796875, Learning Rate: 0.01\n",
      "Epoch [3259/10000], Loss: 92.30754852294922, Learning Rate: 0.01\n",
      "Epoch [3260/10000], Loss: 92.2989501953125, Learning Rate: 0.01\n",
      "Epoch [3261/10000], Loss: 92.2902603149414, Learning Rate: 0.01\n",
      "Epoch [3262/10000], Loss: 92.28152465820312, Learning Rate: 0.01\n",
      "Epoch [3263/10000], Loss: 92.27284240722656, Learning Rate: 0.01\n",
      "Epoch [3264/10000], Loss: 92.26419830322266, Learning Rate: 0.01\n",
      "Epoch [3265/10000], Loss: 92.25555419921875, Learning Rate: 0.01\n",
      "Epoch [3266/10000], Loss: 92.24691772460938, Learning Rate: 0.01\n",
      "Epoch [3267/10000], Loss: 92.23817443847656, Learning Rate: 0.01\n",
      "Epoch [3268/10000], Loss: 92.22958374023438, Learning Rate: 0.01\n",
      "Epoch [3269/10000], Loss: 92.22087860107422, Learning Rate: 0.01\n",
      "Epoch [3270/10000], Loss: 92.2121810913086, Learning Rate: 0.01\n",
      "Epoch [3271/10000], Loss: 92.20357513427734, Learning Rate: 0.01\n",
      "Epoch [3272/10000], Loss: 92.19489288330078, Learning Rate: 0.01\n",
      "Epoch [3273/10000], Loss: 92.18624114990234, Learning Rate: 0.01\n",
      "Epoch [3274/10000], Loss: 92.17758178710938, Learning Rate: 0.01\n",
      "Epoch [3275/10000], Loss: 92.16888427734375, Learning Rate: 0.01\n",
      "Epoch [3276/10000], Loss: 92.1603012084961, Learning Rate: 0.01\n",
      "Epoch [3277/10000], Loss: 92.15161895751953, Learning Rate: 0.01\n",
      "Epoch [3278/10000], Loss: 92.14291381835938, Learning Rate: 0.01\n",
      "Epoch [3279/10000], Loss: 92.13423919677734, Learning Rate: 0.01\n",
      "Epoch [3280/10000], Loss: 92.1256332397461, Learning Rate: 0.01\n",
      "Epoch [3281/10000], Loss: 92.11695861816406, Learning Rate: 0.01\n",
      "Epoch [3282/10000], Loss: 92.10830688476562, Learning Rate: 0.01\n",
      "Epoch [3283/10000], Loss: 92.0996322631836, Learning Rate: 0.01\n",
      "Epoch [3284/10000], Loss: 92.09101867675781, Learning Rate: 0.01\n",
      "Epoch [3285/10000], Loss: 92.08234405517578, Learning Rate: 0.01\n",
      "Epoch [3286/10000], Loss: 92.07365417480469, Learning Rate: 0.01\n",
      "Epoch [3287/10000], Loss: 92.06507110595703, Learning Rate: 0.01\n",
      "Epoch [3288/10000], Loss: 92.05642700195312, Learning Rate: 0.01\n",
      "Epoch [3289/10000], Loss: 92.04777526855469, Learning Rate: 0.01\n",
      "Epoch [3290/10000], Loss: 92.03910827636719, Learning Rate: 0.01\n",
      "Epoch [3291/10000], Loss: 92.0304183959961, Learning Rate: 0.01\n",
      "Epoch [3292/10000], Loss: 92.02189636230469, Learning Rate: 0.01\n",
      "Epoch [3293/10000], Loss: 92.01313781738281, Learning Rate: 0.01\n",
      "Epoch [3294/10000], Loss: 92.00452423095703, Learning Rate: 0.01\n",
      "Epoch [3295/10000], Loss: 91.99590301513672, Learning Rate: 0.01\n",
      "Epoch [3296/10000], Loss: 91.98725891113281, Learning Rate: 0.01\n",
      "Epoch [3297/10000], Loss: 91.97858428955078, Learning Rate: 0.01\n",
      "Epoch [3298/10000], Loss: 91.97005462646484, Learning Rate: 0.01\n",
      "Epoch [3299/10000], Loss: 91.96134185791016, Learning Rate: 0.01\n",
      "Epoch [3300/10000], Loss: 91.95269012451172, Learning Rate: 0.01\n",
      "Epoch [3301/10000], Loss: 91.94403839111328, Learning Rate: 0.01\n",
      "Epoch [3302/10000], Loss: 91.93547058105469, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3303/10000], Loss: 91.92682647705078, Learning Rate: 0.01\n",
      "Epoch [3304/10000], Loss: 91.91814422607422, Learning Rate: 0.01\n",
      "Epoch [3305/10000], Loss: 91.90957641601562, Learning Rate: 0.01\n",
      "Epoch [3306/10000], Loss: 91.9009017944336, Learning Rate: 0.01\n",
      "Epoch [3307/10000], Loss: 91.89226531982422, Learning Rate: 0.01\n",
      "Epoch [3308/10000], Loss: 91.88365936279297, Learning Rate: 0.01\n",
      "Epoch [3309/10000], Loss: 91.8750228881836, Learning Rate: 0.01\n",
      "Epoch [3310/10000], Loss: 91.86646270751953, Learning Rate: 0.01\n",
      "Epoch [3311/10000], Loss: 91.85778045654297, Learning Rate: 0.01\n",
      "Epoch [3312/10000], Loss: 91.84915924072266, Learning Rate: 0.01\n",
      "Epoch [3313/10000], Loss: 91.84052276611328, Learning Rate: 0.01\n",
      "Epoch [3314/10000], Loss: 91.83190155029297, Learning Rate: 0.01\n",
      "Epoch [3315/10000], Loss: 91.8232421875, Learning Rate: 0.01\n",
      "Epoch [3316/10000], Loss: 91.81459045410156, Learning Rate: 0.01\n",
      "Epoch [3317/10000], Loss: 91.80597686767578, Learning Rate: 0.01\n",
      "Epoch [3318/10000], Loss: 91.79740142822266, Learning Rate: 0.01\n",
      "Epoch [3319/10000], Loss: 91.78878021240234, Learning Rate: 0.01\n",
      "Epoch [3320/10000], Loss: 91.78021240234375, Learning Rate: 0.01\n",
      "Epoch [3321/10000], Loss: 91.7716064453125, Learning Rate: 0.01\n",
      "Epoch [3322/10000], Loss: 91.7629623413086, Learning Rate: 0.01\n",
      "Epoch [3323/10000], Loss: 91.75439453125, Learning Rate: 0.01\n",
      "Epoch [3324/10000], Loss: 91.74575805664062, Learning Rate: 0.01\n",
      "Epoch [3325/10000], Loss: 91.73711395263672, Learning Rate: 0.01\n",
      "Epoch [3326/10000], Loss: 91.72850799560547, Learning Rate: 0.01\n",
      "Epoch [3327/10000], Loss: 91.71986389160156, Learning Rate: 0.01\n",
      "Epoch [3328/10000], Loss: 91.71133422851562, Learning Rate: 0.01\n",
      "Epoch [3329/10000], Loss: 91.70266723632812, Learning Rate: 0.01\n",
      "Epoch [3330/10000], Loss: 91.69408416748047, Learning Rate: 0.01\n",
      "Epoch [3331/10000], Loss: 91.6854476928711, Learning Rate: 0.01\n",
      "Epoch [3332/10000], Loss: 91.67684936523438, Learning Rate: 0.01\n",
      "Epoch [3333/10000], Loss: 91.66818237304688, Learning Rate: 0.01\n",
      "Epoch [3334/10000], Loss: 91.65962219238281, Learning Rate: 0.01\n",
      "Epoch [3335/10000], Loss: 91.6510009765625, Learning Rate: 0.01\n",
      "Epoch [3336/10000], Loss: 91.64234161376953, Learning Rate: 0.01\n",
      "Epoch [3337/10000], Loss: 91.6337890625, Learning Rate: 0.01\n",
      "Epoch [3338/10000], Loss: 91.62519836425781, Learning Rate: 0.01\n",
      "Epoch [3339/10000], Loss: 91.6166000366211, Learning Rate: 0.01\n",
      "Epoch [3340/10000], Loss: 91.60804748535156, Learning Rate: 0.01\n",
      "Epoch [3341/10000], Loss: 91.59942626953125, Learning Rate: 0.01\n",
      "Epoch [3342/10000], Loss: 91.5907974243164, Learning Rate: 0.01\n",
      "Epoch [3343/10000], Loss: 91.58226013183594, Learning Rate: 0.01\n",
      "Epoch [3344/10000], Loss: 91.5736312866211, Learning Rate: 0.01\n",
      "Epoch [3345/10000], Loss: 91.56507110595703, Learning Rate: 0.01\n",
      "Epoch [3346/10000], Loss: 91.55644226074219, Learning Rate: 0.01\n",
      "Epoch [3347/10000], Loss: 91.54785919189453, Learning Rate: 0.01\n",
      "Epoch [3348/10000], Loss: 91.5391616821289, Learning Rate: 0.01\n",
      "Epoch [3349/10000], Loss: 91.5306625366211, Learning Rate: 0.01\n",
      "Epoch [3350/10000], Loss: 91.52203369140625, Learning Rate: 0.01\n",
      "Epoch [3351/10000], Loss: 91.51339721679688, Learning Rate: 0.01\n",
      "Epoch [3352/10000], Loss: 91.50495910644531, Learning Rate: 0.01\n",
      "Epoch [3353/10000], Loss: 91.49629974365234, Learning Rate: 0.01\n",
      "Epoch [3354/10000], Loss: 91.48774719238281, Learning Rate: 0.01\n",
      "Epoch [3355/10000], Loss: 91.47923278808594, Learning Rate: 0.01\n",
      "Epoch [3356/10000], Loss: 91.47049713134766, Learning Rate: 0.01\n",
      "Epoch [3357/10000], Loss: 91.4620132446289, Learning Rate: 0.01\n",
      "Epoch [3358/10000], Loss: 91.45343017578125, Learning Rate: 0.01\n",
      "Epoch [3359/10000], Loss: 91.44478607177734, Learning Rate: 0.01\n",
      "Epoch [3360/10000], Loss: 91.43626403808594, Learning Rate: 0.01\n",
      "Epoch [3361/10000], Loss: 91.42759704589844, Learning Rate: 0.01\n",
      "Epoch [3362/10000], Loss: 91.41907501220703, Learning Rate: 0.01\n",
      "Epoch [3363/10000], Loss: 91.41050720214844, Learning Rate: 0.01\n",
      "Epoch [3364/10000], Loss: 91.40187072753906, Learning Rate: 0.01\n",
      "Epoch [3365/10000], Loss: 91.39334869384766, Learning Rate: 0.01\n",
      "Epoch [3366/10000], Loss: 91.3847427368164, Learning Rate: 0.01\n",
      "Epoch [3367/10000], Loss: 91.37617492675781, Learning Rate: 0.01\n",
      "Epoch [3368/10000], Loss: 91.36766052246094, Learning Rate: 0.01\n",
      "Epoch [3369/10000], Loss: 91.35893249511719, Learning Rate: 0.01\n",
      "Epoch [3370/10000], Loss: 91.35043334960938, Learning Rate: 0.01\n",
      "Epoch [3371/10000], Loss: 91.34195709228516, Learning Rate: 0.01\n",
      "Epoch [3372/10000], Loss: 91.3333740234375, Learning Rate: 0.01\n",
      "Epoch [3373/10000], Loss: 91.32477569580078, Learning Rate: 0.01\n",
      "Epoch [3374/10000], Loss: 91.31614685058594, Learning Rate: 0.01\n",
      "Epoch [3375/10000], Loss: 91.30769348144531, Learning Rate: 0.01\n",
      "Epoch [3376/10000], Loss: 91.29905700683594, Learning Rate: 0.01\n",
      "Epoch [3377/10000], Loss: 91.29046630859375, Learning Rate: 0.01\n",
      "Epoch [3378/10000], Loss: 91.2819595336914, Learning Rate: 0.01\n",
      "Epoch [3379/10000], Loss: 91.27337646484375, Learning Rate: 0.01\n",
      "Epoch [3380/10000], Loss: 91.26480102539062, Learning Rate: 0.01\n",
      "Epoch [3381/10000], Loss: 91.25623321533203, Learning Rate: 0.01\n",
      "Epoch [3382/10000], Loss: 91.24767303466797, Learning Rate: 0.01\n",
      "Epoch [3383/10000], Loss: 91.23915100097656, Learning Rate: 0.01\n",
      "Epoch [3384/10000], Loss: 91.23062896728516, Learning Rate: 0.01\n",
      "Epoch [3385/10000], Loss: 91.22200775146484, Learning Rate: 0.01\n",
      "Epoch [3386/10000], Loss: 91.21341705322266, Learning Rate: 0.01\n",
      "Epoch [3387/10000], Loss: 91.20490264892578, Learning Rate: 0.01\n",
      "Epoch [3388/10000], Loss: 91.19635009765625, Learning Rate: 0.01\n",
      "Epoch [3389/10000], Loss: 91.18773651123047, Learning Rate: 0.01\n",
      "Epoch [3390/10000], Loss: 91.17924499511719, Learning Rate: 0.01\n",
      "Epoch [3391/10000], Loss: 91.1706314086914, Learning Rate: 0.01\n",
      "Epoch [3392/10000], Loss: 91.16214752197266, Learning Rate: 0.01\n",
      "Epoch [3393/10000], Loss: 91.15354919433594, Learning Rate: 0.01\n",
      "Epoch [3394/10000], Loss: 91.14505767822266, Learning Rate: 0.01\n",
      "Epoch [3395/10000], Loss: 91.13651275634766, Learning Rate: 0.01\n",
      "Epoch [3396/10000], Loss: 91.12796020507812, Learning Rate: 0.01\n",
      "Epoch [3397/10000], Loss: 91.11941528320312, Learning Rate: 0.01\n",
      "Epoch [3398/10000], Loss: 91.1108627319336, Learning Rate: 0.01\n",
      "Epoch [3399/10000], Loss: 91.10234069824219, Learning Rate: 0.01\n",
      "Epoch [3400/10000], Loss: 91.09381103515625, Learning Rate: 0.01\n",
      "Epoch [3401/10000], Loss: 91.085205078125, Learning Rate: 0.01\n",
      "Epoch [3402/10000], Loss: 91.07673645019531, Learning Rate: 0.01\n",
      "Epoch [3403/10000], Loss: 91.06809997558594, Learning Rate: 0.01\n",
      "Epoch [3404/10000], Loss: 91.05960845947266, Learning Rate: 0.01\n",
      "Epoch [3405/10000], Loss: 91.0510482788086, Learning Rate: 0.01\n",
      "Epoch [3406/10000], Loss: 91.04251861572266, Learning Rate: 0.01\n",
      "Epoch [3407/10000], Loss: 91.03399658203125, Learning Rate: 0.01\n",
      "Epoch [3408/10000], Loss: 91.0254898071289, Learning Rate: 0.01\n",
      "Epoch [3409/10000], Loss: 91.0169677734375, Learning Rate: 0.01\n",
      "Epoch [3410/10000], Loss: 91.00833892822266, Learning Rate: 0.01\n",
      "Epoch [3411/10000], Loss: 90.99978637695312, Learning Rate: 0.01\n",
      "Epoch [3412/10000], Loss: 90.99134826660156, Learning Rate: 0.01\n",
      "Epoch [3413/10000], Loss: 90.98282623291016, Learning Rate: 0.01\n",
      "Epoch [3414/10000], Loss: 90.974365234375, Learning Rate: 0.01\n",
      "Epoch [3415/10000], Loss: 90.9657974243164, Learning Rate: 0.01\n",
      "Epoch [3416/10000], Loss: 90.95720672607422, Learning Rate: 0.01\n",
      "Epoch [3417/10000], Loss: 90.94879150390625, Learning Rate: 0.01\n",
      "Epoch [3418/10000], Loss: 90.9401626586914, Learning Rate: 0.01\n",
      "Epoch [3419/10000], Loss: 90.9317398071289, Learning Rate: 0.01\n",
      "Epoch [3420/10000], Loss: 90.92315673828125, Learning Rate: 0.01\n",
      "Epoch [3421/10000], Loss: 90.91455841064453, Learning Rate: 0.01\n",
      "Epoch [3422/10000], Loss: 90.90611267089844, Learning Rate: 0.01\n",
      "Epoch [3423/10000], Loss: 90.89751434326172, Learning Rate: 0.01\n",
      "Epoch [3424/10000], Loss: 90.88905334472656, Learning Rate: 0.01\n",
      "Epoch [3425/10000], Loss: 90.8804702758789, Learning Rate: 0.01\n",
      "Epoch [3426/10000], Loss: 90.87201690673828, Learning Rate: 0.01\n",
      "Epoch [3427/10000], Loss: 90.86347961425781, Learning Rate: 0.01\n",
      "Epoch [3428/10000], Loss: 90.85491943359375, Learning Rate: 0.01\n",
      "Epoch [3429/10000], Loss: 90.84645080566406, Learning Rate: 0.01\n",
      "Epoch [3430/10000], Loss: 90.83787536621094, Learning Rate: 0.01\n",
      "Epoch [3431/10000], Loss: 90.82942199707031, Learning Rate: 0.01\n",
      "Epoch [3432/10000], Loss: 90.82081604003906, Learning Rate: 0.01\n",
      "Epoch [3433/10000], Loss: 90.81238555908203, Learning Rate: 0.01\n",
      "Epoch [3434/10000], Loss: 90.80384063720703, Learning Rate: 0.01\n",
      "Epoch [3435/10000], Loss: 90.79536437988281, Learning Rate: 0.01\n",
      "Epoch [3436/10000], Loss: 90.78687286376953, Learning Rate: 0.01\n",
      "Epoch [3437/10000], Loss: 90.77830505371094, Learning Rate: 0.01\n",
      "Epoch [3438/10000], Loss: 90.76986694335938, Learning Rate: 0.01\n",
      "Epoch [3439/10000], Loss: 90.76132202148438, Learning Rate: 0.01\n",
      "Epoch [3440/10000], Loss: 90.75276947021484, Learning Rate: 0.01\n",
      "Epoch [3441/10000], Loss: 90.74437713623047, Learning Rate: 0.01\n",
      "Epoch [3442/10000], Loss: 90.73587036132812, Learning Rate: 0.01\n",
      "Epoch [3443/10000], Loss: 90.727294921875, Learning Rate: 0.01\n",
      "Epoch [3444/10000], Loss: 90.71873474121094, Learning Rate: 0.01\n",
      "Epoch [3445/10000], Loss: 90.71037292480469, Learning Rate: 0.01\n",
      "Epoch [3446/10000], Loss: 90.70172882080078, Learning Rate: 0.01\n",
      "Epoch [3447/10000], Loss: 90.69331359863281, Learning Rate: 0.01\n",
      "Epoch [3448/10000], Loss: 90.684814453125, Learning Rate: 0.01\n",
      "Epoch [3449/10000], Loss: 90.67633056640625, Learning Rate: 0.01\n",
      "Epoch [3450/10000], Loss: 90.66778564453125, Learning Rate: 0.01\n",
      "Epoch [3451/10000], Loss: 90.65933227539062, Learning Rate: 0.01\n",
      "Epoch [3452/10000], Loss: 90.65087127685547, Learning Rate: 0.01\n",
      "Epoch [3453/10000], Loss: 90.64244079589844, Learning Rate: 0.01\n",
      "Epoch [3454/10000], Loss: 90.63387298583984, Learning Rate: 0.01\n",
      "Epoch [3455/10000], Loss: 90.6253662109375, Learning Rate: 0.01\n",
      "Epoch [3456/10000], Loss: 90.61695861816406, Learning Rate: 0.01\n",
      "Epoch [3457/10000], Loss: 90.6083984375, Learning Rate: 0.01\n",
      "Epoch [3458/10000], Loss: 90.59988403320312, Learning Rate: 0.01\n",
      "Epoch [3459/10000], Loss: 90.5914306640625, Learning Rate: 0.01\n",
      "Epoch [3460/10000], Loss: 90.58289337158203, Learning Rate: 0.01\n",
      "Epoch [3461/10000], Loss: 90.5744857788086, Learning Rate: 0.01\n",
      "Epoch [3462/10000], Loss: 90.56597137451172, Learning Rate: 0.01\n",
      "Epoch [3463/10000], Loss: 90.55742645263672, Learning Rate: 0.01\n",
      "Epoch [3464/10000], Loss: 90.54901885986328, Learning Rate: 0.01\n",
      "Epoch [3465/10000], Loss: 90.54051971435547, Learning Rate: 0.01\n",
      "Epoch [3466/10000], Loss: 90.53199005126953, Learning Rate: 0.01\n",
      "Epoch [3467/10000], Loss: 90.52346801757812, Learning Rate: 0.01\n",
      "Epoch [3468/10000], Loss: 90.51504516601562, Learning Rate: 0.01\n",
      "Epoch [3469/10000], Loss: 90.50650787353516, Learning Rate: 0.01\n",
      "Epoch [3470/10000], Loss: 90.49808502197266, Learning Rate: 0.01\n",
      "Epoch [3471/10000], Loss: 90.4896011352539, Learning Rate: 0.01\n",
      "Epoch [3472/10000], Loss: 90.481201171875, Learning Rate: 0.01\n",
      "Epoch [3473/10000], Loss: 90.47264099121094, Learning Rate: 0.01\n",
      "Epoch [3474/10000], Loss: 90.46415710449219, Learning Rate: 0.01\n",
      "Epoch [3475/10000], Loss: 90.45574951171875, Learning Rate: 0.01\n",
      "Epoch [3476/10000], Loss: 90.4471435546875, Learning Rate: 0.01\n",
      "Epoch [3477/10000], Loss: 90.4387435913086, Learning Rate: 0.01\n",
      "Epoch [3478/10000], Loss: 90.4301986694336, Learning Rate: 0.01\n",
      "Epoch [3479/10000], Loss: 90.42181396484375, Learning Rate: 0.01\n",
      "Epoch [3480/10000], Loss: 90.41338348388672, Learning Rate: 0.01\n",
      "Epoch [3481/10000], Loss: 90.40487670898438, Learning Rate: 0.01\n",
      "Epoch [3482/10000], Loss: 90.39640808105469, Learning Rate: 0.01\n",
      "Epoch [3483/10000], Loss: 90.38787841796875, Learning Rate: 0.01\n",
      "Epoch [3484/10000], Loss: 90.37952423095703, Learning Rate: 0.01\n",
      "Epoch [3485/10000], Loss: 90.3709945678711, Learning Rate: 0.01\n",
      "Epoch [3486/10000], Loss: 90.36251831054688, Learning Rate: 0.01\n",
      "Epoch [3487/10000], Loss: 90.35403442382812, Learning Rate: 0.01\n",
      "Epoch [3488/10000], Loss: 90.34563446044922, Learning Rate: 0.01\n",
      "Epoch [3489/10000], Loss: 90.337158203125, Learning Rate: 0.01\n",
      "Epoch [3490/10000], Loss: 90.32869720458984, Learning Rate: 0.01\n",
      "Epoch [3491/10000], Loss: 90.32025909423828, Learning Rate: 0.01\n",
      "Epoch [3492/10000], Loss: 90.31175994873047, Learning Rate: 0.01\n",
      "Epoch [3493/10000], Loss: 90.30326843261719, Learning Rate: 0.01\n",
      "Epoch [3494/10000], Loss: 90.29491424560547, Learning Rate: 0.01\n",
      "Epoch [3495/10000], Loss: 90.28643798828125, Learning Rate: 0.01\n",
      "Epoch [3496/10000], Loss: 90.27792358398438, Learning Rate: 0.01\n",
      "Epoch [3497/10000], Loss: 90.26951599121094, Learning Rate: 0.01\n",
      "Epoch [3498/10000], Loss: 90.26107788085938, Learning Rate: 0.01\n",
      "Epoch [3499/10000], Loss: 90.25257110595703, Learning Rate: 0.01\n",
      "Epoch [3500/10000], Loss: 90.24413299560547, Learning Rate: 0.01\n",
      "Epoch [3501/10000], Loss: 90.23567199707031, Learning Rate: 0.01\n",
      "Epoch [3502/10000], Loss: 90.22724914550781, Learning Rate: 0.01\n",
      "Epoch [3503/10000], Loss: 90.21871948242188, Learning Rate: 0.01\n",
      "Epoch [3504/10000], Loss: 90.21035766601562, Learning Rate: 0.01\n",
      "Epoch [3505/10000], Loss: 90.20184326171875, Learning Rate: 0.01\n",
      "Epoch [3506/10000], Loss: 90.19347381591797, Learning Rate: 0.01\n",
      "Epoch [3507/10000], Loss: 90.18498229980469, Learning Rate: 0.01\n",
      "Epoch [3508/10000], Loss: 90.17650604248047, Learning Rate: 0.01\n",
      "Epoch [3509/10000], Loss: 90.16815185546875, Learning Rate: 0.01\n",
      "Epoch [3510/10000], Loss: 90.15970611572266, Learning Rate: 0.01\n",
      "Epoch [3511/10000], Loss: 90.15121459960938, Learning Rate: 0.01\n",
      "Epoch [3512/10000], Loss: 90.14279174804688, Learning Rate: 0.01\n",
      "Epoch [3513/10000], Loss: 90.13431549072266, Learning Rate: 0.01\n",
      "Epoch [3514/10000], Loss: 90.12591552734375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3515/10000], Loss: 90.11750793457031, Learning Rate: 0.01\n",
      "Epoch [3516/10000], Loss: 90.10903930664062, Learning Rate: 0.01\n",
      "Epoch [3517/10000], Loss: 90.1005630493164, Learning Rate: 0.01\n",
      "Epoch [3518/10000], Loss: 90.09208679199219, Learning Rate: 0.01\n",
      "Epoch [3519/10000], Loss: 90.0837173461914, Learning Rate: 0.01\n",
      "Epoch [3520/10000], Loss: 90.07524108886719, Learning Rate: 0.01\n",
      "Epoch [3521/10000], Loss: 90.06685638427734, Learning Rate: 0.01\n",
      "Epoch [3522/10000], Loss: 90.05843353271484, Learning Rate: 0.01\n",
      "Epoch [3523/10000], Loss: 90.0499496459961, Learning Rate: 0.01\n",
      "Epoch [3524/10000], Loss: 90.04157257080078, Learning Rate: 0.01\n",
      "Epoch [3525/10000], Loss: 90.03314208984375, Learning Rate: 0.01\n",
      "Epoch [3526/10000], Loss: 90.02460479736328, Learning Rate: 0.01\n",
      "Epoch [3527/10000], Loss: 90.0162582397461, Learning Rate: 0.01\n",
      "Epoch [3528/10000], Loss: 90.00782775878906, Learning Rate: 0.01\n",
      "Epoch [3529/10000], Loss: 89.99948120117188, Learning Rate: 0.01\n",
      "Epoch [3530/10000], Loss: 89.99103546142578, Learning Rate: 0.01\n",
      "Epoch [3531/10000], Loss: 89.98262023925781, Learning Rate: 0.01\n",
      "Epoch [3532/10000], Loss: 89.97415161132812, Learning Rate: 0.01\n",
      "Epoch [3533/10000], Loss: 89.96565246582031, Learning Rate: 0.01\n",
      "Epoch [3534/10000], Loss: 89.95729064941406, Learning Rate: 0.01\n",
      "Epoch [3535/10000], Loss: 89.94886016845703, Learning Rate: 0.01\n",
      "Epoch [3536/10000], Loss: 89.94049835205078, Learning Rate: 0.01\n",
      "Epoch [3537/10000], Loss: 89.93207550048828, Learning Rate: 0.01\n",
      "Epoch [3538/10000], Loss: 89.92369079589844, Learning Rate: 0.01\n",
      "Epoch [3539/10000], Loss: 89.91519927978516, Learning Rate: 0.01\n",
      "Epoch [3540/10000], Loss: 89.90684509277344, Learning Rate: 0.01\n",
      "Epoch [3541/10000], Loss: 89.89834594726562, Learning Rate: 0.01\n",
      "Epoch [3542/10000], Loss: 89.88992309570312, Learning Rate: 0.01\n",
      "Epoch [3543/10000], Loss: 89.88155364990234, Learning Rate: 0.01\n",
      "Epoch [3544/10000], Loss: 89.87318420410156, Learning Rate: 0.01\n",
      "Epoch [3545/10000], Loss: 89.86479187011719, Learning Rate: 0.01\n",
      "Epoch [3546/10000], Loss: 89.8563461303711, Learning Rate: 0.01\n",
      "Epoch [3547/10000], Loss: 89.8479232788086, Learning Rate: 0.01\n",
      "Epoch [3548/10000], Loss: 89.8395004272461, Learning Rate: 0.01\n",
      "Epoch [3549/10000], Loss: 89.83116149902344, Learning Rate: 0.01\n",
      "Epoch [3550/10000], Loss: 89.82269287109375, Learning Rate: 0.01\n",
      "Epoch [3551/10000], Loss: 89.8143081665039, Learning Rate: 0.01\n",
      "Epoch [3552/10000], Loss: 89.80589294433594, Learning Rate: 0.01\n",
      "Epoch [3553/10000], Loss: 89.7975082397461, Learning Rate: 0.01\n",
      "Epoch [3554/10000], Loss: 89.78907775878906, Learning Rate: 0.01\n",
      "Epoch [3555/10000], Loss: 89.78063201904297, Learning Rate: 0.01\n",
      "Epoch [3556/10000], Loss: 89.77230072021484, Learning Rate: 0.01\n",
      "Epoch [3557/10000], Loss: 89.76380157470703, Learning Rate: 0.01\n",
      "Epoch [3558/10000], Loss: 89.75548553466797, Learning Rate: 0.01\n",
      "Epoch [3559/10000], Loss: 89.74706268310547, Learning Rate: 0.01\n",
      "Epoch [3560/10000], Loss: 89.73863983154297, Learning Rate: 0.01\n",
      "Epoch [3561/10000], Loss: 89.73026275634766, Learning Rate: 0.01\n",
      "Epoch [3562/10000], Loss: 89.72185516357422, Learning Rate: 0.01\n",
      "Epoch [3563/10000], Loss: 89.7134780883789, Learning Rate: 0.01\n",
      "Epoch [3564/10000], Loss: 89.705078125, Learning Rate: 0.01\n",
      "Epoch [3565/10000], Loss: 89.69670104980469, Learning Rate: 0.01\n",
      "Epoch [3566/10000], Loss: 89.68829345703125, Learning Rate: 0.01\n",
      "Epoch [3567/10000], Loss: 89.67989349365234, Learning Rate: 0.01\n",
      "Epoch [3568/10000], Loss: 89.67147064208984, Learning Rate: 0.01\n",
      "Epoch [3569/10000], Loss: 89.66312408447266, Learning Rate: 0.01\n",
      "Epoch [3570/10000], Loss: 89.65472412109375, Learning Rate: 0.01\n",
      "Epoch [3571/10000], Loss: 89.64634704589844, Learning Rate: 0.01\n",
      "Epoch [3572/10000], Loss: 89.63800811767578, Learning Rate: 0.01\n",
      "Epoch [3573/10000], Loss: 89.62965393066406, Learning Rate: 0.01\n",
      "Epoch [3574/10000], Loss: 89.6212158203125, Learning Rate: 0.01\n",
      "Epoch [3575/10000], Loss: 89.61283111572266, Learning Rate: 0.01\n",
      "Epoch [3576/10000], Loss: 89.60447692871094, Learning Rate: 0.01\n",
      "Epoch [3577/10000], Loss: 89.59615325927734, Learning Rate: 0.01\n",
      "Epoch [3578/10000], Loss: 89.587646484375, Learning Rate: 0.01\n",
      "Epoch [3579/10000], Loss: 89.57929229736328, Learning Rate: 0.01\n",
      "Epoch [3580/10000], Loss: 89.57091522216797, Learning Rate: 0.01\n",
      "Epoch [3581/10000], Loss: 89.56254577636719, Learning Rate: 0.01\n",
      "Epoch [3582/10000], Loss: 89.55416870117188, Learning Rate: 0.01\n",
      "Epoch [3583/10000], Loss: 89.54583740234375, Learning Rate: 0.01\n",
      "Epoch [3584/10000], Loss: 89.53736114501953, Learning Rate: 0.01\n",
      "Epoch [3585/10000], Loss: 89.52908325195312, Learning Rate: 0.01\n",
      "Epoch [3586/10000], Loss: 89.52066040039062, Learning Rate: 0.01\n",
      "Epoch [3587/10000], Loss: 89.51228332519531, Learning Rate: 0.01\n",
      "Epoch [3588/10000], Loss: 89.50395965576172, Learning Rate: 0.01\n",
      "Epoch [3589/10000], Loss: 89.49555969238281, Learning Rate: 0.01\n",
      "Epoch [3590/10000], Loss: 89.4872055053711, Learning Rate: 0.01\n",
      "Epoch [3591/10000], Loss: 89.47887420654297, Learning Rate: 0.01\n",
      "Epoch [3592/10000], Loss: 89.47049713134766, Learning Rate: 0.01\n",
      "Epoch [3593/10000], Loss: 89.46212768554688, Learning Rate: 0.01\n",
      "Epoch [3594/10000], Loss: 89.45380401611328, Learning Rate: 0.01\n",
      "Epoch [3595/10000], Loss: 89.44535064697266, Learning Rate: 0.01\n",
      "Epoch [3596/10000], Loss: 89.43704223632812, Learning Rate: 0.01\n",
      "Epoch [3597/10000], Loss: 89.4288330078125, Learning Rate: 0.01\n",
      "Epoch [3598/10000], Loss: 89.42057800292969, Learning Rate: 0.01\n",
      "Epoch [3599/10000], Loss: 89.41236114501953, Learning Rate: 0.01\n",
      "Epoch [3600/10000], Loss: 89.4041519165039, Learning Rate: 0.01\n",
      "Epoch [3601/10000], Loss: 89.39610290527344, Learning Rate: 0.01\n",
      "Epoch [3602/10000], Loss: 89.38829040527344, Learning Rate: 0.01\n",
      "Epoch [3603/10000], Loss: 89.380615234375, Learning Rate: 0.01\n",
      "Epoch [3604/10000], Loss: 89.37329864501953, Learning Rate: 0.01\n",
      "Epoch [3605/10000], Loss: 89.36658477783203, Learning Rate: 0.01\n",
      "Epoch [3606/10000], Loss: 89.36050415039062, Learning Rate: 0.01\n",
      "Epoch [3607/10000], Loss: 89.35509490966797, Learning Rate: 0.01\n",
      "Epoch [3608/10000], Loss: 89.35011291503906, Learning Rate: 0.01\n",
      "Epoch [3609/10000], Loss: 89.34459686279297, Learning Rate: 0.01\n",
      "Epoch [3610/10000], Loss: 89.33759307861328, Learning Rate: 0.01\n",
      "Epoch [3611/10000], Loss: 89.32740020751953, Learning Rate: 0.01\n",
      "Epoch [3612/10000], Loss: 89.31424713134766, Learning Rate: 0.01\n",
      "Epoch [3613/10000], Loss: 89.29995727539062, Learning Rate: 0.01\n",
      "Epoch [3614/10000], Loss: 89.28750610351562, Learning Rate: 0.01\n",
      "Epoch [3615/10000], Loss: 89.27843475341797, Learning Rate: 0.01\n",
      "Epoch [3616/10000], Loss: 89.27220153808594, Learning Rate: 0.01\n",
      "Epoch [3617/10000], Loss: 89.26678466796875, Learning Rate: 0.01\n",
      "Epoch [3618/10000], Loss: 89.26002502441406, Learning Rate: 0.01\n",
      "Epoch [3619/10000], Loss: 89.25086212158203, Learning Rate: 0.01\n",
      "Epoch [3620/10000], Loss: 89.24007415771484, Learning Rate: 0.01\n",
      "Epoch [3621/10000], Loss: 89.22917938232422, Learning Rate: 0.01\n",
      "Epoch [3622/10000], Loss: 89.21988677978516, Learning Rate: 0.01\n",
      "Epoch [3623/10000], Loss: 89.21228790283203, Learning Rate: 0.01\n",
      "Epoch [3624/10000], Loss: 89.20545959472656, Learning Rate: 0.01\n",
      "Epoch [3625/10000], Loss: 89.19807434082031, Learning Rate: 0.01\n",
      "Epoch [3626/10000], Loss: 89.18944549560547, Learning Rate: 0.01\n",
      "Epoch [3627/10000], Loss: 89.1799087524414, Learning Rate: 0.01\n",
      "Epoch [3628/10000], Loss: 89.17032623291016, Learning Rate: 0.01\n",
      "Epoch [3629/10000], Loss: 89.16151428222656, Learning Rate: 0.01\n",
      "Epoch [3630/10000], Loss: 89.15363311767578, Learning Rate: 0.01\n",
      "Epoch [3631/10000], Loss: 89.14602661132812, Learning Rate: 0.01\n",
      "Epoch [3632/10000], Loss: 89.13809967041016, Learning Rate: 0.01\n",
      "Epoch [3633/10000], Loss: 89.12962341308594, Learning Rate: 0.01\n",
      "Epoch [3634/10000], Loss: 89.12065887451172, Learning Rate: 0.01\n",
      "Epoch [3635/10000], Loss: 89.1117935180664, Learning Rate: 0.01\n",
      "Epoch [3636/10000], Loss: 89.103271484375, Learning Rate: 0.01\n",
      "Epoch [3637/10000], Loss: 89.09513854980469, Learning Rate: 0.01\n",
      "Epoch [3638/10000], Loss: 89.08711242675781, Learning Rate: 0.01\n",
      "Epoch [3639/10000], Loss: 89.07902526855469, Learning Rate: 0.01\n",
      "Epoch [3640/10000], Loss: 89.07064056396484, Learning Rate: 0.01\n",
      "Epoch [3641/10000], Loss: 89.06202697753906, Learning Rate: 0.01\n",
      "Epoch [3642/10000], Loss: 89.05343627929688, Learning Rate: 0.01\n",
      "Epoch [3643/10000], Loss: 89.04501342773438, Learning Rate: 0.01\n",
      "Epoch [3644/10000], Loss: 89.03679656982422, Learning Rate: 0.01\n",
      "Epoch [3645/10000], Loss: 89.02864837646484, Learning Rate: 0.01\n",
      "Epoch [3646/10000], Loss: 89.02037811279297, Learning Rate: 0.01\n",
      "Epoch [3647/10000], Loss: 89.01210021972656, Learning Rate: 0.01\n",
      "Epoch [3648/10000], Loss: 89.00362396240234, Learning Rate: 0.01\n",
      "Epoch [3649/10000], Loss: 88.9952621459961, Learning Rate: 0.01\n",
      "Epoch [3650/10000], Loss: 88.98683166503906, Learning Rate: 0.01\n",
      "Epoch [3651/10000], Loss: 88.97850799560547, Learning Rate: 0.01\n",
      "Epoch [3652/10000], Loss: 88.97022247314453, Learning Rate: 0.01\n",
      "Epoch [3653/10000], Loss: 88.96198272705078, Learning Rate: 0.01\n",
      "Epoch [3654/10000], Loss: 88.9537353515625, Learning Rate: 0.01\n",
      "Epoch [3655/10000], Loss: 88.94537353515625, Learning Rate: 0.01\n",
      "Epoch [3656/10000], Loss: 88.93708801269531, Learning Rate: 0.01\n",
      "Epoch [3657/10000], Loss: 88.92859649658203, Learning Rate: 0.01\n",
      "Epoch [3658/10000], Loss: 88.92031860351562, Learning Rate: 0.01\n",
      "Epoch [3659/10000], Loss: 88.91204833984375, Learning Rate: 0.01\n",
      "Epoch [3660/10000], Loss: 88.90371704101562, Learning Rate: 0.01\n",
      "Epoch [3661/10000], Loss: 88.89544677734375, Learning Rate: 0.01\n",
      "Epoch [3662/10000], Loss: 88.88723754882812, Learning Rate: 0.01\n",
      "Epoch [3663/10000], Loss: 88.87882995605469, Learning Rate: 0.01\n",
      "Epoch [3664/10000], Loss: 88.87053680419922, Learning Rate: 0.01\n",
      "Epoch [3665/10000], Loss: 88.86222839355469, Learning Rate: 0.01\n",
      "Epoch [3666/10000], Loss: 88.85395050048828, Learning Rate: 0.01\n",
      "Epoch [3667/10000], Loss: 88.84554290771484, Learning Rate: 0.01\n",
      "Epoch [3668/10000], Loss: 88.83733367919922, Learning Rate: 0.01\n",
      "Epoch [3669/10000], Loss: 88.82908630371094, Learning Rate: 0.01\n",
      "Epoch [3670/10000], Loss: 88.8207778930664, Learning Rate: 0.01\n",
      "Epoch [3671/10000], Loss: 88.81238555908203, Learning Rate: 0.01\n",
      "Epoch [3672/10000], Loss: 88.80415344238281, Learning Rate: 0.01\n",
      "Epoch [3673/10000], Loss: 88.79584503173828, Learning Rate: 0.01\n",
      "Epoch [3674/10000], Loss: 88.78759765625, Learning Rate: 0.01\n",
      "Epoch [3675/10000], Loss: 88.77919006347656, Learning Rate: 0.01\n",
      "Epoch [3676/10000], Loss: 88.77095031738281, Learning Rate: 0.01\n",
      "Epoch [3677/10000], Loss: 88.7627182006836, Learning Rate: 0.01\n",
      "Epoch [3678/10000], Loss: 88.75436401367188, Learning Rate: 0.01\n",
      "Epoch [3679/10000], Loss: 88.74614715576172, Learning Rate: 0.01\n",
      "Epoch [3680/10000], Loss: 88.73783874511719, Learning Rate: 0.01\n",
      "Epoch [3681/10000], Loss: 88.72959899902344, Learning Rate: 0.01\n",
      "Epoch [3682/10000], Loss: 88.72127532958984, Learning Rate: 0.01\n",
      "Epoch [3683/10000], Loss: 88.71295166015625, Learning Rate: 0.01\n",
      "Epoch [3684/10000], Loss: 88.70475769042969, Learning Rate: 0.01\n",
      "Epoch [3685/10000], Loss: 88.69640350341797, Learning Rate: 0.01\n",
      "Epoch [3686/10000], Loss: 88.68814849853516, Learning Rate: 0.01\n",
      "Epoch [3687/10000], Loss: 88.67989349365234, Learning Rate: 0.01\n",
      "Epoch [3688/10000], Loss: 88.67156982421875, Learning Rate: 0.01\n",
      "Epoch [3689/10000], Loss: 88.6633071899414, Learning Rate: 0.01\n",
      "Epoch [3690/10000], Loss: 88.655029296875, Learning Rate: 0.01\n",
      "Epoch [3691/10000], Loss: 88.64679718017578, Learning Rate: 0.01\n",
      "Epoch [3692/10000], Loss: 88.63842010498047, Learning Rate: 0.01\n",
      "Epoch [3693/10000], Loss: 88.63023376464844, Learning Rate: 0.01\n",
      "Epoch [3694/10000], Loss: 88.62194061279297, Learning Rate: 0.01\n",
      "Epoch [3695/10000], Loss: 88.6137466430664, Learning Rate: 0.01\n",
      "Epoch [3696/10000], Loss: 88.60536193847656, Learning Rate: 0.01\n",
      "Epoch [3697/10000], Loss: 88.59713745117188, Learning Rate: 0.01\n",
      "Epoch [3698/10000], Loss: 88.5888671875, Learning Rate: 0.01\n",
      "Epoch [3699/10000], Loss: 88.5805435180664, Learning Rate: 0.01\n",
      "Epoch [3700/10000], Loss: 88.57237243652344, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3701/10000], Loss: 88.56407165527344, Learning Rate: 0.01\n",
      "Epoch [3702/10000], Loss: 88.5558090209961, Learning Rate: 0.01\n",
      "Epoch [3703/10000], Loss: 88.54749298095703, Learning Rate: 0.01\n",
      "Epoch [3704/10000], Loss: 88.53927612304688, Learning Rate: 0.01\n",
      "Epoch [3705/10000], Loss: 88.53099060058594, Learning Rate: 0.01\n",
      "Epoch [3706/10000], Loss: 88.52275085449219, Learning Rate: 0.01\n",
      "Epoch [3707/10000], Loss: 88.51445007324219, Learning Rate: 0.01\n",
      "Epoch [3708/10000], Loss: 88.50630187988281, Learning Rate: 0.01\n",
      "Epoch [3709/10000], Loss: 88.4979248046875, Learning Rate: 0.01\n",
      "Epoch [3710/10000], Loss: 88.48971557617188, Learning Rate: 0.01\n",
      "Epoch [3711/10000], Loss: 88.48149108886719, Learning Rate: 0.01\n",
      "Epoch [3712/10000], Loss: 88.4731674194336, Learning Rate: 0.01\n",
      "Epoch [3713/10000], Loss: 88.46495819091797, Learning Rate: 0.01\n",
      "Epoch [3714/10000], Loss: 88.45671081542969, Learning Rate: 0.01\n",
      "Epoch [3715/10000], Loss: 88.44851684570312, Learning Rate: 0.01\n",
      "Epoch [3716/10000], Loss: 88.44017791748047, Learning Rate: 0.01\n",
      "Epoch [3717/10000], Loss: 88.4319076538086, Learning Rate: 0.01\n",
      "Epoch [3718/10000], Loss: 88.4236831665039, Learning Rate: 0.01\n",
      "Epoch [3719/10000], Loss: 88.41547393798828, Learning Rate: 0.01\n",
      "Epoch [3720/10000], Loss: 88.4072494506836, Learning Rate: 0.01\n",
      "Epoch [3721/10000], Loss: 88.39896392822266, Learning Rate: 0.01\n",
      "Epoch [3722/10000], Loss: 88.39073944091797, Learning Rate: 0.01\n",
      "Epoch [3723/10000], Loss: 88.38241577148438, Learning Rate: 0.01\n",
      "Epoch [3724/10000], Loss: 88.37422943115234, Learning Rate: 0.01\n",
      "Epoch [3725/10000], Loss: 88.36593627929688, Learning Rate: 0.01\n",
      "Epoch [3726/10000], Loss: 88.35774993896484, Learning Rate: 0.01\n",
      "Epoch [3727/10000], Loss: 88.34954071044922, Learning Rate: 0.01\n",
      "Epoch [3728/10000], Loss: 88.34123229980469, Learning Rate: 0.01\n",
      "Epoch [3729/10000], Loss: 88.3331298828125, Learning Rate: 0.01\n",
      "Epoch [3730/10000], Loss: 88.32479095458984, Learning Rate: 0.01\n",
      "Epoch [3731/10000], Loss: 88.31654357910156, Learning Rate: 0.01\n",
      "Epoch [3732/10000], Loss: 88.30826568603516, Learning Rate: 0.01\n",
      "Epoch [3733/10000], Loss: 88.29999542236328, Learning Rate: 0.01\n",
      "Epoch [3734/10000], Loss: 88.29187774658203, Learning Rate: 0.01\n",
      "Epoch [3735/10000], Loss: 88.28363800048828, Learning Rate: 0.01\n",
      "Epoch [3736/10000], Loss: 88.27538299560547, Learning Rate: 0.01\n",
      "Epoch [3737/10000], Loss: 88.26712799072266, Learning Rate: 0.01\n",
      "Epoch [3738/10000], Loss: 88.2589111328125, Learning Rate: 0.01\n",
      "Epoch [3739/10000], Loss: 88.2507095336914, Learning Rate: 0.01\n",
      "Epoch [3740/10000], Loss: 88.24237823486328, Learning Rate: 0.01\n",
      "Epoch [3741/10000], Loss: 88.23423767089844, Learning Rate: 0.01\n",
      "Epoch [3742/10000], Loss: 88.22601318359375, Learning Rate: 0.01\n",
      "Epoch [3743/10000], Loss: 88.21769714355469, Learning Rate: 0.01\n",
      "Epoch [3744/10000], Loss: 88.20951080322266, Learning Rate: 0.01\n",
      "Epoch [3745/10000], Loss: 88.2012710571289, Learning Rate: 0.01\n",
      "Epoch [3746/10000], Loss: 88.19306945800781, Learning Rate: 0.01\n",
      "Epoch [3747/10000], Loss: 88.18485260009766, Learning Rate: 0.01\n",
      "Epoch [3748/10000], Loss: 88.17658996582031, Learning Rate: 0.01\n",
      "Epoch [3749/10000], Loss: 88.16846466064453, Learning Rate: 0.01\n",
      "Epoch [3750/10000], Loss: 88.16010284423828, Learning Rate: 0.01\n",
      "Epoch [3751/10000], Loss: 88.15192413330078, Learning Rate: 0.01\n",
      "Epoch [3752/10000], Loss: 88.143798828125, Learning Rate: 0.01\n",
      "Epoch [3753/10000], Loss: 88.13553619384766, Learning Rate: 0.01\n",
      "Epoch [3754/10000], Loss: 88.12732696533203, Learning Rate: 0.01\n",
      "Epoch [3755/10000], Loss: 88.1191177368164, Learning Rate: 0.01\n",
      "Epoch [3756/10000], Loss: 88.11090087890625, Learning Rate: 0.01\n",
      "Epoch [3757/10000], Loss: 88.10261535644531, Learning Rate: 0.01\n",
      "Epoch [3758/10000], Loss: 88.09449005126953, Learning Rate: 0.01\n",
      "Epoch [3759/10000], Loss: 88.08631896972656, Learning Rate: 0.01\n",
      "Epoch [3760/10000], Loss: 88.07801818847656, Learning Rate: 0.01\n",
      "Epoch [3761/10000], Loss: 88.06991577148438, Learning Rate: 0.01\n",
      "Epoch [3762/10000], Loss: 88.06166076660156, Learning Rate: 0.01\n",
      "Epoch [3763/10000], Loss: 88.05340576171875, Learning Rate: 0.01\n",
      "Epoch [3764/10000], Loss: 88.04521179199219, Learning Rate: 0.01\n",
      "Epoch [3765/10000], Loss: 88.0370101928711, Learning Rate: 0.01\n",
      "Epoch [3766/10000], Loss: 88.02877807617188, Learning Rate: 0.01\n",
      "Epoch [3767/10000], Loss: 88.0206527709961, Learning Rate: 0.01\n",
      "Epoch [3768/10000], Loss: 88.0124740600586, Learning Rate: 0.01\n",
      "Epoch [3769/10000], Loss: 88.00426483154297, Learning Rate: 0.01\n",
      "Epoch [3770/10000], Loss: 87.99618530273438, Learning Rate: 0.01\n",
      "Epoch [3771/10000], Loss: 87.98809814453125, Learning Rate: 0.01\n",
      "Epoch [3772/10000], Loss: 87.98008728027344, Learning Rate: 0.01\n",
      "Epoch [3773/10000], Loss: 87.97225952148438, Learning Rate: 0.01\n",
      "Epoch [3774/10000], Loss: 87.96475219726562, Learning Rate: 0.01\n",
      "Epoch [3775/10000], Loss: 87.95746612548828, Learning Rate: 0.01\n",
      "Epoch [3776/10000], Loss: 87.95105743408203, Learning Rate: 0.01\n",
      "Epoch [3777/10000], Loss: 87.94562530517578, Learning Rate: 0.01\n",
      "Epoch [3778/10000], Loss: 87.94200897216797, Learning Rate: 0.01\n",
      "Epoch [3779/10000], Loss: 87.94049835205078, Learning Rate: 0.01\n",
      "Epoch [3780/10000], Loss: 87.9407730102539, Learning Rate: 0.01\n",
      "Epoch [3781/10000], Loss: 87.94095611572266, Learning Rate: 0.01\n",
      "Epoch [3782/10000], Loss: 87.93646240234375, Learning Rate: 0.01\n",
      "Epoch [3783/10000], Loss: 87.92269134521484, Learning Rate: 0.01\n",
      "Epoch [3784/10000], Loss: 87.90033721923828, Learning Rate: 0.01\n",
      "Epoch [3785/10000], Loss: 87.87813568115234, Learning Rate: 0.01\n",
      "Epoch [3786/10000], Loss: 87.86492919921875, Learning Rate: 0.01\n",
      "Epoch [3787/10000], Loss: 87.86193084716797, Learning Rate: 0.01\n",
      "Epoch [3788/10000], Loss: 87.86219024658203, Learning Rate: 0.01\n",
      "Epoch [3789/10000], Loss: 87.85721588134766, Learning Rate: 0.01\n",
      "Epoch [3790/10000], Loss: 87.84432983398438, Learning Rate: 0.01\n",
      "Epoch [3791/10000], Loss: 87.82799530029297, Learning Rate: 0.01\n",
      "Epoch [3792/10000], Loss: 87.81590270996094, Learning Rate: 0.01\n",
      "Epoch [3793/10000], Loss: 87.81021881103516, Learning Rate: 0.01\n",
      "Epoch [3794/10000], Loss: 87.80685424804688, Learning Rate: 0.01\n",
      "Epoch [3795/10000], Loss: 87.80016326904297, Learning Rate: 0.01\n",
      "Epoch [3796/10000], Loss: 87.78883361816406, Learning Rate: 0.01\n",
      "Epoch [3797/10000], Loss: 87.77640533447266, Learning Rate: 0.01\n",
      "Epoch [3798/10000], Loss: 87.76698303222656, Learning Rate: 0.01\n",
      "Epoch [3799/10000], Loss: 87.7608413696289, Learning Rate: 0.01\n",
      "Epoch [3800/10000], Loss: 87.75515747070312, Learning Rate: 0.01\n",
      "Epoch [3801/10000], Loss: 87.74694061279297, Learning Rate: 0.01\n",
      "Epoch [3802/10000], Loss: 87.73637390136719, Learning Rate: 0.01\n",
      "Epoch [3803/10000], Loss: 87.72631072998047, Learning Rate: 0.01\n",
      "Epoch [3804/10000], Loss: 87.71826171875, Learning Rate: 0.01\n",
      "Epoch [3805/10000], Loss: 87.71154022216797, Learning Rate: 0.01\n",
      "Epoch [3806/10000], Loss: 87.70431518554688, Learning Rate: 0.01\n",
      "Epoch [3807/10000], Loss: 87.69558715820312, Learning Rate: 0.01\n",
      "Epoch [3808/10000], Loss: 87.68596649169922, Learning Rate: 0.01\n",
      "Epoch [3809/10000], Loss: 87.67710876464844, Learning Rate: 0.01\n",
      "Epoch [3810/10000], Loss: 87.66940307617188, Learning Rate: 0.01\n",
      "Epoch [3811/10000], Loss: 87.66209411621094, Learning Rate: 0.01\n",
      "Epoch [3812/10000], Loss: 87.6541748046875, Learning Rate: 0.01\n",
      "Epoch [3813/10000], Loss: 87.64546203613281, Learning Rate: 0.01\n",
      "Epoch [3814/10000], Loss: 87.63645935058594, Learning Rate: 0.01\n",
      "Epoch [3815/10000], Loss: 87.62825012207031, Learning Rate: 0.01\n",
      "Epoch [3816/10000], Loss: 87.62040710449219, Learning Rate: 0.01\n",
      "Epoch [3817/10000], Loss: 87.61268615722656, Learning Rate: 0.01\n",
      "Epoch [3818/10000], Loss: 87.60456848144531, Learning Rate: 0.01\n",
      "Epoch [3819/10000], Loss: 87.59595489501953, Learning Rate: 0.01\n",
      "Epoch [3820/10000], Loss: 87.58747863769531, Learning Rate: 0.01\n",
      "Epoch [3821/10000], Loss: 87.57942199707031, Learning Rate: 0.01\n",
      "Epoch [3822/10000], Loss: 87.57144165039062, Learning Rate: 0.01\n",
      "Epoch [3823/10000], Loss: 87.56349182128906, Learning Rate: 0.01\n",
      "Epoch [3824/10000], Loss: 87.55529022216797, Learning Rate: 0.01\n",
      "Epoch [3825/10000], Loss: 87.5469741821289, Learning Rate: 0.01\n",
      "Epoch [3826/10000], Loss: 87.53857421875, Learning Rate: 0.01\n",
      "Epoch [3827/10000], Loss: 87.53050994873047, Learning Rate: 0.01\n",
      "Epoch [3828/10000], Loss: 87.52249145507812, Learning Rate: 0.01\n",
      "Epoch [3829/10000], Loss: 87.51441955566406, Learning Rate: 0.01\n",
      "Epoch [3830/10000], Loss: 87.50630187988281, Learning Rate: 0.01\n",
      "Epoch [3831/10000], Loss: 87.498046875, Learning Rate: 0.01\n",
      "Epoch [3832/10000], Loss: 87.48978424072266, Learning Rate: 0.01\n",
      "Epoch [3833/10000], Loss: 87.481689453125, Learning Rate: 0.01\n",
      "Epoch [3834/10000], Loss: 87.47364044189453, Learning Rate: 0.01\n",
      "Epoch [3835/10000], Loss: 87.46549987792969, Learning Rate: 0.01\n",
      "Epoch [3836/10000], Loss: 87.45735168457031, Learning Rate: 0.01\n",
      "Epoch [3837/10000], Loss: 87.44915771484375, Learning Rate: 0.01\n",
      "Epoch [3838/10000], Loss: 87.44107818603516, Learning Rate: 0.01\n",
      "Epoch [3839/10000], Loss: 87.4328842163086, Learning Rate: 0.01\n",
      "Epoch [3840/10000], Loss: 87.42487335205078, Learning Rate: 0.01\n",
      "Epoch [3841/10000], Loss: 87.41670989990234, Learning Rate: 0.01\n",
      "Epoch [3842/10000], Loss: 87.40857696533203, Learning Rate: 0.01\n",
      "Epoch [3843/10000], Loss: 87.40045166015625, Learning Rate: 0.01\n",
      "Epoch [3844/10000], Loss: 87.3923110961914, Learning Rate: 0.01\n",
      "Epoch [3845/10000], Loss: 87.38421630859375, Learning Rate: 0.01\n",
      "Epoch [3846/10000], Loss: 87.37603759765625, Learning Rate: 0.01\n",
      "Epoch [3847/10000], Loss: 87.3679428100586, Learning Rate: 0.01\n",
      "Epoch [3848/10000], Loss: 87.35980987548828, Learning Rate: 0.01\n",
      "Epoch [3849/10000], Loss: 87.3517074584961, Learning Rate: 0.01\n",
      "Epoch [3850/10000], Loss: 87.34358215332031, Learning Rate: 0.01\n",
      "Epoch [3851/10000], Loss: 87.3354721069336, Learning Rate: 0.01\n",
      "Epoch [3852/10000], Loss: 87.32735443115234, Learning Rate: 0.01\n",
      "Epoch [3853/10000], Loss: 87.3192367553711, Learning Rate: 0.01\n",
      "Epoch [3854/10000], Loss: 87.31117248535156, Learning Rate: 0.01\n",
      "Epoch [3855/10000], Loss: 87.30307006835938, Learning Rate: 0.01\n",
      "Epoch [3856/10000], Loss: 87.29500579833984, Learning Rate: 0.01\n",
      "Epoch [3857/10000], Loss: 87.2868423461914, Learning Rate: 0.01\n",
      "Epoch [3858/10000], Loss: 87.27869415283203, Learning Rate: 0.01\n",
      "Epoch [3859/10000], Loss: 87.27057647705078, Learning Rate: 0.01\n",
      "Epoch [3860/10000], Loss: 87.26249694824219, Learning Rate: 0.01\n",
      "Epoch [3861/10000], Loss: 87.25436401367188, Learning Rate: 0.01\n",
      "Epoch [3862/10000], Loss: 87.24622344970703, Learning Rate: 0.01\n",
      "Epoch [3863/10000], Loss: 87.23816680908203, Learning Rate: 0.01\n",
      "Epoch [3864/10000], Loss: 87.23009490966797, Learning Rate: 0.01\n",
      "Epoch [3865/10000], Loss: 87.22196960449219, Learning Rate: 0.01\n",
      "Epoch [3866/10000], Loss: 87.21389770507812, Learning Rate: 0.01\n",
      "Epoch [3867/10000], Loss: 87.205810546875, Learning Rate: 0.01\n",
      "Epoch [3868/10000], Loss: 87.19776916503906, Learning Rate: 0.01\n",
      "Epoch [3869/10000], Loss: 87.18953704833984, Learning Rate: 0.01\n",
      "Epoch [3870/10000], Loss: 87.1815414428711, Learning Rate: 0.01\n",
      "Epoch [3871/10000], Loss: 87.17343139648438, Learning Rate: 0.01\n",
      "Epoch [3872/10000], Loss: 87.16532135009766, Learning Rate: 0.01\n",
      "Epoch [3873/10000], Loss: 87.15727233886719, Learning Rate: 0.01\n",
      "Epoch [3874/10000], Loss: 87.14913940429688, Learning Rate: 0.01\n",
      "Epoch [3875/10000], Loss: 87.14102172851562, Learning Rate: 0.01\n",
      "Epoch [3876/10000], Loss: 87.1330337524414, Learning Rate: 0.01\n",
      "Epoch [3877/10000], Loss: 87.12491607666016, Learning Rate: 0.01\n",
      "Epoch [3878/10000], Loss: 87.11676025390625, Learning Rate: 0.01\n",
      "Epoch [3879/10000], Loss: 87.1087417602539, Learning Rate: 0.01\n",
      "Epoch [3880/10000], Loss: 87.10064697265625, Learning Rate: 0.01\n",
      "Epoch [3881/10000], Loss: 87.09258270263672, Learning Rate: 0.01\n",
      "Epoch [3882/10000], Loss: 87.08441925048828, Learning Rate: 0.01\n",
      "Epoch [3883/10000], Loss: 87.0763931274414, Learning Rate: 0.01\n",
      "Epoch [3884/10000], Loss: 87.0683364868164, Learning Rate: 0.01\n",
      "Epoch [3885/10000], Loss: 87.06019592285156, Learning Rate: 0.01\n",
      "Epoch [3886/10000], Loss: 87.05211639404297, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3887/10000], Loss: 87.04409790039062, Learning Rate: 0.01\n",
      "Epoch [3888/10000], Loss: 87.03599548339844, Learning Rate: 0.01\n",
      "Epoch [3889/10000], Loss: 87.02790832519531, Learning Rate: 0.01\n",
      "Epoch [3890/10000], Loss: 87.01988220214844, Learning Rate: 0.01\n",
      "Epoch [3891/10000], Loss: 87.01180267333984, Learning Rate: 0.01\n",
      "Epoch [3892/10000], Loss: 87.0037612915039, Learning Rate: 0.01\n",
      "Epoch [3893/10000], Loss: 86.99569702148438, Learning Rate: 0.01\n",
      "Epoch [3894/10000], Loss: 86.98758697509766, Learning Rate: 0.01\n",
      "Epoch [3895/10000], Loss: 86.9794692993164, Learning Rate: 0.01\n",
      "Epoch [3896/10000], Loss: 86.9714584350586, Learning Rate: 0.01\n",
      "Epoch [3897/10000], Loss: 86.96343231201172, Learning Rate: 0.01\n",
      "Epoch [3898/10000], Loss: 86.9553451538086, Learning Rate: 0.01\n",
      "Epoch [3899/10000], Loss: 86.94721221923828, Learning Rate: 0.01\n",
      "Epoch [3900/10000], Loss: 86.93917846679688, Learning Rate: 0.01\n",
      "Epoch [3901/10000], Loss: 86.93113708496094, Learning Rate: 0.01\n",
      "Epoch [3902/10000], Loss: 86.92306518554688, Learning Rate: 0.01\n",
      "Epoch [3903/10000], Loss: 86.91499328613281, Learning Rate: 0.01\n",
      "Epoch [3904/10000], Loss: 86.90695190429688, Learning Rate: 0.01\n",
      "Epoch [3905/10000], Loss: 86.89891815185547, Learning Rate: 0.01\n",
      "Epoch [3906/10000], Loss: 86.89082336425781, Learning Rate: 0.01\n",
      "Epoch [3907/10000], Loss: 86.88280487060547, Learning Rate: 0.01\n",
      "Epoch [3908/10000], Loss: 86.87477111816406, Learning Rate: 0.01\n",
      "Epoch [3909/10000], Loss: 86.86666870117188, Learning Rate: 0.01\n",
      "Epoch [3910/10000], Loss: 86.85861206054688, Learning Rate: 0.01\n",
      "Epoch [3911/10000], Loss: 86.85061645507812, Learning Rate: 0.01\n",
      "Epoch [3912/10000], Loss: 86.84251403808594, Learning Rate: 0.01\n",
      "Epoch [3913/10000], Loss: 86.8344955444336, Learning Rate: 0.01\n",
      "Epoch [3914/10000], Loss: 86.82645416259766, Learning Rate: 0.01\n",
      "Epoch [3915/10000], Loss: 86.81842803955078, Learning Rate: 0.01\n",
      "Epoch [3916/10000], Loss: 86.81039428710938, Learning Rate: 0.01\n",
      "Epoch [3917/10000], Loss: 86.80227661132812, Learning Rate: 0.01\n",
      "Epoch [3918/10000], Loss: 86.79434204101562, Learning Rate: 0.01\n",
      "Epoch [3919/10000], Loss: 86.78617095947266, Learning Rate: 0.01\n",
      "Epoch [3920/10000], Loss: 86.77821350097656, Learning Rate: 0.01\n",
      "Epoch [3921/10000], Loss: 86.77021789550781, Learning Rate: 0.01\n",
      "Epoch [3922/10000], Loss: 86.76214599609375, Learning Rate: 0.01\n",
      "Epoch [3923/10000], Loss: 86.75411224365234, Learning Rate: 0.01\n",
      "Epoch [3924/10000], Loss: 86.74603271484375, Learning Rate: 0.01\n",
      "Epoch [3925/10000], Loss: 86.73809051513672, Learning Rate: 0.01\n",
      "Epoch [3926/10000], Loss: 86.72998046875, Learning Rate: 0.01\n",
      "Epoch [3927/10000], Loss: 86.72191619873047, Learning Rate: 0.01\n",
      "Epoch [3928/10000], Loss: 86.71395874023438, Learning Rate: 0.01\n",
      "Epoch [3929/10000], Loss: 86.70591735839844, Learning Rate: 0.01\n",
      "Epoch [3930/10000], Loss: 86.6978759765625, Learning Rate: 0.01\n",
      "Epoch [3931/10000], Loss: 86.68984985351562, Learning Rate: 0.01\n",
      "Epoch [3932/10000], Loss: 86.6817855834961, Learning Rate: 0.01\n",
      "Epoch [3933/10000], Loss: 86.67378997802734, Learning Rate: 0.01\n",
      "Epoch [3934/10000], Loss: 86.66580963134766, Learning Rate: 0.01\n",
      "Epoch [3935/10000], Loss: 86.65775299072266, Learning Rate: 0.01\n",
      "Epoch [3936/10000], Loss: 86.64979553222656, Learning Rate: 0.01\n",
      "Epoch [3937/10000], Loss: 86.6417236328125, Learning Rate: 0.01\n",
      "Epoch [3938/10000], Loss: 86.63371276855469, Learning Rate: 0.01\n",
      "Epoch [3939/10000], Loss: 86.62580871582031, Learning Rate: 0.01\n",
      "Epoch [3940/10000], Loss: 86.61761474609375, Learning Rate: 0.01\n",
      "Epoch [3941/10000], Loss: 86.60968780517578, Learning Rate: 0.01\n",
      "Epoch [3942/10000], Loss: 86.60163879394531, Learning Rate: 0.01\n",
      "Epoch [3943/10000], Loss: 86.59367370605469, Learning Rate: 0.01\n",
      "Epoch [3944/10000], Loss: 86.58561706542969, Learning Rate: 0.01\n",
      "Epoch [3945/10000], Loss: 86.57769775390625, Learning Rate: 0.01\n",
      "Epoch [3946/10000], Loss: 86.5696792602539, Learning Rate: 0.01\n",
      "Epoch [3947/10000], Loss: 86.56166076660156, Learning Rate: 0.01\n",
      "Epoch [3948/10000], Loss: 86.55355834960938, Learning Rate: 0.01\n",
      "Epoch [3949/10000], Loss: 86.54560852050781, Learning Rate: 0.01\n",
      "Epoch [3950/10000], Loss: 86.53760528564453, Learning Rate: 0.01\n",
      "Epoch [3951/10000], Loss: 86.52963256835938, Learning Rate: 0.01\n",
      "Epoch [3952/10000], Loss: 86.52159118652344, Learning Rate: 0.01\n",
      "Epoch [3953/10000], Loss: 86.51361846923828, Learning Rate: 0.01\n",
      "Epoch [3954/10000], Loss: 86.50565338134766, Learning Rate: 0.01\n",
      "Epoch [3955/10000], Loss: 86.49755859375, Learning Rate: 0.01\n",
      "Epoch [3956/10000], Loss: 86.48959350585938, Learning Rate: 0.01\n",
      "Epoch [3957/10000], Loss: 86.4816665649414, Learning Rate: 0.01\n",
      "Epoch [3958/10000], Loss: 86.47357177734375, Learning Rate: 0.01\n",
      "Epoch [3959/10000], Loss: 86.46561431884766, Learning Rate: 0.01\n",
      "Epoch [3960/10000], Loss: 86.45771789550781, Learning Rate: 0.01\n",
      "Epoch [3961/10000], Loss: 86.44972229003906, Learning Rate: 0.01\n",
      "Epoch [3962/10000], Loss: 86.44174194335938, Learning Rate: 0.01\n",
      "Epoch [3963/10000], Loss: 86.43363189697266, Learning Rate: 0.01\n",
      "Epoch [3964/10000], Loss: 86.42565155029297, Learning Rate: 0.01\n",
      "Epoch [3965/10000], Loss: 86.41776275634766, Learning Rate: 0.01\n",
      "Epoch [3966/10000], Loss: 86.40975189208984, Learning Rate: 0.01\n",
      "Epoch [3967/10000], Loss: 86.40171813964844, Learning Rate: 0.01\n",
      "Epoch [3968/10000], Loss: 86.39382934570312, Learning Rate: 0.01\n",
      "Epoch [3969/10000], Loss: 86.38573455810547, Learning Rate: 0.01\n",
      "Epoch [3970/10000], Loss: 86.37782287597656, Learning Rate: 0.01\n",
      "Epoch [3971/10000], Loss: 86.36984252929688, Learning Rate: 0.01\n",
      "Epoch [3972/10000], Loss: 86.36185455322266, Learning Rate: 0.01\n",
      "Epoch [3973/10000], Loss: 86.35379791259766, Learning Rate: 0.01\n",
      "Epoch [3974/10000], Loss: 86.34589385986328, Learning Rate: 0.01\n",
      "Epoch [3975/10000], Loss: 86.33792877197266, Learning Rate: 0.01\n",
      "Epoch [3976/10000], Loss: 86.32992553710938, Learning Rate: 0.01\n",
      "Epoch [3977/10000], Loss: 86.32196044921875, Learning Rate: 0.01\n",
      "Epoch [3978/10000], Loss: 86.31405639648438, Learning Rate: 0.01\n",
      "Epoch [3979/10000], Loss: 86.30606079101562, Learning Rate: 0.01\n",
      "Epoch [3980/10000], Loss: 86.29808044433594, Learning Rate: 0.01\n",
      "Epoch [3981/10000], Loss: 86.29000091552734, Learning Rate: 0.01\n",
      "Epoch [3982/10000], Loss: 86.28215789794922, Learning Rate: 0.01\n",
      "Epoch [3983/10000], Loss: 86.27420043945312, Learning Rate: 0.01\n",
      "Epoch [3984/10000], Loss: 86.2662353515625, Learning Rate: 0.01\n",
      "Epoch [3985/10000], Loss: 86.25825500488281, Learning Rate: 0.01\n",
      "Epoch [3986/10000], Loss: 86.25030517578125, Learning Rate: 0.01\n",
      "Epoch [3987/10000], Loss: 86.24237823486328, Learning Rate: 0.01\n",
      "Epoch [3988/10000], Loss: 86.23433685302734, Learning Rate: 0.01\n",
      "Epoch [3989/10000], Loss: 86.2264404296875, Learning Rate: 0.01\n",
      "Epoch [3990/10000], Loss: 86.21846771240234, Learning Rate: 0.01\n",
      "Epoch [3991/10000], Loss: 86.21056365966797, Learning Rate: 0.01\n",
      "Epoch [3992/10000], Loss: 86.20249938964844, Learning Rate: 0.01\n",
      "Epoch [3993/10000], Loss: 86.19464111328125, Learning Rate: 0.01\n",
      "Epoch [3994/10000], Loss: 86.1867446899414, Learning Rate: 0.01\n",
      "Epoch [3995/10000], Loss: 86.17869567871094, Learning Rate: 0.01\n",
      "Epoch [3996/10000], Loss: 86.1707763671875, Learning Rate: 0.01\n",
      "Epoch [3997/10000], Loss: 86.16283416748047, Learning Rate: 0.01\n",
      "Epoch [3998/10000], Loss: 86.15489959716797, Learning Rate: 0.01\n",
      "Epoch [3999/10000], Loss: 86.14698028564453, Learning Rate: 0.01\n",
      "Epoch [4000/10000], Loss: 86.13911437988281, Learning Rate: 0.01\n",
      "Epoch [4001/10000], Loss: 86.1311264038086, Learning Rate: 0.01\n",
      "Epoch [4002/10000], Loss: 86.12319946289062, Learning Rate: 0.01\n",
      "Epoch [4003/10000], Loss: 86.11546325683594, Learning Rate: 0.01\n",
      "Epoch [4004/10000], Loss: 86.10767364501953, Learning Rate: 0.01\n",
      "Epoch [4005/10000], Loss: 86.09996032714844, Learning Rate: 0.01\n",
      "Epoch [4006/10000], Loss: 86.0924072265625, Learning Rate: 0.01\n",
      "Epoch [4007/10000], Loss: 86.08535766601562, Learning Rate: 0.01\n",
      "Epoch [4008/10000], Loss: 86.07896423339844, Learning Rate: 0.01\n",
      "Epoch [4009/10000], Loss: 86.07374572753906, Learning Rate: 0.01\n",
      "Epoch [4010/10000], Loss: 86.07063293457031, Learning Rate: 0.01\n",
      "Epoch [4011/10000], Loss: 86.07078552246094, Learning Rate: 0.01\n",
      "Epoch [4012/10000], Loss: 86.0754623413086, Learning Rate: 0.01\n",
      "Epoch [4013/10000], Loss: 86.08470153808594, Learning Rate: 0.01\n",
      "Epoch [4014/10000], Loss: 86.0943374633789, Learning Rate: 0.01\n",
      "Epoch [4015/10000], Loss: 86.09410095214844, Learning Rate: 0.01\n",
      "Epoch [4016/10000], Loss: 86.07318115234375, Learning Rate: 0.01\n",
      "Epoch [4017/10000], Loss: 86.03508758544922, Learning Rate: 0.01\n",
      "Epoch [4018/10000], Loss: 86.00131225585938, Learning Rate: 0.01\n",
      "Epoch [4019/10000], Loss: 85.99011993408203, Learning Rate: 0.01\n",
      "Epoch [4020/10000], Loss: 85.99795532226562, Learning Rate: 0.01\n",
      "Epoch [4021/10000], Loss: 86.00487518310547, Learning Rate: 0.01\n",
      "Epoch [4022/10000], Loss: 85.99488830566406, Learning Rate: 0.01\n",
      "Epoch [4023/10000], Loss: 85.97075653076172, Learning Rate: 0.01\n",
      "Epoch [4024/10000], Loss: 85.94993591308594, Learning Rate: 0.01\n",
      "Epoch [4025/10000], Loss: 85.9441146850586, Learning Rate: 0.01\n",
      "Epoch [4026/10000], Loss: 85.94690704345703, Learning Rate: 0.01\n",
      "Epoch [4027/10000], Loss: 85.94371032714844, Learning Rate: 0.01\n",
      "Epoch [4028/10000], Loss: 85.92923736572266, Learning Rate: 0.01\n",
      "Epoch [4029/10000], Loss: 85.91184997558594, Learning Rate: 0.01\n",
      "Epoch [4030/10000], Loss: 85.9020004272461, Learning Rate: 0.01\n",
      "Epoch [4031/10000], Loss: 85.8996810913086, Learning Rate: 0.01\n",
      "Epoch [4032/10000], Loss: 85.896240234375, Learning Rate: 0.01\n",
      "Epoch [4033/10000], Loss: 85.8860855102539, Learning Rate: 0.01\n",
      "Epoch [4034/10000], Loss: 85.87244415283203, Learning Rate: 0.01\n",
      "Epoch [4035/10000], Loss: 85.86215209960938, Learning Rate: 0.01\n",
      "Epoch [4036/10000], Loss: 85.85694122314453, Learning Rate: 0.01\n",
      "Epoch [4037/10000], Loss: 85.85224914550781, Learning Rate: 0.01\n",
      "Epoch [4038/10000], Loss: 85.84386444091797, Learning Rate: 0.01\n",
      "Epoch [4039/10000], Loss: 85.83263397216797, Learning Rate: 0.01\n",
      "Epoch [4040/10000], Loss: 85.8227767944336, Learning Rate: 0.01\n",
      "Epoch [4041/10000], Loss: 85.81600952148438, Learning Rate: 0.01\n",
      "Epoch [4042/10000], Loss: 85.81019592285156, Learning Rate: 0.01\n",
      "Epoch [4043/10000], Loss: 85.80249786376953, Learning Rate: 0.01\n",
      "Epoch [4044/10000], Loss: 85.7928466796875, Learning Rate: 0.01\n",
      "Epoch [4045/10000], Loss: 85.783447265625, Learning Rate: 0.01\n",
      "Epoch [4046/10000], Loss: 85.77587890625, Learning Rate: 0.01\n",
      "Epoch [4047/10000], Loss: 85.769287109375, Learning Rate: 0.01\n",
      "Epoch [4048/10000], Loss: 85.76188659667969, Learning Rate: 0.01\n",
      "Epoch [4049/10000], Loss: 85.75309753417969, Learning Rate: 0.01\n",
      "Epoch [4050/10000], Loss: 85.74420928955078, Learning Rate: 0.01\n",
      "Epoch [4051/10000], Loss: 85.73625946044922, Learning Rate: 0.01\n",
      "Epoch [4052/10000], Loss: 85.72913360595703, Learning Rate: 0.01\n",
      "Epoch [4053/10000], Loss: 85.7216796875, Learning Rate: 0.01\n",
      "Epoch [4054/10000], Loss: 85.71354675292969, Learning Rate: 0.01\n",
      "Epoch [4055/10000], Loss: 85.705078125, Learning Rate: 0.01\n",
      "Epoch [4056/10000], Loss: 85.69692993164062, Learning Rate: 0.01\n",
      "Epoch [4057/10000], Loss: 85.6893310546875, Learning Rate: 0.01\n",
      "Epoch [4058/10000], Loss: 85.68185424804688, Learning Rate: 0.01\n",
      "Epoch [4059/10000], Loss: 85.6739730834961, Learning Rate: 0.01\n",
      "Epoch [4060/10000], Loss: 85.665771484375, Learning Rate: 0.01\n",
      "Epoch [4061/10000], Loss: 85.65764617919922, Learning Rate: 0.01\n",
      "Epoch [4062/10000], Loss: 85.64989471435547, Learning Rate: 0.01\n",
      "Epoch [4063/10000], Loss: 85.64228820800781, Learning Rate: 0.01\n",
      "Epoch [4064/10000], Loss: 85.634521484375, Learning Rate: 0.01\n",
      "Epoch [4065/10000], Loss: 85.62663269042969, Learning Rate: 0.01\n",
      "Epoch [4066/10000], Loss: 85.6185531616211, Learning Rate: 0.01\n",
      "Epoch [4067/10000], Loss: 85.61065673828125, Learning Rate: 0.01\n",
      "Epoch [4068/10000], Loss: 85.6029281616211, Learning Rate: 0.01\n",
      "Epoch [4069/10000], Loss: 85.59518432617188, Learning Rate: 0.01\n",
      "Epoch [4070/10000], Loss: 85.58724975585938, Learning Rate: 0.01\n",
      "Epoch [4071/10000], Loss: 85.57940673828125, Learning Rate: 0.01\n",
      "Epoch [4072/10000], Loss: 85.57146453857422, Learning Rate: 0.01\n",
      "Epoch [4073/10000], Loss: 85.56369018554688, Learning Rate: 0.01\n",
      "Epoch [4074/10000], Loss: 85.5559310913086, Learning Rate: 0.01\n",
      "Epoch [4075/10000], Loss: 85.54805755615234, Learning Rate: 0.01\n",
      "Epoch [4076/10000], Loss: 85.54022216796875, Learning Rate: 0.01\n",
      "Epoch [4077/10000], Loss: 85.53239440917969, Learning Rate: 0.01\n",
      "Epoch [4078/10000], Loss: 85.5245590209961, Learning Rate: 0.01\n",
      "Epoch [4079/10000], Loss: 85.51679229736328, Learning Rate: 0.01\n",
      "Epoch [4080/10000], Loss: 85.50896453857422, Learning Rate: 0.01\n",
      "Epoch [4081/10000], Loss: 85.50116729736328, Learning Rate: 0.01\n",
      "Epoch [4082/10000], Loss: 85.49331665039062, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4083/10000], Loss: 85.48544311523438, Learning Rate: 0.01\n",
      "Epoch [4084/10000], Loss: 85.47764587402344, Learning Rate: 0.01\n",
      "Epoch [4085/10000], Loss: 85.46983337402344, Learning Rate: 0.01\n",
      "Epoch [4086/10000], Loss: 85.4620361328125, Learning Rate: 0.01\n",
      "Epoch [4087/10000], Loss: 85.4542007446289, Learning Rate: 0.01\n",
      "Epoch [4088/10000], Loss: 85.44638061523438, Learning Rate: 0.01\n",
      "Epoch [4089/10000], Loss: 85.43865203857422, Learning Rate: 0.01\n",
      "Epoch [4090/10000], Loss: 85.4307861328125, Learning Rate: 0.01\n",
      "Epoch [4091/10000], Loss: 85.42302703857422, Learning Rate: 0.01\n",
      "Epoch [4092/10000], Loss: 85.41522216796875, Learning Rate: 0.01\n",
      "Epoch [4093/10000], Loss: 85.40740203857422, Learning Rate: 0.01\n",
      "Epoch [4094/10000], Loss: 85.39966583251953, Learning Rate: 0.01\n",
      "Epoch [4095/10000], Loss: 85.39171600341797, Learning Rate: 0.01\n",
      "Epoch [4096/10000], Loss: 85.38401794433594, Learning Rate: 0.01\n",
      "Epoch [4097/10000], Loss: 85.37622833251953, Learning Rate: 0.01\n",
      "Epoch [4098/10000], Loss: 85.36851501464844, Learning Rate: 0.01\n",
      "Epoch [4099/10000], Loss: 85.36063385009766, Learning Rate: 0.01\n",
      "Epoch [4100/10000], Loss: 85.35292053222656, Learning Rate: 0.01\n",
      "Epoch [4101/10000], Loss: 85.3450698852539, Learning Rate: 0.01\n",
      "Epoch [4102/10000], Loss: 85.33726501464844, Learning Rate: 0.01\n",
      "Epoch [4103/10000], Loss: 85.32950592041016, Learning Rate: 0.01\n",
      "Epoch [4104/10000], Loss: 85.32179260253906, Learning Rate: 0.01\n",
      "Epoch [4105/10000], Loss: 85.31397247314453, Learning Rate: 0.01\n",
      "Epoch [4106/10000], Loss: 85.30619049072266, Learning Rate: 0.01\n",
      "Epoch [4107/10000], Loss: 85.29840087890625, Learning Rate: 0.01\n",
      "Epoch [4108/10000], Loss: 85.29068756103516, Learning Rate: 0.01\n",
      "Epoch [4109/10000], Loss: 85.28287506103516, Learning Rate: 0.01\n",
      "Epoch [4110/10000], Loss: 85.27505493164062, Learning Rate: 0.01\n",
      "Epoch [4111/10000], Loss: 85.26737976074219, Learning Rate: 0.01\n",
      "Epoch [4112/10000], Loss: 85.25953674316406, Learning Rate: 0.01\n",
      "Epoch [4113/10000], Loss: 85.2518310546875, Learning Rate: 0.01\n",
      "Epoch [4114/10000], Loss: 85.24406433105469, Learning Rate: 0.01\n",
      "Epoch [4115/10000], Loss: 85.23631286621094, Learning Rate: 0.01\n",
      "Epoch [4116/10000], Loss: 85.228515625, Learning Rate: 0.01\n",
      "Epoch [4117/10000], Loss: 85.22079467773438, Learning Rate: 0.01\n",
      "Epoch [4118/10000], Loss: 85.21305084228516, Learning Rate: 0.01\n",
      "Epoch [4119/10000], Loss: 85.20523834228516, Learning Rate: 0.01\n",
      "Epoch [4120/10000], Loss: 85.19742584228516, Learning Rate: 0.01\n",
      "Epoch [4121/10000], Loss: 85.18975830078125, Learning Rate: 0.01\n",
      "Epoch [4122/10000], Loss: 85.18193817138672, Learning Rate: 0.01\n",
      "Epoch [4123/10000], Loss: 85.17424011230469, Learning Rate: 0.01\n",
      "Epoch [4124/10000], Loss: 85.16649627685547, Learning Rate: 0.01\n",
      "Epoch [4125/10000], Loss: 85.15869903564453, Learning Rate: 0.01\n",
      "Epoch [4126/10000], Loss: 85.1509780883789, Learning Rate: 0.01\n",
      "Epoch [4127/10000], Loss: 85.14326477050781, Learning Rate: 0.01\n",
      "Epoch [4128/10000], Loss: 85.1355209350586, Learning Rate: 0.01\n",
      "Epoch [4129/10000], Loss: 85.1278076171875, Learning Rate: 0.01\n",
      "Epoch [4130/10000], Loss: 85.12002563476562, Learning Rate: 0.01\n",
      "Epoch [4131/10000], Loss: 85.1123275756836, Learning Rate: 0.01\n",
      "Epoch [4132/10000], Loss: 85.10456848144531, Learning Rate: 0.01\n",
      "Epoch [4133/10000], Loss: 85.09683227539062, Learning Rate: 0.01\n",
      "Epoch [4134/10000], Loss: 85.08912658691406, Learning Rate: 0.01\n",
      "Epoch [4135/10000], Loss: 85.08138275146484, Learning Rate: 0.01\n",
      "Epoch [4136/10000], Loss: 85.07366180419922, Learning Rate: 0.01\n",
      "Epoch [4137/10000], Loss: 85.06587219238281, Learning Rate: 0.01\n",
      "Epoch [4138/10000], Loss: 85.05826568603516, Learning Rate: 0.01\n",
      "Epoch [4139/10000], Loss: 85.05046844482422, Learning Rate: 0.01\n",
      "Epoch [4140/10000], Loss: 85.04277038574219, Learning Rate: 0.01\n",
      "Epoch [4141/10000], Loss: 85.03504180908203, Learning Rate: 0.01\n",
      "Epoch [4142/10000], Loss: 85.02733612060547, Learning Rate: 0.01\n",
      "Epoch [4143/10000], Loss: 85.0196304321289, Learning Rate: 0.01\n",
      "Epoch [4144/10000], Loss: 85.01193237304688, Learning Rate: 0.01\n",
      "Epoch [4145/10000], Loss: 85.00415802001953, Learning Rate: 0.01\n",
      "Epoch [4146/10000], Loss: 84.9964828491211, Learning Rate: 0.01\n",
      "Epoch [4147/10000], Loss: 84.98872375488281, Learning Rate: 0.01\n",
      "Epoch [4148/10000], Loss: 84.98112487792969, Learning Rate: 0.01\n",
      "Epoch [4149/10000], Loss: 84.97335815429688, Learning Rate: 0.01\n",
      "Epoch [4150/10000], Loss: 84.96562957763672, Learning Rate: 0.01\n",
      "Epoch [4151/10000], Loss: 84.9579849243164, Learning Rate: 0.01\n",
      "Epoch [4152/10000], Loss: 84.95027923583984, Learning Rate: 0.01\n",
      "Epoch [4153/10000], Loss: 84.94255065917969, Learning Rate: 0.01\n",
      "Epoch [4154/10000], Loss: 84.93490600585938, Learning Rate: 0.01\n",
      "Epoch [4155/10000], Loss: 84.92719268798828, Learning Rate: 0.01\n",
      "Epoch [4156/10000], Loss: 84.91944122314453, Learning Rate: 0.01\n",
      "Epoch [4157/10000], Loss: 84.91177368164062, Learning Rate: 0.01\n",
      "Epoch [4158/10000], Loss: 84.90416717529297, Learning Rate: 0.01\n",
      "Epoch [4159/10000], Loss: 84.89646911621094, Learning Rate: 0.01\n",
      "Epoch [4160/10000], Loss: 84.88876342773438, Learning Rate: 0.01\n",
      "Epoch [4161/10000], Loss: 84.88105773925781, Learning Rate: 0.01\n",
      "Epoch [4162/10000], Loss: 84.87336730957031, Learning Rate: 0.01\n",
      "Epoch [4163/10000], Loss: 84.86576843261719, Learning Rate: 0.01\n",
      "Epoch [4164/10000], Loss: 84.8580322265625, Learning Rate: 0.01\n",
      "Epoch [4165/10000], Loss: 84.85041809082031, Learning Rate: 0.01\n",
      "Epoch [4166/10000], Loss: 84.8427734375, Learning Rate: 0.01\n",
      "Epoch [4167/10000], Loss: 84.8349609375, Learning Rate: 0.01\n",
      "Epoch [4168/10000], Loss: 84.82737731933594, Learning Rate: 0.01\n",
      "Epoch [4169/10000], Loss: 84.81967163085938, Learning Rate: 0.01\n",
      "Epoch [4170/10000], Loss: 84.81196594238281, Learning Rate: 0.01\n",
      "Epoch [4171/10000], Loss: 84.80440521240234, Learning Rate: 0.01\n",
      "Epoch [4172/10000], Loss: 84.79667663574219, Learning Rate: 0.01\n",
      "Epoch [4173/10000], Loss: 84.78897094726562, Learning Rate: 0.01\n",
      "Epoch [4174/10000], Loss: 84.7813491821289, Learning Rate: 0.01\n",
      "Epoch [4175/10000], Loss: 84.77375793457031, Learning Rate: 0.01\n",
      "Epoch [4176/10000], Loss: 84.76612091064453, Learning Rate: 0.01\n",
      "Epoch [4177/10000], Loss: 84.75845336914062, Learning Rate: 0.01\n",
      "Epoch [4178/10000], Loss: 84.7507553100586, Learning Rate: 0.01\n",
      "Epoch [4179/10000], Loss: 84.7430419921875, Learning Rate: 0.01\n",
      "Epoch [4180/10000], Loss: 84.73548889160156, Learning Rate: 0.01\n",
      "Epoch [4181/10000], Loss: 84.72784423828125, Learning Rate: 0.01\n",
      "Epoch [4182/10000], Loss: 84.7201156616211, Learning Rate: 0.01\n",
      "Epoch [4183/10000], Loss: 84.71247100830078, Learning Rate: 0.01\n",
      "Epoch [4184/10000], Loss: 84.7048568725586, Learning Rate: 0.01\n",
      "Epoch [4185/10000], Loss: 84.69731140136719, Learning Rate: 0.01\n",
      "Epoch [4186/10000], Loss: 84.68958282470703, Learning Rate: 0.01\n",
      "Epoch [4187/10000], Loss: 84.68195343017578, Learning Rate: 0.01\n",
      "Epoch [4188/10000], Loss: 84.67438507080078, Learning Rate: 0.01\n",
      "Epoch [4189/10000], Loss: 84.66667175292969, Learning Rate: 0.01\n",
      "Epoch [4190/10000], Loss: 84.6591567993164, Learning Rate: 0.01\n",
      "Epoch [4191/10000], Loss: 84.65144348144531, Learning Rate: 0.01\n",
      "Epoch [4192/10000], Loss: 84.6438217163086, Learning Rate: 0.01\n",
      "Epoch [4193/10000], Loss: 84.6362075805664, Learning Rate: 0.01\n",
      "Epoch [4194/10000], Loss: 84.62860870361328, Learning Rate: 0.01\n",
      "Epoch [4195/10000], Loss: 84.62098693847656, Learning Rate: 0.01\n",
      "Epoch [4196/10000], Loss: 84.61339569091797, Learning Rate: 0.01\n",
      "Epoch [4197/10000], Loss: 84.60580444335938, Learning Rate: 0.01\n",
      "Epoch [4198/10000], Loss: 84.59818267822266, Learning Rate: 0.01\n",
      "Epoch [4199/10000], Loss: 84.59048461914062, Learning Rate: 0.01\n",
      "Epoch [4200/10000], Loss: 84.58290100097656, Learning Rate: 0.01\n",
      "Epoch [4201/10000], Loss: 84.57527923583984, Learning Rate: 0.01\n",
      "Epoch [4202/10000], Loss: 84.56765747070312, Learning Rate: 0.01\n",
      "Epoch [4203/10000], Loss: 84.56008911132812, Learning Rate: 0.01\n",
      "Epoch [4204/10000], Loss: 84.55249786376953, Learning Rate: 0.01\n",
      "Epoch [4205/10000], Loss: 84.54491424560547, Learning Rate: 0.01\n",
      "Epoch [4206/10000], Loss: 84.53736114501953, Learning Rate: 0.01\n",
      "Epoch [4207/10000], Loss: 84.52967071533203, Learning Rate: 0.01\n",
      "Epoch [4208/10000], Loss: 84.52215576171875, Learning Rate: 0.01\n",
      "Epoch [4209/10000], Loss: 84.51454162597656, Learning Rate: 0.01\n",
      "Epoch [4210/10000], Loss: 84.50700378417969, Learning Rate: 0.01\n",
      "Epoch [4211/10000], Loss: 84.49927520751953, Learning Rate: 0.01\n",
      "Epoch [4212/10000], Loss: 84.49178314208984, Learning Rate: 0.01\n",
      "Epoch [4213/10000], Loss: 84.48418426513672, Learning Rate: 0.01\n",
      "Epoch [4214/10000], Loss: 84.47657775878906, Learning Rate: 0.01\n",
      "Epoch [4215/10000], Loss: 84.46904754638672, Learning Rate: 0.01\n",
      "Epoch [4216/10000], Loss: 84.4614486694336, Learning Rate: 0.01\n",
      "Epoch [4217/10000], Loss: 84.4539566040039, Learning Rate: 0.01\n",
      "Epoch [4218/10000], Loss: 84.44627380371094, Learning Rate: 0.01\n",
      "Epoch [4219/10000], Loss: 84.43881225585938, Learning Rate: 0.01\n",
      "Epoch [4220/10000], Loss: 84.43121337890625, Learning Rate: 0.01\n",
      "Epoch [4221/10000], Loss: 84.42363739013672, Learning Rate: 0.01\n",
      "Epoch [4222/10000], Loss: 84.41603088378906, Learning Rate: 0.01\n",
      "Epoch [4223/10000], Loss: 84.40839385986328, Learning Rate: 0.01\n",
      "Epoch [4224/10000], Loss: 84.40097045898438, Learning Rate: 0.01\n",
      "Epoch [4225/10000], Loss: 84.39329528808594, Learning Rate: 0.01\n",
      "Epoch [4226/10000], Loss: 84.38572692871094, Learning Rate: 0.01\n",
      "Epoch [4227/10000], Loss: 84.37828826904297, Learning Rate: 0.01\n",
      "Epoch [4228/10000], Loss: 84.3707046508789, Learning Rate: 0.01\n",
      "Epoch [4229/10000], Loss: 84.36307525634766, Learning Rate: 0.01\n",
      "Epoch [4230/10000], Loss: 84.35554504394531, Learning Rate: 0.01\n",
      "Epoch [4231/10000], Loss: 84.34809112548828, Learning Rate: 0.01\n",
      "Epoch [4232/10000], Loss: 84.3405532836914, Learning Rate: 0.01\n",
      "Epoch [4233/10000], Loss: 84.33291625976562, Learning Rate: 0.01\n",
      "Epoch [4234/10000], Loss: 84.32546997070312, Learning Rate: 0.01\n",
      "Epoch [4235/10000], Loss: 84.31785583496094, Learning Rate: 0.01\n",
      "Epoch [4236/10000], Loss: 84.3103256225586, Learning Rate: 0.01\n",
      "Epoch [4237/10000], Loss: 84.30284881591797, Learning Rate: 0.01\n",
      "Epoch [4238/10000], Loss: 84.29528045654297, Learning Rate: 0.01\n",
      "Epoch [4239/10000], Loss: 84.28775024414062, Learning Rate: 0.01\n",
      "Epoch [4240/10000], Loss: 84.28020477294922, Learning Rate: 0.01\n",
      "Epoch [4241/10000], Loss: 84.27273559570312, Learning Rate: 0.01\n",
      "Epoch [4242/10000], Loss: 84.26520538330078, Learning Rate: 0.01\n",
      "Epoch [4243/10000], Loss: 84.25767517089844, Learning Rate: 0.01\n",
      "Epoch [4244/10000], Loss: 84.2501220703125, Learning Rate: 0.01\n",
      "Epoch [4245/10000], Loss: 84.24264526367188, Learning Rate: 0.01\n",
      "Epoch [4246/10000], Loss: 84.23516082763672, Learning Rate: 0.01\n",
      "Epoch [4247/10000], Loss: 84.22758483886719, Learning Rate: 0.01\n",
      "Epoch [4248/10000], Loss: 84.22014617919922, Learning Rate: 0.01\n",
      "Epoch [4249/10000], Loss: 84.21267700195312, Learning Rate: 0.01\n",
      "Epoch [4250/10000], Loss: 84.20513916015625, Learning Rate: 0.01\n",
      "Epoch [4251/10000], Loss: 84.19766998291016, Learning Rate: 0.01\n",
      "Epoch [4252/10000], Loss: 84.190185546875, Learning Rate: 0.01\n",
      "Epoch [4253/10000], Loss: 84.18280029296875, Learning Rate: 0.01\n",
      "Epoch [4254/10000], Loss: 84.17556762695312, Learning Rate: 0.01\n",
      "Epoch [4255/10000], Loss: 84.16828155517578, Learning Rate: 0.01\n",
      "Epoch [4256/10000], Loss: 84.1611557006836, Learning Rate: 0.01\n",
      "Epoch [4257/10000], Loss: 84.15438842773438, Learning Rate: 0.01\n",
      "Epoch [4258/10000], Loss: 84.14801025390625, Learning Rate: 0.01\n",
      "Epoch [4259/10000], Loss: 84.14246368408203, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4260/10000], Loss: 84.13809204101562, Learning Rate: 0.01\n",
      "Epoch [4261/10000], Loss: 84.13571166992188, Learning Rate: 0.01\n",
      "Epoch [4262/10000], Loss: 84.13626861572266, Learning Rate: 0.01\n",
      "Epoch [4263/10000], Loss: 84.14104461669922, Learning Rate: 0.01\n",
      "Epoch [4264/10000], Loss: 84.15017700195312, Learning Rate: 0.01\n",
      "Epoch [4265/10000], Loss: 84.16188049316406, Learning Rate: 0.01\n",
      "Epoch [4266/10000], Loss: 84.16963958740234, Learning Rate: 0.01\n",
      "Epoch [4267/10000], Loss: 84.16337585449219, Learning Rate: 0.01\n",
      "Epoch [4268/10000], Loss: 84.13653564453125, Learning Rate: 0.01\n",
      "Epoch [4269/10000], Loss: 84.09622955322266, Learning Rate: 0.01\n",
      "Epoch [4270/10000], Loss: 84.06207275390625, Learning Rate: 0.01\n",
      "Epoch [4271/10000], Loss: 84.0487289428711, Learning Rate: 0.01\n",
      "Epoch [4272/10000], Loss: 84.05461120605469, Learning Rate: 0.01\n",
      "Epoch [4273/10000], Loss: 84.0644760131836, Learning Rate: 0.01\n",
      "Epoch [4274/10000], Loss: 84.06266021728516, Learning Rate: 0.01\n",
      "Epoch [4275/10000], Loss: 84.04466247558594, Learning Rate: 0.01\n",
      "Epoch [4276/10000], Loss: 84.02037048339844, Learning Rate: 0.01\n",
      "Epoch [4277/10000], Loss: 84.003662109375, Learning Rate: 0.01\n",
      "Epoch [4278/10000], Loss: 83.99992370605469, Learning Rate: 0.01\n",
      "Epoch [4279/10000], Loss: 84.00235748291016, Learning Rate: 0.01\n",
      "Epoch [4280/10000], Loss: 84.00018310546875, Learning Rate: 0.01\n",
      "Epoch [4281/10000], Loss: 83.98847198486328, Learning Rate: 0.01\n",
      "Epoch [4282/10000], Loss: 83.97213745117188, Learning Rate: 0.01\n",
      "Epoch [4283/10000], Loss: 83.9590835571289, Learning Rate: 0.01\n",
      "Epoch [4284/10000], Loss: 83.95335388183594, Learning Rate: 0.01\n",
      "Epoch [4285/10000], Loss: 83.95130157470703, Learning Rate: 0.01\n",
      "Epoch [4286/10000], Loss: 83.94684600830078, Learning Rate: 0.01\n",
      "Epoch [4287/10000], Loss: 83.93710327148438, Learning Rate: 0.01\n",
      "Epoch [4288/10000], Loss: 83.92483520507812, Learning Rate: 0.01\n",
      "Epoch [4289/10000], Loss: 83.91450500488281, Learning Rate: 0.01\n",
      "Epoch [4290/10000], Loss: 83.90811157226562, Learning Rate: 0.01\n",
      "Epoch [4291/10000], Loss: 83.90361785888672, Learning Rate: 0.01\n",
      "Epoch [4292/10000], Loss: 83.89771270751953, Learning Rate: 0.01\n",
      "Epoch [4293/10000], Loss: 83.88903045654297, Learning Rate: 0.01\n",
      "Epoch [4294/10000], Loss: 83.87909698486328, Learning Rate: 0.01\n",
      "Epoch [4295/10000], Loss: 83.87015533447266, Learning Rate: 0.01\n",
      "Epoch [4296/10000], Loss: 83.86326599121094, Learning Rate: 0.01\n",
      "Epoch [4297/10000], Loss: 83.8574447631836, Learning Rate: 0.01\n",
      "Epoch [4298/10000], Loss: 83.85088348388672, Learning Rate: 0.01\n",
      "Epoch [4299/10000], Loss: 83.84285736083984, Learning Rate: 0.01\n",
      "Epoch [4300/10000], Loss: 83.83405303955078, Learning Rate: 0.01\n",
      "Epoch [4301/10000], Loss: 83.82576751708984, Learning Rate: 0.01\n",
      "Epoch [4302/10000], Loss: 83.81859588623047, Learning Rate: 0.01\n",
      "Epoch [4303/10000], Loss: 83.81211853027344, Learning Rate: 0.01\n",
      "Epoch [4304/10000], Loss: 83.80514526367188, Learning Rate: 0.01\n",
      "Epoch [4305/10000], Loss: 83.79762268066406, Learning Rate: 0.01\n",
      "Epoch [4306/10000], Loss: 83.7895278930664, Learning Rate: 0.01\n",
      "Epoch [4307/10000], Loss: 83.7816390991211, Learning Rate: 0.01\n",
      "Epoch [4308/10000], Loss: 83.77430725097656, Learning Rate: 0.01\n",
      "Epoch [4309/10000], Loss: 83.76734161376953, Learning Rate: 0.01\n",
      "Epoch [4310/10000], Loss: 83.76026153564453, Learning Rate: 0.01\n",
      "Epoch [4311/10000], Loss: 83.75294494628906, Learning Rate: 0.01\n",
      "Epoch [4312/10000], Loss: 83.74517059326172, Learning Rate: 0.01\n",
      "Epoch [4313/10000], Loss: 83.73759460449219, Learning Rate: 0.01\n",
      "Epoch [4314/10000], Loss: 83.73014831542969, Learning Rate: 0.01\n",
      "Epoch [4315/10000], Loss: 83.72289276123047, Learning Rate: 0.01\n",
      "Epoch [4316/10000], Loss: 83.71578216552734, Learning Rate: 0.01\n",
      "Epoch [4317/10000], Loss: 83.70849609375, Learning Rate: 0.01\n",
      "Epoch [4318/10000], Loss: 83.7010726928711, Learning Rate: 0.01\n",
      "Epoch [4319/10000], Loss: 83.69353485107422, Learning Rate: 0.01\n",
      "Epoch [4320/10000], Loss: 83.6860580444336, Learning Rate: 0.01\n",
      "Epoch [4321/10000], Loss: 83.67876434326172, Learning Rate: 0.01\n",
      "Epoch [4322/10000], Loss: 83.6715316772461, Learning Rate: 0.01\n",
      "Epoch [4323/10000], Loss: 83.66426086425781, Learning Rate: 0.01\n",
      "Epoch [4324/10000], Loss: 83.6570053100586, Learning Rate: 0.01\n",
      "Epoch [4325/10000], Loss: 83.64964294433594, Learning Rate: 0.01\n",
      "Epoch [4326/10000], Loss: 83.64210510253906, Learning Rate: 0.01\n",
      "Epoch [4327/10000], Loss: 83.63477325439453, Learning Rate: 0.01\n",
      "Epoch [4328/10000], Loss: 83.6275405883789, Learning Rate: 0.01\n",
      "Epoch [4329/10000], Loss: 83.62033081054688, Learning Rate: 0.01\n",
      "Epoch [4330/10000], Loss: 83.61302947998047, Learning Rate: 0.01\n",
      "Epoch [4331/10000], Loss: 83.60570526123047, Learning Rate: 0.01\n",
      "Epoch [4332/10000], Loss: 83.59833526611328, Learning Rate: 0.01\n",
      "Epoch [4333/10000], Loss: 83.5910873413086, Learning Rate: 0.01\n",
      "Epoch [4334/10000], Loss: 83.58373260498047, Learning Rate: 0.01\n",
      "Epoch [4335/10000], Loss: 83.576416015625, Learning Rate: 0.01\n",
      "Epoch [4336/10000], Loss: 83.56918334960938, Learning Rate: 0.01\n",
      "Epoch [4337/10000], Loss: 83.56190490722656, Learning Rate: 0.01\n",
      "Epoch [4338/10000], Loss: 83.55452728271484, Learning Rate: 0.01\n",
      "Epoch [4339/10000], Loss: 83.54723358154297, Learning Rate: 0.01\n",
      "Epoch [4340/10000], Loss: 83.5400390625, Learning Rate: 0.01\n",
      "Epoch [4341/10000], Loss: 83.53269958496094, Learning Rate: 0.01\n",
      "Epoch [4342/10000], Loss: 83.52539825439453, Learning Rate: 0.01\n",
      "Epoch [4343/10000], Loss: 83.5180892944336, Learning Rate: 0.01\n",
      "Epoch [4344/10000], Loss: 83.51091003417969, Learning Rate: 0.01\n",
      "Epoch [4345/10000], Loss: 83.50361633300781, Learning Rate: 0.01\n",
      "Epoch [4346/10000], Loss: 83.49632263183594, Learning Rate: 0.01\n",
      "Epoch [4347/10000], Loss: 83.48907470703125, Learning Rate: 0.01\n",
      "Epoch [4348/10000], Loss: 83.48178100585938, Learning Rate: 0.01\n",
      "Epoch [4349/10000], Loss: 83.47459411621094, Learning Rate: 0.01\n",
      "Epoch [4350/10000], Loss: 83.46732330322266, Learning Rate: 0.01\n",
      "Epoch [4351/10000], Loss: 83.46009826660156, Learning Rate: 0.01\n",
      "Epoch [4352/10000], Loss: 83.45281219482422, Learning Rate: 0.01\n",
      "Epoch [4353/10000], Loss: 83.44556427001953, Learning Rate: 0.01\n",
      "Epoch [4354/10000], Loss: 83.43830108642578, Learning Rate: 0.01\n",
      "Epoch [4355/10000], Loss: 83.4310531616211, Learning Rate: 0.01\n",
      "Epoch [4356/10000], Loss: 83.42386627197266, Learning Rate: 0.01\n",
      "Epoch [4357/10000], Loss: 83.4165267944336, Learning Rate: 0.01\n",
      "Epoch [4358/10000], Loss: 83.4092788696289, Learning Rate: 0.01\n",
      "Epoch [4359/10000], Loss: 83.40213775634766, Learning Rate: 0.01\n",
      "Epoch [4360/10000], Loss: 83.39491271972656, Learning Rate: 0.01\n",
      "Epoch [4361/10000], Loss: 83.38765716552734, Learning Rate: 0.01\n",
      "Epoch [4362/10000], Loss: 83.38043975830078, Learning Rate: 0.01\n",
      "Epoch [4363/10000], Loss: 83.3731689453125, Learning Rate: 0.01\n",
      "Epoch [4364/10000], Loss: 83.36595153808594, Learning Rate: 0.01\n",
      "Epoch [4365/10000], Loss: 83.35879516601562, Learning Rate: 0.01\n",
      "Epoch [4366/10000], Loss: 83.35157012939453, Learning Rate: 0.01\n",
      "Epoch [4367/10000], Loss: 83.34432983398438, Learning Rate: 0.01\n",
      "Epoch [4368/10000], Loss: 83.33711242675781, Learning Rate: 0.01\n",
      "Epoch [4369/10000], Loss: 83.3299331665039, Learning Rate: 0.01\n",
      "Epoch [4370/10000], Loss: 83.3227310180664, Learning Rate: 0.01\n",
      "Epoch [4371/10000], Loss: 83.31553649902344, Learning Rate: 0.01\n",
      "Epoch [4372/10000], Loss: 83.30831146240234, Learning Rate: 0.01\n",
      "Epoch [4373/10000], Loss: 83.30116271972656, Learning Rate: 0.01\n",
      "Epoch [4374/10000], Loss: 83.29396057128906, Learning Rate: 0.01\n",
      "Epoch [4375/10000], Loss: 83.28665924072266, Learning Rate: 0.01\n",
      "Epoch [4376/10000], Loss: 83.27952575683594, Learning Rate: 0.01\n",
      "Epoch [4377/10000], Loss: 83.27237701416016, Learning Rate: 0.01\n",
      "Epoch [4378/10000], Loss: 83.26519012451172, Learning Rate: 0.01\n",
      "Epoch [4379/10000], Loss: 83.25798034667969, Learning Rate: 0.01\n",
      "Epoch [4380/10000], Loss: 83.25082397460938, Learning Rate: 0.01\n",
      "Epoch [4381/10000], Loss: 83.24360656738281, Learning Rate: 0.01\n",
      "Epoch [4382/10000], Loss: 83.23640441894531, Learning Rate: 0.01\n",
      "Epoch [4383/10000], Loss: 83.22930145263672, Learning Rate: 0.01\n",
      "Epoch [4384/10000], Loss: 83.22212982177734, Learning Rate: 0.01\n",
      "Epoch [4385/10000], Loss: 83.21490478515625, Learning Rate: 0.01\n",
      "Epoch [4386/10000], Loss: 83.20780181884766, Learning Rate: 0.01\n",
      "Epoch [4387/10000], Loss: 83.20054626464844, Learning Rate: 0.01\n",
      "Epoch [4388/10000], Loss: 83.19344329833984, Learning Rate: 0.01\n",
      "Epoch [4389/10000], Loss: 83.18629455566406, Learning Rate: 0.01\n",
      "Epoch [4390/10000], Loss: 83.17914581298828, Learning Rate: 0.01\n",
      "Epoch [4391/10000], Loss: 83.17203521728516, Learning Rate: 0.01\n",
      "Epoch [4392/10000], Loss: 83.16483306884766, Learning Rate: 0.01\n",
      "Epoch [4393/10000], Loss: 83.15769958496094, Learning Rate: 0.01\n",
      "Epoch [4394/10000], Loss: 83.15062713623047, Learning Rate: 0.01\n",
      "Epoch [4395/10000], Loss: 83.14344024658203, Learning Rate: 0.01\n",
      "Epoch [4396/10000], Loss: 83.1362533569336, Learning Rate: 0.01\n",
      "Epoch [4397/10000], Loss: 83.12911224365234, Learning Rate: 0.01\n",
      "Epoch [4398/10000], Loss: 83.12203979492188, Learning Rate: 0.01\n",
      "Epoch [4399/10000], Loss: 83.11492919921875, Learning Rate: 0.01\n",
      "Epoch [4400/10000], Loss: 83.1077880859375, Learning Rate: 0.01\n",
      "Epoch [4401/10000], Loss: 83.10063171386719, Learning Rate: 0.01\n",
      "Epoch [4402/10000], Loss: 83.09352111816406, Learning Rate: 0.01\n",
      "Epoch [4403/10000], Loss: 83.08638763427734, Learning Rate: 0.01\n",
      "Epoch [4404/10000], Loss: 83.07926940917969, Learning Rate: 0.01\n",
      "Epoch [4405/10000], Loss: 83.07215881347656, Learning Rate: 0.01\n",
      "Epoch [4406/10000], Loss: 83.0649642944336, Learning Rate: 0.01\n",
      "Epoch [4407/10000], Loss: 83.0579605102539, Learning Rate: 0.01\n",
      "Epoch [4408/10000], Loss: 83.05081939697266, Learning Rate: 0.01\n",
      "Epoch [4409/10000], Loss: 83.04373168945312, Learning Rate: 0.01\n",
      "Epoch [4410/10000], Loss: 83.03663635253906, Learning Rate: 0.01\n",
      "Epoch [4411/10000], Loss: 83.02952575683594, Learning Rate: 0.01\n",
      "Epoch [4412/10000], Loss: 83.02246856689453, Learning Rate: 0.01\n",
      "Epoch [4413/10000], Loss: 83.01530456542969, Learning Rate: 0.01\n",
      "Epoch [4414/10000], Loss: 83.00828552246094, Learning Rate: 0.01\n",
      "Epoch [4415/10000], Loss: 83.00125122070312, Learning Rate: 0.01\n",
      "Epoch [4416/10000], Loss: 82.99417114257812, Learning Rate: 0.01\n",
      "Epoch [4417/10000], Loss: 82.98704528808594, Learning Rate: 0.01\n",
      "Epoch [4418/10000], Loss: 82.9799575805664, Learning Rate: 0.01\n",
      "Epoch [4419/10000], Loss: 82.97298431396484, Learning Rate: 0.01\n",
      "Epoch [4420/10000], Loss: 82.96590423583984, Learning Rate: 0.01\n",
      "Epoch [4421/10000], Loss: 82.9587631225586, Learning Rate: 0.01\n",
      "Epoch [4422/10000], Loss: 82.95172119140625, Learning Rate: 0.01\n",
      "Epoch [4423/10000], Loss: 82.94464111328125, Learning Rate: 0.01\n",
      "Epoch [4424/10000], Loss: 82.93756103515625, Learning Rate: 0.01\n",
      "Epoch [4425/10000], Loss: 82.93054962158203, Learning Rate: 0.01\n",
      "Epoch [4426/10000], Loss: 82.92347717285156, Learning Rate: 0.01\n",
      "Epoch [4427/10000], Loss: 82.91639709472656, Learning Rate: 0.01\n",
      "Epoch [4428/10000], Loss: 82.90939331054688, Learning Rate: 0.01\n",
      "Epoch [4429/10000], Loss: 82.90231323242188, Learning Rate: 0.01\n",
      "Epoch [4430/10000], Loss: 82.89521789550781, Learning Rate: 0.01\n",
      "Epoch [4431/10000], Loss: 82.88823699951172, Learning Rate: 0.01\n",
      "Epoch [4432/10000], Loss: 82.88118743896484, Learning Rate: 0.01\n",
      "Epoch [4433/10000], Loss: 82.87411499023438, Learning Rate: 0.01\n",
      "Epoch [4434/10000], Loss: 82.86709594726562, Learning Rate: 0.01\n",
      "Epoch [4435/10000], Loss: 82.86009979248047, Learning Rate: 0.01\n",
      "Epoch [4436/10000], Loss: 82.85303497314453, Learning Rate: 0.01\n",
      "Epoch [4437/10000], Loss: 82.84599304199219, Learning Rate: 0.01\n",
      "Epoch [4438/10000], Loss: 82.83899688720703, Learning Rate: 0.01\n",
      "Epoch [4439/10000], Loss: 82.83194732666016, Learning Rate: 0.01\n",
      "Epoch [4440/10000], Loss: 82.82496643066406, Learning Rate: 0.01\n",
      "Epoch [4441/10000], Loss: 82.81793975830078, Learning Rate: 0.01\n",
      "Epoch [4442/10000], Loss: 82.81094360351562, Learning Rate: 0.01\n",
      "Epoch [4443/10000], Loss: 82.80390930175781, Learning Rate: 0.01\n",
      "Epoch [4444/10000], Loss: 82.79689025878906, Learning Rate: 0.01\n",
      "Epoch [4445/10000], Loss: 82.78990173339844, Learning Rate: 0.01\n",
      "Epoch [4446/10000], Loss: 82.7829360961914, Learning Rate: 0.01\n",
      "Epoch [4447/10000], Loss: 82.77590942382812, Learning Rate: 0.01\n",
      "Epoch [4448/10000], Loss: 82.76899719238281, Learning Rate: 0.01\n",
      "Epoch [4449/10000], Loss: 82.76193237304688, Learning Rate: 0.01\n",
      "Epoch [4450/10000], Loss: 82.75495910644531, Learning Rate: 0.01\n",
      "Epoch [4451/10000], Loss: 82.74800109863281, Learning Rate: 0.01\n",
      "Epoch [4452/10000], Loss: 82.74103546142578, Learning Rate: 0.01\n",
      "Epoch [4453/10000], Loss: 82.734130859375, Learning Rate: 0.01\n",
      "Epoch [4454/10000], Loss: 82.72725677490234, Learning Rate: 0.01\n",
      "Epoch [4455/10000], Loss: 82.72045135498047, Learning Rate: 0.01\n",
      "Epoch [4456/10000], Loss: 82.71368408203125, Learning Rate: 0.01\n",
      "Epoch [4457/10000], Loss: 82.7069320678711, Learning Rate: 0.01\n",
      "Epoch [4458/10000], Loss: 82.70038604736328, Learning Rate: 0.01\n",
      "Epoch [4459/10000], Loss: 82.69416046142578, Learning Rate: 0.01\n",
      "Epoch [4460/10000], Loss: 82.68822479248047, Learning Rate: 0.01\n",
      "Epoch [4461/10000], Loss: 82.68297576904297, Learning Rate: 0.01\n",
      "Epoch [4462/10000], Loss: 82.67871856689453, Learning Rate: 0.01\n",
      "Epoch [4463/10000], Loss: 82.67585754394531, Learning Rate: 0.01\n",
      "Epoch [4464/10000], Loss: 82.67534637451172, Learning Rate: 0.01\n",
      "Epoch [4465/10000], Loss: 82.67781829833984, Learning Rate: 0.01\n",
      "Epoch [4466/10000], Loss: 82.68444061279297, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4467/10000], Loss: 82.69549560546875, Learning Rate: 0.01\n",
      "Epoch [4468/10000], Loss: 82.70906829833984, Learning Rate: 0.01\n",
      "Epoch [4469/10000], Loss: 82.72010040283203, Learning Rate: 0.01\n",
      "Epoch [4470/10000], Loss: 82.71947479248047, Learning Rate: 0.01\n",
      "Epoch [4471/10000], Loss: 82.6998291015625, Learning Rate: 0.01\n",
      "Epoch [4472/10000], Loss: 82.66210174560547, Learning Rate: 0.01\n",
      "Epoch [4473/10000], Loss: 82.61997985839844, Learning Rate: 0.01\n",
      "Epoch [4474/10000], Loss: 82.59095764160156, Learning Rate: 0.01\n",
      "Epoch [4475/10000], Loss: 82.5837173461914, Learning Rate: 0.01\n",
      "Epoch [4476/10000], Loss: 82.59217834472656, Learning Rate: 0.01\n",
      "Epoch [4477/10000], Loss: 82.60248565673828, Learning Rate: 0.01\n",
      "Epoch [4478/10000], Loss: 82.60177612304688, Learning Rate: 0.01\n",
      "Epoch [4479/10000], Loss: 82.58607482910156, Learning Rate: 0.01\n",
      "Epoch [4480/10000], Loss: 82.56229400634766, Learning Rate: 0.01\n",
      "Epoch [4481/10000], Loss: 82.54236602783203, Learning Rate: 0.01\n",
      "Epoch [4482/10000], Loss: 82.53360748291016, Learning Rate: 0.01\n",
      "Epoch [4483/10000], Loss: 82.53434753417969, Learning Rate: 0.01\n",
      "Epoch [4484/10000], Loss: 82.53620147705078, Learning Rate: 0.01\n",
      "Epoch [4485/10000], Loss: 82.53203582763672, Learning Rate: 0.01\n",
      "Epoch [4486/10000], Loss: 82.52003479003906, Learning Rate: 0.01\n",
      "Epoch [4487/10000], Loss: 82.50475311279297, Learning Rate: 0.01\n",
      "Epoch [4488/10000], Loss: 82.4923095703125, Learning Rate: 0.01\n",
      "Epoch [4489/10000], Loss: 82.48572540283203, Learning Rate: 0.01\n",
      "Epoch [4490/10000], Loss: 82.48319244384766, Learning Rate: 0.01\n",
      "Epoch [4491/10000], Loss: 82.48030853271484, Learning Rate: 0.01\n",
      "Epoch [4492/10000], Loss: 82.47382354736328, Learning Rate: 0.01\n",
      "Epoch [4493/10000], Loss: 82.4637222290039, Learning Rate: 0.01\n",
      "Epoch [4494/10000], Loss: 82.45271301269531, Learning Rate: 0.01\n",
      "Epoch [4495/10000], Loss: 82.4438247680664, Learning Rate: 0.01\n",
      "Epoch [4496/10000], Loss: 82.43766021728516, Learning Rate: 0.01\n",
      "Epoch [4497/10000], Loss: 82.43319702148438, Learning Rate: 0.01\n",
      "Epoch [4498/10000], Loss: 82.42801666259766, Learning Rate: 0.01\n",
      "Epoch [4499/10000], Loss: 82.42090606689453, Learning Rate: 0.01\n",
      "Epoch [4500/10000], Loss: 82.41232299804688, Learning Rate: 0.01\n",
      "Epoch [4501/10000], Loss: 82.40363311767578, Learning Rate: 0.01\n",
      "Epoch [4502/10000], Loss: 82.39591979980469, Learning Rate: 0.01\n",
      "Epoch [4503/10000], Loss: 82.38956451416016, Learning Rate: 0.01\n",
      "Epoch [4504/10000], Loss: 82.38386535644531, Learning Rate: 0.01\n",
      "Epoch [4505/10000], Loss: 82.37793731689453, Learning Rate: 0.01\n",
      "Epoch [4506/10000], Loss: 82.37084197998047, Learning Rate: 0.01\n",
      "Epoch [4507/10000], Loss: 82.36317443847656, Learning Rate: 0.01\n",
      "Epoch [4508/10000], Loss: 82.35539245605469, Learning Rate: 0.01\n",
      "Epoch [4509/10000], Loss: 82.34812927246094, Learning Rate: 0.01\n",
      "Epoch [4510/10000], Loss: 82.3415298461914, Learning Rate: 0.01\n",
      "Epoch [4511/10000], Loss: 82.3353500366211, Learning Rate: 0.01\n",
      "Epoch [4512/10000], Loss: 82.32886505126953, Learning Rate: 0.01\n",
      "Epoch [4513/10000], Loss: 82.32202911376953, Learning Rate: 0.01\n",
      "Epoch [4514/10000], Loss: 82.31493377685547, Learning Rate: 0.01\n",
      "Epoch [4515/10000], Loss: 82.30768585205078, Learning Rate: 0.01\n",
      "Epoch [4516/10000], Loss: 82.30062103271484, Learning Rate: 0.01\n",
      "Epoch [4517/10000], Loss: 82.29380798339844, Learning Rate: 0.01\n",
      "Epoch [4518/10000], Loss: 82.28727722167969, Learning Rate: 0.01\n",
      "Epoch [4519/10000], Loss: 82.28079223632812, Learning Rate: 0.01\n",
      "Epoch [4520/10000], Loss: 82.27400207519531, Learning Rate: 0.01\n",
      "Epoch [4521/10000], Loss: 82.26708984375, Learning Rate: 0.01\n",
      "Epoch [4522/10000], Loss: 82.26020812988281, Learning Rate: 0.01\n",
      "Epoch [4523/10000], Loss: 82.2531967163086, Learning Rate: 0.01\n",
      "Epoch [4524/10000], Loss: 82.246337890625, Learning Rate: 0.01\n",
      "Epoch [4525/10000], Loss: 82.23958587646484, Learning Rate: 0.01\n",
      "Epoch [4526/10000], Loss: 82.23299407958984, Learning Rate: 0.01\n",
      "Epoch [4527/10000], Loss: 82.22634887695312, Learning Rate: 0.01\n",
      "Epoch [4528/10000], Loss: 82.21958923339844, Learning Rate: 0.01\n",
      "Epoch [4529/10000], Loss: 82.21280670166016, Learning Rate: 0.01\n",
      "Epoch [4530/10000], Loss: 82.20592498779297, Learning Rate: 0.01\n",
      "Epoch [4531/10000], Loss: 82.19918060302734, Learning Rate: 0.01\n",
      "Epoch [4532/10000], Loss: 82.1922836303711, Learning Rate: 0.01\n",
      "Epoch [4533/10000], Loss: 82.18558502197266, Learning Rate: 0.01\n",
      "Epoch [4534/10000], Loss: 82.17894744873047, Learning Rate: 0.01\n",
      "Epoch [4535/10000], Loss: 82.17222595214844, Learning Rate: 0.01\n",
      "Epoch [4536/10000], Loss: 82.16549682617188, Learning Rate: 0.01\n",
      "Epoch [4537/10000], Loss: 82.15872955322266, Learning Rate: 0.01\n",
      "Epoch [4538/10000], Loss: 82.15198516845703, Learning Rate: 0.01\n",
      "Epoch [4539/10000], Loss: 82.14518737792969, Learning Rate: 0.01\n",
      "Epoch [4540/10000], Loss: 82.13851165771484, Learning Rate: 0.01\n",
      "Epoch [4541/10000], Loss: 82.13178253173828, Learning Rate: 0.01\n",
      "Epoch [4542/10000], Loss: 82.12506103515625, Learning Rate: 0.01\n",
      "Epoch [4543/10000], Loss: 82.11834716796875, Learning Rate: 0.01\n",
      "Epoch [4544/10000], Loss: 82.1116714477539, Learning Rate: 0.01\n",
      "Epoch [4545/10000], Loss: 82.1049575805664, Learning Rate: 0.01\n",
      "Epoch [4546/10000], Loss: 82.09827423095703, Learning Rate: 0.01\n",
      "Epoch [4547/10000], Loss: 82.09153747558594, Learning Rate: 0.01\n",
      "Epoch [4548/10000], Loss: 82.08475494384766, Learning Rate: 0.01\n",
      "Epoch [4549/10000], Loss: 82.0781021118164, Learning Rate: 0.01\n",
      "Epoch [4550/10000], Loss: 82.07145690917969, Learning Rate: 0.01\n",
      "Epoch [4551/10000], Loss: 82.06470489501953, Learning Rate: 0.01\n",
      "Epoch [4552/10000], Loss: 82.0581283569336, Learning Rate: 0.01\n",
      "Epoch [4553/10000], Loss: 82.0514144897461, Learning Rate: 0.01\n",
      "Epoch [4554/10000], Loss: 82.0447006225586, Learning Rate: 0.01\n",
      "Epoch [4555/10000], Loss: 82.03801727294922, Learning Rate: 0.01\n",
      "Epoch [4556/10000], Loss: 82.03133392333984, Learning Rate: 0.01\n",
      "Epoch [4557/10000], Loss: 82.02465057373047, Learning Rate: 0.01\n",
      "Epoch [4558/10000], Loss: 82.01801300048828, Learning Rate: 0.01\n",
      "Epoch [4559/10000], Loss: 82.0113754272461, Learning Rate: 0.01\n",
      "Epoch [4560/10000], Loss: 82.0046157836914, Learning Rate: 0.01\n",
      "Epoch [4561/10000], Loss: 81.99794006347656, Learning Rate: 0.01\n",
      "Epoch [4562/10000], Loss: 81.9913101196289, Learning Rate: 0.01\n",
      "Epoch [4563/10000], Loss: 81.98468780517578, Learning Rate: 0.01\n",
      "Epoch [4564/10000], Loss: 81.97801971435547, Learning Rate: 0.01\n",
      "Epoch [4565/10000], Loss: 81.97140502929688, Learning Rate: 0.01\n",
      "Epoch [4566/10000], Loss: 81.96475219726562, Learning Rate: 0.01\n",
      "Epoch [4567/10000], Loss: 81.95809173583984, Learning Rate: 0.01\n",
      "Epoch [4568/10000], Loss: 81.95143127441406, Learning Rate: 0.01\n",
      "Epoch [4569/10000], Loss: 81.94481658935547, Learning Rate: 0.01\n",
      "Epoch [4570/10000], Loss: 81.93814086914062, Learning Rate: 0.01\n",
      "Epoch [4571/10000], Loss: 81.93151092529297, Learning Rate: 0.01\n",
      "Epoch [4572/10000], Loss: 81.92488861083984, Learning Rate: 0.01\n",
      "Epoch [4573/10000], Loss: 81.91825103759766, Learning Rate: 0.01\n",
      "Epoch [4574/10000], Loss: 81.9116439819336, Learning Rate: 0.01\n",
      "Epoch [4575/10000], Loss: 81.90502166748047, Learning Rate: 0.01\n",
      "Epoch [4576/10000], Loss: 81.89840698242188, Learning Rate: 0.01\n",
      "Epoch [4577/10000], Loss: 81.89171600341797, Learning Rate: 0.01\n",
      "Epoch [4578/10000], Loss: 81.88513946533203, Learning Rate: 0.01\n",
      "Epoch [4579/10000], Loss: 81.87852478027344, Learning Rate: 0.01\n",
      "Epoch [4580/10000], Loss: 81.8719482421875, Learning Rate: 0.01\n",
      "Epoch [4581/10000], Loss: 81.86531829833984, Learning Rate: 0.01\n",
      "Epoch [4582/10000], Loss: 81.85879516601562, Learning Rate: 0.01\n",
      "Epoch [4583/10000], Loss: 81.85218048095703, Learning Rate: 0.01\n",
      "Epoch [4584/10000], Loss: 81.84550476074219, Learning Rate: 0.01\n",
      "Epoch [4585/10000], Loss: 81.83892059326172, Learning Rate: 0.01\n",
      "Epoch [4586/10000], Loss: 81.83230590820312, Learning Rate: 0.01\n",
      "Epoch [4587/10000], Loss: 81.82577514648438, Learning Rate: 0.01\n",
      "Epoch [4588/10000], Loss: 81.81916809082031, Learning Rate: 0.01\n",
      "Epoch [4589/10000], Loss: 81.81256866455078, Learning Rate: 0.01\n",
      "Epoch [4590/10000], Loss: 81.80606842041016, Learning Rate: 0.01\n",
      "Epoch [4591/10000], Loss: 81.79936981201172, Learning Rate: 0.01\n",
      "Epoch [4592/10000], Loss: 81.7928695678711, Learning Rate: 0.01\n",
      "Epoch [4593/10000], Loss: 81.78633880615234, Learning Rate: 0.01\n",
      "Epoch [4594/10000], Loss: 81.77970886230469, Learning Rate: 0.01\n",
      "Epoch [4595/10000], Loss: 81.77311706542969, Learning Rate: 0.01\n",
      "Epoch [4596/10000], Loss: 81.76655578613281, Learning Rate: 0.01\n",
      "Epoch [4597/10000], Loss: 81.76004028320312, Learning Rate: 0.01\n",
      "Epoch [4598/10000], Loss: 81.75344848632812, Learning Rate: 0.01\n",
      "Epoch [4599/10000], Loss: 81.74691009521484, Learning Rate: 0.01\n",
      "Epoch [4600/10000], Loss: 81.74037170410156, Learning Rate: 0.01\n",
      "Epoch [4601/10000], Loss: 81.73373413085938, Learning Rate: 0.01\n",
      "Epoch [4602/10000], Loss: 81.72731018066406, Learning Rate: 0.01\n",
      "Epoch [4603/10000], Loss: 81.72077178955078, Learning Rate: 0.01\n",
      "Epoch [4604/10000], Loss: 81.71414947509766, Learning Rate: 0.01\n",
      "Epoch [4605/10000], Loss: 81.70769500732422, Learning Rate: 0.01\n",
      "Epoch [4606/10000], Loss: 81.70114135742188, Learning Rate: 0.01\n",
      "Epoch [4607/10000], Loss: 81.69464874267578, Learning Rate: 0.01\n",
      "Epoch [4608/10000], Loss: 81.68802642822266, Learning Rate: 0.01\n",
      "Epoch [4609/10000], Loss: 81.6815185546875, Learning Rate: 0.01\n",
      "Epoch [4610/10000], Loss: 81.6749267578125, Learning Rate: 0.01\n",
      "Epoch [4611/10000], Loss: 81.66841888427734, Learning Rate: 0.01\n",
      "Epoch [4612/10000], Loss: 81.66193389892578, Learning Rate: 0.01\n",
      "Epoch [4613/10000], Loss: 81.65541076660156, Learning Rate: 0.01\n",
      "Epoch [4614/10000], Loss: 81.64891052246094, Learning Rate: 0.01\n",
      "Epoch [4615/10000], Loss: 81.64234924316406, Learning Rate: 0.01\n",
      "Epoch [4616/10000], Loss: 81.63587951660156, Learning Rate: 0.01\n",
      "Epoch [4617/10000], Loss: 81.6293716430664, Learning Rate: 0.01\n",
      "Epoch [4618/10000], Loss: 81.6229019165039, Learning Rate: 0.01\n",
      "Epoch [4619/10000], Loss: 81.61634826660156, Learning Rate: 0.01\n",
      "Epoch [4620/10000], Loss: 81.60987091064453, Learning Rate: 0.01\n",
      "Epoch [4621/10000], Loss: 81.60345458984375, Learning Rate: 0.01\n",
      "Epoch [4622/10000], Loss: 81.59695434570312, Learning Rate: 0.01\n",
      "Epoch [4623/10000], Loss: 81.59040832519531, Learning Rate: 0.01\n",
      "Epoch [4624/10000], Loss: 81.58395385742188, Learning Rate: 0.01\n",
      "Epoch [4625/10000], Loss: 81.57752990722656, Learning Rate: 0.01\n",
      "Epoch [4626/10000], Loss: 81.57115173339844, Learning Rate: 0.01\n",
      "Epoch [4627/10000], Loss: 81.56474304199219, Learning Rate: 0.01\n",
      "Epoch [4628/10000], Loss: 81.55838012695312, Learning Rate: 0.01\n",
      "Epoch [4629/10000], Loss: 81.55213165283203, Learning Rate: 0.01\n",
      "Epoch [4630/10000], Loss: 81.5458984375, Learning Rate: 0.01\n",
      "Epoch [4631/10000], Loss: 81.53993225097656, Learning Rate: 0.01\n",
      "Epoch [4632/10000], Loss: 81.53416442871094, Learning Rate: 0.01\n",
      "Epoch [4633/10000], Loss: 81.52874755859375, Learning Rate: 0.01\n",
      "Epoch [4634/10000], Loss: 81.52392578125, Learning Rate: 0.01\n",
      "Epoch [4635/10000], Loss: 81.52009582519531, Learning Rate: 0.01\n",
      "Epoch [4636/10000], Loss: 81.51765441894531, Learning Rate: 0.01\n",
      "Epoch [4637/10000], Loss: 81.51726531982422, Learning Rate: 0.01\n",
      "Epoch [4638/10000], Loss: 81.52008056640625, Learning Rate: 0.01\n",
      "Epoch [4639/10000], Loss: 81.52739715576172, Learning Rate: 0.01\n",
      "Epoch [4640/10000], Loss: 81.5401840209961, Learning Rate: 0.01\n",
      "Epoch [4641/10000], Loss: 81.55899047851562, Learning Rate: 0.01\n",
      "Epoch [4642/10000], Loss: 81.58109283447266, Learning Rate: 0.01\n",
      "Epoch [4643/10000], Loss: 81.59973907470703, Learning Rate: 0.01\n",
      "Epoch [4644/10000], Loss: 81.60295104980469, Learning Rate: 0.01\n",
      "Epoch [4645/10000], Loss: 81.58060455322266, Learning Rate: 0.01\n",
      "Epoch [4646/10000], Loss: 81.53286743164062, Learning Rate: 0.01\n",
      "Epoch [4647/10000], Loss: 81.47687530517578, Learning Rate: 0.01\n",
      "Epoch [4648/10000], Loss: 81.43612670898438, Learning Rate: 0.01\n",
      "Epoch [4649/10000], Loss: 81.42406463623047, Learning Rate: 0.01\n",
      "Epoch [4650/10000], Loss: 81.4359359741211, Learning Rate: 0.01\n",
      "Epoch [4651/10000], Loss: 81.45435333251953, Learning Rate: 0.01\n",
      "Epoch [4652/10000], Loss: 81.46117401123047, Learning Rate: 0.01\n",
      "Epoch [4653/10000], Loss: 81.44793701171875, Learning Rate: 0.01\n",
      "Epoch [4654/10000], Loss: 81.42020416259766, Learning Rate: 0.01\n",
      "Epoch [4655/10000], Loss: 81.3926773071289, Learning Rate: 0.01\n",
      "Epoch [4656/10000], Loss: 81.3779525756836, Learning Rate: 0.01\n",
      "Epoch [4657/10000], Loss: 81.37784576416016, Learning Rate: 0.01\n",
      "Epoch [4658/10000], Loss: 81.38400268554688, Learning Rate: 0.01\n",
      "Epoch [4659/10000], Loss: 81.38574981689453, Learning Rate: 0.01\n",
      "Epoch [4660/10000], Loss: 81.37718963623047, Learning Rate: 0.01\n",
      "Epoch [4661/10000], Loss: 81.36068725585938, Learning Rate: 0.01\n",
      "Epoch [4662/10000], Loss: 81.34390258789062, Learning Rate: 0.01\n",
      "Epoch [4663/10000], Loss: 81.33345794677734, Learning Rate: 0.01\n",
      "Epoch [4664/10000], Loss: 81.33010864257812, Learning Rate: 0.01\n",
      "Epoch [4665/10000], Loss: 81.3299789428711, Learning Rate: 0.01\n",
      "Epoch [4666/10000], Loss: 81.32750701904297, Learning Rate: 0.01\n",
      "Epoch [4667/10000], Loss: 81.31993865966797, Learning Rate: 0.01\n",
      "Epoch [4668/10000], Loss: 81.3085708618164, Learning Rate: 0.01\n",
      "Epoch [4669/10000], Loss: 81.29721069335938, Learning Rate: 0.01\n",
      "Epoch [4670/10000], Loss: 81.28887176513672, Learning Rate: 0.01\n",
      "Epoch [4671/10000], Loss: 81.2840347290039, Learning Rate: 0.01\n",
      "Epoch [4672/10000], Loss: 81.28070831298828, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4673/10000], Loss: 81.27622985839844, Learning Rate: 0.01\n",
      "Epoch [4674/10000], Loss: 81.26957702636719, Learning Rate: 0.01\n",
      "Epoch [4675/10000], Loss: 81.260986328125, Learning Rate: 0.01\n",
      "Epoch [4676/10000], Loss: 81.25204467773438, Learning Rate: 0.01\n",
      "Epoch [4677/10000], Loss: 81.24455261230469, Learning Rate: 0.01\n",
      "Epoch [4678/10000], Loss: 81.23867797851562, Learning Rate: 0.01\n",
      "Epoch [4679/10000], Loss: 81.23368072509766, Learning Rate: 0.01\n",
      "Epoch [4680/10000], Loss: 81.22850036621094, Learning Rate: 0.01\n",
      "Epoch [4681/10000], Loss: 81.22224426269531, Learning Rate: 0.01\n",
      "Epoch [4682/10000], Loss: 81.21505737304688, Learning Rate: 0.01\n",
      "Epoch [4683/10000], Loss: 81.20753479003906, Learning Rate: 0.01\n",
      "Epoch [4684/10000], Loss: 81.2004165649414, Learning Rate: 0.01\n",
      "Epoch [4685/10000], Loss: 81.19406127929688, Learning Rate: 0.01\n",
      "Epoch [4686/10000], Loss: 81.18826293945312, Learning Rate: 0.01\n",
      "Epoch [4687/10000], Loss: 81.18260955810547, Learning Rate: 0.01\n",
      "Epoch [4688/10000], Loss: 81.17660522460938, Learning Rate: 0.01\n",
      "Epoch [4689/10000], Loss: 81.17010498046875, Learning Rate: 0.01\n",
      "Epoch [4690/10000], Loss: 81.16329956054688, Learning Rate: 0.01\n",
      "Epoch [4691/10000], Loss: 81.15644073486328, Learning Rate: 0.01\n",
      "Epoch [4692/10000], Loss: 81.14991760253906, Learning Rate: 0.01\n",
      "Epoch [4693/10000], Loss: 81.1436767578125, Learning Rate: 0.01\n",
      "Epoch [4694/10000], Loss: 81.13766479492188, Learning Rate: 0.01\n",
      "Epoch [4695/10000], Loss: 81.1316909790039, Learning Rate: 0.01\n",
      "Epoch [4696/10000], Loss: 81.12543487548828, Learning Rate: 0.01\n",
      "Epoch [4697/10000], Loss: 81.11904907226562, Learning Rate: 0.01\n",
      "Epoch [4698/10000], Loss: 81.1125717163086, Learning Rate: 0.01\n",
      "Epoch [4699/10000], Loss: 81.10610961914062, Learning Rate: 0.01\n",
      "Epoch [4700/10000], Loss: 81.09968566894531, Learning Rate: 0.01\n",
      "Epoch [4701/10000], Loss: 81.09349822998047, Learning Rate: 0.01\n",
      "Epoch [4702/10000], Loss: 81.08728790283203, Learning Rate: 0.01\n",
      "Epoch [4703/10000], Loss: 81.08109283447266, Learning Rate: 0.01\n",
      "Epoch [4704/10000], Loss: 81.07498168945312, Learning Rate: 0.01\n",
      "Epoch [4705/10000], Loss: 81.06861114501953, Learning Rate: 0.01\n",
      "Epoch [4706/10000], Loss: 81.06230926513672, Learning Rate: 0.01\n",
      "Epoch [4707/10000], Loss: 81.05592346191406, Learning Rate: 0.01\n",
      "Epoch [4708/10000], Loss: 81.04962921142578, Learning Rate: 0.01\n",
      "Epoch [4709/10000], Loss: 81.04340362548828, Learning Rate: 0.01\n",
      "Epoch [4710/10000], Loss: 81.03717041015625, Learning Rate: 0.01\n",
      "Epoch [4711/10000], Loss: 81.03099060058594, Learning Rate: 0.01\n",
      "Epoch [4712/10000], Loss: 81.02474212646484, Learning Rate: 0.01\n",
      "Epoch [4713/10000], Loss: 81.01852416992188, Learning Rate: 0.01\n",
      "Epoch [4714/10000], Loss: 81.01226806640625, Learning Rate: 0.01\n",
      "Epoch [4715/10000], Loss: 81.00596618652344, Learning Rate: 0.01\n",
      "Epoch [4716/10000], Loss: 80.99983215332031, Learning Rate: 0.01\n",
      "Epoch [4717/10000], Loss: 80.99349975585938, Learning Rate: 0.01\n",
      "Epoch [4718/10000], Loss: 80.98725891113281, Learning Rate: 0.01\n",
      "Epoch [4719/10000], Loss: 80.98103332519531, Learning Rate: 0.01\n",
      "Epoch [4720/10000], Loss: 80.97490692138672, Learning Rate: 0.01\n",
      "Epoch [4721/10000], Loss: 80.96861267089844, Learning Rate: 0.01\n",
      "Epoch [4722/10000], Loss: 80.96244812011719, Learning Rate: 0.01\n",
      "Epoch [4723/10000], Loss: 80.95620727539062, Learning Rate: 0.01\n",
      "Epoch [4724/10000], Loss: 80.9499740600586, Learning Rate: 0.01\n",
      "Epoch [4725/10000], Loss: 80.94376373291016, Learning Rate: 0.01\n",
      "Epoch [4726/10000], Loss: 80.93756103515625, Learning Rate: 0.01\n",
      "Epoch [4727/10000], Loss: 80.9313735961914, Learning Rate: 0.01\n",
      "Epoch [4728/10000], Loss: 80.92516326904297, Learning Rate: 0.01\n",
      "Epoch [4729/10000], Loss: 80.91896057128906, Learning Rate: 0.01\n",
      "Epoch [4730/10000], Loss: 80.9127426147461, Learning Rate: 0.01\n",
      "Epoch [4731/10000], Loss: 80.90657806396484, Learning Rate: 0.01\n",
      "Epoch [4732/10000], Loss: 80.9004135131836, Learning Rate: 0.01\n",
      "Epoch [4733/10000], Loss: 80.89422607421875, Learning Rate: 0.01\n",
      "Epoch [4734/10000], Loss: 80.8880844116211, Learning Rate: 0.01\n",
      "Epoch [4735/10000], Loss: 80.88188171386719, Learning Rate: 0.01\n",
      "Epoch [4736/10000], Loss: 80.87566375732422, Learning Rate: 0.01\n",
      "Epoch [4737/10000], Loss: 80.86949157714844, Learning Rate: 0.01\n",
      "Epoch [4738/10000], Loss: 80.86337280273438, Learning Rate: 0.01\n",
      "Epoch [4739/10000], Loss: 80.8571548461914, Learning Rate: 0.01\n",
      "Epoch [4740/10000], Loss: 80.85099792480469, Learning Rate: 0.01\n",
      "Epoch [4741/10000], Loss: 80.84478759765625, Learning Rate: 0.01\n",
      "Epoch [4742/10000], Loss: 80.83861541748047, Learning Rate: 0.01\n",
      "Epoch [4743/10000], Loss: 80.83251953125, Learning Rate: 0.01\n",
      "Epoch [4744/10000], Loss: 80.82627868652344, Learning Rate: 0.01\n",
      "Epoch [4745/10000], Loss: 80.82012939453125, Learning Rate: 0.01\n",
      "Epoch [4746/10000], Loss: 80.8139877319336, Learning Rate: 0.01\n",
      "Epoch [4747/10000], Loss: 80.8078384399414, Learning Rate: 0.01\n",
      "Epoch [4748/10000], Loss: 80.80171966552734, Learning Rate: 0.01\n",
      "Epoch [4749/10000], Loss: 80.79558563232422, Learning Rate: 0.01\n",
      "Epoch [4750/10000], Loss: 80.78934478759766, Learning Rate: 0.01\n",
      "Epoch [4751/10000], Loss: 80.78333282470703, Learning Rate: 0.01\n",
      "Epoch [4752/10000], Loss: 80.77711486816406, Learning Rate: 0.01\n",
      "Epoch [4753/10000], Loss: 80.77091217041016, Learning Rate: 0.01\n",
      "Epoch [4754/10000], Loss: 80.76485443115234, Learning Rate: 0.01\n",
      "Epoch [4755/10000], Loss: 80.75870513916016, Learning Rate: 0.01\n",
      "Epoch [4756/10000], Loss: 80.75257110595703, Learning Rate: 0.01\n",
      "Epoch [4757/10000], Loss: 80.74644470214844, Learning Rate: 0.01\n",
      "Epoch [4758/10000], Loss: 80.74031829833984, Learning Rate: 0.01\n",
      "Epoch [4759/10000], Loss: 80.73413848876953, Learning Rate: 0.01\n",
      "Epoch [4760/10000], Loss: 80.72806549072266, Learning Rate: 0.01\n",
      "Epoch [4761/10000], Loss: 80.72196960449219, Learning Rate: 0.01\n",
      "Epoch [4762/10000], Loss: 80.7158432006836, Learning Rate: 0.01\n",
      "Epoch [4763/10000], Loss: 80.70966339111328, Learning Rate: 0.01\n",
      "Epoch [4764/10000], Loss: 80.70362091064453, Learning Rate: 0.01\n",
      "Epoch [4765/10000], Loss: 80.697509765625, Learning Rate: 0.01\n",
      "Epoch [4766/10000], Loss: 80.6914291381836, Learning Rate: 0.01\n",
      "Epoch [4767/10000], Loss: 80.6852798461914, Learning Rate: 0.01\n",
      "Epoch [4768/10000], Loss: 80.67918395996094, Learning Rate: 0.01\n",
      "Epoch [4769/10000], Loss: 80.67310333251953, Learning Rate: 0.01\n",
      "Epoch [4770/10000], Loss: 80.66703796386719, Learning Rate: 0.01\n",
      "Epoch [4771/10000], Loss: 80.6608657836914, Learning Rate: 0.01\n",
      "Epoch [4772/10000], Loss: 80.65484619140625, Learning Rate: 0.01\n",
      "Epoch [4773/10000], Loss: 80.64865112304688, Learning Rate: 0.01\n",
      "Epoch [4774/10000], Loss: 80.64265441894531, Learning Rate: 0.01\n",
      "Epoch [4775/10000], Loss: 80.63646697998047, Learning Rate: 0.01\n",
      "Epoch [4776/10000], Loss: 80.6304702758789, Learning Rate: 0.01\n",
      "Epoch [4777/10000], Loss: 80.6243896484375, Learning Rate: 0.01\n",
      "Epoch [4778/10000], Loss: 80.61822509765625, Learning Rate: 0.01\n",
      "Epoch [4779/10000], Loss: 80.61223602294922, Learning Rate: 0.01\n",
      "Epoch [4780/10000], Loss: 80.60615539550781, Learning Rate: 0.01\n",
      "Epoch [4781/10000], Loss: 80.60002136230469, Learning Rate: 0.01\n",
      "Epoch [4782/10000], Loss: 80.59398651123047, Learning Rate: 0.01\n",
      "Epoch [4783/10000], Loss: 80.58792877197266, Learning Rate: 0.01\n",
      "Epoch [4784/10000], Loss: 80.58186340332031, Learning Rate: 0.01\n",
      "Epoch [4785/10000], Loss: 80.57579040527344, Learning Rate: 0.01\n",
      "Epoch [4786/10000], Loss: 80.56966400146484, Learning Rate: 0.01\n",
      "Epoch [4787/10000], Loss: 80.56365966796875, Learning Rate: 0.01\n",
      "Epoch [4788/10000], Loss: 80.55767059326172, Learning Rate: 0.01\n",
      "Epoch [4789/10000], Loss: 80.55156707763672, Learning Rate: 0.01\n",
      "Epoch [4790/10000], Loss: 80.54557800292969, Learning Rate: 0.01\n",
      "Epoch [4791/10000], Loss: 80.53948974609375, Learning Rate: 0.01\n",
      "Epoch [4792/10000], Loss: 80.53343200683594, Learning Rate: 0.01\n",
      "Epoch [4793/10000], Loss: 80.52735900878906, Learning Rate: 0.01\n",
      "Epoch [4794/10000], Loss: 80.52128601074219, Learning Rate: 0.01\n",
      "Epoch [4795/10000], Loss: 80.5152816772461, Learning Rate: 0.01\n",
      "Epoch [4796/10000], Loss: 80.5093765258789, Learning Rate: 0.01\n",
      "Epoch [4797/10000], Loss: 80.50336456298828, Learning Rate: 0.01\n",
      "Epoch [4798/10000], Loss: 80.49738311767578, Learning Rate: 0.01\n",
      "Epoch [4799/10000], Loss: 80.4913558959961, Learning Rate: 0.01\n",
      "Epoch [4800/10000], Loss: 80.48552703857422, Learning Rate: 0.01\n",
      "Epoch [4801/10000], Loss: 80.47968292236328, Learning Rate: 0.01\n",
      "Epoch [4802/10000], Loss: 80.47400665283203, Learning Rate: 0.01\n",
      "Epoch [4803/10000], Loss: 80.46844482421875, Learning Rate: 0.01\n",
      "Epoch [4804/10000], Loss: 80.46308898925781, Learning Rate: 0.01\n",
      "Epoch [4805/10000], Loss: 80.45830535888672, Learning Rate: 0.01\n",
      "Epoch [4806/10000], Loss: 80.4542007446289, Learning Rate: 0.01\n",
      "Epoch [4807/10000], Loss: 80.45111846923828, Learning Rate: 0.01\n",
      "Epoch [4808/10000], Loss: 80.449951171875, Learning Rate: 0.01\n",
      "Epoch [4809/10000], Loss: 80.45155334472656, Learning Rate: 0.01\n",
      "Epoch [4810/10000], Loss: 80.45748138427734, Learning Rate: 0.01\n",
      "Epoch [4811/10000], Loss: 80.4699478149414, Learning Rate: 0.01\n",
      "Epoch [4812/10000], Loss: 80.49177551269531, Learning Rate: 0.01\n",
      "Epoch [4813/10000], Loss: 80.52493286132812, Learning Rate: 0.01\n",
      "Epoch [4814/10000], Loss: 80.56895446777344, Learning Rate: 0.01\n",
      "Epoch [4815/10000], Loss: 80.61611938476562, Learning Rate: 0.01\n",
      "Epoch [4816/10000], Loss: 80.6480941772461, Learning Rate: 0.01\n",
      "Epoch [4817/10000], Loss: 80.63971710205078, Learning Rate: 0.01\n",
      "Epoch [4818/10000], Loss: 80.57793426513672, Learning Rate: 0.01\n",
      "Epoch [4819/10000], Loss: 80.48119354248047, Learning Rate: 0.01\n",
      "Epoch [4820/10000], Loss: 80.39517211914062, Learning Rate: 0.01\n",
      "Epoch [4821/10000], Loss: 80.359130859375, Learning Rate: 0.01\n",
      "Epoch [4822/10000], Loss: 80.37689208984375, Learning Rate: 0.01\n",
      "Epoch [4823/10000], Loss: 80.41876983642578, Learning Rate: 0.01\n",
      "Epoch [4824/10000], Loss: 80.44532012939453, Learning Rate: 0.01\n",
      "Epoch [4825/10000], Loss: 80.43306732177734, Learning Rate: 0.01\n",
      "Epoch [4826/10000], Loss: 80.38818359375, Learning Rate: 0.01\n",
      "Epoch [4827/10000], Loss: 80.34046173095703, Learning Rate: 0.01\n",
      "Epoch [4828/10000], Loss: 80.31755828857422, Learning Rate: 0.01\n",
      "Epoch [4829/10000], Loss: 80.32432556152344, Learning Rate: 0.01\n",
      "Epoch [4830/10000], Loss: 80.3432388305664, Learning Rate: 0.01\n",
      "Epoch [4831/10000], Loss: 80.3512191772461, Learning Rate: 0.01\n",
      "Epoch [4832/10000], Loss: 80.337646484375, Learning Rate: 0.01\n",
      "Epoch [4833/10000], Loss: 80.31011199951172, Learning Rate: 0.01\n",
      "Epoch [4834/10000], Loss: 80.28592681884766, Learning Rate: 0.01\n",
      "Epoch [4835/10000], Loss: 80.27680206298828, Learning Rate: 0.01\n",
      "Epoch [4836/10000], Loss: 80.28081512451172, Learning Rate: 0.01\n",
      "Epoch [4837/10000], Loss: 80.28678131103516, Learning Rate: 0.01\n",
      "Epoch [4838/10000], Loss: 80.28398895263672, Learning Rate: 0.01\n",
      "Epoch [4839/10000], Loss: 80.27066802978516, Learning Rate: 0.01\n",
      "Epoch [4840/10000], Loss: 80.25344848632812, Learning Rate: 0.01\n",
      "Epoch [4841/10000], Loss: 80.2408218383789, Learning Rate: 0.01\n",
      "Epoch [4842/10000], Loss: 80.23629760742188, Learning Rate: 0.01\n",
      "Epoch [4843/10000], Loss: 80.23645782470703, Learning Rate: 0.01\n",
      "Epoch [4844/10000], Loss: 80.23538970947266, Learning Rate: 0.01\n",
      "Epoch [4845/10000], Loss: 80.22911071777344, Learning Rate: 0.01\n",
      "Epoch [4846/10000], Loss: 80.21831512451172, Learning Rate: 0.01\n",
      "Epoch [4847/10000], Loss: 80.20714569091797, Learning Rate: 0.01\n",
      "Epoch [4848/10000], Loss: 80.19908905029297, Learning Rate: 0.01\n",
      "Epoch [4849/10000], Loss: 80.19476318359375, Learning Rate: 0.01\n",
      "Epoch [4850/10000], Loss: 80.19207763671875, Learning Rate: 0.01\n",
      "Epoch [4851/10000], Loss: 80.18801879882812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4852/10000], Loss: 80.18133544921875, Learning Rate: 0.01\n",
      "Epoch [4853/10000], Loss: 80.17282104492188, Learning Rate: 0.01\n",
      "Epoch [4854/10000], Loss: 80.16448211669922, Learning Rate: 0.01\n",
      "Epoch [4855/10000], Loss: 80.15779113769531, Learning Rate: 0.01\n",
      "Epoch [4856/10000], Loss: 80.1528549194336, Learning Rate: 0.01\n",
      "Epoch [4857/10000], Loss: 80.14848327636719, Learning Rate: 0.01\n",
      "Epoch [4858/10000], Loss: 80.14346313476562, Learning Rate: 0.01\n",
      "Epoch [4859/10000], Loss: 80.13720703125, Learning Rate: 0.01\n",
      "Epoch [4860/10000], Loss: 80.13001251220703, Learning Rate: 0.01\n",
      "Epoch [4861/10000], Loss: 80.12299346923828, Learning Rate: 0.01\n",
      "Epoch [4862/10000], Loss: 80.11663055419922, Learning Rate: 0.01\n",
      "Epoch [4863/10000], Loss: 80.11117553710938, Learning Rate: 0.01\n",
      "Epoch [4864/10000], Loss: 80.10601043701172, Learning Rate: 0.01\n",
      "Epoch [4865/10000], Loss: 80.1006088256836, Learning Rate: 0.01\n",
      "Epoch [4866/10000], Loss: 80.09466552734375, Learning Rate: 0.01\n",
      "Epoch [4867/10000], Loss: 80.08828735351562, Learning Rate: 0.01\n",
      "Epoch [4868/10000], Loss: 80.08183288574219, Learning Rate: 0.01\n",
      "Epoch [4869/10000], Loss: 80.0756607055664, Learning Rate: 0.01\n",
      "Epoch [4870/10000], Loss: 80.06977844238281, Learning Rate: 0.01\n",
      "Epoch [4871/10000], Loss: 80.06425476074219, Learning Rate: 0.01\n",
      "Epoch [4872/10000], Loss: 80.05863952636719, Learning Rate: 0.01\n",
      "Epoch [4873/10000], Loss: 80.05293273925781, Learning Rate: 0.01\n",
      "Epoch [4874/10000], Loss: 80.046875, Learning Rate: 0.01\n",
      "Epoch [4875/10000], Loss: 80.04084014892578, Learning Rate: 0.01\n",
      "Epoch [4876/10000], Loss: 80.03474426269531, Learning Rate: 0.01\n",
      "Epoch [4877/10000], Loss: 80.02877044677734, Learning Rate: 0.01\n",
      "Epoch [4878/10000], Loss: 80.02300262451172, Learning Rate: 0.01\n",
      "Epoch [4879/10000], Loss: 80.01734924316406, Learning Rate: 0.01\n",
      "Epoch [4880/10000], Loss: 80.01156616210938, Learning Rate: 0.01\n",
      "Epoch [4881/10000], Loss: 80.00582885742188, Learning Rate: 0.01\n",
      "Epoch [4882/10000], Loss: 79.99983215332031, Learning Rate: 0.01\n",
      "Epoch [4883/10000], Loss: 79.99396514892578, Learning Rate: 0.01\n",
      "Epoch [4884/10000], Loss: 79.98799896240234, Learning Rate: 0.01\n",
      "Epoch [4885/10000], Loss: 79.98212432861328, Learning Rate: 0.01\n",
      "Epoch [4886/10000], Loss: 79.9762954711914, Learning Rate: 0.01\n",
      "Epoch [4887/10000], Loss: 79.97062683105469, Learning Rate: 0.01\n",
      "Epoch [4888/10000], Loss: 79.96476745605469, Learning Rate: 0.01\n",
      "Epoch [4889/10000], Loss: 79.95897674560547, Learning Rate: 0.01\n",
      "Epoch [4890/10000], Loss: 79.95316314697266, Learning Rate: 0.01\n",
      "Epoch [4891/10000], Loss: 79.94731140136719, Learning Rate: 0.01\n",
      "Epoch [4892/10000], Loss: 79.94142150878906, Learning Rate: 0.01\n",
      "Epoch [4893/10000], Loss: 79.93557739257812, Learning Rate: 0.01\n",
      "Epoch [4894/10000], Loss: 79.92974853515625, Learning Rate: 0.01\n",
      "Epoch [4895/10000], Loss: 79.92401123046875, Learning Rate: 0.01\n",
      "Epoch [4896/10000], Loss: 79.91824340820312, Learning Rate: 0.01\n",
      "Epoch [4897/10000], Loss: 79.91239929199219, Learning Rate: 0.01\n",
      "Epoch [4898/10000], Loss: 79.90662384033203, Learning Rate: 0.01\n",
      "Epoch [4899/10000], Loss: 79.90087890625, Learning Rate: 0.01\n",
      "Epoch [4900/10000], Loss: 79.89498901367188, Learning Rate: 0.01\n",
      "Epoch [4901/10000], Loss: 79.88919067382812, Learning Rate: 0.01\n",
      "Epoch [4902/10000], Loss: 79.88336944580078, Learning Rate: 0.01\n",
      "Epoch [4903/10000], Loss: 79.87762451171875, Learning Rate: 0.01\n",
      "Epoch [4904/10000], Loss: 79.87181854248047, Learning Rate: 0.01\n",
      "Epoch [4905/10000], Loss: 79.8659896850586, Learning Rate: 0.01\n",
      "Epoch [4906/10000], Loss: 79.86019134521484, Learning Rate: 0.01\n",
      "Epoch [4907/10000], Loss: 79.85448455810547, Learning Rate: 0.01\n",
      "Epoch [4908/10000], Loss: 79.84864807128906, Learning Rate: 0.01\n",
      "Epoch [4909/10000], Loss: 79.84288787841797, Learning Rate: 0.01\n",
      "Epoch [4910/10000], Loss: 79.83712768554688, Learning Rate: 0.01\n",
      "Epoch [4911/10000], Loss: 79.83134460449219, Learning Rate: 0.01\n",
      "Epoch [4912/10000], Loss: 79.82554626464844, Learning Rate: 0.01\n",
      "Epoch [4913/10000], Loss: 79.81976318359375, Learning Rate: 0.01\n",
      "Epoch [4914/10000], Loss: 79.81401824951172, Learning Rate: 0.01\n",
      "Epoch [4915/10000], Loss: 79.80827331542969, Learning Rate: 0.01\n",
      "Epoch [4916/10000], Loss: 79.80249786376953, Learning Rate: 0.01\n",
      "Epoch [4917/10000], Loss: 79.79676055908203, Learning Rate: 0.01\n",
      "Epoch [4918/10000], Loss: 79.79096221923828, Learning Rate: 0.01\n",
      "Epoch [4919/10000], Loss: 79.78524780273438, Learning Rate: 0.01\n",
      "Epoch [4920/10000], Loss: 79.77942657470703, Learning Rate: 0.01\n",
      "Epoch [4921/10000], Loss: 79.77362823486328, Learning Rate: 0.01\n",
      "Epoch [4922/10000], Loss: 79.76792907714844, Learning Rate: 0.01\n",
      "Epoch [4923/10000], Loss: 79.76217651367188, Learning Rate: 0.01\n",
      "Epoch [4924/10000], Loss: 79.75639343261719, Learning Rate: 0.01\n",
      "Epoch [4925/10000], Loss: 79.75066375732422, Learning Rate: 0.01\n",
      "Epoch [4926/10000], Loss: 79.74488830566406, Learning Rate: 0.01\n",
      "Epoch [4927/10000], Loss: 79.73917388916016, Learning Rate: 0.01\n",
      "Epoch [4928/10000], Loss: 79.73352813720703, Learning Rate: 0.01\n",
      "Epoch [4929/10000], Loss: 79.72772216796875, Learning Rate: 0.01\n",
      "Epoch [4930/10000], Loss: 79.72193908691406, Learning Rate: 0.01\n",
      "Epoch [4931/10000], Loss: 79.71624755859375, Learning Rate: 0.01\n",
      "Epoch [4932/10000], Loss: 79.71044921875, Learning Rate: 0.01\n",
      "Epoch [4933/10000], Loss: 79.7046890258789, Learning Rate: 0.01\n",
      "Epoch [4934/10000], Loss: 79.69904327392578, Learning Rate: 0.01\n",
      "Epoch [4935/10000], Loss: 79.69324493408203, Learning Rate: 0.01\n",
      "Epoch [4936/10000], Loss: 79.6875228881836, Learning Rate: 0.01\n",
      "Epoch [4937/10000], Loss: 79.68180847167969, Learning Rate: 0.01\n",
      "Epoch [4938/10000], Loss: 79.67615509033203, Learning Rate: 0.01\n",
      "Epoch [4939/10000], Loss: 79.67041778564453, Learning Rate: 0.01\n",
      "Epoch [4940/10000], Loss: 79.6646728515625, Learning Rate: 0.01\n",
      "Epoch [4941/10000], Loss: 79.65896606445312, Learning Rate: 0.01\n",
      "Epoch [4942/10000], Loss: 79.65326690673828, Learning Rate: 0.01\n",
      "Epoch [4943/10000], Loss: 79.64756774902344, Learning Rate: 0.01\n",
      "Epoch [4944/10000], Loss: 79.64183044433594, Learning Rate: 0.01\n",
      "Epoch [4945/10000], Loss: 79.63609313964844, Learning Rate: 0.01\n",
      "Epoch [4946/10000], Loss: 79.63041687011719, Learning Rate: 0.01\n",
      "Epoch [4947/10000], Loss: 79.62472534179688, Learning Rate: 0.01\n",
      "Epoch [4948/10000], Loss: 79.61895751953125, Learning Rate: 0.01\n",
      "Epoch [4949/10000], Loss: 79.6132583618164, Learning Rate: 0.01\n",
      "Epoch [4950/10000], Loss: 79.60753631591797, Learning Rate: 0.01\n",
      "Epoch [4951/10000], Loss: 79.60191345214844, Learning Rate: 0.01\n",
      "Epoch [4952/10000], Loss: 79.59615325927734, Learning Rate: 0.01\n",
      "Epoch [4953/10000], Loss: 79.59049224853516, Learning Rate: 0.01\n",
      "Epoch [4954/10000], Loss: 79.5848159790039, Learning Rate: 0.01\n",
      "Epoch [4955/10000], Loss: 79.57906341552734, Learning Rate: 0.01\n",
      "Epoch [4956/10000], Loss: 79.57341003417969, Learning Rate: 0.01\n",
      "Epoch [4957/10000], Loss: 79.56764221191406, Learning Rate: 0.01\n",
      "Epoch [4958/10000], Loss: 79.56204223632812, Learning Rate: 0.01\n",
      "Epoch [4959/10000], Loss: 79.55632019042969, Learning Rate: 0.01\n",
      "Epoch [4960/10000], Loss: 79.55064392089844, Learning Rate: 0.01\n",
      "Epoch [4961/10000], Loss: 79.54503631591797, Learning Rate: 0.01\n",
      "Epoch [4962/10000], Loss: 79.5392837524414, Learning Rate: 0.01\n",
      "Epoch [4963/10000], Loss: 79.53360748291016, Learning Rate: 0.01\n",
      "Epoch [4964/10000], Loss: 79.52792358398438, Learning Rate: 0.01\n",
      "Epoch [4965/10000], Loss: 79.52222442626953, Learning Rate: 0.01\n",
      "Epoch [4966/10000], Loss: 79.51665496826172, Learning Rate: 0.01\n",
      "Epoch [4967/10000], Loss: 79.51091766357422, Learning Rate: 0.01\n",
      "Epoch [4968/10000], Loss: 79.50526428222656, Learning Rate: 0.01\n",
      "Epoch [4969/10000], Loss: 79.49951171875, Learning Rate: 0.01\n",
      "Epoch [4970/10000], Loss: 79.49395751953125, Learning Rate: 0.01\n",
      "Epoch [4971/10000], Loss: 79.48829650878906, Learning Rate: 0.01\n",
      "Epoch [4972/10000], Loss: 79.48262786865234, Learning Rate: 0.01\n",
      "Epoch [4973/10000], Loss: 79.47698211669922, Learning Rate: 0.01\n",
      "Epoch [4974/10000], Loss: 79.47132110595703, Learning Rate: 0.01\n",
      "Epoch [4975/10000], Loss: 79.46566009521484, Learning Rate: 0.01\n",
      "Epoch [4976/10000], Loss: 79.4599609375, Learning Rate: 0.01\n",
      "Epoch [4977/10000], Loss: 79.45431518554688, Learning Rate: 0.01\n",
      "Epoch [4978/10000], Loss: 79.4487075805664, Learning Rate: 0.01\n",
      "Epoch [4979/10000], Loss: 79.44306182861328, Learning Rate: 0.01\n",
      "Epoch [4980/10000], Loss: 79.43750762939453, Learning Rate: 0.01\n",
      "Epoch [4981/10000], Loss: 79.43185424804688, Learning Rate: 0.01\n",
      "Epoch [4982/10000], Loss: 79.4262924194336, Learning Rate: 0.01\n",
      "Epoch [4983/10000], Loss: 79.42070007324219, Learning Rate: 0.01\n",
      "Epoch [4984/10000], Loss: 79.41522216796875, Learning Rate: 0.01\n",
      "Epoch [4985/10000], Loss: 79.40973663330078, Learning Rate: 0.01\n",
      "Epoch [4986/10000], Loss: 79.40438079833984, Learning Rate: 0.01\n",
      "Epoch [4987/10000], Loss: 79.39923095703125, Learning Rate: 0.01\n",
      "Epoch [4988/10000], Loss: 79.39427947998047, Learning Rate: 0.01\n",
      "Epoch [4989/10000], Loss: 79.38976287841797, Learning Rate: 0.01\n",
      "Epoch [4990/10000], Loss: 79.38573455810547, Learning Rate: 0.01\n",
      "Epoch [4991/10000], Loss: 79.38285064697266, Learning Rate: 0.01\n",
      "Epoch [4992/10000], Loss: 79.38145446777344, Learning Rate: 0.01\n",
      "Epoch [4993/10000], Loss: 79.3824691772461, Learning Rate: 0.01\n",
      "Epoch [4994/10000], Loss: 79.38734436035156, Learning Rate: 0.01\n",
      "Epoch [4995/10000], Loss: 79.39810943603516, Learning Rate: 0.01\n",
      "Epoch [4996/10000], Loss: 79.41765594482422, Learning Rate: 0.01\n",
      "Epoch [4997/10000], Loss: 79.44986724853516, Learning Rate: 0.01\n",
      "Epoch [4998/10000], Loss: 79.49827575683594, Learning Rate: 0.01\n",
      "Epoch [4999/10000], Loss: 79.56381225585938, Learning Rate: 0.01\n",
      "Epoch [5000/10000], Loss: 79.63894653320312, Learning Rate: 0.01\n",
      "Epoch [5001/10000], Loss: 79.7027359008789, Learning Rate: 0.01\n",
      "Epoch [5002/10000], Loss: 79.7197494506836, Learning Rate: 0.01\n",
      "Epoch [5003/10000], Loss: 79.6612319946289, Learning Rate: 0.01\n",
      "Epoch [5004/10000], Loss: 79.53348541259766, Learning Rate: 0.01\n",
      "Epoch [5005/10000], Loss: 79.39102935791016, Learning Rate: 0.01\n",
      "Epoch [5006/10000], Loss: 79.3014907836914, Learning Rate: 0.01\n",
      "Epoch [5007/10000], Loss: 79.2958984375, Learning Rate: 0.01\n",
      "Epoch [5008/10000], Loss: 79.35101318359375, Learning Rate: 0.01\n",
      "Epoch [5009/10000], Loss: 79.41214752197266, Learning Rate: 0.01\n",
      "Epoch [5010/10000], Loss: 79.43034362792969, Learning Rate: 0.01\n",
      "Epoch [5011/10000], Loss: 79.39039611816406, Learning Rate: 0.01\n",
      "Epoch [5012/10000], Loss: 79.31932830810547, Learning Rate: 0.01\n",
      "Epoch [5013/10000], Loss: 79.26266479492188, Learning Rate: 0.01\n",
      "Epoch [5014/10000], Loss: 79.24906921386719, Learning Rate: 0.01\n",
      "Epoch [5015/10000], Loss: 79.27214813232422, Learning Rate: 0.01\n",
      "Epoch [5016/10000], Loss: 79.301025390625, Learning Rate: 0.01\n",
      "Epoch [5017/10000], Loss: 79.30706024169922, Learning Rate: 0.01\n",
      "Epoch [5018/10000], Loss: 79.28287506103516, Learning Rate: 0.01\n",
      "Epoch [5019/10000], Loss: 79.24462890625, Learning Rate: 0.01\n",
      "Epoch [5020/10000], Loss: 79.21636199951172, Learning Rate: 0.01\n",
      "Epoch [5021/10000], Loss: 79.21041870117188, Learning Rate: 0.01\n",
      "Epoch [5022/10000], Loss: 79.22090911865234, Learning Rate: 0.01\n",
      "Epoch [5023/10000], Loss: 79.23139190673828, Learning Rate: 0.01\n",
      "Epoch [5024/10000], Loss: 79.22860717773438, Learning Rate: 0.01\n",
      "Epoch [5025/10000], Loss: 79.21153259277344, Learning Rate: 0.01\n",
      "Epoch [5026/10000], Loss: 79.18998718261719, Learning Rate: 0.01\n",
      "Epoch [5027/10000], Loss: 79.17508697509766, Learning Rate: 0.01\n",
      "Epoch [5028/10000], Loss: 79.1712875366211, Learning Rate: 0.01\n",
      "Epoch [5029/10000], Loss: 79.17427825927734, Learning Rate: 0.01\n",
      "Epoch [5030/10000], Loss: 79.1759033203125, Learning Rate: 0.01\n",
      "Epoch [5031/10000], Loss: 79.17052459716797, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5032/10000], Loss: 79.15876007080078, Learning Rate: 0.01\n",
      "Epoch [5033/10000], Loss: 79.14553833007812, Learning Rate: 0.01\n",
      "Epoch [5034/10000], Loss: 79.13584899902344, Learning Rate: 0.01\n",
      "Epoch [5035/10000], Loss: 79.13150787353516, Learning Rate: 0.01\n",
      "Epoch [5036/10000], Loss: 79.13025665283203, Learning Rate: 0.01\n",
      "Epoch [5037/10000], Loss: 79.12805938720703, Learning Rate: 0.01\n",
      "Epoch [5038/10000], Loss: 79.12261962890625, Learning Rate: 0.01\n",
      "Epoch [5039/10000], Loss: 79.11407470703125, Learning Rate: 0.01\n",
      "Epoch [5040/10000], Loss: 79.10491180419922, Learning Rate: 0.01\n",
      "Epoch [5041/10000], Loss: 79.09724426269531, Learning Rate: 0.01\n",
      "Epoch [5042/10000], Loss: 79.09203338623047, Learning Rate: 0.01\n",
      "Epoch [5043/10000], Loss: 79.0883560180664, Learning Rate: 0.01\n",
      "Epoch [5044/10000], Loss: 79.0845947265625, Learning Rate: 0.01\n",
      "Epoch [5045/10000], Loss: 79.0794448852539, Learning Rate: 0.01\n",
      "Epoch [5046/10000], Loss: 79.07292175292969, Learning Rate: 0.01\n",
      "Epoch [5047/10000], Loss: 79.06550598144531, Learning Rate: 0.01\n",
      "Epoch [5048/10000], Loss: 79.05875396728516, Learning Rate: 0.01\n",
      "Epoch [5049/10000], Loss: 79.05302429199219, Learning Rate: 0.01\n",
      "Epoch [5050/10000], Loss: 79.04814147949219, Learning Rate: 0.01\n",
      "Epoch [5051/10000], Loss: 79.04356384277344, Learning Rate: 0.01\n",
      "Epoch [5052/10000], Loss: 79.03861236572266, Learning Rate: 0.01\n",
      "Epoch [5053/10000], Loss: 79.03291320800781, Learning Rate: 0.01\n",
      "Epoch [5054/10000], Loss: 79.02666473388672, Learning Rate: 0.01\n",
      "Epoch [5055/10000], Loss: 79.0203857421875, Learning Rate: 0.01\n",
      "Epoch [5056/10000], Loss: 79.01454162597656, Learning Rate: 0.01\n",
      "Epoch [5057/10000], Loss: 79.00899505615234, Learning Rate: 0.01\n",
      "Epoch [5058/10000], Loss: 79.00386047363281, Learning Rate: 0.01\n",
      "Epoch [5059/10000], Loss: 78.9988021850586, Learning Rate: 0.01\n",
      "Epoch [5060/10000], Loss: 78.99353790283203, Learning Rate: 0.01\n",
      "Epoch [5061/10000], Loss: 78.98776245117188, Learning Rate: 0.01\n",
      "Epoch [5062/10000], Loss: 78.98208618164062, Learning Rate: 0.01\n",
      "Epoch [5063/10000], Loss: 78.97616577148438, Learning Rate: 0.01\n",
      "Epoch [5064/10000], Loss: 78.97057342529297, Learning Rate: 0.01\n",
      "Epoch [5065/10000], Loss: 78.9650650024414, Learning Rate: 0.01\n",
      "Epoch [5066/10000], Loss: 78.95980072021484, Learning Rate: 0.01\n",
      "Epoch [5067/10000], Loss: 78.9544448852539, Learning Rate: 0.01\n",
      "Epoch [5068/10000], Loss: 78.94901275634766, Learning Rate: 0.01\n",
      "Epoch [5069/10000], Loss: 78.94352722167969, Learning Rate: 0.01\n",
      "Epoch [5070/10000], Loss: 78.93795776367188, Learning Rate: 0.01\n",
      "Epoch [5071/10000], Loss: 78.9323501586914, Learning Rate: 0.01\n",
      "Epoch [5072/10000], Loss: 78.92674255371094, Learning Rate: 0.01\n",
      "Epoch [5073/10000], Loss: 78.9212417602539, Learning Rate: 0.01\n",
      "Epoch [5074/10000], Loss: 78.91580200195312, Learning Rate: 0.01\n",
      "Epoch [5075/10000], Loss: 78.91043090820312, Learning Rate: 0.01\n",
      "Epoch [5076/10000], Loss: 78.90501403808594, Learning Rate: 0.01\n",
      "Epoch [5077/10000], Loss: 78.89958953857422, Learning Rate: 0.01\n",
      "Epoch [5078/10000], Loss: 78.89411926269531, Learning Rate: 0.01\n",
      "Epoch [5079/10000], Loss: 78.88858032226562, Learning Rate: 0.01\n",
      "Epoch [5080/10000], Loss: 78.88304901123047, Learning Rate: 0.01\n",
      "Epoch [5081/10000], Loss: 78.87764739990234, Learning Rate: 0.01\n",
      "Epoch [5082/10000], Loss: 78.87211608886719, Learning Rate: 0.01\n",
      "Epoch [5083/10000], Loss: 78.86664581298828, Learning Rate: 0.01\n",
      "Epoch [5084/10000], Loss: 78.86116790771484, Learning Rate: 0.01\n",
      "Epoch [5085/10000], Loss: 78.85579681396484, Learning Rate: 0.01\n",
      "Epoch [5086/10000], Loss: 78.850341796875, Learning Rate: 0.01\n",
      "Epoch [5087/10000], Loss: 78.8449478149414, Learning Rate: 0.01\n",
      "Epoch [5088/10000], Loss: 78.8394546508789, Learning Rate: 0.01\n",
      "Epoch [5089/10000], Loss: 78.8340072631836, Learning Rate: 0.01\n",
      "Epoch [5090/10000], Loss: 78.82850646972656, Learning Rate: 0.01\n",
      "Epoch [5091/10000], Loss: 78.82310485839844, Learning Rate: 0.01\n",
      "Epoch [5092/10000], Loss: 78.81756591796875, Learning Rate: 0.01\n",
      "Epoch [5093/10000], Loss: 78.81217956542969, Learning Rate: 0.01\n",
      "Epoch [5094/10000], Loss: 78.80670166015625, Learning Rate: 0.01\n",
      "Epoch [5095/10000], Loss: 78.80138397216797, Learning Rate: 0.01\n",
      "Epoch [5096/10000], Loss: 78.79588317871094, Learning Rate: 0.01\n",
      "Epoch [5097/10000], Loss: 78.79045867919922, Learning Rate: 0.01\n",
      "Epoch [5098/10000], Loss: 78.7850341796875, Learning Rate: 0.01\n",
      "Epoch [5099/10000], Loss: 78.779541015625, Learning Rate: 0.01\n",
      "Epoch [5100/10000], Loss: 78.774169921875, Learning Rate: 0.01\n",
      "Epoch [5101/10000], Loss: 78.76871490478516, Learning Rate: 0.01\n",
      "Epoch [5102/10000], Loss: 78.76334381103516, Learning Rate: 0.01\n",
      "Epoch [5103/10000], Loss: 78.75790405273438, Learning Rate: 0.01\n",
      "Epoch [5104/10000], Loss: 78.75247955322266, Learning Rate: 0.01\n",
      "Epoch [5105/10000], Loss: 78.74706268310547, Learning Rate: 0.01\n",
      "Epoch [5106/10000], Loss: 78.7416000366211, Learning Rate: 0.01\n",
      "Epoch [5107/10000], Loss: 78.73617553710938, Learning Rate: 0.01\n",
      "Epoch [5108/10000], Loss: 78.73078918457031, Learning Rate: 0.01\n",
      "Epoch [5109/10000], Loss: 78.72540283203125, Learning Rate: 0.01\n",
      "Epoch [5110/10000], Loss: 78.71995544433594, Learning Rate: 0.01\n",
      "Epoch [5111/10000], Loss: 78.7145004272461, Learning Rate: 0.01\n",
      "Epoch [5112/10000], Loss: 78.70916748046875, Learning Rate: 0.01\n",
      "Epoch [5113/10000], Loss: 78.70372772216797, Learning Rate: 0.01\n",
      "Epoch [5114/10000], Loss: 78.69831848144531, Learning Rate: 0.01\n",
      "Epoch [5115/10000], Loss: 78.69291687011719, Learning Rate: 0.01\n",
      "Epoch [5116/10000], Loss: 78.68748474121094, Learning Rate: 0.01\n",
      "Epoch [5117/10000], Loss: 78.68206024169922, Learning Rate: 0.01\n",
      "Epoch [5118/10000], Loss: 78.67668914794922, Learning Rate: 0.01\n",
      "Epoch [5119/10000], Loss: 78.67129516601562, Learning Rate: 0.01\n",
      "Epoch [5120/10000], Loss: 78.66588592529297, Learning Rate: 0.01\n",
      "Epoch [5121/10000], Loss: 78.66050720214844, Learning Rate: 0.01\n",
      "Epoch [5122/10000], Loss: 78.65511322021484, Learning Rate: 0.01\n",
      "Epoch [5123/10000], Loss: 78.64971160888672, Learning Rate: 0.01\n",
      "Epoch [5124/10000], Loss: 78.64429473876953, Learning Rate: 0.01\n",
      "Epoch [5125/10000], Loss: 78.63900756835938, Learning Rate: 0.01\n",
      "Epoch [5126/10000], Loss: 78.6335678100586, Learning Rate: 0.01\n",
      "Epoch [5127/10000], Loss: 78.62810516357422, Learning Rate: 0.01\n",
      "Epoch [5128/10000], Loss: 78.62274169921875, Learning Rate: 0.01\n",
      "Epoch [5129/10000], Loss: 78.61737060546875, Learning Rate: 0.01\n",
      "Epoch [5130/10000], Loss: 78.61197662353516, Learning Rate: 0.01\n",
      "Epoch [5131/10000], Loss: 78.60664367675781, Learning Rate: 0.01\n",
      "Epoch [5132/10000], Loss: 78.60122680664062, Learning Rate: 0.01\n",
      "Epoch [5133/10000], Loss: 78.5958251953125, Learning Rate: 0.01\n",
      "Epoch [5134/10000], Loss: 78.59044647216797, Learning Rate: 0.01\n",
      "Epoch [5135/10000], Loss: 78.58506774902344, Learning Rate: 0.01\n",
      "Epoch [5136/10000], Loss: 78.57967376708984, Learning Rate: 0.01\n",
      "Epoch [5137/10000], Loss: 78.57439422607422, Learning Rate: 0.01\n",
      "Epoch [5138/10000], Loss: 78.5689697265625, Learning Rate: 0.01\n",
      "Epoch [5139/10000], Loss: 78.56356811523438, Learning Rate: 0.01\n",
      "Epoch [5140/10000], Loss: 78.5582275390625, Learning Rate: 0.01\n",
      "Epoch [5141/10000], Loss: 78.55282592773438, Learning Rate: 0.01\n",
      "Epoch [5142/10000], Loss: 78.54753875732422, Learning Rate: 0.01\n",
      "Epoch [5143/10000], Loss: 78.54212188720703, Learning Rate: 0.01\n",
      "Epoch [5144/10000], Loss: 78.53667449951172, Learning Rate: 0.01\n",
      "Epoch [5145/10000], Loss: 78.53140258789062, Learning Rate: 0.01\n",
      "Epoch [5146/10000], Loss: 78.52599334716797, Learning Rate: 0.01\n",
      "Epoch [5147/10000], Loss: 78.52067565917969, Learning Rate: 0.01\n",
      "Epoch [5148/10000], Loss: 78.5152816772461, Learning Rate: 0.01\n",
      "Epoch [5149/10000], Loss: 78.50993347167969, Learning Rate: 0.01\n",
      "Epoch [5150/10000], Loss: 78.5045394897461, Learning Rate: 0.01\n",
      "Epoch [5151/10000], Loss: 78.49922180175781, Learning Rate: 0.01\n",
      "Epoch [5152/10000], Loss: 78.493896484375, Learning Rate: 0.01\n",
      "Epoch [5153/10000], Loss: 78.48855590820312, Learning Rate: 0.01\n",
      "Epoch [5154/10000], Loss: 78.48328399658203, Learning Rate: 0.01\n",
      "Epoch [5155/10000], Loss: 78.47781372070312, Learning Rate: 0.01\n",
      "Epoch [5156/10000], Loss: 78.4725570678711, Learning Rate: 0.01\n",
      "Epoch [5157/10000], Loss: 78.46714782714844, Learning Rate: 0.01\n",
      "Epoch [5158/10000], Loss: 78.46172332763672, Learning Rate: 0.01\n",
      "Epoch [5159/10000], Loss: 78.45646667480469, Learning Rate: 0.01\n",
      "Epoch [5160/10000], Loss: 78.45112609863281, Learning Rate: 0.01\n",
      "Epoch [5161/10000], Loss: 78.44579315185547, Learning Rate: 0.01\n",
      "Epoch [5162/10000], Loss: 78.44047546386719, Learning Rate: 0.01\n",
      "Epoch [5163/10000], Loss: 78.43506622314453, Learning Rate: 0.01\n",
      "Epoch [5164/10000], Loss: 78.42977142333984, Learning Rate: 0.01\n",
      "Epoch [5165/10000], Loss: 78.42449951171875, Learning Rate: 0.01\n",
      "Epoch [5166/10000], Loss: 78.41908264160156, Learning Rate: 0.01\n",
      "Epoch [5167/10000], Loss: 78.41383361816406, Learning Rate: 0.01\n",
      "Epoch [5168/10000], Loss: 78.4084243774414, Learning Rate: 0.01\n",
      "Epoch [5169/10000], Loss: 78.403076171875, Learning Rate: 0.01\n",
      "Epoch [5170/10000], Loss: 78.3978271484375, Learning Rate: 0.01\n",
      "Epoch [5171/10000], Loss: 78.39247131347656, Learning Rate: 0.01\n",
      "Epoch [5172/10000], Loss: 78.38722229003906, Learning Rate: 0.01\n",
      "Epoch [5173/10000], Loss: 78.3819808959961, Learning Rate: 0.01\n",
      "Epoch [5174/10000], Loss: 78.37667846679688, Learning Rate: 0.01\n",
      "Epoch [5175/10000], Loss: 78.37151336669922, Learning Rate: 0.01\n",
      "Epoch [5176/10000], Loss: 78.36641693115234, Learning Rate: 0.01\n",
      "Epoch [5177/10000], Loss: 78.36145782470703, Learning Rate: 0.01\n",
      "Epoch [5178/10000], Loss: 78.35662078857422, Learning Rate: 0.01\n",
      "Epoch [5179/10000], Loss: 78.35226440429688, Learning Rate: 0.01\n",
      "Epoch [5180/10000], Loss: 78.34846496582031, Learning Rate: 0.01\n",
      "Epoch [5181/10000], Loss: 78.34565734863281, Learning Rate: 0.01\n",
      "Epoch [5182/10000], Loss: 78.3446273803711, Learning Rate: 0.01\n",
      "Epoch [5183/10000], Loss: 78.346435546875, Learning Rate: 0.01\n",
      "Epoch [5184/10000], Loss: 78.35310363769531, Learning Rate: 0.01\n",
      "Epoch [5185/10000], Loss: 78.36800384521484, Learning Rate: 0.01\n",
      "Epoch [5186/10000], Loss: 78.39645385742188, Learning Rate: 0.01\n",
      "Epoch [5187/10000], Loss: 78.4469223022461, Learning Rate: 0.01\n",
      "Epoch [5188/10000], Loss: 78.5306396484375, Learning Rate: 0.01\n",
      "Epoch [5189/10000], Loss: 78.65997314453125, Learning Rate: 0.01\n",
      "Epoch [5190/10000], Loss: 78.83829498291016, Learning Rate: 0.01\n",
      "Epoch [5191/10000], Loss: 79.04183197021484, Learning Rate: 0.01\n",
      "Epoch [5192/10000], Loss: 79.19490814208984, Learning Rate: 0.01\n",
      "Epoch [5193/10000], Loss: 79.18815612792969, Learning Rate: 0.01\n",
      "Epoch [5194/10000], Loss: 78.9594497680664, Learning Rate: 0.01\n",
      "Epoch [5195/10000], Loss: 78.60081481933594, Learning Rate: 0.01\n",
      "Epoch [5196/10000], Loss: 78.31974792480469, Learning Rate: 0.01\n",
      "Epoch [5197/10000], Loss: 78.26553344726562, Learning Rate: 0.01\n",
      "Epoch [5198/10000], Loss: 78.40826416015625, Learning Rate: 0.01\n",
      "Epoch [5199/10000], Loss: 78.5847396850586, Learning Rate: 0.01\n",
      "Epoch [5200/10000], Loss: 78.63429260253906, Learning Rate: 0.01\n",
      "Epoch [5201/10000], Loss: 78.51496124267578, Learning Rate: 0.01\n",
      "Epoch [5202/10000], Loss: 78.33007049560547, Learning Rate: 0.01\n",
      "Epoch [5203/10000], Loss: 78.22594451904297, Learning Rate: 0.01\n",
      "Epoch [5204/10000], Loss: 78.25779724121094, Learning Rate: 0.01\n",
      "Epoch [5205/10000], Loss: 78.35665130615234, Learning Rate: 0.01\n",
      "Epoch [5206/10000], Loss: 78.41001892089844, Learning Rate: 0.01\n",
      "Epoch [5207/10000], Loss: 78.3652114868164, Learning Rate: 0.01\n",
      "Epoch [5208/10000], Loss: 78.2652359008789, Learning Rate: 0.01\n",
      "Epoch [5209/10000], Loss: 78.19624328613281, Learning Rate: 0.01\n",
      "Epoch [5210/10000], Loss: 78.2017822265625, Learning Rate: 0.01\n",
      "Epoch [5211/10000], Loss: 78.25170135498047, Learning Rate: 0.01\n",
      "Epoch [5212/10000], Loss: 78.28264617919922, Learning Rate: 0.01\n",
      "Epoch [5213/10000], Loss: 78.26045227050781, Learning Rate: 0.01\n",
      "Epoch [5214/10000], Loss: 78.20516967773438, Learning Rate: 0.01\n",
      "Epoch [5215/10000], Loss: 78.16373443603516, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5216/10000], Loss: 78.16190338134766, Learning Rate: 0.01\n",
      "Epoch [5217/10000], Loss: 78.1857681274414, Learning Rate: 0.01\n",
      "Epoch [5218/10000], Loss: 78.2012939453125, Learning Rate: 0.01\n",
      "Epoch [5219/10000], Loss: 78.18881225585938, Learning Rate: 0.01\n",
      "Epoch [5220/10000], Loss: 78.15752410888672, Learning Rate: 0.01\n",
      "Epoch [5221/10000], Loss: 78.13167572021484, Learning Rate: 0.01\n",
      "Epoch [5222/10000], Loss: 78.12645721435547, Learning Rate: 0.01\n",
      "Epoch [5223/10000], Loss: 78.13602447509766, Learning Rate: 0.01\n",
      "Epoch [5224/10000], Loss: 78.14305877685547, Learning Rate: 0.01\n",
      "Epoch [5225/10000], Loss: 78.13583374023438, Learning Rate: 0.01\n",
      "Epoch [5226/10000], Loss: 78.11743927001953, Learning Rate: 0.01\n",
      "Epoch [5227/10000], Loss: 78.1002197265625, Learning Rate: 0.01\n",
      "Epoch [5228/10000], Loss: 78.0931396484375, Learning Rate: 0.01\n",
      "Epoch [5229/10000], Loss: 78.09489440917969, Learning Rate: 0.01\n",
      "Epoch [5230/10000], Loss: 78.09706115722656, Learning Rate: 0.01\n",
      "Epoch [5231/10000], Loss: 78.09249877929688, Learning Rate: 0.01\n",
      "Epoch [5232/10000], Loss: 78.08135986328125, Learning Rate: 0.01\n",
      "Epoch [5233/10000], Loss: 78.06903839111328, Learning Rate: 0.01\n",
      "Epoch [5234/10000], Loss: 78.06123352050781, Learning Rate: 0.01\n",
      "Epoch [5235/10000], Loss: 78.0587387084961, Learning Rate: 0.01\n",
      "Epoch [5236/10000], Loss: 78.05778503417969, Learning Rate: 0.01\n",
      "Epoch [5237/10000], Loss: 78.05426788330078, Learning Rate: 0.01\n",
      "Epoch [5238/10000], Loss: 78.04696655273438, Learning Rate: 0.01\n",
      "Epoch [5239/10000], Loss: 78.03794860839844, Learning Rate: 0.01\n",
      "Epoch [5240/10000], Loss: 78.03020477294922, Learning Rate: 0.01\n",
      "Epoch [5241/10000], Loss: 78.02536010742188, Learning Rate: 0.01\n",
      "Epoch [5242/10000], Loss: 78.02228546142578, Learning Rate: 0.01\n",
      "Epoch [5243/10000], Loss: 78.01873779296875, Learning Rate: 0.01\n",
      "Epoch [5244/10000], Loss: 78.01339721679688, Learning Rate: 0.01\n",
      "Epoch [5245/10000], Loss: 78.0065689086914, Learning Rate: 0.01\n",
      "Epoch [5246/10000], Loss: 77.99954986572266, Learning Rate: 0.01\n",
      "Epoch [5247/10000], Loss: 77.99378967285156, Learning Rate: 0.01\n",
      "Epoch [5248/10000], Loss: 77.98912048339844, Learning Rate: 0.01\n",
      "Epoch [5249/10000], Loss: 77.98503875732422, Learning Rate: 0.01\n",
      "Epoch [5250/10000], Loss: 77.9804458618164, Learning Rate: 0.01\n",
      "Epoch [5251/10000], Loss: 77.97489929199219, Learning Rate: 0.01\n",
      "Epoch [5252/10000], Loss: 77.96878051757812, Learning Rate: 0.01\n",
      "Epoch [5253/10000], Loss: 77.96281433105469, Learning Rate: 0.01\n",
      "Epoch [5254/10000], Loss: 77.95744323730469, Learning Rate: 0.01\n",
      "Epoch [5255/10000], Loss: 77.95262908935547, Learning Rate: 0.01\n",
      "Epoch [5256/10000], Loss: 77.94794464111328, Learning Rate: 0.01\n",
      "Epoch [5257/10000], Loss: 77.9430160522461, Learning Rate: 0.01\n",
      "Epoch [5258/10000], Loss: 77.9377670288086, Learning Rate: 0.01\n",
      "Epoch [5259/10000], Loss: 77.9320297241211, Learning Rate: 0.01\n",
      "Epoch [5260/10000], Loss: 77.92646026611328, Learning Rate: 0.01\n",
      "Epoch [5261/10000], Loss: 77.92121887207031, Learning Rate: 0.01\n",
      "Epoch [5262/10000], Loss: 77.91622161865234, Learning Rate: 0.01\n",
      "Epoch [5263/10000], Loss: 77.91124725341797, Learning Rate: 0.01\n",
      "Epoch [5264/10000], Loss: 77.90621948242188, Learning Rate: 0.01\n",
      "Epoch [5265/10000], Loss: 77.90098571777344, Learning Rate: 0.01\n",
      "Epoch [5266/10000], Loss: 77.89566040039062, Learning Rate: 0.01\n",
      "Epoch [5267/10000], Loss: 77.89031982421875, Learning Rate: 0.01\n",
      "Epoch [5268/10000], Loss: 77.88509368896484, Learning Rate: 0.01\n",
      "Epoch [5269/10000], Loss: 77.87992095947266, Learning Rate: 0.01\n",
      "Epoch [5270/10000], Loss: 77.87487030029297, Learning Rate: 0.01\n",
      "Epoch [5271/10000], Loss: 77.86986541748047, Learning Rate: 0.01\n",
      "Epoch [5272/10000], Loss: 77.86473846435547, Learning Rate: 0.01\n",
      "Epoch [5273/10000], Loss: 77.8594741821289, Learning Rate: 0.01\n",
      "Epoch [5274/10000], Loss: 77.85419464111328, Learning Rate: 0.01\n",
      "Epoch [5275/10000], Loss: 77.84894561767578, Learning Rate: 0.01\n",
      "Epoch [5276/10000], Loss: 77.84381866455078, Learning Rate: 0.01\n",
      "Epoch [5277/10000], Loss: 77.83879089355469, Learning Rate: 0.01\n",
      "Epoch [5278/10000], Loss: 77.83367919921875, Learning Rate: 0.01\n",
      "Epoch [5279/10000], Loss: 77.8284912109375, Learning Rate: 0.01\n",
      "Epoch [5280/10000], Loss: 77.82333374023438, Learning Rate: 0.01\n",
      "Epoch [5281/10000], Loss: 77.81819915771484, Learning Rate: 0.01\n",
      "Epoch [5282/10000], Loss: 77.81300354003906, Learning Rate: 0.01\n",
      "Epoch [5283/10000], Loss: 77.80775451660156, Learning Rate: 0.01\n",
      "Epoch [5284/10000], Loss: 77.80261993408203, Learning Rate: 0.01\n",
      "Epoch [5285/10000], Loss: 77.79755401611328, Learning Rate: 0.01\n",
      "Epoch [5286/10000], Loss: 77.79238891601562, Learning Rate: 0.01\n",
      "Epoch [5287/10000], Loss: 77.78727722167969, Learning Rate: 0.01\n",
      "Epoch [5288/10000], Loss: 77.78224182128906, Learning Rate: 0.01\n",
      "Epoch [5289/10000], Loss: 77.77698516845703, Learning Rate: 0.01\n",
      "Epoch [5290/10000], Loss: 77.77183532714844, Learning Rate: 0.01\n",
      "Epoch [5291/10000], Loss: 77.76670837402344, Learning Rate: 0.01\n",
      "Epoch [5292/10000], Loss: 77.76153564453125, Learning Rate: 0.01\n",
      "Epoch [5293/10000], Loss: 77.75643920898438, Learning Rate: 0.01\n",
      "Epoch [5294/10000], Loss: 77.7513198852539, Learning Rate: 0.01\n",
      "Epoch [5295/10000], Loss: 77.74615478515625, Learning Rate: 0.01\n",
      "Epoch [5296/10000], Loss: 77.74109649658203, Learning Rate: 0.01\n",
      "Epoch [5297/10000], Loss: 77.73594665527344, Learning Rate: 0.01\n",
      "Epoch [5298/10000], Loss: 77.73080444335938, Learning Rate: 0.01\n",
      "Epoch [5299/10000], Loss: 77.72571563720703, Learning Rate: 0.01\n",
      "Epoch [5300/10000], Loss: 77.72053527832031, Learning Rate: 0.01\n",
      "Epoch [5301/10000], Loss: 77.71541595458984, Learning Rate: 0.01\n",
      "Epoch [5302/10000], Loss: 77.71029663085938, Learning Rate: 0.01\n",
      "Epoch [5303/10000], Loss: 77.7051773071289, Learning Rate: 0.01\n",
      "Epoch [5304/10000], Loss: 77.70011901855469, Learning Rate: 0.01\n",
      "Epoch [5305/10000], Loss: 77.6949462890625, Learning Rate: 0.01\n",
      "Epoch [5306/10000], Loss: 77.68984985351562, Learning Rate: 0.01\n",
      "Epoch [5307/10000], Loss: 77.68475341796875, Learning Rate: 0.01\n",
      "Epoch [5308/10000], Loss: 77.67962646484375, Learning Rate: 0.01\n",
      "Epoch [5309/10000], Loss: 77.6745376586914, Learning Rate: 0.01\n",
      "Epoch [5310/10000], Loss: 77.66938018798828, Learning Rate: 0.01\n",
      "Epoch [5311/10000], Loss: 77.66429138183594, Learning Rate: 0.01\n",
      "Epoch [5312/10000], Loss: 77.65919494628906, Learning Rate: 0.01\n",
      "Epoch [5313/10000], Loss: 77.65404510498047, Learning Rate: 0.01\n",
      "Epoch [5314/10000], Loss: 77.6489028930664, Learning Rate: 0.01\n",
      "Epoch [5315/10000], Loss: 77.64389038085938, Learning Rate: 0.01\n",
      "Epoch [5316/10000], Loss: 77.63880157470703, Learning Rate: 0.01\n",
      "Epoch [5317/10000], Loss: 77.63362884521484, Learning Rate: 0.01\n",
      "Epoch [5318/10000], Loss: 77.6285171508789, Learning Rate: 0.01\n",
      "Epoch [5319/10000], Loss: 77.62352752685547, Learning Rate: 0.01\n",
      "Epoch [5320/10000], Loss: 77.61830139160156, Learning Rate: 0.01\n",
      "Epoch [5321/10000], Loss: 77.61324310302734, Learning Rate: 0.01\n",
      "Epoch [5322/10000], Loss: 77.60820007324219, Learning Rate: 0.01\n",
      "Epoch [5323/10000], Loss: 77.60310363769531, Learning Rate: 0.01\n",
      "Epoch [5324/10000], Loss: 77.59795379638672, Learning Rate: 0.01\n",
      "Epoch [5325/10000], Loss: 77.59290313720703, Learning Rate: 0.01\n",
      "Epoch [5326/10000], Loss: 77.58778381347656, Learning Rate: 0.01\n",
      "Epoch [5327/10000], Loss: 77.58268737792969, Learning Rate: 0.01\n",
      "Epoch [5328/10000], Loss: 77.57764434814453, Learning Rate: 0.01\n",
      "Epoch [5329/10000], Loss: 77.57246398925781, Learning Rate: 0.01\n",
      "Epoch [5330/10000], Loss: 77.56742095947266, Learning Rate: 0.01\n",
      "Epoch [5331/10000], Loss: 77.56232452392578, Learning Rate: 0.01\n",
      "Epoch [5332/10000], Loss: 77.55728149414062, Learning Rate: 0.01\n",
      "Epoch [5333/10000], Loss: 77.5521240234375, Learning Rate: 0.01\n",
      "Epoch [5334/10000], Loss: 77.54708862304688, Learning Rate: 0.01\n",
      "Epoch [5335/10000], Loss: 77.54200744628906, Learning Rate: 0.01\n",
      "Epoch [5336/10000], Loss: 77.5368881225586, Learning Rate: 0.01\n",
      "Epoch [5337/10000], Loss: 77.53181457519531, Learning Rate: 0.01\n",
      "Epoch [5338/10000], Loss: 77.52668762207031, Learning Rate: 0.01\n",
      "Epoch [5339/10000], Loss: 77.52176666259766, Learning Rate: 0.01\n",
      "Epoch [5340/10000], Loss: 77.5165786743164, Learning Rate: 0.01\n",
      "Epoch [5341/10000], Loss: 77.51155090332031, Learning Rate: 0.01\n",
      "Epoch [5342/10000], Loss: 77.50653839111328, Learning Rate: 0.01\n",
      "Epoch [5343/10000], Loss: 77.50135803222656, Learning Rate: 0.01\n",
      "Epoch [5344/10000], Loss: 77.496337890625, Learning Rate: 0.01\n",
      "Epoch [5345/10000], Loss: 77.4912338256836, Learning Rate: 0.01\n",
      "Epoch [5346/10000], Loss: 77.48614501953125, Learning Rate: 0.01\n",
      "Epoch [5347/10000], Loss: 77.4811019897461, Learning Rate: 0.01\n",
      "Epoch [5348/10000], Loss: 77.47601318359375, Learning Rate: 0.01\n",
      "Epoch [5349/10000], Loss: 77.47090911865234, Learning Rate: 0.01\n",
      "Epoch [5350/10000], Loss: 77.46590423583984, Learning Rate: 0.01\n",
      "Epoch [5351/10000], Loss: 77.46086883544922, Learning Rate: 0.01\n",
      "Epoch [5352/10000], Loss: 77.45581817626953, Learning Rate: 0.01\n",
      "Epoch [5353/10000], Loss: 77.45069122314453, Learning Rate: 0.01\n",
      "Epoch [5354/10000], Loss: 77.44563293457031, Learning Rate: 0.01\n",
      "Epoch [5355/10000], Loss: 77.44066619873047, Learning Rate: 0.01\n",
      "Epoch [5356/10000], Loss: 77.43550872802734, Learning Rate: 0.01\n",
      "Epoch [5357/10000], Loss: 77.43050384521484, Learning Rate: 0.01\n",
      "Epoch [5358/10000], Loss: 77.4254150390625, Learning Rate: 0.01\n",
      "Epoch [5359/10000], Loss: 77.42044830322266, Learning Rate: 0.01\n",
      "Epoch [5360/10000], Loss: 77.4153823852539, Learning Rate: 0.01\n",
      "Epoch [5361/10000], Loss: 77.41033935546875, Learning Rate: 0.01\n",
      "Epoch [5362/10000], Loss: 77.40523529052734, Learning Rate: 0.01\n",
      "Epoch [5363/10000], Loss: 77.40019226074219, Learning Rate: 0.01\n",
      "Epoch [5364/10000], Loss: 77.39514923095703, Learning Rate: 0.01\n",
      "Epoch [5365/10000], Loss: 77.39006042480469, Learning Rate: 0.01\n",
      "Epoch [5366/10000], Loss: 77.38500213623047, Learning Rate: 0.01\n",
      "Epoch [5367/10000], Loss: 77.3800048828125, Learning Rate: 0.01\n",
      "Epoch [5368/10000], Loss: 77.3749771118164, Learning Rate: 0.01\n",
      "Epoch [5369/10000], Loss: 77.36994171142578, Learning Rate: 0.01\n",
      "Epoch [5370/10000], Loss: 77.36485290527344, Learning Rate: 0.01\n",
      "Epoch [5371/10000], Loss: 77.3598403930664, Learning Rate: 0.01\n",
      "Epoch [5372/10000], Loss: 77.3548583984375, Learning Rate: 0.01\n",
      "Epoch [5373/10000], Loss: 77.3498306274414, Learning Rate: 0.01\n",
      "Epoch [5374/10000], Loss: 77.34474182128906, Learning Rate: 0.01\n",
      "Epoch [5375/10000], Loss: 77.3397216796875, Learning Rate: 0.01\n",
      "Epoch [5376/10000], Loss: 77.33467102050781, Learning Rate: 0.01\n",
      "Epoch [5377/10000], Loss: 77.32959747314453, Learning Rate: 0.01\n",
      "Epoch [5378/10000], Loss: 77.32462310791016, Learning Rate: 0.01\n",
      "Epoch [5379/10000], Loss: 77.31950378417969, Learning Rate: 0.01\n",
      "Epoch [5380/10000], Loss: 77.3145523071289, Learning Rate: 0.01\n",
      "Epoch [5381/10000], Loss: 77.30949401855469, Learning Rate: 0.01\n",
      "Epoch [5382/10000], Loss: 77.3044662475586, Learning Rate: 0.01\n",
      "Epoch [5383/10000], Loss: 77.29949188232422, Learning Rate: 0.01\n",
      "Epoch [5384/10000], Loss: 77.29450988769531, Learning Rate: 0.01\n",
      "Epoch [5385/10000], Loss: 77.28948211669922, Learning Rate: 0.01\n",
      "Epoch [5386/10000], Loss: 77.28445434570312, Learning Rate: 0.01\n",
      "Epoch [5387/10000], Loss: 77.27943420410156, Learning Rate: 0.01\n",
      "Epoch [5388/10000], Loss: 77.27442932128906, Learning Rate: 0.01\n",
      "Epoch [5389/10000], Loss: 77.26950073242188, Learning Rate: 0.01\n",
      "Epoch [5390/10000], Loss: 77.26451110839844, Learning Rate: 0.01\n",
      "Epoch [5391/10000], Loss: 77.25958251953125, Learning Rate: 0.01\n",
      "Epoch [5392/10000], Loss: 77.2546615600586, Learning Rate: 0.01\n",
      "Epoch [5393/10000], Loss: 77.24990844726562, Learning Rate: 0.01\n",
      "Epoch [5394/10000], Loss: 77.24528503417969, Learning Rate: 0.01\n",
      "Epoch [5395/10000], Loss: 77.24085235595703, Learning Rate: 0.01\n",
      "Epoch [5396/10000], Loss: 77.23674774169922, Learning Rate: 0.01\n",
      "Epoch [5397/10000], Loss: 77.23324584960938, Learning Rate: 0.01\n",
      "Epoch [5398/10000], Loss: 77.23050689697266, Learning Rate: 0.01\n",
      "Epoch [5399/10000], Loss: 77.22929382324219, Learning Rate: 0.01\n",
      "Epoch [5400/10000], Loss: 77.23038482666016, Learning Rate: 0.01\n",
      "Epoch [5401/10000], Loss: 77.23541259765625, Learning Rate: 0.01\n",
      "Epoch [5402/10000], Loss: 77.24666595458984, Learning Rate: 0.01\n",
      "Epoch [5403/10000], Loss: 77.26832580566406, Learning Rate: 0.01\n",
      "Epoch [5404/10000], Loss: 77.3066635131836, Learning Rate: 0.01\n",
      "Epoch [5405/10000], Loss: 77.37162017822266, Learning Rate: 0.01\n",
      "Epoch [5406/10000], Loss: 77.47721862792969, Learning Rate: 0.01\n",
      "Epoch [5407/10000], Loss: 77.64073944091797, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5408/10000], Loss: 77.87549591064453, Learning Rate: 0.01\n",
      "Epoch [5409/10000], Loss: 78.17493438720703, Learning Rate: 0.01\n",
      "Epoch [5410/10000], Loss: 78.48033905029297, Learning Rate: 0.01\n",
      "Epoch [5411/10000], Loss: 78.66551971435547, Learning Rate: 0.01\n",
      "Epoch [5412/10000], Loss: 78.57647705078125, Learning Rate: 0.01\n",
      "Epoch [5413/10000], Loss: 78.17030334472656, Learning Rate: 0.01\n",
      "Epoch [5414/10000], Loss: 77.61744689941406, Learning Rate: 0.01\n",
      "Epoch [5415/10000], Loss: 77.21665954589844, Learning Rate: 0.01\n",
      "Epoch [5416/10000], Loss: 77.15434265136719, Learning Rate: 0.01\n",
      "Epoch [5417/10000], Loss: 77.37259674072266, Learning Rate: 0.01\n",
      "Epoch [5418/10000], Loss: 77.64311981201172, Learning Rate: 0.01\n",
      "Epoch [5419/10000], Loss: 77.74063110351562, Learning Rate: 0.01\n",
      "Epoch [5420/10000], Loss: 77.59282684326172, Learning Rate: 0.01\n",
      "Epoch [5421/10000], Loss: 77.32041931152344, Learning Rate: 0.01\n",
      "Epoch [5422/10000], Loss: 77.12741088867188, Learning Rate: 0.01\n",
      "Epoch [5423/10000], Loss: 77.12535858154297, Learning Rate: 0.01\n",
      "Epoch [5424/10000], Loss: 77.25887298583984, Learning Rate: 0.01\n",
      "Epoch [5425/10000], Loss: 77.3789291381836, Learning Rate: 0.01\n",
      "Epoch [5426/10000], Loss: 77.37520599365234, Learning Rate: 0.01\n",
      "Epoch [5427/10000], Loss: 77.25557708740234, Learning Rate: 0.01\n",
      "Epoch [5428/10000], Loss: 77.12187957763672, Learning Rate: 0.01\n",
      "Epoch [5429/10000], Loss: 77.07047271728516, Learning Rate: 0.01\n",
      "Epoch [5430/10000], Loss: 77.11426544189453, Learning Rate: 0.01\n",
      "Epoch [5431/10000], Loss: 77.1871337890625, Learning Rate: 0.01\n",
      "Epoch [5432/10000], Loss: 77.21314239501953, Learning Rate: 0.01\n",
      "Epoch [5433/10000], Loss: 77.16870880126953, Learning Rate: 0.01\n",
      "Epoch [5434/10000], Loss: 77.09288787841797, Learning Rate: 0.01\n",
      "Epoch [5435/10000], Loss: 77.04367065429688, Learning Rate: 0.01\n",
      "Epoch [5436/10000], Loss: 77.04684448242188, Learning Rate: 0.01\n",
      "Epoch [5437/10000], Loss: 77.0814437866211, Learning Rate: 0.01\n",
      "Epoch [5438/10000], Loss: 77.10567474365234, Learning Rate: 0.01\n",
      "Epoch [5439/10000], Loss: 77.09447479248047, Learning Rate: 0.01\n",
      "Epoch [5440/10000], Loss: 77.05610656738281, Learning Rate: 0.01\n",
      "Epoch [5441/10000], Loss: 77.01964569091797, Learning Rate: 0.01\n",
      "Epoch [5442/10000], Loss: 77.00701904296875, Learning Rate: 0.01\n",
      "Epoch [5443/10000], Loss: 77.01739501953125, Learning Rate: 0.01\n",
      "Epoch [5444/10000], Loss: 77.0322494506836, Learning Rate: 0.01\n",
      "Epoch [5445/10000], Loss: 77.03350067138672, Learning Rate: 0.01\n",
      "Epoch [5446/10000], Loss: 77.01705932617188, Learning Rate: 0.01\n",
      "Epoch [5447/10000], Loss: 76.9937515258789, Learning Rate: 0.01\n",
      "Epoch [5448/10000], Loss: 76.97760009765625, Learning Rate: 0.01\n",
      "Epoch [5449/10000], Loss: 76.974609375, Learning Rate: 0.01\n",
      "Epoch [5450/10000], Loss: 76.97967529296875, Learning Rate: 0.01\n",
      "Epoch [5451/10000], Loss: 76.98275756835938, Learning Rate: 0.01\n",
      "Epoch [5452/10000], Loss: 76.9774169921875, Learning Rate: 0.01\n",
      "Epoch [5453/10000], Loss: 76.96478271484375, Learning Rate: 0.01\n",
      "Epoch [5454/10000], Loss: 76.951171875, Learning Rate: 0.01\n",
      "Epoch [5455/10000], Loss: 76.94253540039062, Learning Rate: 0.01\n",
      "Epoch [5456/10000], Loss: 76.94002532958984, Learning Rate: 0.01\n",
      "Epoch [5457/10000], Loss: 76.94029235839844, Learning Rate: 0.01\n",
      "Epoch [5458/10000], Loss: 76.9386215209961, Learning Rate: 0.01\n",
      "Epoch [5459/10000], Loss: 76.93255615234375, Learning Rate: 0.01\n",
      "Epoch [5460/10000], Loss: 76.92361450195312, Learning Rate: 0.01\n",
      "Epoch [5461/10000], Loss: 76.91458129882812, Learning Rate: 0.01\n",
      "Epoch [5462/10000], Loss: 76.90821838378906, Learning Rate: 0.01\n",
      "Epoch [5463/10000], Loss: 76.9047622680664, Learning Rate: 0.01\n",
      "Epoch [5464/10000], Loss: 76.90228271484375, Learning Rate: 0.01\n",
      "Epoch [5465/10000], Loss: 76.89884948730469, Learning Rate: 0.01\n",
      "Epoch [5466/10000], Loss: 76.89356231689453, Learning Rate: 0.01\n",
      "Epoch [5467/10000], Loss: 76.8867416381836, Learning Rate: 0.01\n",
      "Epoch [5468/10000], Loss: 76.8797836303711, Learning Rate: 0.01\n",
      "Epoch [5469/10000], Loss: 76.87407684326172, Learning Rate: 0.01\n",
      "Epoch [5470/10000], Loss: 76.86959838867188, Learning Rate: 0.01\n",
      "Epoch [5471/10000], Loss: 76.86589050292969, Learning Rate: 0.01\n",
      "Epoch [5472/10000], Loss: 76.86186981201172, Learning Rate: 0.01\n",
      "Epoch [5473/10000], Loss: 76.85698699951172, Learning Rate: 0.01\n",
      "Epoch [5474/10000], Loss: 76.85137176513672, Learning Rate: 0.01\n",
      "Epoch [5475/10000], Loss: 76.84545135498047, Learning Rate: 0.01\n",
      "Epoch [5476/10000], Loss: 76.8399887084961, Learning Rate: 0.01\n",
      "Epoch [5477/10000], Loss: 76.83509063720703, Learning Rate: 0.01\n",
      "Epoch [5478/10000], Loss: 76.83064270019531, Learning Rate: 0.01\n",
      "Epoch [5479/10000], Loss: 76.82627868652344, Learning Rate: 0.01\n",
      "Epoch [5480/10000], Loss: 76.8216552734375, Learning Rate: 0.01\n",
      "Epoch [5481/10000], Loss: 76.81658935546875, Learning Rate: 0.01\n",
      "Epoch [5482/10000], Loss: 76.81138610839844, Learning Rate: 0.01\n",
      "Epoch [5483/10000], Loss: 76.80599212646484, Learning Rate: 0.01\n",
      "Epoch [5484/10000], Loss: 76.8008804321289, Learning Rate: 0.01\n",
      "Epoch [5485/10000], Loss: 76.79601287841797, Learning Rate: 0.01\n",
      "Epoch [5486/10000], Loss: 76.79142761230469, Learning Rate: 0.01\n",
      "Epoch [5487/10000], Loss: 76.78677368164062, Learning Rate: 0.01\n",
      "Epoch [5488/10000], Loss: 76.78199768066406, Learning Rate: 0.01\n",
      "Epoch [5489/10000], Loss: 76.77703094482422, Learning Rate: 0.01\n",
      "Epoch [5490/10000], Loss: 76.77198791503906, Learning Rate: 0.01\n",
      "Epoch [5491/10000], Loss: 76.7669448852539, Learning Rate: 0.01\n",
      "Epoch [5492/10000], Loss: 76.76202392578125, Learning Rate: 0.01\n",
      "Epoch [5493/10000], Loss: 76.75713348388672, Learning Rate: 0.01\n",
      "Epoch [5494/10000], Loss: 76.75228118896484, Learning Rate: 0.01\n",
      "Epoch [5495/10000], Loss: 76.74761199951172, Learning Rate: 0.01\n",
      "Epoch [5496/10000], Loss: 76.74282836914062, Learning Rate: 0.01\n",
      "Epoch [5497/10000], Loss: 76.73789978027344, Learning Rate: 0.01\n",
      "Epoch [5498/10000], Loss: 76.73300170898438, Learning Rate: 0.01\n",
      "Epoch [5499/10000], Loss: 76.72809600830078, Learning Rate: 0.01\n",
      "Epoch [5500/10000], Loss: 76.72312927246094, Learning Rate: 0.01\n",
      "Epoch [5501/10000], Loss: 76.71825408935547, Learning Rate: 0.01\n",
      "Epoch [5502/10000], Loss: 76.71344757080078, Learning Rate: 0.01\n",
      "Epoch [5503/10000], Loss: 76.70861053466797, Learning Rate: 0.01\n",
      "Epoch [5504/10000], Loss: 76.70381927490234, Learning Rate: 0.01\n",
      "Epoch [5505/10000], Loss: 76.69889831542969, Learning Rate: 0.01\n",
      "Epoch [5506/10000], Loss: 76.69414520263672, Learning Rate: 0.01\n",
      "Epoch [5507/10000], Loss: 76.68917846679688, Learning Rate: 0.01\n",
      "Epoch [5508/10000], Loss: 76.68437194824219, Learning Rate: 0.01\n",
      "Epoch [5509/10000], Loss: 76.67948913574219, Learning Rate: 0.01\n",
      "Epoch [5510/10000], Loss: 76.67465209960938, Learning Rate: 0.01\n",
      "Epoch [5511/10000], Loss: 76.6698226928711, Learning Rate: 0.01\n",
      "Epoch [5512/10000], Loss: 76.66493225097656, Learning Rate: 0.01\n",
      "Epoch [5513/10000], Loss: 76.66018676757812, Learning Rate: 0.01\n",
      "Epoch [5514/10000], Loss: 76.65532684326172, Learning Rate: 0.01\n",
      "Epoch [5515/10000], Loss: 76.65047454833984, Learning Rate: 0.01\n",
      "Epoch [5516/10000], Loss: 76.64563751220703, Learning Rate: 0.01\n",
      "Epoch [5517/10000], Loss: 76.64077758789062, Learning Rate: 0.01\n",
      "Epoch [5518/10000], Loss: 76.6358642578125, Learning Rate: 0.01\n",
      "Epoch [5519/10000], Loss: 76.63107299804688, Learning Rate: 0.01\n",
      "Epoch [5520/10000], Loss: 76.62623596191406, Learning Rate: 0.01\n",
      "Epoch [5521/10000], Loss: 76.62141418457031, Learning Rate: 0.01\n",
      "Epoch [5522/10000], Loss: 76.6165771484375, Learning Rate: 0.01\n",
      "Epoch [5523/10000], Loss: 76.61174774169922, Learning Rate: 0.01\n",
      "Epoch [5524/10000], Loss: 76.60693359375, Learning Rate: 0.01\n",
      "Epoch [5525/10000], Loss: 76.60208892822266, Learning Rate: 0.01\n",
      "Epoch [5526/10000], Loss: 76.59725952148438, Learning Rate: 0.01\n",
      "Epoch [5527/10000], Loss: 76.59239196777344, Learning Rate: 0.01\n",
      "Epoch [5528/10000], Loss: 76.587646484375, Learning Rate: 0.01\n",
      "Epoch [5529/10000], Loss: 76.58274841308594, Learning Rate: 0.01\n",
      "Epoch [5530/10000], Loss: 76.57797241210938, Learning Rate: 0.01\n",
      "Epoch [5531/10000], Loss: 76.57306671142578, Learning Rate: 0.01\n",
      "Epoch [5532/10000], Loss: 76.56831359863281, Learning Rate: 0.01\n",
      "Epoch [5533/10000], Loss: 76.5634765625, Learning Rate: 0.01\n",
      "Epoch [5534/10000], Loss: 76.55860900878906, Learning Rate: 0.01\n",
      "Epoch [5535/10000], Loss: 76.55377960205078, Learning Rate: 0.01\n",
      "Epoch [5536/10000], Loss: 76.5489730834961, Learning Rate: 0.01\n",
      "Epoch [5537/10000], Loss: 76.54418182373047, Learning Rate: 0.01\n",
      "Epoch [5538/10000], Loss: 76.53938293457031, Learning Rate: 0.01\n",
      "Epoch [5539/10000], Loss: 76.53453826904297, Learning Rate: 0.01\n",
      "Epoch [5540/10000], Loss: 76.5297622680664, Learning Rate: 0.01\n",
      "Epoch [5541/10000], Loss: 76.52481842041016, Learning Rate: 0.01\n",
      "Epoch [5542/10000], Loss: 76.52005004882812, Learning Rate: 0.01\n",
      "Epoch [5543/10000], Loss: 76.5152587890625, Learning Rate: 0.01\n",
      "Epoch [5544/10000], Loss: 76.510498046875, Learning Rate: 0.01\n",
      "Epoch [5545/10000], Loss: 76.50566864013672, Learning Rate: 0.01\n",
      "Epoch [5546/10000], Loss: 76.50086975097656, Learning Rate: 0.01\n",
      "Epoch [5547/10000], Loss: 76.49596405029297, Learning Rate: 0.01\n",
      "Epoch [5548/10000], Loss: 76.49118041992188, Learning Rate: 0.01\n",
      "Epoch [5549/10000], Loss: 76.48645782470703, Learning Rate: 0.01\n",
      "Epoch [5550/10000], Loss: 76.48162841796875, Learning Rate: 0.01\n",
      "Epoch [5551/10000], Loss: 76.47683715820312, Learning Rate: 0.01\n",
      "Epoch [5552/10000], Loss: 76.47196960449219, Learning Rate: 0.01\n",
      "Epoch [5553/10000], Loss: 76.46712493896484, Learning Rate: 0.01\n",
      "Epoch [5554/10000], Loss: 76.46234130859375, Learning Rate: 0.01\n",
      "Epoch [5555/10000], Loss: 76.45756530761719, Learning Rate: 0.01\n",
      "Epoch [5556/10000], Loss: 76.45278930664062, Learning Rate: 0.01\n",
      "Epoch [5557/10000], Loss: 76.44792938232422, Learning Rate: 0.01\n",
      "Epoch [5558/10000], Loss: 76.44319152832031, Learning Rate: 0.01\n",
      "Epoch [5559/10000], Loss: 76.43838500976562, Learning Rate: 0.01\n",
      "Epoch [5560/10000], Loss: 76.43357849121094, Learning Rate: 0.01\n",
      "Epoch [5561/10000], Loss: 76.42876434326172, Learning Rate: 0.01\n",
      "Epoch [5562/10000], Loss: 76.42395782470703, Learning Rate: 0.01\n",
      "Epoch [5563/10000], Loss: 76.41918182373047, Learning Rate: 0.01\n",
      "Epoch [5564/10000], Loss: 76.41432189941406, Learning Rate: 0.01\n",
      "Epoch [5565/10000], Loss: 76.40962982177734, Learning Rate: 0.01\n",
      "Epoch [5566/10000], Loss: 76.40480041503906, Learning Rate: 0.01\n",
      "Epoch [5567/10000], Loss: 76.39998626708984, Learning Rate: 0.01\n",
      "Epoch [5568/10000], Loss: 76.39523315429688, Learning Rate: 0.01\n",
      "Epoch [5569/10000], Loss: 76.39045715332031, Learning Rate: 0.01\n",
      "Epoch [5570/10000], Loss: 76.38566589355469, Learning Rate: 0.01\n",
      "Epoch [5571/10000], Loss: 76.3808364868164, Learning Rate: 0.01\n",
      "Epoch [5572/10000], Loss: 76.37602996826172, Learning Rate: 0.01\n",
      "Epoch [5573/10000], Loss: 76.37126922607422, Learning Rate: 0.01\n",
      "Epoch [5574/10000], Loss: 76.36650085449219, Learning Rate: 0.01\n",
      "Epoch [5575/10000], Loss: 76.36172485351562, Learning Rate: 0.01\n",
      "Epoch [5576/10000], Loss: 76.3569107055664, Learning Rate: 0.01\n",
      "Epoch [5577/10000], Loss: 76.35218048095703, Learning Rate: 0.01\n",
      "Epoch [5578/10000], Loss: 76.34741973876953, Learning Rate: 0.01\n",
      "Epoch [5579/10000], Loss: 76.34262084960938, Learning Rate: 0.01\n",
      "Epoch [5580/10000], Loss: 76.33775329589844, Learning Rate: 0.01\n",
      "Epoch [5581/10000], Loss: 76.33302307128906, Learning Rate: 0.01\n",
      "Epoch [5582/10000], Loss: 76.3282470703125, Learning Rate: 0.01\n",
      "Epoch [5583/10000], Loss: 76.32343292236328, Learning Rate: 0.01\n",
      "Epoch [5584/10000], Loss: 76.3187255859375, Learning Rate: 0.01\n",
      "Epoch [5585/10000], Loss: 76.31388092041016, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5586/10000], Loss: 76.3091812133789, Learning Rate: 0.01\n",
      "Epoch [5587/10000], Loss: 76.30439758300781, Learning Rate: 0.01\n",
      "Epoch [5588/10000], Loss: 76.29959869384766, Learning Rate: 0.01\n",
      "Epoch [5589/10000], Loss: 76.29488372802734, Learning Rate: 0.01\n",
      "Epoch [5590/10000], Loss: 76.28999328613281, Learning Rate: 0.01\n",
      "Epoch [5591/10000], Loss: 76.28528594970703, Learning Rate: 0.01\n",
      "Epoch [5592/10000], Loss: 76.28050231933594, Learning Rate: 0.01\n",
      "Epoch [5593/10000], Loss: 76.27580261230469, Learning Rate: 0.01\n",
      "Epoch [5594/10000], Loss: 76.27100372314453, Learning Rate: 0.01\n",
      "Epoch [5595/10000], Loss: 76.2662124633789, Learning Rate: 0.01\n",
      "Epoch [5596/10000], Loss: 76.26158142089844, Learning Rate: 0.01\n",
      "Epoch [5597/10000], Loss: 76.2568130493164, Learning Rate: 0.01\n",
      "Epoch [5598/10000], Loss: 76.25218963623047, Learning Rate: 0.01\n",
      "Epoch [5599/10000], Loss: 76.24750518798828, Learning Rate: 0.01\n",
      "Epoch [5600/10000], Loss: 76.24295043945312, Learning Rate: 0.01\n",
      "Epoch [5601/10000], Loss: 76.23855590820312, Learning Rate: 0.01\n",
      "Epoch [5602/10000], Loss: 76.23429870605469, Learning Rate: 0.01\n",
      "Epoch [5603/10000], Loss: 76.23030853271484, Learning Rate: 0.01\n",
      "Epoch [5604/10000], Loss: 76.22676086425781, Learning Rate: 0.01\n",
      "Epoch [5605/10000], Loss: 76.22394561767578, Learning Rate: 0.01\n",
      "Epoch [5606/10000], Loss: 76.22234344482422, Learning Rate: 0.01\n",
      "Epoch [5607/10000], Loss: 76.22272491455078, Learning Rate: 0.01\n",
      "Epoch [5608/10000], Loss: 76.22628021240234, Learning Rate: 0.01\n",
      "Epoch [5609/10000], Loss: 76.23497009277344, Learning Rate: 0.01\n",
      "Epoch [5610/10000], Loss: 76.2524642944336, Learning Rate: 0.01\n",
      "Epoch [5611/10000], Loss: 76.283935546875, Learning Rate: 0.01\n",
      "Epoch [5612/10000], Loss: 76.33876037597656, Learning Rate: 0.01\n",
      "Epoch [5613/10000], Loss: 76.43093872070312, Learning Rate: 0.01\n",
      "Epoch [5614/10000], Loss: 76.58168029785156, Learning Rate: 0.01\n",
      "Epoch [5615/10000], Loss: 76.8199462890625, Learning Rate: 0.01\n",
      "Epoch [5616/10000], Loss: 77.17597961425781, Learning Rate: 0.01\n",
      "Epoch [5617/10000], Loss: 77.6624526977539, Learning Rate: 0.01\n",
      "Epoch [5618/10000], Loss: 78.22838592529297, Learning Rate: 0.01\n",
      "Epoch [5619/10000], Loss: 78.70709991455078, Learning Rate: 0.01\n",
      "Epoch [5620/10000], Loss: 78.82054901123047, Learning Rate: 0.01\n",
      "Epoch [5621/10000], Loss: 78.35466003417969, Learning Rate: 0.01\n",
      "Epoch [5622/10000], Loss: 77.42608642578125, Learning Rate: 0.01\n",
      "Epoch [5623/10000], Loss: 76.52095031738281, Learning Rate: 0.01\n",
      "Epoch [5624/10000], Loss: 76.13086700439453, Learning Rate: 0.01\n",
      "Epoch [5625/10000], Loss: 76.3534927368164, Learning Rate: 0.01\n",
      "Epoch [5626/10000], Loss: 76.86589813232422, Learning Rate: 0.01\n",
      "Epoch [5627/10000], Loss: 77.20709991455078, Learning Rate: 0.01\n",
      "Epoch [5628/10000], Loss: 77.10369873046875, Learning Rate: 0.01\n",
      "Epoch [5629/10000], Loss: 76.6530990600586, Learning Rate: 0.01\n",
      "Epoch [5630/10000], Loss: 76.21923828125, Learning Rate: 0.01\n",
      "Epoch [5631/10000], Loss: 76.10374450683594, Learning Rate: 0.01\n",
      "Epoch [5632/10000], Loss: 76.30242156982422, Learning Rate: 0.01\n",
      "Epoch [5633/10000], Loss: 76.56230163574219, Learning Rate: 0.01\n",
      "Epoch [5634/10000], Loss: 76.62937927246094, Learning Rate: 0.01\n",
      "Epoch [5635/10000], Loss: 76.45352172851562, Learning Rate: 0.01\n",
      "Epoch [5636/10000], Loss: 76.1983871459961, Learning Rate: 0.01\n",
      "Epoch [5637/10000], Loss: 76.0692138671875, Learning Rate: 0.01\n",
      "Epoch [5638/10000], Loss: 76.12908935546875, Learning Rate: 0.01\n",
      "Epoch [5639/10000], Loss: 76.27135467529297, Learning Rate: 0.01\n",
      "Epoch [5640/10000], Loss: 76.34191131591797, Learning Rate: 0.01\n",
      "Epoch [5641/10000], Loss: 76.27684783935547, Learning Rate: 0.01\n",
      "Epoch [5642/10000], Loss: 76.14045715332031, Learning Rate: 0.01\n",
      "Epoch [5643/10000], Loss: 76.04708862304688, Learning Rate: 0.01\n",
      "Epoch [5644/10000], Loss: 76.0541763305664, Learning Rate: 0.01\n",
      "Epoch [5645/10000], Loss: 76.12387084960938, Learning Rate: 0.01\n",
      "Epoch [5646/10000], Loss: 76.17393493652344, Learning Rate: 0.01\n",
      "Epoch [5647/10000], Loss: 76.15486145019531, Learning Rate: 0.01\n",
      "Epoch [5648/10000], Loss: 76.08541870117188, Learning Rate: 0.01\n",
      "Epoch [5649/10000], Loss: 76.02367401123047, Learning Rate: 0.01\n",
      "Epoch [5650/10000], Loss: 76.01054382324219, Learning Rate: 0.01\n",
      "Epoch [5651/10000], Loss: 76.0392837524414, Learning Rate: 0.01\n",
      "Epoch [5652/10000], Loss: 76.0701904296875, Learning Rate: 0.01\n",
      "Epoch [5653/10000], Loss: 76.0699234008789, Learning Rate: 0.01\n",
      "Epoch [5654/10000], Loss: 76.03744506835938, Learning Rate: 0.01\n",
      "Epoch [5655/10000], Loss: 75.99858093261719, Learning Rate: 0.01\n",
      "Epoch [5656/10000], Loss: 75.9794692993164, Learning Rate: 0.01\n",
      "Epoch [5657/10000], Loss: 75.98577880859375, Learning Rate: 0.01\n",
      "Epoch [5658/10000], Loss: 76.00153350830078, Learning Rate: 0.01\n",
      "Epoch [5659/10000], Loss: 76.00689697265625, Learning Rate: 0.01\n",
      "Epoch [5660/10000], Loss: 75.99414825439453, Learning Rate: 0.01\n",
      "Epoch [5661/10000], Loss: 75.97156524658203, Learning Rate: 0.01\n",
      "Epoch [5662/10000], Loss: 75.95355987548828, Learning Rate: 0.01\n",
      "Epoch [5663/10000], Loss: 75.94850158691406, Learning Rate: 0.01\n",
      "Epoch [5664/10000], Loss: 75.95303344726562, Learning Rate: 0.01\n",
      "Epoch [5665/10000], Loss: 75.95726013183594, Learning Rate: 0.01\n",
      "Epoch [5666/10000], Loss: 75.95368194580078, Learning Rate: 0.01\n",
      "Epoch [5667/10000], Loss: 75.94212341308594, Learning Rate: 0.01\n",
      "Epoch [5668/10000], Loss: 75.92860412597656, Learning Rate: 0.01\n",
      "Epoch [5669/10000], Loss: 75.91944885253906, Learning Rate: 0.01\n",
      "Epoch [5670/10000], Loss: 75.91654205322266, Learning Rate: 0.01\n",
      "Epoch [5671/10000], Loss: 75.91693878173828, Learning Rate: 0.01\n",
      "Epoch [5672/10000], Loss: 75.91590881347656, Learning Rate: 0.01\n",
      "Epoch [5673/10000], Loss: 75.91067504882812, Learning Rate: 0.01\n",
      "Epoch [5674/10000], Loss: 75.90204620361328, Learning Rate: 0.01\n",
      "Epoch [5675/10000], Loss: 75.89328002929688, Learning Rate: 0.01\n",
      "Epoch [5676/10000], Loss: 75.8869857788086, Learning Rate: 0.01\n",
      "Epoch [5677/10000], Loss: 75.88347625732422, Learning Rate: 0.01\n",
      "Epoch [5678/10000], Loss: 75.88134765625, Learning Rate: 0.01\n",
      "Epoch [5679/10000], Loss: 75.87824249267578, Learning Rate: 0.01\n",
      "Epoch [5680/10000], Loss: 75.87326049804688, Learning Rate: 0.01\n",
      "Epoch [5681/10000], Loss: 75.8667221069336, Learning Rate: 0.01\n",
      "Epoch [5682/10000], Loss: 75.86015319824219, Learning Rate: 0.01\n",
      "Epoch [5683/10000], Loss: 75.8546371459961, Learning Rate: 0.01\n",
      "Epoch [5684/10000], Loss: 75.85057067871094, Learning Rate: 0.01\n",
      "Epoch [5685/10000], Loss: 75.84696960449219, Learning Rate: 0.01\n",
      "Epoch [5686/10000], Loss: 75.8431625366211, Learning Rate: 0.01\n",
      "Epoch [5687/10000], Loss: 75.838623046875, Learning Rate: 0.01\n",
      "Epoch [5688/10000], Loss: 75.83324432373047, Learning Rate: 0.01\n",
      "Epoch [5689/10000], Loss: 75.82760620117188, Learning Rate: 0.01\n",
      "Epoch [5690/10000], Loss: 75.82240295410156, Learning Rate: 0.01\n",
      "Epoch [5691/10000], Loss: 75.81775665283203, Learning Rate: 0.01\n",
      "Epoch [5692/10000], Loss: 75.81365203857422, Learning Rate: 0.01\n",
      "Epoch [5693/10000], Loss: 75.80956268310547, Learning Rate: 0.01\n",
      "Epoch [5694/10000], Loss: 75.80506896972656, Learning Rate: 0.01\n",
      "Epoch [5695/10000], Loss: 75.80030822753906, Learning Rate: 0.01\n",
      "Epoch [5696/10000], Loss: 75.79528045654297, Learning Rate: 0.01\n",
      "Epoch [5697/10000], Loss: 75.79029083251953, Learning Rate: 0.01\n",
      "Epoch [5698/10000], Loss: 75.78544616699219, Learning Rate: 0.01\n",
      "Epoch [5699/10000], Loss: 75.78099060058594, Learning Rate: 0.01\n",
      "Epoch [5700/10000], Loss: 75.77656555175781, Learning Rate: 0.01\n",
      "Epoch [5701/10000], Loss: 75.77222442626953, Learning Rate: 0.01\n",
      "Epoch [5702/10000], Loss: 75.76760864257812, Learning Rate: 0.01\n",
      "Epoch [5703/10000], Loss: 75.7629165649414, Learning Rate: 0.01\n",
      "Epoch [5704/10000], Loss: 75.75809478759766, Learning Rate: 0.01\n",
      "Epoch [5705/10000], Loss: 75.75331115722656, Learning Rate: 0.01\n",
      "Epoch [5706/10000], Loss: 75.74861907958984, Learning Rate: 0.01\n",
      "Epoch [5707/10000], Loss: 75.74411010742188, Learning Rate: 0.01\n",
      "Epoch [5708/10000], Loss: 75.7396011352539, Learning Rate: 0.01\n",
      "Epoch [5709/10000], Loss: 75.7350082397461, Learning Rate: 0.01\n",
      "Epoch [5710/10000], Loss: 75.73051452636719, Learning Rate: 0.01\n",
      "Epoch [5711/10000], Loss: 75.72592163085938, Learning Rate: 0.01\n",
      "Epoch [5712/10000], Loss: 75.72123718261719, Learning Rate: 0.01\n",
      "Epoch [5713/10000], Loss: 75.71653747558594, Learning Rate: 0.01\n",
      "Epoch [5714/10000], Loss: 75.71192169189453, Learning Rate: 0.01\n",
      "Epoch [5715/10000], Loss: 75.70722961425781, Learning Rate: 0.01\n",
      "Epoch [5716/10000], Loss: 75.70274353027344, Learning Rate: 0.01\n",
      "Epoch [5717/10000], Loss: 75.69816589355469, Learning Rate: 0.01\n",
      "Epoch [5718/10000], Loss: 75.69364166259766, Learning Rate: 0.01\n",
      "Epoch [5719/10000], Loss: 75.68902587890625, Learning Rate: 0.01\n",
      "Epoch [5720/10000], Loss: 75.68435668945312, Learning Rate: 0.01\n",
      "Epoch [5721/10000], Loss: 75.67977142333984, Learning Rate: 0.01\n",
      "Epoch [5722/10000], Loss: 75.67515563964844, Learning Rate: 0.01\n",
      "Epoch [5723/10000], Loss: 75.67061614990234, Learning Rate: 0.01\n",
      "Epoch [5724/10000], Loss: 75.66596984863281, Learning Rate: 0.01\n",
      "Epoch [5725/10000], Loss: 75.66141510009766, Learning Rate: 0.01\n",
      "Epoch [5726/10000], Loss: 75.65681457519531, Learning Rate: 0.01\n",
      "Epoch [5727/10000], Loss: 75.65225982666016, Learning Rate: 0.01\n",
      "Epoch [5728/10000], Loss: 75.64768981933594, Learning Rate: 0.01\n",
      "Epoch [5729/10000], Loss: 75.64315795898438, Learning Rate: 0.01\n",
      "Epoch [5730/10000], Loss: 75.63853454589844, Learning Rate: 0.01\n",
      "Epoch [5731/10000], Loss: 75.63392639160156, Learning Rate: 0.01\n",
      "Epoch [5732/10000], Loss: 75.6292724609375, Learning Rate: 0.01\n",
      "Epoch [5733/10000], Loss: 75.6247787475586, Learning Rate: 0.01\n",
      "Epoch [5734/10000], Loss: 75.62014770507812, Learning Rate: 0.01\n",
      "Epoch [5735/10000], Loss: 75.61551666259766, Learning Rate: 0.01\n",
      "Epoch [5736/10000], Loss: 75.61096954345703, Learning Rate: 0.01\n",
      "Epoch [5737/10000], Loss: 75.6064224243164, Learning Rate: 0.01\n",
      "Epoch [5738/10000], Loss: 75.60187530517578, Learning Rate: 0.01\n",
      "Epoch [5739/10000], Loss: 75.59723663330078, Learning Rate: 0.01\n",
      "Epoch [5740/10000], Loss: 75.59272003173828, Learning Rate: 0.01\n",
      "Epoch [5741/10000], Loss: 75.58810424804688, Learning Rate: 0.01\n",
      "Epoch [5742/10000], Loss: 75.58354187011719, Learning Rate: 0.01\n",
      "Epoch [5743/10000], Loss: 75.57899475097656, Learning Rate: 0.01\n",
      "Epoch [5744/10000], Loss: 75.57437896728516, Learning Rate: 0.01\n",
      "Epoch [5745/10000], Loss: 75.56973266601562, Learning Rate: 0.01\n",
      "Epoch [5746/10000], Loss: 75.56524658203125, Learning Rate: 0.01\n",
      "Epoch [5747/10000], Loss: 75.56063842773438, Learning Rate: 0.01\n",
      "Epoch [5748/10000], Loss: 75.55612182617188, Learning Rate: 0.01\n",
      "Epoch [5749/10000], Loss: 75.55155944824219, Learning Rate: 0.01\n",
      "Epoch [5750/10000], Loss: 75.54700469970703, Learning Rate: 0.01\n",
      "Epoch [5751/10000], Loss: 75.5425033569336, Learning Rate: 0.01\n",
      "Epoch [5752/10000], Loss: 75.53780364990234, Learning Rate: 0.01\n",
      "Epoch [5753/10000], Loss: 75.53329467773438, Learning Rate: 0.01\n",
      "Epoch [5754/10000], Loss: 75.52864837646484, Learning Rate: 0.01\n",
      "Epoch [5755/10000], Loss: 75.52410888671875, Learning Rate: 0.01\n",
      "Epoch [5756/10000], Loss: 75.51953125, Learning Rate: 0.01\n",
      "Epoch [5757/10000], Loss: 75.5151138305664, Learning Rate: 0.01\n",
      "Epoch [5758/10000], Loss: 75.51048278808594, Learning Rate: 0.01\n",
      "Epoch [5759/10000], Loss: 75.5058822631836, Learning Rate: 0.01\n",
      "Epoch [5760/10000], Loss: 75.50130462646484, Learning Rate: 0.01\n",
      "Epoch [5761/10000], Loss: 75.49678039550781, Learning Rate: 0.01\n",
      "Epoch [5762/10000], Loss: 75.49214935302734, Learning Rate: 0.01\n",
      "Epoch [5763/10000], Loss: 75.48759460449219, Learning Rate: 0.01\n",
      "Epoch [5764/10000], Loss: 75.48302459716797, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5765/10000], Loss: 75.47856140136719, Learning Rate: 0.01\n",
      "Epoch [5766/10000], Loss: 75.47391510009766, Learning Rate: 0.01\n",
      "Epoch [5767/10000], Loss: 75.4693374633789, Learning Rate: 0.01\n",
      "Epoch [5768/10000], Loss: 75.46479034423828, Learning Rate: 0.01\n",
      "Epoch [5769/10000], Loss: 75.46035766601562, Learning Rate: 0.01\n",
      "Epoch [5770/10000], Loss: 75.4556655883789, Learning Rate: 0.01\n",
      "Epoch [5771/10000], Loss: 75.45114135742188, Learning Rate: 0.01\n",
      "Epoch [5772/10000], Loss: 75.44657135009766, Learning Rate: 0.01\n",
      "Epoch [5773/10000], Loss: 75.44207763671875, Learning Rate: 0.01\n",
      "Epoch [5774/10000], Loss: 75.43749237060547, Learning Rate: 0.01\n",
      "Epoch [5775/10000], Loss: 75.4329605102539, Learning Rate: 0.01\n",
      "Epoch [5776/10000], Loss: 75.42839050292969, Learning Rate: 0.01\n",
      "Epoch [5777/10000], Loss: 75.4238510131836, Learning Rate: 0.01\n",
      "Epoch [5778/10000], Loss: 75.41932678222656, Learning Rate: 0.01\n",
      "Epoch [5779/10000], Loss: 75.414794921875, Learning Rate: 0.01\n",
      "Epoch [5780/10000], Loss: 75.41018676757812, Learning Rate: 0.01\n",
      "Epoch [5781/10000], Loss: 75.40570831298828, Learning Rate: 0.01\n",
      "Epoch [5782/10000], Loss: 75.40110778808594, Learning Rate: 0.01\n",
      "Epoch [5783/10000], Loss: 75.39659118652344, Learning Rate: 0.01\n",
      "Epoch [5784/10000], Loss: 75.39197540283203, Learning Rate: 0.01\n",
      "Epoch [5785/10000], Loss: 75.38744354248047, Learning Rate: 0.01\n",
      "Epoch [5786/10000], Loss: 75.3829574584961, Learning Rate: 0.01\n",
      "Epoch [5787/10000], Loss: 75.37833404541016, Learning Rate: 0.01\n",
      "Epoch [5788/10000], Loss: 75.37384033203125, Learning Rate: 0.01\n",
      "Epoch [5789/10000], Loss: 75.36927032470703, Learning Rate: 0.01\n",
      "Epoch [5790/10000], Loss: 75.36482238769531, Learning Rate: 0.01\n",
      "Epoch [5791/10000], Loss: 75.36019134521484, Learning Rate: 0.01\n",
      "Epoch [5792/10000], Loss: 75.35566711425781, Learning Rate: 0.01\n",
      "Epoch [5793/10000], Loss: 75.35112762451172, Learning Rate: 0.01\n",
      "Epoch [5794/10000], Loss: 75.3465805053711, Learning Rate: 0.01\n",
      "Epoch [5795/10000], Loss: 75.34199523925781, Learning Rate: 0.01\n",
      "Epoch [5796/10000], Loss: 75.33747100830078, Learning Rate: 0.01\n",
      "Epoch [5797/10000], Loss: 75.33287048339844, Learning Rate: 0.01\n",
      "Epoch [5798/10000], Loss: 75.32843017578125, Learning Rate: 0.01\n",
      "Epoch [5799/10000], Loss: 75.32395935058594, Learning Rate: 0.01\n",
      "Epoch [5800/10000], Loss: 75.31936645507812, Learning Rate: 0.01\n",
      "Epoch [5801/10000], Loss: 75.31484985351562, Learning Rate: 0.01\n",
      "Epoch [5802/10000], Loss: 75.31021118164062, Learning Rate: 0.01\n",
      "Epoch [5803/10000], Loss: 75.3056869506836, Learning Rate: 0.01\n",
      "Epoch [5804/10000], Loss: 75.30122375488281, Learning Rate: 0.01\n",
      "Epoch [5805/10000], Loss: 75.29664611816406, Learning Rate: 0.01\n",
      "Epoch [5806/10000], Loss: 75.29217529296875, Learning Rate: 0.01\n",
      "Epoch [5807/10000], Loss: 75.2875747680664, Learning Rate: 0.01\n",
      "Epoch [5808/10000], Loss: 75.2830810546875, Learning Rate: 0.01\n",
      "Epoch [5809/10000], Loss: 75.27851104736328, Learning Rate: 0.01\n",
      "Epoch [5810/10000], Loss: 75.27400970458984, Learning Rate: 0.01\n",
      "Epoch [5811/10000], Loss: 75.26953887939453, Learning Rate: 0.01\n",
      "Epoch [5812/10000], Loss: 75.26498413085938, Learning Rate: 0.01\n",
      "Epoch [5813/10000], Loss: 75.26044464111328, Learning Rate: 0.01\n",
      "Epoch [5814/10000], Loss: 75.2558364868164, Learning Rate: 0.01\n",
      "Epoch [5815/10000], Loss: 75.25138854980469, Learning Rate: 0.01\n",
      "Epoch [5816/10000], Loss: 75.24683380126953, Learning Rate: 0.01\n",
      "Epoch [5817/10000], Loss: 75.24232482910156, Learning Rate: 0.01\n",
      "Epoch [5818/10000], Loss: 75.23777770996094, Learning Rate: 0.01\n",
      "Epoch [5819/10000], Loss: 75.23324584960938, Learning Rate: 0.01\n",
      "Epoch [5820/10000], Loss: 75.22869110107422, Learning Rate: 0.01\n",
      "Epoch [5821/10000], Loss: 75.22416687011719, Learning Rate: 0.01\n",
      "Epoch [5822/10000], Loss: 75.2197036743164, Learning Rate: 0.01\n",
      "Epoch [5823/10000], Loss: 75.21511840820312, Learning Rate: 0.01\n",
      "Epoch [5824/10000], Loss: 75.2106704711914, Learning Rate: 0.01\n",
      "Epoch [5825/10000], Loss: 75.20613861083984, Learning Rate: 0.01\n",
      "Epoch [5826/10000], Loss: 75.20159149169922, Learning Rate: 0.01\n",
      "Epoch [5827/10000], Loss: 75.19713592529297, Learning Rate: 0.01\n",
      "Epoch [5828/10000], Loss: 75.19255065917969, Learning Rate: 0.01\n",
      "Epoch [5829/10000], Loss: 75.18805694580078, Learning Rate: 0.01\n",
      "Epoch [5830/10000], Loss: 75.18354797363281, Learning Rate: 0.01\n",
      "Epoch [5831/10000], Loss: 75.17913818359375, Learning Rate: 0.01\n",
      "Epoch [5832/10000], Loss: 75.17466735839844, Learning Rate: 0.01\n",
      "Epoch [5833/10000], Loss: 75.17019653320312, Learning Rate: 0.01\n",
      "Epoch [5834/10000], Loss: 75.1658706665039, Learning Rate: 0.01\n",
      "Epoch [5835/10000], Loss: 75.16170501708984, Learning Rate: 0.01\n",
      "Epoch [5836/10000], Loss: 75.15773010253906, Learning Rate: 0.01\n",
      "Epoch [5837/10000], Loss: 75.15413665771484, Learning Rate: 0.01\n",
      "Epoch [5838/10000], Loss: 75.1511459350586, Learning Rate: 0.01\n",
      "Epoch [5839/10000], Loss: 75.14949798583984, Learning Rate: 0.01\n",
      "Epoch [5840/10000], Loss: 75.14995574951172, Learning Rate: 0.01\n",
      "Epoch [5841/10000], Loss: 75.15428161621094, Learning Rate: 0.01\n",
      "Epoch [5842/10000], Loss: 75.16546630859375, Learning Rate: 0.01\n",
      "Epoch [5843/10000], Loss: 75.18904876708984, Learning Rate: 0.01\n",
      "Epoch [5844/10000], Loss: 75.23490142822266, Learning Rate: 0.01\n",
      "Epoch [5845/10000], Loss: 75.3208236694336, Learning Rate: 0.01\n",
      "Epoch [5846/10000], Loss: 75.47819519042969, Learning Rate: 0.01\n",
      "Epoch [5847/10000], Loss: 75.76031494140625, Learning Rate: 0.01\n",
      "Epoch [5848/10000], Loss: 76.25209045410156, Learning Rate: 0.01\n",
      "Epoch [5849/10000], Loss: 77.06768798828125, Learning Rate: 0.01\n",
      "Epoch [5850/10000], Loss: 78.30662536621094, Learning Rate: 0.01\n",
      "Epoch [5851/10000], Loss: 79.89575958251953, Learning Rate: 0.01\n",
      "Epoch [5852/10000], Loss: 81.3352279663086, Learning Rate: 0.01\n",
      "Epoch [5853/10000], Loss: 81.61454010009766, Learning Rate: 0.01\n",
      "Epoch [5854/10000], Loss: 79.99794006347656, Learning Rate: 0.01\n",
      "Epoch [5855/10000], Loss: 77.20794677734375, Learning Rate: 0.01\n",
      "Epoch [5856/10000], Loss: 75.26030731201172, Learning Rate: 0.01\n",
      "Epoch [5857/10000], Loss: 75.40705871582031, Learning Rate: 0.01\n",
      "Epoch [5858/10000], Loss: 76.92678833007812, Learning Rate: 0.01\n",
      "Epoch [5859/10000], Loss: 78.01493072509766, Learning Rate: 0.01\n",
      "Epoch [5860/10000], Loss: 77.51370239257812, Learning Rate: 0.01\n",
      "Epoch [5861/10000], Loss: 76.00102233886719, Learning Rate: 0.01\n",
      "Epoch [5862/10000], Loss: 75.06239318847656, Learning Rate: 0.01\n",
      "Epoch [5863/10000], Loss: 75.44841003417969, Learning Rate: 0.01\n",
      "Epoch [5864/10000], Loss: 76.3729248046875, Learning Rate: 0.01\n",
      "Epoch [5865/10000], Loss: 76.61746978759766, Learning Rate: 0.01\n",
      "Epoch [5866/10000], Loss: 75.92517852783203, Learning Rate: 0.01\n",
      "Epoch [5867/10000], Loss: 75.14676666259766, Learning Rate: 0.01\n",
      "Epoch [5868/10000], Loss: 75.0988998413086, Learning Rate: 0.01\n",
      "Epoch [5869/10000], Loss: 75.62017059326172, Learning Rate: 0.01\n",
      "Epoch [5870/10000], Loss: 75.93325805664062, Learning Rate: 0.01\n",
      "Epoch [5871/10000], Loss: 75.64851379394531, Learning Rate: 0.01\n",
      "Epoch [5872/10000], Loss: 75.14798736572266, Learning Rate: 0.01\n",
      "Epoch [5873/10000], Loss: 75.00535583496094, Learning Rate: 0.01\n",
      "Epoch [5874/10000], Loss: 75.2713394165039, Learning Rate: 0.01\n",
      "Epoch [5875/10000], Loss: 75.51170349121094, Learning Rate: 0.01\n",
      "Epoch [5876/10000], Loss: 75.40736389160156, Learning Rate: 0.01\n",
      "Epoch [5877/10000], Loss: 75.10820770263672, Learning Rate: 0.01\n",
      "Epoch [5878/10000], Loss: 74.97042846679688, Learning Rate: 0.01\n",
      "Epoch [5879/10000], Loss: 75.094970703125, Learning Rate: 0.01\n",
      "Epoch [5880/10000], Loss: 75.2567138671875, Learning Rate: 0.01\n",
      "Epoch [5881/10000], Loss: 75.23199462890625, Learning Rate: 0.01\n",
      "Epoch [5882/10000], Loss: 75.06076049804688, Learning Rate: 0.01\n",
      "Epoch [5883/10000], Loss: 74.94999694824219, Learning Rate: 0.01\n",
      "Epoch [5884/10000], Loss: 74.9979019165039, Learning Rate: 0.01\n",
      "Epoch [5885/10000], Loss: 75.09953308105469, Learning Rate: 0.01\n",
      "Epoch [5886/10000], Loss: 75.10838317871094, Learning Rate: 0.01\n",
      "Epoch [5887/10000], Loss: 75.01547241210938, Learning Rate: 0.01\n",
      "Epoch [5888/10000], Loss: 74.93306732177734, Learning Rate: 0.01\n",
      "Epoch [5889/10000], Loss: 74.94071197509766, Learning Rate: 0.01\n",
      "Epoch [5890/10000], Loss: 74.99939727783203, Learning Rate: 0.01\n",
      "Epoch [5891/10000], Loss: 75.01951599121094, Learning Rate: 0.01\n",
      "Epoch [5892/10000], Loss: 74.97383880615234, Learning Rate: 0.01\n",
      "Epoch [5893/10000], Loss: 74.91616821289062, Learning Rate: 0.01\n",
      "Epoch [5894/10000], Loss: 74.90445709228516, Learning Rate: 0.01\n",
      "Epoch [5895/10000], Loss: 74.93356323242188, Learning Rate: 0.01\n",
      "Epoch [5896/10000], Loss: 74.95364379882812, Learning Rate: 0.01\n",
      "Epoch [5897/10000], Loss: 74.93510437011719, Learning Rate: 0.01\n",
      "Epoch [5898/10000], Loss: 74.89771270751953, Learning Rate: 0.01\n",
      "Epoch [5899/10000], Loss: 74.87903594970703, Learning Rate: 0.01\n",
      "Epoch [5900/10000], Loss: 74.88871002197266, Learning Rate: 0.01\n",
      "Epoch [5901/10000], Loss: 74.90328979492188, Learning Rate: 0.01\n",
      "Epoch [5902/10000], Loss: 74.8988037109375, Learning Rate: 0.01\n",
      "Epoch [5903/10000], Loss: 74.87693786621094, Learning Rate: 0.01\n",
      "Epoch [5904/10000], Loss: 74.85845947265625, Learning Rate: 0.01\n",
      "Epoch [5905/10000], Loss: 74.85655212402344, Learning Rate: 0.01\n",
      "Epoch [5906/10000], Loss: 74.86388397216797, Learning Rate: 0.01\n",
      "Epoch [5907/10000], Loss: 74.8648681640625, Learning Rate: 0.01\n",
      "Epoch [5908/10000], Loss: 74.85379028320312, Learning Rate: 0.01\n",
      "Epoch [5909/10000], Loss: 74.83914947509766, Learning Rate: 0.01\n",
      "Epoch [5910/10000], Loss: 74.83167266845703, Learning Rate: 0.01\n",
      "Epoch [5911/10000], Loss: 74.83250427246094, Learning Rate: 0.01\n",
      "Epoch [5912/10000], Loss: 74.83387756347656, Learning Rate: 0.01\n",
      "Epoch [5913/10000], Loss: 74.82903289794922, Learning Rate: 0.01\n",
      "Epoch [5914/10000], Loss: 74.819091796875, Learning Rate: 0.01\n",
      "Epoch [5915/10000], Loss: 74.81029510498047, Learning Rate: 0.01\n",
      "Epoch [5916/10000], Loss: 74.80652618408203, Learning Rate: 0.01\n",
      "Epoch [5917/10000], Loss: 74.80587005615234, Learning Rate: 0.01\n",
      "Epoch [5918/10000], Loss: 74.80357360839844, Learning Rate: 0.01\n",
      "Epoch [5919/10000], Loss: 74.7974853515625, Learning Rate: 0.01\n",
      "Epoch [5920/10000], Loss: 74.78987884521484, Learning Rate: 0.01\n",
      "Epoch [5921/10000], Loss: 74.78406524658203, Learning Rate: 0.01\n",
      "Epoch [5922/10000], Loss: 74.78097534179688, Learning Rate: 0.01\n",
      "Epoch [5923/10000], Loss: 74.77854919433594, Learning Rate: 0.01\n",
      "Epoch [5924/10000], Loss: 74.7746810913086, Learning Rate: 0.01\n",
      "Epoch [5925/10000], Loss: 74.76892852783203, Learning Rate: 0.01\n",
      "Epoch [5926/10000], Loss: 74.76297760009766, Learning Rate: 0.01\n",
      "Epoch [5927/10000], Loss: 74.75821685791016, Learning Rate: 0.01\n",
      "Epoch [5928/10000], Loss: 74.7547836303711, Learning Rate: 0.01\n",
      "Epoch [5929/10000], Loss: 74.7513656616211, Learning Rate: 0.01\n",
      "Epoch [5930/10000], Loss: 74.74715423583984, Learning Rate: 0.01\n",
      "Epoch [5931/10000], Loss: 74.74197387695312, Learning Rate: 0.01\n",
      "Epoch [5932/10000], Loss: 74.73677825927734, Learning Rate: 0.01\n",
      "Epoch [5933/10000], Loss: 74.73241424560547, Learning Rate: 0.01\n",
      "Epoch [5934/10000], Loss: 74.72866821289062, Learning Rate: 0.01\n",
      "Epoch [5935/10000], Loss: 74.72480773925781, Learning Rate: 0.01\n",
      "Epoch [5936/10000], Loss: 74.72037506103516, Learning Rate: 0.01\n",
      "Epoch [5937/10000], Loss: 74.71563720703125, Learning Rate: 0.01\n",
      "Epoch [5938/10000], Loss: 74.71085357666016, Learning Rate: 0.01\n",
      "Epoch [5939/10000], Loss: 74.70648193359375, Learning Rate: 0.01\n",
      "Epoch [5940/10000], Loss: 74.70249938964844, Learning Rate: 0.01\n",
      "Epoch [5941/10000], Loss: 74.69844055175781, Learning Rate: 0.01\n",
      "Epoch [5942/10000], Loss: 74.69408416748047, Learning Rate: 0.01\n",
      "Epoch [5943/10000], Loss: 74.68959045410156, Learning Rate: 0.01\n",
      "Epoch [5944/10000], Loss: 74.68502807617188, Learning Rate: 0.01\n",
      "Epoch [5945/10000], Loss: 74.68064880371094, Learning Rate: 0.01\n",
      "Epoch [5946/10000], Loss: 74.67652130126953, Learning Rate: 0.01\n",
      "Epoch [5947/10000], Loss: 74.67228698730469, Learning Rate: 0.01\n",
      "Epoch [5948/10000], Loss: 74.66812133789062, Learning Rate: 0.01\n",
      "Epoch [5949/10000], Loss: 74.66361236572266, Learning Rate: 0.01\n",
      "Epoch [5950/10000], Loss: 74.65922546386719, Learning Rate: 0.01\n",
      "Epoch [5951/10000], Loss: 74.65486145019531, Learning Rate: 0.01\n",
      "Epoch [5952/10000], Loss: 74.65054321289062, Learning Rate: 0.01\n",
      "Epoch [5953/10000], Loss: 74.64639282226562, Learning Rate: 0.01\n",
      "Epoch [5954/10000], Loss: 74.6421127319336, Learning Rate: 0.01\n",
      "Epoch [5955/10000], Loss: 74.63777923583984, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5956/10000], Loss: 74.6333999633789, Learning Rate: 0.01\n",
      "Epoch [5957/10000], Loss: 74.62908935546875, Learning Rate: 0.01\n",
      "Epoch [5958/10000], Loss: 74.62474060058594, Learning Rate: 0.01\n",
      "Epoch [5959/10000], Loss: 74.62055206298828, Learning Rate: 0.01\n",
      "Epoch [5960/10000], Loss: 74.6162338256836, Learning Rate: 0.01\n",
      "Epoch [5961/10000], Loss: 74.61192321777344, Learning Rate: 0.01\n",
      "Epoch [5962/10000], Loss: 74.60761260986328, Learning Rate: 0.01\n",
      "Epoch [5963/10000], Loss: 74.60325622558594, Learning Rate: 0.01\n",
      "Epoch [5964/10000], Loss: 74.59892272949219, Learning Rate: 0.01\n",
      "Epoch [5965/10000], Loss: 74.59474182128906, Learning Rate: 0.01\n",
      "Epoch [5966/10000], Loss: 74.5904541015625, Learning Rate: 0.01\n",
      "Epoch [5967/10000], Loss: 74.58614349365234, Learning Rate: 0.01\n",
      "Epoch [5968/10000], Loss: 74.58186340332031, Learning Rate: 0.01\n",
      "Epoch [5969/10000], Loss: 74.57760620117188, Learning Rate: 0.01\n",
      "Epoch [5970/10000], Loss: 74.57322692871094, Learning Rate: 0.01\n",
      "Epoch [5971/10000], Loss: 74.56895446777344, Learning Rate: 0.01\n",
      "Epoch [5972/10000], Loss: 74.56459045410156, Learning Rate: 0.01\n",
      "Epoch [5973/10000], Loss: 74.56036376953125, Learning Rate: 0.01\n",
      "Epoch [5974/10000], Loss: 74.55610656738281, Learning Rate: 0.01\n",
      "Epoch [5975/10000], Loss: 74.5517349243164, Learning Rate: 0.01\n",
      "Epoch [5976/10000], Loss: 74.5474624633789, Learning Rate: 0.01\n",
      "Epoch [5977/10000], Loss: 74.54318237304688, Learning Rate: 0.01\n",
      "Epoch [5978/10000], Loss: 74.53890228271484, Learning Rate: 0.01\n",
      "Epoch [5979/10000], Loss: 74.53460693359375, Learning Rate: 0.01\n",
      "Epoch [5980/10000], Loss: 74.53028869628906, Learning Rate: 0.01\n",
      "Epoch [5981/10000], Loss: 74.52600860595703, Learning Rate: 0.01\n",
      "Epoch [5982/10000], Loss: 74.5217514038086, Learning Rate: 0.01\n",
      "Epoch [5983/10000], Loss: 74.51742553710938, Learning Rate: 0.01\n",
      "Epoch [5984/10000], Loss: 74.51312255859375, Learning Rate: 0.01\n",
      "Epoch [5985/10000], Loss: 74.50892639160156, Learning Rate: 0.01\n",
      "Epoch [5986/10000], Loss: 74.50452423095703, Learning Rate: 0.01\n",
      "Epoch [5987/10000], Loss: 74.50025177001953, Learning Rate: 0.01\n",
      "Epoch [5988/10000], Loss: 74.49595642089844, Learning Rate: 0.01\n",
      "Epoch [5989/10000], Loss: 74.49169921875, Learning Rate: 0.01\n",
      "Epoch [5990/10000], Loss: 74.48741149902344, Learning Rate: 0.01\n",
      "Epoch [5991/10000], Loss: 74.48314666748047, Learning Rate: 0.01\n",
      "Epoch [5992/10000], Loss: 74.47882843017578, Learning Rate: 0.01\n",
      "Epoch [5993/10000], Loss: 74.47450256347656, Learning Rate: 0.01\n",
      "Epoch [5994/10000], Loss: 74.4702377319336, Learning Rate: 0.01\n",
      "Epoch [5995/10000], Loss: 74.46598815917969, Learning Rate: 0.01\n",
      "Epoch [5996/10000], Loss: 74.46174621582031, Learning Rate: 0.01\n",
      "Epoch [5997/10000], Loss: 74.45738983154297, Learning Rate: 0.01\n",
      "Epoch [5998/10000], Loss: 74.45305633544922, Learning Rate: 0.01\n",
      "Epoch [5999/10000], Loss: 74.44882202148438, Learning Rate: 0.01\n",
      "Epoch [6000/10000], Loss: 74.44459533691406, Learning Rate: 0.01\n",
      "Epoch [6001/10000], Loss: 74.44027709960938, Learning Rate: 0.01\n",
      "Epoch [6002/10000], Loss: 74.43597412109375, Learning Rate: 0.01\n",
      "Epoch [6003/10000], Loss: 74.43170166015625, Learning Rate: 0.01\n",
      "Epoch [6004/10000], Loss: 74.42741394042969, Learning Rate: 0.01\n",
      "Epoch [6005/10000], Loss: 74.42313385009766, Learning Rate: 0.01\n",
      "Epoch [6006/10000], Loss: 74.41886901855469, Learning Rate: 0.01\n",
      "Epoch [6007/10000], Loss: 74.41461181640625, Learning Rate: 0.01\n",
      "Epoch [6008/10000], Loss: 74.4102554321289, Learning Rate: 0.01\n",
      "Epoch [6009/10000], Loss: 74.40596771240234, Learning Rate: 0.01\n",
      "Epoch [6010/10000], Loss: 74.40180969238281, Learning Rate: 0.01\n",
      "Epoch [6011/10000], Loss: 74.39743041992188, Learning Rate: 0.01\n",
      "Epoch [6012/10000], Loss: 74.39317321777344, Learning Rate: 0.01\n",
      "Epoch [6013/10000], Loss: 74.38885498046875, Learning Rate: 0.01\n",
      "Epoch [6014/10000], Loss: 74.38452911376953, Learning Rate: 0.01\n",
      "Epoch [6015/10000], Loss: 74.38028717041016, Learning Rate: 0.01\n",
      "Epoch [6016/10000], Loss: 74.3760757446289, Learning Rate: 0.01\n",
      "Epoch [6017/10000], Loss: 74.37174987792969, Learning Rate: 0.01\n",
      "Epoch [6018/10000], Loss: 74.36746978759766, Learning Rate: 0.01\n",
      "Epoch [6019/10000], Loss: 74.36323547363281, Learning Rate: 0.01\n",
      "Epoch [6020/10000], Loss: 74.35890197753906, Learning Rate: 0.01\n",
      "Epoch [6021/10000], Loss: 74.35464477539062, Learning Rate: 0.01\n",
      "Epoch [6022/10000], Loss: 74.35041046142578, Learning Rate: 0.01\n",
      "Epoch [6023/10000], Loss: 74.34612274169922, Learning Rate: 0.01\n",
      "Epoch [6024/10000], Loss: 74.34185028076172, Learning Rate: 0.01\n",
      "Epoch [6025/10000], Loss: 74.3376235961914, Learning Rate: 0.01\n",
      "Epoch [6026/10000], Loss: 74.33329772949219, Learning Rate: 0.01\n",
      "Epoch [6027/10000], Loss: 74.32901763916016, Learning Rate: 0.01\n",
      "Epoch [6028/10000], Loss: 74.3246841430664, Learning Rate: 0.01\n",
      "Epoch [6029/10000], Loss: 74.32048034667969, Learning Rate: 0.01\n",
      "Epoch [6030/10000], Loss: 74.31611633300781, Learning Rate: 0.01\n",
      "Epoch [6031/10000], Loss: 74.31194305419922, Learning Rate: 0.01\n",
      "Epoch [6032/10000], Loss: 74.30757141113281, Learning Rate: 0.01\n",
      "Epoch [6033/10000], Loss: 74.30337524414062, Learning Rate: 0.01\n",
      "Epoch [6034/10000], Loss: 74.2990493774414, Learning Rate: 0.01\n",
      "Epoch [6035/10000], Loss: 74.29488372802734, Learning Rate: 0.01\n",
      "Epoch [6036/10000], Loss: 74.29049682617188, Learning Rate: 0.01\n",
      "Epoch [6037/10000], Loss: 74.28626251220703, Learning Rate: 0.01\n",
      "Epoch [6038/10000], Loss: 74.28197479248047, Learning Rate: 0.01\n",
      "Epoch [6039/10000], Loss: 74.27774810791016, Learning Rate: 0.01\n",
      "Epoch [6040/10000], Loss: 74.27339935302734, Learning Rate: 0.01\n",
      "Epoch [6041/10000], Loss: 74.26919555664062, Learning Rate: 0.01\n",
      "Epoch [6042/10000], Loss: 74.2648696899414, Learning Rate: 0.01\n",
      "Epoch [6043/10000], Loss: 74.26057434082031, Learning Rate: 0.01\n",
      "Epoch [6044/10000], Loss: 74.25636291503906, Learning Rate: 0.01\n",
      "Epoch [6045/10000], Loss: 74.2520523071289, Learning Rate: 0.01\n",
      "Epoch [6046/10000], Loss: 74.24781036376953, Learning Rate: 0.01\n",
      "Epoch [6047/10000], Loss: 74.24354553222656, Learning Rate: 0.01\n",
      "Epoch [6048/10000], Loss: 74.23927307128906, Learning Rate: 0.01\n",
      "Epoch [6049/10000], Loss: 74.23501586914062, Learning Rate: 0.01\n",
      "Epoch [6050/10000], Loss: 74.23069763183594, Learning Rate: 0.01\n",
      "Epoch [6051/10000], Loss: 74.22651672363281, Learning Rate: 0.01\n",
      "Epoch [6052/10000], Loss: 74.22216796875, Learning Rate: 0.01\n",
      "Epoch [6053/10000], Loss: 74.21796417236328, Learning Rate: 0.01\n",
      "Epoch [6054/10000], Loss: 74.2136459350586, Learning Rate: 0.01\n",
      "Epoch [6055/10000], Loss: 74.2094497680664, Learning Rate: 0.01\n",
      "Epoch [6056/10000], Loss: 74.20518493652344, Learning Rate: 0.01\n",
      "Epoch [6057/10000], Loss: 74.20084381103516, Learning Rate: 0.01\n",
      "Epoch [6058/10000], Loss: 74.19660949707031, Learning Rate: 0.01\n",
      "Epoch [6059/10000], Loss: 74.19225311279297, Learning Rate: 0.01\n",
      "Epoch [6060/10000], Loss: 74.18803405761719, Learning Rate: 0.01\n",
      "Epoch [6061/10000], Loss: 74.18380737304688, Learning Rate: 0.01\n",
      "Epoch [6062/10000], Loss: 74.17952728271484, Learning Rate: 0.01\n",
      "Epoch [6063/10000], Loss: 74.17523193359375, Learning Rate: 0.01\n",
      "Epoch [6064/10000], Loss: 74.17095947265625, Learning Rate: 0.01\n",
      "Epoch [6065/10000], Loss: 74.1667709350586, Learning Rate: 0.01\n",
      "Epoch [6066/10000], Loss: 74.16238403320312, Learning Rate: 0.01\n",
      "Epoch [6067/10000], Loss: 74.15818786621094, Learning Rate: 0.01\n",
      "Epoch [6068/10000], Loss: 74.15388488769531, Learning Rate: 0.01\n",
      "Epoch [6069/10000], Loss: 74.14970397949219, Learning Rate: 0.01\n",
      "Epoch [6070/10000], Loss: 74.1453628540039, Learning Rate: 0.01\n",
      "Epoch [6071/10000], Loss: 74.14117431640625, Learning Rate: 0.01\n",
      "Epoch [6072/10000], Loss: 74.13688659667969, Learning Rate: 0.01\n",
      "Epoch [6073/10000], Loss: 74.13261413574219, Learning Rate: 0.01\n",
      "Epoch [6074/10000], Loss: 74.1282958984375, Learning Rate: 0.01\n",
      "Epoch [6075/10000], Loss: 74.12406921386719, Learning Rate: 0.01\n",
      "Epoch [6076/10000], Loss: 74.11986541748047, Learning Rate: 0.01\n",
      "Epoch [6077/10000], Loss: 74.11557006835938, Learning Rate: 0.01\n",
      "Epoch [6078/10000], Loss: 74.11131286621094, Learning Rate: 0.01\n",
      "Epoch [6079/10000], Loss: 74.10710144042969, Learning Rate: 0.01\n",
      "Epoch [6080/10000], Loss: 74.102783203125, Learning Rate: 0.01\n",
      "Epoch [6081/10000], Loss: 74.09852600097656, Learning Rate: 0.01\n",
      "Epoch [6082/10000], Loss: 74.09429168701172, Learning Rate: 0.01\n",
      "Epoch [6083/10000], Loss: 74.09001922607422, Learning Rate: 0.01\n",
      "Epoch [6084/10000], Loss: 74.08573913574219, Learning Rate: 0.01\n",
      "Epoch [6085/10000], Loss: 74.08146667480469, Learning Rate: 0.01\n",
      "Epoch [6086/10000], Loss: 74.07726287841797, Learning Rate: 0.01\n",
      "Epoch [6087/10000], Loss: 74.07293701171875, Learning Rate: 0.01\n",
      "Epoch [6088/10000], Loss: 74.06866455078125, Learning Rate: 0.01\n",
      "Epoch [6089/10000], Loss: 74.06443786621094, Learning Rate: 0.01\n",
      "Epoch [6090/10000], Loss: 74.06023406982422, Learning Rate: 0.01\n",
      "Epoch [6091/10000], Loss: 74.05596923828125, Learning Rate: 0.01\n",
      "Epoch [6092/10000], Loss: 74.05166625976562, Learning Rate: 0.01\n",
      "Epoch [6093/10000], Loss: 74.0473861694336, Learning Rate: 0.01\n",
      "Epoch [6094/10000], Loss: 74.04315185546875, Learning Rate: 0.01\n",
      "Epoch [6095/10000], Loss: 74.03888702392578, Learning Rate: 0.01\n",
      "Epoch [6096/10000], Loss: 74.03470611572266, Learning Rate: 0.01\n",
      "Epoch [6097/10000], Loss: 74.03044891357422, Learning Rate: 0.01\n",
      "Epoch [6098/10000], Loss: 74.026123046875, Learning Rate: 0.01\n",
      "Epoch [6099/10000], Loss: 74.02188873291016, Learning Rate: 0.01\n",
      "Epoch [6100/10000], Loss: 74.0176773071289, Learning Rate: 0.01\n",
      "Epoch [6101/10000], Loss: 74.01332092285156, Learning Rate: 0.01\n",
      "Epoch [6102/10000], Loss: 74.00908660888672, Learning Rate: 0.01\n",
      "Epoch [6103/10000], Loss: 74.00487518310547, Learning Rate: 0.01\n",
      "Epoch [6104/10000], Loss: 74.00057220458984, Learning Rate: 0.01\n",
      "Epoch [6105/10000], Loss: 73.99632263183594, Learning Rate: 0.01\n",
      "Epoch [6106/10000], Loss: 73.9920883178711, Learning Rate: 0.01\n",
      "Epoch [6107/10000], Loss: 73.98784637451172, Learning Rate: 0.01\n",
      "Epoch [6108/10000], Loss: 73.98358154296875, Learning Rate: 0.01\n",
      "Epoch [6109/10000], Loss: 73.97937774658203, Learning Rate: 0.01\n",
      "Epoch [6110/10000], Loss: 73.97508239746094, Learning Rate: 0.01\n",
      "Epoch [6111/10000], Loss: 73.97080993652344, Learning Rate: 0.01\n",
      "Epoch [6112/10000], Loss: 73.96664428710938, Learning Rate: 0.01\n",
      "Epoch [6113/10000], Loss: 73.96239471435547, Learning Rate: 0.01\n",
      "Epoch [6114/10000], Loss: 73.95814514160156, Learning Rate: 0.01\n",
      "Epoch [6115/10000], Loss: 73.95387268066406, Learning Rate: 0.01\n",
      "Epoch [6116/10000], Loss: 73.94976043701172, Learning Rate: 0.01\n",
      "Epoch [6117/10000], Loss: 73.94552612304688, Learning Rate: 0.01\n",
      "Epoch [6118/10000], Loss: 73.9413833618164, Learning Rate: 0.01\n",
      "Epoch [6119/10000], Loss: 73.93730926513672, Learning Rate: 0.01\n",
      "Epoch [6120/10000], Loss: 73.93339538574219, Learning Rate: 0.01\n",
      "Epoch [6121/10000], Loss: 73.92952728271484, Learning Rate: 0.01\n",
      "Epoch [6122/10000], Loss: 73.92587280273438, Learning Rate: 0.01\n",
      "Epoch [6123/10000], Loss: 73.92268371582031, Learning Rate: 0.01\n",
      "Epoch [6124/10000], Loss: 73.919921875, Learning Rate: 0.01\n",
      "Epoch [6125/10000], Loss: 73.9181900024414, Learning Rate: 0.01\n",
      "Epoch [6126/10000], Loss: 73.91768646240234, Learning Rate: 0.01\n",
      "Epoch [6127/10000], Loss: 73.91963195800781, Learning Rate: 0.01\n",
      "Epoch [6128/10000], Loss: 73.92522430419922, Learning Rate: 0.01\n",
      "Epoch [6129/10000], Loss: 73.93695068359375, Learning Rate: 0.01\n",
      "Epoch [6130/10000], Loss: 73.95854949951172, Learning Rate: 0.01\n",
      "Epoch [6131/10000], Loss: 73.99625396728516, Learning Rate: 0.01\n",
      "Epoch [6132/10000], Loss: 74.0608139038086, Learning Rate: 0.01\n",
      "Epoch [6133/10000], Loss: 74.16877746582031, Learning Rate: 0.01\n",
      "Epoch [6134/10000], Loss: 74.34761810302734, Learning Rate: 0.01\n",
      "Epoch [6135/10000], Loss: 74.63855743408203, Learning Rate: 0.01\n",
      "Epoch [6136/10000], Loss: 75.10250091552734, Learning Rate: 0.01\n",
      "Epoch [6137/10000], Loss: 75.81317138671875, Learning Rate: 0.01\n",
      "Epoch [6138/10000], Loss: 76.8371810913086, Learning Rate: 0.01\n",
      "Epoch [6139/10000], Loss: 78.14816284179688, Learning Rate: 0.01\n",
      "Epoch [6140/10000], Loss: 79.51367950439453, Learning Rate: 0.01\n",
      "Epoch [6141/10000], Loss: 80.3498306274414, Learning Rate: 0.01\n",
      "Epoch [6142/10000], Loss: 79.96021270751953, Learning Rate: 0.01\n",
      "Epoch [6143/10000], Loss: 78.10592651367188, Learning Rate: 0.01\n",
      "Epoch [6144/10000], Loss: 75.64999389648438, Learning Rate: 0.01\n",
      "Epoch [6145/10000], Loss: 74.02540588378906, Learning Rate: 0.01\n",
      "Epoch [6146/10000], Loss: 74.02926635742188, Learning Rate: 0.01\n",
      "Epoch [6147/10000], Loss: 75.2003402709961, Learning Rate: 0.01\n",
      "Epoch [6148/10000], Loss: 76.3314437866211, Learning Rate: 0.01\n",
      "Epoch [6149/10000], Loss: 76.4316177368164, Learning Rate: 0.01\n",
      "Epoch [6150/10000], Loss: 75.44259643554688, Learning Rate: 0.01\n",
      "Epoch [6151/10000], Loss: 74.2484359741211, Learning Rate: 0.01\n",
      "Epoch [6152/10000], Loss: 73.79927825927734, Learning Rate: 0.01\n",
      "Epoch [6153/10000], Loss: 74.24996185302734, Learning Rate: 0.01\n",
      "Epoch [6154/10000], Loss: 74.95255279541016, Learning Rate: 0.01\n",
      "Epoch [6155/10000], Loss: 75.16030883789062, Learning Rate: 0.01\n",
      "Epoch [6156/10000], Loss: 74.70133972167969, Learning Rate: 0.01\n",
      "Epoch [6157/10000], Loss: 74.0450439453125, Learning Rate: 0.01\n",
      "Epoch [6158/10000], Loss: 73.77267456054688, Learning Rate: 0.01\n",
      "Epoch [6159/10000], Loss: 74.00916290283203, Learning Rate: 0.01\n",
      "Epoch [6160/10000], Loss: 74.39012145996094, Learning Rate: 0.01\n",
      "Epoch [6161/10000], Loss: 74.49028778076172, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6162/10000], Loss: 74.225830078125, Learning Rate: 0.01\n",
      "Epoch [6163/10000], Loss: 73.87505340576172, Learning Rate: 0.01\n",
      "Epoch [6164/10000], Loss: 73.74967956542969, Learning Rate: 0.01\n",
      "Epoch [6165/10000], Loss: 73.89391326904297, Learning Rate: 0.01\n",
      "Epoch [6166/10000], Loss: 74.09310150146484, Learning Rate: 0.01\n",
      "Epoch [6167/10000], Loss: 74.12437438964844, Learning Rate: 0.01\n",
      "Epoch [6168/10000], Loss: 73.96620178222656, Learning Rate: 0.01\n",
      "Epoch [6169/10000], Loss: 73.780517578125, Learning Rate: 0.01\n",
      "Epoch [6170/10000], Loss: 73.7266845703125, Learning Rate: 0.01\n",
      "Epoch [6171/10000], Loss: 73.81148529052734, Learning Rate: 0.01\n",
      "Epoch [6172/10000], Loss: 73.91316223144531, Learning Rate: 0.01\n",
      "Epoch [6173/10000], Loss: 73.9188232421875, Learning Rate: 0.01\n",
      "Epoch [6174/10000], Loss: 73.82707214355469, Learning Rate: 0.01\n",
      "Epoch [6175/10000], Loss: 73.72774505615234, Learning Rate: 0.01\n",
      "Epoch [6176/10000], Loss: 73.70134735107422, Learning Rate: 0.01\n",
      "Epoch [6177/10000], Loss: 73.74683380126953, Learning Rate: 0.01\n",
      "Epoch [6178/10000], Loss: 73.79862976074219, Learning Rate: 0.01\n",
      "Epoch [6179/10000], Loss: 73.79866790771484, Learning Rate: 0.01\n",
      "Epoch [6180/10000], Loss: 73.74775695800781, Learning Rate: 0.01\n",
      "Epoch [6181/10000], Loss: 73.6925277709961, Learning Rate: 0.01\n",
      "Epoch [6182/10000], Loss: 73.67526245117188, Learning Rate: 0.01\n",
      "Epoch [6183/10000], Loss: 73.6964340209961, Learning Rate: 0.01\n",
      "Epoch [6184/10000], Loss: 73.72276306152344, Learning Rate: 0.01\n",
      "Epoch [6185/10000], Loss: 73.72335815429688, Learning Rate: 0.01\n",
      "Epoch [6186/10000], Loss: 73.6960678100586, Learning Rate: 0.01\n",
      "Epoch [6187/10000], Loss: 73.66393280029297, Learning Rate: 0.01\n",
      "Epoch [6188/10000], Loss: 73.64972686767578, Learning Rate: 0.01\n",
      "Epoch [6189/10000], Loss: 73.6568832397461, Learning Rate: 0.01\n",
      "Epoch [6190/10000], Loss: 73.66970825195312, Learning Rate: 0.01\n",
      "Epoch [6191/10000], Loss: 73.67105865478516, Learning Rate: 0.01\n",
      "Epoch [6192/10000], Loss: 73.65703582763672, Learning Rate: 0.01\n",
      "Epoch [6193/10000], Loss: 73.6377182006836, Learning Rate: 0.01\n",
      "Epoch [6194/10000], Loss: 73.62547302246094, Learning Rate: 0.01\n",
      "Epoch [6195/10000], Loss: 73.62480163574219, Learning Rate: 0.01\n",
      "Epoch [6196/10000], Loss: 73.62958526611328, Learning Rate: 0.01\n",
      "Epoch [6197/10000], Loss: 73.63080596923828, Learning Rate: 0.01\n",
      "Epoch [6198/10000], Loss: 73.6239242553711, Learning Rate: 0.01\n",
      "Epoch [6199/10000], Loss: 73.61212158203125, Learning Rate: 0.01\n",
      "Epoch [6200/10000], Loss: 73.60186767578125, Learning Rate: 0.01\n",
      "Epoch [6201/10000], Loss: 73.5972900390625, Learning Rate: 0.01\n",
      "Epoch [6202/10000], Loss: 73.59716796875, Learning Rate: 0.01\n",
      "Epoch [6203/10000], Loss: 73.59709930419922, Learning Rate: 0.01\n",
      "Epoch [6204/10000], Loss: 73.59347534179688, Learning Rate: 0.01\n",
      "Epoch [6205/10000], Loss: 73.5863265991211, Learning Rate: 0.01\n",
      "Epoch [6206/10000], Loss: 73.5783920288086, Learning Rate: 0.01\n",
      "Epoch [6207/10000], Loss: 73.57242584228516, Learning Rate: 0.01\n",
      "Epoch [6208/10000], Loss: 73.56926727294922, Learning Rate: 0.01\n",
      "Epoch [6209/10000], Loss: 73.56752014160156, Learning Rate: 0.01\n",
      "Epoch [6210/10000], Loss: 73.56481170654297, Learning Rate: 0.01\n",
      "Epoch [6211/10000], Loss: 73.56031036376953, Learning Rate: 0.01\n",
      "Epoch [6212/10000], Loss: 73.55438232421875, Learning Rate: 0.01\n",
      "Epoch [6213/10000], Loss: 73.5485610961914, Learning Rate: 0.01\n",
      "Epoch [6214/10000], Loss: 73.54391479492188, Learning Rate: 0.01\n",
      "Epoch [6215/10000], Loss: 73.54057312011719, Learning Rate: 0.01\n",
      "Epoch [6216/10000], Loss: 73.53759002685547, Learning Rate: 0.01\n",
      "Epoch [6217/10000], Loss: 73.53407287597656, Learning Rate: 0.01\n",
      "Epoch [6218/10000], Loss: 73.52962493896484, Learning Rate: 0.01\n",
      "Epoch [6219/10000], Loss: 73.52465057373047, Learning Rate: 0.01\n",
      "Epoch [6220/10000], Loss: 73.51978302001953, Learning Rate: 0.01\n",
      "Epoch [6221/10000], Loss: 73.51551055908203, Learning Rate: 0.01\n",
      "Epoch [6222/10000], Loss: 73.51176452636719, Learning Rate: 0.01\n",
      "Epoch [6223/10000], Loss: 73.50827026367188, Learning Rate: 0.01\n",
      "Epoch [6224/10000], Loss: 73.50446319580078, Learning Rate: 0.01\n",
      "Epoch [6225/10000], Loss: 73.50028991699219, Learning Rate: 0.01\n",
      "Epoch [6226/10000], Loss: 73.49575805664062, Learning Rate: 0.01\n",
      "Epoch [6227/10000], Loss: 73.49126434326172, Learning Rate: 0.01\n",
      "Epoch [6228/10000], Loss: 73.48699188232422, Learning Rate: 0.01\n",
      "Epoch [6229/10000], Loss: 73.48310852050781, Learning Rate: 0.01\n",
      "Epoch [6230/10000], Loss: 73.4792709350586, Learning Rate: 0.01\n",
      "Epoch [6231/10000], Loss: 73.47540283203125, Learning Rate: 0.01\n",
      "Epoch [6232/10000], Loss: 73.47129821777344, Learning Rate: 0.01\n",
      "Epoch [6233/10000], Loss: 73.46710968017578, Learning Rate: 0.01\n",
      "Epoch [6234/10000], Loss: 73.46273803710938, Learning Rate: 0.01\n",
      "Epoch [6235/10000], Loss: 73.45852661132812, Learning Rate: 0.01\n",
      "Epoch [6236/10000], Loss: 73.4545669555664, Learning Rate: 0.01\n",
      "Epoch [6237/10000], Loss: 73.4505844116211, Learning Rate: 0.01\n",
      "Epoch [6238/10000], Loss: 73.44662475585938, Learning Rate: 0.01\n",
      "Epoch [6239/10000], Loss: 73.442626953125, Learning Rate: 0.01\n",
      "Epoch [6240/10000], Loss: 73.43852996826172, Learning Rate: 0.01\n",
      "Epoch [6241/10000], Loss: 73.43431091308594, Learning Rate: 0.01\n",
      "Epoch [6242/10000], Loss: 73.4301528930664, Learning Rate: 0.01\n",
      "Epoch [6243/10000], Loss: 73.42610931396484, Learning Rate: 0.01\n",
      "Epoch [6244/10000], Loss: 73.42207336425781, Learning Rate: 0.01\n",
      "Epoch [6245/10000], Loss: 73.41805267333984, Learning Rate: 0.01\n",
      "Epoch [6246/10000], Loss: 73.41402435302734, Learning Rate: 0.01\n",
      "Epoch [6247/10000], Loss: 73.40999603271484, Learning Rate: 0.01\n",
      "Epoch [6248/10000], Loss: 73.40580749511719, Learning Rate: 0.01\n",
      "Epoch [6249/10000], Loss: 73.40174865722656, Learning Rate: 0.01\n",
      "Epoch [6250/10000], Loss: 73.39763641357422, Learning Rate: 0.01\n",
      "Epoch [6251/10000], Loss: 73.39360046386719, Learning Rate: 0.01\n",
      "Epoch [6252/10000], Loss: 73.38958740234375, Learning Rate: 0.01\n",
      "Epoch [6253/10000], Loss: 73.38551330566406, Learning Rate: 0.01\n",
      "Epoch [6254/10000], Loss: 73.38156127929688, Learning Rate: 0.01\n",
      "Epoch [6255/10000], Loss: 73.37739562988281, Learning Rate: 0.01\n",
      "Epoch [6256/10000], Loss: 73.37336730957031, Learning Rate: 0.01\n",
      "Epoch [6257/10000], Loss: 73.36933898925781, Learning Rate: 0.01\n",
      "Epoch [6258/10000], Loss: 73.3652114868164, Learning Rate: 0.01\n",
      "Epoch [6259/10000], Loss: 73.36113739013672, Learning Rate: 0.01\n",
      "Epoch [6260/10000], Loss: 73.35704803466797, Learning Rate: 0.01\n",
      "Epoch [6261/10000], Loss: 73.35296630859375, Learning Rate: 0.01\n",
      "Epoch [6262/10000], Loss: 73.34899139404297, Learning Rate: 0.01\n",
      "Epoch [6263/10000], Loss: 73.34490966796875, Learning Rate: 0.01\n",
      "Epoch [6264/10000], Loss: 73.34085845947266, Learning Rate: 0.01\n",
      "Epoch [6265/10000], Loss: 73.33677673339844, Learning Rate: 0.01\n",
      "Epoch [6266/10000], Loss: 73.3326644897461, Learning Rate: 0.01\n",
      "Epoch [6267/10000], Loss: 73.32865142822266, Learning Rate: 0.01\n",
      "Epoch [6268/10000], Loss: 73.32461547851562, Learning Rate: 0.01\n",
      "Epoch [6269/10000], Loss: 73.3205795288086, Learning Rate: 0.01\n",
      "Epoch [6270/10000], Loss: 73.31646728515625, Learning Rate: 0.01\n",
      "Epoch [6271/10000], Loss: 73.31243133544922, Learning Rate: 0.01\n",
      "Epoch [6272/10000], Loss: 73.30836486816406, Learning Rate: 0.01\n",
      "Epoch [6273/10000], Loss: 73.30438232421875, Learning Rate: 0.01\n",
      "Epoch [6274/10000], Loss: 73.30023956298828, Learning Rate: 0.01\n",
      "Epoch [6275/10000], Loss: 73.29619598388672, Learning Rate: 0.01\n",
      "Epoch [6276/10000], Loss: 73.2921142578125, Learning Rate: 0.01\n",
      "Epoch [6277/10000], Loss: 73.28799438476562, Learning Rate: 0.01\n",
      "Epoch [6278/10000], Loss: 73.28398895263672, Learning Rate: 0.01\n",
      "Epoch [6279/10000], Loss: 73.27996063232422, Learning Rate: 0.01\n",
      "Epoch [6280/10000], Loss: 73.27590942382812, Learning Rate: 0.01\n",
      "Epoch [6281/10000], Loss: 73.27183532714844, Learning Rate: 0.01\n",
      "Epoch [6282/10000], Loss: 73.26776885986328, Learning Rate: 0.01\n",
      "Epoch [6283/10000], Loss: 73.26367950439453, Learning Rate: 0.01\n",
      "Epoch [6284/10000], Loss: 73.25965881347656, Learning Rate: 0.01\n",
      "Epoch [6285/10000], Loss: 73.25558471679688, Learning Rate: 0.01\n",
      "Epoch [6286/10000], Loss: 73.25161743164062, Learning Rate: 0.01\n",
      "Epoch [6287/10000], Loss: 73.24748229980469, Learning Rate: 0.01\n",
      "Epoch [6288/10000], Loss: 73.243408203125, Learning Rate: 0.01\n",
      "Epoch [6289/10000], Loss: 73.23934936523438, Learning Rate: 0.01\n",
      "Epoch [6290/10000], Loss: 73.2353286743164, Learning Rate: 0.01\n",
      "Epoch [6291/10000], Loss: 73.23121643066406, Learning Rate: 0.01\n",
      "Epoch [6292/10000], Loss: 73.22725677490234, Learning Rate: 0.01\n",
      "Epoch [6293/10000], Loss: 73.22320556640625, Learning Rate: 0.01\n",
      "Epoch [6294/10000], Loss: 73.2191162109375, Learning Rate: 0.01\n",
      "Epoch [6295/10000], Loss: 73.21495819091797, Learning Rate: 0.01\n",
      "Epoch [6296/10000], Loss: 73.21102142333984, Learning Rate: 0.01\n",
      "Epoch [6297/10000], Loss: 73.20690155029297, Learning Rate: 0.01\n",
      "Epoch [6298/10000], Loss: 73.2028579711914, Learning Rate: 0.01\n",
      "Epoch [6299/10000], Loss: 73.19880676269531, Learning Rate: 0.01\n",
      "Epoch [6300/10000], Loss: 73.19477081298828, Learning Rate: 0.01\n",
      "Epoch [6301/10000], Loss: 73.19068145751953, Learning Rate: 0.01\n",
      "Epoch [6302/10000], Loss: 73.18663024902344, Learning Rate: 0.01\n",
      "Epoch [6303/10000], Loss: 73.1825942993164, Learning Rate: 0.01\n",
      "Epoch [6304/10000], Loss: 73.17854309082031, Learning Rate: 0.01\n",
      "Epoch [6305/10000], Loss: 73.17453002929688, Learning Rate: 0.01\n",
      "Epoch [6306/10000], Loss: 73.17036437988281, Learning Rate: 0.01\n",
      "Epoch [6307/10000], Loss: 73.16631317138672, Learning Rate: 0.01\n",
      "Epoch [6308/10000], Loss: 73.16231536865234, Learning Rate: 0.01\n",
      "Epoch [6309/10000], Loss: 73.15829467773438, Learning Rate: 0.01\n",
      "Epoch [6310/10000], Loss: 73.15421295166016, Learning Rate: 0.01\n",
      "Epoch [6311/10000], Loss: 73.15019989013672, Learning Rate: 0.01\n",
      "Epoch [6312/10000], Loss: 73.14605712890625, Learning Rate: 0.01\n",
      "Epoch [6313/10000], Loss: 73.14202880859375, Learning Rate: 0.01\n",
      "Epoch [6314/10000], Loss: 73.13803100585938, Learning Rate: 0.01\n",
      "Epoch [6315/10000], Loss: 73.13398742675781, Learning Rate: 0.01\n",
      "Epoch [6316/10000], Loss: 73.12991333007812, Learning Rate: 0.01\n",
      "Epoch [6317/10000], Loss: 73.1258544921875, Learning Rate: 0.01\n",
      "Epoch [6318/10000], Loss: 73.12171936035156, Learning Rate: 0.01\n",
      "Epoch [6319/10000], Loss: 73.11774444580078, Learning Rate: 0.01\n",
      "Epoch [6320/10000], Loss: 73.11368560791016, Learning Rate: 0.01\n",
      "Epoch [6321/10000], Loss: 73.10958862304688, Learning Rate: 0.01\n",
      "Epoch [6322/10000], Loss: 73.10555267333984, Learning Rate: 0.01\n",
      "Epoch [6323/10000], Loss: 73.10150909423828, Learning Rate: 0.01\n",
      "Epoch [6324/10000], Loss: 73.09750366210938, Learning Rate: 0.01\n",
      "Epoch [6325/10000], Loss: 73.09336853027344, Learning Rate: 0.01\n",
      "Epoch [6326/10000], Loss: 73.0893783569336, Learning Rate: 0.01\n",
      "Epoch [6327/10000], Loss: 73.08528900146484, Learning Rate: 0.01\n",
      "Epoch [6328/10000], Loss: 73.08124542236328, Learning Rate: 0.01\n",
      "Epoch [6329/10000], Loss: 73.07720947265625, Learning Rate: 0.01\n",
      "Epoch [6330/10000], Loss: 73.07312774658203, Learning Rate: 0.01\n",
      "Epoch [6331/10000], Loss: 73.06903839111328, Learning Rate: 0.01\n",
      "Epoch [6332/10000], Loss: 73.06504821777344, Learning Rate: 0.01\n",
      "Epoch [6333/10000], Loss: 73.06096649169922, Learning Rate: 0.01\n",
      "Epoch [6334/10000], Loss: 73.0569839477539, Learning Rate: 0.01\n",
      "Epoch [6335/10000], Loss: 73.0528793334961, Learning Rate: 0.01\n",
      "Epoch [6336/10000], Loss: 73.04888153076172, Learning Rate: 0.01\n",
      "Epoch [6337/10000], Loss: 73.04480743408203, Learning Rate: 0.01\n",
      "Epoch [6338/10000], Loss: 73.04075622558594, Learning Rate: 0.01\n",
      "Epoch [6339/10000], Loss: 73.03665924072266, Learning Rate: 0.01\n",
      "Epoch [6340/10000], Loss: 73.03266906738281, Learning Rate: 0.01\n",
      "Epoch [6341/10000], Loss: 73.02864837646484, Learning Rate: 0.01\n",
      "Epoch [6342/10000], Loss: 73.02450561523438, Learning Rate: 0.01\n",
      "Epoch [6343/10000], Loss: 73.02055358886719, Learning Rate: 0.01\n",
      "Epoch [6344/10000], Loss: 73.01648712158203, Learning Rate: 0.01\n",
      "Epoch [6345/10000], Loss: 73.0124282836914, Learning Rate: 0.01\n",
      "Epoch [6346/10000], Loss: 73.00836181640625, Learning Rate: 0.01\n",
      "Epoch [6347/10000], Loss: 73.00431060791016, Learning Rate: 0.01\n",
      "Epoch [6348/10000], Loss: 73.00023651123047, Learning Rate: 0.01\n",
      "Epoch [6349/10000], Loss: 72.9962158203125, Learning Rate: 0.01\n",
      "Epoch [6350/10000], Loss: 72.99217987060547, Learning Rate: 0.01\n",
      "Epoch [6351/10000], Loss: 72.9881591796875, Learning Rate: 0.01\n",
      "Epoch [6352/10000], Loss: 72.98405456542969, Learning Rate: 0.01\n",
      "Epoch [6353/10000], Loss: 72.98009490966797, Learning Rate: 0.01\n",
      "Epoch [6354/10000], Loss: 72.97598266601562, Learning Rate: 0.01\n",
      "Epoch [6355/10000], Loss: 72.97196197509766, Learning Rate: 0.01\n",
      "Epoch [6356/10000], Loss: 72.9678955078125, Learning Rate: 0.01\n",
      "Epoch [6357/10000], Loss: 72.96383666992188, Learning Rate: 0.01\n",
      "Epoch [6358/10000], Loss: 72.95978546142578, Learning Rate: 0.01\n",
      "Epoch [6359/10000], Loss: 72.95579528808594, Learning Rate: 0.01\n",
      "Epoch [6360/10000], Loss: 72.95169830322266, Learning Rate: 0.01\n",
      "Epoch [6361/10000], Loss: 72.94767761230469, Learning Rate: 0.01\n",
      "Epoch [6362/10000], Loss: 72.94364166259766, Learning Rate: 0.01\n",
      "Epoch [6363/10000], Loss: 72.93955993652344, Learning Rate: 0.01\n",
      "Epoch [6364/10000], Loss: 72.93557739257812, Learning Rate: 0.01\n",
      "Epoch [6365/10000], Loss: 72.9315414428711, Learning Rate: 0.01\n",
      "Epoch [6366/10000], Loss: 72.9275131225586, Learning Rate: 0.01\n",
      "Epoch [6367/10000], Loss: 72.92353820800781, Learning Rate: 0.01\n",
      "Epoch [6368/10000], Loss: 72.9195556640625, Learning Rate: 0.01\n",
      "Epoch [6369/10000], Loss: 72.91571807861328, Learning Rate: 0.01\n",
      "Epoch [6370/10000], Loss: 72.91188049316406, Learning Rate: 0.01\n",
      "Epoch [6371/10000], Loss: 72.9083023071289, Learning Rate: 0.01\n",
      "Epoch [6372/10000], Loss: 72.9048843383789, Learning Rate: 0.01\n",
      "Epoch [6373/10000], Loss: 72.90188598632812, Learning Rate: 0.01\n",
      "Epoch [6374/10000], Loss: 72.89936065673828, Learning Rate: 0.01\n",
      "Epoch [6375/10000], Loss: 72.89789581298828, Learning Rate: 0.01\n",
      "Epoch [6376/10000], Loss: 72.89808654785156, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6377/10000], Loss: 72.9007797241211, Learning Rate: 0.01\n",
      "Epoch [6378/10000], Loss: 72.90796661376953, Learning Rate: 0.01\n",
      "Epoch [6379/10000], Loss: 72.92235565185547, Learning Rate: 0.01\n",
      "Epoch [6380/10000], Loss: 72.9488296508789, Learning Rate: 0.01\n",
      "Epoch [6381/10000], Loss: 72.99547576904297, Learning Rate: 0.01\n",
      "Epoch [6382/10000], Loss: 73.07598114013672, Learning Rate: 0.01\n",
      "Epoch [6383/10000], Loss: 73.21308898925781, Learning Rate: 0.01\n",
      "Epoch [6384/10000], Loss: 73.4439926147461, Learning Rate: 0.01\n",
      "Epoch [6385/10000], Loss: 73.82705688476562, Learning Rate: 0.01\n",
      "Epoch [6386/10000], Loss: 74.45059204101562, Learning Rate: 0.01\n",
      "Epoch [6387/10000], Loss: 75.42431640625, Learning Rate: 0.01\n",
      "Epoch [6388/10000], Loss: 76.85418701171875, Learning Rate: 0.01\n",
      "Epoch [6389/10000], Loss: 78.7044448852539, Learning Rate: 0.01\n",
      "Epoch [6390/10000], Loss: 80.62244415283203, Learning Rate: 0.01\n",
      "Epoch [6391/10000], Loss: 81.69332122802734, Learning Rate: 0.01\n",
      "Epoch [6392/10000], Loss: 80.88348388671875, Learning Rate: 0.01\n",
      "Epoch [6393/10000], Loss: 77.99655151367188, Learning Rate: 0.01\n",
      "Epoch [6394/10000], Loss: 74.60420227050781, Learning Rate: 0.01\n",
      "Epoch [6395/10000], Loss: 72.85612487792969, Learning Rate: 0.01\n",
      "Epoch [6396/10000], Loss: 73.518310546875, Learning Rate: 0.01\n",
      "Epoch [6397/10000], Loss: 75.40615844726562, Learning Rate: 0.01\n",
      "Epoch [6398/10000], Loss: 76.57449340820312, Learning Rate: 0.01\n",
      "Epoch [6399/10000], Loss: 75.94561004638672, Learning Rate: 0.01\n",
      "Epoch [6400/10000], Loss: 74.1557388305664, Learning Rate: 0.01\n",
      "Epoch [6401/10000], Loss: 72.8835678100586, Learning Rate: 0.01\n",
      "Epoch [6402/10000], Loss: 73.0771255493164, Learning Rate: 0.01\n",
      "Epoch [6403/10000], Loss: 74.13875579833984, Learning Rate: 0.01\n",
      "Epoch [6404/10000], Loss: 74.76174926757812, Learning Rate: 0.01\n",
      "Epoch [6405/10000], Loss: 74.29288482666016, Learning Rate: 0.01\n",
      "Epoch [6406/10000], Loss: 73.28851318359375, Learning Rate: 0.01\n",
      "Epoch [6407/10000], Loss: 72.76516723632812, Learning Rate: 0.01\n",
      "Epoch [6408/10000], Loss: 73.07933044433594, Learning Rate: 0.01\n",
      "Epoch [6409/10000], Loss: 73.67418670654297, Learning Rate: 0.01\n",
      "Epoch [6410/10000], Loss: 73.8154525756836, Learning Rate: 0.01\n",
      "Epoch [6411/10000], Loss: 73.37570190429688, Learning Rate: 0.01\n",
      "Epoch [6412/10000], Loss: 72.85946655273438, Learning Rate: 0.01\n",
      "Epoch [6413/10000], Loss: 72.7668228149414, Learning Rate: 0.01\n",
      "Epoch [6414/10000], Loss: 73.06578826904297, Learning Rate: 0.01\n",
      "Epoch [6415/10000], Loss: 73.32624816894531, Learning Rate: 0.01\n",
      "Epoch [6416/10000], Loss: 73.24351501464844, Learning Rate: 0.01\n",
      "Epoch [6417/10000], Loss: 72.93114471435547, Learning Rate: 0.01\n",
      "Epoch [6418/10000], Loss: 72.72652435302734, Learning Rate: 0.01\n",
      "Epoch [6419/10000], Loss: 72.79293060302734, Learning Rate: 0.01\n",
      "Epoch [6420/10000], Loss: 72.98196411132812, Learning Rate: 0.01\n",
      "Epoch [6421/10000], Loss: 73.04824829101562, Learning Rate: 0.01\n",
      "Epoch [6422/10000], Loss: 72.92256164550781, Learning Rate: 0.01\n",
      "Epoch [6423/10000], Loss: 72.75067138671875, Learning Rate: 0.01\n",
      "Epoch [6424/10000], Loss: 72.70013427734375, Learning Rate: 0.01\n",
      "Epoch [6425/10000], Loss: 72.78236389160156, Learning Rate: 0.01\n",
      "Epoch [6426/10000], Loss: 72.87095642089844, Learning Rate: 0.01\n",
      "Epoch [6427/10000], Loss: 72.8584213256836, Learning Rate: 0.01\n",
      "Epoch [6428/10000], Loss: 72.76283264160156, Learning Rate: 0.01\n",
      "Epoch [6429/10000], Loss: 72.68450927734375, Learning Rate: 0.01\n",
      "Epoch [6430/10000], Loss: 72.68788146972656, Learning Rate: 0.01\n",
      "Epoch [6431/10000], Loss: 72.74279022216797, Learning Rate: 0.01\n",
      "Epoch [6432/10000], Loss: 72.77352142333984, Learning Rate: 0.01\n",
      "Epoch [6433/10000], Loss: 72.74410247802734, Learning Rate: 0.01\n",
      "Epoch [6434/10000], Loss: 72.68620300292969, Learning Rate: 0.01\n",
      "Epoch [6435/10000], Loss: 72.65448760986328, Learning Rate: 0.01\n",
      "Epoch [6436/10000], Loss: 72.6673355102539, Learning Rate: 0.01\n",
      "Epoch [6437/10000], Loss: 72.6959457397461, Learning Rate: 0.01\n",
      "Epoch [6438/10000], Loss: 72.70198059082031, Learning Rate: 0.01\n",
      "Epoch [6439/10000], Loss: 72.67691040039062, Learning Rate: 0.01\n",
      "Epoch [6440/10000], Loss: 72.64461517333984, Learning Rate: 0.01\n",
      "Epoch [6441/10000], Loss: 72.631591796875, Learning Rate: 0.01\n",
      "Epoch [6442/10000], Loss: 72.64082336425781, Learning Rate: 0.01\n",
      "Epoch [6443/10000], Loss: 72.65352630615234, Learning Rate: 0.01\n",
      "Epoch [6444/10000], Loss: 72.65142822265625, Learning Rate: 0.01\n",
      "Epoch [6445/10000], Loss: 72.63407897949219, Learning Rate: 0.01\n",
      "Epoch [6446/10000], Loss: 72.61575317382812, Learning Rate: 0.01\n",
      "Epoch [6447/10000], Loss: 72.60892486572266, Learning Rate: 0.01\n",
      "Epoch [6448/10000], Loss: 72.61307525634766, Learning Rate: 0.01\n",
      "Epoch [6449/10000], Loss: 72.61772155761719, Learning Rate: 0.01\n",
      "Epoch [6450/10000], Loss: 72.61380767822266, Learning Rate: 0.01\n",
      "Epoch [6451/10000], Loss: 72.60234069824219, Learning Rate: 0.01\n",
      "Epoch [6452/10000], Loss: 72.59091186523438, Learning Rate: 0.01\n",
      "Epoch [6453/10000], Loss: 72.58584594726562, Learning Rate: 0.01\n",
      "Epoch [6454/10000], Loss: 72.58633422851562, Learning Rate: 0.01\n",
      "Epoch [6455/10000], Loss: 72.58695220947266, Learning Rate: 0.01\n",
      "Epoch [6456/10000], Loss: 72.58309936523438, Learning Rate: 0.01\n",
      "Epoch [6457/10000], Loss: 72.57535552978516, Learning Rate: 0.01\n",
      "Epoch [6458/10000], Loss: 72.56737518310547, Learning Rate: 0.01\n",
      "Epoch [6459/10000], Loss: 72.5627212524414, Learning Rate: 0.01\n",
      "Epoch [6460/10000], Loss: 72.56095123291016, Learning Rate: 0.01\n",
      "Epoch [6461/10000], Loss: 72.5596923828125, Learning Rate: 0.01\n",
      "Epoch [6462/10000], Loss: 72.55622100830078, Learning Rate: 0.01\n",
      "Epoch [6463/10000], Loss: 72.55044555664062, Learning Rate: 0.01\n",
      "Epoch [6464/10000], Loss: 72.54427337646484, Learning Rate: 0.01\n",
      "Epoch [6465/10000], Loss: 72.53965759277344, Learning Rate: 0.01\n",
      "Epoch [6466/10000], Loss: 72.5367202758789, Learning Rate: 0.01\n",
      "Epoch [6467/10000], Loss: 72.53428649902344, Learning Rate: 0.01\n",
      "Epoch [6468/10000], Loss: 72.53096008300781, Learning Rate: 0.01\n",
      "Epoch [6469/10000], Loss: 72.52644348144531, Learning Rate: 0.01\n",
      "Epoch [6470/10000], Loss: 72.5213851928711, Learning Rate: 0.01\n",
      "Epoch [6471/10000], Loss: 72.51679992675781, Learning Rate: 0.01\n",
      "Epoch [6472/10000], Loss: 72.51317596435547, Learning Rate: 0.01\n",
      "Epoch [6473/10000], Loss: 72.510009765625, Learning Rate: 0.01\n",
      "Epoch [6474/10000], Loss: 72.50672912597656, Learning Rate: 0.01\n",
      "Epoch [6475/10000], Loss: 72.50287628173828, Learning Rate: 0.01\n",
      "Epoch [6476/10000], Loss: 72.4984130859375, Learning Rate: 0.01\n",
      "Epoch [6477/10000], Loss: 72.49405670166016, Learning Rate: 0.01\n",
      "Epoch [6478/10000], Loss: 72.49018096923828, Learning Rate: 0.01\n",
      "Epoch [6479/10000], Loss: 72.48656463623047, Learning Rate: 0.01\n",
      "Epoch [6480/10000], Loss: 72.48306274414062, Learning Rate: 0.01\n",
      "Epoch [6481/10000], Loss: 72.47935485839844, Learning Rate: 0.01\n",
      "Epoch [6482/10000], Loss: 72.47541809082031, Learning Rate: 0.01\n",
      "Epoch [6483/10000], Loss: 72.47123718261719, Learning Rate: 0.01\n",
      "Epoch [6484/10000], Loss: 72.46723175048828, Learning Rate: 0.01\n",
      "Epoch [6485/10000], Loss: 72.46343994140625, Learning Rate: 0.01\n",
      "Epoch [6486/10000], Loss: 72.4598159790039, Learning Rate: 0.01\n",
      "Epoch [6487/10000], Loss: 72.45616149902344, Learning Rate: 0.01\n",
      "Epoch [6488/10000], Loss: 72.45235443115234, Learning Rate: 0.01\n",
      "Epoch [6489/10000], Loss: 72.44844055175781, Learning Rate: 0.01\n",
      "Epoch [6490/10000], Loss: 72.44446563720703, Learning Rate: 0.01\n",
      "Epoch [6491/10000], Loss: 72.44055938720703, Learning Rate: 0.01\n",
      "Epoch [6492/10000], Loss: 72.4367904663086, Learning Rate: 0.01\n",
      "Epoch [6493/10000], Loss: 72.43306732177734, Learning Rate: 0.01\n",
      "Epoch [6494/10000], Loss: 72.42928314208984, Learning Rate: 0.01\n",
      "Epoch [6495/10000], Loss: 72.42550659179688, Learning Rate: 0.01\n",
      "Epoch [6496/10000], Loss: 72.42164611816406, Learning Rate: 0.01\n",
      "Epoch [6497/10000], Loss: 72.41773223876953, Learning Rate: 0.01\n",
      "Epoch [6498/10000], Loss: 72.41390228271484, Learning Rate: 0.01\n",
      "Epoch [6499/10000], Loss: 72.4101333618164, Learning Rate: 0.01\n",
      "Epoch [6500/10000], Loss: 72.40635681152344, Learning Rate: 0.01\n",
      "Epoch [6501/10000], Loss: 72.4025650024414, Learning Rate: 0.01\n",
      "Epoch [6502/10000], Loss: 72.39875030517578, Learning Rate: 0.01\n",
      "Epoch [6503/10000], Loss: 72.39496612548828, Learning Rate: 0.01\n",
      "Epoch [6504/10000], Loss: 72.39114379882812, Learning Rate: 0.01\n",
      "Epoch [6505/10000], Loss: 72.38729095458984, Learning Rate: 0.01\n",
      "Epoch [6506/10000], Loss: 72.38341522216797, Learning Rate: 0.01\n",
      "Epoch [6507/10000], Loss: 72.37968444824219, Learning Rate: 0.01\n",
      "Epoch [6508/10000], Loss: 72.37580108642578, Learning Rate: 0.01\n",
      "Epoch [6509/10000], Loss: 72.37206268310547, Learning Rate: 0.01\n",
      "Epoch [6510/10000], Loss: 72.36822509765625, Learning Rate: 0.01\n",
      "Epoch [6511/10000], Loss: 72.36441802978516, Learning Rate: 0.01\n",
      "Epoch [6512/10000], Loss: 72.36062622070312, Learning Rate: 0.01\n",
      "Epoch [6513/10000], Loss: 72.35678100585938, Learning Rate: 0.01\n",
      "Epoch [6514/10000], Loss: 72.35303497314453, Learning Rate: 0.01\n",
      "Epoch [6515/10000], Loss: 72.3492431640625, Learning Rate: 0.01\n",
      "Epoch [6516/10000], Loss: 72.34535217285156, Learning Rate: 0.01\n",
      "Epoch [6517/10000], Loss: 72.34159851074219, Learning Rate: 0.01\n",
      "Epoch [6518/10000], Loss: 72.33776092529297, Learning Rate: 0.01\n",
      "Epoch [6519/10000], Loss: 72.33397674560547, Learning Rate: 0.01\n",
      "Epoch [6520/10000], Loss: 72.33024597167969, Learning Rate: 0.01\n",
      "Epoch [6521/10000], Loss: 72.32637023925781, Learning Rate: 0.01\n",
      "Epoch [6522/10000], Loss: 72.32264709472656, Learning Rate: 0.01\n",
      "Epoch [6523/10000], Loss: 72.31880950927734, Learning Rate: 0.01\n",
      "Epoch [6524/10000], Loss: 72.31501770019531, Learning Rate: 0.01\n",
      "Epoch [6525/10000], Loss: 72.3112564086914, Learning Rate: 0.01\n",
      "Epoch [6526/10000], Loss: 72.30744171142578, Learning Rate: 0.01\n",
      "Epoch [6527/10000], Loss: 72.30361938476562, Learning Rate: 0.01\n",
      "Epoch [6528/10000], Loss: 72.29983520507812, Learning Rate: 0.01\n",
      "Epoch [6529/10000], Loss: 72.29601287841797, Learning Rate: 0.01\n",
      "Epoch [6530/10000], Loss: 72.2922592163086, Learning Rate: 0.01\n",
      "Epoch [6531/10000], Loss: 72.2884521484375, Learning Rate: 0.01\n",
      "Epoch [6532/10000], Loss: 72.28461456298828, Learning Rate: 0.01\n",
      "Epoch [6533/10000], Loss: 72.28077697753906, Learning Rate: 0.01\n",
      "Epoch [6534/10000], Loss: 72.2769775390625, Learning Rate: 0.01\n",
      "Epoch [6535/10000], Loss: 72.27323150634766, Learning Rate: 0.01\n",
      "Epoch [6536/10000], Loss: 72.26934814453125, Learning Rate: 0.01\n",
      "Epoch [6537/10000], Loss: 72.2656021118164, Learning Rate: 0.01\n",
      "Epoch [6538/10000], Loss: 72.26185607910156, Learning Rate: 0.01\n",
      "Epoch [6539/10000], Loss: 72.25801849365234, Learning Rate: 0.01\n",
      "Epoch [6540/10000], Loss: 72.25420379638672, Learning Rate: 0.01\n",
      "Epoch [6541/10000], Loss: 72.25042724609375, Learning Rate: 0.01\n",
      "Epoch [6542/10000], Loss: 72.24662017822266, Learning Rate: 0.01\n",
      "Epoch [6543/10000], Loss: 72.24284362792969, Learning Rate: 0.01\n",
      "Epoch [6544/10000], Loss: 72.23905944824219, Learning Rate: 0.01\n",
      "Epoch [6545/10000], Loss: 72.2352066040039, Learning Rate: 0.01\n",
      "Epoch [6546/10000], Loss: 72.23141479492188, Learning Rate: 0.01\n",
      "Epoch [6547/10000], Loss: 72.22764587402344, Learning Rate: 0.01\n",
      "Epoch [6548/10000], Loss: 72.22383117675781, Learning Rate: 0.01\n",
      "Epoch [6549/10000], Loss: 72.22010803222656, Learning Rate: 0.01\n",
      "Epoch [6550/10000], Loss: 72.21627044677734, Learning Rate: 0.01\n",
      "Epoch [6551/10000], Loss: 72.21244812011719, Learning Rate: 0.01\n",
      "Epoch [6552/10000], Loss: 72.20865631103516, Learning Rate: 0.01\n",
      "Epoch [6553/10000], Loss: 72.20491790771484, Learning Rate: 0.01\n",
      "Epoch [6554/10000], Loss: 72.2010726928711, Learning Rate: 0.01\n",
      "Epoch [6555/10000], Loss: 72.19731140136719, Learning Rate: 0.01\n",
      "Epoch [6556/10000], Loss: 72.1935806274414, Learning Rate: 0.01\n",
      "Epoch [6557/10000], Loss: 72.18972778320312, Learning Rate: 0.01\n",
      "Epoch [6558/10000], Loss: 72.18589782714844, Learning Rate: 0.01\n",
      "Epoch [6559/10000], Loss: 72.18216705322266, Learning Rate: 0.01\n",
      "Epoch [6560/10000], Loss: 72.1783218383789, Learning Rate: 0.01\n",
      "Epoch [6561/10000], Loss: 72.17466735839844, Learning Rate: 0.01\n",
      "Epoch [6562/10000], Loss: 72.17073822021484, Learning Rate: 0.01\n",
      "Epoch [6563/10000], Loss: 72.16698455810547, Learning Rate: 0.01\n",
      "Epoch [6564/10000], Loss: 72.16320037841797, Learning Rate: 0.01\n",
      "Epoch [6565/10000], Loss: 72.15946197509766, Learning Rate: 0.01\n",
      "Epoch [6566/10000], Loss: 72.15562438964844, Learning Rate: 0.01\n",
      "Epoch [6567/10000], Loss: 72.15178680419922, Learning Rate: 0.01\n",
      "Epoch [6568/10000], Loss: 72.14814758300781, Learning Rate: 0.01\n",
      "Epoch [6569/10000], Loss: 72.14424133300781, Learning Rate: 0.01\n",
      "Epoch [6570/10000], Loss: 72.14042663574219, Learning Rate: 0.01\n",
      "Epoch [6571/10000], Loss: 72.13671112060547, Learning Rate: 0.01\n",
      "Epoch [6572/10000], Loss: 72.13299560546875, Learning Rate: 0.01\n",
      "Epoch [6573/10000], Loss: 72.12915802001953, Learning Rate: 0.01\n",
      "Epoch [6574/10000], Loss: 72.12535095214844, Learning Rate: 0.01\n",
      "Epoch [6575/10000], Loss: 72.12154388427734, Learning Rate: 0.01\n",
      "Epoch [6576/10000], Loss: 72.1178207397461, Learning Rate: 0.01\n",
      "Epoch [6577/10000], Loss: 72.114013671875, Learning Rate: 0.01\n",
      "Epoch [6578/10000], Loss: 72.11019134521484, Learning Rate: 0.01\n",
      "Epoch [6579/10000], Loss: 72.1064682006836, Learning Rate: 0.01\n",
      "Epoch [6580/10000], Loss: 72.10265350341797, Learning Rate: 0.01\n",
      "Epoch [6581/10000], Loss: 72.09892272949219, Learning Rate: 0.01\n",
      "Epoch [6582/10000], Loss: 72.09510040283203, Learning Rate: 0.01\n",
      "Epoch [6583/10000], Loss: 72.09129333496094, Learning Rate: 0.01\n",
      "Epoch [6584/10000], Loss: 72.08750915527344, Learning Rate: 0.01\n",
      "Epoch [6585/10000], Loss: 72.08374786376953, Learning Rate: 0.01\n",
      "Epoch [6586/10000], Loss: 72.0799789428711, Learning Rate: 0.01\n",
      "Epoch [6587/10000], Loss: 72.0761947631836, Learning Rate: 0.01\n",
      "Epoch [6588/10000], Loss: 72.07237243652344, Learning Rate: 0.01\n",
      "Epoch [6589/10000], Loss: 72.068603515625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6590/10000], Loss: 72.06488037109375, Learning Rate: 0.01\n",
      "Epoch [6591/10000], Loss: 72.06109619140625, Learning Rate: 0.01\n",
      "Epoch [6592/10000], Loss: 72.05728912353516, Learning Rate: 0.01\n",
      "Epoch [6593/10000], Loss: 72.05352783203125, Learning Rate: 0.01\n",
      "Epoch [6594/10000], Loss: 72.0497817993164, Learning Rate: 0.01\n",
      "Epoch [6595/10000], Loss: 72.04598999023438, Learning Rate: 0.01\n",
      "Epoch [6596/10000], Loss: 72.04224395751953, Learning Rate: 0.01\n",
      "Epoch [6597/10000], Loss: 72.03839111328125, Learning Rate: 0.01\n",
      "Epoch [6598/10000], Loss: 72.03472137451172, Learning Rate: 0.01\n",
      "Epoch [6599/10000], Loss: 72.03089904785156, Learning Rate: 0.01\n",
      "Epoch [6600/10000], Loss: 72.02709197998047, Learning Rate: 0.01\n",
      "Epoch [6601/10000], Loss: 72.02332305908203, Learning Rate: 0.01\n",
      "Epoch [6602/10000], Loss: 72.01956939697266, Learning Rate: 0.01\n",
      "Epoch [6603/10000], Loss: 72.01588439941406, Learning Rate: 0.01\n",
      "Epoch [6604/10000], Loss: 72.01206970214844, Learning Rate: 0.01\n",
      "Epoch [6605/10000], Loss: 72.0082778930664, Learning Rate: 0.01\n",
      "Epoch [6606/10000], Loss: 72.00447845458984, Learning Rate: 0.01\n",
      "Epoch [6607/10000], Loss: 72.00067138671875, Learning Rate: 0.01\n",
      "Epoch [6608/10000], Loss: 71.99691009521484, Learning Rate: 0.01\n",
      "Epoch [6609/10000], Loss: 71.99317169189453, Learning Rate: 0.01\n",
      "Epoch [6610/10000], Loss: 71.98939514160156, Learning Rate: 0.01\n",
      "Epoch [6611/10000], Loss: 71.98565673828125, Learning Rate: 0.01\n",
      "Epoch [6612/10000], Loss: 71.98191833496094, Learning Rate: 0.01\n",
      "Epoch [6613/10000], Loss: 71.9781494140625, Learning Rate: 0.01\n",
      "Epoch [6614/10000], Loss: 71.97432708740234, Learning Rate: 0.01\n",
      "Epoch [6615/10000], Loss: 71.97055053710938, Learning Rate: 0.01\n",
      "Epoch [6616/10000], Loss: 71.96673583984375, Learning Rate: 0.01\n",
      "Epoch [6617/10000], Loss: 71.96304321289062, Learning Rate: 0.01\n",
      "Epoch [6618/10000], Loss: 71.95930480957031, Learning Rate: 0.01\n",
      "Epoch [6619/10000], Loss: 71.95547485351562, Learning Rate: 0.01\n",
      "Epoch [6620/10000], Loss: 71.95172882080078, Learning Rate: 0.01\n",
      "Epoch [6621/10000], Loss: 71.94803619384766, Learning Rate: 0.01\n",
      "Epoch [6622/10000], Loss: 71.94419860839844, Learning Rate: 0.01\n",
      "Epoch [6623/10000], Loss: 71.94043731689453, Learning Rate: 0.01\n",
      "Epoch [6624/10000], Loss: 71.93673706054688, Learning Rate: 0.01\n",
      "Epoch [6625/10000], Loss: 71.93296813964844, Learning Rate: 0.01\n",
      "Epoch [6626/10000], Loss: 71.92918395996094, Learning Rate: 0.01\n",
      "Epoch [6627/10000], Loss: 71.92548370361328, Learning Rate: 0.01\n",
      "Epoch [6628/10000], Loss: 71.92159271240234, Learning Rate: 0.01\n",
      "Epoch [6629/10000], Loss: 71.91790771484375, Learning Rate: 0.01\n",
      "Epoch [6630/10000], Loss: 71.91421508789062, Learning Rate: 0.01\n",
      "Epoch [6631/10000], Loss: 71.91038513183594, Learning Rate: 0.01\n",
      "Epoch [6632/10000], Loss: 71.90662384033203, Learning Rate: 0.01\n",
      "Epoch [6633/10000], Loss: 71.90288543701172, Learning Rate: 0.01\n",
      "Epoch [6634/10000], Loss: 71.89916229248047, Learning Rate: 0.01\n",
      "Epoch [6635/10000], Loss: 71.89541625976562, Learning Rate: 0.01\n",
      "Epoch [6636/10000], Loss: 71.89167022705078, Learning Rate: 0.01\n",
      "Epoch [6637/10000], Loss: 71.88792419433594, Learning Rate: 0.01\n",
      "Epoch [6638/10000], Loss: 71.88414001464844, Learning Rate: 0.01\n",
      "Epoch [6639/10000], Loss: 71.8804931640625, Learning Rate: 0.01\n",
      "Epoch [6640/10000], Loss: 71.876708984375, Learning Rate: 0.01\n",
      "Epoch [6641/10000], Loss: 71.87310028076172, Learning Rate: 0.01\n",
      "Epoch [6642/10000], Loss: 71.86936950683594, Learning Rate: 0.01\n",
      "Epoch [6643/10000], Loss: 71.86581420898438, Learning Rate: 0.01\n",
      "Epoch [6644/10000], Loss: 71.86224365234375, Learning Rate: 0.01\n",
      "Epoch [6645/10000], Loss: 71.85889434814453, Learning Rate: 0.01\n",
      "Epoch [6646/10000], Loss: 71.8555908203125, Learning Rate: 0.01\n",
      "Epoch [6647/10000], Loss: 71.8525619506836, Learning Rate: 0.01\n",
      "Epoch [6648/10000], Loss: 71.85001373291016, Learning Rate: 0.01\n",
      "Epoch [6649/10000], Loss: 71.8480224609375, Learning Rate: 0.01\n",
      "Epoch [6650/10000], Loss: 71.84712982177734, Learning Rate: 0.01\n",
      "Epoch [6651/10000], Loss: 71.84768676757812, Learning Rate: 0.01\n",
      "Epoch [6652/10000], Loss: 71.85074615478516, Learning Rate: 0.01\n",
      "Epoch [6653/10000], Loss: 71.85794067382812, Learning Rate: 0.01\n",
      "Epoch [6654/10000], Loss: 71.87151336669922, Learning Rate: 0.01\n",
      "Epoch [6655/10000], Loss: 71.89559173583984, Learning Rate: 0.01\n",
      "Epoch [6656/10000], Loss: 71.9364013671875, Learning Rate: 0.01\n",
      "Epoch [6657/10000], Loss: 72.00471496582031, Learning Rate: 0.01\n",
      "Epoch [6658/10000], Loss: 72.11723327636719, Learning Rate: 0.01\n",
      "Epoch [6659/10000], Loss: 72.30115509033203, Learning Rate: 0.01\n",
      "Epoch [6660/10000], Loss: 72.59884643554688, Learning Rate: 0.01\n",
      "Epoch [6661/10000], Loss: 73.07219696044922, Learning Rate: 0.01\n",
      "Epoch [6662/10000], Loss: 73.80831909179688, Learning Rate: 0.01\n",
      "Epoch [6663/10000], Loss: 74.89740753173828, Learning Rate: 0.01\n",
      "Epoch [6664/10000], Loss: 76.39759826660156, Learning Rate: 0.01\n",
      "Epoch [6665/10000], Loss: 78.18013000488281, Learning Rate: 0.01\n",
      "Epoch [6666/10000], Loss: 79.80647277832031, Learning Rate: 0.01\n",
      "Epoch [6667/10000], Loss: 80.39017486572266, Learning Rate: 0.01\n",
      "Epoch [6668/10000], Loss: 79.16694641113281, Learning Rate: 0.01\n",
      "Epoch [6669/10000], Loss: 76.26509094238281, Learning Rate: 0.01\n",
      "Epoch [6670/10000], Loss: 73.2215805053711, Learning Rate: 0.01\n",
      "Epoch [6671/10000], Loss: 71.78372955322266, Learning Rate: 0.01\n",
      "Epoch [6672/10000], Loss: 72.43955993652344, Learning Rate: 0.01\n",
      "Epoch [6673/10000], Loss: 74.1397476196289, Learning Rate: 0.01\n",
      "Epoch [6674/10000], Loss: 75.26436614990234, Learning Rate: 0.01\n",
      "Epoch [6675/10000], Loss: 74.86177062988281, Learning Rate: 0.01\n",
      "Epoch [6676/10000], Loss: 73.31987762451172, Learning Rate: 0.01\n",
      "Epoch [6677/10000], Loss: 71.98294067382812, Learning Rate: 0.01\n",
      "Epoch [6678/10000], Loss: 71.83096313476562, Learning Rate: 0.01\n",
      "Epoch [6679/10000], Loss: 72.65435028076172, Learning Rate: 0.01\n",
      "Epoch [6680/10000], Loss: 73.44075012207031, Learning Rate: 0.01\n",
      "Epoch [6681/10000], Loss: 73.39228820800781, Learning Rate: 0.01\n",
      "Epoch [6682/10000], Loss: 72.60985565185547, Learning Rate: 0.01\n",
      "Epoch [6683/10000], Loss: 71.86145782470703, Learning Rate: 0.01\n",
      "Epoch [6684/10000], Loss: 71.76213836669922, Learning Rate: 0.01\n",
      "Epoch [6685/10000], Loss: 72.21418762207031, Learning Rate: 0.01\n",
      "Epoch [6686/10000], Loss: 72.62998962402344, Learning Rate: 0.01\n",
      "Epoch [6687/10000], Loss: 72.57057189941406, Learning Rate: 0.01\n",
      "Epoch [6688/10000], Loss: 72.12723541259766, Learning Rate: 0.01\n",
      "Epoch [6689/10000], Loss: 71.74641418457031, Learning Rate: 0.01\n",
      "Epoch [6690/10000], Loss: 71.73663330078125, Learning Rate: 0.01\n",
      "Epoch [6691/10000], Loss: 71.99873352050781, Learning Rate: 0.01\n",
      "Epoch [6692/10000], Loss: 72.19831085205078, Learning Rate: 0.01\n",
      "Epoch [6693/10000], Loss: 72.12568664550781, Learning Rate: 0.01\n",
      "Epoch [6694/10000], Loss: 71.87236022949219, Learning Rate: 0.01\n",
      "Epoch [6695/10000], Loss: 71.6868896484375, Learning Rate: 0.01\n",
      "Epoch [6696/10000], Loss: 71.70883178710938, Learning Rate: 0.01\n",
      "Epoch [6697/10000], Loss: 71.85617065429688, Learning Rate: 0.01\n",
      "Epoch [6698/10000], Loss: 71.94630432128906, Learning Rate: 0.01\n",
      "Epoch [6699/10000], Loss: 71.88697814941406, Learning Rate: 0.01\n",
      "Epoch [6700/10000], Loss: 71.74534606933594, Learning Rate: 0.01\n",
      "Epoch [6701/10000], Loss: 71.65465545654297, Learning Rate: 0.01\n",
      "Epoch [6702/10000], Loss: 71.67588806152344, Learning Rate: 0.01\n",
      "Epoch [6703/10000], Loss: 71.75498962402344, Learning Rate: 0.01\n",
      "Epoch [6704/10000], Loss: 71.79571533203125, Learning Rate: 0.01\n",
      "Epoch [6705/10000], Loss: 71.75650787353516, Learning Rate: 0.01\n",
      "Epoch [6706/10000], Loss: 71.67842102050781, Learning Rate: 0.01\n",
      "Epoch [6707/10000], Loss: 71.63096618652344, Learning Rate: 0.01\n",
      "Epoch [6708/10000], Loss: 71.64268493652344, Learning Rate: 0.01\n",
      "Epoch [6709/10000], Loss: 71.68343353271484, Learning Rate: 0.01\n",
      "Epoch [6710/10000], Loss: 71.70304107666016, Learning Rate: 0.01\n",
      "Epoch [6711/10000], Loss: 71.68058776855469, Learning Rate: 0.01\n",
      "Epoch [6712/10000], Loss: 71.63735961914062, Learning Rate: 0.01\n",
      "Epoch [6713/10000], Loss: 71.60948181152344, Learning Rate: 0.01\n",
      "Epoch [6714/10000], Loss: 71.61271667480469, Learning Rate: 0.01\n",
      "Epoch [6715/10000], Loss: 71.63235473632812, Learning Rate: 0.01\n",
      "Epoch [6716/10000], Loss: 71.64275360107422, Learning Rate: 0.01\n",
      "Epoch [6717/10000], Loss: 71.63117980957031, Learning Rate: 0.01\n",
      "Epoch [6718/10000], Loss: 71.60702514648438, Learning Rate: 0.01\n",
      "Epoch [6719/10000], Loss: 71.58878326416016, Learning Rate: 0.01\n",
      "Epoch [6720/10000], Loss: 71.58638000488281, Learning Rate: 0.01\n",
      "Epoch [6721/10000], Loss: 71.59461975097656, Learning Rate: 0.01\n",
      "Epoch [6722/10000], Loss: 71.60016632080078, Learning Rate: 0.01\n",
      "Epoch [6723/10000], Loss: 71.59461975097656, Learning Rate: 0.01\n",
      "Epoch [6724/10000], Loss: 71.58100891113281, Learning Rate: 0.01\n",
      "Epoch [6725/10000], Loss: 71.56825256347656, Learning Rate: 0.01\n",
      "Epoch [6726/10000], Loss: 71.56310272216797, Learning Rate: 0.01\n",
      "Epoch [6727/10000], Loss: 71.56473541259766, Learning Rate: 0.01\n",
      "Epoch [6728/10000], Loss: 71.56707000732422, Learning Rate: 0.01\n",
      "Epoch [6729/10000], Loss: 71.56446075439453, Learning Rate: 0.01\n",
      "Epoch [6730/10000], Loss: 71.55668640136719, Learning Rate: 0.01\n",
      "Epoch [6731/10000], Loss: 71.54777526855469, Learning Rate: 0.01\n",
      "Epoch [6732/10000], Loss: 71.54175567626953, Learning Rate: 0.01\n",
      "Epoch [6733/10000], Loss: 71.53975677490234, Learning Rate: 0.01\n",
      "Epoch [6734/10000], Loss: 71.5394515991211, Learning Rate: 0.01\n",
      "Epoch [6735/10000], Loss: 71.5377197265625, Learning Rate: 0.01\n",
      "Epoch [6736/10000], Loss: 71.53314971923828, Learning Rate: 0.01\n",
      "Epoch [6737/10000], Loss: 71.52688598632812, Learning Rate: 0.01\n",
      "Epoch [6738/10000], Loss: 71.52117919921875, Learning Rate: 0.01\n",
      "Epoch [6739/10000], Loss: 71.51736450195312, Learning Rate: 0.01\n",
      "Epoch [6740/10000], Loss: 71.51518249511719, Learning Rate: 0.01\n",
      "Epoch [6741/10000], Loss: 71.51319885253906, Learning Rate: 0.01\n",
      "Epoch [6742/10000], Loss: 71.51006317138672, Learning Rate: 0.01\n",
      "Epoch [6743/10000], Loss: 71.50558471679688, Learning Rate: 0.01\n",
      "Epoch [6744/10000], Loss: 71.50069427490234, Learning Rate: 0.01\n",
      "Epoch [6745/10000], Loss: 71.49639892578125, Learning Rate: 0.01\n",
      "Epoch [6746/10000], Loss: 71.4930191040039, Learning Rate: 0.01\n",
      "Epoch [6747/10000], Loss: 71.49027252197266, Learning Rate: 0.01\n",
      "Epoch [6748/10000], Loss: 71.48751831054688, Learning Rate: 0.01\n",
      "Epoch [6749/10000], Loss: 71.48395538330078, Learning Rate: 0.01\n",
      "Epoch [6750/10000], Loss: 71.47991180419922, Learning Rate: 0.01\n",
      "Epoch [6751/10000], Loss: 71.47576141357422, Learning Rate: 0.01\n",
      "Epoch [6752/10000], Loss: 71.4719467163086, Learning Rate: 0.01\n",
      "Epoch [6753/10000], Loss: 71.46855926513672, Learning Rate: 0.01\n",
      "Epoch [6754/10000], Loss: 71.4654541015625, Learning Rate: 0.01\n",
      "Epoch [6755/10000], Loss: 71.4623031616211, Learning Rate: 0.01\n",
      "Epoch [6756/10000], Loss: 71.45875549316406, Learning Rate: 0.01\n",
      "Epoch [6757/10000], Loss: 71.4550552368164, Learning Rate: 0.01\n",
      "Epoch [6758/10000], Loss: 71.45123291015625, Learning Rate: 0.01\n",
      "Epoch [6759/10000], Loss: 71.44756317138672, Learning Rate: 0.01\n",
      "Epoch [6760/10000], Loss: 71.44418334960938, Learning Rate: 0.01\n",
      "Epoch [6761/10000], Loss: 71.44091033935547, Learning Rate: 0.01\n",
      "Epoch [6762/10000], Loss: 71.43749237060547, Learning Rate: 0.01\n",
      "Epoch [6763/10000], Loss: 71.43404388427734, Learning Rate: 0.01\n",
      "Epoch [6764/10000], Loss: 71.43048095703125, Learning Rate: 0.01\n",
      "Epoch [6765/10000], Loss: 71.42681121826172, Learning Rate: 0.01\n",
      "Epoch [6766/10000], Loss: 71.4232406616211, Learning Rate: 0.01\n",
      "Epoch [6767/10000], Loss: 71.41974639892578, Learning Rate: 0.01\n",
      "Epoch [6768/10000], Loss: 71.41643524169922, Learning Rate: 0.01\n",
      "Epoch [6769/10000], Loss: 71.4129638671875, Learning Rate: 0.01\n",
      "Epoch [6770/10000], Loss: 71.4095230102539, Learning Rate: 0.01\n",
      "Epoch [6771/10000], Loss: 71.40611267089844, Learning Rate: 0.01\n",
      "Epoch [6772/10000], Loss: 71.40248107910156, Learning Rate: 0.01\n",
      "Epoch [6773/10000], Loss: 71.39899444580078, Learning Rate: 0.01\n",
      "Epoch [6774/10000], Loss: 71.39543151855469, Learning Rate: 0.01\n",
      "Epoch [6775/10000], Loss: 71.39202880859375, Learning Rate: 0.01\n",
      "Epoch [6776/10000], Loss: 71.38863372802734, Learning Rate: 0.01\n",
      "Epoch [6777/10000], Loss: 71.3851547241211, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6778/10000], Loss: 71.38168334960938, Learning Rate: 0.01\n",
      "Epoch [6779/10000], Loss: 71.37825775146484, Learning Rate: 0.01\n",
      "Epoch [6780/10000], Loss: 71.37477111816406, Learning Rate: 0.01\n",
      "Epoch [6781/10000], Loss: 71.37127685546875, Learning Rate: 0.01\n",
      "Epoch [6782/10000], Loss: 71.36776733398438, Learning Rate: 0.01\n",
      "Epoch [6783/10000], Loss: 71.36433410644531, Learning Rate: 0.01\n",
      "Epoch [6784/10000], Loss: 71.36090087890625, Learning Rate: 0.01\n",
      "Epoch [6785/10000], Loss: 71.35746002197266, Learning Rate: 0.01\n",
      "Epoch [6786/10000], Loss: 71.35399627685547, Learning Rate: 0.01\n",
      "Epoch [6787/10000], Loss: 71.35049438476562, Learning Rate: 0.01\n",
      "Epoch [6788/10000], Loss: 71.34703826904297, Learning Rate: 0.01\n",
      "Epoch [6789/10000], Loss: 71.34358978271484, Learning Rate: 0.01\n",
      "Epoch [6790/10000], Loss: 71.3401107788086, Learning Rate: 0.01\n",
      "Epoch [6791/10000], Loss: 71.33662414550781, Learning Rate: 0.01\n",
      "Epoch [6792/10000], Loss: 71.33322143554688, Learning Rate: 0.01\n",
      "Epoch [6793/10000], Loss: 71.32974243164062, Learning Rate: 0.01\n",
      "Epoch [6794/10000], Loss: 71.3262939453125, Learning Rate: 0.01\n",
      "Epoch [6795/10000], Loss: 71.32286071777344, Learning Rate: 0.01\n",
      "Epoch [6796/10000], Loss: 71.3194351196289, Learning Rate: 0.01\n",
      "Epoch [6797/10000], Loss: 71.3158950805664, Learning Rate: 0.01\n",
      "Epoch [6798/10000], Loss: 71.31250762939453, Learning Rate: 0.01\n",
      "Epoch [6799/10000], Loss: 71.3089599609375, Learning Rate: 0.01\n",
      "Epoch [6800/10000], Loss: 71.30558776855469, Learning Rate: 0.01\n",
      "Epoch [6801/10000], Loss: 71.30215454101562, Learning Rate: 0.01\n",
      "Epoch [6802/10000], Loss: 71.2987060546875, Learning Rate: 0.01\n",
      "Epoch [6803/10000], Loss: 71.29527282714844, Learning Rate: 0.01\n",
      "Epoch [6804/10000], Loss: 71.29180908203125, Learning Rate: 0.01\n",
      "Epoch [6805/10000], Loss: 71.2883529663086, Learning Rate: 0.01\n",
      "Epoch [6806/10000], Loss: 71.28486633300781, Learning Rate: 0.01\n",
      "Epoch [6807/10000], Loss: 71.2815170288086, Learning Rate: 0.01\n",
      "Epoch [6808/10000], Loss: 71.27804565429688, Learning Rate: 0.01\n",
      "Epoch [6809/10000], Loss: 71.27461242675781, Learning Rate: 0.01\n",
      "Epoch [6810/10000], Loss: 71.27116394042969, Learning Rate: 0.01\n",
      "Epoch [6811/10000], Loss: 71.26773834228516, Learning Rate: 0.01\n",
      "Epoch [6812/10000], Loss: 71.26427459716797, Learning Rate: 0.01\n",
      "Epoch [6813/10000], Loss: 71.26090240478516, Learning Rate: 0.01\n",
      "Epoch [6814/10000], Loss: 71.25740051269531, Learning Rate: 0.01\n",
      "Epoch [6815/10000], Loss: 71.25392150878906, Learning Rate: 0.01\n",
      "Epoch [6816/10000], Loss: 71.25048065185547, Learning Rate: 0.01\n",
      "Epoch [6817/10000], Loss: 71.24716186523438, Learning Rate: 0.01\n",
      "Epoch [6818/10000], Loss: 71.24365234375, Learning Rate: 0.01\n",
      "Epoch [6819/10000], Loss: 71.24022674560547, Learning Rate: 0.01\n",
      "Epoch [6820/10000], Loss: 71.23680877685547, Learning Rate: 0.01\n",
      "Epoch [6821/10000], Loss: 71.23335266113281, Learning Rate: 0.01\n",
      "Epoch [6822/10000], Loss: 71.22991180419922, Learning Rate: 0.01\n",
      "Epoch [6823/10000], Loss: 71.22657012939453, Learning Rate: 0.01\n",
      "Epoch [6824/10000], Loss: 71.22311401367188, Learning Rate: 0.01\n",
      "Epoch [6825/10000], Loss: 71.21975708007812, Learning Rate: 0.01\n",
      "Epoch [6826/10000], Loss: 71.21623229980469, Learning Rate: 0.01\n",
      "Epoch [6827/10000], Loss: 71.21289825439453, Learning Rate: 0.01\n",
      "Epoch [6828/10000], Loss: 71.2094497680664, Learning Rate: 0.01\n",
      "Epoch [6829/10000], Loss: 71.20597076416016, Learning Rate: 0.01\n",
      "Epoch [6830/10000], Loss: 71.20259857177734, Learning Rate: 0.01\n",
      "Epoch [6831/10000], Loss: 71.19917297363281, Learning Rate: 0.01\n",
      "Epoch [6832/10000], Loss: 71.1957015991211, Learning Rate: 0.01\n",
      "Epoch [6833/10000], Loss: 71.19233703613281, Learning Rate: 0.01\n",
      "Epoch [6834/10000], Loss: 71.1888656616211, Learning Rate: 0.01\n",
      "Epoch [6835/10000], Loss: 71.18549346923828, Learning Rate: 0.01\n",
      "Epoch [6836/10000], Loss: 71.1820068359375, Learning Rate: 0.01\n",
      "Epoch [6837/10000], Loss: 71.17868041992188, Learning Rate: 0.01\n",
      "Epoch [6838/10000], Loss: 71.17529296875, Learning Rate: 0.01\n",
      "Epoch [6839/10000], Loss: 71.17174530029297, Learning Rate: 0.01\n",
      "Epoch [6840/10000], Loss: 71.16837310791016, Learning Rate: 0.01\n",
      "Epoch [6841/10000], Loss: 71.16500091552734, Learning Rate: 0.01\n",
      "Epoch [6842/10000], Loss: 71.16156005859375, Learning Rate: 0.01\n",
      "Epoch [6843/10000], Loss: 71.15811157226562, Learning Rate: 0.01\n",
      "Epoch [6844/10000], Loss: 71.15475463867188, Learning Rate: 0.01\n",
      "Epoch [6845/10000], Loss: 71.15135192871094, Learning Rate: 0.01\n",
      "Epoch [6846/10000], Loss: 71.14796447753906, Learning Rate: 0.01\n",
      "Epoch [6847/10000], Loss: 71.14452362060547, Learning Rate: 0.01\n",
      "Epoch [6848/10000], Loss: 71.14109802246094, Learning Rate: 0.01\n",
      "Epoch [6849/10000], Loss: 71.13772583007812, Learning Rate: 0.01\n",
      "Epoch [6850/10000], Loss: 71.1342544555664, Learning Rate: 0.01\n",
      "Epoch [6851/10000], Loss: 71.13092803955078, Learning Rate: 0.01\n",
      "Epoch [6852/10000], Loss: 71.12747192382812, Learning Rate: 0.01\n",
      "Epoch [6853/10000], Loss: 71.12410736083984, Learning Rate: 0.01\n",
      "Epoch [6854/10000], Loss: 71.12068176269531, Learning Rate: 0.01\n",
      "Epoch [6855/10000], Loss: 71.1173095703125, Learning Rate: 0.01\n",
      "Epoch [6856/10000], Loss: 71.1138687133789, Learning Rate: 0.01\n",
      "Epoch [6857/10000], Loss: 71.11051177978516, Learning Rate: 0.01\n",
      "Epoch [6858/10000], Loss: 71.10720825195312, Learning Rate: 0.01\n",
      "Epoch [6859/10000], Loss: 71.10369873046875, Learning Rate: 0.01\n",
      "Epoch [6860/10000], Loss: 71.1003646850586, Learning Rate: 0.01\n",
      "Epoch [6861/10000], Loss: 71.09685516357422, Learning Rate: 0.01\n",
      "Epoch [6862/10000], Loss: 71.09347534179688, Learning Rate: 0.01\n",
      "Epoch [6863/10000], Loss: 71.09016418457031, Learning Rate: 0.01\n",
      "Epoch [6864/10000], Loss: 71.08671569824219, Learning Rate: 0.01\n",
      "Epoch [6865/10000], Loss: 71.08331298828125, Learning Rate: 0.01\n",
      "Epoch [6866/10000], Loss: 71.07998657226562, Learning Rate: 0.01\n",
      "Epoch [6867/10000], Loss: 71.07661437988281, Learning Rate: 0.01\n",
      "Epoch [6868/10000], Loss: 71.07316589355469, Learning Rate: 0.01\n",
      "Epoch [6869/10000], Loss: 71.06983184814453, Learning Rate: 0.01\n",
      "Epoch [6870/10000], Loss: 71.06645965576172, Learning Rate: 0.01\n",
      "Epoch [6871/10000], Loss: 71.06303405761719, Learning Rate: 0.01\n",
      "Epoch [6872/10000], Loss: 71.05974578857422, Learning Rate: 0.01\n",
      "Epoch [6873/10000], Loss: 71.05631256103516, Learning Rate: 0.01\n",
      "Epoch [6874/10000], Loss: 71.05291748046875, Learning Rate: 0.01\n",
      "Epoch [6875/10000], Loss: 71.04949951171875, Learning Rate: 0.01\n",
      "Epoch [6876/10000], Loss: 71.0461196899414, Learning Rate: 0.01\n",
      "Epoch [6877/10000], Loss: 71.04276275634766, Learning Rate: 0.01\n",
      "Epoch [6878/10000], Loss: 71.03936767578125, Learning Rate: 0.01\n",
      "Epoch [6879/10000], Loss: 71.0359878540039, Learning Rate: 0.01\n",
      "Epoch [6880/10000], Loss: 71.03266906738281, Learning Rate: 0.01\n",
      "Epoch [6881/10000], Loss: 71.02922058105469, Learning Rate: 0.01\n",
      "Epoch [6882/10000], Loss: 71.02582550048828, Learning Rate: 0.01\n",
      "Epoch [6883/10000], Loss: 71.0224838256836, Learning Rate: 0.01\n",
      "Epoch [6884/10000], Loss: 71.01910400390625, Learning Rate: 0.01\n",
      "Epoch [6885/10000], Loss: 71.01570129394531, Learning Rate: 0.01\n",
      "Epoch [6886/10000], Loss: 71.01239013671875, Learning Rate: 0.01\n",
      "Epoch [6887/10000], Loss: 71.00904846191406, Learning Rate: 0.01\n",
      "Epoch [6888/10000], Loss: 71.00563049316406, Learning Rate: 0.01\n",
      "Epoch [6889/10000], Loss: 71.00228881835938, Learning Rate: 0.01\n",
      "Epoch [6890/10000], Loss: 70.99888610839844, Learning Rate: 0.01\n",
      "Epoch [6891/10000], Loss: 70.9955825805664, Learning Rate: 0.01\n",
      "Epoch [6892/10000], Loss: 70.99222564697266, Learning Rate: 0.01\n",
      "Epoch [6893/10000], Loss: 70.98885345458984, Learning Rate: 0.01\n",
      "Epoch [6894/10000], Loss: 70.98546600341797, Learning Rate: 0.01\n",
      "Epoch [6895/10000], Loss: 70.98209381103516, Learning Rate: 0.01\n",
      "Epoch [6896/10000], Loss: 70.97875213623047, Learning Rate: 0.01\n",
      "Epoch [6897/10000], Loss: 70.97541809082031, Learning Rate: 0.01\n",
      "Epoch [6898/10000], Loss: 70.97203063964844, Learning Rate: 0.01\n",
      "Epoch [6899/10000], Loss: 70.96868133544922, Learning Rate: 0.01\n",
      "Epoch [6900/10000], Loss: 70.96533203125, Learning Rate: 0.01\n",
      "Epoch [6901/10000], Loss: 70.96202087402344, Learning Rate: 0.01\n",
      "Epoch [6902/10000], Loss: 70.95866394042969, Learning Rate: 0.01\n",
      "Epoch [6903/10000], Loss: 70.95531463623047, Learning Rate: 0.01\n",
      "Epoch [6904/10000], Loss: 70.95198822021484, Learning Rate: 0.01\n",
      "Epoch [6905/10000], Loss: 70.94866180419922, Learning Rate: 0.01\n",
      "Epoch [6906/10000], Loss: 70.9453125, Learning Rate: 0.01\n",
      "Epoch [6907/10000], Loss: 70.94200897216797, Learning Rate: 0.01\n",
      "Epoch [6908/10000], Loss: 70.93875122070312, Learning Rate: 0.01\n",
      "Epoch [6909/10000], Loss: 70.93550872802734, Learning Rate: 0.01\n",
      "Epoch [6910/10000], Loss: 70.93234252929688, Learning Rate: 0.01\n",
      "Epoch [6911/10000], Loss: 70.92916107177734, Learning Rate: 0.01\n",
      "Epoch [6912/10000], Loss: 70.92611694335938, Learning Rate: 0.01\n",
      "Epoch [6913/10000], Loss: 70.92324829101562, Learning Rate: 0.01\n",
      "Epoch [6914/10000], Loss: 70.9206314086914, Learning Rate: 0.01\n",
      "Epoch [6915/10000], Loss: 70.91829681396484, Learning Rate: 0.01\n",
      "Epoch [6916/10000], Loss: 70.9166488647461, Learning Rate: 0.01\n",
      "Epoch [6917/10000], Loss: 70.91583251953125, Learning Rate: 0.01\n",
      "Epoch [6918/10000], Loss: 70.91654205322266, Learning Rate: 0.01\n",
      "Epoch [6919/10000], Loss: 70.91927337646484, Learning Rate: 0.01\n",
      "Epoch [6920/10000], Loss: 70.92574310302734, Learning Rate: 0.01\n",
      "Epoch [6921/10000], Loss: 70.93785858154297, Learning Rate: 0.01\n",
      "Epoch [6922/10000], Loss: 70.9588851928711, Learning Rate: 0.01\n",
      "Epoch [6923/10000], Loss: 70.9944839477539, Learning Rate: 0.01\n",
      "Epoch [6924/10000], Loss: 71.05342102050781, Learning Rate: 0.01\n",
      "Epoch [6925/10000], Loss: 71.15007781982422, Learning Rate: 0.01\n",
      "Epoch [6926/10000], Loss: 71.30680847167969, Learning Rate: 0.01\n",
      "Epoch [6927/10000], Loss: 71.55946350097656, Learning Rate: 0.01\n",
      "Epoch [6928/10000], Loss: 71.96060943603516, Learning Rate: 0.01\n",
      "Epoch [6929/10000], Loss: 72.58702850341797, Learning Rate: 0.01\n",
      "Epoch [6930/10000], Loss: 73.5279769897461, Learning Rate: 0.01\n",
      "Epoch [6931/10000], Loss: 74.86779022216797, Learning Rate: 0.01\n",
      "Epoch [6932/10000], Loss: 76.57439422607422, Learning Rate: 0.01\n",
      "Epoch [6933/10000], Loss: 78.38899993896484, Learning Rate: 0.01\n",
      "Epoch [6934/10000], Loss: 79.6056900024414, Learning Rate: 0.01\n",
      "Epoch [6935/10000], Loss: 79.34712982177734, Learning Rate: 0.01\n",
      "Epoch [6936/10000], Loss: 77.14083862304688, Learning Rate: 0.01\n",
      "Epoch [6937/10000], Loss: 73.88233947753906, Learning Rate: 0.01\n",
      "Epoch [6938/10000], Loss: 71.37995910644531, Learning Rate: 0.01\n",
      "Epoch [6939/10000], Loss: 70.93106842041016, Learning Rate: 0.01\n",
      "Epoch [6940/10000], Loss: 72.25629425048828, Learning Rate: 0.01\n",
      "Epoch [6941/10000], Loss: 73.90584564208984, Learning Rate: 0.01\n",
      "Epoch [6942/10000], Loss: 74.43404388427734, Learning Rate: 0.01\n",
      "Epoch [6943/10000], Loss: 73.42182159423828, Learning Rate: 0.01\n",
      "Epoch [6944/10000], Loss: 71.78005981445312, Learning Rate: 0.01\n",
      "Epoch [6945/10000], Loss: 70.8453598022461, Learning Rate: 0.01\n",
      "Epoch [6946/10000], Loss: 71.15608215332031, Learning Rate: 0.01\n",
      "Epoch [6947/10000], Loss: 72.09766387939453, Learning Rate: 0.01\n",
      "Epoch [6948/10000], Loss: 72.63174438476562, Learning Rate: 0.01\n",
      "Epoch [6949/10000], Loss: 72.26469421386719, Learning Rate: 0.01\n",
      "Epoch [6950/10000], Loss: 71.39464569091797, Learning Rate: 0.01\n",
      "Epoch [6951/10000], Loss: 70.82604217529297, Learning Rate: 0.01\n",
      "Epoch [6952/10000], Loss: 70.95061492919922, Learning Rate: 0.01\n",
      "Epoch [6953/10000], Loss: 71.45857238769531, Learning Rate: 0.01\n",
      "Epoch [6954/10000], Loss: 71.75321197509766, Learning Rate: 0.01\n",
      "Epoch [6955/10000], Loss: 71.54696655273438, Learning Rate: 0.01\n",
      "Epoch [6956/10000], Loss: 71.07598876953125, Learning Rate: 0.01\n",
      "Epoch [6957/10000], Loss: 70.78517150878906, Learning Rate: 0.01\n",
      "Epoch [6958/10000], Loss: 70.87284088134766, Learning Rate: 0.01\n",
      "Epoch [6959/10000], Loss: 71.1493911743164, Learning Rate: 0.01\n",
      "Epoch [6960/10000], Loss: 71.28813934326172, Learning Rate: 0.01\n",
      "Epoch [6961/10000], Loss: 71.15563201904297, Learning Rate: 0.01\n",
      "Epoch [6962/10000], Loss: 70.89907836914062, Learning Rate: 0.01\n",
      "Epoch [6963/10000], Loss: 70.75718688964844, Learning Rate: 0.01\n",
      "Epoch [6964/10000], Loss: 70.818115234375, Learning Rate: 0.01\n",
      "Epoch [6965/10000], Loss: 70.96607208251953, Learning Rate: 0.01\n",
      "Epoch [6966/10000], Loss: 71.02867889404297, Learning Rate: 0.01\n",
      "Epoch [6967/10000], Loss: 70.94667053222656, Learning Rate: 0.01\n",
      "Epoch [6968/10000], Loss: 70.80769348144531, Learning Rate: 0.01\n",
      "Epoch [6969/10000], Loss: 70.73634338378906, Learning Rate: 0.01\n",
      "Epoch [6970/10000], Loss: 70.77218627929688, Learning Rate: 0.01\n",
      "Epoch [6971/10000], Loss: 70.84986114501953, Learning Rate: 0.01\n",
      "Epoch [6972/10000], Loss: 70.87939453125, Learning Rate: 0.01\n",
      "Epoch [6973/10000], Loss: 70.83261108398438, Learning Rate: 0.01\n",
      "Epoch [6974/10000], Loss: 70.75697326660156, Learning Rate: 0.01\n",
      "Epoch [6975/10000], Loss: 70.71730041503906, Learning Rate: 0.01\n",
      "Epoch [6976/10000], Loss: 70.73426055908203, Learning Rate: 0.01\n",
      "Epoch [6977/10000], Loss: 70.77412414550781, Learning Rate: 0.01\n",
      "Epoch [6978/10000], Loss: 70.78997802734375, Learning Rate: 0.01\n",
      "Epoch [6979/10000], Loss: 70.76546478271484, Learning Rate: 0.01\n",
      "Epoch [6980/10000], Loss: 70.72386932373047, Learning Rate: 0.01\n",
      "Epoch [6981/10000], Loss: 70.69886016845703, Learning Rate: 0.01\n",
      "Epoch [6982/10000], Loss: 70.70386505126953, Learning Rate: 0.01\n",
      "Epoch [6983/10000], Loss: 70.72332763671875, Learning Rate: 0.01\n",
      "Epoch [6984/10000], Loss: 70.73275756835938, Learning Rate: 0.01\n",
      "Epoch [6985/10000], Loss: 70.72119140625, Learning Rate: 0.01\n",
      "Epoch [6986/10000], Loss: 70.69804382324219, Learning Rate: 0.01\n",
      "Epoch [6987/10000], Loss: 70.68102264404297, Learning Rate: 0.01\n",
      "Epoch [6988/10000], Loss: 70.67924499511719, Learning Rate: 0.01\n",
      "Epoch [6989/10000], Loss: 70.6873550415039, Learning Rate: 0.01\n",
      "Epoch [6990/10000], Loss: 70.69297790527344, Learning Rate: 0.01\n",
      "Epoch [6991/10000], Loss: 70.68801879882812, Learning Rate: 0.01\n",
      "Epoch [6992/10000], Loss: 70.67520904541016, Learning Rate: 0.01\n",
      "Epoch [6993/10000], Loss: 70.66326904296875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6994/10000], Loss: 70.65834045410156, Learning Rate: 0.01\n",
      "Epoch [6995/10000], Loss: 70.66004943847656, Learning Rate: 0.01\n",
      "Epoch [6996/10000], Loss: 70.66259765625, Learning Rate: 0.01\n",
      "Epoch [6997/10000], Loss: 70.66056823730469, Learning Rate: 0.01\n",
      "Epoch [6998/10000], Loss: 70.65370178222656, Learning Rate: 0.01\n",
      "Epoch [6999/10000], Loss: 70.64532470703125, Learning Rate: 0.01\n",
      "Epoch [7000/10000], Loss: 70.63958740234375, Learning Rate: 0.01\n",
      "Epoch [7001/10000], Loss: 70.6376724243164, Learning Rate: 0.01\n",
      "Epoch [7002/10000], Loss: 70.63768768310547, Learning Rate: 0.01\n",
      "Epoch [7003/10000], Loss: 70.63643646240234, Learning Rate: 0.01\n",
      "Epoch [7004/10000], Loss: 70.63264465332031, Learning Rate: 0.01\n",
      "Epoch [7005/10000], Loss: 70.62698364257812, Learning Rate: 0.01\n",
      "Epoch [7006/10000], Loss: 70.62163543701172, Learning Rate: 0.01\n",
      "Epoch [7007/10000], Loss: 70.61804962158203, Learning Rate: 0.01\n",
      "Epoch [7008/10000], Loss: 70.61614990234375, Learning Rate: 0.01\n",
      "Epoch [7009/10000], Loss: 70.61450958251953, Learning Rate: 0.01\n",
      "Epoch [7010/10000], Loss: 70.61187744140625, Learning Rate: 0.01\n",
      "Epoch [7011/10000], Loss: 70.60809326171875, Learning Rate: 0.01\n",
      "Epoch [7012/10000], Loss: 70.60377502441406, Learning Rate: 0.01\n",
      "Epoch [7013/10000], Loss: 70.59974670410156, Learning Rate: 0.01\n",
      "Epoch [7014/10000], Loss: 70.59663391113281, Learning Rate: 0.01\n",
      "Epoch [7015/10000], Loss: 70.59423828125, Learning Rate: 0.01\n",
      "Epoch [7016/10000], Loss: 70.59185791015625, Learning Rate: 0.01\n",
      "Epoch [7017/10000], Loss: 70.58897399902344, Learning Rate: 0.01\n",
      "Epoch [7018/10000], Loss: 70.58535766601562, Learning Rate: 0.01\n",
      "Epoch [7019/10000], Loss: 70.58161163330078, Learning Rate: 0.01\n",
      "Epoch [7020/10000], Loss: 70.57823944091797, Learning Rate: 0.01\n",
      "Epoch [7021/10000], Loss: 70.57518005371094, Learning Rate: 0.01\n",
      "Epoch [7022/10000], Loss: 70.57246398925781, Learning Rate: 0.01\n",
      "Epoch [7023/10000], Loss: 70.56971740722656, Learning Rate: 0.01\n",
      "Epoch [7024/10000], Loss: 70.5666732788086, Learning Rate: 0.01\n",
      "Epoch [7025/10000], Loss: 70.5634536743164, Learning Rate: 0.01\n",
      "Epoch [7026/10000], Loss: 70.56011199951172, Learning Rate: 0.01\n",
      "Epoch [7027/10000], Loss: 70.5567855834961, Learning Rate: 0.01\n",
      "Epoch [7028/10000], Loss: 70.55380249023438, Learning Rate: 0.01\n",
      "Epoch [7029/10000], Loss: 70.55091857910156, Learning Rate: 0.01\n",
      "Epoch [7030/10000], Loss: 70.54806518554688, Learning Rate: 0.01\n",
      "Epoch [7031/10000], Loss: 70.54501342773438, Learning Rate: 0.01\n",
      "Epoch [7032/10000], Loss: 70.54182434082031, Learning Rate: 0.01\n",
      "Epoch [7033/10000], Loss: 70.5386734008789, Learning Rate: 0.01\n",
      "Epoch [7034/10000], Loss: 70.53559112548828, Learning Rate: 0.01\n",
      "Epoch [7035/10000], Loss: 70.53240966796875, Learning Rate: 0.01\n",
      "Epoch [7036/10000], Loss: 70.52950286865234, Learning Rate: 0.01\n",
      "Epoch [7037/10000], Loss: 70.5265121459961, Learning Rate: 0.01\n",
      "Epoch [7038/10000], Loss: 70.52352142333984, Learning Rate: 0.01\n",
      "Epoch [7039/10000], Loss: 70.52046966552734, Learning Rate: 0.01\n",
      "Epoch [7040/10000], Loss: 70.5173568725586, Learning Rate: 0.01\n",
      "Epoch [7041/10000], Loss: 70.51428985595703, Learning Rate: 0.01\n",
      "Epoch [7042/10000], Loss: 70.51122283935547, Learning Rate: 0.01\n",
      "Epoch [7043/10000], Loss: 70.50814819335938, Learning Rate: 0.01\n",
      "Epoch [7044/10000], Loss: 70.50516510009766, Learning Rate: 0.01\n",
      "Epoch [7045/10000], Loss: 70.50211334228516, Learning Rate: 0.01\n",
      "Epoch [7046/10000], Loss: 70.49913024902344, Learning Rate: 0.01\n",
      "Epoch [7047/10000], Loss: 70.49610900878906, Learning Rate: 0.01\n",
      "Epoch [7048/10000], Loss: 70.4930648803711, Learning Rate: 0.01\n",
      "Epoch [7049/10000], Loss: 70.48995971679688, Learning Rate: 0.01\n",
      "Epoch [7050/10000], Loss: 70.4869384765625, Learning Rate: 0.01\n",
      "Epoch [7051/10000], Loss: 70.48397827148438, Learning Rate: 0.01\n",
      "Epoch [7052/10000], Loss: 70.48094940185547, Learning Rate: 0.01\n",
      "Epoch [7053/10000], Loss: 70.47795867919922, Learning Rate: 0.01\n",
      "Epoch [7054/10000], Loss: 70.47496795654297, Learning Rate: 0.01\n",
      "Epoch [7055/10000], Loss: 70.47187042236328, Learning Rate: 0.01\n",
      "Epoch [7056/10000], Loss: 70.46880340576172, Learning Rate: 0.01\n",
      "Epoch [7057/10000], Loss: 70.46578979492188, Learning Rate: 0.01\n",
      "Epoch [7058/10000], Loss: 70.46283721923828, Learning Rate: 0.01\n",
      "Epoch [7059/10000], Loss: 70.4597396850586, Learning Rate: 0.01\n",
      "Epoch [7060/10000], Loss: 70.45672607421875, Learning Rate: 0.01\n",
      "Epoch [7061/10000], Loss: 70.45376586914062, Learning Rate: 0.01\n",
      "Epoch [7062/10000], Loss: 70.45069122314453, Learning Rate: 0.01\n",
      "Epoch [7063/10000], Loss: 70.44768524169922, Learning Rate: 0.01\n",
      "Epoch [7064/10000], Loss: 70.44474792480469, Learning Rate: 0.01\n",
      "Epoch [7065/10000], Loss: 70.44168090820312, Learning Rate: 0.01\n",
      "Epoch [7066/10000], Loss: 70.43868255615234, Learning Rate: 0.01\n",
      "Epoch [7067/10000], Loss: 70.43566131591797, Learning Rate: 0.01\n",
      "Epoch [7068/10000], Loss: 70.43265533447266, Learning Rate: 0.01\n",
      "Epoch [7069/10000], Loss: 70.42962646484375, Learning Rate: 0.01\n",
      "Epoch [7070/10000], Loss: 70.4266128540039, Learning Rate: 0.01\n",
      "Epoch [7071/10000], Loss: 70.42366027832031, Learning Rate: 0.01\n",
      "Epoch [7072/10000], Loss: 70.420654296875, Learning Rate: 0.01\n",
      "Epoch [7073/10000], Loss: 70.41764831542969, Learning Rate: 0.01\n",
      "Epoch [7074/10000], Loss: 70.41465759277344, Learning Rate: 0.01\n",
      "Epoch [7075/10000], Loss: 70.41165161132812, Learning Rate: 0.01\n",
      "Epoch [7076/10000], Loss: 70.40860748291016, Learning Rate: 0.01\n",
      "Epoch [7077/10000], Loss: 70.40568542480469, Learning Rate: 0.01\n",
      "Epoch [7078/10000], Loss: 70.4025650024414, Learning Rate: 0.01\n",
      "Epoch [7079/10000], Loss: 70.39956665039062, Learning Rate: 0.01\n",
      "Epoch [7080/10000], Loss: 70.39671325683594, Learning Rate: 0.01\n",
      "Epoch [7081/10000], Loss: 70.39364624023438, Learning Rate: 0.01\n",
      "Epoch [7082/10000], Loss: 70.39066314697266, Learning Rate: 0.01\n",
      "Epoch [7083/10000], Loss: 70.38764953613281, Learning Rate: 0.01\n",
      "Epoch [7084/10000], Loss: 70.38465881347656, Learning Rate: 0.01\n",
      "Epoch [7085/10000], Loss: 70.38162994384766, Learning Rate: 0.01\n",
      "Epoch [7086/10000], Loss: 70.378662109375, Learning Rate: 0.01\n",
      "Epoch [7087/10000], Loss: 70.37559509277344, Learning Rate: 0.01\n",
      "Epoch [7088/10000], Loss: 70.37274932861328, Learning Rate: 0.01\n",
      "Epoch [7089/10000], Loss: 70.36975860595703, Learning Rate: 0.01\n",
      "Epoch [7090/10000], Loss: 70.36669921875, Learning Rate: 0.01\n",
      "Epoch [7091/10000], Loss: 70.36377716064453, Learning Rate: 0.01\n",
      "Epoch [7092/10000], Loss: 70.36080932617188, Learning Rate: 0.01\n",
      "Epoch [7093/10000], Loss: 70.35779571533203, Learning Rate: 0.01\n",
      "Epoch [7094/10000], Loss: 70.35481262207031, Learning Rate: 0.01\n",
      "Epoch [7095/10000], Loss: 70.35192108154297, Learning Rate: 0.01\n",
      "Epoch [7096/10000], Loss: 70.34889221191406, Learning Rate: 0.01\n",
      "Epoch [7097/10000], Loss: 70.34590148925781, Learning Rate: 0.01\n",
      "Epoch [7098/10000], Loss: 70.34293365478516, Learning Rate: 0.01\n",
      "Epoch [7099/10000], Loss: 70.33992004394531, Learning Rate: 0.01\n",
      "Epoch [7100/10000], Loss: 70.3369140625, Learning Rate: 0.01\n",
      "Epoch [7101/10000], Loss: 70.33395385742188, Learning Rate: 0.01\n",
      "Epoch [7102/10000], Loss: 70.33106231689453, Learning Rate: 0.01\n",
      "Epoch [7103/10000], Loss: 70.3280258178711, Learning Rate: 0.01\n",
      "Epoch [7104/10000], Loss: 70.32502746582031, Learning Rate: 0.01\n",
      "Epoch [7105/10000], Loss: 70.3221206665039, Learning Rate: 0.01\n",
      "Epoch [7106/10000], Loss: 70.31916809082031, Learning Rate: 0.01\n",
      "Epoch [7107/10000], Loss: 70.3161849975586, Learning Rate: 0.01\n",
      "Epoch [7108/10000], Loss: 70.31322479248047, Learning Rate: 0.01\n",
      "Epoch [7109/10000], Loss: 70.31021881103516, Learning Rate: 0.01\n",
      "Epoch [7110/10000], Loss: 70.30732727050781, Learning Rate: 0.01\n",
      "Epoch [7111/10000], Loss: 70.30435943603516, Learning Rate: 0.01\n",
      "Epoch [7112/10000], Loss: 70.30139923095703, Learning Rate: 0.01\n",
      "Epoch [7113/10000], Loss: 70.29846954345703, Learning Rate: 0.01\n",
      "Epoch [7114/10000], Loss: 70.29547119140625, Learning Rate: 0.01\n",
      "Epoch [7115/10000], Loss: 70.2924575805664, Learning Rate: 0.01\n",
      "Epoch [7116/10000], Loss: 70.28954315185547, Learning Rate: 0.01\n",
      "Epoch [7117/10000], Loss: 70.28660583496094, Learning Rate: 0.01\n",
      "Epoch [7118/10000], Loss: 70.2835693359375, Learning Rate: 0.01\n",
      "Epoch [7119/10000], Loss: 70.2806625366211, Learning Rate: 0.01\n",
      "Epoch [7120/10000], Loss: 70.27774047851562, Learning Rate: 0.01\n",
      "Epoch [7121/10000], Loss: 70.27471923828125, Learning Rate: 0.01\n",
      "Epoch [7122/10000], Loss: 70.27178955078125, Learning Rate: 0.01\n",
      "Epoch [7123/10000], Loss: 70.26892852783203, Learning Rate: 0.01\n",
      "Epoch [7124/10000], Loss: 70.26590728759766, Learning Rate: 0.01\n",
      "Epoch [7125/10000], Loss: 70.26298522949219, Learning Rate: 0.01\n",
      "Epoch [7126/10000], Loss: 70.26000213623047, Learning Rate: 0.01\n",
      "Epoch [7127/10000], Loss: 70.25709533691406, Learning Rate: 0.01\n",
      "Epoch [7128/10000], Loss: 70.25410461425781, Learning Rate: 0.01\n",
      "Epoch [7129/10000], Loss: 70.25113677978516, Learning Rate: 0.01\n",
      "Epoch [7130/10000], Loss: 70.24822235107422, Learning Rate: 0.01\n",
      "Epoch [7131/10000], Loss: 70.24528503417969, Learning Rate: 0.01\n",
      "Epoch [7132/10000], Loss: 70.24231719970703, Learning Rate: 0.01\n",
      "Epoch [7133/10000], Loss: 70.23941040039062, Learning Rate: 0.01\n",
      "Epoch [7134/10000], Loss: 70.23649597167969, Learning Rate: 0.01\n",
      "Epoch [7135/10000], Loss: 70.23355102539062, Learning Rate: 0.01\n",
      "Epoch [7136/10000], Loss: 70.23067474365234, Learning Rate: 0.01\n",
      "Epoch [7137/10000], Loss: 70.22766876220703, Learning Rate: 0.01\n",
      "Epoch [7138/10000], Loss: 70.2247543334961, Learning Rate: 0.01\n",
      "Epoch [7139/10000], Loss: 70.22181701660156, Learning Rate: 0.01\n",
      "Epoch [7140/10000], Loss: 70.21895599365234, Learning Rate: 0.01\n",
      "Epoch [7141/10000], Loss: 70.21593475341797, Learning Rate: 0.01\n",
      "Epoch [7142/10000], Loss: 70.21302032470703, Learning Rate: 0.01\n",
      "Epoch [7143/10000], Loss: 70.21011352539062, Learning Rate: 0.01\n",
      "Epoch [7144/10000], Loss: 70.20711517333984, Learning Rate: 0.01\n",
      "Epoch [7145/10000], Loss: 70.2042465209961, Learning Rate: 0.01\n",
      "Epoch [7146/10000], Loss: 70.20127868652344, Learning Rate: 0.01\n",
      "Epoch [7147/10000], Loss: 70.1983871459961, Learning Rate: 0.01\n",
      "Epoch [7148/10000], Loss: 70.1955337524414, Learning Rate: 0.01\n",
      "Epoch [7149/10000], Loss: 70.19258880615234, Learning Rate: 0.01\n",
      "Epoch [7150/10000], Loss: 70.18964385986328, Learning Rate: 0.01\n",
      "Epoch [7151/10000], Loss: 70.18677520751953, Learning Rate: 0.01\n",
      "Epoch [7152/10000], Loss: 70.18379974365234, Learning Rate: 0.01\n",
      "Epoch [7153/10000], Loss: 70.180908203125, Learning Rate: 0.01\n",
      "Epoch [7154/10000], Loss: 70.177978515625, Learning Rate: 0.01\n",
      "Epoch [7155/10000], Loss: 70.17499542236328, Learning Rate: 0.01\n",
      "Epoch [7156/10000], Loss: 70.17217254638672, Learning Rate: 0.01\n",
      "Epoch [7157/10000], Loss: 70.16924285888672, Learning Rate: 0.01\n",
      "Epoch [7158/10000], Loss: 70.16632080078125, Learning Rate: 0.01\n",
      "Epoch [7159/10000], Loss: 70.16348266601562, Learning Rate: 0.01\n",
      "Epoch [7160/10000], Loss: 70.16055297851562, Learning Rate: 0.01\n",
      "Epoch [7161/10000], Loss: 70.15757751464844, Learning Rate: 0.01\n",
      "Epoch [7162/10000], Loss: 70.15471649169922, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7163/10000], Loss: 70.15179443359375, Learning Rate: 0.01\n",
      "Epoch [7164/10000], Loss: 70.14889526367188, Learning Rate: 0.01\n",
      "Epoch [7165/10000], Loss: 70.14596557617188, Learning Rate: 0.01\n",
      "Epoch [7166/10000], Loss: 70.14308166503906, Learning Rate: 0.01\n",
      "Epoch [7167/10000], Loss: 70.14019775390625, Learning Rate: 0.01\n",
      "Epoch [7168/10000], Loss: 70.13728332519531, Learning Rate: 0.01\n",
      "Epoch [7169/10000], Loss: 70.1343765258789, Learning Rate: 0.01\n",
      "Epoch [7170/10000], Loss: 70.1314697265625, Learning Rate: 0.01\n",
      "Epoch [7171/10000], Loss: 70.12861633300781, Learning Rate: 0.01\n",
      "Epoch [7172/10000], Loss: 70.12561798095703, Learning Rate: 0.01\n",
      "Epoch [7173/10000], Loss: 70.12286376953125, Learning Rate: 0.01\n",
      "Epoch [7174/10000], Loss: 70.119873046875, Learning Rate: 0.01\n",
      "Epoch [7175/10000], Loss: 70.11700439453125, Learning Rate: 0.01\n",
      "Epoch [7176/10000], Loss: 70.11415100097656, Learning Rate: 0.01\n",
      "Epoch [7177/10000], Loss: 70.11125183105469, Learning Rate: 0.01\n",
      "Epoch [7178/10000], Loss: 70.10832977294922, Learning Rate: 0.01\n",
      "Epoch [7179/10000], Loss: 70.10546112060547, Learning Rate: 0.01\n",
      "Epoch [7180/10000], Loss: 70.10258483886719, Learning Rate: 0.01\n",
      "Epoch [7181/10000], Loss: 70.0996322631836, Learning Rate: 0.01\n",
      "Epoch [7182/10000], Loss: 70.09688568115234, Learning Rate: 0.01\n",
      "Epoch [7183/10000], Loss: 70.09386444091797, Learning Rate: 0.01\n",
      "Epoch [7184/10000], Loss: 70.09109497070312, Learning Rate: 0.01\n",
      "Epoch [7185/10000], Loss: 70.08819580078125, Learning Rate: 0.01\n",
      "Epoch [7186/10000], Loss: 70.08543395996094, Learning Rate: 0.01\n",
      "Epoch [7187/10000], Loss: 70.08247375488281, Learning Rate: 0.01\n",
      "Epoch [7188/10000], Loss: 70.07977294921875, Learning Rate: 0.01\n",
      "Epoch [7189/10000], Loss: 70.07697296142578, Learning Rate: 0.01\n",
      "Epoch [7190/10000], Loss: 70.07415771484375, Learning Rate: 0.01\n",
      "Epoch [7191/10000], Loss: 70.07144165039062, Learning Rate: 0.01\n",
      "Epoch [7192/10000], Loss: 70.06900024414062, Learning Rate: 0.01\n",
      "Epoch [7193/10000], Loss: 70.06653594970703, Learning Rate: 0.01\n",
      "Epoch [7194/10000], Loss: 70.06439971923828, Learning Rate: 0.01\n",
      "Epoch [7195/10000], Loss: 70.06261444091797, Learning Rate: 0.01\n",
      "Epoch [7196/10000], Loss: 70.06160736083984, Learning Rate: 0.01\n",
      "Epoch [7197/10000], Loss: 70.06144714355469, Learning Rate: 0.01\n",
      "Epoch [7198/10000], Loss: 70.06304168701172, Learning Rate: 0.01\n",
      "Epoch [7199/10000], Loss: 70.06696319580078, Learning Rate: 0.01\n",
      "Epoch [7200/10000], Loss: 70.07508850097656, Learning Rate: 0.01\n",
      "Epoch [7201/10000], Loss: 70.08987426757812, Learning Rate: 0.01\n",
      "Epoch [7202/10000], Loss: 70.1153793334961, Learning Rate: 0.01\n",
      "Epoch [7203/10000], Loss: 70.1583023071289, Learning Rate: 0.01\n",
      "Epoch [7204/10000], Loss: 70.2295913696289, Learning Rate: 0.01\n",
      "Epoch [7205/10000], Loss: 70.34703826904297, Learning Rate: 0.01\n",
      "Epoch [7206/10000], Loss: 70.53886413574219, Learning Rate: 0.01\n",
      "Epoch [7207/10000], Loss: 70.85018157958984, Learning Rate: 0.01\n",
      "Epoch [7208/10000], Loss: 71.34706115722656, Learning Rate: 0.01\n",
      "Epoch [7209/10000], Loss: 72.12451171875, Learning Rate: 0.01\n",
      "Epoch [7210/10000], Loss: 73.28711700439453, Learning Rate: 0.01\n",
      "Epoch [7211/10000], Loss: 74.9161376953125, Learning Rate: 0.01\n",
      "Epoch [7212/10000], Loss: 76.9101333618164, Learning Rate: 0.01\n",
      "Epoch [7213/10000], Loss: 78.842529296875, Learning Rate: 0.01\n",
      "Epoch [7214/10000], Loss: 79.75379943847656, Learning Rate: 0.01\n",
      "Epoch [7215/10000], Loss: 78.69473266601562, Learning Rate: 0.01\n",
      "Epoch [7216/10000], Loss: 75.5835189819336, Learning Rate: 0.01\n",
      "Epoch [7217/10000], Loss: 72.01439666748047, Learning Rate: 0.01\n",
      "Epoch [7218/10000], Loss: 70.08006286621094, Learning Rate: 0.01\n",
      "Epoch [7219/10000], Loss: 70.58968353271484, Learning Rate: 0.01\n",
      "Epoch [7220/10000], Loss: 72.49433135986328, Learning Rate: 0.01\n",
      "Epoch [7221/10000], Loss: 73.90975189208984, Learning Rate: 0.01\n",
      "Epoch [7222/10000], Loss: 73.60198211669922, Learning Rate: 0.01\n",
      "Epoch [7223/10000], Loss: 71.88638305664062, Learning Rate: 0.01\n",
      "Epoch [7224/10000], Loss: 70.30106353759766, Learning Rate: 0.01\n",
      "Epoch [7225/10000], Loss: 70.06080627441406, Learning Rate: 0.01\n",
      "Epoch [7226/10000], Loss: 70.9885025024414, Learning Rate: 0.01\n",
      "Epoch [7227/10000], Loss: 71.90666961669922, Learning Rate: 0.01\n",
      "Epoch [7228/10000], Loss: 71.85770416259766, Learning Rate: 0.01\n",
      "Epoch [7229/10000], Loss: 70.95274353027344, Learning Rate: 0.01\n",
      "Epoch [7230/10000], Loss: 70.10391998291016, Learning Rate: 0.01\n",
      "Epoch [7231/10000], Loss: 70.0261001586914, Learning Rate: 0.01\n",
      "Epoch [7232/10000], Loss: 70.5700912475586, Learning Rate: 0.01\n",
      "Epoch [7233/10000], Loss: 71.02429962158203, Learning Rate: 0.01\n",
      "Epoch [7234/10000], Loss: 70.90180969238281, Learning Rate: 0.01\n",
      "Epoch [7235/10000], Loss: 70.367919921875, Learning Rate: 0.01\n",
      "Epoch [7236/10000], Loss: 69.9700927734375, Learning Rate: 0.01\n",
      "Epoch [7237/10000], Loss: 70.02552795410156, Learning Rate: 0.01\n",
      "Epoch [7238/10000], Loss: 70.3504409790039, Learning Rate: 0.01\n",
      "Epoch [7239/10000], Loss: 70.53565216064453, Learning Rate: 0.01\n",
      "Epoch [7240/10000], Loss: 70.3886489868164, Learning Rate: 0.01\n",
      "Epoch [7241/10000], Loss: 70.0844955444336, Learning Rate: 0.01\n",
      "Epoch [7242/10000], Loss: 69.9229965209961, Learning Rate: 0.01\n",
      "Epoch [7243/10000], Loss: 70.00965118408203, Learning Rate: 0.01\n",
      "Epoch [7244/10000], Loss: 70.18795776367188, Learning Rate: 0.01\n",
      "Epoch [7245/10000], Loss: 70.24312591552734, Learning Rate: 0.01\n",
      "Epoch [7246/10000], Loss: 70.12435150146484, Learning Rate: 0.01\n",
      "Epoch [7247/10000], Loss: 69.96217346191406, Learning Rate: 0.01\n",
      "Epoch [7248/10000], Loss: 69.90666961669922, Learning Rate: 0.01\n",
      "Epoch [7249/10000], Loss: 69.97636413574219, Learning Rate: 0.01\n",
      "Epoch [7250/10000], Loss: 70.06559753417969, Learning Rate: 0.01\n",
      "Epoch [7251/10000], Loss: 70.0714111328125, Learning Rate: 0.01\n",
      "Epoch [7252/10000], Loss: 69.9919662475586, Learning Rate: 0.01\n",
      "Epoch [7253/10000], Loss: 69.90922546386719, Learning Rate: 0.01\n",
      "Epoch [7254/10000], Loss: 69.8932113647461, Learning Rate: 0.01\n",
      "Epoch [7255/10000], Loss: 69.93724822998047, Learning Rate: 0.01\n",
      "Epoch [7256/10000], Loss: 69.97936248779297, Learning Rate: 0.01\n",
      "Epoch [7257/10000], Loss: 69.97193908691406, Learning Rate: 0.01\n",
      "Epoch [7258/10000], Loss: 69.9240493774414, Learning Rate: 0.01\n",
      "Epoch [7259/10000], Loss: 69.88158416748047, Learning Rate: 0.01\n",
      "Epoch [7260/10000], Loss: 69.87702941894531, Learning Rate: 0.01\n",
      "Epoch [7261/10000], Loss: 69.9014663696289, Learning Rate: 0.01\n",
      "Epoch [7262/10000], Loss: 69.92071533203125, Learning Rate: 0.01\n",
      "Epoch [7263/10000], Loss: 69.91291809082031, Learning Rate: 0.01\n",
      "Epoch [7264/10000], Loss: 69.88514709472656, Learning Rate: 0.01\n",
      "Epoch [7265/10000], Loss: 69.86215209960938, Learning Rate: 0.01\n",
      "Epoch [7266/10000], Loss: 69.85945892333984, Learning Rate: 0.01\n",
      "Epoch [7267/10000], Loss: 69.8714370727539, Learning Rate: 0.01\n",
      "Epoch [7268/10000], Loss: 69.88037872314453, Learning Rate: 0.01\n",
      "Epoch [7269/10000], Loss: 69.87477111816406, Learning Rate: 0.01\n",
      "Epoch [7270/10000], Loss: 69.85883331298828, Learning Rate: 0.01\n",
      "Epoch [7271/10000], Loss: 69.8450927734375, Learning Rate: 0.01\n",
      "Epoch [7272/10000], Loss: 69.84202575683594, Learning Rate: 0.01\n",
      "Epoch [7273/10000], Loss: 69.84693145751953, Learning Rate: 0.01\n",
      "Epoch [7274/10000], Loss: 69.850830078125, Learning Rate: 0.01\n",
      "Epoch [7275/10000], Loss: 69.84736633300781, Learning Rate: 0.01\n",
      "Epoch [7276/10000], Loss: 69.83795928955078, Learning Rate: 0.01\n",
      "Epoch [7277/10000], Loss: 69.82903289794922, Learning Rate: 0.01\n",
      "Epoch [7278/10000], Loss: 69.82527160644531, Learning Rate: 0.01\n",
      "Epoch [7279/10000], Loss: 69.826171875, Learning Rate: 0.01\n",
      "Epoch [7280/10000], Loss: 69.827392578125, Learning Rate: 0.01\n",
      "Epoch [7281/10000], Loss: 69.82516479492188, Learning Rate: 0.01\n",
      "Epoch [7282/10000], Loss: 69.81939697265625, Learning Rate: 0.01\n",
      "Epoch [7283/10000], Loss: 69.81312561035156, Learning Rate: 0.01\n",
      "Epoch [7284/10000], Loss: 69.80908966064453, Learning Rate: 0.01\n",
      "Epoch [7285/10000], Loss: 69.8078384399414, Learning Rate: 0.01\n",
      "Epoch [7286/10000], Loss: 69.8073959350586, Learning Rate: 0.01\n",
      "Epoch [7287/10000], Loss: 69.8056640625, Learning Rate: 0.01\n",
      "Epoch [7288/10000], Loss: 69.80193328857422, Learning Rate: 0.01\n",
      "Epoch [7289/10000], Loss: 69.79722595214844, Learning Rate: 0.01\n",
      "Epoch [7290/10000], Loss: 69.79341888427734, Learning Rate: 0.01\n",
      "Epoch [7291/10000], Loss: 69.79100799560547, Learning Rate: 0.01\n",
      "Epoch [7292/10000], Loss: 69.78950500488281, Learning Rate: 0.01\n",
      "Epoch [7293/10000], Loss: 69.7876968383789, Learning Rate: 0.01\n",
      "Epoch [7294/10000], Loss: 69.7848892211914, Learning Rate: 0.01\n",
      "Epoch [7295/10000], Loss: 69.78134155273438, Learning Rate: 0.01\n",
      "Epoch [7296/10000], Loss: 69.77782440185547, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7297/10000], Loss: 69.77494812011719, Learning Rate: 0.01\n",
      "Epoch [7298/10000], Loss: 69.77274322509766, Learning Rate: 0.01\n",
      "Epoch [7299/10000], Loss: 69.77070617675781, Learning Rate: 0.01\n",
      "Epoch [7300/10000], Loss: 69.76823425292969, Learning Rate: 0.01\n",
      "Epoch [7301/10000], Loss: 69.76535034179688, Learning Rate: 0.01\n",
      "Epoch [7302/10000], Loss: 69.76221466064453, Learning Rate: 0.01\n",
      "Epoch [7303/10000], Loss: 69.75924682617188, Learning Rate: 0.01\n",
      "Epoch [7304/10000], Loss: 69.75660705566406, Learning Rate: 0.01\n",
      "Epoch [7305/10000], Loss: 69.75434112548828, Learning Rate: 0.01\n",
      "Epoch [7306/10000], Loss: 69.75188446044922, Learning Rate: 0.01\n",
      "Epoch [7307/10000], Loss: 69.74932098388672, Learning Rate: 0.01\n",
      "Epoch [7308/10000], Loss: 69.7464828491211, Learning Rate: 0.01\n",
      "Epoch [7309/10000], Loss: 69.74363708496094, Learning Rate: 0.01\n",
      "Epoch [7310/10000], Loss: 69.74089813232422, Learning Rate: 0.01\n",
      "Epoch [7311/10000], Loss: 69.73835754394531, Learning Rate: 0.01\n",
      "Epoch [7312/10000], Loss: 69.73588562011719, Learning Rate: 0.01\n",
      "Epoch [7313/10000], Loss: 69.73330688476562, Learning Rate: 0.01\n",
      "Epoch [7314/10000], Loss: 69.73077392578125, Learning Rate: 0.01\n",
      "Epoch [7315/10000], Loss: 69.72806549072266, Learning Rate: 0.01\n",
      "Epoch [7316/10000], Loss: 69.72527313232422, Learning Rate: 0.01\n",
      "Epoch [7317/10000], Loss: 69.72270202636719, Learning Rate: 0.01\n",
      "Epoch [7318/10000], Loss: 69.72006225585938, Learning Rate: 0.01\n",
      "Epoch [7319/10000], Loss: 69.71754455566406, Learning Rate: 0.01\n",
      "Epoch [7320/10000], Loss: 69.7149658203125, Learning Rate: 0.01\n",
      "Epoch [7321/10000], Loss: 69.71238708496094, Learning Rate: 0.01\n",
      "Epoch [7322/10000], Loss: 69.709716796875, Learning Rate: 0.01\n",
      "Epoch [7323/10000], Loss: 69.70699310302734, Learning Rate: 0.01\n",
      "Epoch [7324/10000], Loss: 69.70441436767578, Learning Rate: 0.01\n",
      "Epoch [7325/10000], Loss: 69.70184326171875, Learning Rate: 0.01\n",
      "Epoch [7326/10000], Loss: 69.69927215576172, Learning Rate: 0.01\n",
      "Epoch [7327/10000], Loss: 69.69664764404297, Learning Rate: 0.01\n",
      "Epoch [7328/10000], Loss: 69.69412231445312, Learning Rate: 0.01\n",
      "Epoch [7329/10000], Loss: 69.69148254394531, Learning Rate: 0.01\n",
      "Epoch [7330/10000], Loss: 69.68887329101562, Learning Rate: 0.01\n",
      "Epoch [7331/10000], Loss: 69.68629455566406, Learning Rate: 0.01\n",
      "Epoch [7332/10000], Loss: 69.68359375, Learning Rate: 0.01\n",
      "Epoch [7333/10000], Loss: 69.68106079101562, Learning Rate: 0.01\n",
      "Epoch [7334/10000], Loss: 69.67851257324219, Learning Rate: 0.01\n",
      "Epoch [7335/10000], Loss: 69.67591857910156, Learning Rate: 0.01\n",
      "Epoch [7336/10000], Loss: 69.67329406738281, Learning Rate: 0.01\n",
      "Epoch [7337/10000], Loss: 69.6706771850586, Learning Rate: 0.01\n",
      "Epoch [7338/10000], Loss: 69.66812896728516, Learning Rate: 0.01\n",
      "Epoch [7339/10000], Loss: 69.66552734375, Learning Rate: 0.01\n",
      "Epoch [7340/10000], Loss: 69.66290283203125, Learning Rate: 0.01\n",
      "Epoch [7341/10000], Loss: 69.66030883789062, Learning Rate: 0.01\n",
      "Epoch [7342/10000], Loss: 69.65776062011719, Learning Rate: 0.01\n",
      "Epoch [7343/10000], Loss: 69.65515899658203, Learning Rate: 0.01\n",
      "Epoch [7344/10000], Loss: 69.65259552001953, Learning Rate: 0.01\n",
      "Epoch [7345/10000], Loss: 69.64997100830078, Learning Rate: 0.01\n",
      "Epoch [7346/10000], Loss: 69.6474380493164, Learning Rate: 0.01\n",
      "Epoch [7347/10000], Loss: 69.64482879638672, Learning Rate: 0.01\n",
      "Epoch [7348/10000], Loss: 69.64228057861328, Learning Rate: 0.01\n",
      "Epoch [7349/10000], Loss: 69.63968658447266, Learning Rate: 0.01\n",
      "Epoch [7350/10000], Loss: 69.6370620727539, Learning Rate: 0.01\n",
      "Epoch [7351/10000], Loss: 69.6345443725586, Learning Rate: 0.01\n",
      "Epoch [7352/10000], Loss: 69.63192749023438, Learning Rate: 0.01\n",
      "Epoch [7353/10000], Loss: 69.62936401367188, Learning Rate: 0.01\n",
      "Epoch [7354/10000], Loss: 69.62677001953125, Learning Rate: 0.01\n",
      "Epoch [7355/10000], Loss: 69.62417602539062, Learning Rate: 0.01\n",
      "Epoch [7356/10000], Loss: 69.62165069580078, Learning Rate: 0.01\n",
      "Epoch [7357/10000], Loss: 69.61904907226562, Learning Rate: 0.01\n",
      "Epoch [7358/10000], Loss: 69.6164779663086, Learning Rate: 0.01\n",
      "Epoch [7359/10000], Loss: 69.6138916015625, Learning Rate: 0.01\n",
      "Epoch [7360/10000], Loss: 69.61138153076172, Learning Rate: 0.01\n",
      "Epoch [7361/10000], Loss: 69.60874938964844, Learning Rate: 0.01\n",
      "Epoch [7362/10000], Loss: 69.60623931884766, Learning Rate: 0.01\n",
      "Epoch [7363/10000], Loss: 69.60369110107422, Learning Rate: 0.01\n",
      "Epoch [7364/10000], Loss: 69.60108947753906, Learning Rate: 0.01\n",
      "Epoch [7365/10000], Loss: 69.59844970703125, Learning Rate: 0.01\n",
      "Epoch [7366/10000], Loss: 69.59595489501953, Learning Rate: 0.01\n",
      "Epoch [7367/10000], Loss: 69.59339904785156, Learning Rate: 0.01\n",
      "Epoch [7368/10000], Loss: 69.59081268310547, Learning Rate: 0.01\n",
      "Epoch [7369/10000], Loss: 69.58819580078125, Learning Rate: 0.01\n",
      "Epoch [7370/10000], Loss: 69.58570861816406, Learning Rate: 0.01\n",
      "Epoch [7371/10000], Loss: 69.58306884765625, Learning Rate: 0.01\n",
      "Epoch [7372/10000], Loss: 69.58059692382812, Learning Rate: 0.01\n",
      "Epoch [7373/10000], Loss: 69.5780258178711, Learning Rate: 0.01\n",
      "Epoch [7374/10000], Loss: 69.575439453125, Learning Rate: 0.01\n",
      "Epoch [7375/10000], Loss: 69.5728759765625, Learning Rate: 0.01\n",
      "Epoch [7376/10000], Loss: 69.57032012939453, Learning Rate: 0.01\n",
      "Epoch [7377/10000], Loss: 69.56779479980469, Learning Rate: 0.01\n",
      "Epoch [7378/10000], Loss: 69.56525421142578, Learning Rate: 0.01\n",
      "Epoch [7379/10000], Loss: 69.56266021728516, Learning Rate: 0.01\n",
      "Epoch [7380/10000], Loss: 69.56010437011719, Learning Rate: 0.01\n",
      "Epoch [7381/10000], Loss: 69.55755615234375, Learning Rate: 0.01\n",
      "Epoch [7382/10000], Loss: 69.55503845214844, Learning Rate: 0.01\n",
      "Epoch [7383/10000], Loss: 69.55247497558594, Learning Rate: 0.01\n",
      "Epoch [7384/10000], Loss: 69.5499267578125, Learning Rate: 0.01\n",
      "Epoch [7385/10000], Loss: 69.54743957519531, Learning Rate: 0.01\n",
      "Epoch [7386/10000], Loss: 69.54478454589844, Learning Rate: 0.01\n",
      "Epoch [7387/10000], Loss: 69.5422592163086, Learning Rate: 0.01\n",
      "Epoch [7388/10000], Loss: 69.53975677490234, Learning Rate: 0.01\n",
      "Epoch [7389/10000], Loss: 69.53720092773438, Learning Rate: 0.01\n",
      "Epoch [7390/10000], Loss: 69.53463745117188, Learning Rate: 0.01\n",
      "Epoch [7391/10000], Loss: 69.53211212158203, Learning Rate: 0.01\n",
      "Epoch [7392/10000], Loss: 69.52950286865234, Learning Rate: 0.01\n",
      "Epoch [7393/10000], Loss: 69.52698516845703, Learning Rate: 0.01\n",
      "Epoch [7394/10000], Loss: 69.52445220947266, Learning Rate: 0.01\n",
      "Epoch [7395/10000], Loss: 69.52197265625, Learning Rate: 0.01\n",
      "Epoch [7396/10000], Loss: 69.5194091796875, Learning Rate: 0.01\n",
      "Epoch [7397/10000], Loss: 69.5168228149414, Learning Rate: 0.01\n",
      "Epoch [7398/10000], Loss: 69.51428985595703, Learning Rate: 0.01\n",
      "Epoch [7399/10000], Loss: 69.51182556152344, Learning Rate: 0.01\n",
      "Epoch [7400/10000], Loss: 69.50920104980469, Learning Rate: 0.01\n",
      "Epoch [7401/10000], Loss: 69.50669860839844, Learning Rate: 0.01\n",
      "Epoch [7402/10000], Loss: 69.50411224365234, Learning Rate: 0.01\n",
      "Epoch [7403/10000], Loss: 69.50162506103516, Learning Rate: 0.01\n",
      "Epoch [7404/10000], Loss: 69.49909973144531, Learning Rate: 0.01\n",
      "Epoch [7405/10000], Loss: 69.49653625488281, Learning Rate: 0.01\n",
      "Epoch [7406/10000], Loss: 69.49398040771484, Learning Rate: 0.01\n",
      "Epoch [7407/10000], Loss: 69.49148559570312, Learning Rate: 0.01\n",
      "Epoch [7408/10000], Loss: 69.48893737792969, Learning Rate: 0.01\n",
      "Epoch [7409/10000], Loss: 69.4864501953125, Learning Rate: 0.01\n",
      "Epoch [7410/10000], Loss: 69.48388671875, Learning Rate: 0.01\n",
      "Epoch [7411/10000], Loss: 69.48133087158203, Learning Rate: 0.01\n",
      "Epoch [7412/10000], Loss: 69.47882080078125, Learning Rate: 0.01\n",
      "Epoch [7413/10000], Loss: 69.47639465332031, Learning Rate: 0.01\n",
      "Epoch [7414/10000], Loss: 69.47378540039062, Learning Rate: 0.01\n",
      "Epoch [7415/10000], Loss: 69.47120666503906, Learning Rate: 0.01\n",
      "Epoch [7416/10000], Loss: 69.46873474121094, Learning Rate: 0.01\n",
      "Epoch [7417/10000], Loss: 69.46623229980469, Learning Rate: 0.01\n",
      "Epoch [7418/10000], Loss: 69.46365356445312, Learning Rate: 0.01\n",
      "Epoch [7419/10000], Loss: 69.46116638183594, Learning Rate: 0.01\n",
      "Epoch [7420/10000], Loss: 69.4586410522461, Learning Rate: 0.01\n",
      "Epoch [7421/10000], Loss: 69.45608520507812, Learning Rate: 0.01\n",
      "Epoch [7422/10000], Loss: 69.45356750488281, Learning Rate: 0.01\n",
      "Epoch [7423/10000], Loss: 69.45109558105469, Learning Rate: 0.01\n",
      "Epoch [7424/10000], Loss: 69.44860076904297, Learning Rate: 0.01\n",
      "Epoch [7425/10000], Loss: 69.44598388671875, Learning Rate: 0.01\n",
      "Epoch [7426/10000], Loss: 69.44352722167969, Learning Rate: 0.01\n",
      "Epoch [7427/10000], Loss: 69.44096374511719, Learning Rate: 0.01\n",
      "Epoch [7428/10000], Loss: 69.43849182128906, Learning Rate: 0.01\n",
      "Epoch [7429/10000], Loss: 69.43596649169922, Learning Rate: 0.01\n",
      "Epoch [7430/10000], Loss: 69.43348693847656, Learning Rate: 0.01\n",
      "Epoch [7431/10000], Loss: 69.43096160888672, Learning Rate: 0.01\n",
      "Epoch [7432/10000], Loss: 69.42843627929688, Learning Rate: 0.01\n",
      "Epoch [7433/10000], Loss: 69.42589569091797, Learning Rate: 0.01\n",
      "Epoch [7434/10000], Loss: 69.4234619140625, Learning Rate: 0.01\n",
      "Epoch [7435/10000], Loss: 69.42088317871094, Learning Rate: 0.01\n",
      "Epoch [7436/10000], Loss: 69.41841125488281, Learning Rate: 0.01\n",
      "Epoch [7437/10000], Loss: 69.41587829589844, Learning Rate: 0.01\n",
      "Epoch [7438/10000], Loss: 69.41340637207031, Learning Rate: 0.01\n",
      "Epoch [7439/10000], Loss: 69.410888671875, Learning Rate: 0.01\n",
      "Epoch [7440/10000], Loss: 69.40836334228516, Learning Rate: 0.01\n",
      "Epoch [7441/10000], Loss: 69.4058609008789, Learning Rate: 0.01\n",
      "Epoch [7442/10000], Loss: 69.40335083007812, Learning Rate: 0.01\n",
      "Epoch [7443/10000], Loss: 69.40091705322266, Learning Rate: 0.01\n",
      "Epoch [7444/10000], Loss: 69.39845275878906, Learning Rate: 0.01\n",
      "Epoch [7445/10000], Loss: 69.3958511352539, Learning Rate: 0.01\n",
      "Epoch [7446/10000], Loss: 69.39337921142578, Learning Rate: 0.01\n",
      "Epoch [7447/10000], Loss: 69.39082336425781, Learning Rate: 0.01\n",
      "Epoch [7448/10000], Loss: 69.38829803466797, Learning Rate: 0.01\n",
      "Epoch [7449/10000], Loss: 69.38583374023438, Learning Rate: 0.01\n",
      "Epoch [7450/10000], Loss: 69.38331604003906, Learning Rate: 0.01\n",
      "Epoch [7451/10000], Loss: 69.38084411621094, Learning Rate: 0.01\n",
      "Epoch [7452/10000], Loss: 69.37834930419922, Learning Rate: 0.01\n",
      "Epoch [7453/10000], Loss: 69.37586212158203, Learning Rate: 0.01\n",
      "Epoch [7454/10000], Loss: 69.37340545654297, Learning Rate: 0.01\n",
      "Epoch [7455/10000], Loss: 69.37091064453125, Learning Rate: 0.01\n",
      "Epoch [7456/10000], Loss: 69.36839294433594, Learning Rate: 0.01\n",
      "Epoch [7457/10000], Loss: 69.365966796875, Learning Rate: 0.01\n",
      "Epoch [7458/10000], Loss: 69.36337280273438, Learning Rate: 0.01\n",
      "Epoch [7459/10000], Loss: 69.36089324951172, Learning Rate: 0.01\n",
      "Epoch [7460/10000], Loss: 69.35844421386719, Learning Rate: 0.01\n",
      "Epoch [7461/10000], Loss: 69.3559341430664, Learning Rate: 0.01\n",
      "Epoch [7462/10000], Loss: 69.35344696044922, Learning Rate: 0.01\n",
      "Epoch [7463/10000], Loss: 69.35098266601562, Learning Rate: 0.01\n",
      "Epoch [7464/10000], Loss: 69.3485107421875, Learning Rate: 0.01\n",
      "Epoch [7465/10000], Loss: 69.34598541259766, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7466/10000], Loss: 69.34354400634766, Learning Rate: 0.01\n",
      "Epoch [7467/10000], Loss: 69.34104919433594, Learning Rate: 0.01\n",
      "Epoch [7468/10000], Loss: 69.3385238647461, Learning Rate: 0.01\n",
      "Epoch [7469/10000], Loss: 69.33609771728516, Learning Rate: 0.01\n",
      "Epoch [7470/10000], Loss: 69.33361053466797, Learning Rate: 0.01\n",
      "Epoch [7471/10000], Loss: 69.33110046386719, Learning Rate: 0.01\n",
      "Epoch [7472/10000], Loss: 69.32867431640625, Learning Rate: 0.01\n",
      "Epoch [7473/10000], Loss: 69.32615661621094, Learning Rate: 0.01\n",
      "Epoch [7474/10000], Loss: 69.32368469238281, Learning Rate: 0.01\n",
      "Epoch [7475/10000], Loss: 69.32118225097656, Learning Rate: 0.01\n",
      "Epoch [7476/10000], Loss: 69.3187026977539, Learning Rate: 0.01\n",
      "Epoch [7477/10000], Loss: 69.3162612915039, Learning Rate: 0.01\n",
      "Epoch [7478/10000], Loss: 69.31373596191406, Learning Rate: 0.01\n",
      "Epoch [7479/10000], Loss: 69.31130981445312, Learning Rate: 0.01\n",
      "Epoch [7480/10000], Loss: 69.308837890625, Learning Rate: 0.01\n",
      "Epoch [7481/10000], Loss: 69.30638122558594, Learning Rate: 0.01\n",
      "Epoch [7482/10000], Loss: 69.30387115478516, Learning Rate: 0.01\n",
      "Epoch [7483/10000], Loss: 69.30145263671875, Learning Rate: 0.01\n",
      "Epoch [7484/10000], Loss: 69.29895782470703, Learning Rate: 0.01\n",
      "Epoch [7485/10000], Loss: 69.29645538330078, Learning Rate: 0.01\n",
      "Epoch [7486/10000], Loss: 69.29399871826172, Learning Rate: 0.01\n",
      "Epoch [7487/10000], Loss: 69.29154205322266, Learning Rate: 0.01\n",
      "Epoch [7488/10000], Loss: 69.28910064697266, Learning Rate: 0.01\n",
      "Epoch [7489/10000], Loss: 69.28662109375, Learning Rate: 0.01\n",
      "Epoch [7490/10000], Loss: 69.2841796875, Learning Rate: 0.01\n",
      "Epoch [7491/10000], Loss: 69.28174591064453, Learning Rate: 0.01\n",
      "Epoch [7492/10000], Loss: 69.27921295166016, Learning Rate: 0.01\n",
      "Epoch [7493/10000], Loss: 69.27684020996094, Learning Rate: 0.01\n",
      "Epoch [7494/10000], Loss: 69.27440643310547, Learning Rate: 0.01\n",
      "Epoch [7495/10000], Loss: 69.27196502685547, Learning Rate: 0.01\n",
      "Epoch [7496/10000], Loss: 69.26959228515625, Learning Rate: 0.01\n",
      "Epoch [7497/10000], Loss: 69.26720428466797, Learning Rate: 0.01\n",
      "Epoch [7498/10000], Loss: 69.26497650146484, Learning Rate: 0.01\n",
      "Epoch [7499/10000], Loss: 69.26274108886719, Learning Rate: 0.01\n",
      "Epoch [7500/10000], Loss: 69.2607192993164, Learning Rate: 0.01\n",
      "Epoch [7501/10000], Loss: 69.2589111328125, Learning Rate: 0.01\n",
      "Epoch [7502/10000], Loss: 69.25755310058594, Learning Rate: 0.01\n",
      "Epoch [7503/10000], Loss: 69.25674438476562, Learning Rate: 0.01\n",
      "Epoch [7504/10000], Loss: 69.25702667236328, Learning Rate: 0.01\n",
      "Epoch [7505/10000], Loss: 69.25916290283203, Learning Rate: 0.01\n",
      "Epoch [7506/10000], Loss: 69.26396179199219, Learning Rate: 0.01\n",
      "Epoch [7507/10000], Loss: 69.27323150634766, Learning Rate: 0.01\n",
      "Epoch [7508/10000], Loss: 69.29027557373047, Learning Rate: 0.01\n",
      "Epoch [7509/10000], Loss: 69.31991577148438, Learning Rate: 0.01\n",
      "Epoch [7510/10000], Loss: 69.37049102783203, Learning Rate: 0.01\n",
      "Epoch [7511/10000], Loss: 69.45599365234375, Learning Rate: 0.01\n",
      "Epoch [7512/10000], Loss: 69.5993881225586, Learning Rate: 0.01\n",
      "Epoch [7513/10000], Loss: 69.83832550048828, Learning Rate: 0.01\n",
      "Epoch [7514/10000], Loss: 70.23336029052734, Learning Rate: 0.01\n",
      "Epoch [7515/10000], Loss: 70.87440490722656, Learning Rate: 0.01\n",
      "Epoch [7516/10000], Loss: 71.88961029052734, Learning Rate: 0.01\n",
      "Epoch [7517/10000], Loss: 73.41142272949219, Learning Rate: 0.01\n",
      "Epoch [7518/10000], Loss: 75.5120620727539, Learning Rate: 0.01\n",
      "Epoch [7519/10000], Loss: 77.947021484375, Learning Rate: 0.01\n",
      "Epoch [7520/10000], Loss: 79.96308898925781, Learning Rate: 0.01\n",
      "Epoch [7521/10000], Loss: 80.1795654296875, Learning Rate: 0.01\n",
      "Epoch [7522/10000], Loss: 77.7168197631836, Learning Rate: 0.01\n",
      "Epoch [7523/10000], Loss: 73.4134750366211, Learning Rate: 0.01\n",
      "Epoch [7524/10000], Loss: 69.94094848632812, Learning Rate: 0.01\n",
      "Epoch [7525/10000], Loss: 69.35179138183594, Learning Rate: 0.01\n",
      "Epoch [7526/10000], Loss: 71.26626586914062, Learning Rate: 0.01\n",
      "Epoch [7527/10000], Loss: 73.44589233398438, Learning Rate: 0.01\n",
      "Epoch [7528/10000], Loss: 73.76222229003906, Learning Rate: 0.01\n",
      "Epoch [7529/10000], Loss: 71.97792053222656, Learning Rate: 0.01\n",
      "Epoch [7530/10000], Loss: 69.83119201660156, Learning Rate: 0.01\n",
      "Epoch [7531/10000], Loss: 69.21646881103516, Learning Rate: 0.01\n",
      "Epoch [7532/10000], Loss: 70.25504302978516, Learning Rate: 0.01\n",
      "Epoch [7533/10000], Loss: 71.46316528320312, Learning Rate: 0.01\n",
      "Epoch [7534/10000], Loss: 71.46554565429688, Learning Rate: 0.01\n",
      "Epoch [7535/10000], Loss: 70.32696533203125, Learning Rate: 0.01\n",
      "Epoch [7536/10000], Loss: 69.29838562011719, Learning Rate: 0.01\n",
      "Epoch [7537/10000], Loss: 69.3120346069336, Learning Rate: 0.01\n",
      "Epoch [7538/10000], Loss: 70.05487823486328, Learning Rate: 0.01\n",
      "Epoch [7539/10000], Loss: 70.5205307006836, Learning Rate: 0.01\n",
      "Epoch [7540/10000], Loss: 70.1821517944336, Learning Rate: 0.01\n",
      "Epoch [7541/10000], Loss: 69.4723892211914, Learning Rate: 0.01\n",
      "Epoch [7542/10000], Loss: 69.15076446533203, Learning Rate: 0.01\n",
      "Epoch [7543/10000], Loss: 69.42738342285156, Learning Rate: 0.01\n",
      "Epoch [7544/10000], Loss: 69.83585357666016, Learning Rate: 0.01\n",
      "Epoch [7545/10000], Loss: 69.86009979248047, Learning Rate: 0.01\n",
      "Epoch [7546/10000], Loss: 69.49979400634766, Learning Rate: 0.01\n",
      "Epoch [7547/10000], Loss: 69.17449188232422, Learning Rate: 0.01\n",
      "Epoch [7548/10000], Loss: 69.1921615600586, Learning Rate: 0.01\n",
      "Epoch [7549/10000], Loss: 69.4342041015625, Learning Rate: 0.01\n",
      "Epoch [7550/10000], Loss: 69.56529998779297, Learning Rate: 0.01\n",
      "Epoch [7551/10000], Loss: 69.43649291992188, Learning Rate: 0.01\n",
      "Epoch [7552/10000], Loss: 69.20990753173828, Learning Rate: 0.01\n",
      "Epoch [7553/10000], Loss: 69.12545013427734, Learning Rate: 0.01\n",
      "Epoch [7554/10000], Loss: 69.22579956054688, Learning Rate: 0.01\n",
      "Epoch [7555/10000], Loss: 69.34868621826172, Learning Rate: 0.01\n",
      "Epoch [7556/10000], Loss: 69.34029388427734, Learning Rate: 0.01\n",
      "Epoch [7557/10000], Loss: 69.21822357177734, Learning Rate: 0.01\n",
      "Epoch [7558/10000], Loss: 69.11994171142578, Learning Rate: 0.01\n",
      "Epoch [7559/10000], Loss: 69.13211822509766, Learning Rate: 0.01\n",
      "Epoch [7560/10000], Loss: 69.2086410522461, Learning Rate: 0.01\n",
      "Epoch [7561/10000], Loss: 69.24462127685547, Learning Rate: 0.01\n",
      "Epoch [7562/10000], Loss: 69.19921112060547, Learning Rate: 0.01\n",
      "Epoch [7563/10000], Loss: 69.12619018554688, Learning Rate: 0.01\n",
      "Epoch [7564/10000], Loss: 69.09867095947266, Learning Rate: 0.01\n",
      "Epoch [7565/10000], Loss: 69.12862396240234, Learning Rate: 0.01\n",
      "Epoch [7566/10000], Loss: 69.16620635986328, Learning Rate: 0.01\n",
      "Epoch [7567/10000], Loss: 69.16375732421875, Learning Rate: 0.01\n",
      "Epoch [7568/10000], Loss: 69.12478637695312, Learning Rate: 0.01\n",
      "Epoch [7569/10000], Loss: 69.09041595458984, Learning Rate: 0.01\n",
      "Epoch [7570/10000], Loss: 69.0893325805664, Learning Rate: 0.01\n",
      "Epoch [7571/10000], Loss: 69.11099243164062, Learning Rate: 0.01\n",
      "Epoch [7572/10000], Loss: 69.123779296875, Learning Rate: 0.01\n",
      "Epoch [7573/10000], Loss: 69.1115493774414, Learning Rate: 0.01\n",
      "Epoch [7574/10000], Loss: 69.08695220947266, Learning Rate: 0.01\n",
      "Epoch [7575/10000], Loss: 69.07289123535156, Learning Rate: 0.01\n",
      "Epoch [7576/10000], Loss: 69.07738494873047, Learning Rate: 0.01\n",
      "Epoch [7577/10000], Loss: 69.08850860595703, Learning Rate: 0.01\n",
      "Epoch [7578/10000], Loss: 69.09019470214844, Learning Rate: 0.01\n",
      "Epoch [7579/10000], Loss: 69.07888793945312, Learning Rate: 0.01\n",
      "Epoch [7580/10000], Loss: 69.06491088867188, Learning Rate: 0.01\n",
      "Epoch [7581/10000], Loss: 69.05927276611328, Learning Rate: 0.01\n",
      "Epoch [7582/10000], Loss: 69.06269073486328, Learning Rate: 0.01\n",
      "Epoch [7583/10000], Loss: 69.0670394897461, Learning Rate: 0.01\n",
      "Epoch [7584/10000], Loss: 69.06495666503906, Learning Rate: 0.01\n",
      "Epoch [7585/10000], Loss: 69.0567626953125, Learning Rate: 0.01\n",
      "Epoch [7586/10000], Loss: 69.04874420166016, Learning Rate: 0.01\n",
      "Epoch [7587/10000], Loss: 69.04583740234375, Learning Rate: 0.01\n",
      "Epoch [7588/10000], Loss: 69.04718780517578, Learning Rate: 0.01\n",
      "Epoch [7589/10000], Loss: 69.04796600341797, Learning Rate: 0.01\n",
      "Epoch [7590/10000], Loss: 69.04518127441406, Learning Rate: 0.01\n",
      "Epoch [7591/10000], Loss: 69.03950500488281, Learning Rate: 0.01\n",
      "Epoch [7592/10000], Loss: 69.0342788696289, Learning Rate: 0.01\n",
      "Epoch [7593/10000], Loss: 69.03211212158203, Learning Rate: 0.01\n",
      "Epoch [7594/10000], Loss: 69.03185272216797, Learning Rate: 0.01\n",
      "Epoch [7595/10000], Loss: 69.03114318847656, Learning Rate: 0.01\n",
      "Epoch [7596/10000], Loss: 69.02836608886719, Learning Rate: 0.01\n",
      "Epoch [7597/10000], Loss: 69.02424621582031, Learning Rate: 0.01\n",
      "Epoch [7598/10000], Loss: 69.02057647705078, Learning Rate: 0.01\n",
      "Epoch [7599/10000], Loss: 69.0183334350586, Learning Rate: 0.01\n",
      "Epoch [7600/10000], Loss: 69.01722717285156, Learning Rate: 0.01\n",
      "Epoch [7601/10000], Loss: 69.01575469970703, Learning Rate: 0.01\n",
      "Epoch [7602/10000], Loss: 69.01326751708984, Learning Rate: 0.01\n",
      "Epoch [7603/10000], Loss: 69.00997161865234, Learning Rate: 0.01\n",
      "Epoch [7604/10000], Loss: 69.00691223144531, Learning Rate: 0.01\n",
      "Epoch [7605/10000], Loss: 69.00459289550781, Learning Rate: 0.01\n",
      "Epoch [7606/10000], Loss: 69.0029296875, Learning Rate: 0.01\n",
      "Epoch [7607/10000], Loss: 69.0011215209961, Learning Rate: 0.01\n",
      "Epoch [7608/10000], Loss: 68.9988021850586, Learning Rate: 0.01\n",
      "Epoch [7609/10000], Loss: 68.9960708618164, Learning Rate: 0.01\n",
      "Epoch [7610/10000], Loss: 68.9932861328125, Learning Rate: 0.01\n",
      "Epoch [7611/10000], Loss: 68.99093627929688, Learning Rate: 0.01\n",
      "Epoch [7612/10000], Loss: 68.98898315429688, Learning Rate: 0.01\n",
      "Epoch [7613/10000], Loss: 68.98701477050781, Learning Rate: 0.01\n",
      "Epoch [7614/10000], Loss: 68.98480224609375, Learning Rate: 0.01\n",
      "Epoch [7615/10000], Loss: 68.98226928710938, Learning Rate: 0.01\n",
      "Epoch [7616/10000], Loss: 68.97982788085938, Learning Rate: 0.01\n",
      "Epoch [7617/10000], Loss: 68.9774398803711, Learning Rate: 0.01\n",
      "Epoch [7618/10000], Loss: 68.97528839111328, Learning Rate: 0.01\n",
      "Epoch [7619/10000], Loss: 68.97307586669922, Learning Rate: 0.01\n",
      "Epoch [7620/10000], Loss: 68.97105407714844, Learning Rate: 0.01\n",
      "Epoch [7621/10000], Loss: 68.96858978271484, Learning Rate: 0.01\n",
      "Epoch [7622/10000], Loss: 68.9662094116211, Learning Rate: 0.01\n",
      "Epoch [7623/10000], Loss: 68.96387481689453, Learning Rate: 0.01\n",
      "Epoch [7624/10000], Loss: 68.96165466308594, Learning Rate: 0.01\n",
      "Epoch [7625/10000], Loss: 68.95950317382812, Learning Rate: 0.01\n",
      "Epoch [7626/10000], Loss: 68.95731353759766, Learning Rate: 0.01\n",
      "Epoch [7627/10000], Loss: 68.95498657226562, Learning Rate: 0.01\n",
      "Epoch [7628/10000], Loss: 68.95274353027344, Learning Rate: 0.01\n",
      "Epoch [7629/10000], Loss: 68.9503402709961, Learning Rate: 0.01\n",
      "Epoch [7630/10000], Loss: 68.94815063476562, Learning Rate: 0.01\n",
      "Epoch [7631/10000], Loss: 68.94593048095703, Learning Rate: 0.01\n",
      "Epoch [7632/10000], Loss: 68.94373321533203, Learning Rate: 0.01\n",
      "Epoch [7633/10000], Loss: 68.94146728515625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7634/10000], Loss: 68.93922424316406, Learning Rate: 0.01\n",
      "Epoch [7635/10000], Loss: 68.93695068359375, Learning Rate: 0.01\n",
      "Epoch [7636/10000], Loss: 68.93463897705078, Learning Rate: 0.01\n",
      "Epoch [7637/10000], Loss: 68.93242645263672, Learning Rate: 0.01\n",
      "Epoch [7638/10000], Loss: 68.9301986694336, Learning Rate: 0.01\n",
      "Epoch [7639/10000], Loss: 68.92794036865234, Learning Rate: 0.01\n",
      "Epoch [7640/10000], Loss: 68.92571258544922, Learning Rate: 0.01\n",
      "Epoch [7641/10000], Loss: 68.92345428466797, Learning Rate: 0.01\n",
      "Epoch [7642/10000], Loss: 68.9211654663086, Learning Rate: 0.01\n",
      "Epoch [7643/10000], Loss: 68.91897583007812, Learning Rate: 0.01\n",
      "Epoch [7644/10000], Loss: 68.91670989990234, Learning Rate: 0.01\n",
      "Epoch [7645/10000], Loss: 68.91448974609375, Learning Rate: 0.01\n",
      "Epoch [7646/10000], Loss: 68.91221618652344, Learning Rate: 0.01\n",
      "Epoch [7647/10000], Loss: 68.91001892089844, Learning Rate: 0.01\n",
      "Epoch [7648/10000], Loss: 68.90775299072266, Learning Rate: 0.01\n",
      "Epoch [7649/10000], Loss: 68.905517578125, Learning Rate: 0.01\n",
      "Epoch [7650/10000], Loss: 68.9033432006836, Learning Rate: 0.01\n",
      "Epoch [7651/10000], Loss: 68.90098571777344, Learning Rate: 0.01\n",
      "Epoch [7652/10000], Loss: 68.8988037109375, Learning Rate: 0.01\n",
      "Epoch [7653/10000], Loss: 68.89649963378906, Learning Rate: 0.01\n",
      "Epoch [7654/10000], Loss: 68.89434051513672, Learning Rate: 0.01\n",
      "Epoch [7655/10000], Loss: 68.8920669555664, Learning Rate: 0.01\n",
      "Epoch [7656/10000], Loss: 68.88980865478516, Learning Rate: 0.01\n",
      "Epoch [7657/10000], Loss: 68.88762664794922, Learning Rate: 0.01\n",
      "Epoch [7658/10000], Loss: 68.88539123535156, Learning Rate: 0.01\n",
      "Epoch [7659/10000], Loss: 68.88304901123047, Learning Rate: 0.01\n",
      "Epoch [7660/10000], Loss: 68.88084411621094, Learning Rate: 0.01\n",
      "Epoch [7661/10000], Loss: 68.8786849975586, Learning Rate: 0.01\n",
      "Epoch [7662/10000], Loss: 68.87641143798828, Learning Rate: 0.01\n",
      "Epoch [7663/10000], Loss: 68.87422180175781, Learning Rate: 0.01\n",
      "Epoch [7664/10000], Loss: 68.87194061279297, Learning Rate: 0.01\n",
      "Epoch [7665/10000], Loss: 68.86973571777344, Learning Rate: 0.01\n",
      "Epoch [7666/10000], Loss: 68.86746978759766, Learning Rate: 0.01\n",
      "Epoch [7667/10000], Loss: 68.86520385742188, Learning Rate: 0.01\n",
      "Epoch [7668/10000], Loss: 68.8630599975586, Learning Rate: 0.01\n",
      "Epoch [7669/10000], Loss: 68.86079406738281, Learning Rate: 0.01\n",
      "Epoch [7670/10000], Loss: 68.85855865478516, Learning Rate: 0.01\n",
      "Epoch [7671/10000], Loss: 68.85637664794922, Learning Rate: 0.01\n",
      "Epoch [7672/10000], Loss: 68.8541259765625, Learning Rate: 0.01\n",
      "Epoch [7673/10000], Loss: 68.85192108154297, Learning Rate: 0.01\n",
      "Epoch [7674/10000], Loss: 68.84968566894531, Learning Rate: 0.01\n",
      "Epoch [7675/10000], Loss: 68.84740447998047, Learning Rate: 0.01\n",
      "Epoch [7676/10000], Loss: 68.8452377319336, Learning Rate: 0.01\n",
      "Epoch [7677/10000], Loss: 68.84302520751953, Learning Rate: 0.01\n",
      "Epoch [7678/10000], Loss: 68.8406753540039, Learning Rate: 0.01\n",
      "Epoch [7679/10000], Loss: 68.83859252929688, Learning Rate: 0.01\n",
      "Epoch [7680/10000], Loss: 68.83638763427734, Learning Rate: 0.01\n",
      "Epoch [7681/10000], Loss: 68.8340835571289, Learning Rate: 0.01\n",
      "Epoch [7682/10000], Loss: 68.83182525634766, Learning Rate: 0.01\n",
      "Epoch [7683/10000], Loss: 68.82964324951172, Learning Rate: 0.01\n",
      "Epoch [7684/10000], Loss: 68.82743072509766, Learning Rate: 0.01\n",
      "Epoch [7685/10000], Loss: 68.82524871826172, Learning Rate: 0.01\n",
      "Epoch [7686/10000], Loss: 68.822998046875, Learning Rate: 0.01\n",
      "Epoch [7687/10000], Loss: 68.82076263427734, Learning Rate: 0.01\n",
      "Epoch [7688/10000], Loss: 68.8185806274414, Learning Rate: 0.01\n",
      "Epoch [7689/10000], Loss: 68.81632232666016, Learning Rate: 0.01\n",
      "Epoch [7690/10000], Loss: 68.81416320800781, Learning Rate: 0.01\n",
      "Epoch [7691/10000], Loss: 68.81189727783203, Learning Rate: 0.01\n",
      "Epoch [7692/10000], Loss: 68.80963897705078, Learning Rate: 0.01\n",
      "Epoch [7693/10000], Loss: 68.80751037597656, Learning Rate: 0.01\n",
      "Epoch [7694/10000], Loss: 68.80529022216797, Learning Rate: 0.01\n",
      "Epoch [7695/10000], Loss: 68.80296325683594, Learning Rate: 0.01\n",
      "Epoch [7696/10000], Loss: 68.80084228515625, Learning Rate: 0.01\n",
      "Epoch [7697/10000], Loss: 68.79862976074219, Learning Rate: 0.01\n",
      "Epoch [7698/10000], Loss: 68.79634094238281, Learning Rate: 0.01\n",
      "Epoch [7699/10000], Loss: 68.7942123413086, Learning Rate: 0.01\n",
      "Epoch [7700/10000], Loss: 68.7919692993164, Learning Rate: 0.01\n",
      "Epoch [7701/10000], Loss: 68.78973388671875, Learning Rate: 0.01\n",
      "Epoch [7702/10000], Loss: 68.7874984741211, Learning Rate: 0.01\n",
      "Epoch [7703/10000], Loss: 68.78528594970703, Learning Rate: 0.01\n",
      "Epoch [7704/10000], Loss: 68.78314208984375, Learning Rate: 0.01\n",
      "Epoch [7705/10000], Loss: 68.78085327148438, Learning Rate: 0.01\n",
      "Epoch [7706/10000], Loss: 68.77873992919922, Learning Rate: 0.01\n",
      "Epoch [7707/10000], Loss: 68.77648162841797, Learning Rate: 0.01\n",
      "Epoch [7708/10000], Loss: 68.77426147460938, Learning Rate: 0.01\n",
      "Epoch [7709/10000], Loss: 68.77200317382812, Learning Rate: 0.01\n",
      "Epoch [7710/10000], Loss: 68.76988983154297, Learning Rate: 0.01\n",
      "Epoch [7711/10000], Loss: 68.76764678955078, Learning Rate: 0.01\n",
      "Epoch [7712/10000], Loss: 68.76541137695312, Learning Rate: 0.01\n",
      "Epoch [7713/10000], Loss: 68.76325988769531, Learning Rate: 0.01\n",
      "Epoch [7714/10000], Loss: 68.76097106933594, Learning Rate: 0.01\n",
      "Epoch [7715/10000], Loss: 68.7588119506836, Learning Rate: 0.01\n",
      "Epoch [7716/10000], Loss: 68.75662994384766, Learning Rate: 0.01\n",
      "Epoch [7717/10000], Loss: 68.7543716430664, Learning Rate: 0.01\n",
      "Epoch [7718/10000], Loss: 68.75222778320312, Learning Rate: 0.01\n",
      "Epoch [7719/10000], Loss: 68.74996948242188, Learning Rate: 0.01\n",
      "Epoch [7720/10000], Loss: 68.74781036376953, Learning Rate: 0.01\n",
      "Epoch [7721/10000], Loss: 68.74565124511719, Learning Rate: 0.01\n",
      "Epoch [7722/10000], Loss: 68.74335479736328, Learning Rate: 0.01\n",
      "Epoch [7723/10000], Loss: 68.74119567871094, Learning Rate: 0.01\n",
      "Epoch [7724/10000], Loss: 68.73899841308594, Learning Rate: 0.01\n",
      "Epoch [7725/10000], Loss: 68.73684692382812, Learning Rate: 0.01\n",
      "Epoch [7726/10000], Loss: 68.73458099365234, Learning Rate: 0.01\n",
      "Epoch [7727/10000], Loss: 68.73237609863281, Learning Rate: 0.01\n",
      "Epoch [7728/10000], Loss: 68.73023986816406, Learning Rate: 0.01\n",
      "Epoch [7729/10000], Loss: 68.72798156738281, Learning Rate: 0.01\n",
      "Epoch [7730/10000], Loss: 68.72581481933594, Learning Rate: 0.01\n",
      "Epoch [7731/10000], Loss: 68.72364807128906, Learning Rate: 0.01\n",
      "Epoch [7732/10000], Loss: 68.72144317626953, Learning Rate: 0.01\n",
      "Epoch [7733/10000], Loss: 68.71916198730469, Learning Rate: 0.01\n",
      "Epoch [7734/10000], Loss: 68.71699523925781, Learning Rate: 0.01\n",
      "Epoch [7735/10000], Loss: 68.71482849121094, Learning Rate: 0.01\n",
      "Epoch [7736/10000], Loss: 68.71265411376953, Learning Rate: 0.01\n",
      "Epoch [7737/10000], Loss: 68.71046447753906, Learning Rate: 0.01\n",
      "Epoch [7738/10000], Loss: 68.70824432373047, Learning Rate: 0.01\n",
      "Epoch [7739/10000], Loss: 68.70616149902344, Learning Rate: 0.01\n",
      "Epoch [7740/10000], Loss: 68.70384216308594, Learning Rate: 0.01\n",
      "Epoch [7741/10000], Loss: 68.70169067382812, Learning Rate: 0.01\n",
      "Epoch [7742/10000], Loss: 68.69945526123047, Learning Rate: 0.01\n",
      "Epoch [7743/10000], Loss: 68.69730377197266, Learning Rate: 0.01\n",
      "Epoch [7744/10000], Loss: 68.69512176513672, Learning Rate: 0.01\n",
      "Epoch [7745/10000], Loss: 68.69287109375, Learning Rate: 0.01\n",
      "Epoch [7746/10000], Loss: 68.69074249267578, Learning Rate: 0.01\n",
      "Epoch [7747/10000], Loss: 68.68846130371094, Learning Rate: 0.01\n",
      "Epoch [7748/10000], Loss: 68.68636322021484, Learning Rate: 0.01\n",
      "Epoch [7749/10000], Loss: 68.68414306640625, Learning Rate: 0.01\n",
      "Epoch [7750/10000], Loss: 68.68197631835938, Learning Rate: 0.01\n",
      "Epoch [7751/10000], Loss: 68.67976379394531, Learning Rate: 0.01\n",
      "Epoch [7752/10000], Loss: 68.6775131225586, Learning Rate: 0.01\n",
      "Epoch [7753/10000], Loss: 68.67544555664062, Learning Rate: 0.01\n",
      "Epoch [7754/10000], Loss: 68.6732406616211, Learning Rate: 0.01\n",
      "Epoch [7755/10000], Loss: 68.67098999023438, Learning Rate: 0.01\n",
      "Epoch [7756/10000], Loss: 68.66880798339844, Learning Rate: 0.01\n",
      "Epoch [7757/10000], Loss: 68.66663360595703, Learning Rate: 0.01\n",
      "Epoch [7758/10000], Loss: 68.66448974609375, Learning Rate: 0.01\n",
      "Epoch [7759/10000], Loss: 68.6622543334961, Learning Rate: 0.01\n",
      "Epoch [7760/10000], Loss: 68.66009521484375, Learning Rate: 0.01\n",
      "Epoch [7761/10000], Loss: 68.65786743164062, Learning Rate: 0.01\n",
      "Epoch [7762/10000], Loss: 68.65574645996094, Learning Rate: 0.01\n",
      "Epoch [7763/10000], Loss: 68.65359497070312, Learning Rate: 0.01\n",
      "Epoch [7764/10000], Loss: 68.65135955810547, Learning Rate: 0.01\n",
      "Epoch [7765/10000], Loss: 68.64917755126953, Learning Rate: 0.01\n",
      "Epoch [7766/10000], Loss: 68.6469955444336, Learning Rate: 0.01\n",
      "Epoch [7767/10000], Loss: 68.64481353759766, Learning Rate: 0.01\n",
      "Epoch [7768/10000], Loss: 68.64266204833984, Learning Rate: 0.01\n",
      "Epoch [7769/10000], Loss: 68.6405258178711, Learning Rate: 0.01\n",
      "Epoch [7770/10000], Loss: 68.63831329345703, Learning Rate: 0.01\n",
      "Epoch [7771/10000], Loss: 68.63611602783203, Learning Rate: 0.01\n",
      "Epoch [7772/10000], Loss: 68.63392639160156, Learning Rate: 0.01\n",
      "Epoch [7773/10000], Loss: 68.63175201416016, Learning Rate: 0.01\n",
      "Epoch [7774/10000], Loss: 68.62957000732422, Learning Rate: 0.01\n",
      "Epoch [7775/10000], Loss: 68.62739562988281, Learning Rate: 0.01\n",
      "Epoch [7776/10000], Loss: 68.6252670288086, Learning Rate: 0.01\n",
      "Epoch [7777/10000], Loss: 68.62308502197266, Learning Rate: 0.01\n",
      "Epoch [7778/10000], Loss: 68.620849609375, Learning Rate: 0.01\n",
      "Epoch [7779/10000], Loss: 68.61873626708984, Learning Rate: 0.01\n",
      "Epoch [7780/10000], Loss: 68.61658477783203, Learning Rate: 0.01\n",
      "Epoch [7781/10000], Loss: 68.61438751220703, Learning Rate: 0.01\n",
      "Epoch [7782/10000], Loss: 68.61225891113281, Learning Rate: 0.01\n",
      "Epoch [7783/10000], Loss: 68.61003112792969, Learning Rate: 0.01\n",
      "Epoch [7784/10000], Loss: 68.60791778564453, Learning Rate: 0.01\n",
      "Epoch [7785/10000], Loss: 68.60563659667969, Learning Rate: 0.01\n",
      "Epoch [7786/10000], Loss: 68.60345458984375, Learning Rate: 0.01\n",
      "Epoch [7787/10000], Loss: 68.6013412475586, Learning Rate: 0.01\n",
      "Epoch [7788/10000], Loss: 68.59910583496094, Learning Rate: 0.01\n",
      "Epoch [7789/10000], Loss: 68.596923828125, Learning Rate: 0.01\n",
      "Epoch [7790/10000], Loss: 68.59481811523438, Learning Rate: 0.01\n",
      "Epoch [7791/10000], Loss: 68.5926284790039, Learning Rate: 0.01\n",
      "Epoch [7792/10000], Loss: 68.59051513671875, Learning Rate: 0.01\n",
      "Epoch [7793/10000], Loss: 68.58826446533203, Learning Rate: 0.01\n",
      "Epoch [7794/10000], Loss: 68.58612823486328, Learning Rate: 0.01\n",
      "Epoch [7795/10000], Loss: 68.58401489257812, Learning Rate: 0.01\n",
      "Epoch [7796/10000], Loss: 68.58183288574219, Learning Rate: 0.01\n",
      "Epoch [7797/10000], Loss: 68.57966613769531, Learning Rate: 0.01\n",
      "Epoch [7798/10000], Loss: 68.57752990722656, Learning Rate: 0.01\n",
      "Epoch [7799/10000], Loss: 68.57537078857422, Learning Rate: 0.01\n",
      "Epoch [7800/10000], Loss: 68.57319641113281, Learning Rate: 0.01\n",
      "Epoch [7801/10000], Loss: 68.57096099853516, Learning Rate: 0.01\n",
      "Epoch [7802/10000], Loss: 68.56887817382812, Learning Rate: 0.01\n",
      "Epoch [7803/10000], Loss: 68.56670379638672, Learning Rate: 0.01\n",
      "Epoch [7804/10000], Loss: 68.56448364257812, Learning Rate: 0.01\n",
      "Epoch [7805/10000], Loss: 68.56246948242188, Learning Rate: 0.01\n",
      "Epoch [7806/10000], Loss: 68.56013488769531, Learning Rate: 0.01\n",
      "Epoch [7807/10000], Loss: 68.55802154541016, Learning Rate: 0.01\n",
      "Epoch [7808/10000], Loss: 68.55583190917969, Learning Rate: 0.01\n",
      "Epoch [7809/10000], Loss: 68.55372619628906, Learning Rate: 0.01\n",
      "Epoch [7810/10000], Loss: 68.55158233642578, Learning Rate: 0.01\n",
      "Epoch [7811/10000], Loss: 68.54938507080078, Learning Rate: 0.01\n",
      "Epoch [7812/10000], Loss: 68.54718780517578, Learning Rate: 0.01\n",
      "Epoch [7813/10000], Loss: 68.54509735107422, Learning Rate: 0.01\n",
      "Epoch [7814/10000], Loss: 68.54286193847656, Learning Rate: 0.01\n",
      "Epoch [7815/10000], Loss: 68.54075622558594, Learning Rate: 0.01\n",
      "Epoch [7816/10000], Loss: 68.53868865966797, Learning Rate: 0.01\n",
      "Epoch [7817/10000], Loss: 68.53648376464844, Learning Rate: 0.01\n",
      "Epoch [7818/10000], Loss: 68.53435516357422, Learning Rate: 0.01\n",
      "Epoch [7819/10000], Loss: 68.53227233886719, Learning Rate: 0.01\n",
      "Epoch [7820/10000], Loss: 68.53007507324219, Learning Rate: 0.01\n",
      "Epoch [7821/10000], Loss: 68.52815246582031, Learning Rate: 0.01\n",
      "Epoch [7822/10000], Loss: 68.52605438232422, Learning Rate: 0.01\n",
      "Epoch [7823/10000], Loss: 68.52408599853516, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7824/10000], Loss: 68.52218627929688, Learning Rate: 0.01\n",
      "Epoch [7825/10000], Loss: 68.52041625976562, Learning Rate: 0.01\n",
      "Epoch [7826/10000], Loss: 68.51880645751953, Learning Rate: 0.01\n",
      "Epoch [7827/10000], Loss: 68.51744842529297, Learning Rate: 0.01\n",
      "Epoch [7828/10000], Loss: 68.51651763916016, Learning Rate: 0.01\n",
      "Epoch [7829/10000], Loss: 68.5162353515625, Learning Rate: 0.01\n",
      "Epoch [7830/10000], Loss: 68.5169448852539, Learning Rate: 0.01\n",
      "Epoch [7831/10000], Loss: 68.51921844482422, Learning Rate: 0.01\n",
      "Epoch [7832/10000], Loss: 68.52381134033203, Learning Rate: 0.01\n",
      "Epoch [7833/10000], Loss: 68.53223419189453, Learning Rate: 0.01\n",
      "Epoch [7834/10000], Loss: 68.54642486572266, Learning Rate: 0.01\n",
      "Epoch [7835/10000], Loss: 68.56983947753906, Learning Rate: 0.01\n",
      "Epoch [7836/10000], Loss: 68.60785675048828, Learning Rate: 0.01\n",
      "Epoch [7837/10000], Loss: 68.6688003540039, Learning Rate: 0.01\n",
      "Epoch [7838/10000], Loss: 68.76627349853516, Learning Rate: 0.01\n",
      "Epoch [7839/10000], Loss: 68.92105865478516, Learning Rate: 0.01\n",
      "Epoch [7840/10000], Loss: 69.16543579101562, Learning Rate: 0.01\n",
      "Epoch [7841/10000], Loss: 69.54676055908203, Learning Rate: 0.01\n",
      "Epoch [7842/10000], Loss: 70.13289642333984, Learning Rate: 0.01\n",
      "Epoch [7843/10000], Loss: 71.00244140625, Learning Rate: 0.01\n",
      "Epoch [7844/10000], Loss: 72.23441314697266, Learning Rate: 0.01\n",
      "Epoch [7845/10000], Loss: 73.81655883789062, Learning Rate: 0.01\n",
      "Epoch [7846/10000], Loss: 75.5655517578125, Learning Rate: 0.01\n",
      "Epoch [7847/10000], Loss: 76.91519165039062, Learning Rate: 0.01\n",
      "Epoch [7848/10000], Loss: 77.08972930908203, Learning Rate: 0.01\n",
      "Epoch [7849/10000], Loss: 75.4620132446289, Learning Rate: 0.01\n",
      "Epoch [7850/10000], Loss: 72.4938735961914, Learning Rate: 0.01\n",
      "Epoch [7851/10000], Loss: 69.66182708740234, Learning Rate: 0.01\n",
      "Epoch [7852/10000], Loss: 68.46611022949219, Learning Rate: 0.01\n",
      "Epoch [7853/10000], Loss: 69.18257141113281, Learning Rate: 0.01\n",
      "Epoch [7854/10000], Loss: 70.79972076416016, Learning Rate: 0.01\n",
      "Epoch [7855/10000], Loss: 71.8814468383789, Learning Rate: 0.01\n",
      "Epoch [7856/10000], Loss: 71.55923461914062, Learning Rate: 0.01\n",
      "Epoch [7857/10000], Loss: 70.14966583251953, Learning Rate: 0.01\n",
      "Epoch [7858/10000], Loss: 68.80158996582031, Learning Rate: 0.01\n",
      "Epoch [7859/10000], Loss: 68.47283172607422, Learning Rate: 0.01\n",
      "Epoch [7860/10000], Loss: 69.13092803955078, Learning Rate: 0.01\n",
      "Epoch [7861/10000], Loss: 69.94960021972656, Learning Rate: 0.01\n",
      "Epoch [7862/10000], Loss: 70.12698364257812, Learning Rate: 0.01\n",
      "Epoch [7863/10000], Loss: 69.53917694091797, Learning Rate: 0.01\n",
      "Epoch [7864/10000], Loss: 68.75653076171875, Learning Rate: 0.01\n",
      "Epoch [7865/10000], Loss: 68.42822265625, Learning Rate: 0.01\n",
      "Epoch [7866/10000], Loss: 68.69536590576172, Learning Rate: 0.01\n",
      "Epoch [7867/10000], Loss: 69.15592193603516, Learning Rate: 0.01\n",
      "Epoch [7868/10000], Loss: 69.31715393066406, Learning Rate: 0.01\n",
      "Epoch [7869/10000], Loss: 69.04621887207031, Learning Rate: 0.01\n",
      "Epoch [7870/10000], Loss: 68.62194061279297, Learning Rate: 0.01\n",
      "Epoch [7871/10000], Loss: 68.41583251953125, Learning Rate: 0.01\n",
      "Epoch [7872/10000], Loss: 68.53717803955078, Learning Rate: 0.01\n",
      "Epoch [7873/10000], Loss: 68.78461456298828, Learning Rate: 0.01\n",
      "Epoch [7874/10000], Loss: 68.8857421875, Learning Rate: 0.01\n",
      "Epoch [7875/10000], Loss: 68.7518081665039, Learning Rate: 0.01\n",
      "Epoch [7876/10000], Loss: 68.52433013916016, Learning Rate: 0.01\n",
      "Epoch [7877/10000], Loss: 68.40374755859375, Learning Rate: 0.01\n",
      "Epoch [7878/10000], Loss: 68.45855712890625, Learning Rate: 0.01\n",
      "Epoch [7879/10000], Loss: 68.58976745605469, Learning Rate: 0.01\n",
      "Epoch [7880/10000], Loss: 68.65046691894531, Learning Rate: 0.01\n",
      "Epoch [7881/10000], Loss: 68.586181640625, Learning Rate: 0.01\n",
      "Epoch [7882/10000], Loss: 68.46436309814453, Learning Rate: 0.01\n",
      "Epoch [7883/10000], Loss: 68.39181518554688, Learning Rate: 0.01\n",
      "Epoch [7884/10000], Loss: 68.41241455078125, Learning Rate: 0.01\n",
      "Epoch [7885/10000], Loss: 68.4803466796875, Learning Rate: 0.01\n",
      "Epoch [7886/10000], Loss: 68.51846313476562, Learning Rate: 0.01\n",
      "Epoch [7887/10000], Loss: 68.49056243896484, Learning Rate: 0.01\n",
      "Epoch [7888/10000], Loss: 68.42616271972656, Learning Rate: 0.01\n",
      "Epoch [7889/10000], Loss: 68.38067626953125, Learning Rate: 0.01\n",
      "Epoch [7890/10000], Loss: 68.38335418701172, Learning Rate: 0.01\n",
      "Epoch [7891/10000], Loss: 68.41670227050781, Learning Rate: 0.01\n",
      "Epoch [7892/10000], Loss: 68.44080352783203, Learning Rate: 0.01\n",
      "Epoch [7893/10000], Loss: 68.4319076538086, Learning Rate: 0.01\n",
      "Epoch [7894/10000], Loss: 68.39883422851562, Learning Rate: 0.01\n",
      "Epoch [7895/10000], Loss: 68.36983489990234, Learning Rate: 0.01\n",
      "Epoch [7896/10000], Loss: 68.36407470703125, Learning Rate: 0.01\n",
      "Epoch [7897/10000], Loss: 68.37812042236328, Learning Rate: 0.01\n",
      "Epoch [7898/10000], Loss: 68.39285278320312, Learning Rate: 0.01\n",
      "Epoch [7899/10000], Loss: 68.3924560546875, Learning Rate: 0.01\n",
      "Epoch [7900/10000], Loss: 68.37677001953125, Learning Rate: 0.01\n",
      "Epoch [7901/10000], Loss: 68.35858154296875, Learning Rate: 0.01\n",
      "Epoch [7902/10000], Loss: 68.35020446777344, Learning Rate: 0.01\n",
      "Epoch [7903/10000], Loss: 68.3537368774414, Learning Rate: 0.01\n",
      "Epoch [7904/10000], Loss: 68.3613510131836, Learning Rate: 0.01\n",
      "Epoch [7905/10000], Loss: 68.3636474609375, Learning Rate: 0.01\n",
      "Epoch [7906/10000], Loss: 68.35726165771484, Learning Rate: 0.01\n",
      "Epoch [7907/10000], Loss: 68.3466567993164, Learning Rate: 0.01\n",
      "Epoch [7908/10000], Loss: 68.338623046875, Learning Rate: 0.01\n",
      "Epoch [7909/10000], Loss: 68.33687591552734, Learning Rate: 0.01\n",
      "Epoch [7910/10000], Loss: 68.33943176269531, Learning Rate: 0.01\n",
      "Epoch [7911/10000], Loss: 68.34129333496094, Learning Rate: 0.01\n",
      "Epoch [7912/10000], Loss: 68.33946228027344, Learning Rate: 0.01\n",
      "Epoch [7913/10000], Loss: 68.33378601074219, Learning Rate: 0.01\n",
      "Epoch [7914/10000], Loss: 68.3276138305664, Learning Rate: 0.01\n",
      "Epoch [7915/10000], Loss: 68.32382202148438, Learning Rate: 0.01\n",
      "Epoch [7916/10000], Loss: 68.32298278808594, Learning Rate: 0.01\n",
      "Epoch [7917/10000], Loss: 68.3234634399414, Learning Rate: 0.01\n",
      "Epoch [7918/10000], Loss: 68.32280731201172, Learning Rate: 0.01\n",
      "Epoch [7919/10000], Loss: 68.3200454711914, Learning Rate: 0.01\n",
      "Epoch [7920/10000], Loss: 68.31599426269531, Learning Rate: 0.01\n",
      "Epoch [7921/10000], Loss: 68.31212615966797, Learning Rate: 0.01\n",
      "Epoch [7922/10000], Loss: 68.3097152709961, Learning Rate: 0.01\n",
      "Epoch [7923/10000], Loss: 68.30856323242188, Learning Rate: 0.01\n",
      "Epoch [7924/10000], Loss: 68.30766296386719, Learning Rate: 0.01\n",
      "Epoch [7925/10000], Loss: 68.30612182617188, Learning Rate: 0.01\n",
      "Epoch [7926/10000], Loss: 68.3035888671875, Learning Rate: 0.01\n",
      "Epoch [7927/10000], Loss: 68.30048370361328, Learning Rate: 0.01\n",
      "Epoch [7928/10000], Loss: 68.2977294921875, Learning Rate: 0.01\n",
      "Epoch [7929/10000], Loss: 68.29560089111328, Learning Rate: 0.01\n",
      "Epoch [7930/10000], Loss: 68.29393005371094, Learning Rate: 0.01\n",
      "Epoch [7931/10000], Loss: 68.29251098632812, Learning Rate: 0.01\n",
      "Epoch [7932/10000], Loss: 68.29071044921875, Learning Rate: 0.01\n",
      "Epoch [7933/10000], Loss: 68.28848266601562, Learning Rate: 0.01\n",
      "Epoch [7934/10000], Loss: 68.28591918945312, Learning Rate: 0.01\n",
      "Epoch [7935/10000], Loss: 68.28352355957031, Learning Rate: 0.01\n",
      "Epoch [7936/10000], Loss: 68.28141784667969, Learning Rate: 0.01\n",
      "Epoch [7937/10000], Loss: 68.27955627441406, Learning Rate: 0.01\n",
      "Epoch [7938/10000], Loss: 68.27787780761719, Learning Rate: 0.01\n",
      "Epoch [7939/10000], Loss: 68.27603149414062, Learning Rate: 0.01\n",
      "Epoch [7940/10000], Loss: 68.27391052246094, Learning Rate: 0.01\n",
      "Epoch [7941/10000], Loss: 68.27168273925781, Learning Rate: 0.01\n",
      "Epoch [7942/10000], Loss: 68.26947021484375, Learning Rate: 0.01\n",
      "Epoch [7943/10000], Loss: 68.26734924316406, Learning Rate: 0.01\n",
      "Epoch [7944/10000], Loss: 68.26537322998047, Learning Rate: 0.01\n",
      "Epoch [7945/10000], Loss: 68.26345825195312, Learning Rate: 0.01\n",
      "Epoch [7946/10000], Loss: 68.2615737915039, Learning Rate: 0.01\n",
      "Epoch [7947/10000], Loss: 68.25959777832031, Learning Rate: 0.01\n",
      "Epoch [7948/10000], Loss: 68.2574691772461, Learning Rate: 0.01\n",
      "Epoch [7949/10000], Loss: 68.25540924072266, Learning Rate: 0.01\n",
      "Epoch [7950/10000], Loss: 68.25328063964844, Learning Rate: 0.01\n",
      "Epoch [7951/10000], Loss: 68.2513198852539, Learning Rate: 0.01\n",
      "Epoch [7952/10000], Loss: 68.24931335449219, Learning Rate: 0.01\n",
      "Epoch [7953/10000], Loss: 68.24732208251953, Learning Rate: 0.01\n",
      "Epoch [7954/10000], Loss: 68.24540710449219, Learning Rate: 0.01\n",
      "Epoch [7955/10000], Loss: 68.24335479736328, Learning Rate: 0.01\n",
      "Epoch [7956/10000], Loss: 68.24122619628906, Learning Rate: 0.01\n",
      "Epoch [7957/10000], Loss: 68.23918151855469, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7958/10000], Loss: 68.23714447021484, Learning Rate: 0.01\n",
      "Epoch [7959/10000], Loss: 68.23526763916016, Learning Rate: 0.01\n",
      "Epoch [7960/10000], Loss: 68.23326873779297, Learning Rate: 0.01\n",
      "Epoch [7961/10000], Loss: 68.23126220703125, Learning Rate: 0.01\n",
      "Epoch [7962/10000], Loss: 68.22931671142578, Learning Rate: 0.01\n",
      "Epoch [7963/10000], Loss: 68.22724914550781, Learning Rate: 0.01\n",
      "Epoch [7964/10000], Loss: 68.22516632080078, Learning Rate: 0.01\n",
      "Epoch [7965/10000], Loss: 68.22321319580078, Learning Rate: 0.01\n",
      "Epoch [7966/10000], Loss: 68.2212142944336, Learning Rate: 0.01\n",
      "Epoch [7967/10000], Loss: 68.21913146972656, Learning Rate: 0.01\n",
      "Epoch [7968/10000], Loss: 68.21719360351562, Learning Rate: 0.01\n",
      "Epoch [7969/10000], Loss: 68.2151870727539, Learning Rate: 0.01\n",
      "Epoch [7970/10000], Loss: 68.21317291259766, Learning Rate: 0.01\n",
      "Epoch [7971/10000], Loss: 68.21121978759766, Learning Rate: 0.01\n",
      "Epoch [7972/10000], Loss: 68.20915222167969, Learning Rate: 0.01\n",
      "Epoch [7973/10000], Loss: 68.20716094970703, Learning Rate: 0.01\n",
      "Epoch [7974/10000], Loss: 68.2052001953125, Learning Rate: 0.01\n",
      "Epoch [7975/10000], Loss: 68.2032241821289, Learning Rate: 0.01\n",
      "Epoch [7976/10000], Loss: 68.20121002197266, Learning Rate: 0.01\n",
      "Epoch [7977/10000], Loss: 68.19920349121094, Learning Rate: 0.01\n",
      "Epoch [7978/10000], Loss: 68.19725799560547, Learning Rate: 0.01\n",
      "Epoch [7979/10000], Loss: 68.19515991210938, Learning Rate: 0.01\n",
      "Epoch [7980/10000], Loss: 68.19315338134766, Learning Rate: 0.01\n",
      "Epoch [7981/10000], Loss: 68.191162109375, Learning Rate: 0.01\n",
      "Epoch [7982/10000], Loss: 68.18919372558594, Learning Rate: 0.01\n",
      "Epoch [7983/10000], Loss: 68.18717956542969, Learning Rate: 0.01\n",
      "Epoch [7984/10000], Loss: 68.18522644042969, Learning Rate: 0.01\n",
      "Epoch [7985/10000], Loss: 68.18319702148438, Learning Rate: 0.01\n",
      "Epoch [7986/10000], Loss: 68.18122100830078, Learning Rate: 0.01\n",
      "Epoch [7987/10000], Loss: 68.17919921875, Learning Rate: 0.01\n",
      "Epoch [7988/10000], Loss: 68.17723846435547, Learning Rate: 0.01\n",
      "Epoch [7989/10000], Loss: 68.17522430419922, Learning Rate: 0.01\n",
      "Epoch [7990/10000], Loss: 68.17321014404297, Learning Rate: 0.01\n",
      "Epoch [7991/10000], Loss: 68.17121887207031, Learning Rate: 0.01\n",
      "Epoch [7992/10000], Loss: 68.16929626464844, Learning Rate: 0.01\n",
      "Epoch [7993/10000], Loss: 68.16730499267578, Learning Rate: 0.01\n",
      "Epoch [7994/10000], Loss: 68.16520690917969, Learning Rate: 0.01\n",
      "Epoch [7995/10000], Loss: 68.16323852539062, Learning Rate: 0.01\n",
      "Epoch [7996/10000], Loss: 68.1612548828125, Learning Rate: 0.01\n",
      "Epoch [7997/10000], Loss: 68.15926361083984, Learning Rate: 0.01\n",
      "Epoch [7998/10000], Loss: 68.15729522705078, Learning Rate: 0.01\n",
      "Epoch [7999/10000], Loss: 68.15533447265625, Learning Rate: 0.01\n",
      "Epoch [8000/10000], Loss: 68.15326690673828, Learning Rate: 0.01\n",
      "Epoch [8001/10000], Loss: 68.15133666992188, Learning Rate: 0.01\n",
      "Epoch [8002/10000], Loss: 68.14933013916016, Learning Rate: 0.01\n",
      "Epoch [8003/10000], Loss: 68.14743041992188, Learning Rate: 0.01\n",
      "Epoch [8004/10000], Loss: 68.14535522460938, Learning Rate: 0.01\n",
      "Epoch [8005/10000], Loss: 68.14338684082031, Learning Rate: 0.01\n",
      "Epoch [8006/10000], Loss: 68.14136505126953, Learning Rate: 0.01\n",
      "Epoch [8007/10000], Loss: 68.13944244384766, Learning Rate: 0.01\n",
      "Epoch [8008/10000], Loss: 68.1374282836914, Learning Rate: 0.01\n",
      "Epoch [8009/10000], Loss: 68.13543701171875, Learning Rate: 0.01\n",
      "Epoch [8010/10000], Loss: 68.13339233398438, Learning Rate: 0.01\n",
      "Epoch [8011/10000], Loss: 68.13143157958984, Learning Rate: 0.01\n",
      "Epoch [8012/10000], Loss: 68.12944030761719, Learning Rate: 0.01\n",
      "Epoch [8013/10000], Loss: 68.12742614746094, Learning Rate: 0.01\n",
      "Epoch [8014/10000], Loss: 68.12550354003906, Learning Rate: 0.01\n",
      "Epoch [8015/10000], Loss: 68.12349700927734, Learning Rate: 0.01\n",
      "Epoch [8016/10000], Loss: 68.12152099609375, Learning Rate: 0.01\n",
      "Epoch [8017/10000], Loss: 68.11956787109375, Learning Rate: 0.01\n",
      "Epoch [8018/10000], Loss: 68.11753845214844, Learning Rate: 0.01\n",
      "Epoch [8019/10000], Loss: 68.11553192138672, Learning Rate: 0.01\n",
      "Epoch [8020/10000], Loss: 68.11356353759766, Learning Rate: 0.01\n",
      "Epoch [8021/10000], Loss: 68.11158752441406, Learning Rate: 0.01\n",
      "Epoch [8022/10000], Loss: 68.109619140625, Learning Rate: 0.01\n",
      "Epoch [8023/10000], Loss: 68.10762023925781, Learning Rate: 0.01\n",
      "Epoch [8024/10000], Loss: 68.10563659667969, Learning Rate: 0.01\n",
      "Epoch [8025/10000], Loss: 68.1036148071289, Learning Rate: 0.01\n",
      "Epoch [8026/10000], Loss: 68.10164642333984, Learning Rate: 0.01\n",
      "Epoch [8027/10000], Loss: 68.09968566894531, Learning Rate: 0.01\n",
      "Epoch [8028/10000], Loss: 68.0976791381836, Learning Rate: 0.01\n",
      "Epoch [8029/10000], Loss: 68.09571838378906, Learning Rate: 0.01\n",
      "Epoch [8030/10000], Loss: 68.09373474121094, Learning Rate: 0.01\n",
      "Epoch [8031/10000], Loss: 68.09181213378906, Learning Rate: 0.01\n",
      "Epoch [8032/10000], Loss: 68.08977508544922, Learning Rate: 0.01\n",
      "Epoch [8033/10000], Loss: 68.08782196044922, Learning Rate: 0.01\n",
      "Epoch [8034/10000], Loss: 68.08584594726562, Learning Rate: 0.01\n",
      "Epoch [8035/10000], Loss: 68.08385467529297, Learning Rate: 0.01\n",
      "Epoch [8036/10000], Loss: 68.08191680908203, Learning Rate: 0.01\n",
      "Epoch [8037/10000], Loss: 68.07990264892578, Learning Rate: 0.01\n",
      "Epoch [8038/10000], Loss: 68.07794189453125, Learning Rate: 0.01\n",
      "Epoch [8039/10000], Loss: 68.07597351074219, Learning Rate: 0.01\n",
      "Epoch [8040/10000], Loss: 68.0739517211914, Learning Rate: 0.01\n",
      "Epoch [8041/10000], Loss: 68.07193756103516, Learning Rate: 0.01\n",
      "Epoch [8042/10000], Loss: 68.06996154785156, Learning Rate: 0.01\n",
      "Epoch [8043/10000], Loss: 68.0679931640625, Learning Rate: 0.01\n",
      "Epoch [8044/10000], Loss: 68.06607055664062, Learning Rate: 0.01\n",
      "Epoch [8045/10000], Loss: 68.0640869140625, Learning Rate: 0.01\n",
      "Epoch [8046/10000], Loss: 68.06208801269531, Learning Rate: 0.01\n",
      "Epoch [8047/10000], Loss: 68.06009674072266, Learning Rate: 0.01\n",
      "Epoch [8048/10000], Loss: 68.05812072753906, Learning Rate: 0.01\n",
      "Epoch [8049/10000], Loss: 68.0561752319336, Learning Rate: 0.01\n",
      "Epoch [8050/10000], Loss: 68.05420684814453, Learning Rate: 0.01\n",
      "Epoch [8051/10000], Loss: 68.05227661132812, Learning Rate: 0.01\n",
      "Epoch [8052/10000], Loss: 68.05026245117188, Learning Rate: 0.01\n",
      "Epoch [8053/10000], Loss: 68.04827117919922, Learning Rate: 0.01\n",
      "Epoch [8054/10000], Loss: 68.0462417602539, Learning Rate: 0.01\n",
      "Epoch [8055/10000], Loss: 68.04427337646484, Learning Rate: 0.01\n",
      "Epoch [8056/10000], Loss: 68.04235076904297, Learning Rate: 0.01\n",
      "Epoch [8057/10000], Loss: 68.04036712646484, Learning Rate: 0.01\n",
      "Epoch [8058/10000], Loss: 68.03840637207031, Learning Rate: 0.01\n",
      "Epoch [8059/10000], Loss: 68.0363998413086, Learning Rate: 0.01\n",
      "Epoch [8060/10000], Loss: 68.03446197509766, Learning Rate: 0.01\n",
      "Epoch [8061/10000], Loss: 68.03251647949219, Learning Rate: 0.01\n",
      "Epoch [8062/10000], Loss: 68.0305404663086, Learning Rate: 0.01\n",
      "Epoch [8063/10000], Loss: 68.02851867675781, Learning Rate: 0.01\n",
      "Epoch [8064/10000], Loss: 68.0265884399414, Learning Rate: 0.01\n",
      "Epoch [8065/10000], Loss: 68.0245590209961, Learning Rate: 0.01\n",
      "Epoch [8066/10000], Loss: 68.02265167236328, Learning Rate: 0.01\n",
      "Epoch [8067/10000], Loss: 68.02070617675781, Learning Rate: 0.01\n",
      "Epoch [8068/10000], Loss: 68.01871490478516, Learning Rate: 0.01\n",
      "Epoch [8069/10000], Loss: 68.01676940917969, Learning Rate: 0.01\n",
      "Epoch [8070/10000], Loss: 68.01481628417969, Learning Rate: 0.01\n",
      "Epoch [8071/10000], Loss: 68.01278686523438, Learning Rate: 0.01\n",
      "Epoch [8072/10000], Loss: 68.01087188720703, Learning Rate: 0.01\n",
      "Epoch [8073/10000], Loss: 68.00892639160156, Learning Rate: 0.01\n",
      "Epoch [8074/10000], Loss: 68.00690460205078, Learning Rate: 0.01\n",
      "Epoch [8075/10000], Loss: 68.00491333007812, Learning Rate: 0.01\n",
      "Epoch [8076/10000], Loss: 68.00302124023438, Learning Rate: 0.01\n",
      "Epoch [8077/10000], Loss: 68.0009994506836, Learning Rate: 0.01\n",
      "Epoch [8078/10000], Loss: 67.99908447265625, Learning Rate: 0.01\n",
      "Epoch [8079/10000], Loss: 67.99713897705078, Learning Rate: 0.01\n",
      "Epoch [8080/10000], Loss: 67.99510192871094, Learning Rate: 0.01\n",
      "Epoch [8081/10000], Loss: 67.99319458007812, Learning Rate: 0.01\n",
      "Epoch [8082/10000], Loss: 67.99115753173828, Learning Rate: 0.01\n",
      "Epoch [8083/10000], Loss: 67.98924255371094, Learning Rate: 0.01\n",
      "Epoch [8084/10000], Loss: 67.98729705810547, Learning Rate: 0.01\n",
      "Epoch [8085/10000], Loss: 67.98528289794922, Learning Rate: 0.01\n",
      "Epoch [8086/10000], Loss: 67.98336029052734, Learning Rate: 0.01\n",
      "Epoch [8087/10000], Loss: 67.9814453125, Learning Rate: 0.01\n",
      "Epoch [8088/10000], Loss: 67.9794921875, Learning Rate: 0.01\n",
      "Epoch [8089/10000], Loss: 67.97747802734375, Learning Rate: 0.01\n",
      "Epoch [8090/10000], Loss: 67.97543334960938, Learning Rate: 0.01\n",
      "Epoch [8091/10000], Loss: 67.97350311279297, Learning Rate: 0.01\n",
      "Epoch [8092/10000], Loss: 67.9715805053711, Learning Rate: 0.01\n",
      "Epoch [8093/10000], Loss: 67.96959686279297, Learning Rate: 0.01\n",
      "Epoch [8094/10000], Loss: 67.9676742553711, Learning Rate: 0.01\n",
      "Epoch [8095/10000], Loss: 67.96574401855469, Learning Rate: 0.01\n",
      "Epoch [8096/10000], Loss: 67.96382904052734, Learning Rate: 0.01\n",
      "Epoch [8097/10000], Loss: 67.96188354492188, Learning Rate: 0.01\n",
      "Epoch [8098/10000], Loss: 67.9599380493164, Learning Rate: 0.01\n",
      "Epoch [8099/10000], Loss: 67.95806121826172, Learning Rate: 0.01\n",
      "Epoch [8100/10000], Loss: 67.95610809326172, Learning Rate: 0.01\n",
      "Epoch [8101/10000], Loss: 67.95431518554688, Learning Rate: 0.01\n",
      "Epoch [8102/10000], Loss: 67.95243835449219, Learning Rate: 0.01\n",
      "Epoch [8103/10000], Loss: 67.95075988769531, Learning Rate: 0.01\n",
      "Epoch [8104/10000], Loss: 67.94904327392578, Learning Rate: 0.01\n",
      "Epoch [8105/10000], Loss: 67.94755554199219, Learning Rate: 0.01\n",
      "Epoch [8106/10000], Loss: 67.94638061523438, Learning Rate: 0.01\n",
      "Epoch [8107/10000], Loss: 67.94538116455078, Learning Rate: 0.01\n",
      "Epoch [8108/10000], Loss: 67.94487762451172, Learning Rate: 0.01\n",
      "Epoch [8109/10000], Loss: 67.94515228271484, Learning Rate: 0.01\n",
      "Epoch [8110/10000], Loss: 67.94656372070312, Learning Rate: 0.01\n",
      "Epoch [8111/10000], Loss: 67.94965362548828, Learning Rate: 0.01\n",
      "Epoch [8112/10000], Loss: 67.95526885986328, Learning Rate: 0.01\n",
      "Epoch [8113/10000], Loss: 67.96502685546875, Learning Rate: 0.01\n",
      "Epoch [8114/10000], Loss: 67.98094940185547, Learning Rate: 0.01\n",
      "Epoch [8115/10000], Loss: 68.00651550292969, Learning Rate: 0.01\n",
      "Epoch [8116/10000], Loss: 68.0472412109375, Learning Rate: 0.01\n",
      "Epoch [8117/10000], Loss: 68.111328125, Learning Rate: 0.01\n",
      "Epoch [8118/10000], Loss: 68.21206665039062, Learning Rate: 0.01\n",
      "Epoch [8119/10000], Loss: 68.36940002441406, Learning Rate: 0.01\n",
      "Epoch [8120/10000], Loss: 68.61392211914062, Learning Rate: 0.01\n",
      "Epoch [8121/10000], Loss: 68.98870086669922, Learning Rate: 0.01\n",
      "Epoch [8122/10000], Loss: 69.55486297607422, Learning Rate: 0.01\n",
      "Epoch [8123/10000], Loss: 70.3795166015625, Learning Rate: 0.01\n",
      "Epoch [8124/10000], Loss: 71.52632141113281, Learning Rate: 0.01\n",
      "Epoch [8125/10000], Loss: 72.9705810546875, Learning Rate: 0.01\n",
      "Epoch [8126/10000], Loss: 74.5403060913086, Learning Rate: 0.01\n",
      "Epoch [8127/10000], Loss: 75.73210144042969, Learning Rate: 0.01\n",
      "Epoch [8128/10000], Loss: 75.8873519897461, Learning Rate: 0.01\n",
      "Epoch [8129/10000], Loss: 74.46183013916016, Learning Rate: 0.01\n",
      "Epoch [8130/10000], Loss: 71.82676696777344, Learning Rate: 0.01\n",
      "Epoch [8131/10000], Loss: 69.20799255371094, Learning Rate: 0.01\n",
      "Epoch [8132/10000], Loss: 67.93480682373047, Learning Rate: 0.01\n",
      "Epoch [8133/10000], Loss: 68.36827087402344, Learning Rate: 0.01\n",
      "Epoch [8134/10000], Loss: 69.75772094726562, Learning Rate: 0.01\n",
      "Epoch [8135/10000], Loss: 70.88529205322266, Learning Rate: 0.01\n",
      "Epoch [8136/10000], Loss: 70.8777084350586, Learning Rate: 0.01\n",
      "Epoch [8137/10000], Loss: 69.79061126708984, Learning Rate: 0.01\n",
      "Epoch [8138/10000], Loss: 68.48075866699219, Learning Rate: 0.01\n",
      "Epoch [8139/10000], Loss: 67.8829345703125, Learning Rate: 0.01\n",
      "Epoch [8140/10000], Loss: 68.23001861572266, Learning Rate: 0.01\n",
      "Epoch [8141/10000], Loss: 68.98686218261719, Learning Rate: 0.01\n",
      "Epoch [8142/10000], Loss: 69.40370178222656, Learning Rate: 0.01\n",
      "Epoch [8143/10000], Loss: 69.13162994384766, Learning Rate: 0.01\n",
      "Epoch [8144/10000], Loss: 68.44966888427734, Learning Rate: 0.01\n",
      "Epoch [8145/10000], Loss: 67.93277740478516, Learning Rate: 0.01\n",
      "Epoch [8146/10000], Loss: 67.9254150390625, Learning Rate: 0.01\n",
      "Epoch [8147/10000], Loss: 68.2889404296875, Learning Rate: 0.01\n",
      "Epoch [8148/10000], Loss: 68.6067886352539, Learning Rate: 0.01\n",
      "Epoch [8149/10000], Loss: 68.58228302001953, Learning Rate: 0.01\n",
      "Epoch [8150/10000], Loss: 68.26145935058594, Learning Rate: 0.01\n",
      "Epoch [8151/10000], Loss: 67.93801879882812, Learning Rate: 0.01\n",
      "Epoch [8152/10000], Loss: 67.8562240600586, Learning Rate: 0.01\n",
      "Epoch [8153/10000], Loss: 68.0126724243164, Learning Rate: 0.01\n",
      "Epoch [8154/10000], Loss: 68.20598602294922, Learning Rate: 0.01\n",
      "Epoch [8155/10000], Loss: 68.24394226074219, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8156/10000], Loss: 68.10368347167969, Learning Rate: 0.01\n",
      "Epoch [8157/10000], Loss: 67.91871643066406, Learning Rate: 0.01\n",
      "Epoch [8158/10000], Loss: 67.83707427978516, Learning Rate: 0.01\n",
      "Epoch [8159/10000], Loss: 67.89290618896484, Learning Rate: 0.01\n",
      "Epoch [8160/10000], Loss: 67.99955749511719, Learning Rate: 0.01\n",
      "Epoch [8161/10000], Loss: 68.04593658447266, Learning Rate: 0.01\n",
      "Epoch [8162/10000], Loss: 67.99346160888672, Learning Rate: 0.01\n",
      "Epoch [8163/10000], Loss: 67.8941879272461, Learning Rate: 0.01\n",
      "Epoch [8164/10000], Loss: 67.82999420166016, Learning Rate: 0.01\n",
      "Epoch [8165/10000], Loss: 67.83853912353516, Learning Rate: 0.01\n",
      "Epoch [8166/10000], Loss: 67.89116668701172, Learning Rate: 0.01\n",
      "Epoch [8167/10000], Loss: 67.92904663085938, Learning Rate: 0.01\n",
      "Epoch [8168/10000], Loss: 67.91748809814453, Learning Rate: 0.01\n",
      "Epoch [8169/10000], Loss: 67.8692855834961, Learning Rate: 0.01\n",
      "Epoch [8170/10000], Loss: 67.82463836669922, Learning Rate: 0.01\n",
      "Epoch [8171/10000], Loss: 67.81379699707031, Learning Rate: 0.01\n",
      "Epoch [8172/10000], Loss: 67.83406829833984, Learning Rate: 0.01\n",
      "Epoch [8173/10000], Loss: 67.85887908935547, Learning Rate: 0.01\n",
      "Epoch [8174/10000], Loss: 67.86351013183594, Learning Rate: 0.01\n",
      "Epoch [8175/10000], Loss: 67.84429168701172, Learning Rate: 0.01\n",
      "Epoch [8176/10000], Loss: 67.8172607421875, Learning Rate: 0.01\n",
      "Epoch [8177/10000], Loss: 67.80167388916016, Learning Rate: 0.01\n",
      "Epoch [8178/10000], Loss: 67.80426788330078, Learning Rate: 0.01\n",
      "Epoch [8179/10000], Loss: 67.81658172607422, Learning Rate: 0.01\n",
      "Epoch [8180/10000], Loss: 67.82445526123047, Learning Rate: 0.01\n",
      "Epoch [8181/10000], Loss: 67.82003021240234, Learning Rate: 0.01\n",
      "Epoch [8182/10000], Loss: 67.80644226074219, Learning Rate: 0.01\n",
      "Epoch [8183/10000], Loss: 67.79340362548828, Learning Rate: 0.01\n",
      "Epoch [8184/10000], Loss: 67.78820037841797, Learning Rate: 0.01\n",
      "Epoch [8185/10000], Loss: 67.79106903076172, Learning Rate: 0.01\n",
      "Epoch [8186/10000], Loss: 67.79624938964844, Learning Rate: 0.01\n",
      "Epoch [8187/10000], Loss: 67.79745483398438, Learning Rate: 0.01\n",
      "Epoch [8188/10000], Loss: 67.79251098632812, Learning Rate: 0.01\n",
      "Epoch [8189/10000], Loss: 67.78443908691406, Learning Rate: 0.01\n",
      "Epoch [8190/10000], Loss: 67.77790832519531, Learning Rate: 0.01\n",
      "Epoch [8191/10000], Loss: 67.7753677368164, Learning Rate: 0.01\n",
      "Epoch [8192/10000], Loss: 67.7763900756836, Learning Rate: 0.01\n",
      "Epoch [8193/10000], Loss: 67.77774047851562, Learning Rate: 0.01\n",
      "Epoch [8194/10000], Loss: 67.77692413330078, Learning Rate: 0.01\n",
      "Epoch [8195/10000], Loss: 67.77326965332031, Learning Rate: 0.01\n",
      "Epoch [8196/10000], Loss: 67.76830291748047, Learning Rate: 0.01\n",
      "Epoch [8197/10000], Loss: 67.7642822265625, Learning Rate: 0.01\n",
      "Epoch [8198/10000], Loss: 67.76231384277344, Learning Rate: 0.01\n",
      "Epoch [8199/10000], Loss: 67.7618408203125, Learning Rate: 0.01\n",
      "Epoch [8200/10000], Loss: 67.76155853271484, Learning Rate: 0.01\n",
      "Epoch [8201/10000], Loss: 67.76019287109375, Learning Rate: 0.01\n",
      "Epoch [8202/10000], Loss: 67.75747680664062, Learning Rate: 0.01\n",
      "Epoch [8203/10000], Loss: 67.75418853759766, Learning Rate: 0.01\n",
      "Epoch [8204/10000], Loss: 67.75125885009766, Learning Rate: 0.01\n",
      "Epoch [8205/10000], Loss: 67.74923706054688, Learning Rate: 0.01\n",
      "Epoch [8206/10000], Loss: 67.74798583984375, Learning Rate: 0.01\n",
      "Epoch [8207/10000], Loss: 67.74688720703125, Learning Rate: 0.01\n",
      "Epoch [8208/10000], Loss: 67.74542236328125, Learning Rate: 0.01\n",
      "Epoch [8209/10000], Loss: 67.74324035644531, Learning Rate: 0.01\n",
      "Epoch [8210/10000], Loss: 67.74072265625, Learning Rate: 0.01\n",
      "Epoch [8211/10000], Loss: 67.73832702636719, Learning Rate: 0.01\n",
      "Epoch [8212/10000], Loss: 67.73626708984375, Learning Rate: 0.01\n",
      "Epoch [8213/10000], Loss: 67.7345962524414, Learning Rate: 0.01\n",
      "Epoch [8214/10000], Loss: 67.7331771850586, Learning Rate: 0.01\n",
      "Epoch [8215/10000], Loss: 67.73150634765625, Learning Rate: 0.01\n",
      "Epoch [8216/10000], Loss: 67.72966003417969, Learning Rate: 0.01\n",
      "Epoch [8217/10000], Loss: 67.72747039794922, Learning Rate: 0.01\n",
      "Epoch [8218/10000], Loss: 67.72541809082031, Learning Rate: 0.01\n",
      "Epoch [8219/10000], Loss: 67.72333526611328, Learning Rate: 0.01\n",
      "Epoch [8220/10000], Loss: 67.72149658203125, Learning Rate: 0.01\n",
      "Epoch [8221/10000], Loss: 67.7198486328125, Learning Rate: 0.01\n",
      "Epoch [8222/10000], Loss: 67.71813201904297, Learning Rate: 0.01\n",
      "Epoch [8223/10000], Loss: 67.71636199951172, Learning Rate: 0.01\n",
      "Epoch [8224/10000], Loss: 67.71441650390625, Learning Rate: 0.01\n",
      "Epoch [8225/10000], Loss: 67.71249389648438, Learning Rate: 0.01\n",
      "Epoch [8226/10000], Loss: 67.71057891845703, Learning Rate: 0.01\n",
      "Epoch [8227/10000], Loss: 67.7086181640625, Learning Rate: 0.01\n",
      "Epoch [8228/10000], Loss: 67.70679473876953, Learning Rate: 0.01\n",
      "Epoch [8229/10000], Loss: 67.70503997802734, Learning Rate: 0.01\n",
      "Epoch [8230/10000], Loss: 67.70328521728516, Learning Rate: 0.01\n",
      "Epoch [8231/10000], Loss: 67.70147705078125, Learning Rate: 0.01\n",
      "Epoch [8232/10000], Loss: 67.69956970214844, Learning Rate: 0.01\n",
      "Epoch [8233/10000], Loss: 67.6976318359375, Learning Rate: 0.01\n",
      "Epoch [8234/10000], Loss: 67.69573211669922, Learning Rate: 0.01\n",
      "Epoch [8235/10000], Loss: 67.69389343261719, Learning Rate: 0.01\n",
      "Epoch [8236/10000], Loss: 67.69208526611328, Learning Rate: 0.01\n",
      "Epoch [8237/10000], Loss: 67.69023895263672, Learning Rate: 0.01\n",
      "Epoch [8238/10000], Loss: 67.68839263916016, Learning Rate: 0.01\n",
      "Epoch [8239/10000], Loss: 67.68653106689453, Learning Rate: 0.01\n",
      "Epoch [8240/10000], Loss: 67.68482971191406, Learning Rate: 0.01\n",
      "Epoch [8241/10000], Loss: 67.68291473388672, Learning Rate: 0.01\n",
      "Epoch [8242/10000], Loss: 67.68109130859375, Learning Rate: 0.01\n",
      "Epoch [8243/10000], Loss: 67.6791763305664, Learning Rate: 0.01\n",
      "Epoch [8244/10000], Loss: 67.6773681640625, Learning Rate: 0.01\n",
      "Epoch [8245/10000], Loss: 67.67546081542969, Learning Rate: 0.01\n",
      "Epoch [8246/10000], Loss: 67.67369842529297, Learning Rate: 0.01\n",
      "Epoch [8247/10000], Loss: 67.67184448242188, Learning Rate: 0.01\n",
      "Epoch [8248/10000], Loss: 67.67007446289062, Learning Rate: 0.01\n",
      "Epoch [8249/10000], Loss: 67.66815185546875, Learning Rate: 0.01\n",
      "Epoch [8250/10000], Loss: 67.66630554199219, Learning Rate: 0.01\n",
      "Epoch [8251/10000], Loss: 67.66452026367188, Learning Rate: 0.01\n",
      "Epoch [8252/10000], Loss: 67.66265106201172, Learning Rate: 0.01\n",
      "Epoch [8253/10000], Loss: 67.66079711914062, Learning Rate: 0.01\n",
      "Epoch [8254/10000], Loss: 67.6590805053711, Learning Rate: 0.01\n",
      "Epoch [8255/10000], Loss: 67.65715026855469, Learning Rate: 0.01\n",
      "Epoch [8256/10000], Loss: 67.65531158447266, Learning Rate: 0.01\n",
      "Epoch [8257/10000], Loss: 67.65349578857422, Learning Rate: 0.01\n",
      "Epoch [8258/10000], Loss: 67.65168762207031, Learning Rate: 0.01\n",
      "Epoch [8259/10000], Loss: 67.64984130859375, Learning Rate: 0.01\n",
      "Epoch [8260/10000], Loss: 67.64794921875, Learning Rate: 0.01\n",
      "Epoch [8261/10000], Loss: 67.6460952758789, Learning Rate: 0.01\n",
      "Epoch [8262/10000], Loss: 67.6443099975586, Learning Rate: 0.01\n",
      "Epoch [8263/10000], Loss: 67.64251708984375, Learning Rate: 0.01\n",
      "Epoch [8264/10000], Loss: 67.6406478881836, Learning Rate: 0.01\n",
      "Epoch [8265/10000], Loss: 67.63878631591797, Learning Rate: 0.01\n",
      "Epoch [8266/10000], Loss: 67.63705444335938, Learning Rate: 0.01\n",
      "Epoch [8267/10000], Loss: 67.63516235351562, Learning Rate: 0.01\n",
      "Epoch [8268/10000], Loss: 67.63329315185547, Learning Rate: 0.01\n",
      "Epoch [8269/10000], Loss: 67.63151550292969, Learning Rate: 0.01\n",
      "Epoch [8270/10000], Loss: 67.62967681884766, Learning Rate: 0.01\n",
      "Epoch [8271/10000], Loss: 67.62781524658203, Learning Rate: 0.01\n",
      "Epoch [8272/10000], Loss: 67.62593841552734, Learning Rate: 0.01\n",
      "Epoch [8273/10000], Loss: 67.62418365478516, Learning Rate: 0.01\n",
      "Epoch [8274/10000], Loss: 67.62232208251953, Learning Rate: 0.01\n",
      "Epoch [8275/10000], Loss: 67.62049102783203, Learning Rate: 0.01\n",
      "Epoch [8276/10000], Loss: 67.61862182617188, Learning Rate: 0.01\n",
      "Epoch [8277/10000], Loss: 67.61677551269531, Learning Rate: 0.01\n",
      "Epoch [8278/10000], Loss: 67.61494445800781, Learning Rate: 0.01\n",
      "Epoch [8279/10000], Loss: 67.6131362915039, Learning Rate: 0.01\n",
      "Epoch [8280/10000], Loss: 67.61132049560547, Learning Rate: 0.01\n",
      "Epoch [8281/10000], Loss: 67.60944366455078, Learning Rate: 0.01\n",
      "Epoch [8282/10000], Loss: 67.607666015625, Learning Rate: 0.01\n",
      "Epoch [8283/10000], Loss: 67.60585021972656, Learning Rate: 0.01\n",
      "Epoch [8284/10000], Loss: 67.60401153564453, Learning Rate: 0.01\n",
      "Epoch [8285/10000], Loss: 67.60216522216797, Learning Rate: 0.01\n",
      "Epoch [8286/10000], Loss: 67.60026550292969, Learning Rate: 0.01\n",
      "Epoch [8287/10000], Loss: 67.59844970703125, Learning Rate: 0.01\n",
      "Epoch [8288/10000], Loss: 67.59663391113281, Learning Rate: 0.01\n",
      "Epoch [8289/10000], Loss: 67.59486389160156, Learning Rate: 0.01\n",
      "Epoch [8290/10000], Loss: 67.5929946899414, Learning Rate: 0.01\n",
      "Epoch [8291/10000], Loss: 67.59117889404297, Learning Rate: 0.01\n",
      "Epoch [8292/10000], Loss: 67.58934020996094, Learning Rate: 0.01\n",
      "Epoch [8293/10000], Loss: 67.58751678466797, Learning Rate: 0.01\n",
      "Epoch [8294/10000], Loss: 67.58560943603516, Learning Rate: 0.01\n",
      "Epoch [8295/10000], Loss: 67.58384704589844, Learning Rate: 0.01\n",
      "Epoch [8296/10000], Loss: 67.5820541381836, Learning Rate: 0.01\n",
      "Epoch [8297/10000], Loss: 67.58018493652344, Learning Rate: 0.01\n",
      "Epoch [8298/10000], Loss: 67.57839965820312, Learning Rate: 0.01\n",
      "Epoch [8299/10000], Loss: 67.57654571533203, Learning Rate: 0.01\n",
      "Epoch [8300/10000], Loss: 67.57472229003906, Learning Rate: 0.01\n",
      "Epoch [8301/10000], Loss: 67.57291412353516, Learning Rate: 0.01\n",
      "Epoch [8302/10000], Loss: 67.57106018066406, Learning Rate: 0.01\n",
      "Epoch [8303/10000], Loss: 67.5692367553711, Learning Rate: 0.01\n",
      "Epoch [8304/10000], Loss: 67.56737518310547, Learning Rate: 0.01\n",
      "Epoch [8305/10000], Loss: 67.56558227539062, Learning Rate: 0.01\n",
      "Epoch [8306/10000], Loss: 67.5636978149414, Learning Rate: 0.01\n",
      "Epoch [8307/10000], Loss: 67.5619125366211, Learning Rate: 0.01\n",
      "Epoch [8308/10000], Loss: 67.56007385253906, Learning Rate: 0.01\n",
      "Epoch [8309/10000], Loss: 67.55829620361328, Learning Rate: 0.01\n",
      "Epoch [8310/10000], Loss: 67.55635833740234, Learning Rate: 0.01\n",
      "Epoch [8311/10000], Loss: 67.55462646484375, Learning Rate: 0.01\n",
      "Epoch [8312/10000], Loss: 67.552734375, Learning Rate: 0.01\n",
      "Epoch [8313/10000], Loss: 67.55093383789062, Learning Rate: 0.01\n",
      "Epoch [8314/10000], Loss: 67.54904174804688, Learning Rate: 0.01\n",
      "Epoch [8315/10000], Loss: 67.54730224609375, Learning Rate: 0.01\n",
      "Epoch [8316/10000], Loss: 67.54542541503906, Learning Rate: 0.01\n",
      "Epoch [8317/10000], Loss: 67.54359436035156, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8318/10000], Loss: 67.54178619384766, Learning Rate: 0.01\n",
      "Epoch [8319/10000], Loss: 67.53998565673828, Learning Rate: 0.01\n",
      "Epoch [8320/10000], Loss: 67.53813934326172, Learning Rate: 0.01\n",
      "Epoch [8321/10000], Loss: 67.53632354736328, Learning Rate: 0.01\n",
      "Epoch [8322/10000], Loss: 67.5345230102539, Learning Rate: 0.01\n",
      "Epoch [8323/10000], Loss: 67.53267669677734, Learning Rate: 0.01\n",
      "Epoch [8324/10000], Loss: 67.5308609008789, Learning Rate: 0.01\n",
      "Epoch [8325/10000], Loss: 67.52899169921875, Learning Rate: 0.01\n",
      "Epoch [8326/10000], Loss: 67.5271987915039, Learning Rate: 0.01\n",
      "Epoch [8327/10000], Loss: 67.52542877197266, Learning Rate: 0.01\n",
      "Epoch [8328/10000], Loss: 67.52352142333984, Learning Rate: 0.01\n",
      "Epoch [8329/10000], Loss: 67.52172088623047, Learning Rate: 0.01\n",
      "Epoch [8330/10000], Loss: 67.51993560791016, Learning Rate: 0.01\n",
      "Epoch [8331/10000], Loss: 67.51805114746094, Learning Rate: 0.01\n",
      "Epoch [8332/10000], Loss: 67.51624298095703, Learning Rate: 0.01\n",
      "Epoch [8333/10000], Loss: 67.5144271850586, Learning Rate: 0.01\n",
      "Epoch [8334/10000], Loss: 67.51260375976562, Learning Rate: 0.01\n",
      "Epoch [8335/10000], Loss: 67.51072692871094, Learning Rate: 0.01\n",
      "Epoch [8336/10000], Loss: 67.50895690917969, Learning Rate: 0.01\n",
      "Epoch [8337/10000], Loss: 67.50711059570312, Learning Rate: 0.01\n",
      "Epoch [8338/10000], Loss: 67.50530242919922, Learning Rate: 0.01\n",
      "Epoch [8339/10000], Loss: 67.50346374511719, Learning Rate: 0.01\n",
      "Epoch [8340/10000], Loss: 67.50170135498047, Learning Rate: 0.01\n",
      "Epoch [8341/10000], Loss: 67.49978637695312, Learning Rate: 0.01\n",
      "Epoch [8342/10000], Loss: 67.49800109863281, Learning Rate: 0.01\n",
      "Epoch [8343/10000], Loss: 67.4962158203125, Learning Rate: 0.01\n",
      "Epoch [8344/10000], Loss: 67.49435424804688, Learning Rate: 0.01\n",
      "Epoch [8345/10000], Loss: 67.49253845214844, Learning Rate: 0.01\n",
      "Epoch [8346/10000], Loss: 67.49068450927734, Learning Rate: 0.01\n",
      "Epoch [8347/10000], Loss: 67.48895263671875, Learning Rate: 0.01\n",
      "Epoch [8348/10000], Loss: 67.48705291748047, Learning Rate: 0.01\n",
      "Epoch [8349/10000], Loss: 67.48524475097656, Learning Rate: 0.01\n",
      "Epoch [8350/10000], Loss: 67.48342895507812, Learning Rate: 0.01\n",
      "Epoch [8351/10000], Loss: 67.48155975341797, Learning Rate: 0.01\n",
      "Epoch [8352/10000], Loss: 67.47978210449219, Learning Rate: 0.01\n",
      "Epoch [8353/10000], Loss: 67.47795104980469, Learning Rate: 0.01\n",
      "Epoch [8354/10000], Loss: 67.47608184814453, Learning Rate: 0.01\n",
      "Epoch [8355/10000], Loss: 67.47428131103516, Learning Rate: 0.01\n",
      "Epoch [8356/10000], Loss: 67.47252655029297, Learning Rate: 0.01\n",
      "Epoch [8357/10000], Loss: 67.4706802368164, Learning Rate: 0.01\n",
      "Epoch [8358/10000], Loss: 67.46881103515625, Learning Rate: 0.01\n",
      "Epoch [8359/10000], Loss: 67.46703338623047, Learning Rate: 0.01\n",
      "Epoch [8360/10000], Loss: 67.46517944335938, Learning Rate: 0.01\n",
      "Epoch [8361/10000], Loss: 67.46334075927734, Learning Rate: 0.01\n",
      "Epoch [8362/10000], Loss: 67.4615249633789, Learning Rate: 0.01\n",
      "Epoch [8363/10000], Loss: 67.45970916748047, Learning Rate: 0.01\n",
      "Epoch [8364/10000], Loss: 67.45793914794922, Learning Rate: 0.01\n",
      "Epoch [8365/10000], Loss: 67.45604705810547, Learning Rate: 0.01\n",
      "Epoch [8366/10000], Loss: 67.45431518554688, Learning Rate: 0.01\n",
      "Epoch [8367/10000], Loss: 67.45240783691406, Learning Rate: 0.01\n",
      "Epoch [8368/10000], Loss: 67.45069885253906, Learning Rate: 0.01\n",
      "Epoch [8369/10000], Loss: 67.4488754272461, Learning Rate: 0.01\n",
      "Epoch [8370/10000], Loss: 67.44707489013672, Learning Rate: 0.01\n",
      "Epoch [8371/10000], Loss: 67.44537353515625, Learning Rate: 0.01\n",
      "Epoch [8372/10000], Loss: 67.44352722167969, Learning Rate: 0.01\n",
      "Epoch [8373/10000], Loss: 67.44181060791016, Learning Rate: 0.01\n",
      "Epoch [8374/10000], Loss: 67.44013214111328, Learning Rate: 0.01\n",
      "Epoch [8375/10000], Loss: 67.43864440917969, Learning Rate: 0.01\n",
      "Epoch [8376/10000], Loss: 67.43712615966797, Learning Rate: 0.01\n",
      "Epoch [8377/10000], Loss: 67.43585968017578, Learning Rate: 0.01\n",
      "Epoch [8378/10000], Loss: 67.4349136352539, Learning Rate: 0.01\n",
      "Epoch [8379/10000], Loss: 67.4344253540039, Learning Rate: 0.01\n",
      "Epoch [8380/10000], Loss: 67.4346923828125, Learning Rate: 0.01\n",
      "Epoch [8381/10000], Loss: 67.43607330322266, Learning Rate: 0.01\n",
      "Epoch [8382/10000], Loss: 67.43942260742188, Learning Rate: 0.01\n",
      "Epoch [8383/10000], Loss: 67.44561767578125, Learning Rate: 0.01\n",
      "Epoch [8384/10000], Loss: 67.45658111572266, Learning Rate: 0.01\n",
      "Epoch [8385/10000], Loss: 67.47526550292969, Learning Rate: 0.01\n",
      "Epoch [8386/10000], Loss: 67.5059814453125, Learning Rate: 0.01\n",
      "Epoch [8387/10000], Loss: 67.55667114257812, Learning Rate: 0.01\n",
      "Epoch [8388/10000], Loss: 67.6386489868164, Learning Rate: 0.01\n",
      "Epoch [8389/10000], Loss: 67.77093505859375, Learning Rate: 0.01\n",
      "Epoch [8390/10000], Loss: 67.97987365722656, Learning Rate: 0.01\n",
      "Epoch [8391/10000], Loss: 68.30577087402344, Learning Rate: 0.01\n",
      "Epoch [8392/10000], Loss: 68.78964233398438, Learning Rate: 0.01\n",
      "Epoch [8393/10000], Loss: 69.47390747070312, Learning Rate: 0.01\n",
      "Epoch [8394/10000], Loss: 70.32896423339844, Learning Rate: 0.01\n",
      "Epoch [8395/10000], Loss: 71.2427749633789, Learning Rate: 0.01\n",
      "Epoch [8396/10000], Loss: 71.89955139160156, Learning Rate: 0.01\n",
      "Epoch [8397/10000], Loss: 72.0082778930664, Learning Rate: 0.01\n",
      "Epoch [8398/10000], Loss: 71.45232391357422, Learning Rate: 0.01\n",
      "Epoch [8399/10000], Loss: 70.66616821289062, Learning Rate: 0.01\n",
      "Epoch [8400/10000], Loss: 70.32582092285156, Learning Rate: 0.01\n",
      "Epoch [8401/10000], Loss: 70.86325073242188, Learning Rate: 0.01\n",
      "Epoch [8402/10000], Loss: 71.9410400390625, Learning Rate: 0.01\n",
      "Epoch [8403/10000], Loss: 72.57227325439453, Learning Rate: 0.01\n",
      "Epoch [8404/10000], Loss: 71.74073028564453, Learning Rate: 0.01\n",
      "Epoch [8405/10000], Loss: 69.68030548095703, Learning Rate: 0.01\n",
      "Epoch [8406/10000], Loss: 67.90391540527344, Learning Rate: 0.01\n",
      "Epoch [8407/10000], Loss: 67.64317321777344, Learning Rate: 0.01\n",
      "Epoch [8408/10000], Loss: 68.523681640625, Learning Rate: 0.01\n",
      "Epoch [8409/10000], Loss: 69.2281265258789, Learning Rate: 0.01\n",
      "Epoch [8410/10000], Loss: 68.99403381347656, Learning Rate: 0.01\n",
      "Epoch [8411/10000], Loss: 68.29242706298828, Learning Rate: 0.01\n",
      "Epoch [8412/10000], Loss: 68.02970123291016, Learning Rate: 0.01\n",
      "Epoch [8413/10000], Loss: 68.33528137207031, Learning Rate: 0.01\n",
      "Epoch [8414/10000], Loss: 68.5390625, Learning Rate: 0.01\n",
      "Epoch [8415/10000], Loss: 68.16373443603516, Learning Rate: 0.01\n",
      "Epoch [8416/10000], Loss: 67.57339477539062, Learning Rate: 0.01\n",
      "Epoch [8417/10000], Loss: 67.42809295654297, Learning Rate: 0.01\n",
      "Epoch [8418/10000], Loss: 67.80696868896484, Learning Rate: 0.01\n",
      "Epoch [8419/10000], Loss: 68.164794921875, Learning Rate: 0.01\n",
      "Epoch [8420/10000], Loss: 68.06300354003906, Learning Rate: 0.01\n",
      "Epoch [8421/10000], Loss: 67.67220306396484, Learning Rate: 0.01\n",
      "Epoch [8422/10000], Loss: 67.44862365722656, Learning Rate: 0.01\n",
      "Epoch [8423/10000], Loss: 67.53787231445312, Learning Rate: 0.01\n",
      "Epoch [8424/10000], Loss: 67.6802749633789, Learning Rate: 0.01\n",
      "Epoch [8425/10000], Loss: 67.63689422607422, Learning Rate: 0.01\n",
      "Epoch [8426/10000], Loss: 67.4879379272461, Learning Rate: 0.01\n",
      "Epoch [8427/10000], Loss: 67.45048522949219, Learning Rate: 0.01\n",
      "Epoch [8428/10000], Loss: 67.5539779663086, Learning Rate: 0.01\n",
      "Epoch [8429/10000], Loss: 67.62187194824219, Learning Rate: 0.01\n",
      "Epoch [8430/10000], Loss: 67.53585815429688, Learning Rate: 0.01\n",
      "Epoch [8431/10000], Loss: 67.38626861572266, Learning Rate: 0.01\n",
      "Epoch [8432/10000], Loss: 67.33464050292969, Learning Rate: 0.01\n",
      "Epoch [8433/10000], Loss: 67.40453338623047, Learning Rate: 0.01\n",
      "Epoch [8434/10000], Loss: 67.47793579101562, Learning Rate: 0.01\n",
      "Epoch [8435/10000], Loss: 67.46395874023438, Learning Rate: 0.01\n",
      "Epoch [8436/10000], Loss: 67.39726257324219, Learning Rate: 0.01\n",
      "Epoch [8437/10000], Loss: 67.36431121826172, Learning Rate: 0.01\n",
      "Epoch [8438/10000], Loss: 67.38367462158203, Learning Rate: 0.01\n",
      "Epoch [8439/10000], Loss: 67.40045166015625, Learning Rate: 0.01\n",
      "Epoch [8440/10000], Loss: 67.37481689453125, Learning Rate: 0.01\n",
      "Epoch [8441/10000], Loss: 67.33363342285156, Learning Rate: 0.01\n",
      "Epoch [8442/10000], Loss: 67.32489013671875, Learning Rate: 0.01\n",
      "Epoch [8443/10000], Loss: 67.35216522216797, Learning Rate: 0.01\n",
      "Epoch [8444/10000], Loss: 67.37488555908203, Learning Rate: 0.01\n",
      "Epoch [8445/10000], Loss: 67.36344909667969, Learning Rate: 0.01\n",
      "Epoch [8446/10000], Loss: 67.33200073242188, Learning Rate: 0.01\n",
      "Epoch [8447/10000], Loss: 67.31366729736328, Learning Rate: 0.01\n",
      "Epoch [8448/10000], Loss: 67.31843566894531, Learning Rate: 0.01\n",
      "Epoch [8449/10000], Loss: 67.3282241821289, Learning Rate: 0.01\n",
      "Epoch [8450/10000], Loss: 67.3248062133789, Learning Rate: 0.01\n",
      "Epoch [8451/10000], Loss: 67.31201934814453, Learning Rate: 0.01\n",
      "Epoch [8452/10000], Loss: 67.30579376220703, Learning Rate: 0.01\n",
      "Epoch [8453/10000], Loss: 67.31082153320312, Learning Rate: 0.01\n",
      "Epoch [8454/10000], Loss: 67.31658935546875, Learning Rate: 0.01\n",
      "Epoch [8455/10000], Loss: 67.31230163574219, Learning Rate: 0.01\n",
      "Epoch [8456/10000], Loss: 67.30001831054688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8457/10000], Loss: 67.29082489013672, Learning Rate: 0.01\n",
      "Epoch [8458/10000], Loss: 67.29063415527344, Learning Rate: 0.01\n",
      "Epoch [8459/10000], Loss: 67.2945327758789, Learning Rate: 0.01\n",
      "Epoch [8460/10000], Loss: 67.29498291015625, Learning Rate: 0.01\n",
      "Epoch [8461/10000], Loss: 67.2903823852539, Learning Rate: 0.01\n",
      "Epoch [8462/10000], Loss: 67.28541564941406, Learning Rate: 0.01\n",
      "Epoch [8463/10000], Loss: 67.28387451171875, Learning Rate: 0.01\n",
      "Epoch [8464/10000], Loss: 67.28449249267578, Learning Rate: 0.01\n",
      "Epoch [8465/10000], Loss: 67.28324890136719, Learning Rate: 0.01\n",
      "Epoch [8466/10000], Loss: 67.27886199951172, Learning Rate: 0.01\n",
      "Epoch [8467/10000], Loss: 67.27404022216797, Learning Rate: 0.01\n",
      "Epoch [8468/10000], Loss: 67.27159118652344, Learning Rate: 0.01\n",
      "Epoch [8469/10000], Loss: 67.2716064453125, Learning Rate: 0.01\n",
      "Epoch [8470/10000], Loss: 67.27178192138672, Learning Rate: 0.01\n",
      "Epoch [8471/10000], Loss: 67.26996612548828, Learning Rate: 0.01\n",
      "Epoch [8472/10000], Loss: 67.26690673828125, Learning Rate: 0.01\n",
      "Epoch [8473/10000], Loss: 67.26432800292969, Learning Rate: 0.01\n",
      "Epoch [8474/10000], Loss: 67.26288604736328, Learning Rate: 0.01\n",
      "Epoch [8475/10000], Loss: 67.2618408203125, Learning Rate: 0.01\n",
      "Epoch [8476/10000], Loss: 67.26001739501953, Learning Rate: 0.01\n",
      "Epoch [8477/10000], Loss: 67.2573471069336, Learning Rate: 0.01\n",
      "Epoch [8478/10000], Loss: 67.25483703613281, Learning Rate: 0.01\n",
      "Epoch [8479/10000], Loss: 67.25312805175781, Learning Rate: 0.01\n",
      "Epoch [8480/10000], Loss: 67.25212860107422, Learning Rate: 0.01\n",
      "Epoch [8481/10000], Loss: 67.25090026855469, Learning Rate: 0.01\n",
      "Epoch [8482/10000], Loss: 67.24913024902344, Learning Rate: 0.01\n",
      "Epoch [8483/10000], Loss: 67.24702453613281, Learning Rate: 0.01\n",
      "Epoch [8484/10000], Loss: 67.24510955810547, Learning Rate: 0.01\n",
      "Epoch [8485/10000], Loss: 67.24339294433594, Learning Rate: 0.01\n",
      "Epoch [8486/10000], Loss: 67.24198150634766, Learning Rate: 0.01\n",
      "Epoch [8487/10000], Loss: 67.24028778076172, Learning Rate: 0.01\n",
      "Epoch [8488/10000], Loss: 67.23831939697266, Learning Rate: 0.01\n",
      "Epoch [8489/10000], Loss: 67.23625946044922, Learning Rate: 0.01\n",
      "Epoch [8490/10000], Loss: 67.2345962524414, Learning Rate: 0.01\n",
      "Epoch [8491/10000], Loss: 67.23310089111328, Learning Rate: 0.01\n",
      "Epoch [8492/10000], Loss: 67.23152160644531, Learning Rate: 0.01\n",
      "Epoch [8493/10000], Loss: 67.22984313964844, Learning Rate: 0.01\n",
      "Epoch [8494/10000], Loss: 67.22808074951172, Learning Rate: 0.01\n",
      "Epoch [8495/10000], Loss: 67.22631072998047, Learning Rate: 0.01\n",
      "Epoch [8496/10000], Loss: 67.2247085571289, Learning Rate: 0.01\n",
      "Epoch [8497/10000], Loss: 67.22301483154297, Learning Rate: 0.01\n",
      "Epoch [8498/10000], Loss: 67.22144317626953, Learning Rate: 0.01\n",
      "Epoch [8499/10000], Loss: 67.21961975097656, Learning Rate: 0.01\n",
      "Epoch [8500/10000], Loss: 67.21784973144531, Learning Rate: 0.01\n",
      "Epoch [8501/10000], Loss: 67.2160873413086, Learning Rate: 0.01\n",
      "Epoch [8502/10000], Loss: 67.21446990966797, Learning Rate: 0.01\n",
      "Epoch [8503/10000], Loss: 67.21285247802734, Learning Rate: 0.01\n",
      "Epoch [8504/10000], Loss: 67.21114349365234, Learning Rate: 0.01\n",
      "Epoch [8505/10000], Loss: 67.20944213867188, Learning Rate: 0.01\n",
      "Epoch [8506/10000], Loss: 67.20779418945312, Learning Rate: 0.01\n",
      "Epoch [8507/10000], Loss: 67.2060546875, Learning Rate: 0.01\n",
      "Epoch [8508/10000], Loss: 67.20438385009766, Learning Rate: 0.01\n",
      "Epoch [8509/10000], Loss: 67.2027816772461, Learning Rate: 0.01\n",
      "Epoch [8510/10000], Loss: 67.20113372802734, Learning Rate: 0.01\n",
      "Epoch [8511/10000], Loss: 67.19940948486328, Learning Rate: 0.01\n",
      "Epoch [8512/10000], Loss: 67.19766235351562, Learning Rate: 0.01\n",
      "Epoch [8513/10000], Loss: 67.19606018066406, Learning Rate: 0.01\n",
      "Epoch [8514/10000], Loss: 67.19432830810547, Learning Rate: 0.01\n",
      "Epoch [8515/10000], Loss: 67.19266510009766, Learning Rate: 0.01\n",
      "Epoch [8516/10000], Loss: 67.19095611572266, Learning Rate: 0.01\n",
      "Epoch [8517/10000], Loss: 67.1893081665039, Learning Rate: 0.01\n",
      "Epoch [8518/10000], Loss: 67.1876449584961, Learning Rate: 0.01\n",
      "Epoch [8519/10000], Loss: 67.18594360351562, Learning Rate: 0.01\n",
      "Epoch [8520/10000], Loss: 67.18425750732422, Learning Rate: 0.01\n",
      "Epoch [8521/10000], Loss: 67.1826171875, Learning Rate: 0.01\n",
      "Epoch [8522/10000], Loss: 67.18091583251953, Learning Rate: 0.01\n",
      "Epoch [8523/10000], Loss: 67.17926788330078, Learning Rate: 0.01\n",
      "Epoch [8524/10000], Loss: 67.17754364013672, Learning Rate: 0.01\n",
      "Epoch [8525/10000], Loss: 67.17588806152344, Learning Rate: 0.01\n",
      "Epoch [8526/10000], Loss: 67.17420196533203, Learning Rate: 0.01\n",
      "Epoch [8527/10000], Loss: 67.17254638671875, Learning Rate: 0.01\n",
      "Epoch [8528/10000], Loss: 67.17085266113281, Learning Rate: 0.01\n",
      "Epoch [8529/10000], Loss: 67.1692123413086, Learning Rate: 0.01\n",
      "Epoch [8530/10000], Loss: 67.16753387451172, Learning Rate: 0.01\n",
      "Epoch [8531/10000], Loss: 67.16588592529297, Learning Rate: 0.01\n",
      "Epoch [8532/10000], Loss: 67.16413879394531, Learning Rate: 0.01\n",
      "Epoch [8533/10000], Loss: 67.16242218017578, Learning Rate: 0.01\n",
      "Epoch [8534/10000], Loss: 67.16083526611328, Learning Rate: 0.01\n",
      "Epoch [8535/10000], Loss: 67.1590576171875, Learning Rate: 0.01\n",
      "Epoch [8536/10000], Loss: 67.15742492675781, Learning Rate: 0.01\n",
      "Epoch [8537/10000], Loss: 67.1557388305664, Learning Rate: 0.01\n",
      "Epoch [8538/10000], Loss: 67.15410614013672, Learning Rate: 0.01\n",
      "Epoch [8539/10000], Loss: 67.15243530273438, Learning Rate: 0.01\n",
      "Epoch [8540/10000], Loss: 67.1507568359375, Learning Rate: 0.01\n",
      "Epoch [8541/10000], Loss: 67.14905548095703, Learning Rate: 0.01\n",
      "Epoch [8542/10000], Loss: 67.1473388671875, Learning Rate: 0.01\n",
      "Epoch [8543/10000], Loss: 67.14570617675781, Learning Rate: 0.01\n",
      "Epoch [8544/10000], Loss: 67.14404296875, Learning Rate: 0.01\n",
      "Epoch [8545/10000], Loss: 67.14237213134766, Learning Rate: 0.01\n",
      "Epoch [8546/10000], Loss: 67.14067840576172, Learning Rate: 0.01\n",
      "Epoch [8547/10000], Loss: 67.13900756835938, Learning Rate: 0.01\n",
      "Epoch [8548/10000], Loss: 67.13734436035156, Learning Rate: 0.01\n",
      "Epoch [8549/10000], Loss: 67.13565063476562, Learning Rate: 0.01\n",
      "Epoch [8550/10000], Loss: 67.13398742675781, Learning Rate: 0.01\n",
      "Epoch [8551/10000], Loss: 67.13231658935547, Learning Rate: 0.01\n",
      "Epoch [8552/10000], Loss: 67.13067626953125, Learning Rate: 0.01\n",
      "Epoch [8553/10000], Loss: 67.12905883789062, Learning Rate: 0.01\n",
      "Epoch [8554/10000], Loss: 67.12732696533203, Learning Rate: 0.01\n",
      "Epoch [8555/10000], Loss: 67.12571716308594, Learning Rate: 0.01\n",
      "Epoch [8556/10000], Loss: 67.12400817871094, Learning Rate: 0.01\n",
      "Epoch [8557/10000], Loss: 67.12244415283203, Learning Rate: 0.01\n",
      "Epoch [8558/10000], Loss: 67.12076568603516, Learning Rate: 0.01\n",
      "Epoch [8559/10000], Loss: 67.11920166015625, Learning Rate: 0.01\n",
      "Epoch [8560/10000], Loss: 67.11773681640625, Learning Rate: 0.01\n",
      "Epoch [8561/10000], Loss: 67.11608123779297, Learning Rate: 0.01\n",
      "Epoch [8562/10000], Loss: 67.11466979980469, Learning Rate: 0.01\n",
      "Epoch [8563/10000], Loss: 67.11326599121094, Learning Rate: 0.01\n",
      "Epoch [8564/10000], Loss: 67.11203002929688, Learning Rate: 0.01\n",
      "Epoch [8565/10000], Loss: 67.11090087890625, Learning Rate: 0.01\n",
      "Epoch [8566/10000], Loss: 67.10995483398438, Learning Rate: 0.01\n",
      "Epoch [8567/10000], Loss: 67.10948181152344, Learning Rate: 0.01\n",
      "Epoch [8568/10000], Loss: 67.10943603515625, Learning Rate: 0.01\n",
      "Epoch [8569/10000], Loss: 67.11014556884766, Learning Rate: 0.01\n",
      "Epoch [8570/10000], Loss: 67.11177825927734, Learning Rate: 0.01\n",
      "Epoch [8571/10000], Loss: 67.11504364013672, Learning Rate: 0.01\n",
      "Epoch [8572/10000], Loss: 67.12061309814453, Learning Rate: 0.01\n",
      "Epoch [8573/10000], Loss: 67.12960815429688, Learning Rate: 0.01\n",
      "Epoch [8574/10000], Loss: 67.14362335205078, Learning Rate: 0.01\n",
      "Epoch [8575/10000], Loss: 67.16533660888672, Learning Rate: 0.01\n",
      "Epoch [8576/10000], Loss: 67.19850158691406, Learning Rate: 0.01\n",
      "Epoch [8577/10000], Loss: 67.24909973144531, Learning Rate: 0.01\n",
      "Epoch [8578/10000], Loss: 67.32588958740234, Learning Rate: 0.01\n",
      "Epoch [8579/10000], Loss: 67.44204711914062, Learning Rate: 0.01\n",
      "Epoch [8580/10000], Loss: 67.61651611328125, Learning Rate: 0.01\n",
      "Epoch [8581/10000], Loss: 67.8775634765625, Learning Rate: 0.01\n",
      "Epoch [8582/10000], Loss: 68.26008605957031, Learning Rate: 0.01\n",
      "Epoch [8583/10000], Loss: 68.81200408935547, Learning Rate: 0.01\n",
      "Epoch [8584/10000], Loss: 69.5702896118164, Learning Rate: 0.01\n",
      "Epoch [8585/10000], Loss: 70.55924987792969, Learning Rate: 0.01\n",
      "Epoch [8586/10000], Loss: 71.69865417480469, Learning Rate: 0.01\n",
      "Epoch [8587/10000], Loss: 72.80243682861328, Learning Rate: 0.01\n",
      "Epoch [8588/10000], Loss: 73.44436645507812, Learning Rate: 0.01\n",
      "Epoch [8589/10000], Loss: 73.216064453125, Learning Rate: 0.01\n",
      "Epoch [8590/10000], Loss: 71.87487030029297, Learning Rate: 0.01\n",
      "Epoch [8591/10000], Loss: 69.84673309326172, Learning Rate: 0.01\n",
      "Epoch [8592/10000], Loss: 67.98773193359375, Learning Rate: 0.01\n",
      "Epoch [8593/10000], Loss: 67.10693359375, Learning Rate: 0.01\n",
      "Epoch [8594/10000], Loss: 67.38191223144531, Learning Rate: 0.01\n",
      "Epoch [8595/10000], Loss: 68.33358001708984, Learning Rate: 0.01\n",
      "Epoch [8596/10000], Loss: 69.2021255493164, Learning Rate: 0.01\n",
      "Epoch [8597/10000], Loss: 69.38968658447266, Learning Rate: 0.01\n",
      "Epoch [8598/10000], Loss: 68.80883026123047, Learning Rate: 0.01\n",
      "Epoch [8599/10000], Loss: 67.87085723876953, Learning Rate: 0.01\n",
      "Epoch [8600/10000], Loss: 67.18749237060547, Learning Rate: 0.01\n",
      "Epoch [8601/10000], Loss: 67.10334014892578, Learning Rate: 0.01\n",
      "Epoch [8602/10000], Loss: 67.50311279296875, Learning Rate: 0.01\n",
      "Epoch [8603/10000], Loss: 67.97283172607422, Learning Rate: 0.01\n",
      "Epoch [8604/10000], Loss: 68.12860870361328, Learning Rate: 0.01\n",
      "Epoch [8605/10000], Loss: 67.87660217285156, Learning Rate: 0.01\n",
      "Epoch [8606/10000], Loss: 67.42507934570312, Learning Rate: 0.01\n",
      "Epoch [8607/10000], Loss: 67.0958251953125, Learning Rate: 0.01\n",
      "Epoch [8608/10000], Loss: 67.06782531738281, Learning Rate: 0.01\n",
      "Epoch [8609/10000], Loss: 67.27478790283203, Learning Rate: 0.01\n",
      "Epoch [8610/10000], Loss: 67.49893188476562, Learning Rate: 0.01\n",
      "Epoch [8611/10000], Loss: 67.5526123046875, Learning Rate: 0.01\n",
      "Epoch [8612/10000], Loss: 67.4072036743164, Learning Rate: 0.01\n",
      "Epoch [8613/10000], Loss: 67.18346405029297, Learning Rate: 0.01\n",
      "Epoch [8614/10000], Loss: 67.0393295288086, Learning Rate: 0.01\n",
      "Epoch [8615/10000], Loss: 67.04718017578125, Learning Rate: 0.01\n",
      "Epoch [8616/10000], Loss: 67.15824890136719, Learning Rate: 0.01\n",
      "Epoch [8617/10000], Loss: 67.26049041748047, Learning Rate: 0.01\n",
      "Epoch [8618/10000], Loss: 67.270263671875, Learning Rate: 0.01\n",
      "Epoch [8619/10000], Loss: 67.18704223632812, Learning Rate: 0.01\n",
      "Epoch [8620/10000], Loss: 67.07701110839844, Learning Rate: 0.01\n",
      "Epoch [8621/10000], Loss: 67.01439666748047, Learning Rate: 0.01\n",
      "Epoch [8622/10000], Loss: 67.02613067626953, Learning Rate: 0.01\n",
      "Epoch [8623/10000], Loss: 67.08248901367188, Learning Rate: 0.01\n",
      "Epoch [8624/10000], Loss: 67.12869262695312, Learning Rate: 0.01\n",
      "Epoch [8625/10000], Loss: 67.12808227539062, Learning Rate: 0.01\n",
      "Epoch [8626/10000], Loss: 67.08411407470703, Learning Rate: 0.01\n",
      "Epoch [8627/10000], Loss: 67.03001403808594, Learning Rate: 0.01\n",
      "Epoch [8628/10000], Loss: 67.0001449584961, Learning Rate: 0.01\n",
      "Epoch [8629/10000], Loss: 67.00582122802734, Learning Rate: 0.01\n",
      "Epoch [8630/10000], Loss: 67.03231048583984, Learning Rate: 0.01\n",
      "Epoch [8631/10000], Loss: 67.05387878417969, Learning Rate: 0.01\n",
      "Epoch [8632/10000], Loss: 67.05348205566406, Learning Rate: 0.01\n",
      "Epoch [8633/10000], Loss: 67.03231811523438, Learning Rate: 0.01\n",
      "Epoch [8634/10000], Loss: 67.00531768798828, Learning Rate: 0.01\n",
      "Epoch [8635/10000], Loss: 66.98870086669922, Learning Rate: 0.01\n",
      "Epoch [8636/10000], Loss: 66.9886474609375, Learning Rate: 0.01\n",
      "Epoch [8637/10000], Loss: 66.99951934814453, Learning Rate: 0.01\n",
      "Epoch [8638/10000], Loss: 67.00991821289062, Learning Rate: 0.01\n",
      "Epoch [8639/10000], Loss: 67.01102447509766, Learning Rate: 0.01\n",
      "Epoch [8640/10000], Loss: 67.00198364257812, Learning Rate: 0.01\n",
      "Epoch [8641/10000], Loss: 66.98847198486328, Learning Rate: 0.01\n",
      "Epoch [8642/10000], Loss: 66.97799682617188, Learning Rate: 0.01\n",
      "Epoch [8643/10000], Loss: 66.97479248046875, Learning Rate: 0.01\n",
      "Epoch [8644/10000], Loss: 66.97779083251953, Learning Rate: 0.01\n",
      "Epoch [8645/10000], Loss: 66.98233795166016, Learning Rate: 0.01\n",
      "Epoch [8646/10000], Loss: 66.9837875366211, Learning Rate: 0.01\n",
      "Epoch [8647/10000], Loss: 66.98052978515625, Learning Rate: 0.01\n",
      "Epoch [8648/10000], Loss: 66.9739990234375, Learning Rate: 0.01\n",
      "Epoch [8649/10000], Loss: 66.96737670898438, Learning Rate: 0.01\n",
      "Epoch [8650/10000], Loss: 66.96327209472656, Learning Rate: 0.01\n",
      "Epoch [8651/10000], Loss: 66.9623794555664, Learning Rate: 0.01\n",
      "Epoch [8652/10000], Loss: 66.96331024169922, Learning Rate: 0.01\n",
      "Epoch [8653/10000], Loss: 66.9640884399414, Learning Rate: 0.01\n",
      "Epoch [8654/10000], Loss: 66.96306610107422, Learning Rate: 0.01\n",
      "Epoch [8655/10000], Loss: 66.96012115478516, Learning Rate: 0.01\n",
      "Epoch [8656/10000], Loss: 66.95613098144531, Learning Rate: 0.01\n",
      "Epoch [8657/10000], Loss: 66.95252227783203, Learning Rate: 0.01\n",
      "Epoch [8658/10000], Loss: 66.95011901855469, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8659/10000], Loss: 66.94905853271484, Learning Rate: 0.01\n",
      "Epoch [8660/10000], Loss: 66.94855499267578, Learning Rate: 0.01\n",
      "Epoch [8661/10000], Loss: 66.94786834716797, Learning Rate: 0.01\n",
      "Epoch [8662/10000], Loss: 66.94648742675781, Learning Rate: 0.01\n",
      "Epoch [8663/10000], Loss: 66.94424438476562, Learning Rate: 0.01\n",
      "Epoch [8664/10000], Loss: 66.94166564941406, Learning Rate: 0.01\n",
      "Epoch [8665/10000], Loss: 66.93914794921875, Learning Rate: 0.01\n",
      "Epoch [8666/10000], Loss: 66.93709564208984, Learning Rate: 0.01\n",
      "Epoch [8667/10000], Loss: 66.9356689453125, Learning Rate: 0.01\n",
      "Epoch [8668/10000], Loss: 66.93446350097656, Learning Rate: 0.01\n",
      "Epoch [8669/10000], Loss: 66.9333267211914, Learning Rate: 0.01\n",
      "Epoch [8670/10000], Loss: 66.93190002441406, Learning Rate: 0.01\n",
      "Epoch [8671/10000], Loss: 66.9301528930664, Learning Rate: 0.01\n",
      "Epoch [8672/10000], Loss: 66.92815399169922, Learning Rate: 0.01\n",
      "Epoch [8673/10000], Loss: 66.92610931396484, Learning Rate: 0.01\n",
      "Epoch [8674/10000], Loss: 66.92416381835938, Learning Rate: 0.01\n",
      "Epoch [8675/10000], Loss: 66.9225082397461, Learning Rate: 0.01\n",
      "Epoch [8676/10000], Loss: 66.92100524902344, Learning Rate: 0.01\n",
      "Epoch [8677/10000], Loss: 66.91958618164062, Learning Rate: 0.01\n",
      "Epoch [8678/10000], Loss: 66.91808319091797, Learning Rate: 0.01\n",
      "Epoch [8679/10000], Loss: 66.91651153564453, Learning Rate: 0.01\n",
      "Epoch [8680/10000], Loss: 66.91484069824219, Learning Rate: 0.01\n",
      "Epoch [8681/10000], Loss: 66.91301727294922, Learning Rate: 0.01\n",
      "Epoch [8682/10000], Loss: 66.91119384765625, Learning Rate: 0.01\n",
      "Epoch [8683/10000], Loss: 66.9095458984375, Learning Rate: 0.01\n",
      "Epoch [8684/10000], Loss: 66.90787506103516, Learning Rate: 0.01\n",
      "Epoch [8685/10000], Loss: 66.90631866455078, Learning Rate: 0.01\n",
      "Epoch [8686/10000], Loss: 66.90474700927734, Learning Rate: 0.01\n",
      "Epoch [8687/10000], Loss: 66.9032211303711, Learning Rate: 0.01\n",
      "Epoch [8688/10000], Loss: 66.90155029296875, Learning Rate: 0.01\n",
      "Epoch [8689/10000], Loss: 66.89989471435547, Learning Rate: 0.01\n",
      "Epoch [8690/10000], Loss: 66.89830780029297, Learning Rate: 0.01\n",
      "Epoch [8691/10000], Loss: 66.89659881591797, Learning Rate: 0.01\n",
      "Epoch [8692/10000], Loss: 66.89494323730469, Learning Rate: 0.01\n",
      "Epoch [8693/10000], Loss: 66.89321899414062, Learning Rate: 0.01\n",
      "Epoch [8694/10000], Loss: 66.89165496826172, Learning Rate: 0.01\n",
      "Epoch [8695/10000], Loss: 66.89007568359375, Learning Rate: 0.01\n",
      "Epoch [8696/10000], Loss: 66.88842010498047, Learning Rate: 0.01\n",
      "Epoch [8697/10000], Loss: 66.88689422607422, Learning Rate: 0.01\n",
      "Epoch [8698/10000], Loss: 66.88521575927734, Learning Rate: 0.01\n",
      "Epoch [8699/10000], Loss: 66.88359069824219, Learning Rate: 0.01\n",
      "Epoch [8700/10000], Loss: 66.88200378417969, Learning Rate: 0.01\n",
      "Epoch [8701/10000], Loss: 66.88031005859375, Learning Rate: 0.01\n",
      "Epoch [8702/10000], Loss: 66.87862396240234, Learning Rate: 0.01\n",
      "Epoch [8703/10000], Loss: 66.87694549560547, Learning Rate: 0.01\n",
      "Epoch [8704/10000], Loss: 66.87535095214844, Learning Rate: 0.01\n",
      "Epoch [8705/10000], Loss: 66.87370300292969, Learning Rate: 0.01\n",
      "Epoch [8706/10000], Loss: 66.87214660644531, Learning Rate: 0.01\n",
      "Epoch [8707/10000], Loss: 66.87057495117188, Learning Rate: 0.01\n",
      "Epoch [8708/10000], Loss: 66.86891174316406, Learning Rate: 0.01\n",
      "Epoch [8709/10000], Loss: 66.86727905273438, Learning Rate: 0.01\n",
      "Epoch [8710/10000], Loss: 66.8656005859375, Learning Rate: 0.01\n",
      "Epoch [8711/10000], Loss: 66.864013671875, Learning Rate: 0.01\n",
      "Epoch [8712/10000], Loss: 66.86234283447266, Learning Rate: 0.01\n",
      "Epoch [8713/10000], Loss: 66.86071014404297, Learning Rate: 0.01\n",
      "Epoch [8714/10000], Loss: 66.85910034179688, Learning Rate: 0.01\n",
      "Epoch [8715/10000], Loss: 66.85747528076172, Learning Rate: 0.01\n",
      "Epoch [8716/10000], Loss: 66.85584259033203, Learning Rate: 0.01\n",
      "Epoch [8717/10000], Loss: 66.85419464111328, Learning Rate: 0.01\n",
      "Epoch [8718/10000], Loss: 66.85250854492188, Learning Rate: 0.01\n",
      "Epoch [8719/10000], Loss: 66.85103607177734, Learning Rate: 0.01\n",
      "Epoch [8720/10000], Loss: 66.8492431640625, Learning Rate: 0.01\n",
      "Epoch [8721/10000], Loss: 66.84766387939453, Learning Rate: 0.01\n",
      "Epoch [8722/10000], Loss: 66.84598541259766, Learning Rate: 0.01\n",
      "Epoch [8723/10000], Loss: 66.84439086914062, Learning Rate: 0.01\n",
      "Epoch [8724/10000], Loss: 66.84274291992188, Learning Rate: 0.01\n",
      "Epoch [8725/10000], Loss: 66.84119415283203, Learning Rate: 0.01\n",
      "Epoch [8726/10000], Loss: 66.8395004272461, Learning Rate: 0.01\n",
      "Epoch [8727/10000], Loss: 66.83784484863281, Learning Rate: 0.01\n",
      "Epoch [8728/10000], Loss: 66.83622741699219, Learning Rate: 0.01\n",
      "Epoch [8729/10000], Loss: 66.8345718383789, Learning Rate: 0.01\n",
      "Epoch [8730/10000], Loss: 66.83291625976562, Learning Rate: 0.01\n",
      "Epoch [8731/10000], Loss: 66.83126068115234, Learning Rate: 0.01\n",
      "Epoch [8732/10000], Loss: 66.82965850830078, Learning Rate: 0.01\n",
      "Epoch [8733/10000], Loss: 66.82801818847656, Learning Rate: 0.01\n",
      "Epoch [8734/10000], Loss: 66.82643127441406, Learning Rate: 0.01\n",
      "Epoch [8735/10000], Loss: 66.82469177246094, Learning Rate: 0.01\n",
      "Epoch [8736/10000], Loss: 66.82305908203125, Learning Rate: 0.01\n",
      "Epoch [8737/10000], Loss: 66.8214340209961, Learning Rate: 0.01\n",
      "Epoch [8738/10000], Loss: 66.8198471069336, Learning Rate: 0.01\n",
      "Epoch [8739/10000], Loss: 66.8182373046875, Learning Rate: 0.01\n",
      "Epoch [8740/10000], Loss: 66.81660461425781, Learning Rate: 0.01\n",
      "Epoch [8741/10000], Loss: 66.8149185180664, Learning Rate: 0.01\n",
      "Epoch [8742/10000], Loss: 66.813232421875, Learning Rate: 0.01\n",
      "Epoch [8743/10000], Loss: 66.81171417236328, Learning Rate: 0.01\n",
      "Epoch [8744/10000], Loss: 66.8100357055664, Learning Rate: 0.01\n",
      "Epoch [8745/10000], Loss: 66.80841827392578, Learning Rate: 0.01\n",
      "Epoch [8746/10000], Loss: 66.80673217773438, Learning Rate: 0.01\n",
      "Epoch [8747/10000], Loss: 66.80514526367188, Learning Rate: 0.01\n",
      "Epoch [8748/10000], Loss: 66.8034896850586, Learning Rate: 0.01\n",
      "Epoch [8749/10000], Loss: 66.80182647705078, Learning Rate: 0.01\n",
      "Epoch [8750/10000], Loss: 66.8001937866211, Learning Rate: 0.01\n",
      "Epoch [8751/10000], Loss: 66.79855346679688, Learning Rate: 0.01\n",
      "Epoch [8752/10000], Loss: 66.79695129394531, Learning Rate: 0.01\n",
      "Epoch [8753/10000], Loss: 66.7951889038086, Learning Rate: 0.01\n",
      "Epoch [8754/10000], Loss: 66.79364776611328, Learning Rate: 0.01\n",
      "Epoch [8755/10000], Loss: 66.79191589355469, Learning Rate: 0.01\n",
      "Epoch [8756/10000], Loss: 66.79031372070312, Learning Rate: 0.01\n",
      "Epoch [8757/10000], Loss: 66.7886734008789, Learning Rate: 0.01\n",
      "Epoch [8758/10000], Loss: 66.78709411621094, Learning Rate: 0.01\n",
      "Epoch [8759/10000], Loss: 66.78544616699219, Learning Rate: 0.01\n",
      "Epoch [8760/10000], Loss: 66.78372192382812, Learning Rate: 0.01\n",
      "Epoch [8761/10000], Loss: 66.78211212158203, Learning Rate: 0.01\n",
      "Epoch [8762/10000], Loss: 66.78042602539062, Learning Rate: 0.01\n",
      "Epoch [8763/10000], Loss: 66.77877807617188, Learning Rate: 0.01\n",
      "Epoch [8764/10000], Loss: 66.77717590332031, Learning Rate: 0.01\n",
      "Epoch [8765/10000], Loss: 66.7754898071289, Learning Rate: 0.01\n",
      "Epoch [8766/10000], Loss: 66.77386474609375, Learning Rate: 0.01\n",
      "Epoch [8767/10000], Loss: 66.77217102050781, Learning Rate: 0.01\n",
      "Epoch [8768/10000], Loss: 66.77055358886719, Learning Rate: 0.01\n",
      "Epoch [8769/10000], Loss: 66.76893615722656, Learning Rate: 0.01\n",
      "Epoch [8770/10000], Loss: 66.767333984375, Learning Rate: 0.01\n",
      "Epoch [8771/10000], Loss: 66.76567840576172, Learning Rate: 0.01\n",
      "Epoch [8772/10000], Loss: 66.76393127441406, Learning Rate: 0.01\n",
      "Epoch [8773/10000], Loss: 66.76234436035156, Learning Rate: 0.01\n",
      "Epoch [8774/10000], Loss: 66.7607421875, Learning Rate: 0.01\n",
      "Epoch [8775/10000], Loss: 66.7590103149414, Learning Rate: 0.01\n",
      "Epoch [8776/10000], Loss: 66.7574234008789, Learning Rate: 0.01\n",
      "Epoch [8777/10000], Loss: 66.7557373046875, Learning Rate: 0.01\n",
      "Epoch [8778/10000], Loss: 66.7540512084961, Learning Rate: 0.01\n",
      "Epoch [8779/10000], Loss: 66.75243377685547, Learning Rate: 0.01\n",
      "Epoch [8780/10000], Loss: 66.75078582763672, Learning Rate: 0.01\n",
      "Epoch [8781/10000], Loss: 66.7491226196289, Learning Rate: 0.01\n",
      "Epoch [8782/10000], Loss: 66.74747467041016, Learning Rate: 0.01\n",
      "Epoch [8783/10000], Loss: 66.7457275390625, Learning Rate: 0.01\n",
      "Epoch [8784/10000], Loss: 66.74414825439453, Learning Rate: 0.01\n",
      "Epoch [8785/10000], Loss: 66.74256134033203, Learning Rate: 0.01\n",
      "Epoch [8786/10000], Loss: 66.74089050292969, Learning Rate: 0.01\n",
      "Epoch [8787/10000], Loss: 66.73918914794922, Learning Rate: 0.01\n",
      "Epoch [8788/10000], Loss: 66.73753356933594, Learning Rate: 0.01\n",
      "Epoch [8789/10000], Loss: 66.73588562011719, Learning Rate: 0.01\n",
      "Epoch [8790/10000], Loss: 66.7342529296875, Learning Rate: 0.01\n",
      "Epoch [8791/10000], Loss: 66.7325668334961, Learning Rate: 0.01\n",
      "Epoch [8792/10000], Loss: 66.73091888427734, Learning Rate: 0.01\n",
      "Epoch [8793/10000], Loss: 66.72926330566406, Learning Rate: 0.01\n",
      "Epoch [8794/10000], Loss: 66.72765350341797, Learning Rate: 0.01\n",
      "Epoch [8795/10000], Loss: 66.7259521484375, Learning Rate: 0.01\n",
      "Epoch [8796/10000], Loss: 66.72429656982422, Learning Rate: 0.01\n",
      "Epoch [8797/10000], Loss: 66.72265625, Learning Rate: 0.01\n",
      "Epoch [8798/10000], Loss: 66.7210464477539, Learning Rate: 0.01\n",
      "Epoch [8799/10000], Loss: 66.71937561035156, Learning Rate: 0.01\n",
      "Epoch [8800/10000], Loss: 66.71778106689453, Learning Rate: 0.01\n",
      "Epoch [8801/10000], Loss: 66.71621704101562, Learning Rate: 0.01\n",
      "Epoch [8802/10000], Loss: 66.71465301513672, Learning Rate: 0.01\n",
      "Epoch [8803/10000], Loss: 66.71309661865234, Learning Rate: 0.01\n",
      "Epoch [8804/10000], Loss: 66.71166229248047, Learning Rate: 0.01\n",
      "Epoch [8805/10000], Loss: 66.71033477783203, Learning Rate: 0.01\n",
      "Epoch [8806/10000], Loss: 66.70909118652344, Learning Rate: 0.01\n",
      "Epoch [8807/10000], Loss: 66.70812225341797, Learning Rate: 0.01\n",
      "Epoch [8808/10000], Loss: 66.70761108398438, Learning Rate: 0.01\n",
      "Epoch [8809/10000], Loss: 66.70758056640625, Learning Rate: 0.01\n",
      "Epoch [8810/10000], Loss: 66.70860290527344, Learning Rate: 0.01\n",
      "Epoch [8811/10000], Loss: 66.71096801757812, Learning Rate: 0.01\n",
      "Epoch [8812/10000], Loss: 66.71574401855469, Learning Rate: 0.01\n",
      "Epoch [8813/10000], Loss: 66.72416687011719, Learning Rate: 0.01\n",
      "Epoch [8814/10000], Loss: 66.73841857910156, Learning Rate: 0.01\n",
      "Epoch [8815/10000], Loss: 66.76211547851562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8816/10000], Loss: 66.80097961425781, Learning Rate: 0.01\n",
      "Epoch [8817/10000], Loss: 66.86408996582031, Learning Rate: 0.01\n",
      "Epoch [8818/10000], Loss: 66.96644592285156, Learning Rate: 0.01\n",
      "Epoch [8819/10000], Loss: 67.13133239746094, Learning Rate: 0.01\n",
      "Epoch [8820/10000], Loss: 67.3963623046875, Learning Rate: 0.01\n",
      "Epoch [8821/10000], Loss: 67.81561279296875, Learning Rate: 0.01\n",
      "Epoch [8822/10000], Loss: 68.47061157226562, Learning Rate: 0.01\n",
      "Epoch [8823/10000], Loss: 69.45149993896484, Learning Rate: 0.01\n",
      "Epoch [8824/10000], Loss: 70.8522720336914, Learning Rate: 0.01\n",
      "Epoch [8825/10000], Loss: 72.62893676757812, Learning Rate: 0.01\n",
      "Epoch [8826/10000], Loss: 74.53062438964844, Learning Rate: 0.01\n",
      "Epoch [8827/10000], Loss: 75.78428649902344, Learning Rate: 0.01\n",
      "Epoch [8828/10000], Loss: 75.51224517822266, Learning Rate: 0.01\n",
      "Epoch [8829/10000], Loss: 73.1828384399414, Learning Rate: 0.01\n",
      "Epoch [8830/10000], Loss: 69.80706024169922, Learning Rate: 0.01\n",
      "Epoch [8831/10000], Loss: 67.28175354003906, Learning Rate: 0.01\n",
      "Epoch [8832/10000], Loss: 66.93350219726562, Learning Rate: 0.01\n",
      "Epoch [8833/10000], Loss: 68.38082122802734, Learning Rate: 0.01\n",
      "Epoch [8834/10000], Loss: 70.01959991455078, Learning Rate: 0.01\n",
      "Epoch [8835/10000], Loss: 70.35855865478516, Learning Rate: 0.01\n",
      "Epoch [8836/10000], Loss: 69.09085845947266, Learning Rate: 0.01\n",
      "Epoch [8837/10000], Loss: 67.38690948486328, Learning Rate: 0.01\n",
      "Epoch [8838/10000], Loss: 66.65751647949219, Learning Rate: 0.01\n",
      "Epoch [8839/10000], Loss: 67.2419662475586, Learning Rate: 0.01\n",
      "Epoch [8840/10000], Loss: 68.25048828125, Learning Rate: 0.01\n",
      "Epoch [8841/10000], Loss: 68.56607818603516, Learning Rate: 0.01\n",
      "Epoch [8842/10000], Loss: 67.91832733154297, Learning Rate: 0.01\n",
      "Epoch [8843/10000], Loss: 66.9930648803711, Learning Rate: 0.01\n",
      "Epoch [8844/10000], Loss: 66.65193176269531, Learning Rate: 0.01\n",
      "Epoch [8845/10000], Loss: 67.03685760498047, Learning Rate: 0.01\n",
      "Epoch [8846/10000], Loss: 67.56298828125, Learning Rate: 0.01\n",
      "Epoch [8847/10000], Loss: 67.62440490722656, Learning Rate: 0.01\n",
      "Epoch [8848/10000], Loss: 67.18465423583984, Learning Rate: 0.01\n",
      "Epoch [8849/10000], Loss: 66.72499084472656, Learning Rate: 0.01\n",
      "Epoch [8850/10000], Loss: 66.66241455078125, Learning Rate: 0.01\n",
      "Epoch [8851/10000], Loss: 66.94355773925781, Learning Rate: 0.01\n",
      "Epoch [8852/10000], Loss: 67.18550872802734, Learning Rate: 0.01\n",
      "Epoch [8853/10000], Loss: 67.1180419921875, Learning Rate: 0.01\n",
      "Epoch [8854/10000], Loss: 66.83641815185547, Learning Rate: 0.01\n",
      "Epoch [8855/10000], Loss: 66.6351089477539, Learning Rate: 0.01\n",
      "Epoch [8856/10000], Loss: 66.67529296875, Learning Rate: 0.01\n",
      "Epoch [8857/10000], Loss: 66.8469009399414, Learning Rate: 0.01\n",
      "Epoch [8858/10000], Loss: 66.93102264404297, Learning Rate: 0.01\n",
      "Epoch [8859/10000], Loss: 66.84091186523438, Learning Rate: 0.01\n",
      "Epoch [8860/10000], Loss: 66.6819839477539, Learning Rate: 0.01\n",
      "Epoch [8861/10000], Loss: 66.61100769042969, Learning Rate: 0.01\n",
      "Epoch [8862/10000], Loss: 66.66832733154297, Learning Rate: 0.01\n",
      "Epoch [8863/10000], Loss: 66.75971984863281, Learning Rate: 0.01\n",
      "Epoch [8864/10000], Loss: 66.77661895751953, Learning Rate: 0.01\n",
      "Epoch [8865/10000], Loss: 66.70549011230469, Learning Rate: 0.01\n",
      "Epoch [8866/10000], Loss: 66.62345886230469, Learning Rate: 0.01\n",
      "Epoch [8867/10000], Loss: 66.60408782958984, Learning Rate: 0.01\n",
      "Epoch [8868/10000], Loss: 66.64601135253906, Learning Rate: 0.01\n",
      "Epoch [8869/10000], Loss: 66.688720703125, Learning Rate: 0.01\n",
      "Epoch [8870/10000], Loss: 66.68338012695312, Learning Rate: 0.01\n",
      "Epoch [8871/10000], Loss: 66.63784790039062, Learning Rate: 0.01\n",
      "Epoch [8872/10000], Loss: 66.5978012084961, Learning Rate: 0.01\n",
      "Epoch [8873/10000], Loss: 66.5954818725586, Learning Rate: 0.01\n",
      "Epoch [8874/10000], Loss: 66.62098693847656, Learning Rate: 0.01\n",
      "Epoch [8875/10000], Loss: 66.63999938964844, Learning Rate: 0.01\n",
      "Epoch [8876/10000], Loss: 66.63142395019531, Learning Rate: 0.01\n",
      "Epoch [8877/10000], Loss: 66.6046371459961, Learning Rate: 0.01\n",
      "Epoch [8878/10000], Loss: 66.58448028564453, Learning Rate: 0.01\n",
      "Epoch [8879/10000], Loss: 66.58522033691406, Learning Rate: 0.01\n",
      "Epoch [8880/10000], Loss: 66.59867858886719, Learning Rate: 0.01\n",
      "Epoch [8881/10000], Loss: 66.606689453125, Learning Rate: 0.01\n",
      "Epoch [8882/10000], Loss: 66.59981536865234, Learning Rate: 0.01\n",
      "Epoch [8883/10000], Loss: 66.58438110351562, Learning Rate: 0.01\n",
      "Epoch [8884/10000], Loss: 66.5735092163086, Learning Rate: 0.01\n",
      "Epoch [8885/10000], Loss: 66.5736083984375, Learning Rate: 0.01\n",
      "Epoch [8886/10000], Loss: 66.5801773071289, Learning Rate: 0.01\n",
      "Epoch [8887/10000], Loss: 66.5836410522461, Learning Rate: 0.01\n",
      "Epoch [8888/10000], Loss: 66.57926177978516, Learning Rate: 0.01\n",
      "Epoch [8889/10000], Loss: 66.57034301757812, Learning Rate: 0.01\n",
      "Epoch [8890/10000], Loss: 66.56368255615234, Learning Rate: 0.01\n",
      "Epoch [8891/10000], Loss: 66.56279754638672, Learning Rate: 0.01\n",
      "Epoch [8892/10000], Loss: 66.56544494628906, Learning Rate: 0.01\n",
      "Epoch [8893/10000], Loss: 66.56678009033203, Learning Rate: 0.01\n",
      "Epoch [8894/10000], Loss: 66.56400299072266, Learning Rate: 0.01\n",
      "Epoch [8895/10000], Loss: 66.55867004394531, Learning Rate: 0.01\n",
      "Epoch [8896/10000], Loss: 66.553955078125, Learning Rate: 0.01\n",
      "Epoch [8897/10000], Loss: 66.55231475830078, Learning Rate: 0.01\n",
      "Epoch [8898/10000], Loss: 66.5527114868164, Learning Rate: 0.01\n",
      "Epoch [8899/10000], Loss: 66.55292510986328, Learning Rate: 0.01\n",
      "Epoch [8900/10000], Loss: 66.55115509033203, Learning Rate: 0.01\n",
      "Epoch [8901/10000], Loss: 66.54777526855469, Learning Rate: 0.01\n",
      "Epoch [8902/10000], Loss: 66.54425811767578, Learning Rate: 0.01\n",
      "Epoch [8903/10000], Loss: 66.54219818115234, Learning Rate: 0.01\n",
      "Epoch [8904/10000], Loss: 66.54145050048828, Learning Rate: 0.01\n",
      "Epoch [8905/10000], Loss: 66.54102325439453, Learning Rate: 0.01\n",
      "Epoch [8906/10000], Loss: 66.53955841064453, Learning Rate: 0.01\n",
      "Epoch [8907/10000], Loss: 66.5372314453125, Learning Rate: 0.01\n",
      "Epoch [8908/10000], Loss: 66.53460693359375, Learning Rate: 0.01\n",
      "Epoch [8909/10000], Loss: 66.53247833251953, Learning Rate: 0.01\n",
      "Epoch [8910/10000], Loss: 66.53106689453125, Learning Rate: 0.01\n",
      "Epoch [8911/10000], Loss: 66.5300064086914, Learning Rate: 0.01\n",
      "Epoch [8912/10000], Loss: 66.52881622314453, Learning Rate: 0.01\n",
      "Epoch [8913/10000], Loss: 66.52696228027344, Learning Rate: 0.01\n",
      "Epoch [8914/10000], Loss: 66.5248794555664, Learning Rate: 0.01\n",
      "Epoch [8915/10000], Loss: 66.52273559570312, Learning Rate: 0.01\n",
      "Epoch [8916/10000], Loss: 66.5210952758789, Learning Rate: 0.01\n",
      "Epoch [8917/10000], Loss: 66.51971435546875, Learning Rate: 0.01\n",
      "Epoch [8918/10000], Loss: 66.51834106445312, Learning Rate: 0.01\n",
      "Epoch [8919/10000], Loss: 66.51681518554688, Learning Rate: 0.01\n",
      "Epoch [8920/10000], Loss: 66.51493072509766, Learning Rate: 0.01\n",
      "Epoch [8921/10000], Loss: 66.51309204101562, Learning Rate: 0.01\n",
      "Epoch [8922/10000], Loss: 66.51133728027344, Learning Rate: 0.01\n",
      "Epoch [8923/10000], Loss: 66.50965118408203, Learning Rate: 0.01\n",
      "Epoch [8924/10000], Loss: 66.50817108154297, Learning Rate: 0.01\n",
      "Epoch [8925/10000], Loss: 66.5067367553711, Learning Rate: 0.01\n",
      "Epoch [8926/10000], Loss: 66.50511932373047, Learning Rate: 0.01\n",
      "Epoch [8927/10000], Loss: 66.50334930419922, Learning Rate: 0.01\n",
      "Epoch [8928/10000], Loss: 66.50157928466797, Learning Rate: 0.01\n",
      "Epoch [8929/10000], Loss: 66.49988555908203, Learning Rate: 0.01\n",
      "Epoch [8930/10000], Loss: 66.498291015625, Learning Rate: 0.01\n",
      "Epoch [8931/10000], Loss: 66.49674224853516, Learning Rate: 0.01\n",
      "Epoch [8932/10000], Loss: 66.49520111083984, Learning Rate: 0.01\n",
      "Epoch [8933/10000], Loss: 66.49362182617188, Learning Rate: 0.01\n",
      "Epoch [8934/10000], Loss: 66.49188232421875, Learning Rate: 0.01\n",
      "Epoch [8935/10000], Loss: 66.49029541015625, Learning Rate: 0.01\n",
      "Epoch [8936/10000], Loss: 66.48873138427734, Learning Rate: 0.01\n",
      "Epoch [8937/10000], Loss: 66.4871597290039, Learning Rate: 0.01\n",
      "Epoch [8938/10000], Loss: 66.48583221435547, Learning Rate: 0.01\n",
      "Epoch [8939/10000], Loss: 66.48445129394531, Learning Rate: 0.01\n",
      "Epoch [8940/10000], Loss: 66.48320007324219, Learning Rate: 0.01\n",
      "Epoch [8941/10000], Loss: 66.48213195800781, Learning Rate: 0.01\n",
      "Epoch [8942/10000], Loss: 66.4813003540039, Learning Rate: 0.01\n",
      "Epoch [8943/10000], Loss: 66.48088836669922, Learning Rate: 0.01\n",
      "Epoch [8944/10000], Loss: 66.48112487792969, Learning Rate: 0.01\n",
      "Epoch [8945/10000], Loss: 66.48234558105469, Learning Rate: 0.01\n",
      "Epoch [8946/10000], Loss: 66.48511505126953, Learning Rate: 0.01\n",
      "Epoch [8947/10000], Loss: 66.49020385742188, Learning Rate: 0.01\n",
      "Epoch [8948/10000], Loss: 66.49879455566406, Learning Rate: 0.01\n",
      "Epoch [8949/10000], Loss: 66.51314544677734, Learning Rate: 0.01\n",
      "Epoch [8950/10000], Loss: 66.53634643554688, Learning Rate: 0.01\n",
      "Epoch [8951/10000], Loss: 66.57333374023438, Learning Rate: 0.01\n",
      "Epoch [8952/10000], Loss: 66.63191986083984, Learning Rate: 0.01\n",
      "Epoch [8953/10000], Loss: 66.7237548828125, Learning Rate: 0.01\n",
      "Epoch [8954/10000], Loss: 66.8668441772461, Learning Rate: 0.01\n",
      "Epoch [8955/10000], Loss: 67.0841293334961, Learning Rate: 0.01\n",
      "Epoch [8956/10000], Loss: 67.40794372558594, Learning Rate: 0.01\n",
      "Epoch [8957/10000], Loss: 67.86227416992188, Learning Rate: 0.01\n",
      "Epoch [8958/10000], Loss: 68.46003723144531, Learning Rate: 0.01\n",
      "Epoch [8959/10000], Loss: 69.13062286376953, Learning Rate: 0.01\n",
      "Epoch [8960/10000], Loss: 69.7234115600586, Learning Rate: 0.01\n",
      "Epoch [8961/10000], Loss: 69.93235778808594, Learning Rate: 0.01\n",
      "Epoch [8962/10000], Loss: 69.53547668457031, Learning Rate: 0.01\n",
      "Epoch [8963/10000], Loss: 68.52909851074219, Learning Rate: 0.01\n",
      "Epoch [8964/10000], Loss: 67.35676574707031, Learning Rate: 0.01\n",
      "Epoch [8965/10000], Loss: 66.5813980102539, Learning Rate: 0.01\n",
      "Epoch [8966/10000], Loss: 66.5052261352539, Learning Rate: 0.01\n",
      "Epoch [8967/10000], Loss: 66.97200775146484, Learning Rate: 0.01\n",
      "Epoch [8968/10000], Loss: 67.5359878540039, Learning Rate: 0.01\n",
      "Epoch [8969/10000], Loss: 67.77481079101562, Learning Rate: 0.01\n",
      "Epoch [8970/10000], Loss: 67.52919006347656, Learning Rate: 0.01\n",
      "Epoch [8971/10000], Loss: 67.00225830078125, Learning Rate: 0.01\n",
      "Epoch [8972/10000], Loss: 66.56430053710938, Learning Rate: 0.01\n",
      "Epoch [8973/10000], Loss: 66.46758270263672, Learning Rate: 0.01\n",
      "Epoch [8974/10000], Loss: 66.67486572265625, Learning Rate: 0.01\n",
      "Epoch [8975/10000], Loss: 66.94265747070312, Learning Rate: 0.01\n",
      "Epoch [8976/10000], Loss: 67.03713989257812, Learning Rate: 0.01\n",
      "Epoch [8977/10000], Loss: 66.89552307128906, Learning Rate: 0.01\n",
      "Epoch [8978/10000], Loss: 66.646728515625, Learning Rate: 0.01\n",
      "Epoch [8979/10000], Loss: 66.47493743896484, Learning Rate: 0.01\n",
      "Epoch [8980/10000], Loss: 66.4716567993164, Learning Rate: 0.01\n",
      "Epoch [8981/10000], Loss: 66.58381652832031, Learning Rate: 0.01\n",
      "Epoch [8982/10000], Loss: 66.6855697631836, Learning Rate: 0.01\n",
      "Epoch [8983/10000], Loss: 66.68806457519531, Learning Rate: 0.01\n",
      "Epoch [8984/10000], Loss: 66.59710693359375, Learning Rate: 0.01\n",
      "Epoch [8985/10000], Loss: 66.4900131225586, Learning Rate: 0.01\n",
      "Epoch [8986/10000], Loss: 66.4397201538086, Learning Rate: 0.01\n",
      "Epoch [8987/10000], Loss: 66.46080780029297, Learning Rate: 0.01\n",
      "Epoch [8988/10000], Loss: 66.51130676269531, Learning Rate: 0.01\n",
      "Epoch [8989/10000], Loss: 66.537353515625, Learning Rate: 0.01\n",
      "Epoch [8990/10000], Loss: 66.5169448852539, Learning Rate: 0.01\n",
      "Epoch [8991/10000], Loss: 66.46869659423828, Learning Rate: 0.01\n",
      "Epoch [8992/10000], Loss: 66.42950439453125, Learning Rate: 0.01\n",
      "Epoch [8993/10000], Loss: 66.42072296142578, Learning Rate: 0.01\n",
      "Epoch [8994/10000], Loss: 66.4364242553711, Learning Rate: 0.01\n",
      "Epoch [8995/10000], Loss: 66.45359802246094, Learning Rate: 0.01\n",
      "Epoch [8996/10000], Loss: 66.45372772216797, Learning Rate: 0.01\n",
      "Epoch [8997/10000], Loss: 66.4358139038086, Learning Rate: 0.01\n",
      "Epoch [8998/10000], Loss: 66.41339111328125, Learning Rate: 0.01\n",
      "Epoch [8999/10000], Loss: 66.40077209472656, Learning Rate: 0.01\n",
      "Epoch [9000/10000], Loss: 66.40199279785156, Learning Rate: 0.01\n",
      "Epoch [9001/10000], Loss: 66.40996551513672, Learning Rate: 0.01\n",
      "Epoch [9002/10000], Loss: 66.41394805908203, Learning Rate: 0.01\n",
      "Epoch [9003/10000], Loss: 66.40860748291016, Learning Rate: 0.01\n",
      "Epoch [9004/10000], Loss: 66.39673614501953, Learning Rate: 0.01\n",
      "Epoch [9005/10000], Loss: 66.3858413696289, Learning Rate: 0.01\n",
      "Epoch [9006/10000], Loss: 66.38145446777344, Learning Rate: 0.01\n",
      "Epoch [9007/10000], Loss: 66.38346862792969, Learning Rate: 0.01\n",
      "Epoch [9008/10000], Loss: 66.38711547851562, Learning Rate: 0.01\n",
      "Epoch [9009/10000], Loss: 66.38749694824219, Learning Rate: 0.01\n",
      "Epoch [9010/10000], Loss: 66.38268280029297, Learning Rate: 0.01\n",
      "Epoch [9011/10000], Loss: 66.37507629394531, Learning Rate: 0.01\n",
      "Epoch [9012/10000], Loss: 66.36856079101562, Learning Rate: 0.01\n",
      "Epoch [9013/10000], Loss: 66.36589813232422, Learning Rate: 0.01\n",
      "Epoch [9014/10000], Loss: 66.36670684814453, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9015/10000], Loss: 66.36827850341797, Learning Rate: 0.01\n",
      "Epoch [9016/10000], Loss: 66.36810302734375, Learning Rate: 0.01\n",
      "Epoch [9017/10000], Loss: 66.36504364013672, Learning Rate: 0.01\n",
      "Epoch [9018/10000], Loss: 66.36026763916016, Learning Rate: 0.01\n",
      "Epoch [9019/10000], Loss: 66.35559844970703, Learning Rate: 0.01\n",
      "Epoch [9020/10000], Loss: 66.35289001464844, Learning Rate: 0.01\n",
      "Epoch [9021/10000], Loss: 66.35214233398438, Learning Rate: 0.01\n",
      "Epoch [9022/10000], Loss: 66.35233306884766, Learning Rate: 0.01\n",
      "Epoch [9023/10000], Loss: 66.35213470458984, Learning Rate: 0.01\n",
      "Epoch [9024/10000], Loss: 66.35060119628906, Learning Rate: 0.01\n",
      "Epoch [9025/10000], Loss: 66.34765625, Learning Rate: 0.01\n",
      "Epoch [9026/10000], Loss: 66.34430694580078, Learning Rate: 0.01\n",
      "Epoch [9027/10000], Loss: 66.34139251708984, Learning Rate: 0.01\n",
      "Epoch [9028/10000], Loss: 66.33966827392578, Learning Rate: 0.01\n",
      "Epoch [9029/10000], Loss: 66.33879852294922, Learning Rate: 0.01\n",
      "Epoch [9030/10000], Loss: 66.33808898925781, Learning Rate: 0.01\n",
      "Epoch [9031/10000], Loss: 66.33702087402344, Learning Rate: 0.01\n",
      "Epoch [9032/10000], Loss: 66.33535766601562, Learning Rate: 0.01\n",
      "Epoch [9033/10000], Loss: 66.33305358886719, Learning Rate: 0.01\n",
      "Epoch [9034/10000], Loss: 66.3305892944336, Learning Rate: 0.01\n",
      "Epoch [9035/10000], Loss: 66.32849884033203, Learning Rate: 0.01\n",
      "Epoch [9036/10000], Loss: 66.32677459716797, Learning Rate: 0.01\n",
      "Epoch [9037/10000], Loss: 66.32545471191406, Learning Rate: 0.01\n",
      "Epoch [9038/10000], Loss: 66.32427978515625, Learning Rate: 0.01\n",
      "Epoch [9039/10000], Loss: 66.3229751586914, Learning Rate: 0.01\n",
      "Epoch [9040/10000], Loss: 66.32136535644531, Learning Rate: 0.01\n",
      "Epoch [9041/10000], Loss: 66.31951904296875, Learning Rate: 0.01\n",
      "Epoch [9042/10000], Loss: 66.31759643554688, Learning Rate: 0.01\n",
      "Epoch [9043/10000], Loss: 66.31571960449219, Learning Rate: 0.01\n",
      "Epoch [9044/10000], Loss: 66.31400299072266, Learning Rate: 0.01\n",
      "Epoch [9045/10000], Loss: 66.31246948242188, Learning Rate: 0.01\n",
      "Epoch [9046/10000], Loss: 66.31100463867188, Learning Rate: 0.01\n",
      "Epoch [9047/10000], Loss: 66.30948638916016, Learning Rate: 0.01\n",
      "Epoch [9048/10000], Loss: 66.30796813964844, Learning Rate: 0.01\n",
      "Epoch [9049/10000], Loss: 66.3062744140625, Learning Rate: 0.01\n",
      "Epoch [9050/10000], Loss: 66.30470275878906, Learning Rate: 0.01\n",
      "Epoch [9051/10000], Loss: 66.30287170410156, Learning Rate: 0.01\n",
      "Epoch [9052/10000], Loss: 66.30120086669922, Learning Rate: 0.01\n",
      "Epoch [9053/10000], Loss: 66.29955291748047, Learning Rate: 0.01\n",
      "Epoch [9054/10000], Loss: 66.29796600341797, Learning Rate: 0.01\n",
      "Epoch [9055/10000], Loss: 66.29641723632812, Learning Rate: 0.01\n",
      "Epoch [9056/10000], Loss: 66.2947998046875, Learning Rate: 0.01\n",
      "Epoch [9057/10000], Loss: 66.29316711425781, Learning Rate: 0.01\n",
      "Epoch [9058/10000], Loss: 66.29154968261719, Learning Rate: 0.01\n",
      "Epoch [9059/10000], Loss: 66.28995513916016, Learning Rate: 0.01\n",
      "Epoch [9060/10000], Loss: 66.28829956054688, Learning Rate: 0.01\n",
      "Epoch [9061/10000], Loss: 66.2866439819336, Learning Rate: 0.01\n",
      "Epoch [9062/10000], Loss: 66.28498077392578, Learning Rate: 0.01\n",
      "Epoch [9063/10000], Loss: 66.28343200683594, Learning Rate: 0.01\n",
      "Epoch [9064/10000], Loss: 66.28179931640625, Learning Rate: 0.01\n",
      "Epoch [9065/10000], Loss: 66.28022766113281, Learning Rate: 0.01\n",
      "Epoch [9066/10000], Loss: 66.27864837646484, Learning Rate: 0.01\n",
      "Epoch [9067/10000], Loss: 66.27694702148438, Learning Rate: 0.01\n",
      "Epoch [9068/10000], Loss: 66.27538299560547, Learning Rate: 0.01\n",
      "Epoch [9069/10000], Loss: 66.27371215820312, Learning Rate: 0.01\n",
      "Epoch [9070/10000], Loss: 66.27204895019531, Learning Rate: 0.01\n",
      "Epoch [9071/10000], Loss: 66.2703857421875, Learning Rate: 0.01\n",
      "Epoch [9072/10000], Loss: 66.26881408691406, Learning Rate: 0.01\n",
      "Epoch [9073/10000], Loss: 66.26720428466797, Learning Rate: 0.01\n",
      "Epoch [9074/10000], Loss: 66.26557159423828, Learning Rate: 0.01\n",
      "Epoch [9075/10000], Loss: 66.26394653320312, Learning Rate: 0.01\n",
      "Epoch [9076/10000], Loss: 66.26233673095703, Learning Rate: 0.01\n",
      "Epoch [9077/10000], Loss: 66.26072692871094, Learning Rate: 0.01\n",
      "Epoch [9078/10000], Loss: 66.25907897949219, Learning Rate: 0.01\n",
      "Epoch [9079/10000], Loss: 66.2574234008789, Learning Rate: 0.01\n",
      "Epoch [9080/10000], Loss: 66.25575256347656, Learning Rate: 0.01\n",
      "Epoch [9081/10000], Loss: 66.25419616699219, Learning Rate: 0.01\n",
      "Epoch [9082/10000], Loss: 66.25252532958984, Learning Rate: 0.01\n",
      "Epoch [9083/10000], Loss: 66.25090789794922, Learning Rate: 0.01\n",
      "Epoch [9084/10000], Loss: 66.24925994873047, Learning Rate: 0.01\n",
      "Epoch [9085/10000], Loss: 66.24758911132812, Learning Rate: 0.01\n",
      "Epoch [9086/10000], Loss: 66.2459716796875, Learning Rate: 0.01\n",
      "Epoch [9087/10000], Loss: 66.24439239501953, Learning Rate: 0.01\n",
      "Epoch [9088/10000], Loss: 66.24275207519531, Learning Rate: 0.01\n",
      "Epoch [9089/10000], Loss: 66.24109649658203, Learning Rate: 0.01\n",
      "Epoch [9090/10000], Loss: 66.23942565917969, Learning Rate: 0.01\n",
      "Epoch [9091/10000], Loss: 66.23774719238281, Learning Rate: 0.01\n",
      "Epoch [9092/10000], Loss: 66.2362060546875, Learning Rate: 0.01\n",
      "Epoch [9093/10000], Loss: 66.23448181152344, Learning Rate: 0.01\n",
      "Epoch [9094/10000], Loss: 66.2328872680664, Learning Rate: 0.01\n",
      "Epoch [9095/10000], Loss: 66.2312240600586, Learning Rate: 0.01\n",
      "Epoch [9096/10000], Loss: 66.22957611083984, Learning Rate: 0.01\n",
      "Epoch [9097/10000], Loss: 66.22799682617188, Learning Rate: 0.01\n",
      "Epoch [9098/10000], Loss: 66.2263412475586, Learning Rate: 0.01\n",
      "Epoch [9099/10000], Loss: 66.22469329833984, Learning Rate: 0.01\n",
      "Epoch [9100/10000], Loss: 66.22305297851562, Learning Rate: 0.01\n",
      "Epoch [9101/10000], Loss: 66.22135162353516, Learning Rate: 0.01\n",
      "Epoch [9102/10000], Loss: 66.21977996826172, Learning Rate: 0.01\n",
      "Epoch [9103/10000], Loss: 66.21814727783203, Learning Rate: 0.01\n",
      "Epoch [9104/10000], Loss: 66.21646118164062, Learning Rate: 0.01\n",
      "Epoch [9105/10000], Loss: 66.21475982666016, Learning Rate: 0.01\n",
      "Epoch [9106/10000], Loss: 66.213134765625, Learning Rate: 0.01\n",
      "Epoch [9107/10000], Loss: 66.2115249633789, Learning Rate: 0.01\n",
      "Epoch [9108/10000], Loss: 66.2098388671875, Learning Rate: 0.01\n",
      "Epoch [9109/10000], Loss: 66.20819854736328, Learning Rate: 0.01\n",
      "Epoch [9110/10000], Loss: 66.20648193359375, Learning Rate: 0.01\n",
      "Epoch [9111/10000], Loss: 66.20491027832031, Learning Rate: 0.01\n",
      "Epoch [9112/10000], Loss: 66.20320129394531, Learning Rate: 0.01\n",
      "Epoch [9113/10000], Loss: 66.20159149169922, Learning Rate: 0.01\n",
      "Epoch [9114/10000], Loss: 66.19982147216797, Learning Rate: 0.01\n",
      "Epoch [9115/10000], Loss: 66.19819641113281, Learning Rate: 0.01\n",
      "Epoch [9116/10000], Loss: 66.19652557373047, Learning Rate: 0.01\n",
      "Epoch [9117/10000], Loss: 66.19486999511719, Learning Rate: 0.01\n",
      "Epoch [9118/10000], Loss: 66.19317626953125, Learning Rate: 0.01\n",
      "Epoch [9119/10000], Loss: 66.19158935546875, Learning Rate: 0.01\n",
      "Epoch [9120/10000], Loss: 66.18993377685547, Learning Rate: 0.01\n",
      "Epoch [9121/10000], Loss: 66.18827056884766, Learning Rate: 0.01\n",
      "Epoch [9122/10000], Loss: 66.18658447265625, Learning Rate: 0.01\n",
      "Epoch [9123/10000], Loss: 66.1849365234375, Learning Rate: 0.01\n",
      "Epoch [9124/10000], Loss: 66.18329620361328, Learning Rate: 0.01\n",
      "Epoch [9125/10000], Loss: 66.18162536621094, Learning Rate: 0.01\n",
      "Epoch [9126/10000], Loss: 66.17988586425781, Learning Rate: 0.01\n",
      "Epoch [9127/10000], Loss: 66.17820739746094, Learning Rate: 0.01\n",
      "Epoch [9128/10000], Loss: 66.17657470703125, Learning Rate: 0.01\n",
      "Epoch [9129/10000], Loss: 66.17487335205078, Learning Rate: 0.01\n",
      "Epoch [9130/10000], Loss: 66.17326354980469, Learning Rate: 0.01\n",
      "Epoch [9131/10000], Loss: 66.17156982421875, Learning Rate: 0.01\n",
      "Epoch [9132/10000], Loss: 66.16986846923828, Learning Rate: 0.01\n",
      "Epoch [9133/10000], Loss: 66.16815948486328, Learning Rate: 0.01\n",
      "Epoch [9134/10000], Loss: 66.16656494140625, Learning Rate: 0.01\n",
      "Epoch [9135/10000], Loss: 66.164794921875, Learning Rate: 0.01\n",
      "Epoch [9136/10000], Loss: 66.1631851196289, Learning Rate: 0.01\n",
      "Epoch [9137/10000], Loss: 66.1615219116211, Learning Rate: 0.01\n",
      "Epoch [9138/10000], Loss: 66.1598129272461, Learning Rate: 0.01\n",
      "Epoch [9139/10000], Loss: 66.15815734863281, Learning Rate: 0.01\n",
      "Epoch [9140/10000], Loss: 66.15644836425781, Learning Rate: 0.01\n",
      "Epoch [9141/10000], Loss: 66.15479278564453, Learning Rate: 0.01\n",
      "Epoch [9142/10000], Loss: 66.15312957763672, Learning Rate: 0.01\n",
      "Epoch [9143/10000], Loss: 66.15135192871094, Learning Rate: 0.01\n",
      "Epoch [9144/10000], Loss: 66.14974975585938, Learning Rate: 0.01\n",
      "Epoch [9145/10000], Loss: 66.14804077148438, Learning Rate: 0.01\n",
      "Epoch [9146/10000], Loss: 66.14632415771484, Learning Rate: 0.01\n",
      "Epoch [9147/10000], Loss: 66.14464569091797, Learning Rate: 0.01\n",
      "Epoch [9148/10000], Loss: 66.14297485351562, Learning Rate: 0.01\n",
      "Epoch [9149/10000], Loss: 66.1412582397461, Learning Rate: 0.01\n",
      "Epoch [9150/10000], Loss: 66.13961791992188, Learning Rate: 0.01\n",
      "Epoch [9151/10000], Loss: 66.13789367675781, Learning Rate: 0.01\n",
      "Epoch [9152/10000], Loss: 66.13619232177734, Learning Rate: 0.01\n",
      "Epoch [9153/10000], Loss: 66.1344985961914, Learning Rate: 0.01\n",
      "Epoch [9154/10000], Loss: 66.13288879394531, Learning Rate: 0.01\n",
      "Epoch [9155/10000], Loss: 66.13114166259766, Learning Rate: 0.01\n",
      "Epoch [9156/10000], Loss: 66.12958526611328, Learning Rate: 0.01\n",
      "Epoch [9157/10000], Loss: 66.12782287597656, Learning Rate: 0.01\n",
      "Epoch [9158/10000], Loss: 66.12627410888672, Learning Rate: 0.01\n",
      "Epoch [9159/10000], Loss: 66.12471771240234, Learning Rate: 0.01\n",
      "Epoch [9160/10000], Loss: 66.12322998046875, Learning Rate: 0.01\n",
      "Epoch [9161/10000], Loss: 66.12187957763672, Learning Rate: 0.01\n",
      "Epoch [9162/10000], Loss: 66.12066650390625, Learning Rate: 0.01\n",
      "Epoch [9163/10000], Loss: 66.1197738647461, Learning Rate: 0.01\n",
      "Epoch [9164/10000], Loss: 66.11932373046875, Learning Rate: 0.01\n",
      "Epoch [9165/10000], Loss: 66.11952209472656, Learning Rate: 0.01\n",
      "Epoch [9166/10000], Loss: 66.12078857421875, Learning Rate: 0.01\n",
      "Epoch [9167/10000], Loss: 66.12384033203125, Learning Rate: 0.01\n",
      "Epoch [9168/10000], Loss: 66.12964630126953, Learning Rate: 0.01\n",
      "Epoch [9169/10000], Loss: 66.13988494873047, Learning Rate: 0.01\n",
      "Epoch [9170/10000], Loss: 66.15737915039062, Learning Rate: 0.01\n",
      "Epoch [9171/10000], Loss: 66.18665313720703, Learning Rate: 0.01\n",
      "Epoch [9172/10000], Loss: 66.23482513427734, Learning Rate: 0.01\n",
      "Epoch [9173/10000], Loss: 66.31403350830078, Learning Rate: 0.01\n",
      "Epoch [9174/10000], Loss: 66.44316864013672, Learning Rate: 0.01\n",
      "Epoch [9175/10000], Loss: 66.653564453125, Learning Rate: 0.01\n",
      "Epoch [9176/10000], Loss: 66.99201202392578, Learning Rate: 0.01\n",
      "Epoch [9177/10000], Loss: 67.5321273803711, Learning Rate: 0.01\n",
      "Epoch [9178/10000], Loss: 68.36519622802734, Learning Rate: 0.01\n",
      "Epoch [9179/10000], Loss: 69.6066665649414, Learning Rate: 0.01\n",
      "Epoch [9180/10000], Loss: 71.2923355102539, Learning Rate: 0.01\n",
      "Epoch [9181/10000], Loss: 73.3155517578125, Learning Rate: 0.01\n",
      "Epoch [9182/10000], Loss: 75.08118438720703, Learning Rate: 0.01\n",
      "Epoch [9183/10000], Loss: 75.68782806396484, Learning Rate: 0.01\n",
      "Epoch [9184/10000], Loss: 74.14787292480469, Learning Rate: 0.01\n",
      "Epoch [9185/10000], Loss: 70.84054565429688, Learning Rate: 0.01\n",
      "Epoch [9186/10000], Loss: 67.52517700195312, Learning Rate: 0.01\n",
      "Epoch [9187/10000], Loss: 66.18871307373047, Learning Rate: 0.01\n",
      "Epoch [9188/10000], Loss: 67.18724822998047, Learning Rate: 0.01\n",
      "Epoch [9189/10000], Loss: 69.11812591552734, Learning Rate: 0.01\n",
      "Epoch [9190/10000], Loss: 70.10871887207031, Learning Rate: 0.01\n",
      "Epoch [9191/10000], Loss: 69.23912048339844, Learning Rate: 0.01\n",
      "Epoch [9192/10000], Loss: 67.3602523803711, Learning Rate: 0.01\n",
      "Epoch [9193/10000], Loss: 66.13335418701172, Learning Rate: 0.01\n",
      "Epoch [9194/10000], Loss: 66.40821838378906, Learning Rate: 0.01\n",
      "Epoch [9195/10000], Loss: 67.50807189941406, Learning Rate: 0.01\n",
      "Epoch [9196/10000], Loss: 68.11579132080078, Learning Rate: 0.01\n",
      "Epoch [9197/10000], Loss: 67.62657928466797, Learning Rate: 0.01\n",
      "Epoch [9198/10000], Loss: 66.60173797607422, Learning Rate: 0.01\n",
      "Epoch [9199/10000], Loss: 66.06800842285156, Learning Rate: 0.01\n",
      "Epoch [9200/10000], Loss: 66.380859375, Learning Rate: 0.01\n",
      "Epoch [9201/10000], Loss: 66.98676300048828, Learning Rate: 0.01\n",
      "Epoch [9202/10000], Loss: 67.14680480957031, Learning Rate: 0.01\n",
      "Epoch [9203/10000], Loss: 66.7096939086914, Learning Rate: 0.01\n",
      "Epoch [9204/10000], Loss: 66.17647552490234, Learning Rate: 0.01\n",
      "Epoch [9205/10000], Loss: 66.06098175048828, Learning Rate: 0.01\n",
      "Epoch [9206/10000], Loss: 66.35595703125, Learning Rate: 0.01\n",
      "Epoch [9207/10000], Loss: 66.63666534423828, Learning Rate: 0.01\n",
      "Epoch [9208/10000], Loss: 66.57581329345703, Learning Rate: 0.01\n",
      "Epoch [9209/10000], Loss: 66.2672119140625, Learning Rate: 0.01\n",
      "Epoch [9210/10000], Loss: 66.04707336425781, Learning Rate: 0.01\n",
      "Epoch [9211/10000], Loss: 66.09970092773438, Learning Rate: 0.01\n",
      "Epoch [9212/10000], Loss: 66.29227447509766, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9213/10000], Loss: 66.37368774414062, Learning Rate: 0.01\n",
      "Epoch [9214/10000], Loss: 66.2584457397461, Learning Rate: 0.01\n",
      "Epoch [9215/10000], Loss: 66.08277893066406, Learning Rate: 0.01\n",
      "Epoch [9216/10000], Loss: 66.0224380493164, Learning Rate: 0.01\n",
      "Epoch [9217/10000], Loss: 66.10177612304688, Learning Rate: 0.01\n",
      "Epoch [9218/10000], Loss: 66.19776916503906, Learning Rate: 0.01\n",
      "Epoch [9219/10000], Loss: 66.19552612304688, Learning Rate: 0.01\n",
      "Epoch [9220/10000], Loss: 66.10391235351562, Learning Rate: 0.01\n",
      "Epoch [9221/10000], Loss: 66.02313995361328, Learning Rate: 0.01\n",
      "Epoch [9222/10000], Loss: 66.02388763427734, Learning Rate: 0.01\n",
      "Epoch [9223/10000], Loss: 66.08051300048828, Learning Rate: 0.01\n",
      "Epoch [9224/10000], Loss: 66.1163558959961, Learning Rate: 0.01\n",
      "Epoch [9225/10000], Loss: 66.09097290039062, Learning Rate: 0.01\n",
      "Epoch [9226/10000], Loss: 66.03423309326172, Learning Rate: 0.01\n",
      "Epoch [9227/10000], Loss: 66.00239562988281, Learning Rate: 0.01\n",
      "Epoch [9228/10000], Loss: 66.01640319824219, Learning Rate: 0.01\n",
      "Epoch [9229/10000], Loss: 66.0478744506836, Learning Rate: 0.01\n",
      "Epoch [9230/10000], Loss: 66.0567626953125, Learning Rate: 0.01\n",
      "Epoch [9231/10000], Loss: 66.0339584350586, Learning Rate: 0.01\n",
      "Epoch [9232/10000], Loss: 66.0033950805664, Learning Rate: 0.01\n",
      "Epoch [9233/10000], Loss: 65.99256896972656, Learning Rate: 0.01\n",
      "Epoch [9234/10000], Loss: 66.0042953491211, Learning Rate: 0.01\n",
      "Epoch [9235/10000], Loss: 66.01937103271484, Learning Rate: 0.01\n",
      "Epoch [9236/10000], Loss: 66.01876068115234, Learning Rate: 0.01\n",
      "Epoch [9237/10000], Loss: 66.00283813476562, Learning Rate: 0.01\n",
      "Epoch [9238/10000], Loss: 65.98662567138672, Learning Rate: 0.01\n",
      "Epoch [9239/10000], Loss: 65.9827651977539, Learning Rate: 0.01\n",
      "Epoch [9240/10000], Loss: 65.98963928222656, Learning Rate: 0.01\n",
      "Epoch [9241/10000], Loss: 65.99600219726562, Learning Rate: 0.01\n",
      "Epoch [9242/10000], Loss: 65.9932632446289, Learning Rate: 0.01\n",
      "Epoch [9243/10000], Loss: 65.98328399658203, Learning Rate: 0.01\n",
      "Epoch [9244/10000], Loss: 65.97437286376953, Learning Rate: 0.01\n",
      "Epoch [9245/10000], Loss: 65.97240447998047, Learning Rate: 0.01\n",
      "Epoch [9246/10000], Loss: 65.97566223144531, Learning Rate: 0.01\n",
      "Epoch [9247/10000], Loss: 65.97796630859375, Learning Rate: 0.01\n",
      "Epoch [9248/10000], Loss: 65.97545623779297, Learning Rate: 0.01\n",
      "Epoch [9249/10000], Loss: 65.96920013427734, Learning Rate: 0.01\n",
      "Epoch [9250/10000], Loss: 65.96378326416016, Learning Rate: 0.01\n",
      "Epoch [9251/10000], Loss: 65.96206665039062, Learning Rate: 0.01\n",
      "Epoch [9252/10000], Loss: 65.96300506591797, Learning Rate: 0.01\n",
      "Epoch [9253/10000], Loss: 65.96341705322266, Learning Rate: 0.01\n",
      "Epoch [9254/10000], Loss: 65.9612808227539, Learning Rate: 0.01\n",
      "Epoch [9255/10000], Loss: 65.9571762084961, Learning Rate: 0.01\n",
      "Epoch [9256/10000], Loss: 65.9534683227539, Learning Rate: 0.01\n",
      "Epoch [9257/10000], Loss: 65.95156860351562, Learning Rate: 0.01\n",
      "Epoch [9258/10000], Loss: 65.95105743408203, Learning Rate: 0.01\n",
      "Epoch [9259/10000], Loss: 65.95063018798828, Learning Rate: 0.01\n",
      "Epoch [9260/10000], Loss: 65.94884490966797, Learning Rate: 0.01\n",
      "Epoch [9261/10000], Loss: 65.94598388671875, Learning Rate: 0.01\n",
      "Epoch [9262/10000], Loss: 65.94314575195312, Learning Rate: 0.01\n",
      "Epoch [9263/10000], Loss: 65.94107055664062, Learning Rate: 0.01\n",
      "Epoch [9264/10000], Loss: 65.93994140625, Learning Rate: 0.01\n",
      "Epoch [9265/10000], Loss: 65.9389877319336, Learning Rate: 0.01\n",
      "Epoch [9266/10000], Loss: 65.9374008178711, Learning Rate: 0.01\n",
      "Epoch [9267/10000], Loss: 65.93515014648438, Learning Rate: 0.01\n",
      "Epoch [9268/10000], Loss: 65.93286895751953, Learning Rate: 0.01\n",
      "Epoch [9269/10000], Loss: 65.93072509765625, Learning Rate: 0.01\n",
      "Epoch [9270/10000], Loss: 65.92925262451172, Learning Rate: 0.01\n",
      "Epoch [9271/10000], Loss: 65.92790222167969, Learning Rate: 0.01\n",
      "Epoch [9272/10000], Loss: 65.92643737792969, Learning Rate: 0.01\n",
      "Epoch [9273/10000], Loss: 65.92451477050781, Learning Rate: 0.01\n",
      "Epoch [9274/10000], Loss: 65.92251586914062, Learning Rate: 0.01\n",
      "Epoch [9275/10000], Loss: 65.92056274414062, Learning Rate: 0.01\n",
      "Epoch [9276/10000], Loss: 65.91875457763672, Learning Rate: 0.01\n",
      "Epoch [9277/10000], Loss: 65.9172134399414, Learning Rate: 0.01\n",
      "Epoch [9278/10000], Loss: 65.91569519042969, Learning Rate: 0.01\n",
      "Epoch [9279/10000], Loss: 65.91397094726562, Learning Rate: 0.01\n",
      "Epoch [9280/10000], Loss: 65.91218566894531, Learning Rate: 0.01\n",
      "Epoch [9281/10000], Loss: 65.91027069091797, Learning Rate: 0.01\n",
      "Epoch [9282/10000], Loss: 65.9084243774414, Learning Rate: 0.01\n",
      "Epoch [9283/10000], Loss: 65.9068374633789, Learning Rate: 0.01\n",
      "Epoch [9284/10000], Loss: 65.90521240234375, Learning Rate: 0.01\n",
      "Epoch [9285/10000], Loss: 65.9035873413086, Learning Rate: 0.01\n",
      "Epoch [9286/10000], Loss: 65.90189361572266, Learning Rate: 0.01\n",
      "Epoch [9287/10000], Loss: 65.90015411376953, Learning Rate: 0.01\n",
      "Epoch [9288/10000], Loss: 65.89840698242188, Learning Rate: 0.01\n",
      "Epoch [9289/10000], Loss: 65.8968505859375, Learning Rate: 0.01\n",
      "Epoch [9290/10000], Loss: 65.89534759521484, Learning Rate: 0.01\n",
      "Epoch [9291/10000], Loss: 65.89399719238281, Learning Rate: 0.01\n",
      "Epoch [9292/10000], Loss: 65.8926773071289, Learning Rate: 0.01\n",
      "Epoch [9293/10000], Loss: 65.89156341552734, Learning Rate: 0.01\n",
      "Epoch [9294/10000], Loss: 65.8907470703125, Learning Rate: 0.01\n",
      "Epoch [9295/10000], Loss: 65.89031219482422, Learning Rate: 0.01\n",
      "Epoch [9296/10000], Loss: 65.89057922363281, Learning Rate: 0.01\n",
      "Epoch [9297/10000], Loss: 65.89176940917969, Learning Rate: 0.01\n",
      "Epoch [9298/10000], Loss: 65.89437103271484, Learning Rate: 0.01\n",
      "Epoch [9299/10000], Loss: 65.89922332763672, Learning Rate: 0.01\n",
      "Epoch [9300/10000], Loss: 65.90726470947266, Learning Rate: 0.01\n",
      "Epoch [9301/10000], Loss: 65.92028045654297, Learning Rate: 0.01\n",
      "Epoch [9302/10000], Loss: 65.94111633300781, Learning Rate: 0.01\n",
      "Epoch [9303/10000], Loss: 65.97370910644531, Learning Rate: 0.01\n",
      "Epoch [9304/10000], Loss: 66.02432250976562, Learning Rate: 0.01\n",
      "Epoch [9305/10000], Loss: 66.1023178100586, Learning Rate: 0.01\n",
      "Epoch [9306/10000], Loss: 66.22169494628906, Learning Rate: 0.01\n",
      "Epoch [9307/10000], Loss: 66.40131378173828, Learning Rate: 0.01\n",
      "Epoch [9308/10000], Loss: 66.6672134399414, Learning Rate: 0.01\n",
      "Epoch [9309/10000], Loss: 67.04434204101562, Learning Rate: 0.01\n",
      "Epoch [9310/10000], Loss: 67.55403137207031, Learning Rate: 0.01\n",
      "Epoch [9311/10000], Loss: 68.17088317871094, Learning Rate: 0.01\n",
      "Epoch [9312/10000], Loss: 68.80948638916016, Learning Rate: 0.01\n",
      "Epoch [9313/10000], Loss: 69.24950408935547, Learning Rate: 0.01\n",
      "Epoch [9314/10000], Loss: 69.24098205566406, Learning Rate: 0.01\n",
      "Epoch [9315/10000], Loss: 68.59967041015625, Learning Rate: 0.01\n",
      "Epoch [9316/10000], Loss: 67.50509643554688, Learning Rate: 0.01\n",
      "Epoch [9317/10000], Loss: 66.43087005615234, Learning Rate: 0.01\n",
      "Epoch [9318/10000], Loss: 65.87583923339844, Learning Rate: 0.01\n",
      "Epoch [9319/10000], Loss: 65.99028015136719, Learning Rate: 0.01\n",
      "Epoch [9320/10000], Loss: 66.5219497680664, Learning Rate: 0.01\n",
      "Epoch [9321/10000], Loss: 67.02794647216797, Learning Rate: 0.01\n",
      "Epoch [9322/10000], Loss: 67.14668273925781, Learning Rate: 0.01\n",
      "Epoch [9323/10000], Loss: 66.81539154052734, Learning Rate: 0.01\n",
      "Epoch [9324/10000], Loss: 66.2760238647461, Learning Rate: 0.01\n",
      "Epoch [9325/10000], Loss: 65.89277648925781, Learning Rate: 0.01\n",
      "Epoch [9326/10000], Loss: 65.86431884765625, Learning Rate: 0.01\n",
      "Epoch [9327/10000], Loss: 66.10984802246094, Learning Rate: 0.01\n",
      "Epoch [9328/10000], Loss: 66.37608337402344, Learning Rate: 0.01\n",
      "Epoch [9329/10000], Loss: 66.44249725341797, Learning Rate: 0.01\n",
      "Epoch [9330/10000], Loss: 66.2724380493164, Learning Rate: 0.01\n",
      "Epoch [9331/10000], Loss: 66.00700378417969, Learning Rate: 0.01\n",
      "Epoch [9332/10000], Loss: 65.83699035644531, Learning Rate: 0.01\n",
      "Epoch [9333/10000], Loss: 65.84961700439453, Learning Rate: 0.01\n",
      "Epoch [9334/10000], Loss: 65.98356628417969, Learning Rate: 0.01\n",
      "Epoch [9335/10000], Loss: 66.10258483886719, Learning Rate: 0.01\n",
      "Epoch [9336/10000], Loss: 66.10802459716797, Learning Rate: 0.01\n",
      "Epoch [9337/10000], Loss: 66.00416564941406, Learning Rate: 0.01\n",
      "Epoch [9338/10000], Loss: 65.87557983398438, Learning Rate: 0.01\n",
      "Epoch [9339/10000], Loss: 65.81072235107422, Learning Rate: 0.01\n",
      "Epoch [9340/10000], Loss: 65.83512115478516, Learning Rate: 0.01\n",
      "Epoch [9341/10000], Loss: 65.9049301147461, Learning Rate: 0.01\n",
      "Epoch [9342/10000], Loss: 65.95281219482422, Learning Rate: 0.01\n",
      "Epoch [9343/10000], Loss: 65.94071960449219, Learning Rate: 0.01\n",
      "Epoch [9344/10000], Loss: 65.88197326660156, Learning Rate: 0.01\n",
      "Epoch [9345/10000], Loss: 65.82150268554688, Learning Rate: 0.01\n",
      "Epoch [9346/10000], Loss: 65.79737091064453, Learning Rate: 0.01\n",
      "Epoch [9347/10000], Loss: 65.81459045410156, Learning Rate: 0.01\n",
      "Epoch [9348/10000], Loss: 65.84820556640625, Learning Rate: 0.01\n",
      "Epoch [9349/10000], Loss: 65.8668441772461, Learning Rate: 0.01\n",
      "Epoch [9350/10000], Loss: 65.856201171875, Learning Rate: 0.01\n",
      "Epoch [9351/10000], Loss: 65.8255386352539, Learning Rate: 0.01\n",
      "Epoch [9352/10000], Loss: 65.79642486572266, Learning Rate: 0.01\n",
      "Epoch [9353/10000], Loss: 65.78534698486328, Learning Rate: 0.01\n",
      "Epoch [9354/10000], Loss: 65.79350280761719, Learning Rate: 0.01\n",
      "Epoch [9355/10000], Loss: 65.8086929321289, Learning Rate: 0.01\n",
      "Epoch [9356/10000], Loss: 65.81665802001953, Learning Rate: 0.01\n",
      "Epoch [9357/10000], Loss: 65.81084442138672, Learning Rate: 0.01\n",
      "Epoch [9358/10000], Loss: 65.79549407958984, Learning Rate: 0.01\n",
      "Epoch [9359/10000], Loss: 65.7803955078125, Learning Rate: 0.01\n",
      "Epoch [9360/10000], Loss: 65.77336120605469, Learning Rate: 0.01\n",
      "Epoch [9361/10000], Loss: 65.77539825439453, Learning Rate: 0.01\n",
      "Epoch [9362/10000], Loss: 65.78160095214844, Learning Rate: 0.01\n",
      "Epoch [9363/10000], Loss: 65.7852783203125, Learning Rate: 0.01\n",
      "Epoch [9364/10000], Loss: 65.7828598022461, Learning Rate: 0.01\n",
      "Epoch [9365/10000], Loss: 65.77550506591797, Learning Rate: 0.01\n",
      "Epoch [9366/10000], Loss: 65.76702117919922, Learning Rate: 0.01\n",
      "Epoch [9367/10000], Loss: 65.76151275634766, Learning Rate: 0.01\n",
      "Epoch [9368/10000], Loss: 65.76033782958984, Learning Rate: 0.01\n",
      "Epoch [9369/10000], Loss: 65.76197814941406, Learning Rate: 0.01\n",
      "Epoch [9370/10000], Loss: 65.76348876953125, Learning Rate: 0.01\n",
      "Epoch [9371/10000], Loss: 65.76265716552734, Learning Rate: 0.01\n",
      "Epoch [9372/10000], Loss: 65.7591781616211, Learning Rate: 0.01\n",
      "Epoch [9373/10000], Loss: 65.75425720214844, Learning Rate: 0.01\n",
      "Epoch [9374/10000], Loss: 65.75007629394531, Learning Rate: 0.01\n",
      "Epoch [9375/10000], Loss: 65.74758911132812, Learning Rate: 0.01\n",
      "Epoch [9376/10000], Loss: 65.74671173095703, Learning Rate: 0.01\n",
      "Epoch [9377/10000], Loss: 65.74652862548828, Learning Rate: 0.01\n",
      "Epoch [9378/10000], Loss: 65.7459487915039, Learning Rate: 0.01\n",
      "Epoch [9379/10000], Loss: 65.74420928955078, Learning Rate: 0.01\n",
      "Epoch [9380/10000], Loss: 65.74143981933594, Learning Rate: 0.01\n",
      "Epoch [9381/10000], Loss: 65.73827362060547, Learning Rate: 0.01\n",
      "Epoch [9382/10000], Loss: 65.73564147949219, Learning Rate: 0.01\n",
      "Epoch [9383/10000], Loss: 65.73362731933594, Learning Rate: 0.01\n",
      "Epoch [9384/10000], Loss: 65.732421875, Learning Rate: 0.01\n",
      "Epoch [9385/10000], Loss: 65.73138427734375, Learning Rate: 0.01\n",
      "Epoch [9386/10000], Loss: 65.73011779785156, Learning Rate: 0.01\n",
      "Epoch [9387/10000], Loss: 65.7283935546875, Learning Rate: 0.01\n",
      "Epoch [9388/10000], Loss: 65.72618865966797, Learning Rate: 0.01\n",
      "Epoch [9389/10000], Loss: 65.7238998413086, Learning Rate: 0.01\n",
      "Epoch [9390/10000], Loss: 65.72169494628906, Learning Rate: 0.01\n",
      "Epoch [9391/10000], Loss: 65.71981811523438, Learning Rate: 0.01\n",
      "Epoch [9392/10000], Loss: 65.71817779541016, Learning Rate: 0.01\n",
      "Epoch [9393/10000], Loss: 65.71669006347656, Learning Rate: 0.01\n",
      "Epoch [9394/10000], Loss: 65.71527862548828, Learning Rate: 0.01\n",
      "Epoch [9395/10000], Loss: 65.71356964111328, Learning Rate: 0.01\n",
      "Epoch [9396/10000], Loss: 65.7116928100586, Learning Rate: 0.01\n",
      "Epoch [9397/10000], Loss: 65.7096939086914, Learning Rate: 0.01\n",
      "Epoch [9398/10000], Loss: 65.707763671875, Learning Rate: 0.01\n",
      "Epoch [9399/10000], Loss: 65.70584106445312, Learning Rate: 0.01\n",
      "Epoch [9400/10000], Loss: 65.70409393310547, Learning Rate: 0.01\n",
      "Epoch [9401/10000], Loss: 65.70242309570312, Learning Rate: 0.01\n",
      "Epoch [9402/10000], Loss: 65.70084381103516, Learning Rate: 0.01\n",
      "Epoch [9403/10000], Loss: 65.69915771484375, Learning Rate: 0.01\n",
      "Epoch [9404/10000], Loss: 65.69741821289062, Learning Rate: 0.01\n",
      "Epoch [9405/10000], Loss: 65.69556427001953, Learning Rate: 0.01\n",
      "Epoch [9406/10000], Loss: 65.69371795654297, Learning Rate: 0.01\n",
      "Epoch [9407/10000], Loss: 65.69189453125, Learning Rate: 0.01\n",
      "Epoch [9408/10000], Loss: 65.69009399414062, Learning Rate: 0.01\n",
      "Epoch [9409/10000], Loss: 65.68833923339844, Learning Rate: 0.01\n",
      "Epoch [9410/10000], Loss: 65.68653106689453, Learning Rate: 0.01\n",
      "Epoch [9411/10000], Loss: 65.68484497070312, Learning Rate: 0.01\n",
      "Epoch [9412/10000], Loss: 65.68311309814453, Learning Rate: 0.01\n",
      "Epoch [9413/10000], Loss: 65.68135070800781, Learning Rate: 0.01\n",
      "Epoch [9414/10000], Loss: 65.67962646484375, Learning Rate: 0.01\n",
      "Epoch [9415/10000], Loss: 65.67779541015625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9416/10000], Loss: 65.6759262084961, Learning Rate: 0.01\n",
      "Epoch [9417/10000], Loss: 65.67422485351562, Learning Rate: 0.01\n",
      "Epoch [9418/10000], Loss: 65.6725082397461, Learning Rate: 0.01\n",
      "Epoch [9419/10000], Loss: 65.67059326171875, Learning Rate: 0.01\n",
      "Epoch [9420/10000], Loss: 65.66887664794922, Learning Rate: 0.01\n",
      "Epoch [9421/10000], Loss: 65.66706848144531, Learning Rate: 0.01\n",
      "Epoch [9422/10000], Loss: 65.66537475585938, Learning Rate: 0.01\n",
      "Epoch [9423/10000], Loss: 65.66360473632812, Learning Rate: 0.01\n",
      "Epoch [9424/10000], Loss: 65.66185760498047, Learning Rate: 0.01\n",
      "Epoch [9425/10000], Loss: 65.65999603271484, Learning Rate: 0.01\n",
      "Epoch [9426/10000], Loss: 65.65818786621094, Learning Rate: 0.01\n",
      "Epoch [9427/10000], Loss: 65.6563491821289, Learning Rate: 0.01\n",
      "Epoch [9428/10000], Loss: 65.65465545654297, Learning Rate: 0.01\n",
      "Epoch [9429/10000], Loss: 65.65282440185547, Learning Rate: 0.01\n",
      "Epoch [9430/10000], Loss: 65.65101623535156, Learning Rate: 0.01\n",
      "Epoch [9431/10000], Loss: 65.6492919921875, Learning Rate: 0.01\n",
      "Epoch [9432/10000], Loss: 65.6474838256836, Learning Rate: 0.01\n",
      "Epoch [9433/10000], Loss: 65.64569091796875, Learning Rate: 0.01\n",
      "Epoch [9434/10000], Loss: 65.6438217163086, Learning Rate: 0.01\n",
      "Epoch [9435/10000], Loss: 65.6421127319336, Learning Rate: 0.01\n",
      "Epoch [9436/10000], Loss: 65.64028930664062, Learning Rate: 0.01\n",
      "Epoch [9437/10000], Loss: 65.63851165771484, Learning Rate: 0.01\n",
      "Epoch [9438/10000], Loss: 65.63675689697266, Learning Rate: 0.01\n",
      "Epoch [9439/10000], Loss: 65.63483428955078, Learning Rate: 0.01\n",
      "Epoch [9440/10000], Loss: 65.63306427001953, Learning Rate: 0.01\n",
      "Epoch [9441/10000], Loss: 65.6312484741211, Learning Rate: 0.01\n",
      "Epoch [9442/10000], Loss: 65.6295394897461, Learning Rate: 0.01\n",
      "Epoch [9443/10000], Loss: 65.62764739990234, Learning Rate: 0.01\n",
      "Epoch [9444/10000], Loss: 65.62592315673828, Learning Rate: 0.01\n",
      "Epoch [9445/10000], Loss: 65.62406921386719, Learning Rate: 0.01\n",
      "Epoch [9446/10000], Loss: 65.62226104736328, Learning Rate: 0.01\n",
      "Epoch [9447/10000], Loss: 65.62044525146484, Learning Rate: 0.01\n",
      "Epoch [9448/10000], Loss: 65.61864471435547, Learning Rate: 0.01\n",
      "Epoch [9449/10000], Loss: 65.61680603027344, Learning Rate: 0.01\n",
      "Epoch [9450/10000], Loss: 65.6150131225586, Learning Rate: 0.01\n",
      "Epoch [9451/10000], Loss: 65.61308288574219, Learning Rate: 0.01\n",
      "Epoch [9452/10000], Loss: 65.61136627197266, Learning Rate: 0.01\n",
      "Epoch [9453/10000], Loss: 65.6095199584961, Learning Rate: 0.01\n",
      "Epoch [9454/10000], Loss: 65.60771179199219, Learning Rate: 0.01\n",
      "Epoch [9455/10000], Loss: 65.60590362548828, Learning Rate: 0.01\n",
      "Epoch [9456/10000], Loss: 65.6040267944336, Learning Rate: 0.01\n",
      "Epoch [9457/10000], Loss: 65.602294921875, Learning Rate: 0.01\n",
      "Epoch [9458/10000], Loss: 65.60041046142578, Learning Rate: 0.01\n",
      "Epoch [9459/10000], Loss: 65.59860229492188, Learning Rate: 0.01\n",
      "Epoch [9460/10000], Loss: 65.59674072265625, Learning Rate: 0.01\n",
      "Epoch [9461/10000], Loss: 65.59492492675781, Learning Rate: 0.01\n",
      "Epoch [9462/10000], Loss: 65.5931396484375, Learning Rate: 0.01\n",
      "Epoch [9463/10000], Loss: 65.59125518798828, Learning Rate: 0.01\n",
      "Epoch [9464/10000], Loss: 65.58940887451172, Learning Rate: 0.01\n",
      "Epoch [9465/10000], Loss: 65.5875473022461, Learning Rate: 0.01\n",
      "Epoch [9466/10000], Loss: 65.5857162475586, Learning Rate: 0.01\n",
      "Epoch [9467/10000], Loss: 65.58391571044922, Learning Rate: 0.01\n",
      "Epoch [9468/10000], Loss: 65.58204650878906, Learning Rate: 0.01\n",
      "Epoch [9469/10000], Loss: 65.58013916015625, Learning Rate: 0.01\n",
      "Epoch [9470/10000], Loss: 65.57839965820312, Learning Rate: 0.01\n",
      "Epoch [9471/10000], Loss: 65.57653045654297, Learning Rate: 0.01\n",
      "Epoch [9472/10000], Loss: 65.57465362548828, Learning Rate: 0.01\n",
      "Epoch [9473/10000], Loss: 65.57282257080078, Learning Rate: 0.01\n",
      "Epoch [9474/10000], Loss: 65.57096099853516, Learning Rate: 0.01\n",
      "Epoch [9475/10000], Loss: 65.569091796875, Learning Rate: 0.01\n",
      "Epoch [9476/10000], Loss: 65.5672607421875, Learning Rate: 0.01\n",
      "Epoch [9477/10000], Loss: 65.56541442871094, Learning Rate: 0.01\n",
      "Epoch [9478/10000], Loss: 65.56356048583984, Learning Rate: 0.01\n",
      "Epoch [9479/10000], Loss: 65.5616455078125, Learning Rate: 0.01\n",
      "Epoch [9480/10000], Loss: 65.55979919433594, Learning Rate: 0.01\n",
      "Epoch [9481/10000], Loss: 65.55799102783203, Learning Rate: 0.01\n",
      "Epoch [9482/10000], Loss: 65.55606842041016, Learning Rate: 0.01\n",
      "Epoch [9483/10000], Loss: 65.55424499511719, Learning Rate: 0.01\n",
      "Epoch [9484/10000], Loss: 65.55233764648438, Learning Rate: 0.01\n",
      "Epoch [9485/10000], Loss: 65.55046844482422, Learning Rate: 0.01\n",
      "Epoch [9486/10000], Loss: 65.54853820800781, Learning Rate: 0.01\n",
      "Epoch [9487/10000], Loss: 65.54673767089844, Learning Rate: 0.01\n",
      "Epoch [9488/10000], Loss: 65.54487609863281, Learning Rate: 0.01\n",
      "Epoch [9489/10000], Loss: 65.54304504394531, Learning Rate: 0.01\n",
      "Epoch [9490/10000], Loss: 65.5411376953125, Learning Rate: 0.01\n",
      "Epoch [9491/10000], Loss: 65.53925323486328, Learning Rate: 0.01\n",
      "Epoch [9492/10000], Loss: 65.5373764038086, Learning Rate: 0.01\n",
      "Epoch [9493/10000], Loss: 65.53545379638672, Learning Rate: 0.01\n",
      "Epoch [9494/10000], Loss: 65.53355407714844, Learning Rate: 0.01\n",
      "Epoch [9495/10000], Loss: 65.53173828125, Learning Rate: 0.01\n",
      "Epoch [9496/10000], Loss: 65.52981567382812, Learning Rate: 0.01\n",
      "Epoch [9497/10000], Loss: 65.52793884277344, Learning Rate: 0.01\n",
      "Epoch [9498/10000], Loss: 65.52604675292969, Learning Rate: 0.01\n",
      "Epoch [9499/10000], Loss: 65.52420806884766, Learning Rate: 0.01\n",
      "Epoch [9500/10000], Loss: 65.52227020263672, Learning Rate: 0.01\n",
      "Epoch [9501/10000], Loss: 65.52031707763672, Learning Rate: 0.01\n",
      "Epoch [9502/10000], Loss: 65.5184097290039, Learning Rate: 0.01\n",
      "Epoch [9503/10000], Loss: 65.51654052734375, Learning Rate: 0.01\n",
      "Epoch [9504/10000], Loss: 65.51470947265625, Learning Rate: 0.01\n",
      "Epoch [9505/10000], Loss: 65.51280212402344, Learning Rate: 0.01\n",
      "Epoch [9506/10000], Loss: 65.51085662841797, Learning Rate: 0.01\n",
      "Epoch [9507/10000], Loss: 65.50900268554688, Learning Rate: 0.01\n",
      "Epoch [9508/10000], Loss: 65.5071029663086, Learning Rate: 0.01\n",
      "Epoch [9509/10000], Loss: 65.50511932373047, Learning Rate: 0.01\n",
      "Epoch [9510/10000], Loss: 65.50330352783203, Learning Rate: 0.01\n",
      "Epoch [9511/10000], Loss: 65.50138854980469, Learning Rate: 0.01\n",
      "Epoch [9512/10000], Loss: 65.49949645996094, Learning Rate: 0.01\n",
      "Epoch [9513/10000], Loss: 65.49766540527344, Learning Rate: 0.01\n",
      "Epoch [9514/10000], Loss: 65.49580383300781, Learning Rate: 0.01\n",
      "Epoch [9515/10000], Loss: 65.49395751953125, Learning Rate: 0.01\n",
      "Epoch [9516/10000], Loss: 65.49214935302734, Learning Rate: 0.01\n",
      "Epoch [9517/10000], Loss: 65.49053955078125, Learning Rate: 0.01\n",
      "Epoch [9518/10000], Loss: 65.4888687133789, Learning Rate: 0.01\n",
      "Epoch [9519/10000], Loss: 65.48743438720703, Learning Rate: 0.01\n",
      "Epoch [9520/10000], Loss: 65.48626708984375, Learning Rate: 0.01\n",
      "Epoch [9521/10000], Loss: 65.4856185913086, Learning Rate: 0.01\n",
      "Epoch [9522/10000], Loss: 65.48542785644531, Learning Rate: 0.01\n",
      "Epoch [9523/10000], Loss: 65.48639678955078, Learning Rate: 0.01\n",
      "Epoch [9524/10000], Loss: 65.48884582519531, Learning Rate: 0.01\n",
      "Epoch [9525/10000], Loss: 65.49406433105469, Learning Rate: 0.01\n",
      "Epoch [9526/10000], Loss: 65.5033950805664, Learning Rate: 0.01\n",
      "Epoch [9527/10000], Loss: 65.51963806152344, Learning Rate: 0.01\n",
      "Epoch [9528/10000], Loss: 65.54686737060547, Learning Rate: 0.01\n",
      "Epoch [9529/10000], Loss: 65.59241485595703, Learning Rate: 0.01\n",
      "Epoch [9530/10000], Loss: 65.66712188720703, Learning Rate: 0.01\n",
      "Epoch [9531/10000], Loss: 65.7896957397461, Learning Rate: 0.01\n",
      "Epoch [9532/10000], Loss: 65.98821258544922, Learning Rate: 0.01\n",
      "Epoch [9533/10000], Loss: 66.30606842041016, Learning Rate: 0.01\n",
      "Epoch [9534/10000], Loss: 66.80046081542969, Learning Rate: 0.01\n",
      "Epoch [9535/10000], Loss: 67.53846740722656, Learning Rate: 0.01\n",
      "Epoch [9536/10000], Loss: 68.55072784423828, Learning Rate: 0.01\n",
      "Epoch [9537/10000], Loss: 69.75904083251953, Learning Rate: 0.01\n",
      "Epoch [9538/10000], Loss: 70.82225799560547, Learning Rate: 0.01\n",
      "Epoch [9539/10000], Loss: 71.15194702148438, Learning Rate: 0.01\n",
      "Epoch [9540/10000], Loss: 70.19715881347656, Learning Rate: 0.01\n",
      "Epoch [9541/10000], Loss: 68.18701171875, Learning Rate: 0.01\n",
      "Epoch [9542/10000], Loss: 66.23927307128906, Learning Rate: 0.01\n",
      "Epoch [9543/10000], Loss: 65.52349853515625, Learning Rate: 0.01\n",
      "Epoch [9544/10000], Loss: 66.19139099121094, Learning Rate: 0.01\n",
      "Epoch [9545/10000], Loss: 67.36368560791016, Learning Rate: 0.01\n",
      "Epoch [9546/10000], Loss: 67.91032409667969, Learning Rate: 0.01\n",
      "Epoch [9547/10000], Loss: 67.35136413574219, Learning Rate: 0.01\n",
      "Epoch [9548/10000], Loss: 66.21935272216797, Learning Rate: 0.01\n",
      "Epoch [9549/10000], Loss: 65.53060150146484, Learning Rate: 0.01\n",
      "Epoch [9550/10000], Loss: 65.74443817138672, Learning Rate: 0.01\n",
      "Epoch [9551/10000], Loss: 66.40894317626953, Learning Rate: 0.01\n",
      "Epoch [9552/10000], Loss: 66.7329330444336, Learning Rate: 0.01\n",
      "Epoch [9553/10000], Loss: 66.3906478881836, Learning Rate: 0.01\n",
      "Epoch [9554/10000], Loss: 65.76213836669922, Learning Rate: 0.01\n",
      "Epoch [9555/10000], Loss: 65.460693359375, Learning Rate: 0.01\n",
      "Epoch [9556/10000], Loss: 65.66657257080078, Learning Rate: 0.01\n",
      "Epoch [9557/10000], Loss: 66.02364349365234, Learning Rate: 0.01\n",
      "Epoch [9558/10000], Loss: 66.0927505493164, Learning Rate: 0.01\n",
      "Epoch [9559/10000], Loss: 65.81046295166016, Learning Rate: 0.01\n",
      "Epoch [9560/10000], Loss: 65.48863220214844, Learning Rate: 0.01\n",
      "Epoch [9561/10000], Loss: 65.42892456054688, Learning Rate: 0.01\n",
      "Epoch [9562/10000], Loss: 65.610107421875, Learning Rate: 0.01\n",
      "Epoch [9563/10000], Loss: 65.77088928222656, Learning Rate: 0.01\n",
      "Epoch [9564/10000], Loss: 65.72289276123047, Learning Rate: 0.01\n",
      "Epoch [9565/10000], Loss: 65.53141784667969, Learning Rate: 0.01\n",
      "Epoch [9566/10000], Loss: 65.40065002441406, Learning Rate: 0.01\n",
      "Epoch [9567/10000], Loss: 65.43597412109375, Learning Rate: 0.01\n",
      "Epoch [9568/10000], Loss: 65.55305480957031, Learning Rate: 0.01\n",
      "Epoch [9569/10000], Loss: 65.60108184814453, Learning Rate: 0.01\n",
      "Epoch [9570/10000], Loss: 65.5306396484375, Learning Rate: 0.01\n",
      "Epoch [9571/10000], Loss: 65.42445373535156, Learning Rate: 0.01\n",
      "Epoch [9572/10000], Loss: 65.3875961303711, Learning Rate: 0.01\n",
      "Epoch [9573/10000], Loss: 65.43486022949219, Learning Rate: 0.01\n",
      "Epoch [9574/10000], Loss: 65.49329376220703, Learning Rate: 0.01\n",
      "Epoch [9575/10000], Loss: 65.49301147460938, Learning Rate: 0.01\n",
      "Epoch [9576/10000], Loss: 65.43817138671875, Learning Rate: 0.01\n",
      "Epoch [9577/10000], Loss: 65.38761138916016, Learning Rate: 0.01\n",
      "Epoch [9578/10000], Loss: 65.38546752929688, Learning Rate: 0.01\n",
      "Epoch [9579/10000], Loss: 65.41890716552734, Learning Rate: 0.01\n",
      "Epoch [9580/10000], Loss: 65.44246673583984, Learning Rate: 0.01\n",
      "Epoch [9581/10000], Loss: 65.42973327636719, Learning Rate: 0.01\n",
      "Epoch [9582/10000], Loss: 65.39595794677734, Learning Rate: 0.01\n",
      "Epoch [9583/10000], Loss: 65.37477111816406, Learning Rate: 0.01\n",
      "Epoch [9584/10000], Loss: 65.38125610351562, Learning Rate: 0.01\n",
      "Epoch [9585/10000], Loss: 65.40089416503906, Learning Rate: 0.01\n",
      "Epoch [9586/10000], Loss: 65.40947723388672, Learning Rate: 0.01\n",
      "Epoch [9587/10000], Loss: 65.39896392822266, Learning Rate: 0.01\n",
      "Epoch [9588/10000], Loss: 65.3818359375, Learning Rate: 0.01\n",
      "Epoch [9589/10000], Loss: 65.37542724609375, Learning Rate: 0.01\n",
      "Epoch [9590/10000], Loss: 65.3841781616211, Learning Rate: 0.01\n",
      "Epoch [9591/10000], Loss: 65.39794921875, Learning Rate: 0.01\n",
      "Epoch [9592/10000], Loss: 65.40528869628906, Learning Rate: 0.01\n",
      "Epoch [9593/10000], Loss: 65.40430450439453, Learning Rate: 0.01\n",
      "Epoch [9594/10000], Loss: 65.40357208251953, Learning Rate: 0.01\n",
      "Epoch [9595/10000], Loss: 65.4119644165039, Learning Rate: 0.01\n",
      "Epoch [9596/10000], Loss: 65.43151092529297, Learning Rate: 0.01\n",
      "Epoch [9597/10000], Loss: 65.45733642578125, Learning Rate: 0.01\n",
      "Epoch [9598/10000], Loss: 65.48516082763672, Learning Rate: 0.01\n",
      "Epoch [9599/10000], Loss: 65.5162124633789, Learning Rate: 0.01\n",
      "Epoch [9600/10000], Loss: 65.55792999267578, Learning Rate: 0.01\n",
      "Epoch [9601/10000], Loss: 65.61725616455078, Learning Rate: 0.01\n",
      "Epoch [9602/10000], Loss: 65.69893646240234, Learning Rate: 0.01\n",
      "Epoch [9603/10000], Loss: 65.80361938476562, Learning Rate: 0.01\n",
      "Epoch [9604/10000], Loss: 65.93425750732422, Learning Rate: 0.01\n",
      "Epoch [9605/10000], Loss: 66.09384155273438, Learning Rate: 0.01\n",
      "Epoch [9606/10000], Loss: 66.28975677490234, Learning Rate: 0.01\n",
      "Epoch [9607/10000], Loss: 66.52045440673828, Learning Rate: 0.01\n",
      "Epoch [9608/10000], Loss: 66.78137969970703, Learning Rate: 0.01\n",
      "Epoch [9609/10000], Loss: 67.04511260986328, Learning Rate: 0.01\n",
      "Epoch [9610/10000], Loss: 67.28179931640625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9611/10000], Loss: 67.43407440185547, Learning Rate: 0.01\n",
      "Epoch [9612/10000], Loss: 67.46011352539062, Learning Rate: 0.01\n",
      "Epoch [9613/10000], Loss: 67.31057739257812, Learning Rate: 0.01\n",
      "Epoch [9614/10000], Loss: 66.99259185791016, Learning Rate: 0.01\n",
      "Epoch [9615/10000], Loss: 66.54293823242188, Learning Rate: 0.01\n",
      "Epoch [9616/10000], Loss: 66.0589828491211, Learning Rate: 0.01\n",
      "Epoch [9617/10000], Loss: 65.64305114746094, Learning Rate: 0.01\n",
      "Epoch [9618/10000], Loss: 65.3786392211914, Learning Rate: 0.01\n",
      "Epoch [9619/10000], Loss: 65.2950668334961, Learning Rate: 0.01\n",
      "Epoch [9620/10000], Loss: 65.36598205566406, Learning Rate: 0.01\n",
      "Epoch [9621/10000], Loss: 65.52758026123047, Learning Rate: 0.01\n",
      "Epoch [9622/10000], Loss: 65.70365905761719, Learning Rate: 0.01\n",
      "Epoch [9623/10000], Loss: 65.83024597167969, Learning Rate: 0.01\n",
      "Epoch [9624/10000], Loss: 65.8679428100586, Learning Rate: 0.01\n",
      "Epoch [9625/10000], Loss: 65.8121566772461, Learning Rate: 0.01\n",
      "Epoch [9626/10000], Loss: 65.68485260009766, Learning Rate: 0.01\n",
      "Epoch [9627/10000], Loss: 65.52943420410156, Learning Rate: 0.01\n",
      "Epoch [9628/10000], Loss: 65.39044189453125, Learning Rate: 0.01\n",
      "Epoch [9629/10000], Loss: 65.30117797851562, Learning Rate: 0.01\n",
      "Epoch [9630/10000], Loss: 65.27384185791016, Learning Rate: 0.01\n",
      "Epoch [9631/10000], Loss: 65.29932403564453, Learning Rate: 0.01\n",
      "Epoch [9632/10000], Loss: 65.35452270507812, Learning Rate: 0.01\n",
      "Epoch [9633/10000], Loss: 65.41236114501953, Learning Rate: 0.01\n",
      "Epoch [9634/10000], Loss: 65.45093536376953, Learning Rate: 0.01\n",
      "Epoch [9635/10000], Loss: 65.45845794677734, Learning Rate: 0.01\n",
      "Epoch [9636/10000], Loss: 65.4354019165039, Learning Rate: 0.01\n",
      "Epoch [9637/10000], Loss: 65.39073944091797, Learning Rate: 0.01\n",
      "Epoch [9638/10000], Loss: 65.33897399902344, Learning Rate: 0.01\n",
      "Epoch [9639/10000], Loss: 65.29344940185547, Learning Rate: 0.01\n",
      "Epoch [9640/10000], Loss: 65.26360321044922, Learning Rate: 0.01\n",
      "Epoch [9641/10000], Loss: 65.25275421142578, Learning Rate: 0.01\n",
      "Epoch [9642/10000], Loss: 65.25802612304688, Learning Rate: 0.01\n",
      "Epoch [9643/10000], Loss: 65.27326965332031, Learning Rate: 0.01\n",
      "Epoch [9644/10000], Loss: 65.29052734375, Learning Rate: 0.01\n",
      "Epoch [9645/10000], Loss: 65.30333709716797, Learning Rate: 0.01\n",
      "Epoch [9646/10000], Loss: 65.30752563476562, Learning Rate: 0.01\n",
      "Epoch [9647/10000], Loss: 65.30265808105469, Learning Rate: 0.01\n",
      "Epoch [9648/10000], Loss: 65.29022216796875, Learning Rate: 0.01\n",
      "Epoch [9649/10000], Loss: 65.27388763427734, Learning Rate: 0.01\n",
      "Epoch [9650/10000], Loss: 65.25721740722656, Learning Rate: 0.01\n",
      "Epoch [9651/10000], Loss: 65.24342346191406, Learning Rate: 0.01\n",
      "Epoch [9652/10000], Loss: 65.23416137695312, Learning Rate: 0.01\n",
      "Epoch [9653/10000], Loss: 65.22972106933594, Learning Rate: 0.01\n",
      "Epoch [9654/10000], Loss: 65.22964477539062, Learning Rate: 0.01\n",
      "Epoch [9655/10000], Loss: 65.23220825195312, Learning Rate: 0.01\n",
      "Epoch [9656/10000], Loss: 65.23556518554688, Learning Rate: 0.01\n",
      "Epoch [9657/10000], Loss: 65.23824310302734, Learning Rate: 0.01\n",
      "Epoch [9658/10000], Loss: 65.23918914794922, Learning Rate: 0.01\n",
      "Epoch [9659/10000], Loss: 65.23796844482422, Learning Rate: 0.01\n",
      "Epoch [9660/10000], Loss: 65.234619140625, Learning Rate: 0.01\n",
      "Epoch [9661/10000], Loss: 65.22975158691406, Learning Rate: 0.01\n",
      "Epoch [9662/10000], Loss: 65.22412872314453, Learning Rate: 0.01\n",
      "Epoch [9663/10000], Loss: 65.21831512451172, Learning Rate: 0.01\n",
      "Epoch [9664/10000], Loss: 65.213134765625, Learning Rate: 0.01\n",
      "Epoch [9665/10000], Loss: 65.20872497558594, Learning Rate: 0.01\n",
      "Epoch [9666/10000], Loss: 65.20538330078125, Learning Rate: 0.01\n",
      "Epoch [9667/10000], Loss: 65.20294952392578, Learning Rate: 0.01\n",
      "Epoch [9668/10000], Loss: 65.20135498046875, Learning Rate: 0.01\n",
      "Epoch [9669/10000], Loss: 65.2002944946289, Learning Rate: 0.01\n",
      "Epoch [9670/10000], Loss: 65.19952392578125, Learning Rate: 0.01\n",
      "Epoch [9671/10000], Loss: 65.19862365722656, Learning Rate: 0.01\n",
      "Epoch [9672/10000], Loss: 65.19766235351562, Learning Rate: 0.01\n",
      "Epoch [9673/10000], Loss: 65.19630432128906, Learning Rate: 0.01\n",
      "Epoch [9674/10000], Loss: 65.194580078125, Learning Rate: 0.01\n",
      "Epoch [9675/10000], Loss: 65.19247436523438, Learning Rate: 0.01\n",
      "Epoch [9676/10000], Loss: 65.19019317626953, Learning Rate: 0.01\n",
      "Epoch [9677/10000], Loss: 65.18770599365234, Learning Rate: 0.01\n",
      "Epoch [9678/10000], Loss: 65.18509674072266, Learning Rate: 0.01\n",
      "Epoch [9679/10000], Loss: 65.18242645263672, Learning Rate: 0.01\n",
      "Epoch [9680/10000], Loss: 65.17981719970703, Learning Rate: 0.01\n",
      "Epoch [9681/10000], Loss: 65.17723083496094, Learning Rate: 0.01\n",
      "Epoch [9682/10000], Loss: 65.17483520507812, Learning Rate: 0.01\n",
      "Epoch [9683/10000], Loss: 65.17253875732422, Learning Rate: 0.01\n",
      "Epoch [9684/10000], Loss: 65.17028045654297, Learning Rate: 0.01\n",
      "Epoch [9685/10000], Loss: 65.16817474365234, Learning Rate: 0.01\n",
      "Epoch [9686/10000], Loss: 65.16614532470703, Learning Rate: 0.01\n",
      "Epoch [9687/10000], Loss: 65.16425323486328, Learning Rate: 0.01\n",
      "Epoch [9688/10000], Loss: 65.16232299804688, Learning Rate: 0.01\n",
      "Epoch [9689/10000], Loss: 65.16036224365234, Learning Rate: 0.01\n",
      "Epoch [9690/10000], Loss: 65.1585693359375, Learning Rate: 0.01\n",
      "Epoch [9691/10000], Loss: 65.15665435791016, Learning Rate: 0.01\n",
      "Epoch [9692/10000], Loss: 65.15483093261719, Learning Rate: 0.01\n",
      "Epoch [9693/10000], Loss: 65.15300750732422, Learning Rate: 0.01\n",
      "Epoch [9694/10000], Loss: 65.15107727050781, Learning Rate: 0.01\n",
      "Epoch [9695/10000], Loss: 65.14921569824219, Learning Rate: 0.01\n",
      "Epoch [9696/10000], Loss: 65.1473617553711, Learning Rate: 0.01\n",
      "Epoch [9697/10000], Loss: 65.14549255371094, Learning Rate: 0.01\n",
      "Epoch [9698/10000], Loss: 65.14360046386719, Learning Rate: 0.01\n",
      "Epoch [9699/10000], Loss: 65.14167785644531, Learning Rate: 0.01\n",
      "Epoch [9700/10000], Loss: 65.13983917236328, Learning Rate: 0.01\n",
      "Epoch [9701/10000], Loss: 65.13800811767578, Learning Rate: 0.01\n",
      "Epoch [9702/10000], Loss: 65.13614654541016, Learning Rate: 0.01\n",
      "Epoch [9703/10000], Loss: 65.13426971435547, Learning Rate: 0.01\n",
      "Epoch [9704/10000], Loss: 65.13251495361328, Learning Rate: 0.01\n",
      "Epoch [9705/10000], Loss: 65.1307373046875, Learning Rate: 0.01\n",
      "Epoch [9706/10000], Loss: 65.12895965576172, Learning Rate: 0.01\n",
      "Epoch [9707/10000], Loss: 65.12727355957031, Learning Rate: 0.01\n",
      "Epoch [9708/10000], Loss: 65.12574768066406, Learning Rate: 0.01\n",
      "Epoch [9709/10000], Loss: 65.12425994873047, Learning Rate: 0.01\n",
      "Epoch [9710/10000], Loss: 65.12287902832031, Learning Rate: 0.01\n",
      "Epoch [9711/10000], Loss: 65.12171936035156, Learning Rate: 0.01\n",
      "Epoch [9712/10000], Loss: 65.12071228027344, Learning Rate: 0.01\n",
      "Epoch [9713/10000], Loss: 65.1201400756836, Learning Rate: 0.01\n",
      "Epoch [9714/10000], Loss: 65.11976623535156, Learning Rate: 0.01\n",
      "Epoch [9715/10000], Loss: 65.12002563476562, Learning Rate: 0.01\n",
      "Epoch [9716/10000], Loss: 65.12100219726562, Learning Rate: 0.01\n",
      "Epoch [9717/10000], Loss: 65.1228256225586, Learning Rate: 0.01\n",
      "Epoch [9718/10000], Loss: 65.12588500976562, Learning Rate: 0.01\n",
      "Epoch [9719/10000], Loss: 65.13079833984375, Learning Rate: 0.01\n",
      "Epoch [9720/10000], Loss: 65.13806915283203, Learning Rate: 0.01\n",
      "Epoch [9721/10000], Loss: 65.14859771728516, Learning Rate: 0.01\n",
      "Epoch [9722/10000], Loss: 65.16358184814453, Learning Rate: 0.01\n",
      "Epoch [9723/10000], Loss: 65.18470001220703, Learning Rate: 0.01\n",
      "Epoch [9724/10000], Loss: 65.2144546508789, Learning Rate: 0.01\n",
      "Epoch [9725/10000], Loss: 65.25603485107422, Learning Rate: 0.01\n",
      "Epoch [9726/10000], Loss: 65.31417083740234, Learning Rate: 0.01\n",
      "Epoch [9727/10000], Loss: 65.39446258544922, Learning Rate: 0.01\n",
      "Epoch [9728/10000], Loss: 65.50556945800781, Learning Rate: 0.01\n",
      "Epoch [9729/10000], Loss: 65.6569595336914, Learning Rate: 0.01\n",
      "Epoch [9730/10000], Loss: 65.86222076416016, Learning Rate: 0.01\n",
      "Epoch [9731/10000], Loss: 66.13177490234375, Learning Rate: 0.01\n",
      "Epoch [9732/10000], Loss: 66.4787826538086, Learning Rate: 0.01\n",
      "Epoch [9733/10000], Loss: 66.89854431152344, Learning Rate: 0.01\n",
      "Epoch [9734/10000], Loss: 67.37818145751953, Learning Rate: 0.01\n",
      "Epoch [9735/10000], Loss: 67.85186004638672, Learning Rate: 0.01\n",
      "Epoch [9736/10000], Loss: 68.23589324951172, Learning Rate: 0.01\n",
      "Epoch [9737/10000], Loss: 68.38021850585938, Learning Rate: 0.01\n",
      "Epoch [9738/10000], Loss: 68.18714904785156, Learning Rate: 0.01\n",
      "Epoch [9739/10000], Loss: 67.60440063476562, Learning Rate: 0.01\n",
      "Epoch [9740/10000], Loss: 66.76603698730469, Learning Rate: 0.01\n",
      "Epoch [9741/10000], Loss: 65.90338897705078, Learning Rate: 0.01\n",
      "Epoch [9742/10000], Loss: 65.28376770019531, Learning Rate: 0.01\n",
      "Epoch [9743/10000], Loss: 65.0535888671875, Learning Rate: 0.01\n",
      "Epoch [9744/10000], Loss: 65.19161224365234, Learning Rate: 0.01\n",
      "Epoch [9745/10000], Loss: 65.54080963134766, Learning Rate: 0.01\n",
      "Epoch [9746/10000], Loss: 65.89315795898438, Learning Rate: 0.01\n",
      "Epoch [9747/10000], Loss: 66.07913208007812, Learning Rate: 0.01\n",
      "Epoch [9748/10000], Loss: 66.01922607421875, Learning Rate: 0.01\n",
      "Epoch [9749/10000], Loss: 65.7572250366211, Learning Rate: 0.01\n",
      "Epoch [9750/10000], Loss: 65.4170913696289, Learning Rate: 0.01\n",
      "Epoch [9751/10000], Loss: 65.14595031738281, Learning Rate: 0.01\n",
      "Epoch [9752/10000], Loss: 65.03465270996094, Learning Rate: 0.01\n",
      "Epoch [9753/10000], Loss: 65.08690643310547, Learning Rate: 0.01\n",
      "Epoch [9754/10000], Loss: 65.23265838623047, Learning Rate: 0.01\n",
      "Epoch [9755/10000], Loss: 65.3745346069336, Learning Rate: 0.01\n",
      "Epoch [9756/10000], Loss: 65.43699645996094, Learning Rate: 0.01\n",
      "Epoch [9757/10000], Loss: 65.39408874511719, Learning Rate: 0.01\n",
      "Epoch [9758/10000], Loss: 65.27489471435547, Learning Rate: 0.01\n",
      "Epoch [9759/10000], Loss: 65.13811492919922, Learning Rate: 0.01\n",
      "Epoch [9760/10000], Loss: 65.04171752929688, Learning Rate: 0.01\n",
      "Epoch [9761/10000], Loss: 65.01433563232422, Learning Rate: 0.01\n",
      "Epoch [9762/10000], Loss: 65.04804229736328, Learning Rate: 0.01\n",
      "Epoch [9763/10000], Loss: 65.10964965820312, Learning Rate: 0.01\n",
      "Epoch [9764/10000], Loss: 65.16021728515625, Learning Rate: 0.01\n",
      "Epoch [9765/10000], Loss: 65.17396545410156, Learning Rate: 0.01\n",
      "Epoch [9766/10000], Loss: 65.14675903320312, Learning Rate: 0.01\n",
      "Epoch [9767/10000], Loss: 65.09383392333984, Learning Rate: 0.01\n",
      "Epoch [9768/10000], Loss: 65.03943634033203, Learning Rate: 0.01\n",
      "Epoch [9769/10000], Loss: 65.00377655029297, Learning Rate: 0.01\n",
      "Epoch [9770/10000], Loss: 64.99534606933594, Learning Rate: 0.01\n",
      "Epoch [9771/10000], Loss: 65.00940704345703, Learning Rate: 0.01\n",
      "Epoch [9772/10000], Loss: 65.03260040283203, Learning Rate: 0.01\n",
      "Epoch [9773/10000], Loss: 65.05077362060547, Learning Rate: 0.01\n",
      "Epoch [9774/10000], Loss: 65.0548095703125, Learning Rate: 0.01\n",
      "Epoch [9775/10000], Loss: 65.04330444335938, Learning Rate: 0.01\n",
      "Epoch [9776/10000], Loss: 65.02179718017578, Learning Rate: 0.01\n",
      "Epoch [9777/10000], Loss: 64.99896240234375, Learning Rate: 0.01\n",
      "Epoch [9778/10000], Loss: 64.98242950439453, Learning Rate: 0.01\n",
      "Epoch [9779/10000], Loss: 64.9756851196289, Learning Rate: 0.01\n",
      "Epoch [9780/10000], Loss: 64.97793579101562, Learning Rate: 0.01\n",
      "Epoch [9781/10000], Loss: 64.98490905761719, Learning Rate: 0.01\n",
      "Epoch [9782/10000], Loss: 64.9917221069336, Learning Rate: 0.01\n",
      "Epoch [9783/10000], Loss: 64.9942398071289, Learning Rate: 0.01\n",
      "Epoch [9784/10000], Loss: 64.9911117553711, Learning Rate: 0.01\n",
      "Epoch [9785/10000], Loss: 64.9832992553711, Learning Rate: 0.01\n",
      "Epoch [9786/10000], Loss: 64.9733657836914, Learning Rate: 0.01\n",
      "Epoch [9787/10000], Loss: 64.96429443359375, Learning Rate: 0.01\n",
      "Epoch [9788/10000], Loss: 64.95793151855469, Learning Rate: 0.01\n",
      "Epoch [9789/10000], Loss: 64.95504760742188, Learning Rate: 0.01\n",
      "Epoch [9790/10000], Loss: 64.95487213134766, Learning Rate: 0.01\n",
      "Epoch [9791/10000], Loss: 64.95613861083984, Learning Rate: 0.01\n",
      "Epoch [9792/10000], Loss: 64.95701599121094, Learning Rate: 0.01\n",
      "Epoch [9793/10000], Loss: 64.95658874511719, Learning Rate: 0.01\n",
      "Epoch [9794/10000], Loss: 64.95431518554688, Learning Rate: 0.01\n",
      "Epoch [9795/10000], Loss: 64.9504165649414, Learning Rate: 0.01\n",
      "Epoch [9796/10000], Loss: 64.94570922851562, Learning Rate: 0.01\n",
      "Epoch [9797/10000], Loss: 64.9410171508789, Learning Rate: 0.01\n",
      "Epoch [9798/10000], Loss: 64.93708038330078, Learning Rate: 0.01\n",
      "Epoch [9799/10000], Loss: 64.93413543701172, Learning Rate: 0.01\n",
      "Epoch [9800/10000], Loss: 64.93219757080078, Learning Rate: 0.01\n",
      "Epoch [9801/10000], Loss: 64.93087005615234, Learning Rate: 0.01\n",
      "Epoch [9802/10000], Loss: 64.92996215820312, Learning Rate: 0.01\n",
      "Epoch [9803/10000], Loss: 64.92868041992188, Learning Rate: 0.01\n",
      "Epoch [9804/10000], Loss: 64.92704010009766, Learning Rate: 0.01\n",
      "Epoch [9805/10000], Loss: 64.92488098144531, Learning Rate: 0.01\n",
      "Epoch [9806/10000], Loss: 64.92220306396484, Learning Rate: 0.01\n",
      "Epoch [9807/10000], Loss: 64.91938018798828, Learning Rate: 0.01\n",
      "Epoch [9808/10000], Loss: 64.91642761230469, Learning Rate: 0.01\n",
      "Epoch [9809/10000], Loss: 64.91366577148438, Learning Rate: 0.01\n",
      "Epoch [9810/10000], Loss: 64.91114807128906, Learning Rate: 0.01\n",
      "Epoch [9811/10000], Loss: 64.9088134765625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9812/10000], Loss: 64.90682983398438, Learning Rate: 0.01\n",
      "Epoch [9813/10000], Loss: 64.90491485595703, Learning Rate: 0.01\n",
      "Epoch [9814/10000], Loss: 64.90311431884766, Learning Rate: 0.01\n",
      "Epoch [9815/10000], Loss: 64.90125274658203, Learning Rate: 0.01\n",
      "Epoch [9816/10000], Loss: 64.89929962158203, Learning Rate: 0.01\n",
      "Epoch [9817/10000], Loss: 64.89720153808594, Learning Rate: 0.01\n",
      "Epoch [9818/10000], Loss: 64.89505004882812, Learning Rate: 0.01\n",
      "Epoch [9819/10000], Loss: 64.89271545410156, Learning Rate: 0.01\n",
      "Epoch [9820/10000], Loss: 64.89042663574219, Learning Rate: 0.01\n",
      "Epoch [9821/10000], Loss: 64.8879623413086, Learning Rate: 0.01\n",
      "Epoch [9822/10000], Loss: 64.8857192993164, Learning Rate: 0.01\n",
      "Epoch [9823/10000], Loss: 64.88343048095703, Learning Rate: 0.01\n",
      "Epoch [9824/10000], Loss: 64.88117980957031, Learning Rate: 0.01\n",
      "Epoch [9825/10000], Loss: 64.8790283203125, Learning Rate: 0.01\n",
      "Epoch [9826/10000], Loss: 64.87696838378906, Learning Rate: 0.01\n",
      "Epoch [9827/10000], Loss: 64.87492370605469, Learning Rate: 0.01\n",
      "Epoch [9828/10000], Loss: 64.87273406982422, Learning Rate: 0.01\n",
      "Epoch [9829/10000], Loss: 64.87062072753906, Learning Rate: 0.01\n",
      "Epoch [9830/10000], Loss: 64.86857604980469, Learning Rate: 0.01\n",
      "Epoch [9831/10000], Loss: 64.86649322509766, Learning Rate: 0.01\n",
      "Epoch [9832/10000], Loss: 64.86434936523438, Learning Rate: 0.01\n",
      "Epoch [9833/10000], Loss: 64.86209869384766, Learning Rate: 0.01\n",
      "Epoch [9834/10000], Loss: 64.85997009277344, Learning Rate: 0.01\n",
      "Epoch [9835/10000], Loss: 64.85774993896484, Learning Rate: 0.01\n",
      "Epoch [9836/10000], Loss: 64.85556030273438, Learning Rate: 0.01\n",
      "Epoch [9837/10000], Loss: 64.85331726074219, Learning Rate: 0.01\n",
      "Epoch [9838/10000], Loss: 64.85113525390625, Learning Rate: 0.01\n",
      "Epoch [9839/10000], Loss: 64.84894561767578, Learning Rate: 0.01\n",
      "Epoch [9840/10000], Loss: 64.84675598144531, Learning Rate: 0.01\n",
      "Epoch [9841/10000], Loss: 64.84455108642578, Learning Rate: 0.01\n",
      "Epoch [9842/10000], Loss: 64.84239959716797, Learning Rate: 0.01\n",
      "Epoch [9843/10000], Loss: 64.8402328491211, Learning Rate: 0.01\n",
      "Epoch [9844/10000], Loss: 64.83804321289062, Learning Rate: 0.01\n",
      "Epoch [9845/10000], Loss: 64.83584594726562, Learning Rate: 0.01\n",
      "Epoch [9846/10000], Loss: 64.83369445800781, Learning Rate: 0.01\n",
      "Epoch [9847/10000], Loss: 64.83146667480469, Learning Rate: 0.01\n",
      "Epoch [9848/10000], Loss: 64.82928466796875, Learning Rate: 0.01\n",
      "Epoch [9849/10000], Loss: 64.82711029052734, Learning Rate: 0.01\n",
      "Epoch [9850/10000], Loss: 64.82496643066406, Learning Rate: 0.01\n",
      "Epoch [9851/10000], Loss: 64.82283782958984, Learning Rate: 0.01\n",
      "Epoch [9852/10000], Loss: 64.82058715820312, Learning Rate: 0.01\n",
      "Epoch [9853/10000], Loss: 64.81839752197266, Learning Rate: 0.01\n",
      "Epoch [9854/10000], Loss: 64.81623840332031, Learning Rate: 0.01\n",
      "Epoch [9855/10000], Loss: 64.81403350830078, Learning Rate: 0.01\n",
      "Epoch [9856/10000], Loss: 64.81181335449219, Learning Rate: 0.01\n",
      "Epoch [9857/10000], Loss: 64.8096923828125, Learning Rate: 0.01\n",
      "Epoch [9858/10000], Loss: 64.80744934082031, Learning Rate: 0.01\n",
      "Epoch [9859/10000], Loss: 64.80525207519531, Learning Rate: 0.01\n",
      "Epoch [9860/10000], Loss: 64.80302429199219, Learning Rate: 0.01\n",
      "Epoch [9861/10000], Loss: 64.80079650878906, Learning Rate: 0.01\n",
      "Epoch [9862/10000], Loss: 64.79862976074219, Learning Rate: 0.01\n",
      "Epoch [9863/10000], Loss: 64.7964096069336, Learning Rate: 0.01\n",
      "Epoch [9864/10000], Loss: 64.79423522949219, Learning Rate: 0.01\n",
      "Epoch [9865/10000], Loss: 64.79199981689453, Learning Rate: 0.01\n",
      "Epoch [9866/10000], Loss: 64.7898178100586, Learning Rate: 0.01\n",
      "Epoch [9867/10000], Loss: 64.78762817382812, Learning Rate: 0.01\n",
      "Epoch [9868/10000], Loss: 64.78546142578125, Learning Rate: 0.01\n",
      "Epoch [9869/10000], Loss: 64.7832260131836, Learning Rate: 0.01\n",
      "Epoch [9870/10000], Loss: 64.78111267089844, Learning Rate: 0.01\n",
      "Epoch [9871/10000], Loss: 64.77889251708984, Learning Rate: 0.01\n",
      "Epoch [9872/10000], Loss: 64.77679443359375, Learning Rate: 0.01\n",
      "Epoch [9873/10000], Loss: 64.77469635009766, Learning Rate: 0.01\n",
      "Epoch [9874/10000], Loss: 64.77262878417969, Learning Rate: 0.01\n",
      "Epoch [9875/10000], Loss: 64.7706298828125, Learning Rate: 0.01\n",
      "Epoch [9876/10000], Loss: 64.76868438720703, Learning Rate: 0.01\n",
      "Epoch [9877/10000], Loss: 64.76676940917969, Learning Rate: 0.01\n",
      "Epoch [9878/10000], Loss: 64.7651138305664, Learning Rate: 0.01\n",
      "Epoch [9879/10000], Loss: 64.76359558105469, Learning Rate: 0.01\n",
      "Epoch [9880/10000], Loss: 64.76241302490234, Learning Rate: 0.01\n",
      "Epoch [9881/10000], Loss: 64.76145935058594, Learning Rate: 0.01\n",
      "Epoch [9882/10000], Loss: 64.76111602783203, Learning Rate: 0.01\n",
      "Epoch [9883/10000], Loss: 64.761474609375, Learning Rate: 0.01\n",
      "Epoch [9884/10000], Loss: 64.76287078857422, Learning Rate: 0.01\n",
      "Epoch [9885/10000], Loss: 64.76573181152344, Learning Rate: 0.01\n",
      "Epoch [9886/10000], Loss: 64.77085876464844, Learning Rate: 0.01\n",
      "Epoch [9887/10000], Loss: 64.77899932861328, Learning Rate: 0.01\n",
      "Epoch [9888/10000], Loss: 64.79165649414062, Learning Rate: 0.01\n",
      "Epoch [9889/10000], Loss: 64.81079864501953, Learning Rate: 0.01\n",
      "Epoch [9890/10000], Loss: 64.83966064453125, Learning Rate: 0.01\n",
      "Epoch [9891/10000], Loss: 64.8823471069336, Learning Rate: 0.01\n",
      "Epoch [9892/10000], Loss: 64.94551086425781, Learning Rate: 0.01\n",
      "Epoch [9893/10000], Loss: 65.03812408447266, Learning Rate: 0.01\n",
      "Epoch [9894/10000], Loss: 65.1736068725586, Learning Rate: 0.01\n",
      "Epoch [9895/10000], Loss: 65.36912536621094, Learning Rate: 0.01\n",
      "Epoch [9896/10000], Loss: 65.64887237548828, Learning Rate: 0.01\n",
      "Epoch [9897/10000], Loss: 66.03723907470703, Learning Rate: 0.01\n",
      "Epoch [9898/10000], Loss: 66.56227111816406, Learning Rate: 0.01\n",
      "Epoch [9899/10000], Loss: 67.22440338134766, Learning Rate: 0.01\n",
      "Epoch [9900/10000], Loss: 67.99768829345703, Learning Rate: 0.01\n",
      "Epoch [9901/10000], Loss: 68.74788665771484, Learning Rate: 0.01\n",
      "Epoch [9902/10000], Loss: 69.28124237060547, Learning Rate: 0.01\n",
      "Epoch [9903/10000], Loss: 69.2911148071289, Learning Rate: 0.01\n",
      "Epoch [9904/10000], Loss: 68.62959289550781, Learning Rate: 0.01\n",
      "Epoch [9905/10000], Loss: 67.37128448486328, Learning Rate: 0.01\n",
      "Epoch [9906/10000], Loss: 65.98167419433594, Learning Rate: 0.01\n",
      "Epoch [9907/10000], Loss: 65.00173950195312, Learning Rate: 0.01\n",
      "Epoch [9908/10000], Loss: 64.7537612915039, Learning Rate: 0.01\n",
      "Epoch [9909/10000], Loss: 65.15187072753906, Learning Rate: 0.01\n",
      "Epoch [9910/10000], Loss: 65.80753326416016, Learning Rate: 0.01\n",
      "Epoch [9911/10000], Loss: 66.2726821899414, Learning Rate: 0.01\n",
      "Epoch [9912/10000], Loss: 66.26032257080078, Learning Rate: 0.01\n",
      "Epoch [9913/10000], Loss: 65.7995376586914, Learning Rate: 0.01\n",
      "Epoch [9914/10000], Loss: 65.17694091796875, Learning Rate: 0.01\n",
      "Epoch [9915/10000], Loss: 64.75074005126953, Learning Rate: 0.01\n",
      "Epoch [9916/10000], Loss: 64.703125, Learning Rate: 0.01\n",
      "Epoch [9917/10000], Loss: 64.95645141601562, Learning Rate: 0.01\n",
      "Epoch [9918/10000], Loss: 65.26762390136719, Learning Rate: 0.01\n",
      "Epoch [9919/10000], Loss: 65.4035873413086, Learning Rate: 0.01\n",
      "Epoch [9920/10000], Loss: 65.28524017333984, Learning Rate: 0.01\n",
      "Epoch [9921/10000], Loss: 65.0066146850586, Learning Rate: 0.01\n",
      "Epoch [9922/10000], Loss: 64.75322723388672, Learning Rate: 0.01\n",
      "Epoch [9923/10000], Loss: 64.66202545166016, Learning Rate: 0.01\n",
      "Epoch [9924/10000], Loss: 64.74187469482422, Learning Rate: 0.01\n",
      "Epoch [9925/10000], Loss: 64.8919448852539, Learning Rate: 0.01\n",
      "Epoch [9926/10000], Loss: 64.987060546875, Learning Rate: 0.01\n",
      "Epoch [9927/10000], Loss: 64.96249389648438, Learning Rate: 0.01\n",
      "Epoch [9928/10000], Loss: 64.8427505493164, Learning Rate: 0.01\n",
      "Epoch [9929/10000], Loss: 64.71117401123047, Learning Rate: 0.01\n",
      "Epoch [9930/10000], Loss: 64.64361572265625, Learning Rate: 0.01\n",
      "Epoch [9931/10000], Loss: 64.66156768798828, Learning Rate: 0.01\n",
      "Epoch [9932/10000], Loss: 64.72872924804688, Learning Rate: 0.01\n",
      "Epoch [9933/10000], Loss: 64.7853012084961, Learning Rate: 0.01\n",
      "Epoch [9934/10000], Loss: 64.79051971435547, Learning Rate: 0.01\n",
      "Epoch [9935/10000], Loss: 64.7439956665039, Learning Rate: 0.01\n",
      "Epoch [9936/10000], Loss: 64.67875671386719, Learning Rate: 0.01\n",
      "Epoch [9937/10000], Loss: 64.63336181640625, Learning Rate: 0.01\n",
      "Epoch [9938/10000], Loss: 64.62698364257812, Learning Rate: 0.01\n",
      "Epoch [9939/10000], Loss: 64.65110778808594, Learning Rate: 0.01\n",
      "Epoch [9940/10000], Loss: 64.68059539794922, Learning Rate: 0.01\n",
      "Epoch [9941/10000], Loss: 64.69236755371094, Learning Rate: 0.01\n",
      "Epoch [9942/10000], Loss: 64.67852783203125, Learning Rate: 0.01\n",
      "Epoch [9943/10000], Loss: 64.64867401123047, Learning Rate: 0.01\n",
      "Epoch [9944/10000], Loss: 64.62037658691406, Learning Rate: 0.01\n",
      "Epoch [9945/10000], Loss: 64.60680389404297, Learning Rate: 0.01\n",
      "Epoch [9946/10000], Loss: 64.61019134521484, Learning Rate: 0.01\n",
      "Epoch [9947/10000], Loss: 64.62259674072266, Learning Rate: 0.01\n",
      "Epoch [9948/10000], Loss: 64.6322250366211, Learning Rate: 0.01\n",
      "Epoch [9949/10000], Loss: 64.63175964355469, Learning Rate: 0.01\n",
      "Epoch [9950/10000], Loss: 64.6209487915039, Learning Rate: 0.01\n",
      "Epoch [9951/10000], Loss: 64.60569763183594, Learning Rate: 0.01\n",
      "Epoch [9952/10000], Loss: 64.59337615966797, Learning Rate: 0.01\n",
      "Epoch [9953/10000], Loss: 64.58826446533203, Learning Rate: 0.01\n",
      "Epoch [9954/10000], Loss: 64.58987426757812, Learning Rate: 0.01\n",
      "Epoch [9955/10000], Loss: 64.59418487548828, Learning Rate: 0.01\n",
      "Epoch [9956/10000], Loss: 64.5965805053711, Learning Rate: 0.01\n",
      "Epoch [9957/10000], Loss: 64.5943603515625, Learning Rate: 0.01\n",
      "Epoch [9958/10000], Loss: 64.58799743652344, Learning Rate: 0.01\n",
      "Epoch [9959/10000], Loss: 64.580078125, Learning Rate: 0.01\n",
      "Epoch [9960/10000], Loss: 64.57334899902344, Learning Rate: 0.01\n",
      "Epoch [9961/10000], Loss: 64.56981658935547, Learning Rate: 0.01\n",
      "Epoch [9962/10000], Loss: 64.56906127929688, Learning Rate: 0.01\n",
      "Epoch [9963/10000], Loss: 64.56958770751953, Learning Rate: 0.01\n",
      "Epoch [9964/10000], Loss: 64.56937408447266, Learning Rate: 0.01\n",
      "Epoch [9965/10000], Loss: 64.56745910644531, Learning Rate: 0.01\n",
      "Epoch [9966/10000], Loss: 64.56375885009766, Learning Rate: 0.01\n",
      "Epoch [9967/10000], Loss: 64.55908203125, Learning Rate: 0.01\n",
      "Epoch [9968/10000], Loss: 64.55473327636719, Learning Rate: 0.01\n",
      "Epoch [9969/10000], Loss: 64.55147552490234, Learning Rate: 0.01\n",
      "Epoch [9970/10000], Loss: 64.54943084716797, Learning Rate: 0.01\n",
      "Epoch [9971/10000], Loss: 64.54824829101562, Learning Rate: 0.01\n",
      "Epoch [9972/10000], Loss: 64.54708862304688, Learning Rate: 0.01\n",
      "Epoch [9973/10000], Loss: 64.54554748535156, Learning Rate: 0.01\n",
      "Epoch [9974/10000], Loss: 64.54322814941406, Learning Rate: 0.01\n",
      "Epoch [9975/10000], Loss: 64.5403060913086, Learning Rate: 0.01\n",
      "Epoch [9976/10000], Loss: 64.5373764038086, Learning Rate: 0.01\n",
      "Epoch [9977/10000], Loss: 64.53474426269531, Learning Rate: 0.01\n",
      "Epoch [9978/10000], Loss: 64.53277587890625, Learning Rate: 0.01\n",
      "Epoch [9979/10000], Loss: 64.53147888183594, Learning Rate: 0.01\n",
      "Epoch [9980/10000], Loss: 64.53060913085938, Learning Rate: 0.01\n",
      "Epoch [9981/10000], Loss: 64.53013610839844, Learning Rate: 0.01\n",
      "Epoch [9982/10000], Loss: 64.53007507324219, Learning Rate: 0.01\n",
      "Epoch [9983/10000], Loss: 64.5303726196289, Learning Rate: 0.01\n",
      "Epoch [9984/10000], Loss: 64.53144073486328, Learning Rate: 0.01\n",
      "Epoch [9985/10000], Loss: 64.53398132324219, Learning Rate: 0.01\n",
      "Epoch [9986/10000], Loss: 64.53849792480469, Learning Rate: 0.01\n",
      "Epoch [9987/10000], Loss: 64.54603576660156, Learning Rate: 0.01\n",
      "Epoch [9988/10000], Loss: 64.55765533447266, Learning Rate: 0.01\n",
      "Epoch [9989/10000], Loss: 64.57504272460938, Learning Rate: 0.01\n",
      "Epoch [9990/10000], Loss: 64.60062408447266, Learning Rate: 0.01\n",
      "Epoch [9991/10000], Loss: 64.63753509521484, Learning Rate: 0.01\n",
      "Epoch [9992/10000], Loss: 64.6904296875, Learning Rate: 0.01\n",
      "Epoch [9993/10000], Loss: 64.7658462524414, Learning Rate: 0.01\n",
      "Epoch [9994/10000], Loss: 64.87218475341797, Learning Rate: 0.01\n",
      "Epoch [9995/10000], Loss: 65.02025604248047, Learning Rate: 0.01\n",
      "Epoch [9996/10000], Loss: 65.22122192382812, Learning Rate: 0.01\n",
      "Epoch [9997/10000], Loss: 65.48600769042969, Learning Rate: 0.01\n",
      "Epoch [9998/10000], Loss: 65.81493377685547, Learning Rate: 0.01\n",
      "Epoch [9999/10000], Loss: 66.19322967529297, Learning Rate: 0.01\n",
      "Epoch [10000/10000], Loss: 66.56764221191406, Learning Rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "from Inference.PointEstimate import AdamGradientDescent\n",
    "def _MAP(nbiter, std_init,logposterior, dim, device='cpu'):\n",
    "        optimizer = AdamGradientDescent(logposterior, nbiter, .01, .00000001, 50, .5, device, True)\n",
    "\n",
    "        theta0 = torch.empty((1, dim), device=device).normal_(0., std=std_init)\n",
    "        best_theta, best_score, score = optimizer.run(theta0)\n",
    "\n",
    "        return best_theta.detach().clone()\n",
    "\n",
    "theta=_MAP(10000,1., logposterior, param_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mylogpdf(x):\n",
    "    return logposterior(x)\n",
    "def potential(x):\n",
    "    theta=torch.Tensor(x).requires_grad_(True).float()\n",
    "    #print(x)\n",
    "    lp=mylogpdf(theta.unsqueeze(0))\n",
    "    lp.backward()\n",
    "    return -lp.detach().squeeze().numpy(), -theta.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196301444dd54bf9a7c887a8a2b26a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "samples = hamiltonian_monte_carlo(2500, potential,\n",
    "                                  initial_position=theta.squeeze().numpy(), \n",
    "                                  initial_step_size=0.002,\n",
    "                                  path_len=100,\n",
    "                                  tune=5000\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=setup.makePlot(torch.as_tensor(samples),device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52.99842366])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emcee\n",
    "emcee.autocorr.integrated_time(samples, c=5, tol=50, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=torch.load('mlruns/1/07280cf8fae54c91a363e8ad2cc60b22/artifacts/theta.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52.99842366])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emcee.autocorr.integrated_time(theta, c=5, tol=50, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "for i in range(theta.shape[1]):\n",
    "    plt.plot(theta[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a3d6e8610>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.as_tensor(theta).norm(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6000, 151])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
