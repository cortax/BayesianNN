{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/ColCarroll/minimc.git\n",
    "\"\"\"\n",
    "\n",
    "def leapfrog(q, p, dVdq, potential, path_len, step_size):\n",
    "    \"\"\"Leapfrog integrator for Hamiltonian Monte Carlo.\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : np.floatX\n",
    "        Initial position\n",
    "    p : np.floatX\n",
    "        Initial momentum\n",
    "    dVdq : np.floatX\n",
    "        Gradient of the potential at the initial coordinates\n",
    "    potential : callable\n",
    "        Value and gradient of the potential\n",
    "    path_len : float\n",
    "        How long to integrate for\n",
    "    step_size : float\n",
    "        How long each integration step should be\n",
    "    Returns\n",
    "    -------\n",
    "    q, p : np.floatX, np.floatX\n",
    "        New position and momentum\n",
    "    \"\"\"\n",
    "    q, p = np.copy(q), np.copy(p)\n",
    "\n",
    "    p -= step_size * dVdq / 2  # half step\n",
    "    for _ in np.arange(np.round(path_len / step_size) - 1):\n",
    "        q += step_size * p  # whole step\n",
    "        V, dVdq = potential(q)\n",
    "        p -= step_size * dVdq  # whole step\n",
    "    q += step_size * p  # whole step\n",
    "    V, dVdq = potential(q)\n",
    "    p -= step_size * dVdq / 2  # half step\n",
    "\n",
    "    # momentum flip at end\n",
    "    return q, -p, V, dVdq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/ColCarroll/minimc.git\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def hamiltonian_monte_carlo(\n",
    "    n_samples,\n",
    "    potential,\n",
    "    initial_position,\n",
    "    initial_potential=None,\n",
    "    initial_potential_grad=None,\n",
    "    tune=500,\n",
    "    path_len=1,\n",
    "    initial_step_size=0.1,\n",
    "    integrator=leapfrog,\n",
    "    max_energy_change=1000.0,\n",
    "):\n",
    "    \"\"\"Run Hamiltonian Monte Carlo sampling.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples to return\n",
    "    negative_log_prob : callable\n",
    "        The negative log probability to sample from\n",
    "    initial_position : np.array\n",
    "        A place to start sampling from.\n",
    "    tune: int\n",
    "        Number of iterations to run tuning\n",
    "    path_len : float\n",
    "        How long each integration path is. Smaller is faster and more correlated.\n",
    "    initial_step_size : float\n",
    "        How long each integration step is. This will be tuned automatically.\n",
    "    max_energy_change : float\n",
    "        The largest tolerable integration error. Transitions with energy changes\n",
    "        larger than this will be declared divergences.\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        Array of length `n_samples`.\n",
    "    \"\"\"\n",
    "    initial_position = np.array(initial_position)\n",
    "    if initial_potential is None or initial_potential_grad is None:\n",
    "        initial_potential, initial_potential_grad = potential(initial_position)\n",
    "\n",
    "    # collect all our samples in a list\n",
    "    samples = [initial_position]\n",
    "\n",
    "    # Keep a single object for momentum resamplingtqdm\n",
    "    momentum = st.norm(0, 1)\n",
    "\n",
    "    step_size = initial_step_size\n",
    "    step_size_tuning = DualAveragingStepSize(step_size)\n",
    "    \n",
    "    # If initial_position is a 10d vector and n_samples is 100, we want 100 x 10 momentum draws\n",
    "    # we can do this in one call to np.random.normal, and iterate over rows\n",
    "    size = (n_samples + tune,) + initial_position.shape[:1]\n",
    "    for idx, p0 in tqdm(enumerate(momentum.rvs(size=size)), total=size[0]):\n",
    "        # Integrate over our path to get a new position and momentum\n",
    "        q_new, p_new, final_V, final_dVdq = integrator(\n",
    "            samples[-1],\n",
    "            p0,\n",
    "            initial_potential_grad,\n",
    "            potential,\n",
    "            path_len=2\n",
    "            * np.random.rand()\n",
    "            * path_len,  # We jitter the path length a bit\n",
    "            step_size=step_size,\n",
    "        )\n",
    "\n",
    "        \n",
    "        start_log_p = np.sum(momentum.logpdf(p0)) - initial_potential\n",
    "        new_log_p = np.sum(momentum.logpdf(p_new)) - final_V\n",
    "        energy_change = new_log_p - start_log_p\n",
    "\n",
    "        # Check Metropolis acceptance criterion\n",
    "        p_accept = min(1, np.exp(energy_change))\n",
    "        if np.random.rand() < p_accept:\n",
    "            samples.append(q_new)\n",
    "            initial_potential = final_V\n",
    "            initial_potential_grad = final_dVdq\n",
    "        else:\n",
    "            samples.append(np.copy(samples[-1]))\n",
    "        \n",
    "        if idx < tune - 1:\n",
    "            step_size, _ = step_size_tuning.update(p_accept)\n",
    "        elif idx == tune - 1:\n",
    "            _, step_size = step_size_tuning.update(p_accept)\n",
    "        \n",
    "\n",
    "    return np.array(samples[1 + tune :])\n",
    "\n",
    "\n",
    "class DualAveragingStepSize:\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_step_size,\n",
    "        target_accept=0.8,\n",
    "        gamma=0.05,\n",
    "        t0=10.0,\n",
    "        kappa=0.75,\n",
    "    ):\n",
    "        \"\"\"Tune the step size to achieve a desired target acceptance.\n",
    "        Uses stochastic approximation of Robbins and Monro (1951), described in\n",
    "        Hoffman and Gelman (2013), section 3.2.1, and using those default values.\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_step_size: float > 0\n",
    "            Used to set a reasonable value for the stochastic step to drift towards\n",
    "        target_accept: float in (0, 1)\n",
    "            Will try to find a step size that accepts this percent of proposals\n",
    "        gamma: float\n",
    "            How quickly the stochastic step size reverts to a value mu\n",
    "        t0: float > 0\n",
    "            Larger values stabilize step size exploration early, while perhaps slowing\n",
    "            convergence\n",
    "        kappa: float in (0.5, 1]\n",
    "            The smaller kappa is, the faster we forget earlier step size iterates\n",
    "        \"\"\"\n",
    "        self.mu = np.log(10 * initial_step_size)\n",
    "        self.target_accept = target_accept\n",
    "        self.gamma = gamma\n",
    "        self.t = t0\n",
    "        self.kappa = kappa\n",
    "        self.error_sum = 0\n",
    "        self.log_averaged_step = 0\n",
    "\n",
    "    def update(self, p_accept):\n",
    "        \"\"\"Propose a new step size.\n",
    "        This method returns both a stochastic step size and a dual-averaged\n",
    "        step size. While tuning, the HMC algorithm should use the stochastic\n",
    "        step size and call `update` every loop. After tuning, HMC should use\n",
    "        the dual-averaged step size for sampling.\n",
    "        Parameters\n",
    "        ----------\n",
    "        p_accept: float\n",
    "            The probability of the previous HMC proposal being accepted\n",
    "        Returns\n",
    "        -------\n",
    "        float, float\n",
    "            A stochastic step size, and a dual-averaged step size\n",
    "        \"\"\"\n",
    "        self.error_sum += self.target_accept - p_accept\n",
    "        log_step = self.mu - self.error_sum / (np.sqrt(self.t) * self.gamma)\n",
    "        eta = self.t ** -self.kappa\n",
    "        self.log_averaged_step = (\n",
    "            eta * log_step + (1 - eta) * self.log_averaged_step\n",
    "        )\n",
    "        self.t += 1\n",
    "        return np.exp(log_step), np.exp(self.log_averaged_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device ='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools import logmvn01pdf\n",
    "def potential(x):\n",
    "    theta=torch.Tensor(x).requires_grad_(True).float()\n",
    "    #print(x)\n",
    "    lp=logmvn01pdf(theta.unsqueeze(0),'cpu')\n",
    "    lp.backward()\n",
    "    #print(x)\n",
    "    return -lp.detach().numpy(), -theta.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0829787c3b49939c78703d543b0d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8292352e90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "samples = hamiltonian_monte_carlo(5000, potential,\n",
    "                                  initial_position=np.zeros(1), \n",
    "#                                  initial_step_size=0.0001,\n",
    "#                                  tune=,\n",
    "#                                 path_len=10,\n",
    "                                 )\n",
    "sns.distplot(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f829213cd90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXTc5Xno8e8zo32XtdqSZcu7ZWODY2zABGIgxE5ITJLeFkhI0yR1uIVmOTdtuLdNuqQ9vZzbNulC6lCStjnEAdLExIDBhgTCYmws41VeZXnRYu3WZu2a5/4xIzII2fppGf1meT7n+Hjm93vfmed3rHn0+pn3976iqhhjjIleHrcDMMYYE1qW6I0xJspZojfGmChnid4YY6KcJXpjjIlycW4HMJrc3FydO3eu22EYY0zE2L9/f7Oq5o12LiwT/dy5cykvL3c7DGOMiRgicv5K56x0Y4wxUc4SvTHGRDlL9MYYE+Us0RtjTJSzRG+MMVHOEr0xxkQ5R4leRDaIyEkRqRSRh6/S7noRGRKR3xlvX2OMMaExZqIXES/wKLARKAPuFZGyK7R7BNg53r7GGGNCx8mIfg1QqapVqtoPPAlsGqXdHwM/Bxon0NcYY0yIOLkztgioDnpeA6wNbiAiRcAngduA68fTN+g1NgObAUpKShyEZUxobd174X3H7ltrP5sm8jgZ0csox0ZuS/U94JuqOjSBvv6Dqo+p6mpVXZ2XN+pyDcYYYybAyYi+Bpgd9LwYqBvRZjXwpIgA5AIfFZFBh32NMcaEkJNEvw9YKCKlQC1wD3BfcANVLR1+LCL/CTynqs+ISNxYfY0xxoTWmIleVQdF5CH8s2m8wI9UtUJEHgic3zLevlMTujHGGCccLVOsqjuAHSOOjZrgVfXzY/U1xhgzfezOWGOMiXKW6I0xJspZojdmFEM+paWrD5+OOhvYmIgSllsJGuOm3ZXN/PVzxzhR30lKgpcF+WksKUxnRXGW26EZMyGW6I0JaOzs5VvPHGVnRQPF2clsXF5IfXsvpxu7OFzTzuGadu6+roi0RPvYmMhiP7HG4C/VPPSTAxyubeNPPrKYL95cyi/eqQXAp8res608f7iOT33/TR7/3PWU5KS4HLExzlmN3hjg+69U8va5Vv7uU9fw4PoFJMV73z3nEeHGeTl8/qZSGjr62PToGxy4cMnFaI0ZH0v0JuY98sIJvvvyKVYUZ9LdN8TWvRdGXdBsQX4azzy4jozkeD7z+F52n2l2IVpjxs9KNyamdfUN8lR5NRnJ8dx9bRGB9ZquqDQ3lZ99+UY++8O9fP4/9vH9+1ZxR1nBqG1t9UsTLmxEb2LaP+46xaXL/fze6tnvKddcTX5GEk9tvpGlhel8+Yn9bDtQE+IojZkcS/QmZjV29PKTvedZVZLNnJzUcfXNTk3gJ394A2vmzuDrTx3i8derQhSlMZNnpRsTU4LLKTuOXGRgyMeHFk9s/4O0xDj+8wvX8/WnDvI3zx+nqbOPhzcuGbP8Y8x0s0RvYlJn7wB7z7awsjiLnLTECb9OYpyXf7l3FTmpFfzgtSp8qvzZx2xbZBNeLNGbmPRGZTODQ8r6xfnj6nelL1j/etMyPAL//vpZZs9I4XM3zp2iSI2ZPEv0JuZ09Q2yp6qFlbOzyE2f+Gg+mIjw7Y8vo7ath7/cXkFRVvKUvK4xU8G+jDUxZ/cZ/2j+Q4umdm9ir0f453uvY9msTB7aeoC6tp4pfX1jJspG9CamDPp87Dt3iSWF6eRnJE3Ja44s59y1Yib/+utKfnmwli/fOh+PfTlrXOZoRC8iG0TkpIhUisjDo5zfJCKHReSgiJSLyM1B586JyJHhc1MZvDHjdayug8t9g6wpzQnZe6QnxfORZYVUX+rhcE1byN7HGKfGTPQi4gUeBTYCZcC9IjJyWsGvgJWqei3wBeDxEefXq+q1qrp6CmI2ZsL2nWslKzmehQVpIX2fa0uyKMpKZmdFA/2DvpC+lzFjcTKiXwNUqmqVqvYDTwKbghuoapfquzs0pAK2W4MJO+eaL3Om6TKr584IeTnFI8JHr5lJe88Ar1c2hfS9jBmLk0RfBFQHPa8JHHsPEfmkiJwAnsc/qh+mwC4R2S8im6/0JiKyOVD2KW9qsg+GmXo/ffsCHoHVc7Kn5f1Kc1NZXpTJa6eaaO8ZmJb3NGY0ThL9aEOf943YVXWbqi4B7ga+E3Rqnaquwl/6eVBEbhntTVT1MVVdraqr8/KmdjaEMX2DQ/xsfw1LCjPISI6ftvfdsKwQn8IrJxun7T2NGclJoq8BZgc9LwbqrtRYVV8D5otIbuB5XeDvRmAb/lKQMdNqV0UDrZf7WVM6Y1rfd0ZqAiuLszh4oY3egaFpfW9jhjlJ9PuAhSJSKiIJwD3A9uAGIrJAAgt8iMgqIAFoEZFUEUkPHE8F7gSOTuUFGOPE0+XVFGUlsyA/tF/CjuaGeTPoH/LZZiXGNWPOo1fVQRF5CNgJeIEfqWqFiDwQOL8F+DTwOREZAHqA31NVFZECYFvgd0AcsFVVXwzRtRjzruC57Z29A7xxupkPLc5zZU57cXYKxdnJ7Dnbiqraomdm2jm6YUpVdwA7RhzbEvT4EeCRUfpVASsnGaMxk3Kkth0FVhZnuRbD2tIcfv5ODXuqWrlxfujm8BszGlsCwUS9Q9VtzMxMmrI7YSdiRXEmyfFenthz3rUYTOyyRG+iWktXH9WXelwdzQPEez18YE42OyvqaejodTUWE3ss0ZuodqimHfCPqN22tnQGgz7lyberx25szBSyRG+ilqpyqKaNuTkpZKUkuB0OOWmJrFuQw7YDNfz2RnJjQs8SvYla9R29NHX2sXK2u2WbYJ9YOYtzLd0cqW13OxQTQyzRm6h1qLoNj8DyWe6XbYZtWDaTeK/w7KEr3nNozJSzRG+ikqpypLadBflppCaGz7YLmSnx3Looj+cOX8Tns/KNmR6W6E1Uqmvr5VL3QFiN5od9fOUsLrb3su9cq9uhmBhhid5EpaN17XgEymZmuB3K+9yxtICkeA/PHrbyjZkeluhN1FFVKuraKc1NJSWMyjbDUhPjuGNpATuO1DMwZJuSmNCzRG+izqmGLpq7+lkWhmWbrXsvsHXvBbKSE2i93M/fPn/c7ZBMDLBEb6LOC0cvIsCyWeFXthm2qCCNpHiP7SlrpoUlehN1XjxaT0lOCulJ07fByHjFeT0sLczg+MVOBq18Y0LMEr2JKmebL3OivjMsZ9uMtGRmBj0DQ5Sft3XqTWhZojdR5cWj9UB4l22GLcpPw+sRXjrW4HYoJspZojdR5cWjF1lRnBkWa9uMJTHey/y8VF4+3mBr35iQskRvosbF9h4O1bTzkWWFbofi2NKZGZxv6eZ0Y5fboZgo5ijRi8gGETkpIpUi8vAo5zeJyGEROSgi5SJys9O+xkyV4RJIRCX6Qn+Jyco3JpTGTPQi4gUeBTYCZcC9IlI2otmvgJWqei3wBeDxcfQ1ZkrsqmhgXl6qKxuAT1RGcjwrizMt0ZuQcjKiXwNUqmqVqvYDTwKbghuoapf+tsiYCqjTvsZMhfbuAfZUtUTUaH7YHUsLOFjdRqPtPGVCxEmiLwKCt8SpCRx7DxH5pIicAJ7HP6p33DfQf3Og7FPe1NTkJHZj3vXrkw0M+pQ7ywrcDmXcPrzMH/OvTjS6HImJVk4SvYxy7H1TBFR1m6ouAe4GvjOevoH+j6nqalVdnZeX5yAsY35r59EGCjISXd8bdiIWF6RTnJ1s5RsTMk4SfQ0wO+h5MXDFZfdU9TVgvojkjrevMRPROzDEb0418eGyAjye0cYW4U1EuH1JPm+daaFvcMjtcEwUcpLo9wELRaRURBKAe4DtwQ1EZIGISODxKiABaHHS15jJev10Mz0DQxFZnx/2wYV59AwMsd/ukjUhMOYarqo6KCIPATsBL/AjVa0QkQcC57cAnwY+JyIDQA/we4EvZ0ftG6JrMTFqV0U96UlxrC3NcTuUCbthfg5xHuH1083cND/X7XBMlHG0WLeq7gB2jDi2JejxI8AjTvsaM1UGh3z86kQjty3JJyEuMu//27r3AgDF2Sn88mAts7NTALhvbYmbYZkoEn67MhgzDvvPX6L1cj8pCXHvJsxItbAgjZeONdDVN0haGG6YYiJXZA6BjAnYdayBOI+wKIJukrqShYFrqLTlEMwUs0RvIpaqsutYPfPz0kiM97odzqTNykomJcFLZWOn26GYKGOJ3kSsE/WdVLf2UBYBSxI74RFhfl4apxu7bDVLM6Us0ZuItauiARFYUpjudihTZmF+Gp29gzR09rkdioki9o2PiSjBX7g+te8Cs7PDe8vA8RpekK2ywco3ZurYiN5EpLbufuraeymbGR1lm2FZKQnkpSfa+vRmSlmiNxHp2MUOgKipzwdbkJ/GuZbL9A/apuFmaliiNxHp2MUO8tMTyU1LdDuUKTcvN5WBIeVwTZvboZgoYYneRJye/iHONV9maZSVbYaV5qQiwJ6qFrdDMVHCvow1EedkQyc+Jerq88NSEuMozEzimQN1zEh97/9YbFkEMxE2ojcR5/jFDtIS4yjKTnY7lJApzU3lfOtlBoesTm8mzxK9iSiDPh+nGjpZUpiORyJv7Xmnhuv0NZd63A7FRAFL9CainGvupm/QF7X1+WFzc/11+qrmy26HYqKAJXoTUY5f7CDe618qIJqlJPjr9GebbT69mTxL9CZiqCrH6ztYkJcWsWvPj0dpbioXWrutTm8mzdGnRUQ2iMhJEakUkYdHOf8ZETkc+LNbRFYGnTsnIkdE5KCIlE9l8Ca2nKjvpK17IOrLNsOG6/TVVqc3kzTm9EoR8QKPAh/Gv9n3PhHZrqrHgpqdBW5V1UsishF4DFgbdH69qjZPYdwmBr18rAGAxVG0iNnVDNfpzzZ3UZqb6nY4JoI5GdGvASpVtUpV+4EngU3BDVR1t6oO72q8Byie2jCNgZePNzA7OzmqFjG7muE6vX0haybLSaIvAqqDntcEjl3JF4EXgp4rsEtE9ovI5vGHaAw0dvRyqKadJTFSthlWmpvKhRar05vJcZLoR5usPOquCCKyHn+i/2bQ4XWqugrYCDwoIrdcoe9mESkXkfKmpiYHYZlY8uop/89ENK0978S83FQGfTaf3kyOk0RfA8wOel4M1I1sJCIrgMeBTar67iIdqloX+LsR2Ia/FPQ+qvqYqq5W1dV5eXnOr8DEhFdPNlKYkURhRpLboUyruTn+2ryVb8xkOEn0+4CFIlIqIgnAPcD24AYiUgL8ArhfVU8FHU8VkfThx8CdwNGpCt7EhoEhH6+famb9kjwkiu+GHU1KYhyFGUmcs0RvJmHMWTeqOigiDwE7AS/wI1WtEJEHAue3AN8GcoDvBz6Ig6q6GigAtgWOxQFbVfXFkFyJiVrl5y7R2TfIhxbn09LV73Y40640N5Xy860M+qxObybG0eqVqroD2DHi2Jagx18CvjRKvypg5cjjxozHqycbifcK6xbksv3g+6qGUa80N5W3qlqotTq9maDov73QRLxXTjaypnQGaYmxuar28Bz6s1a+MRNkid6EtZpL3Zxq6GL94ny3Q3FNamIc+emJlujNhFmiN2Ht1ZP+aZXrl8RuogeYl5fK+ZZuBmw+vZmA2Py/sAl7W/deAOCJPeeZkZrAnjMt7K1qdTkq95TmprGnqpWjte1cV5LtdjgmwtiI3oStgSEfZ5q6WFSQHnPTKkcartPvPRu7v+zMxFmiN2HLX6pQFhdE99rzTqQlxpGXnmgbhpsJsURvwtbpxk68HqE01xI9+JdDKD93yda9MeNmid6ErcrGLubMSImJTUacmJ+XRlffIIdq2twOxUQY+wSZsNTZO8DF9l4W5ttoftj8vDQ8Aq+dsq0dzPhYojdhqbLRv1fqgoLYWq3yapITvKwozuL107a6qxkfS/QmLFU2dpGS4GVmZmytVjmWWxbmcrC6jfaeAbdDMRHEEr0JO6pKZWMXC/LT8MT4tMqRPrgoD5/CW2ds9o1xzhK9CTsnGzrp7Bu0+vworp2dRVpinJVvzLhYojdh5/XAl40L8q0+P1K818MN83J4/bR9IWucs0Rvws5rp5vIS08kMzk2NgEfr1sW5XKhtZvzLbbImXHGEr0JK70DQ7x9ttXKNlfxwYX+rTZfs1G9ccgSvQkr+89fom/QxwJL9Fc0NyeF4uxkXj9ldXrjjKNELyIbROSkiFSKyMOjnP+MiBwO/NktIiud9jUm2JuVzcR5hNLAptjm/USEDy7M460zLbZssXFkzEQvIl7gUWAjUAbcKyJlI5qdBW5V1RXAd4DHxtHXmHftPtPCytlZJMZ73Q4lrH1ocR6dfYMxvXSzcc7JiH4NUKmqVaraDzwJbApuoKq7VfVS4OkeoNhpX2OGdfQOcLimjXXzc9wOJezdsjCP5HgvLxy96HYoJgI4SfRFQHXQ85rAsSv5IvDCePuKyGYRKReR8qYmqz3GorerWvEp3Dg/1+1Qwl5ygpf1S/LYWdHAkE/dDseEOSeJfrRbE0f9yRKR9fgT/TfH21dVH1PV1aq6Oi8vz0FYJtq8eaaZxDgPq+ZkuR1KRNi4fCbNXX3sP39p7MYmpjlJ9DXA7KDnxUDdyEYisgJ4HNikqi3j6WsM+G/rv37uDBLjrD7vxPol+STEedhxxMo35uqcJPp9wEIRKRWRBOAeYHtwAxEpAX4B3K+qp8bT1xiA5q4+TtR3ctMCq887lZYYxy0L89hZUY/PyjfmKsZM9Ko6CDwE7ASOA0+raoWIPCAiDwSafRvIAb4vIgdFpPxqfUNwHSbCDS/SdZPV58dl4/JCLrb32mYk5qrinDRS1R3AjhHHtgQ9/hLwJad9jRlp95lm0pPiuKYo0+1QIsodSwuI8wgvHq3nupJst8MxYcrujDVhYfeZFm6Yl4PXY8sSj0dmSjw3LcjlhaP1qFr5xozOEr1x3aOvVHK+pZvEOA9b915g694LbocUUT66vJALrd0cqW13OxQTpizRG9dVNfm3DZyXZ+vbTMTG5TNJiPPwi3dq3Q7FhClHNXpjQqmq6TKpiXEUpCe6HUpEykyJZ1FBOk+XVzMvL5U4j3/8dt/aEpcjM+HCRvTGVarKmaYu5uWmIrZt4IStKsmiu3+IU/WdbodiwpAleuOqcy3ddPQOMi/PVqucjIX56aQlxvHOBZtmad7PEr1x1e4z/s0z5udafX4yvB7h2tlZnKzv5HLfoNvhmDBjNXrjqrfOtJCRFEdOWoLboUSEq81Iuq4kizcqmzlU02Y3npn3sBG9cY2qsqeqhXl5aVafnwIzM5OZmZnEASvfmBEs0RvXnG7sormrn/lWn58yq0qyqW3roaGj1+1QTBixRG9cM7y+zTyrz0+ZlbOz8Ag2qjfvYYneuGb3mWZmz0gmO9Xq81MlLTGORQXpHKy+ZBuSmHdZojeu8PmUPVWt3DjPliWeateVZNPRO/ju/5iMsURvXHHsYgftPQPcaPvDTrklhekkxXv4xTs1bodiwoQleuOK4dHmjfNsGuBUi/d6uKYoixeO1tucegNYojcueaOymQX5aRRmJrkdSlRaVZJFz8AQLx6tdzsUEwYs0ZtptXXvBX781jl2n2kmNy3RliQOkZIZKZTMSOEXB6x8YxwmehHZICInRaRSRB4e5fwSEXlLRPpE5Bsjzp0TkSPBWwya2Fbd2sPAkLLAliUOGRHhU6uK2H2mhbq2HrfDMS4bM9GLiBd4FNgIlAH3ikjZiGatwFeAv7/Cy6xX1WtVdfVkgjXRobKxCwFbyCzEPnldEarw3OE6t0MxLnMyol8DVKpqlar2A08Cm4IbqGqjqu4DBkIQo4kyZ5q6KM5OJine63YoUW1OTiorizN59tBFt0MxLnOS6IuA6qDnNYFjTimwS0T2i8jmKzUSkc0iUi4i5U1NTeN4eRNJegeGqLnUzfx8K9tMh7tWzOJIbTvnmi+7HYpxkZNEP9pqU+O55W6dqq7CX/p5UERuGa2Rqj6mqqtVdXVeXt44Xt5EkrPNl/EpzLf6/LT42IqZgJVvYp2TRF8DzA56Xgw4/qlR1brA343ANvylIBOjzjR1Ee8VSmakuB1KTJiVlczqOdlWvolxThL9PmChiJSKSAJwD7DdyYuLSKqIpA8/Bu4Ejk40WBP5Khu7mJuTSrzXZvaG2ta9F9i69wIzM5M42dDJd1865XZIxiVjbjyiqoMi8hCwE/ACP1LVChF5IHB+i4gUAuVABuATka/hn6GTC2wLrDUeB2xV1RdDcykm3DV29NLY2ceqkmy3Q4kpy4syee7wRQ7XtLsdinGJox2mVHUHsGPEsS1Bj+vxl3RG6gBWTiZAEz3eHN420L6InVbpSfGU5qVypLYNVbVNXmKQ/f/ZTJtXTjSRmuBlpi17MO1WFmXR3NVPRV2H26EYF1iiN9NicMjHb041saggHY+NKKfdslkZeASeO2xfysYiS/RmWhyobqO9Z4DFheluhxKTUhLjmJ+Xxo4jF1G1DUlijSV6My1+faIRr0dYmG+J3i3XFGVyobWbo7VWvok1lujNtHjlRCPXz80mOcGWPXBL2cwM4jzC80esfBNrLNGbkKtt6+FEfSe3Lcl3O5SYlpIYx00Lcnn+SJ2Vb2KMJXoTcq+caASwRB8G7rpmJtWtPVa+iTGW6E3IvXKikdkzkm19mzBw57IC4jzCc0ds7ZtYYonehFTvwBBvnmnmtsX5dqNOGMhKSWDdglyeP2yzb2KJJXoTUm9VtdA74GO9lW3CxsdWzKTmUg9Ham1JhFhhid6E1K6KelISvNwwL8ftUEzAR8oKifeK3TwVQxytdWPMRPQNDvH84YvcWVZgu0mFieHN2OflpvHUvmpKZqTgEeG+tSUuR2ZCyUb0JmRePdlER+8gd183ng3JzHRYUZxJe88A1a3dbodipoElehMyvzxYS25aAjcvyHU7FDPC8M1Th2zp4phgid6EREfvAC8fb+SuFbOIs01Gwk5ivJclhekcqW1nyGezb6KdfQJNSLx4tJ7+QZ+VbcLYiuIsLvcNctY2Do96luhNSDxzoJa5OSmsLM50OxRzBYsL00mI83C4ps3tUEyIOUr0IrJBRE6KSKWIPDzK+SUi8paI9InIN8bT10Sf+vZe3jrTwry8NH76dvW7e5cOz/gw4SHe66FsZgYVdR30D/rcDseE0JiJXkS8wKPARvz7wN4rImUjmrUCXwH+fgJ9TZTZfqgWBa6dneV2KGYMK4oz6RkY4o3KJrdDMSHkZES/BqhU1SpV7QeeBDYFN1DVRlXdBwyMt6+JLgNDPv5r93nm5qSQm5bodjhmDAvy00iO9/LMAVv7Jpo5SfRFQHXQ85rAMScc9xWRzSJSLiLlTU02uohUzx2uo7ath1sW5rkdinEgzuPhmuJMXjrWwOW+QbfDMSHiJNGPthKV0/lYjvuq6mOqulpVV+flWZKIRKrKllerWFyQziLbMjBiXFucRc/AELuO1bsdigkRJ4m+Bpgd9LwYcPr/vMn0NRHmlZONnGzo5Mu3zrMNwCNISU4KxdnJbLPyTdRykuj3AQtFpFREEoB7gO0OX38yfU2E2fJqFUVZyXx85Sy3QzHj4BHh7muLeON0E02dfW6HY0JgzEXNVHVQRB4CdgJe4EeqWiEiDwTObxGRQqAcyAB8IvI1oExVO0brG6qLMe7Zf76Vt8+18u27yoi3O2EjTpxH8Cn85fYK1gWWrLCFzqKHo9UrVXUHsGPEsS1Bj+vxl2Uc9TXRRVX53sunyUqJ5541s8fuYMJOfkYSs7KSOFTT9m6iN9HDhl5m0nYda+D108185baFpCTYyteR6triLGou9Vj5JgpZojeT0jswxHeeO8aigjTuv3GO2+GYSVhRnIUAB6svuR2KmWI2/DKT8oPfVFFzqYcv3lzKz8pr3A7HTEJGcjwL8tM4UN3G7UsL3A7HTCEb0ZsJq7nUzfdfreSaokzm56W5HY6ZAqtKsmnrHuCcrWgZVWxEbxwbuSjZE3vO41Nl4/JClyIyU61sVgaJcR7euWDlm2hiI3ozIRV17Ry72MFtSwrISklwOxwzReK9HlYUZ3K0tsOWRIgilujNuPUODLH9UB0zM5Nsm8AotKokm/4hHzuOXHQ7FDNFLNGbcXuxop6u3kE+eV0RXo8tdRBtSmakkJOawH/vty/Xo4UlejMu55ov8/bZVm6an0Nxdorb4ZgQEBFWzclm79lWqlu73Q7HTAFL9MaxQZ+PbQdryUqJ544ym34Xza6bnYUINqqPEpbojWNvnm6mqbOPT6ycRWKc1+1wTAhlpSRw84Jcni6vZnDIthmMdJbojSPVrd38+mQjZTMzWFKY4XY4Zhp8Zu0cLrb38qsTjW6HYibJEr1x5K+erUAQ7lox0+1QzDS5Y2k+hRlJPLHnvNuhmEmyRG/GtKuinpePN3L70nybMx9D4rwe7ltbwuunm6lq6nI7HDMJlujNVXX3D/JXzx5jcUE6N823OfOx5p41s4nzCD8ZcVe0iSyW6M1V/fOvKqlt6+E7dy+3OfMxKD89iQ3LC/lZeTU9/UNuh2MmyBK9uaLTDZ08/noVv/OBYtaUznA7HOOS+2+YQ0fvIM8esj1lI5WjRC8iG0TkpIhUisjDo5wXEfnnwPnDIrIq6Nw5ETkiIgdFpHwqgzeho6r8+TNHSU2M439vXOJ2OMZFa0pnsLggnR++cRafT90Ox0zAmIleRLzAo8BGoAy4V0TKRjTbCCwM/NkM/NuI8+tV9VpVXT35kM102Haglr1nW/nmhiXkpCW6HY5xkYjwR+vnc7Khkx1Hbf2bSORkmeI1QKWqVgGIyJPAJuBYUJtNwI9VVYE9IpIlIjNV1X4qIlDr5X6+9cxRZmcn41N93/LEJjYE/7v7VMlPT+Svnj3GxuUz7fuaCOOkdFMEVAc9rwkcc9pGgV0isl9ENl/pTURks4iUi0h5U1OTg7BMqPzF9gp6B3zcfV0RHrEPtAGPCLcvLaCps89q9RHISaIf7ZM+slB3tTbrVHUV/vLOgyJyy2hvoqqPqepqVV2dl5fnICwTCi8evcizh+pYvySPmZnJbodjwsiyWRkUZiTxT4GrdhMAAApFSURBVL86bcsiRBgnib4GmB30vBgY+Sv9im1UdfjvRmAb/lKQCUOtl/v582eOsmxWBrcuync7HBNmPCLcsTSfs82X2Xag1u1wzDg4SfT7gIUiUioiCcA9wPYRbbYDnwvMvrkBaFfViyKSKiLpACKSCtwJHJ3C+M0U+ovtFbT3DPD3/2Ol1WDNqJbOzOCaokz+Ydcp2nsG3A7HODRmolfVQeAhYCdwHHhaVStE5AEReSDQbAdQBVQC/w78UeB4AfCGiBwC3gaeV9UXp/gazBR4el81zx6q449vW8jSmbZomRmdiPA3dy+nqauPv/iljdkihaPNwVV1B/5kHnxsS9BjBR4cpV8VsHKSMZoQq6hr51u/PMq6BTk8uH6B2+GYMLdydhZfuW0h3335FLcvLeDjK2e5HZIZg6NEb6JXR+8A9//wbRLjPNy6KJ+n9lWP3cnEvAfXz+fXJxv582eOcv3cGRRmJrkdkrkKWwIhhvl8yp/87BBt3f3cu6aEtET7vW/GtnXvBZ4ur+H2xfl09w9y37/voW/Q1sEJZ5boY9gjO0+ws6KBDctnMicn1e1wTITJTU9k07VFVDVf5qGtBxiwKZdhyxJ9jPqPN8/yg99Ucf8Nc1g3P8ftcEyEWlWSzcdXzuKlYw189ckDNr8+TFmij0HPH77IXz93jDvLCvjLTyxD7O5XMwk3zsvhW3eVseNIPV996iC9A1bGCTdWlI0hW/de4GR9B0/svUBJdgrrFuTal69mSnzx5lJ8PuVvdxyn9lIPP7j/AxRk2Be04cJG9DHkSG07T+y5QGFGEvffOId4r/3zm6nzh7fMY8tnP8Dphk7u+pc32H/+ktshmQAb0ceI/95fw5NvX6AkJ4Xfv3EuSfFet0MyUWjD8kLm5a3jD39czu9ueYuN1xRy47yc95QH71tb4mKEscmGdFHO51O+9/IpvvGzQ8zPT+MPbiq1JG9CalFBOtsfvJmFBWk8d/giP337gtXtXWYj+ijW3j3A158+yK9PNPKp64pYOTvLyjUmJEbbs+CzN8zhjdPN7DpWT90rldx7fQlF2bYiqhvsUx+l3rlwiU88+gavnWriO5uW8Q+/u9KSvJlWHhFuWZTHl26ex5BP2fKbM7xZ2Yx/xRQzneyTH2U6ewf49i+P8unv76ate4Av3lyK1+Php2/b7Brjjrm5qfzx+gUsKkjj+SMX+dJ/ldPY0et2WDFFwvG36+rVq7W83PYRH48hn7L9UC2PvHCShs5ebijN4c6yAhKtHm/ChKryVlULu441kOj18Kcbl/CZNSV4bEnsKSEi+6+0L7eN6COcz6fsOHKRj3zvNb7+1CFy0hL4+f+8iY+vnGVJ3oQVEeGm+bns/NotXFOcybeeOcqnt+zm9dNNVs4JMRvRR6j27gF+tr+aJ/ac51xLN3npidyxtIBlszJsn1cT1u5bW4Kqsu1ALX/3wgmaOvtYUpjOF24uZcPyQjKS4t0OMSJdbURviT6C9PQP8erJRl44Ws+uY/X0Dvj4wJxsFuSncU1RpiV4ExGC59H3DQ6x/WAdP3zjLCfqO4nzCB+Yk82HFuezpnQGy2Zl2HRgh66W6G16ZRgb8inH6jrYU9XCnqoWdp9poWdgiOyUeD61qpjPrC1h2azMUae2GROuRvt5feGrH6T8/CV+faKRV0408siLJwCI8whLZqazojiLlcWZrCjOYmF+GnE2g2xcHI3oRWQD8E+AF3hcVf/viPMSOP9RoBv4vKq+46TvaGJxRN/TP0RlYxcnGzqpqGvnSE07FXUd9ARuNJmXl0peWiLLizKZm5Nqe7qaqNbZO0B1aw/Vl7qpudRNbVsPvQP+lTGT471cU5TJiuJMrinOZOnMDOblpsZ88p/UiF5EvMCjwIeBGmCfiGxX1WNBzTYCCwN/1gL/Bqx12Hfa+HzKoE8Z8in9Qz4GAn8Gh5SBIR8+VYZ84FNl+PefiH8+sP9vAAH85xV/W58PlN/2Cf7dqfhfu2/QR/+gj47eQdq6+7l0eYC6Nv8P8oVW/w/ycL/keC/56YlcV5LF7OwUSnNTyUi2uqWJHelJ8ZTNiqdsln//Yp8qrV39/sTf1kNNazfvXLjEoM//oUmI8zA3J4WirGSKspPJT08iPSmO9KR4UhO8xHk9xHkFb+CzLAz/HUSCH/72vEjw4/d2GD7u9ci7f+K9HuI9HuLjAo+9HhK8HrweIc4jrswyclK6WQNUBvZ/RUSeBDYBwcl6E/DjwN6xe0QkS0RmAnMd9J0yH/jOS3T3D72bdBX/lC6f+ssg4SY3LZGUBC+5aYksLkynID2JgowkZqQm2IjdmCAeEXLTE8lNT+S6kmzA/5lu6uzjYnsP2akJnG2+TO2lHt650EZ7z4DLEV+d1yN4JPBLBN795ZObnsDrf3rblL+fk0RfBATfbVODf9Q+Vpsih30BEJHNwObA0y4ROekgtumWCzRP1Yudn6oXmj5Tev0RJpavHWL7+qf12uWbE+4650onnCT60YaWI4fHV2rjpK//oOpjwGMO4nGNiJRfqQYWC2L5+mP52iG2rz8art1Joq8BZgc9LwbqHLZJcNDXGGNMCDn5mnofsFBESkUkAbgH2D6izXbgc+J3A9Cuqhcd9jXGGBNCY47oVXVQRB4CduKfIvkjVa0QkQcC57cAO/BPrazEP73yD67WNyRXMj3CurQ0DWL5+mP52iG2rz/irz0s74w1xhgzdWL7DgNjjIkBluiNMSbKWaKfIBH5hoioiOS6Hct0EZH/JyInROSwiGwTkSy3Y5oOIrJBRE6KSKWIPOx2PNNFRGaLyCsiclxEKkTkq27H5AYR8YrIARF5zu1YJsoS/QSIyGz8yzrE2mpiLwHLVXUFcAr43y7HE3JBy3hsBMqAe0WkzN2ops0g8L9UdSlwA/BgDF17sK8Cx90OYjIs0U/Md4E/5Qo3f0UrVd2lqoOBp3vw3xcR7d5dAkRV+4HhZTyinqpeHF6cUFU78Se7Inejml4iUgx8DHjc7VgmwxL9OInIJ4BaVT3kdiwu+wLwgttBTIMrLe8RU0RkLnAdsNfdSKbd9/AP6nxuBzIZth79KETkZaBwlFN/Bvwf4M7pjWj6XO3aVfWXgTZ/hv+/9T+Zzthc4ngZj2glImnAz4GvqWqH2/FMFxG5C2hU1f0i8iG345kMS/SjUNU7RjsuItcApcAh/xL8FAPviMgaVa2fxhBD5krXPkxEfh+4C7hdY+MmDCdLgEQtEYnHn+R/oqq/cDueabYO+ISIfBRIAjJE5AlV/azLcY2b3TA1CSJyDlitqjGxql9gE5l/BG5V1Sa345kOIhKH/4vn24Fa/Mt63Bfhd3g7EthQ6L+AVlX9mtvxuCkwov+Gqt7ldiwTYTV6Mx7/CqQDL4nIQRHZ4nZAoRb48nl4GY/jwNOxkOQD1gH3A7cF/r0PBka3JsLYiN4YY6KcjeiNMSbKWaI3xpgoZ4neGGOinCV6Y4yJcpbojTEmylmiN8aYKGeJ3hhjotz/B4sPgU/umx26AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns                                 \n",
    "sns.distplot(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Experiments.foong import Setup\n",
    "layerwidth=50\n",
    "nblayers=1\n",
    "setup=Setup(device,layerwidth=layerwidth,nblayers=nblayers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target density #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_count=setup.param_count\n",
    "logposterior=setup.logposterior\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20000], Loss: 456905.125, Learning Rate: 0.01\n",
      "Epoch [2/20000], Loss: 407155.90625, Learning Rate: 0.01\n",
      "Epoch [3/20000], Loss: 360525.625, Learning Rate: 0.01\n",
      "Epoch [4/20000], Loss: 317047.90625, Learning Rate: 0.01\n",
      "Epoch [5/20000], Loss: 276721.84375, Learning Rate: 0.01\n",
      "Epoch [6/20000], Loss: 239513.921875, Learning Rate: 0.01\n",
      "Epoch [7/20000], Loss: 205361.203125, Learning Rate: 0.01\n",
      "Epoch [8/20000], Loss: 174184.78125, Learning Rate: 0.01\n",
      "Epoch [9/20000], Loss: 145909.28125, Learning Rate: 0.01\n",
      "Epoch [10/20000], Loss: 120472.125, Learning Rate: 0.01\n",
      "Epoch [11/20000], Loss: 97819.9140625, Learning Rate: 0.01\n",
      "Epoch [12/20000], Loss: 77898.6640625, Learning Rate: 0.01\n",
      "Epoch [13/20000], Loss: 60643.71484375, Learning Rate: 0.01\n",
      "Epoch [14/20000], Loss: 45970.40625, Learning Rate: 0.01\n",
      "Epoch [15/20000], Loss: 33767.640625, Learning Rate: 0.01\n",
      "Epoch [16/20000], Loss: 23894.46484375, Learning Rate: 0.01\n",
      "Epoch [17/20000], Loss: 16180.939453125, Learning Rate: 0.01\n",
      "Epoch [18/20000], Loss: 10432.404296875, Learning Rate: 0.01\n",
      "Epoch [19/20000], Loss: 6435.3642578125, Learning Rate: 0.01\n",
      "Epoch [20/20000], Loss: 3963.35302734375, Learning Rate: 0.01\n",
      "Epoch [21/20000], Loss: 2781.66162109375, Learning Rate: 0.01\n",
      "Epoch [22/20000], Loss: 2650.986328125, Learning Rate: 0.01\n",
      "Epoch [23/20000], Loss: 3330.82177734375, Learning Rate: 0.01\n",
      "Epoch [24/20000], Loss: 4583.599609375, Learning Rate: 0.01\n",
      "Epoch [25/20000], Loss: 6180.76513671875, Learning Rate: 0.01\n",
      "Epoch [26/20000], Loss: 7911.267578125, Learning Rate: 0.01\n",
      "Epoch [27/20000], Loss: 9591.3046875, Learning Rate: 0.01\n",
      "Epoch [28/20000], Loss: 11072.9169921875, Learning Rate: 0.01\n",
      "Epoch [29/20000], Loss: 12249.2255859375, Learning Rate: 0.01\n",
      "Epoch [30/20000], Loss: 13055.416015625, Learning Rate: 0.01\n",
      "Epoch [31/20000], Loss: 13465.6884765625, Learning Rate: 0.01\n",
      "Epoch [32/20000], Loss: 13487.3525390625, Learning Rate: 0.01\n",
      "Epoch [33/20000], Loss: 13153.4462890625, Learning Rate: 0.01\n",
      "Epoch [34/20000], Loss: 12515.1767578125, Learning Rate: 0.01\n",
      "Epoch [35/20000], Loss: 11635.0244140625, Learning Rate: 0.01\n",
      "Epoch [36/20000], Loss: 10580.9091796875, Learning Rate: 0.01\n",
      "Epoch [37/20000], Loss: 9421.4609375, Learning Rate: 0.01\n",
      "Epoch [38/20000], Loss: 8222.099609375, Learning Rate: 0.01\n",
      "Epoch [39/20000], Loss: 7041.8212890625, Learning Rate: 0.01\n",
      "Epoch [40/20000], Loss: 5930.5859375, Learning Rate: 0.01\n",
      "Epoch [41/20000], Loss: 4927.36865234375, Learning Rate: 0.01\n",
      "Epoch [42/20000], Loss: 4059.125732421875, Learning Rate: 0.01\n",
      "Epoch [43/20000], Loss: 3340.795654296875, Learning Rate: 0.01\n",
      "Epoch [44/20000], Loss: 2776.2822265625, Learning Rate: 0.01\n",
      "Epoch [45/20000], Loss: 2360.231689453125, Learning Rate: 0.01\n",
      "Epoch [46/20000], Loss: 2080.2080078125, Learning Rate: 0.01\n",
      "Epoch [47/20000], Loss: 1918.8748779296875, Learning Rate: 0.01\n",
      "Epoch [48/20000], Loss: 1855.92431640625, Learning Rate: 0.01\n",
      "Epoch [49/20000], Loss: 1869.6187744140625, Learning Rate: 0.01\n",
      "Epoch [50/20000], Loss: 1938.0281982421875, Learning Rate: 0.01\n",
      "Epoch [51/20000], Loss: 2040.08056640625, Learning Rate: 0.01\n",
      "Epoch [52/20000], Loss: 2156.52880859375, Learning Rate: 0.01\n",
      "Epoch [53/20000], Loss: 2270.8203125, Learning Rate: 0.01\n",
      "Epoch [54/20000], Loss: 2369.782958984375, Learning Rate: 0.01\n",
      "Epoch [55/20000], Loss: 2444.01318359375, Learning Rate: 0.01\n",
      "Epoch [56/20000], Loss: 2487.88232421875, Learning Rate: 0.01\n",
      "Epoch [57/20000], Loss: 2499.1904296875, Learning Rate: 0.01\n",
      "Epoch [58/20000], Loss: 2478.60400390625, Learning Rate: 0.01\n",
      "Epoch [59/20000], Loss: 2428.9990234375, Learning Rate: 0.01\n",
      "Epoch [60/20000], Loss: 2354.83642578125, Learning Rate: 0.01\n",
      "Epoch [61/20000], Loss: 2261.64892578125, Learning Rate: 0.01\n",
      "Epoch [62/20000], Loss: 2155.573974609375, Learning Rate: 0.01\n",
      "Epoch [63/20000], Loss: 2042.9305419921875, Learning Rate: 0.01\n",
      "Epoch [64/20000], Loss: 1929.7825927734375, Learning Rate: 0.01\n",
      "Epoch [65/20000], Loss: 1821.51513671875, Learning Rate: 0.01\n",
      "Epoch [66/20000], Loss: 1722.489501953125, Learning Rate: 0.01\n",
      "Epoch [67/20000], Loss: 1635.8114013671875, Learning Rate: 0.01\n",
      "Epoch [68/20000], Loss: 1563.267333984375, Learning Rate: 0.01\n",
      "Epoch [69/20000], Loss: 1505.408935546875, Learning Rate: 0.01\n",
      "Epoch [70/20000], Loss: 1461.7401123046875, Learning Rate: 0.01\n",
      "Epoch [71/20000], Loss: 1430.9410400390625, Learning Rate: 0.01\n",
      "Epoch [72/20000], Loss: 1411.087158203125, Learning Rate: 0.01\n",
      "Epoch [73/20000], Loss: 1399.84716796875, Learning Rate: 0.01\n",
      "Epoch [74/20000], Loss: 1394.6759033203125, Learning Rate: 0.01\n",
      "Epoch [75/20000], Loss: 1393.008056640625, Learning Rate: 0.01\n",
      "Epoch [76/20000], Loss: 1392.462158203125, Learning Rate: 0.01\n",
      "Epoch [77/20000], Loss: 1391.014892578125, Learning Rate: 0.01\n",
      "Epoch [78/20000], Loss: 1387.132568359375, Learning Rate: 0.01\n",
      "Epoch [79/20000], Loss: 1379.8245849609375, Learning Rate: 0.01\n",
      "Epoch [80/20000], Loss: 1368.627197265625, Learning Rate: 0.01\n",
      "Epoch [81/20000], Loss: 1353.53369140625, Learning Rate: 0.01\n",
      "Epoch [82/20000], Loss: 1334.899169921875, Learning Rate: 0.01\n",
      "Epoch [83/20000], Loss: 1313.35009765625, Learning Rate: 0.01\n",
      "Epoch [84/20000], Loss: 1289.6883544921875, Learning Rate: 0.01\n",
      "Epoch [85/20000], Loss: 1264.81103515625, Learning Rate: 0.01\n",
      "Epoch [86/20000], Loss: 1239.6236572265625, Learning Rate: 0.01\n",
      "Epoch [87/20000], Loss: 1214.9525146484375, Learning Rate: 0.01\n",
      "Epoch [88/20000], Loss: 1191.47705078125, Learning Rate: 0.01\n",
      "Epoch [89/20000], Loss: 1169.678955078125, Learning Rate: 0.01\n",
      "Epoch [90/20000], Loss: 1149.828857421875, Learning Rate: 0.01\n",
      "Epoch [91/20000], Loss: 1132.0009765625, Learning Rate: 0.01\n",
      "Epoch [92/20000], Loss: 1116.102783203125, Learning Rate: 0.01\n",
      "Epoch [93/20000], Loss: 1101.91845703125, Learning Rate: 0.01\n",
      "Epoch [94/20000], Loss: 1089.141357421875, Learning Rate: 0.01\n",
      "Epoch [95/20000], Loss: 1077.415283203125, Learning Rate: 0.01\n",
      "Epoch [96/20000], Loss: 1066.3656005859375, Learning Rate: 0.01\n",
      "Epoch [97/20000], Loss: 1055.63623046875, Learning Rate: 0.01\n",
      "Epoch [98/20000], Loss: 1044.9197998046875, Learning Rate: 0.01\n",
      "Epoch [99/20000], Loss: 1033.9842529296875, Learning Rate: 0.01\n",
      "Epoch [100/20000], Loss: 1022.6822509765625, Learning Rate: 0.01\n",
      "Epoch [101/20000], Loss: 1010.9496459960938, Learning Rate: 0.01\n",
      "Epoch [102/20000], Loss: 998.7938232421875, Learning Rate: 0.01\n",
      "Epoch [103/20000], Loss: 986.279052734375, Learning Rate: 0.01\n",
      "Epoch [104/20000], Loss: 973.508544921875, Learning Rate: 0.01\n",
      "Epoch [105/20000], Loss: 960.6100463867188, Learning Rate: 0.01\n",
      "Epoch [106/20000], Loss: 947.7228393554688, Learning Rate: 0.01\n",
      "Epoch [107/20000], Loss: 934.9774780273438, Learning Rate: 0.01\n",
      "Epoch [108/20000], Loss: 922.491455078125, Learning Rate: 0.01\n",
      "Epoch [109/20000], Loss: 910.3494262695312, Learning Rate: 0.01\n",
      "Epoch [110/20000], Loss: 898.6028442382812, Learning Rate: 0.01\n",
      "Epoch [111/20000], Loss: 887.2689208984375, Learning Rate: 0.01\n",
      "Epoch [112/20000], Loss: 876.336181640625, Learning Rate: 0.01\n",
      "Epoch [113/20000], Loss: 865.7718505859375, Learning Rate: 0.01\n",
      "Epoch [114/20000], Loss: 855.5274047851562, Learning Rate: 0.01\n",
      "Epoch [115/20000], Loss: 845.5465087890625, Learning Rate: 0.01\n",
      "Epoch [116/20000], Loss: 835.7703857421875, Learning Rate: 0.01\n",
      "Epoch [117/20000], Loss: 826.1429443359375, Learning Rate: 0.01\n",
      "Epoch [118/20000], Loss: 816.6214599609375, Learning Rate: 0.01\n",
      "Epoch [119/20000], Loss: 807.1724243164062, Learning Rate: 0.01\n",
      "Epoch [120/20000], Loss: 797.7796020507812, Learning Rate: 0.01\n",
      "Epoch [121/20000], Loss: 788.4401245117188, Learning Rate: 0.01\n",
      "Epoch [122/20000], Loss: 779.1617431640625, Learning Rate: 0.01\n",
      "Epoch [123/20000], Loss: 769.96044921875, Learning Rate: 0.01\n",
      "Epoch [124/20000], Loss: 760.855224609375, Learning Rate: 0.01\n",
      "Epoch [125/20000], Loss: 751.86962890625, Learning Rate: 0.01\n",
      "Epoch [126/20000], Loss: 743.0237426757812, Learning Rate: 0.01\n",
      "Epoch [127/20000], Loss: 734.33544921875, Learning Rate: 0.01\n",
      "Epoch [128/20000], Loss: 725.8172607421875, Learning Rate: 0.01\n",
      "Epoch [129/20000], Loss: 717.4747314453125, Learning Rate: 0.01\n",
      "Epoch [130/20000], Loss: 709.3079833984375, Learning Rate: 0.01\n",
      "Epoch [131/20000], Loss: 701.311279296875, Learning Rate: 0.01\n",
      "Epoch [132/20000], Loss: 693.475830078125, Learning Rate: 0.01\n",
      "Epoch [133/20000], Loss: 685.7899780273438, Learning Rate: 0.01\n",
      "Epoch [134/20000], Loss: 678.2418212890625, Learning Rate: 0.01\n",
      "Epoch [135/20000], Loss: 670.8194580078125, Learning Rate: 0.01\n",
      "Epoch [136/20000], Loss: 663.5131225585938, Learning Rate: 0.01\n",
      "Epoch [137/20000], Loss: 656.31396484375, Learning Rate: 0.01\n",
      "Epoch [138/20000], Loss: 649.217529296875, Learning Rate: 0.01\n",
      "Epoch [139/20000], Loss: 642.2206420898438, Learning Rate: 0.01\n",
      "Epoch [140/20000], Loss: 635.3232421875, Learning Rate: 0.01\n",
      "Epoch [141/20000], Loss: 628.52685546875, Learning Rate: 0.01\n",
      "Epoch [142/20000], Loss: 621.8348999023438, Learning Rate: 0.01\n",
      "Epoch [143/20000], Loss: 615.24853515625, Learning Rate: 0.01\n",
      "Epoch [144/20000], Loss: 608.7718505859375, Learning Rate: 0.01\n",
      "Epoch [145/20000], Loss: 602.4047241210938, Learning Rate: 0.01\n",
      "Epoch [146/20000], Loss: 596.1505126953125, Learning Rate: 0.01\n",
      "Epoch [147/20000], Loss: 590.006591796875, Learning Rate: 0.01\n",
      "Epoch [148/20000], Loss: 583.9725341796875, Learning Rate: 0.01\n",
      "Epoch [149/20000], Loss: 578.045654296875, Learning Rate: 0.01\n",
      "Epoch [150/20000], Loss: 572.222412109375, Learning Rate: 0.01\n",
      "Epoch [151/20000], Loss: 566.4993896484375, Learning Rate: 0.01\n",
      "Epoch [152/20000], Loss: 560.8739624023438, Learning Rate: 0.01\n",
      "Epoch [153/20000], Loss: 555.341552734375, Learning Rate: 0.01\n",
      "Epoch [154/20000], Loss: 549.8997192382812, Learning Rate: 0.01\n",
      "Epoch [155/20000], Loss: 544.5469970703125, Learning Rate: 0.01\n",
      "Epoch [156/20000], Loss: 539.2791748046875, Learning Rate: 0.01\n",
      "Epoch [157/20000], Loss: 534.0970458984375, Learning Rate: 0.01\n",
      "Epoch [158/20000], Loss: 528.9986572265625, Learning Rate: 0.01\n",
      "Epoch [159/20000], Loss: 523.9829711914062, Learning Rate: 0.01\n",
      "Epoch [160/20000], Loss: 519.0501708984375, Learning Rate: 0.01\n",
      "Epoch [161/20000], Loss: 514.1982421875, Learning Rate: 0.01\n",
      "Epoch [162/20000], Loss: 509.4273986816406, Learning Rate: 0.01\n",
      "Epoch [163/20000], Loss: 504.73602294921875, Learning Rate: 0.01\n",
      "Epoch [164/20000], Loss: 500.12310791015625, Learning Rate: 0.01\n",
      "Epoch [165/20000], Loss: 495.5867919921875, Learning Rate: 0.01\n",
      "Epoch [166/20000], Loss: 491.1257019042969, Learning Rate: 0.01\n",
      "Epoch [167/20000], Loss: 486.73779296875, Learning Rate: 0.01\n",
      "Epoch [168/20000], Loss: 482.4210510253906, Learning Rate: 0.01\n",
      "Epoch [169/20000], Loss: 478.1748046875, Learning Rate: 0.01\n",
      "Epoch [170/20000], Loss: 473.996337890625, Learning Rate: 0.01\n",
      "Epoch [171/20000], Loss: 469.88421630859375, Learning Rate: 0.01\n",
      "Epoch [172/20000], Loss: 465.8376770019531, Learning Rate: 0.01\n",
      "Epoch [173/20000], Loss: 461.85418701171875, Learning Rate: 0.01\n",
      "Epoch [174/20000], Loss: 457.93389892578125, Learning Rate: 0.01\n",
      "Epoch [175/20000], Loss: 454.0747985839844, Learning Rate: 0.01\n",
      "Epoch [176/20000], Loss: 450.27655029296875, Learning Rate: 0.01\n",
      "Epoch [177/20000], Loss: 446.5378723144531, Learning Rate: 0.01\n",
      "Epoch [178/20000], Loss: 442.85699462890625, Learning Rate: 0.01\n",
      "Epoch [179/20000], Loss: 439.2343444824219, Learning Rate: 0.01\n",
      "Epoch [180/20000], Loss: 435.66790771484375, Learning Rate: 0.01\n",
      "Epoch [181/20000], Loss: 432.15655517578125, Learning Rate: 0.01\n",
      "Epoch [182/20000], Loss: 428.699462890625, Learning Rate: 0.01\n",
      "Epoch [183/20000], Loss: 425.2954406738281, Learning Rate: 0.01\n",
      "Epoch [184/20000], Loss: 421.9436340332031, Learning Rate: 0.01\n",
      "Epoch [185/20000], Loss: 418.64239501953125, Learning Rate: 0.01\n",
      "Epoch [186/20000], Loss: 415.39068603515625, Learning Rate: 0.01\n",
      "Epoch [187/20000], Loss: 412.18829345703125, Learning Rate: 0.01\n",
      "Epoch [188/20000], Loss: 409.03326416015625, Learning Rate: 0.01\n",
      "Epoch [189/20000], Loss: 405.9251708984375, Learning Rate: 0.01\n",
      "Epoch [190/20000], Loss: 402.863037109375, Learning Rate: 0.01\n",
      "Epoch [191/20000], Loss: 399.8463134765625, Learning Rate: 0.01\n",
      "Epoch [192/20000], Loss: 396.8739318847656, Learning Rate: 0.01\n",
      "Epoch [193/20000], Loss: 393.9449462890625, Learning Rate: 0.01\n",
      "Epoch [194/20000], Loss: 391.0581970214844, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [195/20000], Loss: 388.2132568359375, Learning Rate: 0.01\n",
      "Epoch [196/20000], Loss: 385.4100341796875, Learning Rate: 0.01\n",
      "Epoch [197/20000], Loss: 382.6467590332031, Learning Rate: 0.01\n",
      "Epoch [198/20000], Loss: 379.92291259765625, Learning Rate: 0.01\n",
      "Epoch [199/20000], Loss: 377.23785400390625, Learning Rate: 0.01\n",
      "Epoch [200/20000], Loss: 374.5906982421875, Learning Rate: 0.01\n",
      "Epoch [201/20000], Loss: 371.9810791015625, Learning Rate: 0.01\n",
      "Epoch [202/20000], Loss: 369.407958984375, Learning Rate: 0.01\n",
      "Epoch [203/20000], Loss: 366.870849609375, Learning Rate: 0.01\n",
      "Epoch [204/20000], Loss: 364.3687744140625, Learning Rate: 0.01\n",
      "Epoch [205/20000], Loss: 361.90179443359375, Learning Rate: 0.01\n",
      "Epoch [206/20000], Loss: 359.46820068359375, Learning Rate: 0.01\n",
      "Epoch [207/20000], Loss: 357.068359375, Learning Rate: 0.01\n",
      "Epoch [208/20000], Loss: 354.7010498046875, Learning Rate: 0.01\n",
      "Epoch [209/20000], Loss: 352.3660888671875, Learning Rate: 0.01\n",
      "Epoch [210/20000], Loss: 350.06304931640625, Learning Rate: 0.01\n",
      "Epoch [211/20000], Loss: 347.79071044921875, Learning Rate: 0.01\n",
      "Epoch [212/20000], Loss: 345.5489807128906, Learning Rate: 0.01\n",
      "Epoch [213/20000], Loss: 343.3369140625, Learning Rate: 0.01\n",
      "Epoch [214/20000], Loss: 341.15478515625, Learning Rate: 0.01\n",
      "Epoch [215/20000], Loss: 339.0012512207031, Learning Rate: 0.01\n",
      "Epoch [216/20000], Loss: 336.876220703125, Learning Rate: 0.01\n",
      "Epoch [217/20000], Loss: 334.77960205078125, Learning Rate: 0.01\n",
      "Epoch [218/20000], Loss: 332.7101745605469, Learning Rate: 0.01\n",
      "Epoch [219/20000], Loss: 330.6673583984375, Learning Rate: 0.01\n",
      "Epoch [220/20000], Loss: 328.65179443359375, Learning Rate: 0.01\n",
      "Epoch [221/20000], Loss: 326.6622009277344, Learning Rate: 0.01\n",
      "Epoch [222/20000], Loss: 324.6983337402344, Learning Rate: 0.01\n",
      "Epoch [223/20000], Loss: 322.75933837890625, Learning Rate: 0.01\n",
      "Epoch [224/20000], Loss: 320.845458984375, Learning Rate: 0.01\n",
      "Epoch [225/20000], Loss: 318.95574951171875, Learning Rate: 0.01\n",
      "Epoch [226/20000], Loss: 317.09075927734375, Learning Rate: 0.01\n",
      "Epoch [227/20000], Loss: 315.24884033203125, Learning Rate: 0.01\n",
      "Epoch [228/20000], Loss: 313.4306335449219, Learning Rate: 0.01\n",
      "Epoch [229/20000], Loss: 311.6352844238281, Learning Rate: 0.01\n",
      "Epoch [230/20000], Loss: 309.86212158203125, Learning Rate: 0.01\n",
      "Epoch [231/20000], Loss: 308.11126708984375, Learning Rate: 0.01\n",
      "Epoch [232/20000], Loss: 306.38250732421875, Learning Rate: 0.01\n",
      "Epoch [233/20000], Loss: 304.6751403808594, Learning Rate: 0.01\n",
      "Epoch [234/20000], Loss: 302.9892578125, Learning Rate: 0.01\n",
      "Epoch [235/20000], Loss: 301.32415771484375, Learning Rate: 0.01\n",
      "Epoch [236/20000], Loss: 299.67926025390625, Learning Rate: 0.01\n",
      "Epoch [237/20000], Loss: 298.0549011230469, Learning Rate: 0.01\n",
      "Epoch [238/20000], Loss: 296.4503479003906, Learning Rate: 0.01\n",
      "Epoch [239/20000], Loss: 294.8654479980469, Learning Rate: 0.01\n",
      "Epoch [240/20000], Loss: 293.30010986328125, Learning Rate: 0.01\n",
      "Epoch [241/20000], Loss: 291.7536315917969, Learning Rate: 0.01\n",
      "Epoch [242/20000], Loss: 290.2259521484375, Learning Rate: 0.01\n",
      "Epoch [243/20000], Loss: 288.7170715332031, Learning Rate: 0.01\n",
      "Epoch [244/20000], Loss: 287.22650146484375, Learning Rate: 0.01\n",
      "Epoch [245/20000], Loss: 285.753662109375, Learning Rate: 0.01\n",
      "Epoch [246/20000], Loss: 284.29864501953125, Learning Rate: 0.01\n",
      "Epoch [247/20000], Loss: 282.860595703125, Learning Rate: 0.01\n",
      "Epoch [248/20000], Loss: 281.4406433105469, Learning Rate: 0.01\n",
      "Epoch [249/20000], Loss: 280.0369873046875, Learning Rate: 0.01\n",
      "Epoch [250/20000], Loss: 278.6505126953125, Learning Rate: 0.01\n",
      "Epoch [251/20000], Loss: 277.28045654296875, Learning Rate: 0.01\n",
      "Epoch [252/20000], Loss: 275.92657470703125, Learning Rate: 0.01\n",
      "Epoch [253/20000], Loss: 274.58892822265625, Learning Rate: 0.01\n",
      "Epoch [254/20000], Loss: 273.2673034667969, Learning Rate: 0.01\n",
      "Epoch [255/20000], Loss: 271.96087646484375, Learning Rate: 0.01\n",
      "Epoch [256/20000], Loss: 270.67047119140625, Learning Rate: 0.01\n",
      "Epoch [257/20000], Loss: 269.3946838378906, Learning Rate: 0.01\n",
      "Epoch [258/20000], Loss: 268.13397216796875, Learning Rate: 0.01\n",
      "Epoch [259/20000], Loss: 266.88861083984375, Learning Rate: 0.01\n",
      "Epoch [260/20000], Loss: 265.65765380859375, Learning Rate: 0.01\n",
      "Epoch [261/20000], Loss: 264.44091796875, Learning Rate: 0.01\n",
      "Epoch [262/20000], Loss: 263.238525390625, Learning Rate: 0.01\n",
      "Epoch [263/20000], Loss: 262.0499572753906, Learning Rate: 0.01\n",
      "Epoch [264/20000], Loss: 260.87554931640625, Learning Rate: 0.01\n",
      "Epoch [265/20000], Loss: 259.7149353027344, Learning Rate: 0.01\n",
      "Epoch [266/20000], Loss: 258.5677490234375, Learning Rate: 0.01\n",
      "Epoch [267/20000], Loss: 257.43377685546875, Learning Rate: 0.01\n",
      "Epoch [268/20000], Loss: 256.3133239746094, Learning Rate: 0.01\n",
      "Epoch [269/20000], Loss: 255.20565795898438, Learning Rate: 0.01\n",
      "Epoch [270/20000], Loss: 254.1105194091797, Learning Rate: 0.01\n",
      "Epoch [271/20000], Loss: 253.02850341796875, Learning Rate: 0.01\n",
      "Epoch [272/20000], Loss: 251.95904541015625, Learning Rate: 0.01\n",
      "Epoch [273/20000], Loss: 250.90164184570312, Learning Rate: 0.01\n",
      "Epoch [274/20000], Loss: 249.8565216064453, Learning Rate: 0.01\n",
      "Epoch [275/20000], Loss: 248.82318115234375, Learning Rate: 0.01\n",
      "Epoch [276/20000], Loss: 247.80223083496094, Learning Rate: 0.01\n",
      "Epoch [277/20000], Loss: 246.79293823242188, Learning Rate: 0.01\n",
      "Epoch [278/20000], Loss: 245.79501342773438, Learning Rate: 0.01\n",
      "Epoch [279/20000], Loss: 244.8089141845703, Learning Rate: 0.01\n",
      "Epoch [280/20000], Loss: 243.83380126953125, Learning Rate: 0.01\n",
      "Epoch [281/20000], Loss: 242.8700408935547, Learning Rate: 0.01\n",
      "Epoch [282/20000], Loss: 241.9176483154297, Learning Rate: 0.01\n",
      "Epoch [283/20000], Loss: 240.97584533691406, Learning Rate: 0.01\n",
      "Epoch [284/20000], Loss: 240.044677734375, Learning Rate: 0.01\n",
      "Epoch [285/20000], Loss: 239.1243438720703, Learning Rate: 0.01\n",
      "Epoch [286/20000], Loss: 238.21432495117188, Learning Rate: 0.01\n",
      "Epoch [287/20000], Loss: 237.31484985351562, Learning Rate: 0.01\n",
      "Epoch [288/20000], Loss: 236.42576599121094, Learning Rate: 0.01\n",
      "Epoch [289/20000], Loss: 235.54690551757812, Learning Rate: 0.01\n",
      "Epoch [290/20000], Loss: 234.67787170410156, Learning Rate: 0.01\n",
      "Epoch [291/20000], Loss: 233.8190155029297, Learning Rate: 0.01\n",
      "Epoch [292/20000], Loss: 232.96934509277344, Learning Rate: 0.01\n",
      "Epoch [293/20000], Loss: 232.1301727294922, Learning Rate: 0.01\n",
      "Epoch [294/20000], Loss: 231.2999267578125, Learning Rate: 0.01\n",
      "Epoch [295/20000], Loss: 230.4793701171875, Learning Rate: 0.01\n",
      "Epoch [296/20000], Loss: 229.66812133789062, Learning Rate: 0.01\n",
      "Epoch [297/20000], Loss: 228.8662109375, Learning Rate: 0.01\n",
      "Epoch [298/20000], Loss: 228.07315063476562, Learning Rate: 0.01\n",
      "Epoch [299/20000], Loss: 227.28939819335938, Learning Rate: 0.01\n",
      "Epoch [300/20000], Loss: 226.51425170898438, Learning Rate: 0.01\n",
      "Epoch [301/20000], Loss: 225.74803161621094, Learning Rate: 0.01\n",
      "Epoch [302/20000], Loss: 224.99037170410156, Learning Rate: 0.01\n",
      "Epoch [303/20000], Loss: 224.24171447753906, Learning Rate: 0.01\n",
      "Epoch [304/20000], Loss: 223.501220703125, Learning Rate: 0.01\n",
      "Epoch [305/20000], Loss: 222.7688446044922, Learning Rate: 0.01\n",
      "Epoch [306/20000], Loss: 222.045166015625, Learning Rate: 0.01\n",
      "Epoch [307/20000], Loss: 221.3295440673828, Learning Rate: 0.01\n",
      "Epoch [308/20000], Loss: 220.62210083007812, Learning Rate: 0.01\n",
      "Epoch [309/20000], Loss: 219.9226837158203, Learning Rate: 0.01\n",
      "Epoch [310/20000], Loss: 219.23109436035156, Learning Rate: 0.01\n",
      "Epoch [311/20000], Loss: 218.54739379882812, Learning Rate: 0.01\n",
      "Epoch [312/20000], Loss: 217.8711395263672, Learning Rate: 0.01\n",
      "Epoch [313/20000], Loss: 217.202880859375, Learning Rate: 0.01\n",
      "Epoch [314/20000], Loss: 216.541748046875, Learning Rate: 0.01\n",
      "Epoch [315/20000], Loss: 215.88827514648438, Learning Rate: 0.01\n",
      "Epoch [316/20000], Loss: 215.24215698242188, Learning Rate: 0.01\n",
      "Epoch [317/20000], Loss: 214.60340881347656, Learning Rate: 0.01\n",
      "Epoch [318/20000], Loss: 213.9717254638672, Learning Rate: 0.01\n",
      "Epoch [319/20000], Loss: 213.34703063964844, Learning Rate: 0.01\n",
      "Epoch [320/20000], Loss: 212.72976684570312, Learning Rate: 0.01\n",
      "Epoch [321/20000], Loss: 212.11895751953125, Learning Rate: 0.01\n",
      "Epoch [322/20000], Loss: 211.51536560058594, Learning Rate: 0.01\n",
      "Epoch [323/20000], Loss: 210.9183807373047, Learning Rate: 0.01\n",
      "Epoch [324/20000], Loss: 210.32791137695312, Learning Rate: 0.01\n",
      "Epoch [325/20000], Loss: 209.7443389892578, Learning Rate: 0.01\n",
      "Epoch [326/20000], Loss: 209.16720581054688, Learning Rate: 0.01\n",
      "Epoch [327/20000], Loss: 208.59669494628906, Learning Rate: 0.01\n",
      "Epoch [328/20000], Loss: 208.03219604492188, Learning Rate: 0.01\n",
      "Epoch [329/20000], Loss: 207.47422790527344, Learning Rate: 0.01\n",
      "Epoch [330/20000], Loss: 206.92259216308594, Learning Rate: 0.01\n",
      "Epoch [331/20000], Loss: 206.3770294189453, Learning Rate: 0.01\n",
      "Epoch [332/20000], Loss: 205.8377227783203, Learning Rate: 0.01\n",
      "Epoch [333/20000], Loss: 205.30421447753906, Learning Rate: 0.01\n",
      "Epoch [334/20000], Loss: 204.77650451660156, Learning Rate: 0.01\n",
      "Epoch [335/20000], Loss: 204.25477600097656, Learning Rate: 0.01\n",
      "Epoch [336/20000], Loss: 203.73924255371094, Learning Rate: 0.01\n",
      "Epoch [337/20000], Loss: 203.2286376953125, Learning Rate: 0.01\n",
      "Epoch [338/20000], Loss: 202.72434997558594, Learning Rate: 0.01\n",
      "Epoch [339/20000], Loss: 202.22544860839844, Learning Rate: 0.01\n",
      "Epoch [340/20000], Loss: 201.73236083984375, Learning Rate: 0.01\n",
      "Epoch [341/20000], Loss: 201.24427795410156, Learning Rate: 0.01\n",
      "Epoch [342/20000], Loss: 200.7617645263672, Learning Rate: 0.01\n",
      "Epoch [343/20000], Loss: 200.28457641601562, Learning Rate: 0.01\n",
      "Epoch [344/20000], Loss: 199.8126678466797, Learning Rate: 0.01\n",
      "Epoch [345/20000], Loss: 199.34609985351562, Learning Rate: 0.01\n",
      "Epoch [346/20000], Loss: 198.88482666015625, Learning Rate: 0.01\n",
      "Epoch [347/20000], Loss: 198.42803955078125, Learning Rate: 0.01\n",
      "Epoch [348/20000], Loss: 197.976806640625, Learning Rate: 0.01\n",
      "Epoch [349/20000], Loss: 197.53030395507812, Learning Rate: 0.01\n",
      "Epoch [350/20000], Loss: 197.0888671875, Learning Rate: 0.01\n",
      "Epoch [351/20000], Loss: 196.6519012451172, Learning Rate: 0.01\n",
      "Epoch [352/20000], Loss: 196.22023010253906, Learning Rate: 0.01\n",
      "Epoch [353/20000], Loss: 195.7930908203125, Learning Rate: 0.01\n",
      "Epoch [354/20000], Loss: 195.37026977539062, Learning Rate: 0.01\n",
      "Epoch [355/20000], Loss: 194.9522247314453, Learning Rate: 0.01\n",
      "Epoch [356/20000], Loss: 194.5389862060547, Learning Rate: 0.01\n",
      "Epoch [357/20000], Loss: 194.13023376464844, Learning Rate: 0.01\n",
      "Epoch [358/20000], Loss: 193.7254638671875, Learning Rate: 0.01\n",
      "Epoch [359/20000], Loss: 193.32542419433594, Learning Rate: 0.01\n",
      "Epoch [360/20000], Loss: 192.92958068847656, Learning Rate: 0.01\n",
      "Epoch [361/20000], Loss: 192.53836059570312, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [362/20000], Loss: 192.1509246826172, Learning Rate: 0.01\n",
      "Epoch [363/20000], Loss: 191.76792907714844, Learning Rate: 0.01\n",
      "Epoch [364/20000], Loss: 191.38894653320312, Learning Rate: 0.01\n",
      "Epoch [365/20000], Loss: 191.01400756835938, Learning Rate: 0.01\n",
      "Epoch [366/20000], Loss: 190.64317321777344, Learning Rate: 0.01\n",
      "Epoch [367/20000], Loss: 190.27630615234375, Learning Rate: 0.01\n",
      "Epoch [368/20000], Loss: 189.913330078125, Learning Rate: 0.01\n",
      "Epoch [369/20000], Loss: 189.55398559570312, Learning Rate: 0.01\n",
      "Epoch [370/20000], Loss: 189.19882202148438, Learning Rate: 0.01\n",
      "Epoch [371/20000], Loss: 188.8473358154297, Learning Rate: 0.01\n",
      "Epoch [372/20000], Loss: 188.49957275390625, Learning Rate: 0.01\n",
      "Epoch [373/20000], Loss: 188.15538024902344, Learning Rate: 0.01\n",
      "Epoch [374/20000], Loss: 187.815185546875, Learning Rate: 0.01\n",
      "Epoch [375/20000], Loss: 187.4782257080078, Learning Rate: 0.01\n",
      "Epoch [376/20000], Loss: 187.14483642578125, Learning Rate: 0.01\n",
      "Epoch [377/20000], Loss: 186.81517028808594, Learning Rate: 0.01\n",
      "Epoch [378/20000], Loss: 186.48878479003906, Learning Rate: 0.01\n",
      "Epoch [379/20000], Loss: 186.16575622558594, Learning Rate: 0.01\n",
      "Epoch [380/20000], Loss: 185.8461456298828, Learning Rate: 0.01\n",
      "Epoch [381/20000], Loss: 185.52976989746094, Learning Rate: 0.01\n",
      "Epoch [382/20000], Loss: 185.2168731689453, Learning Rate: 0.01\n",
      "Epoch [383/20000], Loss: 184.90721130371094, Learning Rate: 0.01\n",
      "Epoch [384/20000], Loss: 184.60057067871094, Learning Rate: 0.01\n",
      "Epoch [385/20000], Loss: 184.29739379882812, Learning Rate: 0.01\n",
      "Epoch [386/20000], Loss: 183.99713134765625, Learning Rate: 0.01\n",
      "Epoch [387/20000], Loss: 183.70016479492188, Learning Rate: 0.01\n",
      "Epoch [388/20000], Loss: 183.40567016601562, Learning Rate: 0.01\n",
      "Epoch [389/20000], Loss: 183.1145782470703, Learning Rate: 0.01\n",
      "Epoch [390/20000], Loss: 182.8263702392578, Learning Rate: 0.01\n",
      "Epoch [391/20000], Loss: 182.541015625, Learning Rate: 0.01\n",
      "Epoch [392/20000], Loss: 182.25894165039062, Learning Rate: 0.01\n",
      "Epoch [393/20000], Loss: 181.9795379638672, Learning Rate: 0.01\n",
      "Epoch [394/20000], Loss: 181.70281982421875, Learning Rate: 0.01\n",
      "Epoch [395/20000], Loss: 181.4287109375, Learning Rate: 0.01\n",
      "Epoch [396/20000], Loss: 181.15744018554688, Learning Rate: 0.01\n",
      "Epoch [397/20000], Loss: 180.88893127441406, Learning Rate: 0.01\n",
      "Epoch [398/20000], Loss: 180.62318420410156, Learning Rate: 0.01\n",
      "Epoch [399/20000], Loss: 180.36012268066406, Learning Rate: 0.01\n",
      "Epoch [400/20000], Loss: 180.0994415283203, Learning Rate: 0.01\n",
      "Epoch [401/20000], Loss: 179.84130859375, Learning Rate: 0.01\n",
      "Epoch [402/20000], Loss: 179.58602905273438, Learning Rate: 0.01\n",
      "Epoch [403/20000], Loss: 179.33290100097656, Learning Rate: 0.01\n",
      "Epoch [404/20000], Loss: 179.08241271972656, Learning Rate: 0.01\n",
      "Epoch [405/20000], Loss: 178.83445739746094, Learning Rate: 0.01\n",
      "Epoch [406/20000], Loss: 178.58883666992188, Learning Rate: 0.01\n",
      "Epoch [407/20000], Loss: 178.34567260742188, Learning Rate: 0.01\n",
      "Epoch [408/20000], Loss: 178.1046905517578, Learning Rate: 0.01\n",
      "Epoch [409/20000], Loss: 177.86593627929688, Learning Rate: 0.01\n",
      "Epoch [410/20000], Loss: 177.6297607421875, Learning Rate: 0.01\n",
      "Epoch [411/20000], Loss: 177.3958740234375, Learning Rate: 0.01\n",
      "Epoch [412/20000], Loss: 177.16363525390625, Learning Rate: 0.01\n",
      "Epoch [413/20000], Loss: 176.93406677246094, Learning Rate: 0.01\n",
      "Epoch [414/20000], Loss: 176.7063751220703, Learning Rate: 0.01\n",
      "Epoch [415/20000], Loss: 176.48123168945312, Learning Rate: 0.01\n",
      "Epoch [416/20000], Loss: 176.2578582763672, Learning Rate: 0.01\n",
      "Epoch [417/20000], Loss: 176.03646850585938, Learning Rate: 0.01\n",
      "Epoch [418/20000], Loss: 175.8175506591797, Learning Rate: 0.01\n",
      "Epoch [419/20000], Loss: 175.60012817382812, Learning Rate: 0.01\n",
      "Epoch [420/20000], Loss: 175.3852081298828, Learning Rate: 0.01\n",
      "Epoch [421/20000], Loss: 175.17198181152344, Learning Rate: 0.01\n",
      "Epoch [422/20000], Loss: 174.9606170654297, Learning Rate: 0.01\n",
      "Epoch [423/20000], Loss: 174.75128173828125, Learning Rate: 0.01\n",
      "Epoch [424/20000], Loss: 174.54368591308594, Learning Rate: 0.01\n",
      "Epoch [425/20000], Loss: 174.33853149414062, Learning Rate: 0.01\n",
      "Epoch [426/20000], Loss: 174.13458251953125, Learning Rate: 0.01\n",
      "Epoch [427/20000], Loss: 173.9324188232422, Learning Rate: 0.01\n",
      "Epoch [428/20000], Loss: 173.7322998046875, Learning Rate: 0.01\n",
      "Epoch [429/20000], Loss: 173.5340118408203, Learning Rate: 0.01\n",
      "Epoch [430/20000], Loss: 173.33726501464844, Learning Rate: 0.01\n",
      "Epoch [431/20000], Loss: 173.14236450195312, Learning Rate: 0.01\n",
      "Epoch [432/20000], Loss: 172.94924926757812, Learning Rate: 0.01\n",
      "Epoch [433/20000], Loss: 172.75747680664062, Learning Rate: 0.01\n",
      "Epoch [434/20000], Loss: 172.56785583496094, Learning Rate: 0.01\n",
      "Epoch [435/20000], Loss: 172.37937927246094, Learning Rate: 0.01\n",
      "Epoch [436/20000], Loss: 172.19265747070312, Learning Rate: 0.01\n",
      "Epoch [437/20000], Loss: 172.0077362060547, Learning Rate: 0.01\n",
      "Epoch [438/20000], Loss: 171.82437133789062, Learning Rate: 0.01\n",
      "Epoch [439/20000], Loss: 171.64212036132812, Learning Rate: 0.01\n",
      "Epoch [440/20000], Loss: 171.4615478515625, Learning Rate: 0.01\n",
      "Epoch [441/20000], Loss: 171.28280639648438, Learning Rate: 0.01\n",
      "Epoch [442/20000], Loss: 171.10536193847656, Learning Rate: 0.01\n",
      "Epoch [443/20000], Loss: 170.92922973632812, Learning Rate: 0.01\n",
      "Epoch [444/20000], Loss: 170.75466918945312, Learning Rate: 0.01\n",
      "Epoch [445/20000], Loss: 170.58126831054688, Learning Rate: 0.01\n",
      "Epoch [446/20000], Loss: 170.4097900390625, Learning Rate: 0.01\n",
      "Epoch [447/20000], Loss: 170.23916625976562, Learning Rate: 0.01\n",
      "Epoch [448/20000], Loss: 170.0701446533203, Learning Rate: 0.01\n",
      "Epoch [449/20000], Loss: 169.90234375, Learning Rate: 0.01\n",
      "Epoch [450/20000], Loss: 169.7360382080078, Learning Rate: 0.01\n",
      "Epoch [451/20000], Loss: 169.57069396972656, Learning Rate: 0.01\n",
      "Epoch [452/20000], Loss: 169.40713500976562, Learning Rate: 0.01\n",
      "Epoch [453/20000], Loss: 169.24453735351562, Learning Rate: 0.01\n",
      "Epoch [454/20000], Loss: 169.08328247070312, Learning Rate: 0.01\n",
      "Epoch [455/20000], Loss: 168.9231719970703, Learning Rate: 0.01\n",
      "Epoch [456/20000], Loss: 168.76429748535156, Learning Rate: 0.01\n",
      "Epoch [457/20000], Loss: 168.60670471191406, Learning Rate: 0.01\n",
      "Epoch [458/20000], Loss: 168.4502716064453, Learning Rate: 0.01\n",
      "Epoch [459/20000], Loss: 168.2948760986328, Learning Rate: 0.01\n",
      "Epoch [460/20000], Loss: 168.1407928466797, Learning Rate: 0.01\n",
      "Epoch [461/20000], Loss: 167.9876708984375, Learning Rate: 0.01\n",
      "Epoch [462/20000], Loss: 167.8360595703125, Learning Rate: 0.01\n",
      "Epoch [463/20000], Loss: 167.6851348876953, Learning Rate: 0.01\n",
      "Epoch [464/20000], Loss: 167.53518676757812, Learning Rate: 0.01\n",
      "Epoch [465/20000], Loss: 167.38668823242188, Learning Rate: 0.01\n",
      "Epoch [466/20000], Loss: 167.23936462402344, Learning Rate: 0.01\n",
      "Epoch [467/20000], Loss: 167.0927734375, Learning Rate: 0.01\n",
      "Epoch [468/20000], Loss: 166.94720458984375, Learning Rate: 0.01\n",
      "Epoch [469/20000], Loss: 166.80276489257812, Learning Rate: 0.01\n",
      "Epoch [470/20000], Loss: 166.65963745117188, Learning Rate: 0.01\n",
      "Epoch [471/20000], Loss: 166.5167236328125, Learning Rate: 0.01\n",
      "Epoch [472/20000], Loss: 166.37542724609375, Learning Rate: 0.01\n",
      "Epoch [473/20000], Loss: 166.23484802246094, Learning Rate: 0.01\n",
      "Epoch [474/20000], Loss: 166.09515380859375, Learning Rate: 0.01\n",
      "Epoch [475/20000], Loss: 165.95626831054688, Learning Rate: 0.01\n",
      "Epoch [476/20000], Loss: 165.81875610351562, Learning Rate: 0.01\n",
      "Epoch [477/20000], Loss: 165.68167114257812, Learning Rate: 0.01\n",
      "Epoch [478/20000], Loss: 165.5457763671875, Learning Rate: 0.01\n",
      "Epoch [479/20000], Loss: 165.4105682373047, Learning Rate: 0.01\n",
      "Epoch [480/20000], Loss: 165.27622985839844, Learning Rate: 0.01\n",
      "Epoch [481/20000], Loss: 165.1429901123047, Learning Rate: 0.01\n",
      "Epoch [482/20000], Loss: 165.01043701171875, Learning Rate: 0.01\n",
      "Epoch [483/20000], Loss: 164.87890625, Learning Rate: 0.01\n",
      "Epoch [484/20000], Loss: 164.74786376953125, Learning Rate: 0.01\n",
      "Epoch [485/20000], Loss: 164.6176300048828, Learning Rate: 0.01\n",
      "Epoch [486/20000], Loss: 164.48858642578125, Learning Rate: 0.01\n",
      "Epoch [487/20000], Loss: 164.35989379882812, Learning Rate: 0.01\n",
      "Epoch [488/20000], Loss: 164.23220825195312, Learning Rate: 0.01\n",
      "Epoch [489/20000], Loss: 164.10513305664062, Learning Rate: 0.01\n",
      "Epoch [490/20000], Loss: 163.97891235351562, Learning Rate: 0.01\n",
      "Epoch [491/20000], Loss: 163.8535614013672, Learning Rate: 0.01\n",
      "Epoch [492/20000], Loss: 163.72865295410156, Learning Rate: 0.01\n",
      "Epoch [493/20000], Loss: 163.6049041748047, Learning Rate: 0.01\n",
      "Epoch [494/20000], Loss: 163.48143005371094, Learning Rate: 0.01\n",
      "Epoch [495/20000], Loss: 163.3587646484375, Learning Rate: 0.01\n",
      "Epoch [496/20000], Loss: 163.23704528808594, Learning Rate: 0.01\n",
      "Epoch [497/20000], Loss: 163.11578369140625, Learning Rate: 0.01\n",
      "Epoch [498/20000], Loss: 162.9951629638672, Learning Rate: 0.01\n",
      "Epoch [499/20000], Loss: 162.87509155273438, Learning Rate: 0.01\n",
      "Epoch [500/20000], Loss: 162.75564575195312, Learning Rate: 0.01\n",
      "Epoch [501/20000], Loss: 162.6373291015625, Learning Rate: 0.01\n",
      "Epoch [502/20000], Loss: 162.51930236816406, Learning Rate: 0.01\n",
      "Epoch [503/20000], Loss: 162.4020233154297, Learning Rate: 0.01\n",
      "Epoch [504/20000], Loss: 162.2851104736328, Learning Rate: 0.01\n",
      "Epoch [505/20000], Loss: 162.16896057128906, Learning Rate: 0.01\n",
      "Epoch [506/20000], Loss: 162.0533447265625, Learning Rate: 0.01\n",
      "Epoch [507/20000], Loss: 161.93814086914062, Learning Rate: 0.01\n",
      "Epoch [508/20000], Loss: 161.82391357421875, Learning Rate: 0.01\n",
      "Epoch [509/20000], Loss: 161.71006774902344, Learning Rate: 0.01\n",
      "Epoch [510/20000], Loss: 161.59678649902344, Learning Rate: 0.01\n",
      "Epoch [511/20000], Loss: 161.48428344726562, Learning Rate: 0.01\n",
      "Epoch [512/20000], Loss: 161.37196350097656, Learning Rate: 0.01\n",
      "Epoch [513/20000], Loss: 161.26010131835938, Learning Rate: 0.01\n",
      "Epoch [514/20000], Loss: 161.1493377685547, Learning Rate: 0.01\n",
      "Epoch [515/20000], Loss: 161.03868103027344, Learning Rate: 0.01\n",
      "Epoch [516/20000], Loss: 160.92868041992188, Learning Rate: 0.01\n",
      "Epoch [517/20000], Loss: 160.81900024414062, Learning Rate: 0.01\n",
      "Epoch [518/20000], Loss: 160.7099151611328, Learning Rate: 0.01\n",
      "Epoch [519/20000], Loss: 160.60157775878906, Learning Rate: 0.01\n",
      "Epoch [520/20000], Loss: 160.4932403564453, Learning Rate: 0.01\n",
      "Epoch [521/20000], Loss: 160.38572692871094, Learning Rate: 0.01\n",
      "Epoch [522/20000], Loss: 160.27845764160156, Learning Rate: 0.01\n",
      "Epoch [523/20000], Loss: 160.1719970703125, Learning Rate: 0.01\n",
      "Epoch [524/20000], Loss: 160.06558227539062, Learning Rate: 0.01\n",
      "Epoch [525/20000], Loss: 159.9599151611328, Learning Rate: 0.01\n",
      "Epoch [526/20000], Loss: 159.85447692871094, Learning Rate: 0.01\n",
      "Epoch [527/20000], Loss: 159.74984741210938, Learning Rate: 0.01\n",
      "Epoch [528/20000], Loss: 159.64508056640625, Learning Rate: 0.01\n",
      "Epoch [529/20000], Loss: 159.5410614013672, Learning Rate: 0.01\n",
      "Epoch [530/20000], Loss: 159.43756103515625, Learning Rate: 0.01\n",
      "Epoch [531/20000], Loss: 159.3341064453125, Learning Rate: 0.01\n",
      "Epoch [532/20000], Loss: 159.23155212402344, Learning Rate: 0.01\n",
      "Epoch [533/20000], Loss: 159.12908935546875, Learning Rate: 0.01\n",
      "Epoch [534/20000], Loss: 159.0269775390625, Learning Rate: 0.01\n",
      "Epoch [535/20000], Loss: 158.92575073242188, Learning Rate: 0.01\n",
      "Epoch [536/20000], Loss: 158.82421875, Learning Rate: 0.01\n",
      "Epoch [537/20000], Loss: 158.72348022460938, Learning Rate: 0.01\n",
      "Epoch [538/20000], Loss: 158.62269592285156, Learning Rate: 0.01\n",
      "Epoch [539/20000], Loss: 158.52247619628906, Learning Rate: 0.01\n",
      "Epoch [540/20000], Loss: 158.4227752685547, Learning Rate: 0.01\n",
      "Epoch [541/20000], Loss: 158.32354736328125, Learning Rate: 0.01\n",
      "Epoch [542/20000], Loss: 158.22434997558594, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [543/20000], Loss: 158.1256103515625, Learning Rate: 0.01\n",
      "Epoch [544/20000], Loss: 158.02713012695312, Learning Rate: 0.01\n",
      "Epoch [545/20000], Loss: 157.92922973632812, Learning Rate: 0.01\n",
      "Epoch [546/20000], Loss: 157.83108520507812, Learning Rate: 0.01\n",
      "Epoch [547/20000], Loss: 157.73390197753906, Learning Rate: 0.01\n",
      "Epoch [548/20000], Loss: 157.63670349121094, Learning Rate: 0.01\n",
      "Epoch [549/20000], Loss: 157.53985595703125, Learning Rate: 0.01\n",
      "Epoch [550/20000], Loss: 157.44358825683594, Learning Rate: 0.01\n",
      "Epoch [551/20000], Loss: 157.3472900390625, Learning Rate: 0.01\n",
      "Epoch [552/20000], Loss: 157.25140380859375, Learning Rate: 0.01\n",
      "Epoch [553/20000], Loss: 157.15533447265625, Learning Rate: 0.01\n",
      "Epoch [554/20000], Loss: 157.06019592285156, Learning Rate: 0.01\n",
      "Epoch [555/20000], Loss: 156.9654083251953, Learning Rate: 0.01\n",
      "Epoch [556/20000], Loss: 156.87063598632812, Learning Rate: 0.01\n",
      "Epoch [557/20000], Loss: 156.776123046875, Learning Rate: 0.01\n",
      "Epoch [558/20000], Loss: 156.6820526123047, Learning Rate: 0.01\n",
      "Epoch [559/20000], Loss: 156.58804321289062, Learning Rate: 0.01\n",
      "Epoch [560/20000], Loss: 156.49435424804688, Learning Rate: 0.01\n",
      "Epoch [561/20000], Loss: 156.40072631835938, Learning Rate: 0.01\n",
      "Epoch [562/20000], Loss: 156.30787658691406, Learning Rate: 0.01\n",
      "Epoch [563/20000], Loss: 156.21475219726562, Learning Rate: 0.01\n",
      "Epoch [564/20000], Loss: 156.12197875976562, Learning Rate: 0.01\n",
      "Epoch [565/20000], Loss: 156.02957153320312, Learning Rate: 0.01\n",
      "Epoch [566/20000], Loss: 155.9372100830078, Learning Rate: 0.01\n",
      "Epoch [567/20000], Loss: 155.845458984375, Learning Rate: 0.01\n",
      "Epoch [568/20000], Loss: 155.7534942626953, Learning Rate: 0.01\n",
      "Epoch [569/20000], Loss: 155.66204833984375, Learning Rate: 0.01\n",
      "Epoch [570/20000], Loss: 155.5704803466797, Learning Rate: 0.01\n",
      "Epoch [571/20000], Loss: 155.47979736328125, Learning Rate: 0.01\n",
      "Epoch [572/20000], Loss: 155.38856506347656, Learning Rate: 0.01\n",
      "Epoch [573/20000], Loss: 155.2981719970703, Learning Rate: 0.01\n",
      "Epoch [574/20000], Loss: 155.20755004882812, Learning Rate: 0.01\n",
      "Epoch [575/20000], Loss: 155.11749267578125, Learning Rate: 0.01\n",
      "Epoch [576/20000], Loss: 155.02713012695312, Learning Rate: 0.01\n",
      "Epoch [577/20000], Loss: 154.93728637695312, Learning Rate: 0.01\n",
      "Epoch [578/20000], Loss: 154.84793090820312, Learning Rate: 0.01\n",
      "Epoch [579/20000], Loss: 154.7581024169922, Learning Rate: 0.01\n",
      "Epoch [580/20000], Loss: 154.66885375976562, Learning Rate: 0.01\n",
      "Epoch [581/20000], Loss: 154.57992553710938, Learning Rate: 0.01\n",
      "Epoch [582/20000], Loss: 154.49102783203125, Learning Rate: 0.01\n",
      "Epoch [583/20000], Loss: 154.4023895263672, Learning Rate: 0.01\n",
      "Epoch [584/20000], Loss: 154.31373596191406, Learning Rate: 0.01\n",
      "Epoch [585/20000], Loss: 154.22511291503906, Learning Rate: 0.01\n",
      "Epoch [586/20000], Loss: 154.13693237304688, Learning Rate: 0.01\n",
      "Epoch [587/20000], Loss: 154.04873657226562, Learning Rate: 0.01\n",
      "Epoch [588/20000], Loss: 153.96099853515625, Learning Rate: 0.01\n",
      "Epoch [589/20000], Loss: 153.87335205078125, Learning Rate: 0.01\n",
      "Epoch [590/20000], Loss: 153.78575134277344, Learning Rate: 0.01\n",
      "Epoch [591/20000], Loss: 153.6982421875, Learning Rate: 0.01\n",
      "Epoch [592/20000], Loss: 153.61093139648438, Learning Rate: 0.01\n",
      "Epoch [593/20000], Loss: 153.5239715576172, Learning Rate: 0.01\n",
      "Epoch [594/20000], Loss: 153.43679809570312, Learning Rate: 0.01\n",
      "Epoch [595/20000], Loss: 153.3497772216797, Learning Rate: 0.01\n",
      "Epoch [596/20000], Loss: 153.26319885253906, Learning Rate: 0.01\n",
      "Epoch [597/20000], Loss: 153.1766357421875, Learning Rate: 0.01\n",
      "Epoch [598/20000], Loss: 153.09027099609375, Learning Rate: 0.01\n",
      "Epoch [599/20000], Loss: 153.0038299560547, Learning Rate: 0.01\n",
      "Epoch [600/20000], Loss: 152.91766357421875, Learning Rate: 0.01\n",
      "Epoch [601/20000], Loss: 152.83169555664062, Learning Rate: 0.01\n",
      "Epoch [602/20000], Loss: 152.7457733154297, Learning Rate: 0.01\n",
      "Epoch [603/20000], Loss: 152.65969848632812, Learning Rate: 0.01\n",
      "Epoch [604/20000], Loss: 152.57424926757812, Learning Rate: 0.01\n",
      "Epoch [605/20000], Loss: 152.48855590820312, Learning Rate: 0.01\n",
      "Epoch [606/20000], Loss: 152.40309143066406, Learning Rate: 0.01\n",
      "Epoch [607/20000], Loss: 152.31787109375, Learning Rate: 0.01\n",
      "Epoch [608/20000], Loss: 152.23265075683594, Learning Rate: 0.01\n",
      "Epoch [609/20000], Loss: 152.1475372314453, Learning Rate: 0.01\n",
      "Epoch [610/20000], Loss: 152.06219482421875, Learning Rate: 0.01\n",
      "Epoch [611/20000], Loss: 151.977294921875, Learning Rate: 0.01\n",
      "Epoch [612/20000], Loss: 151.8924102783203, Learning Rate: 0.01\n",
      "Epoch [613/20000], Loss: 151.80796813964844, Learning Rate: 0.01\n",
      "Epoch [614/20000], Loss: 151.7230682373047, Learning Rate: 0.01\n",
      "Epoch [615/20000], Loss: 151.6385498046875, Learning Rate: 0.01\n",
      "Epoch [616/20000], Loss: 151.55416870117188, Learning Rate: 0.01\n",
      "Epoch [617/20000], Loss: 151.46975708007812, Learning Rate: 0.01\n",
      "Epoch [618/20000], Loss: 151.38551330566406, Learning Rate: 0.01\n",
      "Epoch [619/20000], Loss: 151.30148315429688, Learning Rate: 0.01\n",
      "Epoch [620/20000], Loss: 151.21725463867188, Learning Rate: 0.01\n",
      "Epoch [621/20000], Loss: 151.13320922851562, Learning Rate: 0.01\n",
      "Epoch [622/20000], Loss: 151.0492401123047, Learning Rate: 0.01\n",
      "Epoch [623/20000], Loss: 150.96517944335938, Learning Rate: 0.01\n",
      "Epoch [624/20000], Loss: 150.88156127929688, Learning Rate: 0.01\n",
      "Epoch [625/20000], Loss: 150.79786682128906, Learning Rate: 0.01\n",
      "Epoch [626/20000], Loss: 150.71397399902344, Learning Rate: 0.01\n",
      "Epoch [627/20000], Loss: 150.63070678710938, Learning Rate: 0.01\n",
      "Epoch [628/20000], Loss: 150.54721069335938, Learning Rate: 0.01\n",
      "Epoch [629/20000], Loss: 150.46377563476562, Learning Rate: 0.01\n",
      "Epoch [630/20000], Loss: 150.38046264648438, Learning Rate: 0.01\n",
      "Epoch [631/20000], Loss: 150.2969970703125, Learning Rate: 0.01\n",
      "Epoch [632/20000], Loss: 150.21383666992188, Learning Rate: 0.01\n",
      "Epoch [633/20000], Loss: 150.13067626953125, Learning Rate: 0.01\n",
      "Epoch [634/20000], Loss: 150.04754638671875, Learning Rate: 0.01\n",
      "Epoch [635/20000], Loss: 149.96432495117188, Learning Rate: 0.01\n",
      "Epoch [636/20000], Loss: 149.88140869140625, Learning Rate: 0.01\n",
      "Epoch [637/20000], Loss: 149.79856872558594, Learning Rate: 0.01\n",
      "Epoch [638/20000], Loss: 149.71563720703125, Learning Rate: 0.01\n",
      "Epoch [639/20000], Loss: 149.6327667236328, Learning Rate: 0.01\n",
      "Epoch [640/20000], Loss: 149.5499267578125, Learning Rate: 0.01\n",
      "Epoch [641/20000], Loss: 149.467041015625, Learning Rate: 0.01\n",
      "Epoch [642/20000], Loss: 149.38441467285156, Learning Rate: 0.01\n",
      "Epoch [643/20000], Loss: 149.30172729492188, Learning Rate: 0.01\n",
      "Epoch [644/20000], Loss: 149.21896362304688, Learning Rate: 0.01\n",
      "Epoch [645/20000], Loss: 149.13641357421875, Learning Rate: 0.01\n",
      "Epoch [646/20000], Loss: 149.0538330078125, Learning Rate: 0.01\n",
      "Epoch [647/20000], Loss: 148.97128295898438, Learning Rate: 0.01\n",
      "Epoch [648/20000], Loss: 148.888916015625, Learning Rate: 0.01\n",
      "Epoch [649/20000], Loss: 148.80636596679688, Learning Rate: 0.01\n",
      "Epoch [650/20000], Loss: 148.72402954101562, Learning Rate: 0.01\n",
      "Epoch [651/20000], Loss: 148.64163208007812, Learning Rate: 0.01\n",
      "Epoch [652/20000], Loss: 148.55938720703125, Learning Rate: 0.01\n",
      "Epoch [653/20000], Loss: 148.47677612304688, Learning Rate: 0.01\n",
      "Epoch [654/20000], Loss: 148.3948516845703, Learning Rate: 0.01\n",
      "Epoch [655/20000], Loss: 148.31248474121094, Learning Rate: 0.01\n",
      "Epoch [656/20000], Loss: 148.2301483154297, Learning Rate: 0.01\n",
      "Epoch [657/20000], Loss: 148.14785766601562, Learning Rate: 0.01\n",
      "Epoch [658/20000], Loss: 148.06568908691406, Learning Rate: 0.01\n",
      "Epoch [659/20000], Loss: 147.98367309570312, Learning Rate: 0.01\n",
      "Epoch [660/20000], Loss: 147.90150451660156, Learning Rate: 0.01\n",
      "Epoch [661/20000], Loss: 147.81936645507812, Learning Rate: 0.01\n",
      "Epoch [662/20000], Loss: 147.73690795898438, Learning Rate: 0.01\n",
      "Epoch [663/20000], Loss: 147.6550750732422, Learning Rate: 0.01\n",
      "Epoch [664/20000], Loss: 147.57302856445312, Learning Rate: 0.01\n",
      "Epoch [665/20000], Loss: 147.4909210205078, Learning Rate: 0.01\n",
      "Epoch [666/20000], Loss: 147.40884399414062, Learning Rate: 0.01\n",
      "Epoch [667/20000], Loss: 147.3268585205078, Learning Rate: 0.01\n",
      "Epoch [668/20000], Loss: 147.24468994140625, Learning Rate: 0.01\n",
      "Epoch [669/20000], Loss: 147.16273498535156, Learning Rate: 0.01\n",
      "Epoch [670/20000], Loss: 147.08065795898438, Learning Rate: 0.01\n",
      "Epoch [671/20000], Loss: 146.99856567382812, Learning Rate: 0.01\n",
      "Epoch [672/20000], Loss: 146.91668701171875, Learning Rate: 0.01\n",
      "Epoch [673/20000], Loss: 146.83486938476562, Learning Rate: 0.01\n",
      "Epoch [674/20000], Loss: 146.75286865234375, Learning Rate: 0.01\n",
      "Epoch [675/20000], Loss: 146.6707305908203, Learning Rate: 0.01\n",
      "Epoch [676/20000], Loss: 146.5889129638672, Learning Rate: 0.01\n",
      "Epoch [677/20000], Loss: 146.5068359375, Learning Rate: 0.01\n",
      "Epoch [678/20000], Loss: 146.42495727539062, Learning Rate: 0.01\n",
      "Epoch [679/20000], Loss: 146.3428955078125, Learning Rate: 0.01\n",
      "Epoch [680/20000], Loss: 146.2610321044922, Learning Rate: 0.01\n",
      "Epoch [681/20000], Loss: 146.17916870117188, Learning Rate: 0.01\n",
      "Epoch [682/20000], Loss: 146.0968017578125, Learning Rate: 0.01\n",
      "Epoch [683/20000], Loss: 146.01492309570312, Learning Rate: 0.01\n",
      "Epoch [684/20000], Loss: 145.9327392578125, Learning Rate: 0.01\n",
      "Epoch [685/20000], Loss: 145.85107421875, Learning Rate: 0.01\n",
      "Epoch [686/20000], Loss: 145.76904296875, Learning Rate: 0.01\n",
      "Epoch [687/20000], Loss: 145.6870880126953, Learning Rate: 0.01\n",
      "Epoch [688/20000], Loss: 145.60475158691406, Learning Rate: 0.01\n",
      "Epoch [689/20000], Loss: 145.52304077148438, Learning Rate: 0.01\n",
      "Epoch [690/20000], Loss: 145.44073486328125, Learning Rate: 0.01\n",
      "Epoch [691/20000], Loss: 145.35906982421875, Learning Rate: 0.01\n",
      "Epoch [692/20000], Loss: 145.27670288085938, Learning Rate: 0.01\n",
      "Epoch [693/20000], Loss: 145.19479370117188, Learning Rate: 0.01\n",
      "Epoch [694/20000], Loss: 145.11257934570312, Learning Rate: 0.01\n",
      "Epoch [695/20000], Loss: 145.0303955078125, Learning Rate: 0.01\n",
      "Epoch [696/20000], Loss: 144.9482421875, Learning Rate: 0.01\n",
      "Epoch [697/20000], Loss: 144.86642456054688, Learning Rate: 0.01\n",
      "Epoch [698/20000], Loss: 144.78419494628906, Learning Rate: 0.01\n",
      "Epoch [699/20000], Loss: 144.70205688476562, Learning Rate: 0.01\n",
      "Epoch [700/20000], Loss: 144.61981201171875, Learning Rate: 0.01\n",
      "Epoch [701/20000], Loss: 144.53765869140625, Learning Rate: 0.01\n",
      "Epoch [702/20000], Loss: 144.45518493652344, Learning Rate: 0.01\n",
      "Epoch [703/20000], Loss: 144.37322998046875, Learning Rate: 0.01\n",
      "Epoch [704/20000], Loss: 144.29110717773438, Learning Rate: 0.01\n",
      "Epoch [705/20000], Loss: 144.20880126953125, Learning Rate: 0.01\n",
      "Epoch [706/20000], Loss: 144.1263427734375, Learning Rate: 0.01\n",
      "Epoch [707/20000], Loss: 144.04397583007812, Learning Rate: 0.01\n",
      "Epoch [708/20000], Loss: 143.961669921875, Learning Rate: 0.01\n",
      "Epoch [709/20000], Loss: 143.8792266845703, Learning Rate: 0.01\n",
      "Epoch [710/20000], Loss: 143.79705810546875, Learning Rate: 0.01\n",
      "Epoch [711/20000], Loss: 143.71444702148438, Learning Rate: 0.01\n",
      "Epoch [712/20000], Loss: 143.63201904296875, Learning Rate: 0.01\n",
      "Epoch [713/20000], Loss: 143.54957580566406, Learning Rate: 0.01\n",
      "Epoch [714/20000], Loss: 143.46719360351562, Learning Rate: 0.01\n",
      "Epoch [715/20000], Loss: 143.38462829589844, Learning Rate: 0.01\n",
      "Epoch [716/20000], Loss: 143.30221557617188, Learning Rate: 0.01\n",
      "Epoch [717/20000], Loss: 143.21949768066406, Learning Rate: 0.01\n",
      "Epoch [718/20000], Loss: 143.13705444335938, Learning Rate: 0.01\n",
      "Epoch [719/20000], Loss: 143.05430603027344, Learning Rate: 0.01\n",
      "Epoch [720/20000], Loss: 142.97161865234375, Learning Rate: 0.01\n",
      "Epoch [721/20000], Loss: 142.88906860351562, Learning Rate: 0.01\n",
      "Epoch [722/20000], Loss: 142.8062744140625, Learning Rate: 0.01\n",
      "Epoch [723/20000], Loss: 142.72341918945312, Learning Rate: 0.01\n",
      "Epoch [724/20000], Loss: 142.64071655273438, Learning Rate: 0.01\n",
      "Epoch [725/20000], Loss: 142.55809020996094, Learning Rate: 0.01\n",
      "Epoch [726/20000], Loss: 142.47511291503906, Learning Rate: 0.01\n",
      "Epoch [727/20000], Loss: 142.3924560546875, Learning Rate: 0.01\n",
      "Epoch [728/20000], Loss: 142.309326171875, Learning Rate: 0.01\n",
      "Epoch [729/20000], Loss: 142.22647094726562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [730/20000], Loss: 142.1435546875, Learning Rate: 0.01\n",
      "Epoch [731/20000], Loss: 142.06045532226562, Learning Rate: 0.01\n",
      "Epoch [732/20000], Loss: 141.97756958007812, Learning Rate: 0.01\n",
      "Epoch [733/20000], Loss: 141.89437866210938, Learning Rate: 0.01\n",
      "Epoch [734/20000], Loss: 141.811279296875, Learning Rate: 0.01\n",
      "Epoch [735/20000], Loss: 141.728271484375, Learning Rate: 0.01\n",
      "Epoch [736/20000], Loss: 141.64508056640625, Learning Rate: 0.01\n",
      "Epoch [737/20000], Loss: 141.56192016601562, Learning Rate: 0.01\n",
      "Epoch [738/20000], Loss: 141.47854614257812, Learning Rate: 0.01\n",
      "Epoch [739/20000], Loss: 141.39512634277344, Learning Rate: 0.01\n",
      "Epoch [740/20000], Loss: 141.31192016601562, Learning Rate: 0.01\n",
      "Epoch [741/20000], Loss: 141.22872924804688, Learning Rate: 0.01\n",
      "Epoch [742/20000], Loss: 141.14505004882812, Learning Rate: 0.01\n",
      "Epoch [743/20000], Loss: 141.061767578125, Learning Rate: 0.01\n",
      "Epoch [744/20000], Loss: 140.97848510742188, Learning Rate: 0.01\n",
      "Epoch [745/20000], Loss: 140.89476013183594, Learning Rate: 0.01\n",
      "Epoch [746/20000], Loss: 140.8112335205078, Learning Rate: 0.01\n",
      "Epoch [747/20000], Loss: 140.72787475585938, Learning Rate: 0.01\n",
      "Epoch [748/20000], Loss: 140.64407348632812, Learning Rate: 0.01\n",
      "Epoch [749/20000], Loss: 140.56072998046875, Learning Rate: 0.01\n",
      "Epoch [750/20000], Loss: 140.47686767578125, Learning Rate: 0.01\n",
      "Epoch [751/20000], Loss: 140.3931884765625, Learning Rate: 0.01\n",
      "Epoch [752/20000], Loss: 140.30950927734375, Learning Rate: 0.01\n",
      "Epoch [753/20000], Loss: 140.22579956054688, Learning Rate: 0.01\n",
      "Epoch [754/20000], Loss: 140.14175415039062, Learning Rate: 0.01\n",
      "Epoch [755/20000], Loss: 140.0578155517578, Learning Rate: 0.01\n",
      "Epoch [756/20000], Loss: 139.97425842285156, Learning Rate: 0.01\n",
      "Epoch [757/20000], Loss: 139.88990783691406, Learning Rate: 0.01\n",
      "Epoch [758/20000], Loss: 139.80584716796875, Learning Rate: 0.01\n",
      "Epoch [759/20000], Loss: 139.72195434570312, Learning Rate: 0.01\n",
      "Epoch [760/20000], Loss: 139.63818359375, Learning Rate: 0.01\n",
      "Epoch [761/20000], Loss: 139.55397033691406, Learning Rate: 0.01\n",
      "Epoch [762/20000], Loss: 139.46998596191406, Learning Rate: 0.01\n",
      "Epoch [763/20000], Loss: 139.38577270507812, Learning Rate: 0.01\n",
      "Epoch [764/20000], Loss: 139.3015594482422, Learning Rate: 0.01\n",
      "Epoch [765/20000], Loss: 139.21734619140625, Learning Rate: 0.01\n",
      "Epoch [766/20000], Loss: 139.133056640625, Learning Rate: 0.01\n",
      "Epoch [767/20000], Loss: 139.04885864257812, Learning Rate: 0.01\n",
      "Epoch [768/20000], Loss: 138.96437072753906, Learning Rate: 0.01\n",
      "Epoch [769/20000], Loss: 138.87991333007812, Learning Rate: 0.01\n",
      "Epoch [770/20000], Loss: 138.7958221435547, Learning Rate: 0.01\n",
      "Epoch [771/20000], Loss: 138.71121215820312, Learning Rate: 0.01\n",
      "Epoch [772/20000], Loss: 138.62637329101562, Learning Rate: 0.01\n",
      "Epoch [773/20000], Loss: 138.5423583984375, Learning Rate: 0.01\n",
      "Epoch [774/20000], Loss: 138.45758056640625, Learning Rate: 0.01\n",
      "Epoch [775/20000], Loss: 138.37327575683594, Learning Rate: 0.01\n",
      "Epoch [776/20000], Loss: 138.28875732421875, Learning Rate: 0.01\n",
      "Epoch [777/20000], Loss: 138.2036590576172, Learning Rate: 0.01\n",
      "Epoch [778/20000], Loss: 138.119140625, Learning Rate: 0.01\n",
      "Epoch [779/20000], Loss: 138.03465270996094, Learning Rate: 0.01\n",
      "Epoch [780/20000], Loss: 137.94964599609375, Learning Rate: 0.01\n",
      "Epoch [781/20000], Loss: 137.86480712890625, Learning Rate: 0.01\n",
      "Epoch [782/20000], Loss: 137.77993774414062, Learning Rate: 0.01\n",
      "Epoch [783/20000], Loss: 137.69525146484375, Learning Rate: 0.01\n",
      "Epoch [784/20000], Loss: 137.61044311523438, Learning Rate: 0.01\n",
      "Epoch [785/20000], Loss: 137.52578735351562, Learning Rate: 0.01\n",
      "Epoch [786/20000], Loss: 137.44073486328125, Learning Rate: 0.01\n",
      "Epoch [787/20000], Loss: 137.3555908203125, Learning Rate: 0.01\n",
      "Epoch [788/20000], Loss: 137.270751953125, Learning Rate: 0.01\n",
      "Epoch [789/20000], Loss: 137.18572998046875, Learning Rate: 0.01\n",
      "Epoch [790/20000], Loss: 137.10049438476562, Learning Rate: 0.01\n",
      "Epoch [791/20000], Loss: 137.01556396484375, Learning Rate: 0.01\n",
      "Epoch [792/20000], Loss: 136.93045043945312, Learning Rate: 0.01\n",
      "Epoch [793/20000], Loss: 136.845458984375, Learning Rate: 0.01\n",
      "Epoch [794/20000], Loss: 136.7601318359375, Learning Rate: 0.01\n",
      "Epoch [795/20000], Loss: 136.67495727539062, Learning Rate: 0.01\n",
      "Epoch [796/20000], Loss: 136.58963012695312, Learning Rate: 0.01\n",
      "Epoch [797/20000], Loss: 136.50469970703125, Learning Rate: 0.01\n",
      "Epoch [798/20000], Loss: 136.419189453125, Learning Rate: 0.01\n",
      "Epoch [799/20000], Loss: 136.33416748046875, Learning Rate: 0.01\n",
      "Epoch [800/20000], Loss: 136.248779296875, Learning Rate: 0.01\n",
      "Epoch [801/20000], Loss: 136.16329956054688, Learning Rate: 0.01\n",
      "Epoch [802/20000], Loss: 136.0780487060547, Learning Rate: 0.01\n",
      "Epoch [803/20000], Loss: 135.99276733398438, Learning Rate: 0.01\n",
      "Epoch [804/20000], Loss: 135.9072265625, Learning Rate: 0.01\n",
      "Epoch [805/20000], Loss: 135.82164001464844, Learning Rate: 0.01\n",
      "Epoch [806/20000], Loss: 135.73648071289062, Learning Rate: 0.01\n",
      "Epoch [807/20000], Loss: 135.65097045898438, Learning Rate: 0.01\n",
      "Epoch [808/20000], Loss: 135.56546020507812, Learning Rate: 0.01\n",
      "Epoch [809/20000], Loss: 135.47996520996094, Learning Rate: 0.01\n",
      "Epoch [810/20000], Loss: 135.3945770263672, Learning Rate: 0.01\n",
      "Epoch [811/20000], Loss: 135.30909729003906, Learning Rate: 0.01\n",
      "Epoch [812/20000], Loss: 135.22335815429688, Learning Rate: 0.01\n",
      "Epoch [813/20000], Loss: 135.1379852294922, Learning Rate: 0.01\n",
      "Epoch [814/20000], Loss: 135.05221557617188, Learning Rate: 0.01\n",
      "Epoch [815/20000], Loss: 134.96685791015625, Learning Rate: 0.01\n",
      "Epoch [816/20000], Loss: 134.88134765625, Learning Rate: 0.01\n",
      "Epoch [817/20000], Loss: 134.79574584960938, Learning Rate: 0.01\n",
      "Epoch [818/20000], Loss: 134.7100830078125, Learning Rate: 0.01\n",
      "Epoch [819/20000], Loss: 134.6243896484375, Learning Rate: 0.01\n",
      "Epoch [820/20000], Loss: 134.538818359375, Learning Rate: 0.01\n",
      "Epoch [821/20000], Loss: 134.45327758789062, Learning Rate: 0.01\n",
      "Epoch [822/20000], Loss: 134.36756896972656, Learning Rate: 0.01\n",
      "Epoch [823/20000], Loss: 134.28199768066406, Learning Rate: 0.01\n",
      "Epoch [824/20000], Loss: 134.1962890625, Learning Rate: 0.01\n",
      "Epoch [825/20000], Loss: 134.1107177734375, Learning Rate: 0.01\n",
      "Epoch [826/20000], Loss: 134.02490234375, Learning Rate: 0.01\n",
      "Epoch [827/20000], Loss: 133.93948364257812, Learning Rate: 0.01\n",
      "Epoch [828/20000], Loss: 133.853759765625, Learning Rate: 0.01\n",
      "Epoch [829/20000], Loss: 133.76800537109375, Learning Rate: 0.01\n",
      "Epoch [830/20000], Loss: 133.682373046875, Learning Rate: 0.01\n",
      "Epoch [831/20000], Loss: 133.59689331054688, Learning Rate: 0.01\n",
      "Epoch [832/20000], Loss: 133.51133728027344, Learning Rate: 0.01\n",
      "Epoch [833/20000], Loss: 133.425537109375, Learning Rate: 0.01\n",
      "Epoch [834/20000], Loss: 133.33990478515625, Learning Rate: 0.01\n",
      "Epoch [835/20000], Loss: 133.25439453125, Learning Rate: 0.01\n",
      "Epoch [836/20000], Loss: 133.16873168945312, Learning Rate: 0.01\n",
      "Epoch [837/20000], Loss: 133.08316040039062, Learning Rate: 0.01\n",
      "Epoch [838/20000], Loss: 132.9974365234375, Learning Rate: 0.01\n",
      "Epoch [839/20000], Loss: 132.91201782226562, Learning Rate: 0.01\n",
      "Epoch [840/20000], Loss: 132.826416015625, Learning Rate: 0.01\n",
      "Epoch [841/20000], Loss: 132.7406463623047, Learning Rate: 0.01\n",
      "Epoch [842/20000], Loss: 132.65516662597656, Learning Rate: 0.01\n",
      "Epoch [843/20000], Loss: 132.56948852539062, Learning Rate: 0.01\n",
      "Epoch [844/20000], Loss: 132.4840087890625, Learning Rate: 0.01\n",
      "Epoch [845/20000], Loss: 132.39865112304688, Learning Rate: 0.01\n",
      "Epoch [846/20000], Loss: 132.31298828125, Learning Rate: 0.01\n",
      "Epoch [847/20000], Loss: 132.22764587402344, Learning Rate: 0.01\n",
      "Epoch [848/20000], Loss: 132.14218139648438, Learning Rate: 0.01\n",
      "Epoch [849/20000], Loss: 132.0567626953125, Learning Rate: 0.01\n",
      "Epoch [850/20000], Loss: 131.97154235839844, Learning Rate: 0.01\n",
      "Epoch [851/20000], Loss: 131.88616943359375, Learning Rate: 0.01\n",
      "Epoch [852/20000], Loss: 131.80059814453125, Learning Rate: 0.01\n",
      "Epoch [853/20000], Loss: 131.71546936035156, Learning Rate: 0.01\n",
      "Epoch [854/20000], Loss: 131.6300048828125, Learning Rate: 0.01\n",
      "Epoch [855/20000], Loss: 131.5445556640625, Learning Rate: 0.01\n",
      "Epoch [856/20000], Loss: 131.45956420898438, Learning Rate: 0.01\n",
      "Epoch [857/20000], Loss: 131.374267578125, Learning Rate: 0.01\n",
      "Epoch [858/20000], Loss: 131.2890625, Learning Rate: 0.01\n",
      "Epoch [859/20000], Loss: 131.2039794921875, Learning Rate: 0.01\n",
      "Epoch [860/20000], Loss: 131.11886596679688, Learning Rate: 0.01\n",
      "Epoch [861/20000], Loss: 131.03382873535156, Learning Rate: 0.01\n",
      "Epoch [862/20000], Loss: 130.94863891601562, Learning Rate: 0.01\n",
      "Epoch [863/20000], Loss: 130.86370849609375, Learning Rate: 0.01\n",
      "Epoch [864/20000], Loss: 130.77859497070312, Learning Rate: 0.01\n",
      "Epoch [865/20000], Loss: 130.69400024414062, Learning Rate: 0.01\n",
      "Epoch [866/20000], Loss: 130.60891723632812, Learning Rate: 0.01\n",
      "Epoch [867/20000], Loss: 130.52407836914062, Learning Rate: 0.01\n",
      "Epoch [868/20000], Loss: 130.43936157226562, Learning Rate: 0.01\n",
      "Epoch [869/20000], Loss: 130.35455322265625, Learning Rate: 0.01\n",
      "Epoch [870/20000], Loss: 130.26992797851562, Learning Rate: 0.01\n",
      "Epoch [871/20000], Loss: 130.18533325195312, Learning Rate: 0.01\n",
      "Epoch [872/20000], Loss: 130.10052490234375, Learning Rate: 0.01\n",
      "Epoch [873/20000], Loss: 130.01611328125, Learning Rate: 0.01\n",
      "Epoch [874/20000], Loss: 129.93148803710938, Learning Rate: 0.01\n",
      "Epoch [875/20000], Loss: 129.84707641601562, Learning Rate: 0.01\n",
      "Epoch [876/20000], Loss: 129.76263427734375, Learning Rate: 0.01\n",
      "Epoch [877/20000], Loss: 129.67837524414062, Learning Rate: 0.01\n",
      "Epoch [878/20000], Loss: 129.5941162109375, Learning Rate: 0.01\n",
      "Epoch [879/20000], Loss: 129.50991821289062, Learning Rate: 0.01\n",
      "Epoch [880/20000], Loss: 129.4254150390625, Learning Rate: 0.01\n",
      "Epoch [881/20000], Loss: 129.34124755859375, Learning Rate: 0.01\n",
      "Epoch [882/20000], Loss: 129.25723266601562, Learning Rate: 0.01\n",
      "Epoch [883/20000], Loss: 129.17332458496094, Learning Rate: 0.01\n",
      "Epoch [884/20000], Loss: 129.08944702148438, Learning Rate: 0.01\n",
      "Epoch [885/20000], Loss: 129.0055694580078, Learning Rate: 0.01\n",
      "Epoch [886/20000], Loss: 128.9217071533203, Learning Rate: 0.01\n",
      "Epoch [887/20000], Loss: 128.837890625, Learning Rate: 0.01\n",
      "Epoch [888/20000], Loss: 128.75430297851562, Learning Rate: 0.01\n",
      "Epoch [889/20000], Loss: 128.67059326171875, Learning Rate: 0.01\n",
      "Epoch [890/20000], Loss: 128.58718872070312, Learning Rate: 0.01\n",
      "Epoch [891/20000], Loss: 128.50347900390625, Learning Rate: 0.01\n",
      "Epoch [892/20000], Loss: 128.42025756835938, Learning Rate: 0.01\n",
      "Epoch [893/20000], Loss: 128.33680725097656, Learning Rate: 0.01\n",
      "Epoch [894/20000], Loss: 128.25369262695312, Learning Rate: 0.01\n",
      "Epoch [895/20000], Loss: 128.1705322265625, Learning Rate: 0.01\n",
      "Epoch [896/20000], Loss: 128.0872802734375, Learning Rate: 0.01\n",
      "Epoch [897/20000], Loss: 128.00436401367188, Learning Rate: 0.01\n",
      "Epoch [898/20000], Loss: 127.9212417602539, Learning Rate: 0.01\n",
      "Epoch [899/20000], Loss: 127.83840942382812, Learning Rate: 0.01\n",
      "Epoch [900/20000], Loss: 127.75569152832031, Learning Rate: 0.01\n",
      "Epoch [901/20000], Loss: 127.67283630371094, Learning Rate: 0.01\n",
      "Epoch [902/20000], Loss: 127.59011840820312, Learning Rate: 0.01\n",
      "Epoch [903/20000], Loss: 127.50775146484375, Learning Rate: 0.01\n",
      "Epoch [904/20000], Loss: 127.42524719238281, Learning Rate: 0.01\n",
      "Epoch [905/20000], Loss: 127.3428955078125, Learning Rate: 0.01\n",
      "Epoch [906/20000], Loss: 127.26070404052734, Learning Rate: 0.01\n",
      "Epoch [907/20000], Loss: 127.17835235595703, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [908/20000], Loss: 127.09632873535156, Learning Rate: 0.01\n",
      "Epoch [909/20000], Loss: 127.01425170898438, Learning Rate: 0.01\n",
      "Epoch [910/20000], Loss: 126.9324951171875, Learning Rate: 0.01\n",
      "Epoch [911/20000], Loss: 126.85049438476562, Learning Rate: 0.01\n",
      "Epoch [912/20000], Loss: 126.76864624023438, Learning Rate: 0.01\n",
      "Epoch [913/20000], Loss: 126.68724060058594, Learning Rate: 0.01\n",
      "Epoch [914/20000], Loss: 126.6056137084961, Learning Rate: 0.01\n",
      "Epoch [915/20000], Loss: 126.52405548095703, Learning Rate: 0.01\n",
      "Epoch [916/20000], Loss: 126.44282531738281, Learning Rate: 0.01\n",
      "Epoch [917/20000], Loss: 126.36116790771484, Learning Rate: 0.01\n",
      "Epoch [918/20000], Loss: 126.2802734375, Learning Rate: 0.01\n",
      "Epoch [919/20000], Loss: 126.19922637939453, Learning Rate: 0.01\n",
      "Epoch [920/20000], Loss: 126.1181869506836, Learning Rate: 0.01\n",
      "Epoch [921/20000], Loss: 126.03742218017578, Learning Rate: 0.01\n",
      "Epoch [922/20000], Loss: 125.95663452148438, Learning Rate: 0.01\n",
      "Epoch [923/20000], Loss: 125.87588500976562, Learning Rate: 0.01\n",
      "Epoch [924/20000], Loss: 125.79541778564453, Learning Rate: 0.01\n",
      "Epoch [925/20000], Loss: 125.71504974365234, Learning Rate: 0.01\n",
      "Epoch [926/20000], Loss: 125.63449096679688, Learning Rate: 0.01\n",
      "Epoch [927/20000], Loss: 125.55425262451172, Learning Rate: 0.01\n",
      "Epoch [928/20000], Loss: 125.47418212890625, Learning Rate: 0.01\n",
      "Epoch [929/20000], Loss: 125.39408874511719, Learning Rate: 0.01\n",
      "Epoch [930/20000], Loss: 125.31400299072266, Learning Rate: 0.01\n",
      "Epoch [931/20000], Loss: 125.2343521118164, Learning Rate: 0.01\n",
      "Epoch [932/20000], Loss: 125.1545639038086, Learning Rate: 0.01\n",
      "Epoch [933/20000], Loss: 125.07504272460938, Learning Rate: 0.01\n",
      "Epoch [934/20000], Loss: 124.99553680419922, Learning Rate: 0.01\n",
      "Epoch [935/20000], Loss: 124.91613006591797, Learning Rate: 0.01\n",
      "Epoch [936/20000], Loss: 124.83671569824219, Learning Rate: 0.01\n",
      "Epoch [937/20000], Loss: 124.75787353515625, Learning Rate: 0.01\n",
      "Epoch [938/20000], Loss: 124.67874908447266, Learning Rate: 0.01\n",
      "Epoch [939/20000], Loss: 124.59971618652344, Learning Rate: 0.01\n",
      "Epoch [940/20000], Loss: 124.52083587646484, Learning Rate: 0.01\n",
      "Epoch [941/20000], Loss: 124.4422836303711, Learning Rate: 0.01\n",
      "Epoch [942/20000], Loss: 124.36371612548828, Learning Rate: 0.01\n",
      "Epoch [943/20000], Loss: 124.28524780273438, Learning Rate: 0.01\n",
      "Epoch [944/20000], Loss: 124.2068862915039, Learning Rate: 0.01\n",
      "Epoch [945/20000], Loss: 124.12883758544922, Learning Rate: 0.01\n",
      "Epoch [946/20000], Loss: 124.0505142211914, Learning Rate: 0.01\n",
      "Epoch [947/20000], Loss: 123.97262573242188, Learning Rate: 0.01\n",
      "Epoch [948/20000], Loss: 123.89473724365234, Learning Rate: 0.01\n",
      "Epoch [949/20000], Loss: 123.81695556640625, Learning Rate: 0.01\n",
      "Epoch [950/20000], Loss: 123.73945617675781, Learning Rate: 0.01\n",
      "Epoch [951/20000], Loss: 123.66178894042969, Learning Rate: 0.01\n",
      "Epoch [952/20000], Loss: 123.58444213867188, Learning Rate: 0.01\n",
      "Epoch [953/20000], Loss: 123.50720977783203, Learning Rate: 0.01\n",
      "Epoch [954/20000], Loss: 123.4300537109375, Learning Rate: 0.01\n",
      "Epoch [955/20000], Loss: 123.35305786132812, Learning Rate: 0.01\n",
      "Epoch [956/20000], Loss: 123.27605438232422, Learning Rate: 0.01\n",
      "Epoch [957/20000], Loss: 123.19943237304688, Learning Rate: 0.01\n",
      "Epoch [958/20000], Loss: 123.12291717529297, Learning Rate: 0.01\n",
      "Epoch [959/20000], Loss: 123.04634094238281, Learning Rate: 0.01\n",
      "Epoch [960/20000], Loss: 122.96996307373047, Learning Rate: 0.01\n",
      "Epoch [961/20000], Loss: 122.89384460449219, Learning Rate: 0.01\n",
      "Epoch [962/20000], Loss: 122.81759643554688, Learning Rate: 0.01\n",
      "Epoch [963/20000], Loss: 122.74177551269531, Learning Rate: 0.01\n",
      "Epoch [964/20000], Loss: 122.66563415527344, Learning Rate: 0.01\n",
      "Epoch [965/20000], Loss: 122.58988952636719, Learning Rate: 0.01\n",
      "Epoch [966/20000], Loss: 122.51448822021484, Learning Rate: 0.01\n",
      "Epoch [967/20000], Loss: 122.43892669677734, Learning Rate: 0.01\n",
      "Epoch [968/20000], Loss: 122.36359405517578, Learning Rate: 0.01\n",
      "Epoch [969/20000], Loss: 122.28826904296875, Learning Rate: 0.01\n",
      "Epoch [970/20000], Loss: 122.21324920654297, Learning Rate: 0.01\n",
      "Epoch [971/20000], Loss: 122.13829803466797, Learning Rate: 0.01\n",
      "Epoch [972/20000], Loss: 122.0632553100586, Learning Rate: 0.01\n",
      "Epoch [973/20000], Loss: 121.98882293701172, Learning Rate: 0.01\n",
      "Epoch [974/20000], Loss: 121.91397857666016, Learning Rate: 0.01\n",
      "Epoch [975/20000], Loss: 121.83956909179688, Learning Rate: 0.01\n",
      "Epoch [976/20000], Loss: 121.76541137695312, Learning Rate: 0.01\n",
      "Epoch [977/20000], Loss: 121.69122314453125, Learning Rate: 0.01\n",
      "Epoch [978/20000], Loss: 121.61725616455078, Learning Rate: 0.01\n",
      "Epoch [979/20000], Loss: 121.54332733154297, Learning Rate: 0.01\n",
      "Epoch [980/20000], Loss: 121.4696044921875, Learning Rate: 0.01\n",
      "Epoch [981/20000], Loss: 121.39590454101562, Learning Rate: 0.01\n",
      "Epoch [982/20000], Loss: 121.322265625, Learning Rate: 0.01\n",
      "Epoch [983/20000], Loss: 121.2488021850586, Learning Rate: 0.01\n",
      "Epoch [984/20000], Loss: 121.17572784423828, Learning Rate: 0.01\n",
      "Epoch [985/20000], Loss: 121.10244750976562, Learning Rate: 0.01\n",
      "Epoch [986/20000], Loss: 121.02960205078125, Learning Rate: 0.01\n",
      "Epoch [987/20000], Loss: 120.95658874511719, Learning Rate: 0.01\n",
      "Epoch [988/20000], Loss: 120.8840560913086, Learning Rate: 0.01\n",
      "Epoch [989/20000], Loss: 120.81153106689453, Learning Rate: 0.01\n",
      "Epoch [990/20000], Loss: 120.73905944824219, Learning Rate: 0.01\n",
      "Epoch [991/20000], Loss: 120.66661071777344, Learning Rate: 0.01\n",
      "Epoch [992/20000], Loss: 120.59464263916016, Learning Rate: 0.01\n",
      "Epoch [993/20000], Loss: 120.52244567871094, Learning Rate: 0.01\n",
      "Epoch [994/20000], Loss: 120.45056915283203, Learning Rate: 0.01\n",
      "Epoch [995/20000], Loss: 120.37887573242188, Learning Rate: 0.01\n",
      "Epoch [996/20000], Loss: 120.3071060180664, Learning Rate: 0.01\n",
      "Epoch [997/20000], Loss: 120.23571014404297, Learning Rate: 0.01\n",
      "Epoch [998/20000], Loss: 120.16407775878906, Learning Rate: 0.01\n",
      "Epoch [999/20000], Loss: 120.09309387207031, Learning Rate: 0.01\n",
      "Epoch [1000/20000], Loss: 120.02191925048828, Learning Rate: 0.01\n",
      "Epoch [1001/20000], Loss: 119.95089721679688, Learning Rate: 0.01\n",
      "Epoch [1002/20000], Loss: 119.88019561767578, Learning Rate: 0.01\n",
      "Epoch [1003/20000], Loss: 119.80938720703125, Learning Rate: 0.01\n",
      "Epoch [1004/20000], Loss: 119.73861694335938, Learning Rate: 0.01\n",
      "Epoch [1005/20000], Loss: 119.6683120727539, Learning Rate: 0.01\n",
      "Epoch [1006/20000], Loss: 119.59788513183594, Learning Rate: 0.01\n",
      "Epoch [1007/20000], Loss: 119.52787780761719, Learning Rate: 0.01\n",
      "Epoch [1008/20000], Loss: 119.45793914794922, Learning Rate: 0.01\n",
      "Epoch [1009/20000], Loss: 119.38787078857422, Learning Rate: 0.01\n",
      "Epoch [1010/20000], Loss: 119.31814575195312, Learning Rate: 0.01\n",
      "Epoch [1011/20000], Loss: 119.24851989746094, Learning Rate: 0.01\n",
      "Epoch [1012/20000], Loss: 119.1790542602539, Learning Rate: 0.01\n",
      "Epoch [1013/20000], Loss: 119.1097640991211, Learning Rate: 0.01\n",
      "Epoch [1014/20000], Loss: 119.0404052734375, Learning Rate: 0.01\n",
      "Epoch [1015/20000], Loss: 118.97148132324219, Learning Rate: 0.01\n",
      "Epoch [1016/20000], Loss: 118.9024429321289, Learning Rate: 0.01\n",
      "Epoch [1017/20000], Loss: 118.83346557617188, Learning Rate: 0.01\n",
      "Epoch [1018/20000], Loss: 118.76470947265625, Learning Rate: 0.01\n",
      "Epoch [1019/20000], Loss: 118.69608306884766, Learning Rate: 0.01\n",
      "Epoch [1020/20000], Loss: 118.62786865234375, Learning Rate: 0.01\n",
      "Epoch [1021/20000], Loss: 118.55956268310547, Learning Rate: 0.01\n",
      "Epoch [1022/20000], Loss: 118.49138641357422, Learning Rate: 0.01\n",
      "Epoch [1023/20000], Loss: 118.42321014404297, Learning Rate: 0.01\n",
      "Epoch [1024/20000], Loss: 118.35523223876953, Learning Rate: 0.01\n",
      "Epoch [1025/20000], Loss: 118.28759765625, Learning Rate: 0.01\n",
      "Epoch [1026/20000], Loss: 118.2198257446289, Learning Rate: 0.01\n",
      "Epoch [1027/20000], Loss: 118.15241241455078, Learning Rate: 0.01\n",
      "Epoch [1028/20000], Loss: 118.08496856689453, Learning Rate: 0.01\n",
      "Epoch [1029/20000], Loss: 118.01792907714844, Learning Rate: 0.01\n",
      "Epoch [1030/20000], Loss: 117.95072174072266, Learning Rate: 0.01\n",
      "Epoch [1031/20000], Loss: 117.8835220336914, Learning Rate: 0.01\n",
      "Epoch [1032/20000], Loss: 117.8167953491211, Learning Rate: 0.01\n",
      "Epoch [1033/20000], Loss: 117.75013732910156, Learning Rate: 0.01\n",
      "Epoch [1034/20000], Loss: 117.68341827392578, Learning Rate: 0.01\n",
      "Epoch [1035/20000], Loss: 117.61708068847656, Learning Rate: 0.01\n",
      "Epoch [1036/20000], Loss: 117.55071258544922, Learning Rate: 0.01\n",
      "Epoch [1037/20000], Loss: 117.4845962524414, Learning Rate: 0.01\n",
      "Epoch [1038/20000], Loss: 117.41840362548828, Learning Rate: 0.01\n",
      "Epoch [1039/20000], Loss: 117.35246276855469, Learning Rate: 0.01\n",
      "Epoch [1040/20000], Loss: 117.28644561767578, Learning Rate: 0.01\n",
      "Epoch [1041/20000], Loss: 117.22101593017578, Learning Rate: 0.01\n",
      "Epoch [1042/20000], Loss: 117.15524291992188, Learning Rate: 0.01\n",
      "Epoch [1043/20000], Loss: 117.08979797363281, Learning Rate: 0.01\n",
      "Epoch [1044/20000], Loss: 117.0243911743164, Learning Rate: 0.01\n",
      "Epoch [1045/20000], Loss: 116.9594497680664, Learning Rate: 0.01\n",
      "Epoch [1046/20000], Loss: 116.89440155029297, Learning Rate: 0.01\n",
      "Epoch [1047/20000], Loss: 116.82955932617188, Learning Rate: 0.01\n",
      "Epoch [1048/20000], Loss: 116.76465606689453, Learning Rate: 0.01\n",
      "Epoch [1049/20000], Loss: 116.69996643066406, Learning Rate: 0.01\n",
      "Epoch [1050/20000], Loss: 116.63522338867188, Learning Rate: 0.01\n",
      "Epoch [1051/20000], Loss: 116.57091522216797, Learning Rate: 0.01\n",
      "Epoch [1052/20000], Loss: 116.50663757324219, Learning Rate: 0.01\n",
      "Epoch [1053/20000], Loss: 116.4423599243164, Learning Rate: 0.01\n",
      "Epoch [1054/20000], Loss: 116.37841033935547, Learning Rate: 0.01\n",
      "Epoch [1055/20000], Loss: 116.31455993652344, Learning Rate: 0.01\n",
      "Epoch [1056/20000], Loss: 116.25053405761719, Learning Rate: 0.01\n",
      "Epoch [1057/20000], Loss: 116.18681335449219, Learning Rate: 0.01\n",
      "Epoch [1058/20000], Loss: 116.12340545654297, Learning Rate: 0.01\n",
      "Epoch [1059/20000], Loss: 116.06007385253906, Learning Rate: 0.01\n",
      "Epoch [1060/20000], Loss: 115.99647521972656, Learning Rate: 0.01\n",
      "Epoch [1061/20000], Loss: 115.93345642089844, Learning Rate: 0.01\n",
      "Epoch [1062/20000], Loss: 115.87055206298828, Learning Rate: 0.01\n",
      "Epoch [1063/20000], Loss: 115.80741119384766, Learning Rate: 0.01\n",
      "Epoch [1064/20000], Loss: 115.74451446533203, Learning Rate: 0.01\n",
      "Epoch [1065/20000], Loss: 115.68181610107422, Learning Rate: 0.01\n",
      "Epoch [1066/20000], Loss: 115.61915588378906, Learning Rate: 0.01\n",
      "Epoch [1067/20000], Loss: 115.55685424804688, Learning Rate: 0.01\n",
      "Epoch [1068/20000], Loss: 115.49456024169922, Learning Rate: 0.01\n",
      "Epoch [1069/20000], Loss: 115.43241119384766, Learning Rate: 0.01\n",
      "Epoch [1070/20000], Loss: 115.3702163696289, Learning Rate: 0.01\n",
      "Epoch [1071/20000], Loss: 115.30826568603516, Learning Rate: 0.01\n",
      "Epoch [1072/20000], Loss: 115.24642944335938, Learning Rate: 0.01\n",
      "Epoch [1073/20000], Loss: 115.18453979492188, Learning Rate: 0.01\n",
      "Epoch [1074/20000], Loss: 115.12313842773438, Learning Rate: 0.01\n",
      "Epoch [1075/20000], Loss: 115.06144714355469, Learning Rate: 0.01\n",
      "Epoch [1076/20000], Loss: 115.00003814697266, Learning Rate: 0.01\n",
      "Epoch [1077/20000], Loss: 114.9388198852539, Learning Rate: 0.01\n",
      "Epoch [1078/20000], Loss: 114.87772369384766, Learning Rate: 0.01\n",
      "Epoch [1079/20000], Loss: 114.81656646728516, Learning Rate: 0.01\n",
      "Epoch [1080/20000], Loss: 114.75569152832031, Learning Rate: 0.01\n",
      "Epoch [1081/20000], Loss: 114.69480895996094, Learning Rate: 0.01\n",
      "Epoch [1082/20000], Loss: 114.6342544555664, Learning Rate: 0.01\n",
      "Epoch [1083/20000], Loss: 114.57357025146484, Learning Rate: 0.01\n",
      "Epoch [1084/20000], Loss: 114.51341247558594, Learning Rate: 0.01\n",
      "Epoch [1085/20000], Loss: 114.45272064208984, Learning Rate: 0.01\n",
      "Epoch [1086/20000], Loss: 114.3924331665039, Learning Rate: 0.01\n",
      "Epoch [1087/20000], Loss: 114.33252716064453, Learning Rate: 0.01\n",
      "Epoch [1088/20000], Loss: 114.27237701416016, Learning Rate: 0.01\n",
      "Epoch [1089/20000], Loss: 114.21233367919922, Learning Rate: 0.01\n",
      "Epoch [1090/20000], Loss: 114.15241241455078, Learning Rate: 0.01\n",
      "Epoch [1091/20000], Loss: 114.09296417236328, Learning Rate: 0.01\n",
      "Epoch [1092/20000], Loss: 114.0333023071289, Learning Rate: 0.01\n",
      "Epoch [1093/20000], Loss: 113.97396087646484, Learning Rate: 0.01\n",
      "Epoch [1094/20000], Loss: 113.91463470458984, Learning Rate: 0.01\n",
      "Epoch [1095/20000], Loss: 113.85541534423828, Learning Rate: 0.01\n",
      "Epoch [1096/20000], Loss: 113.79618072509766, Learning Rate: 0.01\n",
      "Epoch [1097/20000], Loss: 113.73719024658203, Learning Rate: 0.01\n",
      "Epoch [1098/20000], Loss: 113.67828369140625, Learning Rate: 0.01\n",
      "Epoch [1099/20000], Loss: 113.61940002441406, Learning Rate: 0.01\n",
      "Epoch [1100/20000], Loss: 113.56096649169922, Learning Rate: 0.01\n",
      "Epoch [1101/20000], Loss: 113.50213623046875, Learning Rate: 0.01\n",
      "Epoch [1102/20000], Loss: 113.44373321533203, Learning Rate: 0.01\n",
      "Epoch [1103/20000], Loss: 113.38536834716797, Learning Rate: 0.01\n",
      "Epoch [1104/20000], Loss: 113.32695007324219, Learning Rate: 0.01\n",
      "Epoch [1105/20000], Loss: 113.26892852783203, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1106/20000], Loss: 113.21089172363281, Learning Rate: 0.01\n",
      "Epoch [1107/20000], Loss: 113.15302276611328, Learning Rate: 0.01\n",
      "Epoch [1108/20000], Loss: 113.09517669677734, Learning Rate: 0.01\n",
      "Epoch [1109/20000], Loss: 113.03741455078125, Learning Rate: 0.01\n",
      "Epoch [1110/20000], Loss: 112.97972106933594, Learning Rate: 0.01\n",
      "Epoch [1111/20000], Loss: 112.92222595214844, Learning Rate: 0.01\n",
      "Epoch [1112/20000], Loss: 112.86466979980469, Learning Rate: 0.01\n",
      "Epoch [1113/20000], Loss: 112.80742645263672, Learning Rate: 0.01\n",
      "Epoch [1114/20000], Loss: 112.7500228881836, Learning Rate: 0.01\n",
      "Epoch [1115/20000], Loss: 112.69306945800781, Learning Rate: 0.01\n",
      "Epoch [1116/20000], Loss: 112.6359634399414, Learning Rate: 0.01\n",
      "Epoch [1117/20000], Loss: 112.5791015625, Learning Rate: 0.01\n",
      "Epoch [1118/20000], Loss: 112.52220916748047, Learning Rate: 0.01\n",
      "Epoch [1119/20000], Loss: 112.46556091308594, Learning Rate: 0.01\n",
      "Epoch [1120/20000], Loss: 112.40900421142578, Learning Rate: 0.01\n",
      "Epoch [1121/20000], Loss: 112.3524398803711, Learning Rate: 0.01\n",
      "Epoch [1122/20000], Loss: 112.29596710205078, Learning Rate: 0.01\n",
      "Epoch [1123/20000], Loss: 112.23974609375, Learning Rate: 0.01\n",
      "Epoch [1124/20000], Loss: 112.18353271484375, Learning Rate: 0.01\n",
      "Epoch [1125/20000], Loss: 112.12738800048828, Learning Rate: 0.01\n",
      "Epoch [1126/20000], Loss: 112.07119750976562, Learning Rate: 0.01\n",
      "Epoch [1127/20000], Loss: 112.01521301269531, Learning Rate: 0.01\n",
      "Epoch [1128/20000], Loss: 111.9594497680664, Learning Rate: 0.01\n",
      "Epoch [1129/20000], Loss: 111.90371704101562, Learning Rate: 0.01\n",
      "Epoch [1130/20000], Loss: 111.84811401367188, Learning Rate: 0.01\n",
      "Epoch [1131/20000], Loss: 111.79256439208984, Learning Rate: 0.01\n",
      "Epoch [1132/20000], Loss: 111.73705291748047, Learning Rate: 0.01\n",
      "Epoch [1133/20000], Loss: 111.6817398071289, Learning Rate: 0.01\n",
      "Epoch [1134/20000], Loss: 111.62643432617188, Learning Rate: 0.01\n",
      "Epoch [1135/20000], Loss: 111.57123565673828, Learning Rate: 0.01\n",
      "Epoch [1136/20000], Loss: 111.51606750488281, Learning Rate: 0.01\n",
      "Epoch [1137/20000], Loss: 111.4612045288086, Learning Rate: 0.01\n",
      "Epoch [1138/20000], Loss: 111.40621948242188, Learning Rate: 0.01\n",
      "Epoch [1139/20000], Loss: 111.35164642333984, Learning Rate: 0.01\n",
      "Epoch [1140/20000], Loss: 111.29676818847656, Learning Rate: 0.01\n",
      "Epoch [1141/20000], Loss: 111.2423095703125, Learning Rate: 0.01\n",
      "Epoch [1142/20000], Loss: 111.18766021728516, Learning Rate: 0.01\n",
      "Epoch [1143/20000], Loss: 111.13325500488281, Learning Rate: 0.01\n",
      "Epoch [1144/20000], Loss: 111.07872009277344, Learning Rate: 0.01\n",
      "Epoch [1145/20000], Loss: 111.02471923828125, Learning Rate: 0.01\n",
      "Epoch [1146/20000], Loss: 110.9705581665039, Learning Rate: 0.01\n",
      "Epoch [1147/20000], Loss: 110.91639709472656, Learning Rate: 0.01\n",
      "Epoch [1148/20000], Loss: 110.86250305175781, Learning Rate: 0.01\n",
      "Epoch [1149/20000], Loss: 110.80853271484375, Learning Rate: 0.01\n",
      "Epoch [1150/20000], Loss: 110.7547378540039, Learning Rate: 0.01\n",
      "Epoch [1151/20000], Loss: 110.70093536376953, Learning Rate: 0.01\n",
      "Epoch [1152/20000], Loss: 110.64745330810547, Learning Rate: 0.01\n",
      "Epoch [1153/20000], Loss: 110.59378051757812, Learning Rate: 0.01\n",
      "Epoch [1154/20000], Loss: 110.54033660888672, Learning Rate: 0.01\n",
      "Epoch [1155/20000], Loss: 110.487060546875, Learning Rate: 0.01\n",
      "Epoch [1156/20000], Loss: 110.43379211425781, Learning Rate: 0.01\n",
      "Epoch [1157/20000], Loss: 110.38054656982422, Learning Rate: 0.01\n",
      "Epoch [1158/20000], Loss: 110.327392578125, Learning Rate: 0.01\n",
      "Epoch [1159/20000], Loss: 110.27434539794922, Learning Rate: 0.01\n",
      "Epoch [1160/20000], Loss: 110.22146606445312, Learning Rate: 0.01\n",
      "Epoch [1161/20000], Loss: 110.16857147216797, Learning Rate: 0.01\n",
      "Epoch [1162/20000], Loss: 110.1157455444336, Learning Rate: 0.01\n",
      "Epoch [1163/20000], Loss: 110.0630874633789, Learning Rate: 0.01\n",
      "Epoch [1164/20000], Loss: 110.01061248779297, Learning Rate: 0.01\n",
      "Epoch [1165/20000], Loss: 109.95791625976562, Learning Rate: 0.01\n",
      "Epoch [1166/20000], Loss: 109.90542602539062, Learning Rate: 0.01\n",
      "Epoch [1167/20000], Loss: 109.8531494140625, Learning Rate: 0.01\n",
      "Epoch [1168/20000], Loss: 109.80079650878906, Learning Rate: 0.01\n",
      "Epoch [1169/20000], Loss: 109.74858093261719, Learning Rate: 0.01\n",
      "Epoch [1170/20000], Loss: 109.69632720947266, Learning Rate: 0.01\n",
      "Epoch [1171/20000], Loss: 109.64440155029297, Learning Rate: 0.01\n",
      "Epoch [1172/20000], Loss: 109.59233093261719, Learning Rate: 0.01\n",
      "Epoch [1173/20000], Loss: 109.54065704345703, Learning Rate: 0.01\n",
      "Epoch [1174/20000], Loss: 109.48859405517578, Learning Rate: 0.01\n",
      "Epoch [1175/20000], Loss: 109.43705749511719, Learning Rate: 0.01\n",
      "Epoch [1176/20000], Loss: 109.38521575927734, Learning Rate: 0.01\n",
      "Epoch [1177/20000], Loss: 109.33394622802734, Learning Rate: 0.01\n",
      "Epoch [1178/20000], Loss: 109.2823257446289, Learning Rate: 0.01\n",
      "Epoch [1179/20000], Loss: 109.23088836669922, Learning Rate: 0.01\n",
      "Epoch [1180/20000], Loss: 109.17936706542969, Learning Rate: 0.01\n",
      "Epoch [1181/20000], Loss: 109.12805938720703, Learning Rate: 0.01\n",
      "Epoch [1182/20000], Loss: 109.07719421386719, Learning Rate: 0.01\n",
      "Epoch [1183/20000], Loss: 109.02590942382812, Learning Rate: 0.01\n",
      "Epoch [1184/20000], Loss: 108.9748764038086, Learning Rate: 0.01\n",
      "Epoch [1185/20000], Loss: 108.92372131347656, Learning Rate: 0.01\n",
      "Epoch [1186/20000], Loss: 108.87291717529297, Learning Rate: 0.01\n",
      "Epoch [1187/20000], Loss: 108.82225036621094, Learning Rate: 0.01\n",
      "Epoch [1188/20000], Loss: 108.77152252197266, Learning Rate: 0.01\n",
      "Epoch [1189/20000], Loss: 108.72077941894531, Learning Rate: 0.01\n",
      "Epoch [1190/20000], Loss: 108.67013549804688, Learning Rate: 0.01\n",
      "Epoch [1191/20000], Loss: 108.61966705322266, Learning Rate: 0.01\n",
      "Epoch [1192/20000], Loss: 108.56910705566406, Learning Rate: 0.01\n",
      "Epoch [1193/20000], Loss: 108.51863861083984, Learning Rate: 0.01\n",
      "Epoch [1194/20000], Loss: 108.46849060058594, Learning Rate: 0.01\n",
      "Epoch [1195/20000], Loss: 108.41846466064453, Learning Rate: 0.01\n",
      "Epoch [1196/20000], Loss: 108.36817932128906, Learning Rate: 0.01\n",
      "Epoch [1197/20000], Loss: 108.31800842285156, Learning Rate: 0.01\n",
      "Epoch [1198/20000], Loss: 108.26788330078125, Learning Rate: 0.01\n",
      "Epoch [1199/20000], Loss: 108.2179946899414, Learning Rate: 0.01\n",
      "Epoch [1200/20000], Loss: 108.16809844970703, Learning Rate: 0.01\n",
      "Epoch [1201/20000], Loss: 108.11820983886719, Learning Rate: 0.01\n",
      "Epoch [1202/20000], Loss: 108.06865692138672, Learning Rate: 0.01\n",
      "Epoch [1203/20000], Loss: 108.01885223388672, Learning Rate: 0.01\n",
      "Epoch [1204/20000], Loss: 107.96920013427734, Learning Rate: 0.01\n",
      "Epoch [1205/20000], Loss: 107.91967010498047, Learning Rate: 0.01\n",
      "Epoch [1206/20000], Loss: 107.87018585205078, Learning Rate: 0.01\n",
      "Epoch [1207/20000], Loss: 107.82058715820312, Learning Rate: 0.01\n",
      "Epoch [1208/20000], Loss: 107.77127838134766, Learning Rate: 0.01\n",
      "Epoch [1209/20000], Loss: 107.72219848632812, Learning Rate: 0.01\n",
      "Epoch [1210/20000], Loss: 107.67279815673828, Learning Rate: 0.01\n",
      "Epoch [1211/20000], Loss: 107.6236572265625, Learning Rate: 0.01\n",
      "Epoch [1212/20000], Loss: 107.57455444335938, Learning Rate: 0.01\n",
      "Epoch [1213/20000], Loss: 107.52557373046875, Learning Rate: 0.01\n",
      "Epoch [1214/20000], Loss: 107.47650909423828, Learning Rate: 0.01\n",
      "Epoch [1215/20000], Loss: 107.42755889892578, Learning Rate: 0.01\n",
      "Epoch [1216/20000], Loss: 107.37905883789062, Learning Rate: 0.01\n",
      "Epoch [1217/20000], Loss: 107.33013153076172, Learning Rate: 0.01\n",
      "Epoch [1218/20000], Loss: 107.28150939941406, Learning Rate: 0.01\n",
      "Epoch [1219/20000], Loss: 107.23280334472656, Learning Rate: 0.01\n",
      "Epoch [1220/20000], Loss: 107.18427276611328, Learning Rate: 0.01\n",
      "Epoch [1221/20000], Loss: 107.13589477539062, Learning Rate: 0.01\n",
      "Epoch [1222/20000], Loss: 107.08744049072266, Learning Rate: 0.01\n",
      "Epoch [1223/20000], Loss: 107.03894805908203, Learning Rate: 0.01\n",
      "Epoch [1224/20000], Loss: 106.99071502685547, Learning Rate: 0.01\n",
      "Epoch [1225/20000], Loss: 106.94245147705078, Learning Rate: 0.01\n",
      "Epoch [1226/20000], Loss: 106.89427947998047, Learning Rate: 0.01\n",
      "Epoch [1227/20000], Loss: 106.84605407714844, Learning Rate: 0.01\n",
      "Epoch [1228/20000], Loss: 106.79808044433594, Learning Rate: 0.01\n",
      "Epoch [1229/20000], Loss: 106.75000762939453, Learning Rate: 0.01\n",
      "Epoch [1230/20000], Loss: 106.70217895507812, Learning Rate: 0.01\n",
      "Epoch [1231/20000], Loss: 106.65411376953125, Learning Rate: 0.01\n",
      "Epoch [1232/20000], Loss: 106.60625457763672, Learning Rate: 0.01\n",
      "Epoch [1233/20000], Loss: 106.5586929321289, Learning Rate: 0.01\n",
      "Epoch [1234/20000], Loss: 106.51097106933594, Learning Rate: 0.01\n",
      "Epoch [1235/20000], Loss: 106.46321105957031, Learning Rate: 0.01\n",
      "Epoch [1236/20000], Loss: 106.41563415527344, Learning Rate: 0.01\n",
      "Epoch [1237/20000], Loss: 106.3681640625, Learning Rate: 0.01\n",
      "Epoch [1238/20000], Loss: 106.3204345703125, Learning Rate: 0.01\n",
      "Epoch [1239/20000], Loss: 106.27314758300781, Learning Rate: 0.01\n",
      "Epoch [1240/20000], Loss: 106.22563171386719, Learning Rate: 0.01\n",
      "Epoch [1241/20000], Loss: 106.17857360839844, Learning Rate: 0.01\n",
      "Epoch [1242/20000], Loss: 106.13108825683594, Learning Rate: 0.01\n",
      "Epoch [1243/20000], Loss: 106.08391571044922, Learning Rate: 0.01\n",
      "Epoch [1244/20000], Loss: 106.03678131103516, Learning Rate: 0.01\n",
      "Epoch [1245/20000], Loss: 105.9896011352539, Learning Rate: 0.01\n",
      "Epoch [1246/20000], Loss: 105.94239807128906, Learning Rate: 0.01\n",
      "Epoch [1247/20000], Loss: 105.89542388916016, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1248/20000], Loss: 105.84867858886719, Learning Rate: 0.01\n",
      "Epoch [1249/20000], Loss: 105.80169677734375, Learning Rate: 0.01\n",
      "Epoch [1250/20000], Loss: 105.75497436523438, Learning Rate: 0.01\n",
      "Epoch [1251/20000], Loss: 105.70807647705078, Learning Rate: 0.01\n",
      "Epoch [1252/20000], Loss: 105.66155242919922, Learning Rate: 0.01\n",
      "Epoch [1253/20000], Loss: 105.61490631103516, Learning Rate: 0.01\n",
      "Epoch [1254/20000], Loss: 105.56810760498047, Learning Rate: 0.01\n",
      "Epoch [1255/20000], Loss: 105.52174377441406, Learning Rate: 0.01\n",
      "Epoch [1256/20000], Loss: 105.4750747680664, Learning Rate: 0.01\n",
      "Epoch [1257/20000], Loss: 105.42876434326172, Learning Rate: 0.01\n",
      "Epoch [1258/20000], Loss: 105.38217163085938, Learning Rate: 0.01\n",
      "Epoch [1259/20000], Loss: 105.33588409423828, Learning Rate: 0.01\n",
      "Epoch [1260/20000], Loss: 105.28956604003906, Learning Rate: 0.01\n",
      "Epoch [1261/20000], Loss: 105.2433090209961, Learning Rate: 0.01\n",
      "Epoch [1262/20000], Loss: 105.19713592529297, Learning Rate: 0.01\n",
      "Epoch [1263/20000], Loss: 105.15104675292969, Learning Rate: 0.01\n",
      "Epoch [1264/20000], Loss: 105.1050033569336, Learning Rate: 0.01\n",
      "Epoch [1265/20000], Loss: 105.05889129638672, Learning Rate: 0.01\n",
      "Epoch [1266/20000], Loss: 105.01299285888672, Learning Rate: 0.01\n",
      "Epoch [1267/20000], Loss: 104.9668960571289, Learning Rate: 0.01\n",
      "Epoch [1268/20000], Loss: 104.92096710205078, Learning Rate: 0.01\n",
      "Epoch [1269/20000], Loss: 104.8750991821289, Learning Rate: 0.01\n",
      "Epoch [1270/20000], Loss: 104.82926177978516, Learning Rate: 0.01\n",
      "Epoch [1271/20000], Loss: 104.78375244140625, Learning Rate: 0.01\n",
      "Epoch [1272/20000], Loss: 104.73786163330078, Learning Rate: 0.01\n",
      "Epoch [1273/20000], Loss: 104.69214630126953, Learning Rate: 0.01\n",
      "Epoch [1274/20000], Loss: 104.64657592773438, Learning Rate: 0.01\n",
      "Epoch [1275/20000], Loss: 104.60110473632812, Learning Rate: 0.01\n",
      "Epoch [1276/20000], Loss: 104.55558776855469, Learning Rate: 0.01\n",
      "Epoch [1277/20000], Loss: 104.51016998291016, Learning Rate: 0.01\n",
      "Epoch [1278/20000], Loss: 104.4646987915039, Learning Rate: 0.01\n",
      "Epoch [1279/20000], Loss: 104.41927337646484, Learning Rate: 0.01\n",
      "Epoch [1280/20000], Loss: 104.37393188476562, Learning Rate: 0.01\n",
      "Epoch [1281/20000], Loss: 104.32881927490234, Learning Rate: 0.01\n",
      "Epoch [1282/20000], Loss: 104.28348541259766, Learning Rate: 0.01\n",
      "Epoch [1283/20000], Loss: 104.23835754394531, Learning Rate: 0.01\n",
      "Epoch [1284/20000], Loss: 104.19318389892578, Learning Rate: 0.01\n",
      "Epoch [1285/20000], Loss: 104.14811706542969, Learning Rate: 0.01\n",
      "Epoch [1286/20000], Loss: 104.10294342041016, Learning Rate: 0.01\n",
      "Epoch [1287/20000], Loss: 104.05806732177734, Learning Rate: 0.01\n",
      "Epoch [1288/20000], Loss: 104.01300811767578, Learning Rate: 0.01\n",
      "Epoch [1289/20000], Loss: 103.96814727783203, Learning Rate: 0.01\n",
      "Epoch [1290/20000], Loss: 103.92315673828125, Learning Rate: 0.01\n",
      "Epoch [1291/20000], Loss: 103.87848663330078, Learning Rate: 0.01\n",
      "Epoch [1292/20000], Loss: 103.83367919921875, Learning Rate: 0.01\n",
      "Epoch [1293/20000], Loss: 103.78885650634766, Learning Rate: 0.01\n",
      "Epoch [1294/20000], Loss: 103.74422454833984, Learning Rate: 0.01\n",
      "Epoch [1295/20000], Loss: 103.69947052001953, Learning Rate: 0.01\n",
      "Epoch [1296/20000], Loss: 103.65473175048828, Learning Rate: 0.01\n",
      "Epoch [1297/20000], Loss: 103.61014556884766, Learning Rate: 0.01\n",
      "Epoch [1298/20000], Loss: 103.5654525756836, Learning Rate: 0.01\n",
      "Epoch [1299/20000], Loss: 103.52115631103516, Learning Rate: 0.01\n",
      "Epoch [1300/20000], Loss: 103.47683715820312, Learning Rate: 0.01\n",
      "Epoch [1301/20000], Loss: 103.4323501586914, Learning Rate: 0.01\n",
      "Epoch [1302/20000], Loss: 103.3880615234375, Learning Rate: 0.01\n",
      "Epoch [1303/20000], Loss: 103.3436050415039, Learning Rate: 0.01\n",
      "Epoch [1304/20000], Loss: 103.29948425292969, Learning Rate: 0.01\n",
      "Epoch [1305/20000], Loss: 103.25513458251953, Learning Rate: 0.01\n",
      "Epoch [1306/20000], Loss: 103.21066284179688, Learning Rate: 0.01\n",
      "Epoch [1307/20000], Loss: 103.16692352294922, Learning Rate: 0.01\n",
      "Epoch [1308/20000], Loss: 103.12248229980469, Learning Rate: 0.01\n",
      "Epoch [1309/20000], Loss: 103.07866668701172, Learning Rate: 0.01\n",
      "Epoch [1310/20000], Loss: 103.03436279296875, Learning Rate: 0.01\n",
      "Epoch [1311/20000], Loss: 102.99052429199219, Learning Rate: 0.01\n",
      "Epoch [1312/20000], Loss: 102.94647979736328, Learning Rate: 0.01\n",
      "Epoch [1313/20000], Loss: 102.9024887084961, Learning Rate: 0.01\n",
      "Epoch [1314/20000], Loss: 102.8586196899414, Learning Rate: 0.01\n",
      "Epoch [1315/20000], Loss: 102.81476593017578, Learning Rate: 0.01\n",
      "Epoch [1316/20000], Loss: 102.77098846435547, Learning Rate: 0.01\n",
      "Epoch [1317/20000], Loss: 102.72724914550781, Learning Rate: 0.01\n",
      "Epoch [1318/20000], Loss: 102.68348693847656, Learning Rate: 0.01\n",
      "Epoch [1319/20000], Loss: 102.63957214355469, Learning Rate: 0.01\n",
      "Epoch [1320/20000], Loss: 102.59629821777344, Learning Rate: 0.01\n",
      "Epoch [1321/20000], Loss: 102.55242156982422, Learning Rate: 0.01\n",
      "Epoch [1322/20000], Loss: 102.50859832763672, Learning Rate: 0.01\n",
      "Epoch [1323/20000], Loss: 102.46551513671875, Learning Rate: 0.01\n",
      "Epoch [1324/20000], Loss: 102.42176055908203, Learning Rate: 0.01\n",
      "Epoch [1325/20000], Loss: 102.37824249267578, Learning Rate: 0.01\n",
      "Epoch [1326/20000], Loss: 102.33483123779297, Learning Rate: 0.01\n",
      "Epoch [1327/20000], Loss: 102.29127502441406, Learning Rate: 0.01\n",
      "Epoch [1328/20000], Loss: 102.24819946289062, Learning Rate: 0.01\n",
      "Epoch [1329/20000], Loss: 102.2046127319336, Learning Rate: 0.01\n",
      "Epoch [1330/20000], Loss: 102.16141510009766, Learning Rate: 0.01\n",
      "Epoch [1331/20000], Loss: 102.11824798583984, Learning Rate: 0.01\n",
      "Epoch [1332/20000], Loss: 102.07485961914062, Learning Rate: 0.01\n",
      "Epoch [1333/20000], Loss: 102.03176879882812, Learning Rate: 0.01\n",
      "Epoch [1334/20000], Loss: 101.98841094970703, Learning Rate: 0.01\n",
      "Epoch [1335/20000], Loss: 101.94557189941406, Learning Rate: 0.01\n",
      "Epoch [1336/20000], Loss: 101.90230560302734, Learning Rate: 0.01\n",
      "Epoch [1337/20000], Loss: 101.85917663574219, Learning Rate: 0.01\n",
      "Epoch [1338/20000], Loss: 101.81635284423828, Learning Rate: 0.01\n",
      "Epoch [1339/20000], Loss: 101.77325439453125, Learning Rate: 0.01\n",
      "Epoch [1340/20000], Loss: 101.73020935058594, Learning Rate: 0.01\n",
      "Epoch [1341/20000], Loss: 101.68729400634766, Learning Rate: 0.01\n",
      "Epoch [1342/20000], Loss: 101.64449310302734, Learning Rate: 0.01\n",
      "Epoch [1343/20000], Loss: 101.6014633178711, Learning Rate: 0.01\n",
      "Epoch [1344/20000], Loss: 101.55866241455078, Learning Rate: 0.01\n",
      "Epoch [1345/20000], Loss: 101.51602935791016, Learning Rate: 0.01\n",
      "Epoch [1346/20000], Loss: 101.4729995727539, Learning Rate: 0.01\n",
      "Epoch [1347/20000], Loss: 101.43050384521484, Learning Rate: 0.01\n",
      "Epoch [1348/20000], Loss: 101.3878402709961, Learning Rate: 0.01\n",
      "Epoch [1349/20000], Loss: 101.34523010253906, Learning Rate: 0.01\n",
      "Epoch [1350/20000], Loss: 101.30247497558594, Learning Rate: 0.01\n",
      "Epoch [1351/20000], Loss: 101.25984191894531, Learning Rate: 0.01\n",
      "Epoch [1352/20000], Loss: 101.21744537353516, Learning Rate: 0.01\n",
      "Epoch [1353/20000], Loss: 101.17471313476562, Learning Rate: 0.01\n",
      "Epoch [1354/20000], Loss: 101.13233184814453, Learning Rate: 0.01\n",
      "Epoch [1355/20000], Loss: 101.0898208618164, Learning Rate: 0.01\n",
      "Epoch [1356/20000], Loss: 101.04742431640625, Learning Rate: 0.01\n",
      "Epoch [1357/20000], Loss: 101.00505828857422, Learning Rate: 0.01\n",
      "Epoch [1358/20000], Loss: 100.96256256103516, Learning Rate: 0.01\n",
      "Epoch [1359/20000], Loss: 100.92023468017578, Learning Rate: 0.01\n",
      "Epoch [1360/20000], Loss: 100.87771606445312, Learning Rate: 0.01\n",
      "Epoch [1361/20000], Loss: 100.83538818359375, Learning Rate: 0.01\n",
      "Epoch [1362/20000], Loss: 100.79329681396484, Learning Rate: 0.01\n",
      "Epoch [1363/20000], Loss: 100.75106811523438, Learning Rate: 0.01\n",
      "Epoch [1364/20000], Loss: 100.7087173461914, Learning Rate: 0.01\n",
      "Epoch [1365/20000], Loss: 100.66657257080078, Learning Rate: 0.01\n",
      "Epoch [1366/20000], Loss: 100.62452697753906, Learning Rate: 0.01\n",
      "Epoch [1367/20000], Loss: 100.58212280273438, Learning Rate: 0.01\n",
      "Epoch [1368/20000], Loss: 100.54039001464844, Learning Rate: 0.01\n",
      "Epoch [1369/20000], Loss: 100.4983139038086, Learning Rate: 0.01\n",
      "Epoch [1370/20000], Loss: 100.4559555053711, Learning Rate: 0.01\n",
      "Epoch [1371/20000], Loss: 100.41417694091797, Learning Rate: 0.01\n",
      "Epoch [1372/20000], Loss: 100.37226104736328, Learning Rate: 0.01\n",
      "Epoch [1373/20000], Loss: 100.33021545410156, Learning Rate: 0.01\n",
      "Epoch [1374/20000], Loss: 100.2884521484375, Learning Rate: 0.01\n",
      "Epoch [1375/20000], Loss: 100.2464828491211, Learning Rate: 0.01\n",
      "Epoch [1376/20000], Loss: 100.20482635498047, Learning Rate: 0.01\n",
      "Epoch [1377/20000], Loss: 100.16278839111328, Learning Rate: 0.01\n",
      "Epoch [1378/20000], Loss: 100.12100219726562, Learning Rate: 0.01\n",
      "Epoch [1379/20000], Loss: 100.07921600341797, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1380/20000], Loss: 100.0376205444336, Learning Rate: 0.01\n",
      "Epoch [1381/20000], Loss: 99.99588775634766, Learning Rate: 0.01\n",
      "Epoch [1382/20000], Loss: 99.95413970947266, Learning Rate: 0.01\n",
      "Epoch [1383/20000], Loss: 99.91216278076172, Learning Rate: 0.01\n",
      "Epoch [1384/20000], Loss: 99.87068176269531, Learning Rate: 0.01\n",
      "Epoch [1385/20000], Loss: 99.8290023803711, Learning Rate: 0.01\n",
      "Epoch [1386/20000], Loss: 99.78758239746094, Learning Rate: 0.01\n",
      "Epoch [1387/20000], Loss: 99.74591064453125, Learning Rate: 0.01\n",
      "Epoch [1388/20000], Loss: 99.70425415039062, Learning Rate: 0.01\n",
      "Epoch [1389/20000], Loss: 99.66297149658203, Learning Rate: 0.01\n",
      "Epoch [1390/20000], Loss: 99.62126159667969, Learning Rate: 0.01\n",
      "Epoch [1391/20000], Loss: 99.57980346679688, Learning Rate: 0.01\n",
      "Epoch [1392/20000], Loss: 99.53874206542969, Learning Rate: 0.01\n",
      "Epoch [1393/20000], Loss: 99.49700164794922, Learning Rate: 0.01\n",
      "Epoch [1394/20000], Loss: 99.45584106445312, Learning Rate: 0.01\n",
      "Epoch [1395/20000], Loss: 99.41447448730469, Learning Rate: 0.01\n",
      "Epoch [1396/20000], Loss: 99.37313079833984, Learning Rate: 0.01\n",
      "Epoch [1397/20000], Loss: 99.3318862915039, Learning Rate: 0.01\n",
      "Epoch [1398/20000], Loss: 99.29048156738281, Learning Rate: 0.01\n",
      "Epoch [1399/20000], Loss: 99.24952697753906, Learning Rate: 0.01\n",
      "Epoch [1400/20000], Loss: 99.2080078125, Learning Rate: 0.01\n",
      "Epoch [1401/20000], Loss: 99.16682434082031, Learning Rate: 0.01\n",
      "Epoch [1402/20000], Loss: 99.12560272216797, Learning Rate: 0.01\n",
      "Epoch [1403/20000], Loss: 99.08441162109375, Learning Rate: 0.01\n",
      "Epoch [1404/20000], Loss: 99.0434341430664, Learning Rate: 0.01\n",
      "Epoch [1405/20000], Loss: 99.00215911865234, Learning Rate: 0.01\n",
      "Epoch [1406/20000], Loss: 98.96125030517578, Learning Rate: 0.01\n",
      "Epoch [1407/20000], Loss: 98.92010498046875, Learning Rate: 0.01\n",
      "Epoch [1408/20000], Loss: 98.87911224365234, Learning Rate: 0.01\n",
      "Epoch [1409/20000], Loss: 98.83789825439453, Learning Rate: 0.01\n",
      "Epoch [1410/20000], Loss: 98.79696655273438, Learning Rate: 0.01\n",
      "Epoch [1411/20000], Loss: 98.75611114501953, Learning Rate: 0.01\n",
      "Epoch [1412/20000], Loss: 98.71527099609375, Learning Rate: 0.01\n",
      "Epoch [1413/20000], Loss: 98.6742172241211, Learning Rate: 0.01\n",
      "Epoch [1414/20000], Loss: 98.63337707519531, Learning Rate: 0.01\n",
      "Epoch [1415/20000], Loss: 98.59245300292969, Learning Rate: 0.01\n",
      "Epoch [1416/20000], Loss: 98.55168914794922, Learning Rate: 0.01\n",
      "Epoch [1417/20000], Loss: 98.51092529296875, Learning Rate: 0.01\n",
      "Epoch [1418/20000], Loss: 98.4700698852539, Learning Rate: 0.01\n",
      "Epoch [1419/20000], Loss: 98.42916107177734, Learning Rate: 0.01\n",
      "Epoch [1420/20000], Loss: 98.38853454589844, Learning Rate: 0.01\n",
      "Epoch [1421/20000], Loss: 98.34773254394531, Learning Rate: 0.01\n",
      "Epoch [1422/20000], Loss: 98.30704498291016, Learning Rate: 0.01\n",
      "Epoch [1423/20000], Loss: 98.26615142822266, Learning Rate: 0.01\n",
      "Epoch [1424/20000], Loss: 98.22566986083984, Learning Rate: 0.01\n",
      "Epoch [1425/20000], Loss: 98.18506622314453, Learning Rate: 0.01\n",
      "Epoch [1426/20000], Loss: 98.14434814453125, Learning Rate: 0.01\n",
      "Epoch [1427/20000], Loss: 98.10398864746094, Learning Rate: 0.01\n",
      "Epoch [1428/20000], Loss: 98.06334686279297, Learning Rate: 0.01\n",
      "Epoch [1429/20000], Loss: 98.02273559570312, Learning Rate: 0.01\n",
      "Epoch [1430/20000], Loss: 97.98223114013672, Learning Rate: 0.01\n",
      "Epoch [1431/20000], Loss: 97.94184112548828, Learning Rate: 0.01\n",
      "Epoch [1432/20000], Loss: 97.90123748779297, Learning Rate: 0.01\n",
      "Epoch [1433/20000], Loss: 97.8607177734375, Learning Rate: 0.01\n",
      "Epoch [1434/20000], Loss: 97.82032775878906, Learning Rate: 0.01\n",
      "Epoch [1435/20000], Loss: 97.77987670898438, Learning Rate: 0.01\n",
      "Epoch [1436/20000], Loss: 97.73926544189453, Learning Rate: 0.01\n",
      "Epoch [1437/20000], Loss: 97.69898986816406, Learning Rate: 0.01\n",
      "Epoch [1438/20000], Loss: 97.6584701538086, Learning Rate: 0.01\n",
      "Epoch [1439/20000], Loss: 97.61829376220703, Learning Rate: 0.01\n",
      "Epoch [1440/20000], Loss: 97.57794189453125, Learning Rate: 0.01\n",
      "Epoch [1441/20000], Loss: 97.53760528564453, Learning Rate: 0.01\n",
      "Epoch [1442/20000], Loss: 97.49729919433594, Learning Rate: 0.01\n",
      "Epoch [1443/20000], Loss: 97.45726013183594, Learning Rate: 0.01\n",
      "Epoch [1444/20000], Loss: 97.416748046875, Learning Rate: 0.01\n",
      "Epoch [1445/20000], Loss: 97.37687683105469, Learning Rate: 0.01\n",
      "Epoch [1446/20000], Loss: 97.33643341064453, Learning Rate: 0.01\n",
      "Epoch [1447/20000], Loss: 97.29624938964844, Learning Rate: 0.01\n",
      "Epoch [1448/20000], Loss: 97.25631713867188, Learning Rate: 0.01\n",
      "Epoch [1449/20000], Loss: 97.21599578857422, Learning Rate: 0.01\n",
      "Epoch [1450/20000], Loss: 97.17578887939453, Learning Rate: 0.01\n",
      "Epoch [1451/20000], Loss: 97.13573455810547, Learning Rate: 0.01\n",
      "Epoch [1452/20000], Loss: 97.09568786621094, Learning Rate: 0.01\n",
      "Epoch [1453/20000], Loss: 97.05559539794922, Learning Rate: 0.01\n",
      "Epoch [1454/20000], Loss: 97.01569366455078, Learning Rate: 0.01\n",
      "Epoch [1455/20000], Loss: 96.97554016113281, Learning Rate: 0.01\n",
      "Epoch [1456/20000], Loss: 96.9356460571289, Learning Rate: 0.01\n",
      "Epoch [1457/20000], Loss: 96.89547729492188, Learning Rate: 0.01\n",
      "Epoch [1458/20000], Loss: 96.85568237304688, Learning Rate: 0.01\n",
      "Epoch [1459/20000], Loss: 96.8155746459961, Learning Rate: 0.01\n",
      "Epoch [1460/20000], Loss: 96.7757339477539, Learning Rate: 0.01\n",
      "Epoch [1461/20000], Loss: 96.73584747314453, Learning Rate: 0.01\n",
      "Epoch [1462/20000], Loss: 96.69587707519531, Learning Rate: 0.01\n",
      "Epoch [1463/20000], Loss: 96.65589904785156, Learning Rate: 0.01\n",
      "Epoch [1464/20000], Loss: 96.61624908447266, Learning Rate: 0.01\n",
      "Epoch [1465/20000], Loss: 96.57643127441406, Learning Rate: 0.01\n",
      "Epoch [1466/20000], Loss: 96.5365219116211, Learning Rate: 0.01\n",
      "Epoch [1467/20000], Loss: 96.4964370727539, Learning Rate: 0.01\n",
      "Epoch [1468/20000], Loss: 96.45702362060547, Learning Rate: 0.01\n",
      "Epoch [1469/20000], Loss: 96.4172592163086, Learning Rate: 0.01\n",
      "Epoch [1470/20000], Loss: 96.37737274169922, Learning Rate: 0.01\n",
      "Epoch [1471/20000], Loss: 96.33777618408203, Learning Rate: 0.01\n",
      "Epoch [1472/20000], Loss: 96.2980728149414, Learning Rate: 0.01\n",
      "Epoch [1473/20000], Loss: 96.2583999633789, Learning Rate: 0.01\n",
      "Epoch [1474/20000], Loss: 96.21858978271484, Learning Rate: 0.01\n",
      "Epoch [1475/20000], Loss: 96.17877960205078, Learning Rate: 0.01\n",
      "Epoch [1476/20000], Loss: 96.13934326171875, Learning Rate: 0.01\n",
      "Epoch [1477/20000], Loss: 96.09962463378906, Learning Rate: 0.01\n",
      "Epoch [1478/20000], Loss: 96.06005859375, Learning Rate: 0.01\n",
      "Epoch [1479/20000], Loss: 96.02059936523438, Learning Rate: 0.01\n",
      "Epoch [1480/20000], Loss: 95.98088836669922, Learning Rate: 0.01\n",
      "Epoch [1481/20000], Loss: 95.94151306152344, Learning Rate: 0.01\n",
      "Epoch [1482/20000], Loss: 95.90198516845703, Learning Rate: 0.01\n",
      "Epoch [1483/20000], Loss: 95.86239624023438, Learning Rate: 0.01\n",
      "Epoch [1484/20000], Loss: 95.82286834716797, Learning Rate: 0.01\n",
      "Epoch [1485/20000], Loss: 95.78328704833984, Learning Rate: 0.01\n",
      "Epoch [1486/20000], Loss: 95.74409484863281, Learning Rate: 0.01\n",
      "Epoch [1487/20000], Loss: 95.7046127319336, Learning Rate: 0.01\n",
      "Epoch [1488/20000], Loss: 95.66514587402344, Learning Rate: 0.01\n",
      "Epoch [1489/20000], Loss: 95.6257095336914, Learning Rate: 0.01\n",
      "Epoch [1490/20000], Loss: 95.58638000488281, Learning Rate: 0.01\n",
      "Epoch [1491/20000], Loss: 95.54692077636719, Learning Rate: 0.01\n",
      "Epoch [1492/20000], Loss: 95.50753784179688, Learning Rate: 0.01\n",
      "Epoch [1493/20000], Loss: 95.46820831298828, Learning Rate: 0.01\n",
      "Epoch [1494/20000], Loss: 95.42902374267578, Learning Rate: 0.01\n",
      "Epoch [1495/20000], Loss: 95.3897476196289, Learning Rate: 0.01\n",
      "Epoch [1496/20000], Loss: 95.35040283203125, Learning Rate: 0.01\n",
      "Epoch [1497/20000], Loss: 95.31102752685547, Learning Rate: 0.01\n",
      "Epoch [1498/20000], Loss: 95.27183532714844, Learning Rate: 0.01\n",
      "Epoch [1499/20000], Loss: 95.23234558105469, Learning Rate: 0.01\n",
      "Epoch [1500/20000], Loss: 95.19329833984375, Learning Rate: 0.01\n",
      "Epoch [1501/20000], Loss: 95.15411376953125, Learning Rate: 0.01\n",
      "Epoch [1502/20000], Loss: 95.1150894165039, Learning Rate: 0.01\n",
      "Epoch [1503/20000], Loss: 95.07571411132812, Learning Rate: 0.01\n",
      "Epoch [1504/20000], Loss: 95.03658294677734, Learning Rate: 0.01\n",
      "Epoch [1505/20000], Loss: 94.99742889404297, Learning Rate: 0.01\n",
      "Epoch [1506/20000], Loss: 94.9581069946289, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1507/20000], Loss: 94.91923522949219, Learning Rate: 0.01\n",
      "Epoch [1508/20000], Loss: 94.88014221191406, Learning Rate: 0.01\n",
      "Epoch [1509/20000], Loss: 94.8409652709961, Learning Rate: 0.01\n",
      "Epoch [1510/20000], Loss: 94.80200958251953, Learning Rate: 0.01\n",
      "Epoch [1511/20000], Loss: 94.76275634765625, Learning Rate: 0.01\n",
      "Epoch [1512/20000], Loss: 94.72386932373047, Learning Rate: 0.01\n",
      "Epoch [1513/20000], Loss: 94.68489837646484, Learning Rate: 0.01\n",
      "Epoch [1514/20000], Loss: 94.64582824707031, Learning Rate: 0.01\n",
      "Epoch [1515/20000], Loss: 94.60679626464844, Learning Rate: 0.01\n",
      "Epoch [1516/20000], Loss: 94.56790161132812, Learning Rate: 0.01\n",
      "Epoch [1517/20000], Loss: 94.5289535522461, Learning Rate: 0.01\n",
      "Epoch [1518/20000], Loss: 94.4899673461914, Learning Rate: 0.01\n",
      "Epoch [1519/20000], Loss: 94.45111083984375, Learning Rate: 0.01\n",
      "Epoch [1520/20000], Loss: 94.41213989257812, Learning Rate: 0.01\n",
      "Epoch [1521/20000], Loss: 94.37332153320312, Learning Rate: 0.01\n",
      "Epoch [1522/20000], Loss: 94.3344497680664, Learning Rate: 0.01\n",
      "Epoch [1523/20000], Loss: 94.29529571533203, Learning Rate: 0.01\n",
      "Epoch [1524/20000], Loss: 94.25662994384766, Learning Rate: 0.01\n",
      "Epoch [1525/20000], Loss: 94.2178726196289, Learning Rate: 0.01\n",
      "Epoch [1526/20000], Loss: 94.17903137207031, Learning Rate: 0.01\n",
      "Epoch [1527/20000], Loss: 94.14024353027344, Learning Rate: 0.01\n",
      "Epoch [1528/20000], Loss: 94.10137176513672, Learning Rate: 0.01\n",
      "Epoch [1529/20000], Loss: 94.06271362304688, Learning Rate: 0.01\n",
      "Epoch [1530/20000], Loss: 94.02391815185547, Learning Rate: 0.01\n",
      "Epoch [1531/20000], Loss: 93.985107421875, Learning Rate: 0.01\n",
      "Epoch [1532/20000], Loss: 93.94632720947266, Learning Rate: 0.01\n",
      "Epoch [1533/20000], Loss: 93.90796661376953, Learning Rate: 0.01\n",
      "Epoch [1534/20000], Loss: 93.86904907226562, Learning Rate: 0.01\n",
      "Epoch [1535/20000], Loss: 93.83039855957031, Learning Rate: 0.01\n",
      "Epoch [1536/20000], Loss: 93.7914810180664, Learning Rate: 0.01\n",
      "Epoch [1537/20000], Loss: 93.75311279296875, Learning Rate: 0.01\n",
      "Epoch [1538/20000], Loss: 93.71442413330078, Learning Rate: 0.01\n",
      "Epoch [1539/20000], Loss: 93.67578887939453, Learning Rate: 0.01\n",
      "Epoch [1540/20000], Loss: 93.63733673095703, Learning Rate: 0.01\n",
      "Epoch [1541/20000], Loss: 93.59846496582031, Learning Rate: 0.01\n",
      "Epoch [1542/20000], Loss: 93.55996704101562, Learning Rate: 0.01\n",
      "Epoch [1543/20000], Loss: 93.52140808105469, Learning Rate: 0.01\n",
      "Epoch [1544/20000], Loss: 93.48265838623047, Learning Rate: 0.01\n",
      "Epoch [1545/20000], Loss: 93.44422149658203, Learning Rate: 0.01\n",
      "Epoch [1546/20000], Loss: 93.40574645996094, Learning Rate: 0.01\n",
      "Epoch [1547/20000], Loss: 93.36721801757812, Learning Rate: 0.01\n",
      "Epoch [1548/20000], Loss: 93.32867431640625, Learning Rate: 0.01\n",
      "Epoch [1549/20000], Loss: 93.29022979736328, Learning Rate: 0.01\n",
      "Epoch [1550/20000], Loss: 93.25186157226562, Learning Rate: 0.01\n",
      "Epoch [1551/20000], Loss: 93.2133560180664, Learning Rate: 0.01\n",
      "Epoch [1552/20000], Loss: 93.1747817993164, Learning Rate: 0.01\n",
      "Epoch [1553/20000], Loss: 93.1364974975586, Learning Rate: 0.01\n",
      "Epoch [1554/20000], Loss: 93.09807586669922, Learning Rate: 0.01\n",
      "Epoch [1555/20000], Loss: 93.05963134765625, Learning Rate: 0.01\n",
      "Epoch [1556/20000], Loss: 93.02128601074219, Learning Rate: 0.01\n",
      "Epoch [1557/20000], Loss: 92.98291015625, Learning Rate: 0.01\n",
      "Epoch [1558/20000], Loss: 92.94445037841797, Learning Rate: 0.01\n",
      "Epoch [1559/20000], Loss: 92.90628051757812, Learning Rate: 0.01\n",
      "Epoch [1560/20000], Loss: 92.86785125732422, Learning Rate: 0.01\n",
      "Epoch [1561/20000], Loss: 92.82952880859375, Learning Rate: 0.01\n",
      "Epoch [1562/20000], Loss: 92.79125213623047, Learning Rate: 0.01\n",
      "Epoch [1563/20000], Loss: 92.7530517578125, Learning Rate: 0.01\n",
      "Epoch [1564/20000], Loss: 92.71467590332031, Learning Rate: 0.01\n",
      "Epoch [1565/20000], Loss: 92.67652130126953, Learning Rate: 0.01\n",
      "Epoch [1566/20000], Loss: 92.63824462890625, Learning Rate: 0.01\n",
      "Epoch [1567/20000], Loss: 92.59998321533203, Learning Rate: 0.01\n",
      "Epoch [1568/20000], Loss: 92.56195831298828, Learning Rate: 0.01\n",
      "Epoch [1569/20000], Loss: 92.5235595703125, Learning Rate: 0.01\n",
      "Epoch [1570/20000], Loss: 92.48548889160156, Learning Rate: 0.01\n",
      "Epoch [1571/20000], Loss: 92.44733428955078, Learning Rate: 0.01\n",
      "Epoch [1572/20000], Loss: 92.40904998779297, Learning Rate: 0.01\n",
      "Epoch [1573/20000], Loss: 92.3709716796875, Learning Rate: 0.01\n",
      "Epoch [1574/20000], Loss: 92.332763671875, Learning Rate: 0.01\n",
      "Epoch [1575/20000], Loss: 92.29471588134766, Learning Rate: 0.01\n",
      "Epoch [1576/20000], Loss: 92.25654602050781, Learning Rate: 0.01\n",
      "Epoch [1577/20000], Loss: 92.21830749511719, Learning Rate: 0.01\n",
      "Epoch [1578/20000], Loss: 92.1803207397461, Learning Rate: 0.01\n",
      "Epoch [1579/20000], Loss: 92.14228057861328, Learning Rate: 0.01\n",
      "Epoch [1580/20000], Loss: 92.10424041748047, Learning Rate: 0.01\n",
      "Epoch [1581/20000], Loss: 92.06625366210938, Learning Rate: 0.01\n",
      "Epoch [1582/20000], Loss: 92.02821350097656, Learning Rate: 0.01\n",
      "Epoch [1583/20000], Loss: 91.98998260498047, Learning Rate: 0.01\n",
      "Epoch [1584/20000], Loss: 91.95236206054688, Learning Rate: 0.01\n",
      "Epoch [1585/20000], Loss: 91.91407775878906, Learning Rate: 0.01\n",
      "Epoch [1586/20000], Loss: 91.87609100341797, Learning Rate: 0.01\n",
      "Epoch [1587/20000], Loss: 91.8381118774414, Learning Rate: 0.01\n",
      "Epoch [1588/20000], Loss: 91.8002700805664, Learning Rate: 0.01\n",
      "Epoch [1589/20000], Loss: 91.7621841430664, Learning Rate: 0.01\n",
      "Epoch [1590/20000], Loss: 91.7242660522461, Learning Rate: 0.01\n",
      "Epoch [1591/20000], Loss: 91.6865234375, Learning Rate: 0.01\n",
      "Epoch [1592/20000], Loss: 91.64839172363281, Learning Rate: 0.01\n",
      "Epoch [1593/20000], Loss: 91.61071014404297, Learning Rate: 0.01\n",
      "Epoch [1594/20000], Loss: 91.5728530883789, Learning Rate: 0.01\n",
      "Epoch [1595/20000], Loss: 91.53497314453125, Learning Rate: 0.01\n",
      "Epoch [1596/20000], Loss: 91.49714660644531, Learning Rate: 0.01\n",
      "Epoch [1597/20000], Loss: 91.45918273925781, Learning Rate: 0.01\n",
      "Epoch [1598/20000], Loss: 91.42142486572266, Learning Rate: 0.01\n",
      "Epoch [1599/20000], Loss: 91.38350677490234, Learning Rate: 0.01\n",
      "Epoch [1600/20000], Loss: 91.3458480834961, Learning Rate: 0.01\n",
      "Epoch [1601/20000], Loss: 91.30795288085938, Learning Rate: 0.01\n",
      "Epoch [1602/20000], Loss: 91.2699966430664, Learning Rate: 0.01\n",
      "Epoch [1603/20000], Loss: 91.23249816894531, Learning Rate: 0.01\n",
      "Epoch [1604/20000], Loss: 91.19486999511719, Learning Rate: 0.01\n",
      "Epoch [1605/20000], Loss: 91.1570816040039, Learning Rate: 0.01\n",
      "Epoch [1606/20000], Loss: 91.1192855834961, Learning Rate: 0.01\n",
      "Epoch [1607/20000], Loss: 91.08164978027344, Learning Rate: 0.01\n",
      "Epoch [1608/20000], Loss: 91.04393768310547, Learning Rate: 0.01\n",
      "Epoch [1609/20000], Loss: 91.00633239746094, Learning Rate: 0.01\n",
      "Epoch [1610/20000], Loss: 90.96856689453125, Learning Rate: 0.01\n",
      "Epoch [1611/20000], Loss: 90.93107604980469, Learning Rate: 0.01\n",
      "Epoch [1612/20000], Loss: 90.89327239990234, Learning Rate: 0.01\n",
      "Epoch [1613/20000], Loss: 90.85564422607422, Learning Rate: 0.01\n",
      "Epoch [1614/20000], Loss: 90.81807708740234, Learning Rate: 0.01\n",
      "Epoch [1615/20000], Loss: 90.78050231933594, Learning Rate: 0.01\n",
      "Epoch [1616/20000], Loss: 90.74287414550781, Learning Rate: 0.01\n",
      "Epoch [1617/20000], Loss: 90.70541381835938, Learning Rate: 0.01\n",
      "Epoch [1618/20000], Loss: 90.66780853271484, Learning Rate: 0.01\n",
      "Epoch [1619/20000], Loss: 90.63002014160156, Learning Rate: 0.01\n",
      "Epoch [1620/20000], Loss: 90.59253692626953, Learning Rate: 0.01\n",
      "Epoch [1621/20000], Loss: 90.55519104003906, Learning Rate: 0.01\n",
      "Epoch [1622/20000], Loss: 90.51762390136719, Learning Rate: 0.01\n",
      "Epoch [1623/20000], Loss: 90.48020935058594, Learning Rate: 0.01\n",
      "Epoch [1624/20000], Loss: 90.44276428222656, Learning Rate: 0.01\n",
      "Epoch [1625/20000], Loss: 90.40526580810547, Learning Rate: 0.01\n",
      "Epoch [1626/20000], Loss: 90.36752319335938, Learning Rate: 0.01\n",
      "Epoch [1627/20000], Loss: 90.33026123046875, Learning Rate: 0.01\n",
      "Epoch [1628/20000], Loss: 90.292724609375, Learning Rate: 0.01\n",
      "Epoch [1629/20000], Loss: 90.25526428222656, Learning Rate: 0.01\n",
      "Epoch [1630/20000], Loss: 90.21800994873047, Learning Rate: 0.01\n",
      "Epoch [1631/20000], Loss: 90.18067932128906, Learning Rate: 0.01\n",
      "Epoch [1632/20000], Loss: 90.14327239990234, Learning Rate: 0.01\n",
      "Epoch [1633/20000], Loss: 90.10578918457031, Learning Rate: 0.01\n",
      "Epoch [1634/20000], Loss: 90.068359375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1635/20000], Loss: 90.03125, Learning Rate: 0.01\n",
      "Epoch [1636/20000], Loss: 89.99372863769531, Learning Rate: 0.01\n",
      "Epoch [1637/20000], Loss: 89.95624542236328, Learning Rate: 0.01\n",
      "Epoch [1638/20000], Loss: 89.91907501220703, Learning Rate: 0.01\n",
      "Epoch [1639/20000], Loss: 89.88175964355469, Learning Rate: 0.01\n",
      "Epoch [1640/20000], Loss: 89.84445190429688, Learning Rate: 0.01\n",
      "Epoch [1641/20000], Loss: 89.80744171142578, Learning Rate: 0.01\n",
      "Epoch [1642/20000], Loss: 89.76991271972656, Learning Rate: 0.01\n",
      "Epoch [1643/20000], Loss: 89.73278045654297, Learning Rate: 0.01\n",
      "Epoch [1644/20000], Loss: 89.69544219970703, Learning Rate: 0.01\n",
      "Epoch [1645/20000], Loss: 89.65821838378906, Learning Rate: 0.01\n",
      "Epoch [1646/20000], Loss: 89.62100219726562, Learning Rate: 0.01\n",
      "Epoch [1647/20000], Loss: 89.58377838134766, Learning Rate: 0.01\n",
      "Epoch [1648/20000], Loss: 89.5467300415039, Learning Rate: 0.01\n",
      "Epoch [1649/20000], Loss: 89.50936889648438, Learning Rate: 0.01\n",
      "Epoch [1650/20000], Loss: 89.47211456298828, Learning Rate: 0.01\n",
      "Epoch [1651/20000], Loss: 89.43517303466797, Learning Rate: 0.01\n",
      "Epoch [1652/20000], Loss: 89.3980484008789, Learning Rate: 0.01\n",
      "Epoch [1653/20000], Loss: 89.36091613769531, Learning Rate: 0.01\n",
      "Epoch [1654/20000], Loss: 89.32376098632812, Learning Rate: 0.01\n",
      "Epoch [1655/20000], Loss: 89.28655242919922, Learning Rate: 0.01\n",
      "Epoch [1656/20000], Loss: 89.24942779541016, Learning Rate: 0.01\n",
      "Epoch [1657/20000], Loss: 89.21247863769531, Learning Rate: 0.01\n",
      "Epoch [1658/20000], Loss: 89.17526245117188, Learning Rate: 0.01\n",
      "Epoch [1659/20000], Loss: 89.1383285522461, Learning Rate: 0.01\n",
      "Epoch [1660/20000], Loss: 89.1013412475586, Learning Rate: 0.01\n",
      "Epoch [1661/20000], Loss: 89.06414031982422, Learning Rate: 0.01\n",
      "Epoch [1662/20000], Loss: 89.02733612060547, Learning Rate: 0.01\n",
      "Epoch [1663/20000], Loss: 88.99021911621094, Learning Rate: 0.01\n",
      "Epoch [1664/20000], Loss: 88.95338439941406, Learning Rate: 0.01\n",
      "Epoch [1665/20000], Loss: 88.9162368774414, Learning Rate: 0.01\n",
      "Epoch [1666/20000], Loss: 88.87914276123047, Learning Rate: 0.01\n",
      "Epoch [1667/20000], Loss: 88.84239959716797, Learning Rate: 0.01\n",
      "Epoch [1668/20000], Loss: 88.80540466308594, Learning Rate: 0.01\n",
      "Epoch [1669/20000], Loss: 88.7684326171875, Learning Rate: 0.01\n",
      "Epoch [1670/20000], Loss: 88.73130798339844, Learning Rate: 0.01\n",
      "Epoch [1671/20000], Loss: 88.6946029663086, Learning Rate: 0.01\n",
      "Epoch [1672/20000], Loss: 88.65755462646484, Learning Rate: 0.01\n",
      "Epoch [1673/20000], Loss: 88.62104797363281, Learning Rate: 0.01\n",
      "Epoch [1674/20000], Loss: 88.5840072631836, Learning Rate: 0.01\n",
      "Epoch [1675/20000], Loss: 88.54701232910156, Learning Rate: 0.01\n",
      "Epoch [1676/20000], Loss: 88.51014709472656, Learning Rate: 0.01\n",
      "Epoch [1677/20000], Loss: 88.47351837158203, Learning Rate: 0.01\n",
      "Epoch [1678/20000], Loss: 88.43659973144531, Learning Rate: 0.01\n",
      "Epoch [1679/20000], Loss: 88.39985656738281, Learning Rate: 0.01\n",
      "Epoch [1680/20000], Loss: 88.36299133300781, Learning Rate: 0.01\n",
      "Epoch [1681/20000], Loss: 88.32623291015625, Learning Rate: 0.01\n",
      "Epoch [1682/20000], Loss: 88.28958129882812, Learning Rate: 0.01\n",
      "Epoch [1683/20000], Loss: 88.25260162353516, Learning Rate: 0.01\n",
      "Epoch [1684/20000], Loss: 88.21583557128906, Learning Rate: 0.01\n",
      "Epoch [1685/20000], Loss: 88.1792221069336, Learning Rate: 0.01\n",
      "Epoch [1686/20000], Loss: 88.142578125, Learning Rate: 0.01\n",
      "Epoch [1687/20000], Loss: 88.10555267333984, Learning Rate: 0.01\n",
      "Epoch [1688/20000], Loss: 88.0689697265625, Learning Rate: 0.01\n",
      "Epoch [1689/20000], Loss: 88.03227233886719, Learning Rate: 0.01\n",
      "Epoch [1690/20000], Loss: 87.99559020996094, Learning Rate: 0.01\n",
      "Epoch [1691/20000], Loss: 87.95893859863281, Learning Rate: 0.01\n",
      "Epoch [1692/20000], Loss: 87.9222183227539, Learning Rate: 0.01\n",
      "Epoch [1693/20000], Loss: 87.88568115234375, Learning Rate: 0.01\n",
      "Epoch [1694/20000], Loss: 87.84903717041016, Learning Rate: 0.01\n",
      "Epoch [1695/20000], Loss: 87.8122787475586, Learning Rate: 0.01\n",
      "Epoch [1696/20000], Loss: 87.77577209472656, Learning Rate: 0.01\n",
      "Epoch [1697/20000], Loss: 87.73931121826172, Learning Rate: 0.01\n",
      "Epoch [1698/20000], Loss: 87.70259094238281, Learning Rate: 0.01\n",
      "Epoch [1699/20000], Loss: 87.66624450683594, Learning Rate: 0.01\n",
      "Epoch [1700/20000], Loss: 87.6293716430664, Learning Rate: 0.01\n",
      "Epoch [1701/20000], Loss: 87.59297180175781, Learning Rate: 0.01\n",
      "Epoch [1702/20000], Loss: 87.55625915527344, Learning Rate: 0.01\n",
      "Epoch [1703/20000], Loss: 87.51997375488281, Learning Rate: 0.01\n",
      "Epoch [1704/20000], Loss: 87.48331451416016, Learning Rate: 0.01\n",
      "Epoch [1705/20000], Loss: 87.44670104980469, Learning Rate: 0.01\n",
      "Epoch [1706/20000], Loss: 87.41024017333984, Learning Rate: 0.01\n",
      "Epoch [1707/20000], Loss: 87.37384033203125, Learning Rate: 0.01\n",
      "Epoch [1708/20000], Loss: 87.33724975585938, Learning Rate: 0.01\n",
      "Epoch [1709/20000], Loss: 87.3010025024414, Learning Rate: 0.01\n",
      "Epoch [1710/20000], Loss: 87.2646484375, Learning Rate: 0.01\n",
      "Epoch [1711/20000], Loss: 87.22819519042969, Learning Rate: 0.01\n",
      "Epoch [1712/20000], Loss: 87.19176483154297, Learning Rate: 0.01\n",
      "Epoch [1713/20000], Loss: 87.15544128417969, Learning Rate: 0.01\n",
      "Epoch [1714/20000], Loss: 87.11882019042969, Learning Rate: 0.01\n",
      "Epoch [1715/20000], Loss: 87.08253479003906, Learning Rate: 0.01\n",
      "Epoch [1716/20000], Loss: 87.04623413085938, Learning Rate: 0.01\n",
      "Epoch [1717/20000], Loss: 87.00979614257812, Learning Rate: 0.01\n",
      "Epoch [1718/20000], Loss: 86.97354125976562, Learning Rate: 0.01\n",
      "Epoch [1719/20000], Loss: 86.93732452392578, Learning Rate: 0.01\n",
      "Epoch [1720/20000], Loss: 86.90084838867188, Learning Rate: 0.01\n",
      "Epoch [1721/20000], Loss: 86.86453247070312, Learning Rate: 0.01\n",
      "Epoch [1722/20000], Loss: 86.8282470703125, Learning Rate: 0.01\n",
      "Epoch [1723/20000], Loss: 86.7919921875, Learning Rate: 0.01\n",
      "Epoch [1724/20000], Loss: 86.7558364868164, Learning Rate: 0.01\n",
      "Epoch [1725/20000], Loss: 86.7195053100586, Learning Rate: 0.01\n",
      "Epoch [1726/20000], Loss: 86.68318176269531, Learning Rate: 0.01\n",
      "Epoch [1727/20000], Loss: 86.64695739746094, Learning Rate: 0.01\n",
      "Epoch [1728/20000], Loss: 86.61084747314453, Learning Rate: 0.01\n",
      "Epoch [1729/20000], Loss: 86.57456970214844, Learning Rate: 0.01\n",
      "Epoch [1730/20000], Loss: 86.53834533691406, Learning Rate: 0.01\n",
      "Epoch [1731/20000], Loss: 86.5022964477539, Learning Rate: 0.01\n",
      "Epoch [1732/20000], Loss: 86.46595001220703, Learning Rate: 0.01\n",
      "Epoch [1733/20000], Loss: 86.429931640625, Learning Rate: 0.01\n",
      "Epoch [1734/20000], Loss: 86.39376831054688, Learning Rate: 0.01\n",
      "Epoch [1735/20000], Loss: 86.35758972167969, Learning Rate: 0.01\n",
      "Epoch [1736/20000], Loss: 86.32141876220703, Learning Rate: 0.01\n",
      "Epoch [1737/20000], Loss: 86.28543090820312, Learning Rate: 0.01\n",
      "Epoch [1738/20000], Loss: 86.24919891357422, Learning Rate: 0.01\n",
      "Epoch [1739/20000], Loss: 86.21327209472656, Learning Rate: 0.01\n",
      "Epoch [1740/20000], Loss: 86.17710876464844, Learning Rate: 0.01\n",
      "Epoch [1741/20000], Loss: 86.14115142822266, Learning Rate: 0.01\n",
      "Epoch [1742/20000], Loss: 86.1051254272461, Learning Rate: 0.01\n",
      "Epoch [1743/20000], Loss: 86.06890106201172, Learning Rate: 0.01\n",
      "Epoch [1744/20000], Loss: 86.03286743164062, Learning Rate: 0.01\n",
      "Epoch [1745/20000], Loss: 85.99699401855469, Learning Rate: 0.01\n",
      "Epoch [1746/20000], Loss: 85.96082305908203, Learning Rate: 0.01\n",
      "Epoch [1747/20000], Loss: 85.92498779296875, Learning Rate: 0.01\n",
      "Epoch [1748/20000], Loss: 85.8890151977539, Learning Rate: 0.01\n",
      "Epoch [1749/20000], Loss: 85.85307312011719, Learning Rate: 0.01\n",
      "Epoch [1750/20000], Loss: 85.8171157836914, Learning Rate: 0.01\n",
      "Epoch [1751/20000], Loss: 85.78130340576172, Learning Rate: 0.01\n",
      "Epoch [1752/20000], Loss: 85.7453384399414, Learning Rate: 0.01\n",
      "Epoch [1753/20000], Loss: 85.70928955078125, Learning Rate: 0.01\n",
      "Epoch [1754/20000], Loss: 85.67350769042969, Learning Rate: 0.01\n",
      "Epoch [1755/20000], Loss: 85.63748931884766, Learning Rate: 0.01\n",
      "Epoch [1756/20000], Loss: 85.60173797607422, Learning Rate: 0.01\n",
      "Epoch [1757/20000], Loss: 85.56574249267578, Learning Rate: 0.01\n",
      "Epoch [1758/20000], Loss: 85.52975463867188, Learning Rate: 0.01\n",
      "Epoch [1759/20000], Loss: 85.49414825439453, Learning Rate: 0.01\n",
      "Epoch [1760/20000], Loss: 85.4583740234375, Learning Rate: 0.01\n",
      "Epoch [1761/20000], Loss: 85.4224853515625, Learning Rate: 0.01\n",
      "Epoch [1762/20000], Loss: 85.38685607910156, Learning Rate: 0.01\n",
      "Epoch [1763/20000], Loss: 85.350830078125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1764/20000], Loss: 85.31523132324219, Learning Rate: 0.01\n",
      "Epoch [1765/20000], Loss: 85.27952575683594, Learning Rate: 0.01\n",
      "Epoch [1766/20000], Loss: 85.24369812011719, Learning Rate: 0.01\n",
      "Epoch [1767/20000], Loss: 85.20803833007812, Learning Rate: 0.01\n",
      "Epoch [1768/20000], Loss: 85.17228698730469, Learning Rate: 0.01\n",
      "Epoch [1769/20000], Loss: 85.1363525390625, Learning Rate: 0.01\n",
      "Epoch [1770/20000], Loss: 85.10067749023438, Learning Rate: 0.01\n",
      "Epoch [1771/20000], Loss: 85.06500244140625, Learning Rate: 0.01\n",
      "Epoch [1772/20000], Loss: 85.02926635742188, Learning Rate: 0.01\n",
      "Epoch [1773/20000], Loss: 84.99392700195312, Learning Rate: 0.01\n",
      "Epoch [1774/20000], Loss: 84.95816040039062, Learning Rate: 0.01\n",
      "Epoch [1775/20000], Loss: 84.92247009277344, Learning Rate: 0.01\n",
      "Epoch [1776/20000], Loss: 84.88677978515625, Learning Rate: 0.01\n",
      "Epoch [1777/20000], Loss: 84.85124206542969, Learning Rate: 0.01\n",
      "Epoch [1778/20000], Loss: 84.81553649902344, Learning Rate: 0.01\n",
      "Epoch [1779/20000], Loss: 84.78012084960938, Learning Rate: 0.01\n",
      "Epoch [1780/20000], Loss: 84.74458312988281, Learning Rate: 0.01\n",
      "Epoch [1781/20000], Loss: 84.70899963378906, Learning Rate: 0.01\n",
      "Epoch [1782/20000], Loss: 84.67343139648438, Learning Rate: 0.01\n",
      "Epoch [1783/20000], Loss: 84.63777160644531, Learning Rate: 0.01\n",
      "Epoch [1784/20000], Loss: 84.60237121582031, Learning Rate: 0.01\n",
      "Epoch [1785/20000], Loss: 84.56697082519531, Learning Rate: 0.01\n",
      "Epoch [1786/20000], Loss: 84.53117370605469, Learning Rate: 0.01\n",
      "Epoch [1787/20000], Loss: 84.49592590332031, Learning Rate: 0.01\n",
      "Epoch [1788/20000], Loss: 84.46047973632812, Learning Rate: 0.01\n",
      "Epoch [1789/20000], Loss: 84.42500305175781, Learning Rate: 0.01\n",
      "Epoch [1790/20000], Loss: 84.38945007324219, Learning Rate: 0.01\n",
      "Epoch [1791/20000], Loss: 84.35397338867188, Learning Rate: 0.01\n",
      "Epoch [1792/20000], Loss: 84.31857299804688, Learning Rate: 0.01\n",
      "Epoch [1793/20000], Loss: 84.28324890136719, Learning Rate: 0.01\n",
      "Epoch [1794/20000], Loss: 84.24775695800781, Learning Rate: 0.01\n",
      "Epoch [1795/20000], Loss: 84.21232604980469, Learning Rate: 0.01\n",
      "Epoch [1796/20000], Loss: 84.17701721191406, Learning Rate: 0.01\n",
      "Epoch [1797/20000], Loss: 84.14154052734375, Learning Rate: 0.01\n",
      "Epoch [1798/20000], Loss: 84.10600280761719, Learning Rate: 0.01\n",
      "Epoch [1799/20000], Loss: 84.07096862792969, Learning Rate: 0.01\n",
      "Epoch [1800/20000], Loss: 84.03559875488281, Learning Rate: 0.01\n",
      "Epoch [1801/20000], Loss: 84.00032043457031, Learning Rate: 0.01\n",
      "Epoch [1802/20000], Loss: 83.96493530273438, Learning Rate: 0.01\n",
      "Epoch [1803/20000], Loss: 83.9296875, Learning Rate: 0.01\n",
      "Epoch [1804/20000], Loss: 83.89457702636719, Learning Rate: 0.01\n",
      "Epoch [1805/20000], Loss: 83.85910034179688, Learning Rate: 0.01\n",
      "Epoch [1806/20000], Loss: 83.823974609375, Learning Rate: 0.01\n",
      "Epoch [1807/20000], Loss: 83.78868103027344, Learning Rate: 0.01\n",
      "Epoch [1808/20000], Loss: 83.7535400390625, Learning Rate: 0.01\n",
      "Epoch [1809/20000], Loss: 83.71836853027344, Learning Rate: 0.01\n",
      "Epoch [1810/20000], Loss: 83.68316650390625, Learning Rate: 0.01\n",
      "Epoch [1811/20000], Loss: 83.64790344238281, Learning Rate: 0.01\n",
      "Epoch [1812/20000], Loss: 83.61297607421875, Learning Rate: 0.01\n",
      "Epoch [1813/20000], Loss: 83.57774353027344, Learning Rate: 0.01\n",
      "Epoch [1814/20000], Loss: 83.54244995117188, Learning Rate: 0.01\n",
      "Epoch [1815/20000], Loss: 83.50730895996094, Learning Rate: 0.01\n",
      "Epoch [1816/20000], Loss: 83.47209167480469, Learning Rate: 0.01\n",
      "Epoch [1817/20000], Loss: 83.43731689453125, Learning Rate: 0.01\n",
      "Epoch [1818/20000], Loss: 83.40203857421875, Learning Rate: 0.01\n",
      "Epoch [1819/20000], Loss: 83.36698913574219, Learning Rate: 0.01\n",
      "Epoch [1820/20000], Loss: 83.33175659179688, Learning Rate: 0.01\n",
      "Epoch [1821/20000], Loss: 83.29684448242188, Learning Rate: 0.01\n",
      "Epoch [1822/20000], Loss: 83.26167297363281, Learning Rate: 0.01\n",
      "Epoch [1823/20000], Loss: 83.22662353515625, Learning Rate: 0.01\n",
      "Epoch [1824/20000], Loss: 83.19160461425781, Learning Rate: 0.01\n",
      "Epoch [1825/20000], Loss: 83.15681457519531, Learning Rate: 0.01\n",
      "Epoch [1826/20000], Loss: 83.12176513671875, Learning Rate: 0.01\n",
      "Epoch [1827/20000], Loss: 83.08671569824219, Learning Rate: 0.01\n",
      "Epoch [1828/20000], Loss: 83.05174255371094, Learning Rate: 0.01\n",
      "Epoch [1829/20000], Loss: 83.01692199707031, Learning Rate: 0.01\n",
      "Epoch [1830/20000], Loss: 82.98185729980469, Learning Rate: 0.01\n",
      "Epoch [1831/20000], Loss: 82.9468994140625, Learning Rate: 0.01\n",
      "Epoch [1832/20000], Loss: 82.91191101074219, Learning Rate: 0.01\n",
      "Epoch [1833/20000], Loss: 82.87713623046875, Learning Rate: 0.01\n",
      "Epoch [1834/20000], Loss: 82.84234619140625, Learning Rate: 0.01\n",
      "Epoch [1835/20000], Loss: 82.807373046875, Learning Rate: 0.01\n",
      "Epoch [1836/20000], Loss: 82.77241516113281, Learning Rate: 0.01\n",
      "Epoch [1837/20000], Loss: 82.73768615722656, Learning Rate: 0.01\n",
      "Epoch [1838/20000], Loss: 82.70294189453125, Learning Rate: 0.01\n",
      "Epoch [1839/20000], Loss: 82.66786193847656, Learning Rate: 0.01\n",
      "Epoch [1840/20000], Loss: 82.63304138183594, Learning Rate: 0.01\n",
      "Epoch [1841/20000], Loss: 82.59844970703125, Learning Rate: 0.01\n",
      "Epoch [1842/20000], Loss: 82.56346130371094, Learning Rate: 0.01\n",
      "Epoch [1843/20000], Loss: 82.52876281738281, Learning Rate: 0.01\n",
      "Epoch [1844/20000], Loss: 82.49391174316406, Learning Rate: 0.01\n",
      "Epoch [1845/20000], Loss: 82.45947265625, Learning Rate: 0.01\n",
      "Epoch [1846/20000], Loss: 82.42472839355469, Learning Rate: 0.01\n",
      "Epoch [1847/20000], Loss: 82.38987731933594, Learning Rate: 0.01\n",
      "Epoch [1848/20000], Loss: 82.35523986816406, Learning Rate: 0.01\n",
      "Epoch [1849/20000], Loss: 82.32049560546875, Learning Rate: 0.01\n",
      "Epoch [1850/20000], Loss: 82.28581237792969, Learning Rate: 0.01\n",
      "Epoch [1851/20000], Loss: 82.25112915039062, Learning Rate: 0.01\n",
      "Epoch [1852/20000], Loss: 82.21636962890625, Learning Rate: 0.01\n",
      "Epoch [1853/20000], Loss: 82.18171691894531, Learning Rate: 0.01\n",
      "Epoch [1854/20000], Loss: 82.14724731445312, Learning Rate: 0.01\n",
      "Epoch [1855/20000], Loss: 82.11257934570312, Learning Rate: 0.01\n",
      "Epoch [1856/20000], Loss: 82.07791137695312, Learning Rate: 0.01\n",
      "Epoch [1857/20000], Loss: 82.04342651367188, Learning Rate: 0.01\n",
      "Epoch [1858/20000], Loss: 82.00888061523438, Learning Rate: 0.01\n",
      "Epoch [1859/20000], Loss: 81.97441101074219, Learning Rate: 0.01\n",
      "Epoch [1860/20000], Loss: 81.93971252441406, Learning Rate: 0.01\n",
      "Epoch [1861/20000], Loss: 81.90516662597656, Learning Rate: 0.01\n",
      "Epoch [1862/20000], Loss: 81.87060546875, Learning Rate: 0.01\n",
      "Epoch [1863/20000], Loss: 81.83624267578125, Learning Rate: 0.01\n",
      "Epoch [1864/20000], Loss: 81.8017578125, Learning Rate: 0.01\n",
      "Epoch [1865/20000], Loss: 81.76716613769531, Learning Rate: 0.01\n",
      "Epoch [1866/20000], Loss: 81.7325439453125, Learning Rate: 0.01\n",
      "Epoch [1867/20000], Loss: 81.69833374023438, Learning Rate: 0.01\n",
      "Epoch [1868/20000], Loss: 81.66366577148438, Learning Rate: 0.01\n",
      "Epoch [1869/20000], Loss: 81.62950134277344, Learning Rate: 0.01\n",
      "Epoch [1870/20000], Loss: 81.59489440917969, Learning Rate: 0.01\n",
      "Epoch [1871/20000], Loss: 81.56063842773438, Learning Rate: 0.01\n",
      "Epoch [1872/20000], Loss: 81.52621459960938, Learning Rate: 0.01\n",
      "Epoch [1873/20000], Loss: 81.49180603027344, Learning Rate: 0.01\n",
      "Epoch [1874/20000], Loss: 81.45765686035156, Learning Rate: 0.01\n",
      "Epoch [1875/20000], Loss: 81.42300415039062, Learning Rate: 0.01\n",
      "Epoch [1876/20000], Loss: 81.38883972167969, Learning Rate: 0.01\n",
      "Epoch [1877/20000], Loss: 81.3544921875, Learning Rate: 0.01\n",
      "Epoch [1878/20000], Loss: 81.32023620605469, Learning Rate: 0.01\n",
      "Epoch [1879/20000], Loss: 81.28593444824219, Learning Rate: 0.01\n",
      "Epoch [1880/20000], Loss: 81.25160217285156, Learning Rate: 0.01\n",
      "Epoch [1881/20000], Loss: 81.21742248535156, Learning Rate: 0.01\n",
      "Epoch [1882/20000], Loss: 81.18313598632812, Learning Rate: 0.01\n",
      "Epoch [1883/20000], Loss: 81.14881896972656, Learning Rate: 0.01\n",
      "Epoch [1884/20000], Loss: 81.11460876464844, Learning Rate: 0.01\n",
      "Epoch [1885/20000], Loss: 81.08039855957031, Learning Rate: 0.01\n",
      "Epoch [1886/20000], Loss: 81.04623413085938, Learning Rate: 0.01\n",
      "Epoch [1887/20000], Loss: 81.011962890625, Learning Rate: 0.01\n",
      "Epoch [1888/20000], Loss: 80.97781372070312, Learning Rate: 0.01\n",
      "Epoch [1889/20000], Loss: 80.94364929199219, Learning Rate: 0.01\n",
      "Epoch [1890/20000], Loss: 80.90963745117188, Learning Rate: 0.01\n",
      "Epoch [1891/20000], Loss: 80.87541198730469, Learning Rate: 0.01\n",
      "Epoch [1892/20000], Loss: 80.84126281738281, Learning Rate: 0.01\n",
      "Epoch [1893/20000], Loss: 80.80729675292969, Learning Rate: 0.01\n",
      "Epoch [1894/20000], Loss: 80.77326965332031, Learning Rate: 0.01\n",
      "Epoch [1895/20000], Loss: 80.739013671875, Learning Rate: 0.01\n",
      "Epoch [1896/20000], Loss: 80.70501708984375, Learning Rate: 0.01\n",
      "Epoch [1897/20000], Loss: 80.6708984375, Learning Rate: 0.01\n",
      "Epoch [1898/20000], Loss: 80.63699340820312, Learning Rate: 0.01\n",
      "Epoch [1899/20000], Loss: 80.60292053222656, Learning Rate: 0.01\n",
      "Epoch [1900/20000], Loss: 80.56887817382812, Learning Rate: 0.01\n",
      "Epoch [1901/20000], Loss: 80.53466796875, Learning Rate: 0.01\n",
      "Epoch [1902/20000], Loss: 80.50091552734375, Learning Rate: 0.01\n",
      "Epoch [1903/20000], Loss: 80.4669189453125, Learning Rate: 0.01\n",
      "Epoch [1904/20000], Loss: 80.43281555175781, Learning Rate: 0.01\n",
      "Epoch [1905/20000], Loss: 80.39915466308594, Learning Rate: 0.01\n",
      "Epoch [1906/20000], Loss: 80.36505126953125, Learning Rate: 0.01\n",
      "Epoch [1907/20000], Loss: 80.33123779296875, Learning Rate: 0.01\n",
      "Epoch [1908/20000], Loss: 80.29734802246094, Learning Rate: 0.01\n",
      "Epoch [1909/20000], Loss: 80.26344299316406, Learning Rate: 0.01\n",
      "Epoch [1910/20000], Loss: 80.22953796386719, Learning Rate: 0.01\n",
      "Epoch [1911/20000], Loss: 80.19560241699219, Learning Rate: 0.01\n",
      "Epoch [1912/20000], Loss: 80.16178894042969, Learning Rate: 0.01\n",
      "Epoch [1913/20000], Loss: 80.12802124023438, Learning Rate: 0.01\n",
      "Epoch [1914/20000], Loss: 80.09420776367188, Learning Rate: 0.01\n",
      "Epoch [1915/20000], Loss: 80.06040954589844, Learning Rate: 0.01\n",
      "Epoch [1916/20000], Loss: 80.02642822265625, Learning Rate: 0.01\n",
      "Epoch [1917/20000], Loss: 79.99287414550781, Learning Rate: 0.01\n",
      "Epoch [1918/20000], Loss: 79.95899963378906, Learning Rate: 0.01\n",
      "Epoch [1919/20000], Loss: 79.92539978027344, Learning Rate: 0.01\n",
      "Epoch [1920/20000], Loss: 79.8916015625, Learning Rate: 0.01\n",
      "Epoch [1921/20000], Loss: 79.8577880859375, Learning Rate: 0.01\n",
      "Epoch [1922/20000], Loss: 79.82412719726562, Learning Rate: 0.01\n",
      "Epoch [1923/20000], Loss: 79.79057312011719, Learning Rate: 0.01\n",
      "Epoch [1924/20000], Loss: 79.75666809082031, Learning Rate: 0.01\n",
      "Epoch [1925/20000], Loss: 79.72309875488281, Learning Rate: 0.01\n",
      "Epoch [1926/20000], Loss: 79.68946838378906, Learning Rate: 0.01\n",
      "Epoch [1927/20000], Loss: 79.65576171875, Learning Rate: 0.01\n",
      "Epoch [1928/20000], Loss: 79.62222290039062, Learning Rate: 0.01\n",
      "Epoch [1929/20000], Loss: 79.58859252929688, Learning Rate: 0.01\n",
      "Epoch [1930/20000], Loss: 79.55485534667969, Learning Rate: 0.01\n",
      "Epoch [1931/20000], Loss: 79.52163696289062, Learning Rate: 0.01\n",
      "Epoch [1932/20000], Loss: 79.48783874511719, Learning Rate: 0.01\n",
      "Epoch [1933/20000], Loss: 79.45416259765625, Learning Rate: 0.01\n",
      "Epoch [1934/20000], Loss: 79.42073059082031, Learning Rate: 0.01\n",
      "Epoch [1935/20000], Loss: 79.38740539550781, Learning Rate: 0.01\n",
      "Epoch [1936/20000], Loss: 79.35360717773438, Learning Rate: 0.01\n",
      "Epoch [1937/20000], Loss: 79.32037353515625, Learning Rate: 0.01\n",
      "Epoch [1938/20000], Loss: 79.28651428222656, Learning Rate: 0.01\n",
      "Epoch [1939/20000], Loss: 79.25309753417969, Learning Rate: 0.01\n",
      "Epoch [1940/20000], Loss: 79.21987915039062, Learning Rate: 0.01\n",
      "Epoch [1941/20000], Loss: 79.18634033203125, Learning Rate: 0.01\n",
      "Epoch [1942/20000], Loss: 79.15275573730469, Learning Rate: 0.01\n",
      "Epoch [1943/20000], Loss: 79.11967468261719, Learning Rate: 0.01\n",
      "Epoch [1944/20000], Loss: 79.08613586425781, Learning Rate: 0.01\n",
      "Epoch [1945/20000], Loss: 79.05279541015625, Learning Rate: 0.01\n",
      "Epoch [1946/20000], Loss: 79.019287109375, Learning Rate: 0.01\n",
      "Epoch [1947/20000], Loss: 78.98612976074219, Learning Rate: 0.01\n",
      "Epoch [1948/20000], Loss: 78.95249938964844, Learning Rate: 0.01\n",
      "Epoch [1949/20000], Loss: 78.919189453125, Learning Rate: 0.01\n",
      "Epoch [1950/20000], Loss: 78.885986328125, Learning Rate: 0.01\n",
      "Epoch [1951/20000], Loss: 78.85292053222656, Learning Rate: 0.01\n",
      "Epoch [1952/20000], Loss: 78.81956481933594, Learning Rate: 0.01\n",
      "Epoch [1953/20000], Loss: 78.78608703613281, Learning Rate: 0.01\n",
      "Epoch [1954/20000], Loss: 78.75299072265625, Learning Rate: 0.01\n",
      "Epoch [1955/20000], Loss: 78.71971130371094, Learning Rate: 0.01\n",
      "Epoch [1956/20000], Loss: 78.68649291992188, Learning Rate: 0.01\n",
      "Epoch [1957/20000], Loss: 78.65322875976562, Learning Rate: 0.01\n",
      "Epoch [1958/20000], Loss: 78.61997985839844, Learning Rate: 0.01\n",
      "Epoch [1959/20000], Loss: 78.58673095703125, Learning Rate: 0.01\n",
      "Epoch [1960/20000], Loss: 78.55368041992188, Learning Rate: 0.01\n",
      "Epoch [1961/20000], Loss: 78.52070617675781, Learning Rate: 0.01\n",
      "Epoch [1962/20000], Loss: 78.48735046386719, Learning Rate: 0.01\n",
      "Epoch [1963/20000], Loss: 78.45420837402344, Learning Rate: 0.01\n",
      "Epoch [1964/20000], Loss: 78.42105102539062, Learning Rate: 0.01\n",
      "Epoch [1965/20000], Loss: 78.3880615234375, Learning Rate: 0.01\n",
      "Epoch [1966/20000], Loss: 78.35498046875, Learning Rate: 0.01\n",
      "Epoch [1967/20000], Loss: 78.32203674316406, Learning Rate: 0.01\n",
      "Epoch [1968/20000], Loss: 78.28878784179688, Learning Rate: 0.01\n",
      "Epoch [1969/20000], Loss: 78.25578308105469, Learning Rate: 0.01\n",
      "Epoch [1970/20000], Loss: 78.22273254394531, Learning Rate: 0.01\n",
      "Epoch [1971/20000], Loss: 78.18972778320312, Learning Rate: 0.01\n",
      "Epoch [1972/20000], Loss: 78.15672302246094, Learning Rate: 0.01\n",
      "Epoch [1973/20000], Loss: 78.12371826171875, Learning Rate: 0.01\n",
      "Epoch [1974/20000], Loss: 78.09068298339844, Learning Rate: 0.01\n",
      "Epoch [1975/20000], Loss: 78.05781555175781, Learning Rate: 0.01\n",
      "Epoch [1976/20000], Loss: 78.02488708496094, Learning Rate: 0.01\n",
      "Epoch [1977/20000], Loss: 77.99198913574219, Learning Rate: 0.01\n",
      "Epoch [1978/20000], Loss: 77.95909118652344, Learning Rate: 0.01\n",
      "Epoch [1979/20000], Loss: 77.92622375488281, Learning Rate: 0.01\n",
      "Epoch [1980/20000], Loss: 77.8934326171875, Learning Rate: 0.01\n",
      "Epoch [1981/20000], Loss: 77.8604736328125, Learning Rate: 0.01\n",
      "Epoch [1982/20000], Loss: 77.82763671875, Learning Rate: 0.01\n",
      "Epoch [1983/20000], Loss: 77.79470825195312, Learning Rate: 0.01\n",
      "Epoch [1984/20000], Loss: 77.76199340820312, Learning Rate: 0.01\n",
      "Epoch [1985/20000], Loss: 77.72911071777344, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1986/20000], Loss: 77.69633483886719, Learning Rate: 0.01\n",
      "Epoch [1987/20000], Loss: 77.66354370117188, Learning Rate: 0.01\n",
      "Epoch [1988/20000], Loss: 77.63084411621094, Learning Rate: 0.01\n",
      "Epoch [1989/20000], Loss: 77.59796142578125, Learning Rate: 0.01\n",
      "Epoch [1990/20000], Loss: 77.56515502929688, Learning Rate: 0.01\n",
      "Epoch [1991/20000], Loss: 77.53250122070312, Learning Rate: 0.01\n",
      "Epoch [1992/20000], Loss: 77.4998779296875, Learning Rate: 0.01\n",
      "Epoch [1993/20000], Loss: 77.46731567382812, Learning Rate: 0.01\n",
      "Epoch [1994/20000], Loss: 77.43437194824219, Learning Rate: 0.01\n",
      "Epoch [1995/20000], Loss: 77.40194702148438, Learning Rate: 0.01\n",
      "Epoch [1996/20000], Loss: 77.36912536621094, Learning Rate: 0.01\n",
      "Epoch [1997/20000], Loss: 77.33639526367188, Learning Rate: 0.01\n",
      "Epoch [1998/20000], Loss: 77.30377197265625, Learning Rate: 0.01\n",
      "Epoch [1999/20000], Loss: 77.27125549316406, Learning Rate: 0.01\n",
      "Epoch [2000/20000], Loss: 77.2386474609375, Learning Rate: 0.01\n",
      "Epoch [2001/20000], Loss: 77.2060546875, Learning Rate: 0.01\n",
      "Epoch [2002/20000], Loss: 77.17353820800781, Learning Rate: 0.01\n",
      "Epoch [2003/20000], Loss: 77.14089965820312, Learning Rate: 0.01\n",
      "Epoch [2004/20000], Loss: 77.1083984375, Learning Rate: 0.01\n",
      "Epoch [2005/20000], Loss: 77.07609558105469, Learning Rate: 0.01\n",
      "Epoch [2006/20000], Loss: 77.04339599609375, Learning Rate: 0.01\n",
      "Epoch [2007/20000], Loss: 77.01101684570312, Learning Rate: 0.01\n",
      "Epoch [2008/20000], Loss: 76.97862243652344, Learning Rate: 0.01\n",
      "Epoch [2009/20000], Loss: 76.94606018066406, Learning Rate: 0.01\n",
      "Epoch [2010/20000], Loss: 76.9136962890625, Learning Rate: 0.01\n",
      "Epoch [2011/20000], Loss: 76.88113403320312, Learning Rate: 0.01\n",
      "Epoch [2012/20000], Loss: 76.8487548828125, Learning Rate: 0.01\n",
      "Epoch [2013/20000], Loss: 76.81631469726562, Learning Rate: 0.01\n",
      "Epoch [2014/20000], Loss: 76.78398132324219, Learning Rate: 0.01\n",
      "Epoch [2015/20000], Loss: 76.75154113769531, Learning Rate: 0.01\n",
      "Epoch [2016/20000], Loss: 76.71903991699219, Learning Rate: 0.01\n",
      "Epoch [2017/20000], Loss: 76.68670654296875, Learning Rate: 0.01\n",
      "Epoch [2018/20000], Loss: 76.65444946289062, Learning Rate: 0.01\n",
      "Epoch [2019/20000], Loss: 76.62229919433594, Learning Rate: 0.01\n",
      "Epoch [2020/20000], Loss: 76.58998107910156, Learning Rate: 0.01\n",
      "Epoch [2021/20000], Loss: 76.55751037597656, Learning Rate: 0.01\n",
      "Epoch [2022/20000], Loss: 76.525390625, Learning Rate: 0.01\n",
      "Epoch [2023/20000], Loss: 76.49336242675781, Learning Rate: 0.01\n",
      "Epoch [2024/20000], Loss: 76.46090698242188, Learning Rate: 0.01\n",
      "Epoch [2025/20000], Loss: 76.42861938476562, Learning Rate: 0.01\n",
      "Epoch [2026/20000], Loss: 76.39653015136719, Learning Rate: 0.01\n",
      "Epoch [2027/20000], Loss: 76.36421203613281, Learning Rate: 0.01\n",
      "Epoch [2028/20000], Loss: 76.33201599121094, Learning Rate: 0.01\n",
      "Epoch [2029/20000], Loss: 76.29994201660156, Learning Rate: 0.01\n",
      "Epoch [2030/20000], Loss: 76.26788330078125, Learning Rate: 0.01\n",
      "Epoch [2031/20000], Loss: 76.23564147949219, Learning Rate: 0.01\n",
      "Epoch [2032/20000], Loss: 76.20362854003906, Learning Rate: 0.01\n",
      "Epoch [2033/20000], Loss: 76.17149353027344, Learning Rate: 0.01\n",
      "Epoch [2034/20000], Loss: 76.139404296875, Learning Rate: 0.01\n",
      "Epoch [2035/20000], Loss: 76.10736083984375, Learning Rate: 0.01\n",
      "Epoch [2036/20000], Loss: 76.07533264160156, Learning Rate: 0.01\n",
      "Epoch [2037/20000], Loss: 76.04322814941406, Learning Rate: 0.01\n",
      "Epoch [2038/20000], Loss: 76.0111083984375, Learning Rate: 0.01\n",
      "Epoch [2039/20000], Loss: 75.97929382324219, Learning Rate: 0.01\n",
      "Epoch [2040/20000], Loss: 75.9471435546875, Learning Rate: 0.01\n",
      "Epoch [2041/20000], Loss: 75.91508483886719, Learning Rate: 0.01\n",
      "Epoch [2042/20000], Loss: 75.88336181640625, Learning Rate: 0.01\n",
      "Epoch [2043/20000], Loss: 75.85130310058594, Learning Rate: 0.01\n",
      "Epoch [2044/20000], Loss: 75.81948852539062, Learning Rate: 0.01\n",
      "Epoch [2045/20000], Loss: 75.78746032714844, Learning Rate: 0.01\n",
      "Epoch [2046/20000], Loss: 75.75564575195312, Learning Rate: 0.01\n",
      "Epoch [2047/20000], Loss: 75.72373962402344, Learning Rate: 0.01\n",
      "Epoch [2048/20000], Loss: 75.69189453125, Learning Rate: 0.01\n",
      "Epoch [2049/20000], Loss: 75.659912109375, Learning Rate: 0.01\n",
      "Epoch [2050/20000], Loss: 75.62820434570312, Learning Rate: 0.01\n",
      "Epoch [2051/20000], Loss: 75.59628295898438, Learning Rate: 0.01\n",
      "Epoch [2052/20000], Loss: 75.564453125, Learning Rate: 0.01\n",
      "Epoch [2053/20000], Loss: 75.53268432617188, Learning Rate: 0.01\n",
      "Epoch [2054/20000], Loss: 75.50079345703125, Learning Rate: 0.01\n",
      "Epoch [2055/20000], Loss: 75.46905517578125, Learning Rate: 0.01\n",
      "Epoch [2056/20000], Loss: 75.4373779296875, Learning Rate: 0.01\n",
      "Epoch [2057/20000], Loss: 75.4053955078125, Learning Rate: 0.01\n",
      "Epoch [2058/20000], Loss: 75.37384033203125, Learning Rate: 0.01\n",
      "Epoch [2059/20000], Loss: 75.34202575683594, Learning Rate: 0.01\n",
      "Epoch [2060/20000], Loss: 75.31047058105469, Learning Rate: 0.01\n",
      "Epoch [2061/20000], Loss: 75.27874755859375, Learning Rate: 0.01\n",
      "Epoch [2062/20000], Loss: 75.24717712402344, Learning Rate: 0.01\n",
      "Epoch [2063/20000], Loss: 75.21537780761719, Learning Rate: 0.01\n",
      "Epoch [2064/20000], Loss: 75.18386840820312, Learning Rate: 0.01\n",
      "Epoch [2065/20000], Loss: 75.15214538574219, Learning Rate: 0.01\n",
      "Epoch [2066/20000], Loss: 75.120361328125, Learning Rate: 0.01\n",
      "Epoch [2067/20000], Loss: 75.08889770507812, Learning Rate: 0.01\n",
      "Epoch [2068/20000], Loss: 75.0572509765625, Learning Rate: 0.01\n",
      "Epoch [2069/20000], Loss: 75.02577209472656, Learning Rate: 0.01\n",
      "Epoch [2070/20000], Loss: 74.99432373046875, Learning Rate: 0.01\n",
      "Epoch [2071/20000], Loss: 74.96278381347656, Learning Rate: 0.01\n",
      "Epoch [2072/20000], Loss: 74.9312744140625, Learning Rate: 0.01\n",
      "Epoch [2073/20000], Loss: 74.89976501464844, Learning Rate: 0.01\n",
      "Epoch [2074/20000], Loss: 74.86819458007812, Learning Rate: 0.01\n",
      "Epoch [2075/20000], Loss: 74.8367919921875, Learning Rate: 0.01\n",
      "Epoch [2076/20000], Loss: 74.80531311035156, Learning Rate: 0.01\n",
      "Epoch [2077/20000], Loss: 74.77383422851562, Learning Rate: 0.01\n",
      "Epoch [2078/20000], Loss: 74.7425537109375, Learning Rate: 0.01\n",
      "Epoch [2079/20000], Loss: 74.71102905273438, Learning Rate: 0.01\n",
      "Epoch [2080/20000], Loss: 74.6798095703125, Learning Rate: 0.01\n",
      "Epoch [2081/20000], Loss: 74.64834594726562, Learning Rate: 0.01\n",
      "Epoch [2082/20000], Loss: 74.61701965332031, Learning Rate: 0.01\n",
      "Epoch [2083/20000], Loss: 74.58563232421875, Learning Rate: 0.01\n",
      "Epoch [2084/20000], Loss: 74.55426025390625, Learning Rate: 0.01\n",
      "Epoch [2085/20000], Loss: 74.52320861816406, Learning Rate: 0.01\n",
      "Epoch [2086/20000], Loss: 74.49168395996094, Learning Rate: 0.01\n",
      "Epoch [2087/20000], Loss: 74.46047973632812, Learning Rate: 0.01\n",
      "Epoch [2088/20000], Loss: 74.42913818359375, Learning Rate: 0.01\n",
      "Epoch [2089/20000], Loss: 74.39796447753906, Learning Rate: 0.01\n",
      "Epoch [2090/20000], Loss: 74.36679077148438, Learning Rate: 0.01\n",
      "Epoch [2091/20000], Loss: 74.33528137207031, Learning Rate: 0.01\n",
      "Epoch [2092/20000], Loss: 74.30422973632812, Learning Rate: 0.01\n",
      "Epoch [2093/20000], Loss: 74.27322387695312, Learning Rate: 0.01\n",
      "Epoch [2094/20000], Loss: 74.24177551269531, Learning Rate: 0.01\n",
      "Epoch [2095/20000], Loss: 74.21061706542969, Learning Rate: 0.01\n",
      "Epoch [2096/20000], Loss: 74.17976379394531, Learning Rate: 0.01\n",
      "Epoch [2097/20000], Loss: 74.14857482910156, Learning Rate: 0.01\n",
      "Epoch [2098/20000], Loss: 74.11724853515625, Learning Rate: 0.01\n",
      "Epoch [2099/20000], Loss: 74.08619689941406, Learning Rate: 0.01\n",
      "Epoch [2100/20000], Loss: 74.05506896972656, Learning Rate: 0.01\n",
      "Epoch [2101/20000], Loss: 74.02386474609375, Learning Rate: 0.01\n",
      "Epoch [2102/20000], Loss: 73.9930419921875, Learning Rate: 0.01\n",
      "Epoch [2103/20000], Loss: 73.96208190917969, Learning Rate: 0.01\n",
      "Epoch [2104/20000], Loss: 73.93101501464844, Learning Rate: 0.01\n",
      "Epoch [2105/20000], Loss: 73.89999389648438, Learning Rate: 0.01\n",
      "Epoch [2106/20000], Loss: 73.868896484375, Learning Rate: 0.01\n",
      "Epoch [2107/20000], Loss: 73.83810424804688, Learning Rate: 0.01\n",
      "Epoch [2108/20000], Loss: 73.80714416503906, Learning Rate: 0.01\n",
      "Epoch [2109/20000], Loss: 73.7762451171875, Learning Rate: 0.01\n",
      "Epoch [2110/20000], Loss: 73.74514770507812, Learning Rate: 0.01\n",
      "Epoch [2111/20000], Loss: 73.71446228027344, Learning Rate: 0.01\n",
      "Epoch [2112/20000], Loss: 73.68336486816406, Learning Rate: 0.01\n",
      "Epoch [2113/20000], Loss: 73.6524658203125, Learning Rate: 0.01\n",
      "Epoch [2114/20000], Loss: 73.62158203125, Learning Rate: 0.01\n",
      "Epoch [2115/20000], Loss: 73.59060668945312, Learning Rate: 0.01\n",
      "Epoch [2116/20000], Loss: 73.55979919433594, Learning Rate: 0.01\n",
      "Epoch [2117/20000], Loss: 73.52902221679688, Learning Rate: 0.01\n",
      "Epoch [2118/20000], Loss: 73.49832153320312, Learning Rate: 0.01\n",
      "Epoch [2119/20000], Loss: 73.46759033203125, Learning Rate: 0.01\n",
      "Epoch [2120/20000], Loss: 73.43669128417969, Learning Rate: 0.01\n",
      "Epoch [2121/20000], Loss: 73.4058837890625, Learning Rate: 0.01\n",
      "Epoch [2122/20000], Loss: 73.375244140625, Learning Rate: 0.01\n",
      "Epoch [2123/20000], Loss: 73.34458923339844, Learning Rate: 0.01\n",
      "Epoch [2124/20000], Loss: 73.31398010253906, Learning Rate: 0.01\n",
      "Epoch [2125/20000], Loss: 73.28326416015625, Learning Rate: 0.01\n",
      "Epoch [2126/20000], Loss: 73.2525634765625, Learning Rate: 0.01\n",
      "Epoch [2127/20000], Loss: 73.22174072265625, Learning Rate: 0.01\n",
      "Epoch [2128/20000], Loss: 73.19113159179688, Learning Rate: 0.01\n",
      "Epoch [2129/20000], Loss: 73.16055297851562, Learning Rate: 0.01\n",
      "Epoch [2130/20000], Loss: 73.12992858886719, Learning Rate: 0.01\n",
      "Epoch [2131/20000], Loss: 73.09933471679688, Learning Rate: 0.01\n",
      "Epoch [2132/20000], Loss: 73.06881713867188, Learning Rate: 0.01\n",
      "Epoch [2133/20000], Loss: 73.03805541992188, Learning Rate: 0.01\n",
      "Epoch [2134/20000], Loss: 73.00758361816406, Learning Rate: 0.01\n",
      "Epoch [2135/20000], Loss: 72.97706604003906, Learning Rate: 0.01\n",
      "Epoch [2136/20000], Loss: 72.94659423828125, Learning Rate: 0.01\n",
      "Epoch [2137/20000], Loss: 72.91604614257812, Learning Rate: 0.01\n",
      "Epoch [2138/20000], Loss: 72.88555908203125, Learning Rate: 0.01\n",
      "Epoch [2139/20000], Loss: 72.85523986816406, Learning Rate: 0.01\n",
      "Epoch [2140/20000], Loss: 72.82463073730469, Learning Rate: 0.01\n",
      "Epoch [2141/20000], Loss: 72.79434204101562, Learning Rate: 0.01\n",
      "Epoch [2142/20000], Loss: 72.76387023925781, Learning Rate: 0.01\n",
      "Epoch [2143/20000], Loss: 72.73350524902344, Learning Rate: 0.01\n",
      "Epoch [2144/20000], Loss: 72.70295715332031, Learning Rate: 0.01\n",
      "Epoch [2145/20000], Loss: 72.67266845703125, Learning Rate: 0.01\n",
      "Epoch [2146/20000], Loss: 72.64236450195312, Learning Rate: 0.01\n",
      "Epoch [2147/20000], Loss: 72.6119384765625, Learning Rate: 0.01\n",
      "Epoch [2148/20000], Loss: 72.58160400390625, Learning Rate: 0.01\n",
      "Epoch [2149/20000], Loss: 72.55137634277344, Learning Rate: 0.01\n",
      "Epoch [2150/20000], Loss: 72.52096557617188, Learning Rate: 0.01\n",
      "Epoch [2151/20000], Loss: 72.49061584472656, Learning Rate: 0.01\n",
      "Epoch [2152/20000], Loss: 72.46044921875, Learning Rate: 0.01\n",
      "Epoch [2153/20000], Loss: 72.43020629882812, Learning Rate: 0.01\n",
      "Epoch [2154/20000], Loss: 72.40017700195312, Learning Rate: 0.01\n",
      "Epoch [2155/20000], Loss: 72.3697509765625, Learning Rate: 0.01\n",
      "Epoch [2156/20000], Loss: 72.339599609375, Learning Rate: 0.01\n",
      "Epoch [2157/20000], Loss: 72.30917358398438, Learning Rate: 0.01\n",
      "Epoch [2158/20000], Loss: 72.27915954589844, Learning Rate: 0.01\n",
      "Epoch [2159/20000], Loss: 72.24900817871094, Learning Rate: 0.01\n",
      "Epoch [2160/20000], Loss: 72.21888732910156, Learning Rate: 0.01\n",
      "Epoch [2161/20000], Loss: 72.18867492675781, Learning Rate: 0.01\n",
      "Epoch [2162/20000], Loss: 72.15859985351562, Learning Rate: 0.01\n",
      "Epoch [2163/20000], Loss: 72.12850952148438, Learning Rate: 0.01\n",
      "Epoch [2164/20000], Loss: 72.09860229492188, Learning Rate: 0.01\n",
      "Epoch [2165/20000], Loss: 72.06834411621094, Learning Rate: 0.01\n",
      "Epoch [2166/20000], Loss: 72.038330078125, Learning Rate: 0.01\n",
      "Epoch [2167/20000], Loss: 72.00836181640625, Learning Rate: 0.01\n",
      "Epoch [2168/20000], Loss: 71.97842407226562, Learning Rate: 0.01\n",
      "Epoch [2169/20000], Loss: 71.94828796386719, Learning Rate: 0.01\n",
      "Epoch [2170/20000], Loss: 71.91828918457031, Learning Rate: 0.01\n",
      "Epoch [2171/20000], Loss: 71.88832092285156, Learning Rate: 0.01\n",
      "Epoch [2172/20000], Loss: 71.85847473144531, Learning Rate: 0.01\n",
      "Epoch [2173/20000], Loss: 71.82844543457031, Learning Rate: 0.01\n",
      "Epoch [2174/20000], Loss: 71.798583984375, Learning Rate: 0.01\n",
      "Epoch [2175/20000], Loss: 71.76876831054688, Learning Rate: 0.01\n",
      "Epoch [2176/20000], Loss: 71.73870849609375, Learning Rate: 0.01\n",
      "Epoch [2177/20000], Loss: 71.70899963378906, Learning Rate: 0.01\n",
      "Epoch [2178/20000], Loss: 71.67898559570312, Learning Rate: 0.01\n",
      "Epoch [2179/20000], Loss: 71.64926147460938, Learning Rate: 0.01\n",
      "Epoch [2180/20000], Loss: 71.61936950683594, Learning Rate: 0.01\n",
      "Epoch [2181/20000], Loss: 71.58955383300781, Learning Rate: 0.01\n",
      "Epoch [2182/20000], Loss: 71.55987548828125, Learning Rate: 0.01\n",
      "Epoch [2183/20000], Loss: 71.52998352050781, Learning Rate: 0.01\n",
      "Epoch [2184/20000], Loss: 71.500244140625, Learning Rate: 0.01\n",
      "Epoch [2185/20000], Loss: 71.47061157226562, Learning Rate: 0.01\n",
      "Epoch [2186/20000], Loss: 71.44061279296875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2187/20000], Loss: 71.41099548339844, Learning Rate: 0.01\n",
      "Epoch [2188/20000], Loss: 71.38133239746094, Learning Rate: 0.01\n",
      "Epoch [2189/20000], Loss: 71.35154724121094, Learning Rate: 0.01\n",
      "Epoch [2190/20000], Loss: 71.32205200195312, Learning Rate: 0.01\n",
      "Epoch [2191/20000], Loss: 71.29231262207031, Learning Rate: 0.01\n",
      "Epoch [2192/20000], Loss: 71.26278686523438, Learning Rate: 0.01\n",
      "Epoch [2193/20000], Loss: 71.23332214355469, Learning Rate: 0.01\n",
      "Epoch [2194/20000], Loss: 71.20346069335938, Learning Rate: 0.01\n",
      "Epoch [2195/20000], Loss: 71.17402648925781, Learning Rate: 0.01\n",
      "Epoch [2196/20000], Loss: 71.14436340332031, Learning Rate: 0.01\n",
      "Epoch [2197/20000], Loss: 71.11494445800781, Learning Rate: 0.01\n",
      "Epoch [2198/20000], Loss: 71.08526611328125, Learning Rate: 0.01\n",
      "Epoch [2199/20000], Loss: 71.055908203125, Learning Rate: 0.01\n",
      "Epoch [2200/20000], Loss: 71.02622985839844, Learning Rate: 0.01\n",
      "Epoch [2201/20000], Loss: 70.99681091308594, Learning Rate: 0.01\n",
      "Epoch [2202/20000], Loss: 70.96725463867188, Learning Rate: 0.01\n",
      "Epoch [2203/20000], Loss: 70.9378662109375, Learning Rate: 0.01\n",
      "Epoch [2204/20000], Loss: 70.90858459472656, Learning Rate: 0.01\n",
      "Epoch [2205/20000], Loss: 70.87908935546875, Learning Rate: 0.01\n",
      "Epoch [2206/20000], Loss: 70.84959411621094, Learning Rate: 0.01\n",
      "Epoch [2207/20000], Loss: 70.8203125, Learning Rate: 0.01\n",
      "Epoch [2208/20000], Loss: 70.79092407226562, Learning Rate: 0.01\n",
      "Epoch [2209/20000], Loss: 70.76142883300781, Learning Rate: 0.01\n",
      "Epoch [2210/20000], Loss: 70.73228454589844, Learning Rate: 0.01\n",
      "Epoch [2211/20000], Loss: 70.70271301269531, Learning Rate: 0.01\n",
      "Epoch [2212/20000], Loss: 70.673583984375, Learning Rate: 0.01\n",
      "Epoch [2213/20000], Loss: 70.64424133300781, Learning Rate: 0.01\n",
      "Epoch [2214/20000], Loss: 70.61495971679688, Learning Rate: 0.01\n",
      "Epoch [2215/20000], Loss: 70.58583068847656, Learning Rate: 0.01\n",
      "Epoch [2216/20000], Loss: 70.55625915527344, Learning Rate: 0.01\n",
      "Epoch [2217/20000], Loss: 70.52713012695312, Learning Rate: 0.01\n",
      "Epoch [2218/20000], Loss: 70.49803161621094, Learning Rate: 0.01\n",
      "Epoch [2219/20000], Loss: 70.46881103515625, Learning Rate: 0.01\n",
      "Epoch [2220/20000], Loss: 70.43942260742188, Learning Rate: 0.01\n",
      "Epoch [2221/20000], Loss: 70.41046142578125, Learning Rate: 0.01\n",
      "Epoch [2222/20000], Loss: 70.38139343261719, Learning Rate: 0.01\n",
      "Epoch [2223/20000], Loss: 70.352294921875, Learning Rate: 0.01\n",
      "Epoch [2224/20000], Loss: 70.32293701171875, Learning Rate: 0.01\n",
      "Epoch [2225/20000], Loss: 70.29388427734375, Learning Rate: 0.01\n",
      "Epoch [2226/20000], Loss: 70.26495361328125, Learning Rate: 0.01\n",
      "Epoch [2227/20000], Loss: 70.23583984375, Learning Rate: 0.01\n",
      "Epoch [2228/20000], Loss: 70.20672607421875, Learning Rate: 0.01\n",
      "Epoch [2229/20000], Loss: 70.17774963378906, Learning Rate: 0.01\n",
      "Epoch [2230/20000], Loss: 70.14863586425781, Learning Rate: 0.01\n",
      "Epoch [2231/20000], Loss: 70.11959838867188, Learning Rate: 0.01\n",
      "Epoch [2232/20000], Loss: 70.09056091308594, Learning Rate: 0.01\n",
      "Epoch [2233/20000], Loss: 70.06146240234375, Learning Rate: 0.01\n",
      "Epoch [2234/20000], Loss: 70.03263854980469, Learning Rate: 0.01\n",
      "Epoch [2235/20000], Loss: 70.00376892089844, Learning Rate: 0.01\n",
      "Epoch [2236/20000], Loss: 69.97489929199219, Learning Rate: 0.01\n",
      "Epoch [2237/20000], Loss: 69.94590759277344, Learning Rate: 0.01\n",
      "Epoch [2238/20000], Loss: 69.91693115234375, Learning Rate: 0.01\n",
      "Epoch [2239/20000], Loss: 69.88807678222656, Learning Rate: 0.01\n",
      "Epoch [2240/20000], Loss: 69.85916137695312, Learning Rate: 0.01\n",
      "Epoch [2241/20000], Loss: 69.83035278320312, Learning Rate: 0.01\n",
      "Epoch [2242/20000], Loss: 69.80157470703125, Learning Rate: 0.01\n",
      "Epoch [2243/20000], Loss: 69.77276611328125, Learning Rate: 0.01\n",
      "Epoch [2244/20000], Loss: 69.74400329589844, Learning Rate: 0.01\n",
      "Epoch [2245/20000], Loss: 69.7152099609375, Learning Rate: 0.01\n",
      "Epoch [2246/20000], Loss: 69.68637084960938, Learning Rate: 0.01\n",
      "Epoch [2247/20000], Loss: 69.65768432617188, Learning Rate: 0.01\n",
      "Epoch [2248/20000], Loss: 69.62905883789062, Learning Rate: 0.01\n",
      "Epoch [2249/20000], Loss: 69.60029602050781, Learning Rate: 0.01\n",
      "Epoch [2250/20000], Loss: 69.57164001464844, Learning Rate: 0.01\n",
      "Epoch [2251/20000], Loss: 69.54301452636719, Learning Rate: 0.01\n",
      "Epoch [2252/20000], Loss: 69.51412963867188, Learning Rate: 0.01\n",
      "Epoch [2253/20000], Loss: 69.48550415039062, Learning Rate: 0.01\n",
      "Epoch [2254/20000], Loss: 69.45693969726562, Learning Rate: 0.01\n",
      "Epoch [2255/20000], Loss: 69.42811584472656, Learning Rate: 0.01\n",
      "Epoch [2256/20000], Loss: 69.39962768554688, Learning Rate: 0.01\n",
      "Epoch [2257/20000], Loss: 69.37106323242188, Learning Rate: 0.01\n",
      "Epoch [2258/20000], Loss: 69.34254455566406, Learning Rate: 0.01\n",
      "Epoch [2259/20000], Loss: 69.31399536132812, Learning Rate: 0.01\n",
      "Epoch [2260/20000], Loss: 69.28544616699219, Learning Rate: 0.01\n",
      "Epoch [2261/20000], Loss: 69.25698852539062, Learning Rate: 0.01\n",
      "Epoch [2262/20000], Loss: 69.22833251953125, Learning Rate: 0.01\n",
      "Epoch [2263/20000], Loss: 69.19990539550781, Learning Rate: 0.01\n",
      "Epoch [2264/20000], Loss: 69.17155456542969, Learning Rate: 0.01\n",
      "Epoch [2265/20000], Loss: 69.14305114746094, Learning Rate: 0.01\n",
      "Epoch [2266/20000], Loss: 69.11466979980469, Learning Rate: 0.01\n",
      "Epoch [2267/20000], Loss: 69.08610534667969, Learning Rate: 0.01\n",
      "Epoch [2268/20000], Loss: 69.05752563476562, Learning Rate: 0.01\n",
      "Epoch [2269/20000], Loss: 69.02946472167969, Learning Rate: 0.01\n",
      "Epoch [2270/20000], Loss: 69.00105285644531, Learning Rate: 0.01\n",
      "Epoch [2271/20000], Loss: 68.97247314453125, Learning Rate: 0.01\n",
      "Epoch [2272/20000], Loss: 68.9444580078125, Learning Rate: 0.01\n",
      "Epoch [2273/20000], Loss: 68.91590881347656, Learning Rate: 0.01\n",
      "Epoch [2274/20000], Loss: 68.88752746582031, Learning Rate: 0.01\n",
      "Epoch [2275/20000], Loss: 68.85932922363281, Learning Rate: 0.01\n",
      "Epoch [2276/20000], Loss: 68.83088684082031, Learning Rate: 0.01\n",
      "Epoch [2277/20000], Loss: 68.80290222167969, Learning Rate: 0.01\n",
      "Epoch [2278/20000], Loss: 68.77447509765625, Learning Rate: 0.01\n",
      "Epoch [2279/20000], Loss: 68.74624633789062, Learning Rate: 0.01\n",
      "Epoch [2280/20000], Loss: 68.71795654296875, Learning Rate: 0.01\n",
      "Epoch [2281/20000], Loss: 68.69004821777344, Learning Rate: 0.01\n",
      "Epoch [2282/20000], Loss: 68.66168212890625, Learning Rate: 0.01\n",
      "Epoch [2283/20000], Loss: 68.63360595703125, Learning Rate: 0.01\n",
      "Epoch [2284/20000], Loss: 68.60533142089844, Learning Rate: 0.01\n",
      "Epoch [2285/20000], Loss: 68.57728576660156, Learning Rate: 0.01\n",
      "Epoch [2286/20000], Loss: 68.54920959472656, Learning Rate: 0.01\n",
      "Epoch [2287/20000], Loss: 68.52110290527344, Learning Rate: 0.01\n",
      "Epoch [2288/20000], Loss: 68.49278259277344, Learning Rate: 0.01\n",
      "Epoch [2289/20000], Loss: 68.46487426757812, Learning Rate: 0.01\n",
      "Epoch [2290/20000], Loss: 68.43690490722656, Learning Rate: 0.01\n",
      "Epoch [2291/20000], Loss: 68.40867614746094, Learning Rate: 0.01\n",
      "Epoch [2292/20000], Loss: 68.38076782226562, Learning Rate: 0.01\n",
      "Epoch [2293/20000], Loss: 68.35270690917969, Learning Rate: 0.01\n",
      "Epoch [2294/20000], Loss: 68.32469177246094, Learning Rate: 0.01\n",
      "Epoch [2295/20000], Loss: 68.296875, Learning Rate: 0.01\n",
      "Epoch [2296/20000], Loss: 68.26876831054688, Learning Rate: 0.01\n",
      "Epoch [2297/20000], Loss: 68.24069213867188, Learning Rate: 0.01\n",
      "Epoch [2298/20000], Loss: 68.21270751953125, Learning Rate: 0.01\n",
      "Epoch [2299/20000], Loss: 68.18479919433594, Learning Rate: 0.01\n",
      "Epoch [2300/20000], Loss: 68.15701293945312, Learning Rate: 0.01\n",
      "Epoch [2301/20000], Loss: 68.12910461425781, Learning Rate: 0.01\n",
      "Epoch [2302/20000], Loss: 68.10145568847656, Learning Rate: 0.01\n",
      "Epoch [2303/20000], Loss: 68.07368469238281, Learning Rate: 0.01\n",
      "Epoch [2304/20000], Loss: 68.04573059082031, Learning Rate: 0.01\n",
      "Epoch [2305/20000], Loss: 68.01789855957031, Learning Rate: 0.01\n",
      "Epoch [2306/20000], Loss: 67.98991394042969, Learning Rate: 0.01\n",
      "Epoch [2307/20000], Loss: 67.96226501464844, Learning Rate: 0.01\n",
      "Epoch [2308/20000], Loss: 67.93453979492188, Learning Rate: 0.01\n",
      "Epoch [2309/20000], Loss: 67.90673828125, Learning Rate: 0.01\n",
      "Epoch [2310/20000], Loss: 67.87899780273438, Learning Rate: 0.01\n",
      "Epoch [2311/20000], Loss: 67.85139465332031, Learning Rate: 0.01\n",
      "Epoch [2312/20000], Loss: 67.823486328125, Learning Rate: 0.01\n",
      "Epoch [2313/20000], Loss: 67.79595947265625, Learning Rate: 0.01\n",
      "Epoch [2314/20000], Loss: 67.76829528808594, Learning Rate: 0.01\n",
      "Epoch [2315/20000], Loss: 67.74058532714844, Learning Rate: 0.01\n",
      "Epoch [2316/20000], Loss: 67.71287536621094, Learning Rate: 0.01\n",
      "Epoch [2317/20000], Loss: 67.68521118164062, Learning Rate: 0.01\n",
      "Epoch [2318/20000], Loss: 67.6575927734375, Learning Rate: 0.01\n",
      "Epoch [2319/20000], Loss: 67.63026428222656, Learning Rate: 0.01\n",
      "Epoch [2320/20000], Loss: 67.60244750976562, Learning Rate: 0.01\n",
      "Epoch [2321/20000], Loss: 67.57498168945312, Learning Rate: 0.01\n",
      "Epoch [2322/20000], Loss: 67.54745483398438, Learning Rate: 0.01\n",
      "Epoch [2323/20000], Loss: 67.52003479003906, Learning Rate: 0.01\n",
      "Epoch [2324/20000], Loss: 67.49227905273438, Learning Rate: 0.01\n",
      "Epoch [2325/20000], Loss: 67.46498107910156, Learning Rate: 0.01\n",
      "Epoch [2326/20000], Loss: 67.43745422363281, Learning Rate: 0.01\n",
      "Epoch [2327/20000], Loss: 67.40997314453125, Learning Rate: 0.01\n",
      "Epoch [2328/20000], Loss: 67.38253784179688, Learning Rate: 0.01\n",
      "Epoch [2329/20000], Loss: 67.35501098632812, Learning Rate: 0.01\n",
      "Epoch [2330/20000], Loss: 67.32769775390625, Learning Rate: 0.01\n",
      "Epoch [2331/20000], Loss: 67.30039978027344, Learning Rate: 0.01\n",
      "Epoch [2332/20000], Loss: 67.27299499511719, Learning Rate: 0.01\n",
      "Epoch [2333/20000], Loss: 67.24551391601562, Learning Rate: 0.01\n",
      "Epoch [2334/20000], Loss: 67.21821594238281, Learning Rate: 0.01\n",
      "Epoch [2335/20000], Loss: 67.19081115722656, Learning Rate: 0.01\n",
      "Epoch [2336/20000], Loss: 67.16355895996094, Learning Rate: 0.01\n",
      "Epoch [2337/20000], Loss: 67.13616943359375, Learning Rate: 0.01\n",
      "Epoch [2338/20000], Loss: 67.10882568359375, Learning Rate: 0.01\n",
      "Epoch [2339/20000], Loss: 67.08160400390625, Learning Rate: 0.01\n",
      "Epoch [2340/20000], Loss: 67.05427551269531, Learning Rate: 0.01\n",
      "Epoch [2341/20000], Loss: 67.02734375, Learning Rate: 0.01\n",
      "Epoch [2342/20000], Loss: 66.99990844726562, Learning Rate: 0.01\n",
      "Epoch [2343/20000], Loss: 66.97273254394531, Learning Rate: 0.01\n",
      "Epoch [2344/20000], Loss: 66.94566345214844, Learning Rate: 0.01\n",
      "Epoch [2345/20000], Loss: 66.91848754882812, Learning Rate: 0.01\n",
      "Epoch [2346/20000], Loss: 66.89131164550781, Learning Rate: 0.01\n",
      "Epoch [2347/20000], Loss: 66.86402893066406, Learning Rate: 0.01\n",
      "Epoch [2348/20000], Loss: 66.83700561523438, Learning Rate: 0.01\n",
      "Epoch [2349/20000], Loss: 66.809814453125, Learning Rate: 0.01\n",
      "Epoch [2350/20000], Loss: 66.78274536132812, Learning Rate: 0.01\n",
      "Epoch [2351/20000], Loss: 66.75570678710938, Learning Rate: 0.01\n",
      "Epoch [2352/20000], Loss: 66.72862243652344, Learning Rate: 0.01\n",
      "Epoch [2353/20000], Loss: 66.70159912109375, Learning Rate: 0.01\n",
      "Epoch [2354/20000], Loss: 66.67474365234375, Learning Rate: 0.01\n",
      "Epoch [2355/20000], Loss: 66.64759826660156, Learning Rate: 0.01\n",
      "Epoch [2356/20000], Loss: 66.62042236328125, Learning Rate: 0.01\n",
      "Epoch [2357/20000], Loss: 66.593505859375, Learning Rate: 0.01\n",
      "Epoch [2358/20000], Loss: 66.56649780273438, Learning Rate: 0.01\n",
      "Epoch [2359/20000], Loss: 66.53955078125, Learning Rate: 0.01\n",
      "Epoch [2360/20000], Loss: 66.512939453125, Learning Rate: 0.01\n",
      "Epoch [2361/20000], Loss: 66.48585510253906, Learning Rate: 0.01\n",
      "Epoch [2362/20000], Loss: 66.458984375, Learning Rate: 0.01\n",
      "Epoch [2363/20000], Loss: 66.43197631835938, Learning Rate: 0.01\n",
      "Epoch [2364/20000], Loss: 66.40533447265625, Learning Rate: 0.01\n",
      "Epoch [2365/20000], Loss: 66.37831115722656, Learning Rate: 0.01\n",
      "Epoch [2366/20000], Loss: 66.35140991210938, Learning Rate: 0.01\n",
      "Epoch [2367/20000], Loss: 66.32470703125, Learning Rate: 0.01\n",
      "Epoch [2368/20000], Loss: 66.29811096191406, Learning Rate: 0.01\n",
      "Epoch [2369/20000], Loss: 66.27105712890625, Learning Rate: 0.01\n",
      "Epoch [2370/20000], Loss: 66.2442626953125, Learning Rate: 0.01\n",
      "Epoch [2371/20000], Loss: 66.21766662597656, Learning Rate: 0.01\n",
      "Epoch [2372/20000], Loss: 66.19091796875, Learning Rate: 0.01\n",
      "Epoch [2373/20000], Loss: 66.16410827636719, Learning Rate: 0.01\n",
      "Epoch [2374/20000], Loss: 66.13735961914062, Learning Rate: 0.01\n",
      "Epoch [2375/20000], Loss: 66.11077880859375, Learning Rate: 0.01\n",
      "Epoch [2376/20000], Loss: 66.08407592773438, Learning Rate: 0.01\n",
      "Epoch [2377/20000], Loss: 66.0572509765625, Learning Rate: 0.01\n",
      "Epoch [2378/20000], Loss: 66.03089904785156, Learning Rate: 0.01\n",
      "Epoch [2379/20000], Loss: 66.00422668457031, Learning Rate: 0.01\n",
      "Epoch [2380/20000], Loss: 65.97756958007812, Learning Rate: 0.01\n",
      "Epoch [2381/20000], Loss: 65.9510498046875, Learning Rate: 0.01\n",
      "Epoch [2382/20000], Loss: 65.92428588867188, Learning Rate: 0.01\n",
      "Epoch [2383/20000], Loss: 65.8978271484375, Learning Rate: 0.01\n",
      "Epoch [2384/20000], Loss: 65.87130737304688, Learning Rate: 0.01\n",
      "Epoch [2385/20000], Loss: 65.84457397460938, Learning Rate: 0.01\n",
      "Epoch [2386/20000], Loss: 65.8182373046875, Learning Rate: 0.01\n",
      "Epoch [2387/20000], Loss: 65.7916259765625, Learning Rate: 0.01\n",
      "Epoch [2388/20000], Loss: 65.76518249511719, Learning Rate: 0.01\n",
      "Epoch [2389/20000], Loss: 65.73873901367188, Learning Rate: 0.01\n",
      "Epoch [2390/20000], Loss: 65.71231079101562, Learning Rate: 0.01\n",
      "Epoch [2391/20000], Loss: 65.68585205078125, Learning Rate: 0.01\n",
      "Epoch [2392/20000], Loss: 65.65957641601562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2393/20000], Loss: 65.633056640625, Learning Rate: 0.01\n",
      "Epoch [2394/20000], Loss: 65.60658264160156, Learning Rate: 0.01\n",
      "Epoch [2395/20000], Loss: 65.58038330078125, Learning Rate: 0.01\n",
      "Epoch [2396/20000], Loss: 65.553955078125, Learning Rate: 0.01\n",
      "Epoch [2397/20000], Loss: 65.52755737304688, Learning Rate: 0.01\n",
      "Epoch [2398/20000], Loss: 65.50117492675781, Learning Rate: 0.01\n",
      "Epoch [2399/20000], Loss: 65.4749755859375, Learning Rate: 0.01\n",
      "Epoch [2400/20000], Loss: 65.44868469238281, Learning Rate: 0.01\n",
      "Epoch [2401/20000], Loss: 65.4224853515625, Learning Rate: 0.01\n",
      "Epoch [2402/20000], Loss: 65.39619445800781, Learning Rate: 0.01\n",
      "Epoch [2403/20000], Loss: 65.37002563476562, Learning Rate: 0.01\n",
      "Epoch [2404/20000], Loss: 65.34370422363281, Learning Rate: 0.01\n",
      "Epoch [2405/20000], Loss: 65.31756591796875, Learning Rate: 0.01\n",
      "Epoch [2406/20000], Loss: 65.291259765625, Learning Rate: 0.01\n",
      "Epoch [2407/20000], Loss: 65.26502990722656, Learning Rate: 0.01\n",
      "Epoch [2408/20000], Loss: 65.23876953125, Learning Rate: 0.01\n",
      "Epoch [2409/20000], Loss: 65.21281433105469, Learning Rate: 0.01\n",
      "Epoch [2410/20000], Loss: 65.18673706054688, Learning Rate: 0.01\n",
      "Epoch [2411/20000], Loss: 65.16067504882812, Learning Rate: 0.01\n",
      "Epoch [2412/20000], Loss: 65.13447570800781, Learning Rate: 0.01\n",
      "Epoch [2413/20000], Loss: 65.10838317871094, Learning Rate: 0.01\n",
      "Epoch [2414/20000], Loss: 65.08235168457031, Learning Rate: 0.01\n",
      "Epoch [2415/20000], Loss: 65.05616760253906, Learning Rate: 0.01\n",
      "Epoch [2416/20000], Loss: 65.03009033203125, Learning Rate: 0.01\n",
      "Epoch [2417/20000], Loss: 65.00410461425781, Learning Rate: 0.01\n",
      "Epoch [2418/20000], Loss: 64.97802734375, Learning Rate: 0.01\n",
      "Epoch [2419/20000], Loss: 64.95201110839844, Learning Rate: 0.01\n",
      "Epoch [2420/20000], Loss: 64.92607116699219, Learning Rate: 0.01\n",
      "Epoch [2421/20000], Loss: 64.89999389648438, Learning Rate: 0.01\n",
      "Epoch [2422/20000], Loss: 64.87432861328125, Learning Rate: 0.01\n",
      "Epoch [2423/20000], Loss: 64.84849548339844, Learning Rate: 0.01\n",
      "Epoch [2424/20000], Loss: 64.82235717773438, Learning Rate: 0.01\n",
      "Epoch [2425/20000], Loss: 64.79627990722656, Learning Rate: 0.01\n",
      "Epoch [2426/20000], Loss: 64.77044677734375, Learning Rate: 0.01\n",
      "Epoch [2427/20000], Loss: 64.74479675292969, Learning Rate: 0.01\n",
      "Epoch [2428/20000], Loss: 64.71894836425781, Learning Rate: 0.01\n",
      "Epoch [2429/20000], Loss: 64.69297790527344, Learning Rate: 0.01\n",
      "Epoch [2430/20000], Loss: 64.66709899902344, Learning Rate: 0.01\n",
      "Epoch [2431/20000], Loss: 64.64138793945312, Learning Rate: 0.01\n",
      "Epoch [2432/20000], Loss: 64.61546325683594, Learning Rate: 0.01\n",
      "Epoch [2433/20000], Loss: 64.58985900878906, Learning Rate: 0.01\n",
      "Epoch [2434/20000], Loss: 64.56407165527344, Learning Rate: 0.01\n",
      "Epoch [2435/20000], Loss: 64.53831481933594, Learning Rate: 0.01\n",
      "Epoch [2436/20000], Loss: 64.51248168945312, Learning Rate: 0.01\n",
      "Epoch [2437/20000], Loss: 64.48698425292969, Learning Rate: 0.01\n",
      "Epoch [2438/20000], Loss: 64.46122741699219, Learning Rate: 0.01\n",
      "Epoch [2439/20000], Loss: 64.43537902832031, Learning Rate: 0.01\n",
      "Epoch [2440/20000], Loss: 64.40992736816406, Learning Rate: 0.01\n",
      "Epoch [2441/20000], Loss: 64.38414001464844, Learning Rate: 0.01\n",
      "Epoch [2442/20000], Loss: 64.35841369628906, Learning Rate: 0.01\n",
      "Epoch [2443/20000], Loss: 64.33280944824219, Learning Rate: 0.01\n",
      "Epoch [2444/20000], Loss: 64.30747985839844, Learning Rate: 0.01\n",
      "Epoch [2445/20000], Loss: 64.28172302246094, Learning Rate: 0.01\n",
      "Epoch [2446/20000], Loss: 64.25611877441406, Learning Rate: 0.01\n",
      "Epoch [2447/20000], Loss: 64.23075866699219, Learning Rate: 0.01\n",
      "Epoch [2448/20000], Loss: 64.20501708984375, Learning Rate: 0.01\n",
      "Epoch [2449/20000], Loss: 64.17962646484375, Learning Rate: 0.01\n",
      "Epoch [2450/20000], Loss: 64.154052734375, Learning Rate: 0.01\n",
      "Epoch [2451/20000], Loss: 64.12855529785156, Learning Rate: 0.01\n",
      "Epoch [2452/20000], Loss: 64.10305786132812, Learning Rate: 0.01\n",
      "Epoch [2453/20000], Loss: 64.07769775390625, Learning Rate: 0.01\n",
      "Epoch [2454/20000], Loss: 64.05224609375, Learning Rate: 0.01\n",
      "Epoch [2455/20000], Loss: 64.02664184570312, Learning Rate: 0.01\n",
      "Epoch [2456/20000], Loss: 64.0013427734375, Learning Rate: 0.01\n",
      "Epoch [2457/20000], Loss: 63.975921630859375, Learning Rate: 0.01\n",
      "Epoch [2458/20000], Loss: 63.95037841796875, Learning Rate: 0.01\n",
      "Epoch [2459/20000], Loss: 63.92512512207031, Learning Rate: 0.01\n",
      "Epoch [2460/20000], Loss: 63.899749755859375, Learning Rate: 0.01\n",
      "Epoch [2461/20000], Loss: 63.87445068359375, Learning Rate: 0.01\n",
      "Epoch [2462/20000], Loss: 63.849151611328125, Learning Rate: 0.01\n",
      "Epoch [2463/20000], Loss: 63.82377624511719, Learning Rate: 0.01\n",
      "Epoch [2464/20000], Loss: 63.79838562011719, Learning Rate: 0.01\n",
      "Epoch [2465/20000], Loss: 63.773223876953125, Learning Rate: 0.01\n",
      "Epoch [2466/20000], Loss: 63.74815368652344, Learning Rate: 0.01\n",
      "Epoch [2467/20000], Loss: 63.722991943359375, Learning Rate: 0.01\n",
      "Epoch [2468/20000], Loss: 63.697601318359375, Learning Rate: 0.01\n",
      "Epoch [2469/20000], Loss: 63.67254638671875, Learning Rate: 0.01\n",
      "Epoch [2470/20000], Loss: 63.64704895019531, Learning Rate: 0.01\n",
      "Epoch [2471/20000], Loss: 63.62196350097656, Learning Rate: 0.01\n",
      "Epoch [2472/20000], Loss: 63.597015380859375, Learning Rate: 0.01\n",
      "Epoch [2473/20000], Loss: 63.57167053222656, Learning Rate: 0.01\n",
      "Epoch [2474/20000], Loss: 63.54681396484375, Learning Rate: 0.01\n",
      "Epoch [2475/20000], Loss: 63.52142333984375, Learning Rate: 0.01\n",
      "Epoch [2476/20000], Loss: 63.49638366699219, Learning Rate: 0.01\n",
      "Epoch [2477/20000], Loss: 63.47123718261719, Learning Rate: 0.01\n",
      "Epoch [2478/20000], Loss: 63.44627380371094, Learning Rate: 0.01\n",
      "Epoch [2479/20000], Loss: 63.42103576660156, Learning Rate: 0.01\n",
      "Epoch [2480/20000], Loss: 63.39604187011719, Learning Rate: 0.01\n",
      "Epoch [2481/20000], Loss: 63.371124267578125, Learning Rate: 0.01\n",
      "Epoch [2482/20000], Loss: 63.34614562988281, Learning Rate: 0.01\n",
      "Epoch [2483/20000], Loss: 63.32093811035156, Learning Rate: 0.01\n",
      "Epoch [2484/20000], Loss: 63.295989990234375, Learning Rate: 0.01\n",
      "Epoch [2485/20000], Loss: 63.27105712890625, Learning Rate: 0.01\n",
      "Epoch [2486/20000], Loss: 63.24615478515625, Learning Rate: 0.01\n",
      "Epoch [2487/20000], Loss: 63.22129821777344, Learning Rate: 0.01\n",
      "Epoch [2488/20000], Loss: 63.196197509765625, Learning Rate: 0.01\n",
      "Epoch [2489/20000], Loss: 63.17131042480469, Learning Rate: 0.01\n",
      "Epoch [2490/20000], Loss: 63.14659118652344, Learning Rate: 0.01\n",
      "Epoch [2491/20000], Loss: 63.12162780761719, Learning Rate: 0.01\n",
      "Epoch [2492/20000], Loss: 63.09681701660156, Learning Rate: 0.01\n",
      "Epoch [2493/20000], Loss: 63.071990966796875, Learning Rate: 0.01\n",
      "Epoch [2494/20000], Loss: 63.04707336425781, Learning Rate: 0.01\n",
      "Epoch [2495/20000], Loss: 63.02220153808594, Learning Rate: 0.01\n",
      "Epoch [2496/20000], Loss: 62.99723815917969, Learning Rate: 0.01\n",
      "Epoch [2497/20000], Loss: 62.97271728515625, Learning Rate: 0.01\n",
      "Epoch [2498/20000], Loss: 62.94801330566406, Learning Rate: 0.01\n",
      "Epoch [2499/20000], Loss: 62.923126220703125, Learning Rate: 0.01\n",
      "Epoch [2500/20000], Loss: 62.89842224121094, Learning Rate: 0.01\n",
      "Epoch [2501/20000], Loss: 62.873748779296875, Learning Rate: 0.01\n",
      "Epoch [2502/20000], Loss: 62.84910583496094, Learning Rate: 0.01\n",
      "Epoch [2503/20000], Loss: 62.82440185546875, Learning Rate: 0.01\n",
      "Epoch [2504/20000], Loss: 62.799713134765625, Learning Rate: 0.01\n",
      "Epoch [2505/20000], Loss: 62.77507019042969, Learning Rate: 0.01\n",
      "Epoch [2506/20000], Loss: 62.75028991699219, Learning Rate: 0.01\n",
      "Epoch [2507/20000], Loss: 62.72572326660156, Learning Rate: 0.01\n",
      "Epoch [2508/20000], Loss: 62.701171875, Learning Rate: 0.01\n",
      "Epoch [2509/20000], Loss: 62.67657470703125, Learning Rate: 0.01\n",
      "Epoch [2510/20000], Loss: 62.651885986328125, Learning Rate: 0.01\n",
      "Epoch [2511/20000], Loss: 62.62738037109375, Learning Rate: 0.01\n",
      "Epoch [2512/20000], Loss: 62.602752685546875, Learning Rate: 0.01\n",
      "Epoch [2513/20000], Loss: 62.57826232910156, Learning Rate: 0.01\n",
      "Epoch [2514/20000], Loss: 62.55366516113281, Learning Rate: 0.01\n",
      "Epoch [2515/20000], Loss: 62.52923583984375, Learning Rate: 0.01\n",
      "Epoch [2516/20000], Loss: 62.504730224609375, Learning Rate: 0.01\n",
      "Epoch [2517/20000], Loss: 62.48030090332031, Learning Rate: 0.01\n",
      "Epoch [2518/20000], Loss: 62.455841064453125, Learning Rate: 0.01\n",
      "Epoch [2519/20000], Loss: 62.431396484375, Learning Rate: 0.01\n",
      "Epoch [2520/20000], Loss: 62.40678405761719, Learning Rate: 0.01\n",
      "Epoch [2521/20000], Loss: 62.38249206542969, Learning Rate: 0.01\n",
      "Epoch [2522/20000], Loss: 62.3580322265625, Learning Rate: 0.01\n",
      "Epoch [2523/20000], Loss: 62.33378601074219, Learning Rate: 0.01\n",
      "Epoch [2524/20000], Loss: 62.30940246582031, Learning Rate: 0.01\n",
      "Epoch [2525/20000], Loss: 62.28501892089844, Learning Rate: 0.01\n",
      "Epoch [2526/20000], Loss: 62.26060485839844, Learning Rate: 0.01\n",
      "Epoch [2527/20000], Loss: 62.23631286621094, Learning Rate: 0.01\n",
      "Epoch [2528/20000], Loss: 62.21197509765625, Learning Rate: 0.01\n",
      "Epoch [2529/20000], Loss: 62.18768310546875, Learning Rate: 0.01\n",
      "Epoch [2530/20000], Loss: 62.16337585449219, Learning Rate: 0.01\n",
      "Epoch [2531/20000], Loss: 62.1392822265625, Learning Rate: 0.01\n",
      "Epoch [2532/20000], Loss: 62.115081787109375, Learning Rate: 0.01\n",
      "Epoch [2533/20000], Loss: 62.09083557128906, Learning Rate: 0.01\n",
      "Epoch [2534/20000], Loss: 62.066375732421875, Learning Rate: 0.01\n",
      "Epoch [2535/20000], Loss: 62.042266845703125, Learning Rate: 0.01\n",
      "Epoch [2536/20000], Loss: 62.01800537109375, Learning Rate: 0.01\n",
      "Epoch [2537/20000], Loss: 61.994110107421875, Learning Rate: 0.01\n",
      "Epoch [2538/20000], Loss: 61.969757080078125, Learning Rate: 0.01\n",
      "Epoch [2539/20000], Loss: 61.945709228515625, Learning Rate: 0.01\n",
      "Epoch [2540/20000], Loss: 61.9215087890625, Learning Rate: 0.01\n",
      "Epoch [2541/20000], Loss: 61.89735412597656, Learning Rate: 0.01\n",
      "Epoch [2542/20000], Loss: 61.87336730957031, Learning Rate: 0.01\n",
      "Epoch [2543/20000], Loss: 61.84918212890625, Learning Rate: 0.01\n",
      "Epoch [2544/20000], Loss: 61.82502746582031, Learning Rate: 0.01\n",
      "Epoch [2545/20000], Loss: 61.801116943359375, Learning Rate: 0.01\n",
      "Epoch [2546/20000], Loss: 61.77717590332031, Learning Rate: 0.01\n",
      "Epoch [2547/20000], Loss: 61.75297546386719, Learning Rate: 0.01\n",
      "Epoch [2548/20000], Loss: 61.72898864746094, Learning Rate: 0.01\n",
      "Epoch [2549/20000], Loss: 61.704864501953125, Learning Rate: 0.01\n",
      "Epoch [2550/20000], Loss: 61.68098449707031, Learning Rate: 0.01\n",
      "Epoch [2551/20000], Loss: 61.65699768066406, Learning Rate: 0.01\n",
      "Epoch [2552/20000], Loss: 61.63311767578125, Learning Rate: 0.01\n",
      "Epoch [2553/20000], Loss: 61.60932922363281, Learning Rate: 0.01\n",
      "Epoch [2554/20000], Loss: 61.585113525390625, Learning Rate: 0.01\n",
      "Epoch [2555/20000], Loss: 61.561126708984375, Learning Rate: 0.01\n",
      "Epoch [2556/20000], Loss: 61.53721618652344, Learning Rate: 0.01\n",
      "Epoch [2557/20000], Loss: 61.51347351074219, Learning Rate: 0.01\n",
      "Epoch [2558/20000], Loss: 61.48970031738281, Learning Rate: 0.01\n",
      "Epoch [2559/20000], Loss: 61.46571350097656, Learning Rate: 0.01\n",
      "Epoch [2560/20000], Loss: 61.4420166015625, Learning Rate: 0.01\n",
      "Epoch [2561/20000], Loss: 61.41816711425781, Learning Rate: 0.01\n",
      "Epoch [2562/20000], Loss: 61.394287109375, Learning Rate: 0.01\n",
      "Epoch [2563/20000], Loss: 61.370513916015625, Learning Rate: 0.01\n",
      "Epoch [2564/20000], Loss: 61.346832275390625, Learning Rate: 0.01\n",
      "Epoch [2565/20000], Loss: 61.323028564453125, Learning Rate: 0.01\n",
      "Epoch [2566/20000], Loss: 61.29911804199219, Learning Rate: 0.01\n",
      "Epoch [2567/20000], Loss: 61.275604248046875, Learning Rate: 0.01\n",
      "Epoch [2568/20000], Loss: 61.251739501953125, Learning Rate: 0.01\n",
      "Epoch [2569/20000], Loss: 61.2281494140625, Learning Rate: 0.01\n",
      "Epoch [2570/20000], Loss: 61.204498291015625, Learning Rate: 0.01\n",
      "Epoch [2571/20000], Loss: 61.18077087402344, Learning Rate: 0.01\n",
      "Epoch [2572/20000], Loss: 61.15702819824219, Learning Rate: 0.01\n",
      "Epoch [2573/20000], Loss: 61.1334228515625, Learning Rate: 0.01\n",
      "Epoch [2574/20000], Loss: 61.10986328125, Learning Rate: 0.01\n",
      "Epoch [2575/20000], Loss: 61.08624267578125, Learning Rate: 0.01\n",
      "Epoch [2576/20000], Loss: 61.06257629394531, Learning Rate: 0.01\n",
      "Epoch [2577/20000], Loss: 61.03900146484375, Learning Rate: 0.01\n",
      "Epoch [2578/20000], Loss: 61.015167236328125, Learning Rate: 0.01\n",
      "Epoch [2579/20000], Loss: 60.99189758300781, Learning Rate: 0.01\n",
      "Epoch [2580/20000], Loss: 60.96849060058594, Learning Rate: 0.01\n",
      "Epoch [2581/20000], Loss: 60.94483947753906, Learning Rate: 0.01\n",
      "Epoch [2582/20000], Loss: 60.9215087890625, Learning Rate: 0.01\n",
      "Epoch [2583/20000], Loss: 60.89790344238281, Learning Rate: 0.01\n",
      "Epoch [2584/20000], Loss: 60.87437438964844, Learning Rate: 0.01\n",
      "Epoch [2585/20000], Loss: 60.8507080078125, Learning Rate: 0.01\n",
      "Epoch [2586/20000], Loss: 60.82740783691406, Learning Rate: 0.01\n",
      "Epoch [2587/20000], Loss: 60.80406188964844, Learning Rate: 0.01\n",
      "Epoch [2588/20000], Loss: 60.78050231933594, Learning Rate: 0.01\n",
      "Epoch [2589/20000], Loss: 60.75727844238281, Learning Rate: 0.01\n",
      "Epoch [2590/20000], Loss: 60.733795166015625, Learning Rate: 0.01\n",
      "Epoch [2591/20000], Loss: 60.71043395996094, Learning Rate: 0.01\n",
      "Epoch [2592/20000], Loss: 60.6871337890625, Learning Rate: 0.01\n",
      "Epoch [2593/20000], Loss: 60.66371154785156, Learning Rate: 0.01\n",
      "Epoch [2594/20000], Loss: 60.64042663574219, Learning Rate: 0.01\n",
      "Epoch [2595/20000], Loss: 60.617095947265625, Learning Rate: 0.01\n",
      "Epoch [2596/20000], Loss: 60.59370422363281, Learning Rate: 0.01\n",
      "Epoch [2597/20000], Loss: 60.57048034667969, Learning Rate: 0.01\n",
      "Epoch [2598/20000], Loss: 60.54710388183594, Learning Rate: 0.01\n",
      "Epoch [2599/20000], Loss: 60.52388000488281, Learning Rate: 0.01\n",
      "Epoch [2600/20000], Loss: 60.501007080078125, Learning Rate: 0.01\n",
      "Epoch [2601/20000], Loss: 60.47760009765625, Learning Rate: 0.01\n",
      "Epoch [2602/20000], Loss: 60.4542236328125, Learning Rate: 0.01\n",
      "Epoch [2603/20000], Loss: 60.430999755859375, Learning Rate: 0.01\n",
      "Epoch [2604/20000], Loss: 60.40782165527344, Learning Rate: 0.01\n",
      "Epoch [2605/20000], Loss: 60.38465881347656, Learning Rate: 0.01\n",
      "Epoch [2606/20000], Loss: 60.36138916015625, Learning Rate: 0.01\n",
      "Epoch [2607/20000], Loss: 60.33821105957031, Learning Rate: 0.01\n",
      "Epoch [2608/20000], Loss: 60.315277099609375, Learning Rate: 0.01\n",
      "Epoch [2609/20000], Loss: 60.2919921875, Learning Rate: 0.01\n",
      "Epoch [2610/20000], Loss: 60.26890563964844, Learning Rate: 0.01\n",
      "Epoch [2611/20000], Loss: 60.24577331542969, Learning Rate: 0.01\n",
      "Epoch [2612/20000], Loss: 60.222808837890625, Learning Rate: 0.01\n",
      "Epoch [2613/20000], Loss: 60.19978332519531, Learning Rate: 0.01\n",
      "Epoch [2614/20000], Loss: 60.176666259765625, Learning Rate: 0.01\n",
      "Epoch [2615/20000], Loss: 60.15362548828125, Learning Rate: 0.01\n",
      "Epoch [2616/20000], Loss: 60.130584716796875, Learning Rate: 0.01\n",
      "Epoch [2617/20000], Loss: 60.107666015625, Learning Rate: 0.01\n",
      "Epoch [2618/20000], Loss: 60.0845947265625, Learning Rate: 0.01\n",
      "Epoch [2619/20000], Loss: 60.061737060546875, Learning Rate: 0.01\n",
      "Epoch [2620/20000], Loss: 60.038848876953125, Learning Rate: 0.01\n",
      "Epoch [2621/20000], Loss: 60.01567077636719, Learning Rate: 0.01\n",
      "Epoch [2622/20000], Loss: 59.99287414550781, Learning Rate: 0.01\n",
      "Epoch [2623/20000], Loss: 59.96986389160156, Learning Rate: 0.01\n",
      "Epoch [2624/20000], Loss: 59.946868896484375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2625/20000], Loss: 59.92414855957031, Learning Rate: 0.01\n",
      "Epoch [2626/20000], Loss: 59.90110778808594, Learning Rate: 0.01\n",
      "Epoch [2627/20000], Loss: 59.878387451171875, Learning Rate: 0.01\n",
      "Epoch [2628/20000], Loss: 59.85539245605469, Learning Rate: 0.01\n",
      "Epoch [2629/20000], Loss: 59.83250427246094, Learning Rate: 0.01\n",
      "Epoch [2630/20000], Loss: 59.80973815917969, Learning Rate: 0.01\n",
      "Epoch [2631/20000], Loss: 59.7869873046875, Learning Rate: 0.01\n",
      "Epoch [2632/20000], Loss: 59.76414489746094, Learning Rate: 0.01\n",
      "Epoch [2633/20000], Loss: 59.741485595703125, Learning Rate: 0.01\n",
      "Epoch [2634/20000], Loss: 59.71867370605469, Learning Rate: 0.01\n",
      "Epoch [2635/20000], Loss: 59.6959228515625, Learning Rate: 0.01\n",
      "Epoch [2636/20000], Loss: 59.6732177734375, Learning Rate: 0.01\n",
      "Epoch [2637/20000], Loss: 59.650390625, Learning Rate: 0.01\n",
      "Epoch [2638/20000], Loss: 59.62770080566406, Learning Rate: 0.01\n",
      "Epoch [2639/20000], Loss: 59.605010986328125, Learning Rate: 0.01\n",
      "Epoch [2640/20000], Loss: 59.58238220214844, Learning Rate: 0.01\n",
      "Epoch [2641/20000], Loss: 59.559814453125, Learning Rate: 0.01\n",
      "Epoch [2642/20000], Loss: 59.537200927734375, Learning Rate: 0.01\n",
      "Epoch [2643/20000], Loss: 59.51445007324219, Learning Rate: 0.01\n",
      "Epoch [2644/20000], Loss: 59.49192810058594, Learning Rate: 0.01\n",
      "Epoch [2645/20000], Loss: 59.46910095214844, Learning Rate: 0.01\n",
      "Epoch [2646/20000], Loss: 59.44682312011719, Learning Rate: 0.01\n",
      "Epoch [2647/20000], Loss: 59.42396545410156, Learning Rate: 0.01\n",
      "Epoch [2648/20000], Loss: 59.40135192871094, Learning Rate: 0.01\n",
      "Epoch [2649/20000], Loss: 59.378936767578125, Learning Rate: 0.01\n",
      "Epoch [2650/20000], Loss: 59.35633850097656, Learning Rate: 0.01\n",
      "Epoch [2651/20000], Loss: 59.33369445800781, Learning Rate: 0.01\n",
      "Epoch [2652/20000], Loss: 59.31126403808594, Learning Rate: 0.01\n",
      "Epoch [2653/20000], Loss: 59.28871154785156, Learning Rate: 0.01\n",
      "Epoch [2654/20000], Loss: 59.26629638671875, Learning Rate: 0.01\n",
      "Epoch [2655/20000], Loss: 59.24382019042969, Learning Rate: 0.01\n",
      "Epoch [2656/20000], Loss: 59.22126770019531, Learning Rate: 0.01\n",
      "Epoch [2657/20000], Loss: 59.198944091796875, Learning Rate: 0.01\n",
      "Epoch [2658/20000], Loss: 59.176727294921875, Learning Rate: 0.01\n",
      "Epoch [2659/20000], Loss: 59.15415954589844, Learning Rate: 0.01\n",
      "Epoch [2660/20000], Loss: 59.131744384765625, Learning Rate: 0.01\n",
      "Epoch [2661/20000], Loss: 59.109375, Learning Rate: 0.01\n",
      "Epoch [2662/20000], Loss: 59.086883544921875, Learning Rate: 0.01\n",
      "Epoch [2663/20000], Loss: 59.064483642578125, Learning Rate: 0.01\n",
      "Epoch [2664/20000], Loss: 59.042327880859375, Learning Rate: 0.01\n",
      "Epoch [2665/20000], Loss: 59.01988220214844, Learning Rate: 0.01\n",
      "Epoch [2666/20000], Loss: 58.99745178222656, Learning Rate: 0.01\n",
      "Epoch [2667/20000], Loss: 58.975372314453125, Learning Rate: 0.01\n",
      "Epoch [2668/20000], Loss: 58.95292663574219, Learning Rate: 0.01\n",
      "Epoch [2669/20000], Loss: 58.93067932128906, Learning Rate: 0.01\n",
      "Epoch [2670/20000], Loss: 58.90852355957031, Learning Rate: 0.01\n",
      "Epoch [2671/20000], Loss: 58.88618469238281, Learning Rate: 0.01\n",
      "Epoch [2672/20000], Loss: 58.86405944824219, Learning Rate: 0.01\n",
      "Epoch [2673/20000], Loss: 58.84173583984375, Learning Rate: 0.01\n",
      "Epoch [2674/20000], Loss: 58.819671630859375, Learning Rate: 0.01\n",
      "Epoch [2675/20000], Loss: 58.79740905761719, Learning Rate: 0.01\n",
      "Epoch [2676/20000], Loss: 58.775177001953125, Learning Rate: 0.01\n",
      "Epoch [2677/20000], Loss: 58.75303649902344, Learning Rate: 0.01\n",
      "Epoch [2678/20000], Loss: 58.73085021972656, Learning Rate: 0.01\n",
      "Epoch [2679/20000], Loss: 58.708648681640625, Learning Rate: 0.01\n",
      "Epoch [2680/20000], Loss: 58.68658447265625, Learning Rate: 0.01\n",
      "Epoch [2681/20000], Loss: 58.664520263671875, Learning Rate: 0.01\n",
      "Epoch [2682/20000], Loss: 58.6422119140625, Learning Rate: 0.01\n",
      "Epoch [2683/20000], Loss: 58.62022399902344, Learning Rate: 0.01\n",
      "Epoch [2684/20000], Loss: 58.59832763671875, Learning Rate: 0.01\n",
      "Epoch [2685/20000], Loss: 58.576202392578125, Learning Rate: 0.01\n",
      "Epoch [2686/20000], Loss: 58.55427551269531, Learning Rate: 0.01\n",
      "Epoch [2687/20000], Loss: 58.532196044921875, Learning Rate: 0.01\n",
      "Epoch [2688/20000], Loss: 58.51002502441406, Learning Rate: 0.01\n",
      "Epoch [2689/20000], Loss: 58.4881591796875, Learning Rate: 0.01\n",
      "Epoch [2690/20000], Loss: 58.466156005859375, Learning Rate: 0.01\n",
      "Epoch [2691/20000], Loss: 58.444000244140625, Learning Rate: 0.01\n",
      "Epoch [2692/20000], Loss: 58.422088623046875, Learning Rate: 0.01\n",
      "Epoch [2693/20000], Loss: 58.4002685546875, Learning Rate: 0.01\n",
      "Epoch [2694/20000], Loss: 58.37815856933594, Learning Rate: 0.01\n",
      "Epoch [2695/20000], Loss: 58.35624694824219, Learning Rate: 0.01\n",
      "Epoch [2696/20000], Loss: 58.33441162109375, Learning Rate: 0.01\n",
      "Epoch [2697/20000], Loss: 58.312408447265625, Learning Rate: 0.01\n",
      "Epoch [2698/20000], Loss: 58.290771484375, Learning Rate: 0.01\n",
      "Epoch [2699/20000], Loss: 58.268798828125, Learning Rate: 0.01\n",
      "Epoch [2700/20000], Loss: 58.24665832519531, Learning Rate: 0.01\n",
      "Epoch [2701/20000], Loss: 58.22508239746094, Learning Rate: 0.01\n",
      "Epoch [2702/20000], Loss: 58.20335388183594, Learning Rate: 0.01\n",
      "Epoch [2703/20000], Loss: 58.181243896484375, Learning Rate: 0.01\n",
      "Epoch [2704/20000], Loss: 58.15946960449219, Learning Rate: 0.01\n",
      "Epoch [2705/20000], Loss: 58.137725830078125, Learning Rate: 0.01\n",
      "Epoch [2706/20000], Loss: 58.11604309082031, Learning Rate: 0.01\n",
      "Epoch [2707/20000], Loss: 58.094390869140625, Learning Rate: 0.01\n",
      "Epoch [2708/20000], Loss: 58.07244873046875, Learning Rate: 0.01\n",
      "Epoch [2709/20000], Loss: 58.05072021484375, Learning Rate: 0.01\n",
      "Epoch [2710/20000], Loss: 58.02897644042969, Learning Rate: 0.01\n",
      "Epoch [2711/20000], Loss: 58.007293701171875, Learning Rate: 0.01\n",
      "Epoch [2712/20000], Loss: 57.985626220703125, Learning Rate: 0.01\n",
      "Epoch [2713/20000], Loss: 57.964080810546875, Learning Rate: 0.01\n",
      "Epoch [2714/20000], Loss: 57.94224548339844, Learning Rate: 0.01\n",
      "Epoch [2715/20000], Loss: 57.92071533203125, Learning Rate: 0.01\n",
      "Epoch [2716/20000], Loss: 57.8990478515625, Learning Rate: 0.01\n",
      "Epoch [2717/20000], Loss: 57.87738037109375, Learning Rate: 0.01\n",
      "Epoch [2718/20000], Loss: 57.85566711425781, Learning Rate: 0.01\n",
      "Epoch [2719/20000], Loss: 57.83399963378906, Learning Rate: 0.01\n",
      "Epoch [2720/20000], Loss: 57.81248474121094, Learning Rate: 0.01\n",
      "Epoch [2721/20000], Loss: 57.79100036621094, Learning Rate: 0.01\n",
      "Epoch [2722/20000], Loss: 57.76945495605469, Learning Rate: 0.01\n",
      "Epoch [2723/20000], Loss: 57.7479248046875, Learning Rate: 0.01\n",
      "Epoch [2724/20000], Loss: 57.726348876953125, Learning Rate: 0.01\n",
      "Epoch [2725/20000], Loss: 57.70469665527344, Learning Rate: 0.01\n",
      "Epoch [2726/20000], Loss: 57.68327331542969, Learning Rate: 0.01\n",
      "Epoch [2727/20000], Loss: 57.66169738769531, Learning Rate: 0.01\n",
      "Epoch [2728/20000], Loss: 57.640350341796875, Learning Rate: 0.01\n",
      "Epoch [2729/20000], Loss: 57.61863708496094, Learning Rate: 0.01\n",
      "Epoch [2730/20000], Loss: 57.59739685058594, Learning Rate: 0.01\n",
      "Epoch [2731/20000], Loss: 57.57594299316406, Learning Rate: 0.01\n",
      "Epoch [2732/20000], Loss: 57.554412841796875, Learning Rate: 0.01\n",
      "Epoch [2733/20000], Loss: 57.5328369140625, Learning Rate: 0.01\n",
      "Epoch [2734/20000], Loss: 57.51161193847656, Learning Rate: 0.01\n",
      "Epoch [2735/20000], Loss: 57.49018859863281, Learning Rate: 0.01\n",
      "Epoch [2736/20000], Loss: 57.46882629394531, Learning Rate: 0.01\n",
      "Epoch [2737/20000], Loss: 57.44757080078125, Learning Rate: 0.01\n",
      "Epoch [2738/20000], Loss: 57.4261474609375, Learning Rate: 0.01\n",
      "Epoch [2739/20000], Loss: 57.40470886230469, Learning Rate: 0.01\n",
      "Epoch [2740/20000], Loss: 57.383544921875, Learning Rate: 0.01\n",
      "Epoch [2741/20000], Loss: 57.362274169921875, Learning Rate: 0.01\n",
      "Epoch [2742/20000], Loss: 57.34100341796875, Learning Rate: 0.01\n",
      "Epoch [2743/20000], Loss: 57.31959533691406, Learning Rate: 0.01\n",
      "Epoch [2744/20000], Loss: 57.29823303222656, Learning Rate: 0.01\n",
      "Epoch [2745/20000], Loss: 57.27717590332031, Learning Rate: 0.01\n",
      "Epoch [2746/20000], Loss: 57.25593566894531, Learning Rate: 0.01\n",
      "Epoch [2747/20000], Loss: 57.23466491699219, Learning Rate: 0.01\n",
      "Epoch [2748/20000], Loss: 57.21331787109375, Learning Rate: 0.01\n",
      "Epoch [2749/20000], Loss: 57.19203186035156, Learning Rate: 0.01\n",
      "Epoch [2750/20000], Loss: 57.1710205078125, Learning Rate: 0.01\n",
      "Epoch [2751/20000], Loss: 57.149810791015625, Learning Rate: 0.01\n",
      "Epoch [2752/20000], Loss: 57.1285400390625, Learning Rate: 0.01\n",
      "Epoch [2753/20000], Loss: 57.10740661621094, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2754/20000], Loss: 57.0863037109375, Learning Rate: 0.01\n",
      "Epoch [2755/20000], Loss: 57.06512451171875, Learning Rate: 0.01\n",
      "Epoch [2756/20000], Loss: 57.04399108886719, Learning Rate: 0.01\n",
      "Epoch [2757/20000], Loss: 57.02293395996094, Learning Rate: 0.01\n",
      "Epoch [2758/20000], Loss: 57.001861572265625, Learning Rate: 0.01\n",
      "Epoch [2759/20000], Loss: 56.980743408203125, Learning Rate: 0.01\n",
      "Epoch [2760/20000], Loss: 56.95967102050781, Learning Rate: 0.01\n",
      "Epoch [2761/20000], Loss: 56.93849182128906, Learning Rate: 0.01\n",
      "Epoch [2762/20000], Loss: 56.91748046875, Learning Rate: 0.01\n",
      "Epoch [2763/20000], Loss: 56.896484375, Learning Rate: 0.01\n",
      "Epoch [2764/20000], Loss: 56.87547302246094, Learning Rate: 0.01\n",
      "Epoch [2765/20000], Loss: 56.854522705078125, Learning Rate: 0.01\n",
      "Epoch [2766/20000], Loss: 56.83345031738281, Learning Rate: 0.01\n",
      "Epoch [2767/20000], Loss: 56.81257629394531, Learning Rate: 0.01\n",
      "Epoch [2768/20000], Loss: 56.79145812988281, Learning Rate: 0.01\n",
      "Epoch [2769/20000], Loss: 56.77082824707031, Learning Rate: 0.01\n",
      "Epoch [2770/20000], Loss: 56.74951171875, Learning Rate: 0.01\n",
      "Epoch [2771/20000], Loss: 56.72859191894531, Learning Rate: 0.01\n",
      "Epoch [2772/20000], Loss: 56.707733154296875, Learning Rate: 0.01\n",
      "Epoch [2773/20000], Loss: 56.686859130859375, Learning Rate: 0.01\n",
      "Epoch [2774/20000], Loss: 56.665771484375, Learning Rate: 0.01\n",
      "Epoch [2775/20000], Loss: 56.64495849609375, Learning Rate: 0.01\n",
      "Epoch [2776/20000], Loss: 56.62431335449219, Learning Rate: 0.01\n",
      "Epoch [2777/20000], Loss: 56.60319519042969, Learning Rate: 0.01\n",
      "Epoch [2778/20000], Loss: 56.58258056640625, Learning Rate: 0.01\n",
      "Epoch [2779/20000], Loss: 56.56156921386719, Learning Rate: 0.01\n",
      "Epoch [2780/20000], Loss: 56.54081726074219, Learning Rate: 0.01\n",
      "Epoch [2781/20000], Loss: 56.520050048828125, Learning Rate: 0.01\n",
      "Epoch [2782/20000], Loss: 56.49922180175781, Learning Rate: 0.01\n",
      "Epoch [2783/20000], Loss: 56.478515625, Learning Rate: 0.01\n",
      "Epoch [2784/20000], Loss: 56.45771789550781, Learning Rate: 0.01\n",
      "Epoch [2785/20000], Loss: 56.43696594238281, Learning Rate: 0.01\n",
      "Epoch [2786/20000], Loss: 56.416168212890625, Learning Rate: 0.01\n",
      "Epoch [2787/20000], Loss: 56.395355224609375, Learning Rate: 0.01\n",
      "Epoch [2788/20000], Loss: 56.3746337890625, Learning Rate: 0.01\n",
      "Epoch [2789/20000], Loss: 56.3541259765625, Learning Rate: 0.01\n",
      "Epoch [2790/20000], Loss: 56.33338928222656, Learning Rate: 0.01\n",
      "Epoch [2791/20000], Loss: 56.31266784667969, Learning Rate: 0.01\n",
      "Epoch [2792/20000], Loss: 56.29200744628906, Learning Rate: 0.01\n",
      "Epoch [2793/20000], Loss: 56.27140808105469, Learning Rate: 0.01\n",
      "Epoch [2794/20000], Loss: 56.25077819824219, Learning Rate: 0.01\n",
      "Epoch [2795/20000], Loss: 56.23016357421875, Learning Rate: 0.01\n",
      "Epoch [2796/20000], Loss: 56.209381103515625, Learning Rate: 0.01\n",
      "Epoch [2797/20000], Loss: 56.18878173828125, Learning Rate: 0.01\n",
      "Epoch [2798/20000], Loss: 56.16819763183594, Learning Rate: 0.01\n",
      "Epoch [2799/20000], Loss: 56.1475830078125, Learning Rate: 0.01\n",
      "Epoch [2800/20000], Loss: 56.12705993652344, Learning Rate: 0.01\n",
      "Epoch [2801/20000], Loss: 56.1064453125, Learning Rate: 0.01\n",
      "Epoch [2802/20000], Loss: 56.0859375, Learning Rate: 0.01\n",
      "Epoch [2803/20000], Loss: 56.0655517578125, Learning Rate: 0.01\n",
      "Epoch [2804/20000], Loss: 56.04493713378906, Learning Rate: 0.01\n",
      "Epoch [2805/20000], Loss: 56.02449035644531, Learning Rate: 0.01\n",
      "Epoch [2806/20000], Loss: 56.00407409667969, Learning Rate: 0.01\n",
      "Epoch [2807/20000], Loss: 55.98344421386719, Learning Rate: 0.01\n",
      "Epoch [2808/20000], Loss: 55.96290588378906, Learning Rate: 0.01\n",
      "Epoch [2809/20000], Loss: 55.942352294921875, Learning Rate: 0.01\n",
      "Epoch [2810/20000], Loss: 55.92201232910156, Learning Rate: 0.01\n",
      "Epoch [2811/20000], Loss: 55.90130615234375, Learning Rate: 0.01\n",
      "Epoch [2812/20000], Loss: 55.88105773925781, Learning Rate: 0.01\n",
      "Epoch [2813/20000], Loss: 55.860595703125, Learning Rate: 0.01\n",
      "Epoch [2814/20000], Loss: 55.84025573730469, Learning Rate: 0.01\n",
      "Epoch [2815/20000], Loss: 55.820037841796875, Learning Rate: 0.01\n",
      "Epoch [2816/20000], Loss: 55.79949951171875, Learning Rate: 0.01\n",
      "Epoch [2817/20000], Loss: 55.77911376953125, Learning Rate: 0.01\n",
      "Epoch [2818/20000], Loss: 55.75901794433594, Learning Rate: 0.01\n",
      "Epoch [2819/20000], Loss: 55.7384033203125, Learning Rate: 0.01\n",
      "Epoch [2820/20000], Loss: 55.718170166015625, Learning Rate: 0.01\n",
      "Epoch [2821/20000], Loss: 55.69779968261719, Learning Rate: 0.01\n",
      "Epoch [2822/20000], Loss: 55.67755126953125, Learning Rate: 0.01\n",
      "Epoch [2823/20000], Loss: 55.65716552734375, Learning Rate: 0.01\n",
      "Epoch [2824/20000], Loss: 55.636871337890625, Learning Rate: 0.01\n",
      "Epoch [2825/20000], Loss: 55.61680603027344, Learning Rate: 0.01\n",
      "Epoch [2826/20000], Loss: 55.59645080566406, Learning Rate: 0.01\n",
      "Epoch [2827/20000], Loss: 55.57627868652344, Learning Rate: 0.01\n",
      "Epoch [2828/20000], Loss: 55.5560302734375, Learning Rate: 0.01\n",
      "Epoch [2829/20000], Loss: 55.53596496582031, Learning Rate: 0.01\n",
      "Epoch [2830/20000], Loss: 55.515655517578125, Learning Rate: 0.01\n",
      "Epoch [2831/20000], Loss: 55.495391845703125, Learning Rate: 0.01\n",
      "Epoch [2832/20000], Loss: 55.47509765625, Learning Rate: 0.01\n",
      "Epoch [2833/20000], Loss: 55.45500183105469, Learning Rate: 0.01\n",
      "Epoch [2834/20000], Loss: 55.4349365234375, Learning Rate: 0.01\n",
      "Epoch [2835/20000], Loss: 55.41474914550781, Learning Rate: 0.01\n",
      "Epoch [2836/20000], Loss: 55.39454650878906, Learning Rate: 0.01\n",
      "Epoch [2837/20000], Loss: 55.37440490722656, Learning Rate: 0.01\n",
      "Epoch [2838/20000], Loss: 55.354400634765625, Learning Rate: 0.01\n",
      "Epoch [2839/20000], Loss: 55.33442687988281, Learning Rate: 0.01\n",
      "Epoch [2840/20000], Loss: 55.31414794921875, Learning Rate: 0.01\n",
      "Epoch [2841/20000], Loss: 55.293975830078125, Learning Rate: 0.01\n",
      "Epoch [2842/20000], Loss: 55.27394104003906, Learning Rate: 0.01\n",
      "Epoch [2843/20000], Loss: 55.253753662109375, Learning Rate: 0.01\n",
      "Epoch [2844/20000], Loss: 55.23371887207031, Learning Rate: 0.01\n",
      "Epoch [2845/20000], Loss: 55.213897705078125, Learning Rate: 0.01\n",
      "Epoch [2846/20000], Loss: 55.19390869140625, Learning Rate: 0.01\n",
      "Epoch [2847/20000], Loss: 55.17378234863281, Learning Rate: 0.01\n",
      "Epoch [2848/20000], Loss: 55.15388488769531, Learning Rate: 0.01\n",
      "Epoch [2849/20000], Loss: 55.1337890625, Learning Rate: 0.01\n",
      "Epoch [2850/20000], Loss: 55.113739013671875, Learning Rate: 0.01\n",
      "Epoch [2851/20000], Loss: 55.09394836425781, Learning Rate: 0.01\n",
      "Epoch [2852/20000], Loss: 55.07402038574219, Learning Rate: 0.01\n",
      "Epoch [2853/20000], Loss: 55.05412292480469, Learning Rate: 0.01\n",
      "Epoch [2854/20000], Loss: 55.034149169921875, Learning Rate: 0.01\n",
      "Epoch [2855/20000], Loss: 55.014068603515625, Learning Rate: 0.01\n",
      "Epoch [2856/20000], Loss: 54.99421691894531, Learning Rate: 0.01\n",
      "Epoch [2857/20000], Loss: 54.97438049316406, Learning Rate: 0.01\n",
      "Epoch [2858/20000], Loss: 54.95448303222656, Learning Rate: 0.01\n",
      "Epoch [2859/20000], Loss: 54.934814453125, Learning Rate: 0.01\n",
      "Epoch [2860/20000], Loss: 54.91468811035156, Learning Rate: 0.01\n",
      "Epoch [2861/20000], Loss: 54.89497375488281, Learning Rate: 0.01\n",
      "Epoch [2862/20000], Loss: 54.875091552734375, Learning Rate: 0.01\n",
      "Epoch [2863/20000], Loss: 54.85533142089844, Learning Rate: 0.01\n",
      "Epoch [2864/20000], Loss: 54.83564758300781, Learning Rate: 0.01\n",
      "Epoch [2865/20000], Loss: 54.815887451171875, Learning Rate: 0.01\n",
      "Epoch [2866/20000], Loss: 54.79632568359375, Learning Rate: 0.01\n",
      "Epoch [2867/20000], Loss: 54.77690124511719, Learning Rate: 0.01\n",
      "Epoch [2868/20000], Loss: 54.757598876953125, Learning Rate: 0.01\n",
      "Epoch [2869/20000], Loss: 54.73826599121094, Learning Rate: 0.01\n",
      "Epoch [2870/20000], Loss: 54.71929931640625, Learning Rate: 0.01\n",
      "Epoch [2871/20000], Loss: 54.70072937011719, Learning Rate: 0.01\n",
      "Epoch [2872/20000], Loss: 54.68287658691406, Learning Rate: 0.01\n",
      "Epoch [2873/20000], Loss: 54.665435791015625, Learning Rate: 0.01\n",
      "Epoch [2874/20000], Loss: 54.648345947265625, Learning Rate: 0.01\n",
      "Epoch [2875/20000], Loss: 54.631072998046875, Learning Rate: 0.01\n",
      "Epoch [2876/20000], Loss: 54.61309814453125, Learning Rate: 0.01\n",
      "Epoch [2877/20000], Loss: 54.59309387207031, Learning Rate: 0.01\n",
      "Epoch [2878/20000], Loss: 54.57081604003906, Learning Rate: 0.01\n",
      "Epoch [2879/20000], Loss: 54.54705810546875, Learning Rate: 0.01\n",
      "Epoch [2880/20000], Loss: 54.52281188964844, Learning Rate: 0.01\n",
      "Epoch [2881/20000], Loss: 54.50086975097656, Learning Rate: 0.01\n",
      "Epoch [2882/20000], Loss: 54.481048583984375, Learning Rate: 0.01\n",
      "Epoch [2883/20000], Loss: 54.463287353515625, Learning Rate: 0.01\n",
      "Epoch [2884/20000], Loss: 54.445648193359375, Learning Rate: 0.01\n",
      "Epoch [2885/20000], Loss: 54.42742919921875, Learning Rate: 0.01\n",
      "Epoch [2886/20000], Loss: 54.407562255859375, Learning Rate: 0.01\n",
      "Epoch [2887/20000], Loss: 54.386260986328125, Learning Rate: 0.01\n",
      "Epoch [2888/20000], Loss: 54.36482238769531, Learning Rate: 0.01\n",
      "Epoch [2889/20000], Loss: 54.34403991699219, Learning Rate: 0.01\n",
      "Epoch [2890/20000], Loss: 54.324554443359375, Learning Rate: 0.01\n",
      "Epoch [2891/20000], Loss: 54.305938720703125, Learning Rate: 0.01\n",
      "Epoch [2892/20000], Loss: 54.28759765625, Learning Rate: 0.01\n",
      "Epoch [2893/20000], Loss: 54.26853942871094, Learning Rate: 0.01\n",
      "Epoch [2894/20000], Loss: 54.248565673828125, Learning Rate: 0.01\n",
      "Epoch [2895/20000], Loss: 54.2281494140625, Learning Rate: 0.01\n",
      "Epoch [2896/20000], Loss: 54.20790100097656, Learning Rate: 0.01\n",
      "Epoch [2897/20000], Loss: 54.188079833984375, Learning Rate: 0.01\n",
      "Epoch [2898/20000], Loss: 54.16912841796875, Learning Rate: 0.01\n",
      "Epoch [2899/20000], Loss: 54.150146484375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2900/20000], Loss: 54.13069152832031, Learning Rate: 0.01\n",
      "Epoch [2901/20000], Loss: 54.1116943359375, Learning Rate: 0.01\n",
      "Epoch [2902/20000], Loss: 54.09199523925781, Learning Rate: 0.01\n",
      "Epoch [2903/20000], Loss: 54.07206726074219, Learning Rate: 0.01\n",
      "Epoch [2904/20000], Loss: 54.05259704589844, Learning Rate: 0.01\n",
      "Epoch [2905/20000], Loss: 54.03318786621094, Learning Rate: 0.01\n",
      "Epoch [2906/20000], Loss: 54.01387023925781, Learning Rate: 0.01\n",
      "Epoch [2907/20000], Loss: 53.994781494140625, Learning Rate: 0.01\n",
      "Epoch [2908/20000], Loss: 53.975830078125, Learning Rate: 0.01\n",
      "Epoch [2909/20000], Loss: 53.95625305175781, Learning Rate: 0.01\n",
      "Epoch [2910/20000], Loss: 53.9368896484375, Learning Rate: 0.01\n",
      "Epoch [2911/20000], Loss: 53.91734313964844, Learning Rate: 0.01\n",
      "Epoch [2912/20000], Loss: 53.89788818359375, Learning Rate: 0.01\n",
      "Epoch [2913/20000], Loss: 53.878662109375, Learning Rate: 0.01\n",
      "Epoch [2914/20000], Loss: 53.85948181152344, Learning Rate: 0.01\n",
      "Epoch [2915/20000], Loss: 53.84034729003906, Learning Rate: 0.01\n",
      "Epoch [2916/20000], Loss: 53.8212890625, Learning Rate: 0.01\n",
      "Epoch [2917/20000], Loss: 53.801849365234375, Learning Rate: 0.01\n",
      "Epoch [2918/20000], Loss: 53.78257751464844, Learning Rate: 0.01\n",
      "Epoch [2919/20000], Loss: 53.76361083984375, Learning Rate: 0.01\n",
      "Epoch [2920/20000], Loss: 53.74418640136719, Learning Rate: 0.01\n",
      "Epoch [2921/20000], Loss: 53.72505187988281, Learning Rate: 0.01\n",
      "Epoch [2922/20000], Loss: 53.706024169921875, Learning Rate: 0.01\n",
      "Epoch [2923/20000], Loss: 53.686767578125, Learning Rate: 0.01\n",
      "Epoch [2924/20000], Loss: 53.667724609375, Learning Rate: 0.01\n",
      "Epoch [2925/20000], Loss: 53.648590087890625, Learning Rate: 0.01\n",
      "Epoch [2926/20000], Loss: 53.629486083984375, Learning Rate: 0.01\n",
      "Epoch [2927/20000], Loss: 53.61016845703125, Learning Rate: 0.01\n",
      "Epoch [2928/20000], Loss: 53.5911865234375, Learning Rate: 0.01\n",
      "Epoch [2929/20000], Loss: 53.57196044921875, Learning Rate: 0.01\n",
      "Epoch [2930/20000], Loss: 53.55278015136719, Learning Rate: 0.01\n",
      "Epoch [2931/20000], Loss: 53.53388977050781, Learning Rate: 0.01\n",
      "Epoch [2932/20000], Loss: 53.51496887207031, Learning Rate: 0.01\n",
      "Epoch [2933/20000], Loss: 53.495849609375, Learning Rate: 0.01\n",
      "Epoch [2934/20000], Loss: 53.47682189941406, Learning Rate: 0.01\n",
      "Epoch [2935/20000], Loss: 53.45771789550781, Learning Rate: 0.01\n",
      "Epoch [2936/20000], Loss: 53.438690185546875, Learning Rate: 0.01\n",
      "Epoch [2937/20000], Loss: 53.41973876953125, Learning Rate: 0.01\n",
      "Epoch [2938/20000], Loss: 53.400787353515625, Learning Rate: 0.01\n",
      "Epoch [2939/20000], Loss: 53.38185119628906, Learning Rate: 0.01\n",
      "Epoch [2940/20000], Loss: 53.36277770996094, Learning Rate: 0.01\n",
      "Epoch [2941/20000], Loss: 53.34403991699219, Learning Rate: 0.01\n",
      "Epoch [2942/20000], Loss: 53.324920654296875, Learning Rate: 0.01\n",
      "Epoch [2943/20000], Loss: 53.306121826171875, Learning Rate: 0.01\n",
      "Epoch [2944/20000], Loss: 53.287139892578125, Learning Rate: 0.01\n",
      "Epoch [2945/20000], Loss: 53.26823425292969, Learning Rate: 0.01\n",
      "Epoch [2946/20000], Loss: 53.24925231933594, Learning Rate: 0.01\n",
      "Epoch [2947/20000], Loss: 53.230194091796875, Learning Rate: 0.01\n",
      "Epoch [2948/20000], Loss: 53.21131896972656, Learning Rate: 0.01\n",
      "Epoch [2949/20000], Loss: 53.19276428222656, Learning Rate: 0.01\n",
      "Epoch [2950/20000], Loss: 53.173675537109375, Learning Rate: 0.01\n",
      "Epoch [2951/20000], Loss: 53.15483093261719, Learning Rate: 0.01\n",
      "Epoch [2952/20000], Loss: 53.13600158691406, Learning Rate: 0.01\n",
      "Epoch [2953/20000], Loss: 53.117156982421875, Learning Rate: 0.01\n",
      "Epoch [2954/20000], Loss: 53.09846496582031, Learning Rate: 0.01\n",
      "Epoch [2955/20000], Loss: 53.07951354980469, Learning Rate: 0.01\n",
      "Epoch [2956/20000], Loss: 53.06092834472656, Learning Rate: 0.01\n",
      "Epoch [2957/20000], Loss: 53.0421142578125, Learning Rate: 0.01\n",
      "Epoch [2958/20000], Loss: 53.02325439453125, Learning Rate: 0.01\n",
      "Epoch [2959/20000], Loss: 53.00445556640625, Learning Rate: 0.01\n",
      "Epoch [2960/20000], Loss: 52.98579406738281, Learning Rate: 0.01\n",
      "Epoch [2961/20000], Loss: 52.96714782714844, Learning Rate: 0.01\n",
      "Epoch [2962/20000], Loss: 52.94822692871094, Learning Rate: 0.01\n",
      "Epoch [2963/20000], Loss: 52.929412841796875, Learning Rate: 0.01\n",
      "Epoch [2964/20000], Loss: 52.91070556640625, Learning Rate: 0.01\n",
      "Epoch [2965/20000], Loss: 52.89202880859375, Learning Rate: 0.01\n",
      "Epoch [2966/20000], Loss: 52.87351989746094, Learning Rate: 0.01\n",
      "Epoch [2967/20000], Loss: 52.85484313964844, Learning Rate: 0.01\n",
      "Epoch [2968/20000], Loss: 52.83598327636719, Learning Rate: 0.01\n",
      "Epoch [2969/20000], Loss: 52.81724548339844, Learning Rate: 0.01\n",
      "Epoch [2970/20000], Loss: 52.7987060546875, Learning Rate: 0.01\n",
      "Epoch [2971/20000], Loss: 52.780059814453125, Learning Rate: 0.01\n",
      "Epoch [2972/20000], Loss: 52.761474609375, Learning Rate: 0.01\n",
      "Epoch [2973/20000], Loss: 52.74287414550781, Learning Rate: 0.01\n",
      "Epoch [2974/20000], Loss: 52.72431945800781, Learning Rate: 0.01\n",
      "Epoch [2975/20000], Loss: 52.7056884765625, Learning Rate: 0.01\n",
      "Epoch [2976/20000], Loss: 52.68690490722656, Learning Rate: 0.01\n",
      "Epoch [2977/20000], Loss: 52.66847229003906, Learning Rate: 0.01\n",
      "Epoch [2978/20000], Loss: 52.64996337890625, Learning Rate: 0.01\n",
      "Epoch [2979/20000], Loss: 52.63134765625, Learning Rate: 0.01\n",
      "Epoch [2980/20000], Loss: 52.61285400390625, Learning Rate: 0.01\n",
      "Epoch [2981/20000], Loss: 52.59417724609375, Learning Rate: 0.01\n",
      "Epoch [2982/20000], Loss: 52.57554626464844, Learning Rate: 0.01\n",
      "Epoch [2983/20000], Loss: 52.55718994140625, Learning Rate: 0.01\n",
      "Epoch [2984/20000], Loss: 52.53849792480469, Learning Rate: 0.01\n",
      "Epoch [2985/20000], Loss: 52.52018737792969, Learning Rate: 0.01\n",
      "Epoch [2986/20000], Loss: 52.50178527832031, Learning Rate: 0.01\n",
      "Epoch [2987/20000], Loss: 52.483184814453125, Learning Rate: 0.01\n",
      "Epoch [2988/20000], Loss: 52.464752197265625, Learning Rate: 0.01\n",
      "Epoch [2989/20000], Loss: 52.446380615234375, Learning Rate: 0.01\n",
      "Epoch [2990/20000], Loss: 52.427764892578125, Learning Rate: 0.01\n",
      "Epoch [2991/20000], Loss: 52.40931701660156, Learning Rate: 0.01\n",
      "Epoch [2992/20000], Loss: 52.39100646972656, Learning Rate: 0.01\n",
      "Epoch [2993/20000], Loss: 52.3724365234375, Learning Rate: 0.01\n",
      "Epoch [2994/20000], Loss: 52.35404968261719, Learning Rate: 0.01\n",
      "Epoch [2995/20000], Loss: 52.335723876953125, Learning Rate: 0.01\n",
      "Epoch [2996/20000], Loss: 52.317291259765625, Learning Rate: 0.01\n",
      "Epoch [2997/20000], Loss: 52.29908752441406, Learning Rate: 0.01\n",
      "Epoch [2998/20000], Loss: 52.28068542480469, Learning Rate: 0.01\n",
      "Epoch [2999/20000], Loss: 52.26234436035156, Learning Rate: 0.01\n",
      "Epoch [3000/20000], Loss: 52.243865966796875, Learning Rate: 0.01\n",
      "Epoch [3001/20000], Loss: 52.22550964355469, Learning Rate: 0.01\n",
      "Epoch [3002/20000], Loss: 52.20716857910156, Learning Rate: 0.01\n",
      "Epoch [3003/20000], Loss: 52.188995361328125, Learning Rate: 0.01\n",
      "Epoch [3004/20000], Loss: 52.17048645019531, Learning Rate: 0.01\n",
      "Epoch [3005/20000], Loss: 52.15228271484375, Learning Rate: 0.01\n",
      "Epoch [3006/20000], Loss: 52.13397216796875, Learning Rate: 0.01\n",
      "Epoch [3007/20000], Loss: 52.11566162109375, Learning Rate: 0.01\n",
      "Epoch [3008/20000], Loss: 52.09759521484375, Learning Rate: 0.01\n",
      "Epoch [3009/20000], Loss: 52.079254150390625, Learning Rate: 0.01\n",
      "Epoch [3010/20000], Loss: 52.06103515625, Learning Rate: 0.01\n",
      "Epoch [3011/20000], Loss: 52.04280090332031, Learning Rate: 0.01\n",
      "Epoch [3012/20000], Loss: 52.02458190917969, Learning Rate: 0.01\n",
      "Epoch [3013/20000], Loss: 52.00627136230469, Learning Rate: 0.01\n",
      "Epoch [3014/20000], Loss: 51.988128662109375, Learning Rate: 0.01\n",
      "Epoch [3015/20000], Loss: 51.97001647949219, Learning Rate: 0.01\n",
      "Epoch [3016/20000], Loss: 51.9517822265625, Learning Rate: 0.01\n",
      "Epoch [3017/20000], Loss: 51.93353271484375, Learning Rate: 0.01\n",
      "Epoch [3018/20000], Loss: 51.91551208496094, Learning Rate: 0.01\n",
      "Epoch [3019/20000], Loss: 51.89726257324219, Learning Rate: 0.01\n",
      "Epoch [3020/20000], Loss: 51.87904357910156, Learning Rate: 0.01\n",
      "Epoch [3021/20000], Loss: 51.86082458496094, Learning Rate: 0.01\n",
      "Epoch [3022/20000], Loss: 51.842803955078125, Learning Rate: 0.01\n",
      "Epoch [3023/20000], Loss: 51.824737548828125, Learning Rate: 0.01\n",
      "Epoch [3024/20000], Loss: 51.80659484863281, Learning Rate: 0.01\n",
      "Epoch [3025/20000], Loss: 51.78846740722656, Learning Rate: 0.01\n",
      "Epoch [3026/20000], Loss: 51.77055358886719, Learning Rate: 0.01\n",
      "Epoch [3027/20000], Loss: 51.75230407714844, Learning Rate: 0.01\n",
      "Epoch [3028/20000], Loss: 51.7342529296875, Learning Rate: 0.01\n",
      "Epoch [3029/20000], Loss: 51.71598815917969, Learning Rate: 0.01\n",
      "Epoch [3030/20000], Loss: 51.6981201171875, Learning Rate: 0.01\n",
      "Epoch [3031/20000], Loss: 51.680206298828125, Learning Rate: 0.01\n",
      "Epoch [3032/20000], Loss: 51.662139892578125, Learning Rate: 0.01\n",
      "Epoch [3033/20000], Loss: 51.64396667480469, Learning Rate: 0.01\n",
      "Epoch [3034/20000], Loss: 51.62605285644531, Learning Rate: 0.01\n",
      "Epoch [3035/20000], Loss: 51.60809326171875, Learning Rate: 0.01\n",
      "Epoch [3036/20000], Loss: 51.59025573730469, Learning Rate: 0.01\n",
      "Epoch [3037/20000], Loss: 51.572235107421875, Learning Rate: 0.01\n",
      "Epoch [3038/20000], Loss: 51.554412841796875, Learning Rate: 0.01\n",
      "Epoch [3039/20000], Loss: 51.53668212890625, Learning Rate: 0.01\n",
      "Epoch [3040/20000], Loss: 51.5191650390625, Learning Rate: 0.01\n",
      "Epoch [3041/20000], Loss: 51.50196838378906, Learning Rate: 0.01\n",
      "Epoch [3042/20000], Loss: 51.4849853515625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3043/20000], Loss: 51.46891784667969, Learning Rate: 0.01\n",
      "Epoch [3044/20000], Loss: 51.453460693359375, Learning Rate: 0.01\n",
      "Epoch [3045/20000], Loss: 51.43922424316406, Learning Rate: 0.01\n",
      "Epoch [3046/20000], Loss: 51.42662048339844, Learning Rate: 0.01\n",
      "Epoch [3047/20000], Loss: 51.41520690917969, Learning Rate: 0.01\n",
      "Epoch [3048/20000], Loss: 51.40415954589844, Learning Rate: 0.01\n",
      "Epoch [3049/20000], Loss: 51.39030456542969, Learning Rate: 0.01\n",
      "Epoch [3050/20000], Loss: 51.371124267578125, Learning Rate: 0.01\n",
      "Epoch [3051/20000], Loss: 51.345489501953125, Learning Rate: 0.01\n",
      "Epoch [3052/20000], Loss: 51.31634521484375, Learning Rate: 0.01\n",
      "Epoch [3053/20000], Loss: 51.28887939453125, Learning Rate: 0.01\n",
      "Epoch [3054/20000], Loss: 51.26774597167969, Learning Rate: 0.01\n",
      "Epoch [3055/20000], Loss: 51.252777099609375, Learning Rate: 0.01\n",
      "Epoch [3056/20000], Loss: 51.24076843261719, Learning Rate: 0.01\n",
      "Epoch [3057/20000], Loss: 51.22718811035156, Learning Rate: 0.01\n",
      "Epoch [3058/20000], Loss: 51.20909118652344, Learning Rate: 0.01\n",
      "Epoch [3059/20000], Loss: 51.18658447265625, Learning Rate: 0.01\n",
      "Epoch [3060/20000], Loss: 51.163818359375, Learning Rate: 0.01\n",
      "Epoch [3061/20000], Loss: 51.14324951171875, Learning Rate: 0.01\n",
      "Epoch [3062/20000], Loss: 51.1265869140625, Learning Rate: 0.01\n",
      "Epoch [3063/20000], Loss: 51.11140441894531, Learning Rate: 0.01\n",
      "Epoch [3064/20000], Loss: 51.095947265625, Learning Rate: 0.01\n",
      "Epoch [3065/20000], Loss: 51.07810974121094, Learning Rate: 0.01\n",
      "Epoch [3066/20000], Loss: 51.05827331542969, Learning Rate: 0.01\n",
      "Epoch [3067/20000], Loss: 51.037994384765625, Learning Rate: 0.01\n",
      "Epoch [3068/20000], Loss: 51.019195556640625, Learning Rate: 0.01\n",
      "Epoch [3069/20000], Loss: 51.00213623046875, Learning Rate: 0.01\n",
      "Epoch [3070/20000], Loss: 50.98570251464844, Learning Rate: 0.01\n",
      "Epoch [3071/20000], Loss: 50.96907043457031, Learning Rate: 0.01\n",
      "Epoch [3072/20000], Loss: 50.95133972167969, Learning Rate: 0.01\n",
      "Epoch [3073/20000], Loss: 50.932464599609375, Learning Rate: 0.01\n",
      "Epoch [3074/20000], Loss: 50.913665771484375, Learning Rate: 0.01\n",
      "Epoch [3075/20000], Loss: 50.89552307128906, Learning Rate: 0.01\n",
      "Epoch [3076/20000], Loss: 50.878143310546875, Learning Rate: 0.01\n",
      "Epoch [3077/20000], Loss: 50.86126708984375, Learning Rate: 0.01\n",
      "Epoch [3078/20000], Loss: 50.84449768066406, Learning Rate: 0.01\n",
      "Epoch [3079/20000], Loss: 50.82661437988281, Learning Rate: 0.01\n",
      "Epoch [3080/20000], Loss: 50.80827331542969, Learning Rate: 0.01\n",
      "Epoch [3081/20000], Loss: 50.79034423828125, Learning Rate: 0.01\n",
      "Epoch [3082/20000], Loss: 50.7723388671875, Learning Rate: 0.01\n",
      "Epoch [3083/20000], Loss: 50.75506591796875, Learning Rate: 0.01\n",
      "Epoch [3084/20000], Loss: 50.73765563964844, Learning Rate: 0.01\n",
      "Epoch [3085/20000], Loss: 50.72027587890625, Learning Rate: 0.01\n",
      "Epoch [3086/20000], Loss: 50.70280456542969, Learning Rate: 0.01\n",
      "Epoch [3087/20000], Loss: 50.68524169921875, Learning Rate: 0.01\n",
      "Epoch [3088/20000], Loss: 50.66728210449219, Learning Rate: 0.01\n",
      "Epoch [3089/20000], Loss: 50.649627685546875, Learning Rate: 0.01\n",
      "Epoch [3090/20000], Loss: 50.63215637207031, Learning Rate: 0.01\n",
      "Epoch [3091/20000], Loss: 50.614990234375, Learning Rate: 0.01\n",
      "Epoch [3092/20000], Loss: 50.59733581542969, Learning Rate: 0.01\n",
      "Epoch [3093/20000], Loss: 50.58003234863281, Learning Rate: 0.01\n",
      "Epoch [3094/20000], Loss: 50.562469482421875, Learning Rate: 0.01\n",
      "Epoch [3095/20000], Loss: 50.54499816894531, Learning Rate: 0.01\n",
      "Epoch [3096/20000], Loss: 50.5274658203125, Learning Rate: 0.01\n",
      "Epoch [3097/20000], Loss: 50.50996398925781, Learning Rate: 0.01\n",
      "Epoch [3098/20000], Loss: 50.49244689941406, Learning Rate: 0.01\n",
      "Epoch [3099/20000], Loss: 50.475250244140625, Learning Rate: 0.01\n",
      "Epoch [3100/20000], Loss: 50.45806884765625, Learning Rate: 0.01\n",
      "Epoch [3101/20000], Loss: 50.440338134765625, Learning Rate: 0.01\n",
      "Epoch [3102/20000], Loss: 50.42298889160156, Learning Rate: 0.01\n",
      "Epoch [3103/20000], Loss: 50.40559387207031, Learning Rate: 0.01\n",
      "Epoch [3104/20000], Loss: 50.38807678222656, Learning Rate: 0.01\n",
      "Epoch [3105/20000], Loss: 50.37095642089844, Learning Rate: 0.01\n",
      "Epoch [3106/20000], Loss: 50.353485107421875, Learning Rate: 0.01\n",
      "Epoch [3107/20000], Loss: 50.33610534667969, Learning Rate: 0.01\n",
      "Epoch [3108/20000], Loss: 50.318878173828125, Learning Rate: 0.01\n",
      "Epoch [3109/20000], Loss: 50.30157470703125, Learning Rate: 0.01\n",
      "Epoch [3110/20000], Loss: 50.28399658203125, Learning Rate: 0.01\n",
      "Epoch [3111/20000], Loss: 50.26692199707031, Learning Rate: 0.01\n",
      "Epoch [3112/20000], Loss: 50.24952697753906, Learning Rate: 0.01\n",
      "Epoch [3113/20000], Loss: 50.2320556640625, Learning Rate: 0.01\n",
      "Epoch [3114/20000], Loss: 50.21478271484375, Learning Rate: 0.01\n",
      "Epoch [3115/20000], Loss: 50.19781494140625, Learning Rate: 0.01\n",
      "Epoch [3116/20000], Loss: 50.180572509765625, Learning Rate: 0.01\n",
      "Epoch [3117/20000], Loss: 50.16307067871094, Learning Rate: 0.01\n",
      "Epoch [3118/20000], Loss: 50.146026611328125, Learning Rate: 0.01\n",
      "Epoch [3119/20000], Loss: 50.12876892089844, Learning Rate: 0.01\n",
      "Epoch [3120/20000], Loss: 50.11138916015625, Learning Rate: 0.01\n",
      "Epoch [3121/20000], Loss: 50.09422302246094, Learning Rate: 0.01\n",
      "Epoch [3122/20000], Loss: 50.07701110839844, Learning Rate: 0.01\n",
      "Epoch [3123/20000], Loss: 50.05987548828125, Learning Rate: 0.01\n",
      "Epoch [3124/20000], Loss: 50.04264831542969, Learning Rate: 0.01\n",
      "Epoch [3125/20000], Loss: 50.02552795410156, Learning Rate: 0.01\n",
      "Epoch [3126/20000], Loss: 50.00843811035156, Learning Rate: 0.01\n",
      "Epoch [3127/20000], Loss: 49.99113464355469, Learning Rate: 0.01\n",
      "Epoch [3128/20000], Loss: 49.97406005859375, Learning Rate: 0.01\n",
      "Epoch [3129/20000], Loss: 49.956695556640625, Learning Rate: 0.01\n",
      "Epoch [3130/20000], Loss: 49.939666748046875, Learning Rate: 0.01\n",
      "Epoch [3131/20000], Loss: 49.922515869140625, Learning Rate: 0.01\n",
      "Epoch [3132/20000], Loss: 49.90544128417969, Learning Rate: 0.01\n",
      "Epoch [3133/20000], Loss: 49.88818359375, Learning Rate: 0.01\n",
      "Epoch [3134/20000], Loss: 49.871124267578125, Learning Rate: 0.01\n",
      "Epoch [3135/20000], Loss: 49.8541259765625, Learning Rate: 0.01\n",
      "Epoch [3136/20000], Loss: 49.83683776855469, Learning Rate: 0.01\n",
      "Epoch [3137/20000], Loss: 49.82011413574219, Learning Rate: 0.01\n",
      "Epoch [3138/20000], Loss: 49.80291748046875, Learning Rate: 0.01\n",
      "Epoch [3139/20000], Loss: 49.78581237792969, Learning Rate: 0.01\n",
      "Epoch [3140/20000], Loss: 49.76869201660156, Learning Rate: 0.01\n",
      "Epoch [3141/20000], Loss: 49.75163269042969, Learning Rate: 0.01\n",
      "Epoch [3142/20000], Loss: 49.73469543457031, Learning Rate: 0.01\n",
      "Epoch [3143/20000], Loss: 49.717498779296875, Learning Rate: 0.01\n",
      "Epoch [3144/20000], Loss: 49.70068359375, Learning Rate: 0.01\n",
      "Epoch [3145/20000], Loss: 49.683380126953125, Learning Rate: 0.01\n",
      "Epoch [3146/20000], Loss: 49.66658020019531, Learning Rate: 0.01\n",
      "Epoch [3147/20000], Loss: 49.64945983886719, Learning Rate: 0.01\n",
      "Epoch [3148/20000], Loss: 49.63252258300781, Learning Rate: 0.01\n",
      "Epoch [3149/20000], Loss: 49.61540222167969, Learning Rate: 0.01\n",
      "Epoch [3150/20000], Loss: 49.598663330078125, Learning Rate: 0.01\n",
      "Epoch [3151/20000], Loss: 49.58174133300781, Learning Rate: 0.01\n",
      "Epoch [3152/20000], Loss: 49.564544677734375, Learning Rate: 0.01\n",
      "Epoch [3153/20000], Loss: 49.547607421875, Learning Rate: 0.01\n",
      "Epoch [3154/20000], Loss: 49.53076171875, Learning Rate: 0.01\n",
      "Epoch [3155/20000], Loss: 49.51393127441406, Learning Rate: 0.01\n",
      "Epoch [3156/20000], Loss: 49.49687194824219, Learning Rate: 0.01\n",
      "Epoch [3157/20000], Loss: 49.47993469238281, Learning Rate: 0.01\n",
      "Epoch [3158/20000], Loss: 49.46308898925781, Learning Rate: 0.01\n",
      "Epoch [3159/20000], Loss: 49.44630432128906, Learning Rate: 0.01\n",
      "Epoch [3160/20000], Loss: 49.429443359375, Learning Rate: 0.01\n",
      "Epoch [3161/20000], Loss: 49.41242980957031, Learning Rate: 0.01\n",
      "Epoch [3162/20000], Loss: 49.39546203613281, Learning Rate: 0.01\n",
      "Epoch [3163/20000], Loss: 49.37849426269531, Learning Rate: 0.01\n",
      "Epoch [3164/20000], Loss: 49.36180114746094, Learning Rate: 0.01\n",
      "Epoch [3165/20000], Loss: 49.345123291015625, Learning Rate: 0.01\n",
      "Epoch [3166/20000], Loss: 49.328125, Learning Rate: 0.01\n",
      "Epoch [3167/20000], Loss: 49.311309814453125, Learning Rate: 0.01\n",
      "Epoch [3168/20000], Loss: 49.29443359375, Learning Rate: 0.01\n",
      "Epoch [3169/20000], Loss: 49.27763366699219, Learning Rate: 0.01\n",
      "Epoch [3170/20000], Loss: 49.260894775390625, Learning Rate: 0.01\n",
      "Epoch [3171/20000], Loss: 49.24406433105469, Learning Rate: 0.01\n",
      "Epoch [3172/20000], Loss: 49.227386474609375, Learning Rate: 0.01\n",
      "Epoch [3173/20000], Loss: 49.21061706542969, Learning Rate: 0.01\n",
      "Epoch [3174/20000], Loss: 49.193572998046875, Learning Rate: 0.01\n",
      "Epoch [3175/20000], Loss: 49.176910400390625, Learning Rate: 0.01\n",
      "Epoch [3176/20000], Loss: 49.16021728515625, Learning Rate: 0.01\n",
      "Epoch [3177/20000], Loss: 49.143341064453125, Learning Rate: 0.01\n",
      "Epoch [3178/20000], Loss: 49.12675476074219, Learning Rate: 0.01\n",
      "Epoch [3179/20000], Loss: 49.10997009277344, Learning Rate: 0.01\n",
      "Epoch [3180/20000], Loss: 49.09315490722656, Learning Rate: 0.01\n",
      "Epoch [3181/20000], Loss: 49.07649230957031, Learning Rate: 0.01\n",
      "Epoch [3182/20000], Loss: 49.0596923828125, Learning Rate: 0.01\n",
      "Epoch [3183/20000], Loss: 49.04319763183594, Learning Rate: 0.01\n",
      "Epoch [3184/20000], Loss: 49.0263671875, Learning Rate: 0.01\n",
      "Epoch [3185/20000], Loss: 49.00981140136719, Learning Rate: 0.01\n",
      "Epoch [3186/20000], Loss: 48.992950439453125, Learning Rate: 0.01\n",
      "Epoch [3187/20000], Loss: 48.97637939453125, Learning Rate: 0.01\n",
      "Epoch [3188/20000], Loss: 48.95954895019531, Learning Rate: 0.01\n",
      "Epoch [3189/20000], Loss: 48.94297790527344, Learning Rate: 0.01\n",
      "Epoch [3190/20000], Loss: 48.92646789550781, Learning Rate: 0.01\n",
      "Epoch [3191/20000], Loss: 48.909698486328125, Learning Rate: 0.01\n",
      "Epoch [3192/20000], Loss: 48.89295959472656, Learning Rate: 0.01\n",
      "Epoch [3193/20000], Loss: 48.876312255859375, Learning Rate: 0.01\n",
      "Epoch [3194/20000], Loss: 48.85993957519531, Learning Rate: 0.01\n",
      "Epoch [3195/20000], Loss: 48.84312438964844, Learning Rate: 0.01\n",
      "Epoch [3196/20000], Loss: 48.82658386230469, Learning Rate: 0.01\n",
      "Epoch [3197/20000], Loss: 48.810089111328125, Learning Rate: 0.01\n",
      "Epoch [3198/20000], Loss: 48.793426513671875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3199/20000], Loss: 48.77684020996094, Learning Rate: 0.01\n",
      "Epoch [3200/20000], Loss: 48.76043701171875, Learning Rate: 0.01\n",
      "Epoch [3201/20000], Loss: 48.743804931640625, Learning Rate: 0.01\n",
      "Epoch [3202/20000], Loss: 48.72706604003906, Learning Rate: 0.01\n",
      "Epoch [3203/20000], Loss: 48.71070861816406, Learning Rate: 0.01\n",
      "Epoch [3204/20000], Loss: 48.694183349609375, Learning Rate: 0.01\n",
      "Epoch [3205/20000], Loss: 48.67755126953125, Learning Rate: 0.01\n",
      "Epoch [3206/20000], Loss: 48.66119384765625, Learning Rate: 0.01\n",
      "Epoch [3207/20000], Loss: 48.64445495605469, Learning Rate: 0.01\n",
      "Epoch [3208/20000], Loss: 48.62809753417969, Learning Rate: 0.01\n",
      "Epoch [3209/20000], Loss: 48.61158752441406, Learning Rate: 0.01\n",
      "Epoch [3210/20000], Loss: 48.594940185546875, Learning Rate: 0.01\n",
      "Epoch [3211/20000], Loss: 48.57861328125, Learning Rate: 0.01\n",
      "Epoch [3212/20000], Loss: 48.561981201171875, Learning Rate: 0.01\n",
      "Epoch [3213/20000], Loss: 48.54563903808594, Learning Rate: 0.01\n",
      "Epoch [3214/20000], Loss: 48.52912902832031, Learning Rate: 0.01\n",
      "Epoch [3215/20000], Loss: 48.5128173828125, Learning Rate: 0.01\n",
      "Epoch [3216/20000], Loss: 48.49620056152344, Learning Rate: 0.01\n",
      "Epoch [3217/20000], Loss: 48.479827880859375, Learning Rate: 0.01\n",
      "Epoch [3218/20000], Loss: 48.463348388671875, Learning Rate: 0.01\n",
      "Epoch [3219/20000], Loss: 48.447021484375, Learning Rate: 0.01\n",
      "Epoch [3220/20000], Loss: 48.43049621582031, Learning Rate: 0.01\n",
      "Epoch [3221/20000], Loss: 48.414306640625, Learning Rate: 0.01\n",
      "Epoch [3222/20000], Loss: 48.39776611328125, Learning Rate: 0.01\n",
      "Epoch [3223/20000], Loss: 48.38145446777344, Learning Rate: 0.01\n",
      "Epoch [3224/20000], Loss: 48.36494445800781, Learning Rate: 0.01\n",
      "Epoch [3225/20000], Loss: 48.34869384765625, Learning Rate: 0.01\n",
      "Epoch [3226/20000], Loss: 48.332305908203125, Learning Rate: 0.01\n",
      "Epoch [3227/20000], Loss: 48.316070556640625, Learning Rate: 0.01\n",
      "Epoch [3228/20000], Loss: 48.29986572265625, Learning Rate: 0.01\n",
      "Epoch [3229/20000], Loss: 48.28343200683594, Learning Rate: 0.01\n",
      "Epoch [3230/20000], Loss: 48.26725769042969, Learning Rate: 0.01\n",
      "Epoch [3231/20000], Loss: 48.251220703125, Learning Rate: 0.01\n",
      "Epoch [3232/20000], Loss: 48.23558044433594, Learning Rate: 0.01\n",
      "Epoch [3233/20000], Loss: 48.21978759765625, Learning Rate: 0.01\n",
      "Epoch [3234/20000], Loss: 48.204376220703125, Learning Rate: 0.01\n",
      "Epoch [3235/20000], Loss: 48.189971923828125, Learning Rate: 0.01\n",
      "Epoch [3236/20000], Loss: 48.17616271972656, Learning Rate: 0.01\n",
      "Epoch [3237/20000], Loss: 48.16374206542969, Learning Rate: 0.01\n",
      "Epoch [3238/20000], Loss: 48.15327453613281, Learning Rate: 0.01\n",
      "Epoch [3239/20000], Loss: 48.145355224609375, Learning Rate: 0.01\n",
      "Epoch [3240/20000], Loss: 48.139739990234375, Learning Rate: 0.01\n",
      "Epoch [3241/20000], Loss: 48.13545227050781, Learning Rate: 0.01\n",
      "Epoch [3242/20000], Loss: 48.12998962402344, Learning Rate: 0.01\n",
      "Epoch [3243/20000], Loss: 48.1187744140625, Learning Rate: 0.01\n",
      "Epoch [3244/20000], Loss: 48.09735107421875, Learning Rate: 0.01\n",
      "Epoch [3245/20000], Loss: 48.06581115722656, Learning Rate: 0.01\n",
      "Epoch [3246/20000], Loss: 48.02943420410156, Learning Rate: 0.01\n",
      "Epoch [3247/20000], Loss: 47.996612548828125, Learning Rate: 0.01\n",
      "Epoch [3248/20000], Loss: 47.97392272949219, Learning Rate: 0.01\n",
      "Epoch [3249/20000], Loss: 47.962005615234375, Learning Rate: 0.01\n",
      "Epoch [3250/20000], Loss: 47.955230712890625, Learning Rate: 0.01\n",
      "Epoch [3251/20000], Loss: 47.94731140136719, Learning Rate: 0.01\n",
      "Epoch [3252/20000], Loss: 47.93316650390625, Learning Rate: 0.01\n",
      "Epoch [3253/20000], Loss: 47.91188049316406, Learning Rate: 0.01\n",
      "Epoch [3254/20000], Loss: 47.88665771484375, Learning Rate: 0.01\n",
      "Epoch [3255/20000], Loss: 47.86317443847656, Learning Rate: 0.01\n",
      "Epoch [3256/20000], Loss: 47.8446044921875, Learning Rate: 0.01\n",
      "Epoch [3257/20000], Loss: 47.830963134765625, Learning Rate: 0.01\n",
      "Epoch [3258/20000], Loss: 47.81951904296875, Learning Rate: 0.01\n",
      "Epoch [3259/20000], Loss: 47.80668640136719, Learning Rate: 0.01\n",
      "Epoch [3260/20000], Loss: 47.79035949707031, Learning Rate: 0.01\n",
      "Epoch [3261/20000], Loss: 47.771087646484375, Learning Rate: 0.01\n",
      "Epoch [3262/20000], Loss: 47.75105285644531, Learning Rate: 0.01\n",
      "Epoch [3263/20000], Loss: 47.73231506347656, Learning Rate: 0.01\n",
      "Epoch [3264/20000], Loss: 47.71580505371094, Learning Rate: 0.01\n",
      "Epoch [3265/20000], Loss: 47.70166015625, Learning Rate: 0.01\n",
      "Epoch [3266/20000], Loss: 47.687469482421875, Learning Rate: 0.01\n",
      "Epoch [3267/20000], Loss: 47.672515869140625, Learning Rate: 0.01\n",
      "Epoch [3268/20000], Loss: 47.655731201171875, Learning Rate: 0.01\n",
      "Epoch [3269/20000], Loss: 47.6380615234375, Learning Rate: 0.01\n",
      "Epoch [3270/20000], Loss: 47.62055969238281, Learning Rate: 0.01\n",
      "Epoch [3271/20000], Loss: 47.60350036621094, Learning Rate: 0.01\n",
      "Epoch [3272/20000], Loss: 47.58753967285156, Learning Rate: 0.01\n",
      "Epoch [3273/20000], Loss: 47.5723876953125, Learning Rate: 0.01\n",
      "Epoch [3274/20000], Loss: 47.55726623535156, Learning Rate: 0.01\n",
      "Epoch [3275/20000], Loss: 47.54145812988281, Learning Rate: 0.01\n",
      "Epoch [3276/20000], Loss: 47.525054931640625, Learning Rate: 0.01\n",
      "Epoch [3277/20000], Loss: 47.508270263671875, Learning Rate: 0.01\n",
      "Epoch [3278/20000], Loss: 47.4918212890625, Learning Rate: 0.01\n",
      "Epoch [3279/20000], Loss: 47.475250244140625, Learning Rate: 0.01\n",
      "Epoch [3280/20000], Loss: 47.45933532714844, Learning Rate: 0.01\n",
      "Epoch [3281/20000], Loss: 47.44383239746094, Learning Rate: 0.01\n",
      "Epoch [3282/20000], Loss: 47.42829895019531, Learning Rate: 0.01\n",
      "Epoch [3283/20000], Loss: 47.412353515625, Learning Rate: 0.01\n",
      "Epoch [3284/20000], Loss: 47.39649963378906, Learning Rate: 0.01\n",
      "Epoch [3285/20000], Loss: 47.38014221191406, Learning Rate: 0.01\n",
      "Epoch [3286/20000], Loss: 47.36399841308594, Learning Rate: 0.01\n",
      "Epoch [3287/20000], Loss: 47.34776306152344, Learning Rate: 0.01\n",
      "Epoch [3288/20000], Loss: 47.331817626953125, Learning Rate: 0.01\n",
      "Epoch [3289/20000], Loss: 47.31602478027344, Learning Rate: 0.01\n",
      "Epoch [3290/20000], Loss: 47.30039978027344, Learning Rate: 0.01\n",
      "Epoch [3291/20000], Loss: 47.284637451171875, Learning Rate: 0.01\n",
      "Epoch [3292/20000], Loss: 47.26861572265625, Learning Rate: 0.01\n",
      "Epoch [3293/20000], Loss: 47.2528076171875, Learning Rate: 0.01\n",
      "Epoch [3294/20000], Loss: 47.236572265625, Learning Rate: 0.01\n",
      "Epoch [3295/20000], Loss: 47.2205810546875, Learning Rate: 0.01\n",
      "Epoch [3296/20000], Loss: 47.20484924316406, Learning Rate: 0.01\n",
      "Epoch [3297/20000], Loss: 47.188751220703125, Learning Rate: 0.01\n",
      "Epoch [3298/20000], Loss: 47.17308044433594, Learning Rate: 0.01\n",
      "Epoch [3299/20000], Loss: 47.1572265625, Learning Rate: 0.01\n",
      "Epoch [3300/20000], Loss: 47.14158630371094, Learning Rate: 0.01\n",
      "Epoch [3301/20000], Loss: 47.12579345703125, Learning Rate: 0.01\n",
      "Epoch [3302/20000], Loss: 47.10992431640625, Learning Rate: 0.01\n",
      "Epoch [3303/20000], Loss: 47.09394836425781, Learning Rate: 0.01\n",
      "Epoch [3304/20000], Loss: 47.07829284667969, Learning Rate: 0.01\n",
      "Epoch [3305/20000], Loss: 47.062469482421875, Learning Rate: 0.01\n",
      "Epoch [3306/20000], Loss: 47.04649353027344, Learning Rate: 0.01\n",
      "Epoch [3307/20000], Loss: 47.03062438964844, Learning Rate: 0.01\n",
      "Epoch [3308/20000], Loss: 47.01487731933594, Learning Rate: 0.01\n",
      "Epoch [3309/20000], Loss: 46.999176025390625, Learning Rate: 0.01\n",
      "Epoch [3310/20000], Loss: 46.98341369628906, Learning Rate: 0.01\n",
      "Epoch [3311/20000], Loss: 46.96784973144531, Learning Rate: 0.01\n",
      "Epoch [3312/20000], Loss: 46.95198059082031, Learning Rate: 0.01\n",
      "Epoch [3313/20000], Loss: 46.936004638671875, Learning Rate: 0.01\n",
      "Epoch [3314/20000], Loss: 46.92042541503906, Learning Rate: 0.01\n",
      "Epoch [3315/20000], Loss: 46.90467834472656, Learning Rate: 0.01\n",
      "Epoch [3316/20000], Loss: 46.888946533203125, Learning Rate: 0.01\n",
      "Epoch [3317/20000], Loss: 46.8731689453125, Learning Rate: 0.01\n",
      "Epoch [3318/20000], Loss: 46.85758972167969, Learning Rate: 0.01\n",
      "Epoch [3319/20000], Loss: 46.84178161621094, Learning Rate: 0.01\n",
      "Epoch [3320/20000], Loss: 46.82618713378906, Learning Rate: 0.01\n",
      "Epoch [3321/20000], Loss: 46.81044006347656, Learning Rate: 0.01\n",
      "Epoch [3322/20000], Loss: 46.7947998046875, Learning Rate: 0.01\n",
      "Epoch [3323/20000], Loss: 46.77925109863281, Learning Rate: 0.01\n",
      "Epoch [3324/20000], Loss: 46.763397216796875, Learning Rate: 0.01\n",
      "Epoch [3325/20000], Loss: 46.74778747558594, Learning Rate: 0.01\n",
      "Epoch [3326/20000], Loss: 46.73194885253906, Learning Rate: 0.01\n",
      "Epoch [3327/20000], Loss: 46.71650695800781, Learning Rate: 0.01\n",
      "Epoch [3328/20000], Loss: 46.700836181640625, Learning Rate: 0.01\n",
      "Epoch [3329/20000], Loss: 46.68516540527344, Learning Rate: 0.01\n",
      "Epoch [3330/20000], Loss: 46.66950988769531, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3331/20000], Loss: 46.65391540527344, Learning Rate: 0.01\n",
      "Epoch [3332/20000], Loss: 46.63836669921875, Learning Rate: 0.01\n",
      "Epoch [3333/20000], Loss: 46.62278747558594, Learning Rate: 0.01\n",
      "Epoch [3334/20000], Loss: 46.6070556640625, Learning Rate: 0.01\n",
      "Epoch [3335/20000], Loss: 46.59144592285156, Learning Rate: 0.01\n",
      "Epoch [3336/20000], Loss: 46.575775146484375, Learning Rate: 0.01\n",
      "Epoch [3337/20000], Loss: 46.560028076171875, Learning Rate: 0.01\n",
      "Epoch [3338/20000], Loss: 46.544647216796875, Learning Rate: 0.01\n",
      "Epoch [3339/20000], Loss: 46.52911376953125, Learning Rate: 0.01\n",
      "Epoch [3340/20000], Loss: 46.513671875, Learning Rate: 0.01\n",
      "Epoch [3341/20000], Loss: 46.49806213378906, Learning Rate: 0.01\n",
      "Epoch [3342/20000], Loss: 46.482421875, Learning Rate: 0.01\n",
      "Epoch [3343/20000], Loss: 46.46696472167969, Learning Rate: 0.01\n",
      "Epoch [3344/20000], Loss: 46.45143127441406, Learning Rate: 0.01\n",
      "Epoch [3345/20000], Loss: 46.43583679199219, Learning Rate: 0.01\n",
      "Epoch [3346/20000], Loss: 46.420318603515625, Learning Rate: 0.01\n",
      "Epoch [3347/20000], Loss: 46.40478515625, Learning Rate: 0.01\n",
      "Epoch [3348/20000], Loss: 46.389251708984375, Learning Rate: 0.01\n",
      "Epoch [3349/20000], Loss: 46.3739013671875, Learning Rate: 0.01\n",
      "Epoch [3350/20000], Loss: 46.35847473144531, Learning Rate: 0.01\n",
      "Epoch [3351/20000], Loss: 46.34283447265625, Learning Rate: 0.01\n",
      "Epoch [3352/20000], Loss: 46.32746887207031, Learning Rate: 0.01\n",
      "Epoch [3353/20000], Loss: 46.31172180175781, Learning Rate: 0.01\n",
      "Epoch [3354/20000], Loss: 46.29643249511719, Learning Rate: 0.01\n",
      "Epoch [3355/20000], Loss: 46.281097412109375, Learning Rate: 0.01\n",
      "Epoch [3356/20000], Loss: 46.265594482421875, Learning Rate: 0.01\n",
      "Epoch [3357/20000], Loss: 46.25013732910156, Learning Rate: 0.01\n",
      "Epoch [3358/20000], Loss: 46.234649658203125, Learning Rate: 0.01\n",
      "Epoch [3359/20000], Loss: 46.21916198730469, Learning Rate: 0.01\n",
      "Epoch [3360/20000], Loss: 46.20367431640625, Learning Rate: 0.01\n",
      "Epoch [3361/20000], Loss: 46.18829345703125, Learning Rate: 0.01\n",
      "Epoch [3362/20000], Loss: 46.17279052734375, Learning Rate: 0.01\n",
      "Epoch [3363/20000], Loss: 46.15730285644531, Learning Rate: 0.01\n",
      "Epoch [3364/20000], Loss: 46.14210510253906, Learning Rate: 0.01\n",
      "Epoch [3365/20000], Loss: 46.12660217285156, Learning Rate: 0.01\n",
      "Epoch [3366/20000], Loss: 46.111236572265625, Learning Rate: 0.01\n",
      "Epoch [3367/20000], Loss: 46.09562683105469, Learning Rate: 0.01\n",
      "Epoch [3368/20000], Loss: 46.08030700683594, Learning Rate: 0.01\n",
      "Epoch [3369/20000], Loss: 46.065032958984375, Learning Rate: 0.01\n",
      "Epoch [3370/20000], Loss: 46.049896240234375, Learning Rate: 0.01\n",
      "Epoch [3371/20000], Loss: 46.03437805175781, Learning Rate: 0.01\n",
      "Epoch [3372/20000], Loss: 46.018951416015625, Learning Rate: 0.01\n",
      "Epoch [3373/20000], Loss: 46.003631591796875, Learning Rate: 0.01\n",
      "Epoch [3374/20000], Loss: 45.98822021484375, Learning Rate: 0.01\n",
      "Epoch [3375/20000], Loss: 45.972930908203125, Learning Rate: 0.01\n",
      "Epoch [3376/20000], Loss: 45.957733154296875, Learning Rate: 0.01\n",
      "Epoch [3377/20000], Loss: 45.94215393066406, Learning Rate: 0.01\n",
      "Epoch [3378/20000], Loss: 45.927032470703125, Learning Rate: 0.01\n",
      "Epoch [3379/20000], Loss: 45.911651611328125, Learning Rate: 0.01\n",
      "Epoch [3380/20000], Loss: 45.896484375, Learning Rate: 0.01\n",
      "Epoch [3381/20000], Loss: 45.88108825683594, Learning Rate: 0.01\n",
      "Epoch [3382/20000], Loss: 45.86561584472656, Learning Rate: 0.01\n",
      "Epoch [3383/20000], Loss: 45.8504638671875, Learning Rate: 0.01\n",
      "Epoch [3384/20000], Loss: 45.835235595703125, Learning Rate: 0.01\n",
      "Epoch [3385/20000], Loss: 45.81968688964844, Learning Rate: 0.01\n",
      "Epoch [3386/20000], Loss: 45.804656982421875, Learning Rate: 0.01\n",
      "Epoch [3387/20000], Loss: 45.789154052734375, Learning Rate: 0.01\n",
      "Epoch [3388/20000], Loss: 45.77406311035156, Learning Rate: 0.01\n",
      "Epoch [3389/20000], Loss: 45.75886535644531, Learning Rate: 0.01\n",
      "Epoch [3390/20000], Loss: 45.74360656738281, Learning Rate: 0.01\n",
      "Epoch [3391/20000], Loss: 45.728302001953125, Learning Rate: 0.01\n",
      "Epoch [3392/20000], Loss: 45.713104248046875, Learning Rate: 0.01\n",
      "Epoch [3393/20000], Loss: 45.698028564453125, Learning Rate: 0.01\n",
      "Epoch [3394/20000], Loss: 45.682586669921875, Learning Rate: 0.01\n",
      "Epoch [3395/20000], Loss: 45.66748046875, Learning Rate: 0.01\n",
      "Epoch [3396/20000], Loss: 45.652313232421875, Learning Rate: 0.01\n",
      "Epoch [3397/20000], Loss: 45.637115478515625, Learning Rate: 0.01\n",
      "Epoch [3398/20000], Loss: 45.62188720703125, Learning Rate: 0.01\n",
      "Epoch [3399/20000], Loss: 45.606842041015625, Learning Rate: 0.01\n",
      "Epoch [3400/20000], Loss: 45.59193420410156, Learning Rate: 0.01\n",
      "Epoch [3401/20000], Loss: 45.576904296875, Learning Rate: 0.01\n",
      "Epoch [3402/20000], Loss: 45.562164306640625, Learning Rate: 0.01\n",
      "Epoch [3403/20000], Loss: 45.5474853515625, Learning Rate: 0.01\n",
      "Epoch [3404/20000], Loss: 45.533050537109375, Learning Rate: 0.01\n",
      "Epoch [3405/20000], Loss: 45.519134521484375, Learning Rate: 0.01\n",
      "Epoch [3406/20000], Loss: 45.5059814453125, Learning Rate: 0.01\n",
      "Epoch [3407/20000], Loss: 45.49363708496094, Learning Rate: 0.01\n",
      "Epoch [3408/20000], Loss: 45.48295593261719, Learning Rate: 0.01\n",
      "Epoch [3409/20000], Loss: 45.47489929199219, Learning Rate: 0.01\n",
      "Epoch [3410/20000], Loss: 45.469970703125, Learning Rate: 0.01\n",
      "Epoch [3411/20000], Loss: 45.46954345703125, Learning Rate: 0.01\n",
      "Epoch [3412/20000], Loss: 45.47492980957031, Learning Rate: 0.01\n",
      "Epoch [3413/20000], Loss: 45.48554992675781, Learning Rate: 0.01\n",
      "Epoch [3414/20000], Loss: 45.49859619140625, Learning Rate: 0.01\n",
      "Epoch [3415/20000], Loss: 45.507659912109375, Learning Rate: 0.01\n",
      "Epoch [3416/20000], Loss: 45.50321960449219, Learning Rate: 0.01\n",
      "Epoch [3417/20000], Loss: 45.47663879394531, Learning Rate: 0.01\n",
      "Epoch [3418/20000], Loss: 45.427886962890625, Learning Rate: 0.01\n",
      "Epoch [3419/20000], Loss: 45.367340087890625, Learning Rate: 0.01\n",
      "Epoch [3420/20000], Loss: 45.31184387207031, Learning Rate: 0.01\n",
      "Epoch [3421/20000], Loss: 45.275726318359375, Learning Rate: 0.01\n",
      "Epoch [3422/20000], Loss: 45.26251220703125, Learning Rate: 0.01\n",
      "Epoch [3423/20000], Loss: 45.264892578125, Learning Rate: 0.01\n",
      "Epoch [3424/20000], Loss: 45.27067565917969, Learning Rate: 0.01\n",
      "Epoch [3425/20000], Loss: 45.26792907714844, Learning Rate: 0.01\n",
      "Epoch [3426/20000], Loss: 45.25083923339844, Learning Rate: 0.01\n",
      "Epoch [3427/20000], Loss: 45.221527099609375, Learning Rate: 0.01\n",
      "Epoch [3428/20000], Loss: 45.187591552734375, Learning Rate: 0.01\n",
      "Epoch [3429/20000], Loss: 45.15797424316406, Learning Rate: 0.01\n",
      "Epoch [3430/20000], Loss: 45.138580322265625, Learning Rate: 0.01\n",
      "Epoch [3431/20000], Loss: 45.12815856933594, Learning Rate: 0.01\n",
      "Epoch [3432/20000], Loss: 45.122528076171875, Learning Rate: 0.01\n",
      "Epoch [3433/20000], Loss: 45.114898681640625, Learning Rate: 0.01\n",
      "Epoch [3434/20000], Loss: 45.101593017578125, Learning Rate: 0.01\n",
      "Epoch [3435/20000], Loss: 45.08244323730469, Learning Rate: 0.01\n",
      "Epoch [3436/20000], Loss: 45.059661865234375, Learning Rate: 0.01\n",
      "Epoch [3437/20000], Loss: 45.03749084472656, Learning Rate: 0.01\n",
      "Epoch [3438/20000], Loss: 45.018707275390625, Learning Rate: 0.01\n",
      "Epoch [3439/20000], Loss: 45.0045166015625, Learning Rate: 0.01\n",
      "Epoch [3440/20000], Loss: 44.992919921875, Learning Rate: 0.01\n",
      "Epoch [3441/20000], Loss: 44.98173522949219, Learning Rate: 0.01\n",
      "Epoch [3442/20000], Loss: 44.96873474121094, Learning Rate: 0.01\n",
      "Epoch [3443/20000], Loss: 44.953125, Learning Rate: 0.01\n",
      "Epoch [3444/20000], Loss: 44.93556213378906, Learning Rate: 0.01\n",
      "Epoch [3445/20000], Loss: 44.91752624511719, Learning Rate: 0.01\n",
      "Epoch [3446/20000], Loss: 44.9000244140625, Learning Rate: 0.01\n",
      "Epoch [3447/20000], Loss: 44.88441467285156, Learning Rate: 0.01\n",
      "Epoch [3448/20000], Loss: 44.8701171875, Learning Rate: 0.01\n",
      "Epoch [3449/20000], Loss: 44.85664367675781, Learning Rate: 0.01\n",
      "Epoch [3450/20000], Loss: 44.84307861328125, Learning Rate: 0.01\n",
      "Epoch [3451/20000], Loss: 44.82884216308594, Learning Rate: 0.01\n",
      "Epoch [3452/20000], Loss: 44.813629150390625, Learning Rate: 0.01\n",
      "Epoch [3453/20000], Loss: 44.797637939453125, Learning Rate: 0.01\n",
      "Epoch [3454/20000], Loss: 44.78138732910156, Learning Rate: 0.01\n",
      "Epoch [3455/20000], Loss: 44.76536560058594, Learning Rate: 0.01\n",
      "Epoch [3456/20000], Loss: 44.75025939941406, Learning Rate: 0.01\n",
      "Epoch [3457/20000], Loss: 44.735687255859375, Learning Rate: 0.01\n",
      "Epoch [3458/20000], Loss: 44.72129821777344, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3459/20000], Loss: 44.70719909667969, Learning Rate: 0.01\n",
      "Epoch [3460/20000], Loss: 44.69261169433594, Learning Rate: 0.01\n",
      "Epoch [3461/20000], Loss: 44.677886962890625, Learning Rate: 0.01\n",
      "Epoch [3462/20000], Loss: 44.662506103515625, Learning Rate: 0.01\n",
      "Epoch [3463/20000], Loss: 44.64723205566406, Learning Rate: 0.01\n",
      "Epoch [3464/20000], Loss: 44.631988525390625, Learning Rate: 0.01\n",
      "Epoch [3465/20000], Loss: 44.61689758300781, Learning Rate: 0.01\n",
      "Epoch [3466/20000], Loss: 44.60206604003906, Learning Rate: 0.01\n",
      "Epoch [3467/20000], Loss: 44.58734130859375, Learning Rate: 0.01\n",
      "Epoch [3468/20000], Loss: 44.57267761230469, Learning Rate: 0.01\n",
      "Epoch [3469/20000], Loss: 44.55815124511719, Learning Rate: 0.01\n",
      "Epoch [3470/20000], Loss: 44.54351806640625, Learning Rate: 0.01\n",
      "Epoch [3471/20000], Loss: 44.52870178222656, Learning Rate: 0.01\n",
      "Epoch [3472/20000], Loss: 44.513671875, Learning Rate: 0.01\n",
      "Epoch [3473/20000], Loss: 44.49894714355469, Learning Rate: 0.01\n",
      "Epoch [3474/20000], Loss: 44.48394775390625, Learning Rate: 0.01\n",
      "Epoch [3475/20000], Loss: 44.469146728515625, Learning Rate: 0.01\n",
      "Epoch [3476/20000], Loss: 44.454376220703125, Learning Rate: 0.01\n",
      "Epoch [3477/20000], Loss: 44.43949890136719, Learning Rate: 0.01\n",
      "Epoch [3478/20000], Loss: 44.42509460449219, Learning Rate: 0.01\n",
      "Epoch [3479/20000], Loss: 44.41015625, Learning Rate: 0.01\n",
      "Epoch [3480/20000], Loss: 44.39558410644531, Learning Rate: 0.01\n",
      "Epoch [3481/20000], Loss: 44.38087463378906, Learning Rate: 0.01\n",
      "Epoch [3482/20000], Loss: 44.36628723144531, Learning Rate: 0.01\n",
      "Epoch [3483/20000], Loss: 44.35150146484375, Learning Rate: 0.01\n",
      "Epoch [3484/20000], Loss: 44.33671569824219, Learning Rate: 0.01\n",
      "Epoch [3485/20000], Loss: 44.32197570800781, Learning Rate: 0.01\n",
      "Epoch [3486/20000], Loss: 44.30706787109375, Learning Rate: 0.01\n",
      "Epoch [3487/20000], Loss: 44.292449951171875, Learning Rate: 0.01\n",
      "Epoch [3488/20000], Loss: 44.27778625488281, Learning Rate: 0.01\n",
      "Epoch [3489/20000], Loss: 44.26307678222656, Learning Rate: 0.01\n",
      "Epoch [3490/20000], Loss: 44.248382568359375, Learning Rate: 0.01\n",
      "Epoch [3491/20000], Loss: 44.233734130859375, Learning Rate: 0.01\n",
      "Epoch [3492/20000], Loss: 44.21910095214844, Learning Rate: 0.01\n",
      "Epoch [3493/20000], Loss: 44.20452880859375, Learning Rate: 0.01\n",
      "Epoch [3494/20000], Loss: 44.189788818359375, Learning Rate: 0.01\n",
      "Epoch [3495/20000], Loss: 44.175445556640625, Learning Rate: 0.01\n",
      "Epoch [3496/20000], Loss: 44.16053771972656, Learning Rate: 0.01\n",
      "Epoch [3497/20000], Loss: 44.145660400390625, Learning Rate: 0.01\n",
      "Epoch [3498/20000], Loss: 44.131134033203125, Learning Rate: 0.01\n",
      "Epoch [3499/20000], Loss: 44.11671447753906, Learning Rate: 0.01\n",
      "Epoch [3500/20000], Loss: 44.10203552246094, Learning Rate: 0.01\n",
      "Epoch [3501/20000], Loss: 44.08732604980469, Learning Rate: 0.01\n",
      "Epoch [3502/20000], Loss: 44.07276916503906, Learning Rate: 0.01\n",
      "Epoch [3503/20000], Loss: 44.05810546875, Learning Rate: 0.01\n",
      "Epoch [3504/20000], Loss: 44.04353332519531, Learning Rate: 0.01\n",
      "Epoch [3505/20000], Loss: 44.02891540527344, Learning Rate: 0.01\n",
      "Epoch [3506/20000], Loss: 44.01445007324219, Learning Rate: 0.01\n",
      "Epoch [3507/20000], Loss: 43.999603271484375, Learning Rate: 0.01\n",
      "Epoch [3508/20000], Loss: 43.98503112792969, Learning Rate: 0.01\n",
      "Epoch [3509/20000], Loss: 43.970733642578125, Learning Rate: 0.01\n",
      "Epoch [3510/20000], Loss: 43.95597839355469, Learning Rate: 0.01\n",
      "Epoch [3511/20000], Loss: 43.94148254394531, Learning Rate: 0.01\n",
      "Epoch [3512/20000], Loss: 43.926788330078125, Learning Rate: 0.01\n",
      "Epoch [3513/20000], Loss: 43.91246032714844, Learning Rate: 0.01\n",
      "Epoch [3514/20000], Loss: 43.8978271484375, Learning Rate: 0.01\n",
      "Epoch [3515/20000], Loss: 43.88340759277344, Learning Rate: 0.01\n",
      "Epoch [3516/20000], Loss: 43.86866760253906, Learning Rate: 0.01\n",
      "Epoch [3517/20000], Loss: 43.85420227050781, Learning Rate: 0.01\n",
      "Epoch [3518/20000], Loss: 43.83970642089844, Learning Rate: 0.01\n",
      "Epoch [3519/20000], Loss: 43.82514953613281, Learning Rate: 0.01\n",
      "Epoch [3520/20000], Loss: 43.81065368652344, Learning Rate: 0.01\n",
      "Epoch [3521/20000], Loss: 43.79620361328125, Learning Rate: 0.01\n",
      "Epoch [3522/20000], Loss: 43.781646728515625, Learning Rate: 0.01\n",
      "Epoch [3523/20000], Loss: 43.767242431640625, Learning Rate: 0.01\n",
      "Epoch [3524/20000], Loss: 43.75248718261719, Learning Rate: 0.01\n",
      "Epoch [3525/20000], Loss: 43.738067626953125, Learning Rate: 0.01\n",
      "Epoch [3526/20000], Loss: 43.72369384765625, Learning Rate: 0.01\n",
      "Epoch [3527/20000], Loss: 43.70921325683594, Learning Rate: 0.01\n",
      "Epoch [3528/20000], Loss: 43.69477844238281, Learning Rate: 0.01\n",
      "Epoch [3529/20000], Loss: 43.68034362792969, Learning Rate: 0.01\n",
      "Epoch [3530/20000], Loss: 43.6656494140625, Learning Rate: 0.01\n",
      "Epoch [3531/20000], Loss: 43.651214599609375, Learning Rate: 0.01\n",
      "Epoch [3532/20000], Loss: 43.636962890625, Learning Rate: 0.01\n",
      "Epoch [3533/20000], Loss: 43.62237548828125, Learning Rate: 0.01\n",
      "Epoch [3534/20000], Loss: 43.60798645019531, Learning Rate: 0.01\n",
      "Epoch [3535/20000], Loss: 43.593536376953125, Learning Rate: 0.01\n",
      "Epoch [3536/20000], Loss: 43.578948974609375, Learning Rate: 0.01\n",
      "Epoch [3537/20000], Loss: 43.56459045410156, Learning Rate: 0.01\n",
      "Epoch [3538/20000], Loss: 43.55030822753906, Learning Rate: 0.01\n",
      "Epoch [3539/20000], Loss: 43.535736083984375, Learning Rate: 0.01\n",
      "Epoch [3540/20000], Loss: 43.52146911621094, Learning Rate: 0.01\n",
      "Epoch [3541/20000], Loss: 43.50703430175781, Learning Rate: 0.01\n",
      "Epoch [3542/20000], Loss: 43.49269104003906, Learning Rate: 0.01\n",
      "Epoch [3543/20000], Loss: 43.478240966796875, Learning Rate: 0.01\n",
      "Epoch [3544/20000], Loss: 43.46392822265625, Learning Rate: 0.01\n",
      "Epoch [3545/20000], Loss: 43.44950866699219, Learning Rate: 0.01\n",
      "Epoch [3546/20000], Loss: 43.43531799316406, Learning Rate: 0.01\n",
      "Epoch [3547/20000], Loss: 43.42120361328125, Learning Rate: 0.01\n",
      "Epoch [3548/20000], Loss: 43.406768798828125, Learning Rate: 0.01\n",
      "Epoch [3549/20000], Loss: 43.39286804199219, Learning Rate: 0.01\n",
      "Epoch [3550/20000], Loss: 43.37890625, Learning Rate: 0.01\n",
      "Epoch [3551/20000], Loss: 43.365142822265625, Learning Rate: 0.01\n",
      "Epoch [3552/20000], Loss: 43.35166931152344, Learning Rate: 0.01\n",
      "Epoch [3553/20000], Loss: 43.33866882324219, Learning Rate: 0.01\n",
      "Epoch [3554/20000], Loss: 43.32640075683594, Learning Rate: 0.01\n",
      "Epoch [3555/20000], Loss: 43.31480407714844, Learning Rate: 0.01\n",
      "Epoch [3556/20000], Loss: 43.304595947265625, Learning Rate: 0.01\n",
      "Epoch [3557/20000], Loss: 43.29658508300781, Learning Rate: 0.01\n",
      "Epoch [3558/20000], Loss: 43.291595458984375, Learning Rate: 0.01\n",
      "Epoch [3559/20000], Loss: 43.29071044921875, Learning Rate: 0.01\n",
      "Epoch [3560/20000], Loss: 43.29594421386719, Learning Rate: 0.01\n",
      "Epoch [3561/20000], Loss: 43.3094482421875, Learning Rate: 0.01\n",
      "Epoch [3562/20000], Loss: 43.33355712890625, Learning Rate: 0.01\n",
      "Epoch [3563/20000], Loss: 43.36967468261719, Learning Rate: 0.01\n",
      "Epoch [3564/20000], Loss: 43.41654968261719, Learning Rate: 0.01\n",
      "Epoch [3565/20000], Loss: 43.4678955078125, Learning Rate: 0.01\n",
      "Epoch [3566/20000], Loss: 43.51129150390625, Learning Rate: 0.01\n",
      "Epoch [3567/20000], Loss: 43.528167724609375, Learning Rate: 0.01\n",
      "Epoch [3568/20000], Loss: 43.502197265625, Learning Rate: 0.01\n",
      "Epoch [3569/20000], Loss: 43.42677307128906, Learning Rate: 0.01\n",
      "Epoch [3570/20000], Loss: 43.31526184082031, Learning Rate: 0.01\n",
      "Epoch [3571/20000], Loss: 43.19596862792969, Learning Rate: 0.01\n",
      "Epoch [3572/20000], Loss: 43.100921630859375, Learning Rate: 0.01\n",
      "Epoch [3573/20000], Loss: 43.05003356933594, Learning Rate: 0.01\n",
      "Epoch [3574/20000], Loss: 43.04412841796875, Learning Rate: 0.01\n",
      "Epoch [3575/20000], Loss: 43.06787109375, Learning Rate: 0.01\n",
      "Epoch [3576/20000], Loss: 43.09844970703125, Learning Rate: 0.01\n",
      "Epoch [3577/20000], Loss: 43.1151123046875, Learning Rate: 0.01\n",
      "Epoch [3578/20000], Loss: 43.10577392578125, Learning Rate: 0.01\n",
      "Epoch [3579/20000], Loss: 43.07037353515625, Learning Rate: 0.01\n",
      "Epoch [3580/20000], Loss: 43.01902770996094, Learning Rate: 0.01\n",
      "Epoch [3581/20000], Loss: 42.96690368652344, Learning Rate: 0.01\n",
      "Epoch [3582/20000], Loss: 42.927398681640625, Learning Rate: 0.01\n",
      "Epoch [3583/20000], Loss: 42.90614318847656, Learning Rate: 0.01\n",
      "Epoch [3584/20000], Loss: 42.90071105957031, Learning Rate: 0.01\n",
      "Epoch [3585/20000], Loss: 42.90357971191406, Learning Rate: 0.01\n",
      "Epoch [3586/20000], Loss: 42.90510559082031, Learning Rate: 0.01\n",
      "Epoch [3587/20000], Loss: 42.8985595703125, Learning Rate: 0.01\n",
      "Epoch [3588/20000], Loss: 42.88151550292969, Learning Rate: 0.01\n",
      "Epoch [3589/20000], Loss: 42.856353759765625, Learning Rate: 0.01\n",
      "Epoch [3590/20000], Loss: 42.82746887207031, Learning Rate: 0.01\n",
      "Epoch [3591/20000], Loss: 42.80079650878906, Learning Rate: 0.01\n",
      "Epoch [3592/20000], Loss: 42.77943420410156, Learning Rate: 0.01\n",
      "Epoch [3593/20000], Loss: 42.76463317871094, Learning Rate: 0.01\n",
      "Epoch [3594/20000], Loss: 42.755035400390625, Learning Rate: 0.01\n",
      "Epoch [3595/20000], Loss: 42.74725341796875, Learning Rate: 0.01\n",
      "Epoch [3596/20000], Loss: 42.73832702636719, Learning Rate: 0.01\n",
      "Epoch [3597/20000], Loss: 42.72651672363281, Learning Rate: 0.01\n",
      "Epoch [3598/20000], Loss: 42.71087646484375, Learning Rate: 0.01\n",
      "Epoch [3599/20000], Loss: 42.69276428222656, Learning Rate: 0.01\n",
      "Epoch [3600/20000], Loss: 42.673553466796875, Learning Rate: 0.01\n",
      "Epoch [3601/20000], Loss: 42.654693603515625, Learning Rate: 0.01\n",
      "Epoch [3602/20000], Loss: 42.6378173828125, Learning Rate: 0.01\n",
      "Epoch [3603/20000], Loss: 42.62300109863281, Learning Rate: 0.01\n",
      "Epoch [3604/20000], Loss: 42.60986328125, Learning Rate: 0.01\n",
      "Epoch [3605/20000], Loss: 42.59794616699219, Learning Rate: 0.01\n",
      "Epoch [3606/20000], Loss: 42.585845947265625, Learning Rate: 0.01\n",
      "Epoch [3607/20000], Loss: 42.57301330566406, Learning Rate: 0.01\n",
      "Epoch [3608/20000], Loss: 42.55921936035156, Learning Rate: 0.01\n",
      "Epoch [3609/20000], Loss: 42.544403076171875, Learning Rate: 0.01\n",
      "Epoch [3610/20000], Loss: 42.5289306640625, Learning Rate: 0.01\n",
      "Epoch [3611/20000], Loss: 42.5130615234375, Learning Rate: 0.01\n",
      "Epoch [3612/20000], Loss: 42.4976806640625, Learning Rate: 0.01\n",
      "Epoch [3613/20000], Loss: 42.4825439453125, Learning Rate: 0.01\n",
      "Epoch [3614/20000], Loss: 42.46794128417969, Learning Rate: 0.01\n",
      "Epoch [3615/20000], Loss: 42.454071044921875, Learning Rate: 0.01\n",
      "Epoch [3616/20000], Loss: 42.44050598144531, Learning Rate: 0.01\n",
      "Epoch [3617/20000], Loss: 42.427154541015625, Learning Rate: 0.01\n",
      "Epoch [3618/20000], Loss: 42.41363525390625, Learning Rate: 0.01\n",
      "Epoch [3619/20000], Loss: 42.39991760253906, Learning Rate: 0.01\n",
      "Epoch [3620/20000], Loss: 42.38600158691406, Learning Rate: 0.01\n",
      "Epoch [3621/20000], Loss: 42.37187194824219, Learning Rate: 0.01\n",
      "Epoch [3622/20000], Loss: 42.35746765136719, Learning Rate: 0.01\n",
      "Epoch [3623/20000], Loss: 42.3428955078125, Learning Rate: 0.01\n",
      "Epoch [3624/20000], Loss: 42.32844543457031, Learning Rate: 0.01\n",
      "Epoch [3625/20000], Loss: 42.31425476074219, Learning Rate: 0.01\n",
      "Epoch [3626/20000], Loss: 42.299835205078125, Learning Rate: 0.01\n",
      "Epoch [3627/20000], Loss: 42.28575134277344, Learning Rate: 0.01\n",
      "Epoch [3628/20000], Loss: 42.27191162109375, Learning Rate: 0.01\n",
      "Epoch [3629/20000], Loss: 42.25775146484375, Learning Rate: 0.01\n",
      "Epoch [3630/20000], Loss: 42.243896484375, Learning Rate: 0.01\n",
      "Epoch [3631/20000], Loss: 42.230072021484375, Learning Rate: 0.01\n",
      "Epoch [3632/20000], Loss: 42.21624755859375, Learning Rate: 0.01\n",
      "Epoch [3633/20000], Loss: 42.20219421386719, Learning Rate: 0.01\n",
      "Epoch [3634/20000], Loss: 42.1883544921875, Learning Rate: 0.01\n",
      "Epoch [3635/20000], Loss: 42.174407958984375, Learning Rate: 0.01\n",
      "Epoch [3636/20000], Loss: 42.1605224609375, Learning Rate: 0.01\n",
      "Epoch [3637/20000], Loss: 42.146453857421875, Learning Rate: 0.01\n",
      "Epoch [3638/20000], Loss: 42.13236999511719, Learning Rate: 0.01\n",
      "Epoch [3639/20000], Loss: 42.11834716796875, Learning Rate: 0.01\n",
      "Epoch [3640/20000], Loss: 42.10429382324219, Learning Rate: 0.01\n",
      "Epoch [3641/20000], Loss: 42.09034729003906, Learning Rate: 0.01\n",
      "Epoch [3642/20000], Loss: 42.07627868652344, Learning Rate: 0.01\n",
      "Epoch [3643/20000], Loss: 42.0623779296875, Learning Rate: 0.01\n",
      "Epoch [3644/20000], Loss: 42.04835510253906, Learning Rate: 0.01\n",
      "Epoch [3645/20000], Loss: 42.03448486328125, Learning Rate: 0.01\n",
      "Epoch [3646/20000], Loss: 42.02049255371094, Learning Rate: 0.01\n",
      "Epoch [3647/20000], Loss: 42.00653076171875, Learning Rate: 0.01\n",
      "Epoch [3648/20000], Loss: 41.992706298828125, Learning Rate: 0.01\n",
      "Epoch [3649/20000], Loss: 41.9788818359375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3650/20000], Loss: 41.96485900878906, Learning Rate: 0.01\n",
      "Epoch [3651/20000], Loss: 41.95111083984375, Learning Rate: 0.01\n",
      "Epoch [3652/20000], Loss: 41.93707275390625, Learning Rate: 0.01\n",
      "Epoch [3653/20000], Loss: 41.92326354980469, Learning Rate: 0.01\n",
      "Epoch [3654/20000], Loss: 41.90953063964844, Learning Rate: 0.01\n",
      "Epoch [3655/20000], Loss: 41.8955078125, Learning Rate: 0.01\n",
      "Epoch [3656/20000], Loss: 41.881683349609375, Learning Rate: 0.01\n",
      "Epoch [3657/20000], Loss: 41.867767333984375, Learning Rate: 0.01\n",
      "Epoch [3658/20000], Loss: 41.85400390625, Learning Rate: 0.01\n",
      "Epoch [3659/20000], Loss: 41.84014892578125, Learning Rate: 0.01\n",
      "Epoch [3660/20000], Loss: 41.82646179199219, Learning Rate: 0.01\n",
      "Epoch [3661/20000], Loss: 41.81239318847656, Learning Rate: 0.01\n",
      "Epoch [3662/20000], Loss: 41.798675537109375, Learning Rate: 0.01\n",
      "Epoch [3663/20000], Loss: 41.784881591796875, Learning Rate: 0.01\n",
      "Epoch [3664/20000], Loss: 41.77117919921875, Learning Rate: 0.01\n",
      "Epoch [3665/20000], Loss: 41.757293701171875, Learning Rate: 0.01\n",
      "Epoch [3666/20000], Loss: 41.74346923828125, Learning Rate: 0.01\n",
      "Epoch [3667/20000], Loss: 41.7296142578125, Learning Rate: 0.01\n",
      "Epoch [3668/20000], Loss: 41.71612548828125, Learning Rate: 0.01\n",
      "Epoch [3669/20000], Loss: 41.70243835449219, Learning Rate: 0.01\n",
      "Epoch [3670/20000], Loss: 41.68865966796875, Learning Rate: 0.01\n",
      "Epoch [3671/20000], Loss: 41.6751708984375, Learning Rate: 0.01\n",
      "Epoch [3672/20000], Loss: 41.661712646484375, Learning Rate: 0.01\n",
      "Epoch [3673/20000], Loss: 41.648162841796875, Learning Rate: 0.01\n",
      "Epoch [3674/20000], Loss: 41.635223388671875, Learning Rate: 0.01\n",
      "Epoch [3675/20000], Loss: 41.62220764160156, Learning Rate: 0.01\n",
      "Epoch [3676/20000], Loss: 41.609344482421875, Learning Rate: 0.01\n",
      "Epoch [3677/20000], Loss: 41.5970458984375, Learning Rate: 0.01\n",
      "Epoch [3678/20000], Loss: 41.58506774902344, Learning Rate: 0.01\n",
      "Epoch [3679/20000], Loss: 41.57438659667969, Learning Rate: 0.01\n",
      "Epoch [3680/20000], Loss: 41.564453125, Learning Rate: 0.01\n",
      "Epoch [3681/20000], Loss: 41.55609130859375, Learning Rate: 0.01\n",
      "Epoch [3682/20000], Loss: 41.5499267578125, Learning Rate: 0.01\n",
      "Epoch [3683/20000], Loss: 41.54718017578125, Learning Rate: 0.01\n",
      "Epoch [3684/20000], Loss: 41.54864501953125, Learning Rate: 0.01\n",
      "Epoch [3685/20000], Loss: 41.55644226074219, Learning Rate: 0.01\n",
      "Epoch [3686/20000], Loss: 41.57316589355469, Learning Rate: 0.01\n",
      "Epoch [3687/20000], Loss: 41.60186767578125, Learning Rate: 0.01\n",
      "Epoch [3688/20000], Loss: 41.64640808105469, Learning Rate: 0.01\n",
      "Epoch [3689/20000], Loss: 41.71083068847656, Learning Rate: 0.01\n",
      "Epoch [3690/20000], Loss: 41.7982177734375, Learning Rate: 0.01\n",
      "Epoch [3691/20000], Loss: 41.908294677734375, Learning Rate: 0.01\n",
      "Epoch [3692/20000], Loss: 42.03314208984375, Learning Rate: 0.01\n",
      "Epoch [3693/20000], Loss: 42.15544128417969, Learning Rate: 0.01\n",
      "Epoch [3694/20000], Loss: 42.244415283203125, Learning Rate: 0.01\n",
      "Epoch [3695/20000], Loss: 42.2647705078125, Learning Rate: 0.01\n",
      "Epoch [3696/20000], Loss: 42.18701171875, Learning Rate: 0.01\n",
      "Epoch [3697/20000], Loss: 42.010528564453125, Learning Rate: 0.01\n",
      "Epoch [3698/20000], Loss: 41.76983642578125, Learning Rate: 0.01\n",
      "Epoch [3699/20000], Loss: 41.52818298339844, Learning Rate: 0.01\n",
      "Epoch [3700/20000], Loss: 41.34727478027344, Learning Rate: 0.01\n",
      "Epoch [3701/20000], Loss: 41.26348876953125, Learning Rate: 0.01\n",
      "Epoch [3702/20000], Loss: 41.273223876953125, Learning Rate: 0.01\n",
      "Epoch [3703/20000], Loss: 41.343048095703125, Learning Rate: 0.01\n",
      "Epoch [3704/20000], Loss: 41.42573547363281, Learning Rate: 0.01\n",
      "Epoch [3705/20000], Loss: 41.47979736328125, Learning Rate: 0.01\n",
      "Epoch [3706/20000], Loss: 41.48011779785156, Learning Rate: 0.01\n",
      "Epoch [3707/20000], Loss: 41.425201416015625, Learning Rate: 0.01\n",
      "Epoch [3708/20000], Loss: 41.33439636230469, Learning Rate: 0.01\n",
      "Epoch [3709/20000], Loss: 41.23741149902344, Learning Rate: 0.01\n",
      "Epoch [3710/20000], Loss: 41.16270446777344, Learning Rate: 0.01\n",
      "Epoch [3711/20000], Loss: 41.125152587890625, Learning Rate: 0.01\n",
      "Epoch [3712/20000], Loss: 41.12275695800781, Learning Rate: 0.01\n",
      "Epoch [3713/20000], Loss: 41.14204406738281, Learning Rate: 0.01\n",
      "Epoch [3714/20000], Loss: 41.164337158203125, Learning Rate: 0.01\n",
      "Epoch [3715/20000], Loss: 41.173553466796875, Learning Rate: 0.01\n",
      "Epoch [3716/20000], Loss: 41.16218566894531, Learning Rate: 0.01\n",
      "Epoch [3717/20000], Loss: 41.131072998046875, Learning Rate: 0.01\n",
      "Epoch [3718/20000], Loss: 41.08842468261719, Learning Rate: 0.01\n",
      "Epoch [3719/20000], Loss: 41.045074462890625, Learning Rate: 0.01\n",
      "Epoch [3720/20000], Loss: 41.01042175292969, Learning Rate: 0.01\n",
      "Epoch [3721/20000], Loss: 40.98872375488281, Learning Rate: 0.01\n",
      "Epoch [3722/20000], Loss: 40.978912353515625, Learning Rate: 0.01\n",
      "Epoch [3723/20000], Loss: 40.976776123046875, Learning Rate: 0.01\n",
      "Epoch [3724/20000], Loss: 40.976226806640625, Learning Rate: 0.01\n",
      "Epoch [3725/20000], Loss: 40.97145080566406, Learning Rate: 0.01\n",
      "Epoch [3726/20000], Loss: 40.960052490234375, Learning Rate: 0.01\n",
      "Epoch [3727/20000], Loss: 40.94194030761719, Learning Rate: 0.01\n",
      "Epoch [3728/20000], Loss: 40.919464111328125, Learning Rate: 0.01\n",
      "Epoch [3729/20000], Loss: 40.89530944824219, Learning Rate: 0.01\n",
      "Epoch [3730/20000], Loss: 40.87284851074219, Learning Rate: 0.01\n",
      "Epoch [3731/20000], Loss: 40.854156494140625, Learning Rate: 0.01\n",
      "Epoch [3732/20000], Loss: 40.839599609375, Learning Rate: 0.01\n",
      "Epoch [3733/20000], Loss: 40.82786560058594, Learning Rate: 0.01\n",
      "Epoch [3734/20000], Loss: 40.818328857421875, Learning Rate: 0.01\n",
      "Epoch [3735/20000], Loss: 40.80879211425781, Learning Rate: 0.01\n",
      "Epoch [3736/20000], Loss: 40.79817199707031, Learning Rate: 0.01\n",
      "Epoch [3737/20000], Loss: 40.785552978515625, Learning Rate: 0.01\n",
      "Epoch [3738/20000], Loss: 40.770751953125, Learning Rate: 0.01\n",
      "Epoch [3739/20000], Loss: 40.75471496582031, Learning Rate: 0.01\n",
      "Epoch [3740/20000], Loss: 40.73822021484375, Learning Rate: 0.01\n",
      "Epoch [3741/20000], Loss: 40.72151184082031, Learning Rate: 0.01\n",
      "Epoch [3742/20000], Loss: 40.70555114746094, Learning Rate: 0.01\n",
      "Epoch [3743/20000], Loss: 40.690887451171875, Learning Rate: 0.01\n",
      "Epoch [3744/20000], Loss: 40.677398681640625, Learning Rate: 0.01\n",
      "Epoch [3745/20000], Loss: 40.6641845703125, Learning Rate: 0.01\n",
      "Epoch [3746/20000], Loss: 40.651947021484375, Learning Rate: 0.01\n",
      "Epoch [3747/20000], Loss: 40.639373779296875, Learning Rate: 0.01\n",
      "Epoch [3748/20000], Loss: 40.626861572265625, Learning Rate: 0.01\n",
      "Epoch [3749/20000], Loss: 40.613861083984375, Learning Rate: 0.01\n",
      "Epoch [3750/20000], Loss: 40.600555419921875, Learning Rate: 0.01\n",
      "Epoch [3751/20000], Loss: 40.58671569824219, Learning Rate: 0.01\n",
      "Epoch [3752/20000], Loss: 40.57258605957031, Learning Rate: 0.01\n",
      "Epoch [3753/20000], Loss: 40.558197021484375, Learning Rate: 0.01\n",
      "Epoch [3754/20000], Loss: 40.5439453125, Learning Rate: 0.01\n",
      "Epoch [3755/20000], Loss: 40.52992248535156, Learning Rate: 0.01\n",
      "Epoch [3756/20000], Loss: 40.515869140625, Learning Rate: 0.01\n",
      "Epoch [3757/20000], Loss: 40.50215148925781, Learning Rate: 0.01\n",
      "Epoch [3758/20000], Loss: 40.48846435546875, Learning Rate: 0.01\n",
      "Epoch [3759/20000], Loss: 40.475341796875, Learning Rate: 0.01\n",
      "Epoch [3760/20000], Loss: 40.46192932128906, Learning Rate: 0.01\n",
      "Epoch [3761/20000], Loss: 40.44877624511719, Learning Rate: 0.01\n",
      "Epoch [3762/20000], Loss: 40.43540954589844, Learning Rate: 0.01\n",
      "Epoch [3763/20000], Loss: 40.42222595214844, Learning Rate: 0.01\n",
      "Epoch [3764/20000], Loss: 40.40895080566406, Learning Rate: 0.01\n",
      "Epoch [3765/20000], Loss: 40.39561462402344, Learning Rate: 0.01\n",
      "Epoch [3766/20000], Loss: 40.382293701171875, Learning Rate: 0.01\n",
      "Epoch [3767/20000], Loss: 40.36871337890625, Learning Rate: 0.01\n",
      "Epoch [3768/20000], Loss: 40.35533142089844, Learning Rate: 0.01\n",
      "Epoch [3769/20000], Loss: 40.341766357421875, Learning Rate: 0.01\n",
      "Epoch [3770/20000], Loss: 40.328460693359375, Learning Rate: 0.01\n",
      "Epoch [3771/20000], Loss: 40.31489562988281, Learning Rate: 0.01\n",
      "Epoch [3772/20000], Loss: 40.3013916015625, Learning Rate: 0.01\n",
      "Epoch [3773/20000], Loss: 40.28770446777344, Learning Rate: 0.01\n",
      "Epoch [3774/20000], Loss: 40.2744140625, Learning Rate: 0.01\n",
      "Epoch [3775/20000], Loss: 40.26081848144531, Learning Rate: 0.01\n",
      "Epoch [3776/20000], Loss: 40.24732971191406, Learning Rate: 0.01\n",
      "Epoch [3777/20000], Loss: 40.233917236328125, Learning Rate: 0.01\n",
      "Epoch [3778/20000], Loss: 40.220489501953125, Learning Rate: 0.01\n",
      "Epoch [3779/20000], Loss: 40.20716857910156, Learning Rate: 0.01\n",
      "Epoch [3780/20000], Loss: 40.19383239746094, Learning Rate: 0.01\n",
      "Epoch [3781/20000], Loss: 40.18037414550781, Learning Rate: 0.01\n",
      "Epoch [3782/20000], Loss: 40.16694641113281, Learning Rate: 0.01\n",
      "Epoch [3783/20000], Loss: 40.153564453125, Learning Rate: 0.01\n",
      "Epoch [3784/20000], Loss: 40.14024353027344, Learning Rate: 0.01\n",
      "Epoch [3785/20000], Loss: 40.12684631347656, Learning Rate: 0.01\n",
      "Epoch [3786/20000], Loss: 40.11344909667969, Learning Rate: 0.01\n",
      "Epoch [3787/20000], Loss: 40.10029602050781, Learning Rate: 0.01\n",
      "Epoch [3788/20000], Loss: 40.08685302734375, Learning Rate: 0.01\n",
      "Epoch [3789/20000], Loss: 40.07347106933594, Learning Rate: 0.01\n",
      "Epoch [3790/20000], Loss: 40.060211181640625, Learning Rate: 0.01\n",
      "Epoch [3791/20000], Loss: 40.0467529296875, Learning Rate: 0.01\n",
      "Epoch [3792/20000], Loss: 40.033538818359375, Learning Rate: 0.01\n",
      "Epoch [3793/20000], Loss: 40.02001953125, Learning Rate: 0.01\n",
      "Epoch [3794/20000], Loss: 40.00689697265625, Learning Rate: 0.01\n",
      "Epoch [3795/20000], Loss: 39.99346923828125, Learning Rate: 0.01\n",
      "Epoch [3796/20000], Loss: 39.9803466796875, Learning Rate: 0.01\n",
      "Epoch [3797/20000], Loss: 39.96696472167969, Learning Rate: 0.01\n",
      "Epoch [3798/20000], Loss: 39.953643798828125, Learning Rate: 0.01\n",
      "Epoch [3799/20000], Loss: 39.94035339355469, Learning Rate: 0.01\n",
      "Epoch [3800/20000], Loss: 39.92723083496094, Learning Rate: 0.01\n",
      "Epoch [3801/20000], Loss: 39.913787841796875, Learning Rate: 0.01\n",
      "Epoch [3802/20000], Loss: 39.90055847167969, Learning Rate: 0.01\n",
      "Epoch [3803/20000], Loss: 39.88726806640625, Learning Rate: 0.01\n",
      "Epoch [3804/20000], Loss: 39.87394714355469, Learning Rate: 0.01\n",
      "Epoch [3805/20000], Loss: 39.86073303222656, Learning Rate: 0.01\n",
      "Epoch [3806/20000], Loss: 39.84776306152344, Learning Rate: 0.01\n",
      "Epoch [3807/20000], Loss: 39.8345947265625, Learning Rate: 0.01\n",
      "Epoch [3808/20000], Loss: 39.8214111328125, Learning Rate: 0.01\n",
      "Epoch [3809/20000], Loss: 39.808624267578125, Learning Rate: 0.01\n",
      "Epoch [3810/20000], Loss: 39.7957763671875, Learning Rate: 0.01\n",
      "Epoch [3811/20000], Loss: 39.783203125, Learning Rate: 0.01\n",
      "Epoch [3812/20000], Loss: 39.77073669433594, Learning Rate: 0.01\n",
      "Epoch [3813/20000], Loss: 39.75909423828125, Learning Rate: 0.01\n",
      "Epoch [3814/20000], Loss: 39.74794006347656, Learning Rate: 0.01\n",
      "Epoch [3815/20000], Loss: 39.737762451171875, Learning Rate: 0.01\n",
      "Epoch [3816/20000], Loss: 39.72947692871094, Learning Rate: 0.01\n",
      "Epoch [3817/20000], Loss: 39.72319030761719, Learning Rate: 0.01\n",
      "Epoch [3818/20000], Loss: 39.72096252441406, Learning Rate: 0.01\n",
      "Epoch [3819/20000], Loss: 39.724334716796875, Learning Rate: 0.01\n",
      "Epoch [3820/20000], Loss: 39.73670959472656, Learning Rate: 0.01\n",
      "Epoch [3821/20000], Loss: 39.76249694824219, Learning Rate: 0.01\n",
      "Epoch [3822/20000], Loss: 39.809295654296875, Learning Rate: 0.01\n",
      "Epoch [3823/20000], Loss: 39.887359619140625, Learning Rate: 0.01\n",
      "Epoch [3824/20000], Loss: 40.01220703125, Learning Rate: 0.01\n",
      "Epoch [3825/20000], Loss: 40.203521728515625, Learning Rate: 0.01\n",
      "Epoch [3826/20000], Loss: 40.48460388183594, Learning Rate: 0.01\n",
      "Epoch [3827/20000], Loss: 40.87178039550781, Learning Rate: 0.01\n",
      "Epoch [3828/20000], Loss: 41.35990905761719, Learning Rate: 0.01\n",
      "Epoch [3829/20000], Loss: 41.88465881347656, Learning Rate: 0.01\n",
      "Epoch [3830/20000], Loss: 42.30546569824219, Learning Rate: 0.01\n",
      "Epoch [3831/20000], Loss: 42.41032409667969, Learning Rate: 0.01\n",
      "Epoch [3832/20000], Loss: 42.03875732421875, Learning Rate: 0.01\n",
      "Epoch [3833/20000], Loss: 41.228424072265625, Learning Rate: 0.01\n",
      "Epoch [3834/20000], Loss: 40.2879638671875, Learning Rate: 0.01\n",
      "Epoch [3835/20000], Loss: 39.62103271484375, Learning Rate: 0.01\n",
      "Epoch [3836/20000], Loss: 39.46311950683594, Learning Rate: 0.01\n",
      "Epoch [3837/20000], Loss: 39.74824523925781, Learning Rate: 0.01\n",
      "Epoch [3838/20000], Loss: 40.190948486328125, Learning Rate: 0.01\n",
      "Epoch [3839/20000], Loss: 40.47483825683594, Learning Rate: 0.01\n",
      "Epoch [3840/20000], Loss: 40.41813659667969, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3841/20000], Loss: 40.06529235839844, Learning Rate: 0.01\n",
      "Epoch [3842/20000], Loss: 39.63926696777344, Learning Rate: 0.01\n",
      "Epoch [3843/20000], Loss: 39.380950927734375, Learning Rate: 0.01\n",
      "Epoch [3844/20000], Loss: 39.38665771484375, Learning Rate: 0.01\n",
      "Epoch [3845/20000], Loss: 39.57171630859375, Learning Rate: 0.01\n",
      "Epoch [3846/20000], Loss: 39.75733947753906, Learning Rate: 0.01\n",
      "Epoch [3847/20000], Loss: 39.79901123046875, Learning Rate: 0.01\n",
      "Epoch [3848/20000], Loss: 39.67027282714844, Learning Rate: 0.01\n",
      "Epoch [3849/20000], Loss: 39.460601806640625, Learning Rate: 0.01\n",
      "Epoch [3850/20000], Loss: 39.298248291015625, Learning Rate: 0.01\n",
      "Epoch [3851/20000], Loss: 39.25843811035156, Learning Rate: 0.01\n",
      "Epoch [3852/20000], Loss: 39.324066162109375, Learning Rate: 0.01\n",
      "Epoch [3853/20000], Loss: 39.41410827636719, Learning Rate: 0.01\n",
      "Epoch [3854/20000], Loss: 39.4490966796875, Learning Rate: 0.01\n",
      "Epoch [3855/20000], Loss: 39.39973449707031, Learning Rate: 0.01\n",
      "Epoch [3856/20000], Loss: 39.29811096191406, Learning Rate: 0.01\n",
      "Epoch [3857/20000], Loss: 39.20372009277344, Learning Rate: 0.01\n",
      "Epoch [3858/20000], Loss: 39.16151428222656, Learning Rate: 0.01\n",
      "Epoch [3859/20000], Loss: 39.17420959472656, Learning Rate: 0.01\n",
      "Epoch [3860/20000], Loss: 39.21086120605469, Learning Rate: 0.01\n",
      "Epoch [3861/20000], Loss: 39.231109619140625, Learning Rate: 0.01\n",
      "Epoch [3862/20000], Loss: 39.2135009765625, Learning Rate: 0.01\n",
      "Epoch [3863/20000], Loss: 39.16474914550781, Learning Rate: 0.01\n",
      "Epoch [3864/20000], Loss: 39.109466552734375, Learning Rate: 0.01\n",
      "Epoch [3865/20000], Loss: 39.07231140136719, Learning Rate: 0.01\n",
      "Epoch [3866/20000], Loss: 39.06172180175781, Learning Rate: 0.01\n",
      "Epoch [3867/20000], Loss: 39.0692138671875, Learning Rate: 0.01\n",
      "Epoch [3868/20000], Loss: 39.07685852050781, Learning Rate: 0.01\n",
      "Epoch [3869/20000], Loss: 39.0704345703125, Learning Rate: 0.01\n",
      "Epoch [3870/20000], Loss: 39.04722595214844, Learning Rate: 0.01\n",
      "Epoch [3871/20000], Loss: 39.014892578125, Learning Rate: 0.01\n",
      "Epoch [3872/20000], Loss: 38.98530578613281, Learning Rate: 0.01\n",
      "Epoch [3873/20000], Loss: 38.966156005859375, Learning Rate: 0.01\n",
      "Epoch [3874/20000], Loss: 38.95805358886719, Learning Rate: 0.01\n",
      "Epoch [3875/20000], Loss: 38.95512390136719, Learning Rate: 0.01\n",
      "Epoch [3876/20000], Loss: 38.94999694824219, Learning Rate: 0.01\n",
      "Epoch [3877/20000], Loss: 38.93782043457031, Learning Rate: 0.01\n",
      "Epoch [3878/20000], Loss: 38.91886901855469, Learning Rate: 0.01\n",
      "Epoch [3879/20000], Loss: 38.897064208984375, Learning Rate: 0.01\n",
      "Epoch [3880/20000], Loss: 38.87725830078125, Learning Rate: 0.01\n",
      "Epoch [3881/20000], Loss: 38.86256408691406, Learning Rate: 0.01\n",
      "Epoch [3882/20000], Loss: 38.851959228515625, Learning Rate: 0.01\n",
      "Epoch [3883/20000], Loss: 38.84333801269531, Learning Rate: 0.01\n",
      "Epoch [3884/20000], Loss: 38.8336181640625, Learning Rate: 0.01\n",
      "Epoch [3885/20000], Loss: 38.821075439453125, Learning Rate: 0.01\n",
      "Epoch [3886/20000], Loss: 38.80583190917969, Learning Rate: 0.01\n",
      "Epoch [3887/20000], Loss: 38.78912353515625, Learning Rate: 0.01\n",
      "Epoch [3888/20000], Loss: 38.77294921875, Learning Rate: 0.01\n",
      "Epoch [3889/20000], Loss: 38.758636474609375, Learning Rate: 0.01\n",
      "Epoch [3890/20000], Loss: 38.74644470214844, Learning Rate: 0.01\n",
      "Epoch [3891/20000], Loss: 38.73506164550781, Learning Rate: 0.01\n",
      "Epoch [3892/20000], Loss: 38.723876953125, Learning Rate: 0.01\n",
      "Epoch [3893/20000], Loss: 38.7115478515625, Learning Rate: 0.01\n",
      "Epoch [3894/20000], Loss: 38.69828796386719, Learning Rate: 0.01\n",
      "Epoch [3895/20000], Loss: 38.68379211425781, Learning Rate: 0.01\n",
      "Epoch [3896/20000], Loss: 38.669342041015625, Learning Rate: 0.01\n",
      "Epoch [3897/20000], Loss: 38.6556396484375, Learning Rate: 0.01\n",
      "Epoch [3898/20000], Loss: 38.64234924316406, Learning Rate: 0.01\n",
      "Epoch [3899/20000], Loss: 38.62994384765625, Learning Rate: 0.01\n",
      "Epoch [3900/20000], Loss: 38.61773681640625, Learning Rate: 0.01\n",
      "Epoch [3901/20000], Loss: 38.60542297363281, Learning Rate: 0.01\n",
      "Epoch [3902/20000], Loss: 38.59260559082031, Learning Rate: 0.01\n",
      "Epoch [3903/20000], Loss: 38.579681396484375, Learning Rate: 0.01\n",
      "Epoch [3904/20000], Loss: 38.56596374511719, Learning Rate: 0.01\n",
      "Epoch [3905/20000], Loss: 38.552581787109375, Learning Rate: 0.01\n",
      "Epoch [3906/20000], Loss: 38.539215087890625, Learning Rate: 0.01\n",
      "Epoch [3907/20000], Loss: 38.52644348144531, Learning Rate: 0.01\n",
      "Epoch [3908/20000], Loss: 38.51353454589844, Learning Rate: 0.01\n",
      "Epoch [3909/20000], Loss: 38.5009765625, Learning Rate: 0.01\n",
      "Epoch [3910/20000], Loss: 38.488189697265625, Learning Rate: 0.01\n",
      "Epoch [3911/20000], Loss: 38.47550964355469, Learning Rate: 0.01\n",
      "Epoch [3912/20000], Loss: 38.46270751953125, Learning Rate: 0.01\n",
      "Epoch [3913/20000], Loss: 38.449676513671875, Learning Rate: 0.01\n",
      "Epoch [3914/20000], Loss: 38.43656921386719, Learning Rate: 0.01\n",
      "Epoch [3915/20000], Loss: 38.42341613769531, Learning Rate: 0.01\n",
      "Epoch [3916/20000], Loss: 38.410400390625, Learning Rate: 0.01\n",
      "Epoch [3917/20000], Loss: 38.39762878417969, Learning Rate: 0.01\n",
      "Epoch [3918/20000], Loss: 38.38470458984375, Learning Rate: 0.01\n",
      "Epoch [3919/20000], Loss: 38.372039794921875, Learning Rate: 0.01\n",
      "Epoch [3920/20000], Loss: 38.359222412109375, Learning Rate: 0.01\n",
      "Epoch [3921/20000], Loss: 38.346435546875, Learning Rate: 0.01\n",
      "Epoch [3922/20000], Loss: 38.33363342285156, Learning Rate: 0.01\n",
      "Epoch [3923/20000], Loss: 38.320770263671875, Learning Rate: 0.01\n",
      "Epoch [3924/20000], Loss: 38.30796813964844, Learning Rate: 0.01\n",
      "Epoch [3925/20000], Loss: 38.29481506347656, Learning Rate: 0.01\n",
      "Epoch [3926/20000], Loss: 38.28211975097656, Learning Rate: 0.01\n",
      "Epoch [3927/20000], Loss: 38.26933288574219, Learning Rate: 0.01\n",
      "Epoch [3928/20000], Loss: 38.256439208984375, Learning Rate: 0.01\n",
      "Epoch [3929/20000], Loss: 38.24360656738281, Learning Rate: 0.01\n",
      "Epoch [3930/20000], Loss: 38.23089599609375, Learning Rate: 0.01\n",
      "Epoch [3931/20000], Loss: 38.21797180175781, Learning Rate: 0.01\n",
      "Epoch [3932/20000], Loss: 38.205169677734375, Learning Rate: 0.01\n",
      "Epoch [3933/20000], Loss: 38.19232177734375, Learning Rate: 0.01\n",
      "Epoch [3934/20000], Loss: 38.179473876953125, Learning Rate: 0.01\n",
      "Epoch [3935/20000], Loss: 38.1666259765625, Learning Rate: 0.01\n",
      "Epoch [3936/20000], Loss: 38.153900146484375, Learning Rate: 0.01\n",
      "Epoch [3937/20000], Loss: 38.140960693359375, Learning Rate: 0.01\n",
      "Epoch [3938/20000], Loss: 38.12811279296875, Learning Rate: 0.01\n",
      "Epoch [3939/20000], Loss: 38.11543273925781, Learning Rate: 0.01\n",
      "Epoch [3940/20000], Loss: 38.10252380371094, Learning Rate: 0.01\n",
      "Epoch [3941/20000], Loss: 38.08970642089844, Learning Rate: 0.01\n",
      "Epoch [3942/20000], Loss: 38.07707214355469, Learning Rate: 0.01\n",
      "Epoch [3943/20000], Loss: 38.06431579589844, Learning Rate: 0.01\n",
      "Epoch [3944/20000], Loss: 38.05126953125, Learning Rate: 0.01\n",
      "Epoch [3945/20000], Loss: 38.03865051269531, Learning Rate: 0.01\n",
      "Epoch [3946/20000], Loss: 38.02598571777344, Learning Rate: 0.01\n",
      "Epoch [3947/20000], Loss: 38.01313781738281, Learning Rate: 0.01\n",
      "Epoch [3948/20000], Loss: 38.000457763671875, Learning Rate: 0.01\n",
      "Epoch [3949/20000], Loss: 37.987518310546875, Learning Rate: 0.01\n",
      "Epoch [3950/20000], Loss: 37.974884033203125, Learning Rate: 0.01\n",
      "Epoch [3951/20000], Loss: 37.9619140625, Learning Rate: 0.01\n",
      "Epoch [3952/20000], Loss: 37.94935607910156, Learning Rate: 0.01\n",
      "Epoch [3953/20000], Loss: 37.93647766113281, Learning Rate: 0.01\n",
      "Epoch [3954/20000], Loss: 37.92364501953125, Learning Rate: 0.01\n",
      "Epoch [3955/20000], Loss: 37.91105651855469, Learning Rate: 0.01\n",
      "Epoch [3956/20000], Loss: 37.89833068847656, Learning Rate: 0.01\n",
      "Epoch [3957/20000], Loss: 37.885528564453125, Learning Rate: 0.01\n",
      "Epoch [3958/20000], Loss: 37.87281799316406, Learning Rate: 0.01\n",
      "Epoch [3959/20000], Loss: 37.86000061035156, Learning Rate: 0.01\n",
      "Epoch [3960/20000], Loss: 37.847320556640625, Learning Rate: 0.01\n",
      "Epoch [3961/20000], Loss: 37.83448791503906, Learning Rate: 0.01\n",
      "Epoch [3962/20000], Loss: 37.82176208496094, Learning Rate: 0.01\n",
      "Epoch [3963/20000], Loss: 37.809051513671875, Learning Rate: 0.01\n",
      "Epoch [3964/20000], Loss: 37.79638671875, Learning Rate: 0.01\n",
      "Epoch [3965/20000], Loss: 37.78370666503906, Learning Rate: 0.01\n",
      "Epoch [3966/20000], Loss: 37.77073669433594, Learning Rate: 0.01\n",
      "Epoch [3967/20000], Loss: 37.758026123046875, Learning Rate: 0.01\n",
      "Epoch [3968/20000], Loss: 37.745361328125, Learning Rate: 0.01\n",
      "Epoch [3969/20000], Loss: 37.73265075683594, Learning Rate: 0.01\n",
      "Epoch [3970/20000], Loss: 37.72007751464844, Learning Rate: 0.01\n",
      "Epoch [3971/20000], Loss: 37.707427978515625, Learning Rate: 0.01\n",
      "Epoch [3972/20000], Loss: 37.69447326660156, Learning Rate: 0.01\n",
      "Epoch [3973/20000], Loss: 37.681793212890625, Learning Rate: 0.01\n",
      "Epoch [3974/20000], Loss: 37.66917419433594, Learning Rate: 0.01\n",
      "Epoch [3975/20000], Loss: 37.65653991699219, Learning Rate: 0.01\n",
      "Epoch [3976/20000], Loss: 37.64384460449219, Learning Rate: 0.01\n",
      "Epoch [3977/20000], Loss: 37.63108825683594, Learning Rate: 0.01\n",
      "Epoch [3978/20000], Loss: 37.61824035644531, Learning Rate: 0.01\n",
      "Epoch [3979/20000], Loss: 37.60577392578125, Learning Rate: 0.01\n",
      "Epoch [3980/20000], Loss: 37.59291076660156, Learning Rate: 0.01\n",
      "Epoch [3981/20000], Loss: 37.58013916015625, Learning Rate: 0.01\n",
      "Epoch [3982/20000], Loss: 37.56748962402344, Learning Rate: 0.01\n",
      "Epoch [3983/20000], Loss: 37.555023193359375, Learning Rate: 0.01\n",
      "Epoch [3984/20000], Loss: 37.542236328125, Learning Rate: 0.01\n",
      "Epoch [3985/20000], Loss: 37.52937316894531, Learning Rate: 0.01\n",
      "Epoch [3986/20000], Loss: 37.516876220703125, Learning Rate: 0.01\n",
      "Epoch [3987/20000], Loss: 37.50419616699219, Learning Rate: 0.01\n",
      "Epoch [3988/20000], Loss: 37.491485595703125, Learning Rate: 0.01\n",
      "Epoch [3989/20000], Loss: 37.47892761230469, Learning Rate: 0.01\n",
      "Epoch [3990/20000], Loss: 37.46614074707031, Learning Rate: 0.01\n",
      "Epoch [3991/20000], Loss: 37.45375061035156, Learning Rate: 0.01\n",
      "Epoch [3992/20000], Loss: 37.44093322753906, Learning Rate: 0.01\n",
      "Epoch [3993/20000], Loss: 37.42840576171875, Learning Rate: 0.01\n",
      "Epoch [3994/20000], Loss: 37.415679931640625, Learning Rate: 0.01\n",
      "Epoch [3995/20000], Loss: 37.40290832519531, Learning Rate: 0.01\n",
      "Epoch [3996/20000], Loss: 37.39027404785156, Learning Rate: 0.01\n",
      "Epoch [3997/20000], Loss: 37.37776184082031, Learning Rate: 0.01\n",
      "Epoch [3998/20000], Loss: 37.36512756347656, Learning Rate: 0.01\n",
      "Epoch [3999/20000], Loss: 37.352630615234375, Learning Rate: 0.01\n",
      "Epoch [4000/20000], Loss: 37.34013366699219, Learning Rate: 0.01\n",
      "Epoch [4001/20000], Loss: 37.32745361328125, Learning Rate: 0.01\n",
      "Epoch [4002/20000], Loss: 37.31526184082031, Learning Rate: 0.01\n",
      "Epoch [4003/20000], Loss: 37.302764892578125, Learning Rate: 0.01\n",
      "Epoch [4004/20000], Loss: 37.29051208496094, Learning Rate: 0.01\n",
      "Epoch [4005/20000], Loss: 37.27854919433594, Learning Rate: 0.01\n",
      "Epoch [4006/20000], Loss: 37.266632080078125, Learning Rate: 0.01\n",
      "Epoch [4007/20000], Loss: 37.255279541015625, Learning Rate: 0.01\n",
      "Epoch [4008/20000], Loss: 37.24482727050781, Learning Rate: 0.01\n",
      "Epoch [4009/20000], Loss: 37.23504638671875, Learning Rate: 0.01\n",
      "Epoch [4010/20000], Loss: 37.22711181640625, Learning Rate: 0.01\n",
      "Epoch [4011/20000], Loss: 37.22100830078125, Learning Rate: 0.01\n",
      "Epoch [4012/20000], Loss: 37.21900939941406, Learning Rate: 0.01\n",
      "Epoch [4013/20000], Loss: 37.22239685058594, Learning Rate: 0.01\n",
      "Epoch [4014/20000], Loss: 37.23439025878906, Learning Rate: 0.01\n",
      "Epoch [4015/20000], Loss: 37.26008605957031, Learning Rate: 0.01\n",
      "Epoch [4016/20000], Loss: 37.30677795410156, Learning Rate: 0.01\n",
      "Epoch [4017/20000], Loss: 37.38639831542969, Learning Rate: 0.01\n",
      "Epoch [4018/20000], Loss: 37.51646423339844, Learning Rate: 0.01\n",
      "Epoch [4019/20000], Loss: 37.72242736816406, Learning Rate: 0.01\n",
      "Epoch [4020/20000], Loss: 38.03907775878906, Learning Rate: 0.01\n",
      "Epoch [4021/20000], Loss: 38.506378173828125, Learning Rate: 0.01\n",
      "Epoch [4022/20000], Loss: 39.1575927734375, Learning Rate: 0.01\n",
      "Epoch [4023/20000], Loss: 39.9827880859375, Learning Rate: 0.01\n",
      "Epoch [4024/20000], Loss: 40.87335205078125, Learning Rate: 0.01\n",
      "Epoch [4025/20000], Loss: 41.55853271484375, Learning Rate: 0.01\n",
      "Epoch [4026/20000], Loss: 41.65428161621094, Learning Rate: 0.01\n",
      "Epoch [4027/20000], Loss: 40.88031005859375, Learning Rate: 0.01\n",
      "Epoch [4028/20000], Loss: 39.40074157714844, Learning Rate: 0.01\n",
      "Epoch [4029/20000], Loss: 37.87263488769531, Learning Rate: 0.01\n",
      "Epoch [4030/20000], Loss: 37.02442932128906, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4031/20000], Loss: 37.12158203125, Learning Rate: 0.01\n",
      "Epoch [4032/20000], Loss: 37.83766174316406, Learning Rate: 0.01\n",
      "Epoch [4033/20000], Loss: 38.54487609863281, Learning Rate: 0.01\n",
      "Epoch [4034/20000], Loss: 38.719970703125, Learning Rate: 0.01\n",
      "Epoch [4035/20000], Loss: 38.25347900390625, Learning Rate: 0.01\n",
      "Epoch [4036/20000], Loss: 37.49165344238281, Learning Rate: 0.01\n",
      "Epoch [4037/20000], Loss: 36.95024108886719, Learning Rate: 0.01\n",
      "Epoch [4038/20000], Loss: 36.912078857421875, Learning Rate: 0.01\n",
      "Epoch [4039/20000], Loss: 37.254241943359375, Learning Rate: 0.01\n",
      "Epoch [4040/20000], Loss: 37.611724853515625, Learning Rate: 0.01\n",
      "Epoch [4041/20000], Loss: 37.676910400390625, Learning Rate: 0.01\n",
      "Epoch [4042/20000], Loss: 37.410797119140625, Learning Rate: 0.01\n",
      "Epoch [4043/20000], Loss: 37.028472900390625, Learning Rate: 0.01\n",
      "Epoch [4044/20000], Loss: 36.79866027832031, Learning Rate: 0.01\n",
      "Epoch [4045/20000], Loss: 36.82904052734375, Learning Rate: 0.01\n",
      "Epoch [4046/20000], Loss: 37.01611328125, Learning Rate: 0.01\n",
      "Epoch [4047/20000], Loss: 37.161895751953125, Learning Rate: 0.01\n",
      "Epoch [4048/20000], Loss: 37.13752746582031, Learning Rate: 0.01\n",
      "Epoch [4049/20000], Loss: 36.967254638671875, Learning Rate: 0.01\n",
      "Epoch [4050/20000], Loss: 36.78089904785156, Learning Rate: 0.01\n",
      "Epoch [4051/20000], Loss: 36.698028564453125, Learning Rate: 0.01\n",
      "Epoch [4052/20000], Loss: 36.73936462402344, Learning Rate: 0.01\n",
      "Epoch [4053/20000], Loss: 36.83067321777344, Learning Rate: 0.01\n",
      "Epoch [4054/20000], Loss: 36.876220703125, Learning Rate: 0.01\n",
      "Epoch [4055/20000], Loss: 36.83296203613281, Learning Rate: 0.01\n",
      "Epoch [4056/20000], Loss: 36.73274230957031, Learning Rate: 0.01\n",
      "Epoch [4057/20000], Loss: 36.64324951171875, Learning Rate: 0.01\n",
      "Epoch [4058/20000], Loss: 36.61262512207031, Learning Rate: 0.01\n",
      "Epoch [4059/20000], Loss: 36.63697814941406, Learning Rate: 0.01\n",
      "Epoch [4060/20000], Loss: 36.67424011230469, Learning Rate: 0.01\n",
      "Epoch [4061/20000], Loss: 36.681793212890625, Learning Rate: 0.01\n",
      "Epoch [4062/20000], Loss: 36.64683532714844, Learning Rate: 0.01\n",
      "Epoch [4063/20000], Loss: 36.58985900878906, Learning Rate: 0.01\n",
      "Epoch [4064/20000], Loss: 36.54307556152344, Learning Rate: 0.01\n",
      "Epoch [4065/20000], Loss: 36.52593994140625, Learning Rate: 0.01\n",
      "Epoch [4066/20000], Loss: 36.53338623046875, Learning Rate: 0.01\n",
      "Epoch [4067/20000], Loss: 36.544525146484375, Learning Rate: 0.01\n",
      "Epoch [4068/20000], Loss: 36.540191650390625, Learning Rate: 0.01\n",
      "Epoch [4069/20000], Loss: 36.516082763671875, Learning Rate: 0.01\n",
      "Epoch [4070/20000], Loss: 36.482208251953125, Learning Rate: 0.01\n",
      "Epoch [4071/20000], Loss: 36.4534912109375, Learning Rate: 0.01\n",
      "Epoch [4072/20000], Loss: 36.43817138671875, Learning Rate: 0.01\n",
      "Epoch [4073/20000], Loss: 36.434783935546875, Learning Rate: 0.01\n",
      "Epoch [4074/20000], Loss: 36.433837890625, Learning Rate: 0.01\n",
      "Epoch [4075/20000], Loss: 36.42633056640625, Learning Rate: 0.01\n",
      "Epoch [4076/20000], Loss: 36.4097900390625, Learning Rate: 0.01\n",
      "Epoch [4077/20000], Loss: 36.38740539550781, Learning Rate: 0.01\n",
      "Epoch [4078/20000], Loss: 36.36622619628906, Learning Rate: 0.01\n",
      "Epoch [4079/20000], Loss: 36.35096740722656, Learning Rate: 0.01\n",
      "Epoch [4080/20000], Loss: 36.34178161621094, Learning Rate: 0.01\n",
      "Epoch [4081/20000], Loss: 36.33500671386719, Learning Rate: 0.01\n",
      "Epoch [4082/20000], Loss: 36.32609558105469, Learning Rate: 0.01\n",
      "Epoch [4083/20000], Loss: 36.31318664550781, Learning Rate: 0.01\n",
      "Epoch [4084/20000], Loss: 36.296844482421875, Learning Rate: 0.01\n",
      "Epoch [4085/20000], Loss: 36.2799072265625, Learning Rate: 0.01\n",
      "Epoch [4086/20000], Loss: 36.264801025390625, Learning Rate: 0.01\n",
      "Epoch [4087/20000], Loss: 36.25285339355469, Learning Rate: 0.01\n",
      "Epoch [4088/20000], Loss: 36.24261474609375, Learning Rate: 0.01\n",
      "Epoch [4089/20000], Loss: 36.23274230957031, Learning Rate: 0.01\n",
      "Epoch [4090/20000], Loss: 36.22117614746094, Learning Rate: 0.01\n",
      "Epoch [4091/20000], Loss: 36.2078857421875, Learning Rate: 0.01\n",
      "Epoch [4092/20000], Loss: 36.19352722167969, Learning Rate: 0.01\n",
      "Epoch [4093/20000], Loss: 36.17938232421875, Learning Rate: 0.01\n",
      "Epoch [4094/20000], Loss: 36.166107177734375, Learning Rate: 0.01\n",
      "Epoch [4095/20000], Loss: 36.154144287109375, Learning Rate: 0.01\n",
      "Epoch [4096/20000], Loss: 36.14289855957031, Learning Rate: 0.01\n",
      "Epoch [4097/20000], Loss: 36.13140869140625, Learning Rate: 0.01\n",
      "Epoch [4098/20000], Loss: 36.11943054199219, Learning Rate: 0.01\n",
      "Epoch [4099/20000], Loss: 36.10667419433594, Learning Rate: 0.01\n",
      "Epoch [4100/20000], Loss: 36.09352111816406, Learning Rate: 0.01\n",
      "Epoch [4101/20000], Loss: 36.08039855957031, Learning Rate: 0.01\n",
      "Epoch [4102/20000], Loss: 36.0677490234375, Learning Rate: 0.01\n",
      "Epoch [4103/20000], Loss: 36.055633544921875, Learning Rate: 0.01\n",
      "Epoch [4104/20000], Loss: 36.043701171875, Learning Rate: 0.01\n",
      "Epoch [4105/20000], Loss: 36.031890869140625, Learning Rate: 0.01\n",
      "Epoch [4106/20000], Loss: 36.019378662109375, Learning Rate: 0.01\n",
      "Epoch [4107/20000], Loss: 36.007232666015625, Learning Rate: 0.01\n",
      "Epoch [4108/20000], Loss: 35.99455261230469, Learning Rate: 0.01\n",
      "Epoch [4109/20000], Loss: 35.98194885253906, Learning Rate: 0.01\n",
      "Epoch [4110/20000], Loss: 35.969451904296875, Learning Rate: 0.01\n",
      "Epoch [4111/20000], Loss: 35.95707702636719, Learning Rate: 0.01\n",
      "Epoch [4112/20000], Loss: 35.944976806640625, Learning Rate: 0.01\n",
      "Epoch [4113/20000], Loss: 35.93296813964844, Learning Rate: 0.01\n",
      "Epoch [4114/20000], Loss: 35.920928955078125, Learning Rate: 0.01\n",
      "Epoch [4115/20000], Loss: 35.90864562988281, Learning Rate: 0.01\n",
      "Epoch [4116/20000], Loss: 35.896240234375, Learning Rate: 0.01\n",
      "Epoch [4117/20000], Loss: 35.8837890625, Learning Rate: 0.01\n",
      "Epoch [4118/20000], Loss: 35.87138366699219, Learning Rate: 0.01\n",
      "Epoch [4119/20000], Loss: 35.859100341796875, Learning Rate: 0.01\n",
      "Epoch [4120/20000], Loss: 35.8468017578125, Learning Rate: 0.01\n",
      "Epoch [4121/20000], Loss: 35.83448791503906, Learning Rate: 0.01\n",
      "Epoch [4122/20000], Loss: 35.822296142578125, Learning Rate: 0.01\n",
      "Epoch [4123/20000], Loss: 35.81005859375, Learning Rate: 0.01\n",
      "Epoch [4124/20000], Loss: 35.797943115234375, Learning Rate: 0.01\n",
      "Epoch [4125/20000], Loss: 35.78553771972656, Learning Rate: 0.01\n",
      "Epoch [4126/20000], Loss: 35.77342224121094, Learning Rate: 0.01\n",
      "Epoch [4127/20000], Loss: 35.76100158691406, Learning Rate: 0.01\n",
      "Epoch [4128/20000], Loss: 35.74885559082031, Learning Rate: 0.01\n",
      "Epoch [4129/20000], Loss: 35.73663330078125, Learning Rate: 0.01\n",
      "Epoch [4130/20000], Loss: 35.724273681640625, Learning Rate: 0.01\n",
      "Epoch [4131/20000], Loss: 35.71202087402344, Learning Rate: 0.01\n",
      "Epoch [4132/20000], Loss: 35.699920654296875, Learning Rate: 0.01\n",
      "Epoch [4133/20000], Loss: 35.68775939941406, Learning Rate: 0.01\n",
      "Epoch [4134/20000], Loss: 35.67530822753906, Learning Rate: 0.01\n",
      "Epoch [4135/20000], Loss: 35.66319274902344, Learning Rate: 0.01\n",
      "Epoch [4136/20000], Loss: 35.65092468261719, Learning Rate: 0.01\n",
      "Epoch [4137/20000], Loss: 35.63871765136719, Learning Rate: 0.01\n",
      "Epoch [4138/20000], Loss: 35.62644958496094, Learning Rate: 0.01\n",
      "Epoch [4139/20000], Loss: 35.61421203613281, Learning Rate: 0.01\n",
      "Epoch [4140/20000], Loss: 35.60194396972656, Learning Rate: 0.01\n",
      "Epoch [4141/20000], Loss: 35.58984375, Learning Rate: 0.01\n",
      "Epoch [4142/20000], Loss: 35.57780456542969, Learning Rate: 0.01\n",
      "Epoch [4143/20000], Loss: 35.56561279296875, Learning Rate: 0.01\n",
      "Epoch [4144/20000], Loss: 35.55332946777344, Learning Rate: 0.01\n",
      "Epoch [4145/20000], Loss: 35.540985107421875, Learning Rate: 0.01\n",
      "Epoch [4146/20000], Loss: 35.52888488769531, Learning Rate: 0.01\n",
      "Epoch [4147/20000], Loss: 35.516632080078125, Learning Rate: 0.01\n",
      "Epoch [4148/20000], Loss: 35.504547119140625, Learning Rate: 0.01\n",
      "Epoch [4149/20000], Loss: 35.492279052734375, Learning Rate: 0.01\n",
      "Epoch [4150/20000], Loss: 35.480010986328125, Learning Rate: 0.01\n",
      "Epoch [4151/20000], Loss: 35.467864990234375, Learning Rate: 0.01\n",
      "Epoch [4152/20000], Loss: 35.45579528808594, Learning Rate: 0.01\n",
      "Epoch [4153/20000], Loss: 35.4434814453125, Learning Rate: 0.01\n",
      "Epoch [4154/20000], Loss: 35.43125915527344, Learning Rate: 0.01\n",
      "Epoch [4155/20000], Loss: 35.41908264160156, Learning Rate: 0.01\n",
      "Epoch [4156/20000], Loss: 35.40696716308594, Learning Rate: 0.01\n",
      "Epoch [4157/20000], Loss: 35.39469909667969, Learning Rate: 0.01\n",
      "Epoch [4158/20000], Loss: 35.38255310058594, Learning Rate: 0.01\n",
      "Epoch [4159/20000], Loss: 35.370391845703125, Learning Rate: 0.01\n",
      "Epoch [4160/20000], Loss: 35.358154296875, Learning Rate: 0.01\n",
      "Epoch [4161/20000], Loss: 35.3460693359375, Learning Rate: 0.01\n",
      "Epoch [4162/20000], Loss: 35.333892822265625, Learning Rate: 0.01\n",
      "Epoch [4163/20000], Loss: 35.32167053222656, Learning Rate: 0.01\n",
      "Epoch [4164/20000], Loss: 35.30958557128906, Learning Rate: 0.01\n",
      "Epoch [4165/20000], Loss: 35.29718017578125, Learning Rate: 0.01\n",
      "Epoch [4166/20000], Loss: 35.285186767578125, Learning Rate: 0.01\n",
      "Epoch [4167/20000], Loss: 35.27299499511719, Learning Rate: 0.01\n",
      "Epoch [4168/20000], Loss: 35.26094055175781, Learning Rate: 0.01\n",
      "Epoch [4169/20000], Loss: 35.24876403808594, Learning Rate: 0.01\n",
      "Epoch [4170/20000], Loss: 35.23651123046875, Learning Rate: 0.01\n",
      "Epoch [4171/20000], Loss: 35.224578857421875, Learning Rate: 0.01\n",
      "Epoch [4172/20000], Loss: 35.21217346191406, Learning Rate: 0.01\n",
      "Epoch [4173/20000], Loss: 35.199981689453125, Learning Rate: 0.01\n",
      "Epoch [4174/20000], Loss: 35.18791198730469, Learning Rate: 0.01\n",
      "Epoch [4175/20000], Loss: 35.17576599121094, Learning Rate: 0.01\n",
      "Epoch [4176/20000], Loss: 35.16368103027344, Learning Rate: 0.01\n",
      "Epoch [4177/20000], Loss: 35.15153503417969, Learning Rate: 0.01\n",
      "Epoch [4178/20000], Loss: 35.13920593261719, Learning Rate: 0.01\n",
      "Epoch [4179/20000], Loss: 35.1273193359375, Learning Rate: 0.01\n",
      "Epoch [4180/20000], Loss: 35.11524963378906, Learning Rate: 0.01\n",
      "Epoch [4181/20000], Loss: 35.103057861328125, Learning Rate: 0.01\n",
      "Epoch [4182/20000], Loss: 35.0908203125, Learning Rate: 0.01\n",
      "Epoch [4183/20000], Loss: 35.0787353515625, Learning Rate: 0.01\n",
      "Epoch [4184/20000], Loss: 35.0665283203125, Learning Rate: 0.01\n",
      "Epoch [4185/20000], Loss: 35.0543212890625, Learning Rate: 0.01\n",
      "Epoch [4186/20000], Loss: 35.04231262207031, Learning Rate: 0.01\n",
      "Epoch [4187/20000], Loss: 35.03038024902344, Learning Rate: 0.01\n",
      "Epoch [4188/20000], Loss: 35.018157958984375, Learning Rate: 0.01\n",
      "Epoch [4189/20000], Loss: 35.00584411621094, Learning Rate: 0.01\n",
      "Epoch [4190/20000], Loss: 34.993988037109375, Learning Rate: 0.01\n",
      "Epoch [4191/20000], Loss: 34.98179626464844, Learning Rate: 0.01\n",
      "Epoch [4192/20000], Loss: 34.969573974609375, Learning Rate: 0.01\n",
      "Epoch [4193/20000], Loss: 34.95747375488281, Learning Rate: 0.01\n",
      "Epoch [4194/20000], Loss: 34.9454345703125, Learning Rate: 0.01\n",
      "Epoch [4195/20000], Loss: 34.933380126953125, Learning Rate: 0.01\n",
      "Epoch [4196/20000], Loss: 34.921173095703125, Learning Rate: 0.01\n",
      "Epoch [4197/20000], Loss: 34.90904235839844, Learning Rate: 0.01\n",
      "Epoch [4198/20000], Loss: 34.89686584472656, Learning Rate: 0.01\n",
      "Epoch [4199/20000], Loss: 34.884796142578125, Learning Rate: 0.01\n",
      "Epoch [4200/20000], Loss: 34.8729248046875, Learning Rate: 0.01\n",
      "Epoch [4201/20000], Loss: 34.860504150390625, Learning Rate: 0.01\n",
      "Epoch [4202/20000], Loss: 34.8486328125, Learning Rate: 0.01\n",
      "Epoch [4203/20000], Loss: 34.83650207519531, Learning Rate: 0.01\n",
      "Epoch [4204/20000], Loss: 34.82435607910156, Learning Rate: 0.01\n",
      "Epoch [4205/20000], Loss: 34.81236267089844, Learning Rate: 0.01\n",
      "Epoch [4206/20000], Loss: 34.80010986328125, Learning Rate: 0.01\n",
      "Epoch [4207/20000], Loss: 34.78797912597656, Learning Rate: 0.01\n",
      "Epoch [4208/20000], Loss: 34.77618408203125, Learning Rate: 0.01\n",
      "Epoch [4209/20000], Loss: 34.763916015625, Learning Rate: 0.01\n",
      "Epoch [4210/20000], Loss: 34.75178527832031, Learning Rate: 0.01\n",
      "Epoch [4211/20000], Loss: 34.73976135253906, Learning Rate: 0.01\n",
      "Epoch [4212/20000], Loss: 34.72772216796875, Learning Rate: 0.01\n",
      "Epoch [4213/20000], Loss: 34.715606689453125, Learning Rate: 0.01\n",
      "Epoch [4214/20000], Loss: 34.703521728515625, Learning Rate: 0.01\n",
      "Epoch [4215/20000], Loss: 34.69134521484375, Learning Rate: 0.01\n",
      "Epoch [4216/20000], Loss: 34.67942810058594, Learning Rate: 0.01\n",
      "Epoch [4217/20000], Loss: 34.667266845703125, Learning Rate: 0.01\n",
      "Epoch [4218/20000], Loss: 34.65531921386719, Learning Rate: 0.01\n",
      "Epoch [4219/20000], Loss: 34.643096923828125, Learning Rate: 0.01\n",
      "Epoch [4220/20000], Loss: 34.63117980957031, Learning Rate: 0.01\n",
      "Epoch [4221/20000], Loss: 34.619293212890625, Learning Rate: 0.01\n",
      "Epoch [4222/20000], Loss: 34.607147216796875, Learning Rate: 0.01\n",
      "Epoch [4223/20000], Loss: 34.595123291015625, Learning Rate: 0.01\n",
      "Epoch [4224/20000], Loss: 34.583282470703125, Learning Rate: 0.01\n",
      "Epoch [4225/20000], Loss: 34.571136474609375, Learning Rate: 0.01\n",
      "Epoch [4226/20000], Loss: 34.55943298339844, Learning Rate: 0.01\n",
      "Epoch [4227/20000], Loss: 34.54780578613281, Learning Rate: 0.01\n",
      "Epoch [4228/20000], Loss: 34.53614807128906, Learning Rate: 0.01\n",
      "Epoch [4229/20000], Loss: 34.524932861328125, Learning Rate: 0.01\n",
      "Epoch [4230/20000], Loss: 34.51405334472656, Learning Rate: 0.01\n",
      "Epoch [4231/20000], Loss: 34.50390625, Learning Rate: 0.01\n",
      "Epoch [4232/20000], Loss: 34.49488830566406, Learning Rate: 0.01\n",
      "Epoch [4233/20000], Loss: 34.487274169921875, Learning Rate: 0.01\n",
      "Epoch [4234/20000], Loss: 34.48216247558594, Learning Rate: 0.01\n",
      "Epoch [4235/20000], Loss: 34.48121643066406, Learning Rate: 0.01\n",
      "Epoch [4236/20000], Loss: 34.48664855957031, Learning Rate: 0.01\n",
      "Epoch [4237/20000], Loss: 34.502685546875, Learning Rate: 0.01\n",
      "Epoch [4238/20000], Loss: 34.535125732421875, Learning Rate: 0.01\n",
      "Epoch [4239/20000], Loss: 34.594482421875, Learning Rate: 0.01\n",
      "Epoch [4240/20000], Loss: 34.69667053222656, Learning Rate: 0.01\n",
      "Epoch [4241/20000], Loss: 34.86714172363281, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4242/20000], Loss: 35.14439392089844, Learning Rate: 0.01\n",
      "Epoch [4243/20000], Loss: 35.584197998046875, Learning Rate: 0.01\n",
      "Epoch [4244/20000], Loss: 36.2576904296875, Learning Rate: 0.01\n",
      "Epoch [4245/20000], Loss: 37.234039306640625, Learning Rate: 0.01\n",
      "Epoch [4246/20000], Loss: 38.527435302734375, Learning Rate: 0.01\n",
      "Epoch [4247/20000], Loss: 39.98638916015625, Learning Rate: 0.01\n",
      "Epoch [4248/20000], Loss: 41.17022705078125, Learning Rate: 0.01\n",
      "Epoch [4249/20000], Loss: 41.37762451171875, Learning Rate: 0.01\n",
      "Epoch [4250/20000], Loss: 40.08660888671875, Learning Rate: 0.01\n",
      "Epoch [4251/20000], Loss: 37.62327575683594, Learning Rate: 0.01\n",
      "Epoch [4252/20000], Loss: 35.25157165527344, Learning Rate: 0.01\n",
      "Epoch [4253/20000], Loss: 34.23834228515625, Learning Rate: 0.01\n",
      "Epoch [4254/20000], Loss: 34.82701110839844, Learning Rate: 0.01\n",
      "Epoch [4255/20000], Loss: 36.17047119140625, Learning Rate: 0.01\n",
      "Epoch [4256/20000], Loss: 37.06248474121094, Learning Rate: 0.01\n",
      "Epoch [4257/20000], Loss: 36.78639221191406, Learning Rate: 0.01\n",
      "Epoch [4258/20000], Loss: 35.59855651855469, Learning Rate: 0.01\n",
      "Epoch [4259/20000], Loss: 34.462860107421875, Learning Rate: 0.01\n",
      "Epoch [4260/20000], Loss: 34.17286682128906, Learning Rate: 0.01\n",
      "Epoch [4261/20000], Loss: 34.70672607421875, Learning Rate: 0.01\n",
      "Epoch [4262/20000], Loss: 35.38490295410156, Learning Rate: 0.01\n",
      "Epoch [4263/20000], Loss: 35.53785705566406, Learning Rate: 0.01\n",
      "Epoch [4264/20000], Loss: 35.05279541015625, Learning Rate: 0.01\n",
      "Epoch [4265/20000], Loss: 34.38496398925781, Learning Rate: 0.01\n",
      "Epoch [4266/20000], Loss: 34.07743835449219, Learning Rate: 0.01\n",
      "Epoch [4267/20000], Loss: 34.2677001953125, Learning Rate: 0.01\n",
      "Epoch [4268/20000], Loss: 34.64381408691406, Learning Rate: 0.01\n",
      "Epoch [4269/20000], Loss: 34.796051025390625, Learning Rate: 0.01\n",
      "Epoch [4270/20000], Loss: 34.58746337890625, Learning Rate: 0.01\n",
      "Epoch [4271/20000], Loss: 34.22340393066406, Learning Rate: 0.01\n",
      "Epoch [4272/20000], Loss: 34.01124572753906, Learning Rate: 0.01\n",
      "Epoch [4273/20000], Loss: 34.071258544921875, Learning Rate: 0.01\n",
      "Epoch [4274/20000], Loss: 34.2652587890625, Learning Rate: 0.01\n",
      "Epoch [4275/20000], Loss: 34.36793518066406, Learning Rate: 0.01\n",
      "Epoch [4276/20000], Loss: 34.27803039550781, Learning Rate: 0.01\n",
      "Epoch [4277/20000], Loss: 34.081787109375, Learning Rate: 0.01\n",
      "Epoch [4278/20000], Loss: 33.94410705566406, Learning Rate: 0.01\n",
      "Epoch [4279/20000], Loss: 33.94889831542969, Learning Rate: 0.01\n",
      "Epoch [4280/20000], Loss: 34.042083740234375, Learning Rate: 0.01\n",
      "Epoch [4281/20000], Loss: 34.10566711425781, Learning Rate: 0.01\n",
      "Epoch [4282/20000], Loss: 34.070404052734375, Learning Rate: 0.01\n",
      "Epoch [4283/20000], Loss: 33.96614074707031, Learning Rate: 0.01\n",
      "Epoch [4284/20000], Loss: 33.87629699707031, Learning Rate: 0.01\n",
      "Epoch [4285/20000], Loss: 33.85691833496094, Learning Rate: 0.01\n",
      "Epoch [4286/20000], Loss: 33.894683837890625, Learning Rate: 0.01\n",
      "Epoch [4287/20000], Loss: 33.930816650390625, Learning Rate: 0.01\n",
      "Epoch [4288/20000], Loss: 33.920654296875, Learning Rate: 0.01\n",
      "Epoch [4289/20000], Loss: 33.86676025390625, Learning Rate: 0.01\n",
      "Epoch [4290/20000], Loss: 33.80796813964844, Learning Rate: 0.01\n",
      "Epoch [4291/20000], Loss: 33.780303955078125, Learning Rate: 0.01\n",
      "Epoch [4292/20000], Loss: 33.787109375, Learning Rate: 0.01\n",
      "Epoch [4293/20000], Loss: 33.803741455078125, Learning Rate: 0.01\n",
      "Epoch [4294/20000], Loss: 33.80244445800781, Learning Rate: 0.01\n",
      "Epoch [4295/20000], Loss: 33.77577209472656, Learning Rate: 0.01\n",
      "Epoch [4296/20000], Loss: 33.73832702636719, Learning Rate: 0.01\n",
      "Epoch [4297/20000], Loss: 33.71026611328125, Learning Rate: 0.01\n",
      "Epoch [4298/20000], Loss: 33.701385498046875, Learning Rate: 0.01\n",
      "Epoch [4299/20000], Loss: 33.70384216308594, Learning Rate: 0.01\n",
      "Epoch [4300/20000], Loss: 33.702850341796875, Learning Rate: 0.01\n",
      "Epoch [4301/20000], Loss: 33.68951416015625, Learning Rate: 0.01\n",
      "Epoch [4302/20000], Loss: 33.66621398925781, Learning Rate: 0.01\n",
      "Epoch [4303/20000], Loss: 33.64253234863281, Learning Rate: 0.01\n",
      "Epoch [4304/20000], Loss: 33.6268310546875, Learning Rate: 0.01\n",
      "Epoch [4305/20000], Loss: 33.619720458984375, Learning Rate: 0.01\n",
      "Epoch [4306/20000], Loss: 33.615142822265625, Learning Rate: 0.01\n",
      "Epoch [4307/20000], Loss: 33.606689453125, Learning Rate: 0.01\n",
      "Epoch [4308/20000], Loss: 33.59193420410156, Learning Rate: 0.01\n",
      "Epoch [4309/20000], Loss: 33.57380676269531, Learning Rate: 0.01\n",
      "Epoch [4310/20000], Loss: 33.55718994140625, Learning Rate: 0.01\n",
      "Epoch [4311/20000], Loss: 33.54469299316406, Learning Rate: 0.01\n",
      "Epoch [4312/20000], Loss: 33.53594970703125, Learning Rate: 0.01\n",
      "Epoch [4313/20000], Loss: 33.52754211425781, Learning Rate: 0.01\n",
      "Epoch [4314/20000], Loss: 33.516754150390625, Learning Rate: 0.01\n",
      "Epoch [4315/20000], Loss: 33.50306701660156, Learning Rate: 0.01\n",
      "Epoch [4316/20000], Loss: 33.48808288574219, Learning Rate: 0.01\n",
      "Epoch [4317/20000], Loss: 33.47425842285156, Learning Rate: 0.01\n",
      "Epoch [4318/20000], Loss: 33.46234130859375, Learning Rate: 0.01\n",
      "Epoch [4319/20000], Loss: 33.45207214355469, Learning Rate: 0.01\n",
      "Epoch [4320/20000], Loss: 33.4420166015625, Learning Rate: 0.01\n",
      "Epoch [4321/20000], Loss: 33.430694580078125, Learning Rate: 0.01\n",
      "Epoch [4322/20000], Loss: 33.41790771484375, Learning Rate: 0.01\n",
      "Epoch [4323/20000], Loss: 33.40470886230469, Learning Rate: 0.01\n",
      "Epoch [4324/20000], Loss: 33.39189147949219, Learning Rate: 0.01\n",
      "Epoch [4325/20000], Loss: 33.3800048828125, Learning Rate: 0.01\n",
      "Epoch [4326/20000], Loss: 33.368988037109375, Learning Rate: 0.01\n",
      "Epoch [4327/20000], Loss: 33.35795593261719, Learning Rate: 0.01\n",
      "Epoch [4328/20000], Loss: 33.34663391113281, Learning Rate: 0.01\n",
      "Epoch [4329/20000], Loss: 33.334503173828125, Learning Rate: 0.01\n",
      "Epoch [4330/20000], Loss: 33.321929931640625, Learning Rate: 0.01\n",
      "Epoch [4331/20000], Loss: 33.30989074707031, Learning Rate: 0.01\n",
      "Epoch [4332/20000], Loss: 33.297821044921875, Learning Rate: 0.01\n",
      "Epoch [4333/20000], Loss: 33.28636169433594, Learning Rate: 0.01\n",
      "Epoch [4334/20000], Loss: 33.27500915527344, Learning Rate: 0.01\n",
      "Epoch [4335/20000], Loss: 33.26335144042969, Learning Rate: 0.01\n",
      "Epoch [4336/20000], Loss: 33.251739501953125, Learning Rate: 0.01\n",
      "Epoch [4337/20000], Loss: 33.23976135253906, Learning Rate: 0.01\n",
      "Epoch [4338/20000], Loss: 33.227874755859375, Learning Rate: 0.01\n",
      "Epoch [4339/20000], Loss: 33.21577453613281, Learning Rate: 0.01\n",
      "Epoch [4340/20000], Loss: 33.2041015625, Learning Rate: 0.01\n",
      "Epoch [4341/20000], Loss: 33.19267272949219, Learning Rate: 0.01\n",
      "Epoch [4342/20000], Loss: 33.180999755859375, Learning Rate: 0.01\n",
      "Epoch [4343/20000], Loss: 33.169219970703125, Learning Rate: 0.01\n",
      "Epoch [4344/20000], Loss: 33.15763854980469, Learning Rate: 0.01\n",
      "Epoch [4345/20000], Loss: 33.145782470703125, Learning Rate: 0.01\n",
      "Epoch [4346/20000], Loss: 33.133941650390625, Learning Rate: 0.01\n",
      "Epoch [4347/20000], Loss: 33.122161865234375, Learning Rate: 0.01\n",
      "Epoch [4348/20000], Loss: 33.110321044921875, Learning Rate: 0.01\n",
      "Epoch [4349/20000], Loss: 33.09877014160156, Learning Rate: 0.01\n",
      "Epoch [4350/20000], Loss: 33.08711242675781, Learning Rate: 0.01\n",
      "Epoch [4351/20000], Loss: 33.07548522949219, Learning Rate: 0.01\n",
      "Epoch [4352/20000], Loss: 33.06373596191406, Learning Rate: 0.01\n",
      "Epoch [4353/20000], Loss: 33.0521240234375, Learning Rate: 0.01\n",
      "Epoch [4354/20000], Loss: 33.04026794433594, Learning Rate: 0.01\n",
      "Epoch [4355/20000], Loss: 33.028472900390625, Learning Rate: 0.01\n",
      "Epoch [4356/20000], Loss: 33.01678466796875, Learning Rate: 0.01\n",
      "Epoch [4357/20000], Loss: 33.00532531738281, Learning Rate: 0.01\n",
      "Epoch [4358/20000], Loss: 32.99360656738281, Learning Rate: 0.01\n",
      "Epoch [4359/20000], Loss: 32.98167419433594, Learning Rate: 0.01\n",
      "Epoch [4360/20000], Loss: 32.97013854980469, Learning Rate: 0.01\n",
      "Epoch [4361/20000], Loss: 32.95841979980469, Learning Rate: 0.01\n",
      "Epoch [4362/20000], Loss: 32.946807861328125, Learning Rate: 0.01\n",
      "Epoch [4363/20000], Loss: 32.93519592285156, Learning Rate: 0.01\n",
      "Epoch [4364/20000], Loss: 32.92326354980469, Learning Rate: 0.01\n",
      "Epoch [4365/20000], Loss: 32.91160583496094, Learning Rate: 0.01\n",
      "Epoch [4366/20000], Loss: 32.90000915527344, Learning Rate: 0.01\n",
      "Epoch [4367/20000], Loss: 32.88822937011719, Learning Rate: 0.01\n",
      "Epoch [4368/20000], Loss: 32.87664794921875, Learning Rate: 0.01\n",
      "Epoch [4369/20000], Loss: 32.86494445800781, Learning Rate: 0.01\n",
      "Epoch [4370/20000], Loss: 32.853363037109375, Learning Rate: 0.01\n",
      "Epoch [4371/20000], Loss: 32.84161376953125, Learning Rate: 0.01\n",
      "Epoch [4372/20000], Loss: 32.82997131347656, Learning Rate: 0.01\n",
      "Epoch [4373/20000], Loss: 32.818389892578125, Learning Rate: 0.01\n",
      "Epoch [4374/20000], Loss: 32.80653381347656, Learning Rate: 0.01\n",
      "Epoch [4375/20000], Loss: 32.7950439453125, Learning Rate: 0.01\n",
      "Epoch [4376/20000], Loss: 32.78321838378906, Learning Rate: 0.01\n",
      "Epoch [4377/20000], Loss: 32.771636962890625, Learning Rate: 0.01\n",
      "Epoch [4378/20000], Loss: 32.759918212890625, Learning Rate: 0.01\n",
      "Epoch [4379/20000], Loss: 32.74842834472656, Learning Rate: 0.01\n",
      "Epoch [4380/20000], Loss: 32.73664855957031, Learning Rate: 0.01\n",
      "Epoch [4381/20000], Loss: 32.72509765625, Learning Rate: 0.01\n",
      "Epoch [4382/20000], Loss: 32.71339416503906, Learning Rate: 0.01\n",
      "Epoch [4383/20000], Loss: 32.70167541503906, Learning Rate: 0.01\n",
      "Epoch [4384/20000], Loss: 32.69004821777344, Learning Rate: 0.01\n",
      "Epoch [4385/20000], Loss: 32.678375244140625, Learning Rate: 0.01\n",
      "Epoch [4386/20000], Loss: 32.66658020019531, Learning Rate: 0.01\n",
      "Epoch [4387/20000], Loss: 32.65495300292969, Learning Rate: 0.01\n",
      "Epoch [4388/20000], Loss: 32.6434326171875, Learning Rate: 0.01\n",
      "Epoch [4389/20000], Loss: 32.631927490234375, Learning Rate: 0.01\n",
      "Epoch [4390/20000], Loss: 32.62013244628906, Learning Rate: 0.01\n",
      "Epoch [4391/20000], Loss: 32.608489990234375, Learning Rate: 0.01\n",
      "Epoch [4392/20000], Loss: 32.59687805175781, Learning Rate: 0.01\n",
      "Epoch [4393/20000], Loss: 32.5853271484375, Learning Rate: 0.01\n",
      "Epoch [4394/20000], Loss: 32.57354736328125, Learning Rate: 0.01\n",
      "Epoch [4395/20000], Loss: 32.56187438964844, Learning Rate: 0.01\n",
      "Epoch [4396/20000], Loss: 32.55049133300781, Learning Rate: 0.01\n",
      "Epoch [4397/20000], Loss: 32.53868103027344, Learning Rate: 0.01\n",
      "Epoch [4398/20000], Loss: 32.5269775390625, Learning Rate: 0.01\n",
      "Epoch [4399/20000], Loss: 32.51548767089844, Learning Rate: 0.01\n",
      "Epoch [4400/20000], Loss: 32.503692626953125, Learning Rate: 0.01\n",
      "Epoch [4401/20000], Loss: 32.49220275878906, Learning Rate: 0.01\n",
      "Epoch [4402/20000], Loss: 32.48060607910156, Learning Rate: 0.01\n",
      "Epoch [4403/20000], Loss: 32.46879577636719, Learning Rate: 0.01\n",
      "Epoch [4404/20000], Loss: 32.457244873046875, Learning Rate: 0.01\n",
      "Epoch [4405/20000], Loss: 32.44563293457031, Learning Rate: 0.01\n",
      "Epoch [4406/20000], Loss: 32.43403625488281, Learning Rate: 0.01\n",
      "Epoch [4407/20000], Loss: 32.42230224609375, Learning Rate: 0.01\n",
      "Epoch [4408/20000], Loss: 32.410797119140625, Learning Rate: 0.01\n",
      "Epoch [4409/20000], Loss: 32.399139404296875, Learning Rate: 0.01\n",
      "Epoch [4410/20000], Loss: 32.38755798339844, Learning Rate: 0.01\n",
      "Epoch [4411/20000], Loss: 32.37593078613281, Learning Rate: 0.01\n",
      "Epoch [4412/20000], Loss: 32.36424255371094, Learning Rate: 0.01\n",
      "Epoch [4413/20000], Loss: 32.3526611328125, Learning Rate: 0.01\n",
      "Epoch [4414/20000], Loss: 32.341094970703125, Learning Rate: 0.01\n",
      "Epoch [4415/20000], Loss: 32.32942199707031, Learning Rate: 0.01\n",
      "Epoch [4416/20000], Loss: 32.31791687011719, Learning Rate: 0.01\n",
      "Epoch [4417/20000], Loss: 32.30622863769531, Learning Rate: 0.01\n",
      "Epoch [4418/20000], Loss: 32.29441833496094, Learning Rate: 0.01\n",
      "Epoch [4419/20000], Loss: 32.282958984375, Learning Rate: 0.01\n",
      "Epoch [4420/20000], Loss: 32.27134704589844, Learning Rate: 0.01\n",
      "Epoch [4421/20000], Loss: 32.25978088378906, Learning Rate: 0.01\n",
      "Epoch [4422/20000], Loss: 32.24822998046875, Learning Rate: 0.01\n",
      "Epoch [4423/20000], Loss: 32.236541748046875, Learning Rate: 0.01\n",
      "Epoch [4424/20000], Loss: 32.22486877441406, Learning Rate: 0.01\n",
      "Epoch [4425/20000], Loss: 32.213409423828125, Learning Rate: 0.01\n",
      "Epoch [4426/20000], Loss: 32.201904296875, Learning Rate: 0.01\n",
      "Epoch [4427/20000], Loss: 32.19020080566406, Learning Rate: 0.01\n",
      "Epoch [4428/20000], Loss: 32.17863464355469, Learning Rate: 0.01\n",
      "Epoch [4429/20000], Loss: 32.1671142578125, Learning Rate: 0.01\n",
      "Epoch [4430/20000], Loss: 32.15528869628906, Learning Rate: 0.01\n",
      "Epoch [4431/20000], Loss: 32.14399719238281, Learning Rate: 0.01\n",
      "Epoch [4432/20000], Loss: 32.13226318359375, Learning Rate: 0.01\n",
      "Epoch [4433/20000], Loss: 32.12071228027344, Learning Rate: 0.01\n",
      "Epoch [4434/20000], Loss: 32.10914611816406, Learning Rate: 0.01\n",
      "Epoch [4435/20000], Loss: 32.09739685058594, Learning Rate: 0.01\n",
      "Epoch [4436/20000], Loss: 32.08604431152344, Learning Rate: 0.01\n",
      "Epoch [4437/20000], Loss: 32.07432556152344, Learning Rate: 0.01\n",
      "Epoch [4438/20000], Loss: 32.06266784667969, Learning Rate: 0.01\n",
      "Epoch [4439/20000], Loss: 32.05126953125, Learning Rate: 0.01\n",
      "Epoch [4440/20000], Loss: 32.03955078125, Learning Rate: 0.01\n",
      "Epoch [4441/20000], Loss: 32.027862548828125, Learning Rate: 0.01\n",
      "Epoch [4442/20000], Loss: 32.016448974609375, Learning Rate: 0.01\n",
      "Epoch [4443/20000], Loss: 32.0048828125, Learning Rate: 0.01\n",
      "Epoch [4444/20000], Loss: 31.993255615234375, Learning Rate: 0.01\n",
      "Epoch [4445/20000], Loss: 31.98193359375, Learning Rate: 0.01\n",
      "Epoch [4446/20000], Loss: 31.970382690429688, Learning Rate: 0.01\n",
      "Epoch [4447/20000], Loss: 31.958602905273438, Learning Rate: 0.01\n",
      "Epoch [4448/20000], Loss: 31.946975708007812, Learning Rate: 0.01\n",
      "Epoch [4449/20000], Loss: 31.935455322265625, Learning Rate: 0.01\n",
      "Epoch [4450/20000], Loss: 31.924057006835938, Learning Rate: 0.01\n",
      "Epoch [4451/20000], Loss: 31.912338256835938, Learning Rate: 0.01\n",
      "Epoch [4452/20000], Loss: 31.900863647460938, Learning Rate: 0.01\n",
      "Epoch [4453/20000], Loss: 31.889266967773438, Learning Rate: 0.01\n",
      "Epoch [4454/20000], Loss: 31.877731323242188, Learning Rate: 0.01\n",
      "Epoch [4455/20000], Loss: 31.866165161132812, Learning Rate: 0.01\n",
      "Epoch [4456/20000], Loss: 31.854537963867188, Learning Rate: 0.01\n",
      "Epoch [4457/20000], Loss: 31.843032836914062, Learning Rate: 0.01\n",
      "Epoch [4458/20000], Loss: 31.831390380859375, Learning Rate: 0.01\n",
      "Epoch [4459/20000], Loss: 31.819961547851562, Learning Rate: 0.01\n",
      "Epoch [4460/20000], Loss: 31.808212280273438, Learning Rate: 0.01\n",
      "Epoch [4461/20000], Loss: 31.796768188476562, Learning Rate: 0.01\n",
      "Epoch [4462/20000], Loss: 31.785293579101562, Learning Rate: 0.01\n",
      "Epoch [4463/20000], Loss: 31.773712158203125, Learning Rate: 0.01\n",
      "Epoch [4464/20000], Loss: 31.7620849609375, Learning Rate: 0.01\n",
      "Epoch [4465/20000], Loss: 31.750564575195312, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4466/20000], Loss: 31.739089965820312, Learning Rate: 0.01\n",
      "Epoch [4467/20000], Loss: 31.72760009765625, Learning Rate: 0.01\n",
      "Epoch [4468/20000], Loss: 31.715957641601562, Learning Rate: 0.01\n",
      "Epoch [4469/20000], Loss: 31.704757690429688, Learning Rate: 0.01\n",
      "Epoch [4470/20000], Loss: 31.693161010742188, Learning Rate: 0.01\n",
      "Epoch [4471/20000], Loss: 31.681732177734375, Learning Rate: 0.01\n",
      "Epoch [4472/20000], Loss: 31.670196533203125, Learning Rate: 0.01\n",
      "Epoch [4473/20000], Loss: 31.6590576171875, Learning Rate: 0.01\n",
      "Epoch [4474/20000], Loss: 31.647796630859375, Learning Rate: 0.01\n",
      "Epoch [4475/20000], Loss: 31.636932373046875, Learning Rate: 0.01\n",
      "Epoch [4476/20000], Loss: 31.626007080078125, Learning Rate: 0.01\n",
      "Epoch [4477/20000], Loss: 31.61578369140625, Learning Rate: 0.01\n",
      "Epoch [4478/20000], Loss: 31.605804443359375, Learning Rate: 0.01\n",
      "Epoch [4479/20000], Loss: 31.596878051757812, Learning Rate: 0.01\n",
      "Epoch [4480/20000], Loss: 31.5894775390625, Learning Rate: 0.01\n",
      "Epoch [4481/20000], Loss: 31.584060668945312, Learning Rate: 0.01\n",
      "Epoch [4482/20000], Loss: 31.581878662109375, Learning Rate: 0.01\n",
      "Epoch [4483/20000], Loss: 31.584976196289062, Learning Rate: 0.01\n",
      "Epoch [4484/20000], Loss: 31.595916748046875, Learning Rate: 0.01\n",
      "Epoch [4485/20000], Loss: 31.619461059570312, Learning Rate: 0.01\n",
      "Epoch [4486/20000], Loss: 31.663345336914062, Learning Rate: 0.01\n",
      "Epoch [4487/20000], Loss: 31.738311767578125, Learning Rate: 0.01\n",
      "Epoch [4488/20000], Loss: 31.862808227539062, Learning Rate: 0.01\n",
      "Epoch [4489/20000], Loss: 32.06425476074219, Learning Rate: 0.01\n",
      "Epoch [4490/20000], Loss: 32.38389587402344, Learning Rate: 0.01\n",
      "Epoch [4491/20000], Loss: 32.879669189453125, Learning Rate: 0.01\n",
      "Epoch [4492/20000], Loss: 33.6240234375, Learning Rate: 0.01\n",
      "Epoch [4493/20000], Loss: 34.6861572265625, Learning Rate: 0.01\n",
      "Epoch [4494/20000], Loss: 36.07981872558594, Learning Rate: 0.01\n",
      "Epoch [4495/20000], Loss: 37.65931701660156, Learning Rate: 0.01\n",
      "Epoch [4496/20000], Loss: 38.99507141113281, Learning Rate: 0.01\n",
      "Epoch [4497/20000], Loss: 39.3873291015625, Learning Rate: 0.01\n",
      "Epoch [4498/20000], Loss: 38.23487854003906, Learning Rate: 0.01\n",
      "Epoch [4499/20000], Loss: 35.707794189453125, Learning Rate: 0.01\n",
      "Epoch [4500/20000], Loss: 32.97412109375, Learning Rate: 0.01\n",
      "Epoch [4501/20000], Loss: 31.443283081054688, Learning Rate: 0.01\n",
      "Epoch [4502/20000], Loss: 31.6590576171875, Learning Rate: 0.01\n",
      "Epoch [4503/20000], Loss: 32.994049072265625, Learning Rate: 0.01\n",
      "Epoch [4504/20000], Loss: 34.22998046875, Learning Rate: 0.01\n",
      "Epoch [4505/20000], Loss: 34.39997863769531, Learning Rate: 0.01\n",
      "Epoch [4506/20000], Loss: 33.40773010253906, Learning Rate: 0.01\n",
      "Epoch [4507/20000], Loss: 32.04071044921875, Learning Rate: 0.01\n",
      "Epoch [4508/20000], Loss: 31.28277587890625, Learning Rate: 0.01\n",
      "Epoch [4509/20000], Loss: 31.4967041015625, Learning Rate: 0.01\n",
      "Epoch [4510/20000], Loss: 32.2401123046875, Learning Rate: 0.01\n",
      "Epoch [4511/20000], Loss: 32.75126647949219, Learning Rate: 0.01\n",
      "Epoch [4512/20000], Loss: 32.5882568359375, Learning Rate: 0.01\n",
      "Epoch [4513/20000], Loss: 31.926101684570312, Learning Rate: 0.01\n",
      "Epoch [4514/20000], Loss: 31.322311401367188, Learning Rate: 0.01\n",
      "Epoch [4515/20000], Loss: 31.195114135742188, Learning Rate: 0.01\n",
      "Epoch [4516/20000], Loss: 31.501251220703125, Learning Rate: 0.01\n",
      "Epoch [4517/20000], Loss: 31.857955932617188, Learning Rate: 0.01\n",
      "Epoch [4518/20000], Loss: 31.917449951171875, Learning Rate: 0.01\n",
      "Epoch [4519/20000], Loss: 31.641220092773438, Learning Rate: 0.01\n",
      "Epoch [4520/20000], Loss: 31.2779541015625, Learning Rate: 0.01\n",
      "Epoch [4521/20000], Loss: 31.105133056640625, Learning Rate: 0.01\n",
      "Epoch [4522/20000], Loss: 31.19403076171875, Learning Rate: 0.01\n",
      "Epoch [4523/20000], Loss: 31.389984130859375, Learning Rate: 0.01\n",
      "Epoch [4524/20000], Loss: 31.483016967773438, Learning Rate: 0.01\n",
      "Epoch [4525/20000], Loss: 31.388839721679688, Learning Rate: 0.01\n",
      "Epoch [4526/20000], Loss: 31.193588256835938, Learning Rate: 0.01\n",
      "Epoch [4527/20000], Loss: 31.051010131835938, Learning Rate: 0.01\n",
      "Epoch [4528/20000], Loss: 31.044418334960938, Learning Rate: 0.01\n",
      "Epoch [4529/20000], Loss: 31.1319580078125, Learning Rate: 0.01\n",
      "Epoch [4530/20000], Loss: 31.205398559570312, Learning Rate: 0.01\n",
      "Epoch [4531/20000], Loss: 31.190170288085938, Learning Rate: 0.01\n",
      "Epoch [4532/20000], Loss: 31.096969604492188, Learning Rate: 0.01\n",
      "Epoch [4533/20000], Loss: 30.998367309570312, Learning Rate: 0.01\n",
      "Epoch [4534/20000], Loss: 30.957046508789062, Learning Rate: 0.01\n",
      "Epoch [4535/20000], Loss: 30.979568481445312, Learning Rate: 0.01\n",
      "Epoch [4536/20000], Loss: 31.021713256835938, Learning Rate: 0.01\n",
      "Epoch [4537/20000], Loss: 31.032943725585938, Learning Rate: 0.01\n",
      "Epoch [4538/20000], Loss: 30.997161865234375, Learning Rate: 0.01\n",
      "Epoch [4539/20000], Loss: 30.938156127929688, Learning Rate: 0.01\n",
      "Epoch [4540/20000], Loss: 30.893051147460938, Learning Rate: 0.01\n",
      "Epoch [4541/20000], Loss: 30.882080078125, Learning Rate: 0.01\n",
      "Epoch [4542/20000], Loss: 30.895217895507812, Learning Rate: 0.01\n",
      "Epoch [4543/20000], Loss: 30.90679931640625, Learning Rate: 0.01\n",
      "Epoch [4544/20000], Loss: 30.897735595703125, Learning Rate: 0.01\n",
      "Epoch [4545/20000], Loss: 30.868133544921875, Learning Rate: 0.01\n",
      "Epoch [4546/20000], Loss: 30.83331298828125, Learning Rate: 0.01\n",
      "Epoch [4547/20000], Loss: 30.810150146484375, Learning Rate: 0.01\n",
      "Epoch [4548/20000], Loss: 30.803482055664062, Learning Rate: 0.01\n",
      "Epoch [4549/20000], Loss: 30.805328369140625, Learning Rate: 0.01\n",
      "Epoch [4550/20000], Loss: 30.803253173828125, Learning Rate: 0.01\n",
      "Epoch [4551/20000], Loss: 30.79046630859375, Learning Rate: 0.01\n",
      "Epoch [4552/20000], Loss: 30.769195556640625, Learning Rate: 0.01\n",
      "Epoch [4553/20000], Loss: 30.746871948242188, Learning Rate: 0.01\n",
      "Epoch [4554/20000], Loss: 30.730819702148438, Learning Rate: 0.01\n",
      "Epoch [4555/20000], Loss: 30.72247314453125, Learning Rate: 0.01\n",
      "Epoch [4556/20000], Loss: 30.717605590820312, Learning Rate: 0.01\n",
      "Epoch [4557/20000], Loss: 30.710647583007812, Learning Rate: 0.01\n",
      "Epoch [4558/20000], Loss: 30.69842529296875, Learning Rate: 0.01\n",
      "Epoch [4559/20000], Loss: 30.682281494140625, Learning Rate: 0.01\n",
      "Epoch [4560/20000], Loss: 30.665664672851562, Learning Rate: 0.01\n",
      "Epoch [4561/20000], Loss: 30.65191650390625, Learning Rate: 0.01\n",
      "Epoch [4562/20000], Loss: 30.641799926757812, Learning Rate: 0.01\n",
      "Epoch [4563/20000], Loss: 30.633529663085938, Learning Rate: 0.01\n",
      "Epoch [4564/20000], Loss: 30.624496459960938, Learning Rate: 0.01\n",
      "Epoch [4565/20000], Loss: 30.613189697265625, Learning Rate: 0.01\n",
      "Epoch [4566/20000], Loss: 30.599822998046875, Learning Rate: 0.01\n",
      "Epoch [4567/20000], Loss: 30.586135864257812, Learning Rate: 0.01\n",
      "Epoch [4568/20000], Loss: 30.573410034179688, Learning Rate: 0.01\n",
      "Epoch [4569/20000], Loss: 30.561981201171875, Learning Rate: 0.01\n",
      "Epoch [4570/20000], Loss: 30.55194091796875, Learning Rate: 0.01\n",
      "Epoch [4571/20000], Loss: 30.542098999023438, Learning Rate: 0.01\n",
      "Epoch [4572/20000], Loss: 30.531295776367188, Learning Rate: 0.01\n",
      "Epoch [4573/20000], Loss: 30.519439697265625, Learning Rate: 0.01\n",
      "Epoch [4574/20000], Loss: 30.506973266601562, Learning Rate: 0.01\n",
      "Epoch [4575/20000], Loss: 30.494720458984375, Learning Rate: 0.01\n",
      "Epoch [4576/20000], Loss: 30.483108520507812, Learning Rate: 0.01\n",
      "Epoch [4577/20000], Loss: 30.47210693359375, Learning Rate: 0.01\n",
      "Epoch [4578/20000], Loss: 30.461471557617188, Learning Rate: 0.01\n",
      "Epoch [4579/20000], Loss: 30.450759887695312, Learning Rate: 0.01\n",
      "Epoch [4580/20000], Loss: 30.4395751953125, Learning Rate: 0.01\n",
      "Epoch [4581/20000], Loss: 30.42767333984375, Learning Rate: 0.01\n",
      "Epoch [4582/20000], Loss: 30.416107177734375, Learning Rate: 0.01\n",
      "Epoch [4583/20000], Loss: 30.404251098632812, Learning Rate: 0.01\n",
      "Epoch [4584/20000], Loss: 30.39306640625, Learning Rate: 0.01\n",
      "Epoch [4585/20000], Loss: 30.382034301757812, Learning Rate: 0.01\n",
      "Epoch [4586/20000], Loss: 30.371200561523438, Learning Rate: 0.01\n",
      "Epoch [4587/20000], Loss: 30.359939575195312, Learning Rate: 0.01\n",
      "Epoch [4588/20000], Loss: 30.348770141601562, Learning Rate: 0.01\n",
      "Epoch [4589/20000], Loss: 30.337387084960938, Learning Rate: 0.01\n",
      "Epoch [4590/20000], Loss: 30.32574462890625, Learning Rate: 0.01\n",
      "Epoch [4591/20000], Loss: 30.314437866210938, Learning Rate: 0.01\n",
      "Epoch [4592/20000], Loss: 30.303085327148438, Learning Rate: 0.01\n",
      "Epoch [4593/20000], Loss: 30.291946411132812, Learning Rate: 0.01\n",
      "Epoch [4594/20000], Loss: 30.280929565429688, Learning Rate: 0.01\n",
      "Epoch [4595/20000], Loss: 30.269821166992188, Learning Rate: 0.01\n",
      "Epoch [4596/20000], Loss: 30.258544921875, Learning Rate: 0.01\n",
      "Epoch [4597/20000], Loss: 30.247329711914062, Learning Rate: 0.01\n",
      "Epoch [4598/20000], Loss: 30.235992431640625, Learning Rate: 0.01\n",
      "Epoch [4599/20000], Loss: 30.224624633789062, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4600/20000], Loss: 30.213226318359375, Learning Rate: 0.01\n",
      "Epoch [4601/20000], Loss: 30.202041625976562, Learning Rate: 0.01\n",
      "Epoch [4602/20000], Loss: 30.190811157226562, Learning Rate: 0.01\n",
      "Epoch [4603/20000], Loss: 30.17962646484375, Learning Rate: 0.01\n",
      "Epoch [4604/20000], Loss: 30.1685791015625, Learning Rate: 0.01\n",
      "Epoch [4605/20000], Loss: 30.157302856445312, Learning Rate: 0.01\n",
      "Epoch [4606/20000], Loss: 30.146041870117188, Learning Rate: 0.01\n",
      "Epoch [4607/20000], Loss: 30.134780883789062, Learning Rate: 0.01\n",
      "Epoch [4608/20000], Loss: 30.123550415039062, Learning Rate: 0.01\n",
      "Epoch [4609/20000], Loss: 30.112335205078125, Learning Rate: 0.01\n",
      "Epoch [4610/20000], Loss: 30.101211547851562, Learning Rate: 0.01\n",
      "Epoch [4611/20000], Loss: 30.09002685546875, Learning Rate: 0.01\n",
      "Epoch [4612/20000], Loss: 30.078704833984375, Learning Rate: 0.01\n",
      "Epoch [4613/20000], Loss: 30.067428588867188, Learning Rate: 0.01\n",
      "Epoch [4614/20000], Loss: 30.0562744140625, Learning Rate: 0.01\n",
      "Epoch [4615/20000], Loss: 30.045028686523438, Learning Rate: 0.01\n",
      "Epoch [4616/20000], Loss: 30.033889770507812, Learning Rate: 0.01\n",
      "Epoch [4617/20000], Loss: 30.022598266601562, Learning Rate: 0.01\n",
      "Epoch [4618/20000], Loss: 30.011444091796875, Learning Rate: 0.01\n",
      "Epoch [4619/20000], Loss: 30.00018310546875, Learning Rate: 0.01\n",
      "Epoch [4620/20000], Loss: 29.9888916015625, Learning Rate: 0.01\n",
      "Epoch [4621/20000], Loss: 29.977691650390625, Learning Rate: 0.01\n",
      "Epoch [4622/20000], Loss: 29.966659545898438, Learning Rate: 0.01\n",
      "Epoch [4623/20000], Loss: 29.95538330078125, Learning Rate: 0.01\n",
      "Epoch [4624/20000], Loss: 29.944091796875, Learning Rate: 0.01\n",
      "Epoch [4625/20000], Loss: 29.932998657226562, Learning Rate: 0.01\n",
      "Epoch [4626/20000], Loss: 29.921798706054688, Learning Rate: 0.01\n",
      "Epoch [4627/20000], Loss: 29.910568237304688, Learning Rate: 0.01\n",
      "Epoch [4628/20000], Loss: 29.89910888671875, Learning Rate: 0.01\n",
      "Epoch [4629/20000], Loss: 29.88824462890625, Learning Rate: 0.01\n",
      "Epoch [4630/20000], Loss: 29.876907348632812, Learning Rate: 0.01\n",
      "Epoch [4631/20000], Loss: 29.865646362304688, Learning Rate: 0.01\n",
      "Epoch [4632/20000], Loss: 29.8546142578125, Learning Rate: 0.01\n",
      "Epoch [4633/20000], Loss: 29.843231201171875, Learning Rate: 0.01\n",
      "Epoch [4634/20000], Loss: 29.832199096679688, Learning Rate: 0.01\n",
      "Epoch [4635/20000], Loss: 29.821029663085938, Learning Rate: 0.01\n",
      "Epoch [4636/20000], Loss: 29.809738159179688, Learning Rate: 0.01\n",
      "Epoch [4637/20000], Loss: 29.798538208007812, Learning Rate: 0.01\n",
      "Epoch [4638/20000], Loss: 29.787277221679688, Learning Rate: 0.01\n",
      "Epoch [4639/20000], Loss: 29.77606201171875, Learning Rate: 0.01\n",
      "Epoch [4640/20000], Loss: 29.764907836914062, Learning Rate: 0.01\n",
      "Epoch [4641/20000], Loss: 29.753890991210938, Learning Rate: 0.01\n",
      "Epoch [4642/20000], Loss: 29.74267578125, Learning Rate: 0.01\n",
      "Epoch [4643/20000], Loss: 29.731201171875, Learning Rate: 0.01\n",
      "Epoch [4644/20000], Loss: 29.720291137695312, Learning Rate: 0.01\n",
      "Epoch [4645/20000], Loss: 29.709091186523438, Learning Rate: 0.01\n",
      "Epoch [4646/20000], Loss: 29.697967529296875, Learning Rate: 0.01\n",
      "Epoch [4647/20000], Loss: 29.686676025390625, Learning Rate: 0.01\n",
      "Epoch [4648/20000], Loss: 29.675460815429688, Learning Rate: 0.01\n",
      "Epoch [4649/20000], Loss: 29.664306640625, Learning Rate: 0.01\n",
      "Epoch [4650/20000], Loss: 29.653244018554688, Learning Rate: 0.01\n",
      "Epoch [4651/20000], Loss: 29.641891479492188, Learning Rate: 0.01\n",
      "Epoch [4652/20000], Loss: 29.6307373046875, Learning Rate: 0.01\n",
      "Epoch [4653/20000], Loss: 29.619598388671875, Learning Rate: 0.01\n",
      "Epoch [4654/20000], Loss: 29.60845947265625, Learning Rate: 0.01\n",
      "Epoch [4655/20000], Loss: 29.597427368164062, Learning Rate: 0.01\n",
      "Epoch [4656/20000], Loss: 29.586044311523438, Learning Rate: 0.01\n",
      "Epoch [4657/20000], Loss: 29.574874877929688, Learning Rate: 0.01\n",
      "Epoch [4658/20000], Loss: 29.563735961914062, Learning Rate: 0.01\n",
      "Epoch [4659/20000], Loss: 29.552536010742188, Learning Rate: 0.01\n",
      "Epoch [4660/20000], Loss: 29.541305541992188, Learning Rate: 0.01\n",
      "Epoch [4661/20000], Loss: 29.530044555664062, Learning Rate: 0.01\n",
      "Epoch [4662/20000], Loss: 29.51904296875, Learning Rate: 0.01\n",
      "Epoch [4663/20000], Loss: 29.50775146484375, Learning Rate: 0.01\n",
      "Epoch [4664/20000], Loss: 29.496597290039062, Learning Rate: 0.01\n",
      "Epoch [4665/20000], Loss: 29.485366821289062, Learning Rate: 0.01\n",
      "Epoch [4666/20000], Loss: 29.474258422851562, Learning Rate: 0.01\n",
      "Epoch [4667/20000], Loss: 29.463104248046875, Learning Rate: 0.01\n",
      "Epoch [4668/20000], Loss: 29.452041625976562, Learning Rate: 0.01\n",
      "Epoch [4669/20000], Loss: 29.440841674804688, Learning Rate: 0.01\n",
      "Epoch [4670/20000], Loss: 29.429641723632812, Learning Rate: 0.01\n",
      "Epoch [4671/20000], Loss: 29.418380737304688, Learning Rate: 0.01\n",
      "Epoch [4672/20000], Loss: 29.407363891601562, Learning Rate: 0.01\n",
      "Epoch [4673/20000], Loss: 29.396224975585938, Learning Rate: 0.01\n",
      "Epoch [4674/20000], Loss: 29.384933471679688, Learning Rate: 0.01\n",
      "Epoch [4675/20000], Loss: 29.373825073242188, Learning Rate: 0.01\n",
      "Epoch [4676/20000], Loss: 29.362747192382812, Learning Rate: 0.01\n",
      "Epoch [4677/20000], Loss: 29.351409912109375, Learning Rate: 0.01\n",
      "Epoch [4678/20000], Loss: 29.34033203125, Learning Rate: 0.01\n",
      "Epoch [4679/20000], Loss: 29.32916259765625, Learning Rate: 0.01\n",
      "Epoch [4680/20000], Loss: 29.317947387695312, Learning Rate: 0.01\n",
      "Epoch [4681/20000], Loss: 29.306930541992188, Learning Rate: 0.01\n",
      "Epoch [4682/20000], Loss: 29.295761108398438, Learning Rate: 0.01\n",
      "Epoch [4683/20000], Loss: 29.28466796875, Learning Rate: 0.01\n",
      "Epoch [4684/20000], Loss: 29.27337646484375, Learning Rate: 0.01\n",
      "Epoch [4685/20000], Loss: 29.262420654296875, Learning Rate: 0.01\n",
      "Epoch [4686/20000], Loss: 29.251251220703125, Learning Rate: 0.01\n",
      "Epoch [4687/20000], Loss: 29.239898681640625, Learning Rate: 0.01\n",
      "Epoch [4688/20000], Loss: 29.228836059570312, Learning Rate: 0.01\n",
      "Epoch [4689/20000], Loss: 29.217727661132812, Learning Rate: 0.01\n",
      "Epoch [4690/20000], Loss: 29.206558227539062, Learning Rate: 0.01\n",
      "Epoch [4691/20000], Loss: 29.19525146484375, Learning Rate: 0.01\n",
      "Epoch [4692/20000], Loss: 29.184234619140625, Learning Rate: 0.01\n",
      "Epoch [4693/20000], Loss: 29.173126220703125, Learning Rate: 0.01\n",
      "Epoch [4694/20000], Loss: 29.162017822265625, Learning Rate: 0.01\n",
      "Epoch [4695/20000], Loss: 29.150741577148438, Learning Rate: 0.01\n",
      "Epoch [4696/20000], Loss: 29.139785766601562, Learning Rate: 0.01\n",
      "Epoch [4697/20000], Loss: 29.128570556640625, Learning Rate: 0.01\n",
      "Epoch [4698/20000], Loss: 29.11737060546875, Learning Rate: 0.01\n",
      "Epoch [4699/20000], Loss: 29.106185913085938, Learning Rate: 0.01\n",
      "Epoch [4700/20000], Loss: 29.09503173828125, Learning Rate: 0.01\n",
      "Epoch [4701/20000], Loss: 29.083969116210938, Learning Rate: 0.01\n",
      "Epoch [4702/20000], Loss: 29.072967529296875, Learning Rate: 0.01\n",
      "Epoch [4703/20000], Loss: 29.061737060546875, Learning Rate: 0.01\n",
      "Epoch [4704/20000], Loss: 29.050521850585938, Learning Rate: 0.01\n",
      "Epoch [4705/20000], Loss: 29.039535522460938, Learning Rate: 0.01\n",
      "Epoch [4706/20000], Loss: 29.028335571289062, Learning Rate: 0.01\n",
      "Epoch [4707/20000], Loss: 29.017288208007812, Learning Rate: 0.01\n",
      "Epoch [4708/20000], Loss: 29.006149291992188, Learning Rate: 0.01\n",
      "Epoch [4709/20000], Loss: 28.995010375976562, Learning Rate: 0.01\n",
      "Epoch [4710/20000], Loss: 28.98388671875, Learning Rate: 0.01\n",
      "Epoch [4711/20000], Loss: 28.972869873046875, Learning Rate: 0.01\n",
      "Epoch [4712/20000], Loss: 28.96197509765625, Learning Rate: 0.01\n",
      "Epoch [4713/20000], Loss: 28.9510498046875, Learning Rate: 0.01\n",
      "Epoch [4714/20000], Loss: 28.940185546875, Learning Rate: 0.01\n",
      "Epoch [4715/20000], Loss: 28.92950439453125, Learning Rate: 0.01\n",
      "Epoch [4716/20000], Loss: 28.9189453125, Learning Rate: 0.01\n",
      "Epoch [4717/20000], Loss: 28.9088134765625, Learning Rate: 0.01\n",
      "Epoch [4718/20000], Loss: 28.899276733398438, Learning Rate: 0.01\n",
      "Epoch [4719/20000], Loss: 28.890213012695312, Learning Rate: 0.01\n",
      "Epoch [4720/20000], Loss: 28.882476806640625, Learning Rate: 0.01\n",
      "Epoch [4721/20000], Loss: 28.876724243164062, Learning Rate: 0.01\n",
      "Epoch [4722/20000], Loss: 28.873504638671875, Learning Rate: 0.01\n",
      "Epoch [4723/20000], Loss: 28.875137329101562, Learning Rate: 0.01\n",
      "Epoch [4724/20000], Loss: 28.883804321289062, Learning Rate: 0.01\n",
      "Epoch [4725/20000], Loss: 28.903732299804688, Learning Rate: 0.01\n",
      "Epoch [4726/20000], Loss: 28.942031860351562, Learning Rate: 0.01\n",
      "Epoch [4727/20000], Loss: 29.00885009765625, Learning Rate: 0.01\n",
      "Epoch [4728/20000], Loss: 29.12158203125, Learning Rate: 0.01\n",
      "Epoch [4729/20000], Loss: 29.307037353515625, Learning Rate: 0.01\n",
      "Epoch [4730/20000], Loss: 29.606613159179688, Learning Rate: 0.01\n",
      "Epoch [4731/20000], Loss: 30.081649780273438, Learning Rate: 0.01\n",
      "Epoch [4732/20000], Loss: 30.816665649414062, Learning Rate: 0.01\n",
      "Epoch [4733/20000], Loss: 31.911102294921875, Learning Rate: 0.01\n",
      "Epoch [4734/20000], Loss: 33.44268798828125, Learning Rate: 0.01\n",
      "Epoch [4735/20000], Loss: 35.36601257324219, Learning Rate: 0.01\n",
      "Epoch [4736/20000], Loss: 37.35237121582031, Learning Rate: 0.01\n",
      "Epoch [4737/20000], Loss: 38.64595031738281, Learning Rate: 0.01\n",
      "Epoch [4738/20000], Loss: 38.28082275390625, Learning Rate: 0.01\n",
      "Epoch [4739/20000], Loss: 35.82698059082031, Learning Rate: 0.01\n",
      "Epoch [4740/20000], Loss: 32.213958740234375, Learning Rate: 0.01\n",
      "Epoch [4741/20000], Loss: 29.362442016601562, Learning Rate: 0.01\n",
      "Epoch [4742/20000], Loss: 28.682327270507812, Learning Rate: 0.01\n",
      "Epoch [4743/20000], Loss: 29.988601684570312, Learning Rate: 0.01\n",
      "Epoch [4744/20000], Loss: 31.837722778320312, Learning Rate: 0.01\n",
      "Epoch [4745/20000], Loss: 32.659759521484375, Learning Rate: 0.01\n",
      "Epoch [4746/20000], Loss: 31.8245849609375, Learning Rate: 0.01\n",
      "Epoch [4747/20000], Loss: 30.051925659179688, Learning Rate: 0.01\n",
      "Epoch [4748/20000], Loss: 28.73516845703125, Learning Rate: 0.01\n",
      "Epoch [4749/20000], Loss: 28.7027587890625, Learning Rate: 0.01\n",
      "Epoch [4750/20000], Loss: 29.611541748046875, Learning Rate: 0.01\n",
      "Epoch [4751/20000], Loss: 30.430908203125, Learning Rate: 0.01\n",
      "Epoch [4752/20000], Loss: 30.39434814453125, Learning Rate: 0.01\n",
      "Epoch [4753/20000], Loss: 29.582656860351562, Learning Rate: 0.01\n",
      "Epoch [4754/20000], Loss: 28.732955932617188, Learning Rate: 0.01\n",
      "Epoch [4755/20000], Loss: 28.496063232421875, Learning Rate: 0.01\n",
      "Epoch [4756/20000], Loss: 28.884002685546875, Learning Rate: 0.01\n",
      "Epoch [4757/20000], Loss: 29.377975463867188, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4758/20000], Loss: 29.470916748046875, Learning Rate: 0.01\n",
      "Epoch [4759/20000], Loss: 29.098419189453125, Learning Rate: 0.01\n",
      "Epoch [4760/20000], Loss: 28.616119384765625, Learning Rate: 0.01\n",
      "Epoch [4761/20000], Loss: 28.415435791015625, Learning Rate: 0.01\n",
      "Epoch [4762/20000], Loss: 28.5721435546875, Learning Rate: 0.01\n",
      "Epoch [4763/20000], Loss: 28.841796875, Learning Rate: 0.01\n",
      "Epoch [4764/20000], Loss: 28.931686401367188, Learning Rate: 0.01\n",
      "Epoch [4765/20000], Loss: 28.762893676757812, Learning Rate: 0.01\n",
      "Epoch [4766/20000], Loss: 28.497238159179688, Learning Rate: 0.01\n",
      "Epoch [4767/20000], Loss: 28.351760864257812, Learning Rate: 0.01\n",
      "Epoch [4768/20000], Loss: 28.401290893554688, Learning Rate: 0.01\n",
      "Epoch [4769/20000], Loss: 28.53961181640625, Learning Rate: 0.01\n",
      "Epoch [4770/20000], Loss: 28.607955932617188, Learning Rate: 0.01\n",
      "Epoch [4771/20000], Loss: 28.538238525390625, Learning Rate: 0.01\n",
      "Epoch [4772/20000], Loss: 28.394271850585938, Learning Rate: 0.01\n",
      "Epoch [4773/20000], Loss: 28.292098999023438, Learning Rate: 0.01\n",
      "Epoch [4774/20000], Loss: 28.290603637695312, Learning Rate: 0.01\n",
      "Epoch [4775/20000], Loss: 28.354034423828125, Learning Rate: 0.01\n",
      "Epoch [4776/20000], Loss: 28.4000244140625, Learning Rate: 0.01\n",
      "Epoch [4777/20000], Loss: 28.377761840820312, Learning Rate: 0.01\n",
      "Epoch [4778/20000], Loss: 28.302734375, Learning Rate: 0.01\n",
      "Epoch [4779/20000], Loss: 28.23248291015625, Learning Rate: 0.01\n",
      "Epoch [4780/20000], Loss: 28.209335327148438, Learning Rate: 0.01\n",
      "Epoch [4781/20000], Loss: 28.229934692382812, Learning Rate: 0.01\n",
      "Epoch [4782/20000], Loss: 28.2559814453125, Learning Rate: 0.01\n",
      "Epoch [4783/20000], Loss: 28.253173828125, Learning Rate: 0.01\n",
      "Epoch [4784/20000], Loss: 28.21722412109375, Learning Rate: 0.01\n",
      "Epoch [4785/20000], Loss: 28.171356201171875, Learning Rate: 0.01\n",
      "Epoch [4786/20000], Loss: 28.142486572265625, Learning Rate: 0.01\n",
      "Epoch [4787/20000], Loss: 28.13909912109375, Learning Rate: 0.01\n",
      "Epoch [4788/20000], Loss: 28.148406982421875, Learning Rate: 0.01\n",
      "Epoch [4789/20000], Loss: 28.150283813476562, Learning Rate: 0.01\n",
      "Epoch [4790/20000], Loss: 28.134368896484375, Learning Rate: 0.01\n",
      "Epoch [4791/20000], Loss: 28.106613159179688, Learning Rate: 0.01\n",
      "Epoch [4792/20000], Loss: 28.080718994140625, Learning Rate: 0.01\n",
      "Epoch [4793/20000], Loss: 28.066452026367188, Learning Rate: 0.01\n",
      "Epoch [4794/20000], Loss: 28.062896728515625, Learning Rate: 0.01\n",
      "Epoch [4795/20000], Loss: 28.061859130859375, Learning Rate: 0.01\n",
      "Epoch [4796/20000], Loss: 28.054183959960938, Learning Rate: 0.01\n",
      "Epoch [4797/20000], Loss: 28.03826904296875, Learning Rate: 0.01\n",
      "Epoch [4798/20000], Loss: 28.01861572265625, Learning Rate: 0.01\n",
      "Epoch [4799/20000], Loss: 28.001480102539062, Learning Rate: 0.01\n",
      "Epoch [4800/20000], Loss: 27.99053955078125, Learning Rate: 0.01\n",
      "Epoch [4801/20000], Loss: 27.984222412109375, Learning Rate: 0.01\n",
      "Epoch [4802/20000], Loss: 27.977706909179688, Learning Rate: 0.01\n",
      "Epoch [4803/20000], Loss: 27.967620849609375, Learning Rate: 0.01\n",
      "Epoch [4804/20000], Loss: 27.953750610351562, Learning Rate: 0.01\n",
      "Epoch [4805/20000], Loss: 27.938568115234375, Learning Rate: 0.01\n",
      "Epoch [4806/20000], Loss: 27.925186157226562, Learning Rate: 0.01\n",
      "Epoch [4807/20000], Loss: 27.914474487304688, Learning Rate: 0.01\n",
      "Epoch [4808/20000], Loss: 27.90576171875, Learning Rate: 0.01\n",
      "Epoch [4809/20000], Loss: 27.896896362304688, Learning Rate: 0.01\n",
      "Epoch [4810/20000], Loss: 27.886383056640625, Learning Rate: 0.01\n",
      "Epoch [4811/20000], Loss: 27.87408447265625, Learning Rate: 0.01\n",
      "Epoch [4812/20000], Loss: 27.861404418945312, Learning Rate: 0.01\n",
      "Epoch [4813/20000], Loss: 27.84906005859375, Learning Rate: 0.01\n",
      "Epoch [4814/20000], Loss: 27.838226318359375, Learning Rate: 0.01\n",
      "Epoch [4815/20000], Loss: 27.828216552734375, Learning Rate: 0.01\n",
      "Epoch [4816/20000], Loss: 27.818344116210938, Learning Rate: 0.01\n",
      "Epoch [4817/20000], Loss: 27.808029174804688, Learning Rate: 0.01\n",
      "Epoch [4818/20000], Loss: 27.796371459960938, Learning Rate: 0.01\n",
      "Epoch [4819/20000], Loss: 27.784835815429688, Learning Rate: 0.01\n",
      "Epoch [4820/20000], Loss: 27.773147583007812, Learning Rate: 0.01\n",
      "Epoch [4821/20000], Loss: 27.762222290039062, Learning Rate: 0.01\n",
      "Epoch [4822/20000], Loss: 27.75164794921875, Learning Rate: 0.01\n",
      "Epoch [4823/20000], Loss: 27.741378784179688, Learning Rate: 0.01\n",
      "Epoch [4824/20000], Loss: 27.730804443359375, Learning Rate: 0.01\n",
      "Epoch [4825/20000], Loss: 27.719863891601562, Learning Rate: 0.01\n",
      "Epoch [4826/20000], Loss: 27.708587646484375, Learning Rate: 0.01\n",
      "Epoch [4827/20000], Loss: 27.697219848632812, Learning Rate: 0.01\n",
      "Epoch [4828/20000], Loss: 27.686264038085938, Learning Rate: 0.01\n",
      "Epoch [4829/20000], Loss: 27.67547607421875, Learning Rate: 0.01\n",
      "Epoch [4830/20000], Loss: 27.66473388671875, Learning Rate: 0.01\n",
      "Epoch [4831/20000], Loss: 27.654144287109375, Learning Rate: 0.01\n",
      "Epoch [4832/20000], Loss: 27.64349365234375, Learning Rate: 0.01\n",
      "Epoch [4833/20000], Loss: 27.63250732421875, Learning Rate: 0.01\n",
      "Epoch [4834/20000], Loss: 27.621475219726562, Learning Rate: 0.01\n",
      "Epoch [4835/20000], Loss: 27.610427856445312, Learning Rate: 0.01\n",
      "Epoch [4836/20000], Loss: 27.599533081054688, Learning Rate: 0.01\n",
      "Epoch [4837/20000], Loss: 27.58868408203125, Learning Rate: 0.01\n",
      "Epoch [4838/20000], Loss: 27.57806396484375, Learning Rate: 0.01\n",
      "Epoch [4839/20000], Loss: 27.567230224609375, Learning Rate: 0.01\n",
      "Epoch [4840/20000], Loss: 27.556365966796875, Learning Rate: 0.01\n",
      "Epoch [4841/20000], Loss: 27.545486450195312, Learning Rate: 0.01\n",
      "Epoch [4842/20000], Loss: 27.534713745117188, Learning Rate: 0.01\n",
      "Epoch [4843/20000], Loss: 27.523681640625, Learning Rate: 0.01\n",
      "Epoch [4844/20000], Loss: 27.512969970703125, Learning Rate: 0.01\n",
      "Epoch [4845/20000], Loss: 27.502059936523438, Learning Rate: 0.01\n",
      "Epoch [4846/20000], Loss: 27.491363525390625, Learning Rate: 0.01\n",
      "Epoch [4847/20000], Loss: 27.48046875, Learning Rate: 0.01\n",
      "Epoch [4848/20000], Loss: 27.469512939453125, Learning Rate: 0.01\n",
      "Epoch [4849/20000], Loss: 27.458908081054688, Learning Rate: 0.01\n",
      "Epoch [4850/20000], Loss: 27.447952270507812, Learning Rate: 0.01\n",
      "Epoch [4851/20000], Loss: 27.437225341796875, Learning Rate: 0.01\n",
      "Epoch [4852/20000], Loss: 27.426239013671875, Learning Rate: 0.01\n",
      "Epoch [4853/20000], Loss: 27.415496826171875, Learning Rate: 0.01\n",
      "Epoch [4854/20000], Loss: 27.404815673828125, Learning Rate: 0.01\n",
      "Epoch [4855/20000], Loss: 27.393722534179688, Learning Rate: 0.01\n",
      "Epoch [4856/20000], Loss: 27.38287353515625, Learning Rate: 0.01\n",
      "Epoch [4857/20000], Loss: 27.372085571289062, Learning Rate: 0.01\n",
      "Epoch [4858/20000], Loss: 27.361129760742188, Learning Rate: 0.01\n",
      "Epoch [4859/20000], Loss: 27.350448608398438, Learning Rate: 0.01\n",
      "Epoch [4860/20000], Loss: 27.339706420898438, Learning Rate: 0.01\n",
      "Epoch [4861/20000], Loss: 27.328765869140625, Learning Rate: 0.01\n",
      "Epoch [4862/20000], Loss: 27.317962646484375, Learning Rate: 0.01\n",
      "Epoch [4863/20000], Loss: 27.30718994140625, Learning Rate: 0.01\n",
      "Epoch [4864/20000], Loss: 27.296401977539062, Learning Rate: 0.01\n",
      "Epoch [4865/20000], Loss: 27.285629272460938, Learning Rate: 0.01\n",
      "Epoch [4866/20000], Loss: 27.274627685546875, Learning Rate: 0.01\n",
      "Epoch [4867/20000], Loss: 27.264129638671875, Learning Rate: 0.01\n",
      "Epoch [4868/20000], Loss: 27.253173828125, Learning Rate: 0.01\n",
      "Epoch [4869/20000], Loss: 27.242111206054688, Learning Rate: 0.01\n",
      "Epoch [4870/20000], Loss: 27.231430053710938, Learning Rate: 0.01\n",
      "Epoch [4871/20000], Loss: 27.220733642578125, Learning Rate: 0.01\n",
      "Epoch [4872/20000], Loss: 27.209945678710938, Learning Rate: 0.01\n",
      "Epoch [4873/20000], Loss: 27.19903564453125, Learning Rate: 0.01\n",
      "Epoch [4874/20000], Loss: 27.188262939453125, Learning Rate: 0.01\n",
      "Epoch [4875/20000], Loss: 27.177520751953125, Learning Rate: 0.01\n",
      "Epoch [4876/20000], Loss: 27.166671752929688, Learning Rate: 0.01\n",
      "Epoch [4877/20000], Loss: 27.155899047851562, Learning Rate: 0.01\n",
      "Epoch [4878/20000], Loss: 27.14508056640625, Learning Rate: 0.01\n",
      "Epoch [4879/20000], Loss: 27.134185791015625, Learning Rate: 0.01\n",
      "Epoch [4880/20000], Loss: 27.12347412109375, Learning Rate: 0.01\n",
      "Epoch [4881/20000], Loss: 27.112533569335938, Learning Rate: 0.01\n",
      "Epoch [4882/20000], Loss: 27.101715087890625, Learning Rate: 0.01\n",
      "Epoch [4883/20000], Loss: 27.090972900390625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4884/20000], Loss: 27.0802001953125, Learning Rate: 0.01\n",
      "Epoch [4885/20000], Loss: 27.069198608398438, Learning Rate: 0.01\n",
      "Epoch [4886/20000], Loss: 27.058441162109375, Learning Rate: 0.01\n",
      "Epoch [4887/20000], Loss: 27.047653198242188, Learning Rate: 0.01\n",
      "Epoch [4888/20000], Loss: 27.03692626953125, Learning Rate: 0.01\n",
      "Epoch [4889/20000], Loss: 27.02593994140625, Learning Rate: 0.01\n",
      "Epoch [4890/20000], Loss: 27.015365600585938, Learning Rate: 0.01\n",
      "Epoch [4891/20000], Loss: 27.004425048828125, Learning Rate: 0.01\n",
      "Epoch [4892/20000], Loss: 26.9937744140625, Learning Rate: 0.01\n",
      "Epoch [4893/20000], Loss: 26.982650756835938, Learning Rate: 0.01\n",
      "Epoch [4894/20000], Loss: 26.97198486328125, Learning Rate: 0.01\n",
      "Epoch [4895/20000], Loss: 26.96124267578125, Learning Rate: 0.01\n",
      "Epoch [4896/20000], Loss: 26.950393676757812, Learning Rate: 0.01\n",
      "Epoch [4897/20000], Loss: 26.939666748046875, Learning Rate: 0.01\n",
      "Epoch [4898/20000], Loss: 26.92889404296875, Learning Rate: 0.01\n",
      "Epoch [4899/20000], Loss: 26.918121337890625, Learning Rate: 0.01\n",
      "Epoch [4900/20000], Loss: 26.907257080078125, Learning Rate: 0.01\n",
      "Epoch [4901/20000], Loss: 26.896408081054688, Learning Rate: 0.01\n",
      "Epoch [4902/20000], Loss: 26.8856201171875, Learning Rate: 0.01\n",
      "Epoch [4903/20000], Loss: 26.874908447265625, Learning Rate: 0.01\n",
      "Epoch [4904/20000], Loss: 26.864273071289062, Learning Rate: 0.01\n",
      "Epoch [4905/20000], Loss: 26.85321044921875, Learning Rate: 0.01\n",
      "Epoch [4906/20000], Loss: 26.842514038085938, Learning Rate: 0.01\n",
      "Epoch [4907/20000], Loss: 26.831634521484375, Learning Rate: 0.01\n",
      "Epoch [4908/20000], Loss: 26.820892333984375, Learning Rate: 0.01\n",
      "Epoch [4909/20000], Loss: 26.810089111328125, Learning Rate: 0.01\n",
      "Epoch [4910/20000], Loss: 26.799407958984375, Learning Rate: 0.01\n",
      "Epoch [4911/20000], Loss: 26.788360595703125, Learning Rate: 0.01\n",
      "Epoch [4912/20000], Loss: 26.777801513671875, Learning Rate: 0.01\n",
      "Epoch [4913/20000], Loss: 26.766952514648438, Learning Rate: 0.01\n",
      "Epoch [4914/20000], Loss: 26.756088256835938, Learning Rate: 0.01\n",
      "Epoch [4915/20000], Loss: 26.74530029296875, Learning Rate: 0.01\n",
      "Epoch [4916/20000], Loss: 26.734481811523438, Learning Rate: 0.01\n",
      "Epoch [4917/20000], Loss: 26.723678588867188, Learning Rate: 0.01\n",
      "Epoch [4918/20000], Loss: 26.713088989257812, Learning Rate: 0.01\n",
      "Epoch [4919/20000], Loss: 26.702224731445312, Learning Rate: 0.01\n",
      "Epoch [4920/20000], Loss: 26.691497802734375, Learning Rate: 0.01\n",
      "Epoch [4921/20000], Loss: 26.6805419921875, Learning Rate: 0.01\n",
      "Epoch [4922/20000], Loss: 26.669845581054688, Learning Rate: 0.01\n",
      "Epoch [4923/20000], Loss: 26.658981323242188, Learning Rate: 0.01\n",
      "Epoch [4924/20000], Loss: 26.648284912109375, Learning Rate: 0.01\n",
      "Epoch [4925/20000], Loss: 26.637451171875, Learning Rate: 0.01\n",
      "Epoch [4926/20000], Loss: 26.626632690429688, Learning Rate: 0.01\n",
      "Epoch [4927/20000], Loss: 26.615921020507812, Learning Rate: 0.01\n",
      "Epoch [4928/20000], Loss: 26.605056762695312, Learning Rate: 0.01\n",
      "Epoch [4929/20000], Loss: 26.594314575195312, Learning Rate: 0.01\n",
      "Epoch [4930/20000], Loss: 26.583297729492188, Learning Rate: 0.01\n",
      "Epoch [4931/20000], Loss: 26.57257080078125, Learning Rate: 0.01\n",
      "Epoch [4932/20000], Loss: 26.5618896484375, Learning Rate: 0.01\n",
      "Epoch [4933/20000], Loss: 26.551162719726562, Learning Rate: 0.01\n",
      "Epoch [4934/20000], Loss: 26.540359497070312, Learning Rate: 0.01\n",
      "Epoch [4935/20000], Loss: 26.529556274414062, Learning Rate: 0.01\n",
      "Epoch [4936/20000], Loss: 26.518966674804688, Learning Rate: 0.01\n",
      "Epoch [4937/20000], Loss: 26.508041381835938, Learning Rate: 0.01\n",
      "Epoch [4938/20000], Loss: 26.497299194335938, Learning Rate: 0.01\n",
      "Epoch [4939/20000], Loss: 26.4864501953125, Learning Rate: 0.01\n",
      "Epoch [4940/20000], Loss: 26.475677490234375, Learning Rate: 0.01\n",
      "Epoch [4941/20000], Loss: 26.464767456054688, Learning Rate: 0.01\n",
      "Epoch [4942/20000], Loss: 26.454071044921875, Learning Rate: 0.01\n",
      "Epoch [4943/20000], Loss: 26.443252563476562, Learning Rate: 0.01\n",
      "Epoch [4944/20000], Loss: 26.432525634765625, Learning Rate: 0.01\n",
      "Epoch [4945/20000], Loss: 26.421722412109375, Learning Rate: 0.01\n",
      "Epoch [4946/20000], Loss: 26.410888671875, Learning Rate: 0.01\n",
      "Epoch [4947/20000], Loss: 26.400299072265625, Learning Rate: 0.01\n",
      "Epoch [4948/20000], Loss: 26.38946533203125, Learning Rate: 0.01\n",
      "Epoch [4949/20000], Loss: 26.378692626953125, Learning Rate: 0.01\n",
      "Epoch [4950/20000], Loss: 26.367965698242188, Learning Rate: 0.01\n",
      "Epoch [4951/20000], Loss: 26.357315063476562, Learning Rate: 0.01\n",
      "Epoch [4952/20000], Loss: 26.346221923828125, Learning Rate: 0.01\n",
      "Epoch [4953/20000], Loss: 26.335678100585938, Learning Rate: 0.01\n",
      "Epoch [4954/20000], Loss: 26.324737548828125, Learning Rate: 0.01\n",
      "Epoch [4955/20000], Loss: 26.314041137695312, Learning Rate: 0.01\n",
      "Epoch [4956/20000], Loss: 26.303298950195312, Learning Rate: 0.01\n",
      "Epoch [4957/20000], Loss: 26.29248046875, Learning Rate: 0.01\n",
      "Epoch [4958/20000], Loss: 26.28179931640625, Learning Rate: 0.01\n",
      "Epoch [4959/20000], Loss: 26.271072387695312, Learning Rate: 0.01\n",
      "Epoch [4960/20000], Loss: 26.260269165039062, Learning Rate: 0.01\n",
      "Epoch [4961/20000], Loss: 26.249496459960938, Learning Rate: 0.01\n",
      "Epoch [4962/20000], Loss: 26.238601684570312, Learning Rate: 0.01\n",
      "Epoch [4963/20000], Loss: 26.22796630859375, Learning Rate: 0.01\n",
      "Epoch [4964/20000], Loss: 26.216995239257812, Learning Rate: 0.01\n",
      "Epoch [4965/20000], Loss: 26.206405639648438, Learning Rate: 0.01\n",
      "Epoch [4966/20000], Loss: 26.195571899414062, Learning Rate: 0.01\n",
      "Epoch [4967/20000], Loss: 26.184967041015625, Learning Rate: 0.01\n",
      "Epoch [4968/20000], Loss: 26.17413330078125, Learning Rate: 0.01\n",
      "Epoch [4969/20000], Loss: 26.163406372070312, Learning Rate: 0.01\n",
      "Epoch [4970/20000], Loss: 26.1527099609375, Learning Rate: 0.01\n",
      "Epoch [4971/20000], Loss: 26.141998291015625, Learning Rate: 0.01\n",
      "Epoch [4972/20000], Loss: 26.131484985351562, Learning Rate: 0.01\n",
      "Epoch [4973/20000], Loss: 26.12091064453125, Learning Rate: 0.01\n",
      "Epoch [4974/20000], Loss: 26.110458374023438, Learning Rate: 0.01\n",
      "Epoch [4975/20000], Loss: 26.099929809570312, Learning Rate: 0.01\n",
      "Epoch [4976/20000], Loss: 26.089950561523438, Learning Rate: 0.01\n",
      "Epoch [4977/20000], Loss: 26.080276489257812, Learning Rate: 0.01\n",
      "Epoch [4978/20000], Loss: 26.071365356445312, Learning Rate: 0.01\n",
      "Epoch [4979/20000], Loss: 26.06378173828125, Learning Rate: 0.01\n",
      "Epoch [4980/20000], Loss: 26.057769775390625, Learning Rate: 0.01\n",
      "Epoch [4981/20000], Loss: 26.054641723632812, Learning Rate: 0.01\n",
      "Epoch [4982/20000], Loss: 26.056381225585938, Learning Rate: 0.01\n",
      "Epoch [4983/20000], Loss: 26.065963745117188, Learning Rate: 0.01\n",
      "Epoch [4984/20000], Loss: 26.088836669921875, Learning Rate: 0.01\n",
      "Epoch [4985/20000], Loss: 26.1334228515625, Learning Rate: 0.01\n",
      "Epoch [4986/20000], Loss: 26.214248657226562, Learning Rate: 0.01\n",
      "Epoch [4987/20000], Loss: 26.35528564453125, Learning Rate: 0.01\n",
      "Epoch [4988/20000], Loss: 26.596450805664062, Learning Rate: 0.01\n",
      "Epoch [4989/20000], Loss: 27.0013427734375, Learning Rate: 0.01\n",
      "Epoch [4990/20000], Loss: 27.670089721679688, Learning Rate: 0.01\n",
      "Epoch [4991/20000], Loss: 28.74639892578125, Learning Rate: 0.01\n",
      "Epoch [4992/20000], Loss: 30.410797119140625, Learning Rate: 0.01\n",
      "Epoch [4993/20000], Loss: 32.807952880859375, Learning Rate: 0.01\n",
      "Epoch [4994/20000], Loss: 35.8531494140625, Learning Rate: 0.01\n",
      "Epoch [4995/20000], Loss: 38.874664306640625, Learning Rate: 0.01\n",
      "Epoch [4996/20000], Loss: 40.40751647949219, Learning Rate: 0.01\n",
      "Epoch [4997/20000], Loss: 38.787017822265625, Learning Rate: 0.01\n",
      "Epoch [4998/20000], Loss: 33.963958740234375, Learning Rate: 0.01\n",
      "Epoch [4999/20000], Loss: 28.529830932617188, Learning Rate: 0.01\n",
      "Epoch [5000/20000], Loss: 25.874679565429688, Learning Rate: 0.01\n",
      "Epoch [5001/20000], Loss: 27.052139282226562, Learning Rate: 0.01\n",
      "Epoch [5002/20000], Loss: 30.0926513671875, Learning Rate: 0.01\n",
      "Epoch [5003/20000], Loss: 31.931198120117188, Learning Rate: 0.01\n",
      "Epoch [5004/20000], Loss: 30.887847900390625, Learning Rate: 0.01\n",
      "Epoch [5005/20000], Loss: 28.009841918945312, Learning Rate: 0.01\n",
      "Epoch [5006/20000], Loss: 25.93658447265625, Learning Rate: 0.01\n",
      "Epoch [5007/20000], Loss: 26.19183349609375, Learning Rate: 0.01\n",
      "Epoch [5008/20000], Loss: 27.875411987304688, Learning Rate: 0.01\n",
      "Epoch [5009/20000], Loss: 28.924575805664062, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5010/20000], Loss: 28.25006103515625, Learning Rate: 0.01\n",
      "Epoch [5011/20000], Loss: 26.642486572265625, Learning Rate: 0.01\n",
      "Epoch [5012/20000], Loss: 25.714157104492188, Learning Rate: 0.01\n",
      "Epoch [5013/20000], Loss: 26.124893188476562, Learning Rate: 0.01\n",
      "Epoch [5014/20000], Loss: 27.080368041992188, Learning Rate: 0.01\n",
      "Epoch [5015/20000], Loss: 27.3974609375, Learning Rate: 0.01\n",
      "Epoch [5016/20000], Loss: 26.761932373046875, Learning Rate: 0.01\n",
      "Epoch [5017/20000], Loss: 25.90093994140625, Learning Rate: 0.01\n",
      "Epoch [5018/20000], Loss: 25.65093994140625, Learning Rate: 0.01\n",
      "Epoch [5019/20000], Loss: 26.071517944335938, Learning Rate: 0.01\n",
      "Epoch [5020/20000], Loss: 26.529342651367188, Learning Rate: 0.01\n",
      "Epoch [5021/20000], Loss: 26.477981567382812, Learning Rate: 0.01\n",
      "Epoch [5022/20000], Loss: 26.002700805664062, Learning Rate: 0.01\n",
      "Epoch [5023/20000], Loss: 25.615509033203125, Learning Rate: 0.01\n",
      "Epoch [5024/20000], Loss: 25.646087646484375, Learning Rate: 0.01\n",
      "Epoch [5025/20000], Loss: 25.935104370117188, Learning Rate: 0.01\n",
      "Epoch [5026/20000], Loss: 26.092208862304688, Learning Rate: 0.01\n",
      "Epoch [5027/20000], Loss: 25.939956665039062, Learning Rate: 0.01\n",
      "Epoch [5028/20000], Loss: 25.65411376953125, Learning Rate: 0.01\n",
      "Epoch [5029/20000], Loss: 25.516845703125, Learning Rate: 0.01\n",
      "Epoch [5030/20000], Loss: 25.607086181640625, Learning Rate: 0.01\n",
      "Epoch [5031/20000], Loss: 25.759140014648438, Learning Rate: 0.01\n",
      "Epoch [5032/20000], Loss: 25.778656005859375, Learning Rate: 0.01\n",
      "Epoch [5033/20000], Loss: 25.644195556640625, Learning Rate: 0.01\n",
      "Epoch [5034/20000], Loss: 25.495529174804688, Learning Rate: 0.01\n",
      "Epoch [5035/20000], Loss: 25.461029052734375, Learning Rate: 0.01\n",
      "Epoch [5036/20000], Loss: 25.531387329101562, Learning Rate: 0.01\n",
      "Epoch [5037/20000], Loss: 25.595596313476562, Learning Rate: 0.01\n",
      "Epoch [5038/20000], Loss: 25.57122802734375, Learning Rate: 0.01\n",
      "Epoch [5039/20000], Loss: 25.480239868164062, Learning Rate: 0.01\n",
      "Epoch [5040/20000], Loss: 25.407241821289062, Learning Rate: 0.01\n",
      "Epoch [5041/20000], Loss: 25.403182983398438, Learning Rate: 0.01\n",
      "Epoch [5042/20000], Loss: 25.442596435546875, Learning Rate: 0.01\n",
      "Epoch [5043/20000], Loss: 25.463119506835938, Learning Rate: 0.01\n",
      "Epoch [5044/20000], Loss: 25.433334350585938, Learning Rate: 0.01\n",
      "Epoch [5045/20000], Loss: 25.376861572265625, Learning Rate: 0.01\n",
      "Epoch [5046/20000], Loss: 25.339096069335938, Learning Rate: 0.01\n",
      "Epoch [5047/20000], Loss: 25.338882446289062, Learning Rate: 0.01\n",
      "Epoch [5048/20000], Loss: 25.356231689453125, Learning Rate: 0.01\n",
      "Epoch [5049/20000], Loss: 25.35845947265625, Learning Rate: 0.01\n",
      "Epoch [5050/20000], Loss: 25.333755493164062, Learning Rate: 0.01\n",
      "Epoch [5051/20000], Loss: 25.298736572265625, Learning Rate: 0.01\n",
      "Epoch [5052/20000], Loss: 25.275802612304688, Learning Rate: 0.01\n",
      "Epoch [5053/20000], Loss: 25.272476196289062, Learning Rate: 0.01\n",
      "Epoch [5054/20000], Loss: 25.276809692382812, Learning Rate: 0.01\n",
      "Epoch [5055/20000], Loss: 25.272140502929688, Learning Rate: 0.01\n",
      "Epoch [5056/20000], Loss: 25.253341674804688, Learning Rate: 0.01\n",
      "Epoch [5057/20000], Loss: 25.229827880859375, Learning Rate: 0.01\n",
      "Epoch [5058/20000], Loss: 25.212860107421875, Learning Rate: 0.01\n",
      "Epoch [5059/20000], Loss: 25.206375122070312, Learning Rate: 0.01\n",
      "Epoch [5060/20000], Loss: 25.203872680664062, Learning Rate: 0.01\n",
      "Epoch [5061/20000], Loss: 25.196563720703125, Learning Rate: 0.01\n",
      "Epoch [5062/20000], Loss: 25.18206787109375, Learning Rate: 0.01\n",
      "Epoch [5063/20000], Loss: 25.164627075195312, Learning Rate: 0.01\n",
      "Epoch [5064/20000], Loss: 25.150283813476562, Learning Rate: 0.01\n",
      "Epoch [5065/20000], Loss: 25.141433715820312, Learning Rate: 0.01\n",
      "Epoch [5066/20000], Loss: 25.135162353515625, Learning Rate: 0.01\n",
      "Epoch [5067/20000], Loss: 25.127029418945312, Learning Rate: 0.01\n",
      "Epoch [5068/20000], Loss: 25.114700317382812, Learning Rate: 0.01\n",
      "Epoch [5069/20000], Loss: 25.100540161132812, Learning Rate: 0.01\n",
      "Epoch [5070/20000], Loss: 25.087799072265625, Learning Rate: 0.01\n",
      "Epoch [5071/20000], Loss: 25.077545166015625, Learning Rate: 0.01\n",
      "Epoch [5072/20000], Loss: 25.0689697265625, Learning Rate: 0.01\n",
      "Epoch [5073/20000], Loss: 25.060287475585938, Learning Rate: 0.01\n",
      "Epoch [5074/20000], Loss: 25.049484252929688, Learning Rate: 0.01\n",
      "Epoch [5075/20000], Loss: 25.037277221679688, Learning Rate: 0.01\n",
      "Epoch [5076/20000], Loss: 25.02508544921875, Learning Rate: 0.01\n",
      "Epoch [5077/20000], Loss: 25.014328002929688, Learning Rate: 0.01\n",
      "Epoch [5078/20000], Loss: 25.004806518554688, Learning Rate: 0.01\n",
      "Epoch [5079/20000], Loss: 24.995269775390625, Learning Rate: 0.01\n",
      "Epoch [5080/20000], Loss: 24.985061645507812, Learning Rate: 0.01\n",
      "Epoch [5081/20000], Loss: 24.973846435546875, Learning Rate: 0.01\n",
      "Epoch [5082/20000], Loss: 24.962631225585938, Learning Rate: 0.01\n",
      "Epoch [5083/20000], Loss: 24.951614379882812, Learning Rate: 0.01\n",
      "Epoch [5084/20000], Loss: 24.941375732421875, Learning Rate: 0.01\n",
      "Epoch [5085/20000], Loss: 24.931411743164062, Learning Rate: 0.01\n",
      "Epoch [5086/20000], Loss: 24.921539306640625, Learning Rate: 0.01\n",
      "Epoch [5087/20000], Loss: 24.91082763671875, Learning Rate: 0.01\n",
      "Epoch [5088/20000], Loss: 24.89984130859375, Learning Rate: 0.01\n",
      "Epoch [5089/20000], Loss: 24.88916015625, Learning Rate: 0.01\n",
      "Epoch [5090/20000], Loss: 24.878433227539062, Learning Rate: 0.01\n",
      "Epoch [5091/20000], Loss: 24.868255615234375, Learning Rate: 0.01\n",
      "Epoch [5092/20000], Loss: 24.858062744140625, Learning Rate: 0.01\n",
      "Epoch [5093/20000], Loss: 24.847686767578125, Learning Rate: 0.01\n",
      "Epoch [5094/20000], Loss: 24.837203979492188, Learning Rate: 0.01\n",
      "Epoch [5095/20000], Loss: 24.82666015625, Learning Rate: 0.01\n",
      "Epoch [5096/20000], Loss: 24.81597900390625, Learning Rate: 0.01\n",
      "Epoch [5097/20000], Loss: 24.805419921875, Learning Rate: 0.01\n",
      "Epoch [5098/20000], Loss: 24.795074462890625, Learning Rate: 0.01\n",
      "Epoch [5099/20000], Loss: 24.784744262695312, Learning Rate: 0.01\n",
      "Epoch [5100/20000], Loss: 24.774398803710938, Learning Rate: 0.01\n",
      "Epoch [5101/20000], Loss: 24.763885498046875, Learning Rate: 0.01\n",
      "Epoch [5102/20000], Loss: 24.753265380859375, Learning Rate: 0.01\n",
      "Epoch [5103/20000], Loss: 24.742843627929688, Learning Rate: 0.01\n",
      "Epoch [5104/20000], Loss: 24.732391357421875, Learning Rate: 0.01\n",
      "Epoch [5105/20000], Loss: 24.722137451171875, Learning Rate: 0.01\n",
      "Epoch [5106/20000], Loss: 24.71173095703125, Learning Rate: 0.01\n",
      "Epoch [5107/20000], Loss: 24.701217651367188, Learning Rate: 0.01\n",
      "Epoch [5108/20000], Loss: 24.690780639648438, Learning Rate: 0.01\n",
      "Epoch [5109/20000], Loss: 24.680374145507812, Learning Rate: 0.01\n",
      "Epoch [5110/20000], Loss: 24.669708251953125, Learning Rate: 0.01\n",
      "Epoch [5111/20000], Loss: 24.65924072265625, Learning Rate: 0.01\n",
      "Epoch [5112/20000], Loss: 24.648941040039062, Learning Rate: 0.01\n",
      "Epoch [5113/20000], Loss: 24.638442993164062, Learning Rate: 0.01\n",
      "Epoch [5114/20000], Loss: 24.62811279296875, Learning Rate: 0.01\n",
      "Epoch [5115/20000], Loss: 24.617645263671875, Learning Rate: 0.01\n",
      "Epoch [5116/20000], Loss: 24.60723876953125, Learning Rate: 0.01\n",
      "Epoch [5117/20000], Loss: 24.596832275390625, Learning Rate: 0.01\n",
      "Epoch [5118/20000], Loss: 24.586257934570312, Learning Rate: 0.01\n",
      "Epoch [5119/20000], Loss: 24.57574462890625, Learning Rate: 0.01\n",
      "Epoch [5120/20000], Loss: 24.5654296875, Learning Rate: 0.01\n",
      "Epoch [5121/20000], Loss: 24.554962158203125, Learning Rate: 0.01\n",
      "Epoch [5122/20000], Loss: 24.544540405273438, Learning Rate: 0.01\n",
      "Epoch [5123/20000], Loss: 24.534133911132812, Learning Rate: 0.01\n",
      "Epoch [5124/20000], Loss: 24.523727416992188, Learning Rate: 0.01\n",
      "Epoch [5125/20000], Loss: 24.513107299804688, Learning Rate: 0.01\n",
      "Epoch [5126/20000], Loss: 24.502716064453125, Learning Rate: 0.01\n",
      "Epoch [5127/20000], Loss: 24.492385864257812, Learning Rate: 0.01\n",
      "Epoch [5128/20000], Loss: 24.481964111328125, Learning Rate: 0.01\n",
      "Epoch [5129/20000], Loss: 24.471466064453125, Learning Rate: 0.01\n",
      "Epoch [5130/20000], Loss: 24.46112060546875, Learning Rate: 0.01\n",
      "Epoch [5131/20000], Loss: 24.45068359375, Learning Rate: 0.01\n",
      "Epoch [5132/20000], Loss: 24.4403076171875, Learning Rate: 0.01\n",
      "Epoch [5133/20000], Loss: 24.429840087890625, Learning Rate: 0.01\n",
      "Epoch [5134/20000], Loss: 24.419418334960938, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5135/20000], Loss: 24.408950805664062, Learning Rate: 0.01\n",
      "Epoch [5136/20000], Loss: 24.398468017578125, Learning Rate: 0.01\n",
      "Epoch [5137/20000], Loss: 24.388153076171875, Learning Rate: 0.01\n",
      "Epoch [5138/20000], Loss: 24.37762451171875, Learning Rate: 0.01\n",
      "Epoch [5139/20000], Loss: 24.367202758789062, Learning Rate: 0.01\n",
      "Epoch [5140/20000], Loss: 24.356735229492188, Learning Rate: 0.01\n",
      "Epoch [5141/20000], Loss: 24.346450805664062, Learning Rate: 0.01\n",
      "Epoch [5142/20000], Loss: 24.335861206054688, Learning Rate: 0.01\n",
      "Epoch [5143/20000], Loss: 24.325546264648438, Learning Rate: 0.01\n",
      "Epoch [5144/20000], Loss: 24.315017700195312, Learning Rate: 0.01\n",
      "Epoch [5145/20000], Loss: 24.30450439453125, Learning Rate: 0.01\n",
      "Epoch [5146/20000], Loss: 24.294189453125, Learning Rate: 0.01\n",
      "Epoch [5147/20000], Loss: 24.283889770507812, Learning Rate: 0.01\n",
      "Epoch [5148/20000], Loss: 24.273452758789062, Learning Rate: 0.01\n",
      "Epoch [5149/20000], Loss: 24.262832641601562, Learning Rate: 0.01\n",
      "Epoch [5150/20000], Loss: 24.252410888671875, Learning Rate: 0.01\n",
      "Epoch [5151/20000], Loss: 24.242034912109375, Learning Rate: 0.01\n",
      "Epoch [5152/20000], Loss: 24.231689453125, Learning Rate: 0.01\n",
      "Epoch [5153/20000], Loss: 24.221160888671875, Learning Rate: 0.01\n",
      "Epoch [5154/20000], Loss: 24.210739135742188, Learning Rate: 0.01\n",
      "Epoch [5155/20000], Loss: 24.200469970703125, Learning Rate: 0.01\n",
      "Epoch [5156/20000], Loss: 24.189956665039062, Learning Rate: 0.01\n",
      "Epoch [5157/20000], Loss: 24.17962646484375, Learning Rate: 0.01\n",
      "Epoch [5158/20000], Loss: 24.168991088867188, Learning Rate: 0.01\n",
      "Epoch [5159/20000], Loss: 24.15875244140625, Learning Rate: 0.01\n",
      "Epoch [5160/20000], Loss: 24.1483154296875, Learning Rate: 0.01\n",
      "Epoch [5161/20000], Loss: 24.13775634765625, Learning Rate: 0.01\n",
      "Epoch [5162/20000], Loss: 24.127487182617188, Learning Rate: 0.01\n",
      "Epoch [5163/20000], Loss: 24.117141723632812, Learning Rate: 0.01\n",
      "Epoch [5164/20000], Loss: 24.106491088867188, Learning Rate: 0.01\n",
      "Epoch [5165/20000], Loss: 24.096054077148438, Learning Rate: 0.01\n",
      "Epoch [5166/20000], Loss: 24.085708618164062, Learning Rate: 0.01\n",
      "Epoch [5167/20000], Loss: 24.075225830078125, Learning Rate: 0.01\n",
      "Epoch [5168/20000], Loss: 24.064788818359375, Learning Rate: 0.01\n",
      "Epoch [5169/20000], Loss: 24.054550170898438, Learning Rate: 0.01\n",
      "Epoch [5170/20000], Loss: 24.043930053710938, Learning Rate: 0.01\n",
      "Epoch [5171/20000], Loss: 24.033584594726562, Learning Rate: 0.01\n",
      "Epoch [5172/20000], Loss: 24.02313232421875, Learning Rate: 0.01\n",
      "Epoch [5173/20000], Loss: 24.012786865234375, Learning Rate: 0.01\n",
      "Epoch [5174/20000], Loss: 24.0023193359375, Learning Rate: 0.01\n",
      "Epoch [5175/20000], Loss: 23.991851806640625, Learning Rate: 0.01\n",
      "Epoch [5176/20000], Loss: 23.981369018554688, Learning Rate: 0.01\n",
      "Epoch [5177/20000], Loss: 23.971160888671875, Learning Rate: 0.01\n",
      "Epoch [5178/20000], Loss: 23.9605712890625, Learning Rate: 0.01\n",
      "Epoch [5179/20000], Loss: 23.950164794921875, Learning Rate: 0.01\n",
      "Epoch [5180/20000], Loss: 23.939910888671875, Learning Rate: 0.01\n",
      "Epoch [5181/20000], Loss: 23.929428100585938, Learning Rate: 0.01\n",
      "Epoch [5182/20000], Loss: 23.918930053710938, Learning Rate: 0.01\n",
      "Epoch [5183/20000], Loss: 23.908645629882812, Learning Rate: 0.01\n",
      "Epoch [5184/20000], Loss: 23.898025512695312, Learning Rate: 0.01\n",
      "Epoch [5185/20000], Loss: 23.88775634765625, Learning Rate: 0.01\n",
      "Epoch [5186/20000], Loss: 23.877426147460938, Learning Rate: 0.01\n",
      "Epoch [5187/20000], Loss: 23.867095947265625, Learning Rate: 0.01\n",
      "Epoch [5188/20000], Loss: 23.856536865234375, Learning Rate: 0.01\n",
      "Epoch [5189/20000], Loss: 23.846084594726562, Learning Rate: 0.01\n",
      "Epoch [5190/20000], Loss: 23.835678100585938, Learning Rate: 0.01\n",
      "Epoch [5191/20000], Loss: 23.825347900390625, Learning Rate: 0.01\n",
      "Epoch [5192/20000], Loss: 23.814910888671875, Learning Rate: 0.01\n",
      "Epoch [5193/20000], Loss: 23.804367065429688, Learning Rate: 0.01\n",
      "Epoch [5194/20000], Loss: 23.794052124023438, Learning Rate: 0.01\n",
      "Epoch [5195/20000], Loss: 23.783432006835938, Learning Rate: 0.01\n",
      "Epoch [5196/20000], Loss: 23.773193359375, Learning Rate: 0.01\n",
      "Epoch [5197/20000], Loss: 23.762710571289062, Learning Rate: 0.01\n",
      "Epoch [5198/20000], Loss: 23.752426147460938, Learning Rate: 0.01\n",
      "Epoch [5199/20000], Loss: 23.741790771484375, Learning Rate: 0.01\n",
      "Epoch [5200/20000], Loss: 23.73138427734375, Learning Rate: 0.01\n",
      "Epoch [5201/20000], Loss: 23.720977783203125, Learning Rate: 0.01\n",
      "Epoch [5202/20000], Loss: 23.710662841796875, Learning Rate: 0.01\n",
      "Epoch [5203/20000], Loss: 23.70025634765625, Learning Rate: 0.01\n",
      "Epoch [5204/20000], Loss: 23.689712524414062, Learning Rate: 0.01\n",
      "Epoch [5205/20000], Loss: 23.679214477539062, Learning Rate: 0.01\n",
      "Epoch [5206/20000], Loss: 23.6689453125, Learning Rate: 0.01\n",
      "Epoch [5207/20000], Loss: 23.658706665039062, Learning Rate: 0.01\n",
      "Epoch [5208/20000], Loss: 23.648239135742188, Learning Rate: 0.01\n",
      "Epoch [5209/20000], Loss: 23.637588500976562, Learning Rate: 0.01\n",
      "Epoch [5210/20000], Loss: 23.627227783203125, Learning Rate: 0.01\n",
      "Epoch [5211/20000], Loss: 23.616851806640625, Learning Rate: 0.01\n",
      "Epoch [5212/20000], Loss: 23.606536865234375, Learning Rate: 0.01\n",
      "Epoch [5213/20000], Loss: 23.596160888671875, Learning Rate: 0.01\n",
      "Epoch [5214/20000], Loss: 23.58563232421875, Learning Rate: 0.01\n",
      "Epoch [5215/20000], Loss: 23.575210571289062, Learning Rate: 0.01\n",
      "Epoch [5216/20000], Loss: 23.564849853515625, Learning Rate: 0.01\n",
      "Epoch [5217/20000], Loss: 23.554473876953125, Learning Rate: 0.01\n",
      "Epoch [5218/20000], Loss: 23.544052124023438, Learning Rate: 0.01\n",
      "Epoch [5219/20000], Loss: 23.533599853515625, Learning Rate: 0.01\n",
      "Epoch [5220/20000], Loss: 23.523086547851562, Learning Rate: 0.01\n",
      "Epoch [5221/20000], Loss: 23.512725830078125, Learning Rate: 0.01\n",
      "Epoch [5222/20000], Loss: 23.502288818359375, Learning Rate: 0.01\n",
      "Epoch [5223/20000], Loss: 23.491943359375, Learning Rate: 0.01\n",
      "Epoch [5224/20000], Loss: 23.48138427734375, Learning Rate: 0.01\n",
      "Epoch [5225/20000], Loss: 23.47119140625, Learning Rate: 0.01\n",
      "Epoch [5226/20000], Loss: 23.460662841796875, Learning Rate: 0.01\n",
      "Epoch [5227/20000], Loss: 23.450347900390625, Learning Rate: 0.01\n",
      "Epoch [5228/20000], Loss: 23.43988037109375, Learning Rate: 0.01\n",
      "Epoch [5229/20000], Loss: 23.429550170898438, Learning Rate: 0.01\n",
      "Epoch [5230/20000], Loss: 23.419036865234375, Learning Rate: 0.01\n",
      "Epoch [5231/20000], Loss: 23.408676147460938, Learning Rate: 0.01\n",
      "Epoch [5232/20000], Loss: 23.398300170898438, Learning Rate: 0.01\n",
      "Epoch [5233/20000], Loss: 23.387908935546875, Learning Rate: 0.01\n",
      "Epoch [5234/20000], Loss: 23.377304077148438, Learning Rate: 0.01\n",
      "Epoch [5235/20000], Loss: 23.366973876953125, Learning Rate: 0.01\n",
      "Epoch [5236/20000], Loss: 23.356719970703125, Learning Rate: 0.01\n",
      "Epoch [5237/20000], Loss: 23.346145629882812, Learning Rate: 0.01\n",
      "Epoch [5238/20000], Loss: 23.335739135742188, Learning Rate: 0.01\n",
      "Epoch [5239/20000], Loss: 23.325469970703125, Learning Rate: 0.01\n",
      "Epoch [5240/20000], Loss: 23.314971923828125, Learning Rate: 0.01\n",
      "Epoch [5241/20000], Loss: 23.304458618164062, Learning Rate: 0.01\n",
      "Epoch [5242/20000], Loss: 23.294174194335938, Learning Rate: 0.01\n",
      "Epoch [5243/20000], Loss: 23.283721923828125, Learning Rate: 0.01\n",
      "Epoch [5244/20000], Loss: 23.27325439453125, Learning Rate: 0.01\n",
      "Epoch [5245/20000], Loss: 23.262969970703125, Learning Rate: 0.01\n",
      "Epoch [5246/20000], Loss: 23.252548217773438, Learning Rate: 0.01\n",
      "Epoch [5247/20000], Loss: 23.24212646484375, Learning Rate: 0.01\n",
      "Epoch [5248/20000], Loss: 23.231643676757812, Learning Rate: 0.01\n",
      "Epoch [5249/20000], Loss: 23.221405029296875, Learning Rate: 0.01\n",
      "Epoch [5250/20000], Loss: 23.210968017578125, Learning Rate: 0.01\n",
      "Epoch [5251/20000], Loss: 23.200592041015625, Learning Rate: 0.01\n",
      "Epoch [5252/20000], Loss: 23.190216064453125, Learning Rate: 0.01\n",
      "Epoch [5253/20000], Loss: 23.17999267578125, Learning Rate: 0.01\n",
      "Epoch [5254/20000], Loss: 23.16949462890625, Learning Rate: 0.01\n",
      "Epoch [5255/20000], Loss: 23.159271240234375, Learning Rate: 0.01\n",
      "Epoch [5256/20000], Loss: 23.149032592773438, Learning Rate: 0.01\n",
      "Epoch [5257/20000], Loss: 23.138961791992188, Learning Rate: 0.01\n",
      "Epoch [5258/20000], Loss: 23.128768920898438, Learning Rate: 0.01\n",
      "Epoch [5259/20000], Loss: 23.118804931640625, Learning Rate: 0.01\n",
      "Epoch [5260/20000], Loss: 23.10919189453125, Learning Rate: 0.01\n",
      "Epoch [5261/20000], Loss: 23.099746704101562, Learning Rate: 0.01\n",
      "Epoch [5262/20000], Loss: 23.090988159179688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5263/20000], Loss: 23.0830078125, Learning Rate: 0.01\n",
      "Epoch [5264/20000], Loss: 23.076126098632812, Learning Rate: 0.01\n",
      "Epoch [5265/20000], Loss: 23.071121215820312, Learning Rate: 0.01\n",
      "Epoch [5266/20000], Loss: 23.068893432617188, Learning Rate: 0.01\n",
      "Epoch [5267/20000], Loss: 23.071044921875, Learning Rate: 0.01\n",
      "Epoch [5268/20000], Loss: 23.079849243164062, Learning Rate: 0.01\n",
      "Epoch [5269/20000], Loss: 23.099197387695312, Learning Rate: 0.01\n",
      "Epoch [5270/20000], Loss: 23.134994506835938, Learning Rate: 0.01\n",
      "Epoch [5271/20000], Loss: 23.197235107421875, Learning Rate: 0.01\n",
      "Epoch [5272/20000], Loss: 23.299850463867188, Learning Rate: 0.01\n",
      "Epoch [5273/20000], Loss: 23.466903686523438, Learning Rate: 0.01\n",
      "Epoch [5274/20000], Loss: 23.733963012695312, Learning Rate: 0.01\n",
      "Epoch [5275/20000], Loss: 24.15570068359375, Learning Rate: 0.01\n",
      "Epoch [5276/20000], Loss: 24.810516357421875, Learning Rate: 0.01\n",
      "Epoch [5277/20000], Loss: 25.802536010742188, Learning Rate: 0.01\n",
      "Epoch [5278/20000], Loss: 27.24554443359375, Learning Rate: 0.01\n",
      "Epoch [5279/20000], Loss: 29.21240234375, Learning Rate: 0.01\n",
      "Epoch [5280/20000], Loss: 31.601959228515625, Learning Rate: 0.01\n",
      "Epoch [5281/20000], Loss: 33.95680236816406, Learning Rate: 0.01\n",
      "Epoch [5282/20000], Loss: 35.32615661621094, Learning Rate: 0.01\n",
      "Epoch [5283/20000], Loss: 34.60478210449219, Learning Rate: 0.01\n",
      "Epoch [5284/20000], Loss: 31.419754028320312, Learning Rate: 0.01\n",
      "Epoch [5285/20000], Loss: 27.008453369140625, Learning Rate: 0.01\n",
      "Epoch [5286/20000], Loss: 23.645462036132812, Learning Rate: 0.01\n",
      "Epoch [5287/20000], Loss: 22.904052734375, Learning Rate: 0.01\n",
      "Epoch [5288/20000], Loss: 24.499984741210938, Learning Rate: 0.01\n",
      "Epoch [5289/20000], Loss: 26.730438232421875, Learning Rate: 0.01\n",
      "Epoch [5290/20000], Loss: 27.764404296875, Learning Rate: 0.01\n",
      "Epoch [5291/20000], Loss: 26.83709716796875, Learning Rate: 0.01\n",
      "Epoch [5292/20000], Loss: 24.731674194335938, Learning Rate: 0.01\n",
      "Epoch [5293/20000], Loss: 23.058547973632812, Learning Rate: 0.01\n",
      "Epoch [5294/20000], Loss: 22.867507934570312, Learning Rate: 0.01\n",
      "Epoch [5295/20000], Loss: 23.879241943359375, Learning Rate: 0.01\n",
      "Epoch [5296/20000], Loss: 24.939712524414062, Learning Rate: 0.01\n",
      "Epoch [5297/20000], Loss: 25.072830200195312, Learning Rate: 0.01\n",
      "Epoch [5298/20000], Loss: 24.217864990234375, Learning Rate: 0.01\n",
      "Epoch [5299/20000], Loss: 23.146743774414062, Learning Rate: 0.01\n",
      "Epoch [5300/20000], Loss: 22.687820434570312, Learning Rate: 0.01\n",
      "Epoch [5301/20000], Loss: 23.019180297851562, Learning Rate: 0.01\n",
      "Epoch [5302/20000], Loss: 23.638580322265625, Learning Rate: 0.01\n",
      "Epoch [5303/20000], Loss: 23.906906127929688, Learning Rate: 0.01\n",
      "Epoch [5304/20000], Loss: 23.599716186523438, Learning Rate: 0.01\n",
      "Epoch [5305/20000], Loss: 23.019317626953125, Learning Rate: 0.01\n",
      "Epoch [5306/20000], Loss: 22.648788452148438, Learning Rate: 0.01\n",
      "Epoch [5307/20000], Loss: 22.7083740234375, Learning Rate: 0.01\n",
      "Epoch [5308/20000], Loss: 23.020675659179688, Learning Rate: 0.01\n",
      "Epoch [5309/20000], Loss: 23.235061645507812, Learning Rate: 0.01\n",
      "Epoch [5310/20000], Loss: 23.152664184570312, Learning Rate: 0.01\n",
      "Epoch [5311/20000], Loss: 22.861618041992188, Learning Rate: 0.01\n",
      "Epoch [5312/20000], Loss: 22.610427856445312, Learning Rate: 0.01\n",
      "Epoch [5313/20000], Loss: 22.569366455078125, Learning Rate: 0.01\n",
      "Epoch [5314/20000], Loss: 22.704086303710938, Learning Rate: 0.01\n",
      "Epoch [5315/20000], Loss: 22.84423828125, Learning Rate: 0.01\n",
      "Epoch [5316/20000], Loss: 22.849105834960938, Learning Rate: 0.01\n",
      "Epoch [5317/20000], Loss: 22.71807861328125, Learning Rate: 0.01\n",
      "Epoch [5318/20000], Loss: 22.562179565429688, Learning Rate: 0.01\n",
      "Epoch [5319/20000], Loss: 22.492767333984375, Learning Rate: 0.01\n",
      "Epoch [5320/20000], Loss: 22.531082153320312, Learning Rate: 0.01\n",
      "Epoch [5321/20000], Loss: 22.608184814453125, Learning Rate: 0.01\n",
      "Epoch [5322/20000], Loss: 22.638381958007812, Learning Rate: 0.01\n",
      "Epoch [5323/20000], Loss: 22.590957641601562, Learning Rate: 0.01\n",
      "Epoch [5324/20000], Loss: 22.503555297851562, Learning Rate: 0.01\n",
      "Epoch [5325/20000], Loss: 22.438522338867188, Learning Rate: 0.01\n",
      "Epoch [5326/20000], Loss: 22.428970336914062, Learning Rate: 0.01\n",
      "Epoch [5327/20000], Loss: 22.459228515625, Learning Rate: 0.01\n",
      "Epoch [5328/20000], Loss: 22.485809326171875, Learning Rate: 0.01\n",
      "Epoch [5329/20000], Loss: 22.477142333984375, Learning Rate: 0.01\n",
      "Epoch [5330/20000], Loss: 22.435272216796875, Learning Rate: 0.01\n",
      "Epoch [5331/20000], Loss: 22.387664794921875, Learning Rate: 0.01\n",
      "Epoch [5332/20000], Loss: 22.360580444335938, Learning Rate: 0.01\n",
      "Epoch [5333/20000], Loss: 22.360824584960938, Learning Rate: 0.01\n",
      "Epoch [5334/20000], Loss: 22.372604370117188, Learning Rate: 0.01\n",
      "Epoch [5335/20000], Loss: 22.375717163085938, Learning Rate: 0.01\n",
      "Epoch [5336/20000], Loss: 22.360031127929688, Learning Rate: 0.01\n",
      "Epoch [5337/20000], Loss: 22.3316650390625, Learning Rate: 0.01\n",
      "Epoch [5338/20000], Loss: 22.3048095703125, Learning Rate: 0.01\n",
      "Epoch [5339/20000], Loss: 22.289840698242188, Learning Rate: 0.01\n",
      "Epoch [5340/20000], Loss: 22.2869873046875, Learning Rate: 0.01\n",
      "Epoch [5341/20000], Loss: 22.287368774414062, Learning Rate: 0.01\n",
      "Epoch [5342/20000], Loss: 22.282211303710938, Learning Rate: 0.01\n",
      "Epoch [5343/20000], Loss: 22.268020629882812, Learning Rate: 0.01\n",
      "Epoch [5344/20000], Loss: 22.24871826171875, Learning Rate: 0.01\n",
      "Epoch [5345/20000], Loss: 22.230911254882812, Learning Rate: 0.01\n",
      "Epoch [5346/20000], Loss: 22.218780517578125, Learning Rate: 0.01\n",
      "Epoch [5347/20000], Loss: 22.212448120117188, Learning Rate: 0.01\n",
      "Epoch [5348/20000], Loss: 22.207366943359375, Learning Rate: 0.01\n",
      "Epoch [5349/20000], Loss: 22.199630737304688, Learning Rate: 0.01\n",
      "Epoch [5350/20000], Loss: 22.187606811523438, Learning Rate: 0.01\n",
      "Epoch [5351/20000], Loss: 22.173019409179688, Learning Rate: 0.01\n",
      "Epoch [5352/20000], Loss: 22.159210205078125, Learning Rate: 0.01\n",
      "Epoch [5353/20000], Loss: 22.147857666015625, Learning Rate: 0.01\n",
      "Epoch [5354/20000], Loss: 22.138885498046875, Learning Rate: 0.01\n",
      "Epoch [5355/20000], Loss: 22.131454467773438, Learning Rate: 0.01\n",
      "Epoch [5356/20000], Loss: 22.122802734375, Learning Rate: 0.01\n",
      "Epoch [5357/20000], Loss: 22.111953735351562, Learning Rate: 0.01\n",
      "Epoch [5358/20000], Loss: 22.1002197265625, Learning Rate: 0.01\n",
      "Epoch [5359/20000], Loss: 22.08795166015625, Learning Rate: 0.01\n",
      "Epoch [5360/20000], Loss: 22.076980590820312, Learning Rate: 0.01\n",
      "Epoch [5361/20000], Loss: 22.067062377929688, Learning Rate: 0.01\n",
      "Epoch [5362/20000], Loss: 22.058029174804688, Learning Rate: 0.01\n",
      "Epoch [5363/20000], Loss: 22.048782348632812, Learning Rate: 0.01\n",
      "Epoch [5364/20000], Loss: 22.038803100585938, Learning Rate: 0.01\n",
      "Epoch [5365/20000], Loss: 22.028106689453125, Learning Rate: 0.01\n",
      "Epoch [5366/20000], Loss: 22.016983032226562, Learning Rate: 0.01\n",
      "Epoch [5367/20000], Loss: 22.006179809570312, Learning Rate: 0.01\n",
      "Epoch [5368/20000], Loss: 21.99578857421875, Learning Rate: 0.01\n",
      "Epoch [5369/20000], Loss: 21.985870361328125, Learning Rate: 0.01\n",
      "Epoch [5370/20000], Loss: 21.976425170898438, Learning Rate: 0.01\n",
      "Epoch [5371/20000], Loss: 21.966506958007812, Learning Rate: 0.01\n",
      "Epoch [5372/20000], Loss: 21.95635986328125, Learning Rate: 0.01\n",
      "Epoch [5373/20000], Loss: 21.946029663085938, Learning Rate: 0.01\n",
      "Epoch [5374/20000], Loss: 21.93548583984375, Learning Rate: 0.01\n",
      "Epoch [5375/20000], Loss: 21.925003051757812, Learning Rate: 0.01\n",
      "Epoch [5376/20000], Loss: 21.91473388671875, Learning Rate: 0.01\n",
      "Epoch [5377/20000], Loss: 21.904937744140625, Learning Rate: 0.01\n",
      "Epoch [5378/20000], Loss: 21.8948974609375, Learning Rate: 0.01\n",
      "Epoch [5379/20000], Loss: 21.884841918945312, Learning Rate: 0.01\n",
      "Epoch [5380/20000], Loss: 21.874664306640625, Learning Rate: 0.01\n",
      "Epoch [5381/20000], Loss: 21.86456298828125, Learning Rate: 0.01\n",
      "Epoch [5382/20000], Loss: 21.854217529296875, Learning Rate: 0.01\n",
      "Epoch [5383/20000], Loss: 21.843780517578125, Learning Rate: 0.01\n",
      "Epoch [5384/20000], Loss: 21.833770751953125, Learning Rate: 0.01\n",
      "Epoch [5385/20000], Loss: 21.82366943359375, Learning Rate: 0.01\n",
      "Epoch [5386/20000], Loss: 21.813507080078125, Learning Rate: 0.01\n",
      "Epoch [5387/20000], Loss: 21.803466796875, Learning Rate: 0.01\n",
      "Epoch [5388/20000], Loss: 21.793411254882812, Learning Rate: 0.01\n",
      "Epoch [5389/20000], Loss: 21.783248901367188, Learning Rate: 0.01\n",
      "Epoch [5390/20000], Loss: 21.772994995117188, Learning Rate: 0.01\n",
      "Epoch [5391/20000], Loss: 21.762863159179688, Learning Rate: 0.01\n",
      "Epoch [5392/20000], Loss: 21.752609252929688, Learning Rate: 0.01\n",
      "Epoch [5393/20000], Loss: 21.742584228515625, Learning Rate: 0.01\n",
      "Epoch [5394/20000], Loss: 21.732406616210938, Learning Rate: 0.01\n",
      "Epoch [5395/20000], Loss: 21.722396850585938, Learning Rate: 0.01\n",
      "Epoch [5396/20000], Loss: 21.712112426757812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5397/20000], Loss: 21.702056884765625, Learning Rate: 0.01\n",
      "Epoch [5398/20000], Loss: 21.692001342773438, Learning Rate: 0.01\n",
      "Epoch [5399/20000], Loss: 21.681838989257812, Learning Rate: 0.01\n",
      "Epoch [5400/20000], Loss: 21.671676635742188, Learning Rate: 0.01\n",
      "Epoch [5401/20000], Loss: 21.661575317382812, Learning Rate: 0.01\n",
      "Epoch [5402/20000], Loss: 21.6513671875, Learning Rate: 0.01\n",
      "Epoch [5403/20000], Loss: 21.641189575195312, Learning Rate: 0.01\n",
      "Epoch [5404/20000], Loss: 21.631195068359375, Learning Rate: 0.01\n",
      "Epoch [5405/20000], Loss: 21.621109008789062, Learning Rate: 0.01\n",
      "Epoch [5406/20000], Loss: 21.610916137695312, Learning Rate: 0.01\n",
      "Epoch [5407/20000], Loss: 21.600830078125, Learning Rate: 0.01\n",
      "Epoch [5408/20000], Loss: 21.590652465820312, Learning Rate: 0.01\n",
      "Epoch [5409/20000], Loss: 21.5804443359375, Learning Rate: 0.01\n",
      "Epoch [5410/20000], Loss: 21.570281982421875, Learning Rate: 0.01\n",
      "Epoch [5411/20000], Loss: 21.560256958007812, Learning Rate: 0.01\n",
      "Epoch [5412/20000], Loss: 21.550064086914062, Learning Rate: 0.01\n",
      "Epoch [5413/20000], Loss: 21.539871215820312, Learning Rate: 0.01\n",
      "Epoch [5414/20000], Loss: 21.52996826171875, Learning Rate: 0.01\n",
      "Epoch [5415/20000], Loss: 21.519638061523438, Learning Rate: 0.01\n",
      "Epoch [5416/20000], Loss: 21.509750366210938, Learning Rate: 0.01\n",
      "Epoch [5417/20000], Loss: 21.4993896484375, Learning Rate: 0.01\n",
      "Epoch [5418/20000], Loss: 21.489547729492188, Learning Rate: 0.01\n",
      "Epoch [5419/20000], Loss: 21.4791259765625, Learning Rate: 0.01\n",
      "Epoch [5420/20000], Loss: 21.469085693359375, Learning Rate: 0.01\n",
      "Epoch [5421/20000], Loss: 21.458877563476562, Learning Rate: 0.01\n",
      "Epoch [5422/20000], Loss: 21.448715209960938, Learning Rate: 0.01\n",
      "Epoch [5423/20000], Loss: 21.438690185546875, Learning Rate: 0.01\n",
      "Epoch [5424/20000], Loss: 21.428359985351562, Learning Rate: 0.01\n",
      "Epoch [5425/20000], Loss: 21.4183349609375, Learning Rate: 0.01\n",
      "Epoch [5426/20000], Loss: 21.40838623046875, Learning Rate: 0.01\n",
      "Epoch [5427/20000], Loss: 21.398193359375, Learning Rate: 0.01\n",
      "Epoch [5428/20000], Loss: 21.388214111328125, Learning Rate: 0.01\n",
      "Epoch [5429/20000], Loss: 21.377960205078125, Learning Rate: 0.01\n",
      "Epoch [5430/20000], Loss: 21.36767578125, Learning Rate: 0.01\n",
      "Epoch [5431/20000], Loss: 21.357681274414062, Learning Rate: 0.01\n",
      "Epoch [5432/20000], Loss: 21.3475341796875, Learning Rate: 0.01\n",
      "Epoch [5433/20000], Loss: 21.337356567382812, Learning Rate: 0.01\n",
      "Epoch [5434/20000], Loss: 21.327285766601562, Learning Rate: 0.01\n",
      "Epoch [5435/20000], Loss: 21.317062377929688, Learning Rate: 0.01\n",
      "Epoch [5436/20000], Loss: 21.307144165039062, Learning Rate: 0.01\n",
      "Epoch [5437/20000], Loss: 21.29693603515625, Learning Rate: 0.01\n",
      "Epoch [5438/20000], Loss: 21.28680419921875, Learning Rate: 0.01\n",
      "Epoch [5439/20000], Loss: 21.276580810546875, Learning Rate: 0.01\n",
      "Epoch [5440/20000], Loss: 21.26654052734375, Learning Rate: 0.01\n",
      "Epoch [5441/20000], Loss: 21.256332397460938, Learning Rate: 0.01\n",
      "Epoch [5442/20000], Loss: 21.246109008789062, Learning Rate: 0.01\n",
      "Epoch [5443/20000], Loss: 21.2362060546875, Learning Rate: 0.01\n",
      "Epoch [5444/20000], Loss: 21.225997924804688, Learning Rate: 0.01\n",
      "Epoch [5445/20000], Loss: 21.2156982421875, Learning Rate: 0.01\n",
      "Epoch [5446/20000], Loss: 21.205886840820312, Learning Rate: 0.01\n",
      "Epoch [5447/20000], Loss: 21.195770263671875, Learning Rate: 0.01\n",
      "Epoch [5448/20000], Loss: 21.185501098632812, Learning Rate: 0.01\n",
      "Epoch [5449/20000], Loss: 21.175247192382812, Learning Rate: 0.01\n",
      "Epoch [5450/20000], Loss: 21.165176391601562, Learning Rate: 0.01\n",
      "Epoch [5451/20000], Loss: 21.154983520507812, Learning Rate: 0.01\n",
      "Epoch [5452/20000], Loss: 21.145050048828125, Learning Rate: 0.01\n",
      "Epoch [5453/20000], Loss: 21.134841918945312, Learning Rate: 0.01\n",
      "Epoch [5454/20000], Loss: 21.124786376953125, Learning Rate: 0.01\n",
      "Epoch [5455/20000], Loss: 21.114669799804688, Learning Rate: 0.01\n",
      "Epoch [5456/20000], Loss: 21.104461669921875, Learning Rate: 0.01\n",
      "Epoch [5457/20000], Loss: 21.094223022460938, Learning Rate: 0.01\n",
      "Epoch [5458/20000], Loss: 21.084014892578125, Learning Rate: 0.01\n",
      "Epoch [5459/20000], Loss: 21.07391357421875, Learning Rate: 0.01\n",
      "Epoch [5460/20000], Loss: 21.063934326171875, Learning Rate: 0.01\n",
      "Epoch [5461/20000], Loss: 21.053817749023438, Learning Rate: 0.01\n",
      "Epoch [5462/20000], Loss: 21.043731689453125, Learning Rate: 0.01\n",
      "Epoch [5463/20000], Loss: 21.03350830078125, Learning Rate: 0.01\n",
      "Epoch [5464/20000], Loss: 21.023406982421875, Learning Rate: 0.01\n",
      "Epoch [5465/20000], Loss: 21.013397216796875, Learning Rate: 0.01\n",
      "Epoch [5466/20000], Loss: 21.003021240234375, Learning Rate: 0.01\n",
      "Epoch [5467/20000], Loss: 20.993057250976562, Learning Rate: 0.01\n",
      "Epoch [5468/20000], Loss: 20.983047485351562, Learning Rate: 0.01\n",
      "Epoch [5469/20000], Loss: 20.972946166992188, Learning Rate: 0.01\n",
      "Epoch [5470/20000], Loss: 20.962631225585938, Learning Rate: 0.01\n",
      "Epoch [5471/20000], Loss: 20.9525146484375, Learning Rate: 0.01\n",
      "Epoch [5472/20000], Loss: 20.942459106445312, Learning Rate: 0.01\n",
      "Epoch [5473/20000], Loss: 20.93231201171875, Learning Rate: 0.01\n",
      "Epoch [5474/20000], Loss: 20.922164916992188, Learning Rate: 0.01\n",
      "Epoch [5475/20000], Loss: 20.912185668945312, Learning Rate: 0.01\n",
      "Epoch [5476/20000], Loss: 20.901962280273438, Learning Rate: 0.01\n",
      "Epoch [5477/20000], Loss: 20.891830444335938, Learning Rate: 0.01\n",
      "Epoch [5478/20000], Loss: 20.881637573242188, Learning Rate: 0.01\n",
      "Epoch [5479/20000], Loss: 20.87152099609375, Learning Rate: 0.01\n",
      "Epoch [5480/20000], Loss: 20.861343383789062, Learning Rate: 0.01\n",
      "Epoch [5481/20000], Loss: 20.851272583007812, Learning Rate: 0.01\n",
      "Epoch [5482/20000], Loss: 20.841201782226562, Learning Rate: 0.01\n",
      "Epoch [5483/20000], Loss: 20.831085205078125, Learning Rate: 0.01\n",
      "Epoch [5484/20000], Loss: 20.820846557617188, Learning Rate: 0.01\n",
      "Epoch [5485/20000], Loss: 20.8106689453125, Learning Rate: 0.01\n",
      "Epoch [5486/20000], Loss: 20.8006591796875, Learning Rate: 0.01\n",
      "Epoch [5487/20000], Loss: 20.790573120117188, Learning Rate: 0.01\n",
      "Epoch [5488/20000], Loss: 20.7803955078125, Learning Rate: 0.01\n",
      "Epoch [5489/20000], Loss: 20.77032470703125, Learning Rate: 0.01\n",
      "Epoch [5490/20000], Loss: 20.76007080078125, Learning Rate: 0.01\n",
      "Epoch [5491/20000], Loss: 20.75006103515625, Learning Rate: 0.01\n",
      "Epoch [5492/20000], Loss: 20.739776611328125, Learning Rate: 0.01\n",
      "Epoch [5493/20000], Loss: 20.729949951171875, Learning Rate: 0.01\n",
      "Epoch [5494/20000], Loss: 20.719573974609375, Learning Rate: 0.01\n",
      "Epoch [5495/20000], Loss: 20.709609985351562, Learning Rate: 0.01\n",
      "Epoch [5496/20000], Loss: 20.69921875, Learning Rate: 0.01\n",
      "Epoch [5497/20000], Loss: 20.689315795898438, Learning Rate: 0.01\n",
      "Epoch [5498/20000], Loss: 20.679214477539062, Learning Rate: 0.01\n",
      "Epoch [5499/20000], Loss: 20.66925048828125, Learning Rate: 0.01\n",
      "Epoch [5500/20000], Loss: 20.6590576171875, Learning Rate: 0.01\n",
      "Epoch [5501/20000], Loss: 20.649002075195312, Learning Rate: 0.01\n",
      "Epoch [5502/20000], Loss: 20.639129638671875, Learning Rate: 0.01\n",
      "Epoch [5503/20000], Loss: 20.628921508789062, Learning Rate: 0.01\n",
      "Epoch [5504/20000], Loss: 20.619110107421875, Learning Rate: 0.01\n",
      "Epoch [5505/20000], Loss: 20.609207153320312, Learning Rate: 0.01\n",
      "Epoch [5506/20000], Loss: 20.599594116210938, Learning Rate: 0.01\n",
      "Epoch [5507/20000], Loss: 20.590179443359375, Learning Rate: 0.01\n",
      "Epoch [5508/20000], Loss: 20.580902099609375, Learning Rate: 0.01\n",
      "Epoch [5509/20000], Loss: 20.572372436523438, Learning Rate: 0.01\n",
      "Epoch [5510/20000], Loss: 20.564407348632812, Learning Rate: 0.01\n",
      "Epoch [5511/20000], Loss: 20.55755615234375, Learning Rate: 0.01\n",
      "Epoch [5512/20000], Loss: 20.552581787109375, Learning Rate: 0.01\n",
      "Epoch [5513/20000], Loss: 20.550552368164062, Learning Rate: 0.01\n",
      "Epoch [5514/20000], Loss: 20.552780151367188, Learning Rate: 0.01\n",
      "Epoch [5515/20000], Loss: 20.5618896484375, Learning Rate: 0.01\n",
      "Epoch [5516/20000], Loss: 20.581710815429688, Learning Rate: 0.01\n",
      "Epoch [5517/20000], Loss: 20.619308471679688, Learning Rate: 0.01\n",
      "Epoch [5518/20000], Loss: 20.684188842773438, Learning Rate: 0.01\n",
      "Epoch [5519/20000], Loss: 20.792739868164062, Learning Rate: 0.01\n",
      "Epoch [5520/20000], Loss: 20.971054077148438, Learning Rate: 0.01\n",
      "Epoch [5521/20000], Loss: 21.259796142578125, Learning Rate: 0.01\n",
      "Epoch [5522/20000], Loss: 21.721115112304688, Learning Rate: 0.01\n",
      "Epoch [5523/20000], Loss: 22.446945190429688, Learning Rate: 0.01\n",
      "Epoch [5524/20000], Loss: 23.562362670898438, Learning Rate: 0.01\n",
      "Epoch [5525/20000], Loss: 25.21307373046875, Learning Rate: 0.01\n",
      "Epoch [5526/20000], Loss: 27.500686645507812, Learning Rate: 0.01\n",
      "Epoch [5527/20000], Loss: 30.34075927734375, Learning Rate: 0.01\n",
      "Epoch [5528/20000], Loss: 33.19764709472656, Learning Rate: 0.01\n",
      "Epoch [5529/20000], Loss: 34.93525695800781, Learning Rate: 0.01\n",
      "Epoch [5530/20000], Loss: 34.13081359863281, Learning Rate: 0.01\n",
      "Epoch [5531/20000], Loss: 30.314208984375, Learning Rate: 0.01\n",
      "Epoch [5532/20000], Loss: 25.017349243164062, Learning Rate: 0.01\n",
      "Epoch [5533/20000], Loss: 21.1146240234375, Learning Rate: 0.01\n",
      "Epoch [5534/20000], Loss: 20.489013671875, Learning Rate: 0.01\n",
      "Epoch [5535/20000], Loss: 22.619918823242188, Learning Rate: 0.01\n",
      "Epoch [5536/20000], Loss: 25.262939453125, Learning Rate: 0.01\n",
      "Epoch [5537/20000], Loss: 26.179351806640625, Learning Rate: 0.01\n",
      "Epoch [5538/20000], Loss: 24.683242797851562, Learning Rate: 0.01\n",
      "Epoch [5539/20000], Loss: 22.058685302734375, Learning Rate: 0.01\n",
      "Epoch [5540/20000], Loss: 20.374114990234375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5541/20000], Loss: 20.624237060546875, Learning Rate: 0.01\n",
      "Epoch [5542/20000], Loss: 22.057174682617188, Learning Rate: 0.01\n",
      "Epoch [5543/20000], Loss: 23.103408813476562, Learning Rate: 0.01\n",
      "Epoch [5544/20000], Loss: 22.797836303710938, Learning Rate: 0.01\n",
      "Epoch [5545/20000], Loss: 21.4991455078125, Learning Rate: 0.01\n",
      "Epoch [5546/20000], Loss: 20.381683349609375, Learning Rate: 0.01\n",
      "Epoch [5547/20000], Loss: 20.269912719726562, Learning Rate: 0.01\n",
      "Epoch [5548/20000], Loss: 20.966552734375, Learning Rate: 0.01\n",
      "Epoch [5549/20000], Loss: 21.61541748046875, Learning Rate: 0.01\n",
      "Epoch [5550/20000], Loss: 21.5689697265625, Learning Rate: 0.01\n",
      "Epoch [5551/20000], Loss: 20.915420532226562, Learning Rate: 0.01\n",
      "Epoch [5552/20000], Loss: 20.274459838867188, Learning Rate: 0.01\n",
      "Epoch [5553/20000], Loss: 20.146011352539062, Learning Rate: 0.01\n",
      "Epoch [5554/20000], Loss: 20.486968994140625, Learning Rate: 0.01\n",
      "Epoch [5555/20000], Loss: 20.851181030273438, Learning Rate: 0.01\n",
      "Epoch [5556/20000], Loss: 20.86199951171875, Learning Rate: 0.01\n",
      "Epoch [5557/20000], Loss: 20.529571533203125, Learning Rate: 0.01\n",
      "Epoch [5558/20000], Loss: 20.169265747070312, Learning Rate: 0.01\n",
      "Epoch [5559/20000], Loss: 20.065902709960938, Learning Rate: 0.01\n",
      "Epoch [5560/20000], Loss: 20.225479125976562, Learning Rate: 0.01\n",
      "Epoch [5561/20000], Loss: 20.424697875976562, Learning Rate: 0.01\n",
      "Epoch [5562/20000], Loss: 20.451263427734375, Learning Rate: 0.01\n",
      "Epoch [5563/20000], Loss: 20.286102294921875, Learning Rate: 0.01\n",
      "Epoch [5564/20000], Loss: 20.082672119140625, Learning Rate: 0.01\n",
      "Epoch [5565/20000], Loss: 20.000091552734375, Learning Rate: 0.01\n",
      "Epoch [5566/20000], Loss: 20.06378173828125, Learning Rate: 0.01\n",
      "Epoch [5567/20000], Loss: 20.169769287109375, Learning Rate: 0.01\n",
      "Epoch [5568/20000], Loss: 20.199127197265625, Learning Rate: 0.01\n",
      "Epoch [5569/20000], Loss: 20.122665405273438, Learning Rate: 0.01\n",
      "Epoch [5570/20000], Loss: 20.007431030273438, Learning Rate: 0.01\n",
      "Epoch [5571/20000], Loss: 19.941802978515625, Learning Rate: 0.01\n",
      "Epoch [5572/20000], Loss: 19.95562744140625, Learning Rate: 0.01\n",
      "Epoch [5573/20000], Loss: 20.007095336914062, Learning Rate: 0.01\n",
      "Epoch [5574/20000], Loss: 20.032028198242188, Learning Rate: 0.01\n",
      "Epoch [5575/20000], Loss: 20.00115966796875, Learning Rate: 0.01\n",
      "Epoch [5576/20000], Loss: 19.937362670898438, Learning Rate: 0.01\n",
      "Epoch [5577/20000], Loss: 19.88702392578125, Learning Rate: 0.01\n",
      "Epoch [5578/20000], Loss: 19.87646484375, Learning Rate: 0.01\n",
      "Epoch [5579/20000], Loss: 19.895095825195312, Learning Rate: 0.01\n",
      "Epoch [5580/20000], Loss: 19.911346435546875, Learning Rate: 0.01\n",
      "Epoch [5581/20000], Loss: 19.90167236328125, Learning Rate: 0.01\n",
      "Epoch [5582/20000], Loss: 19.868484497070312, Learning Rate: 0.01\n",
      "Epoch [5583/20000], Loss: 19.832275390625, Learning Rate: 0.01\n",
      "Epoch [5584/20000], Loss: 19.812576293945312, Learning Rate: 0.01\n",
      "Epoch [5585/20000], Loss: 19.811965942382812, Learning Rate: 0.01\n",
      "Epoch [5586/20000], Loss: 19.817825317382812, Learning Rate: 0.01\n",
      "Epoch [5587/20000], Loss: 19.815582275390625, Learning Rate: 0.01\n",
      "Epoch [5588/20000], Loss: 19.799407958984375, Learning Rate: 0.01\n",
      "Epoch [5589/20000], Loss: 19.775497436523438, Learning Rate: 0.01\n",
      "Epoch [5590/20000], Loss: 19.755172729492188, Learning Rate: 0.01\n",
      "Epoch [5591/20000], Loss: 19.744720458984375, Learning Rate: 0.01\n",
      "Epoch [5592/20000], Loss: 19.741622924804688, Learning Rate: 0.01\n",
      "Epoch [5593/20000], Loss: 19.738922119140625, Learning Rate: 0.01\n",
      "Epoch [5594/20000], Loss: 19.73046875, Learning Rate: 0.01\n",
      "Epoch [5595/20000], Loss: 19.7159423828125, Learning Rate: 0.01\n",
      "Epoch [5596/20000], Loss: 19.699050903320312, Learning Rate: 0.01\n",
      "Epoch [5597/20000], Loss: 19.684967041015625, Learning Rate: 0.01\n",
      "Epoch [5598/20000], Loss: 19.67596435546875, Learning Rate: 0.01\n",
      "Epoch [5599/20000], Loss: 19.669815063476562, Learning Rate: 0.01\n",
      "Epoch [5600/20000], Loss: 19.663162231445312, Learning Rate: 0.01\n",
      "Epoch [5601/20000], Loss: 19.6534423828125, Learning Rate: 0.01\n",
      "Epoch [5602/20000], Loss: 19.640838623046875, Learning Rate: 0.01\n",
      "Epoch [5603/20000], Loss: 19.62774658203125, Learning Rate: 0.01\n",
      "Epoch [5604/20000], Loss: 19.616165161132812, Learning Rate: 0.01\n",
      "Epoch [5605/20000], Loss: 19.606781005859375, Learning Rate: 0.01\n",
      "Epoch [5606/20000], Loss: 19.598800659179688, Learning Rate: 0.01\n",
      "Epoch [5607/20000], Loss: 19.590438842773438, Learning Rate: 0.01\n",
      "Epoch [5608/20000], Loss: 19.580612182617188, Learning Rate: 0.01\n",
      "Epoch [5609/20000], Loss: 19.569686889648438, Learning Rate: 0.01\n",
      "Epoch [5610/20000], Loss: 19.557937622070312, Learning Rate: 0.01\n",
      "Epoch [5611/20000], Loss: 19.54736328125, Learning Rate: 0.01\n",
      "Epoch [5612/20000], Loss: 19.537521362304688, Learning Rate: 0.01\n",
      "Epoch [5613/20000], Loss: 19.528411865234375, Learning Rate: 0.01\n",
      "Epoch [5614/20000], Loss: 19.519500732421875, Learning Rate: 0.01\n",
      "Epoch [5615/20000], Loss: 19.509750366210938, Learning Rate: 0.01\n",
      "Epoch [5616/20000], Loss: 19.499588012695312, Learning Rate: 0.01\n",
      "Epoch [5617/20000], Loss: 19.489044189453125, Learning Rate: 0.01\n",
      "Epoch [5618/20000], Loss: 19.47857666015625, Learning Rate: 0.01\n",
      "Epoch [5619/20000], Loss: 19.468673706054688, Learning Rate: 0.01\n",
      "Epoch [5620/20000], Loss: 19.4591064453125, Learning Rate: 0.01\n",
      "Epoch [5621/20000], Loss: 19.449478149414062, Learning Rate: 0.01\n",
      "Epoch [5622/20000], Loss: 19.440078735351562, Learning Rate: 0.01\n",
      "Epoch [5623/20000], Loss: 19.430130004882812, Learning Rate: 0.01\n",
      "Epoch [5624/20000], Loss: 19.420074462890625, Learning Rate: 0.01\n",
      "Epoch [5625/20000], Loss: 19.409927368164062, Learning Rate: 0.01\n",
      "Epoch [5626/20000], Loss: 19.399917602539062, Learning Rate: 0.01\n",
      "Epoch [5627/20000], Loss: 19.389999389648438, Learning Rate: 0.01\n",
      "Epoch [5628/20000], Loss: 19.380172729492188, Learning Rate: 0.01\n",
      "Epoch [5629/20000], Loss: 19.37066650390625, Learning Rate: 0.01\n",
      "Epoch [5630/20000], Loss: 19.361160278320312, Learning Rate: 0.01\n",
      "Epoch [5631/20000], Loss: 19.35107421875, Learning Rate: 0.01\n",
      "Epoch [5632/20000], Loss: 19.341156005859375, Learning Rate: 0.01\n",
      "Epoch [5633/20000], Loss: 19.331130981445312, Learning Rate: 0.01\n",
      "Epoch [5634/20000], Loss: 19.321212768554688, Learning Rate: 0.01\n",
      "Epoch [5635/20000], Loss: 19.311431884765625, Learning Rate: 0.01\n",
      "Epoch [5636/20000], Loss: 19.301727294921875, Learning Rate: 0.01\n",
      "Epoch [5637/20000], Loss: 19.29193115234375, Learning Rate: 0.01\n",
      "Epoch [5638/20000], Loss: 19.282028198242188, Learning Rate: 0.01\n",
      "Epoch [5639/20000], Loss: 19.27227783203125, Learning Rate: 0.01\n",
      "Epoch [5640/20000], Loss: 19.262481689453125, Learning Rate: 0.01\n",
      "Epoch [5641/20000], Loss: 19.252426147460938, Learning Rate: 0.01\n",
      "Epoch [5642/20000], Loss: 19.242507934570312, Learning Rate: 0.01\n",
      "Epoch [5643/20000], Loss: 19.232757568359375, Learning Rate: 0.01\n",
      "Epoch [5644/20000], Loss: 19.223068237304688, Learning Rate: 0.01\n",
      "Epoch [5645/20000], Loss: 19.213363647460938, Learning Rate: 0.01\n",
      "Epoch [5646/20000], Loss: 19.203536987304688, Learning Rate: 0.01\n",
      "Epoch [5647/20000], Loss: 19.193511962890625, Learning Rate: 0.01\n",
      "Epoch [5648/20000], Loss: 19.183853149414062, Learning Rate: 0.01\n",
      "Epoch [5649/20000], Loss: 19.173934936523438, Learning Rate: 0.01\n",
      "Epoch [5650/20000], Loss: 19.164031982421875, Learning Rate: 0.01\n",
      "Epoch [5651/20000], Loss: 19.154205322265625, Learning Rate: 0.01\n",
      "Epoch [5652/20000], Loss: 19.144363403320312, Learning Rate: 0.01\n",
      "Epoch [5653/20000], Loss: 19.134506225585938, Learning Rate: 0.01\n",
      "Epoch [5654/20000], Loss: 19.124755859375, Learning Rate: 0.01\n",
      "Epoch [5655/20000], Loss: 19.114898681640625, Learning Rate: 0.01\n",
      "Epoch [5656/20000], Loss: 19.105117797851562, Learning Rate: 0.01\n",
      "Epoch [5657/20000], Loss: 19.095352172851562, Learning Rate: 0.01\n",
      "Epoch [5658/20000], Loss: 19.08544921875, Learning Rate: 0.01\n",
      "Epoch [5659/20000], Loss: 19.075485229492188, Learning Rate: 0.01\n",
      "Epoch [5660/20000], Loss: 19.0657958984375, Learning Rate: 0.01\n",
      "Epoch [5661/20000], Loss: 19.055892944335938, Learning Rate: 0.01\n",
      "Epoch [5662/20000], Loss: 19.046142578125, Learning Rate: 0.01\n",
      "Epoch [5663/20000], Loss: 19.03619384765625, Learning Rate: 0.01\n",
      "Epoch [5664/20000], Loss: 19.026397705078125, Learning Rate: 0.01\n",
      "Epoch [5665/20000], Loss: 19.016571044921875, Learning Rate: 0.01\n",
      "Epoch [5666/20000], Loss: 19.0068359375, Learning Rate: 0.01\n",
      "Epoch [5667/20000], Loss: 18.996917724609375, Learning Rate: 0.01\n",
      "Epoch [5668/20000], Loss: 18.986953735351562, Learning Rate: 0.01\n",
      "Epoch [5669/20000], Loss: 18.977340698242188, Learning Rate: 0.01\n",
      "Epoch [5670/20000], Loss: 18.967575073242188, Learning Rate: 0.01\n",
      "Epoch [5671/20000], Loss: 18.957534790039062, Learning Rate: 0.01\n",
      "Epoch [5672/20000], Loss: 18.947845458984375, Learning Rate: 0.01\n",
      "Epoch [5673/20000], Loss: 18.93792724609375, Learning Rate: 0.01\n",
      "Epoch [5674/20000], Loss: 18.928314208984375, Learning Rate: 0.01\n",
      "Epoch [5675/20000], Loss: 18.918289184570312, Learning Rate: 0.01\n",
      "Epoch [5676/20000], Loss: 18.90863037109375, Learning Rate: 0.01\n",
      "Epoch [5677/20000], Loss: 18.898681640625, Learning Rate: 0.01\n",
      "Epoch [5678/20000], Loss: 18.888961791992188, Learning Rate: 0.01\n",
      "Epoch [5679/20000], Loss: 18.879043579101562, Learning Rate: 0.01\n",
      "Epoch [5680/20000], Loss: 18.869216918945312, Learning Rate: 0.01\n",
      "Epoch [5681/20000], Loss: 18.8594970703125, Learning Rate: 0.01\n",
      "Epoch [5682/20000], Loss: 18.84954833984375, Learning Rate: 0.01\n",
      "Epoch [5683/20000], Loss: 18.83978271484375, Learning Rate: 0.01\n",
      "Epoch [5684/20000], Loss: 18.829849243164062, Learning Rate: 0.01\n",
      "Epoch [5685/20000], Loss: 18.819915771484375, Learning Rate: 0.01\n",
      "Epoch [5686/20000], Loss: 18.81024169921875, Learning Rate: 0.01\n",
      "Epoch [5687/20000], Loss: 18.800506591796875, Learning Rate: 0.01\n",
      "Epoch [5688/20000], Loss: 18.7906494140625, Learning Rate: 0.01\n",
      "Epoch [5689/20000], Loss: 18.78082275390625, Learning Rate: 0.01\n",
      "Epoch [5690/20000], Loss: 18.770828247070312, Learning Rate: 0.01\n",
      "Epoch [5691/20000], Loss: 18.761016845703125, Learning Rate: 0.01\n",
      "Epoch [5692/20000], Loss: 18.751266479492188, Learning Rate: 0.01\n",
      "Epoch [5693/20000], Loss: 18.74151611328125, Learning Rate: 0.01\n",
      "Epoch [5694/20000], Loss: 18.731735229492188, Learning Rate: 0.01\n",
      "Epoch [5695/20000], Loss: 18.72174072265625, Learning Rate: 0.01\n",
      "Epoch [5696/20000], Loss: 18.71197509765625, Learning Rate: 0.01\n",
      "Epoch [5697/20000], Loss: 18.702194213867188, Learning Rate: 0.01\n",
      "Epoch [5698/20000], Loss: 18.69244384765625, Learning Rate: 0.01\n",
      "Epoch [5699/20000], Loss: 18.682510375976562, Learning Rate: 0.01\n",
      "Epoch [5700/20000], Loss: 18.672653198242188, Learning Rate: 0.01\n",
      "Epoch [5701/20000], Loss: 18.662887573242188, Learning Rate: 0.01\n",
      "Epoch [5702/20000], Loss: 18.653076171875, Learning Rate: 0.01\n",
      "Epoch [5703/20000], Loss: 18.64312744140625, Learning Rate: 0.01\n",
      "Epoch [5704/20000], Loss: 18.633316040039062, Learning Rate: 0.01\n",
      "Epoch [5705/20000], Loss: 18.623550415039062, Learning Rate: 0.01\n",
      "Epoch [5706/20000], Loss: 18.613815307617188, Learning Rate: 0.01\n",
      "Epoch [5707/20000], Loss: 18.6038818359375, Learning Rate: 0.01\n",
      "Epoch [5708/20000], Loss: 18.594192504882812, Learning Rate: 0.01\n",
      "Epoch [5709/20000], Loss: 18.58428955078125, Learning Rate: 0.01\n",
      "Epoch [5710/20000], Loss: 18.57440185546875, Learning Rate: 0.01\n",
      "Epoch [5711/20000], Loss: 18.564682006835938, Learning Rate: 0.01\n",
      "Epoch [5712/20000], Loss: 18.554779052734375, Learning Rate: 0.01\n",
      "Epoch [5713/20000], Loss: 18.545013427734375, Learning Rate: 0.01\n",
      "Epoch [5714/20000], Loss: 18.53509521484375, Learning Rate: 0.01\n",
      "Epoch [5715/20000], Loss: 18.5252685546875, Learning Rate: 0.01\n",
      "Epoch [5716/20000], Loss: 18.515518188476562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5717/20000], Loss: 18.505767822265625, Learning Rate: 0.01\n",
      "Epoch [5718/20000], Loss: 18.495681762695312, Learning Rate: 0.01\n",
      "Epoch [5719/20000], Loss: 18.485977172851562, Learning Rate: 0.01\n",
      "Epoch [5720/20000], Loss: 18.47607421875, Learning Rate: 0.01\n",
      "Epoch [5721/20000], Loss: 18.46624755859375, Learning Rate: 0.01\n",
      "Epoch [5722/20000], Loss: 18.456344604492188, Learning Rate: 0.01\n",
      "Epoch [5723/20000], Loss: 18.446578979492188, Learning Rate: 0.01\n",
      "Epoch [5724/20000], Loss: 18.436981201171875, Learning Rate: 0.01\n",
      "Epoch [5725/20000], Loss: 18.426986694335938, Learning Rate: 0.01\n",
      "Epoch [5726/20000], Loss: 18.417205810546875, Learning Rate: 0.01\n",
      "Epoch [5727/20000], Loss: 18.407455444335938, Learning Rate: 0.01\n",
      "Epoch [5728/20000], Loss: 18.397476196289062, Learning Rate: 0.01\n",
      "Epoch [5729/20000], Loss: 18.387588500976562, Learning Rate: 0.01\n",
      "Epoch [5730/20000], Loss: 18.377883911132812, Learning Rate: 0.01\n",
      "Epoch [5731/20000], Loss: 18.36798095703125, Learning Rate: 0.01\n",
      "Epoch [5732/20000], Loss: 18.35821533203125, Learning Rate: 0.01\n",
      "Epoch [5733/20000], Loss: 18.348358154296875, Learning Rate: 0.01\n",
      "Epoch [5734/20000], Loss: 18.338638305664062, Learning Rate: 0.01\n",
      "Epoch [5735/20000], Loss: 18.328811645507812, Learning Rate: 0.01\n",
      "Epoch [5736/20000], Loss: 18.318710327148438, Learning Rate: 0.01\n",
      "Epoch [5737/20000], Loss: 18.308975219726562, Learning Rate: 0.01\n",
      "Epoch [5738/20000], Loss: 18.299285888671875, Learning Rate: 0.01\n",
      "Epoch [5739/20000], Loss: 18.289398193359375, Learning Rate: 0.01\n",
      "Epoch [5740/20000], Loss: 18.27960205078125, Learning Rate: 0.01\n",
      "Epoch [5741/20000], Loss: 18.269607543945312, Learning Rate: 0.01\n",
      "Epoch [5742/20000], Loss: 18.259918212890625, Learning Rate: 0.01\n",
      "Epoch [5743/20000], Loss: 18.250045776367188, Learning Rate: 0.01\n",
      "Epoch [5744/20000], Loss: 18.240188598632812, Learning Rate: 0.01\n",
      "Epoch [5745/20000], Loss: 18.230392456054688, Learning Rate: 0.01\n",
      "Epoch [5746/20000], Loss: 18.220474243164062, Learning Rate: 0.01\n",
      "Epoch [5747/20000], Loss: 18.210739135742188, Learning Rate: 0.01\n",
      "Epoch [5748/20000], Loss: 18.201019287109375, Learning Rate: 0.01\n",
      "Epoch [5749/20000], Loss: 18.191177368164062, Learning Rate: 0.01\n",
      "Epoch [5750/20000], Loss: 18.181365966796875, Learning Rate: 0.01\n",
      "Epoch [5751/20000], Loss: 18.171279907226562, Learning Rate: 0.01\n",
      "Epoch [5752/20000], Loss: 18.161666870117188, Learning Rate: 0.01\n",
      "Epoch [5753/20000], Loss: 18.151824951171875, Learning Rate: 0.01\n",
      "Epoch [5754/20000], Loss: 18.14190673828125, Learning Rate: 0.01\n",
      "Epoch [5755/20000], Loss: 18.132247924804688, Learning Rate: 0.01\n",
      "Epoch [5756/20000], Loss: 18.122390747070312, Learning Rate: 0.01\n",
      "Epoch [5757/20000], Loss: 18.112564086914062, Learning Rate: 0.01\n",
      "Epoch [5758/20000], Loss: 18.102783203125, Learning Rate: 0.01\n",
      "Epoch [5759/20000], Loss: 18.092971801757812, Learning Rate: 0.01\n",
      "Epoch [5760/20000], Loss: 18.08319091796875, Learning Rate: 0.01\n",
      "Epoch [5761/20000], Loss: 18.073272705078125, Learning Rate: 0.01\n",
      "Epoch [5762/20000], Loss: 18.06365966796875, Learning Rate: 0.01\n",
      "Epoch [5763/20000], Loss: 18.053939819335938, Learning Rate: 0.01\n",
      "Epoch [5764/20000], Loss: 18.044235229492188, Learning Rate: 0.01\n",
      "Epoch [5765/20000], Loss: 18.034576416015625, Learning Rate: 0.01\n",
      "Epoch [5766/20000], Loss: 18.025177001953125, Learning Rate: 0.01\n",
      "Epoch [5767/20000], Loss: 18.015960693359375, Learning Rate: 0.01\n",
      "Epoch [5768/20000], Loss: 18.006912231445312, Learning Rate: 0.01\n",
      "Epoch [5769/20000], Loss: 17.998153686523438, Learning Rate: 0.01\n",
      "Epoch [5770/20000], Loss: 17.990234375, Learning Rate: 0.01\n",
      "Epoch [5771/20000], Loss: 17.983383178710938, Learning Rate: 0.01\n",
      "Epoch [5772/20000], Loss: 17.9779052734375, Learning Rate: 0.01\n",
      "Epoch [5773/20000], Loss: 17.974960327148438, Learning Rate: 0.01\n",
      "Epoch [5774/20000], Loss: 17.975784301757812, Learning Rate: 0.01\n",
      "Epoch [5775/20000], Loss: 17.982757568359375, Learning Rate: 0.01\n",
      "Epoch [5776/20000], Loss: 17.99951171875, Learning Rate: 0.01\n",
      "Epoch [5777/20000], Loss: 18.031570434570312, Learning Rate: 0.01\n",
      "Epoch [5778/20000], Loss: 18.088851928710938, Learning Rate: 0.01\n",
      "Epoch [5779/20000], Loss: 18.18597412109375, Learning Rate: 0.01\n",
      "Epoch [5780/20000], Loss: 18.347366333007812, Learning Rate: 0.01\n",
      "Epoch [5781/20000], Loss: 18.61236572265625, Learning Rate: 0.01\n",
      "Epoch [5782/20000], Loss: 19.041473388671875, Learning Rate: 0.01\n",
      "Epoch [5783/20000], Loss: 19.727676391601562, Learning Rate: 0.01\n",
      "Epoch [5784/20000], Loss: 20.80328369140625, Learning Rate: 0.01\n",
      "Epoch [5785/20000], Loss: 22.438278198242188, Learning Rate: 0.01\n",
      "Epoch [5786/20000], Loss: 24.7911376953125, Learning Rate: 0.01\n",
      "Epoch [5787/20000], Loss: 27.88397216796875, Learning Rate: 0.01\n",
      "Epoch [5788/20000], Loss: 31.315078735351562, Learning Rate: 0.01\n",
      "Epoch [5789/20000], Loss: 33.98602294921875, Learning Rate: 0.01\n",
      "Epoch [5790/20000], Loss: 34.180694580078125, Learning Rate: 0.01\n",
      "Epoch [5791/20000], Loss: 30.772018432617188, Learning Rate: 0.01\n",
      "Epoch [5792/20000], Loss: 24.797500610351562, Learning Rate: 0.01\n",
      "Epoch [5793/20000], Loss: 19.503875732421875, Learning Rate: 0.01\n",
      "Epoch [5794/20000], Loss: 17.761947631835938, Learning Rate: 0.01\n",
      "Epoch [5795/20000], Loss: 19.705795288085938, Learning Rate: 0.01\n",
      "Epoch [5796/20000], Loss: 22.937911987304688, Learning Rate: 0.01\n",
      "Epoch [5797/20000], Loss: 24.548492431640625, Learning Rate: 0.01\n",
      "Epoch [5798/20000], Loss: 23.23040771484375, Learning Rate: 0.01\n",
      "Epoch [5799/20000], Loss: 20.180435180664062, Learning Rate: 0.01\n",
      "Epoch [5800/20000], Loss: 17.942520141601562, Learning Rate: 0.01\n",
      "Epoch [5801/20000], Loss: 18.010116577148438, Learning Rate: 0.01\n",
      "Epoch [5802/20000], Loss: 19.670578002929688, Learning Rate: 0.01\n",
      "Epoch [5803/20000], Loss: 20.998992919921875, Learning Rate: 0.01\n",
      "Epoch [5804/20000], Loss: 20.705184936523438, Learning Rate: 0.01\n",
      "Epoch [5805/20000], Loss: 19.165084838867188, Learning Rate: 0.01\n",
      "Epoch [5806/20000], Loss: 17.836380004882812, Learning Rate: 0.01\n",
      "Epoch [5807/20000], Loss: 17.747207641601562, Learning Rate: 0.01\n",
      "Epoch [5808/20000], Loss: 18.618057250976562, Learning Rate: 0.01\n",
      "Epoch [5809/20000], Loss: 19.358505249023438, Learning Rate: 0.01\n",
      "Epoch [5810/20000], Loss: 19.20794677734375, Learning Rate: 0.01\n",
      "Epoch [5811/20000], Loss: 18.3721923828125, Learning Rate: 0.01\n",
      "Epoch [5812/20000], Loss: 17.666412353515625, Learning Rate: 0.01\n",
      "Epoch [5813/20000], Loss: 17.642120361328125, Learning Rate: 0.01\n",
      "Epoch [5814/20000], Loss: 18.120437622070312, Learning Rate: 0.01\n",
      "Epoch [5815/20000], Loss: 18.497207641601562, Learning Rate: 0.01\n",
      "Epoch [5816/20000], Loss: 18.381393432617188, Learning Rate: 0.01\n",
      "Epoch [5817/20000], Loss: 17.917572021484375, Learning Rate: 0.01\n",
      "Epoch [5818/20000], Loss: 17.550949096679688, Learning Rate: 0.01\n",
      "Epoch [5819/20000], Loss: 17.556549072265625, Learning Rate: 0.01\n",
      "Epoch [5820/20000], Loss: 17.816207885742188, Learning Rate: 0.01\n",
      "Epoch [5821/20000], Loss: 18.00341796875, Learning Rate: 0.01\n",
      "Epoch [5822/20000], Loss: 17.923538208007812, Learning Rate: 0.01\n",
      "Epoch [5823/20000], Loss: 17.667388916015625, Learning Rate: 0.01\n",
      "Epoch [5824/20000], Loss: 17.47174072265625, Learning Rate: 0.01\n",
      "Epoch [5825/20000], Loss: 17.475509643554688, Learning Rate: 0.01\n",
      "Epoch [5826/20000], Loss: 17.610580444335938, Learning Rate: 0.01\n",
      "Epoch [5827/20000], Loss: 17.705169677734375, Learning Rate: 0.01\n",
      "Epoch [5828/20000], Loss: 17.658828735351562, Learning Rate: 0.01\n",
      "Epoch [5829/20000], Loss: 17.518402099609375, Learning Rate: 0.01\n",
      "Epoch [5830/20000], Loss: 17.407394409179688, Learning Rate: 0.01\n",
      "Epoch [5831/20000], Loss: 17.400421142578125, Learning Rate: 0.01\n",
      "Epoch [5832/20000], Loss: 17.466033935546875, Learning Rate: 0.01\n",
      "Epoch [5833/20000], Loss: 17.515914916992188, Learning Rate: 0.01\n",
      "Epoch [5834/20000], Loss: 17.492889404296875, Learning Rate: 0.01\n",
      "Epoch [5835/20000], Loss: 17.416244506835938, Learning Rate: 0.01\n",
      "Epoch [5836/20000], Loss: 17.348648071289062, Learning Rate: 0.01\n",
      "Epoch [5837/20000], Loss: 17.333114624023438, Learning Rate: 0.01\n",
      "Epoch [5838/20000], Loss: 17.359909057617188, Learning Rate: 0.01\n",
      "Epoch [5839/20000], Loss: 17.385696411132812, Learning Rate: 0.01\n",
      "Epoch [5840/20000], Loss: 17.376510620117188, Learning Rate: 0.01\n",
      "Epoch [5841/20000], Loss: 17.335052490234375, Learning Rate: 0.01\n",
      "Epoch [5842/20000], Loss: 17.291427612304688, Learning Rate: 0.01\n",
      "Epoch [5843/20000], Loss: 17.272109985351562, Learning Rate: 0.01\n",
      "Epoch [5844/20000], Loss: 17.277740478515625, Learning Rate: 0.01\n",
      "Epoch [5845/20000], Loss: 17.289016723632812, Learning Rate: 0.01\n",
      "Epoch [5846/20000], Loss: 17.28533935546875, Learning Rate: 0.01\n",
      "Epoch [5847/20000], Loss: 17.263168334960938, Learning Rate: 0.01\n",
      "Epoch [5848/20000], Loss: 17.234619140625, Learning Rate: 0.01\n",
      "Epoch [5849/20000], Loss: 17.214706420898438, Learning Rate: 0.01\n",
      "Epoch [5850/20000], Loss: 17.209213256835938, Learning Rate: 0.01\n",
      "Epoch [5851/20000], Loss: 17.21063232421875, Learning Rate: 0.01\n",
      "Epoch [5852/20000], Loss: 17.208160400390625, Learning Rate: 0.01\n",
      "Epoch [5853/20000], Loss: 17.195663452148438, Learning Rate: 0.01\n",
      "Epoch [5854/20000], Loss: 17.176971435546875, Learning Rate: 0.01\n",
      "Epoch [5855/20000], Loss: 17.159255981445312, Learning Rate: 0.01\n",
      "Epoch [5856/20000], Loss: 17.148483276367188, Learning Rate: 0.01\n",
      "Epoch [5857/20000], Loss: 17.143356323242188, Learning Rate: 0.01\n",
      "Epoch [5858/20000], Loss: 17.1390380859375, Learning Rate: 0.01\n",
      "Epoch [5859/20000], Loss: 17.130996704101562, Learning Rate: 0.01\n",
      "Epoch [5860/20000], Loss: 17.1182861328125, Learning Rate: 0.01\n",
      "Epoch [5861/20000], Loss: 17.1036376953125, Learning Rate: 0.01\n",
      "Epoch [5862/20000], Loss: 17.091278076171875, Learning Rate: 0.01\n",
      "Epoch [5863/20000], Loss: 17.082427978515625, Learning Rate: 0.01\n",
      "Epoch [5864/20000], Loss: 17.07550048828125, Learning Rate: 0.01\n",
      "Epoch [5865/20000], Loss: 17.0682373046875, Learning Rate: 0.01\n",
      "Epoch [5866/20000], Loss: 17.058685302734375, Learning Rate: 0.01\n",
      "Epoch [5867/20000], Loss: 17.047103881835938, Learning Rate: 0.01\n",
      "Epoch [5868/20000], Loss: 17.03533935546875, Learning Rate: 0.01\n",
      "Epoch [5869/20000], Loss: 17.024612426757812, Learning Rate: 0.01\n",
      "Epoch [5870/20000], Loss: 17.015792846679688, Learning Rate: 0.01\n",
      "Epoch [5871/20000], Loss: 17.007644653320312, Learning Rate: 0.01\n",
      "Epoch [5872/20000], Loss: 16.999053955078125, Learning Rate: 0.01\n",
      "Epoch [5873/20000], Loss: 16.989410400390625, Learning Rate: 0.01\n",
      "Epoch [5874/20000], Loss: 16.97894287109375, Learning Rate: 0.01\n",
      "Epoch [5875/20000], Loss: 16.968460083007812, Learning Rate: 0.01\n",
      "Epoch [5876/20000], Loss: 16.958328247070312, Learning Rate: 0.01\n",
      "Epoch [5877/20000], Loss: 16.949050903320312, Learning Rate: 0.01\n",
      "Epoch [5878/20000], Loss: 16.9400634765625, Learning Rate: 0.01\n",
      "Epoch [5879/20000], Loss: 16.931167602539062, Learning Rate: 0.01\n",
      "Epoch [5880/20000], Loss: 16.921493530273438, Learning Rate: 0.01\n",
      "Epoch [5881/20000], Loss: 16.911636352539062, Learning Rate: 0.01\n",
      "Epoch [5882/20000], Loss: 16.901687622070312, Learning Rate: 0.01\n",
      "Epoch [5883/20000], Loss: 16.891860961914062, Learning Rate: 0.01\n",
      "Epoch [5884/20000], Loss: 16.882537841796875, Learning Rate: 0.01\n",
      "Epoch [5885/20000], Loss: 16.87322998046875, Learning Rate: 0.01\n",
      "Epoch [5886/20000], Loss: 16.863983154296875, Learning Rate: 0.01\n",
      "Epoch [5887/20000], Loss: 16.854629516601562, Learning Rate: 0.01\n",
      "Epoch [5888/20000], Loss: 16.844924926757812, Learning Rate: 0.01\n",
      "Epoch [5889/20000], Loss: 16.835174560546875, Learning Rate: 0.01\n",
      "Epoch [5890/20000], Loss: 16.825515747070312, Learning Rate: 0.01\n",
      "Epoch [5891/20000], Loss: 16.815948486328125, Learning Rate: 0.01\n",
      "Epoch [5892/20000], Loss: 16.806625366210938, Learning Rate: 0.01\n",
      "Epoch [5893/20000], Loss: 16.797149658203125, Learning Rate: 0.01\n",
      "Epoch [5894/20000], Loss: 16.78778076171875, Learning Rate: 0.01\n",
      "Epoch [5895/20000], Loss: 16.778350830078125, Learning Rate: 0.01\n",
      "Epoch [5896/20000], Loss: 16.76885986328125, Learning Rate: 0.01\n",
      "Epoch [5897/20000], Loss: 16.759048461914062, Learning Rate: 0.01\n",
      "Epoch [5898/20000], Loss: 16.749771118164062, Learning Rate: 0.01\n",
      "Epoch [5899/20000], Loss: 16.740158081054688, Learning Rate: 0.01\n",
      "Epoch [5900/20000], Loss: 16.7308349609375, Learning Rate: 0.01\n",
      "Epoch [5901/20000], Loss: 16.721099853515625, Learning Rate: 0.01\n",
      "Epoch [5902/20000], Loss: 16.711883544921875, Learning Rate: 0.01\n",
      "Epoch [5903/20000], Loss: 16.702392578125, Learning Rate: 0.01\n",
      "Epoch [5904/20000], Loss: 16.692886352539062, Learning Rate: 0.01\n",
      "Epoch [5905/20000], Loss: 16.683242797851562, Learning Rate: 0.01\n",
      "Epoch [5906/20000], Loss: 16.6737060546875, Learning Rate: 0.01\n",
      "Epoch [5907/20000], Loss: 16.664291381835938, Learning Rate: 0.01\n",
      "Epoch [5908/20000], Loss: 16.65478515625, Learning Rate: 0.01\n",
      "Epoch [5909/20000], Loss: 16.645355224609375, Learning Rate: 0.01\n",
      "Epoch [5910/20000], Loss: 16.635787963867188, Learning Rate: 0.01\n",
      "Epoch [5911/20000], Loss: 16.626480102539062, Learning Rate: 0.01\n",
      "Epoch [5912/20000], Loss: 16.61688232421875, Learning Rate: 0.01\n",
      "Epoch [5913/20000], Loss: 16.607437133789062, Learning Rate: 0.01\n",
      "Epoch [5914/20000], Loss: 16.597793579101562, Learning Rate: 0.01\n",
      "Epoch [5915/20000], Loss: 16.588409423828125, Learning Rate: 0.01\n",
      "Epoch [5916/20000], Loss: 16.578933715820312, Learning Rate: 0.01\n",
      "Epoch [5917/20000], Loss: 16.569610595703125, Learning Rate: 0.01\n",
      "Epoch [5918/20000], Loss: 16.559967041015625, Learning Rate: 0.01\n",
      "Epoch [5919/20000], Loss: 16.550460815429688, Learning Rate: 0.01\n",
      "Epoch [5920/20000], Loss: 16.541015625, Learning Rate: 0.01\n",
      "Epoch [5921/20000], Loss: 16.531341552734375, Learning Rate: 0.01\n",
      "Epoch [5922/20000], Loss: 16.522079467773438, Learning Rate: 0.01\n",
      "Epoch [5923/20000], Loss: 16.512466430664062, Learning Rate: 0.01\n",
      "Epoch [5924/20000], Loss: 16.50299072265625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5925/20000], Loss: 16.493560791015625, Learning Rate: 0.01\n",
      "Epoch [5926/20000], Loss: 16.48419189453125, Learning Rate: 0.01\n",
      "Epoch [5927/20000], Loss: 16.474563598632812, Learning Rate: 0.01\n",
      "Epoch [5928/20000], Loss: 16.4652099609375, Learning Rate: 0.01\n",
      "Epoch [5929/20000], Loss: 16.455734252929688, Learning Rate: 0.01\n",
      "Epoch [5930/20000], Loss: 16.446243286132812, Learning Rate: 0.01\n",
      "Epoch [5931/20000], Loss: 16.436614990234375, Learning Rate: 0.01\n",
      "Epoch [5932/20000], Loss: 16.427352905273438, Learning Rate: 0.01\n",
      "Epoch [5933/20000], Loss: 16.41778564453125, Learning Rate: 0.01\n",
      "Epoch [5934/20000], Loss: 16.408233642578125, Learning Rate: 0.01\n",
      "Epoch [5935/20000], Loss: 16.39874267578125, Learning Rate: 0.01\n",
      "Epoch [5936/20000], Loss: 16.389205932617188, Learning Rate: 0.01\n",
      "Epoch [5937/20000], Loss: 16.37982177734375, Learning Rate: 0.01\n",
      "Epoch [5938/20000], Loss: 16.370590209960938, Learning Rate: 0.01\n",
      "Epoch [5939/20000], Loss: 16.360855102539062, Learning Rate: 0.01\n",
      "Epoch [5940/20000], Loss: 16.351409912109375, Learning Rate: 0.01\n",
      "Epoch [5941/20000], Loss: 16.341934204101562, Learning Rate: 0.01\n",
      "Epoch [5942/20000], Loss: 16.332366943359375, Learning Rate: 0.01\n",
      "Epoch [5943/20000], Loss: 16.322891235351562, Learning Rate: 0.01\n",
      "Epoch [5944/20000], Loss: 16.313613891601562, Learning Rate: 0.01\n",
      "Epoch [5945/20000], Loss: 16.303985595703125, Learning Rate: 0.01\n",
      "Epoch [5946/20000], Loss: 16.294403076171875, Learning Rate: 0.01\n",
      "Epoch [5947/20000], Loss: 16.28509521484375, Learning Rate: 0.01\n",
      "Epoch [5948/20000], Loss: 16.27569580078125, Learning Rate: 0.01\n",
      "Epoch [5949/20000], Loss: 16.266189575195312, Learning Rate: 0.01\n",
      "Epoch [5950/20000], Loss: 16.256439208984375, Learning Rate: 0.01\n",
      "Epoch [5951/20000], Loss: 16.247238159179688, Learning Rate: 0.01\n",
      "Epoch [5952/20000], Loss: 16.237579345703125, Learning Rate: 0.01\n",
      "Epoch [5953/20000], Loss: 16.22821044921875, Learning Rate: 0.01\n",
      "Epoch [5954/20000], Loss: 16.218856811523438, Learning Rate: 0.01\n",
      "Epoch [5955/20000], Loss: 16.209304809570312, Learning Rate: 0.01\n",
      "Epoch [5956/20000], Loss: 16.199813842773438, Learning Rate: 0.01\n",
      "Epoch [5957/20000], Loss: 16.190139770507812, Learning Rate: 0.01\n",
      "Epoch [5958/20000], Loss: 16.180801391601562, Learning Rate: 0.01\n",
      "Epoch [5959/20000], Loss: 16.1712646484375, Learning Rate: 0.01\n",
      "Epoch [5960/20000], Loss: 16.161911010742188, Learning Rate: 0.01\n",
      "Epoch [5961/20000], Loss: 16.15252685546875, Learning Rate: 0.01\n",
      "Epoch [5962/20000], Loss: 16.142776489257812, Learning Rate: 0.01\n",
      "Epoch [5963/20000], Loss: 16.133438110351562, Learning Rate: 0.01\n",
      "Epoch [5964/20000], Loss: 16.123992919921875, Learning Rate: 0.01\n",
      "Epoch [5965/20000], Loss: 16.114501953125, Learning Rate: 0.01\n",
      "Epoch [5966/20000], Loss: 16.1048583984375, Learning Rate: 0.01\n",
      "Epoch [5967/20000], Loss: 16.095474243164062, Learning Rate: 0.01\n",
      "Epoch [5968/20000], Loss: 16.086105346679688, Learning Rate: 0.01\n",
      "Epoch [5969/20000], Loss: 16.076507568359375, Learning Rate: 0.01\n",
      "Epoch [5970/20000], Loss: 16.066970825195312, Learning Rate: 0.01\n",
      "Epoch [5971/20000], Loss: 16.05755615234375, Learning Rate: 0.01\n",
      "Epoch [5972/20000], Loss: 16.048202514648438, Learning Rate: 0.01\n",
      "Epoch [5973/20000], Loss: 16.038681030273438, Learning Rate: 0.01\n",
      "Epoch [5974/20000], Loss: 16.029052734375, Learning Rate: 0.01\n",
      "Epoch [5975/20000], Loss: 16.019607543945312, Learning Rate: 0.01\n",
      "Epoch [5976/20000], Loss: 16.010238647460938, Learning Rate: 0.01\n",
      "Epoch [5977/20000], Loss: 16.000747680664062, Learning Rate: 0.01\n",
      "Epoch [5978/20000], Loss: 15.99127197265625, Learning Rate: 0.01\n",
      "Epoch [5979/20000], Loss: 15.981964111328125, Learning Rate: 0.01\n",
      "Epoch [5980/20000], Loss: 15.972198486328125, Learning Rate: 0.01\n",
      "Epoch [5981/20000], Loss: 15.962783813476562, Learning Rate: 0.01\n",
      "Epoch [5982/20000], Loss: 15.953399658203125, Learning Rate: 0.01\n",
      "Epoch [5983/20000], Loss: 15.94390869140625, Learning Rate: 0.01\n",
      "Epoch [5984/20000], Loss: 15.934616088867188, Learning Rate: 0.01\n",
      "Epoch [5985/20000], Loss: 15.92510986328125, Learning Rate: 0.01\n",
      "Epoch [5986/20000], Loss: 15.915298461914062, Learning Rate: 0.01\n",
      "Epoch [5987/20000], Loss: 15.906036376953125, Learning Rate: 0.01\n",
      "Epoch [5988/20000], Loss: 15.8966064453125, Learning Rate: 0.01\n",
      "Epoch [5989/20000], Loss: 15.887115478515625, Learning Rate: 0.01\n",
      "Epoch [5990/20000], Loss: 15.87750244140625, Learning Rate: 0.01\n",
      "Epoch [5991/20000], Loss: 15.868133544921875, Learning Rate: 0.01\n",
      "Epoch [5992/20000], Loss: 15.858734130859375, Learning Rate: 0.01\n",
      "Epoch [5993/20000], Loss: 15.849258422851562, Learning Rate: 0.01\n",
      "Epoch [5994/20000], Loss: 15.839767456054688, Learning Rate: 0.01\n",
      "Epoch [5995/20000], Loss: 15.830093383789062, Learning Rate: 0.01\n",
      "Epoch [5996/20000], Loss: 15.820877075195312, Learning Rate: 0.01\n",
      "Epoch [5997/20000], Loss: 15.81121826171875, Learning Rate: 0.01\n",
      "Epoch [5998/20000], Loss: 15.801864624023438, Learning Rate: 0.01\n",
      "Epoch [5999/20000], Loss: 15.792282104492188, Learning Rate: 0.01\n",
      "Epoch [6000/20000], Loss: 15.782791137695312, Learning Rate: 0.01\n",
      "Epoch [6001/20000], Loss: 15.773422241210938, Learning Rate: 0.01\n",
      "Epoch [6002/20000], Loss: 15.763961791992188, Learning Rate: 0.01\n",
      "Epoch [6003/20000], Loss: 15.754547119140625, Learning Rate: 0.01\n",
      "Epoch [6004/20000], Loss: 15.7449951171875, Learning Rate: 0.01\n",
      "Epoch [6005/20000], Loss: 15.735733032226562, Learning Rate: 0.01\n",
      "Epoch [6006/20000], Loss: 15.726211547851562, Learning Rate: 0.01\n",
      "Epoch [6007/20000], Loss: 15.716659545898438, Learning Rate: 0.01\n",
      "Epoch [6008/20000], Loss: 15.707244873046875, Learning Rate: 0.01\n",
      "Epoch [6009/20000], Loss: 15.697799682617188, Learning Rate: 0.01\n",
      "Epoch [6010/20000], Loss: 15.688247680664062, Learning Rate: 0.01\n",
      "Epoch [6011/20000], Loss: 15.678741455078125, Learning Rate: 0.01\n",
      "Epoch [6012/20000], Loss: 15.66943359375, Learning Rate: 0.01\n",
      "Epoch [6013/20000], Loss: 15.659866333007812, Learning Rate: 0.01\n",
      "Epoch [6014/20000], Loss: 15.650482177734375, Learning Rate: 0.01\n",
      "Epoch [6015/20000], Loss: 15.6409912109375, Learning Rate: 0.01\n",
      "Epoch [6016/20000], Loss: 15.63153076171875, Learning Rate: 0.01\n",
      "Epoch [6017/20000], Loss: 15.62188720703125, Learning Rate: 0.01\n",
      "Epoch [6018/20000], Loss: 15.612472534179688, Learning Rate: 0.01\n",
      "Epoch [6019/20000], Loss: 15.603073120117188, Learning Rate: 0.01\n",
      "Epoch [6020/20000], Loss: 15.593521118164062, Learning Rate: 0.01\n",
      "Epoch [6021/20000], Loss: 15.584182739257812, Learning Rate: 0.01\n",
      "Epoch [6022/20000], Loss: 15.574798583984375, Learning Rate: 0.01\n",
      "Epoch [6023/20000], Loss: 15.565170288085938, Learning Rate: 0.01\n",
      "Epoch [6024/20000], Loss: 15.555679321289062, Learning Rate: 0.01\n",
      "Epoch [6025/20000], Loss: 15.546340942382812, Learning Rate: 0.01\n",
      "Epoch [6026/20000], Loss: 15.536773681640625, Learning Rate: 0.01\n",
      "Epoch [6027/20000], Loss: 15.527328491210938, Learning Rate: 0.01\n",
      "Epoch [6028/20000], Loss: 15.51800537109375, Learning Rate: 0.01\n",
      "Epoch [6029/20000], Loss: 15.5084228515625, Learning Rate: 0.01\n",
      "Epoch [6030/20000], Loss: 15.49896240234375, Learning Rate: 0.01\n",
      "Epoch [6031/20000], Loss: 15.489517211914062, Learning Rate: 0.01\n",
      "Epoch [6032/20000], Loss: 15.4801025390625, Learning Rate: 0.01\n",
      "Epoch [6033/20000], Loss: 15.470550537109375, Learning Rate: 0.01\n",
      "Epoch [6034/20000], Loss: 15.461273193359375, Learning Rate: 0.01\n",
      "Epoch [6035/20000], Loss: 15.451675415039062, Learning Rate: 0.01\n",
      "Epoch [6036/20000], Loss: 15.442276000976562, Learning Rate: 0.01\n",
      "Epoch [6037/20000], Loss: 15.432907104492188, Learning Rate: 0.01\n",
      "Epoch [6038/20000], Loss: 15.423583984375, Learning Rate: 0.01\n",
      "Epoch [6039/20000], Loss: 15.4139404296875, Learning Rate: 0.01\n",
      "Epoch [6040/20000], Loss: 15.404632568359375, Learning Rate: 0.01\n",
      "Epoch [6041/20000], Loss: 15.395294189453125, Learning Rate: 0.01\n",
      "Epoch [6042/20000], Loss: 15.386260986328125, Learning Rate: 0.01\n",
      "Epoch [6043/20000], Loss: 15.376998901367188, Learning Rate: 0.01\n",
      "Epoch [6044/20000], Loss: 15.368026733398438, Learning Rate: 0.01\n",
      "Epoch [6045/20000], Loss: 15.359344482421875, Learning Rate: 0.01\n",
      "Epoch [6046/20000], Loss: 15.351226806640625, Learning Rate: 0.01\n",
      "Epoch [6047/20000], Loss: 15.343887329101562, Learning Rate: 0.01\n",
      "Epoch [6048/20000], Loss: 15.337753295898438, Learning Rate: 0.01\n",
      "Epoch [6049/20000], Loss: 15.333465576171875, Learning Rate: 0.01\n",
      "Epoch [6050/20000], Loss: 15.332260131835938, Learning Rate: 0.01\n",
      "Epoch [6051/20000], Loss: 15.336334228515625, Learning Rate: 0.01\n",
      "Epoch [6052/20000], Loss: 15.349090576171875, Learning Rate: 0.01\n",
      "Epoch [6053/20000], Loss: 15.375885009765625, Learning Rate: 0.01\n",
      "Epoch [6054/20000], Loss: 15.426406860351562, Learning Rate: 0.01\n",
      "Epoch [6055/20000], Loss: 15.515640258789062, Learning Rate: 0.01\n",
      "Epoch [6056/20000], Loss: 15.670166015625, Learning Rate: 0.01\n",
      "Epoch [6057/20000], Loss: 15.932510375976562, Learning Rate: 0.01\n",
      "Epoch [6058/20000], Loss: 16.373046875, Learning Rate: 0.01\n",
      "Epoch [6059/20000], Loss: 17.104446411132812, Learning Rate: 0.01\n",
      "Epoch [6060/20000], Loss: 18.29840087890625, Learning Rate: 0.01\n",
      "Epoch [6061/20000], Loss: 20.190521240234375, Learning Rate: 0.01\n",
      "Epoch [6062/20000], Loss: 23.047256469726562, Learning Rate: 0.01\n",
      "Epoch [6063/20000], Loss: 26.990310668945312, Learning Rate: 0.01\n",
      "Epoch [6064/20000], Loss: 31.626754760742188, Learning Rate: 0.01\n",
      "Epoch [6065/20000], Loss: 35.47541809082031, Learning Rate: 0.01\n",
      "Epoch [6066/20000], Loss: 36.0479736328125, Learning Rate: 0.01\n",
      "Epoch [6067/20000], Loss: 31.471923828125, Learning Rate: 0.01\n",
      "Epoch [6068/20000], Loss: 23.359619140625, Learning Rate: 0.01\n",
      "Epoch [6069/20000], Loss: 16.655014038085938, Learning Rate: 0.01\n",
      "Epoch [6070/20000], Loss: 15.3631591796875, Learning Rate: 0.01\n",
      "Epoch [6071/20000], Loss: 18.898269653320312, Learning Rate: 0.01\n",
      "Epoch [6072/20000], Loss: 23.084823608398438, Learning Rate: 0.01\n",
      "Epoch [6073/20000], Loss: 23.865509033203125, Learning Rate: 0.01\n",
      "Epoch [6074/20000], Loss: 20.588607788085938, Learning Rate: 0.01\n",
      "Epoch [6075/20000], Loss: 16.45330810546875, Learning Rate: 0.01\n",
      "Epoch [6076/20000], Loss: 15.09765625, Learning Rate: 0.01\n",
      "Epoch [6077/20000], Loss: 16.94580078125, Learning Rate: 0.01\n",
      "Epoch [6078/20000], Loss: 19.305099487304688, Learning Rate: 0.01\n",
      "Epoch [6079/20000], Loss: 19.480331420898438, Learning Rate: 0.01\n",
      "Epoch [6080/20000], Loss: 17.4112548828125, Learning Rate: 0.01\n",
      "Epoch [6081/20000], Loss: 15.356643676757812, Learning Rate: 0.01\n",
      "Epoch [6082/20000], Loss: 15.203689575195312, Learning Rate: 0.01\n",
      "Epoch [6083/20000], Loss: 16.553207397460938, Learning Rate: 0.01\n",
      "Epoch [6084/20000], Loss: 17.548416137695312, Learning Rate: 0.01\n",
      "Epoch [6085/20000], Loss: 17.053497314453125, Learning Rate: 0.01\n",
      "Epoch [6086/20000], Loss: 15.7110595703125, Learning Rate: 0.01\n",
      "Epoch [6087/20000], Loss: 14.962203979492188, Learning Rate: 0.01\n",
      "Epoch [6088/20000], Loss: 15.364974975585938, Learning Rate: 0.01\n",
      "Epoch [6089/20000], Loss: 16.164108276367188, Learning Rate: 0.01\n",
      "Epoch [6090/20000], Loss: 16.33770751953125, Learning Rate: 0.01\n",
      "Epoch [6091/20000], Loss: 15.725067138671875, Learning Rate: 0.01\n",
      "Epoch [6092/20000], Loss: 15.0413818359375, Learning Rate: 0.01\n",
      "Epoch [6093/20000], Loss: 14.951873779296875, Learning Rate: 0.01\n",
      "Epoch [6094/20000], Loss: 15.368820190429688, Learning Rate: 0.01\n",
      "Epoch [6095/20000], Loss: 15.690597534179688, Learning Rate: 0.01\n",
      "Epoch [6096/20000], Loss: 15.535079956054688, Learning Rate: 0.01\n",
      "Epoch [6097/20000], Loss: 15.103805541992188, Learning Rate: 0.01\n",
      "Epoch [6098/20000], Loss: 14.8564453125, Learning Rate: 0.01\n",
      "Epoch [6099/20000], Loss: 14.974578857421875, Learning Rate: 0.01\n",
      "Epoch [6100/20000], Loss: 15.222991943359375, Learning Rate: 0.01\n",
      "Epoch [6101/20000], Loss: 15.277938842773438, Learning Rate: 0.01\n",
      "Epoch [6102/20000], Loss: 15.083206176757812, Learning Rate: 0.01\n",
      "Epoch [6103/20000], Loss: 14.856964111328125, Learning Rate: 0.01\n",
      "Epoch [6104/20000], Loss: 14.81134033203125, Learning Rate: 0.01\n",
      "Epoch [6105/20000], Loss: 14.930953979492188, Learning Rate: 0.01\n",
      "Epoch [6106/20000], Loss: 15.034835815429688, Learning Rate: 0.01\n",
      "Epoch [6107/20000], Loss: 14.993637084960938, Learning Rate: 0.01\n",
      "Epoch [6108/20000], Loss: 14.855010986328125, Learning Rate: 0.01\n",
      "Epoch [6109/20000], Loss: 14.75775146484375, Learning Rate: 0.01\n",
      "Epoch [6110/20000], Loss: 14.773483276367188, Learning Rate: 0.01\n",
      "Epoch [6111/20000], Loss: 14.845626831054688, Learning Rate: 0.01\n",
      "Epoch [6112/20000], Loss: 14.872116088867188, Learning Rate: 0.01\n",
      "Epoch [6113/20000], Loss: 14.818161010742188, Learning Rate: 0.01\n",
      "Epoch [6114/20000], Loss: 14.736831665039062, Learning Rate: 0.01\n",
      "Epoch [6115/20000], Loss: 14.69940185546875, Learning Rate: 0.01\n",
      "Epoch [6116/20000], Loss: 14.72015380859375, Learning Rate: 0.01\n",
      "Epoch [6117/20000], Loss: 14.753265380859375, Learning Rate: 0.01\n",
      "Epoch [6118/20000], Loss: 14.750106811523438, Learning Rate: 0.01\n",
      "Epoch [6119/20000], Loss: 14.70751953125, Learning Rate: 0.01\n",
      "Epoch [6120/20000], Loss: 14.66241455078125, Learning Rate: 0.01\n",
      "Epoch [6121/20000], Loss: 14.646774291992188, Learning Rate: 0.01\n",
      "Epoch [6122/20000], Loss: 14.658645629882812, Learning Rate: 0.01\n",
      "Epoch [6123/20000], Loss: 14.669677734375, Learning Rate: 0.01\n",
      "Epoch [6124/20000], Loss: 14.658462524414062, Learning Rate: 0.01\n",
      "Epoch [6125/20000], Loss: 14.628997802734375, Learning Rate: 0.01\n",
      "Epoch [6126/20000], Loss: 14.602386474609375, Learning Rate: 0.01\n",
      "Epoch [6127/20000], Loss: 14.593017578125, Learning Rate: 0.01\n",
      "Epoch [6128/20000], Loss: 14.596389770507812, Learning Rate: 0.01\n",
      "Epoch [6129/20000], Loss: 14.59710693359375, Learning Rate: 0.01\n",
      "Epoch [6130/20000], Loss: 14.585052490234375, Learning Rate: 0.01\n",
      "Epoch [6131/20000], Loss: 14.564544677734375, Learning Rate: 0.01\n",
      "Epoch [6132/20000], Loss: 14.54669189453125, Learning Rate: 0.01\n",
      "Epoch [6133/20000], Loss: 14.538162231445312, Learning Rate: 0.01\n",
      "Epoch [6134/20000], Loss: 14.535919189453125, Learning Rate: 0.01\n",
      "Epoch [6135/20000], Loss: 14.531692504882812, Learning Rate: 0.01\n",
      "Epoch [6136/20000], Loss: 14.521163940429688, Learning Rate: 0.01\n",
      "Epoch [6137/20000], Loss: 14.506072998046875, Learning Rate: 0.01\n",
      "Epoch [6138/20000], Loss: 14.492294311523438, Learning Rate: 0.01\n",
      "Epoch [6139/20000], Loss: 14.4832763671875, Learning Rate: 0.01\n",
      "Epoch [6140/20000], Loss: 14.477920532226562, Learning Rate: 0.01\n",
      "Epoch [6141/20000], Loss: 14.471572875976562, Learning Rate: 0.01\n",
      "Epoch [6142/20000], Loss: 14.46197509765625, Learning Rate: 0.01\n",
      "Epoch [6143/20000], Loss: 14.449874877929688, Learning Rate: 0.01\n",
      "Epoch [6144/20000], Loss: 14.438156127929688, Learning Rate: 0.01\n",
      "Epoch [6145/20000], Loss: 14.428848266601562, Learning Rate: 0.01\n",
      "Epoch [6146/20000], Loss: 14.42156982421875, Learning Rate: 0.01\n",
      "Epoch [6147/20000], Loss: 14.414199829101562, Learning Rate: 0.01\n",
      "Epoch [6148/20000], Loss: 14.405288696289062, Learning Rate: 0.01\n",
      "Epoch [6149/20000], Loss: 14.394638061523438, Learning Rate: 0.01\n",
      "Epoch [6150/20000], Loss: 14.384033203125, Learning Rate: 0.01\n",
      "Epoch [6151/20000], Loss: 14.3746337890625, Learning Rate: 0.01\n",
      "Epoch [6152/20000], Loss: 14.36627197265625, Learning Rate: 0.01\n",
      "Epoch [6153/20000], Loss: 14.358062744140625, Learning Rate: 0.01\n",
      "Epoch [6154/20000], Loss: 14.349533081054688, Learning Rate: 0.01\n",
      "Epoch [6155/20000], Loss: 14.339981079101562, Learning Rate: 0.01\n",
      "Epoch [6156/20000], Loss: 14.330108642578125, Learning Rate: 0.01\n",
      "Epoch [6157/20000], Loss: 14.320526123046875, Learning Rate: 0.01\n",
      "Epoch [6158/20000], Loss: 14.311813354492188, Learning Rate: 0.01\n",
      "Epoch [6159/20000], Loss: 14.3033447265625, Learning Rate: 0.01\n",
      "Epoch [6160/20000], Loss: 14.294525146484375, Learning Rate: 0.01\n",
      "Epoch [6161/20000], Loss: 14.285568237304688, Learning Rate: 0.01\n",
      "Epoch [6162/20000], Loss: 14.276046752929688, Learning Rate: 0.01\n",
      "Epoch [6163/20000], Loss: 14.2667236328125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6164/20000], Loss: 14.257553100585938, Learning Rate: 0.01\n",
      "Epoch [6165/20000], Loss: 14.248855590820312, Learning Rate: 0.01\n",
      "Epoch [6166/20000], Loss: 14.240097045898438, Learning Rate: 0.01\n",
      "Epoch [6167/20000], Loss: 14.231094360351562, Learning Rate: 0.01\n",
      "Epoch [6168/20000], Loss: 14.2220458984375, Learning Rate: 0.01\n",
      "Epoch [6169/20000], Loss: 14.212799072265625, Learning Rate: 0.01\n",
      "Epoch [6170/20000], Loss: 14.203720092773438, Learning Rate: 0.01\n",
      "Epoch [6171/20000], Loss: 14.194580078125, Learning Rate: 0.01\n",
      "Epoch [6172/20000], Loss: 14.185867309570312, Learning Rate: 0.01\n",
      "Epoch [6173/20000], Loss: 14.176834106445312, Learning Rate: 0.01\n",
      "Epoch [6174/20000], Loss: 14.167861938476562, Learning Rate: 0.01\n",
      "Epoch [6175/20000], Loss: 14.158920288085938, Learning Rate: 0.01\n",
      "Epoch [6176/20000], Loss: 14.14971923828125, Learning Rate: 0.01\n",
      "Epoch [6177/20000], Loss: 14.140640258789062, Learning Rate: 0.01\n",
      "Epoch [6178/20000], Loss: 14.131637573242188, Learning Rate: 0.01\n",
      "Epoch [6179/20000], Loss: 14.12274169921875, Learning Rate: 0.01\n",
      "Epoch [6180/20000], Loss: 14.113784790039062, Learning Rate: 0.01\n",
      "Epoch [6181/20000], Loss: 14.1048583984375, Learning Rate: 0.01\n",
      "Epoch [6182/20000], Loss: 14.095993041992188, Learning Rate: 0.01\n",
      "Epoch [6183/20000], Loss: 14.086959838867188, Learning Rate: 0.01\n",
      "Epoch [6184/20000], Loss: 14.077926635742188, Learning Rate: 0.01\n",
      "Epoch [6185/20000], Loss: 14.06884765625, Learning Rate: 0.01\n",
      "Epoch [6186/20000], Loss: 14.059829711914062, Learning Rate: 0.01\n",
      "Epoch [6187/20000], Loss: 14.051132202148438, Learning Rate: 0.01\n",
      "Epoch [6188/20000], Loss: 14.041824340820312, Learning Rate: 0.01\n",
      "Epoch [6189/20000], Loss: 14.033126831054688, Learning Rate: 0.01\n",
      "Epoch [6190/20000], Loss: 14.023941040039062, Learning Rate: 0.01\n",
      "Epoch [6191/20000], Loss: 14.01483154296875, Learning Rate: 0.01\n",
      "Epoch [6192/20000], Loss: 14.006027221679688, Learning Rate: 0.01\n",
      "Epoch [6193/20000], Loss: 13.996978759765625, Learning Rate: 0.01\n",
      "Epoch [6194/20000], Loss: 13.988037109375, Learning Rate: 0.01\n",
      "Epoch [6195/20000], Loss: 13.979095458984375, Learning Rate: 0.01\n",
      "Epoch [6196/20000], Loss: 13.970123291015625, Learning Rate: 0.01\n",
      "Epoch [6197/20000], Loss: 13.961166381835938, Learning Rate: 0.01\n",
      "Epoch [6198/20000], Loss: 13.952239990234375, Learning Rate: 0.01\n",
      "Epoch [6199/20000], Loss: 13.9432373046875, Learning Rate: 0.01\n",
      "Epoch [6200/20000], Loss: 13.934326171875, Learning Rate: 0.01\n",
      "Epoch [6201/20000], Loss: 13.925445556640625, Learning Rate: 0.01\n",
      "Epoch [6202/20000], Loss: 13.916244506835938, Learning Rate: 0.01\n",
      "Epoch [6203/20000], Loss: 13.907516479492188, Learning Rate: 0.01\n",
      "Epoch [6204/20000], Loss: 13.898361206054688, Learning Rate: 0.01\n",
      "Epoch [6205/20000], Loss: 13.889450073242188, Learning Rate: 0.01\n",
      "Epoch [6206/20000], Loss: 13.880401611328125, Learning Rate: 0.01\n",
      "Epoch [6207/20000], Loss: 13.871490478515625, Learning Rate: 0.01\n",
      "Epoch [6208/20000], Loss: 13.862533569335938, Learning Rate: 0.01\n",
      "Epoch [6209/20000], Loss: 13.853530883789062, Learning Rate: 0.01\n",
      "Epoch [6210/20000], Loss: 13.844650268554688, Learning Rate: 0.01\n",
      "Epoch [6211/20000], Loss: 13.83575439453125, Learning Rate: 0.01\n",
      "Epoch [6212/20000], Loss: 13.82659912109375, Learning Rate: 0.01\n",
      "Epoch [6213/20000], Loss: 13.817825317382812, Learning Rate: 0.01\n",
      "Epoch [6214/20000], Loss: 13.808761596679688, Learning Rate: 0.01\n",
      "Epoch [6215/20000], Loss: 13.799957275390625, Learning Rate: 0.01\n",
      "Epoch [6216/20000], Loss: 13.790939331054688, Learning Rate: 0.01\n",
      "Epoch [6217/20000], Loss: 13.781997680664062, Learning Rate: 0.01\n",
      "Epoch [6218/20000], Loss: 13.773056030273438, Learning Rate: 0.01\n",
      "Epoch [6219/20000], Loss: 13.7640380859375, Learning Rate: 0.01\n",
      "Epoch [6220/20000], Loss: 13.755096435546875, Learning Rate: 0.01\n",
      "Epoch [6221/20000], Loss: 13.746078491210938, Learning Rate: 0.01\n",
      "Epoch [6222/20000], Loss: 13.737136840820312, Learning Rate: 0.01\n",
      "Epoch [6223/20000], Loss: 13.728057861328125, Learning Rate: 0.01\n",
      "Epoch [6224/20000], Loss: 13.7193603515625, Learning Rate: 0.01\n",
      "Epoch [6225/20000], Loss: 13.710311889648438, Learning Rate: 0.01\n",
      "Epoch [6226/20000], Loss: 13.701339721679688, Learning Rate: 0.01\n",
      "Epoch [6227/20000], Loss: 13.692306518554688, Learning Rate: 0.01\n",
      "Epoch [6228/20000], Loss: 13.683486938476562, Learning Rate: 0.01\n",
      "Epoch [6229/20000], Loss: 13.674530029296875, Learning Rate: 0.01\n",
      "Epoch [6230/20000], Loss: 13.665573120117188, Learning Rate: 0.01\n",
      "Epoch [6231/20000], Loss: 13.656661987304688, Learning Rate: 0.01\n",
      "Epoch [6232/20000], Loss: 13.647720336914062, Learning Rate: 0.01\n",
      "Epoch [6233/20000], Loss: 13.638671875, Learning Rate: 0.01\n",
      "Epoch [6234/20000], Loss: 13.629837036132812, Learning Rate: 0.01\n",
      "Epoch [6235/20000], Loss: 13.620880126953125, Learning Rate: 0.01\n",
      "Epoch [6236/20000], Loss: 13.61187744140625, Learning Rate: 0.01\n",
      "Epoch [6237/20000], Loss: 13.602874755859375, Learning Rate: 0.01\n",
      "Epoch [6238/20000], Loss: 13.593917846679688, Learning Rate: 0.01\n",
      "Epoch [6239/20000], Loss: 13.58502197265625, Learning Rate: 0.01\n",
      "Epoch [6240/20000], Loss: 13.576095581054688, Learning Rate: 0.01\n",
      "Epoch [6241/20000], Loss: 13.567153930664062, Learning Rate: 0.01\n",
      "Epoch [6242/20000], Loss: 13.558197021484375, Learning Rate: 0.01\n",
      "Epoch [6243/20000], Loss: 13.54937744140625, Learning Rate: 0.01\n",
      "Epoch [6244/20000], Loss: 13.540451049804688, Learning Rate: 0.01\n",
      "Epoch [6245/20000], Loss: 13.531524658203125, Learning Rate: 0.01\n",
      "Epoch [6246/20000], Loss: 13.522537231445312, Learning Rate: 0.01\n",
      "Epoch [6247/20000], Loss: 13.5135498046875, Learning Rate: 0.01\n",
      "Epoch [6248/20000], Loss: 13.504638671875, Learning Rate: 0.01\n",
      "Epoch [6249/20000], Loss: 13.495773315429688, Learning Rate: 0.01\n",
      "Epoch [6250/20000], Loss: 13.486831665039062, Learning Rate: 0.01\n",
      "Epoch [6251/20000], Loss: 13.47802734375, Learning Rate: 0.01\n",
      "Epoch [6252/20000], Loss: 13.468948364257812, Learning Rate: 0.01\n",
      "Epoch [6253/20000], Loss: 13.4599609375, Learning Rate: 0.01\n",
      "Epoch [6254/20000], Loss: 13.451324462890625, Learning Rate: 0.01\n",
      "Epoch [6255/20000], Loss: 13.442214965820312, Learning Rate: 0.01\n",
      "Epoch [6256/20000], Loss: 13.433395385742188, Learning Rate: 0.01\n",
      "Epoch [6257/20000], Loss: 13.42431640625, Learning Rate: 0.01\n",
      "Epoch [6258/20000], Loss: 13.415618896484375, Learning Rate: 0.01\n",
      "Epoch [6259/20000], Loss: 13.4066162109375, Learning Rate: 0.01\n",
      "Epoch [6260/20000], Loss: 13.397705078125, Learning Rate: 0.01\n",
      "Epoch [6261/20000], Loss: 13.388824462890625, Learning Rate: 0.01\n",
      "Epoch [6262/20000], Loss: 13.379913330078125, Learning Rate: 0.01\n",
      "Epoch [6263/20000], Loss: 13.37091064453125, Learning Rate: 0.01\n",
      "Epoch [6264/20000], Loss: 13.362106323242188, Learning Rate: 0.01\n",
      "Epoch [6265/20000], Loss: 13.353012084960938, Learning Rate: 0.01\n",
      "Epoch [6266/20000], Loss: 13.34429931640625, Learning Rate: 0.01\n",
      "Epoch [6267/20000], Loss: 13.335281372070312, Learning Rate: 0.01\n",
      "Epoch [6268/20000], Loss: 13.326431274414062, Learning Rate: 0.01\n",
      "Epoch [6269/20000], Loss: 13.317398071289062, Learning Rate: 0.01\n",
      "Epoch [6270/20000], Loss: 13.30853271484375, Learning Rate: 0.01\n",
      "Epoch [6271/20000], Loss: 13.299530029296875, Learning Rate: 0.01\n",
      "Epoch [6272/20000], Loss: 13.290740966796875, Learning Rate: 0.01\n",
      "Epoch [6273/20000], Loss: 13.28192138671875, Learning Rate: 0.01\n",
      "Epoch [6274/20000], Loss: 13.272842407226562, Learning Rate: 0.01\n",
      "Epoch [6275/20000], Loss: 13.264114379882812, Learning Rate: 0.01\n",
      "Epoch [6276/20000], Loss: 13.255111694335938, Learning Rate: 0.01\n",
      "Epoch [6277/20000], Loss: 13.246047973632812, Learning Rate: 0.01\n",
      "Epoch [6278/20000], Loss: 13.237213134765625, Learning Rate: 0.01\n",
      "Epoch [6279/20000], Loss: 13.22845458984375, Learning Rate: 0.01\n",
      "Epoch [6280/20000], Loss: 13.2196044921875, Learning Rate: 0.01\n",
      "Epoch [6281/20000], Loss: 13.21051025390625, Learning Rate: 0.01\n",
      "Epoch [6282/20000], Loss: 13.20159912109375, Learning Rate: 0.01\n",
      "Epoch [6283/20000], Loss: 13.192703247070312, Learning Rate: 0.01\n",
      "Epoch [6284/20000], Loss: 13.184036254882812, Learning Rate: 0.01\n",
      "Epoch [6285/20000], Loss: 13.17510986328125, Learning Rate: 0.01\n",
      "Epoch [6286/20000], Loss: 13.166168212890625, Learning Rate: 0.01\n",
      "Epoch [6287/20000], Loss: 13.1572265625, Learning Rate: 0.01\n",
      "Epoch [6288/20000], Loss: 13.148284912109375, Learning Rate: 0.01\n",
      "Epoch [6289/20000], Loss: 13.139663696289062, Learning Rate: 0.01\n",
      "Epoch [6290/20000], Loss: 13.130752563476562, Learning Rate: 0.01\n",
      "Epoch [6291/20000], Loss: 13.121780395507812, Learning Rate: 0.01\n",
      "Epoch [6292/20000], Loss: 13.1129150390625, Learning Rate: 0.01\n",
      "Epoch [6293/20000], Loss: 13.103988647460938, Learning Rate: 0.01\n",
      "Epoch [6294/20000], Loss: 13.095169067382812, Learning Rate: 0.01\n",
      "Epoch [6295/20000], Loss: 13.086288452148438, Learning Rate: 0.01\n",
      "Epoch [6296/20000], Loss: 13.077285766601562, Learning Rate: 0.01\n",
      "Epoch [6297/20000], Loss: 13.068389892578125, Learning Rate: 0.01\n",
      "Epoch [6298/20000], Loss: 13.059677124023438, Learning Rate: 0.01\n",
      "Epoch [6299/20000], Loss: 13.050811767578125, Learning Rate: 0.01\n",
      "Epoch [6300/20000], Loss: 13.041946411132812, Learning Rate: 0.01\n",
      "Epoch [6301/20000], Loss: 13.032943725585938, Learning Rate: 0.01\n",
      "Epoch [6302/20000], Loss: 13.024093627929688, Learning Rate: 0.01\n",
      "Epoch [6303/20000], Loss: 13.015090942382812, Learning Rate: 0.01\n",
      "Epoch [6304/20000], Loss: 13.006393432617188, Learning Rate: 0.01\n",
      "Epoch [6305/20000], Loss: 12.997634887695312, Learning Rate: 0.01\n",
      "Epoch [6306/20000], Loss: 12.988555908203125, Learning Rate: 0.01\n",
      "Epoch [6307/20000], Loss: 12.97979736328125, Learning Rate: 0.01\n",
      "Epoch [6308/20000], Loss: 12.97088623046875, Learning Rate: 0.01\n",
      "Epoch [6309/20000], Loss: 12.962127685546875, Learning Rate: 0.01\n",
      "Epoch [6310/20000], Loss: 12.953201293945312, Learning Rate: 0.01\n",
      "Epoch [6311/20000], Loss: 12.944412231445312, Learning Rate: 0.01\n",
      "Epoch [6312/20000], Loss: 12.935470581054688, Learning Rate: 0.01\n",
      "Epoch [6313/20000], Loss: 12.9266357421875, Learning Rate: 0.01\n",
      "Epoch [6314/20000], Loss: 12.917739868164062, Learning Rate: 0.01\n",
      "Epoch [6315/20000], Loss: 12.90887451171875, Learning Rate: 0.01\n",
      "Epoch [6316/20000], Loss: 12.900009155273438, Learning Rate: 0.01\n",
      "Epoch [6317/20000], Loss: 12.89129638671875, Learning Rate: 0.01\n",
      "Epoch [6318/20000], Loss: 12.882476806640625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6319/20000], Loss: 12.87359619140625, Learning Rate: 0.01\n",
      "Epoch [6320/20000], Loss: 12.864669799804688, Learning Rate: 0.01\n",
      "Epoch [6321/20000], Loss: 12.855712890625, Learning Rate: 0.01\n",
      "Epoch [6322/20000], Loss: 12.846908569335938, Learning Rate: 0.01\n",
      "Epoch [6323/20000], Loss: 12.838165283203125, Learning Rate: 0.01\n",
      "Epoch [6324/20000], Loss: 12.829269409179688, Learning Rate: 0.01\n",
      "Epoch [6325/20000], Loss: 12.820404052734375, Learning Rate: 0.01\n",
      "Epoch [6326/20000], Loss: 12.81170654296875, Learning Rate: 0.01\n",
      "Epoch [6327/20000], Loss: 12.802764892578125, Learning Rate: 0.01\n",
      "Epoch [6328/20000], Loss: 12.79400634765625, Learning Rate: 0.01\n",
      "Epoch [6329/20000], Loss: 12.785110473632812, Learning Rate: 0.01\n",
      "Epoch [6330/20000], Loss: 12.776397705078125, Learning Rate: 0.01\n",
      "Epoch [6331/20000], Loss: 12.767288208007812, Learning Rate: 0.01\n",
      "Epoch [6332/20000], Loss: 12.758560180664062, Learning Rate: 0.01\n",
      "Epoch [6333/20000], Loss: 12.749771118164062, Learning Rate: 0.01\n",
      "Epoch [6334/20000], Loss: 12.741119384765625, Learning Rate: 0.01\n",
      "Epoch [6335/20000], Loss: 12.732223510742188, Learning Rate: 0.01\n",
      "Epoch [6336/20000], Loss: 12.723342895507812, Learning Rate: 0.01\n",
      "Epoch [6337/20000], Loss: 12.71478271484375, Learning Rate: 0.01\n",
      "Epoch [6338/20000], Loss: 12.70574951171875, Learning Rate: 0.01\n",
      "Epoch [6339/20000], Loss: 12.697128295898438, Learning Rate: 0.01\n",
      "Epoch [6340/20000], Loss: 12.688674926757812, Learning Rate: 0.01\n",
      "Epoch [6341/20000], Loss: 12.67987060546875, Learning Rate: 0.01\n",
      "Epoch [6342/20000], Loss: 12.671279907226562, Learning Rate: 0.01\n",
      "Epoch [6343/20000], Loss: 12.662918090820312, Learning Rate: 0.01\n",
      "Epoch [6344/20000], Loss: 12.654815673828125, Learning Rate: 0.01\n",
      "Epoch [6345/20000], Loss: 12.647003173828125, Learning Rate: 0.01\n",
      "Epoch [6346/20000], Loss: 12.639694213867188, Learning Rate: 0.01\n",
      "Epoch [6347/20000], Loss: 12.633132934570312, Learning Rate: 0.01\n",
      "Epoch [6348/20000], Loss: 12.627777099609375, Learning Rate: 0.01\n",
      "Epoch [6349/20000], Loss: 12.624603271484375, Learning Rate: 0.01\n",
      "Epoch [6350/20000], Loss: 12.624465942382812, Learning Rate: 0.01\n",
      "Epoch [6351/20000], Loss: 12.628875732421875, Learning Rate: 0.01\n",
      "Epoch [6352/20000], Loss: 12.640838623046875, Learning Rate: 0.01\n",
      "Epoch [6353/20000], Loss: 12.6650390625, Learning Rate: 0.01\n",
      "Epoch [6354/20000], Loss: 12.7086181640625, Learning Rate: 0.01\n",
      "Epoch [6355/20000], Loss: 12.7830810546875, Learning Rate: 0.01\n",
      "Epoch [6356/20000], Loss: 12.907424926757812, Learning Rate: 0.01\n",
      "Epoch [6357/20000], Loss: 13.111480712890625, Learning Rate: 0.01\n",
      "Epoch [6358/20000], Loss: 13.443267822265625, Learning Rate: 0.01\n",
      "Epoch [6359/20000], Loss: 13.977523803710938, Learning Rate: 0.01\n",
      "Epoch [6360/20000], Loss: 14.826080322265625, Learning Rate: 0.01\n",
      "Epoch [6361/20000], Loss: 16.147491455078125, Learning Rate: 0.01\n",
      "Epoch [6362/20000], Loss: 18.133087158203125, Learning Rate: 0.01\n",
      "Epoch [6363/20000], Loss: 20.94964599609375, Learning Rate: 0.01\n",
      "Epoch [6364/20000], Loss: 24.546157836914062, Learning Rate: 0.01\n",
      "Epoch [6365/20000], Loss: 28.354507446289062, Learning Rate: 0.01\n",
      "Epoch [6366/20000], Loss: 30.944961547851562, Learning Rate: 0.01\n",
      "Epoch [6367/20000], Loss: 30.419540405273438, Learning Rate: 0.01\n",
      "Epoch [6368/20000], Loss: 25.825973510742188, Learning Rate: 0.01\n",
      "Epoch [6369/20000], Loss: 18.955795288085938, Learning Rate: 0.01\n",
      "Epoch [6370/20000], Loss: 13.624526977539062, Learning Rate: 0.01\n",
      "Epoch [6371/20000], Loss: 12.591781616210938, Learning Rate: 0.01\n",
      "Epoch [6372/20000], Loss: 15.3306884765625, Learning Rate: 0.01\n",
      "Epoch [6373/20000], Loss: 18.826278686523438, Learning Rate: 0.01\n",
      "Epoch [6374/20000], Loss: 20.007110595703125, Learning Rate: 0.01\n",
      "Epoch [6375/20000], Loss: 17.942764282226562, Learning Rate: 0.01\n",
      "Epoch [6376/20000], Loss: 14.479080200195312, Learning Rate: 0.01\n",
      "Epoch [6377/20000], Loss: 12.44281005859375, Learning Rate: 0.01\n",
      "Epoch [6378/20000], Loss: 13.03009033203125, Learning Rate: 0.01\n",
      "Epoch [6379/20000], Loss: 15.002761840820312, Learning Rate: 0.01\n",
      "Epoch [6380/20000], Loss: 16.175827026367188, Learning Rate: 0.01\n",
      "Epoch [6381/20000], Loss: 15.454254150390625, Learning Rate: 0.01\n",
      "Epoch [6382/20000], Loss: 13.62896728515625, Learning Rate: 0.01\n",
      "Epoch [6383/20000], Loss: 12.389373779296875, Learning Rate: 0.01\n",
      "Epoch [6384/20000], Loss: 12.604904174804688, Learning Rate: 0.01\n",
      "Epoch [6385/20000], Loss: 13.665512084960938, Learning Rate: 0.01\n",
      "Epoch [6386/20000], Loss: 14.314956665039062, Learning Rate: 0.01\n",
      "Epoch [6387/20000], Loss: 13.914199829101562, Learning Rate: 0.01\n",
      "Epoch [6388/20000], Loss: 12.92462158203125, Learning Rate: 0.01\n",
      "Epoch [6389/20000], Loss: 12.2843017578125, Learning Rate: 0.01\n",
      "Epoch [6390/20000], Loss: 12.439865112304688, Learning Rate: 0.01\n",
      "Epoch [6391/20000], Loss: 13.017486572265625, Learning Rate: 0.01\n",
      "Epoch [6392/20000], Loss: 13.328689575195312, Learning Rate: 0.01\n",
      "Epoch [6393/20000], Loss: 13.06842041015625, Learning Rate: 0.01\n",
      "Epoch [6394/20000], Loss: 12.527435302734375, Learning Rate: 0.01\n",
      "Epoch [6395/20000], Loss: 12.209747314453125, Learning Rate: 0.01\n",
      "Epoch [6396/20000], Loss: 12.319961547851562, Learning Rate: 0.01\n",
      "Epoch [6397/20000], Loss: 12.62945556640625, Learning Rate: 0.01\n",
      "Epoch [6398/20000], Loss: 12.772613525390625, Learning Rate: 0.01\n",
      "Epoch [6399/20000], Loss: 12.610610961914062, Learning Rate: 0.01\n",
      "Epoch [6400/20000], Loss: 12.315872192382812, Learning Rate: 0.01\n",
      "Epoch [6401/20000], Loss: 12.152862548828125, Learning Rate: 0.01\n",
      "Epoch [6402/20000], Loss: 12.216751098632812, Learning Rate: 0.01\n",
      "Epoch [6403/20000], Loss: 12.378814697265625, Learning Rate: 0.01\n",
      "Epoch [6404/20000], Loss: 12.44732666015625, Learning Rate: 0.01\n",
      "Epoch [6405/20000], Loss: 12.354629516601562, Learning Rate: 0.01\n",
      "Epoch [6406/20000], Loss: 12.193222045898438, Learning Rate: 0.01\n",
      "Epoch [6407/20000], Loss: 12.101287841796875, Learning Rate: 0.01\n",
      "Epoch [6408/20000], Loss: 12.129379272460938, Learning Rate: 0.01\n",
      "Epoch [6409/20000], Loss: 12.211380004882812, Learning Rate: 0.01\n",
      "Epoch [6410/20000], Loss: 12.247634887695312, Learning Rate: 0.01\n",
      "Epoch [6411/20000], Loss: 12.199234008789062, Learning Rate: 0.01\n",
      "Epoch [6412/20000], Loss: 12.109771728515625, Learning Rate: 0.01\n",
      "Epoch [6413/20000], Loss: 12.051712036132812, Learning Rate: 0.01\n",
      "Epoch [6414/20000], Loss: 12.056289672851562, Learning Rate: 0.01\n",
      "Epoch [6415/20000], Loss: 12.095230102539062, Learning Rate: 0.01\n",
      "Epoch [6416/20000], Loss: 12.11578369140625, Learning Rate: 0.01\n",
      "Epoch [6417/20000], Loss: 12.092514038085938, Learning Rate: 0.01\n",
      "Epoch [6418/20000], Loss: 12.042449951171875, Learning Rate: 0.01\n",
      "Epoch [6419/20000], Loss: 12.002838134765625, Learning Rate: 0.01\n",
      "Epoch [6420/20000], Loss: 11.994842529296875, Learning Rate: 0.01\n",
      "Epoch [6421/20000], Loss: 12.009445190429688, Learning Rate: 0.01\n",
      "Epoch [6422/20000], Loss: 12.02056884765625, Learning Rate: 0.01\n",
      "Epoch [6423/20000], Loss: 12.010223388671875, Learning Rate: 0.01\n",
      "Epoch [6424/20000], Loss: 11.982635498046875, Learning Rate: 0.01\n",
      "Epoch [6425/20000], Loss: 11.954330444335938, Learning Rate: 0.01\n",
      "Epoch [6426/20000], Loss: 11.940887451171875, Learning Rate: 0.01\n",
      "Epoch [6427/20000], Loss: 11.941787719726562, Learning Rate: 0.01\n",
      "Epoch [6428/20000], Loss: 11.9454345703125, Learning Rate: 0.01\n",
      "Epoch [6429/20000], Loss: 11.940597534179688, Learning Rate: 0.01\n",
      "Epoch [6430/20000], Loss: 11.925079345703125, Learning Rate: 0.01\n",
      "Epoch [6431/20000], Loss: 11.905563354492188, Learning Rate: 0.01\n",
      "Epoch [6432/20000], Loss: 11.890609741210938, Learning Rate: 0.01\n",
      "Epoch [6433/20000], Loss: 11.884262084960938, Learning Rate: 0.01\n",
      "Epoch [6434/20000], Loss: 11.882171630859375, Learning Rate: 0.01\n",
      "Epoch [6435/20000], Loss: 11.878280639648438, Learning Rate: 0.01\n",
      "Epoch [6436/20000], Loss: 11.868789672851562, Learning Rate: 0.01\n",
      "Epoch [6437/20000], Loss: 11.85552978515625, Learning Rate: 0.01\n",
      "Epoch [6438/20000], Loss: 11.841995239257812, Learning Rate: 0.01\n",
      "Epoch [6439/20000], Loss: 11.832168579101562, Learning Rate: 0.01\n",
      "Epoch [6440/20000], Loss: 11.826156616210938, Learning Rate: 0.01\n",
      "Epoch [6441/20000], Loss: 11.820892333984375, Learning Rate: 0.01\n",
      "Epoch [6442/20000], Loss: 11.813720703125, Learning Rate: 0.01\n",
      "Epoch [6443/20000], Loss: 11.804168701171875, Learning Rate: 0.01\n",
      "Epoch [6444/20000], Loss: 11.793121337890625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6445/20000], Loss: 11.7825927734375, Learning Rate: 0.01\n",
      "Epoch [6446/20000], Loss: 11.774154663085938, Learning Rate: 0.01\n",
      "Epoch [6447/20000], Loss: 11.767105102539062, Learning Rate: 0.01\n",
      "Epoch [6448/20000], Loss: 11.760330200195312, Learning Rate: 0.01\n",
      "Epoch [6449/20000], Loss: 11.752410888671875, Learning Rate: 0.01\n",
      "Epoch [6450/20000], Loss: 11.743331909179688, Learning Rate: 0.01\n",
      "Epoch [6451/20000], Loss: 11.733688354492188, Learning Rate: 0.01\n",
      "Epoch [6452/20000], Loss: 11.724395751953125, Learning Rate: 0.01\n",
      "Epoch [6453/20000], Loss: 11.716033935546875, Learning Rate: 0.01\n",
      "Epoch [6454/20000], Loss: 11.7083740234375, Learning Rate: 0.01\n",
      "Epoch [6455/20000], Loss: 11.70074462890625, Learning Rate: 0.01\n",
      "Epoch [6456/20000], Loss: 11.692672729492188, Learning Rate: 0.01\n",
      "Epoch [6457/20000], Loss: 11.684158325195312, Learning Rate: 0.01\n",
      "Epoch [6458/20000], Loss: 11.675201416015625, Learning Rate: 0.01\n",
      "Epoch [6459/20000], Loss: 11.666458129882812, Learning Rate: 0.01\n",
      "Epoch [6460/20000], Loss: 11.657928466796875, Learning Rate: 0.01\n",
      "Epoch [6461/20000], Loss: 11.64996337890625, Learning Rate: 0.01\n",
      "Epoch [6462/20000], Loss: 11.641952514648438, Learning Rate: 0.01\n",
      "Epoch [6463/20000], Loss: 11.634033203125, Learning Rate: 0.01\n",
      "Epoch [6464/20000], Loss: 11.62548828125, Learning Rate: 0.01\n",
      "Epoch [6465/20000], Loss: 11.616958618164062, Learning Rate: 0.01\n",
      "Epoch [6466/20000], Loss: 11.6083984375, Learning Rate: 0.01\n",
      "Epoch [6467/20000], Loss: 11.60015869140625, Learning Rate: 0.01\n",
      "Epoch [6468/20000], Loss: 11.591964721679688, Learning Rate: 0.01\n",
      "Epoch [6469/20000], Loss: 11.583740234375, Learning Rate: 0.01\n",
      "Epoch [6470/20000], Loss: 11.57568359375, Learning Rate: 0.01\n",
      "Epoch [6471/20000], Loss: 11.567367553710938, Learning Rate: 0.01\n",
      "Epoch [6472/20000], Loss: 11.559112548828125, Learning Rate: 0.01\n",
      "Epoch [6473/20000], Loss: 11.550567626953125, Learning Rate: 0.01\n",
      "Epoch [6474/20000], Loss: 11.54229736328125, Learning Rate: 0.01\n",
      "Epoch [6475/20000], Loss: 11.533981323242188, Learning Rate: 0.01\n",
      "Epoch [6476/20000], Loss: 11.525909423828125, Learning Rate: 0.01\n",
      "Epoch [6477/20000], Loss: 11.517669677734375, Learning Rate: 0.01\n",
      "Epoch [6478/20000], Loss: 11.509536743164062, Learning Rate: 0.01\n",
      "Epoch [6479/20000], Loss: 11.501235961914062, Learning Rate: 0.01\n",
      "Epoch [6480/20000], Loss: 11.492828369140625, Learning Rate: 0.01\n",
      "Epoch [6481/20000], Loss: 11.484481811523438, Learning Rate: 0.01\n",
      "Epoch [6482/20000], Loss: 11.4761962890625, Learning Rate: 0.01\n",
      "Epoch [6483/20000], Loss: 11.46795654296875, Learning Rate: 0.01\n",
      "Epoch [6484/20000], Loss: 11.459869384765625, Learning Rate: 0.01\n",
      "Epoch [6485/20000], Loss: 11.45166015625, Learning Rate: 0.01\n",
      "Epoch [6486/20000], Loss: 11.443313598632812, Learning Rate: 0.01\n",
      "Epoch [6487/20000], Loss: 11.435104370117188, Learning Rate: 0.01\n",
      "Epoch [6488/20000], Loss: 11.427017211914062, Learning Rate: 0.01\n",
      "Epoch [6489/20000], Loss: 11.418746948242188, Learning Rate: 0.01\n",
      "Epoch [6490/20000], Loss: 11.410354614257812, Learning Rate: 0.01\n",
      "Epoch [6491/20000], Loss: 11.40216064453125, Learning Rate: 0.01\n",
      "Epoch [6492/20000], Loss: 11.39410400390625, Learning Rate: 0.01\n",
      "Epoch [6493/20000], Loss: 11.385696411132812, Learning Rate: 0.01\n",
      "Epoch [6494/20000], Loss: 11.377685546875, Learning Rate: 0.01\n",
      "Epoch [6495/20000], Loss: 11.369369506835938, Learning Rate: 0.01\n",
      "Epoch [6496/20000], Loss: 11.36126708984375, Learning Rate: 0.01\n",
      "Epoch [6497/20000], Loss: 11.352981567382812, Learning Rate: 0.01\n",
      "Epoch [6498/20000], Loss: 11.34478759765625, Learning Rate: 0.01\n",
      "Epoch [6499/20000], Loss: 11.33642578125, Learning Rate: 0.01\n",
      "Epoch [6500/20000], Loss: 11.328384399414062, Learning Rate: 0.01\n",
      "Epoch [6501/20000], Loss: 11.32012939453125, Learning Rate: 0.01\n",
      "Epoch [6502/20000], Loss: 11.311920166015625, Learning Rate: 0.01\n",
      "Epoch [6503/20000], Loss: 11.30364990234375, Learning Rate: 0.01\n",
      "Epoch [6504/20000], Loss: 11.295425415039062, Learning Rate: 0.01\n",
      "Epoch [6505/20000], Loss: 11.287216186523438, Learning Rate: 0.01\n",
      "Epoch [6506/20000], Loss: 11.279098510742188, Learning Rate: 0.01\n",
      "Epoch [6507/20000], Loss: 11.270736694335938, Learning Rate: 0.01\n",
      "Epoch [6508/20000], Loss: 11.262542724609375, Learning Rate: 0.01\n",
      "Epoch [6509/20000], Loss: 11.254470825195312, Learning Rate: 0.01\n",
      "Epoch [6510/20000], Loss: 11.246337890625, Learning Rate: 0.01\n",
      "Epoch [6511/20000], Loss: 11.238204956054688, Learning Rate: 0.01\n",
      "Epoch [6512/20000], Loss: 11.229934692382812, Learning Rate: 0.01\n",
      "Epoch [6513/20000], Loss: 11.221694946289062, Learning Rate: 0.01\n",
      "Epoch [6514/20000], Loss: 11.213668823242188, Learning Rate: 0.01\n",
      "Epoch [6515/20000], Loss: 11.205307006835938, Learning Rate: 0.01\n",
      "Epoch [6516/20000], Loss: 11.19708251953125, Learning Rate: 0.01\n",
      "Epoch [6517/20000], Loss: 11.189071655273438, Learning Rate: 0.01\n",
      "Epoch [6518/20000], Loss: 11.18084716796875, Learning Rate: 0.01\n",
      "Epoch [6519/20000], Loss: 11.172714233398438, Learning Rate: 0.01\n",
      "Epoch [6520/20000], Loss: 11.164535522460938, Learning Rate: 0.01\n",
      "Epoch [6521/20000], Loss: 11.156280517578125, Learning Rate: 0.01\n",
      "Epoch [6522/20000], Loss: 11.14825439453125, Learning Rate: 0.01\n",
      "Epoch [6523/20000], Loss: 11.1400146484375, Learning Rate: 0.01\n",
      "Epoch [6524/20000], Loss: 11.13177490234375, Learning Rate: 0.01\n",
      "Epoch [6525/20000], Loss: 11.123870849609375, Learning Rate: 0.01\n",
      "Epoch [6526/20000], Loss: 11.11553955078125, Learning Rate: 0.01\n",
      "Epoch [6527/20000], Loss: 11.107513427734375, Learning Rate: 0.01\n",
      "Epoch [6528/20000], Loss: 11.09930419921875, Learning Rate: 0.01\n",
      "Epoch [6529/20000], Loss: 11.091018676757812, Learning Rate: 0.01\n",
      "Epoch [6530/20000], Loss: 11.082901000976562, Learning Rate: 0.01\n",
      "Epoch [6531/20000], Loss: 11.07476806640625, Learning Rate: 0.01\n",
      "Epoch [6532/20000], Loss: 11.066558837890625, Learning Rate: 0.01\n",
      "Epoch [6533/20000], Loss: 11.058319091796875, Learning Rate: 0.01\n",
      "Epoch [6534/20000], Loss: 11.05047607421875, Learning Rate: 0.01\n",
      "Epoch [6535/20000], Loss: 11.0423583984375, Learning Rate: 0.01\n",
      "Epoch [6536/20000], Loss: 11.034011840820312, Learning Rate: 0.01\n",
      "Epoch [6537/20000], Loss: 11.025833129882812, Learning Rate: 0.01\n",
      "Epoch [6538/20000], Loss: 11.01776123046875, Learning Rate: 0.01\n",
      "Epoch [6539/20000], Loss: 11.009780883789062, Learning Rate: 0.01\n",
      "Epoch [6540/20000], Loss: 11.001571655273438, Learning Rate: 0.01\n",
      "Epoch [6541/20000], Loss: 10.993240356445312, Learning Rate: 0.01\n",
      "Epoch [6542/20000], Loss: 10.9852294921875, Learning Rate: 0.01\n",
      "Epoch [6543/20000], Loss: 10.977020263671875, Learning Rate: 0.01\n",
      "Epoch [6544/20000], Loss: 10.968948364257812, Learning Rate: 0.01\n",
      "Epoch [6545/20000], Loss: 10.96087646484375, Learning Rate: 0.01\n",
      "Epoch [6546/20000], Loss: 10.952880859375, Learning Rate: 0.01\n",
      "Epoch [6547/20000], Loss: 10.94464111328125, Learning Rate: 0.01\n",
      "Epoch [6548/20000], Loss: 10.936630249023438, Learning Rate: 0.01\n",
      "Epoch [6549/20000], Loss: 10.928375244140625, Learning Rate: 0.01\n",
      "Epoch [6550/20000], Loss: 10.920272827148438, Learning Rate: 0.01\n",
      "Epoch [6551/20000], Loss: 10.911941528320312, Learning Rate: 0.01\n",
      "Epoch [6552/20000], Loss: 10.904083251953125, Learning Rate: 0.01\n",
      "Epoch [6553/20000], Loss: 10.895782470703125, Learning Rate: 0.01\n",
      "Epoch [6554/20000], Loss: 10.887786865234375, Learning Rate: 0.01\n",
      "Epoch [6555/20000], Loss: 10.879653930664062, Learning Rate: 0.01\n",
      "Epoch [6556/20000], Loss: 10.87158203125, Learning Rate: 0.01\n",
      "Epoch [6557/20000], Loss: 10.8634033203125, Learning Rate: 0.01\n",
      "Epoch [6558/20000], Loss: 10.855392456054688, Learning Rate: 0.01\n",
      "Epoch [6559/20000], Loss: 10.8472900390625, Learning Rate: 0.01\n",
      "Epoch [6560/20000], Loss: 10.839187622070312, Learning Rate: 0.01\n",
      "Epoch [6561/20000], Loss: 10.831130981445312, Learning Rate: 0.01\n",
      "Epoch [6562/20000], Loss: 10.822982788085938, Learning Rate: 0.01\n",
      "Epoch [6563/20000], Loss: 10.81494140625, Learning Rate: 0.01\n",
      "Epoch [6564/20000], Loss: 10.807052612304688, Learning Rate: 0.01\n",
      "Epoch [6565/20000], Loss: 10.798675537109375, Learning Rate: 0.01\n",
      "Epoch [6566/20000], Loss: 10.790695190429688, Learning Rate: 0.01\n",
      "Epoch [6567/20000], Loss: 10.782577514648438, Learning Rate: 0.01\n",
      "Epoch [6568/20000], Loss: 10.774490356445312, Learning Rate: 0.01\n",
      "Epoch [6569/20000], Loss: 10.766326904296875, Learning Rate: 0.01\n",
      "Epoch [6570/20000], Loss: 10.75836181640625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6571/20000], Loss: 10.750244140625, Learning Rate: 0.01\n",
      "Epoch [6572/20000], Loss: 10.742156982421875, Learning Rate: 0.01\n",
      "Epoch [6573/20000], Loss: 10.733978271484375, Learning Rate: 0.01\n",
      "Epoch [6574/20000], Loss: 10.726089477539062, Learning Rate: 0.01\n",
      "Epoch [6575/20000], Loss: 10.717926025390625, Learning Rate: 0.01\n",
      "Epoch [6576/20000], Loss: 10.7098388671875, Learning Rate: 0.01\n",
      "Epoch [6577/20000], Loss: 10.701980590820312, Learning Rate: 0.01\n",
      "Epoch [6578/20000], Loss: 10.69384765625, Learning Rate: 0.01\n",
      "Epoch [6579/20000], Loss: 10.685821533203125, Learning Rate: 0.01\n",
      "Epoch [6580/20000], Loss: 10.677627563476562, Learning Rate: 0.01\n",
      "Epoch [6581/20000], Loss: 10.669586181640625, Learning Rate: 0.01\n",
      "Epoch [6582/20000], Loss: 10.661590576171875, Learning Rate: 0.01\n",
      "Epoch [6583/20000], Loss: 10.653457641601562, Learning Rate: 0.01\n",
      "Epoch [6584/20000], Loss: 10.64556884765625, Learning Rate: 0.01\n",
      "Epoch [6585/20000], Loss: 10.637405395507812, Learning Rate: 0.01\n",
      "Epoch [6586/20000], Loss: 10.629287719726562, Learning Rate: 0.01\n",
      "Epoch [6587/20000], Loss: 10.621292114257812, Learning Rate: 0.01\n",
      "Epoch [6588/20000], Loss: 10.61328125, Learning Rate: 0.01\n",
      "Epoch [6589/20000], Loss: 10.605224609375, Learning Rate: 0.01\n",
      "Epoch [6590/20000], Loss: 10.597274780273438, Learning Rate: 0.01\n",
      "Epoch [6591/20000], Loss: 10.589035034179688, Learning Rate: 0.01\n",
      "Epoch [6592/20000], Loss: 10.581069946289062, Learning Rate: 0.01\n",
      "Epoch [6593/20000], Loss: 10.573043823242188, Learning Rate: 0.01\n",
      "Epoch [6594/20000], Loss: 10.565032958984375, Learning Rate: 0.01\n",
      "Epoch [6595/20000], Loss: 10.556976318359375, Learning Rate: 0.01\n",
      "Epoch [6596/20000], Loss: 10.548980712890625, Learning Rate: 0.01\n",
      "Epoch [6597/20000], Loss: 10.541015625, Learning Rate: 0.01\n",
      "Epoch [6598/20000], Loss: 10.532913208007812, Learning Rate: 0.01\n",
      "Epoch [6599/20000], Loss: 10.525009155273438, Learning Rate: 0.01\n",
      "Epoch [6600/20000], Loss: 10.516830444335938, Learning Rate: 0.01\n",
      "Epoch [6601/20000], Loss: 10.509017944335938, Learning Rate: 0.01\n",
      "Epoch [6602/20000], Loss: 10.50091552734375, Learning Rate: 0.01\n",
      "Epoch [6603/20000], Loss: 10.492691040039062, Learning Rate: 0.01\n",
      "Epoch [6604/20000], Loss: 10.484817504882812, Learning Rate: 0.01\n",
      "Epoch [6605/20000], Loss: 10.476821899414062, Learning Rate: 0.01\n",
      "Epoch [6606/20000], Loss: 10.46875, Learning Rate: 0.01\n",
      "Epoch [6607/20000], Loss: 10.460784912109375, Learning Rate: 0.01\n",
      "Epoch [6608/20000], Loss: 10.452728271484375, Learning Rate: 0.01\n",
      "Epoch [6609/20000], Loss: 10.444778442382812, Learning Rate: 0.01\n",
      "Epoch [6610/20000], Loss: 10.436904907226562, Learning Rate: 0.01\n",
      "Epoch [6611/20000], Loss: 10.428848266601562, Learning Rate: 0.01\n",
      "Epoch [6612/20000], Loss: 10.420944213867188, Learning Rate: 0.01\n",
      "Epoch [6613/20000], Loss: 10.412948608398438, Learning Rate: 0.01\n",
      "Epoch [6614/20000], Loss: 10.40478515625, Learning Rate: 0.01\n",
      "Epoch [6615/20000], Loss: 10.396957397460938, Learning Rate: 0.01\n",
      "Epoch [6616/20000], Loss: 10.389053344726562, Learning Rate: 0.01\n",
      "Epoch [6617/20000], Loss: 10.380935668945312, Learning Rate: 0.01\n",
      "Epoch [6618/20000], Loss: 10.373046875, Learning Rate: 0.01\n",
      "Epoch [6619/20000], Loss: 10.365188598632812, Learning Rate: 0.01\n",
      "Epoch [6620/20000], Loss: 10.357330322265625, Learning Rate: 0.01\n",
      "Epoch [6621/20000], Loss: 10.34967041015625, Learning Rate: 0.01\n",
      "Epoch [6622/20000], Loss: 10.342071533203125, Learning Rate: 0.01\n",
      "Epoch [6623/20000], Loss: 10.334640502929688, Learning Rate: 0.01\n",
      "Epoch [6624/20000], Loss: 10.327545166015625, Learning Rate: 0.01\n",
      "Epoch [6625/20000], Loss: 10.321136474609375, Learning Rate: 0.01\n",
      "Epoch [6626/20000], Loss: 10.3153076171875, Learning Rate: 0.01\n",
      "Epoch [6627/20000], Loss: 10.310897827148438, Learning Rate: 0.01\n",
      "Epoch [6628/20000], Loss: 10.308563232421875, Learning Rate: 0.01\n",
      "Epoch [6629/20000], Loss: 10.309661865234375, Learning Rate: 0.01\n",
      "Epoch [6630/20000], Loss: 10.316375732421875, Learning Rate: 0.01\n",
      "Epoch [6631/20000], Loss: 10.3321533203125, Learning Rate: 0.01\n",
      "Epoch [6632/20000], Loss: 10.362625122070312, Learning Rate: 0.01\n",
      "Epoch [6633/20000], Loss: 10.4176025390625, Learning Rate: 0.01\n",
      "Epoch [6634/20000], Loss: 10.512893676757812, Learning Rate: 0.01\n",
      "Epoch [6635/20000], Loss: 10.674728393554688, Learning Rate: 0.01\n",
      "Epoch [6636/20000], Loss: 10.946517944335938, Learning Rate: 0.01\n",
      "Epoch [6637/20000], Loss: 11.398681640625, Learning Rate: 0.01\n",
      "Epoch [6638/20000], Loss: 12.143295288085938, Learning Rate: 0.01\n",
      "Epoch [6639/20000], Loss: 13.35150146484375, Learning Rate: 0.01\n",
      "Epoch [6640/20000], Loss: 15.259414672851562, Learning Rate: 0.01\n",
      "Epoch [6641/20000], Loss: 18.141769409179688, Learning Rate: 0.01\n",
      "Epoch [6642/20000], Loss: 22.150177001953125, Learning Rate: 0.01\n",
      "Epoch [6643/20000], Loss: 26.977325439453125, Learning Rate: 0.01\n",
      "Epoch [6644/20000], Loss: 31.264404296875, Learning Rate: 0.01\n",
      "Epoch [6645/20000], Loss: 32.556488037109375, Learning Rate: 0.01\n",
      "Epoch [6646/20000], Loss: 28.6248779296875, Learning Rate: 0.01\n",
      "Epoch [6647/20000], Loss: 20.438735961914062, Learning Rate: 0.01\n",
      "Epoch [6648/20000], Loss: 12.715621948242188, Learning Rate: 0.01\n",
      "Epoch [6649/20000], Loss: 10.150680541992188, Learning Rate: 0.01\n",
      "Epoch [6650/20000], Loss: 13.1038818359375, Learning Rate: 0.01\n",
      "Epoch [6651/20000], Loss: 17.75091552734375, Learning Rate: 0.01\n",
      "Epoch [6652/20000], Loss: 19.567230224609375, Learning Rate: 0.01\n",
      "Epoch [6653/20000], Loss: 16.9371337890625, Learning Rate: 0.01\n",
      "Epoch [6654/20000], Loss: 12.434585571289062, Learning Rate: 0.01\n",
      "Epoch [6655/20000], Loss: 10.096954345703125, Learning Rate: 0.01\n",
      "Epoch [6656/20000], Loss: 11.34063720703125, Learning Rate: 0.01\n",
      "Epoch [6657/20000], Loss: 13.988739013671875, Learning Rate: 0.01\n",
      "Epoch [6658/20000], Loss: 14.942718505859375, Learning Rate: 0.01\n",
      "Epoch [6659/20000], Loss: 13.28814697265625, Learning Rate: 0.01\n",
      "Epoch [6660/20000], Loss: 10.875732421875, Learning Rate: 0.01\n",
      "Epoch [6661/20000], Loss: 10.045120239257812, Learning Rate: 0.01\n",
      "Epoch [6662/20000], Loss: 11.132217407226562, Learning Rate: 0.01\n",
      "Epoch [6663/20000], Loss: 12.484298706054688, Learning Rate: 0.01\n",
      "Epoch [6664/20000], Loss: 12.499557495117188, Learning Rate: 0.01\n",
      "Epoch [6665/20000], Loss: 11.250015258789062, Learning Rate: 0.01\n",
      "Epoch [6666/20000], Loss: 10.125335693359375, Learning Rate: 0.01\n",
      "Epoch [6667/20000], Loss: 10.144821166992188, Learning Rate: 0.01\n",
      "Epoch [6668/20000], Loss: 10.956344604492188, Learning Rate: 0.01\n",
      "Epoch [6669/20000], Loss: 11.46099853515625, Learning Rate: 0.01\n",
      "Epoch [6670/20000], Loss: 11.093856811523438, Learning Rate: 0.01\n",
      "Epoch [6671/20000], Loss: 10.315216064453125, Learning Rate: 0.01\n",
      "Epoch [6672/20000], Loss: 9.938690185546875, Learning Rate: 0.01\n",
      "Epoch [6673/20000], Loss: 10.210678100585938, Learning Rate: 0.01\n",
      "Epoch [6674/20000], Loss: 10.659103393554688, Learning Rate: 0.01\n",
      "Epoch [6675/20000], Loss: 10.723678588867188, Learning Rate: 0.01\n",
      "Epoch [6676/20000], Loss: 10.355209350585938, Learning Rate: 0.01\n",
      "Epoch [6677/20000], Loss: 9.969192504882812, Learning Rate: 0.01\n",
      "Epoch [6678/20000], Loss: 9.925308227539062, Learning Rate: 0.01\n",
      "Epoch [6679/20000], Loss: 10.1624755859375, Learning Rate: 0.01\n",
      "Epoch [6680/20000], Loss: 10.344009399414062, Learning Rate: 0.01\n",
      "Epoch [6681/20000], Loss: 10.258560180664062, Learning Rate: 0.01\n",
      "Epoch [6682/20000], Loss: 10.012176513671875, Learning Rate: 0.01\n",
      "Epoch [6683/20000], Loss: 9.858367919921875, Learning Rate: 0.01\n",
      "Epoch [6684/20000], Loss: 9.909683227539062, Learning Rate: 0.01\n",
      "Epoch [6685/20000], Loss: 10.049118041992188, Learning Rate: 0.01\n",
      "Epoch [6686/20000], Loss: 10.095062255859375, Learning Rate: 0.01\n",
      "Epoch [6687/20000], Loss: 9.996627807617188, Learning Rate: 0.01\n",
      "Epoch [6688/20000], Loss: 9.86004638671875, Learning Rate: 0.01\n",
      "Epoch [6689/20000], Loss: 9.810958862304688, Learning Rate: 0.01\n",
      "Epoch [6690/20000], Loss: 9.863861083984375, Learning Rate: 0.01\n",
      "Epoch [6691/20000], Loss: 9.929779052734375, Learning Rate: 0.01\n",
      "Epoch [6692/20000], Loss: 9.925018310546875, Learning Rate: 0.01\n",
      "Epoch [6693/20000], Loss: 9.852432250976562, Learning Rate: 0.01\n",
      "Epoch [6694/20000], Loss: 9.78271484375, Learning Rate: 0.01\n",
      "Epoch [6695/20000], Loss: 9.770187377929688, Learning Rate: 0.01\n",
      "Epoch [6696/20000], Loss: 9.803253173828125, Learning Rate: 0.01\n",
      "Epoch [6697/20000], Loss: 9.829132080078125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6698/20000], Loss: 9.812210083007812, Learning Rate: 0.01\n",
      "Epoch [6699/20000], Loss: 9.76611328125, Learning Rate: 0.01\n",
      "Epoch [6700/20000], Loss: 9.729705810546875, Learning Rate: 0.01\n",
      "Epoch [6701/20000], Loss: 9.725738525390625, Learning Rate: 0.01\n",
      "Epoch [6702/20000], Loss: 9.741729736328125, Learning Rate: 0.01\n",
      "Epoch [6703/20000], Loss: 9.749343872070312, Learning Rate: 0.01\n",
      "Epoch [6704/20000], Loss: 9.733367919921875, Learning Rate: 0.01\n",
      "Epoch [6705/20000], Loss: 9.70440673828125, Learning Rate: 0.01\n",
      "Epoch [6706/20000], Loss: 9.683074951171875, Learning Rate: 0.01\n",
      "Epoch [6707/20000], Loss: 9.679183959960938, Learning Rate: 0.01\n",
      "Epoch [6708/20000], Loss: 9.684432983398438, Learning Rate: 0.01\n",
      "Epoch [6709/20000], Loss: 9.68426513671875, Learning Rate: 0.01\n",
      "Epoch [6710/20000], Loss: 9.6717529296875, Learning Rate: 0.01\n",
      "Epoch [6711/20000], Loss: 9.652694702148438, Learning Rate: 0.01\n",
      "Epoch [6712/20000], Loss: 9.637969970703125, Learning Rate: 0.01\n",
      "Epoch [6713/20000], Loss: 9.632308959960938, Learning Rate: 0.01\n",
      "Epoch [6714/20000], Loss: 9.6318359375, Learning Rate: 0.01\n",
      "Epoch [6715/20000], Loss: 9.628631591796875, Learning Rate: 0.01\n",
      "Epoch [6716/20000], Loss: 9.61871337890625, Learning Rate: 0.01\n",
      "Epoch [6717/20000], Loss: 9.605117797851562, Learning Rate: 0.01\n",
      "Epoch [6718/20000], Loss: 9.593582153320312, Learning Rate: 0.01\n",
      "Epoch [6719/20000], Loss: 9.586639404296875, Learning Rate: 0.01\n",
      "Epoch [6720/20000], Loss: 9.582717895507812, Learning Rate: 0.01\n",
      "Epoch [6721/20000], Loss: 9.5777587890625, Learning Rate: 0.01\n",
      "Epoch [6722/20000], Loss: 9.56951904296875, Learning Rate: 0.01\n",
      "Epoch [6723/20000], Loss: 9.55902099609375, Learning Rate: 0.01\n",
      "Epoch [6724/20000], Loss: 9.549057006835938, Learning Rate: 0.01\n",
      "Epoch [6725/20000], Loss: 9.541336059570312, Learning Rate: 0.01\n",
      "Epoch [6726/20000], Loss: 9.535568237304688, Learning Rate: 0.01\n",
      "Epoch [6727/20000], Loss: 9.529769897460938, Learning Rate: 0.01\n",
      "Epoch [6728/20000], Loss: 9.52252197265625, Learning Rate: 0.01\n",
      "Epoch [6729/20000], Loss: 9.51385498046875, Learning Rate: 0.01\n",
      "Epoch [6730/20000], Loss: 9.504684448242188, Learning Rate: 0.01\n",
      "Epoch [6731/20000], Loss: 9.49688720703125, Learning Rate: 0.01\n",
      "Epoch [6732/20000], Loss: 9.4901123046875, Learning Rate: 0.01\n",
      "Epoch [6733/20000], Loss: 9.483688354492188, Learning Rate: 0.01\n",
      "Epoch [6734/20000], Loss: 9.476699829101562, Learning Rate: 0.01\n",
      "Epoch [6735/20000], Loss: 9.468948364257812, Learning Rate: 0.01\n",
      "Epoch [6736/20000], Loss: 9.460556030273438, Learning Rate: 0.01\n",
      "Epoch [6737/20000], Loss: 9.45245361328125, Learning Rate: 0.01\n",
      "Epoch [6738/20000], Loss: 9.445175170898438, Learning Rate: 0.01\n",
      "Epoch [6739/20000], Loss: 9.438247680664062, Learning Rate: 0.01\n",
      "Epoch [6740/20000], Loss: 9.431228637695312, Learning Rate: 0.01\n",
      "Epoch [6741/20000], Loss: 9.423995971679688, Learning Rate: 0.01\n",
      "Epoch [6742/20000], Loss: 9.416122436523438, Learning Rate: 0.01\n",
      "Epoch [6743/20000], Loss: 9.408447265625, Learning Rate: 0.01\n",
      "Epoch [6744/20000], Loss: 9.400848388671875, Learning Rate: 0.01\n",
      "Epoch [6745/20000], Loss: 9.393569946289062, Learning Rate: 0.01\n",
      "Epoch [6746/20000], Loss: 9.386581420898438, Learning Rate: 0.01\n",
      "Epoch [6747/20000], Loss: 9.379318237304688, Learning Rate: 0.01\n",
      "Epoch [6748/20000], Loss: 9.371841430664062, Learning Rate: 0.01\n",
      "Epoch [6749/20000], Loss: 9.364303588867188, Learning Rate: 0.01\n",
      "Epoch [6750/20000], Loss: 9.356719970703125, Learning Rate: 0.01\n",
      "Epoch [6751/20000], Loss: 9.349197387695312, Learning Rate: 0.01\n",
      "Epoch [6752/20000], Loss: 9.34210205078125, Learning Rate: 0.01\n",
      "Epoch [6753/20000], Loss: 9.334884643554688, Learning Rate: 0.01\n",
      "Epoch [6754/20000], Loss: 9.32733154296875, Learning Rate: 0.01\n",
      "Epoch [6755/20000], Loss: 9.320175170898438, Learning Rate: 0.01\n",
      "Epoch [6756/20000], Loss: 9.312530517578125, Learning Rate: 0.01\n",
      "Epoch [6757/20000], Loss: 9.305130004882812, Learning Rate: 0.01\n",
      "Epoch [6758/20000], Loss: 9.297897338867188, Learning Rate: 0.01\n",
      "Epoch [6759/20000], Loss: 9.29046630859375, Learning Rate: 0.01\n",
      "Epoch [6760/20000], Loss: 9.283294677734375, Learning Rate: 0.01\n",
      "Epoch [6761/20000], Loss: 9.275924682617188, Learning Rate: 0.01\n",
      "Epoch [6762/20000], Loss: 9.26849365234375, Learning Rate: 0.01\n",
      "Epoch [6763/20000], Loss: 9.261245727539062, Learning Rate: 0.01\n",
      "Epoch [6764/20000], Loss: 9.253829956054688, Learning Rate: 0.01\n",
      "Epoch [6765/20000], Loss: 9.24639892578125, Learning Rate: 0.01\n",
      "Epoch [6766/20000], Loss: 9.239120483398438, Learning Rate: 0.01\n",
      "Epoch [6767/20000], Loss: 9.231765747070312, Learning Rate: 0.01\n",
      "Epoch [6768/20000], Loss: 9.2244873046875, Learning Rate: 0.01\n",
      "Epoch [6769/20000], Loss: 9.217178344726562, Learning Rate: 0.01\n",
      "Epoch [6770/20000], Loss: 9.209564208984375, Learning Rate: 0.01\n",
      "Epoch [6771/20000], Loss: 9.202423095703125, Learning Rate: 0.01\n",
      "Epoch [6772/20000], Loss: 9.195175170898438, Learning Rate: 0.01\n",
      "Epoch [6773/20000], Loss: 9.187698364257812, Learning Rate: 0.01\n",
      "Epoch [6774/20000], Loss: 9.180465698242188, Learning Rate: 0.01\n",
      "Epoch [6775/20000], Loss: 9.173049926757812, Learning Rate: 0.01\n",
      "Epoch [6776/20000], Loss: 9.165863037109375, Learning Rate: 0.01\n",
      "Epoch [6777/20000], Loss: 9.158493041992188, Learning Rate: 0.01\n",
      "Epoch [6778/20000], Loss: 9.1512451171875, Learning Rate: 0.01\n",
      "Epoch [6779/20000], Loss: 9.143814086914062, Learning Rate: 0.01\n",
      "Epoch [6780/20000], Loss: 9.13653564453125, Learning Rate: 0.01\n",
      "Epoch [6781/20000], Loss: 9.1292724609375, Learning Rate: 0.01\n",
      "Epoch [6782/20000], Loss: 9.1219482421875, Learning Rate: 0.01\n",
      "Epoch [6783/20000], Loss: 9.114578247070312, Learning Rate: 0.01\n",
      "Epoch [6784/20000], Loss: 9.107345581054688, Learning Rate: 0.01\n",
      "Epoch [6785/20000], Loss: 9.099990844726562, Learning Rate: 0.01\n",
      "Epoch [6786/20000], Loss: 9.092666625976562, Learning Rate: 0.01\n",
      "Epoch [6787/20000], Loss: 9.085296630859375, Learning Rate: 0.01\n",
      "Epoch [6788/20000], Loss: 9.07794189453125, Learning Rate: 0.01\n",
      "Epoch [6789/20000], Loss: 9.070632934570312, Learning Rate: 0.01\n",
      "Epoch [6790/20000], Loss: 9.063552856445312, Learning Rate: 0.01\n",
      "Epoch [6791/20000], Loss: 9.056182861328125, Learning Rate: 0.01\n",
      "Epoch [6792/20000], Loss: 9.049057006835938, Learning Rate: 0.01\n",
      "Epoch [6793/20000], Loss: 9.041671752929688, Learning Rate: 0.01\n",
      "Epoch [6794/20000], Loss: 9.034286499023438, Learning Rate: 0.01\n",
      "Epoch [6795/20000], Loss: 9.026962280273438, Learning Rate: 0.01\n",
      "Epoch [6796/20000], Loss: 9.019729614257812, Learning Rate: 0.01\n",
      "Epoch [6797/20000], Loss: 9.012466430664062, Learning Rate: 0.01\n",
      "Epoch [6798/20000], Loss: 9.005126953125, Learning Rate: 0.01\n",
      "Epoch [6799/20000], Loss: 8.997879028320312, Learning Rate: 0.01\n",
      "Epoch [6800/20000], Loss: 8.99053955078125, Learning Rate: 0.01\n",
      "Epoch [6801/20000], Loss: 8.983169555664062, Learning Rate: 0.01\n",
      "Epoch [6802/20000], Loss: 8.975997924804688, Learning Rate: 0.01\n",
      "Epoch [6803/20000], Loss: 8.968795776367188, Learning Rate: 0.01\n",
      "Epoch [6804/20000], Loss: 8.961517333984375, Learning Rate: 0.01\n",
      "Epoch [6805/20000], Loss: 8.954055786132812, Learning Rate: 0.01\n",
      "Epoch [6806/20000], Loss: 8.946807861328125, Learning Rate: 0.01\n",
      "Epoch [6807/20000], Loss: 8.93963623046875, Learning Rate: 0.01\n",
      "Epoch [6808/20000], Loss: 8.932357788085938, Learning Rate: 0.01\n",
      "Epoch [6809/20000], Loss: 8.924972534179688, Learning Rate: 0.01\n",
      "Epoch [6810/20000], Loss: 8.917831420898438, Learning Rate: 0.01\n",
      "Epoch [6811/20000], Loss: 8.910476684570312, Learning Rate: 0.01\n",
      "Epoch [6812/20000], Loss: 8.903182983398438, Learning Rate: 0.01\n",
      "Epoch [6813/20000], Loss: 8.896148681640625, Learning Rate: 0.01\n",
      "Epoch [6814/20000], Loss: 8.888565063476562, Learning Rate: 0.01\n",
      "Epoch [6815/20000], Loss: 8.881515502929688, Learning Rate: 0.01\n",
      "Epoch [6816/20000], Loss: 8.874267578125, Learning Rate: 0.01\n",
      "Epoch [6817/20000], Loss: 8.866897583007812, Learning Rate: 0.01\n",
      "Epoch [6818/20000], Loss: 8.85980224609375, Learning Rate: 0.01\n",
      "Epoch [6819/20000], Loss: 8.85247802734375, Learning Rate: 0.01\n",
      "Epoch [6820/20000], Loss: 8.8450927734375, Learning Rate: 0.01\n",
      "Epoch [6821/20000], Loss: 8.838058471679688, Learning Rate: 0.01\n",
      "Epoch [6822/20000], Loss: 8.830642700195312, Learning Rate: 0.01\n",
      "Epoch [6823/20000], Loss: 8.823379516601562, Learning Rate: 0.01\n",
      "Epoch [6824/20000], Loss: 8.8162841796875, Learning Rate: 0.01\n",
      "Epoch [6825/20000], Loss: 8.80902099609375, Learning Rate: 0.01\n",
      "Epoch [6826/20000], Loss: 8.801666259765625, Learning Rate: 0.01\n",
      "Epoch [6827/20000], Loss: 8.794464111328125, Learning Rate: 0.01\n",
      "Epoch [6828/20000], Loss: 8.787200927734375, Learning Rate: 0.01\n",
      "Epoch [6829/20000], Loss: 8.780105590820312, Learning Rate: 0.01\n",
      "Epoch [6830/20000], Loss: 8.77276611328125, Learning Rate: 0.01\n",
      "Epoch [6831/20000], Loss: 8.765472412109375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6832/20000], Loss: 8.7581787109375, Learning Rate: 0.01\n",
      "Epoch [6833/20000], Loss: 8.750869750976562, Learning Rate: 0.01\n",
      "Epoch [6834/20000], Loss: 8.743728637695312, Learning Rate: 0.01\n",
      "Epoch [6835/20000], Loss: 8.736541748046875, Learning Rate: 0.01\n",
      "Epoch [6836/20000], Loss: 8.72943115234375, Learning Rate: 0.01\n",
      "Epoch [6837/20000], Loss: 8.721954345703125, Learning Rate: 0.01\n",
      "Epoch [6838/20000], Loss: 8.71484375, Learning Rate: 0.01\n",
      "Epoch [6839/20000], Loss: 8.707534790039062, Learning Rate: 0.01\n",
      "Epoch [6840/20000], Loss: 8.700363159179688, Learning Rate: 0.01\n",
      "Epoch [6841/20000], Loss: 8.693099975585938, Learning Rate: 0.01\n",
      "Epoch [6842/20000], Loss: 8.686004638671875, Learning Rate: 0.01\n",
      "Epoch [6843/20000], Loss: 8.678741455078125, Learning Rate: 0.01\n",
      "Epoch [6844/20000], Loss: 8.671417236328125, Learning Rate: 0.01\n",
      "Epoch [6845/20000], Loss: 8.664230346679688, Learning Rate: 0.01\n",
      "Epoch [6846/20000], Loss: 8.657180786132812, Learning Rate: 0.01\n",
      "Epoch [6847/20000], Loss: 8.649856567382812, Learning Rate: 0.01\n",
      "Epoch [6848/20000], Loss: 8.642654418945312, Learning Rate: 0.01\n",
      "Epoch [6849/20000], Loss: 8.635467529296875, Learning Rate: 0.01\n",
      "Epoch [6850/20000], Loss: 8.628097534179688, Learning Rate: 0.01\n",
      "Epoch [6851/20000], Loss: 8.62109375, Learning Rate: 0.01\n",
      "Epoch [6852/20000], Loss: 8.613906860351562, Learning Rate: 0.01\n",
      "Epoch [6853/20000], Loss: 8.606552124023438, Learning Rate: 0.01\n",
      "Epoch [6854/20000], Loss: 8.59942626953125, Learning Rate: 0.01\n",
      "Epoch [6855/20000], Loss: 8.592269897460938, Learning Rate: 0.01\n",
      "Epoch [6856/20000], Loss: 8.585006713867188, Learning Rate: 0.01\n",
      "Epoch [6857/20000], Loss: 8.577774047851562, Learning Rate: 0.01\n",
      "Epoch [6858/20000], Loss: 8.570465087890625, Learning Rate: 0.01\n",
      "Epoch [6859/20000], Loss: 8.563385009765625, Learning Rate: 0.01\n",
      "Epoch [6860/20000], Loss: 8.556167602539062, Learning Rate: 0.01\n",
      "Epoch [6861/20000], Loss: 8.549057006835938, Learning Rate: 0.01\n",
      "Epoch [6862/20000], Loss: 8.541885375976562, Learning Rate: 0.01\n",
      "Epoch [6863/20000], Loss: 8.534591674804688, Learning Rate: 0.01\n",
      "Epoch [6864/20000], Loss: 8.527374267578125, Learning Rate: 0.01\n",
      "Epoch [6865/20000], Loss: 8.520263671875, Learning Rate: 0.01\n",
      "Epoch [6866/20000], Loss: 8.513046264648438, Learning Rate: 0.01\n",
      "Epoch [6867/20000], Loss: 8.50579833984375, Learning Rate: 0.01\n",
      "Epoch [6868/20000], Loss: 8.49871826171875, Learning Rate: 0.01\n",
      "Epoch [6869/20000], Loss: 8.49163818359375, Learning Rate: 0.01\n",
      "Epoch [6870/20000], Loss: 8.484283447265625, Learning Rate: 0.01\n",
      "Epoch [6871/20000], Loss: 8.47711181640625, Learning Rate: 0.01\n",
      "Epoch [6872/20000], Loss: 8.469894409179688, Learning Rate: 0.01\n",
      "Epoch [6873/20000], Loss: 8.462631225585938, Learning Rate: 0.01\n",
      "Epoch [6874/20000], Loss: 8.45556640625, Learning Rate: 0.01\n",
      "Epoch [6875/20000], Loss: 8.4483642578125, Learning Rate: 0.01\n",
      "Epoch [6876/20000], Loss: 8.441238403320312, Learning Rate: 0.01\n",
      "Epoch [6877/20000], Loss: 8.434158325195312, Learning Rate: 0.01\n",
      "Epoch [6878/20000], Loss: 8.426895141601562, Learning Rate: 0.01\n",
      "Epoch [6879/20000], Loss: 8.419830322265625, Learning Rate: 0.01\n",
      "Epoch [6880/20000], Loss: 8.412643432617188, Learning Rate: 0.01\n",
      "Epoch [6881/20000], Loss: 8.405380249023438, Learning Rate: 0.01\n",
      "Epoch [6882/20000], Loss: 8.398284912109375, Learning Rate: 0.01\n",
      "Epoch [6883/20000], Loss: 8.391067504882812, Learning Rate: 0.01\n",
      "Epoch [6884/20000], Loss: 8.383880615234375, Learning Rate: 0.01\n",
      "Epoch [6885/20000], Loss: 8.376815795898438, Learning Rate: 0.01\n",
      "Epoch [6886/20000], Loss: 8.369659423828125, Learning Rate: 0.01\n",
      "Epoch [6887/20000], Loss: 8.362411499023438, Learning Rate: 0.01\n",
      "Epoch [6888/20000], Loss: 8.355239868164062, Learning Rate: 0.01\n",
      "Epoch [6889/20000], Loss: 8.348129272460938, Learning Rate: 0.01\n",
      "Epoch [6890/20000], Loss: 8.340927124023438, Learning Rate: 0.01\n",
      "Epoch [6891/20000], Loss: 8.333847045898438, Learning Rate: 0.01\n",
      "Epoch [6892/20000], Loss: 8.326568603515625, Learning Rate: 0.01\n",
      "Epoch [6893/20000], Loss: 8.319534301757812, Learning Rate: 0.01\n",
      "Epoch [6894/20000], Loss: 8.312423706054688, Learning Rate: 0.01\n",
      "Epoch [6895/20000], Loss: 8.30517578125, Learning Rate: 0.01\n",
      "Epoch [6896/20000], Loss: 8.29803466796875, Learning Rate: 0.01\n",
      "Epoch [6897/20000], Loss: 8.29083251953125, Learning Rate: 0.01\n",
      "Epoch [6898/20000], Loss: 8.283706665039062, Learning Rate: 0.01\n",
      "Epoch [6899/20000], Loss: 8.27655029296875, Learning Rate: 0.01\n",
      "Epoch [6900/20000], Loss: 8.26934814453125, Learning Rate: 0.01\n",
      "Epoch [6901/20000], Loss: 8.2623291015625, Learning Rate: 0.01\n",
      "Epoch [6902/20000], Loss: 8.255142211914062, Learning Rate: 0.01\n",
      "Epoch [6903/20000], Loss: 8.248092651367188, Learning Rate: 0.01\n",
      "Epoch [6904/20000], Loss: 8.240890502929688, Learning Rate: 0.01\n",
      "Epoch [6905/20000], Loss: 8.233810424804688, Learning Rate: 0.01\n",
      "Epoch [6906/20000], Loss: 8.226608276367188, Learning Rate: 0.01\n",
      "Epoch [6907/20000], Loss: 8.219558715820312, Learning Rate: 0.01\n",
      "Epoch [6908/20000], Loss: 8.212249755859375, Learning Rate: 0.01\n",
      "Epoch [6909/20000], Loss: 8.205276489257812, Learning Rate: 0.01\n",
      "Epoch [6910/20000], Loss: 8.198104858398438, Learning Rate: 0.01\n",
      "Epoch [6911/20000], Loss: 8.191024780273438, Learning Rate: 0.01\n",
      "Epoch [6912/20000], Loss: 8.183807373046875, Learning Rate: 0.01\n",
      "Epoch [6913/20000], Loss: 8.176681518554688, Learning Rate: 0.01\n",
      "Epoch [6914/20000], Loss: 8.1695556640625, Learning Rate: 0.01\n",
      "Epoch [6915/20000], Loss: 8.162551879882812, Learning Rate: 0.01\n",
      "Epoch [6916/20000], Loss: 8.155288696289062, Learning Rate: 0.01\n",
      "Epoch [6917/20000], Loss: 8.148208618164062, Learning Rate: 0.01\n",
      "Epoch [6918/20000], Loss: 8.141082763671875, Learning Rate: 0.01\n",
      "Epoch [6919/20000], Loss: 8.133895874023438, Learning Rate: 0.01\n",
      "Epoch [6920/20000], Loss: 8.126800537109375, Learning Rate: 0.01\n",
      "Epoch [6921/20000], Loss: 8.1197509765625, Learning Rate: 0.01\n",
      "Epoch [6922/20000], Loss: 8.112640380859375, Learning Rate: 0.01\n",
      "Epoch [6923/20000], Loss: 8.10540771484375, Learning Rate: 0.01\n",
      "Epoch [6924/20000], Loss: 8.098358154296875, Learning Rate: 0.01\n",
      "Epoch [6925/20000], Loss: 8.091400146484375, Learning Rate: 0.01\n",
      "Epoch [6926/20000], Loss: 8.084121704101562, Learning Rate: 0.01\n",
      "Epoch [6927/20000], Loss: 8.0770263671875, Learning Rate: 0.01\n",
      "Epoch [6928/20000], Loss: 8.069992065429688, Learning Rate: 0.01\n",
      "Epoch [6929/20000], Loss: 8.062728881835938, Learning Rate: 0.01\n",
      "Epoch [6930/20000], Loss: 8.055709838867188, Learning Rate: 0.01\n",
      "Epoch [6931/20000], Loss: 8.048553466796875, Learning Rate: 0.01\n",
      "Epoch [6932/20000], Loss: 8.041748046875, Learning Rate: 0.01\n",
      "Epoch [6933/20000], Loss: 8.034454345703125, Learning Rate: 0.01\n",
      "Epoch [6934/20000], Loss: 8.027496337890625, Learning Rate: 0.01\n",
      "Epoch [6935/20000], Loss: 8.020706176757812, Learning Rate: 0.01\n",
      "Epoch [6936/20000], Loss: 8.013656616210938, Learning Rate: 0.01\n",
      "Epoch [6937/20000], Loss: 8.006973266601562, Learning Rate: 0.01\n",
      "Epoch [6938/20000], Loss: 8.000473022460938, Learning Rate: 0.01\n",
      "Epoch [6939/20000], Loss: 7.994354248046875, Learning Rate: 0.01\n",
      "Epoch [6940/20000], Loss: 7.9889373779296875, Learning Rate: 0.01\n",
      "Epoch [6941/20000], Loss: 7.984649658203125, Learning Rate: 0.01\n",
      "Epoch [6942/20000], Loss: 7.9820709228515625, Learning Rate: 0.01\n",
      "Epoch [6943/20000], Loss: 7.9825439453125, Learning Rate: 0.01\n",
      "Epoch [6944/20000], Loss: 7.9880523681640625, Learning Rate: 0.01\n",
      "Epoch [6945/20000], Loss: 8.00250244140625, Learning Rate: 0.01\n",
      "Epoch [6946/20000], Loss: 8.031845092773438, Learning Rate: 0.01\n",
      "Epoch [6947/20000], Loss: 8.086532592773438, Learning Rate: 0.01\n",
      "Epoch [6948/20000], Loss: 8.185501098632812, Learning Rate: 0.01\n",
      "Epoch [6949/20000], Loss: 8.35968017578125, Learning Rate: 0.01\n",
      "Epoch [6950/20000], Loss: 8.663314819335938, Learning Rate: 0.01\n",
      "Epoch [6951/20000], Loss: 9.188629150390625, Learning Rate: 0.01\n",
      "Epoch [6952/20000], Loss: 10.088058471679688, Learning Rate: 0.01\n",
      "Epoch [6953/20000], Loss: 11.60406494140625, Learning Rate: 0.01\n",
      "Epoch [6954/20000], Loss: 14.090499877929688, Learning Rate: 0.01\n",
      "Epoch [6955/20000], Loss: 17.960540771484375, Learning Rate: 0.01\n",
      "Epoch [6956/20000], Loss: 23.458892822265625, Learning Rate: 0.01\n",
      "Epoch [6957/20000], Loss: 29.98211669921875, Learning Rate: 0.01\n",
      "Epoch [6958/20000], Loss: 35.233489990234375, Learning Rate: 0.01\n",
      "Epoch [6959/20000], Loss: 35.20631408691406, Learning Rate: 0.01\n",
      "Epoch [6960/20000], Loss: 27.573379516601562, Learning Rate: 0.01\n",
      "Epoch [6961/20000], Loss: 15.930633544921875, Learning Rate: 0.01\n",
      "Epoch [6962/20000], Loss: 8.407089233398438, Learning Rate: 0.01\n",
      "Epoch [6963/20000], Loss: 9.48870849609375, Learning Rate: 0.01\n",
      "Epoch [6964/20000], Loss: 15.830307006835938, Learning Rate: 0.01\n",
      "Epoch [6965/20000], Loss: 20.117904663085938, Learning Rate: 0.01\n",
      "Epoch [6966/20000], Loss: 17.873672485351562, Learning Rate: 0.01\n",
      "Epoch [6967/20000], Loss: 11.627090454101562, Learning Rate: 0.01\n",
      "Epoch [6968/20000], Loss: 7.8565673828125, Learning Rate: 0.01\n",
      "Epoch [6969/20000], Loss: 9.520614624023438, Learning Rate: 0.01\n",
      "Epoch [6970/20000], Loss: 13.319732666015625, Learning Rate: 0.01\n",
      "Epoch [6971/20000], Loss: 14.267745971679688, Learning Rate: 0.01\n",
      "Epoch [6972/20000], Loss: 11.383895874023438, Learning Rate: 0.01\n",
      "Epoch [6973/20000], Loss: 8.228134155273438, Learning Rate: 0.01\n",
      "Epoch [6974/20000], Loss: 8.12518310546875, Learning Rate: 0.01\n",
      "Epoch [6975/20000], Loss: 10.304611206054688, Learning Rate: 0.01\n",
      "Epoch [6976/20000], Loss: 11.522369384765625, Learning Rate: 0.01\n",
      "Epoch [6977/20000], Loss: 10.267501831054688, Learning Rate: 0.01\n",
      "Epoch [6978/20000], Loss: 8.242034912109375, Learning Rate: 0.01\n",
      "Epoch [6979/20000], Loss: 7.7909393310546875, Learning Rate: 0.01\n",
      "Epoch [6980/20000], Loss: 8.9637451171875, Learning Rate: 0.01\n",
      "Epoch [6981/20000], Loss: 9.877822875976562, Learning Rate: 0.01\n",
      "Epoch [6982/20000], Loss: 9.327072143554688, Learning Rate: 0.01\n",
      "Epoch [6983/20000], Loss: 8.104263305664062, Learning Rate: 0.01\n",
      "Epoch [6984/20000], Loss: 7.6837615966796875, Learning Rate: 0.01\n",
      "Epoch [6985/20000], Loss: 8.304641723632812, Learning Rate: 0.01\n",
      "Epoch [6986/20000], Loss: 8.913925170898438, Learning Rate: 0.01\n",
      "Epoch [6987/20000], Loss: 8.681488037109375, Learning Rate: 0.01\n",
      "Epoch [6988/20000], Loss: 7.95843505859375, Learning Rate: 0.01\n",
      "Epoch [6989/20000], Loss: 7.6322479248046875, Learning Rate: 0.01\n",
      "Epoch [6990/20000], Loss: 7.94927978515625, Learning Rate: 0.01\n",
      "Epoch [6991/20000], Loss: 8.3394775390625, Learning Rate: 0.01\n",
      "Epoch [6992/20000], Loss: 8.259170532226562, Learning Rate: 0.01\n",
      "Epoch [6993/20000], Loss: 7.838775634765625, Learning Rate: 0.01\n",
      "Epoch [6994/20000], Loss: 7.597625732421875, Learning Rate: 0.01\n",
      "Epoch [6995/20000], Loss: 7.7452392578125, Learning Rate: 0.01\n",
      "Epoch [6996/20000], Loss: 7.9891357421875, Learning Rate: 0.01\n",
      "Epoch [6997/20000], Loss: 7.9816436767578125, Learning Rate: 0.01\n",
      "Epoch [6998/20000], Loss: 7.7435760498046875, Learning Rate: 0.01\n",
      "Epoch [6999/20000], Loss: 7.569091796875, Learning Rate: 0.01\n",
      "Epoch [7000/20000], Loss: 7.6225433349609375, Learning Rate: 0.01\n",
      "Epoch [7001/20000], Loss: 7.769744873046875, Learning Rate: 0.01\n",
      "Epoch [7002/20000], Loss: 7.793701171875, Learning Rate: 0.01\n",
      "Epoch [7003/20000], Loss: 7.6655426025390625, Learning Rate: 0.01\n",
      "Epoch [7004/20000], Loss: 7.542266845703125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7005/20000], Loss: 7.5460357666015625, Learning Rate: 0.01\n",
      "Epoch [7006/20000], Loss: 7.62896728515625, Learning Rate: 0.01\n",
      "Epoch [7007/20000], Loss: 7.6617279052734375, Learning Rate: 0.01\n",
      "Epoch [7008/20000], Loss: 7.5985260009765625, Learning Rate: 0.01\n",
      "Epoch [7009/20000], Loss: 7.5145721435546875, Learning Rate: 0.01\n",
      "Epoch [7010/20000], Loss: 7.49481201171875, Learning Rate: 0.01\n",
      "Epoch [7011/20000], Loss: 7.5352630615234375, Learning Rate: 0.01\n",
      "Epoch [7012/20000], Loss: 7.564605712890625, Learning Rate: 0.01\n",
      "Epoch [7013/20000], Loss: 7.53851318359375, Learning Rate: 0.01\n",
      "Epoch [7014/20000], Loss: 7.484893798828125, Learning Rate: 0.01\n",
      "Epoch [7015/20000], Loss: 7.45721435546875, Learning Rate: 0.01\n",
      "Epoch [7016/20000], Loss: 7.4703826904296875, Learning Rate: 0.01\n",
      "Epoch [7017/20000], Loss: 7.4909515380859375, Learning Rate: 0.01\n",
      "Epoch [7018/20000], Loss: 7.4837646484375, Learning Rate: 0.01\n",
      "Epoch [7019/20000], Loss: 7.4518280029296875, Learning Rate: 0.01\n",
      "Epoch [7020/20000], Loss: 7.42547607421875, Learning Rate: 0.01\n",
      "Epoch [7021/20000], Loss: 7.4229278564453125, Learning Rate: 0.01\n",
      "Epoch [7022/20000], Loss: 7.4331512451171875, Learning Rate: 0.01\n",
      "Epoch [7023/20000], Loss: 7.4334869384765625, Learning Rate: 0.01\n",
      "Epoch [7024/20000], Loss: 7.4164886474609375, Learning Rate: 0.01\n",
      "Epoch [7025/20000], Loss: 7.395263671875, Learning Rate: 0.01\n",
      "Epoch [7026/20000], Loss: 7.385009765625, Learning Rate: 0.01\n",
      "Epoch [7027/20000], Loss: 7.3864288330078125, Learning Rate: 0.01\n",
      "Epoch [7028/20000], Loss: 7.3876190185546875, Learning Rate: 0.01\n",
      "Epoch [7029/20000], Loss: 7.3792724609375, Learning Rate: 0.01\n",
      "Epoch [7030/20000], Loss: 7.3641357421875, Learning Rate: 0.01\n",
      "Epoch [7031/20000], Loss: 7.351898193359375, Learning Rate: 0.01\n",
      "Epoch [7032/20000], Loss: 7.3470916748046875, Learning Rate: 0.01\n",
      "Epoch [7033/20000], Loss: 7.3458251953125, Learning Rate: 0.01\n",
      "Epoch [7034/20000], Loss: 7.341339111328125, Learning Rate: 0.01\n",
      "Epoch [7035/20000], Loss: 7.3314971923828125, Learning Rate: 0.01\n",
      "Epoch [7036/20000], Loss: 7.320404052734375, Learning Rate: 0.01\n",
      "Epoch [7037/20000], Loss: 7.3123931884765625, Learning Rate: 0.01\n",
      "Epoch [7038/20000], Loss: 7.30804443359375, Learning Rate: 0.01\n",
      "Epoch [7039/20000], Loss: 7.3040618896484375, Learning Rate: 0.01\n",
      "Epoch [7040/20000], Loss: 7.297332763671875, Learning Rate: 0.01\n",
      "Epoch [7041/20000], Loss: 7.2883453369140625, Learning Rate: 0.01\n",
      "Epoch [7042/20000], Loss: 7.2798004150390625, Learning Rate: 0.01\n",
      "Epoch [7043/20000], Loss: 7.2733154296875, Learning Rate: 0.01\n",
      "Epoch [7044/20000], Loss: 7.2681732177734375, Learning Rate: 0.01\n",
      "Epoch [7045/20000], Loss: 7.2627410888671875, Learning Rate: 0.01\n",
      "Epoch [7046/20000], Loss: 7.2554779052734375, Learning Rate: 0.01\n",
      "Epoch [7047/20000], Loss: 7.24749755859375, Learning Rate: 0.01\n",
      "Epoch [7048/20000], Loss: 7.2400054931640625, Learning Rate: 0.01\n",
      "Epoch [7049/20000], Loss: 7.2339324951171875, Learning Rate: 0.01\n",
      "Epoch [7050/20000], Loss: 7.2281951904296875, Learning Rate: 0.01\n",
      "Epoch [7051/20000], Loss: 7.222076416015625, Learning Rate: 0.01\n",
      "Epoch [7052/20000], Loss: 7.2148895263671875, Learning Rate: 0.01\n",
      "Epoch [7053/20000], Loss: 7.2075958251953125, Learning Rate: 0.01\n",
      "Epoch [7054/20000], Loss: 7.20074462890625, Learning Rate: 0.01\n",
      "Epoch [7055/20000], Loss: 7.194305419921875, Learning Rate: 0.01\n",
      "Epoch [7056/20000], Loss: 7.188385009765625, Learning Rate: 0.01\n",
      "Epoch [7057/20000], Loss: 7.18182373046875, Learning Rate: 0.01\n",
      "Epoch [7058/20000], Loss: 7.1750335693359375, Learning Rate: 0.01\n",
      "Epoch [7059/20000], Loss: 7.167999267578125, Learning Rate: 0.01\n",
      "Epoch [7060/20000], Loss: 7.1613006591796875, Learning Rate: 0.01\n",
      "Epoch [7061/20000], Loss: 7.1548919677734375, Learning Rate: 0.01\n",
      "Epoch [7062/20000], Loss: 7.1486358642578125, Learning Rate: 0.01\n",
      "Epoch [7063/20000], Loss: 7.142242431640625, Learning Rate: 0.01\n",
      "Epoch [7064/20000], Loss: 7.1354522705078125, Learning Rate: 0.01\n",
      "Epoch [7065/20000], Loss: 7.1286468505859375, Learning Rate: 0.01\n",
      "Epoch [7066/20000], Loss: 7.122100830078125, Learning Rate: 0.01\n",
      "Epoch [7067/20000], Loss: 7.115570068359375, Learning Rate: 0.01\n",
      "Epoch [7068/20000], Loss: 7.1091156005859375, Learning Rate: 0.01\n",
      "Epoch [7069/20000], Loss: 7.102783203125, Learning Rate: 0.01\n",
      "Epoch [7070/20000], Loss: 7.0960845947265625, Learning Rate: 0.01\n",
      "Epoch [7071/20000], Loss: 7.08941650390625, Learning Rate: 0.01\n",
      "Epoch [7072/20000], Loss: 7.0828399658203125, Learning Rate: 0.01\n",
      "Epoch [7073/20000], Loss: 7.0763092041015625, Learning Rate: 0.01\n",
      "Epoch [7074/20000], Loss: 7.06988525390625, Learning Rate: 0.01\n",
      "Epoch [7075/20000], Loss: 7.063262939453125, Learning Rate: 0.01\n",
      "Epoch [7076/20000], Loss: 7.0567626953125, Learning Rate: 0.01\n",
      "Epoch [7077/20000], Loss: 7.05023193359375, Learning Rate: 0.01\n",
      "Epoch [7078/20000], Loss: 7.0435791015625, Learning Rate: 0.01\n",
      "Epoch [7079/20000], Loss: 7.0370635986328125, Learning Rate: 0.01\n",
      "Epoch [7080/20000], Loss: 7.030609130859375, Learning Rate: 0.01\n",
      "Epoch [7081/20000], Loss: 7.0240020751953125, Learning Rate: 0.01\n",
      "Epoch [7082/20000], Loss: 7.0174102783203125, Learning Rate: 0.01\n",
      "Epoch [7083/20000], Loss: 7.0110015869140625, Learning Rate: 0.01\n",
      "Epoch [7084/20000], Loss: 7.0043487548828125, Learning Rate: 0.01\n",
      "Epoch [7085/20000], Loss: 6.9978485107421875, Learning Rate: 0.01\n",
      "Epoch [7086/20000], Loss: 6.991363525390625, Learning Rate: 0.01\n",
      "Epoch [7087/20000], Loss: 6.984771728515625, Learning Rate: 0.01\n",
      "Epoch [7088/20000], Loss: 6.97821044921875, Learning Rate: 0.01\n",
      "Epoch [7089/20000], Loss: 6.9717254638671875, Learning Rate: 0.01\n",
      "Epoch [7090/20000], Loss: 6.965240478515625, Learning Rate: 0.01\n",
      "Epoch [7091/20000], Loss: 6.958709716796875, Learning Rate: 0.01\n",
      "Epoch [7092/20000], Loss: 6.952056884765625, Learning Rate: 0.01\n",
      "Epoch [7093/20000], Loss: 6.94561767578125, Learning Rate: 0.01\n",
      "Epoch [7094/20000], Loss: 6.9390106201171875, Learning Rate: 0.01\n",
      "Epoch [7095/20000], Loss: 6.93267822265625, Learning Rate: 0.01\n",
      "Epoch [7096/20000], Loss: 6.9260101318359375, Learning Rate: 0.01\n",
      "Epoch [7097/20000], Loss: 6.9195404052734375, Learning Rate: 0.01\n",
      "Epoch [7098/20000], Loss: 6.91290283203125, Learning Rate: 0.01\n",
      "Epoch [7099/20000], Loss: 6.906463623046875, Learning Rate: 0.01\n",
      "Epoch [7100/20000], Loss: 6.89990234375, Learning Rate: 0.01\n",
      "Epoch [7101/20000], Loss: 6.8935089111328125, Learning Rate: 0.01\n",
      "Epoch [7102/20000], Loss: 6.8868865966796875, Learning Rate: 0.01\n",
      "Epoch [7103/20000], Loss: 6.880340576171875, Learning Rate: 0.01\n",
      "Epoch [7104/20000], Loss: 6.8738555908203125, Learning Rate: 0.01\n",
      "Epoch [7105/20000], Loss: 6.867279052734375, Learning Rate: 0.01\n",
      "Epoch [7106/20000], Loss: 6.860870361328125, Learning Rate: 0.01\n",
      "Epoch [7107/20000], Loss: 6.85430908203125, Learning Rate: 0.01\n",
      "Epoch [7108/20000], Loss: 6.8477325439453125, Learning Rate: 0.01\n",
      "Epoch [7109/20000], Loss: 6.8412933349609375, Learning Rate: 0.01\n",
      "Epoch [7110/20000], Loss: 6.83477783203125, Learning Rate: 0.01\n",
      "Epoch [7111/20000], Loss: 6.828155517578125, Learning Rate: 0.01\n",
      "Epoch [7112/20000], Loss: 6.821807861328125, Learning Rate: 0.01\n",
      "Epoch [7113/20000], Loss: 6.815093994140625, Learning Rate: 0.01\n",
      "Epoch [7114/20000], Loss: 6.808746337890625, Learning Rate: 0.01\n",
      "Epoch [7115/20000], Loss: 6.802093505859375, Learning Rate: 0.01\n",
      "Epoch [7116/20000], Loss: 6.79571533203125, Learning Rate: 0.01\n",
      "Epoch [7117/20000], Loss: 6.7890167236328125, Learning Rate: 0.01\n",
      "Epoch [7118/20000], Loss: 6.7826385498046875, Learning Rate: 0.01\n",
      "Epoch [7119/20000], Loss: 6.776123046875, Learning Rate: 0.01\n",
      "Epoch [7120/20000], Loss: 6.7696380615234375, Learning Rate: 0.01\n",
      "Epoch [7121/20000], Loss: 6.76318359375, Learning Rate: 0.01\n",
      "Epoch [7122/20000], Loss: 6.7567138671875, Learning Rate: 0.01\n",
      "Epoch [7123/20000], Loss: 6.7502593994140625, Learning Rate: 0.01\n",
      "Epoch [7124/20000], Loss: 6.7436370849609375, Learning Rate: 0.01\n",
      "Epoch [7125/20000], Loss: 6.737060546875, Learning Rate: 0.01\n",
      "Epoch [7126/20000], Loss: 6.730621337890625, Learning Rate: 0.01\n",
      "Epoch [7127/20000], Loss: 6.723968505859375, Learning Rate: 0.01\n",
      "Epoch [7128/20000], Loss: 6.7175140380859375, Learning Rate: 0.01\n",
      "Epoch [7129/20000], Loss: 6.7110443115234375, Learning Rate: 0.01\n",
      "Epoch [7130/20000], Loss: 6.70440673828125, Learning Rate: 0.01\n",
      "Epoch [7131/20000], Loss: 6.6979827880859375, Learning Rate: 0.01\n",
      "Epoch [7132/20000], Loss: 6.691497802734375, Learning Rate: 0.01\n",
      "Epoch [7133/20000], Loss: 6.6850128173828125, Learning Rate: 0.01\n",
      "Epoch [7134/20000], Loss: 6.6785430908203125, Learning Rate: 0.01\n",
      "Epoch [7135/20000], Loss: 6.6718597412109375, Learning Rate: 0.01\n",
      "Epoch [7136/20000], Loss: 6.665496826171875, Learning Rate: 0.01\n",
      "Epoch [7137/20000], Loss: 6.65911865234375, Learning Rate: 0.01\n",
      "Epoch [7138/20000], Loss: 6.652679443359375, Learning Rate: 0.01\n",
      "Epoch [7139/20000], Loss: 6.64605712890625, Learning Rate: 0.01\n",
      "Epoch [7140/20000], Loss: 6.6394195556640625, Learning Rate: 0.01\n",
      "Epoch [7141/20000], Loss: 6.6329193115234375, Learning Rate: 0.01\n",
      "Epoch [7142/20000], Loss: 6.6264801025390625, Learning Rate: 0.01\n",
      "Epoch [7143/20000], Loss: 6.62005615234375, Learning Rate: 0.01\n",
      "Epoch [7144/20000], Loss: 6.6135406494140625, Learning Rate: 0.01\n",
      "Epoch [7145/20000], Loss: 6.607086181640625, Learning Rate: 0.01\n",
      "Epoch [7146/20000], Loss: 6.6005096435546875, Learning Rate: 0.01\n",
      "Epoch [7147/20000], Loss: 6.594024658203125, Learning Rate: 0.01\n",
      "Epoch [7148/20000], Loss: 6.587493896484375, Learning Rate: 0.01\n",
      "Epoch [7149/20000], Loss: 6.5810089111328125, Learning Rate: 0.01\n",
      "Epoch [7150/20000], Loss: 6.574493408203125, Learning Rate: 0.01\n",
      "Epoch [7151/20000], Loss: 6.5681610107421875, Learning Rate: 0.01\n",
      "Epoch [7152/20000], Loss: 6.56158447265625, Learning Rate: 0.01\n",
      "Epoch [7153/20000], Loss: 6.5550689697265625, Learning Rate: 0.01\n",
      "Epoch [7154/20000], Loss: 6.5485687255859375, Learning Rate: 0.01\n",
      "Epoch [7155/20000], Loss: 6.5421600341796875, Learning Rate: 0.01\n",
      "Epoch [7156/20000], Loss: 6.5355377197265625, Learning Rate: 0.01\n",
      "Epoch [7157/20000], Loss: 6.5289764404296875, Learning Rate: 0.01\n",
      "Epoch [7158/20000], Loss: 6.522735595703125, Learning Rate: 0.01\n",
      "Epoch [7159/20000], Loss: 6.5160369873046875, Learning Rate: 0.01\n",
      "Epoch [7160/20000], Loss: 6.5096893310546875, Learning Rate: 0.01\n",
      "Epoch [7161/20000], Loss: 6.5033111572265625, Learning Rate: 0.01\n",
      "Epoch [7162/20000], Loss: 6.49664306640625, Learning Rate: 0.01\n",
      "Epoch [7163/20000], Loss: 6.4901885986328125, Learning Rate: 0.01\n",
      "Epoch [7164/20000], Loss: 6.483673095703125, Learning Rate: 0.01\n",
      "Epoch [7165/20000], Loss: 6.4771575927734375, Learning Rate: 0.01\n",
      "Epoch [7166/20000], Loss: 6.4707794189453125, Learning Rate: 0.01\n",
      "Epoch [7167/20000], Loss: 6.4643402099609375, Learning Rate: 0.01\n",
      "Epoch [7168/20000], Loss: 6.4576416015625, Learning Rate: 0.01\n",
      "Epoch [7169/20000], Loss: 6.4513092041015625, Learning Rate: 0.01\n",
      "Epoch [7170/20000], Loss: 6.4448089599609375, Learning Rate: 0.01\n",
      "Epoch [7171/20000], Loss: 6.438385009765625, Learning Rate: 0.01\n",
      "Epoch [7172/20000], Loss: 6.4318084716796875, Learning Rate: 0.01\n",
      "Epoch [7173/20000], Loss: 6.42529296875, Learning Rate: 0.01\n",
      "Epoch [7174/20000], Loss: 6.4187774658203125, Learning Rate: 0.01\n",
      "Epoch [7175/20000], Loss: 6.412384033203125, Learning Rate: 0.01\n",
      "Epoch [7176/20000], Loss: 6.405914306640625, Learning Rate: 0.01\n",
      "Epoch [7177/20000], Loss: 6.3994903564453125, Learning Rate: 0.01\n",
      "Epoch [7178/20000], Loss: 6.3930511474609375, Learning Rate: 0.01\n",
      "Epoch [7179/20000], Loss: 6.3864288330078125, Learning Rate: 0.01\n",
      "Epoch [7180/20000], Loss: 6.38006591796875, Learning Rate: 0.01\n",
      "Epoch [7181/20000], Loss: 6.3734588623046875, Learning Rate: 0.01\n",
      "Epoch [7182/20000], Loss: 6.3671112060546875, Learning Rate: 0.01\n",
      "Epoch [7183/20000], Loss: 6.3605499267578125, Learning Rate: 0.01\n",
      "Epoch [7184/20000], Loss: 6.354034423828125, Learning Rate: 0.01\n",
      "Epoch [7185/20000], Loss: 6.347503662109375, Learning Rate: 0.01\n",
      "Epoch [7186/20000], Loss: 6.341094970703125, Learning Rate: 0.01\n",
      "Epoch [7187/20000], Loss: 6.3345947265625, Learning Rate: 0.01\n",
      "Epoch [7188/20000], Loss: 6.328369140625, Learning Rate: 0.01\n",
      "Epoch [7189/20000], Loss: 6.3216552734375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7190/20000], Loss: 6.315277099609375, Learning Rate: 0.01\n",
      "Epoch [7191/20000], Loss: 6.3087921142578125, Learning Rate: 0.01\n",
      "Epoch [7192/20000], Loss: 6.302154541015625, Learning Rate: 0.01\n",
      "Epoch [7193/20000], Loss: 6.295867919921875, Learning Rate: 0.01\n",
      "Epoch [7194/20000], Loss: 6.2894134521484375, Learning Rate: 0.01\n",
      "Epoch [7195/20000], Loss: 6.2828826904296875, Learning Rate: 0.01\n",
      "Epoch [7196/20000], Loss: 6.2763824462890625, Learning Rate: 0.01\n",
      "Epoch [7197/20000], Loss: 6.2700042724609375, Learning Rate: 0.01\n",
      "Epoch [7198/20000], Loss: 6.263336181640625, Learning Rate: 0.01\n",
      "Epoch [7199/20000], Loss: 6.25701904296875, Learning Rate: 0.01\n",
      "Epoch [7200/20000], Loss: 6.250579833984375, Learning Rate: 0.01\n",
      "Epoch [7201/20000], Loss: 6.243988037109375, Learning Rate: 0.01\n",
      "Epoch [7202/20000], Loss: 6.23748779296875, Learning Rate: 0.01\n",
      "Epoch [7203/20000], Loss: 6.231201171875, Learning Rate: 0.01\n",
      "Epoch [7204/20000], Loss: 6.224578857421875, Learning Rate: 0.01\n",
      "Epoch [7205/20000], Loss: 6.2181243896484375, Learning Rate: 0.01\n",
      "Epoch [7206/20000], Loss: 6.2115325927734375, Learning Rate: 0.01\n",
      "Epoch [7207/20000], Loss: 6.2052764892578125, Learning Rate: 0.01\n",
      "Epoch [7208/20000], Loss: 6.19879150390625, Learning Rate: 0.01\n",
      "Epoch [7209/20000], Loss: 6.19244384765625, Learning Rate: 0.01\n",
      "Epoch [7210/20000], Loss: 6.185821533203125, Learning Rate: 0.01\n",
      "Epoch [7211/20000], Loss: 6.1793212890625, Learning Rate: 0.01\n",
      "Epoch [7212/20000], Loss: 6.1729278564453125, Learning Rate: 0.01\n",
      "Epoch [7213/20000], Loss: 6.16644287109375, Learning Rate: 0.01\n",
      "Epoch [7214/20000], Loss: 6.1600189208984375, Learning Rate: 0.01\n",
      "Epoch [7215/20000], Loss: 6.153594970703125, Learning Rate: 0.01\n",
      "Epoch [7216/20000], Loss: 6.146942138671875, Learning Rate: 0.01\n",
      "Epoch [7217/20000], Loss: 6.1407012939453125, Learning Rate: 0.01\n",
      "Epoch [7218/20000], Loss: 6.1340789794921875, Learning Rate: 0.01\n",
      "Epoch [7219/20000], Loss: 6.1277618408203125, Learning Rate: 0.01\n",
      "Epoch [7220/20000], Loss: 6.1210479736328125, Learning Rate: 0.01\n",
      "Epoch [7221/20000], Loss: 6.114715576171875, Learning Rate: 0.01\n",
      "Epoch [7222/20000], Loss: 6.10821533203125, Learning Rate: 0.01\n",
      "Epoch [7223/20000], Loss: 6.101806640625, Learning Rate: 0.01\n",
      "Epoch [7224/20000], Loss: 6.0953521728515625, Learning Rate: 0.01\n",
      "Epoch [7225/20000], Loss: 6.0889892578125, Learning Rate: 0.01\n",
      "Epoch [7226/20000], Loss: 6.0824127197265625, Learning Rate: 0.01\n",
      "Epoch [7227/20000], Loss: 6.0759735107421875, Learning Rate: 0.01\n",
      "Epoch [7228/20000], Loss: 6.0695037841796875, Learning Rate: 0.01\n",
      "Epoch [7229/20000], Loss: 6.0631256103515625, Learning Rate: 0.01\n",
      "Epoch [7230/20000], Loss: 6.0567169189453125, Learning Rate: 0.01\n",
      "Epoch [7231/20000], Loss: 6.0501861572265625, Learning Rate: 0.01\n",
      "Epoch [7232/20000], Loss: 6.043701171875, Learning Rate: 0.01\n",
      "Epoch [7233/20000], Loss: 6.037261962890625, Learning Rate: 0.01\n",
      "Epoch [7234/20000], Loss: 6.0308990478515625, Learning Rate: 0.01\n",
      "Epoch [7235/20000], Loss: 6.0244598388671875, Learning Rate: 0.01\n",
      "Epoch [7236/20000], Loss: 6.017974853515625, Learning Rate: 0.01\n",
      "Epoch [7237/20000], Loss: 6.011505126953125, Learning Rate: 0.01\n",
      "Epoch [7238/20000], Loss: 6.0048828125, Learning Rate: 0.01\n",
      "Epoch [7239/20000], Loss: 5.9984283447265625, Learning Rate: 0.01\n",
      "Epoch [7240/20000], Loss: 5.9921112060546875, Learning Rate: 0.01\n",
      "Epoch [7241/20000], Loss: 5.98565673828125, Learning Rate: 0.01\n",
      "Epoch [7242/20000], Loss: 5.979156494140625, Learning Rate: 0.01\n",
      "Epoch [7243/20000], Loss: 5.972808837890625, Learning Rate: 0.01\n",
      "Epoch [7244/20000], Loss: 5.9661712646484375, Learning Rate: 0.01\n",
      "Epoch [7245/20000], Loss: 5.9599609375, Learning Rate: 0.01\n",
      "Epoch [7246/20000], Loss: 5.95318603515625, Learning Rate: 0.01\n",
      "Epoch [7247/20000], Loss: 5.946929931640625, Learning Rate: 0.01\n",
      "Epoch [7248/20000], Loss: 5.9405517578125, Learning Rate: 0.01\n",
      "Epoch [7249/20000], Loss: 5.93408203125, Learning Rate: 0.01\n",
      "Epoch [7250/20000], Loss: 5.9274749755859375, Learning Rate: 0.01\n",
      "Epoch [7251/20000], Loss: 5.9210357666015625, Learning Rate: 0.01\n",
      "Epoch [7252/20000], Loss: 5.9148101806640625, Learning Rate: 0.01\n",
      "Epoch [7253/20000], Loss: 5.9081878662109375, Learning Rate: 0.01\n",
      "Epoch [7254/20000], Loss: 5.9018096923828125, Learning Rate: 0.01\n",
      "Epoch [7255/20000], Loss: 5.8953094482421875, Learning Rate: 0.01\n",
      "Epoch [7256/20000], Loss: 5.888885498046875, Learning Rate: 0.01\n",
      "Epoch [7257/20000], Loss: 5.8824005126953125, Learning Rate: 0.01\n",
      "Epoch [7258/20000], Loss: 5.87603759765625, Learning Rate: 0.01\n",
      "Epoch [7259/20000], Loss: 5.8695220947265625, Learning Rate: 0.01\n",
      "Epoch [7260/20000], Loss: 5.86297607421875, Learning Rate: 0.01\n",
      "Epoch [7261/20000], Loss: 5.85675048828125, Learning Rate: 0.01\n",
      "Epoch [7262/20000], Loss: 5.8502349853515625, Learning Rate: 0.01\n",
      "Epoch [7263/20000], Loss: 5.843780517578125, Learning Rate: 0.01\n",
      "Epoch [7264/20000], Loss: 5.8373565673828125, Learning Rate: 0.01\n",
      "Epoch [7265/20000], Loss: 5.83074951171875, Learning Rate: 0.01\n",
      "Epoch [7266/20000], Loss: 5.8245697021484375, Learning Rate: 0.01\n",
      "Epoch [7267/20000], Loss: 5.81793212890625, Learning Rate: 0.01\n",
      "Epoch [7268/20000], Loss: 5.8115081787109375, Learning Rate: 0.01\n",
      "Epoch [7269/20000], Loss: 5.8052215576171875, Learning Rate: 0.01\n",
      "Epoch [7270/20000], Loss: 5.798583984375, Learning Rate: 0.01\n",
      "Epoch [7271/20000], Loss: 5.79217529296875, Learning Rate: 0.01\n",
      "Epoch [7272/20000], Loss: 5.7858428955078125, Learning Rate: 0.01\n",
      "Epoch [7273/20000], Loss: 5.779510498046875, Learning Rate: 0.01\n",
      "Epoch [7274/20000], Loss: 5.7730560302734375, Learning Rate: 0.01\n",
      "Epoch [7275/20000], Loss: 5.7665252685546875, Learning Rate: 0.01\n",
      "Epoch [7276/20000], Loss: 5.760040283203125, Learning Rate: 0.01\n",
      "Epoch [7277/20000], Loss: 5.75347900390625, Learning Rate: 0.01\n",
      "Epoch [7278/20000], Loss: 5.7471923828125, Learning Rate: 0.01\n",
      "Epoch [7279/20000], Loss: 5.74072265625, Learning Rate: 0.01\n",
      "Epoch [7280/20000], Loss: 5.734344482421875, Learning Rate: 0.01\n",
      "Epoch [7281/20000], Loss: 5.72796630859375, Learning Rate: 0.01\n",
      "Epoch [7282/20000], Loss: 5.7215118408203125, Learning Rate: 0.01\n",
      "Epoch [7283/20000], Loss: 5.7151947021484375, Learning Rate: 0.01\n",
      "Epoch [7284/20000], Loss: 5.7088623046875, Learning Rate: 0.01\n",
      "Epoch [7285/20000], Loss: 5.7025604248046875, Learning Rate: 0.01\n",
      "Epoch [7286/20000], Loss: 5.6965789794921875, Learning Rate: 0.01\n",
      "Epoch [7287/20000], Loss: 5.690460205078125, Learning Rate: 0.01\n",
      "Epoch [7288/20000], Loss: 5.6847991943359375, Learning Rate: 0.01\n",
      "Epoch [7289/20000], Loss: 5.679534912109375, Learning Rate: 0.01\n",
      "Epoch [7290/20000], Loss: 5.67498779296875, Learning Rate: 0.01\n",
      "Epoch [7291/20000], Loss: 5.6715545654296875, Learning Rate: 0.01\n",
      "Epoch [7292/20000], Loss: 5.6702880859375, Learning Rate: 0.01\n",
      "Epoch [7293/20000], Loss: 5.67193603515625, Learning Rate: 0.01\n",
      "Epoch [7294/20000], Loss: 5.6790008544921875, Learning Rate: 0.01\n",
      "Epoch [7295/20000], Loss: 5.6950225830078125, Learning Rate: 0.01\n",
      "Epoch [7296/20000], Loss: 5.726226806640625, Learning Rate: 0.01\n",
      "Epoch [7297/20000], Loss: 5.78228759765625, Learning Rate: 0.01\n",
      "Epoch [7298/20000], Loss: 5.881256103515625, Learning Rate: 0.01\n",
      "Epoch [7299/20000], Loss: 6.0519561767578125, Learning Rate: 0.01\n",
      "Epoch [7300/20000], Loss: 6.344512939453125, Learning Rate: 0.01\n",
      "Epoch [7301/20000], Loss: 6.8415374755859375, Learning Rate: 0.01\n",
      "Epoch [7302/20000], Loss: 7.6792144775390625, Learning Rate: 0.01\n",
      "Epoch [7303/20000], Loss: 9.069473266601562, Learning Rate: 0.01\n",
      "Epoch [7304/20000], Loss: 11.320785522460938, Learning Rate: 0.01\n",
      "Epoch [7305/20000], Loss: 14.796768188476562, Learning Rate: 0.01\n",
      "Epoch [7306/20000], Loss: 19.742691040039062, Learning Rate: 0.01\n",
      "Epoch [7307/20000], Loss: 25.750411987304688, Learning Rate: 0.01\n",
      "Epoch [7308/20000], Loss: 31.040145874023438, Learning Rate: 0.01\n",
      "Epoch [7309/20000], Loss: 32.19560241699219, Learning Rate: 0.01\n",
      "Epoch [7310/20000], Loss: 26.538284301757812, Learning Rate: 0.01\n",
      "Epoch [7311/20000], Loss: 15.955535888671875, Learning Rate: 0.01\n",
      "Epoch [7312/20000], Loss: 7.2835540771484375, Learning Rate: 0.01\n",
      "Epoch [7313/20000], Loss: 5.98681640625, Learning Rate: 0.01\n",
      "Epoch [7314/20000], Loss: 10.980987548828125, Learning Rate: 0.01\n",
      "Epoch [7315/20000], Loss: 16.25860595703125, Learning Rate: 0.01\n",
      "Epoch [7316/20000], Loss: 16.436355590820312, Learning Rate: 0.01\n",
      "Epoch [7317/20000], Loss: 11.478042602539062, Learning Rate: 0.01\n",
      "Epoch [7318/20000], Loss: 6.444183349609375, Learning Rate: 0.01\n",
      "Epoch [7319/20000], Loss: 5.8643798828125, Learning Rate: 0.01\n",
      "Epoch [7320/20000], Loss: 9.02410888671875, Learning Rate: 0.01\n",
      "Epoch [7321/20000], Loss: 11.55072021484375, Learning Rate: 0.01\n",
      "Epoch [7322/20000], Loss: 10.511444091796875, Learning Rate: 0.01\n",
      "Epoch [7323/20000], Loss: 7.2827911376953125, Learning Rate: 0.01\n",
      "Epoch [7324/20000], Loss: 5.457611083984375, Learning Rate: 0.01\n",
      "Epoch [7325/20000], Loss: 6.4872283935546875, Learning Rate: 0.01\n",
      "Epoch [7326/20000], Loss: 8.433990478515625, Learning Rate: 0.01\n",
      "Epoch [7327/20000], Loss: 8.725143432617188, Learning Rate: 0.01\n",
      "Epoch [7328/20000], Loss: 7.1173858642578125, Learning Rate: 0.01\n",
      "Epoch [7329/20000], Loss: 5.5834197998046875, Learning Rate: 0.01\n",
      "Epoch [7330/20000], Loss: 5.67388916015625, Learning Rate: 0.01\n",
      "Epoch [7331/20000], Loss: 6.818603515625, Learning Rate: 0.01\n",
      "Epoch [7332/20000], Loss: 7.3700408935546875, Learning Rate: 0.01\n",
      "Epoch [7333/20000], Loss: 6.675994873046875, Learning Rate: 0.01\n",
      "Epoch [7334/20000], Loss: 5.645477294921875, Learning Rate: 0.01\n",
      "Epoch [7335/20000], Loss: 5.417083740234375, Learning Rate: 0.01\n",
      "Epoch [7336/20000], Loss: 6.0033416748046875, Learning Rate: 0.01\n",
      "Epoch [7337/20000], Loss: 6.489013671875, Learning Rate: 0.01\n",
      "Epoch [7338/20000], Loss: 6.2547760009765625, Learning Rate: 0.01\n",
      "Epoch [7339/20000], Loss: 5.631561279296875, Learning Rate: 0.01\n",
      "Epoch [7340/20000], Loss: 5.3411865234375, Learning Rate: 0.01\n",
      "Epoch [7341/20000], Loss: 5.592376708984375, Learning Rate: 0.01\n",
      "Epoch [7342/20000], Loss: 5.9405670166015625, Learning Rate: 0.01\n",
      "Epoch [7343/20000], Loss: 5.9191131591796875, Learning Rate: 0.01\n",
      "Epoch [7344/20000], Loss: 5.57684326171875, Learning Rate: 0.01\n",
      "Epoch [7345/20000], Loss: 5.3217315673828125, Learning Rate: 0.01\n",
      "Epoch [7346/20000], Loss: 5.38836669921875, Learning Rate: 0.01\n",
      "Epoch [7347/20000], Loss: 5.60504150390625, Learning Rate: 0.01\n",
      "Epoch [7348/20000], Loss: 5.666534423828125, Learning Rate: 0.01\n",
      "Epoch [7349/20000], Loss: 5.5029144287109375, Learning Rate: 0.01\n",
      "Epoch [7350/20000], Loss: 5.3134765625, Learning Rate: 0.01\n",
      "Epoch [7351/20000], Loss: 5.29010009765625, Learning Rate: 0.01\n",
      "Epoch [7352/20000], Loss: 5.4035186767578125, Learning Rate: 0.01\n",
      "Epoch [7353/20000], Loss: 5.4802703857421875, Learning Rate: 0.01\n",
      "Epoch [7354/20000], Loss: 5.4219970703125, Learning Rate: 0.01\n",
      "Epoch [7355/20000], Loss: 5.2998809814453125, Learning Rate: 0.01\n",
      "Epoch [7356/20000], Loss: 5.2431640625, Learning Rate: 0.01\n",
      "Epoch [7357/20000], Loss: 5.285400390625, Learning Rate: 0.01\n",
      "Epoch [7358/20000], Loss: 5.3461456298828125, Learning Rate: 0.01\n",
      "Epoch [7359/20000], Loss: 5.3411865234375, Learning Rate: 0.01\n",
      "Epoch [7360/20000], Loss: 5.274627685546875, Learning Rate: 0.01\n",
      "Epoch [7361/20000], Loss: 5.2176361083984375, Learning Rate: 0.01\n",
      "Epoch [7362/20000], Loss: 5.2172088623046875, Learning Rate: 0.01\n",
      "Epoch [7363/20000], Loss: 5.2519073486328125, Learning Rate: 0.01\n",
      "Epoch [7364/20000], Loss: 5.2665252685546875, Learning Rate: 0.01\n",
      "Epoch [7365/20000], Loss: 5.239013671875, Learning Rate: 0.01\n",
      "Epoch [7366/20000], Loss: 5.1961669921875, Learning Rate: 0.01\n",
      "Epoch [7367/20000], Loss: 5.1768035888671875, Learning Rate: 0.01\n",
      "Epoch [7368/20000], Loss: 5.1871795654296875, Learning Rate: 0.01\n",
      "Epoch [7369/20000], Loss: 5.2018890380859375, Learning Rate: 0.01\n",
      "Epoch [7370/20000], Loss: 5.1959686279296875, Learning Rate: 0.01\n",
      "Epoch [7371/20000], Loss: 5.1711273193359375, Learning Rate: 0.01\n",
      "Epoch [7372/20000], Loss: 5.14849853515625, Learning Rate: 0.01\n",
      "Epoch [7373/20000], Loss: 5.1430816650390625, Learning Rate: 0.01\n",
      "Epoch [7374/20000], Loss: 5.1493377685546875, Learning Rate: 0.01\n",
      "Epoch [7375/20000], Loss: 5.1512298583984375, Learning Rate: 0.01\n",
      "Epoch [7376/20000], Loss: 5.1401214599609375, Learning Rate: 0.01\n",
      "Epoch [7377/20000], Loss: 5.1225433349609375, Learning Rate: 0.01\n",
      "Epoch [7378/20000], Loss: 5.1104736328125, Learning Rate: 0.01\n",
      "Epoch [7379/20000], Loss: 5.1077117919921875, Learning Rate: 0.01\n",
      "Epoch [7380/20000], Loss: 5.1085662841796875, Learning Rate: 0.01\n",
      "Epoch [7381/20000], Loss: 5.104736328125, Learning Rate: 0.01\n",
      "Epoch [7382/20000], Loss: 5.0943450927734375, Learning Rate: 0.01\n",
      "Epoch [7383/20000], Loss: 5.0821685791015625, Learning Rate: 0.01\n",
      "Epoch [7384/20000], Loss: 5.07427978515625, Learning Rate: 0.01\n",
      "Epoch [7385/20000], Loss: 5.0711212158203125, Learning Rate: 0.01\n",
      "Epoch [7386/20000], Loss: 5.0686798095703125, Learning Rate: 0.01\n",
      "Epoch [7387/20000], Loss: 5.062835693359375, Learning Rate: 0.01\n",
      "Epoch [7388/20000], Loss: 5.05377197265625, Learning Rate: 0.01\n",
      "Epoch [7389/20000], Loss: 5.0448150634765625, Learning Rate: 0.01\n",
      "Epoch [7390/20000], Loss: 5.038330078125, Learning Rate: 0.01\n",
      "Epoch [7391/20000], Loss: 5.0341644287109375, Learning Rate: 0.01\n",
      "Epoch [7392/20000], Loss: 5.029815673828125, Learning Rate: 0.01\n",
      "Epoch [7393/20000], Loss: 5.02362060546875, Learning Rate: 0.01\n",
      "Epoch [7394/20000], Loss: 5.015899658203125, Learning Rate: 0.01\n",
      "Epoch [7395/20000], Loss: 5.0082855224609375, Learning Rate: 0.01\n",
      "Epoch [7396/20000], Loss: 5.0021209716796875, Learning Rate: 0.01\n",
      "Epoch [7397/20000], Loss: 4.9971771240234375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7398/20000], Loss: 4.9919281005859375, Learning Rate: 0.01\n",
      "Epoch [7399/20000], Loss: 4.9859619140625, Learning Rate: 0.01\n",
      "Epoch [7400/20000], Loss: 4.9790496826171875, Learning Rate: 0.01\n",
      "Epoch [7401/20000], Loss: 4.972076416015625, Learning Rate: 0.01\n",
      "Epoch [7402/20000], Loss: 4.9660797119140625, Learning Rate: 0.01\n",
      "Epoch [7403/20000], Loss: 4.9605712890625, Learning Rate: 0.01\n",
      "Epoch [7404/20000], Loss: 4.9550323486328125, Learning Rate: 0.01\n",
      "Epoch [7405/20000], Loss: 4.9491729736328125, Learning Rate: 0.01\n",
      "Epoch [7406/20000], Loss: 4.942474365234375, Learning Rate: 0.01\n",
      "Epoch [7407/20000], Loss: 4.9361114501953125, Learning Rate: 0.01\n",
      "Epoch [7408/20000], Loss: 4.929962158203125, Learning Rate: 0.01\n",
      "Epoch [7409/20000], Loss: 4.92413330078125, Learning Rate: 0.01\n",
      "Epoch [7410/20000], Loss: 4.91845703125, Learning Rate: 0.01\n",
      "Epoch [7411/20000], Loss: 4.91241455078125, Learning Rate: 0.01\n",
      "Epoch [7412/20000], Loss: 4.9063873291015625, Learning Rate: 0.01\n",
      "Epoch [7413/20000], Loss: 4.8999786376953125, Learning Rate: 0.01\n",
      "Epoch [7414/20000], Loss: 4.893829345703125, Learning Rate: 0.01\n",
      "Epoch [7415/20000], Loss: 4.8879241943359375, Learning Rate: 0.01\n",
      "Epoch [7416/20000], Loss: 4.8821563720703125, Learning Rate: 0.01\n",
      "Epoch [7417/20000], Loss: 4.8761444091796875, Learning Rate: 0.01\n",
      "Epoch [7418/20000], Loss: 4.8701934814453125, Learning Rate: 0.01\n",
      "Epoch [7419/20000], Loss: 4.8639068603515625, Learning Rate: 0.01\n",
      "Epoch [7420/20000], Loss: 4.8578643798828125, Learning Rate: 0.01\n",
      "Epoch [7421/20000], Loss: 4.8519439697265625, Learning Rate: 0.01\n",
      "Epoch [7422/20000], Loss: 4.8459625244140625, Learning Rate: 0.01\n",
      "Epoch [7423/20000], Loss: 4.8399658203125, Learning Rate: 0.01\n",
      "Epoch [7424/20000], Loss: 4.8337554931640625, Learning Rate: 0.01\n",
      "Epoch [7425/20000], Loss: 4.827850341796875, Learning Rate: 0.01\n",
      "Epoch [7426/20000], Loss: 4.821624755859375, Learning Rate: 0.01\n",
      "Epoch [7427/20000], Loss: 4.815704345703125, Learning Rate: 0.01\n",
      "Epoch [7428/20000], Loss: 4.809783935546875, Learning Rate: 0.01\n",
      "Epoch [7429/20000], Loss: 4.8036956787109375, Learning Rate: 0.01\n",
      "Epoch [7430/20000], Loss: 4.7978515625, Learning Rate: 0.01\n",
      "Epoch [7431/20000], Loss: 4.7917633056640625, Learning Rate: 0.01\n",
      "Epoch [7432/20000], Loss: 4.7857818603515625, Learning Rate: 0.01\n",
      "Epoch [7433/20000], Loss: 4.77972412109375, Learning Rate: 0.01\n",
      "Epoch [7434/20000], Loss: 4.7737274169921875, Learning Rate: 0.01\n",
      "Epoch [7435/20000], Loss: 4.767730712890625, Learning Rate: 0.01\n",
      "Epoch [7436/20000], Loss: 4.7616119384765625, Learning Rate: 0.01\n",
      "Epoch [7437/20000], Loss: 4.7556915283203125, Learning Rate: 0.01\n",
      "Epoch [7438/20000], Loss: 4.7496795654296875, Learning Rate: 0.01\n",
      "Epoch [7439/20000], Loss: 4.7435302734375, Learning Rate: 0.01\n",
      "Epoch [7440/20000], Loss: 4.737701416015625, Learning Rate: 0.01\n",
      "Epoch [7441/20000], Loss: 4.731719970703125, Learning Rate: 0.01\n",
      "Epoch [7442/20000], Loss: 4.725616455078125, Learning Rate: 0.01\n",
      "Epoch [7443/20000], Loss: 4.7196197509765625, Learning Rate: 0.01\n",
      "Epoch [7444/20000], Loss: 4.713592529296875, Learning Rate: 0.01\n",
      "Epoch [7445/20000], Loss: 4.7075347900390625, Learning Rate: 0.01\n",
      "Epoch [7446/20000], Loss: 4.70166015625, Learning Rate: 0.01\n",
      "Epoch [7447/20000], Loss: 4.69549560546875, Learning Rate: 0.01\n",
      "Epoch [7448/20000], Loss: 4.6894683837890625, Learning Rate: 0.01\n",
      "Epoch [7449/20000], Loss: 4.683746337890625, Learning Rate: 0.01\n",
      "Epoch [7450/20000], Loss: 4.677703857421875, Learning Rate: 0.01\n",
      "Epoch [7451/20000], Loss: 4.6715850830078125, Learning Rate: 0.01\n",
      "Epoch [7452/20000], Loss: 4.6655120849609375, Learning Rate: 0.01\n",
      "Epoch [7453/20000], Loss: 4.6593780517578125, Learning Rate: 0.01\n",
      "Epoch [7454/20000], Loss: 4.6533050537109375, Learning Rate: 0.01\n",
      "Epoch [7455/20000], Loss: 4.6475677490234375, Learning Rate: 0.01\n",
      "Epoch [7456/20000], Loss: 4.641510009765625, Learning Rate: 0.01\n",
      "Epoch [7457/20000], Loss: 4.635467529296875, Learning Rate: 0.01\n",
      "Epoch [7458/20000], Loss: 4.6295013427734375, Learning Rate: 0.01\n",
      "Epoch [7459/20000], Loss: 4.6234130859375, Learning Rate: 0.01\n",
      "Epoch [7460/20000], Loss: 4.617401123046875, Learning Rate: 0.01\n",
      "Epoch [7461/20000], Loss: 4.6113128662109375, Learning Rate: 0.01\n",
      "Epoch [7462/20000], Loss: 4.605377197265625, Learning Rate: 0.01\n",
      "Epoch [7463/20000], Loss: 4.5993194580078125, Learning Rate: 0.01\n",
      "Epoch [7464/20000], Loss: 4.5933990478515625, Learning Rate: 0.01\n",
      "Epoch [7465/20000], Loss: 4.58734130859375, Learning Rate: 0.01\n",
      "Epoch [7466/20000], Loss: 4.5813751220703125, Learning Rate: 0.01\n",
      "Epoch [7467/20000], Loss: 4.57537841796875, Learning Rate: 0.01\n",
      "Epoch [7468/20000], Loss: 4.5693817138671875, Learning Rate: 0.01\n",
      "Epoch [7469/20000], Loss: 4.5633544921875, Learning Rate: 0.01\n",
      "Epoch [7470/20000], Loss: 4.557373046875, Learning Rate: 0.01\n",
      "Epoch [7471/20000], Loss: 4.5514068603515625, Learning Rate: 0.01\n",
      "Epoch [7472/20000], Loss: 4.545196533203125, Learning Rate: 0.01\n",
      "Epoch [7473/20000], Loss: 4.539276123046875, Learning Rate: 0.01\n",
      "Epoch [7474/20000], Loss: 4.5332794189453125, Learning Rate: 0.01\n",
      "Epoch [7475/20000], Loss: 4.5273284912109375, Learning Rate: 0.01\n",
      "Epoch [7476/20000], Loss: 4.5213623046875, Learning Rate: 0.01\n",
      "Epoch [7477/20000], Loss: 4.5153045654296875, Learning Rate: 0.01\n",
      "Epoch [7478/20000], Loss: 4.50927734375, Learning Rate: 0.01\n",
      "Epoch [7479/20000], Loss: 4.503173828125, Learning Rate: 0.01\n",
      "Epoch [7480/20000], Loss: 4.4972686767578125, Learning Rate: 0.01\n",
      "Epoch [7481/20000], Loss: 4.4913177490234375, Learning Rate: 0.01\n",
      "Epoch [7482/20000], Loss: 4.4852752685546875, Learning Rate: 0.01\n",
      "Epoch [7483/20000], Loss: 4.47918701171875, Learning Rate: 0.01\n",
      "Epoch [7484/20000], Loss: 4.473052978515625, Learning Rate: 0.01\n",
      "Epoch [7485/20000], Loss: 4.467041015625, Learning Rate: 0.01\n",
      "Epoch [7486/20000], Loss: 4.4610595703125, Learning Rate: 0.01\n",
      "Epoch [7487/20000], Loss: 4.4551239013671875, Learning Rate: 0.01\n",
      "Epoch [7488/20000], Loss: 4.44921875, Learning Rate: 0.01\n",
      "Epoch [7489/20000], Loss: 4.443267822265625, Learning Rate: 0.01\n",
      "Epoch [7490/20000], Loss: 4.4371185302734375, Learning Rate: 0.01\n",
      "Epoch [7491/20000], Loss: 4.43121337890625, Learning Rate: 0.01\n",
      "Epoch [7492/20000], Loss: 4.42523193359375, Learning Rate: 0.01\n",
      "Epoch [7493/20000], Loss: 4.4190826416015625, Learning Rate: 0.01\n",
      "Epoch [7494/20000], Loss: 4.413116455078125, Learning Rate: 0.01\n",
      "Epoch [7495/20000], Loss: 4.4071044921875, Learning Rate: 0.01\n",
      "Epoch [7496/20000], Loss: 4.400970458984375, Learning Rate: 0.01\n",
      "Epoch [7497/20000], Loss: 4.395050048828125, Learning Rate: 0.01\n",
      "Epoch [7498/20000], Loss: 4.3890838623046875, Learning Rate: 0.01\n",
      "Epoch [7499/20000], Loss: 4.3831329345703125, Learning Rate: 0.01\n",
      "Epoch [7500/20000], Loss: 4.3769683837890625, Learning Rate: 0.01\n",
      "Epoch [7501/20000], Loss: 4.3710784912109375, Learning Rate: 0.01\n",
      "Epoch [7502/20000], Loss: 4.364990234375, Learning Rate: 0.01\n",
      "Epoch [7503/20000], Loss: 4.3590545654296875, Learning Rate: 0.01\n",
      "Epoch [7504/20000], Loss: 4.3529510498046875, Learning Rate: 0.01\n",
      "Epoch [7505/20000], Loss: 4.3469696044921875, Learning Rate: 0.01\n",
      "Epoch [7506/20000], Loss: 4.3409423828125, Learning Rate: 0.01\n",
      "Epoch [7507/20000], Loss: 4.3350067138671875, Learning Rate: 0.01\n",
      "Epoch [7508/20000], Loss: 4.328887939453125, Learning Rate: 0.01\n",
      "Epoch [7509/20000], Loss: 4.322998046875, Learning Rate: 0.01\n",
      "Epoch [7510/20000], Loss: 4.316925048828125, Learning Rate: 0.01\n",
      "Epoch [7511/20000], Loss: 4.31085205078125, Learning Rate: 0.01\n",
      "Epoch [7512/20000], Loss: 4.3049774169921875, Learning Rate: 0.01\n",
      "Epoch [7513/20000], Loss: 4.298858642578125, Learning Rate: 0.01\n",
      "Epoch [7514/20000], Loss: 4.292877197265625, Learning Rate: 0.01\n",
      "Epoch [7515/20000], Loss: 4.2868194580078125, Learning Rate: 0.01\n",
      "Epoch [7516/20000], Loss: 4.2808380126953125, Learning Rate: 0.01\n",
      "Epoch [7517/20000], Loss: 4.274871826171875, Learning Rate: 0.01\n",
      "Epoch [7518/20000], Loss: 4.2688446044921875, Learning Rate: 0.01\n",
      "Epoch [7519/20000], Loss: 4.2628173828125, Learning Rate: 0.01\n",
      "Epoch [7520/20000], Loss: 4.2567596435546875, Learning Rate: 0.01\n",
      "Epoch [7521/20000], Loss: 4.2508544921875, Learning Rate: 0.01\n",
      "Epoch [7522/20000], Loss: 4.2447662353515625, Learning Rate: 0.01\n",
      "Epoch [7523/20000], Loss: 4.238800048828125, Learning Rate: 0.01\n",
      "Epoch [7524/20000], Loss: 4.2327423095703125, Learning Rate: 0.01\n",
      "Epoch [7525/20000], Loss: 4.22674560546875, Learning Rate: 0.01\n",
      "Epoch [7526/20000], Loss: 4.220672607421875, Learning Rate: 0.01\n",
      "Epoch [7527/20000], Loss: 4.2147216796875, Learning Rate: 0.01\n",
      "Epoch [7528/20000], Loss: 4.208709716796875, Learning Rate: 0.01\n",
      "Epoch [7529/20000], Loss: 4.2026824951171875, Learning Rate: 0.01\n",
      "Epoch [7530/20000], Loss: 4.1965484619140625, Learning Rate: 0.01\n",
      "Epoch [7531/20000], Loss: 4.190521240234375, Learning Rate: 0.01\n",
      "Epoch [7532/20000], Loss: 4.18463134765625, Learning Rate: 0.01\n",
      "Epoch [7533/20000], Loss: 4.1785888671875, Learning Rate: 0.01\n",
      "Epoch [7534/20000], Loss: 4.1726531982421875, Learning Rate: 0.01\n",
      "Epoch [7535/20000], Loss: 4.16650390625, Learning Rate: 0.01\n",
      "Epoch [7536/20000], Loss: 4.1605377197265625, Learning Rate: 0.01\n",
      "Epoch [7537/20000], Loss: 4.1545257568359375, Learning Rate: 0.01\n",
      "Epoch [7538/20000], Loss: 4.14849853515625, Learning Rate: 0.01\n",
      "Epoch [7539/20000], Loss: 4.142547607421875, Learning Rate: 0.01\n",
      "Epoch [7540/20000], Loss: 4.136474609375, Learning Rate: 0.01\n",
      "Epoch [7541/20000], Loss: 4.1303558349609375, Learning Rate: 0.01\n",
      "Epoch [7542/20000], Loss: 4.124420166015625, Learning Rate: 0.01\n",
      "Epoch [7543/20000], Loss: 4.1184234619140625, Learning Rate: 0.01\n",
      "Epoch [7544/20000], Loss: 4.1124725341796875, Learning Rate: 0.01\n",
      "Epoch [7545/20000], Loss: 4.1066131591796875, Learning Rate: 0.01\n",
      "Epoch [7546/20000], Loss: 4.100341796875, Learning Rate: 0.01\n",
      "Epoch [7547/20000], Loss: 4.0943450927734375, Learning Rate: 0.01\n",
      "Epoch [7548/20000], Loss: 4.0883636474609375, Learning Rate: 0.01\n",
      "Epoch [7549/20000], Loss: 4.0823974609375, Learning Rate: 0.01\n",
      "Epoch [7550/20000], Loss: 4.0764007568359375, Learning Rate: 0.01\n",
      "Epoch [7551/20000], Loss: 4.070404052734375, Learning Rate: 0.01\n",
      "Epoch [7552/20000], Loss: 4.06439208984375, Learning Rate: 0.01\n",
      "Epoch [7553/20000], Loss: 4.05828857421875, Learning Rate: 0.01\n",
      "Epoch [7554/20000], Loss: 4.0521697998046875, Learning Rate: 0.01\n",
      "Epoch [7555/20000], Loss: 4.0462493896484375, Learning Rate: 0.01\n",
      "Epoch [7556/20000], Loss: 4.0403289794921875, Learning Rate: 0.01\n",
      "Epoch [7557/20000], Loss: 4.034210205078125, Learning Rate: 0.01\n",
      "Epoch [7558/20000], Loss: 4.0283660888671875, Learning Rate: 0.01\n",
      "Epoch [7559/20000], Loss: 4.02227783203125, Learning Rate: 0.01\n",
      "Epoch [7560/20000], Loss: 4.0162200927734375, Learning Rate: 0.01\n",
      "Epoch [7561/20000], Loss: 4.010162353515625, Learning Rate: 0.01\n",
      "Epoch [7562/20000], Loss: 4.00408935546875, Learning Rate: 0.01\n",
      "Epoch [7563/20000], Loss: 3.9981689453125, Learning Rate: 0.01\n",
      "Epoch [7564/20000], Loss: 3.99200439453125, Learning Rate: 0.01\n",
      "Epoch [7565/20000], Loss: 3.9861602783203125, Learning Rate: 0.01\n",
      "Epoch [7566/20000], Loss: 3.9801483154296875, Learning Rate: 0.01\n",
      "Epoch [7567/20000], Loss: 3.9740447998046875, Learning Rate: 0.01\n",
      "Epoch [7568/20000], Loss: 3.96807861328125, Learning Rate: 0.01\n",
      "Epoch [7569/20000], Loss: 3.9620208740234375, Learning Rate: 0.01\n",
      "Epoch [7570/20000], Loss: 3.9561767578125, Learning Rate: 0.01\n",
      "Epoch [7571/20000], Loss: 3.949920654296875, Learning Rate: 0.01\n",
      "Epoch [7572/20000], Loss: 3.9439697265625, Learning Rate: 0.01\n",
      "Epoch [7573/20000], Loss: 3.9379730224609375, Learning Rate: 0.01\n",
      "Epoch [7574/20000], Loss: 3.9319610595703125, Learning Rate: 0.01\n",
      "Epoch [7575/20000], Loss: 3.9257659912109375, Learning Rate: 0.01\n",
      "Epoch [7576/20000], Loss: 3.9198760986328125, Learning Rate: 0.01\n",
      "Epoch [7577/20000], Loss: 3.9139404296875, Learning Rate: 0.01\n",
      "Epoch [7578/20000], Loss: 3.907867431640625, Learning Rate: 0.01\n",
      "Epoch [7579/20000], Loss: 3.9017791748046875, Learning Rate: 0.01\n",
      "Epoch [7580/20000], Loss: 3.8957672119140625, Learning Rate: 0.01\n",
      "Epoch [7581/20000], Loss: 3.88983154296875, Learning Rate: 0.01\n",
      "Epoch [7582/20000], Loss: 3.8837890625, Learning Rate: 0.01\n",
      "Epoch [7583/20000], Loss: 3.877716064453125, Learning Rate: 0.01\n",
      "Epoch [7584/20000], Loss: 3.8717193603515625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7585/20000], Loss: 3.865875244140625, Learning Rate: 0.01\n",
      "Epoch [7586/20000], Loss: 3.85980224609375, Learning Rate: 0.01\n",
      "Epoch [7587/20000], Loss: 3.853759765625, Learning Rate: 0.01\n",
      "Epoch [7588/20000], Loss: 3.8476104736328125, Learning Rate: 0.01\n",
      "Epoch [7589/20000], Loss: 3.841705322265625, Learning Rate: 0.01\n",
      "Epoch [7590/20000], Loss: 3.8355865478515625, Learning Rate: 0.01\n",
      "Epoch [7591/20000], Loss: 3.82958984375, Learning Rate: 0.01\n",
      "Epoch [7592/20000], Loss: 3.8235931396484375, Learning Rate: 0.01\n",
      "Epoch [7593/20000], Loss: 3.817413330078125, Learning Rate: 0.01\n",
      "Epoch [7594/20000], Loss: 3.8114166259765625, Learning Rate: 0.01\n",
      "Epoch [7595/20000], Loss: 3.805572509765625, Learning Rate: 0.01\n",
      "Epoch [7596/20000], Loss: 3.7994232177734375, Learning Rate: 0.01\n",
      "Epoch [7597/20000], Loss: 3.793548583984375, Learning Rate: 0.01\n",
      "Epoch [7598/20000], Loss: 3.7875518798828125, Learning Rate: 0.01\n",
      "Epoch [7599/20000], Loss: 3.781524658203125, Learning Rate: 0.01\n",
      "Epoch [7600/20000], Loss: 3.7755126953125, Learning Rate: 0.01\n",
      "Epoch [7601/20000], Loss: 3.7694549560546875, Learning Rate: 0.01\n",
      "Epoch [7602/20000], Loss: 3.76348876953125, Learning Rate: 0.01\n",
      "Epoch [7603/20000], Loss: 3.75750732421875, Learning Rate: 0.01\n",
      "Epoch [7604/20000], Loss: 3.75152587890625, Learning Rate: 0.01\n",
      "Epoch [7605/20000], Loss: 3.745361328125, Learning Rate: 0.01\n",
      "Epoch [7606/20000], Loss: 3.73931884765625, Learning Rate: 0.01\n",
      "Epoch [7607/20000], Loss: 3.7332763671875, Learning Rate: 0.01\n",
      "Epoch [7608/20000], Loss: 3.7273101806640625, Learning Rate: 0.01\n",
      "Epoch [7609/20000], Loss: 3.721405029296875, Learning Rate: 0.01\n",
      "Epoch [7610/20000], Loss: 3.7153778076171875, Learning Rate: 0.01\n",
      "Epoch [7611/20000], Loss: 3.7095947265625, Learning Rate: 0.01\n",
      "Epoch [7612/20000], Loss: 3.70361328125, Learning Rate: 0.01\n",
      "Epoch [7613/20000], Loss: 3.697662353515625, Learning Rate: 0.01\n",
      "Epoch [7614/20000], Loss: 3.6917877197265625, Learning Rate: 0.01\n",
      "Epoch [7615/20000], Loss: 3.6863250732421875, Learning Rate: 0.01\n",
      "Epoch [7616/20000], Loss: 3.6807708740234375, Learning Rate: 0.01\n",
      "Epoch [7617/20000], Loss: 3.675628662109375, Learning Rate: 0.01\n",
      "Epoch [7618/20000], Loss: 3.6710052490234375, Learning Rate: 0.01\n",
      "Epoch [7619/20000], Loss: 3.667266845703125, Learning Rate: 0.01\n",
      "Epoch [7620/20000], Loss: 3.664520263671875, Learning Rate: 0.01\n",
      "Epoch [7621/20000], Loss: 3.6636810302734375, Learning Rate: 0.01\n",
      "Epoch [7622/20000], Loss: 3.6661529541015625, Learning Rate: 0.01\n",
      "Epoch [7623/20000], Loss: 3.673553466796875, Learning Rate: 0.01\n",
      "Epoch [7624/20000], Loss: 3.6888580322265625, Learning Rate: 0.01\n",
      "Epoch [7625/20000], Loss: 3.717041015625, Learning Rate: 0.01\n",
      "Epoch [7626/20000], Loss: 3.7666778564453125, Learning Rate: 0.01\n",
      "Epoch [7627/20000], Loss: 3.850128173828125, Learning Rate: 0.01\n",
      "Epoch [7628/20000], Loss: 3.9899139404296875, Learning Rate: 0.01\n",
      "Epoch [7629/20000], Loss: 4.2211456298828125, Learning Rate: 0.01\n",
      "Epoch [7630/20000], Loss: 4.6018218994140625, Learning Rate: 0.01\n",
      "Epoch [7631/20000], Loss: 5.223297119140625, Learning Rate: 0.01\n",
      "Epoch [7632/20000], Loss: 6.226837158203125, Learning Rate: 0.01\n",
      "Epoch [7633/20000], Loss: 7.8157501220703125, Learning Rate: 0.01\n",
      "Epoch [7634/20000], Loss: 10.250350952148438, Learning Rate: 0.01\n",
      "Epoch [7635/20000], Loss: 13.761322021484375, Learning Rate: 0.01\n",
      "Epoch [7636/20000], Loss: 18.3304443359375, Learning Rate: 0.01\n",
      "Epoch [7637/20000], Loss: 23.200302124023438, Learning Rate: 0.01\n",
      "Epoch [7638/20000], Loss: 26.493789672851562, Learning Rate: 0.01\n",
      "Epoch [7639/20000], Loss: 25.576568603515625, Learning Rate: 0.01\n",
      "Epoch [7640/20000], Loss: 19.371307373046875, Learning Rate: 0.01\n",
      "Epoch [7641/20000], Loss: 10.553085327148438, Learning Rate: 0.01\n",
      "Epoch [7642/20000], Loss: 4.4038543701171875, Learning Rate: 0.01\n",
      "Epoch [7643/20000], Loss: 4.1401824951171875, Learning Rate: 0.01\n",
      "Epoch [7644/20000], Loss: 8.293899536132812, Learning Rate: 0.01\n",
      "Epoch [7645/20000], Loss: 12.41070556640625, Learning Rate: 0.01\n",
      "Epoch [7646/20000], Loss: 12.714019775390625, Learning Rate: 0.01\n",
      "Epoch [7647/20000], Loss: 9.053375244140625, Learning Rate: 0.01\n",
      "Epoch [7648/20000], Loss: 4.818634033203125, Learning Rate: 0.01\n",
      "Epoch [7649/20000], Loss: 3.523712158203125, Learning Rate: 0.01\n",
      "Epoch [7650/20000], Loss: 5.44927978515625, Learning Rate: 0.01\n",
      "Epoch [7651/20000], Loss: 7.90533447265625, Learning Rate: 0.01\n",
      "Epoch [7652/20000], Loss: 8.2080078125, Learning Rate: 0.01\n",
      "Epoch [7653/20000], Loss: 6.1754302978515625, Learning Rate: 0.01\n",
      "Epoch [7654/20000], Loss: 3.9565887451171875, Learning Rate: 0.01\n",
      "Epoch [7655/20000], Loss: 3.5420379638671875, Learning Rate: 0.01\n",
      "Epoch [7656/20000], Loss: 4.8030853271484375, Learning Rate: 0.01\n",
      "Epoch [7657/20000], Loss: 6.0023345947265625, Learning Rate: 0.01\n",
      "Epoch [7658/20000], Loss: 5.7962493896484375, Learning Rate: 0.01\n",
      "Epoch [7659/20000], Loss: 4.501953125, Learning Rate: 0.01\n",
      "Epoch [7660/20000], Loss: 3.4925537109375, Learning Rate: 0.01\n",
      "Epoch [7661/20000], Loss: 3.615325927734375, Learning Rate: 0.01\n",
      "Epoch [7662/20000], Loss: 4.4235076904296875, Learning Rate: 0.01\n",
      "Epoch [7663/20000], Loss: 4.87213134765625, Learning Rate: 0.01\n",
      "Epoch [7664/20000], Loss: 4.4854736328125, Learning Rate: 0.01\n",
      "Epoch [7665/20000], Loss: 3.730560302734375, Learning Rate: 0.01\n",
      "Epoch [7666/20000], Loss: 3.370330810546875, Learning Rate: 0.01\n",
      "Epoch [7667/20000], Loss: 3.628875732421875, Learning Rate: 0.01\n",
      "Epoch [7668/20000], Loss: 4.0668792724609375, Learning Rate: 0.01\n",
      "Epoch [7669/20000], Loss: 4.1524658203125, Learning Rate: 0.01\n",
      "Epoch [7670/20000], Loss: 3.81549072265625, Learning Rate: 0.01\n",
      "Epoch [7671/20000], Loss: 3.430450439453125, Learning Rate: 0.01\n",
      "Epoch [7672/20000], Loss: 3.3526763916015625, Learning Rate: 0.01\n",
      "Epoch [7673/20000], Loss: 3.5643310546875, Learning Rate: 0.01\n",
      "Epoch [7674/20000], Loss: 3.763641357421875, Learning Rate: 0.01\n",
      "Epoch [7675/20000], Loss: 3.7225494384765625, Learning Rate: 0.01\n",
      "Epoch [7676/20000], Loss: 3.4981842041015625, Learning Rate: 0.01\n",
      "Epoch [7677/20000], Loss: 3.32171630859375, Learning Rate: 0.01\n",
      "Epoch [7678/20000], Loss: 3.3345184326171875, Learning Rate: 0.01\n",
      "Epoch [7679/20000], Loss: 3.46527099609375, Learning Rate: 0.01\n",
      "Epoch [7680/20000], Loss: 3.5422515869140625, Learning Rate: 0.01\n",
      "Epoch [7681/20000], Loss: 3.4808502197265625, Learning Rate: 0.01\n",
      "Epoch [7682/20000], Loss: 3.349822998046875, Learning Rate: 0.01\n",
      "Epoch [7683/20000], Loss: 3.272857666015625, Learning Rate: 0.01\n",
      "Epoch [7684/20000], Loss: 3.298980712890625, Learning Rate: 0.01\n",
      "Epoch [7685/20000], Loss: 3.369049072265625, Learning Rate: 0.01\n",
      "Epoch [7686/20000], Loss: 3.39361572265625, Learning Rate: 0.01\n",
      "Epoch [7687/20000], Loss: 3.3455047607421875, Learning Rate: 0.01\n",
      "Epoch [7688/20000], Loss: 3.272308349609375, Learning Rate: 0.01\n",
      "Epoch [7689/20000], Loss: 3.2371826171875, Learning Rate: 0.01\n",
      "Epoch [7690/20000], Loss: 3.255401611328125, Learning Rate: 0.01\n",
      "Epoch [7691/20000], Loss: 3.28961181640625, Learning Rate: 0.01\n",
      "Epoch [7692/20000], Loss: 3.29541015625, Learning Rate: 0.01\n",
      "Epoch [7693/20000], Loss: 3.263519287109375, Learning Rate: 0.01\n",
      "Epoch [7694/20000], Loss: 3.22210693359375, Learning Rate: 0.01\n",
      "Epoch [7695/20000], Loss: 3.2029266357421875, Learning Rate: 0.01\n",
      "Epoch [7696/20000], Loss: 3.2115631103515625, Learning Rate: 0.01\n",
      "Epoch [7697/20000], Loss: 3.2268218994140625, Learning Rate: 0.01\n",
      "Epoch [7698/20000], Loss: 3.2264556884765625, Learning Rate: 0.01\n",
      "Epoch [7699/20000], Loss: 3.206634521484375, Learning Rate: 0.01\n",
      "Epoch [7700/20000], Loss: 3.18194580078125, Learning Rate: 0.01\n",
      "Epoch [7701/20000], Loss: 3.1687774658203125, Learning Rate: 0.01\n",
      "Epoch [7702/20000], Loss: 3.1701507568359375, Learning Rate: 0.01\n",
      "Epoch [7703/20000], Loss: 3.1758575439453125, Learning Rate: 0.01\n",
      "Epoch [7704/20000], Loss: 3.17376708984375, Learning Rate: 0.01\n",
      "Epoch [7705/20000], Loss: 3.1611785888671875, Learning Rate: 0.01\n",
      "Epoch [7706/20000], Loss: 3.1453399658203125, Learning Rate: 0.01\n",
      "Epoch [7707/20000], Loss: 3.134490966796875, Learning Rate: 0.01\n",
      "Epoch [7708/20000], Loss: 3.1318206787109375, Learning Rate: 0.01\n",
      "Epoch [7709/20000], Loss: 3.1322479248046875, Learning Rate: 0.01\n",
      "Epoch [7710/20000], Loss: 3.1294097900390625, Learning Rate: 0.01\n",
      "Epoch [7711/20000], Loss: 3.1210784912109375, Learning Rate: 0.01\n",
      "Epoch [7712/20000], Loss: 3.1098785400390625, Learning Rate: 0.01\n",
      "Epoch [7713/20000], Loss: 3.1007537841796875, Learning Rate: 0.01\n",
      "Epoch [7714/20000], Loss: 3.0955657958984375, Learning Rate: 0.01\n",
      "Epoch [7715/20000], Loss: 3.0929107666015625, Learning Rate: 0.01\n",
      "Epoch [7716/20000], Loss: 3.089447021484375, Learning Rate: 0.01\n",
      "Epoch [7717/20000], Loss: 3.0831451416015625, Learning Rate: 0.01\n",
      "Epoch [7718/20000], Loss: 3.0749664306640625, Learning Rate: 0.01\n",
      "Epoch [7719/20000], Loss: 3.0668792724609375, Learning Rate: 0.01\n",
      "Epoch [7720/20000], Loss: 3.06060791015625, Learning Rate: 0.01\n",
      "Epoch [7721/20000], Loss: 3.0562896728515625, Learning Rate: 0.01\n",
      "Epoch [7722/20000], Loss: 3.052032470703125, Learning Rate: 0.01\n",
      "Epoch [7723/20000], Loss: 3.0467071533203125, Learning Rate: 0.01\n",
      "Epoch [7724/20000], Loss: 3.040130615234375, Learning Rate: 0.01\n",
      "Epoch [7725/20000], Loss: 3.0328826904296875, Learning Rate: 0.01\n",
      "Epoch [7726/20000], Loss: 3.02642822265625, Learning Rate: 0.01\n",
      "Epoch [7727/20000], Loss: 3.0210723876953125, Learning Rate: 0.01\n",
      "Epoch [7728/20000], Loss: 3.0161590576171875, Learning Rate: 0.01\n",
      "Epoch [7729/20000], Loss: 3.0110931396484375, Learning Rate: 0.01\n",
      "Epoch [7730/20000], Loss: 3.005218505859375, Learning Rate: 0.01\n",
      "Epoch [7731/20000], Loss: 2.9989166259765625, Learning Rate: 0.01\n",
      "Epoch [7732/20000], Loss: 2.992431640625, Learning Rate: 0.01\n",
      "Epoch [7733/20000], Loss: 2.9867095947265625, Learning Rate: 0.01\n",
      "Epoch [7734/20000], Loss: 2.98114013671875, Learning Rate: 0.01\n",
      "Epoch [7735/20000], Loss: 2.9758758544921875, Learning Rate: 0.01\n",
      "Epoch [7736/20000], Loss: 2.97027587890625, Learning Rate: 0.01\n",
      "Epoch [7737/20000], Loss: 2.9645233154296875, Learning Rate: 0.01\n",
      "Epoch [7738/20000], Loss: 2.95843505859375, Learning Rate: 0.01\n",
      "Epoch [7739/20000], Loss: 2.95245361328125, Learning Rate: 0.01\n",
      "Epoch [7740/20000], Loss: 2.94671630859375, Learning Rate: 0.01\n",
      "Epoch [7741/20000], Loss: 2.9412078857421875, Learning Rate: 0.01\n",
      "Epoch [7742/20000], Loss: 2.9356231689453125, Learning Rate: 0.01\n",
      "Epoch [7743/20000], Loss: 2.92999267578125, Learning Rate: 0.01\n",
      "Epoch [7744/20000], Loss: 2.92413330078125, Learning Rate: 0.01\n",
      "Epoch [7745/20000], Loss: 2.9184112548828125, Learning Rate: 0.01\n",
      "Epoch [7746/20000], Loss: 2.9126129150390625, Learning Rate: 0.01\n",
      "Epoch [7747/20000], Loss: 2.9068145751953125, Learning Rate: 0.01\n",
      "Epoch [7748/20000], Loss: 2.9011077880859375, Learning Rate: 0.01\n",
      "Epoch [7749/20000], Loss: 2.8956451416015625, Learning Rate: 0.01\n",
      "Epoch [7750/20000], Loss: 2.8900146484375, Learning Rate: 0.01\n",
      "Epoch [7751/20000], Loss: 2.88421630859375, Learning Rate: 0.01\n",
      "Epoch [7752/20000], Loss: 2.8785400390625, Learning Rate: 0.01\n",
      "Epoch [7753/20000], Loss: 2.8727264404296875, Learning Rate: 0.01\n",
      "Epoch [7754/20000], Loss: 2.86712646484375, Learning Rate: 0.01\n",
      "Epoch [7755/20000], Loss: 2.8611297607421875, Learning Rate: 0.01\n",
      "Epoch [7756/20000], Loss: 2.8555908203125, Learning Rate: 0.01\n",
      "Epoch [7757/20000], Loss: 2.849945068359375, Learning Rate: 0.01\n",
      "Epoch [7758/20000], Loss: 2.84423828125, Learning Rate: 0.01\n",
      "Epoch [7759/20000], Loss: 2.8383941650390625, Learning Rate: 0.01\n",
      "Epoch [7760/20000], Loss: 2.832733154296875, Learning Rate: 0.01\n",
      "Epoch [7761/20000], Loss: 2.8269805908203125, Learning Rate: 0.01\n",
      "Epoch [7762/20000], Loss: 2.8213043212890625, Learning Rate: 0.01\n",
      "Epoch [7763/20000], Loss: 2.8155517578125, Learning Rate: 0.01\n",
      "Epoch [7764/20000], Loss: 2.809967041015625, Learning Rate: 0.01\n",
      "Epoch [7765/20000], Loss: 2.80419921875, Learning Rate: 0.01\n",
      "Epoch [7766/20000], Loss: 2.7985076904296875, Learning Rate: 0.01\n",
      "Epoch [7767/20000], Loss: 2.7928314208984375, Learning Rate: 0.01\n",
      "Epoch [7768/20000], Loss: 2.7871246337890625, Learning Rate: 0.01\n",
      "Epoch [7769/20000], Loss: 2.78143310546875, Learning Rate: 0.01\n",
      "Epoch [7770/20000], Loss: 2.77569580078125, Learning Rate: 0.01\n",
      "Epoch [7771/20000], Loss: 2.7700042724609375, Learning Rate: 0.01\n",
      "Epoch [7772/20000], Loss: 2.7642974853515625, Learning Rate: 0.01\n",
      "Epoch [7773/20000], Loss: 2.7585601806640625, Learning Rate: 0.01\n",
      "Epoch [7774/20000], Loss: 2.7529144287109375, Learning Rate: 0.01\n",
      "Epoch [7775/20000], Loss: 2.7471923828125, Learning Rate: 0.01\n",
      "Epoch [7776/20000], Loss: 2.741424560546875, Learning Rate: 0.01\n",
      "Epoch [7777/20000], Loss: 2.7357177734375, Learning Rate: 0.01\n",
      "Epoch [7778/20000], Loss: 2.7300262451171875, Learning Rate: 0.01\n",
      "Epoch [7779/20000], Loss: 2.7242584228515625, Learning Rate: 0.01\n",
      "Epoch [7780/20000], Loss: 2.7187957763671875, Learning Rate: 0.01\n",
      "Epoch [7781/20000], Loss: 2.7129669189453125, Learning Rate: 0.01\n",
      "Epoch [7782/20000], Loss: 2.707122802734375, Learning Rate: 0.01\n",
      "Epoch [7783/20000], Loss: 2.7014617919921875, Learning Rate: 0.01\n",
      "Epoch [7784/20000], Loss: 2.695831298828125, Learning Rate: 0.01\n",
      "Epoch [7785/20000], Loss: 2.6901092529296875, Learning Rate: 0.01\n",
      "Epoch [7786/20000], Loss: 2.6843414306640625, Learning Rate: 0.01\n",
      "Epoch [7787/20000], Loss: 2.6787261962890625, Learning Rate: 0.01\n",
      "Epoch [7788/20000], Loss: 2.6728973388671875, Learning Rate: 0.01\n",
      "Epoch [7789/20000], Loss: 2.6671600341796875, Learning Rate: 0.01\n",
      "Epoch [7790/20000], Loss: 2.661590576171875, Learning Rate: 0.01\n",
      "Epoch [7791/20000], Loss: 2.6556854248046875, Learning Rate: 0.01\n",
      "Epoch [7792/20000], Loss: 2.6500701904296875, Learning Rate: 0.01\n",
      "Epoch [7793/20000], Loss: 2.6443634033203125, Learning Rate: 0.01\n",
      "Epoch [7794/20000], Loss: 2.6385955810546875, Learning Rate: 0.01\n",
      "Epoch [7795/20000], Loss: 2.63299560546875, Learning Rate: 0.01\n",
      "Epoch [7796/20000], Loss: 2.6272125244140625, Learning Rate: 0.01\n",
      "Epoch [7797/20000], Loss: 2.6215972900390625, Learning Rate: 0.01\n",
      "Epoch [7798/20000], Loss: 2.61578369140625, Learning Rate: 0.01\n",
      "Epoch [7799/20000], Loss: 2.610076904296875, Learning Rate: 0.01\n",
      "Epoch [7800/20000], Loss: 2.6043243408203125, Learning Rate: 0.01\n",
      "Epoch [7801/20000], Loss: 2.5985565185546875, Learning Rate: 0.01\n",
      "Epoch [7802/20000], Loss: 2.5929412841796875, Learning Rate: 0.01\n",
      "Epoch [7803/20000], Loss: 2.5872039794921875, Learning Rate: 0.01\n",
      "Epoch [7804/20000], Loss: 2.5815277099609375, Learning Rate: 0.01\n",
      "Epoch [7805/20000], Loss: 2.5758514404296875, Learning Rate: 0.01\n",
      "Epoch [7806/20000], Loss: 2.5700836181640625, Learning Rate: 0.01\n",
      "Epoch [7807/20000], Loss: 2.564361572265625, Learning Rate: 0.01\n",
      "Epoch [7808/20000], Loss: 2.55853271484375, Learning Rate: 0.01\n",
      "Epoch [7809/20000], Loss: 2.5529327392578125, Learning Rate: 0.01\n",
      "Epoch [7810/20000], Loss: 2.5471343994140625, Learning Rate: 0.01\n",
      "Epoch [7811/20000], Loss: 2.5413818359375, Learning Rate: 0.01\n",
      "Epoch [7812/20000], Loss: 2.5355377197265625, Learning Rate: 0.01\n",
      "Epoch [7813/20000], Loss: 2.53009033203125, Learning Rate: 0.01\n",
      "Epoch [7814/20000], Loss: 2.5243377685546875, Learning Rate: 0.01\n",
      "Epoch [7815/20000], Loss: 2.5185546875, Learning Rate: 0.01\n",
      "Epoch [7816/20000], Loss: 2.5128936767578125, Learning Rate: 0.01\n",
      "Epoch [7817/20000], Loss: 2.5071258544921875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7818/20000], Loss: 2.501495361328125, Learning Rate: 0.01\n",
      "Epoch [7819/20000], Loss: 2.4956817626953125, Learning Rate: 0.01\n",
      "Epoch [7820/20000], Loss: 2.4899444580078125, Learning Rate: 0.01\n",
      "Epoch [7821/20000], Loss: 2.48431396484375, Learning Rate: 0.01\n",
      "Epoch [7822/20000], Loss: 2.4786376953125, Learning Rate: 0.01\n",
      "Epoch [7823/20000], Loss: 2.472808837890625, Learning Rate: 0.01\n",
      "Epoch [7824/20000], Loss: 2.4669647216796875, Learning Rate: 0.01\n",
      "Epoch [7825/20000], Loss: 2.46142578125, Learning Rate: 0.01\n",
      "Epoch [7826/20000], Loss: 2.4555816650390625, Learning Rate: 0.01\n",
      "Epoch [7827/20000], Loss: 2.44989013671875, Learning Rate: 0.01\n",
      "Epoch [7828/20000], Loss: 2.4441680908203125, Learning Rate: 0.01\n",
      "Epoch [7829/20000], Loss: 2.438385009765625, Learning Rate: 0.01\n",
      "Epoch [7830/20000], Loss: 2.4326629638671875, Learning Rate: 0.01\n",
      "Epoch [7831/20000], Loss: 2.4269866943359375, Learning Rate: 0.01\n",
      "Epoch [7832/20000], Loss: 2.4211273193359375, Learning Rate: 0.01\n",
      "Epoch [7833/20000], Loss: 2.41558837890625, Learning Rate: 0.01\n",
      "Epoch [7834/20000], Loss: 2.4097442626953125, Learning Rate: 0.01\n",
      "Epoch [7835/20000], Loss: 2.4040069580078125, Learning Rate: 0.01\n",
      "Epoch [7836/20000], Loss: 2.3983001708984375, Learning Rate: 0.01\n",
      "Epoch [7837/20000], Loss: 2.3927001953125, Learning Rate: 0.01\n",
      "Epoch [7838/20000], Loss: 2.3868408203125, Learning Rate: 0.01\n",
      "Epoch [7839/20000], Loss: 2.3811492919921875, Learning Rate: 0.01\n",
      "Epoch [7840/20000], Loss: 2.3754730224609375, Learning Rate: 0.01\n",
      "Epoch [7841/20000], Loss: 2.369720458984375, Learning Rate: 0.01\n",
      "Epoch [7842/20000], Loss: 2.3640594482421875, Learning Rate: 0.01\n",
      "Epoch [7843/20000], Loss: 2.3582916259765625, Learning Rate: 0.01\n",
      "Epoch [7844/20000], Loss: 2.352508544921875, Learning Rate: 0.01\n",
      "Epoch [7845/20000], Loss: 2.3468017578125, Learning Rate: 0.01\n",
      "Epoch [7846/20000], Loss: 2.3409271240234375, Learning Rate: 0.01\n",
      "Epoch [7847/20000], Loss: 2.335235595703125, Learning Rate: 0.01\n",
      "Epoch [7848/20000], Loss: 2.32952880859375, Learning Rate: 0.01\n",
      "Epoch [7849/20000], Loss: 2.3238372802734375, Learning Rate: 0.01\n",
      "Epoch [7850/20000], Loss: 2.3181610107421875, Learning Rate: 0.01\n",
      "Epoch [7851/20000], Loss: 2.3122711181640625, Learning Rate: 0.01\n",
      "Epoch [7852/20000], Loss: 2.3066558837890625, Learning Rate: 0.01\n",
      "Epoch [7853/20000], Loss: 2.30084228515625, Learning Rate: 0.01\n",
      "Epoch [7854/20000], Loss: 2.295013427734375, Learning Rate: 0.01\n",
      "Epoch [7855/20000], Loss: 2.289459228515625, Learning Rate: 0.01\n",
      "Epoch [7856/20000], Loss: 2.2836761474609375, Learning Rate: 0.01\n",
      "Epoch [7857/20000], Loss: 2.2779541015625, Learning Rate: 0.01\n",
      "Epoch [7858/20000], Loss: 2.272247314453125, Learning Rate: 0.01\n",
      "Epoch [7859/20000], Loss: 2.2664642333984375, Learning Rate: 0.01\n",
      "Epoch [7860/20000], Loss: 2.260772705078125, Learning Rate: 0.01\n",
      "Epoch [7861/20000], Loss: 2.2550506591796875, Learning Rate: 0.01\n",
      "Epoch [7862/20000], Loss: 2.2490997314453125, Learning Rate: 0.01\n",
      "Epoch [7863/20000], Loss: 2.243560791015625, Learning Rate: 0.01\n",
      "Epoch [7864/20000], Loss: 2.2378082275390625, Learning Rate: 0.01\n",
      "Epoch [7865/20000], Loss: 2.2320404052734375, Learning Rate: 0.01\n",
      "Epoch [7866/20000], Loss: 2.2263641357421875, Learning Rate: 0.01\n",
      "Epoch [7867/20000], Loss: 2.2205657958984375, Learning Rate: 0.01\n",
      "Epoch [7868/20000], Loss: 2.214813232421875, Learning Rate: 0.01\n",
      "Epoch [7869/20000], Loss: 2.2091827392578125, Learning Rate: 0.01\n",
      "Epoch [7870/20000], Loss: 2.2032623291015625, Learning Rate: 0.01\n",
      "Epoch [7871/20000], Loss: 2.197540283203125, Learning Rate: 0.01\n",
      "Epoch [7872/20000], Loss: 2.1918487548828125, Learning Rate: 0.01\n",
      "Epoch [7873/20000], Loss: 2.186187744140625, Learning Rate: 0.01\n",
      "Epoch [7874/20000], Loss: 2.1803741455078125, Learning Rate: 0.01\n",
      "Epoch [7875/20000], Loss: 2.17462158203125, Learning Rate: 0.01\n",
      "Epoch [7876/20000], Loss: 2.16888427734375, Learning Rate: 0.01\n",
      "Epoch [7877/20000], Loss: 2.1631622314453125, Learning Rate: 0.01\n",
      "Epoch [7878/20000], Loss: 2.1573638916015625, Learning Rate: 0.01\n",
      "Epoch [7879/20000], Loss: 2.151702880859375, Learning Rate: 0.01\n",
      "Epoch [7880/20000], Loss: 2.1458892822265625, Learning Rate: 0.01\n",
      "Epoch [7881/20000], Loss: 2.1401519775390625, Learning Rate: 0.01\n",
      "Epoch [7882/20000], Loss: 2.13433837890625, Learning Rate: 0.01\n",
      "Epoch [7883/20000], Loss: 2.1286773681640625, Learning Rate: 0.01\n",
      "Epoch [7884/20000], Loss: 2.1228485107421875, Learning Rate: 0.01\n",
      "Epoch [7885/20000], Loss: 2.11737060546875, Learning Rate: 0.01\n",
      "Epoch [7886/20000], Loss: 2.1113433837890625, Learning Rate: 0.01\n",
      "Epoch [7887/20000], Loss: 2.105682373046875, Learning Rate: 0.01\n",
      "Epoch [7888/20000], Loss: 2.0999755859375, Learning Rate: 0.01\n",
      "Epoch [7889/20000], Loss: 2.094085693359375, Learning Rate: 0.01\n",
      "Epoch [7890/20000], Loss: 2.08843994140625, Learning Rate: 0.01\n",
      "Epoch [7891/20000], Loss: 2.082611083984375, Learning Rate: 0.01\n",
      "Epoch [7892/20000], Loss: 2.0768585205078125, Learning Rate: 0.01\n",
      "Epoch [7893/20000], Loss: 2.0712890625, Learning Rate: 0.01\n",
      "Epoch [7894/20000], Loss: 2.0653533935546875, Learning Rate: 0.01\n",
      "Epoch [7895/20000], Loss: 2.05963134765625, Learning Rate: 0.01\n",
      "Epoch [7896/20000], Loss: 2.0537109375, Learning Rate: 0.01\n",
      "Epoch [7897/20000], Loss: 2.04827880859375, Learning Rate: 0.01\n",
      "Epoch [7898/20000], Loss: 2.0423431396484375, Learning Rate: 0.01\n",
      "Epoch [7899/20000], Loss: 2.0366363525390625, Learning Rate: 0.01\n",
      "Epoch [7900/20000], Loss: 2.031005859375, Learning Rate: 0.01\n",
      "Epoch [7901/20000], Loss: 2.0252532958984375, Learning Rate: 0.01\n",
      "Epoch [7902/20000], Loss: 2.01934814453125, Learning Rate: 0.01\n",
      "Epoch [7903/20000], Loss: 2.01373291015625, Learning Rate: 0.01\n",
      "Epoch [7904/20000], Loss: 2.008056640625, Learning Rate: 0.01\n",
      "Epoch [7905/20000], Loss: 2.0022125244140625, Learning Rate: 0.01\n",
      "Epoch [7906/20000], Loss: 1.9966888427734375, Learning Rate: 0.01\n",
      "Epoch [7907/20000], Loss: 1.990936279296875, Learning Rate: 0.01\n",
      "Epoch [7908/20000], Loss: 1.9855804443359375, Learning Rate: 0.01\n",
      "Epoch [7909/20000], Loss: 1.980010986328125, Learning Rate: 0.01\n",
      "Epoch [7910/20000], Loss: 1.97467041015625, Learning Rate: 0.01\n",
      "Epoch [7911/20000], Loss: 1.969512939453125, Learning Rate: 0.01\n",
      "Epoch [7912/20000], Loss: 1.9646759033203125, Learning Rate: 0.01\n",
      "Epoch [7913/20000], Loss: 1.9602508544921875, Learning Rate: 0.01\n",
      "Epoch [7914/20000], Loss: 1.9565582275390625, Learning Rate: 0.01\n",
      "Epoch [7915/20000], Loss: 1.95404052734375, Learning Rate: 0.01\n",
      "Epoch [7916/20000], Loss: 1.9533538818359375, Learning Rate: 0.01\n",
      "Epoch [7917/20000], Loss: 1.9552001953125, Learning Rate: 0.01\n",
      "Epoch [7918/20000], Loss: 1.961273193359375, Learning Rate: 0.01\n",
      "Epoch [7919/20000], Loss: 1.97412109375, Learning Rate: 0.01\n",
      "Epoch [7920/20000], Loss: 1.9972381591796875, Learning Rate: 0.01\n",
      "Epoch [7921/20000], Loss: 2.037017822265625, Learning Rate: 0.01\n",
      "Epoch [7922/20000], Loss: 2.10357666015625, Learning Rate: 0.01\n",
      "Epoch [7923/20000], Loss: 2.212310791015625, Learning Rate: 0.01\n",
      "Epoch [7924/20000], Loss: 2.3893280029296875, Learning Rate: 0.01\n",
      "Epoch [7925/20000], Loss: 2.6748199462890625, Learning Rate: 0.01\n",
      "Epoch [7926/20000], Loss: 3.132720947265625, Learning Rate: 0.01\n",
      "Epoch [7927/20000], Loss: 3.86090087890625, Learning Rate: 0.01\n",
      "Epoch [7928/20000], Loss: 5.00390625, Learning Rate: 0.01\n",
      "Epoch [7929/20000], Loss: 6.75323486328125, Learning Rate: 0.01\n",
      "Epoch [7930/20000], Loss: 9.324844360351562, Learning Rate: 0.01\n",
      "Epoch [7931/20000], Loss: 12.839202880859375, Learning Rate: 0.01\n",
      "Epoch [7932/20000], Loss: 17.081817626953125, Learning Rate: 0.01\n",
      "Epoch [7933/20000], Loss: 21.080551147460938, Learning Rate: 0.01\n",
      "Epoch [7934/20000], Loss: 22.984588623046875, Learning Rate: 0.01\n",
      "Epoch [7935/20000], Loss: 20.78125, Learning Rate: 0.01\n",
      "Epoch [7936/20000], Loss: 14.380447387695312, Learning Rate: 0.01\n",
      "Epoch [7937/20000], Loss: 6.76806640625, Learning Rate: 0.01\n",
      "Epoch [7938/20000], Loss: 2.2160186767578125, Learning Rate: 0.01\n",
      "Epoch [7939/20000], Loss: 2.671173095703125, Learning Rate: 0.01\n",
      "Epoch [7940/20000], Loss: 6.4243621826171875, Learning Rate: 0.01\n",
      "Epoch [7941/20000], Loss: 9.8046875, Learning Rate: 0.01\n",
      "Epoch [7942/20000], Loss: 9.93798828125, Learning Rate: 0.01\n",
      "Epoch [7943/20000], Loss: 6.81884765625, Learning Rate: 0.01\n",
      "Epoch [7944/20000], Loss: 3.1457672119140625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7945/20000], Loss: 1.770477294921875, Learning Rate: 0.01\n",
      "Epoch [7946/20000], Loss: 3.14239501953125, Learning Rate: 0.01\n",
      "Epoch [7947/20000], Loss: 5.311676025390625, Learning Rate: 0.01\n",
      "Epoch [7948/20000], Loss: 6.0157623291015625, Learning Rate: 0.01\n",
      "Epoch [7949/20000], Loss: 4.6724090576171875, Learning Rate: 0.01\n",
      "Epoch [7950/20000], Loss: 2.652496337890625, Learning Rate: 0.01\n",
      "Epoch [7951/20000], Loss: 1.7319183349609375, Learning Rate: 0.01\n",
      "Epoch [7952/20000], Loss: 2.382049560546875, Learning Rate: 0.01\n",
      "Epoch [7953/20000], Loss: 3.5679779052734375, Learning Rate: 0.01\n",
      "Epoch [7954/20000], Loss: 3.9695281982421875, Learning Rate: 0.01\n",
      "Epoch [7955/20000], Loss: 3.238250732421875, Learning Rate: 0.01\n",
      "Epoch [7956/20000], Loss: 2.1529998779296875, Learning Rate: 0.01\n",
      "Epoch [7957/20000], Loss: 1.6944122314453125, Learning Rate: 0.01\n",
      "Epoch [7958/20000], Loss: 2.082855224609375, Learning Rate: 0.01\n",
      "Epoch [7959/20000], Loss: 2.7144775390625, Learning Rate: 0.01\n",
      "Epoch [7960/20000], Loss: 2.8816070556640625, Learning Rate: 0.01\n",
      "Epoch [7961/20000], Loss: 2.4488067626953125, Learning Rate: 0.01\n",
      "Epoch [7962/20000], Loss: 1.8730316162109375, Learning Rate: 0.01\n",
      "Epoch [7963/20000], Loss: 1.6633758544921875, Learning Rate: 0.01\n",
      "Epoch [7964/20000], Loss: 1.895965576171875, Learning Rate: 0.01\n",
      "Epoch [7965/20000], Loss: 2.2257537841796875, Learning Rate: 0.01\n",
      "Epoch [7966/20000], Loss: 2.2874755859375, Learning Rate: 0.01\n",
      "Epoch [7967/20000], Loss: 2.0369415283203125, Learning Rate: 0.01\n",
      "Epoch [7968/20000], Loss: 1.73004150390625, Learning Rate: 0.01\n",
      "Epoch [7969/20000], Loss: 1.6296844482421875, Learning Rate: 0.01\n",
      "Epoch [7970/20000], Loss: 1.76019287109375, Learning Rate: 0.01\n",
      "Epoch [7971/20000], Loss: 1.9318695068359375, Learning Rate: 0.01\n",
      "Epoch [7972/20000], Loss: 1.9562530517578125, Learning Rate: 0.01\n",
      "Epoch [7973/20000], Loss: 1.81695556640625, Learning Rate: 0.01\n",
      "Epoch [7974/20000], Loss: 1.6514129638671875, Learning Rate: 0.01\n",
      "Epoch [7975/20000], Loss: 1.594970703125, Learning Rate: 0.01\n",
      "Epoch [7976/20000], Loss: 1.66033935546875, Learning Rate: 0.01\n",
      "Epoch [7977/20000], Loss: 1.749908447265625, Learning Rate: 0.01\n",
      "Epoch [7978/20000], Loss: 1.7643585205078125, Learning Rate: 0.01\n",
      "Epoch [7979/20000], Loss: 1.69140625, Learning Rate: 0.01\n",
      "Epoch [7980/20000], Loss: 1.599273681640625, Learning Rate: 0.01\n",
      "Epoch [7981/20000], Loss: 1.560272216796875, Learning Rate: 0.01\n",
      "Epoch [7982/20000], Loss: 1.5876007080078125, Learning Rate: 0.01\n",
      "Epoch [7983/20000], Loss: 1.63397216796875, Learning Rate: 0.01\n",
      "Epoch [7984/20000], Loss: 1.6454925537109375, Learning Rate: 0.01\n",
      "Epoch [7985/20000], Loss: 1.6094207763671875, Learning Rate: 0.01\n",
      "Epoch [7986/20000], Loss: 1.5572052001953125, Learning Rate: 0.01\n",
      "Epoch [7987/20000], Loss: 1.5276641845703125, Learning Rate: 0.01\n",
      "Epoch [7988/20000], Loss: 1.5338287353515625, Learning Rate: 0.01\n",
      "Epoch [7989/20000], Loss: 1.55584716796875, Learning Rate: 0.01\n",
      "Epoch [7990/20000], Loss: 1.5648193359375, Learning Rate: 0.01\n",
      "Epoch [7991/20000], Loss: 1.54876708984375, Learning Rate: 0.01\n",
      "Epoch [7992/20000], Loss: 1.51898193359375, Learning Rate: 0.01\n",
      "Epoch [7993/20000], Loss: 1.496429443359375, Learning Rate: 0.01\n",
      "Epoch [7994/20000], Loss: 1.49169921875, Learning Rate: 0.01\n",
      "Epoch [7995/20000], Loss: 1.4994049072265625, Learning Rate: 0.01\n",
      "Epoch [7996/20000], Loss: 1.505126953125, Learning Rate: 0.01\n",
      "Epoch [7997/20000], Loss: 1.498565673828125, Learning Rate: 0.01\n",
      "Epoch [7998/20000], Loss: 1.4820098876953125, Learning Rate: 0.01\n",
      "Epoch [7999/20000], Loss: 1.4652252197265625, Learning Rate: 0.01\n",
      "Epoch [8000/20000], Loss: 1.4562225341796875, Learning Rate: 0.01\n",
      "Epoch [8001/20000], Loss: 1.4556121826171875, Learning Rate: 0.01\n",
      "Epoch [8002/20000], Loss: 1.457000732421875, Learning Rate: 0.01\n",
      "Epoch [8003/20000], Loss: 1.454193115234375, Learning Rate: 0.01\n",
      "Epoch [8004/20000], Loss: 1.445281982421875, Learning Rate: 0.01\n",
      "Epoch [8005/20000], Loss: 1.4334259033203125, Learning Rate: 0.01\n",
      "Epoch [8006/20000], Loss: 1.4238433837890625, Learning Rate: 0.01\n",
      "Epoch [8007/20000], Loss: 1.418609619140625, Learning Rate: 0.01\n",
      "Epoch [8008/20000], Loss: 1.4163970947265625, Learning Rate: 0.01\n",
      "Epoch [8009/20000], Loss: 1.4136505126953125, Learning Rate: 0.01\n",
      "Epoch [8010/20000], Loss: 1.408447265625, Learning Rate: 0.01\n",
      "Epoch [8011/20000], Loss: 1.400390625, Learning Rate: 0.01\n",
      "Epoch [8012/20000], Loss: 1.3917999267578125, Learning Rate: 0.01\n",
      "Epoch [8013/20000], Loss: 1.3850555419921875, Learning Rate: 0.01\n",
      "Epoch [8014/20000], Loss: 1.3800506591796875, Learning Rate: 0.01\n",
      "Epoch [8015/20000], Loss: 1.3763885498046875, Learning Rate: 0.01\n",
      "Epoch [8016/20000], Loss: 1.372283935546875, Learning Rate: 0.01\n",
      "Epoch [8017/20000], Loss: 1.3663482666015625, Learning Rate: 0.01\n",
      "Epoch [8018/20000], Loss: 1.3594818115234375, Learning Rate: 0.01\n",
      "Epoch [8019/20000], Loss: 1.3526763916015625, Learning Rate: 0.01\n",
      "Epoch [8020/20000], Loss: 1.3465423583984375, Learning Rate: 0.01\n",
      "Epoch [8021/20000], Loss: 1.3415374755859375, Learning Rate: 0.01\n",
      "Epoch [8022/20000], Loss: 1.336883544921875, Learning Rate: 0.01\n",
      "Epoch [8023/20000], Loss: 1.3319549560546875, Learning Rate: 0.01\n",
      "Epoch [8024/20000], Loss: 1.3263702392578125, Learning Rate: 0.01\n",
      "Epoch [8025/20000], Loss: 1.320159912109375, Learning Rate: 0.01\n",
      "Epoch [8026/20000], Loss: 1.3139190673828125, Learning Rate: 0.01\n",
      "Epoch [8027/20000], Loss: 1.308380126953125, Learning Rate: 0.01\n",
      "Epoch [8028/20000], Loss: 1.3029632568359375, Learning Rate: 0.01\n",
      "Epoch [8029/20000], Loss: 1.2979278564453125, Learning Rate: 0.01\n",
      "Epoch [8030/20000], Loss: 1.2926177978515625, Learning Rate: 0.01\n",
      "Epoch [8031/20000], Loss: 1.2870635986328125, Learning Rate: 0.01\n",
      "Epoch [8032/20000], Loss: 1.28143310546875, Learning Rate: 0.01\n",
      "Epoch [8033/20000], Loss: 1.2756195068359375, Learning Rate: 0.01\n",
      "Epoch [8034/20000], Loss: 1.26995849609375, Learning Rate: 0.01\n",
      "Epoch [8035/20000], Loss: 1.2644195556640625, Learning Rate: 0.01\n",
      "Epoch [8036/20000], Loss: 1.259124755859375, Learning Rate: 0.01\n",
      "Epoch [8037/20000], Loss: 1.2537841796875, Learning Rate: 0.01\n",
      "Epoch [8038/20000], Loss: 1.248321533203125, Learning Rate: 0.01\n",
      "Epoch [8039/20000], Loss: 1.242828369140625, Learning Rate: 0.01\n",
      "Epoch [8040/20000], Loss: 1.2370758056640625, Learning Rate: 0.01\n",
      "Epoch [8041/20000], Loss: 1.231414794921875, Learning Rate: 0.01\n",
      "Epoch [8042/20000], Loss: 1.225830078125, Learning Rate: 0.01\n",
      "Epoch [8043/20000], Loss: 1.22039794921875, Learning Rate: 0.01\n",
      "Epoch [8044/20000], Loss: 1.2151641845703125, Learning Rate: 0.01\n",
      "Epoch [8045/20000], Loss: 1.2097320556640625, Learning Rate: 0.01\n",
      "Epoch [8046/20000], Loss: 1.2042388916015625, Learning Rate: 0.01\n",
      "Epoch [8047/20000], Loss: 1.198699951171875, Learning Rate: 0.01\n",
      "Epoch [8048/20000], Loss: 1.19305419921875, Learning Rate: 0.01\n",
      "Epoch [8049/20000], Loss: 1.1875457763671875, Learning Rate: 0.01\n",
      "Epoch [8050/20000], Loss: 1.1821441650390625, Learning Rate: 0.01\n",
      "Epoch [8051/20000], Loss: 1.176605224609375, Learning Rate: 0.01\n",
      "Epoch [8052/20000], Loss: 1.171234130859375, Learning Rate: 0.01\n",
      "Epoch [8053/20000], Loss: 1.165771484375, Learning Rate: 0.01\n",
      "Epoch [8054/20000], Loss: 1.160125732421875, Learning Rate: 0.01\n",
      "Epoch [8055/20000], Loss: 1.1546783447265625, Learning Rate: 0.01\n",
      "Epoch [8056/20000], Loss: 1.1490325927734375, Learning Rate: 0.01\n",
      "Epoch [8057/20000], Loss: 1.143524169921875, Learning Rate: 0.01\n",
      "Epoch [8058/20000], Loss: 1.1381072998046875, Learning Rate: 0.01\n",
      "Epoch [8059/20000], Loss: 1.132568359375, Learning Rate: 0.01\n",
      "Epoch [8060/20000], Loss: 1.127197265625, Learning Rate: 0.01\n",
      "Epoch [8061/20000], Loss: 1.1217498779296875, Learning Rate: 0.01\n",
      "Epoch [8062/20000], Loss: 1.1161651611328125, Learning Rate: 0.01\n",
      "Epoch [8063/20000], Loss: 1.110687255859375, Learning Rate: 0.01\n",
      "Epoch [8064/20000], Loss: 1.1051177978515625, Learning Rate: 0.01\n",
      "Epoch [8065/20000], Loss: 1.0994873046875, Learning Rate: 0.01\n",
      "Epoch [8066/20000], Loss: 1.094085693359375, Learning Rate: 0.01\n",
      "Epoch [8067/20000], Loss: 1.0886383056640625, Learning Rate: 0.01\n",
      "Epoch [8068/20000], Loss: 1.0831146240234375, Learning Rate: 0.01\n",
      "Epoch [8069/20000], Loss: 1.07769775390625, Learning Rate: 0.01\n",
      "Epoch [8070/20000], Loss: 1.0721435546875, Learning Rate: 0.01\n",
      "Epoch [8071/20000], Loss: 1.066650390625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8072/20000], Loss: 1.0612335205078125, Learning Rate: 0.01\n",
      "Epoch [8073/20000], Loss: 1.0554962158203125, Learning Rate: 0.01\n",
      "Epoch [8074/20000], Loss: 1.0501708984375, Learning Rate: 0.01\n",
      "Epoch [8075/20000], Loss: 1.0444793701171875, Learning Rate: 0.01\n",
      "Epoch [8076/20000], Loss: 1.0389556884765625, Learning Rate: 0.01\n",
      "Epoch [8077/20000], Loss: 1.0334320068359375, Learning Rate: 0.01\n",
      "Epoch [8078/20000], Loss: 1.028045654296875, Learning Rate: 0.01\n",
      "Epoch [8079/20000], Loss: 1.0227203369140625, Learning Rate: 0.01\n",
      "Epoch [8080/20000], Loss: 1.01708984375, Learning Rate: 0.01\n",
      "Epoch [8081/20000], Loss: 1.0115966796875, Learning Rate: 0.01\n",
      "Epoch [8082/20000], Loss: 1.005889892578125, Learning Rate: 0.01\n",
      "Epoch [8083/20000], Loss: 1.0004119873046875, Learning Rate: 0.01\n",
      "Epoch [8084/20000], Loss: 0.9949951171875, Learning Rate: 0.01\n",
      "Epoch [8085/20000], Loss: 0.9896697998046875, Learning Rate: 0.01\n",
      "Epoch [8086/20000], Loss: 0.9839935302734375, Learning Rate: 0.01\n",
      "Epoch [8087/20000], Loss: 0.9785003662109375, Learning Rate: 0.01\n",
      "Epoch [8088/20000], Loss: 0.972930908203125, Learning Rate: 0.01\n",
      "Epoch [8089/20000], Loss: 0.967559814453125, Learning Rate: 0.01\n",
      "Epoch [8090/20000], Loss: 0.962066650390625, Learning Rate: 0.01\n",
      "Epoch [8091/20000], Loss: 0.956512451171875, Learning Rate: 0.01\n",
      "Epoch [8092/20000], Loss: 0.9509735107421875, Learning Rate: 0.01\n",
      "Epoch [8093/20000], Loss: 0.94537353515625, Learning Rate: 0.01\n",
      "Epoch [8094/20000], Loss: 0.9400177001953125, Learning Rate: 0.01\n",
      "Epoch [8095/20000], Loss: 0.934326171875, Learning Rate: 0.01\n",
      "Epoch [8096/20000], Loss: 0.9289398193359375, Learning Rate: 0.01\n",
      "Epoch [8097/20000], Loss: 0.92333984375, Learning Rate: 0.01\n",
      "Epoch [8098/20000], Loss: 0.917755126953125, Learning Rate: 0.01\n",
      "Epoch [8099/20000], Loss: 0.9123077392578125, Learning Rate: 0.01\n",
      "Epoch [8100/20000], Loss: 0.906768798828125, Learning Rate: 0.01\n",
      "Epoch [8101/20000], Loss: 0.9011993408203125, Learning Rate: 0.01\n",
      "Epoch [8102/20000], Loss: 0.8958892822265625, Learning Rate: 0.01\n",
      "Epoch [8103/20000], Loss: 0.8902435302734375, Learning Rate: 0.01\n",
      "Epoch [8104/20000], Loss: 0.8848114013671875, Learning Rate: 0.01\n",
      "Epoch [8105/20000], Loss: 0.879180908203125, Learning Rate: 0.01\n",
      "Epoch [8106/20000], Loss: 0.8736572265625, Learning Rate: 0.01\n",
      "Epoch [8107/20000], Loss: 0.8680877685546875, Learning Rate: 0.01\n",
      "Epoch [8108/20000], Loss: 0.862518310546875, Learning Rate: 0.01\n",
      "Epoch [8109/20000], Loss: 0.8570404052734375, Learning Rate: 0.01\n",
      "Epoch [8110/20000], Loss: 0.851531982421875, Learning Rate: 0.01\n",
      "Epoch [8111/20000], Loss: 0.846038818359375, Learning Rate: 0.01\n",
      "Epoch [8112/20000], Loss: 0.8405914306640625, Learning Rate: 0.01\n",
      "Epoch [8113/20000], Loss: 0.834991455078125, Learning Rate: 0.01\n",
      "Epoch [8114/20000], Loss: 0.8295135498046875, Learning Rate: 0.01\n",
      "Epoch [8115/20000], Loss: 0.824066162109375, Learning Rate: 0.01\n",
      "Epoch [8116/20000], Loss: 0.818450927734375, Learning Rate: 0.01\n",
      "Epoch [8117/20000], Loss: 0.8128662109375, Learning Rate: 0.01\n",
      "Epoch [8118/20000], Loss: 0.8074493408203125, Learning Rate: 0.01\n",
      "Epoch [8119/20000], Loss: 0.8018341064453125, Learning Rate: 0.01\n",
      "Epoch [8120/20000], Loss: 0.7963104248046875, Learning Rate: 0.01\n",
      "Epoch [8121/20000], Loss: 0.790863037109375, Learning Rate: 0.01\n",
      "Epoch [8122/20000], Loss: 0.7852020263671875, Learning Rate: 0.01\n",
      "Epoch [8123/20000], Loss: 0.7799072265625, Learning Rate: 0.01\n",
      "Epoch [8124/20000], Loss: 0.7741851806640625, Learning Rate: 0.01\n",
      "Epoch [8125/20000], Loss: 0.768646240234375, Learning Rate: 0.01\n",
      "Epoch [8126/20000], Loss: 0.763031005859375, Learning Rate: 0.01\n",
      "Epoch [8127/20000], Loss: 0.75762939453125, Learning Rate: 0.01\n",
      "Epoch [8128/20000], Loss: 0.7520599365234375, Learning Rate: 0.01\n",
      "Epoch [8129/20000], Loss: 0.746673583984375, Learning Rate: 0.01\n",
      "Epoch [8130/20000], Loss: 0.7409820556640625, Learning Rate: 0.01\n",
      "Epoch [8131/20000], Loss: 0.7354278564453125, Learning Rate: 0.01\n",
      "Epoch [8132/20000], Loss: 0.729888916015625, Learning Rate: 0.01\n",
      "Epoch [8133/20000], Loss: 0.724365234375, Learning Rate: 0.01\n",
      "Epoch [8134/20000], Loss: 0.718841552734375, Learning Rate: 0.01\n",
      "Epoch [8135/20000], Loss: 0.713348388671875, Learning Rate: 0.01\n",
      "Epoch [8136/20000], Loss: 0.7077789306640625, Learning Rate: 0.01\n",
      "Epoch [8137/20000], Loss: 0.702178955078125, Learning Rate: 0.01\n",
      "Epoch [8138/20000], Loss: 0.696685791015625, Learning Rate: 0.01\n",
      "Epoch [8139/20000], Loss: 0.6910552978515625, Learning Rate: 0.01\n",
      "Epoch [8140/20000], Loss: 0.685638427734375, Learning Rate: 0.01\n",
      "Epoch [8141/20000], Loss: 0.6800079345703125, Learning Rate: 0.01\n",
      "Epoch [8142/20000], Loss: 0.6746063232421875, Learning Rate: 0.01\n",
      "Epoch [8143/20000], Loss: 0.6690216064453125, Learning Rate: 0.01\n",
      "Epoch [8144/20000], Loss: 0.6634063720703125, Learning Rate: 0.01\n",
      "Epoch [8145/20000], Loss: 0.6579132080078125, Learning Rate: 0.01\n",
      "Epoch [8146/20000], Loss: 0.652374267578125, Learning Rate: 0.01\n",
      "Epoch [8147/20000], Loss: 0.64691162109375, Learning Rate: 0.01\n",
      "Epoch [8148/20000], Loss: 0.641357421875, Learning Rate: 0.01\n",
      "Epoch [8149/20000], Loss: 0.63580322265625, Learning Rate: 0.01\n",
      "Epoch [8150/20000], Loss: 0.6301727294921875, Learning Rate: 0.01\n",
      "Epoch [8151/20000], Loss: 0.6245574951171875, Learning Rate: 0.01\n",
      "Epoch [8152/20000], Loss: 0.6189422607421875, Learning Rate: 0.01\n",
      "Epoch [8153/20000], Loss: 0.613555908203125, Learning Rate: 0.01\n",
      "Epoch [8154/20000], Loss: 0.6078338623046875, Learning Rate: 0.01\n",
      "Epoch [8155/20000], Loss: 0.6024932861328125, Learning Rate: 0.01\n",
      "Epoch [8156/20000], Loss: 0.596893310546875, Learning Rate: 0.01\n",
      "Epoch [8157/20000], Loss: 0.5912322998046875, Learning Rate: 0.01\n",
      "Epoch [8158/20000], Loss: 0.58587646484375, Learning Rate: 0.01\n",
      "Epoch [8159/20000], Loss: 0.58026123046875, Learning Rate: 0.01\n",
      "Epoch [8160/20000], Loss: 0.57470703125, Learning Rate: 0.01\n",
      "Epoch [8161/20000], Loss: 0.569122314453125, Learning Rate: 0.01\n",
      "Epoch [8162/20000], Loss: 0.5636749267578125, Learning Rate: 0.01\n",
      "Epoch [8163/20000], Loss: 0.557952880859375, Learning Rate: 0.01\n",
      "Epoch [8164/20000], Loss: 0.5525054931640625, Learning Rate: 0.01\n",
      "Epoch [8165/20000], Loss: 0.546875, Learning Rate: 0.01\n",
      "Epoch [8166/20000], Loss: 0.54132080078125, Learning Rate: 0.01\n",
      "Epoch [8167/20000], Loss: 0.5357818603515625, Learning Rate: 0.01\n",
      "Epoch [8168/20000], Loss: 0.530242919921875, Learning Rate: 0.01\n",
      "Epoch [8169/20000], Loss: 0.5247650146484375, Learning Rate: 0.01\n",
      "Epoch [8170/20000], Loss: 0.51910400390625, Learning Rate: 0.01\n",
      "Epoch [8171/20000], Loss: 0.513702392578125, Learning Rate: 0.01\n",
      "Epoch [8172/20000], Loss: 0.508056640625, Learning Rate: 0.01\n",
      "Epoch [8173/20000], Loss: 0.502410888671875, Learning Rate: 0.01\n",
      "Epoch [8174/20000], Loss: 0.496795654296875, Learning Rate: 0.01\n",
      "Epoch [8175/20000], Loss: 0.49127197265625, Learning Rate: 0.01\n",
      "Epoch [8176/20000], Loss: 0.4857177734375, Learning Rate: 0.01\n",
      "Epoch [8177/20000], Loss: 0.4802093505859375, Learning Rate: 0.01\n",
      "Epoch [8178/20000], Loss: 0.474761962890625, Learning Rate: 0.01\n",
      "Epoch [8179/20000], Loss: 0.4691314697265625, Learning Rate: 0.01\n",
      "Epoch [8180/20000], Loss: 0.4634857177734375, Learning Rate: 0.01\n",
      "Epoch [8181/20000], Loss: 0.4580230712890625, Learning Rate: 0.01\n",
      "Epoch [8182/20000], Loss: 0.4523162841796875, Learning Rate: 0.01\n",
      "Epoch [8183/20000], Loss: 0.446929931640625, Learning Rate: 0.01\n",
      "Epoch [8184/20000], Loss: 0.4413299560546875, Learning Rate: 0.01\n",
      "Epoch [8185/20000], Loss: 0.4356842041015625, Learning Rate: 0.01\n",
      "Epoch [8186/20000], Loss: 0.4301910400390625, Learning Rate: 0.01\n",
      "Epoch [8187/20000], Loss: 0.4247894287109375, Learning Rate: 0.01\n",
      "Epoch [8188/20000], Loss: 0.4190673828125, Learning Rate: 0.01\n",
      "Epoch [8189/20000], Loss: 0.4135284423828125, Learning Rate: 0.01\n",
      "Epoch [8190/20000], Loss: 0.40814208984375, Learning Rate: 0.01\n",
      "Epoch [8191/20000], Loss: 0.4025726318359375, Learning Rate: 0.01\n",
      "Epoch [8192/20000], Loss: 0.3970489501953125, Learning Rate: 0.01\n",
      "Epoch [8193/20000], Loss: 0.3917388916015625, Learning Rate: 0.01\n",
      "Epoch [8194/20000], Loss: 0.3862457275390625, Learning Rate: 0.01\n",
      "Epoch [8195/20000], Loss: 0.3811492919921875, Learning Rate: 0.01\n",
      "Epoch [8196/20000], Loss: 0.376129150390625, Learning Rate: 0.01\n",
      "Epoch [8197/20000], Loss: 0.3717041015625, Learning Rate: 0.01\n",
      "Epoch [8198/20000], Loss: 0.3674468994140625, Learning Rate: 0.01\n",
      "Epoch [8199/20000], Loss: 0.364288330078125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8200/20000], Loss: 0.362152099609375, Learning Rate: 0.01\n",
      "Epoch [8201/20000], Loss: 0.3624114990234375, Learning Rate: 0.01\n",
      "Epoch [8202/20000], Loss: 0.3657989501953125, Learning Rate: 0.01\n",
      "Epoch [8203/20000], Loss: 0.37469482421875, Learning Rate: 0.01\n",
      "Epoch [8204/20000], Loss: 0.392822265625, Learning Rate: 0.01\n",
      "Epoch [8205/20000], Loss: 0.425140380859375, Learning Rate: 0.01\n",
      "Epoch [8206/20000], Loss: 0.481231689453125, Learning Rate: 0.01\n",
      "Epoch [8207/20000], Loss: 0.576416015625, Learning Rate: 0.01\n",
      "Epoch [8208/20000], Loss: 0.7357025146484375, Learning Rate: 0.01\n",
      "Epoch [8209/20000], Loss: 1.00042724609375, Learning Rate: 0.01\n",
      "Epoch [8210/20000], Loss: 1.43804931640625, Learning Rate: 0.01\n",
      "Epoch [8211/20000], Loss: 2.1547393798828125, Learning Rate: 0.01\n",
      "Epoch [8212/20000], Loss: 3.316070556640625, Learning Rate: 0.01\n",
      "Epoch [8213/20000], Loss: 5.155792236328125, Learning Rate: 0.01\n",
      "Epoch [8214/20000], Loss: 7.964019775390625, Learning Rate: 0.01\n",
      "Epoch [8215/20000], Loss: 11.966232299804688, Learning Rate: 0.01\n",
      "Epoch [8216/20000], Loss: 17.031417846679688, Learning Rate: 0.01\n",
      "Epoch [8217/20000], Loss: 22.091934204101562, Learning Rate: 0.01\n",
      "Epoch [8218/20000], Loss: 24.821624755859375, Learning Rate: 0.01\n",
      "Epoch [8219/20000], Loss: 22.453475952148438, Learning Rate: 0.01\n",
      "Epoch [8220/20000], Loss: 14.679779052734375, Learning Rate: 0.01\n",
      "Epoch [8221/20000], Loss: 5.4747314453125, Learning Rate: 0.01\n",
      "Epoch [8222/20000], Loss: 0.5008087158203125, Learning Rate: 0.01\n",
      "Epoch [8223/20000], Loss: 1.91741943359375, Learning Rate: 0.01\n",
      "Epoch [8224/20000], Loss: 6.8500213623046875, Learning Rate: 0.01\n",
      "Epoch [8225/20000], Loss: 10.266830444335938, Learning Rate: 0.01\n",
      "Epoch [8226/20000], Loss: 9.047637939453125, Learning Rate: 0.01\n",
      "Epoch [8227/20000], Loss: 4.46240234375, Learning Rate: 0.01\n",
      "Epoch [8228/20000], Loss: 0.73388671875, Learning Rate: 0.01\n",
      "Epoch [8229/20000], Loss: 0.73382568359375, Learning Rate: 0.01\n",
      "Epoch [8230/20000], Loss: 3.3969879150390625, Learning Rate: 0.01\n",
      "Epoch [8231/20000], Loss: 5.37725830078125, Learning Rate: 0.01\n",
      "Epoch [8232/20000], Loss: 4.5736846923828125, Learning Rate: 0.01\n",
      "Epoch [8233/20000], Loss: 1.9733428955078125, Learning Rate: 0.01\n",
      "Epoch [8234/20000], Loss: 0.2460174560546875, Learning Rate: 0.01\n",
      "Epoch [8235/20000], Loss: 0.746856689453125, Learning Rate: 0.01\n",
      "Epoch [8236/20000], Loss: 2.34698486328125, Learning Rate: 0.01\n",
      "Epoch [8237/20000], Loss: 3.0226287841796875, Learning Rate: 0.01\n",
      "Epoch [8238/20000], Loss: 2.0701751708984375, Learning Rate: 0.01\n",
      "Epoch [8239/20000], Loss: 0.619049072265625, Learning Rate: 0.01\n",
      "Epoch [8240/20000], Loss: 0.1360321044921875, Learning Rate: 0.01\n",
      "Epoch [8241/20000], Loss: 0.8179473876953125, Learning Rate: 0.01\n",
      "Epoch [8242/20000], Loss: 1.6177520751953125, Learning Rate: 0.01\n",
      "Epoch [8243/20000], Loss: 1.5704803466796875, Learning Rate: 0.01\n",
      "Epoch [8244/20000], Loss: 0.790740966796875, Learning Rate: 0.01\n",
      "Epoch [8245/20000], Loss: 0.1514434814453125, Learning Rate: 0.01\n",
      "Epoch [8246/20000], Loss: 0.2204437255859375, Learning Rate: 0.01\n",
      "Epoch [8247/20000], Loss: 0.7183380126953125, Learning Rate: 0.01\n",
      "Epoch [8248/20000], Loss: 0.9778594970703125, Learning Rate: 0.01\n",
      "Epoch [8249/20000], Loss: 0.716796875, Learning Rate: 0.01\n",
      "Epoch [8250/20000], Loss: 0.255859375, Learning Rate: 0.01\n",
      "Epoch [8251/20000], Loss: 0.0662841796875, Learning Rate: 0.01\n",
      "Epoch [8252/20000], Loss: 0.248260498046875, Learning Rate: 0.01\n",
      "Epoch [8253/20000], Loss: 0.5015716552734375, Learning Rate: 0.01\n",
      "Epoch [8254/20000], Loss: 0.515899658203125, Learning Rate: 0.01\n",
      "Epoch [8255/20000], Loss: 0.290313720703125, Learning Rate: 0.01\n",
      "Epoch [8256/20000], Loss: 0.0721435546875, Learning Rate: 0.01\n",
      "Epoch [8257/20000], Loss: 0.0554656982421875, Learning Rate: 0.01\n",
      "Epoch [8258/20000], Loss: 0.193267822265625, Learning Rate: 0.01\n",
      "Epoch [8259/20000], Loss: 0.2927703857421875, Learning Rate: 0.01\n",
      "Epoch [8260/20000], Loss: 0.2398834228515625, Learning Rate: 0.01\n",
      "Epoch [8261/20000], Loss: 0.09661865234375, Learning Rate: 0.01\n",
      "Epoch [8262/20000], Loss: 0.0062103271484375, Learning Rate: 0.01\n",
      "Epoch [8263/20000], Loss: 0.0320892333984375, Learning Rate: 0.01\n",
      "Epoch [8264/20000], Loss: 0.1112823486328125, Learning Rate: 0.01\n",
      "Epoch [8265/20000], Loss: 0.140045166015625, Learning Rate: 0.01\n",
      "Epoch [8266/20000], Loss: 0.085357666015625, Learning Rate: 0.01\n",
      "Epoch [8267/20000], Loss: 0.0042266845703125, Learning Rate: 0.01\n",
      "Epoch [8268/20000], Loss: -0.03045654296875, Learning Rate: 0.01\n",
      "Epoch [8269/20000], Loss: -0.0041961669921875, Learning Rate: 0.01\n",
      "Epoch [8270/20000], Loss: 0.0352935791015625, Learning Rate: 0.01\n",
      "Epoch [8271/20000], Loss: 0.03662109375, Learning Rate: 0.01\n",
      "Epoch [8272/20000], Loss: -0.0039825439453125, Learning Rate: 0.01\n",
      "Epoch [8273/20000], Loss: -0.0483856201171875, Learning Rate: 0.01\n",
      "Epoch [8274/20000], Loss: -0.0615692138671875, Learning Rate: 0.01\n",
      "Epoch [8275/20000], Loss: -0.0449981689453125, Learning Rate: 0.01\n",
      "Epoch [8276/20000], Loss: -0.0279998779296875, Learning Rate: 0.01\n",
      "Epoch [8277/20000], Loss: -0.0346832275390625, Learning Rate: 0.01\n",
      "Epoch [8278/20000], Loss: -0.0615692138671875, Learning Rate: 0.01\n",
      "Epoch [8279/20000], Loss: -0.086395263671875, Learning Rate: 0.01\n",
      "Epoch [8280/20000], Loss: -0.09295654296875, Learning Rate: 0.01\n",
      "Epoch [8281/20000], Loss: -0.0854644775390625, Learning Rate: 0.01\n",
      "Epoch [8282/20000], Loss: -0.079864501953125, Learning Rate: 0.01\n",
      "Epoch [8283/20000], Loss: -0.0874481201171875, Learning Rate: 0.01\n",
      "Epoch [8284/20000], Loss: -0.1044921875, Learning Rate: 0.01\n",
      "Epoch [8285/20000], Loss: -0.119659423828125, Learning Rate: 0.01\n",
      "Epoch [8286/20000], Loss: -0.1249847412109375, Learning Rate: 0.01\n",
      "Epoch [8287/20000], Loss: -0.1234588623046875, Learning Rate: 0.01\n",
      "Epoch [8288/20000], Loss: -0.123199462890625, Learning Rate: 0.01\n",
      "Epoch [8289/20000], Loss: -0.1295928955078125, Learning Rate: 0.01\n",
      "Epoch [8290/20000], Loss: -0.141021728515625, Learning Rate: 0.01\n",
      "Epoch [8291/20000], Loss: -0.1517181396484375, Learning Rate: 0.01\n",
      "Epoch [8292/20000], Loss: -0.1574554443359375, Learning Rate: 0.01\n",
      "Epoch [8293/20000], Loss: -0.1593170166015625, Learning Rate: 0.01\n",
      "Epoch [8294/20000], Loss: -0.1613922119140625, Learning Rate: 0.01\n",
      "Epoch [8295/20000], Loss: -0.1666259765625, Learning Rate: 0.01\n",
      "Epoch [8296/20000], Loss: -0.1748504638671875, Learning Rate: 0.01\n",
      "Epoch [8297/20000], Loss: -0.18328857421875, Learning Rate: 0.01\n",
      "Epoch [8298/20000], Loss: -0.1894073486328125, Learning Rate: 0.01\n",
      "Epoch [8299/20000], Loss: -0.1931610107421875, Learning Rate: 0.01\n",
      "Epoch [8300/20000], Loss: -0.196563720703125, Learning Rate: 0.01\n",
      "Epoch [8301/20000], Loss: -0.20135498046875, Learning Rate: 0.01\n",
      "Epoch [8302/20000], Loss: -0.2079315185546875, Learning Rate: 0.01\n",
      "Epoch [8303/20000], Loss: -0.21502685546875, Learning Rate: 0.01\n",
      "Epoch [8304/20000], Loss: -0.221160888671875, Learning Rate: 0.01\n",
      "Epoch [8305/20000], Loss: -0.2257537841796875, Learning Rate: 0.01\n",
      "Epoch [8306/20000], Loss: -0.23016357421875, Learning Rate: 0.01\n",
      "Epoch [8307/20000], Loss: -0.234893798828125, Learning Rate: 0.01\n",
      "Epoch [8308/20000], Loss: -0.2403717041015625, Learning Rate: 0.01\n",
      "Epoch [8309/20000], Loss: -0.246612548828125, Learning Rate: 0.01\n",
      "Epoch [8310/20000], Loss: -0.252716064453125, Learning Rate: 0.01\n",
      "Epoch [8311/20000], Loss: -0.257904052734375, Learning Rate: 0.01\n",
      "Epoch [8312/20000], Loss: -0.2627716064453125, Learning Rate: 0.01\n",
      "Epoch [8313/20000], Loss: -0.26751708984375, Learning Rate: 0.01\n",
      "Epoch [8314/20000], Loss: -0.272674560546875, Learning Rate: 0.01\n",
      "Epoch [8315/20000], Loss: -0.278411865234375, Learning Rate: 0.01\n",
      "Epoch [8316/20000], Loss: -0.284088134765625, Learning Rate: 0.01\n",
      "Epoch [8317/20000], Loss: -0.289703369140625, Learning Rate: 0.01\n",
      "Epoch [8318/20000], Loss: -0.2947235107421875, Learning Rate: 0.01\n",
      "Epoch [8319/20000], Loss: -0.2996673583984375, Learning Rate: 0.01\n",
      "Epoch [8320/20000], Loss: -0.3048553466796875, Learning Rate: 0.01\n",
      "Epoch [8321/20000], Loss: -0.3101959228515625, Learning Rate: 0.01\n",
      "Epoch [8322/20000], Loss: -0.31573486328125, Learning Rate: 0.01\n",
      "Epoch [8323/20000], Loss: -0.321197509765625, Learning Rate: 0.01\n",
      "Epoch [8324/20000], Loss: -0.3265533447265625, Learning Rate: 0.01\n",
      "Epoch [8325/20000], Loss: -0.331634521484375, Learning Rate: 0.01\n",
      "Epoch [8326/20000], Loss: -0.336700439453125, Learning Rate: 0.01\n",
      "Epoch [8327/20000], Loss: -0.342041015625, Learning Rate: 0.01\n",
      "Epoch [8328/20000], Loss: -0.347412109375, Learning Rate: 0.01\n",
      "Epoch [8329/20000], Loss: -0.3528900146484375, Learning Rate: 0.01\n",
      "Epoch [8330/20000], Loss: -0.358184814453125, Learning Rate: 0.01\n",
      "Epoch [8331/20000], Loss: -0.363525390625, Learning Rate: 0.01\n",
      "Epoch [8332/20000], Loss: -0.368804931640625, Learning Rate: 0.01\n",
      "Epoch [8333/20000], Loss: -0.3739776611328125, Learning Rate: 0.01\n",
      "Epoch [8334/20000], Loss: -0.3791961669921875, Learning Rate: 0.01\n",
      "Epoch [8335/20000], Loss: -0.384521484375, Learning Rate: 0.01\n",
      "Epoch [8336/20000], Loss: -0.3899078369140625, Learning Rate: 0.01\n",
      "Epoch [8337/20000], Loss: -0.395172119140625, Learning Rate: 0.01\n",
      "Epoch [8338/20000], Loss: -0.4004364013671875, Learning Rate: 0.01\n",
      "Epoch [8339/20000], Loss: -0.40576171875, Learning Rate: 0.01\n",
      "Epoch [8340/20000], Loss: -0.4109344482421875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8341/20000], Loss: -0.4162139892578125, Learning Rate: 0.01\n",
      "Epoch [8342/20000], Loss: -0.4214935302734375, Learning Rate: 0.01\n",
      "Epoch [8343/20000], Loss: -0.4269256591796875, Learning Rate: 0.01\n",
      "Epoch [8344/20000], Loss: -0.432037353515625, Learning Rate: 0.01\n",
      "Epoch [8345/20000], Loss: -0.437469482421875, Learning Rate: 0.01\n",
      "Epoch [8346/20000], Loss: -0.4427947998046875, Learning Rate: 0.01\n",
      "Epoch [8347/20000], Loss: -0.4479217529296875, Learning Rate: 0.01\n",
      "Epoch [8348/20000], Loss: -0.4533233642578125, Learning Rate: 0.01\n",
      "Epoch [8349/20000], Loss: -0.45867919921875, Learning Rate: 0.01\n",
      "Epoch [8350/20000], Loss: -0.4638214111328125, Learning Rate: 0.01\n",
      "Epoch [8351/20000], Loss: -0.4693145751953125, Learning Rate: 0.01\n",
      "Epoch [8352/20000], Loss: -0.474609375, Learning Rate: 0.01\n",
      "Epoch [8353/20000], Loss: -0.4798431396484375, Learning Rate: 0.01\n",
      "Epoch [8354/20000], Loss: -0.48529052734375, Learning Rate: 0.01\n",
      "Epoch [8355/20000], Loss: -0.490509033203125, Learning Rate: 0.01\n",
      "Epoch [8356/20000], Loss: -0.4957122802734375, Learning Rate: 0.01\n",
      "Epoch [8357/20000], Loss: -0.500946044921875, Learning Rate: 0.01\n",
      "Epoch [8358/20000], Loss: -0.5062713623046875, Learning Rate: 0.01\n",
      "Epoch [8359/20000], Loss: -0.511688232421875, Learning Rate: 0.01\n",
      "Epoch [8360/20000], Loss: -0.5167999267578125, Learning Rate: 0.01\n",
      "Epoch [8361/20000], Loss: -0.5223388671875, Learning Rate: 0.01\n",
      "Epoch [8362/20000], Loss: -0.5275726318359375, Learning Rate: 0.01\n",
      "Epoch [8363/20000], Loss: -0.532867431640625, Learning Rate: 0.01\n",
      "Epoch [8364/20000], Loss: -0.5382080078125, Learning Rate: 0.01\n",
      "Epoch [8365/20000], Loss: -0.5433349609375, Learning Rate: 0.01\n",
      "Epoch [8366/20000], Loss: -0.548828125, Learning Rate: 0.01\n",
      "Epoch [8367/20000], Loss: -0.5540771484375, Learning Rate: 0.01\n",
      "Epoch [8368/20000], Loss: -0.55926513671875, Learning Rate: 0.01\n",
      "Epoch [8369/20000], Loss: -0.5646820068359375, Learning Rate: 0.01\n",
      "Epoch [8370/20000], Loss: -0.57000732421875, Learning Rate: 0.01\n",
      "Epoch [8371/20000], Loss: -0.5753173828125, Learning Rate: 0.01\n",
      "Epoch [8372/20000], Loss: -0.5806732177734375, Learning Rate: 0.01\n",
      "Epoch [8373/20000], Loss: -0.585906982421875, Learning Rate: 0.01\n",
      "Epoch [8374/20000], Loss: -0.5912628173828125, Learning Rate: 0.01\n",
      "Epoch [8375/20000], Loss: -0.596527099609375, Learning Rate: 0.01\n",
      "Epoch [8376/20000], Loss: -0.601806640625, Learning Rate: 0.01\n",
      "Epoch [8377/20000], Loss: -0.60711669921875, Learning Rate: 0.01\n",
      "Epoch [8378/20000], Loss: -0.612396240234375, Learning Rate: 0.01\n",
      "Epoch [8379/20000], Loss: -0.6179046630859375, Learning Rate: 0.01\n",
      "Epoch [8380/20000], Loss: -0.6230926513671875, Learning Rate: 0.01\n",
      "Epoch [8381/20000], Loss: -0.6285247802734375, Learning Rate: 0.01\n",
      "Epoch [8382/20000], Loss: -0.6337890625, Learning Rate: 0.01\n",
      "Epoch [8383/20000], Loss: -0.6390838623046875, Learning Rate: 0.01\n",
      "Epoch [8384/20000], Loss: -0.644287109375, Learning Rate: 0.01\n",
      "Epoch [8385/20000], Loss: -0.64959716796875, Learning Rate: 0.01\n",
      "Epoch [8386/20000], Loss: -0.6550445556640625, Learning Rate: 0.01\n",
      "Epoch [8387/20000], Loss: -0.6603546142578125, Learning Rate: 0.01\n",
      "Epoch [8388/20000], Loss: -0.6656036376953125, Learning Rate: 0.01\n",
      "Epoch [8389/20000], Loss: -0.670867919921875, Learning Rate: 0.01\n",
      "Epoch [8390/20000], Loss: -0.676361083984375, Learning Rate: 0.01\n",
      "Epoch [8391/20000], Loss: -0.681640625, Learning Rate: 0.01\n",
      "Epoch [8392/20000], Loss: -0.686981201171875, Learning Rate: 0.01\n",
      "Epoch [8393/20000], Loss: -0.6920928955078125, Learning Rate: 0.01\n",
      "Epoch [8394/20000], Loss: -0.6976318359375, Learning Rate: 0.01\n",
      "Epoch [8395/20000], Loss: -0.702850341796875, Learning Rate: 0.01\n",
      "Epoch [8396/20000], Loss: -0.708221435546875, Learning Rate: 0.01\n",
      "Epoch [8397/20000], Loss: -0.71356201171875, Learning Rate: 0.01\n",
      "Epoch [8398/20000], Loss: -0.7188873291015625, Learning Rate: 0.01\n",
      "Epoch [8399/20000], Loss: -0.724151611328125, Learning Rate: 0.01\n",
      "Epoch [8400/20000], Loss: -0.7295989990234375, Learning Rate: 0.01\n",
      "Epoch [8401/20000], Loss: -0.7349395751953125, Learning Rate: 0.01\n",
      "Epoch [8402/20000], Loss: -0.7402191162109375, Learning Rate: 0.01\n",
      "Epoch [8403/20000], Loss: -0.7456207275390625, Learning Rate: 0.01\n",
      "Epoch [8404/20000], Loss: -0.750885009765625, Learning Rate: 0.01\n",
      "Epoch [8405/20000], Loss: -0.7560272216796875, Learning Rate: 0.01\n",
      "Epoch [8406/20000], Loss: -0.7615814208984375, Learning Rate: 0.01\n",
      "Epoch [8407/20000], Loss: -0.766937255859375, Learning Rate: 0.01\n",
      "Epoch [8408/20000], Loss: -0.7721405029296875, Learning Rate: 0.01\n",
      "Epoch [8409/20000], Loss: -0.777557373046875, Learning Rate: 0.01\n",
      "Epoch [8410/20000], Loss: -0.78271484375, Learning Rate: 0.01\n",
      "Epoch [8411/20000], Loss: -0.7882232666015625, Learning Rate: 0.01\n",
      "Epoch [8412/20000], Loss: -0.793548583984375, Learning Rate: 0.01\n",
      "Epoch [8413/20000], Loss: -0.79888916015625, Learning Rate: 0.01\n",
      "Epoch [8414/20000], Loss: -0.8042144775390625, Learning Rate: 0.01\n",
      "Epoch [8415/20000], Loss: -0.80950927734375, Learning Rate: 0.01\n",
      "Epoch [8416/20000], Loss: -0.8148193359375, Learning Rate: 0.01\n",
      "Epoch [8417/20000], Loss: -0.8202362060546875, Learning Rate: 0.01\n",
      "Epoch [8418/20000], Loss: -0.825592041015625, Learning Rate: 0.01\n",
      "Epoch [8419/20000], Loss: -0.8308868408203125, Learning Rate: 0.01\n",
      "Epoch [8420/20000], Loss: -0.8361968994140625, Learning Rate: 0.01\n",
      "Epoch [8421/20000], Loss: -0.8415374755859375, Learning Rate: 0.01\n",
      "Epoch [8422/20000], Loss: -0.8469085693359375, Learning Rate: 0.01\n",
      "Epoch [8423/20000], Loss: -0.85223388671875, Learning Rate: 0.01\n",
      "Epoch [8424/20000], Loss: -0.8575592041015625, Learning Rate: 0.01\n",
      "Epoch [8425/20000], Loss: -0.863006591796875, Learning Rate: 0.01\n",
      "Epoch [8426/20000], Loss: -0.8682708740234375, Learning Rate: 0.01\n",
      "Epoch [8427/20000], Loss: -0.873626708984375, Learning Rate: 0.01\n",
      "Epoch [8428/20000], Loss: -0.8790283203125, Learning Rate: 0.01\n",
      "Epoch [8429/20000], Loss: -0.8842620849609375, Learning Rate: 0.01\n",
      "Epoch [8430/20000], Loss: -0.8897247314453125, Learning Rate: 0.01\n",
      "Epoch [8431/20000], Loss: -0.895050048828125, Learning Rate: 0.01\n",
      "Epoch [8432/20000], Loss: -0.900360107421875, Learning Rate: 0.01\n",
      "Epoch [8433/20000], Loss: -0.90576171875, Learning Rate: 0.01\n",
      "Epoch [8434/20000], Loss: -0.910919189453125, Learning Rate: 0.01\n",
      "Epoch [8435/20000], Loss: -0.916534423828125, Learning Rate: 0.01\n",
      "Epoch [8436/20000], Loss: -0.9219207763671875, Learning Rate: 0.01\n",
      "Epoch [8437/20000], Loss: -0.92718505859375, Learning Rate: 0.01\n",
      "Epoch [8438/20000], Loss: -0.932586669921875, Learning Rate: 0.01\n",
      "Epoch [8439/20000], Loss: -0.9379730224609375, Learning Rate: 0.01\n",
      "Epoch [8440/20000], Loss: -0.94329833984375, Learning Rate: 0.01\n",
      "Epoch [8441/20000], Loss: -0.9484100341796875, Learning Rate: 0.01\n",
      "Epoch [8442/20000], Loss: -0.953857421875, Learning Rate: 0.01\n",
      "Epoch [8443/20000], Loss: -0.95941162109375, Learning Rate: 0.01\n",
      "Epoch [8444/20000], Loss: -0.9646148681640625, Learning Rate: 0.01\n",
      "Epoch [8445/20000], Loss: -0.970062255859375, Learning Rate: 0.01\n",
      "Epoch [8446/20000], Loss: -0.9752960205078125, Learning Rate: 0.01\n",
      "Epoch [8447/20000], Loss: -0.9807586669921875, Learning Rate: 0.01\n",
      "Epoch [8448/20000], Loss: -0.9860992431640625, Learning Rate: 0.01\n",
      "Epoch [8449/20000], Loss: -0.9914093017578125, Learning Rate: 0.01\n",
      "Epoch [8450/20000], Loss: -0.99688720703125, Learning Rate: 0.01\n",
      "Epoch [8451/20000], Loss: -1.00238037109375, Learning Rate: 0.01\n",
      "Epoch [8452/20000], Loss: -1.007568359375, Learning Rate: 0.01\n",
      "Epoch [8453/20000], Loss: -1.012939453125, Learning Rate: 0.01\n",
      "Epoch [8454/20000], Loss: -1.018402099609375, Learning Rate: 0.01\n",
      "Epoch [8455/20000], Loss: -1.02374267578125, Learning Rate: 0.01\n",
      "Epoch [8456/20000], Loss: -1.029083251953125, Learning Rate: 0.01\n",
      "Epoch [8457/20000], Loss: -1.03448486328125, Learning Rate: 0.01\n",
      "Epoch [8458/20000], Loss: -1.039825439453125, Learning Rate: 0.01\n",
      "Epoch [8459/20000], Loss: -1.045135498046875, Learning Rate: 0.01\n",
      "Epoch [8460/20000], Loss: -1.050628662109375, Learning Rate: 0.01\n",
      "Epoch [8461/20000], Loss: -1.055908203125, Learning Rate: 0.01\n",
      "Epoch [8462/20000], Loss: -1.06121826171875, Learning Rate: 0.01\n",
      "Epoch [8463/20000], Loss: -1.0665740966796875, Learning Rate: 0.01\n",
      "Epoch [8464/20000], Loss: -1.0721893310546875, Learning Rate: 0.01\n",
      "Epoch [8465/20000], Loss: -1.0775146484375, Learning Rate: 0.01\n",
      "Epoch [8466/20000], Loss: -1.082733154296875, Learning Rate: 0.01\n",
      "Epoch [8467/20000], Loss: -1.0880889892578125, Learning Rate: 0.01\n",
      "Epoch [8468/20000], Loss: -1.0934295654296875, Learning Rate: 0.01\n",
      "Epoch [8469/20000], Loss: -1.0988006591796875, Learning Rate: 0.01\n",
      "Epoch [8470/20000], Loss: -1.1042633056640625, Learning Rate: 0.01\n",
      "Epoch [8471/20000], Loss: -1.109649658203125, Learning Rate: 0.01\n",
      "Epoch [8472/20000], Loss: -1.1150054931640625, Learning Rate: 0.01\n",
      "Epoch [8473/20000], Loss: -1.1204376220703125, Learning Rate: 0.01\n",
      "Epoch [8474/20000], Loss: -1.125732421875, Learning Rate: 0.01\n",
      "Epoch [8475/20000], Loss: -1.1311798095703125, Learning Rate: 0.01\n",
      "Epoch [8476/20000], Loss: -1.1364898681640625, Learning Rate: 0.01\n",
      "Epoch [8477/20000], Loss: -1.1418304443359375, Learning Rate: 0.01\n",
      "Epoch [8478/20000], Loss: -1.14715576171875, Learning Rate: 0.01\n",
      "Epoch [8479/20000], Loss: -1.1526031494140625, Learning Rate: 0.01\n",
      "Epoch [8480/20000], Loss: -1.15814208984375, Learning Rate: 0.01\n",
      "Epoch [8481/20000], Loss: -1.163543701171875, Learning Rate: 0.01\n",
      "Epoch [8482/20000], Loss: -1.1688232421875, Learning Rate: 0.01\n",
      "Epoch [8483/20000], Loss: -1.1743011474609375, Learning Rate: 0.01\n",
      "Epoch [8484/20000], Loss: -1.179718017578125, Learning Rate: 0.01\n",
      "Epoch [8485/20000], Loss: -1.1851043701171875, Learning Rate: 0.01\n",
      "Epoch [8486/20000], Loss: -1.190460205078125, Learning Rate: 0.01\n",
      "Epoch [8487/20000], Loss: -1.19580078125, Learning Rate: 0.01\n",
      "Epoch [8488/20000], Loss: -1.201171875, Learning Rate: 0.01\n",
      "Epoch [8489/20000], Loss: -1.2064971923828125, Learning Rate: 0.01\n",
      "Epoch [8490/20000], Loss: -1.212066650390625, Learning Rate: 0.01\n",
      "Epoch [8491/20000], Loss: -1.217254638671875, Learning Rate: 0.01\n",
      "Epoch [8492/20000], Loss: -1.2228851318359375, Learning Rate: 0.01\n",
      "Epoch [8493/20000], Loss: -1.228057861328125, Learning Rate: 0.01\n",
      "Epoch [8494/20000], Loss: -1.2334136962890625, Learning Rate: 0.01\n",
      "Epoch [8495/20000], Loss: -1.238983154296875, Learning Rate: 0.01\n",
      "Epoch [8496/20000], Loss: -1.244293212890625, Learning Rate: 0.01\n",
      "Epoch [8497/20000], Loss: -1.2496795654296875, Learning Rate: 0.01\n",
      "Epoch [8498/20000], Loss: -1.25494384765625, Learning Rate: 0.01\n",
      "Epoch [8499/20000], Loss: -1.26031494140625, Learning Rate: 0.01\n",
      "Epoch [8500/20000], Loss: -1.2657012939453125, Learning Rate: 0.01\n",
      "Epoch [8501/20000], Loss: -1.2710113525390625, Learning Rate: 0.01\n",
      "Epoch [8502/20000], Loss: -1.2762451171875, Learning Rate: 0.01\n",
      "Epoch [8503/20000], Loss: -1.2814178466796875, Learning Rate: 0.01\n",
      "Epoch [8504/20000], Loss: -1.286468505859375, Learning Rate: 0.01\n",
      "Epoch [8505/20000], Loss: -1.2912750244140625, Learning Rate: 0.01\n",
      "Epoch [8506/20000], Loss: -1.2957763671875, Learning Rate: 0.01\n",
      "Epoch [8507/20000], Loss: -1.3000030517578125, Learning Rate: 0.01\n",
      "Epoch [8508/20000], Loss: -1.3034515380859375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8509/20000], Loss: -1.3060455322265625, Learning Rate: 0.01\n",
      "Epoch [8510/20000], Loss: -1.30682373046875, Learning Rate: 0.01\n",
      "Epoch [8511/20000], Loss: -1.30511474609375, Learning Rate: 0.01\n",
      "Epoch [8512/20000], Loss: -1.2994537353515625, Learning Rate: 0.01\n",
      "Epoch [8513/20000], Loss: -1.2871246337890625, Learning Rate: 0.01\n",
      "Epoch [8514/20000], Loss: -1.2644195556640625, Learning Rate: 0.01\n",
      "Epoch [8515/20000], Loss: -1.2252197265625, Learning Rate: 0.01\n",
      "Epoch [8516/20000], Loss: -1.1592559814453125, Learning Rate: 0.01\n",
      "Epoch [8517/20000], Loss: -1.0501708984375, Learning Rate: 0.01\n",
      "Epoch [8518/20000], Loss: -0.87115478515625, Learning Rate: 0.01\n",
      "Epoch [8519/20000], Loss: -0.5793304443359375, Learning Rate: 0.01\n",
      "Epoch [8520/20000], Loss: -0.1069183349609375, Learning Rate: 0.01\n",
      "Epoch [8521/20000], Loss: 0.6512451171875, Learning Rate: 0.01\n",
      "Epoch [8522/20000], Loss: 1.85003662109375, Learning Rate: 0.01\n",
      "Epoch [8523/20000], Loss: 3.6986541748046875, Learning Rate: 0.01\n",
      "Epoch [8524/20000], Loss: 6.4228973388671875, Learning Rate: 0.01\n",
      "Epoch [8525/20000], Loss: 10.141464233398438, Learning Rate: 0.01\n",
      "Epoch [8526/20000], Loss: 14.557418823242188, Learning Rate: 0.01\n",
      "Epoch [8527/20000], Loss: 18.555496215820312, Learning Rate: 0.01\n",
      "Epoch [8528/20000], Loss: 20.064849853515625, Learning Rate: 0.01\n",
      "Epoch [8529/20000], Loss: 17.193756103515625, Learning Rate: 0.01\n",
      "Epoch [8530/20000], Loss: 10.335693359375, Learning Rate: 0.01\n",
      "Epoch [8531/20000], Loss: 3.0884552001953125, Learning Rate: 0.01\n",
      "Epoch [8532/20000], Loss: -0.44830322265625, Learning Rate: 0.01\n",
      "Epoch [8533/20000], Loss: 0.8311614990234375, Learning Rate: 0.01\n",
      "Epoch [8534/20000], Loss: 4.4404449462890625, Learning Rate: 0.01\n",
      "Epoch [8535/20000], Loss: 6.668304443359375, Learning Rate: 0.01\n",
      "Epoch [8536/20000], Loss: 5.5137939453125, Learning Rate: 0.01\n",
      "Epoch [8537/20000], Loss: 2.1745147705078125, Learning Rate: 0.01\n",
      "Epoch [8538/20000], Loss: -0.3279266357421875, Learning Rate: 0.01\n",
      "Epoch [8539/20000], Loss: -0.242584228515625, Learning Rate: 0.01\n",
      "Epoch [8540/20000], Loss: 1.4725799560546875, Learning Rate: 0.01\n",
      "Epoch [8541/20000], Loss: 2.5357208251953125, Learning Rate: 0.01\n",
      "Epoch [8542/20000], Loss: 1.751922607421875, Learning Rate: 0.01\n",
      "Epoch [8543/20000], Loss: -0.0010833740234375, Learning Rate: 0.01\n",
      "Epoch [8544/20000], Loss: -0.9781036376953125, Learning Rate: 0.01\n",
      "Epoch [8545/20000], Loss: -0.514739990234375, Learning Rate: 0.01\n",
      "Epoch [8546/20000], Loss: 0.4595947265625, Learning Rate: 0.01\n",
      "Epoch [8547/20000], Loss: 0.6626739501953125, Learning Rate: 0.01\n",
      "Epoch [8548/20000], Loss: -0.15716552734375, Learning Rate: 0.01\n",
      "Epoch [8549/20000], Loss: -1.1056365966796875, Learning Rate: 0.01\n",
      "Epoch [8550/20000], Loss: -1.2782745361328125, Learning Rate: 0.01\n",
      "Epoch [8551/20000], Loss: -0.7269439697265625, Learning Rate: 0.01\n",
      "Epoch [8552/20000], Loss: -0.2510833740234375, Learning Rate: 0.01\n",
      "Epoch [8553/20000], Loss: -0.4400787353515625, Learning Rate: 0.01\n",
      "Epoch [8554/20000], Loss: -1.076446533203125, Learning Rate: 0.01\n",
      "Epoch [8555/20000], Loss: -1.495513916015625, Learning Rate: 0.01\n",
      "Epoch [8556/20000], Loss: -1.359649658203125, Learning Rate: 0.01\n",
      "Epoch [8557/20000], Loss: -0.960723876953125, Learning Rate: 0.01\n",
      "Epoch [8558/20000], Loss: -0.8130950927734375, Learning Rate: 0.01\n",
      "Epoch [8559/20000], Loss: -1.079132080078125, Learning Rate: 0.01\n",
      "Epoch [8560/20000], Loss: -1.4627685546875, Learning Rate: 0.01\n",
      "Epoch [8561/20000], Loss: -1.590576171875, Learning Rate: 0.01\n",
      "Epoch [8562/20000], Loss: -1.4141693115234375, Learning Rate: 0.01\n",
      "Epoch [8563/20000], Loss: -1.200897216796875, Learning Rate: 0.01\n",
      "Epoch [8564/20000], Loss: -1.204498291015625, Learning Rate: 0.01\n",
      "Epoch [8565/20000], Loss: -1.411285400390625, Learning Rate: 0.01\n",
      "Epoch [8566/20000], Loss: -1.602203369140625, Learning Rate: 0.01\n",
      "Epoch [8567/20000], Loss: -1.6125946044921875, Learning Rate: 0.01\n",
      "Epoch [8568/20000], Loss: -1.4891357421875, Learning Rate: 0.01\n",
      "Epoch [8569/20000], Loss: -1.4001007080078125, Learning Rate: 0.01\n",
      "Epoch [8570/20000], Loss: -1.4458160400390625, Learning Rate: 0.01\n",
      "Epoch [8571/20000], Loss: -1.5715484619140625, Learning Rate: 0.01\n",
      "Epoch [8572/20000], Loss: -1.6537322998046875, Learning Rate: 0.01\n",
      "Epoch [8573/20000], Loss: -1.6357269287109375, Learning Rate: 0.01\n",
      "Epoch [8574/20000], Loss: -1.569244384765625, Learning Rate: 0.01\n",
      "Epoch [8575/20000], Loss: -1.542572021484375, Learning Rate: 0.01\n",
      "Epoch [8576/20000], Loss: -1.58544921875, Learning Rate: 0.01\n",
      "Epoch [8577/20000], Loss: -1.653228759765625, Learning Rate: 0.01\n",
      "Epoch [8578/20000], Loss: -1.685455322265625, Learning Rate: 0.01\n",
      "Epoch [8579/20000], Loss: -1.6687164306640625, Learning Rate: 0.01\n",
      "Epoch [8580/20000], Loss: -1.639007568359375, Learning Rate: 0.01\n",
      "Epoch [8581/20000], Loss: -1.637176513671875, Learning Rate: 0.01\n",
      "Epoch [8582/20000], Loss: -1.6680145263671875, Learning Rate: 0.01\n",
      "Epoch [8583/20000], Loss: -1.703338623046875, Learning Rate: 0.01\n",
      "Epoch [8584/20000], Loss: -1.71630859375, Learning Rate: 0.01\n",
      "Epoch [8585/20000], Loss: -1.7065277099609375, Learning Rate: 0.01\n",
      "Epoch [8586/20000], Loss: -1.69512939453125, Learning Rate: 0.01\n",
      "Epoch [8587/20000], Loss: -1.7009124755859375, Learning Rate: 0.01\n",
      "Epoch [8588/20000], Loss: -1.721832275390625, Learning Rate: 0.01\n",
      "Epoch [8589/20000], Loss: -1.741912841796875, Learning Rate: 0.01\n",
      "Epoch [8590/20000], Loss: -1.748626708984375, Learning Rate: 0.01\n",
      "Epoch [8591/20000], Loss: -1.74456787109375, Learning Rate: 0.01\n",
      "Epoch [8592/20000], Loss: -1.741241455078125, Learning Rate: 0.01\n",
      "Epoch [8593/20000], Loss: -1.7477874755859375, Learning Rate: 0.01\n",
      "Epoch [8594/20000], Loss: -1.76239013671875, Learning Rate: 0.01\n",
      "Epoch [8595/20000], Loss: -1.7755889892578125, Learning Rate: 0.01\n",
      "Epoch [8596/20000], Loss: -1.7813568115234375, Learning Rate: 0.01\n",
      "Epoch [8597/20000], Loss: -1.781005859375, Learning Rate: 0.01\n",
      "Epoch [8598/20000], Loss: -1.781219482421875, Learning Rate: 0.01\n",
      "Epoch [8599/20000], Loss: -1.7868499755859375, Learning Rate: 0.01\n",
      "Epoch [8600/20000], Loss: -1.7969512939453125, Learning Rate: 0.01\n",
      "Epoch [8601/20000], Loss: -1.806976318359375, Learning Rate: 0.01\n",
      "Epoch [8602/20000], Loss: -1.8131561279296875, Learning Rate: 0.01\n",
      "Epoch [8603/20000], Loss: -1.8155670166015625, Learning Rate: 0.01\n",
      "Epoch [8604/20000], Loss: -1.8175811767578125, Learning Rate: 0.01\n",
      "Epoch [8605/20000], Loss: -1.822265625, Learning Rate: 0.01\n",
      "Epoch [8606/20000], Loss: -1.829681396484375, Learning Rate: 0.01\n",
      "Epoch [8607/20000], Loss: -1.8376007080078125, Learning Rate: 0.01\n",
      "Epoch [8608/20000], Loss: -1.843841552734375, Learning Rate: 0.01\n",
      "Epoch [8609/20000], Loss: -1.8480072021484375, Learning Rate: 0.01\n",
      "Epoch [8610/20000], Loss: -1.851470947265625, Learning Rate: 0.01\n",
      "Epoch [8611/20000], Loss: -1.8559112548828125, Learning Rate: 0.01\n",
      "Epoch [8612/20000], Loss: -1.8616180419921875, Learning Rate: 0.01\n",
      "Epoch [8613/20000], Loss: -1.8681793212890625, Learning Rate: 0.01\n",
      "Epoch [8614/20000], Loss: -1.87432861328125, Learning Rate: 0.01\n",
      "Epoch [8615/20000], Loss: -1.879241943359375, Learning Rate: 0.01\n",
      "Epoch [8616/20000], Loss: -1.8836517333984375, Learning Rate: 0.01\n",
      "Epoch [8617/20000], Loss: -1.8881988525390625, Learning Rate: 0.01\n",
      "Epoch [8618/20000], Loss: -1.8932952880859375, Learning Rate: 0.01\n",
      "Epoch [8619/20000], Loss: -1.899200439453125, Learning Rate: 0.01\n",
      "Epoch [8620/20000], Loss: -1.9048004150390625, Learning Rate: 0.01\n",
      "Epoch [8621/20000], Loss: -1.9100494384765625, Learning Rate: 0.01\n",
      "Epoch [8622/20000], Loss: -1.9149932861328125, Learning Rate: 0.01\n",
      "Epoch [8623/20000], Loss: -1.91986083984375, Learning Rate: 0.01\n",
      "Epoch [8624/20000], Loss: -1.92474365234375, Learning Rate: 0.01\n",
      "Epoch [8625/20000], Loss: -1.9300537109375, Learning Rate: 0.01\n",
      "Epoch [8626/20000], Loss: -1.9354095458984375, Learning Rate: 0.01\n",
      "Epoch [8627/20000], Loss: -1.9406890869140625, Learning Rate: 0.01\n",
      "Epoch [8628/20000], Loss: -1.9459686279296875, Learning Rate: 0.01\n",
      "Epoch [8629/20000], Loss: -1.950836181640625, Learning Rate: 0.01\n",
      "Epoch [8630/20000], Loss: -1.955841064453125, Learning Rate: 0.01\n",
      "Epoch [8631/20000], Loss: -1.961029052734375, Learning Rate: 0.01\n",
      "Epoch [8632/20000], Loss: -1.966064453125, Learning Rate: 0.01\n",
      "Epoch [8633/20000], Loss: -1.97161865234375, Learning Rate: 0.01\n",
      "Epoch [8634/20000], Loss: -1.9767303466796875, Learning Rate: 0.01\n",
      "Epoch [8635/20000], Loss: -1.9818115234375, Learning Rate: 0.01\n",
      "Epoch [8636/20000], Loss: -1.9868927001953125, Learning Rate: 0.01\n",
      "Epoch [8637/20000], Loss: -1.991851806640625, Learning Rate: 0.01\n",
      "Epoch [8638/20000], Loss: -1.9968414306640625, Learning Rate: 0.01\n",
      "Epoch [8639/20000], Loss: -2.00213623046875, Learning Rate: 0.01\n",
      "Epoch [8640/20000], Loss: -2.0073089599609375, Learning Rate: 0.01\n",
      "Epoch [8641/20000], Loss: -2.01263427734375, Learning Rate: 0.01\n",
      "Epoch [8642/20000], Loss: -2.0176544189453125, Learning Rate: 0.01\n",
      "Epoch [8643/20000], Loss: -2.022735595703125, Learning Rate: 0.01\n",
      "Epoch [8644/20000], Loss: -2.0278778076171875, Learning Rate: 0.01\n",
      "Epoch [8645/20000], Loss: -2.0329132080078125, Learning Rate: 0.01\n",
      "Epoch [8646/20000], Loss: -2.0379791259765625, Learning Rate: 0.01\n",
      "Epoch [8647/20000], Loss: -2.0433197021484375, Learning Rate: 0.01\n",
      "Epoch [8648/20000], Loss: -2.0484161376953125, Learning Rate: 0.01\n",
      "Epoch [8649/20000], Loss: -2.0535736083984375, Learning Rate: 0.01\n",
      "Epoch [8650/20000], Loss: -2.0587005615234375, Learning Rate: 0.01\n",
      "Epoch [8651/20000], Loss: -2.063690185546875, Learning Rate: 0.01\n",
      "Epoch [8652/20000], Loss: -2.069091796875, Learning Rate: 0.01\n",
      "Epoch [8653/20000], Loss: -2.074005126953125, Learning Rate: 0.01\n",
      "Epoch [8654/20000], Loss: -2.0791778564453125, Learning Rate: 0.01\n",
      "Epoch [8655/20000], Loss: -2.084320068359375, Learning Rate: 0.01\n",
      "Epoch [8656/20000], Loss: -2.089447021484375, Learning Rate: 0.01\n",
      "Epoch [8657/20000], Loss: -2.0946044921875, Learning Rate: 0.01\n",
      "Epoch [8658/20000], Loss: -2.099853515625, Learning Rate: 0.01\n",
      "Epoch [8659/20000], Loss: -2.10498046875, Learning Rate: 0.01\n",
      "Epoch [8660/20000], Loss: -2.11004638671875, Learning Rate: 0.01\n",
      "Epoch [8661/20000], Loss: -2.115081787109375, Learning Rate: 0.01\n",
      "Epoch [8662/20000], Loss: -2.120330810546875, Learning Rate: 0.01\n",
      "Epoch [8663/20000], Loss: -2.125457763671875, Learning Rate: 0.01\n",
      "Epoch [8664/20000], Loss: -2.1305389404296875, Learning Rate: 0.01\n",
      "Epoch [8665/20000], Loss: -2.1357879638671875, Learning Rate: 0.01\n",
      "Epoch [8666/20000], Loss: -2.140869140625, Learning Rate: 0.01\n",
      "Epoch [8667/20000], Loss: -2.14593505859375, Learning Rate: 0.01\n",
      "Epoch [8668/20000], Loss: -2.15118408203125, Learning Rate: 0.01\n",
      "Epoch [8669/20000], Loss: -2.1562652587890625, Learning Rate: 0.01\n",
      "Epoch [8670/20000], Loss: -2.1614990234375, Learning Rate: 0.01\n",
      "Epoch [8671/20000], Loss: -2.1666107177734375, Learning Rate: 0.01\n",
      "Epoch [8672/20000], Loss: -2.1717071533203125, Learning Rate: 0.01\n",
      "Epoch [8673/20000], Loss: -2.1769866943359375, Learning Rate: 0.01\n",
      "Epoch [8674/20000], Loss: -2.1820526123046875, Learning Rate: 0.01\n",
      "Epoch [8675/20000], Loss: -2.1870880126953125, Learning Rate: 0.01\n",
      "Epoch [8676/20000], Loss: -2.1923065185546875, Learning Rate: 0.01\n",
      "Epoch [8677/20000], Loss: -2.1975555419921875, Learning Rate: 0.01\n",
      "Epoch [8678/20000], Loss: -2.2027740478515625, Learning Rate: 0.01\n",
      "Epoch [8679/20000], Loss: -2.207977294921875, Learning Rate: 0.01\n",
      "Epoch [8680/20000], Loss: -2.2129364013671875, Learning Rate: 0.01\n",
      "Epoch [8681/20000], Loss: -2.2181243896484375, Learning Rate: 0.01\n",
      "Epoch [8682/20000], Loss: -2.223388671875, Learning Rate: 0.01\n",
      "Epoch [8683/20000], Loss: -2.228424072265625, Learning Rate: 0.01\n",
      "Epoch [8684/20000], Loss: -2.2334747314453125, Learning Rate: 0.01\n",
      "Epoch [8685/20000], Loss: -2.2388763427734375, Learning Rate: 0.01\n",
      "Epoch [8686/20000], Loss: -2.243896484375, Learning Rate: 0.01\n",
      "Epoch [8687/20000], Loss: -2.24908447265625, Learning Rate: 0.01\n",
      "Epoch [8688/20000], Loss: -2.254180908203125, Learning Rate: 0.01\n",
      "Epoch [8689/20000], Loss: -2.259429931640625, Learning Rate: 0.01\n",
      "Epoch [8690/20000], Loss: -2.264434814453125, Learning Rate: 0.01\n",
      "Epoch [8691/20000], Loss: -2.2697601318359375, Learning Rate: 0.01\n",
      "Epoch [8692/20000], Loss: -2.27496337890625, Learning Rate: 0.01\n",
      "Epoch [8693/20000], Loss: -2.280029296875, Learning Rate: 0.01\n",
      "Epoch [8694/20000], Loss: -2.2851409912109375, Learning Rate: 0.01\n",
      "Epoch [8695/20000], Loss: -2.290435791015625, Learning Rate: 0.01\n",
      "Epoch [8696/20000], Loss: -2.295562744140625, Learning Rate: 0.01\n",
      "Epoch [8697/20000], Loss: -2.300811767578125, Learning Rate: 0.01\n",
      "Epoch [8698/20000], Loss: -2.305938720703125, Learning Rate: 0.01\n",
      "Epoch [8699/20000], Loss: -2.3109283447265625, Learning Rate: 0.01\n",
      "Epoch [8700/20000], Loss: -2.3162689208984375, Learning Rate: 0.01\n",
      "Epoch [8701/20000], Loss: -2.3213958740234375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8702/20000], Loss: -2.326568603515625, Learning Rate: 0.01\n",
      "Epoch [8703/20000], Loss: -2.3317718505859375, Learning Rate: 0.01\n",
      "Epoch [8704/20000], Loss: -2.33697509765625, Learning Rate: 0.01\n",
      "Epoch [8705/20000], Loss: -2.3420562744140625, Learning Rate: 0.01\n",
      "Epoch [8706/20000], Loss: -2.34722900390625, Learning Rate: 0.01\n",
      "Epoch [8707/20000], Loss: -2.352325439453125, Learning Rate: 0.01\n",
      "Epoch [8708/20000], Loss: -2.357696533203125, Learning Rate: 0.01\n",
      "Epoch [8709/20000], Loss: -2.36285400390625, Learning Rate: 0.01\n",
      "Epoch [8710/20000], Loss: -2.3680267333984375, Learning Rate: 0.01\n",
      "Epoch [8711/20000], Loss: -2.3732452392578125, Learning Rate: 0.01\n",
      "Epoch [8712/20000], Loss: -2.3783111572265625, Learning Rate: 0.01\n",
      "Epoch [8713/20000], Loss: -2.3834381103515625, Learning Rate: 0.01\n",
      "Epoch [8714/20000], Loss: -2.388641357421875, Learning Rate: 0.01\n",
      "Epoch [8715/20000], Loss: -2.3938140869140625, Learning Rate: 0.01\n",
      "Epoch [8716/20000], Loss: -2.399169921875, Learning Rate: 0.01\n",
      "Epoch [8717/20000], Loss: -2.4042510986328125, Learning Rate: 0.01\n",
      "Epoch [8718/20000], Loss: -2.4094085693359375, Learning Rate: 0.01\n",
      "Epoch [8719/20000], Loss: -2.414581298828125, Learning Rate: 0.01\n",
      "Epoch [8720/20000], Loss: -2.4198455810546875, Learning Rate: 0.01\n",
      "Epoch [8721/20000], Loss: -2.4248809814453125, Learning Rate: 0.01\n",
      "Epoch [8722/20000], Loss: -2.430084228515625, Learning Rate: 0.01\n",
      "Epoch [8723/20000], Loss: -2.4354248046875, Learning Rate: 0.01\n",
      "Epoch [8724/20000], Loss: -2.4404754638671875, Learning Rate: 0.01\n",
      "Epoch [8725/20000], Loss: -2.4457550048828125, Learning Rate: 0.01\n",
      "Epoch [8726/20000], Loss: -2.45098876953125, Learning Rate: 0.01\n",
      "Epoch [8727/20000], Loss: -2.4560699462890625, Learning Rate: 0.01\n",
      "Epoch [8728/20000], Loss: -2.461334228515625, Learning Rate: 0.01\n",
      "Epoch [8729/20000], Loss: -2.46661376953125, Learning Rate: 0.01\n",
      "Epoch [8730/20000], Loss: -2.4716796875, Learning Rate: 0.01\n",
      "Epoch [8731/20000], Loss: -2.476837158203125, Learning Rate: 0.01\n",
      "Epoch [8732/20000], Loss: -2.4820709228515625, Learning Rate: 0.01\n",
      "Epoch [8733/20000], Loss: -2.4872283935546875, Learning Rate: 0.01\n",
      "Epoch [8734/20000], Loss: -2.492462158203125, Learning Rate: 0.01\n",
      "Epoch [8735/20000], Loss: -2.4976043701171875, Learning Rate: 0.01\n",
      "Epoch [8736/20000], Loss: -2.5029144287109375, Learning Rate: 0.01\n",
      "Epoch [8737/20000], Loss: -2.5081939697265625, Learning Rate: 0.01\n",
      "Epoch [8738/20000], Loss: -2.5133514404296875, Learning Rate: 0.01\n",
      "Epoch [8739/20000], Loss: -2.5185394287109375, Learning Rate: 0.01\n",
      "Epoch [8740/20000], Loss: -2.52374267578125, Learning Rate: 0.01\n",
      "Epoch [8741/20000], Loss: -2.5289459228515625, Learning Rate: 0.01\n",
      "Epoch [8742/20000], Loss: -2.5340576171875, Learning Rate: 0.01\n",
      "Epoch [8743/20000], Loss: -2.53924560546875, Learning Rate: 0.01\n",
      "Epoch [8744/20000], Loss: -2.54449462890625, Learning Rate: 0.01\n",
      "Epoch [8745/20000], Loss: -2.54962158203125, Learning Rate: 0.01\n",
      "Epoch [8746/20000], Loss: -2.55499267578125, Learning Rate: 0.01\n",
      "Epoch [8747/20000], Loss: -2.5601043701171875, Learning Rate: 0.01\n",
      "Epoch [8748/20000], Loss: -2.56524658203125, Learning Rate: 0.01\n",
      "Epoch [8749/20000], Loss: -2.5705108642578125, Learning Rate: 0.01\n",
      "Epoch [8750/20000], Loss: -2.575653076171875, Learning Rate: 0.01\n",
      "Epoch [8751/20000], Loss: -2.5809326171875, Learning Rate: 0.01\n",
      "Epoch [8752/20000], Loss: -2.5861358642578125, Learning Rate: 0.01\n",
      "Epoch [8753/20000], Loss: -2.5911712646484375, Learning Rate: 0.01\n",
      "Epoch [8754/20000], Loss: -2.5964508056640625, Learning Rate: 0.01\n",
      "Epoch [8755/20000], Loss: -2.6015625, Learning Rate: 0.01\n",
      "Epoch [8756/20000], Loss: -2.60675048828125, Learning Rate: 0.01\n",
      "Epoch [8757/20000], Loss: -2.61175537109375, Learning Rate: 0.01\n",
      "Epoch [8758/20000], Loss: -2.6168975830078125, Learning Rate: 0.01\n",
      "Epoch [8759/20000], Loss: -2.621734619140625, Learning Rate: 0.01\n",
      "Epoch [8760/20000], Loss: -2.62640380859375, Learning Rate: 0.01\n",
      "Epoch [8761/20000], Loss: -2.630859375, Learning Rate: 0.01\n",
      "Epoch [8762/20000], Loss: -2.6349639892578125, Learning Rate: 0.01\n",
      "Epoch [8763/20000], Loss: -2.63812255859375, Learning Rate: 0.01\n",
      "Epoch [8764/20000], Loss: -2.64056396484375, Learning Rate: 0.01\n",
      "Epoch [8765/20000], Loss: -2.6409149169921875, Learning Rate: 0.01\n",
      "Epoch [8766/20000], Loss: -2.6386260986328125, Learning Rate: 0.01\n",
      "Epoch [8767/20000], Loss: -2.6314239501953125, Learning Rate: 0.01\n",
      "Epoch [8768/20000], Loss: -2.616973876953125, Learning Rate: 0.01\n",
      "Epoch [8769/20000], Loss: -2.58990478515625, Learning Rate: 0.01\n",
      "Epoch [8770/20000], Loss: -2.5430450439453125, Learning Rate: 0.01\n",
      "Epoch [8771/20000], Loss: -2.4631195068359375, Learning Rate: 0.01\n",
      "Epoch [8772/20000], Loss: -2.3308563232421875, Learning Rate: 0.01\n",
      "Epoch [8773/20000], Loss: -2.11358642578125, Learning Rate: 0.01\n",
      "Epoch [8774/20000], Loss: -1.7657012939453125, Learning Rate: 0.01\n",
      "Epoch [8775/20000], Loss: -1.2223663330078125, Learning Rate: 0.01\n",
      "Epoch [8776/20000], Loss: -0.4187164306640625, Learning Rate: 0.01\n",
      "Epoch [8777/20000], Loss: 0.68914794921875, Learning Rate: 0.01\n",
      "Epoch [8778/20000], Loss: 2.001922607421875, Learning Rate: 0.01\n",
      "Epoch [8779/20000], Loss: 3.2326507568359375, Learning Rate: 0.01\n",
      "Epoch [8780/20000], Loss: 3.7978515625, Learning Rate: 0.01\n",
      "Epoch [8781/20000], Loss: 3.277313232421875, Learning Rate: 0.01\n",
      "Epoch [8782/20000], Loss: 1.7830963134765625, Learning Rate: 0.01\n",
      "Epoch [8783/20000], Loss: 0.2683258056640625, Learning Rate: 0.01\n",
      "Epoch [8784/20000], Loss: -0.2665557861328125, Learning Rate: 0.01\n",
      "Epoch [8785/20000], Loss: 0.410186767578125, Learning Rate: 0.01\n",
      "Epoch [8786/20000], Loss: 1.5816497802734375, Learning Rate: 0.01\n",
      "Epoch [8787/20000], Loss: 2.1397705078125, Learning Rate: 0.01\n",
      "Epoch [8788/20000], Loss: 1.4691009521484375, Learning Rate: 0.01\n",
      "Epoch [8789/20000], Loss: -0.1056060791015625, Learning Rate: 0.01\n",
      "Epoch [8790/20000], Loss: -1.5242156982421875, Learning Rate: 0.01\n",
      "Epoch [8791/20000], Loss: -2.02581787109375, Learning Rate: 0.01\n",
      "Epoch [8792/20000], Loss: -1.7464599609375, Learning Rate: 0.01\n",
      "Epoch [8793/20000], Loss: -1.4189910888671875, Learning Rate: 0.01\n",
      "Epoch [8794/20000], Loss: -1.6006927490234375, Learning Rate: 0.01\n",
      "Epoch [8795/20000], Loss: -2.171112060546875, Learning Rate: 0.01\n",
      "Epoch [8796/20000], Loss: -2.5725860595703125, Learning Rate: 0.01\n",
      "Epoch [8797/20000], Loss: -2.4354400634765625, Learning Rate: 0.01\n",
      "Epoch [8798/20000], Loss: -1.94158935546875, Learning Rate: 0.01\n",
      "Epoch [8799/20000], Loss: -1.587249755859375, Learning Rate: 0.01\n",
      "Epoch [8800/20000], Loss: -1.6795806884765625, Learning Rate: 0.01\n",
      "Epoch [8801/20000], Loss: -2.0897979736328125, Learning Rate: 0.01\n",
      "Epoch [8802/20000], Loss: -2.44097900390625, Learning Rate: 0.01\n",
      "Epoch [8803/20000], Loss: -2.5126953125, Learning Rate: 0.01\n",
      "Epoch [8804/20000], Loss: -2.4106903076171875, Learning Rate: 0.01\n",
      "Epoch [8805/20000], Loss: -2.3839874267578125, Learning Rate: 0.01\n",
      "Epoch [8806/20000], Loss: -2.5437774658203125, Learning Rate: 0.01\n",
      "Epoch [8807/20000], Loss: -2.7692718505859375, Learning Rate: 0.01\n",
      "Epoch [8808/20000], Loss: -2.8693084716796875, Learning Rate: 0.01\n",
      "Epoch [8809/20000], Loss: -2.7855377197265625, Learning Rate: 0.01\n",
      "Epoch [8810/20000], Loss: -2.633026123046875, Learning Rate: 0.01\n",
      "Epoch [8811/20000], Loss: -2.5630645751953125, Learning Rate: 0.01\n",
      "Epoch [8812/20000], Loss: -2.6190338134765625, Learning Rate: 0.01\n",
      "Epoch [8813/20000], Loss: -2.7210540771484375, Learning Rate: 0.01\n",
      "Epoch [8814/20000], Loss: -2.7701568603515625, Learning Rate: 0.01\n",
      "Epoch [8815/20000], Loss: -2.74700927734375, Learning Rate: 0.01\n",
      "Epoch [8816/20000], Loss: -2.7147369384765625, Learning Rate: 0.01\n",
      "Epoch [8817/20000], Loss: -2.7378387451171875, Learning Rate: 0.01\n",
      "Epoch [8818/20000], Loss: -2.817413330078125, Learning Rate: 0.01\n",
      "Epoch [8819/20000], Loss: -2.898529052734375, Learning Rate: 0.01\n",
      "Epoch [8820/20000], Loss: -2.9309844970703125, Learning Rate: 0.01\n",
      "Epoch [8821/20000], Loss: -2.91455078125, Learning Rate: 0.01\n",
      "Epoch [8822/20000], Loss: -2.8890380859375, Learning Rate: 0.01\n",
      "Epoch [8823/20000], Loss: -2.8895416259765625, Learning Rate: 0.01\n",
      "Epoch [8824/20000], Loss: -2.9162750244140625, Learning Rate: 0.01\n",
      "Epoch [8825/20000], Loss: -2.9420318603515625, Learning Rate: 0.01\n",
      "Epoch [8826/20000], Loss: -2.9448089599609375, Learning Rate: 0.01\n",
      "Epoch [8827/20000], Loss: -2.927947998046875, Learning Rate: 0.01\n",
      "Epoch [8828/20000], Loss: -2.9130401611328125, Learning Rate: 0.01\n",
      "Epoch [8829/20000], Loss: -2.917022705078125, Learning Rate: 0.01\n",
      "Epoch [8830/20000], Loss: -2.9382476806640625, Learning Rate: 0.01\n",
      "Epoch [8831/20000], Loss: -2.9610595703125, Learning Rate: 0.01\n",
      "Epoch [8832/20000], Loss: -2.972747802734375, Learning Rate: 0.01\n",
      "Epoch [8833/20000], Loss: -2.9737396240234375, Learning Rate: 0.01\n",
      "Epoch [8834/20000], Loss: -2.9746551513671875, Learning Rate: 0.01\n",
      "Epoch [8835/20000], Loss: -2.9836273193359375, Learning Rate: 0.01\n",
      "Epoch [8836/20000], Loss: -3.0006103515625, Learning Rate: 0.01\n",
      "Epoch [8837/20000], Loss: -3.0175018310546875, Learning Rate: 0.01\n",
      "Epoch [8838/20000], Loss: -3.0273895263671875, Learning Rate: 0.01\n",
      "Epoch [8839/20000], Loss: -3.030426025390625, Learning Rate: 0.01\n",
      "Epoch [8840/20000], Loss: -3.031890869140625, Learning Rate: 0.01\n",
      "Epoch [8841/20000], Loss: -3.0364990234375, Learning Rate: 0.01\n",
      "Epoch [8842/20000], Loss: -3.045379638671875, Learning Rate: 0.01\n",
      "Epoch [8843/20000], Loss: -3.0550079345703125, Learning Rate: 0.01\n",
      "Epoch [8844/20000], Loss: -3.06182861328125, Learning Rate: 0.01\n",
      "Epoch [8845/20000], Loss: -3.0650787353515625, Learning Rate: 0.01\n",
      "Epoch [8846/20000], Loss: -3.066436767578125, Learning Rate: 0.01\n",
      "Epoch [8847/20000], Loss: -3.06964111328125, Learning Rate: 0.01\n",
      "Epoch [8848/20000], Loss: -3.0753173828125, Learning Rate: 0.01\n",
      "Epoch [8849/20000], Loss: -3.082244873046875, Learning Rate: 0.01\n",
      "Epoch [8850/20000], Loss: -3.0882110595703125, Learning Rate: 0.01\n",
      "Epoch [8851/20000], Loss: -3.09234619140625, Learning Rate: 0.01\n",
      "Epoch [8852/20000], Loss: -3.095367431640625, Learning Rate: 0.01\n",
      "Epoch [8853/20000], Loss: -3.0988311767578125, Learning Rate: 0.01\n",
      "Epoch [8854/20000], Loss: -3.103302001953125, Learning Rate: 0.01\n",
      "Epoch [8855/20000], Loss: -3.1088409423828125, Learning Rate: 0.01\n",
      "Epoch [8856/20000], Loss: -3.11407470703125, Learning Rate: 0.01\n",
      "Epoch [8857/20000], Loss: -3.118560791015625, Learning Rate: 0.01\n",
      "Epoch [8858/20000], Loss: -3.122039794921875, Learning Rate: 0.01\n",
      "Epoch [8859/20000], Loss: -3.124969482421875, Learning Rate: 0.01\n",
      "Epoch [8860/20000], Loss: -3.1283416748046875, Learning Rate: 0.01\n",
      "Epoch [8861/20000], Loss: -3.13177490234375, Learning Rate: 0.01\n",
      "Epoch [8862/20000], Loss: -3.1350250244140625, Learning Rate: 0.01\n",
      "Epoch [8863/20000], Loss: -3.1375274658203125, Learning Rate: 0.01\n",
      "Epoch [8864/20000], Loss: -3.138916015625, Learning Rate: 0.01\n",
      "Epoch [8865/20000], Loss: -3.1389007568359375, Learning Rate: 0.01\n",
      "Epoch [8866/20000], Loss: -3.1377716064453125, Learning Rate: 0.01\n",
      "Epoch [8867/20000], Loss: -3.135284423828125, Learning Rate: 0.01\n",
      "Epoch [8868/20000], Loss: -3.1309356689453125, Learning Rate: 0.01\n",
      "Epoch [8869/20000], Loss: -3.1237945556640625, Learning Rate: 0.01\n",
      "Epoch [8870/20000], Loss: -3.1125946044921875, Learning Rate: 0.01\n",
      "Epoch [8871/20000], Loss: -3.0963592529296875, Learning Rate: 0.01\n",
      "Epoch [8872/20000], Loss: -3.0731353759765625, Learning Rate: 0.01\n",
      "Epoch [8873/20000], Loss: -3.0408782958984375, Learning Rate: 0.01\n",
      "Epoch [8874/20000], Loss: -2.996551513671875, Learning Rate: 0.01\n",
      "Epoch [8875/20000], Loss: -2.9358673095703125, Learning Rate: 0.01\n",
      "Epoch [8876/20000], Loss: -2.8531494140625, Learning Rate: 0.01\n",
      "Epoch [8877/20000], Loss: -2.74066162109375, Learning Rate: 0.01\n",
      "Epoch [8878/20000], Loss: -2.587921142578125, Learning Rate: 0.01\n",
      "Epoch [8879/20000], Loss: -2.38287353515625, Learning Rate: 0.01\n",
      "Epoch [8880/20000], Loss: -2.10888671875, Learning Rate: 0.01\n",
      "Epoch [8881/20000], Loss: -1.747528076171875, Learning Rate: 0.01\n",
      "Epoch [8882/20000], Loss: -1.2791748046875, Learning Rate: 0.01\n",
      "Epoch [8883/20000], Loss: -0.6871185302734375, Learning Rate: 0.01\n",
      "Epoch [8884/20000], Loss: 0.0333099365234375, Learning Rate: 0.01\n",
      "Epoch [8885/20000], Loss: 0.86181640625, Learning Rate: 0.01\n",
      "Epoch [8886/20000], Loss: 1.7340850830078125, Learning Rate: 0.01\n",
      "Epoch [8887/20000], Loss: 2.5272979736328125, Learning Rate: 0.01\n",
      "Epoch [8888/20000], Loss: 3.057281494140625, Learning Rate: 0.01\n",
      "Epoch [8889/20000], Loss: 3.1185302734375, Learning Rate: 0.01\n",
      "Epoch [8890/20000], Loss: 2.561309814453125, Learning Rate: 0.01\n",
      "Epoch [8891/20000], Loss: 1.4010772705078125, Learning Rate: 0.01\n",
      "Epoch [8892/20000], Loss: -0.1343994140625, Learning Rate: 0.01\n",
      "Epoch [8893/20000], Loss: -1.6574859619140625, Learning Rate: 0.01\n",
      "Epoch [8894/20000], Loss: -2.7825775146484375, Learning Rate: 0.01\n",
      "Epoch [8895/20000], Loss: -3.2877655029296875, Learning Rate: 0.01\n",
      "Epoch [8896/20000], Loss: -3.1847381591796875, Learning Rate: 0.01\n",
      "Epoch [8897/20000], Loss: -2.6758270263671875, Learning Rate: 0.01\n",
      "Epoch [8898/20000], Loss: -2.052398681640625, Learning Rate: 0.01\n",
      "Epoch [8899/20000], Loss: -1.5879058837890625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8900/20000], Loss: -1.455291748046875, Learning Rate: 0.01\n",
      "Epoch [8901/20000], Loss: -1.6863861083984375, Learning Rate: 0.01\n",
      "Epoch [8902/20000], Loss: -2.1740264892578125, Learning Rate: 0.01\n",
      "Epoch [8903/20000], Loss: -2.7297210693359375, Learning Rate: 0.01\n",
      "Epoch [8904/20000], Loss: -3.1642913818359375, Learning Rate: 0.01\n",
      "Epoch [8905/20000], Loss: -3.3654937744140625, Learning Rate: 0.01\n",
      "Epoch [8906/20000], Loss: -3.326446533203125, Learning Rate: 0.01\n",
      "Epoch [8907/20000], Loss: -3.12939453125, Learning Rate: 0.01\n",
      "Epoch [8908/20000], Loss: -2.8966217041015625, Learning Rate: 0.01\n",
      "Epoch [8909/20000], Loss: -2.73809814453125, Learning Rate: 0.01\n",
      "Epoch [8910/20000], Loss: -2.7138824462890625, Learning Rate: 0.01\n",
      "Epoch [8911/20000], Loss: -2.8227386474609375, Learning Rate: 0.01\n",
      "Epoch [8912/20000], Loss: -3.013671875, Learning Rate: 0.01\n",
      "Epoch [8913/20000], Loss: -3.2144775390625, Learning Rate: 0.01\n",
      "Epoch [8914/20000], Loss: -3.3618621826171875, Learning Rate: 0.01\n",
      "Epoch [8915/20000], Loss: -3.423980712890625, Learning Rate: 0.01\n",
      "Epoch [8916/20000], Loss: -3.4042510986328125, Learning Rate: 0.01\n",
      "Epoch [8917/20000], Loss: -3.33404541015625, Learning Rate: 0.01\n",
      "Epoch [8918/20000], Loss: -3.2551422119140625, Learning Rate: 0.01\n",
      "Epoch [8919/20000], Loss: -3.2032928466796875, Learning Rate: 0.01\n",
      "Epoch [8920/20000], Loss: -3.1977386474609375, Learning Rate: 0.01\n",
      "Epoch [8921/20000], Loss: -3.2374267578125, Learning Rate: 0.01\n",
      "Epoch [8922/20000], Loss: -3.306365966796875, Learning Rate: 0.01\n",
      "Epoch [8923/20000], Loss: -3.38092041015625, Learning Rate: 0.01\n",
      "Epoch [8924/20000], Loss: -3.4407501220703125, Learning Rate: 0.01\n",
      "Epoch [8925/20000], Loss: -3.4732208251953125, Learning Rate: 0.01\n",
      "Epoch [8926/20000], Loss: -3.4775848388671875, Learning Rate: 0.01\n",
      "Epoch [8927/20000], Loss: -3.461578369140625, Learning Rate: 0.01\n",
      "Epoch [8928/20000], Loss: -3.4377593994140625, Learning Rate: 0.01\n",
      "Epoch [8929/20000], Loss: -3.4181671142578125, Learning Rate: 0.01\n",
      "Epoch [8930/20000], Loss: -3.4111328125, Learning Rate: 0.01\n",
      "Epoch [8931/20000], Loss: -3.4191131591796875, Learning Rate: 0.01\n",
      "Epoch [8932/20000], Loss: -3.439453125, Learning Rate: 0.01\n",
      "Epoch [8933/20000], Loss: -3.4663543701171875, Learning Rate: 0.01\n",
      "Epoch [8934/20000], Loss: -3.4936370849609375, Learning Rate: 0.01\n",
      "Epoch [8935/20000], Loss: -3.515533447265625, Learning Rate: 0.01\n",
      "Epoch [8936/20000], Loss: -3.529632568359375, Learning Rate: 0.01\n",
      "Epoch [8937/20000], Loss: -3.5352935791015625, Learning Rate: 0.01\n",
      "Epoch [8938/20000], Loss: -3.534912109375, Learning Rate: 0.01\n",
      "Epoch [8939/20000], Loss: -3.5316162109375, Learning Rate: 0.01\n",
      "Epoch [8940/20000], Loss: -3.5285186767578125, Learning Rate: 0.01\n",
      "Epoch [8941/20000], Loss: -3.527740478515625, Learning Rate: 0.01\n",
      "Epoch [8942/20000], Loss: -3.531219482421875, Learning Rate: 0.01\n",
      "Epoch [8943/20000], Loss: -3.5383453369140625, Learning Rate: 0.01\n",
      "Epoch [8944/20000], Loss: -3.54840087890625, Learning Rate: 0.01\n",
      "Epoch [8945/20000], Loss: -3.559844970703125, Learning Rate: 0.01\n",
      "Epoch [8946/20000], Loss: -3.5713043212890625, Learning Rate: 0.01\n",
      "Epoch [8947/20000], Loss: -3.5813446044921875, Learning Rate: 0.01\n",
      "Epoch [8948/20000], Loss: -3.5896148681640625, Learning Rate: 0.01\n",
      "Epoch [8949/20000], Loss: -3.5957489013671875, Learning Rate: 0.01\n",
      "Epoch [8950/20000], Loss: -3.6001739501953125, Learning Rate: 0.01\n",
      "Epoch [8951/20000], Loss: -3.60333251953125, Learning Rate: 0.01\n",
      "Epoch [8952/20000], Loss: -3.6060791015625, Learning Rate: 0.01\n",
      "Epoch [8953/20000], Loss: -3.609130859375, Learning Rate: 0.01\n",
      "Epoch [8954/20000], Loss: -3.6126251220703125, Learning Rate: 0.01\n",
      "Epoch [8955/20000], Loss: -3.61700439453125, Learning Rate: 0.01\n",
      "Epoch [8956/20000], Loss: -3.622406005859375, Learning Rate: 0.01\n",
      "Epoch [8957/20000], Loss: -3.6280517578125, Learning Rate: 0.01\n",
      "Epoch [8958/20000], Loss: -3.6344451904296875, Learning Rate: 0.01\n",
      "Epoch [8959/20000], Loss: -3.6410980224609375, Learning Rate: 0.01\n",
      "Epoch [8960/20000], Loss: -3.647613525390625, Learning Rate: 0.01\n",
      "Epoch [8961/20000], Loss: -3.6538238525390625, Learning Rate: 0.01\n",
      "Epoch [8962/20000], Loss: -3.659698486328125, Learning Rate: 0.01\n",
      "Epoch [8963/20000], Loss: -3.6652984619140625, Learning Rate: 0.01\n",
      "Epoch [8964/20000], Loss: -3.670623779296875, Learning Rate: 0.01\n",
      "Epoch [8965/20000], Loss: -3.675445556640625, Learning Rate: 0.01\n",
      "Epoch [8966/20000], Loss: -3.6802520751953125, Learning Rate: 0.01\n",
      "Epoch [8967/20000], Loss: -3.6848297119140625, Learning Rate: 0.01\n",
      "Epoch [8968/20000], Loss: -3.6893463134765625, Learning Rate: 0.01\n",
      "Epoch [8969/20000], Loss: -3.6939239501953125, Learning Rate: 0.01\n",
      "Epoch [8970/20000], Loss: -3.69873046875, Learning Rate: 0.01\n",
      "Epoch [8971/20000], Loss: -3.7035675048828125, Learning Rate: 0.01\n",
      "Epoch [8972/20000], Loss: -3.70843505859375, Learning Rate: 0.01\n",
      "Epoch [8973/20000], Loss: -3.7133331298828125, Learning Rate: 0.01\n",
      "Epoch [8974/20000], Loss: -3.718353271484375, Learning Rate: 0.01\n",
      "Epoch [8975/20000], Loss: -3.7236328125, Learning Rate: 0.01\n",
      "Epoch [8976/20000], Loss: -3.728912353515625, Learning Rate: 0.01\n",
      "Epoch [8977/20000], Loss: -3.7340850830078125, Learning Rate: 0.01\n",
      "Epoch [8978/20000], Loss: -3.7391204833984375, Learning Rate: 0.01\n",
      "Epoch [8979/20000], Loss: -3.7444915771484375, Learning Rate: 0.01\n",
      "Epoch [8980/20000], Loss: -3.749755859375, Learning Rate: 0.01\n",
      "Epoch [8981/20000], Loss: -3.7547760009765625, Learning Rate: 0.01\n",
      "Epoch [8982/20000], Loss: -3.7600250244140625, Learning Rate: 0.01\n",
      "Epoch [8983/20000], Loss: -3.7651214599609375, Learning Rate: 0.01\n",
      "Epoch [8984/20000], Loss: -3.7701873779296875, Learning Rate: 0.01\n",
      "Epoch [8985/20000], Loss: -3.77532958984375, Learning Rate: 0.01\n",
      "Epoch [8986/20000], Loss: -3.78021240234375, Learning Rate: 0.01\n",
      "Epoch [8987/20000], Loss: -3.78546142578125, Learning Rate: 0.01\n",
      "Epoch [8988/20000], Loss: -3.790496826171875, Learning Rate: 0.01\n",
      "Epoch [8989/20000], Loss: -3.795501708984375, Learning Rate: 0.01\n",
      "Epoch [8990/20000], Loss: -3.8006591796875, Learning Rate: 0.01\n",
      "Epoch [8991/20000], Loss: -3.8056182861328125, Learning Rate: 0.01\n",
      "Epoch [8992/20000], Loss: -3.810577392578125, Learning Rate: 0.01\n",
      "Epoch [8993/20000], Loss: -3.81573486328125, Learning Rate: 0.01\n",
      "Epoch [8994/20000], Loss: -3.8206939697265625, Learning Rate: 0.01\n",
      "Epoch [8995/20000], Loss: -3.8257904052734375, Learning Rate: 0.01\n",
      "Epoch [8996/20000], Loss: -3.8306427001953125, Learning Rate: 0.01\n",
      "Epoch [8997/20000], Loss: -3.835693359375, Learning Rate: 0.01\n",
      "Epoch [8998/20000], Loss: -3.84088134765625, Learning Rate: 0.01\n",
      "Epoch [8999/20000], Loss: -3.8457183837890625, Learning Rate: 0.01\n",
      "Epoch [9000/20000], Loss: -3.8507537841796875, Learning Rate: 0.01\n",
      "Epoch [9001/20000], Loss: -3.8559112548828125, Learning Rate: 0.01\n",
      "Epoch [9002/20000], Loss: -3.8608551025390625, Learning Rate: 0.01\n",
      "Epoch [9003/20000], Loss: -3.86590576171875, Learning Rate: 0.01\n",
      "Epoch [9004/20000], Loss: -3.8708953857421875, Learning Rate: 0.01\n",
      "Epoch [9005/20000], Loss: -3.8758544921875, Learning Rate: 0.01\n",
      "Epoch [9006/20000], Loss: -3.880950927734375, Learning Rate: 0.01\n",
      "Epoch [9007/20000], Loss: -3.8858642578125, Learning Rate: 0.01\n",
      "Epoch [9008/20000], Loss: -3.8908538818359375, Learning Rate: 0.01\n",
      "Epoch [9009/20000], Loss: -3.895782470703125, Learning Rate: 0.01\n",
      "Epoch [9010/20000], Loss: -3.900787353515625, Learning Rate: 0.01\n",
      "Epoch [9011/20000], Loss: -3.905792236328125, Learning Rate: 0.01\n",
      "Epoch [9012/20000], Loss: -3.910797119140625, Learning Rate: 0.01\n",
      "Epoch [9013/20000], Loss: -3.9156494140625, Learning Rate: 0.01\n",
      "Epoch [9014/20000], Loss: -3.9205780029296875, Learning Rate: 0.01\n",
      "Epoch [9015/20000], Loss: -3.9255218505859375, Learning Rate: 0.01\n",
      "Epoch [9016/20000], Loss: -3.930206298828125, Learning Rate: 0.01\n",
      "Epoch [9017/20000], Loss: -3.934783935546875, Learning Rate: 0.01\n",
      "Epoch [9018/20000], Loss: -3.93951416015625, Learning Rate: 0.01\n",
      "Epoch [9019/20000], Loss: -3.9436798095703125, Learning Rate: 0.01\n",
      "Epoch [9020/20000], Loss: -3.9477691650390625, Learning Rate: 0.01\n",
      "Epoch [9021/20000], Loss: -3.95147705078125, Learning Rate: 0.01\n",
      "Epoch [9022/20000], Loss: -3.954437255859375, Learning Rate: 0.01\n",
      "Epoch [9023/20000], Loss: -3.9565277099609375, Learning Rate: 0.01\n",
      "Epoch [9024/20000], Loss: -3.9572906494140625, Learning Rate: 0.01\n",
      "Epoch [9025/20000], Loss: -3.9560089111328125, Learning Rate: 0.01\n",
      "Epoch [9026/20000], Loss: -3.952117919921875, Learning Rate: 0.01\n",
      "Epoch [9027/20000], Loss: -3.943603515625, Learning Rate: 0.01\n",
      "Epoch [9028/20000], Loss: -3.928558349609375, Learning Rate: 0.01\n",
      "Epoch [9029/20000], Loss: -3.9039154052734375, Learning Rate: 0.01\n",
      "Epoch [9030/20000], Loss: -3.8643341064453125, Learning Rate: 0.01\n",
      "Epoch [9031/20000], Loss: -3.80181884765625, Learning Rate: 0.01\n",
      "Epoch [9032/20000], Loss: -3.7053070068359375, Learning Rate: 0.01\n",
      "Epoch [9033/20000], Loss: -3.5564117431640625, Learning Rate: 0.01\n",
      "Epoch [9034/20000], Loss: -3.32806396484375, Learning Rate: 0.01\n",
      "Epoch [9035/20000], Loss: -2.98028564453125, Learning Rate: 0.01\n",
      "Epoch [9036/20000], Loss: -2.455078125, Learning Rate: 0.01\n",
      "Epoch [9037/20000], Loss: -1.671966552734375, Learning Rate: 0.01\n",
      "Epoch [9038/20000], Loss: -0.529632568359375, Learning Rate: 0.01\n",
      "Epoch [9039/20000], Loss: 1.0789031982421875, Learning Rate: 0.01\n",
      "Epoch [9040/20000], Loss: 3.2131805419921875, Learning Rate: 0.01\n",
      "Epoch [9041/20000], Loss: 5.782562255859375, Learning Rate: 0.01\n",
      "Epoch [9042/20000], Loss: 8.375900268554688, Learning Rate: 0.01\n",
      "Epoch [9043/20000], Loss: 10.162460327148438, Learning Rate: 0.01\n",
      "Epoch [9044/20000], Loss: 10.058456420898438, Learning Rate: 0.01\n",
      "Epoch [9045/20000], Loss: 7.4579925537109375, Learning Rate: 0.01\n",
      "Epoch [9046/20000], Loss: 3.027618408203125, Learning Rate: 0.01\n",
      "Epoch [9047/20000], Loss: -1.285308837890625, Learning Rate: 0.01\n",
      "Epoch [9048/20000], Loss: -3.5553131103515625, Learning Rate: 0.01\n",
      "Epoch [9049/20000], Loss: -3.216888427734375, Learning Rate: 0.01\n",
      "Epoch [9050/20000], Loss: -1.28070068359375, Learning Rate: 0.01\n",
      "Epoch [9051/20000], Loss: 0.527923583984375, Learning Rate: 0.01\n",
      "Epoch [9052/20000], Loss: 0.8791351318359375, Learning Rate: 0.01\n",
      "Epoch [9053/20000], Loss: -0.38671875, Learning Rate: 0.01\n",
      "Epoch [9054/20000], Loss: -2.2735137939453125, Learning Rate: 0.01\n",
      "Epoch [9055/20000], Loss: -3.49615478515625, Learning Rate: 0.01\n",
      "Epoch [9056/20000], Loss: -3.4810943603515625, Learning Rate: 0.01\n",
      "Epoch [9057/20000], Loss: -2.66778564453125, Learning Rate: 0.01\n",
      "Epoch [9058/20000], Loss: -1.9777374267578125, Learning Rate: 0.01\n",
      "Epoch [9059/20000], Loss: -2.0434722900390625, Learning Rate: 0.01\n",
      "Epoch [9060/20000], Loss: -2.77545166015625, Learning Rate: 0.01\n",
      "Epoch [9061/20000], Loss: -3.5597381591796875, Learning Rate: 0.01\n",
      "Epoch [9062/20000], Loss: -3.8439788818359375, Learning Rate: 0.01\n",
      "Epoch [9063/20000], Loss: -3.575958251953125, Learning Rate: 0.01\n",
      "Epoch [9064/20000], Loss: -3.15704345703125, Learning Rate: 0.01\n",
      "Epoch [9065/20000], Loss: -3.032989501953125, Learning Rate: 0.01\n",
      "Epoch [9066/20000], Loss: -3.328765869140625, Learning Rate: 0.01\n",
      "Epoch [9067/20000], Loss: -3.79400634765625, Learning Rate: 0.01\n",
      "Epoch [9068/20000], Loss: -4.071929931640625, Learning Rate: 0.01\n",
      "Epoch [9069/20000], Loss: -4.009765625, Learning Rate: 0.01\n",
      "Epoch [9070/20000], Loss: -3.750946044921875, Learning Rate: 0.01\n",
      "Epoch [9071/20000], Loss: -3.5733489990234375, Learning Rate: 0.01\n",
      "Epoch [9072/20000], Loss: -3.6428070068359375, Learning Rate: 0.01\n",
      "Epoch [9073/20000], Loss: -3.90167236328125, Learning Rate: 0.01\n",
      "Epoch [9074/20000], Loss: -4.1469879150390625, Learning Rate: 0.01\n",
      "Epoch [9075/20000], Loss: -4.216094970703125, Learning Rate: 0.01\n",
      "Epoch [9076/20000], Loss: -4.107452392578125, Learning Rate: 0.01\n",
      "Epoch [9077/20000], Loss: -3.954254150390625, Learning Rate: 0.01\n",
      "Epoch [9078/20000], Loss: -3.8994903564453125, Learning Rate: 0.01\n",
      "Epoch [9079/20000], Loss: -3.9851837158203125, Learning Rate: 0.01\n",
      "Epoch [9080/20000], Loss: -4.1396636962890625, Learning Rate: 0.01\n",
      "Epoch [9081/20000], Loss: -4.25115966796875, Learning Rate: 0.01\n",
      "Epoch [9082/20000], Loss: -4.2599639892578125, Learning Rate: 0.01\n",
      "Epoch [9083/20000], Loss: -4.19195556640625, Learning Rate: 0.01\n",
      "Epoch [9084/20000], Loss: -4.1233673095703125, Learning Rate: 0.01\n",
      "Epoch [9085/20000], Loss: -4.1142425537109375, Learning Rate: 0.01\n",
      "Epoch [9086/20000], Loss: -4.1688079833984375, Learning Rate: 0.01\n",
      "Epoch [9087/20000], Loss: -4.2442474365234375, Learning Rate: 0.01\n",
      "Epoch [9088/20000], Loss: -4.2911376953125, Learning Rate: 0.01\n",
      "Epoch [9089/20000], Loss: -4.2905426025390625, Learning Rate: 0.01\n",
      "Epoch [9090/20000], Loss: -4.2614593505859375, Learning Rate: 0.01\n",
      "Epoch [9091/20000], Loss: -4.2382659912109375, Learning Rate: 0.01\n",
      "Epoch [9092/20000], Loss: -4.243072509765625, Learning Rate: 0.01\n",
      "Epoch [9093/20000], Loss: -4.2725677490234375, Learning Rate: 0.01\n",
      "Epoch [9094/20000], Loss: -4.306488037109375, Learning Rate: 0.01\n",
      "Epoch [9095/20000], Loss: -4.325653076171875, Learning Rate: 0.01\n",
      "Epoch [9096/20000], Loss: -4.325408935546875, Learning Rate: 0.01\n",
      "Epoch [9097/20000], Loss: -4.3156585693359375, Learning Rate: 0.01\n",
      "Epoch [9098/20000], Loss: -4.3110504150390625, Learning Rate: 0.01\n",
      "Epoch [9099/20000], Loss: -4.3193511962890625, Learning Rate: 0.01\n",
      "Epoch [9100/20000], Loss: -4.3369140625, Learning Rate: 0.01\n",
      "Epoch [9101/20000], Loss: -4.3544464111328125, Learning Rate: 0.01\n",
      "Epoch [9102/20000], Loss: -4.3641204833984375, Learning Rate: 0.01\n",
      "Epoch [9103/20000], Loss: -4.364776611328125, Learning Rate: 0.01\n",
      "Epoch [9104/20000], Loss: -4.3620452880859375, Learning Rate: 0.01\n",
      "Epoch [9105/20000], Loss: -4.362579345703125, Learning Rate: 0.01\n",
      "Epoch [9106/20000], Loss: -4.3695220947265625, Learning Rate: 0.01\n",
      "Epoch [9107/20000], Loss: -4.38140869140625, Learning Rate: 0.01\n",
      "Epoch [9108/20000], Loss: -4.393402099609375, Learning Rate: 0.01\n",
      "Epoch [9109/20000], Loss: -4.40142822265625, Learning Rate: 0.01\n",
      "Epoch [9110/20000], Loss: -4.4046173095703125, Learning Rate: 0.01\n",
      "Epoch [9111/20000], Loss: -4.4051055908203125, Learning Rate: 0.01\n",
      "Epoch [9112/20000], Loss: -4.40655517578125, Learning Rate: 0.01\n",
      "Epoch [9113/20000], Loss: -4.4111480712890625, Learning Rate: 0.01\n",
      "Epoch [9114/20000], Loss: -4.418853759765625, Learning Rate: 0.01\n",
      "Epoch [9115/20000], Loss: -4.42779541015625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9116/20000], Loss: -4.4352264404296875, Learning Rate: 0.01\n",
      "Epoch [9117/20000], Loss: -4.440643310546875, Learning Rate: 0.01\n",
      "Epoch [9118/20000], Loss: -4.4439544677734375, Learning Rate: 0.01\n",
      "Epoch [9119/20000], Loss: -4.4469146728515625, Learning Rate: 0.01\n",
      "Epoch [9120/20000], Loss: -4.450653076171875, Learning Rate: 0.01\n",
      "Epoch [9121/20000], Loss: -4.455718994140625, Learning Rate: 0.01\n",
      "Epoch [9122/20000], Loss: -4.4620208740234375, Learning Rate: 0.01\n",
      "Epoch [9123/20000], Loss: -4.4685516357421875, Learning Rate: 0.01\n",
      "Epoch [9124/20000], Loss: -4.474365234375, Learning Rate: 0.01\n",
      "Epoch [9125/20000], Loss: -4.4790802001953125, Learning Rate: 0.01\n",
      "Epoch [9126/20000], Loss: -4.483367919921875, Learning Rate: 0.01\n",
      "Epoch [9127/20000], Loss: -4.4876251220703125, Learning Rate: 0.01\n",
      "Epoch [9128/20000], Loss: -4.492156982421875, Learning Rate: 0.01\n",
      "Epoch [9129/20000], Loss: -4.4971771240234375, Learning Rate: 0.01\n",
      "Epoch [9130/20000], Loss: -4.5027618408203125, Learning Rate: 0.01\n",
      "Epoch [9131/20000], Loss: -4.5081024169921875, Learning Rate: 0.01\n",
      "Epoch [9132/20000], Loss: -4.513214111328125, Learning Rate: 0.01\n",
      "Epoch [9133/20000], Loss: -4.5179901123046875, Learning Rate: 0.01\n",
      "Epoch [9134/20000], Loss: -4.522430419921875, Learning Rate: 0.01\n",
      "Epoch [9135/20000], Loss: -4.527252197265625, Learning Rate: 0.01\n",
      "Epoch [9136/20000], Loss: -4.53179931640625, Learning Rate: 0.01\n",
      "Epoch [9137/20000], Loss: -4.5369720458984375, Learning Rate: 0.01\n",
      "Epoch [9138/20000], Loss: -4.5421295166015625, Learning Rate: 0.01\n",
      "Epoch [9139/20000], Loss: -4.5473480224609375, Learning Rate: 0.01\n",
      "Epoch [9140/20000], Loss: -4.55224609375, Learning Rate: 0.01\n",
      "Epoch [9141/20000], Loss: -4.557159423828125, Learning Rate: 0.01\n",
      "Epoch [9142/20000], Loss: -4.56182861328125, Learning Rate: 0.01\n",
      "Epoch [9143/20000], Loss: -4.5665130615234375, Learning Rate: 0.01\n",
      "Epoch [9144/20000], Loss: -4.571441650390625, Learning Rate: 0.01\n",
      "Epoch [9145/20000], Loss: -4.576263427734375, Learning Rate: 0.01\n",
      "Epoch [9146/20000], Loss: -4.5811920166015625, Learning Rate: 0.01\n",
      "Epoch [9147/20000], Loss: -4.5861663818359375, Learning Rate: 0.01\n",
      "Epoch [9148/20000], Loss: -4.5911407470703125, Learning Rate: 0.01\n",
      "Epoch [9149/20000], Loss: -4.5961151123046875, Learning Rate: 0.01\n",
      "Epoch [9150/20000], Loss: -4.6009521484375, Learning Rate: 0.01\n",
      "Epoch [9151/20000], Loss: -4.6058349609375, Learning Rate: 0.01\n",
      "Epoch [9152/20000], Loss: -4.61065673828125, Learning Rate: 0.01\n",
      "Epoch [9153/20000], Loss: -4.615509033203125, Learning Rate: 0.01\n",
      "Epoch [9154/20000], Loss: -4.620452880859375, Learning Rate: 0.01\n",
      "Epoch [9155/20000], Loss: -4.625335693359375, Learning Rate: 0.01\n",
      "Epoch [9156/20000], Loss: -4.6300201416015625, Learning Rate: 0.01\n",
      "Epoch [9157/20000], Loss: -4.6350250244140625, Learning Rate: 0.01\n",
      "Epoch [9158/20000], Loss: -4.639862060546875, Learning Rate: 0.01\n",
      "Epoch [9159/20000], Loss: -4.64483642578125, Learning Rate: 0.01\n",
      "Epoch [9160/20000], Loss: -4.6496124267578125, Learning Rate: 0.01\n",
      "Epoch [9161/20000], Loss: -4.654449462890625, Learning Rate: 0.01\n",
      "Epoch [9162/20000], Loss: -4.6594085693359375, Learning Rate: 0.01\n",
      "Epoch [9163/20000], Loss: -4.664276123046875, Learning Rate: 0.01\n",
      "Epoch [9164/20000], Loss: -4.6690216064453125, Learning Rate: 0.01\n",
      "Epoch [9165/20000], Loss: -4.674072265625, Learning Rate: 0.01\n",
      "Epoch [9166/20000], Loss: -4.678985595703125, Learning Rate: 0.01\n",
      "Epoch [9167/20000], Loss: -4.683837890625, Learning Rate: 0.01\n",
      "Epoch [9168/20000], Loss: -4.688690185546875, Learning Rate: 0.01\n",
      "Epoch [9169/20000], Loss: -4.693603515625, Learning Rate: 0.01\n",
      "Epoch [9170/20000], Loss: -4.6985321044921875, Learning Rate: 0.01\n",
      "Epoch [9171/20000], Loss: -4.7034149169921875, Learning Rate: 0.01\n",
      "Epoch [9172/20000], Loss: -4.708221435546875, Learning Rate: 0.01\n",
      "Epoch [9173/20000], Loss: -4.7131195068359375, Learning Rate: 0.01\n",
      "Epoch [9174/20000], Loss: -4.718109130859375, Learning Rate: 0.01\n",
      "Epoch [9175/20000], Loss: -4.7229156494140625, Learning Rate: 0.01\n",
      "Epoch [9176/20000], Loss: -4.7277679443359375, Learning Rate: 0.01\n",
      "Epoch [9177/20000], Loss: -4.7327728271484375, Learning Rate: 0.01\n",
      "Epoch [9178/20000], Loss: -4.7376861572265625, Learning Rate: 0.01\n",
      "Epoch [9179/20000], Loss: -4.7426605224609375, Learning Rate: 0.01\n",
      "Epoch [9180/20000], Loss: -4.7473907470703125, Learning Rate: 0.01\n",
      "Epoch [9181/20000], Loss: -4.7523345947265625, Learning Rate: 0.01\n",
      "Epoch [9182/20000], Loss: -4.75738525390625, Learning Rate: 0.01\n",
      "Epoch [9183/20000], Loss: -4.762054443359375, Learning Rate: 0.01\n",
      "Epoch [9184/20000], Loss: -4.7669525146484375, Learning Rate: 0.01\n",
      "Epoch [9185/20000], Loss: -4.77203369140625, Learning Rate: 0.01\n",
      "Epoch [9186/20000], Loss: -4.7768096923828125, Learning Rate: 0.01\n",
      "Epoch [9187/20000], Loss: -4.7818145751953125, Learning Rate: 0.01\n",
      "Epoch [9188/20000], Loss: -4.78656005859375, Learning Rate: 0.01\n",
      "Epoch [9189/20000], Loss: -4.791534423828125, Learning Rate: 0.01\n",
      "Epoch [9190/20000], Loss: -4.79656982421875, Learning Rate: 0.01\n",
      "Epoch [9191/20000], Loss: -4.8012847900390625, Learning Rate: 0.01\n",
      "Epoch [9192/20000], Loss: -4.8062896728515625, Learning Rate: 0.01\n",
      "Epoch [9193/20000], Loss: -4.8111572265625, Learning Rate: 0.01\n",
      "Epoch [9194/20000], Loss: -4.8160552978515625, Learning Rate: 0.01\n",
      "Epoch [9195/20000], Loss: -4.8209686279296875, Learning Rate: 0.01\n",
      "Epoch [9196/20000], Loss: -4.8258209228515625, Learning Rate: 0.01\n",
      "Epoch [9197/20000], Loss: -4.8306732177734375, Learning Rate: 0.01\n",
      "Epoch [9198/20000], Loss: -4.8358001708984375, Learning Rate: 0.01\n",
      "Epoch [9199/20000], Loss: -4.8406829833984375, Learning Rate: 0.01\n",
      "Epoch [9200/20000], Loss: -4.8455963134765625, Learning Rate: 0.01\n",
      "Epoch [9201/20000], Loss: -4.8504638671875, Learning Rate: 0.01\n",
      "Epoch [9202/20000], Loss: -4.85528564453125, Learning Rate: 0.01\n",
      "Epoch [9203/20000], Loss: -4.8601837158203125, Learning Rate: 0.01\n",
      "Epoch [9204/20000], Loss: -4.8650054931640625, Learning Rate: 0.01\n",
      "Epoch [9205/20000], Loss: -4.8699493408203125, Learning Rate: 0.01\n",
      "Epoch [9206/20000], Loss: -4.8748321533203125, Learning Rate: 0.01\n",
      "Epoch [9207/20000], Loss: -4.8798065185546875, Learning Rate: 0.01\n",
      "Epoch [9208/20000], Loss: -4.8846893310546875, Learning Rate: 0.01\n",
      "Epoch [9209/20000], Loss: -4.8894805908203125, Learning Rate: 0.01\n",
      "Epoch [9210/20000], Loss: -4.894134521484375, Learning Rate: 0.01\n",
      "Epoch [9211/20000], Loss: -4.8989410400390625, Learning Rate: 0.01\n",
      "Epoch [9212/20000], Loss: -4.90362548828125, Learning Rate: 0.01\n",
      "Epoch [9213/20000], Loss: -4.9080657958984375, Learning Rate: 0.01\n",
      "Epoch [9214/20000], Loss: -4.912322998046875, Learning Rate: 0.01\n",
      "Epoch [9215/20000], Loss: -4.9164581298828125, Learning Rate: 0.01\n",
      "Epoch [9216/20000], Loss: -4.9202728271484375, Learning Rate: 0.01\n",
      "Epoch [9217/20000], Loss: -4.9234161376953125, Learning Rate: 0.01\n",
      "Epoch [9218/20000], Loss: -4.925994873046875, Learning Rate: 0.01\n",
      "Epoch [9219/20000], Loss: -4.926971435546875, Learning Rate: 0.01\n",
      "Epoch [9220/20000], Loss: -4.9268646240234375, Learning Rate: 0.01\n",
      "Epoch [9221/20000], Loss: -4.923736572265625, Learning Rate: 0.01\n",
      "Epoch [9222/20000], Loss: -4.9170684814453125, Learning Rate: 0.01\n",
      "Epoch [9223/20000], Loss: -4.904541015625, Learning Rate: 0.01\n",
      "Epoch [9224/20000], Loss: -4.8832550048828125, Learning Rate: 0.01\n",
      "Epoch [9225/20000], Loss: -4.84881591796875, Learning Rate: 0.01\n",
      "Epoch [9226/20000], Loss: -4.7943878173828125, Learning Rate: 0.01\n",
      "Epoch [9227/20000], Loss: -4.7099151611328125, Learning Rate: 0.01\n",
      "Epoch [9228/20000], Loss: -4.579986572265625, Learning Rate: 0.01\n",
      "Epoch [9229/20000], Loss: -4.384185791015625, Learning Rate: 0.01\n",
      "Epoch [9230/20000], Loss: -4.0919189453125, Learning Rate: 0.01\n",
      "Epoch [9231/20000], Loss: -3.670745849609375, Learning Rate: 0.01\n",
      "Epoch [9232/20000], Loss: -3.081207275390625, Learning Rate: 0.01\n",
      "Epoch [9233/20000], Loss: -2.3181915283203125, Learning Rate: 0.01\n",
      "Epoch [9234/20000], Loss: -1.414031982421875, Learning Rate: 0.01\n",
      "Epoch [9235/20000], Loss: -0.5495147705078125, Learning Rate: 0.01\n",
      "Epoch [9236/20000], Loss: 0.0126495361328125, Learning Rate: 0.01\n",
      "Epoch [9237/20000], Loss: -0.106689453125, Learning Rate: 0.01\n",
      "Epoch [9238/20000], Loss: -1.0152587890625, Learning Rate: 0.01\n",
      "Epoch [9239/20000], Loss: -2.4661102294921875, Learning Rate: 0.01\n",
      "Epoch [9240/20000], Loss: -3.79638671875, Learning Rate: 0.01\n",
      "Epoch [9241/20000], Loss: -4.42742919921875, Learning Rate: 0.01\n",
      "Epoch [9242/20000], Loss: -4.2230682373046875, Learning Rate: 0.01\n",
      "Epoch [9243/20000], Loss: -3.5373077392578125, Learning Rate: 0.01\n",
      "Epoch [9244/20000], Loss: -2.9489288330078125, Learning Rate: 0.01\n",
      "Epoch [9245/20000], Loss: -2.8945465087890625, Learning Rate: 0.01\n",
      "Epoch [9246/20000], Loss: -3.4509735107421875, Learning Rate: 0.01\n",
      "Epoch [9247/20000], Loss: -4.2663116455078125, Learning Rate: 0.01\n",
      "Epoch [9248/20000], Loss: -4.8639068603515625, Learning Rate: 0.01\n",
      "Epoch [9249/20000], Loss: -4.9719085693359375, Learning Rate: 0.01\n",
      "Epoch [9250/20000], Loss: -4.689361572265625, Learning Rate: 0.01\n",
      "Epoch [9251/20000], Loss: -4.3483123779296875, Learning Rate: 0.01\n",
      "Epoch [9252/20000], Loss: -4.2468109130859375, Learning Rate: 0.01\n",
      "Epoch [9253/20000], Loss: -4.4587554931640625, Learning Rate: 0.01\n",
      "Epoch [9254/20000], Loss: -4.8106689453125, Learning Rate: 0.01\n",
      "Epoch [9255/20000], Loss: -5.0545501708984375, Learning Rate: 0.01\n",
      "Epoch [9256/20000], Loss: -5.0567626953125, Learning Rate: 0.01\n",
      "Epoch [9257/20000], Loss: -4.877044677734375, Learning Rate: 0.01\n",
      "Epoch [9258/20000], Loss: -4.6916961669921875, Learning Rate: 0.01\n",
      "Epoch [9259/20000], Loss: -4.6462249755859375, Learning Rate: 0.01\n",
      "Epoch [9260/20000], Loss: -4.7622528076171875, Learning Rate: 0.01\n",
      "Epoch [9261/20000], Loss: -4.9418182373046875, Learning Rate: 0.01\n",
      "Epoch [9262/20000], Loss: -5.0612640380859375, Learning Rate: 0.01\n",
      "Epoch [9263/20000], Loss: -5.0635528564453125, Learning Rate: 0.01\n",
      "Epoch [9264/20000], Loss: -4.9865264892578125, Learning Rate: 0.01\n",
      "Epoch [9265/20000], Loss: -4.91705322265625, Learning Rate: 0.01\n",
      "Epoch [9266/20000], Loss: -4.9196319580078125, Learning Rate: 0.01\n",
      "Epoch [9267/20000], Loss: -4.99609375, Learning Rate: 0.01\n",
      "Epoch [9268/20000], Loss: -5.094390869140625, Learning Rate: 0.01\n",
      "Epoch [9269/20000], Loss: -5.1565704345703125, Learning Rate: 0.01\n",
      "Epoch [9270/20000], Loss: -5.1596832275390625, Learning Rate: 0.01\n",
      "Epoch [9271/20000], Loss: -5.1253662109375, Learning Rate: 0.01\n",
      "Epoch [9272/20000], Loss: -5.09576416015625, Learning Rate: 0.01\n",
      "Epoch [9273/20000], Loss: -5.1002197265625, Learning Rate: 0.01\n",
      "Epoch [9274/20000], Loss: -5.13861083984375, Learning Rate: 0.01\n",
      "Epoch [9275/20000], Loss: -5.1861572265625, Learning Rate: 0.01\n",
      "Epoch [9276/20000], Loss: -5.216644287109375, Learning Rate: 0.01\n",
      "Epoch [9277/20000], Loss: -5.2193450927734375, Learning Rate: 0.01\n",
      "Epoch [9278/20000], Loss: -5.2034759521484375, Learning Rate: 0.01\n",
      "Epoch [9279/20000], Loss: -5.1889190673828125, Learning Rate: 0.01\n",
      "Epoch [9280/20000], Loss: -5.1900787353515625, Learning Rate: 0.01\n",
      "Epoch [9281/20000], Loss: -5.2075347900390625, Learning Rate: 0.01\n",
      "Epoch [9282/20000], Loss: -5.23175048828125, Learning Rate: 0.01\n",
      "Epoch [9283/20000], Loss: -5.2496337890625, Learning Rate: 0.01\n",
      "Epoch [9284/20000], Loss: -5.25494384765625, Learning Rate: 0.01\n",
      "Epoch [9285/20000], Loss: -5.2505340576171875, Learning Rate: 0.01\n",
      "Epoch [9286/20000], Loss: -5.2445831298828125, Learning Rate: 0.01\n",
      "Epoch [9287/20000], Loss: -5.2445220947265625, Learning Rate: 0.01\n",
      "Epoch [9288/20000], Loss: -5.2527618408203125, Learning Rate: 0.01\n",
      "Epoch [9289/20000], Loss: -5.2653656005859375, Learning Rate: 0.01\n",
      "Epoch [9290/20000], Loss: -5.2769012451171875, Learning Rate: 0.01\n",
      "Epoch [9291/20000], Loss: -5.2829437255859375, Learning Rate: 0.01\n",
      "Epoch [9292/20000], Loss: -5.2837066650390625, Learning Rate: 0.01\n",
      "Epoch [9293/20000], Loss: -5.2817230224609375, Learning Rate: 0.01\n",
      "Epoch [9294/20000], Loss: -5.280609130859375, Learning Rate: 0.01\n",
      "Epoch [9295/20000], Loss: -5.2823944091796875, Learning Rate: 0.01\n",
      "Epoch [9296/20000], Loss: -5.2864532470703125, Learning Rate: 0.01\n",
      "Epoch [9297/20000], Loss: -5.290496826171875, Learning Rate: 0.01\n",
      "Epoch [9298/20000], Loss: -5.2914886474609375, Learning Rate: 0.01\n",
      "Epoch [9299/20000], Loss: -5.288238525390625, Learning Rate: 0.01\n",
      "Epoch [9300/20000], Loss: -5.2806396484375, Learning Rate: 0.01\n",
      "Epoch [9301/20000], Loss: -5.26898193359375, Learning Rate: 0.01\n",
      "Epoch [9302/20000], Loss: -5.2539520263671875, Learning Rate: 0.01\n",
      "Epoch [9303/20000], Loss: -5.234283447265625, Learning Rate: 0.01\n",
      "Epoch [9304/20000], Loss: -5.207489013671875, Learning Rate: 0.01\n",
      "Epoch [9305/20000], Loss: -5.1696929931640625, Learning Rate: 0.01\n",
      "Epoch [9306/20000], Loss: -5.1156768798828125, Learning Rate: 0.01\n",
      "Epoch [9307/20000], Loss: -5.04010009765625, Learning Rate: 0.01\n",
      "Epoch [9308/20000], Loss: -4.9357147216796875, Learning Rate: 0.01\n",
      "Epoch [9309/20000], Loss: -4.7932891845703125, Learning Rate: 0.01\n",
      "Epoch [9310/20000], Loss: -4.600555419921875, Learning Rate: 0.01\n",
      "Epoch [9311/20000], Loss: -4.341827392578125, Learning Rate: 0.01\n",
      "Epoch [9312/20000], Loss: -3.9975433349609375, Learning Rate: 0.01\n",
      "Epoch [9313/20000], Loss: -3.547119140625, Learning Rate: 0.01\n",
      "Epoch [9314/20000], Loss: -2.971771240234375, Learning Rate: 0.01\n",
      "Epoch [9315/20000], Loss: -2.2646484375, Learning Rate: 0.01\n",
      "Epoch [9316/20000], Loss: -1.4447784423828125, Learning Rate: 0.01\n",
      "Epoch [9317/20000], Loss: -0.57476806640625, Learning Rate: 0.01\n",
      "Epoch [9318/20000], Loss: 0.2182159423828125, Learning Rate: 0.01\n",
      "Epoch [9319/20000], Loss: 0.745880126953125, Learning Rate: 0.01\n",
      "Epoch [9320/20000], Loss: 0.7938995361328125, Learning Rate: 0.01\n",
      "Epoch [9321/20000], Loss: 0.2140655517578125, Learning Rate: 0.01\n",
      "Epoch [9322/20000], Loss: -0.9683074951171875, Learning Rate: 0.01\n",
      "Epoch [9323/20000], Loss: -2.5015716552734375, Learning Rate: 0.01\n",
      "Epoch [9324/20000], Loss: -3.97979736328125, Learning Rate: 0.01\n",
      "Epoch [9325/20000], Loss: -5.01959228515625, Learning Rate: 0.01\n",
      "Epoch [9326/20000], Loss: -5.42413330078125, Learning Rate: 0.01\n",
      "Epoch [9327/20000], Loss: -5.2405242919921875, Learning Rate: 0.01\n",
      "Epoch [9328/20000], Loss: -4.6988372802734375, Learning Rate: 0.01\n",
      "Epoch [9329/20000], Loss: -4.0996856689453125, Learning Rate: 0.01\n",
      "Epoch [9330/20000], Loss: -3.7053375244140625, Learning Rate: 0.01\n",
      "Epoch [9331/20000], Loss: -3.662139892578125, Learning Rate: 0.01\n",
      "Epoch [9332/20000], Loss: -3.966278076171875, Learning Rate: 0.01\n",
      "Epoch [9333/20000], Loss: -4.482940673828125, Learning Rate: 0.01\n",
      "Epoch [9334/20000], Loss: -5.0126953125, Learning Rate: 0.01\n",
      "Epoch [9335/20000], Loss: -5.3794708251953125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9336/20000], Loss: -5.4995269775390625, Learning Rate: 0.01\n",
      "Epoch [9337/20000], Loss: -5.396392822265625, Learning Rate: 0.01\n",
      "Epoch [9338/20000], Loss: -5.172821044921875, Learning Rate: 0.01\n",
      "Epoch [9339/20000], Loss: -4.956085205078125, Learning Rate: 0.01\n",
      "Epoch [9340/20000], Loss: -4.8462066650390625, Learning Rate: 0.01\n",
      "Epoch [9341/20000], Loss: -4.882781982421875, Learning Rate: 0.01\n",
      "Epoch [9342/20000], Loss: -5.0408935546875, Learning Rate: 0.01\n",
      "Epoch [9343/20000], Loss: -5.2518768310546875, Learning Rate: 0.01\n",
      "Epoch [9344/20000], Loss: -5.4368896484375, Learning Rate: 0.01\n",
      "Epoch [9345/20000], Loss: -5.541778564453125, Learning Rate: 0.01\n",
      "Epoch [9346/20000], Loss: -5.5516510009765625, Learning Rate: 0.01\n",
      "Epoch [9347/20000], Loss: -5.4906158447265625, Learning Rate: 0.01\n",
      "Epoch [9348/20000], Loss: -5.4037933349609375, Learning Rate: 0.01\n",
      "Epoch [9349/20000], Loss: -5.3364410400390625, Learning Rate: 0.01\n",
      "Epoch [9350/20000], Loss: -5.3169403076171875, Learning Rate: 0.01\n",
      "Epoch [9351/20000], Loss: -5.349884033203125, Learning Rate: 0.01\n",
      "Epoch [9352/20000], Loss: -5.4194183349609375, Learning Rate: 0.01\n",
      "Epoch [9353/20000], Loss: -5.4991302490234375, Learning Rate: 0.01\n",
      "Epoch [9354/20000], Loss: -5.5642242431640625, Learning Rate: 0.01\n",
      "Epoch [9355/20000], Loss: -5.59918212890625, Learning Rate: 0.01\n",
      "Epoch [9356/20000], Loss: -5.6028289794921875, Learning Rate: 0.01\n",
      "Epoch [9357/20000], Loss: -5.5843048095703125, Learning Rate: 0.01\n",
      "Epoch [9358/20000], Loss: -5.5578460693359375, Learning Rate: 0.01\n",
      "Epoch [9359/20000], Loss: -5.5377197265625, Learning Rate: 0.01\n",
      "Epoch [9360/20000], Loss: -5.53216552734375, Learning Rate: 0.01\n",
      "Epoch [9361/20000], Loss: -5.543426513671875, Learning Rate: 0.01\n",
      "Epoch [9362/20000], Loss: -5.567291259765625, Learning Rate: 0.01\n",
      "Epoch [9363/20000], Loss: -5.5968170166015625, Learning Rate: 0.01\n",
      "Epoch [9364/20000], Loss: -5.6241607666015625, Learning Rate: 0.01\n",
      "Epoch [9365/20000], Loss: -5.6441650390625, Learning Rate: 0.01\n",
      "Epoch [9366/20000], Loss: -5.6544647216796875, Learning Rate: 0.01\n",
      "Epoch [9367/20000], Loss: -5.656097412109375, Learning Rate: 0.01\n",
      "Epoch [9368/20000], Loss: -5.652496337890625, Learning Rate: 0.01\n",
      "Epoch [9369/20000], Loss: -5.647430419921875, Learning Rate: 0.01\n",
      "Epoch [9370/20000], Loss: -5.6446990966796875, Learning Rate: 0.01\n",
      "Epoch [9371/20000], Loss: -5.6463623046875, Learning Rate: 0.01\n",
      "Epoch [9372/20000], Loss: -5.652740478515625, Learning Rate: 0.01\n",
      "Epoch [9373/20000], Loss: -5.6633148193359375, Learning Rate: 0.01\n",
      "Epoch [9374/20000], Loss: -5.6756439208984375, Learning Rate: 0.01\n",
      "Epoch [9375/20000], Loss: -5.6880340576171875, Learning Rate: 0.01\n",
      "Epoch [9376/20000], Loss: -5.69866943359375, Learning Rate: 0.01\n",
      "Epoch [9377/20000], Loss: -5.7066192626953125, Learning Rate: 0.01\n",
      "Epoch [9378/20000], Loss: -5.7118988037109375, Learning Rate: 0.01\n",
      "Epoch [9379/20000], Loss: -5.7150421142578125, Learning Rate: 0.01\n",
      "Epoch [9380/20000], Loss: -5.7170867919921875, Learning Rate: 0.01\n",
      "Epoch [9381/20000], Loss: -5.7188262939453125, Learning Rate: 0.01\n",
      "Epoch [9382/20000], Loss: -5.72149658203125, Learning Rate: 0.01\n",
      "Epoch [9383/20000], Loss: -5.725341796875, Learning Rate: 0.01\n",
      "Epoch [9384/20000], Loss: -5.7302703857421875, Learning Rate: 0.01\n",
      "Epoch [9385/20000], Loss: -5.7363128662109375, Learning Rate: 0.01\n",
      "Epoch [9386/20000], Loss: -5.7431182861328125, Learning Rate: 0.01\n",
      "Epoch [9387/20000], Loss: -5.7499542236328125, Learning Rate: 0.01\n",
      "Epoch [9388/20000], Loss: -5.756561279296875, Learning Rate: 0.01\n",
      "Epoch [9389/20000], Loss: -5.7627105712890625, Learning Rate: 0.01\n",
      "Epoch [9390/20000], Loss: -5.76837158203125, Learning Rate: 0.01\n",
      "Epoch [9391/20000], Loss: -5.7733612060546875, Learning Rate: 0.01\n",
      "Epoch [9392/20000], Loss: -5.777801513671875, Learning Rate: 0.01\n",
      "Epoch [9393/20000], Loss: -5.782073974609375, Learning Rate: 0.01\n",
      "Epoch [9394/20000], Loss: -5.7862701416015625, Learning Rate: 0.01\n",
      "Epoch [9395/20000], Loss: -5.7901458740234375, Learning Rate: 0.01\n",
      "Epoch [9396/20000], Loss: -5.7945098876953125, Learning Rate: 0.01\n",
      "Epoch [9397/20000], Loss: -5.798828125, Learning Rate: 0.01\n",
      "Epoch [9398/20000], Loss: -5.803619384765625, Learning Rate: 0.01\n",
      "Epoch [9399/20000], Loss: -5.80859375, Learning Rate: 0.01\n",
      "Epoch [9400/20000], Loss: -5.8137054443359375, Learning Rate: 0.01\n",
      "Epoch [9401/20000], Loss: -5.8188323974609375, Learning Rate: 0.01\n",
      "Epoch [9402/20000], Loss: -5.8238372802734375, Learning Rate: 0.01\n",
      "Epoch [9403/20000], Loss: -5.828948974609375, Learning Rate: 0.01\n",
      "Epoch [9404/20000], Loss: -5.8340911865234375, Learning Rate: 0.01\n",
      "Epoch [9405/20000], Loss: -5.8391876220703125, Learning Rate: 0.01\n",
      "Epoch [9406/20000], Loss: -5.8440704345703125, Learning Rate: 0.01\n",
      "Epoch [9407/20000], Loss: -5.848968505859375, Learning Rate: 0.01\n",
      "Epoch [9408/20000], Loss: -5.853759765625, Learning Rate: 0.01\n",
      "Epoch [9409/20000], Loss: -5.8585205078125, Learning Rate: 0.01\n",
      "Epoch [9410/20000], Loss: -5.86309814453125, Learning Rate: 0.01\n",
      "Epoch [9411/20000], Loss: -5.8678436279296875, Learning Rate: 0.01\n",
      "Epoch [9412/20000], Loss: -5.872406005859375, Learning Rate: 0.01\n",
      "Epoch [9413/20000], Loss: -5.877105712890625, Learning Rate: 0.01\n",
      "Epoch [9414/20000], Loss: -5.881744384765625, Learning Rate: 0.01\n",
      "Epoch [9415/20000], Loss: -5.8864288330078125, Learning Rate: 0.01\n",
      "Epoch [9416/20000], Loss: -5.8910980224609375, Learning Rate: 0.01\n",
      "Epoch [9417/20000], Loss: -5.8959808349609375, Learning Rate: 0.01\n",
      "Epoch [9418/20000], Loss: -5.900604248046875, Learning Rate: 0.01\n",
      "Epoch [9419/20000], Loss: -5.90533447265625, Learning Rate: 0.01\n",
      "Epoch [9420/20000], Loss: -5.9100799560546875, Learning Rate: 0.01\n",
      "Epoch [9421/20000], Loss: -5.9147796630859375, Learning Rate: 0.01\n",
      "Epoch [9422/20000], Loss: -5.919677734375, Learning Rate: 0.01\n",
      "Epoch [9423/20000], Loss: -5.92437744140625, Learning Rate: 0.01\n",
      "Epoch [9424/20000], Loss: -5.92913818359375, Learning Rate: 0.01\n",
      "Epoch [9425/20000], Loss: -5.9338836669921875, Learning Rate: 0.01\n",
      "Epoch [9426/20000], Loss: -5.938568115234375, Learning Rate: 0.01\n",
      "Epoch [9427/20000], Loss: -5.9434356689453125, Learning Rate: 0.01\n",
      "Epoch [9428/20000], Loss: -5.9482421875, Learning Rate: 0.01\n",
      "Epoch [9429/20000], Loss: -5.952972412109375, Learning Rate: 0.01\n",
      "Epoch [9430/20000], Loss: -5.957733154296875, Learning Rate: 0.01\n",
      "Epoch [9431/20000], Loss: -5.9624481201171875, Learning Rate: 0.01\n",
      "Epoch [9432/20000], Loss: -5.9673309326171875, Learning Rate: 0.01\n",
      "Epoch [9433/20000], Loss: -5.972076416015625, Learning Rate: 0.01\n",
      "Epoch [9434/20000], Loss: -5.9765777587890625, Learning Rate: 0.01\n",
      "Epoch [9435/20000], Loss: -5.9813079833984375, Learning Rate: 0.01\n",
      "Epoch [9436/20000], Loss: -5.9860687255859375, Learning Rate: 0.01\n",
      "Epoch [9437/20000], Loss: -5.9908599853515625, Learning Rate: 0.01\n",
      "Epoch [9438/20000], Loss: -5.9954986572265625, Learning Rate: 0.01\n",
      "Epoch [9439/20000], Loss: -6.0000762939453125, Learning Rate: 0.01\n",
      "Epoch [9440/20000], Loss: -6.0047454833984375, Learning Rate: 0.01\n",
      "Epoch [9441/20000], Loss: -6.0093231201171875, Learning Rate: 0.01\n",
      "Epoch [9442/20000], Loss: -6.0139007568359375, Learning Rate: 0.01\n",
      "Epoch [9443/20000], Loss: -6.0183258056640625, Learning Rate: 0.01\n",
      "Epoch [9444/20000], Loss: -6.022613525390625, Learning Rate: 0.01\n",
      "Epoch [9445/20000], Loss: -6.02691650390625, Learning Rate: 0.01\n",
      "Epoch [9446/20000], Loss: -6.0312347412109375, Learning Rate: 0.01\n",
      "Epoch [9447/20000], Loss: -6.0350799560546875, Learning Rate: 0.01\n",
      "Epoch [9448/20000], Loss: -6.03887939453125, Learning Rate: 0.01\n",
      "Epoch [9449/20000], Loss: -6.0420684814453125, Learning Rate: 0.01\n",
      "Epoch [9450/20000], Loss: -6.044769287109375, Learning Rate: 0.01\n",
      "Epoch [9451/20000], Loss: -6.0469818115234375, Learning Rate: 0.01\n",
      "Epoch [9452/20000], Loss: -6.0481414794921875, Learning Rate: 0.01\n",
      "Epoch [9453/20000], Loss: -6.0480194091796875, Learning Rate: 0.01\n",
      "Epoch [9454/20000], Loss: -6.0460662841796875, Learning Rate: 0.01\n",
      "Epoch [9455/20000], Loss: -6.041717529296875, Learning Rate: 0.01\n",
      "Epoch [9456/20000], Loss: -6.033782958984375, Learning Rate: 0.01\n",
      "Epoch [9457/20000], Loss: -6.0208282470703125, Learning Rate: 0.01\n",
      "Epoch [9458/20000], Loss: -6.0006866455078125, Learning Rate: 0.01\n",
      "Epoch [9459/20000], Loss: -5.970458984375, Learning Rate: 0.01\n",
      "Epoch [9460/20000], Loss: -5.9255523681640625, Learning Rate: 0.01\n",
      "Epoch [9461/20000], Loss: -5.8601531982421875, Learning Rate: 0.01\n",
      "Epoch [9462/20000], Loss: -5.764739990234375, Learning Rate: 0.01\n",
      "Epoch [9463/20000], Loss: -5.6269683837890625, Learning Rate: 0.01\n",
      "Epoch [9464/20000], Loss: -5.42852783203125, Learning Rate: 0.01\n",
      "Epoch [9465/20000], Loss: -5.1445465087890625, Learning Rate: 0.01\n",
      "Epoch [9466/20000], Loss: -4.742401123046875, Learning Rate: 0.01\n",
      "Epoch [9467/20000], Loss: -4.180999755859375, Learning Rate: 0.01\n",
      "Epoch [9468/20000], Loss: -3.4145965576171875, Learning Rate: 0.01\n",
      "Epoch [9469/20000], Loss: -2.4040985107421875, Learning Rate: 0.01\n",
      "Epoch [9470/20000], Loss: -1.142059326171875, Learning Rate: 0.01\n",
      "Epoch [9471/20000], Loss: 0.3035736083984375, Learning Rate: 0.01\n",
      "Epoch [9472/20000], Loss: 1.7350006103515625, Learning Rate: 0.01\n",
      "Epoch [9473/20000], Loss: 2.7903900146484375, Learning Rate: 0.01\n",
      "Epoch [9474/20000], Loss: 3.0092315673828125, Learning Rate: 0.01\n",
      "Epoch [9475/20000], Loss: 2.043670654296875, Learning Rate: 0.01\n",
      "Epoch [9476/20000], Loss: -0.030517578125, Learning Rate: 0.01\n",
      "Epoch [9477/20000], Loss: -2.5938720703125, Learning Rate: 0.01\n",
      "Epoch [9478/20000], Loss: -4.7302093505859375, Learning Rate: 0.01\n",
      "Epoch [9479/20000], Loss: -5.7557220458984375, Learning Rate: 0.01\n",
      "Epoch [9480/20000], Loss: -5.5722198486328125, Learning Rate: 0.01\n",
      "Epoch [9481/20000], Loss: -4.643280029296875, Learning Rate: 0.01\n",
      "Epoch [9482/20000], Loss: -3.678802490234375, Learning Rate: 0.01\n",
      "Epoch [9483/20000], Loss: -3.2794189453125, Learning Rate: 0.01\n",
      "Epoch [9484/20000], Loss: -3.656646728515625, Learning Rate: 0.01\n",
      "Epoch [9485/20000], Loss: -4.5682830810546875, Learning Rate: 0.01\n",
      "Epoch [9486/20000], Loss: -5.4962310791015625, Learning Rate: 0.01\n",
      "Epoch [9487/20000], Loss: -5.984619140625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9488/20000], Loss: -5.901031494140625, Learning Rate: 0.01\n",
      "Epoch [9489/20000], Loss: -5.457794189453125, Learning Rate: 0.01\n",
      "Epoch [9490/20000], Loss: -5.0330963134765625, Learning Rate: 0.01\n",
      "Epoch [9491/20000], Loss: -4.925262451171875, Learning Rate: 0.01\n",
      "Epoch [9492/20000], Loss: -5.2014312744140625, Learning Rate: 0.01\n",
      "Epoch [9493/20000], Loss: -5.6852874755859375, Learning Rate: 0.01\n",
      "Epoch [9494/20000], Loss: -6.1012420654296875, Learning Rate: 0.01\n",
      "Epoch [9495/20000], Loss: -6.252838134765625, Learning Rate: 0.01\n",
      "Epoch [9496/20000], Loss: -6.1287078857421875, Learning Rate: 0.01\n",
      "Epoch [9497/20000], Loss: -5.874420166015625, Learning Rate: 0.01\n",
      "Epoch [9498/20000], Loss: -5.6818695068359375, Learning Rate: 0.01\n",
      "Epoch [9499/20000], Loss: -5.6722869873046875, Learning Rate: 0.01\n",
      "Epoch [9500/20000], Loss: -5.8393707275390625, Learning Rate: 0.01\n",
      "Epoch [9501/20000], Loss: -6.076873779296875, Learning Rate: 0.01\n",
      "Epoch [9502/20000], Loss: -6.256195068359375, Learning Rate: 0.01\n",
      "Epoch [9503/20000], Loss: -6.305389404296875, Learning Rate: 0.01\n",
      "Epoch [9504/20000], Loss: -6.2385101318359375, Learning Rate: 0.01\n",
      "Epoch [9505/20000], Loss: -6.1305389404296875, Learning Rate: 0.01\n",
      "Epoch [9506/20000], Loss: -6.062408447265625, Learning Rate: 0.01\n",
      "Epoch [9507/20000], Loss: -6.074951171875, Learning Rate: 0.01\n",
      "Epoch [9508/20000], Loss: -6.1545562744140625, Learning Rate: 0.01\n",
      "Epoch [9509/20000], Loss: -6.252471923828125, Learning Rate: 0.01\n",
      "Epoch [9510/20000], Loss: -6.3185577392578125, Learning Rate: 0.01\n",
      "Epoch [9511/20000], Loss: -6.330596923828125, Learning Rate: 0.01\n",
      "Epoch [9512/20000], Loss: -6.3007354736328125, Learning Rate: 0.01\n",
      "Epoch [9513/20000], Loss: -6.26141357421875, Learning Rate: 0.01\n",
      "Epoch [9514/20000], Loss: -6.24346923828125, Learning Rate: 0.01\n",
      "Epoch [9515/20000], Loss: -6.25970458984375, Learning Rate: 0.01\n",
      "Epoch [9516/20000], Loss: -6.3013153076171875, Learning Rate: 0.01\n",
      "Epoch [9517/20000], Loss: -6.3473968505859375, Learning Rate: 0.01\n",
      "Epoch [9518/20000], Loss: -6.3776702880859375, Learning Rate: 0.01\n",
      "Epoch [9519/20000], Loss: -6.3839874267578125, Learning Rate: 0.01\n",
      "Epoch [9520/20000], Loss: -6.3717193603515625, Learning Rate: 0.01\n",
      "Epoch [9521/20000], Loss: -6.3543853759765625, Learning Rate: 0.01\n",
      "Epoch [9522/20000], Loss: -6.345733642578125, Learning Rate: 0.01\n",
      "Epoch [9523/20000], Loss: -6.352020263671875, Learning Rate: 0.01\n",
      "Epoch [9524/20000], Loss: -6.37115478515625, Learning Rate: 0.01\n",
      "Epoch [9525/20000], Loss: -6.3946990966796875, Learning Rate: 0.01\n",
      "Epoch [9526/20000], Loss: -6.414031982421875, Learning Rate: 0.01\n",
      "Epoch [9527/20000], Loss: -6.4240570068359375, Learning Rate: 0.01\n",
      "Epoch [9528/20000], Loss: -6.42523193359375, Learning Rate: 0.01\n",
      "Epoch [9529/20000], Loss: -6.421783447265625, Learning Rate: 0.01\n",
      "Epoch [9530/20000], Loss: -6.419219970703125, Learning Rate: 0.01\n",
      "Epoch [9531/20000], Loss: -6.4214019775390625, Learning Rate: 0.01\n",
      "Epoch [9532/20000], Loss: -6.4293670654296875, Learning Rate: 0.01\n",
      "Epoch [9533/20000], Loss: -6.440521240234375, Learning Rate: 0.01\n",
      "Epoch [9534/20000], Loss: -6.451904296875, Learning Rate: 0.01\n",
      "Epoch [9535/20000], Loss: -6.460723876953125, Learning Rate: 0.01\n",
      "Epoch [9536/20000], Loss: -6.4657745361328125, Learning Rate: 0.01\n",
      "Epoch [9537/20000], Loss: -6.4680938720703125, Learning Rate: 0.01\n",
      "Epoch [9538/20000], Loss: -6.4691162109375, Learning Rate: 0.01\n",
      "Epoch [9539/20000], Loss: -6.471282958984375, Learning Rate: 0.01\n",
      "Epoch [9540/20000], Loss: -6.4755096435546875, Learning Rate: 0.01\n",
      "Epoch [9541/20000], Loss: -6.4818267822265625, Learning Rate: 0.01\n",
      "Epoch [9542/20000], Loss: -6.4891815185546875, Learning Rate: 0.01\n",
      "Epoch [9543/20000], Loss: -6.4968719482421875, Learning Rate: 0.01\n",
      "Epoch [9544/20000], Loss: -6.5035247802734375, Learning Rate: 0.01\n",
      "Epoch [9545/20000], Loss: -6.5086822509765625, Learning Rate: 0.01\n",
      "Epoch [9546/20000], Loss: -6.512725830078125, Learning Rate: 0.01\n",
      "Epoch [9547/20000], Loss: -6.5162353515625, Learning Rate: 0.01\n",
      "Epoch [9548/20000], Loss: -6.5197906494140625, Learning Rate: 0.01\n",
      "Epoch [9549/20000], Loss: -6.5238189697265625, Learning Rate: 0.01\n",
      "Epoch [9550/20000], Loss: -6.5287628173828125, Learning Rate: 0.01\n",
      "Epoch [9551/20000], Loss: -6.534271240234375, Learning Rate: 0.01\n",
      "Epoch [9552/20000], Loss: -6.539886474609375, Learning Rate: 0.01\n",
      "Epoch [9553/20000], Loss: -6.545623779296875, Learning Rate: 0.01\n",
      "Epoch [9554/20000], Loss: -6.5505828857421875, Learning Rate: 0.01\n",
      "Epoch [9555/20000], Loss: -6.5553131103515625, Learning Rate: 0.01\n",
      "Epoch [9556/20000], Loss: -6.5594940185546875, Learning Rate: 0.01\n",
      "Epoch [9557/20000], Loss: -6.563690185546875, Learning Rate: 0.01\n",
      "Epoch [9558/20000], Loss: -6.5679931640625, Learning Rate: 0.01\n",
      "Epoch [9559/20000], Loss: -6.572418212890625, Learning Rate: 0.01\n",
      "Epoch [9560/20000], Loss: -6.577056884765625, Learning Rate: 0.01\n",
      "Epoch [9561/20000], Loss: -6.58184814453125, Learning Rate: 0.01\n",
      "Epoch [9562/20000], Loss: -6.5871124267578125, Learning Rate: 0.01\n",
      "Epoch [9563/20000], Loss: -6.592041015625, Learning Rate: 0.01\n",
      "Epoch [9564/20000], Loss: -6.596832275390625, Learning Rate: 0.01\n",
      "Epoch [9565/20000], Loss: -6.60162353515625, Learning Rate: 0.01\n",
      "Epoch [9566/20000], Loss: -6.606231689453125, Learning Rate: 0.01\n",
      "Epoch [9567/20000], Loss: -6.6107635498046875, Learning Rate: 0.01\n",
      "Epoch [9568/20000], Loss: -6.615142822265625, Learning Rate: 0.01\n",
      "Epoch [9569/20000], Loss: -6.6195220947265625, Learning Rate: 0.01\n",
      "Epoch [9570/20000], Loss: -6.624114990234375, Learning Rate: 0.01\n",
      "Epoch [9571/20000], Loss: -6.6288604736328125, Learning Rate: 0.01\n",
      "Epoch [9572/20000], Loss: -6.633392333984375, Learning Rate: 0.01\n",
      "Epoch [9573/20000], Loss: -6.6381072998046875, Learning Rate: 0.01\n",
      "Epoch [9574/20000], Loss: -6.6425323486328125, Learning Rate: 0.01\n",
      "Epoch [9575/20000], Loss: -6.647186279296875, Learning Rate: 0.01\n",
      "Epoch [9576/20000], Loss: -6.6517333984375, Learning Rate: 0.01\n",
      "Epoch [9577/20000], Loss: -6.656036376953125, Learning Rate: 0.01\n",
      "Epoch [9578/20000], Loss: -6.660186767578125, Learning Rate: 0.01\n",
      "Epoch [9579/20000], Loss: -6.6642608642578125, Learning Rate: 0.01\n",
      "Epoch [9580/20000], Loss: -6.668243408203125, Learning Rate: 0.01\n",
      "Epoch [9581/20000], Loss: -6.671905517578125, Learning Rate: 0.01\n",
      "Epoch [9582/20000], Loss: -6.6753387451171875, Learning Rate: 0.01\n",
      "Epoch [9583/20000], Loss: -6.6782684326171875, Learning Rate: 0.01\n",
      "Epoch [9584/20000], Loss: -6.680877685546875, Learning Rate: 0.01\n",
      "Epoch [9585/20000], Loss: -6.6826324462890625, Learning Rate: 0.01\n",
      "Epoch [9586/20000], Loss: -6.683319091796875, Learning Rate: 0.01\n",
      "Epoch [9587/20000], Loss: -6.682464599609375, Learning Rate: 0.01\n",
      "Epoch [9588/20000], Loss: -6.6795501708984375, Learning Rate: 0.01\n",
      "Epoch [9589/20000], Loss: -6.673797607421875, Learning Rate: 0.01\n",
      "Epoch [9590/20000], Loss: -6.6641387939453125, Learning Rate: 0.01\n",
      "Epoch [9591/20000], Loss: -6.6487884521484375, Learning Rate: 0.01\n",
      "Epoch [9592/20000], Loss: -6.6254425048828125, Learning Rate: 0.01\n",
      "Epoch [9593/20000], Loss: -6.5908660888671875, Learning Rate: 0.01\n",
      "Epoch [9594/20000], Loss: -6.5404052734375, Learning Rate: 0.01\n",
      "Epoch [9595/20000], Loss: -6.4679718017578125, Learning Rate: 0.01\n",
      "Epoch [9596/20000], Loss: -6.364471435546875, Learning Rate: 0.01\n",
      "Epoch [9597/20000], Loss: -6.2198028564453125, Learning Rate: 0.01\n",
      "Epoch [9598/20000], Loss: -6.01898193359375, Learning Rate: 0.01\n",
      "Epoch [9599/20000], Loss: -5.7497711181640625, Learning Rate: 0.01\n",
      "Epoch [9600/20000], Loss: -5.397186279296875, Learning Rate: 0.01\n",
      "Epoch [9601/20000], Loss: -4.965576171875, Learning Rate: 0.01\n",
      "Epoch [9602/20000], Loss: -4.4694061279296875, Learning Rate: 0.01\n",
      "Epoch [9603/20000], Loss: -3.9846649169921875, Learning Rate: 0.01\n",
      "Epoch [9604/20000], Loss: -3.607757568359375, Learning Rate: 0.01\n",
      "Epoch [9605/20000], Loss: -3.5041046142578125, Learning Rate: 0.01\n",
      "Epoch [9606/20000], Loss: -3.768310546875, Learning Rate: 0.01\n",
      "Epoch [9607/20000], Loss: -4.4241180419921875, Learning Rate: 0.01\n",
      "Epoch [9608/20000], Loss: -5.288848876953125, Learning Rate: 0.01\n",
      "Epoch [9609/20000], Loss: -6.0940093994140625, Learning Rate: 0.01\n",
      "Epoch [9610/20000], Loss: -6.585601806640625, Learning Rate: 0.01\n",
      "Epoch [9611/20000], Loss: -6.67169189453125, Learning Rate: 0.01\n",
      "Epoch [9612/20000], Loss: -6.4411468505859375, Learning Rate: 0.01\n",
      "Epoch [9613/20000], Loss: -6.0937957763671875, Learning Rate: 0.01\n",
      "Epoch [9614/20000], Loss: -5.842193603515625, Learning Rate: 0.01\n",
      "Epoch [9615/20000], Loss: -5.8156280517578125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9616/20000], Loss: -6.0272979736328125, Learning Rate: 0.01\n",
      "Epoch [9617/20000], Loss: -6.36468505859375, Learning Rate: 0.01\n",
      "Epoch [9618/20000], Loss: -6.671630859375, Learning Rate: 0.01\n",
      "Epoch [9619/20000], Loss: -6.8247833251953125, Learning Rate: 0.01\n",
      "Epoch [9620/20000], Loss: -6.7957763671875, Learning Rate: 0.01\n",
      "Epoch [9621/20000], Loss: -6.6479644775390625, Learning Rate: 0.01\n",
      "Epoch [9622/20000], Loss: -6.48809814453125, Learning Rate: 0.01\n",
      "Epoch [9623/20000], Loss: -6.4092864990234375, Learning Rate: 0.01\n",
      "Epoch [9624/20000], Loss: -6.447509765625, Learning Rate: 0.01\n",
      "Epoch [9625/20000], Loss: -6.5771942138671875, Learning Rate: 0.01\n",
      "Epoch [9626/20000], Loss: -6.7307586669921875, Learning Rate: 0.01\n",
      "Epoch [9627/20000], Loss: -6.8419036865234375, Learning Rate: 0.01\n",
      "Epoch [9628/20000], Loss: -6.87640380859375, Learning Rate: 0.01\n",
      "Epoch [9629/20000], Loss: -6.843994140625, Learning Rate: 0.01\n",
      "Epoch [9630/20000], Loss: -6.7839813232421875, Learning Rate: 0.01\n",
      "Epoch [9631/20000], Loss: -6.7398223876953125, Learning Rate: 0.01\n",
      "Epoch [9632/20000], Loss: -6.7386627197265625, Learning Rate: 0.01\n",
      "Epoch [9633/20000], Loss: -6.7800140380859375, Learning Rate: 0.01\n",
      "Epoch [9634/20000], Loss: -6.8435516357421875, Learning Rate: 0.01\n",
      "Epoch [9635/20000], Loss: -6.90020751953125, Learning Rate: 0.01\n",
      "Epoch [9636/20000], Loss: -6.9300384521484375, Learning Rate: 0.01\n",
      "Epoch [9637/20000], Loss: -6.9286651611328125, Learning Rate: 0.01\n",
      "Epoch [9638/20000], Loss: -6.9071197509765625, Learning Rate: 0.01\n",
      "Epoch [9639/20000], Loss: -6.882568359375, Learning Rate: 0.01\n",
      "Epoch [9640/20000], Loss: -6.8708648681640625, Learning Rate: 0.01\n",
      "Epoch [9641/20000], Loss: -6.87811279296875, Learning Rate: 0.01\n",
      "Epoch [9642/20000], Loss: -6.901123046875, Learning Rate: 0.01\n",
      "Epoch [9643/20000], Loss: -6.929656982421875, Learning Rate: 0.01\n",
      "Epoch [9644/20000], Loss: -6.953704833984375, Learning Rate: 0.01\n",
      "Epoch [9645/20000], Loss: -6.9666290283203125, Learning Rate: 0.01\n",
      "Epoch [9646/20000], Loss: -6.9681243896484375, Learning Rate: 0.01\n",
      "Epoch [9647/20000], Loss: -6.9628753662109375, Learning Rate: 0.01\n",
      "Epoch [9648/20000], Loss: -6.9571990966796875, Learning Rate: 0.01\n",
      "Epoch [9649/20000], Loss: -6.95660400390625, Learning Rate: 0.01\n",
      "Epoch [9650/20000], Loss: -6.9628143310546875, Learning Rate: 0.01\n",
      "Epoch [9651/20000], Loss: -6.9748382568359375, Learning Rate: 0.01\n",
      "Epoch [9652/20000], Loss: -6.989349365234375, Learning Rate: 0.01\n",
      "Epoch [9653/20000], Loss: -7.002288818359375, Learning Rate: 0.01\n",
      "Epoch [9654/20000], Loss: -7.01116943359375, Learning Rate: 0.01\n",
      "Epoch [9655/20000], Loss: -7.0159759521484375, Learning Rate: 0.01\n",
      "Epoch [9656/20000], Loss: -7.0174560546875, Learning Rate: 0.01\n",
      "Epoch [9657/20000], Loss: -7.017913818359375, Learning Rate: 0.01\n",
      "Epoch [9658/20000], Loss: -7.0196533203125, Learning Rate: 0.01\n",
      "Epoch [9659/20000], Loss: -7.0235137939453125, Learning Rate: 0.01\n",
      "Epoch [9660/20000], Loss: -7.0295562744140625, Learning Rate: 0.01\n",
      "Epoch [9661/20000], Loss: -7.0371551513671875, Learning Rate: 0.01\n",
      "Epoch [9662/20000], Loss: -7.0450286865234375, Learning Rate: 0.01\n",
      "Epoch [9663/20000], Loss: -7.05194091796875, Learning Rate: 0.01\n",
      "Epoch [9664/20000], Loss: -7.0572357177734375, Learning Rate: 0.01\n",
      "Epoch [9665/20000], Loss: -7.0612030029296875, Learning Rate: 0.01\n",
      "Epoch [9666/20000], Loss: -7.0639190673828125, Learning Rate: 0.01\n",
      "Epoch [9667/20000], Loss: -7.06622314453125, Learning Rate: 0.01\n",
      "Epoch [9668/20000], Loss: -7.0685577392578125, Learning Rate: 0.01\n",
      "Epoch [9669/20000], Loss: -7.0714111328125, Learning Rate: 0.01\n",
      "Epoch [9670/20000], Loss: -7.07470703125, Learning Rate: 0.01\n",
      "Epoch [9671/20000], Loss: -7.0780487060546875, Learning Rate: 0.01\n",
      "Epoch [9672/20000], Loss: -7.08087158203125, Learning Rate: 0.01\n",
      "Epoch [9673/20000], Loss: -7.0829315185546875, Learning Rate: 0.01\n",
      "Epoch [9674/20000], Loss: -7.083404541015625, Learning Rate: 0.01\n",
      "Epoch [9675/20000], Loss: -7.0821075439453125, Learning Rate: 0.01\n",
      "Epoch [9676/20000], Loss: -7.078125, Learning Rate: 0.01\n",
      "Epoch [9677/20000], Loss: -7.0716400146484375, Learning Rate: 0.01\n",
      "Epoch [9678/20000], Loss: -7.0613861083984375, Learning Rate: 0.01\n",
      "Epoch [9679/20000], Loss: -7.0465240478515625, Learning Rate: 0.01\n",
      "Epoch [9680/20000], Loss: -7.025360107421875, Learning Rate: 0.01\n",
      "Epoch [9681/20000], Loss: -6.995452880859375, Learning Rate: 0.01\n",
      "Epoch [9682/20000], Loss: -6.953704833984375, Learning Rate: 0.01\n",
      "Epoch [9683/20000], Loss: -6.8952789306640625, Learning Rate: 0.01\n",
      "Epoch [9684/20000], Loss: -6.81365966796875, Learning Rate: 0.01\n",
      "Epoch [9685/20000], Loss: -6.7006378173828125, Learning Rate: 0.01\n",
      "Epoch [9686/20000], Loss: -6.5451507568359375, Learning Rate: 0.01\n",
      "Epoch [9687/20000], Loss: -6.33245849609375, Learning Rate: 0.01\n",
      "Epoch [9688/20000], Loss: -6.0445098876953125, Learning Rate: 0.01\n",
      "Epoch [9689/20000], Loss: -5.6605987548828125, Learning Rate: 0.01\n",
      "Epoch [9690/20000], Loss: -5.1595611572265625, Learning Rate: 0.01\n",
      "Epoch [9691/20000], Loss: -4.5252227783203125, Learning Rate: 0.01\n",
      "Epoch [9692/20000], Loss: -3.7603607177734375, Learning Rate: 0.01\n",
      "Epoch [9693/20000], Loss: -2.9004058837890625, Learning Rate: 0.01\n",
      "Epoch [9694/20000], Loss: -2.041015625, Learning Rate: 0.01\n",
      "Epoch [9695/20000], Loss: -1.3435516357421875, Learning Rate: 0.01\n",
      "Epoch [9696/20000], Loss: -1.0279083251953125, Learning Rate: 0.01\n",
      "Epoch [9697/20000], Loss: -1.293182373046875, Learning Rate: 0.01\n",
      "Epoch [9698/20000], Loss: -2.2118682861328125, Learning Rate: 0.01\n",
      "Epoch [9699/20000], Loss: -3.626190185546875, Learning Rate: 0.01\n",
      "Epoch [9700/20000], Loss: -5.1666717529296875, Learning Rate: 0.01\n",
      "Epoch [9701/20000], Loss: -6.4030303955078125, Learning Rate: 0.01\n",
      "Epoch [9702/20000], Loss: -7.0444488525390625, Learning Rate: 0.01\n",
      "Epoch [9703/20000], Loss: -7.0535736083984375, Learning Rate: 0.01\n",
      "Epoch [9704/20000], Loss: -6.620849609375, Learning Rate: 0.01\n",
      "Epoch [9705/20000], Loss: -6.04937744140625, Learning Rate: 0.01\n",
      "Epoch [9706/20000], Loss: -5.6247711181640625, Learning Rate: 0.01\n",
      "Epoch [9707/20000], Loss: -5.521148681640625, Learning Rate: 0.01\n",
      "Epoch [9708/20000], Loss: -5.754241943359375, Learning Rate: 0.01\n",
      "Epoch [9709/20000], Loss: -6.200408935546875, Learning Rate: 0.01\n",
      "Epoch [9710/20000], Loss: -6.669708251953125, Learning Rate: 0.01\n",
      "Epoch [9711/20000], Loss: -6.9984893798828125, Learning Rate: 0.01\n",
      "Epoch [9712/20000], Loss: -7.1159515380859375, Learning Rate: 0.01\n",
      "Epoch [9713/20000], Loss: -7.052764892578125, Learning Rate: 0.01\n",
      "Epoch [9714/20000], Loss: -6.9046783447265625, Learning Rate: 0.01\n",
      "Epoch [9715/20000], Loss: -6.7755889892578125, Learning Rate: 0.01\n",
      "Epoch [9716/20000], Loss: -6.7335968017578125, Learning Rate: 0.01\n",
      "Epoch [9717/20000], Loss: -6.790924072265625, Learning Rate: 0.01\n",
      "Epoch [9718/20000], Loss: -6.9127655029296875, Learning Rate: 0.01\n",
      "Epoch [9719/20000], Loss: -7.0450286865234375, Learning Rate: 0.01\n",
      "Epoch [9720/20000], Loss: -7.1423492431640625, Learning Rate: 0.01\n",
      "Epoch [9721/20000], Loss: -7.186309814453125, Learning Rate: 0.01\n",
      "Epoch [9722/20000], Loss: -7.186248779296875, Learning Rate: 0.01\n",
      "Epoch [9723/20000], Loss: -7.1674652099609375, Learning Rate: 0.01\n",
      "Epoch [9724/20000], Loss: -7.154693603515625, Learning Rate: 0.01\n",
      "Epoch [9725/20000], Loss: -7.1616668701171875, Learning Rate: 0.01\n",
      "Epoch [9726/20000], Loss: -7.188018798828125, Learning Rate: 0.01\n",
      "Epoch [9727/20000], Loss: -7.223388671875, Learning Rate: 0.01\n",
      "Epoch [9728/20000], Loss: -7.2553558349609375, Learning Rate: 0.01\n",
      "Epoch [9729/20000], Loss: -7.2754058837890625, Learning Rate: 0.01\n",
      "Epoch [9730/20000], Loss: -7.2839813232421875, Learning Rate: 0.01\n",
      "Epoch [9731/20000], Loss: -7.286346435546875, Learning Rate: 0.01\n",
      "Epoch [9732/20000], Loss: -7.2900848388671875, Learning Rate: 0.01\n",
      "Epoch [9733/20000], Loss: -7.3003692626953125, Learning Rate: 0.01\n",
      "Epoch [9734/20000], Loss: -7.31781005859375, Learning Rate: 0.01\n",
      "Epoch [9735/20000], Loss: -7.338775634765625, Learning Rate: 0.01\n",
      "Epoch [9736/20000], Loss: -7.358123779296875, Learning Rate: 0.01\n",
      "Epoch [9737/20000], Loss: -7.3714599609375, Learning Rate: 0.01\n",
      "Epoch [9738/20000], Loss: -7.376983642578125, Learning Rate: 0.01\n",
      "Epoch [9739/20000], Loss: -7.3768157958984375, Learning Rate: 0.01\n",
      "Epoch [9740/20000], Loss: -7.3743438720703125, Learning Rate: 0.01\n",
      "Epoch [9741/20000], Loss: -7.374176025390625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9742/20000], Loss: -7.378875732421875, Learning Rate: 0.01\n",
      "Epoch [9743/20000], Loss: -7.3892364501953125, Learning Rate: 0.01\n",
      "Epoch [9744/20000], Loss: -7.40338134765625, Learning Rate: 0.01\n",
      "Epoch [9745/20000], Loss: -7.4185028076171875, Learning Rate: 0.01\n",
      "Epoch [9746/20000], Loss: -7.431549072265625, Learning Rate: 0.01\n",
      "Epoch [9747/20000], Loss: -7.4409027099609375, Learning Rate: 0.01\n",
      "Epoch [9748/20000], Loss: -7.44580078125, Learning Rate: 0.01\n",
      "Epoch [9749/20000], Loss: -7.447265625, Learning Rate: 0.01\n",
      "Epoch [9750/20000], Loss: -7.4471893310546875, Learning Rate: 0.01\n",
      "Epoch [9751/20000], Loss: -7.4472808837890625, Learning Rate: 0.01\n",
      "Epoch [9752/20000], Loss: -7.449249267578125, Learning Rate: 0.01\n",
      "Epoch [9753/20000], Loss: -7.4540252685546875, Learning Rate: 0.01\n",
      "Epoch [9754/20000], Loss: -7.4611358642578125, Learning Rate: 0.01\n",
      "Epoch [9755/20000], Loss: -7.4699249267578125, Learning Rate: 0.01\n",
      "Epoch [9756/20000], Loss: -7.478851318359375, Learning Rate: 0.01\n",
      "Epoch [9757/20000], Loss: -7.4871978759765625, Learning Rate: 0.01\n",
      "Epoch [9758/20000], Loss: -7.49420166015625, Learning Rate: 0.01\n",
      "Epoch [9759/20000], Loss: -7.4995880126953125, Learning Rate: 0.01\n",
      "Epoch [9760/20000], Loss: -7.5033721923828125, Learning Rate: 0.01\n",
      "Epoch [9761/20000], Loss: -7.506439208984375, Learning Rate: 0.01\n",
      "Epoch [9762/20000], Loss: -7.50933837890625, Learning Rate: 0.01\n",
      "Epoch [9763/20000], Loss: -7.51226806640625, Learning Rate: 0.01\n",
      "Epoch [9764/20000], Loss: -7.5159912109375, Learning Rate: 0.01\n",
      "Epoch [9765/20000], Loss: -7.5202484130859375, Learning Rate: 0.01\n",
      "Epoch [9766/20000], Loss: -7.5252838134765625, Learning Rate: 0.01\n",
      "Epoch [9767/20000], Loss: -7.5308837890625, Learning Rate: 0.01\n",
      "Epoch [9768/20000], Loss: -7.53656005859375, Learning Rate: 0.01\n",
      "Epoch [9769/20000], Loss: -7.5424041748046875, Learning Rate: 0.01\n",
      "Epoch [9770/20000], Loss: -7.5477142333984375, Learning Rate: 0.01\n",
      "Epoch [9771/20000], Loss: -7.5529022216796875, Learning Rate: 0.01\n",
      "Epoch [9772/20000], Loss: -7.5576934814453125, Learning Rate: 0.01\n",
      "Epoch [9773/20000], Loss: -7.5621337890625, Learning Rate: 0.01\n",
      "Epoch [9774/20000], Loss: -7.5664520263671875, Learning Rate: 0.01\n",
      "Epoch [9775/20000], Loss: -7.570648193359375, Learning Rate: 0.01\n",
      "Epoch [9776/20000], Loss: -7.5748443603515625, Learning Rate: 0.01\n",
      "Epoch [9777/20000], Loss: -7.5791015625, Learning Rate: 0.01\n",
      "Epoch [9778/20000], Loss: -7.583465576171875, Learning Rate: 0.01\n",
      "Epoch [9779/20000], Loss: -7.5878753662109375, Learning Rate: 0.01\n",
      "Epoch [9780/20000], Loss: -7.5926055908203125, Learning Rate: 0.01\n",
      "Epoch [9781/20000], Loss: -7.5971527099609375, Learning Rate: 0.01\n",
      "Epoch [9782/20000], Loss: -7.6018218994140625, Learning Rate: 0.01\n",
      "Epoch [9783/20000], Loss: -7.6064605712890625, Learning Rate: 0.01\n",
      "Epoch [9784/20000], Loss: -7.6111602783203125, Learning Rate: 0.01\n",
      "Epoch [9785/20000], Loss: -7.61578369140625, Learning Rate: 0.01\n",
      "Epoch [9786/20000], Loss: -7.62030029296875, Learning Rate: 0.01\n",
      "Epoch [9787/20000], Loss: -7.6248779296875, Learning Rate: 0.01\n",
      "Epoch [9788/20000], Loss: -7.6293792724609375, Learning Rate: 0.01\n",
      "Epoch [9789/20000], Loss: -7.633880615234375, Learning Rate: 0.01\n",
      "Epoch [9790/20000], Loss: -7.638275146484375, Learning Rate: 0.01\n",
      "Epoch [9791/20000], Loss: -7.642974853515625, Learning Rate: 0.01\n",
      "Epoch [9792/20000], Loss: -7.6474761962890625, Learning Rate: 0.01\n",
      "Epoch [9793/20000], Loss: -7.6520233154296875, Learning Rate: 0.01\n",
      "Epoch [9794/20000], Loss: -7.6565704345703125, Learning Rate: 0.01\n",
      "Epoch [9795/20000], Loss: -7.6610870361328125, Learning Rate: 0.01\n",
      "Epoch [9796/20000], Loss: -7.6656341552734375, Learning Rate: 0.01\n",
      "Epoch [9797/20000], Loss: -7.6702423095703125, Learning Rate: 0.01\n",
      "Epoch [9798/20000], Loss: -7.67474365234375, Learning Rate: 0.01\n",
      "Epoch [9799/20000], Loss: -7.67938232421875, Learning Rate: 0.01\n",
      "Epoch [9800/20000], Loss: -7.68377685546875, Learning Rate: 0.01\n",
      "Epoch [9801/20000], Loss: -7.6883544921875, Learning Rate: 0.01\n",
      "Epoch [9802/20000], Loss: -7.69287109375, Learning Rate: 0.01\n",
      "Epoch [9803/20000], Loss: -7.6974029541015625, Learning Rate: 0.01\n",
      "Epoch [9804/20000], Loss: -7.7019195556640625, Learning Rate: 0.01\n",
      "Epoch [9805/20000], Loss: -7.706573486328125, Learning Rate: 0.01\n",
      "Epoch [9806/20000], Loss: -7.711029052734375, Learning Rate: 0.01\n",
      "Epoch [9807/20000], Loss: -7.7154388427734375, Learning Rate: 0.01\n",
      "Epoch [9808/20000], Loss: -7.719940185546875, Learning Rate: 0.01\n",
      "Epoch [9809/20000], Loss: -7.724517822265625, Learning Rate: 0.01\n",
      "Epoch [9810/20000], Loss: -7.729034423828125, Learning Rate: 0.01\n",
      "Epoch [9811/20000], Loss: -7.733489990234375, Learning Rate: 0.01\n",
      "Epoch [9812/20000], Loss: -7.737884521484375, Learning Rate: 0.01\n",
      "Epoch [9813/20000], Loss: -7.7425384521484375, Learning Rate: 0.01\n",
      "Epoch [9814/20000], Loss: -7.74700927734375, Learning Rate: 0.01\n",
      "Epoch [9815/20000], Loss: -7.7515411376953125, Learning Rate: 0.01\n",
      "Epoch [9816/20000], Loss: -7.7559051513671875, Learning Rate: 0.01\n",
      "Epoch [9817/20000], Loss: -7.7605133056640625, Learning Rate: 0.01\n",
      "Epoch [9818/20000], Loss: -7.7649688720703125, Learning Rate: 0.01\n",
      "Epoch [9819/20000], Loss: -7.769500732421875, Learning Rate: 0.01\n",
      "Epoch [9820/20000], Loss: -7.77392578125, Learning Rate: 0.01\n",
      "Epoch [9821/20000], Loss: -7.778350830078125, Learning Rate: 0.01\n",
      "Epoch [9822/20000], Loss: -7.78277587890625, Learning Rate: 0.01\n",
      "Epoch [9823/20000], Loss: -7.78729248046875, Learning Rate: 0.01\n",
      "Epoch [9824/20000], Loss: -7.7915802001953125, Learning Rate: 0.01\n",
      "Epoch [9825/20000], Loss: -7.7958984375, Learning Rate: 0.01\n",
      "Epoch [9826/20000], Loss: -7.79998779296875, Learning Rate: 0.01\n",
      "Epoch [9827/20000], Loss: -7.8042755126953125, Learning Rate: 0.01\n",
      "Epoch [9828/20000], Loss: -7.808349609375, Learning Rate: 0.01\n",
      "Epoch [9829/20000], Loss: -7.81207275390625, Learning Rate: 0.01\n",
      "Epoch [9830/20000], Loss: -7.8156890869140625, Learning Rate: 0.01\n",
      "Epoch [9831/20000], Loss: -7.8190155029296875, Learning Rate: 0.01\n",
      "Epoch [9832/20000], Loss: -7.821563720703125, Learning Rate: 0.01\n",
      "Epoch [9833/20000], Loss: -7.823516845703125, Learning Rate: 0.01\n",
      "Epoch [9834/20000], Loss: -7.8244171142578125, Learning Rate: 0.01\n",
      "Epoch [9835/20000], Loss: -7.823577880859375, Learning Rate: 0.01\n",
      "Epoch [9836/20000], Loss: -7.820526123046875, Learning Rate: 0.01\n",
      "Epoch [9837/20000], Loss: -7.8142852783203125, Learning Rate: 0.01\n",
      "Epoch [9838/20000], Loss: -7.80303955078125, Learning Rate: 0.01\n",
      "Epoch [9839/20000], Loss: -7.784576416015625, Learning Rate: 0.01\n",
      "Epoch [9840/20000], Loss: -7.755462646484375, Learning Rate: 0.01\n",
      "Epoch [9841/20000], Loss: -7.71075439453125, Learning Rate: 0.01\n",
      "Epoch [9842/20000], Loss: -7.6427154541015625, Learning Rate: 0.01\n",
      "Epoch [9843/20000], Loss: -7.5404815673828125, Learning Rate: 0.01\n",
      "Epoch [9844/20000], Loss: -7.387786865234375, Learning Rate: 0.01\n",
      "Epoch [9845/20000], Loss: -7.16253662109375, Learning Rate: 0.01\n",
      "Epoch [9846/20000], Loss: -6.8331451416015625, Learning Rate: 0.01\n",
      "Epoch [9847/20000], Loss: -6.3634490966796875, Learning Rate: 0.01\n",
      "Epoch [9848/20000], Loss: -5.7095184326171875, Learning Rate: 0.01\n",
      "Epoch [9849/20000], Loss: -4.84844970703125, Learning Rate: 0.01\n",
      "Epoch [9850/20000], Loss: -3.78704833984375, Learning Rate: 0.01\n",
      "Epoch [9851/20000], Loss: -2.645111083984375, Learning Rate: 0.01\n",
      "Epoch [9852/20000], Loss: -1.6399688720703125, Learning Rate: 0.01\n",
      "Epoch [9853/20000], Loss: -1.13201904296875, Learning Rate: 0.01\n",
      "Epoch [9854/20000], Loss: -1.3613433837890625, Learning Rate: 0.01\n",
      "Epoch [9855/20000], Loss: -2.3039703369140625, Learning Rate: 0.01\n",
      "Epoch [9856/20000], Loss: -3.4839324951171875, Learning Rate: 0.01\n",
      "Epoch [9857/20000], Loss: -4.285614013671875, Learning Rate: 0.01\n",
      "Epoch [9858/20000], Loss: -4.3522186279296875, Learning Rate: 0.01\n",
      "Epoch [9859/20000], Loss: -3.844573974609375, Learning Rate: 0.01\n",
      "Epoch [9860/20000], Loss: -3.37872314453125, Learning Rate: 0.01\n",
      "Epoch [9861/20000], Loss: -3.6002197265625, Learning Rate: 0.01\n",
      "Epoch [9862/20000], Loss: -4.7884979248046875, Learning Rate: 0.01\n",
      "Epoch [9863/20000], Loss: -6.443450927734375, Learning Rate: 0.01\n",
      "Epoch [9864/20000], Loss: -7.6923828125, Learning Rate: 0.01\n",
      "Epoch [9865/20000], Loss: -7.9400634765625, Learning Rate: 0.01\n",
      "Epoch [9866/20000], Loss: -7.32867431640625, Learning Rate: 0.01\n",
      "Epoch [9867/20000], Loss: -6.525543212890625, Learning Rate: 0.01\n",
      "Epoch [9868/20000], Loss: -6.165924072265625, Learning Rate: 0.01\n",
      "Epoch [9869/20000], Loss: -6.43914794921875, Learning Rate: 0.01\n",
      "Epoch [9870/20000], Loss: -7.013946533203125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9871/20000], Loss: -7.4237060546875, Learning Rate: 0.01\n",
      "Epoch [9872/20000], Loss: -7.4647216796875, Learning Rate: 0.01\n",
      "Epoch [9873/20000], Loss: -7.3116302490234375, Learning Rate: 0.01\n",
      "Epoch [9874/20000], Loss: -7.2719879150390625, Learning Rate: 0.01\n",
      "Epoch [9875/20000], Loss: -7.4751434326171875, Learning Rate: 0.01\n",
      "Epoch [9876/20000], Loss: -7.780364990234375, Learning Rate: 0.01\n",
      "Epoch [9877/20000], Loss: -7.9385528564453125, Learning Rate: 0.01\n",
      "Epoch [9878/20000], Loss: -7.8425140380859375, Learning Rate: 0.01\n",
      "Epoch [9879/20000], Loss: -7.611602783203125, Learning Rate: 0.01\n",
      "Epoch [9880/20000], Loss: -7.4651031494140625, Learning Rate: 0.01\n",
      "Epoch [9881/20000], Loss: -7.528411865234375, Learning Rate: 0.01\n",
      "Epoch [9882/20000], Loss: -7.7433624267578125, Learning Rate: 0.01\n",
      "Epoch [9883/20000], Loss: -7.94696044921875, Learning Rate: 0.01\n",
      "Epoch [9884/20000], Loss: -8.019973754882812, Learning Rate: 0.01\n",
      "Epoch [9885/20000], Loss: -7.9710540771484375, Learning Rate: 0.01\n",
      "Epoch [9886/20000], Loss: -7.898956298828125, Learning Rate: 0.01\n",
      "Epoch [9887/20000], Loss: -7.8875274658203125, Learning Rate: 0.01\n",
      "Epoch [9888/20000], Loss: -7.9414520263671875, Learning Rate: 0.01\n",
      "Epoch [9889/20000], Loss: -8.001922607421875, Learning Rate: 0.01\n",
      "Epoch [9890/20000], Loss: -8.015640258789062, Learning Rate: 0.01\n",
      "Epoch [9891/20000], Loss: -7.9829254150390625, Learning Rate: 0.01\n",
      "Epoch [9892/20000], Loss: -7.950042724609375, Learning Rate: 0.01\n",
      "Epoch [9893/20000], Loss: -7.9600372314453125, Learning Rate: 0.01\n",
      "Epoch [9894/20000], Loss: -8.015625, Learning Rate: 0.01\n",
      "Epoch [9895/20000], Loss: -8.081436157226562, Learning Rate: 0.01\n",
      "Epoch [9896/20000], Loss: -8.117752075195312, Learning Rate: 0.01\n",
      "Epoch [9897/20000], Loss: -8.112991333007812, Learning Rate: 0.01\n",
      "Epoch [9898/20000], Loss: -8.087112426757812, Learning Rate: 0.01\n",
      "Epoch [9899/20000], Loss: -8.069686889648438, Learning Rate: 0.01\n",
      "Epoch [9900/20000], Loss: -8.0758056640625, Learning Rate: 0.01\n",
      "Epoch [9901/20000], Loss: -8.0975341796875, Learning Rate: 0.01\n",
      "Epoch [9902/20000], Loss: -8.117233276367188, Learning Rate: 0.01\n",
      "Epoch [9903/20000], Loss: -8.123626708984375, Learning Rate: 0.01\n",
      "Epoch [9904/20000], Loss: -8.120498657226562, Learning Rate: 0.01\n",
      "Epoch [9905/20000], Loss: -8.11968994140625, Learning Rate: 0.01\n",
      "Epoch [9906/20000], Loss: -8.129623413085938, Learning Rate: 0.01\n",
      "Epoch [9907/20000], Loss: -8.148284912109375, Learning Rate: 0.01\n",
      "Epoch [9908/20000], Loss: -8.16650390625, Learning Rate: 0.01\n",
      "Epoch [9909/20000], Loss: -8.176025390625, Learning Rate: 0.01\n",
      "Epoch [9910/20000], Loss: -8.1756591796875, Learning Rate: 0.01\n",
      "Epoch [9911/20000], Loss: -8.171371459960938, Learning Rate: 0.01\n",
      "Epoch [9912/20000], Loss: -8.170135498046875, Learning Rate: 0.01\n",
      "Epoch [9913/20000], Loss: -8.175140380859375, Learning Rate: 0.01\n",
      "Epoch [9914/20000], Loss: -8.184173583984375, Learning Rate: 0.01\n",
      "Epoch [9915/20000], Loss: -8.192916870117188, Learning Rate: 0.01\n",
      "Epoch [9916/20000], Loss: -8.198211669921875, Learning Rate: 0.01\n",
      "Epoch [9917/20000], Loss: -8.201065063476562, Learning Rate: 0.01\n",
      "Epoch [9918/20000], Loss: -8.203643798828125, Learning Rate: 0.01\n",
      "Epoch [9919/20000], Loss: -8.208770751953125, Learning Rate: 0.01\n",
      "Epoch [9920/20000], Loss: -8.216293334960938, Learning Rate: 0.01\n",
      "Epoch [9921/20000], Loss: -8.224533081054688, Learning Rate: 0.01\n",
      "Epoch [9922/20000], Loss: -8.231216430664062, Learning Rate: 0.01\n",
      "Epoch [9923/20000], Loss: -8.235382080078125, Learning Rate: 0.01\n",
      "Epoch [9924/20000], Loss: -8.237991333007812, Learning Rate: 0.01\n",
      "Epoch [9925/20000], Loss: -8.240615844726562, Learning Rate: 0.01\n",
      "Epoch [9926/20000], Loss: -8.244369506835938, Learning Rate: 0.01\n",
      "Epoch [9927/20000], Loss: -8.249160766601562, Learning Rate: 0.01\n",
      "Epoch [9928/20000], Loss: -8.254669189453125, Learning Rate: 0.01\n",
      "Epoch [9929/20000], Loss: -8.259368896484375, Learning Rate: 0.01\n",
      "Epoch [9930/20000], Loss: -8.263519287109375, Learning Rate: 0.01\n",
      "Epoch [9931/20000], Loss: -8.26690673828125, Learning Rate: 0.01\n",
      "Epoch [9932/20000], Loss: -8.270782470703125, Learning Rate: 0.01\n",
      "Epoch [9933/20000], Loss: -8.27532958984375, Learning Rate: 0.01\n",
      "Epoch [9934/20000], Loss: -8.2803955078125, Learning Rate: 0.01\n",
      "Epoch [9935/20000], Loss: -8.28570556640625, Learning Rate: 0.01\n",
      "Epoch [9936/20000], Loss: -8.29052734375, Learning Rate: 0.01\n",
      "Epoch [9937/20000], Loss: -8.294998168945312, Learning Rate: 0.01\n",
      "Epoch [9938/20000], Loss: -8.299224853515625, Learning Rate: 0.01\n",
      "Epoch [9939/20000], Loss: -8.303253173828125, Learning Rate: 0.01\n",
      "Epoch [9940/20000], Loss: -8.307601928710938, Learning Rate: 0.01\n",
      "Epoch [9941/20000], Loss: -8.312210083007812, Learning Rate: 0.01\n",
      "Epoch [9942/20000], Loss: -8.316741943359375, Learning Rate: 0.01\n",
      "Epoch [9943/20000], Loss: -8.3211669921875, Learning Rate: 0.01\n",
      "Epoch [9944/20000], Loss: -8.32562255859375, Learning Rate: 0.01\n",
      "Epoch [9945/20000], Loss: -8.329635620117188, Learning Rate: 0.01\n",
      "Epoch [9946/20000], Loss: -8.333648681640625, Learning Rate: 0.01\n",
      "Epoch [9947/20000], Loss: -8.337753295898438, Learning Rate: 0.01\n",
      "Epoch [9948/20000], Loss: -8.342086791992188, Learning Rate: 0.01\n",
      "Epoch [9949/20000], Loss: -8.3465576171875, Learning Rate: 0.01\n",
      "Epoch [9950/20000], Loss: -8.350860595703125, Learning Rate: 0.01\n",
      "Epoch [9951/20000], Loss: -8.355255126953125, Learning Rate: 0.01\n",
      "Epoch [9952/20000], Loss: -8.359466552734375, Learning Rate: 0.01\n",
      "Epoch [9953/20000], Loss: -8.363662719726562, Learning Rate: 0.01\n",
      "Epoch [9954/20000], Loss: -8.367843627929688, Learning Rate: 0.01\n",
      "Epoch [9955/20000], Loss: -8.372116088867188, Learning Rate: 0.01\n",
      "Epoch [9956/20000], Loss: -8.376434326171875, Learning Rate: 0.01\n",
      "Epoch [9957/20000], Loss: -8.380722045898438, Learning Rate: 0.01\n",
      "Epoch [9958/20000], Loss: -8.384979248046875, Learning Rate: 0.01\n",
      "Epoch [9959/20000], Loss: -8.389251708984375, Learning Rate: 0.01\n",
      "Epoch [9960/20000], Loss: -8.393569946289062, Learning Rate: 0.01\n",
      "Epoch [9961/20000], Loss: -8.397781372070312, Learning Rate: 0.01\n",
      "Epoch [9962/20000], Loss: -8.402053833007812, Learning Rate: 0.01\n",
      "Epoch [9963/20000], Loss: -8.406356811523438, Learning Rate: 0.01\n",
      "Epoch [9964/20000], Loss: -8.410629272460938, Learning Rate: 0.01\n",
      "Epoch [9965/20000], Loss: -8.414840698242188, Learning Rate: 0.01\n",
      "Epoch [9966/20000], Loss: -8.419189453125, Learning Rate: 0.01\n",
      "Epoch [9967/20000], Loss: -8.423263549804688, Learning Rate: 0.01\n",
      "Epoch [9968/20000], Loss: -8.427291870117188, Learning Rate: 0.01\n",
      "Epoch [9969/20000], Loss: -8.431411743164062, Learning Rate: 0.01\n",
      "Epoch [9970/20000], Loss: -8.435562133789062, Learning Rate: 0.01\n",
      "Epoch [9971/20000], Loss: -8.43951416015625, Learning Rate: 0.01\n",
      "Epoch [9972/20000], Loss: -8.443527221679688, Learning Rate: 0.01\n",
      "Epoch [9973/20000], Loss: -8.447509765625, Learning Rate: 0.01\n",
      "Epoch [9974/20000], Loss: -8.451263427734375, Learning Rate: 0.01\n",
      "Epoch [9975/20000], Loss: -8.454803466796875, Learning Rate: 0.01\n",
      "Epoch [9976/20000], Loss: -8.458389282226562, Learning Rate: 0.01\n",
      "Epoch [9977/20000], Loss: -8.461456298828125, Learning Rate: 0.01\n",
      "Epoch [9978/20000], Loss: -8.464157104492188, Learning Rate: 0.01\n",
      "Epoch [9979/20000], Loss: -8.46649169921875, Learning Rate: 0.01\n",
      "Epoch [9980/20000], Loss: -8.467987060546875, Learning Rate: 0.01\n",
      "Epoch [9981/20000], Loss: -8.468765258789062, Learning Rate: 0.01\n",
      "Epoch [9982/20000], Loss: -8.468170166015625, Learning Rate: 0.01\n",
      "Epoch [9983/20000], Loss: -8.465850830078125, Learning Rate: 0.01\n",
      "Epoch [9984/20000], Loss: -8.46124267578125, Learning Rate: 0.01\n",
      "Epoch [9985/20000], Loss: -8.453414916992188, Learning Rate: 0.01\n",
      "Epoch [9986/20000], Loss: -8.441009521484375, Learning Rate: 0.01\n",
      "Epoch [9987/20000], Loss: -8.422042846679688, Learning Rate: 0.01\n",
      "Epoch [9988/20000], Loss: -8.394058227539062, Learning Rate: 0.01\n",
      "Epoch [9989/20000], Loss: -8.353271484375, Learning Rate: 0.01\n",
      "Epoch [9990/20000], Loss: -8.294692993164062, Learning Rate: 0.01\n",
      "Epoch [9991/20000], Loss: -8.210662841796875, Learning Rate: 0.01\n",
      "Epoch [9992/20000], Loss: -8.091217041015625, Learning Rate: 0.01\n",
      "Epoch [9993/20000], Loss: -7.92236328125, Learning Rate: 0.01\n",
      "Epoch [9994/20000], Loss: -7.6851806640625, Learning Rate: 0.01\n",
      "Epoch [9995/20000], Loss: -7.356048583984375, Learning Rate: 0.01\n",
      "Epoch [9996/20000], Loss: -6.9062652587890625, Learning Rate: 0.01\n",
      "Epoch [9997/20000], Loss: -6.307525634765625, Learning Rate: 0.01\n",
      "Epoch [9998/20000], Loss: -5.5383758544921875, Learning Rate: 0.01\n",
      "Epoch [9999/20000], Loss: -4.60882568359375, Learning Rate: 0.01\n",
      "Epoch [10000/20000], Loss: -3.58001708984375, Learning Rate: 0.01\n",
      "Epoch [10001/20000], Loss: -2.611297607421875, Learning Rate: 0.01\n",
      "Epoch [10002/20000], Loss: -1.9528656005859375, Learning Rate: 0.01\n",
      "Epoch [10003/20000], Loss: -1.9133148193359375, Learning Rate: 0.01\n",
      "Epoch [10004/20000], Loss: -2.6899261474609375, Learning Rate: 0.01\n",
      "Epoch [10005/20000], Loss: -4.21295166015625, Learning Rate: 0.01\n",
      "Epoch [10006/20000], Loss: -6.0575714111328125, Learning Rate: 0.01\n",
      "Epoch [10007/20000], Loss: -7.626922607421875, Learning Rate: 0.01\n",
      "Epoch [10008/20000], Loss: -8.45660400390625, Learning Rate: 0.01\n",
      "Epoch [10009/20000], Loss: -8.443527221679688, Learning Rate: 0.01\n",
      "Epoch [10010/20000], Loss: -7.8402252197265625, Learning Rate: 0.01\n",
      "Epoch [10011/20000], Loss: -7.0817718505859375, Learning Rate: 0.01\n",
      "Epoch [10012/20000], Loss: -6.5809478759765625, Learning Rate: 0.01\n",
      "Epoch [10013/20000], Loss: -6.56512451171875, Learning Rate: 0.01\n",
      "Epoch [10014/20000], Loss: -7.0088348388671875, Learning Rate: 0.01\n",
      "Epoch [10015/20000], Loss: -7.6743927001953125, Learning Rate: 0.01\n",
      "Epoch [10016/20000], Loss: -8.255706787109375, Learning Rate: 0.01\n",
      "Epoch [10017/20000], Loss: -8.540084838867188, Learning Rate: 0.01\n",
      "Epoch [10018/20000], Loss: -8.495513916015625, Learning Rate: 0.01\n",
      "Epoch [10019/20000], Loss: -8.247573852539062, Learning Rate: 0.01\n",
      "Epoch [10020/20000], Loss: -7.9858245849609375, Learning Rate: 0.01\n",
      "Epoch [10021/20000], Loss: -7.86224365234375, Learning Rate: 0.01\n",
      "Epoch [10022/20000], Loss: -7.92938232421875, Learning Rate: 0.01\n",
      "Epoch [10023/20000], Loss: -8.13751220703125, Learning Rate: 0.01\n",
      "Epoch [10024/20000], Loss: -8.379623413085938, Learning Rate: 0.01\n",
      "Epoch [10025/20000], Loss: -8.554412841796875, Learning Rate: 0.01\n",
      "Epoch [10026/20000], Loss: -8.613082885742188, Learning Rate: 0.01\n",
      "Epoch [10027/20000], Loss: -8.570343017578125, Learning Rate: 0.01\n",
      "Epoch [10028/20000], Loss: -8.482192993164062, Learning Rate: 0.01\n",
      "Epoch [10029/20000], Loss: -8.410293579101562, Learning Rate: 0.01\n",
      "Epoch [10030/20000], Loss: -8.39447021484375, Learning Rate: 0.01\n",
      "Epoch [10031/20000], Loss: -8.439407348632812, Learning Rate: 0.01\n",
      "Epoch [10032/20000], Loss: -8.521453857421875, Learning Rate: 0.01\n",
      "Epoch [10033/20000], Loss: -8.605758666992188, Learning Rate: 0.01\n",
      "Epoch [10034/20000], Loss: -8.663284301757812, Learning Rate: 0.01\n",
      "Epoch [10035/20000], Loss: -8.682861328125, Learning Rate: 0.01\n",
      "Epoch [10036/20000], Loss: -8.670852661132812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10037/20000], Loss: -8.64483642578125, Learning Rate: 0.01\n",
      "Epoch [10038/20000], Loss: -8.623764038085938, Learning Rate: 0.01\n",
      "Epoch [10039/20000], Loss: -8.620620727539062, Learning Rate: 0.01\n",
      "Epoch [10040/20000], Loss: -8.637557983398438, Learning Rate: 0.01\n",
      "Epoch [10041/20000], Loss: -8.668594360351562, Learning Rate: 0.01\n",
      "Epoch [10042/20000], Loss: -8.703140258789062, Learning Rate: 0.01\n",
      "Epoch [10043/20000], Loss: -8.7303466796875, Learning Rate: 0.01\n",
      "Epoch [10044/20000], Loss: -8.744796752929688, Learning Rate: 0.01\n",
      "Epoch [10045/20000], Loss: -8.746322631835938, Learning Rate: 0.01\n",
      "Epoch [10046/20000], Loss: -8.739822387695312, Learning Rate: 0.01\n",
      "Epoch [10047/20000], Loss: -8.732330322265625, Learning Rate: 0.01\n",
      "Epoch [10048/20000], Loss: -8.729690551757812, Learning Rate: 0.01\n",
      "Epoch [10049/20000], Loss: -8.735183715820312, Learning Rate: 0.01\n",
      "Epoch [10050/20000], Loss: -8.748092651367188, Learning Rate: 0.01\n",
      "Epoch [10051/20000], Loss: -8.76470947265625, Learning Rate: 0.01\n",
      "Epoch [10052/20000], Loss: -8.780731201171875, Learning Rate: 0.01\n",
      "Epoch [10053/20000], Loss: -8.792434692382812, Learning Rate: 0.01\n",
      "Epoch [10054/20000], Loss: -8.798187255859375, Learning Rate: 0.01\n",
      "Epoch [10055/20000], Loss: -8.799301147460938, Learning Rate: 0.01\n",
      "Epoch [10056/20000], Loss: -8.798019409179688, Learning Rate: 0.01\n",
      "Epoch [10057/20000], Loss: -8.797393798828125, Learning Rate: 0.01\n",
      "Epoch [10058/20000], Loss: -8.799285888671875, Learning Rate: 0.01\n",
      "Epoch [10059/20000], Loss: -8.804855346679688, Learning Rate: 0.01\n",
      "Epoch [10060/20000], Loss: -8.813247680664062, Learning Rate: 0.01\n",
      "Epoch [10061/20000], Loss: -8.823013305664062, Learning Rate: 0.01\n",
      "Epoch [10062/20000], Loss: -8.831863403320312, Learning Rate: 0.01\n",
      "Epoch [10063/20000], Loss: -8.838851928710938, Learning Rate: 0.01\n",
      "Epoch [10064/20000], Loss: -8.843429565429688, Learning Rate: 0.01\n",
      "Epoch [10065/20000], Loss: -8.846160888671875, Learning Rate: 0.01\n",
      "Epoch [10066/20000], Loss: -8.847793579101562, Learning Rate: 0.01\n",
      "Epoch [10067/20000], Loss: -8.849884033203125, Learning Rate: 0.01\n",
      "Epoch [10068/20000], Loss: -8.853134155273438, Learning Rate: 0.01\n",
      "Epoch [10069/20000], Loss: -8.85772705078125, Learning Rate: 0.01\n",
      "Epoch [10070/20000], Loss: -8.863449096679688, Learning Rate: 0.01\n",
      "Epoch [10071/20000], Loss: -8.86968994140625, Learning Rate: 0.01\n",
      "Epoch [10072/20000], Loss: -8.875717163085938, Learning Rate: 0.01\n",
      "Epoch [10073/20000], Loss: -8.88128662109375, Learning Rate: 0.01\n",
      "Epoch [10074/20000], Loss: -8.885955810546875, Learning Rate: 0.01\n",
      "Epoch [10075/20000], Loss: -8.889923095703125, Learning Rate: 0.01\n",
      "Epoch [10076/20000], Loss: -8.893295288085938, Learning Rate: 0.01\n",
      "Epoch [10077/20000], Loss: -8.89654541015625, Learning Rate: 0.01\n",
      "Epoch [10078/20000], Loss: -8.900070190429688, Learning Rate: 0.01\n",
      "Epoch [10079/20000], Loss: -8.904144287109375, Learning Rate: 0.01\n",
      "Epoch [10080/20000], Loss: -8.908538818359375, Learning Rate: 0.01\n",
      "Epoch [10081/20000], Loss: -8.913314819335938, Learning Rate: 0.01\n",
      "Epoch [10082/20000], Loss: -8.918045043945312, Learning Rate: 0.01\n",
      "Epoch [10083/20000], Loss: -8.922943115234375, Learning Rate: 0.01\n",
      "Epoch [10084/20000], Loss: -8.927597045898438, Learning Rate: 0.01\n",
      "Epoch [10085/20000], Loss: -8.931915283203125, Learning Rate: 0.01\n",
      "Epoch [10086/20000], Loss: -8.936141967773438, Learning Rate: 0.01\n",
      "Epoch [10087/20000], Loss: -8.940032958984375, Learning Rate: 0.01\n",
      "Epoch [10088/20000], Loss: -8.944091796875, Learning Rate: 0.01\n",
      "Epoch [10089/20000], Loss: -8.948028564453125, Learning Rate: 0.01\n",
      "Epoch [10090/20000], Loss: -8.952194213867188, Learning Rate: 0.01\n",
      "Epoch [10091/20000], Loss: -8.956283569335938, Learning Rate: 0.01\n",
      "Epoch [10092/20000], Loss: -8.960479736328125, Learning Rate: 0.01\n",
      "Epoch [10093/20000], Loss: -8.964828491210938, Learning Rate: 0.01\n",
      "Epoch [10094/20000], Loss: -8.96917724609375, Learning Rate: 0.01\n",
      "Epoch [10095/20000], Loss: -8.973602294921875, Learning Rate: 0.01\n",
      "Epoch [10096/20000], Loss: -8.9779052734375, Learning Rate: 0.01\n",
      "Epoch [10097/20000], Loss: -8.982192993164062, Learning Rate: 0.01\n",
      "Epoch [10098/20000], Loss: -8.9864501953125, Learning Rate: 0.01\n",
      "Epoch [10099/20000], Loss: -8.990524291992188, Learning Rate: 0.01\n",
      "Epoch [10100/20000], Loss: -8.994674682617188, Learning Rate: 0.01\n",
      "Epoch [10101/20000], Loss: -8.9989013671875, Learning Rate: 0.01\n",
      "Epoch [10102/20000], Loss: -9.003067016601562, Learning Rate: 0.01\n",
      "Epoch [10103/20000], Loss: -9.00732421875, Learning Rate: 0.01\n",
      "Epoch [10104/20000], Loss: -9.01141357421875, Learning Rate: 0.01\n",
      "Epoch [10105/20000], Loss: -9.015609741210938, Learning Rate: 0.01\n",
      "Epoch [10106/20000], Loss: -9.019790649414062, Learning Rate: 0.01\n",
      "Epoch [10107/20000], Loss: -9.023910522460938, Learning Rate: 0.01\n",
      "Epoch [10108/20000], Loss: -9.028244018554688, Learning Rate: 0.01\n",
      "Epoch [10109/20000], Loss: -9.032394409179688, Learning Rate: 0.01\n",
      "Epoch [10110/20000], Loss: -9.036590576171875, Learning Rate: 0.01\n",
      "Epoch [10111/20000], Loss: -9.040878295898438, Learning Rate: 0.01\n",
      "Epoch [10112/20000], Loss: -9.04510498046875, Learning Rate: 0.01\n",
      "Epoch [10113/20000], Loss: -9.049285888671875, Learning Rate: 0.01\n",
      "Epoch [10114/20000], Loss: -9.053512573242188, Learning Rate: 0.01\n",
      "Epoch [10115/20000], Loss: -9.0577392578125, Learning Rate: 0.01\n",
      "Epoch [10116/20000], Loss: -9.061904907226562, Learning Rate: 0.01\n",
      "Epoch [10117/20000], Loss: -9.066055297851562, Learning Rate: 0.01\n",
      "Epoch [10118/20000], Loss: -9.0703125, Learning Rate: 0.01\n",
      "Epoch [10119/20000], Loss: -9.07452392578125, Learning Rate: 0.01\n",
      "Epoch [10120/20000], Loss: -9.078659057617188, Learning Rate: 0.01\n",
      "Epoch [10121/20000], Loss: -9.082901000976562, Learning Rate: 0.01\n",
      "Epoch [10122/20000], Loss: -9.086990356445312, Learning Rate: 0.01\n",
      "Epoch [10123/20000], Loss: -9.09130859375, Learning Rate: 0.01\n",
      "Epoch [10124/20000], Loss: -9.09539794921875, Learning Rate: 0.01\n",
      "Epoch [10125/20000], Loss: -9.099609375, Learning Rate: 0.01\n",
      "Epoch [10126/20000], Loss: -9.103836059570312, Learning Rate: 0.01\n",
      "Epoch [10127/20000], Loss: -9.108047485351562, Learning Rate: 0.01\n",
      "Epoch [10128/20000], Loss: -9.112335205078125, Learning Rate: 0.01\n",
      "Epoch [10129/20000], Loss: -9.116455078125, Learning Rate: 0.01\n",
      "Epoch [10130/20000], Loss: -9.120773315429688, Learning Rate: 0.01\n",
      "Epoch [10131/20000], Loss: -9.124893188476562, Learning Rate: 0.01\n",
      "Epoch [10132/20000], Loss: -9.129104614257812, Learning Rate: 0.01\n",
      "Epoch [10133/20000], Loss: -9.13336181640625, Learning Rate: 0.01\n",
      "Epoch [10134/20000], Loss: -9.137481689453125, Learning Rate: 0.01\n",
      "Epoch [10135/20000], Loss: -9.141693115234375, Learning Rate: 0.01\n",
      "Epoch [10136/20000], Loss: -9.14593505859375, Learning Rate: 0.01\n",
      "Epoch [10137/20000], Loss: -9.150222778320312, Learning Rate: 0.01\n",
      "Epoch [10138/20000], Loss: -9.154403686523438, Learning Rate: 0.01\n",
      "Epoch [10139/20000], Loss: -9.158584594726562, Learning Rate: 0.01\n",
      "Epoch [10140/20000], Loss: -9.162765502929688, Learning Rate: 0.01\n",
      "Epoch [10141/20000], Loss: -9.167022705078125, Learning Rate: 0.01\n",
      "Epoch [10142/20000], Loss: -9.171173095703125, Learning Rate: 0.01\n",
      "Epoch [10143/20000], Loss: -9.175384521484375, Learning Rate: 0.01\n",
      "Epoch [10144/20000], Loss: -9.179611206054688, Learning Rate: 0.01\n",
      "Epoch [10145/20000], Loss: -9.183853149414062, Learning Rate: 0.01\n",
      "Epoch [10146/20000], Loss: -9.188064575195312, Learning Rate: 0.01\n",
      "Epoch [10147/20000], Loss: -9.192153930664062, Learning Rate: 0.01\n",
      "Epoch [10148/20000], Loss: -9.196533203125, Learning Rate: 0.01\n",
      "Epoch [10149/20000], Loss: -9.200653076171875, Learning Rate: 0.01\n",
      "Epoch [10150/20000], Loss: -9.204849243164062, Learning Rate: 0.01\n",
      "Epoch [10151/20000], Loss: -9.209030151367188, Learning Rate: 0.01\n",
      "Epoch [10152/20000], Loss: -9.213302612304688, Learning Rate: 0.01\n",
      "Epoch [10153/20000], Loss: -9.217483520507812, Learning Rate: 0.01\n",
      "Epoch [10154/20000], Loss: -9.22174072265625, Learning Rate: 0.01\n",
      "Epoch [10155/20000], Loss: -9.225921630859375, Learning Rate: 0.01\n",
      "Epoch [10156/20000], Loss: -9.230178833007812, Learning Rate: 0.01\n",
      "Epoch [10157/20000], Loss: -9.234344482421875, Learning Rate: 0.01\n",
      "Epoch [10158/20000], Loss: -9.238525390625, Learning Rate: 0.01\n",
      "Epoch [10159/20000], Loss: -9.24267578125, Learning Rate: 0.01\n",
      "Epoch [10160/20000], Loss: -9.2469482421875, Learning Rate: 0.01\n",
      "Epoch [10161/20000], Loss: -9.251113891601562, Learning Rate: 0.01\n",
      "Epoch [10162/20000], Loss: -9.255401611328125, Learning Rate: 0.01\n",
      "Epoch [10163/20000], Loss: -9.259521484375, Learning Rate: 0.01\n",
      "Epoch [10164/20000], Loss: -9.263717651367188, Learning Rate: 0.01\n",
      "Epoch [10165/20000], Loss: -9.268020629882812, Learning Rate: 0.01\n",
      "Epoch [10166/20000], Loss: -9.272125244140625, Learning Rate: 0.01\n",
      "Epoch [10167/20000], Loss: -9.276199340820312, Learning Rate: 0.01\n",
      "Epoch [10168/20000], Loss: -9.28033447265625, Learning Rate: 0.01\n",
      "Epoch [10169/20000], Loss: -9.28436279296875, Learning Rate: 0.01\n",
      "Epoch [10170/20000], Loss: -9.288360595703125, Learning Rate: 0.01\n",
      "Epoch [10171/20000], Loss: -9.292251586914062, Learning Rate: 0.01\n",
      "Epoch [10172/20000], Loss: -9.296005249023438, Learning Rate: 0.01\n",
      "Epoch [10173/20000], Loss: -9.299484252929688, Learning Rate: 0.01\n",
      "Epoch [10174/20000], Loss: -9.303024291992188, Learning Rate: 0.01\n",
      "Epoch [10175/20000], Loss: -9.306076049804688, Learning Rate: 0.01\n",
      "Epoch [10176/20000], Loss: -9.308563232421875, Learning Rate: 0.01\n",
      "Epoch [10177/20000], Loss: -9.310317993164062, Learning Rate: 0.01\n",
      "Epoch [10178/20000], Loss: -9.310882568359375, Learning Rate: 0.01\n",
      "Epoch [10179/20000], Loss: -9.30999755859375, Learning Rate: 0.01\n",
      "Epoch [10180/20000], Loss: -9.306640625, Learning Rate: 0.01\n",
      "Epoch [10181/20000], Loss: -9.299774169921875, Learning Rate: 0.01\n",
      "Epoch [10182/20000], Loss: -9.287628173828125, Learning Rate: 0.01\n",
      "Epoch [10183/20000], Loss: -9.26763916015625, Learning Rate: 0.01\n",
      "Epoch [10184/20000], Loss: -9.235992431640625, Learning Rate: 0.01\n",
      "Epoch [10185/20000], Loss: -9.186569213867188, Learning Rate: 0.01\n",
      "Epoch [10186/20000], Loss: -9.11065673828125, Learning Rate: 0.01\n",
      "Epoch [10187/20000], Loss: -8.994857788085938, Learning Rate: 0.01\n",
      "Epoch [10188/20000], Loss: -8.819427490234375, Learning Rate: 0.01\n",
      "Epoch [10189/20000], Loss: -8.554672241210938, Learning Rate: 0.01\n",
      "Epoch [10190/20000], Loss: -8.159439086914062, Learning Rate: 0.01\n",
      "Epoch [10191/20000], Loss: -7.5762176513671875, Learning Rate: 0.01\n",
      "Epoch [10192/20000], Loss: -6.734039306640625, Learning Rate: 0.01\n",
      "Epoch [10193/20000], Loss: -5.5568389892578125, Learning Rate: 0.01\n",
      "Epoch [10194/20000], Loss: -3.9983062744140625, Learning Rate: 0.01\n",
      "Epoch [10195/20000], Loss: -2.108123779296875, Learning Rate: 0.01\n",
      "Epoch [10196/20000], Loss: -0.14263916015625, Learning Rate: 0.01\n",
      "Epoch [10197/20000], Loss: 1.3478240966796875, Learning Rate: 0.01\n",
      "Epoch [10198/20000], Loss: 1.5966949462890625, Learning Rate: 0.01\n",
      "Epoch [10199/20000], Loss: 0.0357513427734375, Learning Rate: 0.01\n",
      "Epoch [10200/20000], Loss: -3.0939788818359375, Learning Rate: 0.01\n",
      "Epoch [10201/20000], Loss: -6.5609130859375, Learning Rate: 0.01\n",
      "Epoch [10202/20000], Loss: -8.827423095703125, Learning Rate: 0.01\n",
      "Epoch [10203/20000], Loss: -9.110443115234375, Learning Rate: 0.01\n",
      "Epoch [10204/20000], Loss: -7.821044921875, Learning Rate: 0.01\n",
      "Epoch [10205/20000], Loss: -6.166412353515625, Learning Rate: 0.01\n",
      "Epoch [10206/20000], Loss: -5.3660430908203125, Learning Rate: 0.01\n",
      "Epoch [10207/20000], Loss: -5.95782470703125, Learning Rate: 0.01\n",
      "Epoch [10208/20000], Loss: -7.4988250732421875, Learning Rate: 0.01\n",
      "Epoch [10209/20000], Loss: -8.939056396484375, Learning Rate: 0.01\n",
      "Epoch [10210/20000], Loss: -9.432083129882812, Learning Rate: 0.01\n",
      "Epoch [10211/20000], Loss: -8.9176025390625, Learning Rate: 0.01\n",
      "Epoch [10212/20000], Loss: -8.03216552734375, Learning Rate: 0.01\n",
      "Epoch [10213/20000], Loss: -7.5490570068359375, Learning Rate: 0.01\n",
      "Epoch [10214/20000], Loss: -7.82421875, Learning Rate: 0.01\n",
      "Epoch [10215/20000], Loss: -8.603317260742188, Learning Rate: 0.01\n",
      "Epoch [10216/20000], Loss: -9.299545288085938, Learning Rate: 0.01\n",
      "Epoch [10217/20000], Loss: -9.481338500976562, Learning Rate: 0.01\n",
      "Epoch [10218/20000], Loss: -9.170074462890625, Learning Rate: 0.01\n",
      "Epoch [10219/20000], Loss: -8.73931884765625, Learning Rate: 0.01\n",
      "Epoch [10220/20000], Loss: -8.574295043945312, Learning Rate: 0.01\n",
      "Epoch [10221/20000], Loss: -8.789077758789062, Learning Rate: 0.01\n",
      "Epoch [10222/20000], Loss: -9.189544677734375, Learning Rate: 0.01\n",
      "Epoch [10223/20000], Loss: -9.475173950195312, Learning Rate: 0.01\n",
      "Epoch [10224/20000], Loss: -9.48443603515625, Learning Rate: 0.01\n",
      "Epoch [10225/20000], Loss: -9.290863037109375, Learning Rate: 0.01\n",
      "Epoch [10226/20000], Loss: -9.103271484375, Learning Rate: 0.01\n",
      "Epoch [10227/20000], Loss: -9.083877563476562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10228/20000], Loss: -9.23809814453125, Learning Rate: 0.01\n",
      "Epoch [10229/20000], Loss: -9.439132690429688, Learning Rate: 0.01\n",
      "Epoch [10230/20000], Loss: -9.547622680664062, Learning Rate: 0.01\n",
      "Epoch [10231/20000], Loss: -9.517578125, Learning Rate: 0.01\n",
      "Epoch [10232/20000], Loss: -9.41217041015625, Learning Rate: 0.01\n",
      "Epoch [10233/20000], Loss: -9.336517333984375, Learning Rate: 0.01\n",
      "Epoch [10234/20000], Loss: -9.352218627929688, Learning Rate: 0.01\n",
      "Epoch [10235/20000], Loss: -9.441970825195312, Learning Rate: 0.01\n",
      "Epoch [10236/20000], Loss: -9.536865234375, Learning Rate: 0.01\n",
      "Epoch [10237/20000], Loss: -9.57720947265625, Learning Rate: 0.01\n",
      "Epoch [10238/20000], Loss: -9.553665161132812, Learning Rate: 0.01\n",
      "Epoch [10239/20000], Loss: -9.503936767578125, Learning Rate: 0.01\n",
      "Epoch [10240/20000], Loss: -9.476821899414062, Learning Rate: 0.01\n",
      "Epoch [10241/20000], Loss: -9.494964599609375, Learning Rate: 0.01\n",
      "Epoch [10242/20000], Loss: -9.54388427734375, Learning Rate: 0.01\n",
      "Epoch [10243/20000], Loss: -9.590179443359375, Learning Rate: 0.01\n",
      "Epoch [10244/20000], Loss: -9.607711791992188, Learning Rate: 0.01\n",
      "Epoch [10245/20000], Loss: -9.595779418945312, Learning Rate: 0.01\n",
      "Epoch [10246/20000], Loss: -9.573532104492188, Learning Rate: 0.01\n",
      "Epoch [10247/20000], Loss: -9.563079833984375, Learning Rate: 0.01\n",
      "Epoch [10248/20000], Loss: -9.574493408203125, Learning Rate: 0.01\n",
      "Epoch [10249/20000], Loss: -9.600021362304688, Learning Rate: 0.01\n",
      "Epoch [10250/20000], Loss: -9.624465942382812, Learning Rate: 0.01\n",
      "Epoch [10251/20000], Loss: -9.635299682617188, Learning Rate: 0.01\n",
      "Epoch [10252/20000], Loss: -9.632217407226562, Learning Rate: 0.01\n",
      "Epoch [10253/20000], Loss: -9.623977661132812, Learning Rate: 0.01\n",
      "Epoch [10254/20000], Loss: -9.620681762695312, Learning Rate: 0.01\n",
      "Epoch [10255/20000], Loss: -9.627410888671875, Learning Rate: 0.01\n",
      "Epoch [10256/20000], Loss: -9.641311645507812, Learning Rate: 0.01\n",
      "Epoch [10257/20000], Loss: -9.655487060546875, Learning Rate: 0.01\n",
      "Epoch [10258/20000], Loss: -9.664047241210938, Learning Rate: 0.01\n",
      "Epoch [10259/20000], Loss: -9.665603637695312, Learning Rate: 0.01\n",
      "Epoch [10260/20000], Loss: -9.663909912109375, Learning Rate: 0.01\n",
      "Epoch [10261/20000], Loss: -9.663558959960938, Learning Rate: 0.01\n",
      "Epoch [10262/20000], Loss: -9.667449951171875, Learning Rate: 0.01\n",
      "Epoch [10263/20000], Loss: -9.675262451171875, Learning Rate: 0.01\n",
      "Epoch [10264/20000], Loss: -9.68426513671875, Learning Rate: 0.01\n",
      "Epoch [10265/20000], Loss: -9.691543579101562, Learning Rate: 0.01\n",
      "Epoch [10266/20000], Loss: -9.69549560546875, Learning Rate: 0.01\n",
      "Epoch [10267/20000], Loss: -9.697174072265625, Learning Rate: 0.01\n",
      "Epoch [10268/20000], Loss: -9.698715209960938, Learning Rate: 0.01\n",
      "Epoch [10269/20000], Loss: -9.70172119140625, Learning Rate: 0.01\n",
      "Epoch [10270/20000], Loss: -9.706787109375, Learning Rate: 0.01\n",
      "Epoch [10271/20000], Loss: -9.713043212890625, Learning Rate: 0.01\n",
      "Epoch [10272/20000], Loss: -9.719100952148438, Learning Rate: 0.01\n",
      "Epoch [10273/20000], Loss: -9.72393798828125, Learning Rate: 0.01\n",
      "Epoch [10274/20000], Loss: -9.727371215820312, Learning Rate: 0.01\n",
      "Epoch [10275/20000], Loss: -9.730209350585938, Learning Rate: 0.01\n",
      "Epoch [10276/20000], Loss: -9.7332763671875, Learning Rate: 0.01\n",
      "Epoch [10277/20000], Loss: -9.7371826171875, Learning Rate: 0.01\n",
      "Epoch [10278/20000], Loss: -9.74188232421875, Learning Rate: 0.01\n",
      "Epoch [10279/20000], Loss: -9.746978759765625, Learning Rate: 0.01\n",
      "Epoch [10280/20000], Loss: -9.751739501953125, Learning Rate: 0.01\n",
      "Epoch [10281/20000], Loss: -9.7559814453125, Learning Rate: 0.01\n",
      "Epoch [10282/20000], Loss: -9.759536743164062, Learning Rate: 0.01\n",
      "Epoch [10283/20000], Loss: -9.763015747070312, Learning Rate: 0.01\n",
      "Epoch [10284/20000], Loss: -9.76666259765625, Learning Rate: 0.01\n",
      "Epoch [10285/20000], Loss: -9.770721435546875, Learning Rate: 0.01\n",
      "Epoch [10286/20000], Loss: -9.775161743164062, Learning Rate: 0.01\n",
      "Epoch [10287/20000], Loss: -9.779586791992188, Learning Rate: 0.01\n",
      "Epoch [10288/20000], Loss: -9.783798217773438, Learning Rate: 0.01\n",
      "Epoch [10289/20000], Loss: -9.787841796875, Learning Rate: 0.01\n",
      "Epoch [10290/20000], Loss: -9.79180908203125, Learning Rate: 0.01\n",
      "Epoch [10291/20000], Loss: -9.795501708984375, Learning Rate: 0.01\n",
      "Epoch [10292/20000], Loss: -9.799392700195312, Learning Rate: 0.01\n",
      "Epoch [10293/20000], Loss: -9.8033447265625, Learning Rate: 0.01\n",
      "Epoch [10294/20000], Loss: -9.807373046875, Learning Rate: 0.01\n",
      "Epoch [10295/20000], Loss: -9.811553955078125, Learning Rate: 0.01\n",
      "Epoch [10296/20000], Loss: -9.815765380859375, Learning Rate: 0.01\n",
      "Epoch [10297/20000], Loss: -9.81982421875, Learning Rate: 0.01\n",
      "Epoch [10298/20000], Loss: -9.823776245117188, Learning Rate: 0.01\n",
      "Epoch [10299/20000], Loss: -9.827682495117188, Learning Rate: 0.01\n",
      "Epoch [10300/20000], Loss: -9.831527709960938, Learning Rate: 0.01\n",
      "Epoch [10301/20000], Loss: -9.83551025390625, Learning Rate: 0.01\n",
      "Epoch [10302/20000], Loss: -9.839431762695312, Learning Rate: 0.01\n",
      "Epoch [10303/20000], Loss: -9.843597412109375, Learning Rate: 0.01\n",
      "Epoch [10304/20000], Loss: -9.84765625, Learning Rate: 0.01\n",
      "Epoch [10305/20000], Loss: -9.851638793945312, Learning Rate: 0.01\n",
      "Epoch [10306/20000], Loss: -9.85565185546875, Learning Rate: 0.01\n",
      "Epoch [10307/20000], Loss: -9.85943603515625, Learning Rate: 0.01\n",
      "Epoch [10308/20000], Loss: -9.8634033203125, Learning Rate: 0.01\n",
      "Epoch [10309/20000], Loss: -9.8671875, Learning Rate: 0.01\n",
      "Epoch [10310/20000], Loss: -9.871002197265625, Learning Rate: 0.01\n",
      "Epoch [10311/20000], Loss: -9.874771118164062, Learning Rate: 0.01\n",
      "Epoch [10312/20000], Loss: -9.878494262695312, Learning Rate: 0.01\n",
      "Epoch [10313/20000], Loss: -9.88214111328125, Learning Rate: 0.01\n",
      "Epoch [10314/20000], Loss: -9.885452270507812, Learning Rate: 0.01\n",
      "Epoch [10315/20000], Loss: -9.888641357421875, Learning Rate: 0.01\n",
      "Epoch [10316/20000], Loss: -9.89166259765625, Learning Rate: 0.01\n",
      "Epoch [10317/20000], Loss: -9.893936157226562, Learning Rate: 0.01\n",
      "Epoch [10318/20000], Loss: -9.895751953125, Learning Rate: 0.01\n",
      "Epoch [10319/20000], Loss: -9.896591186523438, Learning Rate: 0.01\n",
      "Epoch [10320/20000], Loss: -9.896163940429688, Learning Rate: 0.01\n",
      "Epoch [10321/20000], Loss: -9.893646240234375, Learning Rate: 0.01\n",
      "Epoch [10322/20000], Loss: -9.88861083984375, Learning Rate: 0.01\n",
      "Epoch [10323/20000], Loss: -9.879348754882812, Learning Rate: 0.01\n",
      "Epoch [10324/20000], Loss: -9.864212036132812, Learning Rate: 0.01\n",
      "Epoch [10325/20000], Loss: -9.840316772460938, Learning Rate: 0.01\n",
      "Epoch [10326/20000], Loss: -9.803741455078125, Learning Rate: 0.01\n",
      "Epoch [10327/20000], Loss: -9.748687744140625, Learning Rate: 0.01\n",
      "Epoch [10328/20000], Loss: -9.66644287109375, Learning Rate: 0.01\n",
      "Epoch [10329/20000], Loss: -9.545913696289062, Learning Rate: 0.01\n",
      "Epoch [10330/20000], Loss: -9.370346069335938, Learning Rate: 0.01\n",
      "Epoch [10331/20000], Loss: -9.121597290039062, Learning Rate: 0.01\n",
      "Epoch [10332/20000], Loss: -8.775177001953125, Learning Rate: 0.01\n",
      "Epoch [10333/20000], Loss: -8.319015502929688, Learning Rate: 0.01\n",
      "Epoch [10334/20000], Loss: -7.7485198974609375, Learning Rate: 0.01\n",
      "Epoch [10335/20000], Loss: -7.123321533203125, Learning Rate: 0.01\n",
      "Epoch [10336/20000], Loss: -6.543182373046875, Learning Rate: 0.01\n",
      "Epoch [10337/20000], Loss: -6.228363037109375, Learning Rate: 0.01\n",
      "Epoch [10338/20000], Loss: -6.358489990234375, Learning Rate: 0.01\n",
      "Epoch [10339/20000], Loss: -7.0548248291015625, Learning Rate: 0.01\n",
      "Epoch [10340/20000], Loss: -8.125350952148438, Learning Rate: 0.01\n",
      "Epoch [10341/20000], Loss: -9.195709228515625, Learning Rate: 0.01\n",
      "Epoch [10342/20000], Loss: -9.855575561523438, Learning Rate: 0.01\n",
      "Epoch [10343/20000], Loss: -9.937911987304688, Learning Rate: 0.01\n",
      "Epoch [10344/20000], Loss: -9.572402954101562, Learning Rate: 0.01\n",
      "Epoch [10345/20000], Loss: -9.074234008789062, Learning Rate: 0.01\n",
      "Epoch [10346/20000], Loss: -8.765167236328125, Learning Rate: 0.01\n",
      "Epoch [10347/20000], Loss: -8.811050415039062, Learning Rate: 0.01\n",
      "Epoch [10348/20000], Loss: -9.169631958007812, Learning Rate: 0.01\n",
      "Epoch [10349/20000], Loss: -9.618179321289062, Learning Rate: 0.01\n",
      "Epoch [10350/20000], Loss: -9.919708251953125, Learning Rate: 0.01\n",
      "Epoch [10351/20000], Loss: -9.9609375, Learning Rate: 0.01\n",
      "Epoch [10352/20000], Loss: -9.801544189453125, Learning Rate: 0.01\n",
      "Epoch [10353/20000], Loss: -9.6038818359375, Learning Rate: 0.01\n",
      "Epoch [10354/20000], Loss: -9.516815185546875, Learning Rate: 0.01\n",
      "Epoch [10355/20000], Loss: -9.5941162109375, Learning Rate: 0.01\n",
      "Epoch [10356/20000], Loss: -9.775543212890625, Learning Rate: 0.01\n",
      "Epoch [10357/20000], Loss: -9.94757080078125, Learning Rate: 0.01\n",
      "Epoch [10358/20000], Loss: -10.021148681640625, Learning Rate: 0.01\n",
      "Epoch [10359/20000], Loss: -9.984710693359375, Learning Rate: 0.01\n",
      "Epoch [10360/20000], Loss: -9.896514892578125, Learning Rate: 0.01\n",
      "Epoch [10361/20000], Loss: -9.834518432617188, Learning Rate: 0.01\n",
      "Epoch [10362/20000], Loss: -9.845733642578125, Learning Rate: 0.01\n",
      "Epoch [10363/20000], Loss: -9.921676635742188, Learning Rate: 0.01\n",
      "Epoch [10364/20000], Loss: -10.015243530273438, Learning Rate: 0.01\n",
      "Epoch [10365/20000], Loss: -10.074630737304688, Learning Rate: 0.01\n",
      "Epoch [10366/20000], Loss: -10.077743530273438, Learning Rate: 0.01\n",
      "Epoch [10367/20000], Loss: -10.039779663085938, Learning Rate: 0.01\n",
      "Epoch [10368/20000], Loss: -9.997589111328125, Learning Rate: 0.01\n",
      "Epoch [10369/20000], Loss: -9.98358154296875, Learning Rate: 0.01\n",
      "Epoch [10370/20000], Loss: -10.007781982421875, Learning Rate: 0.01\n",
      "Epoch [10371/20000], Loss: -10.055389404296875, Learning Rate: 0.01\n",
      "Epoch [10372/20000], Loss: -10.1005859375, Learning Rate: 0.01\n",
      "Epoch [10373/20000], Loss: -10.12322998046875, Learning Rate: 0.01\n",
      "Epoch [10374/20000], Loss: -10.119293212890625, Learning Rate: 0.01\n",
      "Epoch [10375/20000], Loss: -10.100433349609375, Learning Rate: 0.01\n",
      "Epoch [10376/20000], Loss: -10.084030151367188, Learning Rate: 0.01\n",
      "Epoch [10377/20000], Loss: -10.082672119140625, Learning Rate: 0.01\n",
      "Epoch [10378/20000], Loss: -10.097442626953125, Learning Rate: 0.01\n",
      "Epoch [10379/20000], Loss: -10.120880126953125, Learning Rate: 0.01\n",
      "Epoch [10380/20000], Loss: -10.141647338867188, Learning Rate: 0.01\n",
      "Epoch [10381/20000], Loss: -10.152542114257812, Learning Rate: 0.01\n",
      "Epoch [10382/20000], Loss: -10.152862548828125, Learning Rate: 0.01\n",
      "Epoch [10383/20000], Loss: -10.14788818359375, Learning Rate: 0.01\n",
      "Epoch [10384/20000], Loss: -10.14434814453125, Learning Rate: 0.01\n",
      "Epoch [10385/20000], Loss: -10.1468505859375, Learning Rate: 0.01\n",
      "Epoch [10386/20000], Loss: -10.15545654296875, Learning Rate: 0.01\n",
      "Epoch [10387/20000], Loss: -10.166900634765625, Learning Rate: 0.01\n",
      "Epoch [10388/20000], Loss: -10.17718505859375, Learning Rate: 0.01\n",
      "Epoch [10389/20000], Loss: -10.183303833007812, Learning Rate: 0.01\n",
      "Epoch [10390/20000], Loss: -10.185577392578125, Learning Rate: 0.01\n",
      "Epoch [10391/20000], Loss: -10.185928344726562, Learning Rate: 0.01\n",
      "Epoch [10392/20000], Loss: -10.18670654296875, Learning Rate: 0.01\n",
      "Epoch [10393/20000], Loss: -10.190155029296875, Learning Rate: 0.01\n",
      "Epoch [10394/20000], Loss: -10.196090698242188, Learning Rate: 0.01\n",
      "Epoch [10395/20000], Loss: -10.2034912109375, Learning Rate: 0.01\n",
      "Epoch [10396/20000], Loss: -10.210540771484375, Learning Rate: 0.01\n",
      "Epoch [10397/20000], Loss: -10.215774536132812, Learning Rate: 0.01\n",
      "Epoch [10398/20000], Loss: -10.219207763671875, Learning Rate: 0.01\n",
      "Epoch [10399/20000], Loss: -10.221527099609375, Learning Rate: 0.01\n",
      "Epoch [10400/20000], Loss: -10.223602294921875, Learning Rate: 0.01\n",
      "Epoch [10401/20000], Loss: -10.226608276367188, Learning Rate: 0.01\n",
      "Epoch [10402/20000], Loss: -10.230758666992188, Learning Rate: 0.01\n",
      "Epoch [10403/20000], Loss: -10.235824584960938, Learning Rate: 0.01\n",
      "Epoch [10404/20000], Loss: -10.241195678710938, Learning Rate: 0.01\n",
      "Epoch [10405/20000], Loss: -10.246170043945312, Learning Rate: 0.01\n",
      "Epoch [10406/20000], Loss: -10.25054931640625, Learning Rate: 0.01\n",
      "Epoch [10407/20000], Loss: -10.253997802734375, Learning Rate: 0.01\n",
      "Epoch [10408/20000], Loss: -10.257339477539062, Learning Rate: 0.01\n",
      "Epoch [10409/20000], Loss: -10.260498046875, Learning Rate: 0.01\n",
      "Epoch [10410/20000], Loss: -10.26416015625, Learning Rate: 0.01\n",
      "Epoch [10411/20000], Loss: -10.26824951171875, Learning Rate: 0.01\n",
      "Epoch [10412/20000], Loss: -10.272491455078125, Learning Rate: 0.01\n",
      "Epoch [10413/20000], Loss: -10.27679443359375, Learning Rate: 0.01\n",
      "Epoch [10414/20000], Loss: -10.28106689453125, Learning Rate: 0.01\n",
      "Epoch [10415/20000], Loss: -10.28497314453125, Learning Rate: 0.01\n",
      "Epoch [10416/20000], Loss: -10.28875732421875, Learning Rate: 0.01\n",
      "Epoch [10417/20000], Loss: -10.292404174804688, Learning Rate: 0.01\n",
      "Epoch [10418/20000], Loss: -10.295989990234375, Learning Rate: 0.01\n",
      "Epoch [10419/20000], Loss: -10.299758911132812, Learning Rate: 0.01\n",
      "Epoch [10420/20000], Loss: -10.303680419921875, Learning Rate: 0.01\n",
      "Epoch [10421/20000], Loss: -10.307647705078125, Learning Rate: 0.01\n",
      "Epoch [10422/20000], Loss: -10.311798095703125, Learning Rate: 0.01\n",
      "Epoch [10423/20000], Loss: -10.315826416015625, Learning Rate: 0.01\n",
      "Epoch [10424/20000], Loss: -10.319717407226562, Learning Rate: 0.01\n",
      "Epoch [10425/20000], Loss: -10.323593139648438, Learning Rate: 0.01\n",
      "Epoch [10426/20000], Loss: -10.327438354492188, Learning Rate: 0.01\n",
      "Epoch [10427/20000], Loss: -10.331130981445312, Learning Rate: 0.01\n",
      "Epoch [10428/20000], Loss: -10.3349609375, Learning Rate: 0.01\n",
      "Epoch [10429/20000], Loss: -10.338714599609375, Learning Rate: 0.01\n",
      "Epoch [10430/20000], Loss: -10.342498779296875, Learning Rate: 0.01\n",
      "Epoch [10431/20000], Loss: -10.346511840820312, Learning Rate: 0.01\n",
      "Epoch [10432/20000], Loss: -10.350448608398438, Learning Rate: 0.01\n",
      "Epoch [10433/20000], Loss: -10.354415893554688, Learning Rate: 0.01\n",
      "Epoch [10434/20000], Loss: -10.35809326171875, Learning Rate: 0.01\n",
      "Epoch [10435/20000], Loss: -10.362014770507812, Learning Rate: 0.01\n",
      "Epoch [10436/20000], Loss: -10.36590576171875, Learning Rate: 0.01\n",
      "Epoch [10437/20000], Loss: -10.369644165039062, Learning Rate: 0.01\n",
      "Epoch [10438/20000], Loss: -10.373382568359375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10439/20000], Loss: -10.377304077148438, Learning Rate: 0.01\n",
      "Epoch [10440/20000], Loss: -10.381210327148438, Learning Rate: 0.01\n",
      "Epoch [10441/20000], Loss: -10.385040283203125, Learning Rate: 0.01\n",
      "Epoch [10442/20000], Loss: -10.388946533203125, Learning Rate: 0.01\n",
      "Epoch [10443/20000], Loss: -10.392776489257812, Learning Rate: 0.01\n",
      "Epoch [10444/20000], Loss: -10.396697998046875, Learning Rate: 0.01\n",
      "Epoch [10445/20000], Loss: -10.400543212890625, Learning Rate: 0.01\n",
      "Epoch [10446/20000], Loss: -10.404388427734375, Learning Rate: 0.01\n",
      "Epoch [10447/20000], Loss: -10.408248901367188, Learning Rate: 0.01\n",
      "Epoch [10448/20000], Loss: -10.41204833984375, Learning Rate: 0.01\n",
      "Epoch [10449/20000], Loss: -10.4158935546875, Learning Rate: 0.01\n",
      "Epoch [10450/20000], Loss: -10.419692993164062, Learning Rate: 0.01\n",
      "Epoch [10451/20000], Loss: -10.423538208007812, Learning Rate: 0.01\n",
      "Epoch [10452/20000], Loss: -10.427398681640625, Learning Rate: 0.01\n",
      "Epoch [10453/20000], Loss: -10.4312744140625, Learning Rate: 0.01\n",
      "Epoch [10454/20000], Loss: -10.43524169921875, Learning Rate: 0.01\n",
      "Epoch [10455/20000], Loss: -10.439056396484375, Learning Rate: 0.01\n",
      "Epoch [10456/20000], Loss: -10.442794799804688, Learning Rate: 0.01\n",
      "Epoch [10457/20000], Loss: -10.446731567382812, Learning Rate: 0.01\n",
      "Epoch [10458/20000], Loss: -10.450531005859375, Learning Rate: 0.01\n",
      "Epoch [10459/20000], Loss: -10.454452514648438, Learning Rate: 0.01\n",
      "Epoch [10460/20000], Loss: -10.458251953125, Learning Rate: 0.01\n",
      "Epoch [10461/20000], Loss: -10.462081909179688, Learning Rate: 0.01\n",
      "Epoch [10462/20000], Loss: -10.465866088867188, Learning Rate: 0.01\n",
      "Epoch [10463/20000], Loss: -10.469863891601562, Learning Rate: 0.01\n",
      "Epoch [10464/20000], Loss: -10.473617553710938, Learning Rate: 0.01\n",
      "Epoch [10465/20000], Loss: -10.477432250976562, Learning Rate: 0.01\n",
      "Epoch [10466/20000], Loss: -10.4813232421875, Learning Rate: 0.01\n",
      "Epoch [10467/20000], Loss: -10.485198974609375, Learning Rate: 0.01\n",
      "Epoch [10468/20000], Loss: -10.488983154296875, Learning Rate: 0.01\n",
      "Epoch [10469/20000], Loss: -10.492919921875, Learning Rate: 0.01\n",
      "Epoch [10470/20000], Loss: -10.496780395507812, Learning Rate: 0.01\n",
      "Epoch [10471/20000], Loss: -10.500534057617188, Learning Rate: 0.01\n",
      "Epoch [10472/20000], Loss: -10.504440307617188, Learning Rate: 0.01\n",
      "Epoch [10473/20000], Loss: -10.508209228515625, Learning Rate: 0.01\n",
      "Epoch [10474/20000], Loss: -10.51202392578125, Learning Rate: 0.01\n",
      "Epoch [10475/20000], Loss: -10.516036987304688, Learning Rate: 0.01\n",
      "Epoch [10476/20000], Loss: -10.519775390625, Learning Rate: 0.01\n",
      "Epoch [10477/20000], Loss: -10.523651123046875, Learning Rate: 0.01\n",
      "Epoch [10478/20000], Loss: -10.527450561523438, Learning Rate: 0.01\n",
      "Epoch [10479/20000], Loss: -10.53125, Learning Rate: 0.01\n",
      "Epoch [10480/20000], Loss: -10.535232543945312, Learning Rate: 0.01\n",
      "Epoch [10481/20000], Loss: -10.53900146484375, Learning Rate: 0.01\n",
      "Epoch [10482/20000], Loss: -10.54290771484375, Learning Rate: 0.01\n",
      "Epoch [10483/20000], Loss: -10.546646118164062, Learning Rate: 0.01\n",
      "Epoch [10484/20000], Loss: -10.550521850585938, Learning Rate: 0.01\n",
      "Epoch [10485/20000], Loss: -10.554351806640625, Learning Rate: 0.01\n",
      "Epoch [10486/20000], Loss: -10.558120727539062, Learning Rate: 0.01\n",
      "Epoch [10487/20000], Loss: -10.561859130859375, Learning Rate: 0.01\n",
      "Epoch [10488/20000], Loss: -10.56549072265625, Learning Rate: 0.01\n",
      "Epoch [10489/20000], Loss: -10.569091796875, Learning Rate: 0.01\n",
      "Epoch [10490/20000], Loss: -10.5726318359375, Learning Rate: 0.01\n",
      "Epoch [10491/20000], Loss: -10.576080322265625, Learning Rate: 0.01\n",
      "Epoch [10492/20000], Loss: -10.579132080078125, Learning Rate: 0.01\n",
      "Epoch [10493/20000], Loss: -10.58197021484375, Learning Rate: 0.01\n",
      "Epoch [10494/20000], Loss: -10.584121704101562, Learning Rate: 0.01\n",
      "Epoch [10495/20000], Loss: -10.585647583007812, Learning Rate: 0.01\n",
      "Epoch [10496/20000], Loss: -10.585922241210938, Learning Rate: 0.01\n",
      "Epoch [10497/20000], Loss: -10.584365844726562, Learning Rate: 0.01\n",
      "Epoch [10498/20000], Loss: -10.579864501953125, Learning Rate: 0.01\n",
      "Epoch [10499/20000], Loss: -10.571243286132812, Learning Rate: 0.01\n",
      "Epoch [10500/20000], Loss: -10.55596923828125, Learning Rate: 0.01\n",
      "Epoch [10501/20000], Loss: -10.53033447265625, Learning Rate: 0.01\n",
      "Epoch [10502/20000], Loss: -10.488800048828125, Learning Rate: 0.01\n",
      "Epoch [10503/20000], Loss: -10.422454833984375, Learning Rate: 0.01\n",
      "Epoch [10504/20000], Loss: -10.317642211914062, Learning Rate: 0.01\n",
      "Epoch [10505/20000], Loss: -10.15350341796875, Learning Rate: 0.01\n",
      "Epoch [10506/20000], Loss: -9.897872924804688, Learning Rate: 0.01\n",
      "Epoch [10507/20000], Loss: -9.504837036132812, Learning Rate: 0.01\n",
      "Epoch [10508/20000], Loss: -8.908477783203125, Learning Rate: 0.01\n",
      "Epoch [10509/20000], Loss: -8.029205322265625, Learning Rate: 0.01\n",
      "Epoch [10510/20000], Loss: -6.782135009765625, Learning Rate: 0.01\n",
      "Epoch [10511/20000], Loss: -5.1417083740234375, Learning Rate: 0.01\n",
      "Epoch [10512/20000], Loss: -3.216461181640625, Learning Rate: 0.01\n",
      "Epoch [10513/20000], Loss: -1.43218994140625, Learning Rate: 0.01\n",
      "Epoch [10514/20000], Loss: -0.5100860595703125, Learning Rate: 0.01\n",
      "Epoch [10515/20000], Loss: -1.2746429443359375, Learning Rate: 0.01\n",
      "Epoch [10516/20000], Loss: -3.84881591796875, Learning Rate: 0.01\n",
      "Epoch [10517/20000], Loss: -7.2403717041015625, Learning Rate: 0.01\n",
      "Epoch [10518/20000], Loss: -9.75042724609375, Learning Rate: 0.01\n",
      "Epoch [10519/20000], Loss: -10.288955688476562, Learning Rate: 0.01\n",
      "Epoch [10520/20000], Loss: -9.126266479492188, Learning Rate: 0.01\n",
      "Epoch [10521/20000], Loss: -7.541961669921875, Learning Rate: 0.01\n",
      "Epoch [10522/20000], Loss: -6.88934326171875, Learning Rate: 0.01\n",
      "Epoch [10523/20000], Loss: -7.668243408203125, Learning Rate: 0.01\n",
      "Epoch [10524/20000], Loss: -9.2535400390625, Learning Rate: 0.01\n",
      "Epoch [10525/20000], Loss: -10.434906005859375, Learning Rate: 0.01\n",
      "Epoch [10526/20000], Loss: -10.483993530273438, Learning Rate: 0.01\n",
      "Epoch [10527/20000], Loss: -9.673416137695312, Learning Rate: 0.01\n",
      "Epoch [10528/20000], Loss: -8.897308349609375, Learning Rate: 0.01\n",
      "Epoch [10529/20000], Loss: -8.87860107421875, Learning Rate: 0.01\n",
      "Epoch [10530/20000], Loss: -9.600112915039062, Learning Rate: 0.01\n",
      "Epoch [10531/20000], Loss: -10.432830810546875, Learning Rate: 0.01\n",
      "Epoch [10532/20000], Loss: -10.741958618164062, Learning Rate: 0.01\n",
      "Epoch [10533/20000], Loss: -10.43707275390625, Learning Rate: 0.01\n",
      "Epoch [10534/20000], Loss: -9.94854736328125, Learning Rate: 0.01\n",
      "Epoch [10535/20000], Loss: -9.771560668945312, Learning Rate: 0.01\n",
      "Epoch [10536/20000], Loss: -10.044769287109375, Learning Rate: 0.01\n",
      "Epoch [10537/20000], Loss: -10.491729736328125, Learning Rate: 0.01\n",
      "Epoch [10538/20000], Loss: -10.736846923828125, Learning Rate: 0.01\n",
      "Epoch [10539/20000], Loss: -10.645950317382812, Learning Rate: 0.01\n",
      "Epoch [10540/20000], Loss: -10.398452758789062, Learning Rate: 0.01\n",
      "Epoch [10541/20000], Loss: -10.270965576171875, Learning Rate: 0.01\n",
      "Epoch [10542/20000], Loss: -10.38153076171875, Learning Rate: 0.01\n",
      "Epoch [10543/20000], Loss: -10.61724853515625, Learning Rate: 0.01\n",
      "Epoch [10544/20000], Loss: -10.77374267578125, Learning Rate: 0.01\n",
      "Epoch [10545/20000], Loss: -10.750930786132812, Learning Rate: 0.01\n",
      "Epoch [10546/20000], Loss: -10.619857788085938, Learning Rate: 0.01\n",
      "Epoch [10547/20000], Loss: -10.52947998046875, Learning Rate: 0.01\n",
      "Epoch [10548/20000], Loss: -10.565322875976562, Learning Rate: 0.01\n",
      "Epoch [10549/20000], Loss: -10.688629150390625, Learning Rate: 0.01\n",
      "Epoch [10550/20000], Loss: -10.79278564453125, Learning Rate: 0.01\n",
      "Epoch [10551/20000], Loss: -10.807022094726562, Learning Rate: 0.01\n",
      "Epoch [10552/20000], Loss: -10.749252319335938, Learning Rate: 0.01\n",
      "Epoch [10553/20000], Loss: -10.693084716796875, Learning Rate: 0.01\n",
      "Epoch [10554/20000], Loss: -10.695205688476562, Learning Rate: 0.01\n",
      "Epoch [10555/20000], Loss: -10.751541137695312, Learning Rate: 0.01\n",
      "Epoch [10556/20000], Loss: -10.81268310546875, Learning Rate: 0.01\n",
      "Epoch [10557/20000], Loss: -10.834197998046875, Learning Rate: 0.01\n",
      "Epoch [10558/20000], Loss: -10.813034057617188, Learning Rate: 0.01\n",
      "Epoch [10559/20000], Loss: -10.782058715820312, Learning Rate: 0.01\n",
      "Epoch [10560/20000], Loss: -10.775634765625, Learning Rate: 0.01\n",
      "Epoch [10561/20000], Loss: -10.801055908203125, Learning Rate: 0.01\n",
      "Epoch [10562/20000], Loss: -10.837905883789062, Learning Rate: 0.01\n",
      "Epoch [10563/20000], Loss: -10.859832763671875, Learning Rate: 0.01\n",
      "Epoch [10564/20000], Loss: -10.857406616210938, Learning Rate: 0.01\n",
      "Epoch [10565/20000], Loss: -10.842147827148438, Learning Rate: 0.01\n",
      "Epoch [10566/20000], Loss: -10.833389282226562, Learning Rate: 0.01\n",
      "Epoch [10567/20000], Loss: -10.841293334960938, Learning Rate: 0.01\n",
      "Epoch [10568/20000], Loss: -10.860687255859375, Learning Rate: 0.01\n",
      "Epoch [10569/20000], Loss: -10.878097534179688, Learning Rate: 0.01\n",
      "Epoch [10570/20000], Loss: -10.884414672851562, Learning Rate: 0.01\n",
      "Epoch [10571/20000], Loss: -10.880905151367188, Learning Rate: 0.01\n",
      "Epoch [10572/20000], Loss: -10.875991821289062, Learning Rate: 0.01\n",
      "Epoch [10573/20000], Loss: -10.877899169921875, Learning Rate: 0.01\n",
      "Epoch [10574/20000], Loss: -10.887374877929688, Learning Rate: 0.01\n",
      "Epoch [10575/20000], Loss: -10.899337768554688, Learning Rate: 0.01\n",
      "Epoch [10576/20000], Loss: -10.907623291015625, Learning Rate: 0.01\n",
      "Epoch [10577/20000], Loss: -10.909912109375, Learning Rate: 0.01\n",
      "Epoch [10578/20000], Loss: -10.90875244140625, Learning Rate: 0.01\n",
      "Epoch [10579/20000], Loss: -10.90899658203125, Learning Rate: 0.01\n",
      "Epoch [10580/20000], Loss: -10.913116455078125, Learning Rate: 0.01\n",
      "Epoch [10581/20000], Loss: -10.920501708984375, Learning Rate: 0.01\n",
      "Epoch [10582/20000], Loss: -10.92803955078125, Learning Rate: 0.01\n",
      "Epoch [10583/20000], Loss: -10.933090209960938, Learning Rate: 0.01\n",
      "Epoch [10584/20000], Loss: -10.935577392578125, Learning Rate: 0.01\n",
      "Epoch [10585/20000], Loss: -10.936981201171875, Learning Rate: 0.01\n",
      "Epoch [10586/20000], Loss: -10.939483642578125, Learning Rate: 0.01\n",
      "Epoch [10587/20000], Loss: -10.94384765625, Learning Rate: 0.01\n",
      "Epoch [10588/20000], Loss: -10.949356079101562, Learning Rate: 0.01\n",
      "Epoch [10589/20000], Loss: -10.954757690429688, Learning Rate: 0.01\n",
      "Epoch [10590/20000], Loss: -10.958816528320312, Learning Rate: 0.01\n",
      "Epoch [10591/20000], Loss: -10.96160888671875, Learning Rate: 0.01\n",
      "Epoch [10592/20000], Loss: -10.964141845703125, Learning Rate: 0.01\n",
      "Epoch [10593/20000], Loss: -10.967239379882812, Learning Rate: 0.01\n",
      "Epoch [10594/20000], Loss: -10.971328735351562, Learning Rate: 0.01\n",
      "Epoch [10595/20000], Loss: -10.9759521484375, Learning Rate: 0.01\n",
      "Epoch [10596/20000], Loss: -10.980422973632812, Learning Rate: 0.01\n",
      "Epoch [10597/20000], Loss: -10.98431396484375, Learning Rate: 0.01\n",
      "Epoch [10598/20000], Loss: -10.98748779296875, Learning Rate: 0.01\n",
      "Epoch [10599/20000], Loss: -10.990570068359375, Learning Rate: 0.01\n",
      "Epoch [10600/20000], Loss: -10.994110107421875, Learning Rate: 0.01\n",
      "Epoch [10601/20000], Loss: -10.997894287109375, Learning Rate: 0.01\n",
      "Epoch [10602/20000], Loss: -11.001998901367188, Learning Rate: 0.01\n",
      "Epoch [10603/20000], Loss: -11.005996704101562, Learning Rate: 0.01\n",
      "Epoch [10604/20000], Loss: -11.009765625, Learning Rate: 0.01\n",
      "Epoch [10605/20000], Loss: -11.013198852539062, Learning Rate: 0.01\n",
      "Epoch [10606/20000], Loss: -11.016632080078125, Learning Rate: 0.01\n",
      "Epoch [10607/20000], Loss: -11.020187377929688, Learning Rate: 0.01\n",
      "Epoch [10608/20000], Loss: -11.023834228515625, Learning Rate: 0.01\n",
      "Epoch [10609/20000], Loss: -11.02777099609375, Learning Rate: 0.01\n",
      "Epoch [10610/20000], Loss: -11.031494140625, Learning Rate: 0.01\n",
      "Epoch [10611/20000], Loss: -11.035308837890625, Learning Rate: 0.01\n",
      "Epoch [10612/20000], Loss: -11.038833618164062, Learning Rate: 0.01\n",
      "Epoch [10613/20000], Loss: -11.042373657226562, Learning Rate: 0.01\n",
      "Epoch [10614/20000], Loss: -11.045944213867188, Learning Rate: 0.01\n",
      "Epoch [10615/20000], Loss: -11.049514770507812, Learning Rate: 0.01\n",
      "Epoch [10616/20000], Loss: -11.053298950195312, Learning Rate: 0.01\n",
      "Epoch [10617/20000], Loss: -11.056976318359375, Learning Rate: 0.01\n",
      "Epoch [10618/20000], Loss: -11.06060791015625, Learning Rate: 0.01\n",
      "Epoch [10619/20000], Loss: -11.064376831054688, Learning Rate: 0.01\n",
      "Epoch [10620/20000], Loss: -11.0679931640625, Learning Rate: 0.01\n",
      "Epoch [10621/20000], Loss: -11.071563720703125, Learning Rate: 0.01\n",
      "Epoch [10622/20000], Loss: -11.075225830078125, Learning Rate: 0.01\n",
      "Epoch [10623/20000], Loss: -11.0787353515625, Learning Rate: 0.01\n",
      "Epoch [10624/20000], Loss: -11.08245849609375, Learning Rate: 0.01\n",
      "Epoch [10625/20000], Loss: -11.086181640625, Learning Rate: 0.01\n",
      "Epoch [10626/20000], Loss: -11.089859008789062, Learning Rate: 0.01\n",
      "Epoch [10627/20000], Loss: -11.093460083007812, Learning Rate: 0.01\n",
      "Epoch [10628/20000], Loss: -11.09716796875, Learning Rate: 0.01\n",
      "Epoch [10629/20000], Loss: -11.100616455078125, Learning Rate: 0.01\n",
      "Epoch [10630/20000], Loss: -11.104324340820312, Learning Rate: 0.01\n",
      "Epoch [10631/20000], Loss: -11.107894897460938, Learning Rate: 0.01\n",
      "Epoch [10632/20000], Loss: -11.111648559570312, Learning Rate: 0.01\n",
      "Epoch [10633/20000], Loss: -11.115371704101562, Learning Rate: 0.01\n",
      "Epoch [10634/20000], Loss: -11.118972778320312, Learning Rate: 0.01\n",
      "Epoch [10635/20000], Loss: -11.122711181640625, Learning Rate: 0.01\n",
      "Epoch [10636/20000], Loss: -11.126251220703125, Learning Rate: 0.01\n",
      "Epoch [10637/20000], Loss: -11.1297607421875, Learning Rate: 0.01\n",
      "Epoch [10638/20000], Loss: -11.13348388671875, Learning Rate: 0.01\n",
      "Epoch [10639/20000], Loss: -11.137130737304688, Learning Rate: 0.01\n",
      "Epoch [10640/20000], Loss: -11.140777587890625, Learning Rate: 0.01\n",
      "Epoch [10641/20000], Loss: -11.144424438476562, Learning Rate: 0.01\n",
      "Epoch [10642/20000], Loss: -11.148040771484375, Learning Rate: 0.01\n",
      "Epoch [10643/20000], Loss: -11.151718139648438, Learning Rate: 0.01\n",
      "Epoch [10644/20000], Loss: -11.15533447265625, Learning Rate: 0.01\n",
      "Epoch [10645/20000], Loss: -11.158935546875, Learning Rate: 0.01\n",
      "Epoch [10646/20000], Loss: -11.16253662109375, Learning Rate: 0.01\n",
      "Epoch [10647/20000], Loss: -11.166305541992188, Learning Rate: 0.01\n",
      "Epoch [10648/20000], Loss: -11.169952392578125, Learning Rate: 0.01\n",
      "Epoch [10649/20000], Loss: -11.173553466796875, Learning Rate: 0.01\n",
      "Epoch [10650/20000], Loss: -11.177154541015625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10651/20000], Loss: -11.18084716796875, Learning Rate: 0.01\n",
      "Epoch [10652/20000], Loss: -11.184478759765625, Learning Rate: 0.01\n",
      "Epoch [10653/20000], Loss: -11.188156127929688, Learning Rate: 0.01\n",
      "Epoch [10654/20000], Loss: -11.191558837890625, Learning Rate: 0.01\n",
      "Epoch [10655/20000], Loss: -11.195297241210938, Learning Rate: 0.01\n",
      "Epoch [10656/20000], Loss: -11.198806762695312, Learning Rate: 0.01\n",
      "Epoch [10657/20000], Loss: -11.2025146484375, Learning Rate: 0.01\n",
      "Epoch [10658/20000], Loss: -11.206039428710938, Learning Rate: 0.01\n",
      "Epoch [10659/20000], Loss: -11.209716796875, Learning Rate: 0.01\n",
      "Epoch [10660/20000], Loss: -11.213226318359375, Learning Rate: 0.01\n",
      "Epoch [10661/20000], Loss: -11.216720581054688, Learning Rate: 0.01\n",
      "Epoch [10662/20000], Loss: -11.220138549804688, Learning Rate: 0.01\n",
      "Epoch [10663/20000], Loss: -11.223541259765625, Learning Rate: 0.01\n",
      "Epoch [10664/20000], Loss: -11.226898193359375, Learning Rate: 0.01\n",
      "Epoch [10665/20000], Loss: -11.230026245117188, Learning Rate: 0.01\n",
      "Epoch [10666/20000], Loss: -11.23309326171875, Learning Rate: 0.01\n",
      "Epoch [10667/20000], Loss: -11.235885620117188, Learning Rate: 0.01\n",
      "Epoch [10668/20000], Loss: -11.238311767578125, Learning Rate: 0.01\n",
      "Epoch [10669/20000], Loss: -11.240249633789062, Learning Rate: 0.01\n",
      "Epoch [10670/20000], Loss: -11.241439819335938, Learning Rate: 0.01\n",
      "Epoch [10671/20000], Loss: -11.24163818359375, Learning Rate: 0.01\n",
      "Epoch [10672/20000], Loss: -11.240341186523438, Learning Rate: 0.01\n",
      "Epoch [10673/20000], Loss: -11.23687744140625, Learning Rate: 0.01\n",
      "Epoch [10674/20000], Loss: -11.2303466796875, Learning Rate: 0.01\n",
      "Epoch [10675/20000], Loss: -11.21917724609375, Learning Rate: 0.01\n",
      "Epoch [10676/20000], Loss: -11.201507568359375, Learning Rate: 0.01\n",
      "Epoch [10677/20000], Loss: -11.173934936523438, Learning Rate: 0.01\n",
      "Epoch [10678/20000], Loss: -11.131912231445312, Learning Rate: 0.01\n",
      "Epoch [10679/20000], Loss: -11.069168090820312, Learning Rate: 0.01\n",
      "Epoch [10680/20000], Loss: -10.975906372070312, Learning Rate: 0.01\n",
      "Epoch [10681/20000], Loss: -10.83892822265625, Learning Rate: 0.01\n",
      "Epoch [10682/20000], Loss: -10.639907836914062, Learning Rate: 0.01\n",
      "Epoch [10683/20000], Loss: -10.356842041015625, Learning Rate: 0.01\n",
      "Epoch [10684/20000], Loss: -9.962646484375, Learning Rate: 0.01\n",
      "Epoch [10685/20000], Loss: -9.438568115234375, Learning Rate: 0.01\n",
      "Epoch [10686/20000], Loss: -8.77838134765625, Learning Rate: 0.01\n",
      "Epoch [10687/20000], Loss: -8.034027099609375, Learning Rate: 0.01\n",
      "Epoch [10688/20000], Loss: -7.317596435546875, Learning Rate: 0.01\n",
      "Epoch [10689/20000], Loss: -6.8526763916015625, Learning Rate: 0.01\n",
      "Epoch [10690/20000], Loss: -6.8624420166015625, Learning Rate: 0.01\n",
      "Epoch [10691/20000], Loss: -7.490509033203125, Learning Rate: 0.01\n",
      "Epoch [10692/20000], Loss: -8.591644287109375, Learning Rate: 0.01\n",
      "Epoch [10693/20000], Loss: -9.790298461914062, Learning Rate: 0.01\n",
      "Epoch [10694/20000], Loss: -10.66278076171875, Learning Rate: 0.01\n",
      "Epoch [10695/20000], Loss: -11.000640869140625, Learning Rate: 0.01\n",
      "Epoch [10696/20000], Loss: -10.882614135742188, Learning Rate: 0.01\n",
      "Epoch [10697/20000], Loss: -10.562713623046875, Learning Rate: 0.01\n",
      "Epoch [10698/20000], Loss: -10.302642822265625, Learning Rate: 0.01\n",
      "Epoch [10699/20000], Loss: -10.23809814453125, Learning Rate: 0.01\n",
      "Epoch [10700/20000], Loss: -10.356231689453125, Learning Rate: 0.01\n",
      "Epoch [10701/20000], Loss: -10.547393798828125, Learning Rate: 0.01\n",
      "Epoch [10702/20000], Loss: -10.711151123046875, Learning Rate: 0.01\n",
      "Epoch [10703/20000], Loss: -10.819793701171875, Learning Rate: 0.01\n",
      "Epoch [10704/20000], Loss: -10.90521240234375, Learning Rate: 0.01\n",
      "Epoch [10705/20000], Loss: -11.004165649414062, Learning Rate: 0.01\n",
      "Epoch [10706/20000], Loss: -11.110977172851562, Learning Rate: 0.01\n",
      "Epoch [10707/20000], Loss: -11.1844482421875, Learning Rate: 0.01\n",
      "Epoch [10708/20000], Loss: -11.185028076171875, Learning Rate: 0.01\n",
      "Epoch [10709/20000], Loss: -11.117034912109375, Learning Rate: 0.01\n",
      "Epoch [10710/20000], Loss: -11.03289794921875, Learning Rate: 0.01\n",
      "Epoch [10711/20000], Loss: -10.999969482421875, Learning Rate: 0.01\n",
      "Epoch [10712/20000], Loss: -11.056045532226562, Learning Rate: 0.01\n",
      "Epoch [10713/20000], Loss: -11.182586669921875, Learning Rate: 0.01\n",
      "Epoch [10714/20000], Loss: -11.31927490234375, Learning Rate: 0.01\n",
      "Epoch [10715/20000], Loss: -11.402206420898438, Learning Rate: 0.01\n",
      "Epoch [10716/20000], Loss: -11.403976440429688, Learning Rate: 0.01\n",
      "Epoch [10717/20000], Loss: -11.344039916992188, Learning Rate: 0.01\n",
      "Epoch [10718/20000], Loss: -11.2724609375, Learning Rate: 0.01\n",
      "Epoch [10719/20000], Loss: -11.236465454101562, Learning Rate: 0.01\n",
      "Epoch [10720/20000], Loss: -11.2548828125, Learning Rate: 0.01\n",
      "Epoch [10721/20000], Loss: -11.313705444335938, Learning Rate: 0.01\n",
      "Epoch [10722/20000], Loss: -11.380523681640625, Learning Rate: 0.01\n",
      "Epoch [10723/20000], Loss: -11.425674438476562, Learning Rate: 0.01\n",
      "Epoch [10724/20000], Loss: -11.438125610351562, Learning Rate: 0.01\n",
      "Epoch [10725/20000], Loss: -11.425933837890625, Learning Rate: 0.01\n",
      "Epoch [10726/20000], Loss: -11.4072265625, Learning Rate: 0.01\n",
      "Epoch [10727/20000], Loss: -11.397048950195312, Learning Rate: 0.01\n",
      "Epoch [10728/20000], Loss: -11.40087890625, Learning Rate: 0.01\n",
      "Epoch [10729/20000], Loss: -11.414581298828125, Learning Rate: 0.01\n",
      "Epoch [10730/20000], Loss: -11.430557250976562, Learning Rate: 0.01\n",
      "Epoch [10731/20000], Loss: -11.443313598632812, Learning Rate: 0.01\n",
      "Epoch [10732/20000], Loss: -11.4515380859375, Learning Rate: 0.01\n",
      "Epoch [10733/20000], Loss: -11.458038330078125, Learning Rate: 0.01\n",
      "Epoch [10734/20000], Loss: -11.464691162109375, Learning Rate: 0.01\n",
      "Epoch [10735/20000], Loss: -11.472732543945312, Learning Rate: 0.01\n",
      "Epoch [10736/20000], Loss: -11.4798583984375, Learning Rate: 0.01\n",
      "Epoch [10737/20000], Loss: -11.484344482421875, Learning Rate: 0.01\n",
      "Epoch [10738/20000], Loss: -11.485382080078125, Learning Rate: 0.01\n",
      "Epoch [10739/20000], Loss: -11.484786987304688, Learning Rate: 0.01\n",
      "Epoch [10740/20000], Loss: -11.485153198242188, Learning Rate: 0.01\n",
      "Epoch [10741/20000], Loss: -11.488693237304688, Learning Rate: 0.01\n",
      "Epoch [10742/20000], Loss: -11.49609375, Learning Rate: 0.01\n",
      "Epoch [10743/20000], Loss: -11.505844116210938, Learning Rate: 0.01\n",
      "Epoch [10744/20000], Loss: -11.515213012695312, Learning Rate: 0.01\n",
      "Epoch [10745/20000], Loss: -11.522232055664062, Learning Rate: 0.01\n",
      "Epoch [10746/20000], Loss: -11.52581787109375, Learning Rate: 0.01\n",
      "Epoch [10747/20000], Loss: -11.526901245117188, Learning Rate: 0.01\n",
      "Epoch [10748/20000], Loss: -11.52728271484375, Learning Rate: 0.01\n",
      "Epoch [10749/20000], Loss: -11.528457641601562, Learning Rate: 0.01\n",
      "Epoch [10750/20000], Loss: -11.53143310546875, Learning Rate: 0.01\n",
      "Epoch [10751/20000], Loss: -11.536224365234375, Learning Rate: 0.01\n",
      "Epoch [10752/20000], Loss: -11.541900634765625, Learning Rate: 0.01\n",
      "Epoch [10753/20000], Loss: -11.547561645507812, Learning Rate: 0.01\n",
      "Epoch [10754/20000], Loss: -11.552352905273438, Learning Rate: 0.01\n",
      "Epoch [10755/20000], Loss: -11.55633544921875, Learning Rate: 0.01\n",
      "Epoch [10756/20000], Loss: -11.559600830078125, Learning Rate: 0.01\n",
      "Epoch [10757/20000], Loss: -11.56292724609375, Learning Rate: 0.01\n",
      "Epoch [10758/20000], Loss: -11.566177368164062, Learning Rate: 0.01\n",
      "Epoch [10759/20000], Loss: -11.569686889648438, Learning Rate: 0.01\n",
      "Epoch [10760/20000], Loss: -11.573379516601562, Learning Rate: 0.01\n",
      "Epoch [10761/20000], Loss: -11.576766967773438, Learning Rate: 0.01\n",
      "Epoch [10762/20000], Loss: -11.580291748046875, Learning Rate: 0.01\n",
      "Epoch [10763/20000], Loss: -11.583694458007812, Learning Rate: 0.01\n",
      "Epoch [10764/20000], Loss: -11.587127685546875, Learning Rate: 0.01\n",
      "Epoch [10765/20000], Loss: -11.590667724609375, Learning Rate: 0.01\n",
      "Epoch [10766/20000], Loss: -11.594436645507812, Learning Rate: 0.01\n",
      "Epoch [10767/20000], Loss: -11.5982666015625, Learning Rate: 0.01\n",
      "Epoch [10768/20000], Loss: -11.602218627929688, Learning Rate: 0.01\n",
      "Epoch [10769/20000], Loss: -11.60601806640625, Learning Rate: 0.01\n",
      "Epoch [10770/20000], Loss: -11.609649658203125, Learning Rate: 0.01\n",
      "Epoch [10771/20000], Loss: -11.613174438476562, Learning Rate: 0.01\n",
      "Epoch [10772/20000], Loss: -11.616546630859375, Learning Rate: 0.01\n",
      "Epoch [10773/20000], Loss: -11.619735717773438, Learning Rate: 0.01\n",
      "Epoch [10774/20000], Loss: -11.623031616210938, Learning Rate: 0.01\n",
      "Epoch [10775/20000], Loss: -11.626358032226562, Learning Rate: 0.01\n",
      "Epoch [10776/20000], Loss: -11.629898071289062, Learning Rate: 0.01\n",
      "Epoch [10777/20000], Loss: -11.633499145507812, Learning Rate: 0.01\n",
      "Epoch [10778/20000], Loss: -11.63702392578125, Learning Rate: 0.01\n",
      "Epoch [10779/20000], Loss: -11.64068603515625, Learning Rate: 0.01\n",
      "Epoch [10780/20000], Loss: -11.64434814453125, Learning Rate: 0.01\n",
      "Epoch [10781/20000], Loss: -11.647796630859375, Learning Rate: 0.01\n",
      "Epoch [10782/20000], Loss: -11.6513671875, Learning Rate: 0.01\n",
      "Epoch [10783/20000], Loss: -11.654754638671875, Learning Rate: 0.01\n",
      "Epoch [10784/20000], Loss: -11.658340454101562, Learning Rate: 0.01\n",
      "Epoch [10785/20000], Loss: -11.661788940429688, Learning Rate: 0.01\n",
      "Epoch [10786/20000], Loss: -11.665374755859375, Learning Rate: 0.01\n",
      "Epoch [10787/20000], Loss: -11.668838500976562, Learning Rate: 0.01\n",
      "Epoch [10788/20000], Loss: -11.672317504882812, Learning Rate: 0.01\n",
      "Epoch [10789/20000], Loss: -11.675735473632812, Learning Rate: 0.01\n",
      "Epoch [10790/20000], Loss: -11.679214477539062, Learning Rate: 0.01\n",
      "Epoch [10791/20000], Loss: -11.682662963867188, Learning Rate: 0.01\n",
      "Epoch [10792/20000], Loss: -11.68621826171875, Learning Rate: 0.01\n",
      "Epoch [10793/20000], Loss: -11.689529418945312, Learning Rate: 0.01\n",
      "Epoch [10794/20000], Loss: -11.693115234375, Learning Rate: 0.01\n",
      "Epoch [10795/20000], Loss: -11.69647216796875, Learning Rate: 0.01\n",
      "Epoch [10796/20000], Loss: -11.700164794921875, Learning Rate: 0.01\n",
      "Epoch [10797/20000], Loss: -11.703598022460938, Learning Rate: 0.01\n",
      "Epoch [10798/20000], Loss: -11.70703125, Learning Rate: 0.01\n",
      "Epoch [10799/20000], Loss: -11.710525512695312, Learning Rate: 0.01\n",
      "Epoch [10800/20000], Loss: -11.714065551757812, Learning Rate: 0.01\n",
      "Epoch [10801/20000], Loss: -11.717575073242188, Learning Rate: 0.01\n",
      "Epoch [10802/20000], Loss: -11.72100830078125, Learning Rate: 0.01\n",
      "Epoch [10803/20000], Loss: -11.724472045898438, Learning Rate: 0.01\n",
      "Epoch [10804/20000], Loss: -11.72796630859375, Learning Rate: 0.01\n",
      "Epoch [10805/20000], Loss: -11.731552124023438, Learning Rate: 0.01\n",
      "Epoch [10806/20000], Loss: -11.73486328125, Learning Rate: 0.01\n",
      "Epoch [10807/20000], Loss: -11.73846435546875, Learning Rate: 0.01\n",
      "Epoch [10808/20000], Loss: -11.741897583007812, Learning Rate: 0.01\n",
      "Epoch [10809/20000], Loss: -11.745437622070312, Learning Rate: 0.01\n",
      "Epoch [10810/20000], Loss: -11.748947143554688, Learning Rate: 0.01\n",
      "Epoch [10811/20000], Loss: -11.752426147460938, Learning Rate: 0.01\n",
      "Epoch [10812/20000], Loss: -11.755889892578125, Learning Rate: 0.01\n",
      "Epoch [10813/20000], Loss: -11.759323120117188, Learning Rate: 0.01\n",
      "Epoch [10814/20000], Loss: -11.762786865234375, Learning Rate: 0.01\n",
      "Epoch [10815/20000], Loss: -11.76629638671875, Learning Rate: 0.01\n",
      "Epoch [10816/20000], Loss: -11.769760131835938, Learning Rate: 0.01\n",
      "Epoch [10817/20000], Loss: -11.773239135742188, Learning Rate: 0.01\n",
      "Epoch [10818/20000], Loss: -11.77679443359375, Learning Rate: 0.01\n",
      "Epoch [10819/20000], Loss: -11.780227661132812, Learning Rate: 0.01\n",
      "Epoch [10820/20000], Loss: -11.78369140625, Learning Rate: 0.01\n",
      "Epoch [10821/20000], Loss: -11.787033081054688, Learning Rate: 0.01\n",
      "Epoch [10822/20000], Loss: -11.790664672851562, Learning Rate: 0.01\n",
      "Epoch [10823/20000], Loss: -11.79412841796875, Learning Rate: 0.01\n",
      "Epoch [10824/20000], Loss: -11.797622680664062, Learning Rate: 0.01\n",
      "Epoch [10825/20000], Loss: -11.801071166992188, Learning Rate: 0.01\n",
      "Epoch [10826/20000], Loss: -11.804519653320312, Learning Rate: 0.01\n",
      "Epoch [10827/20000], Loss: -11.807907104492188, Learning Rate: 0.01\n",
      "Epoch [10828/20000], Loss: -11.811538696289062, Learning Rate: 0.01\n",
      "Epoch [10829/20000], Loss: -11.814895629882812, Learning Rate: 0.01\n",
      "Epoch [10830/20000], Loss: -11.818405151367188, Learning Rate: 0.01\n",
      "Epoch [10831/20000], Loss: -11.821945190429688, Learning Rate: 0.01\n",
      "Epoch [10832/20000], Loss: -11.825363159179688, Learning Rate: 0.01\n",
      "Epoch [10833/20000], Loss: -11.828887939453125, Learning Rate: 0.01\n",
      "Epoch [10834/20000], Loss: -11.83233642578125, Learning Rate: 0.01\n",
      "Epoch [10835/20000], Loss: -11.835739135742188, Learning Rate: 0.01\n",
      "Epoch [10836/20000], Loss: -11.83935546875, Learning Rate: 0.01\n",
      "Epoch [10837/20000], Loss: -11.842697143554688, Learning Rate: 0.01\n",
      "Epoch [10838/20000], Loss: -11.846115112304688, Learning Rate: 0.01\n",
      "Epoch [10839/20000], Loss: -11.849655151367188, Learning Rate: 0.01\n",
      "Epoch [10840/20000], Loss: -11.85308837890625, Learning Rate: 0.01\n",
      "Epoch [10841/20000], Loss: -11.8564453125, Learning Rate: 0.01\n",
      "Epoch [10842/20000], Loss: -11.859878540039062, Learning Rate: 0.01\n",
      "Epoch [10843/20000], Loss: -11.863235473632812, Learning Rate: 0.01\n",
      "Epoch [10844/20000], Loss: -11.866455078125, Learning Rate: 0.01\n",
      "Epoch [10845/20000], Loss: -11.869842529296875, Learning Rate: 0.01\n",
      "Epoch [10846/20000], Loss: -11.873031616210938, Learning Rate: 0.01\n",
      "Epoch [10847/20000], Loss: -11.876174926757812, Learning Rate: 0.01\n",
      "Epoch [10848/20000], Loss: -11.879150390625, Learning Rate: 0.01\n",
      "Epoch [10849/20000], Loss: -11.881988525390625, Learning Rate: 0.01\n",
      "Epoch [10850/20000], Loss: -11.884689331054688, Learning Rate: 0.01\n",
      "Epoch [10851/20000], Loss: -11.886978149414062, Learning Rate: 0.01\n",
      "Epoch [10852/20000], Loss: -11.888778686523438, Learning Rate: 0.01\n",
      "Epoch [10853/20000], Loss: -11.890151977539062, Learning Rate: 0.01\n",
      "Epoch [10854/20000], Loss: -11.89056396484375, Learning Rate: 0.01\n",
      "Epoch [10855/20000], Loss: -11.88958740234375, Learning Rate: 0.01\n",
      "Epoch [10856/20000], Loss: -11.88720703125, Learning Rate: 0.01\n",
      "Epoch [10857/20000], Loss: -11.882110595703125, Learning Rate: 0.01\n",
      "Epoch [10858/20000], Loss: -11.873641967773438, Learning Rate: 0.01\n",
      "Epoch [10859/20000], Loss: -11.860031127929688, Learning Rate: 0.01\n",
      "Epoch [10860/20000], Loss: -11.839111328125, Learning Rate: 0.01\n",
      "Epoch [10861/20000], Loss: -11.8077392578125, Learning Rate: 0.01\n",
      "Epoch [10862/20000], Loss: -11.761367797851562, Learning Rate: 0.01\n",
      "Epoch [10863/20000], Loss: -11.693023681640625, Learning Rate: 0.01\n",
      "Epoch [10864/20000], Loss: -11.593063354492188, Learning Rate: 0.01\n",
      "Epoch [10865/20000], Loss: -11.44805908203125, Learning Rate: 0.01\n",
      "Epoch [10866/20000], Loss: -11.23907470703125, Learning Rate: 0.01\n",
      "Epoch [10867/20000], Loss: -10.940872192382812, Learning Rate: 0.01\n",
      "Epoch [10868/20000], Loss: -10.522079467773438, Learning Rate: 0.01\n",
      "Epoch [10869/20000], Loss: -9.946304321289062, Learning Rate: 0.01\n",
      "Epoch [10870/20000], Loss: -9.184219360351562, Learning Rate: 0.01\n",
      "Epoch [10871/20000], Loss: -8.22723388671875, Learning Rate: 0.01\n",
      "Epoch [10872/20000], Loss: -7.1307373046875, Learning Rate: 0.01\n",
      "Epoch [10873/20000], Loss: -6.044464111328125, Learning Rate: 0.01\n",
      "Epoch [10874/20000], Loss: -5.2600860595703125, Learning Rate: 0.01\n",
      "Epoch [10875/20000], Loss: -5.1311492919921875, Learning Rate: 0.01\n",
      "Epoch [10876/20000], Loss: -5.9340057373046875, Learning Rate: 0.01\n",
      "Epoch [10877/20000], Loss: -7.5864715576171875, Learning Rate: 0.01\n",
      "Epoch [10878/20000], Loss: -9.582244873046875, Learning Rate: 0.01\n",
      "Epoch [10879/20000], Loss: -11.187881469726562, Learning Rate: 0.01\n",
      "Epoch [10880/20000], Loss: -11.880508422851562, Learning Rate: 0.01\n",
      "Epoch [10881/20000], Loss: -11.62103271484375, Learning Rate: 0.01\n",
      "Epoch [10882/20000], Loss: -10.811508178710938, Learning Rate: 0.01\n",
      "Epoch [10883/20000], Loss: -10.032333374023438, Learning Rate: 0.01\n",
      "Epoch [10884/20000], Loss: -9.744857788085938, Learning Rate: 0.01\n",
      "Epoch [10885/20000], Loss: -10.089935302734375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10886/20000], Loss: -10.838363647460938, Learning Rate: 0.01\n",
      "Epoch [10887/20000], Loss: -11.563400268554688, Learning Rate: 0.01\n",
      "Epoch [10888/20000], Loss: -11.913864135742188, Learning Rate: 0.01\n",
      "Epoch [10889/20000], Loss: -11.812957763671875, Learning Rate: 0.01\n",
      "Epoch [10890/20000], Loss: -11.454757690429688, Learning Rate: 0.01\n",
      "Epoch [10891/20000], Loss: -11.140701293945312, Learning Rate: 0.01\n",
      "Epoch [10892/20000], Loss: -11.08929443359375, Learning Rate: 0.01\n",
      "Epoch [10893/20000], Loss: -11.320419311523438, Learning Rate: 0.01\n",
      "Epoch [10894/20000], Loss: -11.67724609375, Learning Rate: 0.01\n",
      "Epoch [10895/20000], Loss: -11.9486083984375, Learning Rate: 0.01\n",
      "Epoch [10896/20000], Loss: -12.010528564453125, Learning Rate: 0.01\n",
      "Epoch [10897/20000], Loss: -11.8863525390625, Learning Rate: 0.01\n",
      "Epoch [10898/20000], Loss: -11.706344604492188, Learning Rate: 0.01\n",
      "Epoch [10899/20000], Loss: -11.609390258789062, Learning Rate: 0.01\n",
      "Epoch [10900/20000], Loss: -11.658432006835938, Learning Rate: 0.01\n",
      "Epoch [10901/20000], Loss: -11.815994262695312, Learning Rate: 0.01\n",
      "Epoch [10902/20000], Loss: -11.983734130859375, Learning Rate: 0.01\n",
      "Epoch [10903/20000], Loss: -12.073028564453125, Learning Rate: 0.01\n",
      "Epoch [10904/20000], Loss: -12.056915283203125, Learning Rate: 0.01\n",
      "Epoch [10905/20000], Loss: -11.975173950195312, Learning Rate: 0.01\n",
      "Epoch [10906/20000], Loss: -11.897918701171875, Learning Rate: 0.01\n",
      "Epoch [10907/20000], Loss: -11.878402709960938, Learning Rate: 0.01\n",
      "Epoch [10908/20000], Loss: -11.926239013671875, Learning Rate: 0.01\n",
      "Epoch [10909/20000], Loss: -12.008560180664062, Learning Rate: 0.01\n",
      "Epoch [10910/20000], Loss: -12.0787353515625, Learning Rate: 0.01\n",
      "Epoch [10911/20000], Loss: -12.105819702148438, Learning Rate: 0.01\n",
      "Epoch [10912/20000], Loss: -12.08905029296875, Learning Rate: 0.01\n",
      "Epoch [10913/20000], Loss: -12.052978515625, Learning Rate: 0.01\n",
      "Epoch [10914/20000], Loss: -12.027328491210938, Learning Rate: 0.01\n",
      "Epoch [10915/20000], Loss: -12.029693603515625, Learning Rate: 0.01\n",
      "Epoch [10916/20000], Loss: -12.057540893554688, Learning Rate: 0.01\n",
      "Epoch [10917/20000], Loss: -12.094314575195312, Learning Rate: 0.01\n",
      "Epoch [10918/20000], Loss: -12.121627807617188, Learning Rate: 0.01\n",
      "Epoch [10919/20000], Loss: -12.129623413085938, Learning Rate: 0.01\n",
      "Epoch [10920/20000], Loss: -12.121475219726562, Learning Rate: 0.01\n",
      "Epoch [10921/20000], Loss: -12.108673095703125, Learning Rate: 0.01\n",
      "Epoch [10922/20000], Loss: -12.102752685546875, Learning Rate: 0.01\n",
      "Epoch [10923/20000], Loss: -12.108993530273438, Learning Rate: 0.01\n",
      "Epoch [10924/20000], Loss: -12.12493896484375, Learning Rate: 0.01\n",
      "Epoch [10925/20000], Loss: -12.142623901367188, Learning Rate: 0.01\n",
      "Epoch [10926/20000], Loss: -12.155120849609375, Learning Rate: 0.01\n",
      "Epoch [10927/20000], Loss: -12.159088134765625, Learning Rate: 0.01\n",
      "Epoch [10928/20000], Loss: -12.156524658203125, Learning Rate: 0.01\n",
      "Epoch [10929/20000], Loss: -12.152374267578125, Learning Rate: 0.01\n",
      "Epoch [10930/20000], Loss: -12.151321411132812, Learning Rate: 0.01\n",
      "Epoch [10931/20000], Loss: -12.155853271484375, Learning Rate: 0.01\n",
      "Epoch [10932/20000], Loss: -12.16473388671875, Learning Rate: 0.01\n",
      "Epoch [10933/20000], Loss: -12.174728393554688, Learning Rate: 0.01\n",
      "Epoch [10934/20000], Loss: -12.182815551757812, Learning Rate: 0.01\n",
      "Epoch [10935/20000], Loss: -12.187423706054688, Learning Rate: 0.01\n",
      "Epoch [10936/20000], Loss: -12.188735961914062, Learning Rate: 0.01\n",
      "Epoch [10937/20000], Loss: -12.188812255859375, Learning Rate: 0.01\n",
      "Epoch [10938/20000], Loss: -12.18975830078125, Learning Rate: 0.01\n",
      "Epoch [10939/20000], Loss: -12.192596435546875, Learning Rate: 0.01\n",
      "Epoch [10940/20000], Loss: -12.197525024414062, Learning Rate: 0.01\n",
      "Epoch [10941/20000], Loss: -12.20343017578125, Learning Rate: 0.01\n",
      "Epoch [10942/20000], Loss: -12.209197998046875, Learning Rate: 0.01\n",
      "Epoch [10943/20000], Loss: -12.2135009765625, Learning Rate: 0.01\n",
      "Epoch [10944/20000], Loss: -12.2166748046875, Learning Rate: 0.01\n",
      "Epoch [10945/20000], Loss: -12.218887329101562, Learning Rate: 0.01\n",
      "Epoch [10946/20000], Loss: -12.221160888671875, Learning Rate: 0.01\n",
      "Epoch [10947/20000], Loss: -12.223785400390625, Learning Rate: 0.01\n",
      "Epoch [10948/20000], Loss: -12.227462768554688, Learning Rate: 0.01\n",
      "Epoch [10949/20000], Loss: -12.231689453125, Learning Rate: 0.01\n",
      "Epoch [10950/20000], Loss: -12.236007690429688, Learning Rate: 0.01\n",
      "Epoch [10951/20000], Loss: -12.240081787109375, Learning Rate: 0.01\n",
      "Epoch [10952/20000], Loss: -12.243682861328125, Learning Rate: 0.01\n",
      "Epoch [10953/20000], Loss: -12.246749877929688, Learning Rate: 0.01\n",
      "Epoch [10954/20000], Loss: -12.249649047851562, Learning Rate: 0.01\n",
      "Epoch [10955/20000], Loss: -12.252426147460938, Learning Rate: 0.01\n",
      "Epoch [10956/20000], Loss: -12.255508422851562, Learning Rate: 0.01\n",
      "Epoch [10957/20000], Loss: -12.258941650390625, Learning Rate: 0.01\n",
      "Epoch [10958/20000], Loss: -12.262603759765625, Learning Rate: 0.01\n",
      "Epoch [10959/20000], Loss: -12.266342163085938, Learning Rate: 0.01\n",
      "Epoch [10960/20000], Loss: -12.269927978515625, Learning Rate: 0.01\n",
      "Epoch [10961/20000], Loss: -12.273483276367188, Learning Rate: 0.01\n",
      "Epoch [10962/20000], Loss: -12.276763916015625, Learning Rate: 0.01\n",
      "Epoch [10963/20000], Loss: -12.279937744140625, Learning Rate: 0.01\n",
      "Epoch [10964/20000], Loss: -12.283004760742188, Learning Rate: 0.01\n",
      "Epoch [10965/20000], Loss: -12.286209106445312, Learning Rate: 0.01\n",
      "Epoch [10966/20000], Loss: -12.28955078125, Learning Rate: 0.01\n",
      "Epoch [10967/20000], Loss: -12.292953491210938, Learning Rate: 0.01\n",
      "Epoch [10968/20000], Loss: -12.296356201171875, Learning Rate: 0.01\n",
      "Epoch [10969/20000], Loss: -12.299850463867188, Learning Rate: 0.01\n",
      "Epoch [10970/20000], Loss: -12.30322265625, Learning Rate: 0.01\n",
      "Epoch [10971/20000], Loss: -12.3065185546875, Learning Rate: 0.01\n",
      "Epoch [10972/20000], Loss: -12.309829711914062, Learning Rate: 0.01\n",
      "Epoch [10973/20000], Loss: -12.3131103515625, Learning Rate: 0.01\n",
      "Epoch [10974/20000], Loss: -12.31622314453125, Learning Rate: 0.01\n",
      "Epoch [10975/20000], Loss: -12.319564819335938, Learning Rate: 0.01\n",
      "Epoch [10976/20000], Loss: -12.322967529296875, Learning Rate: 0.01\n",
      "Epoch [10977/20000], Loss: -12.3262939453125, Learning Rate: 0.01\n",
      "Epoch [10978/20000], Loss: -12.3297119140625, Learning Rate: 0.01\n",
      "Epoch [10979/20000], Loss: -12.332992553710938, Learning Rate: 0.01\n",
      "Epoch [10980/20000], Loss: -12.33636474609375, Learning Rate: 0.01\n",
      "Epoch [10981/20000], Loss: -12.339599609375, Learning Rate: 0.01\n",
      "Epoch [10982/20000], Loss: -12.343002319335938, Learning Rate: 0.01\n",
      "Epoch [10983/20000], Loss: -12.346298217773438, Learning Rate: 0.01\n",
      "Epoch [10984/20000], Loss: -12.34954833984375, Learning Rate: 0.01\n",
      "Epoch [10985/20000], Loss: -12.352813720703125, Learning Rate: 0.01\n",
      "Epoch [10986/20000], Loss: -12.356048583984375, Learning Rate: 0.01\n",
      "Epoch [10987/20000], Loss: -12.359466552734375, Learning Rate: 0.01\n",
      "Epoch [10988/20000], Loss: -12.362762451171875, Learning Rate: 0.01\n",
      "Epoch [10989/20000], Loss: -12.366134643554688, Learning Rate: 0.01\n",
      "Epoch [10990/20000], Loss: -12.369354248046875, Learning Rate: 0.01\n",
      "Epoch [10991/20000], Loss: -12.372772216796875, Learning Rate: 0.01\n",
      "Epoch [10992/20000], Loss: -12.376113891601562, Learning Rate: 0.01\n",
      "Epoch [10993/20000], Loss: -12.379440307617188, Learning Rate: 0.01\n",
      "Epoch [10994/20000], Loss: -12.382720947265625, Learning Rate: 0.01\n",
      "Epoch [10995/20000], Loss: -12.385986328125, Learning Rate: 0.01\n",
      "Epoch [10996/20000], Loss: -12.389266967773438, Learning Rate: 0.01\n",
      "Epoch [10997/20000], Loss: -12.39251708984375, Learning Rate: 0.01\n",
      "Epoch [10998/20000], Loss: -12.39593505859375, Learning Rate: 0.01\n",
      "Epoch [10999/20000], Loss: -12.399139404296875, Learning Rate: 0.01\n",
      "Epoch [11000/20000], Loss: -12.4024658203125, Learning Rate: 0.01\n",
      "Epoch [11001/20000], Loss: -12.405807495117188, Learning Rate: 0.01\n",
      "Epoch [11002/20000], Loss: -12.4090576171875, Learning Rate: 0.01\n",
      "Epoch [11003/20000], Loss: -12.412307739257812, Learning Rate: 0.01\n",
      "Epoch [11004/20000], Loss: -12.415786743164062, Learning Rate: 0.01\n",
      "Epoch [11005/20000], Loss: -12.419036865234375, Learning Rate: 0.01\n",
      "Epoch [11006/20000], Loss: -12.422378540039062, Learning Rate: 0.01\n",
      "Epoch [11007/20000], Loss: -12.425537109375, Learning Rate: 0.01\n",
      "Epoch [11008/20000], Loss: -12.428939819335938, Learning Rate: 0.01\n",
      "Epoch [11009/20000], Loss: -12.43212890625, Learning Rate: 0.01\n",
      "Epoch [11010/20000], Loss: -12.4354248046875, Learning Rate: 0.01\n",
      "Epoch [11011/20000], Loss: -12.43865966796875, Learning Rate: 0.01\n",
      "Epoch [11012/20000], Loss: -12.441986083984375, Learning Rate: 0.01\n",
      "Epoch [11013/20000], Loss: -12.445220947265625, Learning Rate: 0.01\n",
      "Epoch [11014/20000], Loss: -12.448501586914062, Learning Rate: 0.01\n",
      "Epoch [11015/20000], Loss: -12.451690673828125, Learning Rate: 0.01\n",
      "Epoch [11016/20000], Loss: -12.454864501953125, Learning Rate: 0.01\n",
      "Epoch [11017/20000], Loss: -12.457870483398438, Learning Rate: 0.01\n",
      "Epoch [11018/20000], Loss: -12.460845947265625, Learning Rate: 0.01\n",
      "Epoch [11019/20000], Loss: -12.463851928710938, Learning Rate: 0.01\n",
      "Epoch [11020/20000], Loss: -12.466476440429688, Learning Rate: 0.01\n",
      "Epoch [11021/20000], Loss: -12.4691162109375, Learning Rate: 0.01\n",
      "Epoch [11022/20000], Loss: -12.471389770507812, Learning Rate: 0.01\n",
      "Epoch [11023/20000], Loss: -12.473281860351562, Learning Rate: 0.01\n",
      "Epoch [11024/20000], Loss: -12.474502563476562, Learning Rate: 0.01\n",
      "Epoch [11025/20000], Loss: -12.47515869140625, Learning Rate: 0.01\n",
      "Epoch [11026/20000], Loss: -12.474472045898438, Learning Rate: 0.01\n",
      "Epoch [11027/20000], Loss: -12.472091674804688, Learning Rate: 0.01\n",
      "Epoch [11028/20000], Loss: -12.467437744140625, Learning Rate: 0.01\n",
      "Epoch [11029/20000], Loss: -12.45947265625, Learning Rate: 0.01\n",
      "Epoch [11030/20000], Loss: -12.446578979492188, Learning Rate: 0.01\n",
      "Epoch [11031/20000], Loss: -12.426712036132812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11032/20000], Loss: -12.3963623046875, Learning Rate: 0.01\n",
      "Epoch [11033/20000], Loss: -12.351303100585938, Learning Rate: 0.01\n",
      "Epoch [11034/20000], Loss: -12.285064697265625, Learning Rate: 0.01\n",
      "Epoch [11035/20000], Loss: -12.18890380859375, Learning Rate: 0.01\n",
      "Epoch [11036/20000], Loss: -12.050613403320312, Learning Rate: 0.01\n",
      "Epoch [11037/20000], Loss: -11.85662841796875, Learning Rate: 0.01\n",
      "Epoch [11038/20000], Loss: -11.588836669921875, Learning Rate: 0.01\n",
      "Epoch [11039/20000], Loss: -11.237197875976562, Learning Rate: 0.01\n",
      "Epoch [11040/20000], Loss: -10.796371459960938, Learning Rate: 0.01\n",
      "Epoch [11041/20000], Loss: -10.302810668945312, Learning Rate: 0.01\n",
      "Epoch [11042/20000], Loss: -9.82330322265625, Learning Rate: 0.01\n",
      "Epoch [11043/20000], Loss: -9.509735107421875, Learning Rate: 0.01\n",
      "Epoch [11044/20000], Loss: -9.506515502929688, Learning Rate: 0.01\n",
      "Epoch [11045/20000], Loss: -9.940567016601562, Learning Rate: 0.01\n",
      "Epoch [11046/20000], Loss: -10.730224609375, Learning Rate: 0.01\n",
      "Epoch [11047/20000], Loss: -11.633590698242188, Learning Rate: 0.01\n",
      "Epoch [11048/20000], Loss: -12.312698364257812, Learning Rate: 0.01\n",
      "Epoch [11049/20000], Loss: -12.558212280273438, Learning Rate: 0.01\n",
      "Epoch [11050/20000], Loss: -12.383132934570312, Learning Rate: 0.01\n",
      "Epoch [11051/20000], Loss: -11.987686157226562, Learning Rate: 0.01\n",
      "Epoch [11052/20000], Loss: -11.637298583984375, Learning Rate: 0.01\n",
      "Epoch [11053/20000], Loss: -11.52825927734375, Learning Rate: 0.01\n",
      "Epoch [11054/20000], Loss: -11.714126586914062, Learning Rate: 0.01\n",
      "Epoch [11055/20000], Loss: -12.077590942382812, Learning Rate: 0.01\n",
      "Epoch [11056/20000], Loss: -12.421981811523438, Learning Rate: 0.01\n",
      "Epoch [11057/20000], Loss: -12.58624267578125, Learning Rate: 0.01\n",
      "Epoch [11058/20000], Loss: -12.535934448242188, Learning Rate: 0.01\n",
      "Epoch [11059/20000], Loss: -12.3599853515625, Learning Rate: 0.01\n",
      "Epoch [11060/20000], Loss: -12.197845458984375, Learning Rate: 0.01\n",
      "Epoch [11061/20000], Loss: -12.155410766601562, Learning Rate: 0.01\n",
      "Epoch [11062/20000], Loss: -12.250457763671875, Learning Rate: 0.01\n",
      "Epoch [11063/20000], Loss: -12.419342041015625, Learning Rate: 0.01\n",
      "Epoch [11064/20000], Loss: -12.564697265625, Learning Rate: 0.01\n",
      "Epoch [11065/20000], Loss: -12.61956787109375, Learning Rate: 0.01\n",
      "Epoch [11066/20000], Loss: -12.5809326171875, Learning Rate: 0.01\n",
      "Epoch [11067/20000], Loss: -12.499053955078125, Learning Rate: 0.01\n",
      "Epoch [11068/20000], Loss: -12.438156127929688, Learning Rate: 0.01\n",
      "Epoch [11069/20000], Loss: -12.437820434570312, Learning Rate: 0.01\n",
      "Epoch [11070/20000], Loss: -12.495071411132812, Learning Rate: 0.01\n",
      "Epoch [11071/20000], Loss: -12.573379516601562, Learning Rate: 0.01\n",
      "Epoch [11072/20000], Loss: -12.630615234375, Learning Rate: 0.01\n",
      "Epoch [11073/20000], Loss: -12.644378662109375, Learning Rate: 0.01\n",
      "Epoch [11074/20000], Loss: -12.620849609375, Learning Rate: 0.01\n",
      "Epoch [11075/20000], Loss: -12.585708618164062, Learning Rate: 0.01\n",
      "Epoch [11076/20000], Loss: -12.566116333007812, Learning Rate: 0.01\n",
      "Epoch [11077/20000], Loss: -12.574478149414062, Learning Rate: 0.01\n",
      "Epoch [11078/20000], Loss: -12.605575561523438, Learning Rate: 0.01\n",
      "Epoch [11079/20000], Loss: -12.6416015625, Learning Rate: 0.01\n",
      "Epoch [11080/20000], Loss: -12.66571044921875, Learning Rate: 0.01\n",
      "Epoch [11081/20000], Loss: -12.670257568359375, Learning Rate: 0.01\n",
      "Epoch [11082/20000], Loss: -12.660308837890625, Learning Rate: 0.01\n",
      "Epoch [11083/20000], Loss: -12.646957397460938, Learning Rate: 0.01\n",
      "Epoch [11084/20000], Loss: -12.641525268554688, Learning Rate: 0.01\n",
      "Epoch [11085/20000], Loss: -12.648269653320312, Learning Rate: 0.01\n",
      "Epoch [11086/20000], Loss: -12.664031982421875, Learning Rate: 0.01\n",
      "Epoch [11087/20000], Loss: -12.681289672851562, Learning Rate: 0.01\n",
      "Epoch [11088/20000], Loss: -12.693450927734375, Learning Rate: 0.01\n",
      "Epoch [11089/20000], Loss: -12.697311401367188, Learning Rate: 0.01\n",
      "Epoch [11090/20000], Loss: -12.694854736328125, Learning Rate: 0.01\n",
      "Epoch [11091/20000], Loss: -12.69085693359375, Learning Rate: 0.01\n",
      "Epoch [11092/20000], Loss: -12.69000244140625, Learning Rate: 0.01\n",
      "Epoch [11093/20000], Loss: -12.6942138671875, Learning Rate: 0.01\n",
      "Epoch [11094/20000], Loss: -12.702224731445312, Learning Rate: 0.01\n",
      "Epoch [11095/20000], Loss: -12.711654663085938, Learning Rate: 0.01\n",
      "Epoch [11096/20000], Loss: -12.719146728515625, Learning Rate: 0.01\n",
      "Epoch [11097/20000], Loss: -12.723526000976562, Learning Rate: 0.01\n",
      "Epoch [11098/20000], Loss: -12.724853515625, Learning Rate: 0.01\n",
      "Epoch [11099/20000], Loss: -12.72515869140625, Learning Rate: 0.01\n",
      "Epoch [11100/20000], Loss: -12.726028442382812, Learning Rate: 0.01\n",
      "Epoch [11101/20000], Loss: -12.728866577148438, Learning Rate: 0.01\n",
      "Epoch [11102/20000], Loss: -12.733566284179688, Learning Rate: 0.01\n",
      "Epoch [11103/20000], Loss: -12.739395141601562, Learning Rate: 0.01\n",
      "Epoch [11104/20000], Loss: -12.744827270507812, Learning Rate: 0.01\n",
      "Epoch [11105/20000], Loss: -12.749130249023438, Learning Rate: 0.01\n",
      "Epoch [11106/20000], Loss: -12.752227783203125, Learning Rate: 0.01\n",
      "Epoch [11107/20000], Loss: -12.754379272460938, Learning Rate: 0.01\n",
      "Epoch [11108/20000], Loss: -12.756423950195312, Learning Rate: 0.01\n",
      "Epoch [11109/20000], Loss: -12.758956909179688, Learning Rate: 0.01\n",
      "Epoch [11110/20000], Loss: -12.76226806640625, Learning Rate: 0.01\n",
      "Epoch [11111/20000], Loss: -12.766265869140625, Learning Rate: 0.01\n",
      "Epoch [11112/20000], Loss: -12.770553588867188, Learning Rate: 0.01\n",
      "Epoch [11113/20000], Loss: -12.774581909179688, Learning Rate: 0.01\n",
      "Epoch [11114/20000], Loss: -12.778167724609375, Learning Rate: 0.01\n",
      "Epoch [11115/20000], Loss: -12.781219482421875, Learning Rate: 0.01\n",
      "Epoch [11116/20000], Loss: -12.7840576171875, Learning Rate: 0.01\n",
      "Epoch [11117/20000], Loss: -12.78680419921875, Learning Rate: 0.01\n",
      "Epoch [11118/20000], Loss: -12.789688110351562, Learning Rate: 0.01\n",
      "Epoch [11119/20000], Loss: -12.79296875, Learning Rate: 0.01\n",
      "Epoch [11120/20000], Loss: -12.796432495117188, Learning Rate: 0.01\n",
      "Epoch [11121/20000], Loss: -12.800079345703125, Learning Rate: 0.01\n",
      "Epoch [11122/20000], Loss: -12.803497314453125, Learning Rate: 0.01\n",
      "Epoch [11123/20000], Loss: -12.8070068359375, Learning Rate: 0.01\n",
      "Epoch [11124/20000], Loss: -12.81011962890625, Learning Rate: 0.01\n",
      "Epoch [11125/20000], Loss: -12.81317138671875, Learning Rate: 0.01\n",
      "Epoch [11126/20000], Loss: -12.816207885742188, Learning Rate: 0.01\n",
      "Epoch [11127/20000], Loss: -12.819351196289062, Learning Rate: 0.01\n",
      "Epoch [11128/20000], Loss: -12.822402954101562, Learning Rate: 0.01\n",
      "Epoch [11129/20000], Loss: -12.82568359375, Learning Rate: 0.01\n",
      "Epoch [11130/20000], Loss: -12.829010009765625, Learning Rate: 0.01\n",
      "Epoch [11131/20000], Loss: -12.832412719726562, Learning Rate: 0.01\n",
      "Epoch [11132/20000], Loss: -12.835708618164062, Learning Rate: 0.01\n",
      "Epoch [11133/20000], Loss: -12.83880615234375, Learning Rate: 0.01\n",
      "Epoch [11134/20000], Loss: -12.841949462890625, Learning Rate: 0.01\n",
      "Epoch [11135/20000], Loss: -12.844940185546875, Learning Rate: 0.01\n",
      "Epoch [11136/20000], Loss: -12.8480224609375, Learning Rate: 0.01\n",
      "Epoch [11137/20000], Loss: -12.851028442382812, Learning Rate: 0.01\n",
      "Epoch [11138/20000], Loss: -12.853927612304688, Learning Rate: 0.01\n",
      "Epoch [11139/20000], Loss: -12.8568115234375, Learning Rate: 0.01\n",
      "Epoch [11140/20000], Loss: -12.859649658203125, Learning Rate: 0.01\n",
      "Epoch [11141/20000], Loss: -12.862442016601562, Learning Rate: 0.01\n",
      "Epoch [11142/20000], Loss: -12.864990234375, Learning Rate: 0.01\n",
      "Epoch [11143/20000], Loss: -12.867218017578125, Learning Rate: 0.01\n",
      "Epoch [11144/20000], Loss: -12.869140625, Learning Rate: 0.01\n",
      "Epoch [11145/20000], Loss: -12.870361328125, Learning Rate: 0.01\n",
      "Epoch [11146/20000], Loss: -12.871047973632812, Learning Rate: 0.01\n",
      "Epoch [11147/20000], Loss: -12.870620727539062, Learning Rate: 0.01\n",
      "Epoch [11148/20000], Loss: -12.868804931640625, Learning Rate: 0.01\n",
      "Epoch [11149/20000], Loss: -12.864883422851562, Learning Rate: 0.01\n",
      "Epoch [11150/20000], Loss: -12.858169555664062, Learning Rate: 0.01\n",
      "Epoch [11151/20000], Loss: -12.847152709960938, Learning Rate: 0.01\n",
      "Epoch [11152/20000], Loss: -12.830276489257812, Learning Rate: 0.01\n",
      "Epoch [11153/20000], Loss: -12.804595947265625, Learning Rate: 0.01\n",
      "Epoch [11154/20000], Loss: -12.766586303710938, Learning Rate: 0.01\n",
      "Epoch [11155/20000], Loss: -12.710494995117188, Learning Rate: 0.01\n",
      "Epoch [11156/20000], Loss: -12.628707885742188, Learning Rate: 0.01\n",
      "Epoch [11157/20000], Loss: -12.510086059570312, Learning Rate: 0.01\n",
      "Epoch [11158/20000], Loss: -12.339508056640625, Learning Rate: 0.01\n",
      "Epoch [11159/20000], Loss: -12.09625244140625, Learning Rate: 0.01\n",
      "Epoch [11160/20000], Loss: -11.755569458007812, Learning Rate: 0.01\n",
      "Epoch [11161/20000], Loss: -11.287582397460938, Learning Rate: 0.01\n",
      "Epoch [11162/20000], Loss: -10.669723510742188, Learning Rate: 0.01\n",
      "Epoch [11163/20000], Loss: -9.893035888671875, Learning Rate: 0.01\n",
      "Epoch [11164/20000], Loss: -9.003204345703125, Learning Rate: 0.01\n",
      "Epoch [11165/20000], Loss: -8.114639282226562, Learning Rate: 0.01\n",
      "Epoch [11166/20000], Loss: -7.4625091552734375, Learning Rate: 0.01\n",
      "Epoch [11167/20000], Loss: -7.3237457275390625, Learning Rate: 0.01\n",
      "Epoch [11168/20000], Loss: -7.933502197265625, Learning Rate: 0.01\n",
      "Epoch [11169/20000], Loss: -9.237899780273438, Learning Rate: 0.01\n",
      "Epoch [11170/20000], Loss: -10.856796264648438, Learning Rate: 0.01\n",
      "Epoch [11171/20000], Loss: -12.206192016601562, Learning Rate: 0.01\n",
      "Epoch [11172/20000], Loss: -12.850662231445312, Learning Rate: 0.01\n",
      "Epoch [11173/20000], Loss: -12.724960327148438, Learning Rate: 0.01\n",
      "Epoch [11174/20000], Loss: -12.11846923828125, Learning Rate: 0.01\n",
      "Epoch [11175/20000], Loss: -11.477981567382812, Learning Rate: 0.01\n",
      "Epoch [11176/20000], Loss: -11.176651000976562, Learning Rate: 0.01\n",
      "Epoch [11177/20000], Loss: -11.359619140625, Learning Rate: 0.01\n",
      "Epoch [11178/20000], Loss: -11.890762329101562, Learning Rate: 0.01\n",
      "Epoch [11179/20000], Loss: -12.468704223632812, Learning Rate: 0.01\n",
      "Epoch [11180/20000], Loss: -12.815277099609375, Learning Rate: 0.01\n",
      "Epoch [11181/20000], Loss: -12.833526611328125, Learning Rate: 0.01\n",
      "Epoch [11182/20000], Loss: -12.62664794921875, Learning Rate: 0.01\n",
      "Epoch [11183/20000], Loss: -12.396209716796875, Learning Rate: 0.01\n",
      "Epoch [11184/20000], Loss: -12.3096923828125, Learning Rate: 0.01\n",
      "Epoch [11185/20000], Loss: -12.411331176757812, Learning Rate: 0.01\n",
      "Epoch [11186/20000], Loss: -12.6241455078125, Learning Rate: 0.01\n",
      "Epoch [11187/20000], Loss: -12.818832397460938, Learning Rate: 0.01\n",
      "Epoch [11188/20000], Loss: -12.90411376953125, Learning Rate: 0.01\n",
      "Epoch [11189/20000], Loss: -12.874465942382812, Learning Rate: 0.01\n",
      "Epoch [11190/20000], Loss: -12.793548583984375, Learning Rate: 0.01\n",
      "Epoch [11191/20000], Loss: -12.741226196289062, Learning Rate: 0.01\n",
      "Epoch [11192/20000], Loss: -12.760025024414062, Learning Rate: 0.01\n",
      "Epoch [11193/20000], Loss: -12.83746337890625, Learning Rate: 0.01\n",
      "Epoch [11194/20000], Loss: -12.924774169921875, Learning Rate: 0.01\n",
      "Epoch [11195/20000], Loss: -12.975799560546875, Learning Rate: 0.01\n",
      "Epoch [11196/20000], Loss: -12.97503662109375, Learning Rate: 0.01\n",
      "Epoch [11197/20000], Loss: -12.942779541015625, Learning Rate: 0.01\n",
      "Epoch [11198/20000], Loss: -12.914764404296875, Learning Rate: 0.01\n",
      "Epoch [11199/20000], Loss: -12.917404174804688, Learning Rate: 0.01\n",
      "Epoch [11200/20000], Loss: -12.952789306640625, Learning Rate: 0.01\n",
      "Epoch [11201/20000], Loss: -13.001663208007812, Learning Rate: 0.01\n",
      "Epoch [11202/20000], Loss: -13.038299560546875, Learning Rate: 0.01\n",
      "Epoch [11203/20000], Loss: -13.048141479492188, Learning Rate: 0.01\n",
      "Epoch [11204/20000], Loss: -13.03472900390625, Learning Rate: 0.01\n",
      "Epoch [11205/20000], Loss: -13.0140380859375, Learning Rate: 0.01\n",
      "Epoch [11206/20000], Loss: -13.004318237304688, Learning Rate: 0.01\n",
      "Epoch [11207/20000], Loss: -13.013946533203125, Learning Rate: 0.01\n",
      "Epoch [11208/20000], Loss: -13.039108276367188, Learning Rate: 0.01\n",
      "Epoch [11209/20000], Loss: -13.067367553710938, Learning Rate: 0.01\n",
      "Epoch [11210/20000], Loss: -13.086257934570312, Learning Rate: 0.01\n",
      "Epoch [11211/20000], Loss: -13.090194702148438, Learning Rate: 0.01\n",
      "Epoch [11212/20000], Loss: -13.082595825195312, Learning Rate: 0.01\n",
      "Epoch [11213/20000], Loss: -13.072174072265625, Learning Rate: 0.01\n",
      "Epoch [11214/20000], Loss: -13.06768798828125, Learning Rate: 0.01\n",
      "Epoch [11215/20000], Loss: -13.073257446289062, Learning Rate: 0.01\n",
      "Epoch [11216/20000], Loss: -13.086715698242188, Learning Rate: 0.01\n",
      "Epoch [11217/20000], Loss: -13.102325439453125, Learning Rate: 0.01\n",
      "Epoch [11218/20000], Loss: -13.114151000976562, Learning Rate: 0.01\n",
      "Epoch [11219/20000], Loss: -13.119369506835938, Learning Rate: 0.01\n",
      "Epoch [11220/20000], Loss: -13.118515014648438, Learning Rate: 0.01\n",
      "Epoch [11221/20000], Loss: -13.11541748046875, Learning Rate: 0.01\n",
      "Epoch [11222/20000], Loss: -13.113861083984375, Learning Rate: 0.01\n",
      "Epoch [11223/20000], Loss: -13.1163330078125, Learning Rate: 0.01\n",
      "Epoch [11224/20000], Loss: -13.122512817382812, Learning Rate: 0.01\n",
      "Epoch [11225/20000], Loss: -13.130615234375, Learning Rate: 0.01\n",
      "Epoch [11226/20000], Loss: -13.138168334960938, Learning Rate: 0.01\n",
      "Epoch [11227/20000], Loss: -13.143402099609375, Learning Rate: 0.01\n",
      "Epoch [11228/20000], Loss: -13.146133422851562, Learning Rate: 0.01\n",
      "Epoch [11229/20000], Loss: -13.147430419921875, Learning Rate: 0.01\n",
      "Epoch [11230/20000], Loss: -13.148422241210938, Learning Rate: 0.01\n",
      "Epoch [11231/20000], Loss: -13.150482177734375, Learning Rate: 0.01\n",
      "Epoch [11232/20000], Loss: -13.153884887695312, Learning Rate: 0.01\n",
      "Epoch [11233/20000], Loss: -13.158279418945312, Learning Rate: 0.01\n",
      "Epoch [11234/20000], Loss: -13.163070678710938, Learning Rate: 0.01\n",
      "Epoch [11235/20000], Loss: -13.167388916015625, Learning Rate: 0.01\n",
      "Epoch [11236/20000], Loss: -13.170928955078125, Learning Rate: 0.01\n",
      "Epoch [11237/20000], Loss: -13.173721313476562, Learning Rate: 0.01\n",
      "Epoch [11238/20000], Loss: -13.17633056640625, Learning Rate: 0.01\n",
      "Epoch [11239/20000], Loss: -13.178970336914062, Learning Rate: 0.01\n",
      "Epoch [11240/20000], Loss: -13.181976318359375, Learning Rate: 0.01\n",
      "Epoch [11241/20000], Loss: -13.185287475585938, Learning Rate: 0.01\n",
      "Epoch [11242/20000], Loss: -13.188934326171875, Learning Rate: 0.01\n",
      "Epoch [11243/20000], Loss: -13.192337036132812, Learning Rate: 0.01\n",
      "Epoch [11244/20000], Loss: -13.195816040039062, Learning Rate: 0.01\n",
      "Epoch [11245/20000], Loss: -13.198867797851562, Learning Rate: 0.01\n",
      "Epoch [11246/20000], Loss: -13.201751708984375, Learning Rate: 0.01\n",
      "Epoch [11247/20000], Loss: -13.204681396484375, Learning Rate: 0.01\n",
      "Epoch [11248/20000], Loss: -13.207565307617188, Learning Rate: 0.01\n",
      "Epoch [11249/20000], Loss: -13.210800170898438, Learning Rate: 0.01\n",
      "Epoch [11250/20000], Loss: -13.214096069335938, Learning Rate: 0.01\n",
      "Epoch [11251/20000], Loss: -13.217437744140625, Learning Rate: 0.01\n",
      "Epoch [11252/20000], Loss: -13.220703125, Learning Rate: 0.01\n",
      "Epoch [11253/20000], Loss: -13.223846435546875, Learning Rate: 0.01\n",
      "Epoch [11254/20000], Loss: -13.2269287109375, Learning Rate: 0.01\n",
      "Epoch [11255/20000], Loss: -13.229888916015625, Learning Rate: 0.01\n",
      "Epoch [11256/20000], Loss: -13.23284912109375, Learning Rate: 0.01\n",
      "Epoch [11257/20000], Loss: -13.235855102539062, Learning Rate: 0.01\n",
      "Epoch [11258/20000], Loss: -13.238800048828125, Learning Rate: 0.01\n",
      "Epoch [11259/20000], Loss: -13.241958618164062, Learning Rate: 0.01\n",
      "Epoch [11260/20000], Loss: -13.245269775390625, Learning Rate: 0.01\n",
      "Epoch [11261/20000], Loss: -13.24835205078125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11262/20000], Loss: -13.251571655273438, Learning Rate: 0.01\n",
      "Epoch [11263/20000], Loss: -13.254684448242188, Learning Rate: 0.01\n",
      "Epoch [11264/20000], Loss: -13.257720947265625, Learning Rate: 0.01\n",
      "Epoch [11265/20000], Loss: -13.2608642578125, Learning Rate: 0.01\n",
      "Epoch [11266/20000], Loss: -13.263763427734375, Learning Rate: 0.01\n",
      "Epoch [11267/20000], Loss: -13.266830444335938, Learning Rate: 0.01\n",
      "Epoch [11268/20000], Loss: -13.269866943359375, Learning Rate: 0.01\n",
      "Epoch [11269/20000], Loss: -13.272994995117188, Learning Rate: 0.01\n",
      "Epoch [11270/20000], Loss: -13.276123046875, Learning Rate: 0.01\n",
      "Epoch [11271/20000], Loss: -13.279220581054688, Learning Rate: 0.01\n",
      "Epoch [11272/20000], Loss: -13.282333374023438, Learning Rate: 0.01\n",
      "Epoch [11273/20000], Loss: -13.285324096679688, Learning Rate: 0.01\n",
      "Epoch [11274/20000], Loss: -13.28851318359375, Learning Rate: 0.01\n",
      "Epoch [11275/20000], Loss: -13.291549682617188, Learning Rate: 0.01\n",
      "Epoch [11276/20000], Loss: -13.294601440429688, Learning Rate: 0.01\n",
      "Epoch [11277/20000], Loss: -13.297683715820312, Learning Rate: 0.01\n",
      "Epoch [11278/20000], Loss: -13.300704956054688, Learning Rate: 0.01\n",
      "Epoch [11279/20000], Loss: -13.3038330078125, Learning Rate: 0.01\n",
      "Epoch [11280/20000], Loss: -13.306930541992188, Learning Rate: 0.01\n",
      "Epoch [11281/20000], Loss: -13.310028076171875, Learning Rate: 0.01\n",
      "Epoch [11282/20000], Loss: -13.313156127929688, Learning Rate: 0.01\n",
      "Epoch [11283/20000], Loss: -13.316131591796875, Learning Rate: 0.01\n",
      "Epoch [11284/20000], Loss: -13.319290161132812, Learning Rate: 0.01\n",
      "Epoch [11285/20000], Loss: -13.32232666015625, Learning Rate: 0.01\n",
      "Epoch [11286/20000], Loss: -13.325408935546875, Learning Rate: 0.01\n",
      "Epoch [11287/20000], Loss: -13.328460693359375, Learning Rate: 0.01\n",
      "Epoch [11288/20000], Loss: -13.33148193359375, Learning Rate: 0.01\n",
      "Epoch [11289/20000], Loss: -13.334686279296875, Learning Rate: 0.01\n",
      "Epoch [11290/20000], Loss: -13.3377685546875, Learning Rate: 0.01\n",
      "Epoch [11291/20000], Loss: -13.340774536132812, Learning Rate: 0.01\n",
      "Epoch [11292/20000], Loss: -13.3438720703125, Learning Rate: 0.01\n",
      "Epoch [11293/20000], Loss: -13.346908569335938, Learning Rate: 0.01\n",
      "Epoch [11294/20000], Loss: -13.350006103515625, Learning Rate: 0.01\n",
      "Epoch [11295/20000], Loss: -13.353042602539062, Learning Rate: 0.01\n",
      "Epoch [11296/20000], Loss: -13.356185913085938, Learning Rate: 0.01\n",
      "Epoch [11297/20000], Loss: -13.359237670898438, Learning Rate: 0.01\n",
      "Epoch [11298/20000], Loss: -13.362335205078125, Learning Rate: 0.01\n",
      "Epoch [11299/20000], Loss: -13.365341186523438, Learning Rate: 0.01\n",
      "Epoch [11300/20000], Loss: -13.368515014648438, Learning Rate: 0.01\n",
      "Epoch [11301/20000], Loss: -13.37158203125, Learning Rate: 0.01\n",
      "Epoch [11302/20000], Loss: -13.374664306640625, Learning Rate: 0.01\n",
      "Epoch [11303/20000], Loss: -13.377716064453125, Learning Rate: 0.01\n",
      "Epoch [11304/20000], Loss: -13.380767822265625, Learning Rate: 0.01\n",
      "Epoch [11305/20000], Loss: -13.3839111328125, Learning Rate: 0.01\n",
      "Epoch [11306/20000], Loss: -13.386978149414062, Learning Rate: 0.01\n",
      "Epoch [11307/20000], Loss: -13.39007568359375, Learning Rate: 0.01\n",
      "Epoch [11308/20000], Loss: -13.393096923828125, Learning Rate: 0.01\n",
      "Epoch [11309/20000], Loss: -13.396163940429688, Learning Rate: 0.01\n",
      "Epoch [11310/20000], Loss: -13.399185180664062, Learning Rate: 0.01\n",
      "Epoch [11311/20000], Loss: -13.402328491210938, Learning Rate: 0.01\n",
      "Epoch [11312/20000], Loss: -13.40545654296875, Learning Rate: 0.01\n",
      "Epoch [11313/20000], Loss: -13.408554077148438, Learning Rate: 0.01\n",
      "Epoch [11314/20000], Loss: -13.41162109375, Learning Rate: 0.01\n",
      "Epoch [11315/20000], Loss: -13.414642333984375, Learning Rate: 0.01\n",
      "Epoch [11316/20000], Loss: -13.417739868164062, Learning Rate: 0.01\n",
      "Epoch [11317/20000], Loss: -13.420883178710938, Learning Rate: 0.01\n",
      "Epoch [11318/20000], Loss: -13.423965454101562, Learning Rate: 0.01\n",
      "Epoch [11319/20000], Loss: -13.427078247070312, Learning Rate: 0.01\n",
      "Epoch [11320/20000], Loss: -13.43011474609375, Learning Rate: 0.01\n",
      "Epoch [11321/20000], Loss: -13.433151245117188, Learning Rate: 0.01\n",
      "Epoch [11322/20000], Loss: -13.436248779296875, Learning Rate: 0.01\n",
      "Epoch [11323/20000], Loss: -13.43939208984375, Learning Rate: 0.01\n",
      "Epoch [11324/20000], Loss: -13.442459106445312, Learning Rate: 0.01\n",
      "Epoch [11325/20000], Loss: -13.445541381835938, Learning Rate: 0.01\n",
      "Epoch [11326/20000], Loss: -13.448532104492188, Learning Rate: 0.01\n",
      "Epoch [11327/20000], Loss: -13.451705932617188, Learning Rate: 0.01\n",
      "Epoch [11328/20000], Loss: -13.454788208007812, Learning Rate: 0.01\n",
      "Epoch [11329/20000], Loss: -13.457870483398438, Learning Rate: 0.01\n",
      "Epoch [11330/20000], Loss: -13.460968017578125, Learning Rate: 0.01\n",
      "Epoch [11331/20000], Loss: -13.463958740234375, Learning Rate: 0.01\n",
      "Epoch [11332/20000], Loss: -13.467010498046875, Learning Rate: 0.01\n",
      "Epoch [11333/20000], Loss: -13.470184326171875, Learning Rate: 0.01\n",
      "Epoch [11334/20000], Loss: -13.47320556640625, Learning Rate: 0.01\n",
      "Epoch [11335/20000], Loss: -13.476364135742188, Learning Rate: 0.01\n",
      "Epoch [11336/20000], Loss: -13.479415893554688, Learning Rate: 0.01\n",
      "Epoch [11337/20000], Loss: -13.482421875, Learning Rate: 0.01\n",
      "Epoch [11338/20000], Loss: -13.48553466796875, Learning Rate: 0.01\n",
      "Epoch [11339/20000], Loss: -13.488571166992188, Learning Rate: 0.01\n",
      "Epoch [11340/20000], Loss: -13.491744995117188, Learning Rate: 0.01\n",
      "Epoch [11341/20000], Loss: -13.494857788085938, Learning Rate: 0.01\n",
      "Epoch [11342/20000], Loss: -13.497787475585938, Learning Rate: 0.01\n",
      "Epoch [11343/20000], Loss: -13.5009765625, Learning Rate: 0.01\n",
      "Epoch [11344/20000], Loss: -13.503875732421875, Learning Rate: 0.01\n",
      "Epoch [11345/20000], Loss: -13.507095336914062, Learning Rate: 0.01\n",
      "Epoch [11346/20000], Loss: -13.510101318359375, Learning Rate: 0.01\n",
      "Epoch [11347/20000], Loss: -13.513076782226562, Learning Rate: 0.01\n",
      "Epoch [11348/20000], Loss: -13.51617431640625, Learning Rate: 0.01\n",
      "Epoch [11349/20000], Loss: -13.519088745117188, Learning Rate: 0.01\n",
      "Epoch [11350/20000], Loss: -13.5220947265625, Learning Rate: 0.01\n",
      "Epoch [11351/20000], Loss: -13.524917602539062, Learning Rate: 0.01\n",
      "Epoch [11352/20000], Loss: -13.527740478515625, Learning Rate: 0.01\n",
      "Epoch [11353/20000], Loss: -13.53045654296875, Learning Rate: 0.01\n",
      "Epoch [11354/20000], Loss: -13.533065795898438, Learning Rate: 0.01\n",
      "Epoch [11355/20000], Loss: -13.53546142578125, Learning Rate: 0.01\n",
      "Epoch [11356/20000], Loss: -13.53753662109375, Learning Rate: 0.01\n",
      "Epoch [11357/20000], Loss: -13.539260864257812, Learning Rate: 0.01\n",
      "Epoch [11358/20000], Loss: -13.5404052734375, Learning Rate: 0.01\n",
      "Epoch [11359/20000], Loss: -13.540573120117188, Learning Rate: 0.01\n",
      "Epoch [11360/20000], Loss: -13.539749145507812, Learning Rate: 0.01\n",
      "Epoch [11361/20000], Loss: -13.537063598632812, Learning Rate: 0.01\n",
      "Epoch [11362/20000], Loss: -13.531845092773438, Learning Rate: 0.01\n",
      "Epoch [11363/20000], Loss: -13.523040771484375, Learning Rate: 0.01\n",
      "Epoch [11364/20000], Loss: -13.508895874023438, Learning Rate: 0.01\n",
      "Epoch [11365/20000], Loss: -13.486892700195312, Learning Rate: 0.01\n",
      "Epoch [11366/20000], Loss: -13.45379638671875, Learning Rate: 0.01\n",
      "Epoch [11367/20000], Loss: -13.403961181640625, Learning Rate: 0.01\n",
      "Epoch [11368/20000], Loss: -13.3304443359375, Learning Rate: 0.01\n",
      "Epoch [11369/20000], Loss: -13.223007202148438, Learning Rate: 0.01\n",
      "Epoch [11370/20000], Loss: -13.068023681640625, Learning Rate: 0.01\n",
      "Epoch [11371/20000], Loss: -12.849212646484375, Learning Rate: 0.01\n",
      "Epoch [11372/20000], Loss: -12.547653198242188, Learning Rate: 0.01\n",
      "Epoch [11373/20000], Loss: -12.151718139648438, Learning Rate: 0.01\n",
      "Epoch [11374/20000], Loss: -11.661300659179688, Learning Rate: 0.01\n",
      "Epoch [11375/20000], Loss: -11.12200927734375, Learning Rate: 0.01\n",
      "Epoch [11376/20000], Loss: -10.624526977539062, Learning Rate: 0.01\n",
      "Epoch [11377/20000], Loss: -10.339309692382812, Learning Rate: 0.01\n",
      "Epoch [11378/20000], Loss: -10.425628662109375, Learning Rate: 0.01\n",
      "Epoch [11379/20000], Loss: -10.965957641601562, Learning Rate: 0.01\n",
      "Epoch [11380/20000], Loss: -11.815399169921875, Learning Rate: 0.01\n",
      "Epoch [11381/20000], Loss: -12.656112670898438, Learning Rate: 0.01\n",
      "Epoch [11382/20000], Loss: -13.154998779296875, Learning Rate: 0.01\n",
      "Epoch [11383/20000], Loss: -13.171218872070312, Learning Rate: 0.01\n",
      "Epoch [11384/20000], Loss: -12.817581176757812, Learning Rate: 0.01\n",
      "Epoch [11385/20000], Loss: -12.369125366210938, Learning Rate: 0.01\n",
      "Epoch [11386/20000], Loss: -12.119918823242188, Learning Rate: 0.01\n",
      "Epoch [11387/20000], Loss: -12.219894409179688, Learning Rate: 0.01\n",
      "Epoch [11388/20000], Loss: -12.623062133789062, Learning Rate: 0.01\n",
      "Epoch [11389/20000], Loss: -13.097625732421875, Learning Rate: 0.01\n",
      "Epoch [11390/20000], Loss: -13.404327392578125, Learning Rate: 0.01\n",
      "Epoch [11391/20000], Loss: -13.43939208984375, Learning Rate: 0.01\n",
      "Epoch [11392/20000], Loss: -13.283676147460938, Learning Rate: 0.01\n",
      "Epoch [11393/20000], Loss: -13.117446899414062, Learning Rate: 0.01\n",
      "Epoch [11394/20000], Loss: -13.090866088867188, Learning Rate: 0.01\n",
      "Epoch [11395/20000], Loss: -13.235595703125, Learning Rate: 0.01\n",
      "Epoch [11396/20000], Loss: -13.46051025390625, Learning Rate: 0.01\n",
      "Epoch [11397/20000], Loss: -13.632431030273438, Learning Rate: 0.01\n",
      "Epoch [11398/20000], Loss: -13.669845581054688, Learning Rate: 0.01\n",
      "Epoch [11399/20000], Loss: -13.5892333984375, Learning Rate: 0.01\n",
      "Epoch [11400/20000], Loss: -13.476516723632812, Learning Rate: 0.01\n",
      "Epoch [11401/20000], Loss: -13.4189453125, Learning Rate: 0.01\n",
      "Epoch [11402/20000], Loss: -13.450668334960938, Learning Rate: 0.01\n",
      "Epoch [11403/20000], Loss: -13.539840698242188, Learning Rate: 0.01\n",
      "Epoch [11404/20000], Loss: -13.622909545898438, Learning Rate: 0.01\n",
      "Epoch [11405/20000], Loss: -13.6522216796875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11406/20000], Loss: -13.624649047851562, Learning Rate: 0.01\n",
      "Epoch [11407/20000], Loss: -13.575439453125, Learning Rate: 0.01\n",
      "Epoch [11408/20000], Loss: -13.547515869140625, Learning Rate: 0.01\n",
      "Epoch [11409/20000], Loss: -13.562774658203125, Learning Rate: 0.01\n",
      "Epoch [11410/20000], Loss: -13.610885620117188, Learning Rate: 0.01\n",
      "Epoch [11411/20000], Loss: -13.662628173828125, Learning Rate: 0.01\n",
      "Epoch [11412/20000], Loss: -13.692092895507812, Learning Rate: 0.01\n",
      "Epoch [11413/20000], Loss: -13.692062377929688, Learning Rate: 0.01\n",
      "Epoch [11414/20000], Loss: -13.675613403320312, Learning Rate: 0.01\n",
      "Epoch [11415/20000], Loss: -13.663101196289062, Learning Rate: 0.01\n",
      "Epoch [11416/20000], Loss: -13.667877197265625, Learning Rate: 0.01\n",
      "Epoch [11417/20000], Loss: -13.689422607421875, Learning Rate: 0.01\n",
      "Epoch [11418/20000], Loss: -13.716094970703125, Learning Rate: 0.01\n",
      "Epoch [11419/20000], Loss: -13.734725952148438, Learning Rate: 0.01\n",
      "Epoch [11420/20000], Loss: -13.73931884765625, Learning Rate: 0.01\n",
      "Epoch [11421/20000], Loss: -13.733184814453125, Learning Rate: 0.01\n",
      "Epoch [11422/20000], Loss: -13.7255859375, Learning Rate: 0.01\n",
      "Epoch [11423/20000], Loss: -13.724365234375, Learning Rate: 0.01\n",
      "Epoch [11424/20000], Loss: -13.73162841796875, Learning Rate: 0.01\n",
      "Epoch [11425/20000], Loss: -13.7440185546875, Learning Rate: 0.01\n",
      "Epoch [11426/20000], Loss: -13.755264282226562, Learning Rate: 0.01\n",
      "Epoch [11427/20000], Loss: -13.760971069335938, Learning Rate: 0.01\n",
      "Epoch [11428/20000], Loss: -13.761077880859375, Learning Rate: 0.01\n",
      "Epoch [11429/20000], Loss: -13.758346557617188, Learning Rate: 0.01\n",
      "Epoch [11430/20000], Loss: -13.757049560546875, Learning Rate: 0.01\n",
      "Epoch [11431/20000], Loss: -13.759445190429688, Learning Rate: 0.01\n",
      "Epoch [11432/20000], Loss: -13.765365600585938, Learning Rate: 0.01\n",
      "Epoch [11433/20000], Loss: -13.77227783203125, Learning Rate: 0.01\n",
      "Epoch [11434/20000], Loss: -13.777801513671875, Learning Rate: 0.01\n",
      "Epoch [11435/20000], Loss: -13.780776977539062, Learning Rate: 0.01\n",
      "Epoch [11436/20000], Loss: -13.781570434570312, Learning Rate: 0.01\n",
      "Epoch [11437/20000], Loss: -13.781539916992188, Learning Rate: 0.01\n",
      "Epoch [11438/20000], Loss: -13.782623291015625, Learning Rate: 0.01\n",
      "Epoch [11439/20000], Loss: -13.785293579101562, Learning Rate: 0.01\n",
      "Epoch [11440/20000], Loss: -13.789230346679688, Learning Rate: 0.01\n",
      "Epoch [11441/20000], Loss: -13.79296875, Learning Rate: 0.01\n",
      "Epoch [11442/20000], Loss: -13.795989990234375, Learning Rate: 0.01\n",
      "Epoch [11443/20000], Loss: -13.797454833984375, Learning Rate: 0.01\n",
      "Epoch [11444/20000], Loss: -13.797943115234375, Learning Rate: 0.01\n",
      "Epoch [11445/20000], Loss: -13.7978515625, Learning Rate: 0.01\n",
      "Epoch [11446/20000], Loss: -13.79779052734375, Learning Rate: 0.01\n",
      "Epoch [11447/20000], Loss: -13.797805786132812, Learning Rate: 0.01\n",
      "Epoch [11448/20000], Loss: -13.797698974609375, Learning Rate: 0.01\n",
      "Epoch [11449/20000], Loss: -13.796722412109375, Learning Rate: 0.01\n",
      "Epoch [11450/20000], Loss: -13.79437255859375, Learning Rate: 0.01\n",
      "Epoch [11451/20000], Loss: -13.7901611328125, Learning Rate: 0.01\n",
      "Epoch [11452/20000], Loss: -13.783584594726562, Learning Rate: 0.01\n",
      "Epoch [11453/20000], Loss: -13.774810791015625, Learning Rate: 0.01\n",
      "Epoch [11454/20000], Loss: -13.763107299804688, Learning Rate: 0.01\n",
      "Epoch [11455/20000], Loss: -13.747833251953125, Learning Rate: 0.01\n",
      "Epoch [11456/20000], Loss: -13.728073120117188, Learning Rate: 0.01\n",
      "Epoch [11457/20000], Loss: -13.702178955078125, Learning Rate: 0.01\n",
      "Epoch [11458/20000], Loss: -13.668411254882812, Learning Rate: 0.01\n",
      "Epoch [11459/20000], Loss: -13.624404907226562, Learning Rate: 0.01\n",
      "Epoch [11460/20000], Loss: -13.567916870117188, Learning Rate: 0.01\n",
      "Epoch [11461/20000], Loss: -13.495925903320312, Learning Rate: 0.01\n",
      "Epoch [11462/20000], Loss: -13.405197143554688, Learning Rate: 0.01\n",
      "Epoch [11463/20000], Loss: -13.292495727539062, Learning Rate: 0.01\n",
      "Epoch [11464/20000], Loss: -13.154800415039062, Learning Rate: 0.01\n",
      "Epoch [11465/20000], Loss: -12.989791870117188, Learning Rate: 0.01\n",
      "Epoch [11466/20000], Loss: -12.79925537109375, Learning Rate: 0.01\n",
      "Epoch [11467/20000], Loss: -12.5867919921875, Learning Rate: 0.01\n",
      "Epoch [11468/20000], Loss: -12.366653442382812, Learning Rate: 0.01\n",
      "Epoch [11469/20000], Loss: -12.157470703125, Learning Rate: 0.01\n",
      "Epoch [11470/20000], Loss: -11.99249267578125, Learning Rate: 0.01\n",
      "Epoch [11471/20000], Loss: -11.904556274414062, Learning Rate: 0.01\n",
      "Epoch [11472/20000], Loss: -11.932022094726562, Learning Rate: 0.01\n",
      "Epoch [11473/20000], Loss: -12.091690063476562, Learning Rate: 0.01\n",
      "Epoch [11474/20000], Loss: -12.381195068359375, Learning Rate: 0.01\n",
      "Epoch [11475/20000], Loss: -12.759567260742188, Learning Rate: 0.01\n",
      "Epoch [11476/20000], Loss: -13.165054321289062, Learning Rate: 0.01\n",
      "Epoch [11477/20000], Loss: -13.524063110351562, Learning Rate: 0.01\n",
      "Epoch [11478/20000], Loss: -13.779739379882812, Learning Rate: 0.01\n",
      "Epoch [11479/20000], Loss: -13.905105590820312, Learning Rate: 0.01\n",
      "Epoch [11480/20000], Loss: -13.907455444335938, Learning Rate: 0.01\n",
      "Epoch [11481/20000], Loss: -13.820465087890625, Learning Rate: 0.01\n",
      "Epoch [11482/20000], Loss: -13.691314697265625, Learning Rate: 0.01\n",
      "Epoch [11483/20000], Loss: -13.566940307617188, Learning Rate: 0.01\n",
      "Epoch [11484/20000], Loss: -13.483657836914062, Learning Rate: 0.01\n",
      "Epoch [11485/20000], Loss: -13.462265014648438, Learning Rate: 0.01\n",
      "Epoch [11486/20000], Loss: -13.503799438476562, Learning Rate: 0.01\n",
      "Epoch [11487/20000], Loss: -13.594100952148438, Learning Rate: 0.01\n",
      "Epoch [11488/20000], Loss: -13.7078857421875, Learning Rate: 0.01\n",
      "Epoch [11489/20000], Loss: -13.818008422851562, Learning Rate: 0.01\n",
      "Epoch [11490/20000], Loss: -13.902191162109375, Learning Rate: 0.01\n",
      "Epoch [11491/20000], Loss: -13.948638916015625, Learning Rate: 0.01\n",
      "Epoch [11492/20000], Loss: -13.957122802734375, Learning Rate: 0.01\n",
      "Epoch [11493/20000], Loss: -13.936309814453125, Learning Rate: 0.01\n",
      "Epoch [11494/20000], Loss: -13.900833129882812, Learning Rate: 0.01\n",
      "Epoch [11495/20000], Loss: -13.864898681640625, Learning Rate: 0.01\n",
      "Epoch [11496/20000], Loss: -13.840377807617188, Learning Rate: 0.01\n",
      "Epoch [11497/20000], Loss: -13.833633422851562, Learning Rate: 0.01\n",
      "Epoch [11498/20000], Loss: -13.845260620117188, Learning Rate: 0.01\n",
      "Epoch [11499/20000], Loss: -13.871261596679688, Learning Rate: 0.01\n",
      "Epoch [11500/20000], Loss: -13.90509033203125, Learning Rate: 0.01\n",
      "Epoch [11501/20000], Loss: -13.939163208007812, Learning Rate: 0.01\n",
      "Epoch [11502/20000], Loss: -13.967376708984375, Learning Rate: 0.01\n",
      "Epoch [11503/20000], Loss: -13.986175537109375, Learning Rate: 0.01\n",
      "Epoch [11504/20000], Loss: -13.994537353515625, Learning Rate: 0.01\n",
      "Epoch [11505/20000], Loss: -13.994354248046875, Learning Rate: 0.01\n",
      "Epoch [11506/20000], Loss: -13.98846435546875, Learning Rate: 0.01\n",
      "Epoch [11507/20000], Loss: -13.98065185546875, Learning Rate: 0.01\n",
      "Epoch [11508/20000], Loss: -13.97406005859375, Learning Rate: 0.01\n",
      "Epoch [11509/20000], Loss: -13.97100830078125, Learning Rate: 0.01\n",
      "Epoch [11510/20000], Loss: -13.972427368164062, Learning Rate: 0.01\n",
      "Epoch [11511/20000], Loss: -13.978271484375, Learning Rate: 0.01\n",
      "Epoch [11512/20000], Loss: -13.9873046875, Learning Rate: 0.01\n",
      "Epoch [11513/20000], Loss: -13.998214721679688, Learning Rate: 0.01\n",
      "Epoch [11514/20000], Loss: -14.00909423828125, Learning Rate: 0.01\n",
      "Epoch [11515/20000], Loss: -14.018997192382812, Learning Rate: 0.01\n",
      "Epoch [11516/20000], Loss: -14.026779174804688, Learning Rate: 0.01\n",
      "Epoch [11517/20000], Loss: -14.032241821289062, Learning Rate: 0.01\n",
      "Epoch [11518/20000], Loss: -14.03570556640625, Learning Rate: 0.01\n",
      "Epoch [11519/20000], Loss: -14.037384033203125, Learning Rate: 0.01\n",
      "Epoch [11520/20000], Loss: -14.038055419921875, Learning Rate: 0.01\n",
      "Epoch [11521/20000], Loss: -14.038558959960938, Learning Rate: 0.01\n",
      "Epoch [11522/20000], Loss: -14.039398193359375, Learning Rate: 0.01\n",
      "Epoch [11523/20000], Loss: -14.040740966796875, Learning Rate: 0.01\n",
      "Epoch [11524/20000], Loss: -14.043045043945312, Learning Rate: 0.01\n",
      "Epoch [11525/20000], Loss: -14.04620361328125, Learning Rate: 0.01\n",
      "Epoch [11526/20000], Loss: -14.050094604492188, Learning Rate: 0.01\n",
      "Epoch [11527/20000], Loss: -14.05462646484375, Learning Rate: 0.01\n",
      "Epoch [11528/20000], Loss: -14.059280395507812, Learning Rate: 0.01\n",
      "Epoch [11529/20000], Loss: -14.063949584960938, Learning Rate: 0.01\n",
      "Epoch [11530/20000], Loss: -14.068267822265625, Learning Rate: 0.01\n",
      "Epoch [11531/20000], Loss: -14.07244873046875, Learning Rate: 0.01\n",
      "Epoch [11532/20000], Loss: -14.076202392578125, Learning Rate: 0.01\n",
      "Epoch [11533/20000], Loss: -14.079421997070312, Learning Rate: 0.01\n",
      "Epoch [11534/20000], Loss: -14.082565307617188, Learning Rate: 0.01\n",
      "Epoch [11535/20000], Loss: -14.085296630859375, Learning Rate: 0.01\n",
      "Epoch [11536/20000], Loss: -14.087905883789062, Learning Rate: 0.01\n",
      "Epoch [11537/20000], Loss: -14.090377807617188, Learning Rate: 0.01\n",
      "Epoch [11538/20000], Loss: -14.09283447265625, Learning Rate: 0.01\n",
      "Epoch [11539/20000], Loss: -14.09539794921875, Learning Rate: 0.01\n",
      "Epoch [11540/20000], Loss: -14.098159790039062, Learning Rate: 0.01\n",
      "Epoch [11541/20000], Loss: -14.10089111328125, Learning Rate: 0.01\n",
      "Epoch [11542/20000], Loss: -14.103775024414062, Learning Rate: 0.01\n",
      "Epoch [11543/20000], Loss: -14.106765747070312, Learning Rate: 0.01\n",
      "Epoch [11544/20000], Loss: -14.109771728515625, Learning Rate: 0.01\n",
      "Epoch [11545/20000], Loss: -14.112899780273438, Learning Rate: 0.01\n",
      "Epoch [11546/20000], Loss: -14.115997314453125, Learning Rate: 0.01\n",
      "Epoch [11547/20000], Loss: -14.1192626953125, Learning Rate: 0.01\n",
      "Epoch [11548/20000], Loss: -14.122360229492188, Learning Rate: 0.01\n",
      "Epoch [11549/20000], Loss: -14.12554931640625, Learning Rate: 0.01\n",
      "Epoch [11550/20000], Loss: -14.128646850585938, Learning Rate: 0.01\n",
      "Epoch [11551/20000], Loss: -14.131668090820312, Learning Rate: 0.01\n",
      "Epoch [11552/20000], Loss: -14.134719848632812, Learning Rate: 0.01\n",
      "Epoch [11553/20000], Loss: -14.137847900390625, Learning Rate: 0.01\n",
      "Epoch [11554/20000], Loss: -14.140731811523438, Learning Rate: 0.01\n",
      "Epoch [11555/20000], Loss: -14.14373779296875, Learning Rate: 0.01\n",
      "Epoch [11556/20000], Loss: -14.146652221679688, Learning Rate: 0.01\n",
      "Epoch [11557/20000], Loss: -14.149642944335938, Learning Rate: 0.01\n",
      "Epoch [11558/20000], Loss: -14.152633666992188, Learning Rate: 0.01\n",
      "Epoch [11559/20000], Loss: -14.155502319335938, Learning Rate: 0.01\n",
      "Epoch [11560/20000], Loss: -14.158401489257812, Learning Rate: 0.01\n",
      "Epoch [11561/20000], Loss: -14.161392211914062, Learning Rate: 0.01\n",
      "Epoch [11562/20000], Loss: -14.164215087890625, Learning Rate: 0.01\n",
      "Epoch [11563/20000], Loss: -14.167083740234375, Learning Rate: 0.01\n",
      "Epoch [11564/20000], Loss: -14.16998291015625, Learning Rate: 0.01\n",
      "Epoch [11565/20000], Loss: -14.172897338867188, Learning Rate: 0.01\n",
      "Epoch [11566/20000], Loss: -14.175872802734375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11567/20000], Loss: -14.178695678710938, Learning Rate: 0.01\n",
      "Epoch [11568/20000], Loss: -14.181655883789062, Learning Rate: 0.01\n",
      "Epoch [11569/20000], Loss: -14.18450927734375, Learning Rate: 0.01\n",
      "Epoch [11570/20000], Loss: -14.187454223632812, Learning Rate: 0.01\n",
      "Epoch [11571/20000], Loss: -14.190353393554688, Learning Rate: 0.01\n",
      "Epoch [11572/20000], Loss: -14.193191528320312, Learning Rate: 0.01\n",
      "Epoch [11573/20000], Loss: -14.196151733398438, Learning Rate: 0.01\n",
      "Epoch [11574/20000], Loss: -14.199081420898438, Learning Rate: 0.01\n",
      "Epoch [11575/20000], Loss: -14.201889038085938, Learning Rate: 0.01\n",
      "Epoch [11576/20000], Loss: -14.204757690429688, Learning Rate: 0.01\n",
      "Epoch [11577/20000], Loss: -14.20758056640625, Learning Rate: 0.01\n",
      "Epoch [11578/20000], Loss: -14.21051025390625, Learning Rate: 0.01\n",
      "Epoch [11579/20000], Loss: -14.213287353515625, Learning Rate: 0.01\n",
      "Epoch [11580/20000], Loss: -14.2161865234375, Learning Rate: 0.01\n",
      "Epoch [11581/20000], Loss: -14.218887329101562, Learning Rate: 0.01\n",
      "Epoch [11582/20000], Loss: -14.221649169921875, Learning Rate: 0.01\n",
      "Epoch [11583/20000], Loss: -14.224349975585938, Learning Rate: 0.01\n",
      "Epoch [11584/20000], Loss: -14.226959228515625, Learning Rate: 0.01\n",
      "Epoch [11585/20000], Loss: -14.229766845703125, Learning Rate: 0.01\n",
      "Epoch [11586/20000], Loss: -14.232254028320312, Learning Rate: 0.01\n",
      "Epoch [11587/20000], Loss: -14.23468017578125, Learning Rate: 0.01\n",
      "Epoch [11588/20000], Loss: -14.23699951171875, Learning Rate: 0.01\n",
      "Epoch [11589/20000], Loss: -14.239044189453125, Learning Rate: 0.01\n",
      "Epoch [11590/20000], Loss: -14.240966796875, Learning Rate: 0.01\n",
      "Epoch [11591/20000], Loss: -14.242691040039062, Learning Rate: 0.01\n",
      "Epoch [11592/20000], Loss: -14.243789672851562, Learning Rate: 0.01\n",
      "Epoch [11593/20000], Loss: -14.244552612304688, Learning Rate: 0.01\n",
      "Epoch [11594/20000], Loss: -14.244461059570312, Learning Rate: 0.01\n",
      "Epoch [11595/20000], Loss: -14.243606567382812, Learning Rate: 0.01\n",
      "Epoch [11596/20000], Loss: -14.241531372070312, Learning Rate: 0.01\n",
      "Epoch [11597/20000], Loss: -14.237701416015625, Learning Rate: 0.01\n",
      "Epoch [11598/20000], Loss: -14.231643676757812, Learning Rate: 0.01\n",
      "Epoch [11599/20000], Loss: -14.222457885742188, Learning Rate: 0.01\n",
      "Epoch [11600/20000], Loss: -14.209259033203125, Learning Rate: 0.01\n",
      "Epoch [11601/20000], Loss: -14.190399169921875, Learning Rate: 0.01\n",
      "Epoch [11602/20000], Loss: -14.163848876953125, Learning Rate: 0.01\n",
      "Epoch [11603/20000], Loss: -14.1268310546875, Learning Rate: 0.01\n",
      "Epoch [11604/20000], Loss: -14.07574462890625, Learning Rate: 0.01\n",
      "Epoch [11605/20000], Loss: -14.005767822265625, Learning Rate: 0.01\n",
      "Epoch [11606/20000], Loss: -13.91033935546875, Learning Rate: 0.01\n",
      "Epoch [11607/20000], Loss: -13.78131103515625, Learning Rate: 0.01\n",
      "Epoch [11608/20000], Loss: -13.609451293945312, Learning Rate: 0.01\n",
      "Epoch [11609/20000], Loss: -13.383926391601562, Learning Rate: 0.01\n",
      "Epoch [11610/20000], Loss: -13.096771240234375, Learning Rate: 0.01\n",
      "Epoch [11611/20000], Loss: -12.742401123046875, Learning Rate: 0.01\n",
      "Epoch [11612/20000], Loss: -12.331161499023438, Learning Rate: 0.01\n",
      "Epoch [11613/20000], Loss: -11.888458251953125, Learning Rate: 0.01\n",
      "Epoch [11614/20000], Loss: -11.478851318359375, Learning Rate: 0.01\n",
      "Epoch [11615/20000], Loss: -11.186599731445312, Learning Rate: 0.01\n",
      "Epoch [11616/20000], Loss: -11.1259765625, Learning Rate: 0.01\n",
      "Epoch [11617/20000], Loss: -11.369720458984375, Learning Rate: 0.01\n",
      "Epoch [11618/20000], Loss: -11.927841186523438, Learning Rate: 0.01\n",
      "Epoch [11619/20000], Loss: -12.683563232421875, Learning Rate: 0.01\n",
      "Epoch [11620/20000], Loss: -13.445175170898438, Learning Rate: 0.01\n",
      "Epoch [11621/20000], Loss: -14.0101318359375, Learning Rate: 0.01\n",
      "Epoch [11622/20000], Loss: -14.263397216796875, Learning Rate: 0.01\n",
      "Epoch [11623/20000], Loss: -14.213027954101562, Learning Rate: 0.01\n",
      "Epoch [11624/20000], Loss: -13.968597412109375, Learning Rate: 0.01\n",
      "Epoch [11625/20000], Loss: -13.684234619140625, Learning Rate: 0.01\n",
      "Epoch [11626/20000], Loss: -13.496292114257812, Learning Rate: 0.01\n",
      "Epoch [11627/20000], Loss: -13.481979370117188, Learning Rate: 0.01\n",
      "Epoch [11628/20000], Loss: -13.635162353515625, Learning Rate: 0.01\n",
      "Epoch [11629/20000], Loss: -13.88372802734375, Learning Rate: 0.01\n",
      "Epoch [11630/20000], Loss: -14.125091552734375, Learning Rate: 0.01\n",
      "Epoch [11631/20000], Loss: -14.275985717773438, Learning Rate: 0.01\n",
      "Epoch [11632/20000], Loss: -14.30596923828125, Learning Rate: 0.01\n",
      "Epoch [11633/20000], Loss: -14.240402221679688, Learning Rate: 0.01\n",
      "Epoch [11634/20000], Loss: -14.139053344726562, Learning Rate: 0.01\n",
      "Epoch [11635/20000], Loss: -14.064010620117188, Learning Rate: 0.01\n",
      "Epoch [11636/20000], Loss: -14.054473876953125, Learning Rate: 0.01\n",
      "Epoch [11637/20000], Loss: -14.112808227539062, Learning Rate: 0.01\n",
      "Epoch [11638/20000], Loss: -14.211471557617188, Learning Rate: 0.01\n",
      "Epoch [11639/20000], Loss: -14.30853271484375, Learning Rate: 0.01\n",
      "Epoch [11640/20000], Loss: -14.369598388671875, Learning Rate: 0.01\n",
      "Epoch [11641/20000], Loss: -14.381393432617188, Learning Rate: 0.01\n",
      "Epoch [11642/20000], Loss: -14.35308837890625, Learning Rate: 0.01\n",
      "Epoch [11643/20000], Loss: -14.309097290039062, Learning Rate: 0.01\n",
      "Epoch [11644/20000], Loss: -14.275375366210938, Learning Rate: 0.01\n",
      "Epoch [11645/20000], Loss: -14.26910400390625, Learning Rate: 0.01\n",
      "Epoch [11646/20000], Loss: -14.292343139648438, Learning Rate: 0.01\n",
      "Epoch [11647/20000], Loss: -14.33502197265625, Learning Rate: 0.01\n",
      "Epoch [11648/20000], Loss: -14.380325317382812, Learning Rate: 0.01\n",
      "Epoch [11649/20000], Loss: -14.413436889648438, Learning Rate: 0.01\n",
      "Epoch [11650/20000], Loss: -14.426513671875, Learning Rate: 0.01\n",
      "Epoch [11651/20000], Loss: -14.42132568359375, Learning Rate: 0.01\n",
      "Epoch [11652/20000], Loss: -14.406005859375, Learning Rate: 0.01\n",
      "Epoch [11653/20000], Loss: -14.39068603515625, Learning Rate: 0.01\n",
      "Epoch [11654/20000], Loss: -14.383438110351562, Learning Rate: 0.01\n",
      "Epoch [11655/20000], Loss: -14.38775634765625, Learning Rate: 0.01\n",
      "Epoch [11656/20000], Loss: -14.401580810546875, Learning Rate: 0.01\n",
      "Epoch [11657/20000], Loss: -14.41986083984375, Learning Rate: 0.01\n",
      "Epoch [11658/20000], Loss: -14.436767578125, Learning Rate: 0.01\n",
      "Epoch [11659/20000], Loss: -14.44818115234375, Learning Rate: 0.01\n",
      "Epoch [11660/20000], Loss: -14.452850341796875, Learning Rate: 0.01\n",
      "Epoch [11661/20000], Loss: -14.452239990234375, Learning Rate: 0.01\n",
      "Epoch [11662/20000], Loss: -14.449234008789062, Learning Rate: 0.01\n",
      "Epoch [11663/20000], Loss: -14.447021484375, Learning Rate: 0.01\n",
      "Epoch [11664/20000], Loss: -14.447982788085938, Learning Rate: 0.01\n",
      "Epoch [11665/20000], Loss: -14.452224731445312, Learning Rate: 0.01\n",
      "Epoch [11666/20000], Loss: -14.459136962890625, Learning Rate: 0.01\n",
      "Epoch [11667/20000], Loss: -14.467010498046875, Learning Rate: 0.01\n",
      "Epoch [11668/20000], Loss: -14.47406005859375, Learning Rate: 0.01\n",
      "Epoch [11669/20000], Loss: -14.479339599609375, Learning Rate: 0.01\n",
      "Epoch [11670/20000], Loss: -14.482269287109375, Learning Rate: 0.01\n",
      "Epoch [11671/20000], Loss: -14.483688354492188, Learning Rate: 0.01\n",
      "Epoch [11672/20000], Loss: -14.484588623046875, Learning Rate: 0.01\n",
      "Epoch [11673/20000], Loss: -14.48565673828125, Learning Rate: 0.01\n",
      "Epoch [11674/20000], Loss: -14.487686157226562, Learning Rate: 0.01\n",
      "Epoch [11675/20000], Loss: -14.491043090820312, Learning Rate: 0.01\n",
      "Epoch [11676/20000], Loss: -14.495346069335938, Learning Rate: 0.01\n",
      "Epoch [11677/20000], Loss: -14.500152587890625, Learning Rate: 0.01\n",
      "Epoch [11678/20000], Loss: -14.504867553710938, Learning Rate: 0.01\n",
      "Epoch [11679/20000], Loss: -14.509017944335938, Learning Rate: 0.01\n",
      "Epoch [11680/20000], Loss: -14.512359619140625, Learning Rate: 0.01\n",
      "Epoch [11681/20000], Loss: -14.51513671875, Learning Rate: 0.01\n",
      "Epoch [11682/20000], Loss: -14.517318725585938, Learning Rate: 0.01\n",
      "Epoch [11683/20000], Loss: -14.519546508789062, Learning Rate: 0.01\n",
      "Epoch [11684/20000], Loss: -14.521636962890625, Learning Rate: 0.01\n",
      "Epoch [11685/20000], Loss: -14.524078369140625, Learning Rate: 0.01\n",
      "Epoch [11686/20000], Loss: -14.527206420898438, Learning Rate: 0.01\n",
      "Epoch [11687/20000], Loss: -14.530242919921875, Learning Rate: 0.01\n",
      "Epoch [11688/20000], Loss: -14.53375244140625, Learning Rate: 0.01\n",
      "Epoch [11689/20000], Loss: -14.537185668945312, Learning Rate: 0.01\n",
      "Epoch [11690/20000], Loss: -14.540420532226562, Learning Rate: 0.01\n",
      "Epoch [11691/20000], Loss: -14.54351806640625, Learning Rate: 0.01\n",
      "Epoch [11692/20000], Loss: -14.546554565429688, Learning Rate: 0.01\n",
      "Epoch [11693/20000], Loss: -14.549163818359375, Learning Rate: 0.01\n",
      "Epoch [11694/20000], Loss: -14.551925659179688, Learning Rate: 0.01\n",
      "Epoch [11695/20000], Loss: -14.554473876953125, Learning Rate: 0.01\n",
      "Epoch [11696/20000], Loss: -14.557327270507812, Learning Rate: 0.01\n",
      "Epoch [11697/20000], Loss: -14.560089111328125, Learning Rate: 0.01\n",
      "Epoch [11698/20000], Loss: -14.562942504882812, Learning Rate: 0.01\n",
      "Epoch [11699/20000], Loss: -14.565994262695312, Learning Rate: 0.01\n",
      "Epoch [11700/20000], Loss: -14.568939208984375, Learning Rate: 0.01\n",
      "Epoch [11701/20000], Loss: -14.572097778320312, Learning Rate: 0.01\n",
      "Epoch [11702/20000], Loss: -14.574920654296875, Learning Rate: 0.01\n",
      "Epoch [11703/20000], Loss: -14.577957153320312, Learning Rate: 0.01\n",
      "Epoch [11704/20000], Loss: -14.58087158203125, Learning Rate: 0.01\n",
      "Epoch [11705/20000], Loss: -14.583770751953125, Learning Rate: 0.01\n",
      "Epoch [11706/20000], Loss: -14.58648681640625, Learning Rate: 0.01\n",
      "Epoch [11707/20000], Loss: -14.58935546875, Learning Rate: 0.01\n",
      "Epoch [11708/20000], Loss: -14.592178344726562, Learning Rate: 0.01\n",
      "Epoch [11709/20000], Loss: -14.595001220703125, Learning Rate: 0.01\n",
      "Epoch [11710/20000], Loss: -14.597793579101562, Learning Rate: 0.01\n",
      "Epoch [11711/20000], Loss: -14.600601196289062, Learning Rate: 0.01\n",
      "Epoch [11712/20000], Loss: -14.603591918945312, Learning Rate: 0.01\n",
      "Epoch [11713/20000], Loss: -14.606491088867188, Learning Rate: 0.01\n",
      "Epoch [11714/20000], Loss: -14.609298706054688, Learning Rate: 0.01\n",
      "Epoch [11715/20000], Loss: -14.612258911132812, Learning Rate: 0.01\n",
      "Epoch [11716/20000], Loss: -14.615158081054688, Learning Rate: 0.01\n",
      "Epoch [11717/20000], Loss: -14.617965698242188, Learning Rate: 0.01\n",
      "Epoch [11718/20000], Loss: -14.620819091796875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11719/20000], Loss: -14.623687744140625, Learning Rate: 0.01\n",
      "Epoch [11720/20000], Loss: -14.626571655273438, Learning Rate: 0.01\n",
      "Epoch [11721/20000], Loss: -14.629409790039062, Learning Rate: 0.01\n",
      "Epoch [11722/20000], Loss: -14.632232666015625, Learning Rate: 0.01\n",
      "Epoch [11723/20000], Loss: -14.635009765625, Learning Rate: 0.01\n",
      "Epoch [11724/20000], Loss: -14.637847900390625, Learning Rate: 0.01\n",
      "Epoch [11725/20000], Loss: -14.640823364257812, Learning Rate: 0.01\n",
      "Epoch [11726/20000], Loss: -14.643524169921875, Learning Rate: 0.01\n",
      "Epoch [11727/20000], Loss: -14.646438598632812, Learning Rate: 0.01\n",
      "Epoch [11728/20000], Loss: -14.649383544921875, Learning Rate: 0.01\n",
      "Epoch [11729/20000], Loss: -14.65216064453125, Learning Rate: 0.01\n",
      "Epoch [11730/20000], Loss: -14.655044555664062, Learning Rate: 0.01\n",
      "Epoch [11731/20000], Loss: -14.657882690429688, Learning Rate: 0.01\n",
      "Epoch [11732/20000], Loss: -14.660675048828125, Learning Rate: 0.01\n",
      "Epoch [11733/20000], Loss: -14.663558959960938, Learning Rate: 0.01\n",
      "Epoch [11734/20000], Loss: -14.666351318359375, Learning Rate: 0.01\n",
      "Epoch [11735/20000], Loss: -14.669204711914062, Learning Rate: 0.01\n",
      "Epoch [11736/20000], Loss: -14.671920776367188, Learning Rate: 0.01\n",
      "Epoch [11737/20000], Loss: -14.6746826171875, Learning Rate: 0.01\n",
      "Epoch [11738/20000], Loss: -14.677444458007812, Learning Rate: 0.01\n",
      "Epoch [11739/20000], Loss: -14.68017578125, Learning Rate: 0.01\n",
      "Epoch [11740/20000], Loss: -14.682723999023438, Learning Rate: 0.01\n",
      "Epoch [11741/20000], Loss: -14.685409545898438, Learning Rate: 0.01\n",
      "Epoch [11742/20000], Loss: -14.687759399414062, Learning Rate: 0.01\n",
      "Epoch [11743/20000], Loss: -14.6900634765625, Learning Rate: 0.01\n",
      "Epoch [11744/20000], Loss: -14.692230224609375, Learning Rate: 0.01\n",
      "Epoch [11745/20000], Loss: -14.694046020507812, Learning Rate: 0.01\n",
      "Epoch [11746/20000], Loss: -14.695465087890625, Learning Rate: 0.01\n",
      "Epoch [11747/20000], Loss: -14.696426391601562, Learning Rate: 0.01\n",
      "Epoch [11748/20000], Loss: -14.69659423828125, Learning Rate: 0.01\n",
      "Epoch [11749/20000], Loss: -14.695693969726562, Learning Rate: 0.01\n",
      "Epoch [11750/20000], Loss: -14.693572998046875, Learning Rate: 0.01\n",
      "Epoch [11751/20000], Loss: -14.689285278320312, Learning Rate: 0.01\n",
      "Epoch [11752/20000], Loss: -14.681930541992188, Learning Rate: 0.01\n",
      "Epoch [11753/20000], Loss: -14.670852661132812, Learning Rate: 0.01\n",
      "Epoch [11754/20000], Loss: -14.653854370117188, Learning Rate: 0.01\n",
      "Epoch [11755/20000], Loss: -14.628570556640625, Learning Rate: 0.01\n",
      "Epoch [11756/20000], Loss: -14.591842651367188, Learning Rate: 0.01\n",
      "Epoch [11757/20000], Loss: -14.538909912109375, Learning Rate: 0.01\n",
      "Epoch [11758/20000], Loss: -14.463409423828125, Learning Rate: 0.01\n",
      "Epoch [11759/20000], Loss: -14.357391357421875, Learning Rate: 0.01\n",
      "Epoch [11760/20000], Loss: -14.210784912109375, Learning Rate: 0.01\n",
      "Epoch [11761/20000], Loss: -14.0135498046875, Learning Rate: 0.01\n",
      "Epoch [11762/20000], Loss: -13.756256103515625, Learning Rate: 0.01\n",
      "Epoch [11763/20000], Loss: -13.440719604492188, Learning Rate: 0.01\n",
      "Epoch [11764/20000], Loss: -13.0811767578125, Learning Rate: 0.01\n",
      "Epoch [11765/20000], Loss: -12.729354858398438, Learning Rate: 0.01\n",
      "Epoch [11766/20000], Loss: -12.462783813476562, Learning Rate: 0.01\n",
      "Epoch [11767/20000], Loss: -12.394317626953125, Learning Rate: 0.01\n",
      "Epoch [11768/20000], Loss: -12.600494384765625, Learning Rate: 0.01\n",
      "Epoch [11769/20000], Loss: -13.083175659179688, Learning Rate: 0.01\n",
      "Epoch [11770/20000], Loss: -13.7095947265625, Learning Rate: 0.01\n",
      "Epoch [11771/20000], Loss: -14.273788452148438, Learning Rate: 0.01\n",
      "Epoch [11772/20000], Loss: -14.595626831054688, Learning Rate: 0.01\n",
      "Epoch [11773/20000], Loss: -14.617034912109375, Learning Rate: 0.01\n",
      "Epoch [11774/20000], Loss: -14.416290283203125, Learning Rate: 0.01\n",
      "Epoch [11775/20000], Loss: -14.150924682617188, Learning Rate: 0.01\n",
      "Epoch [11776/20000], Loss: -13.982635498046875, Learning Rate: 0.01\n",
      "Epoch [11777/20000], Loss: -14.001983642578125, Learning Rate: 0.01\n",
      "Epoch [11778/20000], Loss: -14.202377319335938, Learning Rate: 0.01\n",
      "Epoch [11779/20000], Loss: -14.48077392578125, Learning Rate: 0.01\n",
      "Epoch [11780/20000], Loss: -14.708587646484375, Learning Rate: 0.01\n",
      "Epoch [11781/20000], Loss: -14.798828125, Learning Rate: 0.01\n",
      "Epoch [11782/20000], Loss: -14.7491455078125, Learning Rate: 0.01\n",
      "Epoch [11783/20000], Loss: -14.627243041992188, Learning Rate: 0.01\n",
      "Epoch [11784/20000], Loss: -14.522415161132812, Learning Rate: 0.01\n",
      "Epoch [11785/20000], Loss: -14.497695922851562, Learning Rate: 0.01\n",
      "Epoch [11786/20000], Loss: -14.560989379882812, Learning Rate: 0.01\n",
      "Epoch [11787/20000], Loss: -14.671905517578125, Learning Rate: 0.01\n",
      "Epoch [11788/20000], Loss: -14.769866943359375, Learning Rate: 0.01\n",
      "Epoch [11789/20000], Loss: -14.8121337890625, Learning Rate: 0.01\n",
      "Epoch [11790/20000], Loss: -14.793197631835938, Learning Rate: 0.01\n",
      "Epoch [11791/20000], Loss: -14.740997314453125, Learning Rate: 0.01\n",
      "Epoch [11792/20000], Loss: -14.695755004882812, Learning Rate: 0.01\n",
      "Epoch [11793/20000], Loss: -14.686141967773438, Learning Rate: 0.01\n",
      "Epoch [11794/20000], Loss: -14.7164306640625, Learning Rate: 0.01\n",
      "Epoch [11795/20000], Loss: -14.768890380859375, Learning Rate: 0.01\n",
      "Epoch [11796/20000], Loss: -14.816802978515625, Learning Rate: 0.01\n",
      "Epoch [11797/20000], Loss: -14.8411865234375, Learning Rate: 0.01\n",
      "Epoch [11798/20000], Loss: -14.83819580078125, Learning Rate: 0.01\n",
      "Epoch [11799/20000], Loss: -14.819580078125, Learning Rate: 0.01\n",
      "Epoch [11800/20000], Loss: -14.801956176757812, Learning Rate: 0.01\n",
      "Epoch [11801/20000], Loss: -14.798507690429688, Learning Rate: 0.01\n",
      "Epoch [11802/20000], Loss: -14.81195068359375, Learning Rate: 0.01\n",
      "Epoch [11803/20000], Loss: -14.83544921875, Learning Rate: 0.01\n",
      "Epoch [11804/20000], Loss: -14.85809326171875, Learning Rate: 0.01\n",
      "Epoch [11805/20000], Loss: -14.871353149414062, Learning Rate: 0.01\n",
      "Epoch [11806/20000], Loss: -14.873046875, Learning Rate: 0.01\n",
      "Epoch [11807/20000], Loss: -14.866729736328125, Learning Rate: 0.01\n",
      "Epoch [11808/20000], Loss: -14.859573364257812, Learning Rate: 0.01\n",
      "Epoch [11809/20000], Loss: -14.85748291015625, Learning Rate: 0.01\n",
      "Epoch [11810/20000], Loss: -14.862472534179688, Learning Rate: 0.01\n",
      "Epoch [11811/20000], Loss: -14.872726440429688, Learning Rate: 0.01\n",
      "Epoch [11812/20000], Loss: -14.884048461914062, Learning Rate: 0.01\n",
      "Epoch [11813/20000], Loss: -14.892501831054688, Learning Rate: 0.01\n",
      "Epoch [11814/20000], Loss: -14.896255493164062, Learning Rate: 0.01\n",
      "Epoch [11815/20000], Loss: -14.895965576171875, Learning Rate: 0.01\n",
      "Epoch [11816/20000], Loss: -14.894271850585938, Learning Rate: 0.01\n",
      "Epoch [11817/20000], Loss: -14.893905639648438, Learning Rate: 0.01\n",
      "Epoch [11818/20000], Loss: -14.896102905273438, Learning Rate: 0.01\n",
      "Epoch [11819/20000], Loss: -14.900924682617188, Learning Rate: 0.01\n",
      "Epoch [11820/20000], Loss: -14.907333374023438, Learning Rate: 0.01\n",
      "Epoch [11821/20000], Loss: -14.913116455078125, Learning Rate: 0.01\n",
      "Epoch [11822/20000], Loss: -14.917449951171875, Learning Rate: 0.01\n",
      "Epoch [11823/20000], Loss: -14.919815063476562, Learning Rate: 0.01\n",
      "Epoch [11824/20000], Loss: -14.920989990234375, Learning Rate: 0.01\n",
      "Epoch [11825/20000], Loss: -14.921859741210938, Learning Rate: 0.01\n",
      "Epoch [11826/20000], Loss: -14.923431396484375, Learning Rate: 0.01\n",
      "Epoch [11827/20000], Loss: -14.926040649414062, Learning Rate: 0.01\n",
      "Epoch [11828/20000], Loss: -14.929580688476562, Learning Rate: 0.01\n",
      "Epoch [11829/20000], Loss: -14.93341064453125, Learning Rate: 0.01\n",
      "Epoch [11830/20000], Loss: -14.936813354492188, Learning Rate: 0.01\n",
      "Epoch [11831/20000], Loss: -14.939666748046875, Learning Rate: 0.01\n",
      "Epoch [11832/20000], Loss: -14.941619873046875, Learning Rate: 0.01\n",
      "Epoch [11833/20000], Loss: -14.943069458007812, Learning Rate: 0.01\n",
      "Epoch [11834/20000], Loss: -14.944122314453125, Learning Rate: 0.01\n",
      "Epoch [11835/20000], Loss: -14.945266723632812, Learning Rate: 0.01\n",
      "Epoch [11836/20000], Loss: -14.9464111328125, Learning Rate: 0.01\n",
      "Epoch [11837/20000], Loss: -14.947647094726562, Learning Rate: 0.01\n",
      "Epoch [11838/20000], Loss: -14.948501586914062, Learning Rate: 0.01\n",
      "Epoch [11839/20000], Loss: -14.94873046875, Learning Rate: 0.01\n",
      "Epoch [11840/20000], Loss: -14.947952270507812, Learning Rate: 0.01\n",
      "Epoch [11841/20000], Loss: -14.945907592773438, Learning Rate: 0.01\n",
      "Epoch [11842/20000], Loss: -14.942535400390625, Learning Rate: 0.01\n",
      "Epoch [11843/20000], Loss: -14.9371337890625, Learning Rate: 0.01\n",
      "Epoch [11844/20000], Loss: -14.92987060546875, Learning Rate: 0.01\n",
      "Epoch [11845/20000], Loss: -14.91998291015625, Learning Rate: 0.01\n",
      "Epoch [11846/20000], Loss: -14.906768798828125, Learning Rate: 0.01\n",
      "Epoch [11847/20000], Loss: -14.8887939453125, Learning Rate: 0.01\n",
      "Epoch [11848/20000], Loss: -14.865127563476562, Learning Rate: 0.01\n",
      "Epoch [11849/20000], Loss: -14.833633422851562, Learning Rate: 0.01\n",
      "Epoch [11850/20000], Loss: -14.792434692382812, Learning Rate: 0.01\n",
      "Epoch [11851/20000], Loss: -14.738479614257812, Learning Rate: 0.01\n",
      "Epoch [11852/20000], Loss: -14.668991088867188, Learning Rate: 0.01\n",
      "Epoch [11853/20000], Loss: -14.579910278320312, Learning Rate: 0.01\n",
      "Epoch [11854/20000], Loss: -14.4678955078125, Learning Rate: 0.01\n",
      "Epoch [11855/20000], Loss: -14.328842163085938, Learning Rate: 0.01\n",
      "Epoch [11856/20000], Loss: -14.161300659179688, Learning Rate: 0.01\n",
      "Epoch [11857/20000], Loss: -13.964675903320312, Learning Rate: 0.01\n",
      "Epoch [11858/20000], Loss: -13.746658325195312, Learning Rate: 0.01\n",
      "Epoch [11859/20000], Loss: -13.519195556640625, Learning Rate: 0.01\n",
      "Epoch [11860/20000], Loss: -13.30963134765625, Learning Rate: 0.01\n",
      "Epoch [11861/20000], Loss: -13.149459838867188, Learning Rate: 0.01\n",
      "Epoch [11862/20000], Loss: -13.084274291992188, Learning Rate: 0.01\n",
      "Epoch [11863/20000], Loss: -13.146041870117188, Learning Rate: 0.01\n",
      "Epoch [11864/20000], Loss: -13.355255126953125, Learning Rate: 0.01\n",
      "Epoch [11865/20000], Loss: -13.689971923828125, Learning Rate: 0.01\n",
      "Epoch [11866/20000], Loss: -14.097991943359375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11867/20000], Loss: -14.498565673828125, Learning Rate: 0.01\n",
      "Epoch [11868/20000], Loss: -14.816665649414062, Learning Rate: 0.01\n",
      "Epoch [11869/20000], Loss: -15.002975463867188, Learning Rate: 0.01\n",
      "Epoch [11870/20000], Loss: -15.048965454101562, Learning Rate: 0.01\n",
      "Epoch [11871/20000], Loss: -14.982681274414062, Learning Rate: 0.01\n",
      "Epoch [11872/20000], Loss: -14.853851318359375, Learning Rate: 0.01\n",
      "Epoch [11873/20000], Loss: -14.717803955078125, Learning Rate: 0.01\n",
      "Epoch [11874/20000], Loss: -14.620025634765625, Learning Rate: 0.01\n",
      "Epoch [11875/20000], Loss: -14.588455200195312, Learning Rate: 0.01\n",
      "Epoch [11876/20000], Loss: -14.627365112304688, Learning Rate: 0.01\n",
      "Epoch [11877/20000], Loss: -14.721725463867188, Learning Rate: 0.01\n",
      "Epoch [11878/20000], Loss: -14.842330932617188, Learning Rate: 0.01\n",
      "Epoch [11879/20000], Loss: -14.957305908203125, Learning Rate: 0.01\n",
      "Epoch [11880/20000], Loss: -15.041244506835938, Learning Rate: 0.01\n",
      "Epoch [11881/20000], Loss: -15.081649780273438, Learning Rate: 0.01\n",
      "Epoch [11882/20000], Loss: -15.080291748046875, Learning Rate: 0.01\n",
      "Epoch [11883/20000], Loss: -15.049911499023438, Learning Rate: 0.01\n",
      "Epoch [11884/20000], Loss: -15.00823974609375, Learning Rate: 0.01\n",
      "Epoch [11885/20000], Loss: -14.972320556640625, Learning Rate: 0.01\n",
      "Epoch [11886/20000], Loss: -14.954071044921875, Learning Rate: 0.01\n",
      "Epoch [11887/20000], Loss: -14.958328247070312, Learning Rate: 0.01\n",
      "Epoch [11888/20000], Loss: -14.982757568359375, Learning Rate: 0.01\n",
      "Epoch [11889/20000], Loss: -15.0194091796875, Learning Rate: 0.01\n",
      "Epoch [11890/20000], Loss: -15.058670043945312, Learning Rate: 0.01\n",
      "Epoch [11891/20000], Loss: -15.091751098632812, Learning Rate: 0.01\n",
      "Epoch [11892/20000], Loss: -15.1129150390625, Learning Rate: 0.01\n",
      "Epoch [11893/20000], Loss: -15.121109008789062, Learning Rate: 0.01\n",
      "Epoch [11894/20000], Loss: -15.118133544921875, Learning Rate: 0.01\n",
      "Epoch [11895/20000], Loss: -15.108627319335938, Learning Rate: 0.01\n",
      "Epoch [11896/20000], Loss: -15.097869873046875, Learning Rate: 0.01\n",
      "Epoch [11897/20000], Loss: -15.090316772460938, Learning Rate: 0.01\n",
      "Epoch [11898/20000], Loss: -15.088516235351562, Learning Rate: 0.01\n",
      "Epoch [11899/20000], Loss: -15.0933837890625, Learning Rate: 0.01\n",
      "Epoch [11900/20000], Loss: -15.103378295898438, Learning Rate: 0.01\n",
      "Epoch [11901/20000], Loss: -15.116561889648438, Learning Rate: 0.01\n",
      "Epoch [11902/20000], Loss: -15.13006591796875, Learning Rate: 0.01\n",
      "Epoch [11903/20000], Loss: -15.141632080078125, Learning Rate: 0.01\n",
      "Epoch [11904/20000], Loss: -15.14996337890625, Learning Rate: 0.01\n",
      "Epoch [11905/20000], Loss: -15.154647827148438, Learning Rate: 0.01\n",
      "Epoch [11906/20000], Loss: -15.1563720703125, Learning Rate: 0.01\n",
      "Epoch [11907/20000], Loss: -15.15606689453125, Learning Rate: 0.01\n",
      "Epoch [11908/20000], Loss: -15.155014038085938, Learning Rate: 0.01\n",
      "Epoch [11909/20000], Loss: -15.154541015625, Learning Rate: 0.01\n",
      "Epoch [11910/20000], Loss: -15.155288696289062, Learning Rate: 0.01\n",
      "Epoch [11911/20000], Loss: -15.157669067382812, Learning Rate: 0.01\n",
      "Epoch [11912/20000], Loss: -15.16156005859375, Learning Rate: 0.01\n",
      "Epoch [11913/20000], Loss: -15.166458129882812, Learning Rate: 0.01\n",
      "Epoch [11914/20000], Loss: -15.171981811523438, Learning Rate: 0.01\n",
      "Epoch [11915/20000], Loss: -15.177383422851562, Learning Rate: 0.01\n",
      "Epoch [11916/20000], Loss: -15.182479858398438, Learning Rate: 0.01\n",
      "Epoch [11917/20000], Loss: -15.186630249023438, Learning Rate: 0.01\n",
      "Epoch [11918/20000], Loss: -15.190185546875, Learning Rate: 0.01\n",
      "Epoch [11919/20000], Loss: -15.192947387695312, Learning Rate: 0.01\n",
      "Epoch [11920/20000], Loss: -15.195266723632812, Learning Rate: 0.01\n",
      "Epoch [11921/20000], Loss: -15.19720458984375, Learning Rate: 0.01\n",
      "Epoch [11922/20000], Loss: -15.199142456054688, Learning Rate: 0.01\n",
      "Epoch [11923/20000], Loss: -15.201248168945312, Learning Rate: 0.01\n",
      "Epoch [11924/20000], Loss: -15.203598022460938, Learning Rate: 0.01\n",
      "Epoch [11925/20000], Loss: -15.206161499023438, Learning Rate: 0.01\n",
      "Epoch [11926/20000], Loss: -15.209014892578125, Learning Rate: 0.01\n",
      "Epoch [11927/20000], Loss: -15.212326049804688, Learning Rate: 0.01\n",
      "Epoch [11928/20000], Loss: -15.215530395507812, Learning Rate: 0.01\n",
      "Epoch [11929/20000], Loss: -15.218856811523438, Learning Rate: 0.01\n",
      "Epoch [11930/20000], Loss: -15.222076416015625, Learning Rate: 0.01\n",
      "Epoch [11931/20000], Loss: -15.225387573242188, Learning Rate: 0.01\n",
      "Epoch [11932/20000], Loss: -15.228439331054688, Learning Rate: 0.01\n",
      "Epoch [11933/20000], Loss: -15.231460571289062, Learning Rate: 0.01\n",
      "Epoch [11934/20000], Loss: -15.234222412109375, Learning Rate: 0.01\n",
      "Epoch [11935/20000], Loss: -15.236953735351562, Learning Rate: 0.01\n",
      "Epoch [11936/20000], Loss: -15.239639282226562, Learning Rate: 0.01\n",
      "Epoch [11937/20000], Loss: -15.242233276367188, Learning Rate: 0.01\n",
      "Epoch [11938/20000], Loss: -15.24493408203125, Learning Rate: 0.01\n",
      "Epoch [11939/20000], Loss: -15.247467041015625, Learning Rate: 0.01\n",
      "Epoch [11940/20000], Loss: -15.250228881835938, Learning Rate: 0.01\n",
      "Epoch [11941/20000], Loss: -15.252761840820312, Learning Rate: 0.01\n",
      "Epoch [11942/20000], Loss: -15.255569458007812, Learning Rate: 0.01\n",
      "Epoch [11943/20000], Loss: -15.25830078125, Learning Rate: 0.01\n",
      "Epoch [11944/20000], Loss: -15.261077880859375, Learning Rate: 0.01\n",
      "Epoch [11945/20000], Loss: -15.263900756835938, Learning Rate: 0.01\n",
      "Epoch [11946/20000], Loss: -15.266860961914062, Learning Rate: 0.01\n",
      "Epoch [11947/20000], Loss: -15.26953125, Learning Rate: 0.01\n",
      "Epoch [11948/20000], Loss: -15.2724609375, Learning Rate: 0.01\n",
      "Epoch [11949/20000], Loss: -15.275177001953125, Learning Rate: 0.01\n",
      "Epoch [11950/20000], Loss: -15.277999877929688, Learning Rate: 0.01\n",
      "Epoch [11951/20000], Loss: -15.280731201171875, Learning Rate: 0.01\n",
      "Epoch [11952/20000], Loss: -15.283554077148438, Learning Rate: 0.01\n",
      "Epoch [11953/20000], Loss: -15.286361694335938, Learning Rate: 0.01\n",
      "Epoch [11954/20000], Loss: -15.289260864257812, Learning Rate: 0.01\n",
      "Epoch [11955/20000], Loss: -15.291900634765625, Learning Rate: 0.01\n",
      "Epoch [11956/20000], Loss: -15.294601440429688, Learning Rate: 0.01\n",
      "Epoch [11957/20000], Loss: -15.29742431640625, Learning Rate: 0.01\n",
      "Epoch [11958/20000], Loss: -15.30010986328125, Learning Rate: 0.01\n",
      "Epoch [11959/20000], Loss: -15.30279541015625, Learning Rate: 0.01\n",
      "Epoch [11960/20000], Loss: -15.305694580078125, Learning Rate: 0.01\n",
      "Epoch [11961/20000], Loss: -15.308456420898438, Learning Rate: 0.01\n",
      "Epoch [11962/20000], Loss: -15.311080932617188, Learning Rate: 0.01\n",
      "Epoch [11963/20000], Loss: -15.3138427734375, Learning Rate: 0.01\n",
      "Epoch [11964/20000], Loss: -15.3165283203125, Learning Rate: 0.01\n",
      "Epoch [11965/20000], Loss: -15.319244384765625, Learning Rate: 0.01\n",
      "Epoch [11966/20000], Loss: -15.322021484375, Learning Rate: 0.01\n",
      "Epoch [11967/20000], Loss: -15.324798583984375, Learning Rate: 0.01\n",
      "Epoch [11968/20000], Loss: -15.3275146484375, Learning Rate: 0.01\n",
      "Epoch [11969/20000], Loss: -15.330276489257812, Learning Rate: 0.01\n",
      "Epoch [11970/20000], Loss: -15.333114624023438, Learning Rate: 0.01\n",
      "Epoch [11971/20000], Loss: -15.3358154296875, Learning Rate: 0.01\n",
      "Epoch [11972/20000], Loss: -15.338516235351562, Learning Rate: 0.01\n",
      "Epoch [11973/20000], Loss: -15.341278076171875, Learning Rate: 0.01\n",
      "Epoch [11974/20000], Loss: -15.343994140625, Learning Rate: 0.01\n",
      "Epoch [11975/20000], Loss: -15.346694946289062, Learning Rate: 0.01\n",
      "Epoch [11976/20000], Loss: -15.349533081054688, Learning Rate: 0.01\n",
      "Epoch [11977/20000], Loss: -15.352279663085938, Learning Rate: 0.01\n",
      "Epoch [11978/20000], Loss: -15.355072021484375, Learning Rate: 0.01\n",
      "Epoch [11979/20000], Loss: -15.357772827148438, Learning Rate: 0.01\n",
      "Epoch [11980/20000], Loss: -15.360488891601562, Learning Rate: 0.01\n",
      "Epoch [11981/20000], Loss: -15.363250732421875, Learning Rate: 0.01\n",
      "Epoch [11982/20000], Loss: -15.365921020507812, Learning Rate: 0.01\n",
      "Epoch [11983/20000], Loss: -15.368637084960938, Learning Rate: 0.01\n",
      "Epoch [11984/20000], Loss: -15.371429443359375, Learning Rate: 0.01\n",
      "Epoch [11985/20000], Loss: -15.373977661132812, Learning Rate: 0.01\n",
      "Epoch [11986/20000], Loss: -15.376678466796875, Learning Rate: 0.01\n",
      "Epoch [11987/20000], Loss: -15.379348754882812, Learning Rate: 0.01\n",
      "Epoch [11988/20000], Loss: -15.381973266601562, Learning Rate: 0.01\n",
      "Epoch [11989/20000], Loss: -15.384735107421875, Learning Rate: 0.01\n",
      "Epoch [11990/20000], Loss: -15.387405395507812, Learning Rate: 0.01\n",
      "Epoch [11991/20000], Loss: -15.389877319335938, Learning Rate: 0.01\n",
      "Epoch [11992/20000], Loss: -15.392333984375, Learning Rate: 0.01\n",
      "Epoch [11993/20000], Loss: -15.394805908203125, Learning Rate: 0.01\n",
      "Epoch [11994/20000], Loss: -15.3970947265625, Learning Rate: 0.01\n",
      "Epoch [11995/20000], Loss: -15.399368286132812, Learning Rate: 0.01\n",
      "Epoch [11996/20000], Loss: -15.401504516601562, Learning Rate: 0.01\n",
      "Epoch [11997/20000], Loss: -15.4033203125, Learning Rate: 0.01\n",
      "Epoch [11998/20000], Loss: -15.404998779296875, Learning Rate: 0.01\n",
      "Epoch [11999/20000], Loss: -15.406341552734375, Learning Rate: 0.01\n",
      "Epoch [12000/20000], Loss: -15.407150268554688, Learning Rate: 0.01\n",
      "Epoch [12001/20000], Loss: -15.40740966796875, Learning Rate: 0.01\n",
      "Epoch [12002/20000], Loss: -15.40679931640625, Learning Rate: 0.01\n",
      "Epoch [12003/20000], Loss: -15.405227661132812, Learning Rate: 0.01\n",
      "Epoch [12004/20000], Loss: -15.402084350585938, Learning Rate: 0.01\n",
      "Epoch [12005/20000], Loss: -15.39691162109375, Learning Rate: 0.01\n",
      "Epoch [12006/20000], Loss: -15.38916015625, Learning Rate: 0.01\n",
      "Epoch [12007/20000], Loss: -15.377578735351562, Learning Rate: 0.01\n",
      "Epoch [12008/20000], Loss: -15.36102294921875, Learning Rate: 0.01\n",
      "Epoch [12009/20000], Loss: -15.337677001953125, Learning Rate: 0.01\n",
      "Epoch [12010/20000], Loss: -15.304855346679688, Learning Rate: 0.01\n",
      "Epoch [12011/20000], Loss: -15.259552001953125, Learning Rate: 0.01\n",
      "Epoch [12012/20000], Loss: -15.196914672851562, Learning Rate: 0.01\n",
      "Epoch [12013/20000], Loss: -15.111618041992188, Learning Rate: 0.01\n",
      "Epoch [12014/20000], Loss: -14.996124267578125, Learning Rate: 0.01\n",
      "Epoch [12015/20000], Loss: -14.842910766601562, Learning Rate: 0.01\n",
      "Epoch [12016/20000], Loss: -14.642013549804688, Learning Rate: 0.01\n",
      "Epoch [12017/20000], Loss: -14.38800048828125, Learning Rate: 0.01\n",
      "Epoch [12018/20000], Loss: -14.07708740234375, Learning Rate: 0.01\n",
      "Epoch [12019/20000], Loss: -13.722457885742188, Learning Rate: 0.01\n",
      "Epoch [12020/20000], Loss: -13.350448608398438, Learning Rate: 0.01\n",
      "Epoch [12021/20000], Loss: -13.023513793945312, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12022/20000], Loss: -12.817245483398438, Learning Rate: 0.01\n",
      "Epoch [12023/20000], Loss: -12.828659057617188, Learning Rate: 0.01\n",
      "Epoch [12024/20000], Loss: -13.1044921875, Learning Rate: 0.01\n",
      "Epoch [12025/20000], Loss: -13.630081176757812, Learning Rate: 0.01\n",
      "Epoch [12026/20000], Loss: -14.280715942382812, Learning Rate: 0.01\n",
      "Epoch [12027/20000], Loss: -14.884140014648438, Learning Rate: 0.01\n",
      "Epoch [12028/20000], Loss: -15.28155517578125, Learning Rate: 0.01\n",
      "Epoch [12029/20000], Loss: -15.403961181640625, Learning Rate: 0.01\n",
      "Epoch [12030/20000], Loss: -15.288330078125, Learning Rate: 0.01\n",
      "Epoch [12031/20000], Loss: -15.04754638671875, Learning Rate: 0.01\n",
      "Epoch [12032/20000], Loss: -14.818954467773438, Learning Rate: 0.01\n",
      "Epoch [12033/20000], Loss: -14.709884643554688, Learning Rate: 0.01\n",
      "Epoch [12034/20000], Loss: -14.768112182617188, Learning Rate: 0.01\n",
      "Epoch [12035/20000], Loss: -14.963088989257812, Learning Rate: 0.01\n",
      "Epoch [12036/20000], Loss: -15.212310791015625, Learning Rate: 0.01\n",
      "Epoch [12037/20000], Loss: -15.418060302734375, Learning Rate: 0.01\n",
      "Epoch [12038/20000], Loss: -15.515731811523438, Learning Rate: 0.01\n",
      "Epoch [12039/20000], Loss: -15.496429443359375, Learning Rate: 0.01\n",
      "Epoch [12040/20000], Loss: -15.40057373046875, Learning Rate: 0.01\n",
      "Epoch [12041/20000], Loss: -15.292465209960938, Learning Rate: 0.01\n",
      "Epoch [12042/20000], Loss: -15.2281494140625, Learning Rate: 0.01\n",
      "Epoch [12043/20000], Loss: -15.235763549804688, Learning Rate: 0.01\n",
      "Epoch [12044/20000], Loss: -15.30615234375, Learning Rate: 0.01\n",
      "Epoch [12045/20000], Loss: -15.405715942382812, Learning Rate: 0.01\n",
      "Epoch [12046/20000], Loss: -15.492782592773438, Learning Rate: 0.01\n",
      "Epoch [12047/20000], Loss: -15.539199829101562, Learning Rate: 0.01\n",
      "Epoch [12048/20000], Loss: -15.5386962890625, Learning Rate: 0.01\n",
      "Epoch [12049/20000], Loss: -15.506393432617188, Learning Rate: 0.01\n",
      "Epoch [12050/20000], Loss: -15.467391967773438, Learning Rate: 0.01\n",
      "Epoch [12051/20000], Loss: -15.444244384765625, Learning Rate: 0.01\n",
      "Epoch [12052/20000], Loss: -15.44842529296875, Learning Rate: 0.01\n",
      "Epoch [12053/20000], Loss: -15.476608276367188, Learning Rate: 0.01\n",
      "Epoch [12054/20000], Loss: -15.516128540039062, Learning Rate: 0.01\n",
      "Epoch [12055/20000], Loss: -15.551681518554688, Learning Rate: 0.01\n",
      "Epoch [12056/20000], Loss: -15.57171630859375, Learning Rate: 0.01\n",
      "Epoch [12057/20000], Loss: -15.574020385742188, Learning Rate: 0.01\n",
      "Epoch [12058/20000], Loss: -15.563156127929688, Learning Rate: 0.01\n",
      "Epoch [12059/20000], Loss: -15.548690795898438, Learning Rate: 0.01\n",
      "Epoch [12060/20000], Loss: -15.539276123046875, Learning Rate: 0.01\n",
      "Epoch [12061/20000], Loss: -15.539901733398438, Learning Rate: 0.01\n",
      "Epoch [12062/20000], Loss: -15.550399780273438, Learning Rate: 0.01\n",
      "Epoch [12063/20000], Loss: -15.566680908203125, Learning Rate: 0.01\n",
      "Epoch [12064/20000], Loss: -15.583175659179688, Learning Rate: 0.01\n",
      "Epoch [12065/20000], Loss: -15.59503173828125, Learning Rate: 0.01\n",
      "Epoch [12066/20000], Loss: -15.60040283203125, Learning Rate: 0.01\n",
      "Epoch [12067/20000], Loss: -15.600112915039062, Learning Rate: 0.01\n",
      "Epoch [12068/20000], Loss: -15.5970458984375, Learning Rate: 0.01\n",
      "Epoch [12069/20000], Loss: -15.594268798828125, Learning Rate: 0.01\n",
      "Epoch [12070/20000], Loss: -15.594375610351562, Learning Rate: 0.01\n",
      "Epoch [12071/20000], Loss: -15.598388671875, Learning Rate: 0.01\n",
      "Epoch [12072/20000], Loss: -15.605072021484375, Learning Rate: 0.01\n",
      "Epoch [12073/20000], Loss: -15.613037109375, Learning Rate: 0.01\n",
      "Epoch [12074/20000], Loss: -15.620391845703125, Learning Rate: 0.01\n",
      "Epoch [12075/20000], Loss: -15.625701904296875, Learning Rate: 0.01\n",
      "Epoch [12076/20000], Loss: -15.6287841796875, Learning Rate: 0.01\n",
      "Epoch [12077/20000], Loss: -15.630020141601562, Learning Rate: 0.01\n",
      "Epoch [12078/20000], Loss: -15.630447387695312, Learning Rate: 0.01\n",
      "Epoch [12079/20000], Loss: -15.631271362304688, Learning Rate: 0.01\n",
      "Epoch [12080/20000], Loss: -15.633071899414062, Learning Rate: 0.01\n",
      "Epoch [12081/20000], Loss: -15.63604736328125, Learning Rate: 0.01\n",
      "Epoch [12082/20000], Loss: -15.64013671875, Learning Rate: 0.01\n",
      "Epoch [12083/20000], Loss: -15.644638061523438, Learning Rate: 0.01\n",
      "Epoch [12084/20000], Loss: -15.648849487304688, Learning Rate: 0.01\n",
      "Epoch [12085/20000], Loss: -15.652511596679688, Learning Rate: 0.01\n",
      "Epoch [12086/20000], Loss: -15.65557861328125, Learning Rate: 0.01\n",
      "Epoch [12087/20000], Loss: -15.657852172851562, Learning Rate: 0.01\n",
      "Epoch [12088/20000], Loss: -15.659942626953125, Learning Rate: 0.01\n",
      "Epoch [12089/20000], Loss: -15.661941528320312, Learning Rate: 0.01\n",
      "Epoch [12090/20000], Loss: -15.664093017578125, Learning Rate: 0.01\n",
      "Epoch [12091/20000], Loss: -15.666748046875, Learning Rate: 0.01\n",
      "Epoch [12092/20000], Loss: -15.669723510742188, Learning Rate: 0.01\n",
      "Epoch [12093/20000], Loss: -15.672897338867188, Learning Rate: 0.01\n",
      "Epoch [12094/20000], Loss: -15.676162719726562, Learning Rate: 0.01\n",
      "Epoch [12095/20000], Loss: -15.679397583007812, Learning Rate: 0.01\n",
      "Epoch [12096/20000], Loss: -15.68231201171875, Learning Rate: 0.01\n",
      "Epoch [12097/20000], Loss: -15.685043334960938, Learning Rate: 0.01\n",
      "Epoch [12098/20000], Loss: -15.687606811523438, Learning Rate: 0.01\n",
      "Epoch [12099/20000], Loss: -15.690048217773438, Learning Rate: 0.01\n",
      "Epoch [12100/20000], Loss: -15.692459106445312, Learning Rate: 0.01\n",
      "Epoch [12101/20000], Loss: -15.694915771484375, Learning Rate: 0.01\n",
      "Epoch [12102/20000], Loss: -15.69744873046875, Learning Rate: 0.01\n",
      "Epoch [12103/20000], Loss: -15.700164794921875, Learning Rate: 0.01\n",
      "Epoch [12104/20000], Loss: -15.702911376953125, Learning Rate: 0.01\n",
      "Epoch [12105/20000], Loss: -15.70556640625, Learning Rate: 0.01\n",
      "Epoch [12106/20000], Loss: -15.70819091796875, Learning Rate: 0.01\n",
      "Epoch [12107/20000], Loss: -15.710830688476562, Learning Rate: 0.01\n",
      "Epoch [12108/20000], Loss: -15.713363647460938, Learning Rate: 0.01\n",
      "Epoch [12109/20000], Loss: -15.715713500976562, Learning Rate: 0.01\n",
      "Epoch [12110/20000], Loss: -15.717987060546875, Learning Rate: 0.01\n",
      "Epoch [12111/20000], Loss: -15.7200927734375, Learning Rate: 0.01\n",
      "Epoch [12112/20000], Loss: -15.722152709960938, Learning Rate: 0.01\n",
      "Epoch [12113/20000], Loss: -15.723907470703125, Learning Rate: 0.01\n",
      "Epoch [12114/20000], Loss: -15.725494384765625, Learning Rate: 0.01\n",
      "Epoch [12115/20000], Loss: -15.726882934570312, Learning Rate: 0.01\n",
      "Epoch [12116/20000], Loss: -15.727920532226562, Learning Rate: 0.01\n",
      "Epoch [12117/20000], Loss: -15.728515625, Learning Rate: 0.01\n",
      "Epoch [12118/20000], Loss: -15.728408813476562, Learning Rate: 0.01\n",
      "Epoch [12119/20000], Loss: -15.727523803710938, Learning Rate: 0.01\n",
      "Epoch [12120/20000], Loss: -15.725494384765625, Learning Rate: 0.01\n",
      "Epoch [12121/20000], Loss: -15.7220458984375, Learning Rate: 0.01\n",
      "Epoch [12122/20000], Loss: -15.7166748046875, Learning Rate: 0.01\n",
      "Epoch [12123/20000], Loss: -15.708740234375, Learning Rate: 0.01\n",
      "Epoch [12124/20000], Loss: -15.697433471679688, Learning Rate: 0.01\n",
      "Epoch [12125/20000], Loss: -15.68182373046875, Learning Rate: 0.01\n",
      "Epoch [12126/20000], Loss: -15.66033935546875, Learning Rate: 0.01\n",
      "Epoch [12127/20000], Loss: -15.631500244140625, Learning Rate: 0.01\n",
      "Epoch [12128/20000], Loss: -15.592620849609375, Learning Rate: 0.01\n",
      "Epoch [12129/20000], Loss: -15.541610717773438, Learning Rate: 0.01\n",
      "Epoch [12130/20000], Loss: -15.474777221679688, Learning Rate: 0.01\n",
      "Epoch [12131/20000], Loss: -15.38946533203125, Learning Rate: 0.01\n",
      "Epoch [12132/20000], Loss: -15.282470703125, Learning Rate: 0.01\n",
      "Epoch [12133/20000], Loss: -15.153274536132812, Learning Rate: 0.01\n",
      "Epoch [12134/20000], Loss: -15.003494262695312, Learning Rate: 0.01\n",
      "Epoch [12135/20000], Loss: -14.841629028320312, Learning Rate: 0.01\n",
      "Epoch [12136/20000], Loss: -14.68328857421875, Learning Rate: 0.01\n",
      "Epoch [12137/20000], Loss: -14.554275512695312, Learning Rate: 0.01\n",
      "Epoch [12138/20000], Loss: -14.485794067382812, Learning Rate: 0.01\n",
      "Epoch [12139/20000], Loss: -14.508987426757812, Learning Rate: 0.01\n",
      "Epoch [12140/20000], Loss: -14.639175415039062, Learning Rate: 0.01\n",
      "Epoch [12141/20000], Loss: -14.865585327148438, Learning Rate: 0.01\n",
      "Epoch [12142/20000], Loss: -15.146072387695312, Learning Rate: 0.01\n",
      "Epoch [12143/20000], Loss: -15.420181274414062, Learning Rate: 0.01\n",
      "Epoch [12144/20000], Loss: -15.630691528320312, Learning Rate: 0.01\n",
      "Epoch [12145/20000], Loss: -15.74481201171875, Learning Rate: 0.01\n",
      "Epoch [12146/20000], Loss: -15.762710571289062, Learning Rate: 0.01\n",
      "Epoch [12147/20000], Loss: -15.711456298828125, Learning Rate: 0.01\n",
      "Epoch [12148/20000], Loss: -15.631805419921875, Learning Rate: 0.01\n",
      "Epoch [12149/20000], Loss: -15.5626220703125, Learning Rate: 0.01\n",
      "Epoch [12150/20000], Loss: -15.531021118164062, Learning Rate: 0.01\n",
      "Epoch [12151/20000], Loss: -15.54547119140625, Learning Rate: 0.01\n",
      "Epoch [12152/20000], Loss: -15.597702026367188, Learning Rate: 0.01\n",
      "Epoch [12153/20000], Loss: -15.667312622070312, Learning Rate: 0.01\n",
      "Epoch [12154/20000], Loss: -15.731399536132812, Learning Rate: 0.01\n",
      "Epoch [12155/20000], Loss: -15.773788452148438, Learning Rate: 0.01\n",
      "Epoch [12156/20000], Loss: -15.789291381835938, Learning Rate: 0.01\n",
      "Epoch [12157/20000], Loss: -15.783187866210938, Learning Rate: 0.01\n",
      "Epoch [12158/20000], Loss: -15.767654418945312, Learning Rate: 0.01\n",
      "Epoch [12159/20000], Loss: -15.755340576171875, Learning Rate: 0.01\n",
      "Epoch [12160/20000], Loss: -15.754898071289062, Learning Rate: 0.01\n",
      "Epoch [12161/20000], Loss: -15.76873779296875, Learning Rate: 0.01\n",
      "Epoch [12162/20000], Loss: -15.7933349609375, Learning Rate: 0.01\n",
      "Epoch [12163/20000], Loss: -15.821029663085938, Learning Rate: 0.01\n",
      "Epoch [12164/20000], Loss: -15.84405517578125, Learning Rate: 0.01\n",
      "Epoch [12165/20000], Loss: -15.857528686523438, Learning Rate: 0.01\n",
      "Epoch [12166/20000], Loss: -15.86016845703125, Learning Rate: 0.01\n",
      "Epoch [12167/20000], Loss: -15.854537963867188, Learning Rate: 0.01\n",
      "Epoch [12168/20000], Loss: -15.845458984375, Learning Rate: 0.01\n",
      "Epoch [12169/20000], Loss: -15.838455200195312, Learning Rate: 0.01\n",
      "Epoch [12170/20000], Loss: -15.83697509765625, Learning Rate: 0.01\n",
      "Epoch [12171/20000], Loss: -15.842254638671875, Learning Rate: 0.01\n",
      "Epoch [12172/20000], Loss: -15.853500366210938, Learning Rate: 0.01\n",
      "Epoch [12173/20000], Loss: -15.867721557617188, Learning Rate: 0.01\n",
      "Epoch [12174/20000], Loss: -15.88177490234375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12175/20000], Loss: -15.893020629882812, Learning Rate: 0.01\n",
      "Epoch [12176/20000], Loss: -15.900100708007812, Learning Rate: 0.01\n",
      "Epoch [12177/20000], Loss: -15.90289306640625, Learning Rate: 0.01\n",
      "Epoch [12178/20000], Loss: -15.902755737304688, Learning Rate: 0.01\n",
      "Epoch [12179/20000], Loss: -15.90106201171875, Learning Rate: 0.01\n",
      "Epoch [12180/20000], Loss: -15.89984130859375, Learning Rate: 0.01\n",
      "Epoch [12181/20000], Loss: -15.900115966796875, Learning Rate: 0.01\n",
      "Epoch [12182/20000], Loss: -15.90240478515625, Learning Rate: 0.01\n",
      "Epoch [12183/20000], Loss: -15.906387329101562, Learning Rate: 0.01\n",
      "Epoch [12184/20000], Loss: -15.911529541015625, Learning Rate: 0.01\n",
      "Epoch [12185/20000], Loss: -15.917160034179688, Learning Rate: 0.01\n",
      "Epoch [12186/20000], Loss: -15.922332763671875, Learning Rate: 0.01\n",
      "Epoch [12187/20000], Loss: -15.926544189453125, Learning Rate: 0.01\n",
      "Epoch [12188/20000], Loss: -15.930099487304688, Learning Rate: 0.01\n",
      "Epoch [12189/20000], Loss: -15.932785034179688, Learning Rate: 0.01\n",
      "Epoch [12190/20000], Loss: -15.934890747070312, Learning Rate: 0.01\n",
      "Epoch [12191/20000], Loss: -15.937042236328125, Learning Rate: 0.01\n",
      "Epoch [12192/20000], Loss: -15.939422607421875, Learning Rate: 0.01\n",
      "Epoch [12193/20000], Loss: -15.942138671875, Learning Rate: 0.01\n",
      "Epoch [12194/20000], Loss: -15.945297241210938, Learning Rate: 0.01\n",
      "Epoch [12195/20000], Loss: -15.948684692382812, Learning Rate: 0.01\n",
      "Epoch [12196/20000], Loss: -15.9521484375, Learning Rate: 0.01\n",
      "Epoch [12197/20000], Loss: -15.955474853515625, Learning Rate: 0.01\n",
      "Epoch [12198/20000], Loss: -15.958602905273438, Learning Rate: 0.01\n",
      "Epoch [12199/20000], Loss: -15.961532592773438, Learning Rate: 0.01\n",
      "Epoch [12200/20000], Loss: -15.964080810546875, Learning Rate: 0.01\n",
      "Epoch [12201/20000], Loss: -15.966552734375, Learning Rate: 0.01\n",
      "Epoch [12202/20000], Loss: -15.96893310546875, Learning Rate: 0.01\n",
      "Epoch [12203/20000], Loss: -15.97119140625, Learning Rate: 0.01\n",
      "Epoch [12204/20000], Loss: -15.9736328125, Learning Rate: 0.01\n",
      "Epoch [12205/20000], Loss: -15.976119995117188, Learning Rate: 0.01\n",
      "Epoch [12206/20000], Loss: -15.978790283203125, Learning Rate: 0.01\n",
      "Epoch [12207/20000], Loss: -15.981689453125, Learning Rate: 0.01\n",
      "Epoch [12208/20000], Loss: -15.984451293945312, Learning Rate: 0.01\n",
      "Epoch [12209/20000], Loss: -15.9874267578125, Learning Rate: 0.01\n",
      "Epoch [12210/20000], Loss: -15.990386962890625, Learning Rate: 0.01\n",
      "Epoch [12211/20000], Loss: -15.993270874023438, Learning Rate: 0.01\n",
      "Epoch [12212/20000], Loss: -15.996246337890625, Learning Rate: 0.01\n",
      "Epoch [12213/20000], Loss: -15.998855590820312, Learning Rate: 0.01\n",
      "Epoch [12214/20000], Loss: -16.001449584960938, Learning Rate: 0.01\n",
      "Epoch [12215/20000], Loss: -16.00421142578125, Learning Rate: 0.01\n",
      "Epoch [12216/20000], Loss: -16.006759643554688, Learning Rate: 0.01\n",
      "Epoch [12217/20000], Loss: -16.009353637695312, Learning Rate: 0.01\n",
      "Epoch [12218/20000], Loss: -16.012039184570312, Learning Rate: 0.01\n",
      "Epoch [12219/20000], Loss: -16.014663696289062, Learning Rate: 0.01\n",
      "Epoch [12220/20000], Loss: -16.017379760742188, Learning Rate: 0.01\n",
      "Epoch [12221/20000], Loss: -16.019989013671875, Learning Rate: 0.01\n",
      "Epoch [12222/20000], Loss: -16.022598266601562, Learning Rate: 0.01\n",
      "Epoch [12223/20000], Loss: -16.02545166015625, Learning Rate: 0.01\n",
      "Epoch [12224/20000], Loss: -16.028152465820312, Learning Rate: 0.01\n",
      "Epoch [12225/20000], Loss: -16.030929565429688, Learning Rate: 0.01\n",
      "Epoch [12226/20000], Loss: -16.03363037109375, Learning Rate: 0.01\n",
      "Epoch [12227/20000], Loss: -16.036270141601562, Learning Rate: 0.01\n",
      "Epoch [12228/20000], Loss: -16.038970947265625, Learning Rate: 0.01\n",
      "Epoch [12229/20000], Loss: -16.041702270507812, Learning Rate: 0.01\n",
      "Epoch [12230/20000], Loss: -16.044265747070312, Learning Rate: 0.01\n",
      "Epoch [12231/20000], Loss: -16.046966552734375, Learning Rate: 0.01\n",
      "Epoch [12232/20000], Loss: -16.049560546875, Learning Rate: 0.01\n",
      "Epoch [12233/20000], Loss: -16.052276611328125, Learning Rate: 0.01\n",
      "Epoch [12234/20000], Loss: -16.054885864257812, Learning Rate: 0.01\n",
      "Epoch [12235/20000], Loss: -16.057449340820312, Learning Rate: 0.01\n",
      "Epoch [12236/20000], Loss: -16.060150146484375, Learning Rate: 0.01\n",
      "Epoch [12237/20000], Loss: -16.062774658203125, Learning Rate: 0.01\n",
      "Epoch [12238/20000], Loss: -16.065414428710938, Learning Rate: 0.01\n",
      "Epoch [12239/20000], Loss: -16.068069458007812, Learning Rate: 0.01\n",
      "Epoch [12240/20000], Loss: -16.070648193359375, Learning Rate: 0.01\n",
      "Epoch [12241/20000], Loss: -16.073272705078125, Learning Rate: 0.01\n",
      "Epoch [12242/20000], Loss: -16.075912475585938, Learning Rate: 0.01\n",
      "Epoch [12243/20000], Loss: -16.07843017578125, Learning Rate: 0.01\n",
      "Epoch [12244/20000], Loss: -16.08099365234375, Learning Rate: 0.01\n",
      "Epoch [12245/20000], Loss: -16.083587646484375, Learning Rate: 0.01\n",
      "Epoch [12246/20000], Loss: -16.086044311523438, Learning Rate: 0.01\n",
      "Epoch [12247/20000], Loss: -16.088485717773438, Learning Rate: 0.01\n",
      "Epoch [12248/20000], Loss: -16.090774536132812, Learning Rate: 0.01\n",
      "Epoch [12249/20000], Loss: -16.092971801757812, Learning Rate: 0.01\n",
      "Epoch [12250/20000], Loss: -16.095184326171875, Learning Rate: 0.01\n",
      "Epoch [12251/20000], Loss: -16.097274780273438, Learning Rate: 0.01\n",
      "Epoch [12252/20000], Loss: -16.098876953125, Learning Rate: 0.01\n",
      "Epoch [12253/20000], Loss: -16.100357055664062, Learning Rate: 0.01\n",
      "Epoch [12254/20000], Loss: -16.101409912109375, Learning Rate: 0.01\n",
      "Epoch [12255/20000], Loss: -16.101943969726562, Learning Rate: 0.01\n",
      "Epoch [12256/20000], Loss: -16.101821899414062, Learning Rate: 0.01\n",
      "Epoch [12257/20000], Loss: -16.100631713867188, Learning Rate: 0.01\n",
      "Epoch [12258/20000], Loss: -16.0982666015625, Learning Rate: 0.01\n",
      "Epoch [12259/20000], Loss: -16.093887329101562, Learning Rate: 0.01\n",
      "Epoch [12260/20000], Loss: -16.087265014648438, Learning Rate: 0.01\n",
      "Epoch [12261/20000], Loss: -16.077117919921875, Learning Rate: 0.01\n",
      "Epoch [12262/20000], Loss: -16.062286376953125, Learning Rate: 0.01\n",
      "Epoch [12263/20000], Loss: -16.040939331054688, Learning Rate: 0.01\n",
      "Epoch [12264/20000], Loss: -16.010971069335938, Learning Rate: 0.01\n",
      "Epoch [12265/20000], Loss: -15.968948364257812, Learning Rate: 0.01\n",
      "Epoch [12266/20000], Loss: -15.910614013671875, Learning Rate: 0.01\n",
      "Epoch [12267/20000], Loss: -15.830108642578125, Learning Rate: 0.01\n",
      "Epoch [12268/20000], Loss: -15.720657348632812, Learning Rate: 0.01\n",
      "Epoch [12269/20000], Loss: -15.573211669921875, Learning Rate: 0.01\n",
      "Epoch [12270/20000], Loss: -15.379531860351562, Learning Rate: 0.01\n",
      "Epoch [12271/20000], Loss: -15.130706787109375, Learning Rate: 0.01\n",
      "Epoch [12272/20000], Loss: -14.826263427734375, Learning Rate: 0.01\n",
      "Epoch [12273/20000], Loss: -14.472976684570312, Learning Rate: 0.01\n",
      "Epoch [12274/20000], Loss: -14.105072021484375, Learning Rate: 0.01\n",
      "Epoch [12275/20000], Loss: -13.775650024414062, Learning Rate: 0.01\n",
      "Epoch [12276/20000], Loss: -13.57733154296875, Learning Rate: 0.01\n",
      "Epoch [12277/20000], Loss: -13.593215942382812, Learning Rate: 0.01\n",
      "Epoch [12278/20000], Loss: -13.887649536132812, Learning Rate: 0.01\n",
      "Epoch [12279/20000], Loss: -14.423568725585938, Learning Rate: 0.01\n",
      "Epoch [12280/20000], Loss: -15.078262329101562, Learning Rate: 0.01\n",
      "Epoch [12281/20000], Loss: -15.66375732421875, Learning Rate: 0.01\n",
      "Epoch [12282/20000], Loss: -16.0263671875, Learning Rate: 0.01\n",
      "Epoch [12283/20000], Loss: -16.107666015625, Learning Rate: 0.01\n",
      "Epoch [12284/20000], Loss: -15.961074829101562, Learning Rate: 0.01\n",
      "Epoch [12285/20000], Loss: -15.712295532226562, Learning Rate: 0.01\n",
      "Epoch [12286/20000], Loss: -15.500640869140625, Learning Rate: 0.01\n",
      "Epoch [12287/20000], Loss: -15.428512573242188, Learning Rate: 0.01\n",
      "Epoch [12288/20000], Loss: -15.52288818359375, Learning Rate: 0.01\n",
      "Epoch [12289/20000], Loss: -15.737533569335938, Learning Rate: 0.01\n",
      "Epoch [12290/20000], Loss: -15.975723266601562, Learning Rate: 0.01\n",
      "Epoch [12291/20000], Loss: -16.14453125, Learning Rate: 0.01\n",
      "Epoch [12292/20000], Loss: -16.194595336914062, Learning Rate: 0.01\n",
      "Epoch [12293/20000], Loss: -16.137786865234375, Learning Rate: 0.01\n",
      "Epoch [12294/20000], Loss: -16.029434204101562, Learning Rate: 0.01\n",
      "Epoch [12295/20000], Loss: -15.936996459960938, Learning Rate: 0.01\n",
      "Epoch [12296/20000], Loss: -15.908554077148438, Learning Rate: 0.01\n",
      "Epoch [12297/20000], Loss: -15.954559326171875, Learning Rate: 0.01\n",
      "Epoch [12298/20000], Loss: -16.050155639648438, Learning Rate: 0.01\n",
      "Epoch [12299/20000], Loss: -16.15155029296875, Learning Rate: 0.01\n",
      "Epoch [12300/20000], Loss: -16.219467163085938, Learning Rate: 0.01\n",
      "Epoch [12301/20000], Loss: -16.23626708984375, Learning Rate: 0.01\n",
      "Epoch [12302/20000], Loss: -16.2103271484375, Learning Rate: 0.01\n",
      "Epoch [12303/20000], Loss: -16.166671752929688, Learning Rate: 0.01\n",
      "Epoch [12304/20000], Loss: -16.13311767578125, Learning Rate: 0.01\n",
      "Epoch [12305/20000], Loss: -16.126678466796875, Learning Rate: 0.01\n",
      "Epoch [12306/20000], Loss: -16.149063110351562, Learning Rate: 0.01\n",
      "Epoch [12307/20000], Loss: -16.188385009765625, Learning Rate: 0.01\n",
      "Epoch [12308/20000], Loss: -16.227249145507812, Learning Rate: 0.01\n",
      "Epoch [12309/20000], Loss: -16.251708984375, Learning Rate: 0.01\n",
      "Epoch [12310/20000], Loss: -16.2567138671875, Learning Rate: 0.01\n",
      "Epoch [12311/20000], Loss: -16.246841430664062, Learning Rate: 0.01\n",
      "Epoch [12312/20000], Loss: -16.232025146484375, Learning Rate: 0.01\n",
      "Epoch [12313/20000], Loss: -16.2222900390625, Learning Rate: 0.01\n",
      "Epoch [12314/20000], Loss: -16.22357177734375, Learning Rate: 0.01\n",
      "Epoch [12315/20000], Loss: -16.23565673828125, Learning Rate: 0.01\n",
      "Epoch [12316/20000], Loss: -16.253387451171875, Learning Rate: 0.01\n",
      "Epoch [12317/20000], Loss: -16.270462036132812, Learning Rate: 0.01\n",
      "Epoch [12318/20000], Loss: -16.281585693359375, Learning Rate: 0.01\n",
      "Epoch [12319/20000], Loss: -16.2852783203125, Learning Rate: 0.01\n",
      "Epoch [12320/20000], Loss: -16.282852172851562, Learning Rate: 0.01\n",
      "Epoch [12321/20000], Loss: -16.2783203125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12322/20000], Loss: -16.275283813476562, Learning Rate: 0.01\n",
      "Epoch [12323/20000], Loss: -16.276214599609375, Learning Rate: 0.01\n",
      "Epoch [12324/20000], Loss: -16.281463623046875, Learning Rate: 0.01\n",
      "Epoch [12325/20000], Loss: -16.289459228515625, Learning Rate: 0.01\n",
      "Epoch [12326/20000], Loss: -16.297836303710938, Learning Rate: 0.01\n",
      "Epoch [12327/20000], Loss: -16.304550170898438, Learning Rate: 0.01\n",
      "Epoch [12328/20000], Loss: -16.308609008789062, Learning Rate: 0.01\n",
      "Epoch [12329/20000], Loss: -16.310256958007812, Learning Rate: 0.01\n",
      "Epoch [12330/20000], Loss: -16.310638427734375, Learning Rate: 0.01\n",
      "Epoch [12331/20000], Loss: -16.310760498046875, Learning Rate: 0.01\n",
      "Epoch [12332/20000], Loss: -16.312164306640625, Learning Rate: 0.01\n",
      "Epoch [12333/20000], Loss: -16.315139770507812, Learning Rate: 0.01\n",
      "Epoch [12334/20000], Loss: -16.3192138671875, Learning Rate: 0.01\n",
      "Epoch [12335/20000], Loss: -16.324020385742188, Learning Rate: 0.01\n",
      "Epoch [12336/20000], Loss: -16.328659057617188, Learning Rate: 0.01\n",
      "Epoch [12337/20000], Loss: -16.332473754882812, Learning Rate: 0.01\n",
      "Epoch [12338/20000], Loss: -16.33538818359375, Learning Rate: 0.01\n",
      "Epoch [12339/20000], Loss: -16.337387084960938, Learning Rate: 0.01\n",
      "Epoch [12340/20000], Loss: -16.339141845703125, Learning Rate: 0.01\n",
      "Epoch [12341/20000], Loss: -16.340988159179688, Learning Rate: 0.01\n",
      "Epoch [12342/20000], Loss: -16.343048095703125, Learning Rate: 0.01\n",
      "Epoch [12343/20000], Loss: -16.345794677734375, Learning Rate: 0.01\n",
      "Epoch [12344/20000], Loss: -16.348876953125, Learning Rate: 0.01\n",
      "Epoch [12345/20000], Loss: -16.352203369140625, Learning Rate: 0.01\n",
      "Epoch [12346/20000], Loss: -16.355392456054688, Learning Rate: 0.01\n",
      "Epoch [12347/20000], Loss: -16.358489990234375, Learning Rate: 0.01\n",
      "Epoch [12348/20000], Loss: -16.361175537109375, Learning Rate: 0.01\n",
      "Epoch [12349/20000], Loss: -16.363754272460938, Learning Rate: 0.01\n",
      "Epoch [12350/20000], Loss: -16.366119384765625, Learning Rate: 0.01\n",
      "Epoch [12351/20000], Loss: -16.368453979492188, Learning Rate: 0.01\n",
      "Epoch [12352/20000], Loss: -16.370819091796875, Learning Rate: 0.01\n",
      "Epoch [12353/20000], Loss: -16.373336791992188, Learning Rate: 0.01\n",
      "Epoch [12354/20000], Loss: -16.376113891601562, Learning Rate: 0.01\n",
      "Epoch [12355/20000], Loss: -16.378814697265625, Learning Rate: 0.01\n",
      "Epoch [12356/20000], Loss: -16.381790161132812, Learning Rate: 0.01\n",
      "Epoch [12357/20000], Loss: -16.384613037109375, Learning Rate: 0.01\n",
      "Epoch [12358/20000], Loss: -16.387313842773438, Learning Rate: 0.01\n",
      "Epoch [12359/20000], Loss: -16.389938354492188, Learning Rate: 0.01\n",
      "Epoch [12360/20000], Loss: -16.392547607421875, Learning Rate: 0.01\n",
      "Epoch [12361/20000], Loss: -16.395095825195312, Learning Rate: 0.01\n",
      "Epoch [12362/20000], Loss: -16.397552490234375, Learning Rate: 0.01\n",
      "Epoch [12363/20000], Loss: -16.400161743164062, Learning Rate: 0.01\n",
      "Epoch [12364/20000], Loss: -16.4027099609375, Learning Rate: 0.01\n",
      "Epoch [12365/20000], Loss: -16.405288696289062, Learning Rate: 0.01\n",
      "Epoch [12366/20000], Loss: -16.407913208007812, Learning Rate: 0.01\n",
      "Epoch [12367/20000], Loss: -16.410797119140625, Learning Rate: 0.01\n",
      "Epoch [12368/20000], Loss: -16.413421630859375, Learning Rate: 0.01\n",
      "Epoch [12369/20000], Loss: -16.416061401367188, Learning Rate: 0.01\n",
      "Epoch [12370/20000], Loss: -16.418594360351562, Learning Rate: 0.01\n",
      "Epoch [12371/20000], Loss: -16.421279907226562, Learning Rate: 0.01\n",
      "Epoch [12372/20000], Loss: -16.423812866210938, Learning Rate: 0.01\n",
      "Epoch [12373/20000], Loss: -16.42645263671875, Learning Rate: 0.01\n",
      "Epoch [12374/20000], Loss: -16.429031372070312, Learning Rate: 0.01\n",
      "Epoch [12375/20000], Loss: -16.43157958984375, Learning Rate: 0.01\n",
      "Epoch [12376/20000], Loss: -16.434173583984375, Learning Rate: 0.01\n",
      "Epoch [12377/20000], Loss: -16.436752319335938, Learning Rate: 0.01\n",
      "Epoch [12378/20000], Loss: -16.43939208984375, Learning Rate: 0.01\n",
      "Epoch [12379/20000], Loss: -16.441864013671875, Learning Rate: 0.01\n",
      "Epoch [12380/20000], Loss: -16.444442749023438, Learning Rate: 0.01\n",
      "Epoch [12381/20000], Loss: -16.447052001953125, Learning Rate: 0.01\n",
      "Epoch [12382/20000], Loss: -16.449600219726562, Learning Rate: 0.01\n",
      "Epoch [12383/20000], Loss: -16.4521484375, Learning Rate: 0.01\n",
      "Epoch [12384/20000], Loss: -16.454681396484375, Learning Rate: 0.01\n",
      "Epoch [12385/20000], Loss: -16.457138061523438, Learning Rate: 0.01\n",
      "Epoch [12386/20000], Loss: -16.459457397460938, Learning Rate: 0.01\n",
      "Epoch [12387/20000], Loss: -16.46185302734375, Learning Rate: 0.01\n",
      "Epoch [12388/20000], Loss: -16.464157104492188, Learning Rate: 0.01\n",
      "Epoch [12389/20000], Loss: -16.466445922851562, Learning Rate: 0.01\n",
      "Epoch [12390/20000], Loss: -16.468612670898438, Learning Rate: 0.01\n",
      "Epoch [12391/20000], Loss: -16.470382690429688, Learning Rate: 0.01\n",
      "Epoch [12392/20000], Loss: -16.472259521484375, Learning Rate: 0.01\n",
      "Epoch [12393/20000], Loss: -16.473785400390625, Learning Rate: 0.01\n",
      "Epoch [12394/20000], Loss: -16.474838256835938, Learning Rate: 0.01\n",
      "Epoch [12395/20000], Loss: -16.475494384765625, Learning Rate: 0.01\n",
      "Epoch [12396/20000], Loss: -16.475479125976562, Learning Rate: 0.01\n",
      "Epoch [12397/20000], Loss: -16.47467041015625, Learning Rate: 0.01\n",
      "Epoch [12398/20000], Loss: -16.47271728515625, Learning Rate: 0.01\n",
      "Epoch [12399/20000], Loss: -16.469039916992188, Learning Rate: 0.01\n",
      "Epoch [12400/20000], Loss: -16.463363647460938, Learning Rate: 0.01\n",
      "Epoch [12401/20000], Loss: -16.454818725585938, Learning Rate: 0.01\n",
      "Epoch [12402/20000], Loss: -16.44232177734375, Learning Rate: 0.01\n",
      "Epoch [12403/20000], Loss: -16.424530029296875, Learning Rate: 0.01\n",
      "Epoch [12404/20000], Loss: -16.399612426757812, Learning Rate: 0.01\n",
      "Epoch [12405/20000], Loss: -16.365234375, Learning Rate: 0.01\n",
      "Epoch [12406/20000], Loss: -16.318389892578125, Learning Rate: 0.01\n",
      "Epoch [12407/20000], Loss: -16.255294799804688, Learning Rate: 0.01\n",
      "Epoch [12408/20000], Loss: -16.17169189453125, Learning Rate: 0.01\n",
      "Epoch [12409/20000], Loss: -16.063583374023438, Learning Rate: 0.01\n",
      "Epoch [12410/20000], Loss: -15.928009033203125, Learning Rate: 0.01\n",
      "Epoch [12411/20000], Loss: -15.765396118164062, Learning Rate: 0.01\n",
      "Epoch [12412/20000], Loss: -15.582244873046875, Learning Rate: 0.01\n",
      "Epoch [12413/20000], Loss: -15.396682739257812, Learning Rate: 0.01\n",
      "Epoch [12414/20000], Loss: -15.238128662109375, Learning Rate: 0.01\n",
      "Epoch [12415/20000], Loss: -15.149246215820312, Learning Rate: 0.01\n",
      "Epoch [12416/20000], Loss: -15.170791625976562, Learning Rate: 0.01\n",
      "Epoch [12417/20000], Loss: -15.327072143554688, Learning Rate: 0.01\n",
      "Epoch [12418/20000], Loss: -15.601287841796875, Learning Rate: 0.01\n",
      "Epoch [12419/20000], Loss: -15.933578491210938, Learning Rate: 0.01\n",
      "Epoch [12420/20000], Loss: -16.238754272460938, Learning Rate: 0.01\n",
      "Epoch [12421/20000], Loss: -16.443115234375, Learning Rate: 0.01\n",
      "Epoch [12422/20000], Loss: -16.514877319335938, Learning Rate: 0.01\n",
      "Epoch [12423/20000], Loss: -16.4710693359375, Learning Rate: 0.01\n",
      "Epoch [12424/20000], Loss: -16.363632202148438, Learning Rate: 0.01\n",
      "Epoch [12425/20000], Loss: -16.254898071289062, Learning Rate: 0.01\n",
      "Epoch [12426/20000], Loss: -16.195709228515625, Learning Rate: 0.01\n",
      "Epoch [12427/20000], Loss: -16.20867919921875, Learning Rate: 0.01\n",
      "Epoch [12428/20000], Loss: -16.285720825195312, Learning Rate: 0.01\n",
      "Epoch [12429/20000], Loss: -16.393218994140625, Learning Rate: 0.01\n",
      "Epoch [12430/20000], Loss: -16.489990234375, Learning Rate: 0.01\n",
      "Epoch [12431/20000], Loss: -16.54583740234375, Learning Rate: 0.01\n",
      "Epoch [12432/20000], Loss: -16.551589965820312, Learning Rate: 0.01\n",
      "Epoch [12433/20000], Loss: -16.520355224609375, Learning Rate: 0.01\n",
      "Epoch [12434/20000], Loss: -16.476821899414062, Learning Rate: 0.01\n",
      "Epoch [12435/20000], Loss: -16.446701049804688, Learning Rate: 0.01\n",
      "Epoch [12436/20000], Loss: -16.444656372070312, Learning Rate: 0.01\n",
      "Epoch [12437/20000], Loss: -16.471893310546875, Learning Rate: 0.01\n",
      "Epoch [12438/20000], Loss: -16.517105102539062, Learning Rate: 0.01\n",
      "Epoch [12439/20000], Loss: -16.563125610351562, Learning Rate: 0.01\n",
      "Epoch [12440/20000], Loss: -16.59503173828125, Learning Rate: 0.01\n",
      "Epoch [12441/20000], Loss: -16.606002807617188, Learning Rate: 0.01\n",
      "Epoch [12442/20000], Loss: -16.598464965820312, Learning Rate: 0.01\n",
      "Epoch [12443/20000], Loss: -16.581329345703125, Learning Rate: 0.01\n",
      "Epoch [12444/20000], Loss: -16.565200805664062, Learning Rate: 0.01\n",
      "Epoch [12445/20000], Loss: -16.558425903320312, Learning Rate: 0.01\n",
      "Epoch [12446/20000], Loss: -16.5638427734375, Learning Rate: 0.01\n",
      "Epoch [12447/20000], Loss: -16.57879638671875, Learning Rate: 0.01\n",
      "Epoch [12448/20000], Loss: -16.597686767578125, Learning Rate: 0.01\n",
      "Epoch [12449/20000], Loss: -16.614532470703125, Learning Rate: 0.01\n",
      "Epoch [12450/20000], Loss: -16.625106811523438, Learning Rate: 0.01\n",
      "Epoch [12451/20000], Loss: -16.62847900390625, Learning Rate: 0.01\n",
      "Epoch [12452/20000], Loss: -16.626693725585938, Learning Rate: 0.01\n",
      "Epoch [12453/20000], Loss: -16.623123168945312, Learning Rate: 0.01\n",
      "Epoch [12454/20000], Loss: -16.621139526367188, Learning Rate: 0.01\n",
      "Epoch [12455/20000], Loss: -16.622879028320312, Learning Rate: 0.01\n",
      "Epoch [12456/20000], Loss: -16.62811279296875, Learning Rate: 0.01\n",
      "Epoch [12457/20000], Loss: -16.6358642578125, Learning Rate: 0.01\n",
      "Epoch [12458/20000], Loss: -16.643753051757812, Learning Rate: 0.01\n",
      "Epoch [12459/20000], Loss: -16.650299072265625, Learning Rate: 0.01\n",
      "Epoch [12460/20000], Loss: -16.654510498046875, Learning Rate: 0.01\n",
      "Epoch [12461/20000], Loss: -16.65625, Learning Rate: 0.01\n",
      "Epoch [12462/20000], Loss: -16.656539916992188, Learning Rate: 0.01\n",
      "Epoch [12463/20000], Loss: -16.656661987304688, Learning Rate: 0.01\n",
      "Epoch [12464/20000], Loss: -16.65753173828125, Learning Rate: 0.01\n",
      "Epoch [12465/20000], Loss: -16.6597900390625, Learning Rate: 0.01\n",
      "Epoch [12466/20000], Loss: -16.663543701171875, Learning Rate: 0.01\n",
      "Epoch [12467/20000], Loss: -16.66796875, Learning Rate: 0.01\n",
      "Epoch [12468/20000], Loss: -16.672866821289062, Learning Rate: 0.01\n",
      "Epoch [12469/20000], Loss: -16.677139282226562, Learning Rate: 0.01\n",
      "Epoch [12470/20000], Loss: -16.68060302734375, Learning Rate: 0.01\n",
      "Epoch [12471/20000], Loss: -16.683334350585938, Learning Rate: 0.01\n",
      "Epoch [12472/20000], Loss: -16.685394287109375, Learning Rate: 0.01\n",
      "Epoch [12473/20000], Loss: -16.68719482421875, Learning Rate: 0.01\n",
      "Epoch [12474/20000], Loss: -16.689163208007812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12475/20000], Loss: -16.69146728515625, Learning Rate: 0.01\n",
      "Epoch [12476/20000], Loss: -16.694061279296875, Learning Rate: 0.01\n",
      "Epoch [12477/20000], Loss: -16.697113037109375, Learning Rate: 0.01\n",
      "Epoch [12478/20000], Loss: -16.700332641601562, Learning Rate: 0.01\n",
      "Epoch [12479/20000], Loss: -16.703475952148438, Learning Rate: 0.01\n",
      "Epoch [12480/20000], Loss: -16.706558227539062, Learning Rate: 0.01\n",
      "Epoch [12481/20000], Loss: -16.709243774414062, Learning Rate: 0.01\n",
      "Epoch [12482/20000], Loss: -16.711807250976562, Learning Rate: 0.01\n",
      "Epoch [12483/20000], Loss: -16.714141845703125, Learning Rate: 0.01\n",
      "Epoch [12484/20000], Loss: -16.716323852539062, Learning Rate: 0.01\n",
      "Epoch [12485/20000], Loss: -16.718643188476562, Learning Rate: 0.01\n",
      "Epoch [12486/20000], Loss: -16.721282958984375, Learning Rate: 0.01\n",
      "Epoch [12487/20000], Loss: -16.7237548828125, Learning Rate: 0.01\n",
      "Epoch [12488/20000], Loss: -16.726348876953125, Learning Rate: 0.01\n",
      "Epoch [12489/20000], Loss: -16.729232788085938, Learning Rate: 0.01\n",
      "Epoch [12490/20000], Loss: -16.731964111328125, Learning Rate: 0.01\n",
      "Epoch [12491/20000], Loss: -16.734725952148438, Learning Rate: 0.01\n",
      "Epoch [12492/20000], Loss: -16.737319946289062, Learning Rate: 0.01\n",
      "Epoch [12493/20000], Loss: -16.739974975585938, Learning Rate: 0.01\n",
      "Epoch [12494/20000], Loss: -16.742462158203125, Learning Rate: 0.01\n",
      "Epoch [12495/20000], Loss: -16.745071411132812, Learning Rate: 0.01\n",
      "Epoch [12496/20000], Loss: -16.747467041015625, Learning Rate: 0.01\n",
      "Epoch [12497/20000], Loss: -16.750015258789062, Learning Rate: 0.01\n",
      "Epoch [12498/20000], Loss: -16.7525634765625, Learning Rate: 0.01\n",
      "Epoch [12499/20000], Loss: -16.75506591796875, Learning Rate: 0.01\n",
      "Epoch [12500/20000], Loss: -16.75762939453125, Learning Rate: 0.01\n",
      "Epoch [12501/20000], Loss: -16.760269165039062, Learning Rate: 0.01\n",
      "Epoch [12502/20000], Loss: -16.762908935546875, Learning Rate: 0.01\n",
      "Epoch [12503/20000], Loss: -16.7655029296875, Learning Rate: 0.01\n",
      "Epoch [12504/20000], Loss: -16.76812744140625, Learning Rate: 0.01\n",
      "Epoch [12505/20000], Loss: -16.770706176757812, Learning Rate: 0.01\n",
      "Epoch [12506/20000], Loss: -16.773223876953125, Learning Rate: 0.01\n",
      "Epoch [12507/20000], Loss: -16.775787353515625, Learning Rate: 0.01\n",
      "Epoch [12508/20000], Loss: -16.77838134765625, Learning Rate: 0.01\n",
      "Epoch [12509/20000], Loss: -16.78076171875, Learning Rate: 0.01\n",
      "Epoch [12510/20000], Loss: -16.783279418945312, Learning Rate: 0.01\n",
      "Epoch [12511/20000], Loss: -16.785797119140625, Learning Rate: 0.01\n",
      "Epoch [12512/20000], Loss: -16.788330078125, Learning Rate: 0.01\n",
      "Epoch [12513/20000], Loss: -16.790817260742188, Learning Rate: 0.01\n",
      "Epoch [12514/20000], Loss: -16.793167114257812, Learning Rate: 0.01\n",
      "Epoch [12515/20000], Loss: -16.795623779296875, Learning Rate: 0.01\n",
      "Epoch [12516/20000], Loss: -16.798004150390625, Learning Rate: 0.01\n",
      "Epoch [12517/20000], Loss: -16.800552368164062, Learning Rate: 0.01\n",
      "Epoch [12518/20000], Loss: -16.802658081054688, Learning Rate: 0.01\n",
      "Epoch [12519/20000], Loss: -16.804901123046875, Learning Rate: 0.01\n",
      "Epoch [12520/20000], Loss: -16.80694580078125, Learning Rate: 0.01\n",
      "Epoch [12521/20000], Loss: -16.809005737304688, Learning Rate: 0.01\n",
      "Epoch [12522/20000], Loss: -16.810897827148438, Learning Rate: 0.01\n",
      "Epoch [12523/20000], Loss: -16.812393188476562, Learning Rate: 0.01\n",
      "Epoch [12524/20000], Loss: -16.81378173828125, Learning Rate: 0.01\n",
      "Epoch [12525/20000], Loss: -16.814727783203125, Learning Rate: 0.01\n",
      "Epoch [12526/20000], Loss: -16.815231323242188, Learning Rate: 0.01\n",
      "Epoch [12527/20000], Loss: -16.81512451171875, Learning Rate: 0.01\n",
      "Epoch [12528/20000], Loss: -16.814208984375, Learning Rate: 0.01\n",
      "Epoch [12529/20000], Loss: -16.812118530273438, Learning Rate: 0.01\n",
      "Epoch [12530/20000], Loss: -16.808502197265625, Learning Rate: 0.01\n",
      "Epoch [12531/20000], Loss: -16.803009033203125, Learning Rate: 0.01\n",
      "Epoch [12532/20000], Loss: -16.794830322265625, Learning Rate: 0.01\n",
      "Epoch [12533/20000], Loss: -16.78314208984375, Learning Rate: 0.01\n",
      "Epoch [12534/20000], Loss: -16.7667236328125, Learning Rate: 0.01\n",
      "Epoch [12535/20000], Loss: -16.743698120117188, Learning Rate: 0.01\n",
      "Epoch [12536/20000], Loss: -16.712234497070312, Learning Rate: 0.01\n",
      "Epoch [12537/20000], Loss: -16.6695556640625, Learning Rate: 0.01\n",
      "Epoch [12538/20000], Loss: -16.612060546875, Learning Rate: 0.01\n",
      "Epoch [12539/20000], Loss: -16.5350341796875, Learning Rate: 0.01\n",
      "Epoch [12540/20000], Loss: -16.434112548828125, Learning Rate: 0.01\n",
      "Epoch [12541/20000], Loss: -16.303024291992188, Learning Rate: 0.01\n",
      "Epoch [12542/20000], Loss: -16.13836669921875, Learning Rate: 0.01\n",
      "Epoch [12543/20000], Loss: -15.936599731445312, Learning Rate: 0.01\n",
      "Epoch [12544/20000], Loss: -15.704086303710938, Learning Rate: 0.01\n",
      "Epoch [12545/20000], Loss: -15.4525146484375, Learning Rate: 0.01\n",
      "Epoch [12546/20000], Loss: -15.214920043945312, Learning Rate: 0.01\n",
      "Epoch [12547/20000], Loss: -15.032058715820312, Learning Rate: 0.01\n",
      "Epoch [12548/20000], Loss: -14.964950561523438, Learning Rate: 0.01\n",
      "Epoch [12549/20000], Loss: -15.055404663085938, Learning Rate: 0.01\n",
      "Epoch [12550/20000], Loss: -15.324722290039062, Learning Rate: 0.01\n",
      "Epoch [12551/20000], Loss: -15.72833251953125, Learning Rate: 0.01\n",
      "Epoch [12552/20000], Loss: -16.180160522460938, Learning Rate: 0.01\n",
      "Epoch [12553/20000], Loss: -16.56793212890625, Learning Rate: 0.01\n",
      "Epoch [12554/20000], Loss: -16.808090209960938, Learning Rate: 0.01\n",
      "Epoch [12555/20000], Loss: -16.872390747070312, Learning Rate: 0.01\n",
      "Epoch [12556/20000], Loss: -16.79248046875, Learning Rate: 0.01\n",
      "Epoch [12557/20000], Loss: -16.638534545898438, Learning Rate: 0.01\n",
      "Epoch [12558/20000], Loss: -16.489181518554688, Learning Rate: 0.01\n",
      "Epoch [12559/20000], Loss: -16.406951904296875, Learning Rate: 0.01\n",
      "Epoch [12560/20000], Loss: -16.418991088867188, Learning Rate: 0.01\n",
      "Epoch [12561/20000], Loss: -16.515625, Learning Rate: 0.01\n",
      "Epoch [12562/20000], Loss: -16.655410766601562, Learning Rate: 0.01\n",
      "Epoch [12563/20000], Loss: -16.788330078125, Learning Rate: 0.01\n",
      "Epoch [12564/20000], Loss: -16.874267578125, Learning Rate: 0.01\n",
      "Epoch [12565/20000], Loss: -16.898101806640625, Learning Rate: 0.01\n",
      "Epoch [12566/20000], Loss: -16.871078491210938, Learning Rate: 0.01\n",
      "Epoch [12567/20000], Loss: -16.820343017578125, Learning Rate: 0.01\n",
      "Epoch [12568/20000], Loss: -16.776519775390625, Learning Rate: 0.01\n",
      "Epoch [12569/20000], Loss: -16.760818481445312, Learning Rate: 0.01\n",
      "Epoch [12570/20000], Loss: -16.779373168945312, Learning Rate: 0.01\n",
      "Epoch [12571/20000], Loss: -16.823257446289062, Learning Rate: 0.01\n",
      "Epoch [12572/20000], Loss: -16.875015258789062, Learning Rate: 0.01\n",
      "Epoch [12573/20000], Loss: -16.916778564453125, Learning Rate: 0.01\n",
      "Epoch [12574/20000], Loss: -16.93798828125, Learning Rate: 0.01\n",
      "Epoch [12575/20000], Loss: -16.937637329101562, Learning Rate: 0.01\n",
      "Epoch [12576/20000], Loss: -16.922836303710938, Learning Rate: 0.01\n",
      "Epoch [12577/20000], Loss: -16.9051513671875, Learning Rate: 0.01\n",
      "Epoch [12578/20000], Loss: -16.894363403320312, Learning Rate: 0.01\n",
      "Epoch [12579/20000], Loss: -16.896163940429688, Learning Rate: 0.01\n",
      "Epoch [12580/20000], Loss: -16.91009521484375, Learning Rate: 0.01\n",
      "Epoch [12581/20000], Loss: -16.931045532226562, Learning Rate: 0.01\n",
      "Epoch [12582/20000], Loss: -16.9525146484375, Learning Rate: 0.01\n",
      "Epoch [12583/20000], Loss: -16.968353271484375, Learning Rate: 0.01\n",
      "Epoch [12584/20000], Loss: -16.975677490234375, Learning Rate: 0.01\n",
      "Epoch [12585/20000], Loss: -16.975067138671875, Learning Rate: 0.01\n",
      "Epoch [12586/20000], Loss: -16.969879150390625, Learning Rate: 0.01\n",
      "Epoch [12587/20000], Loss: -16.964035034179688, Learning Rate: 0.01\n",
      "Epoch [12588/20000], Loss: -16.961273193359375, Learning Rate: 0.01\n",
      "Epoch [12589/20000], Loss: -16.963104248046875, Learning Rate: 0.01\n",
      "Epoch [12590/20000], Loss: -16.9696044921875, Learning Rate: 0.01\n",
      "Epoch [12591/20000], Loss: -16.979019165039062, Learning Rate: 0.01\n",
      "Epoch [12592/20000], Loss: -16.988800048828125, Learning Rate: 0.01\n",
      "Epoch [12593/20000], Loss: -16.99700927734375, Learning Rate: 0.01\n",
      "Epoch [12594/20000], Loss: -17.00238037109375, Learning Rate: 0.01\n",
      "Epoch [12595/20000], Loss: -17.004745483398438, Learning Rate: 0.01\n",
      "Epoch [12596/20000], Loss: -17.005157470703125, Learning Rate: 0.01\n",
      "Epoch [12597/20000], Loss: -17.004714965820312, Learning Rate: 0.01\n",
      "Epoch [12598/20000], Loss: -17.004898071289062, Learning Rate: 0.01\n",
      "Epoch [12599/20000], Loss: -17.006118774414062, Learning Rate: 0.01\n",
      "Epoch [12600/20000], Loss: -17.009033203125, Learning Rate: 0.01\n",
      "Epoch [12601/20000], Loss: -17.012924194335938, Learning Rate: 0.01\n",
      "Epoch [12602/20000], Loss: -17.017807006835938, Learning Rate: 0.01\n",
      "Epoch [12603/20000], Loss: -17.022506713867188, Learning Rate: 0.01\n",
      "Epoch [12604/20000], Loss: -17.026626586914062, Learning Rate: 0.01\n",
      "Epoch [12605/20000], Loss: -17.029876708984375, Learning Rate: 0.01\n",
      "Epoch [12606/20000], Loss: -17.032333374023438, Learning Rate: 0.01\n",
      "Epoch [12607/20000], Loss: -17.034210205078125, Learning Rate: 0.01\n",
      "Epoch [12608/20000], Loss: -17.036117553710938, Learning Rate: 0.01\n",
      "Epoch [12609/20000], Loss: -17.037933349609375, Learning Rate: 0.01\n",
      "Epoch [12610/20000], Loss: -17.040237426757812, Learning Rate: 0.01\n",
      "Epoch [12611/20000], Loss: -17.042816162109375, Learning Rate: 0.01\n",
      "Epoch [12612/20000], Loss: -17.045806884765625, Learning Rate: 0.01\n",
      "Epoch [12613/20000], Loss: -17.048919677734375, Learning Rate: 0.01\n",
      "Epoch [12614/20000], Loss: -17.052032470703125, Learning Rate: 0.01\n",
      "Epoch [12615/20000], Loss: -17.055023193359375, Learning Rate: 0.01\n",
      "Epoch [12616/20000], Loss: -17.057846069335938, Learning Rate: 0.01\n",
      "Epoch [12617/20000], Loss: -17.060287475585938, Learning Rate: 0.01\n",
      "Epoch [12618/20000], Loss: -17.062698364257812, Learning Rate: 0.01\n",
      "Epoch [12619/20000], Loss: -17.06500244140625, Learning Rate: 0.01\n",
      "Epoch [12620/20000], Loss: -17.067337036132812, Learning Rate: 0.01\n",
      "Epoch [12621/20000], Loss: -17.069778442382812, Learning Rate: 0.01\n",
      "Epoch [12622/20000], Loss: -17.072235107421875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12623/20000], Loss: -17.07476806640625, Learning Rate: 0.01\n",
      "Epoch [12624/20000], Loss: -17.077484130859375, Learning Rate: 0.01\n",
      "Epoch [12625/20000], Loss: -17.0802001953125, Learning Rate: 0.01\n",
      "Epoch [12626/20000], Loss: -17.0830078125, Learning Rate: 0.01\n",
      "Epoch [12627/20000], Loss: -17.085662841796875, Learning Rate: 0.01\n",
      "Epoch [12628/20000], Loss: -17.0882568359375, Learning Rate: 0.01\n",
      "Epoch [12629/20000], Loss: -17.090774536132812, Learning Rate: 0.01\n",
      "Epoch [12630/20000], Loss: -17.093246459960938, Learning Rate: 0.01\n",
      "Epoch [12631/20000], Loss: -17.095779418945312, Learning Rate: 0.01\n",
      "Epoch [12632/20000], Loss: -17.09808349609375, Learning Rate: 0.01\n",
      "Epoch [12633/20000], Loss: -17.100631713867188, Learning Rate: 0.01\n",
      "Epoch [12634/20000], Loss: -17.1031494140625, Learning Rate: 0.01\n",
      "Epoch [12635/20000], Loss: -17.105621337890625, Learning Rate: 0.01\n",
      "Epoch [12636/20000], Loss: -17.108200073242188, Learning Rate: 0.01\n",
      "Epoch [12637/20000], Loss: -17.1107177734375, Learning Rate: 0.01\n",
      "Epoch [12638/20000], Loss: -17.113311767578125, Learning Rate: 0.01\n",
      "Epoch [12639/20000], Loss: -17.115859985351562, Learning Rate: 0.01\n",
      "Epoch [12640/20000], Loss: -17.118484497070312, Learning Rate: 0.01\n",
      "Epoch [12641/20000], Loss: -17.120895385742188, Learning Rate: 0.01\n",
      "Epoch [12642/20000], Loss: -17.123519897460938, Learning Rate: 0.01\n",
      "Epoch [12643/20000], Loss: -17.126144409179688, Learning Rate: 0.01\n",
      "Epoch [12644/20000], Loss: -17.128616333007812, Learning Rate: 0.01\n",
      "Epoch [12645/20000], Loss: -17.131057739257812, Learning Rate: 0.01\n",
      "Epoch [12646/20000], Loss: -17.133499145507812, Learning Rate: 0.01\n",
      "Epoch [12647/20000], Loss: -17.13604736328125, Learning Rate: 0.01\n",
      "Epoch [12648/20000], Loss: -17.138641357421875, Learning Rate: 0.01\n",
      "Epoch [12649/20000], Loss: -17.14117431640625, Learning Rate: 0.01\n",
      "Epoch [12650/20000], Loss: -17.143798828125, Learning Rate: 0.01\n",
      "Epoch [12651/20000], Loss: -17.146194458007812, Learning Rate: 0.01\n",
      "Epoch [12652/20000], Loss: -17.148712158203125, Learning Rate: 0.01\n",
      "Epoch [12653/20000], Loss: -17.151290893554688, Learning Rate: 0.01\n",
      "Epoch [12654/20000], Loss: -17.153656005859375, Learning Rate: 0.01\n",
      "Epoch [12655/20000], Loss: -17.156295776367188, Learning Rate: 0.01\n",
      "Epoch [12656/20000], Loss: -17.1588134765625, Learning Rate: 0.01\n",
      "Epoch [12657/20000], Loss: -17.161392211914062, Learning Rate: 0.01\n",
      "Epoch [12658/20000], Loss: -17.163955688476562, Learning Rate: 0.01\n",
      "Epoch [12659/20000], Loss: -17.166458129882812, Learning Rate: 0.01\n",
      "Epoch [12660/20000], Loss: -17.16888427734375, Learning Rate: 0.01\n",
      "Epoch [12661/20000], Loss: -17.171432495117188, Learning Rate: 0.01\n",
      "Epoch [12662/20000], Loss: -17.173904418945312, Learning Rate: 0.01\n",
      "Epoch [12663/20000], Loss: -17.176467895507812, Learning Rate: 0.01\n",
      "Epoch [12664/20000], Loss: -17.17901611328125, Learning Rate: 0.01\n",
      "Epoch [12665/20000], Loss: -17.181533813476562, Learning Rate: 0.01\n",
      "Epoch [12666/20000], Loss: -17.183975219726562, Learning Rate: 0.01\n",
      "Epoch [12667/20000], Loss: -17.1865234375, Learning Rate: 0.01\n",
      "Epoch [12668/20000], Loss: -17.18902587890625, Learning Rate: 0.01\n",
      "Epoch [12669/20000], Loss: -17.191604614257812, Learning Rate: 0.01\n",
      "Epoch [12670/20000], Loss: -17.194168090820312, Learning Rate: 0.01\n",
      "Epoch [12671/20000], Loss: -17.19677734375, Learning Rate: 0.01\n",
      "Epoch [12672/20000], Loss: -17.19927978515625, Learning Rate: 0.01\n",
      "Epoch [12673/20000], Loss: -17.201751708984375, Learning Rate: 0.01\n",
      "Epoch [12674/20000], Loss: -17.20416259765625, Learning Rate: 0.01\n",
      "Epoch [12675/20000], Loss: -17.206817626953125, Learning Rate: 0.01\n",
      "Epoch [12676/20000], Loss: -17.20928955078125, Learning Rate: 0.01\n",
      "Epoch [12677/20000], Loss: -17.211746215820312, Learning Rate: 0.01\n",
      "Epoch [12678/20000], Loss: -17.214309692382812, Learning Rate: 0.01\n",
      "Epoch [12679/20000], Loss: -17.216964721679688, Learning Rate: 0.01\n",
      "Epoch [12680/20000], Loss: -17.219436645507812, Learning Rate: 0.01\n",
      "Epoch [12681/20000], Loss: -17.221893310546875, Learning Rate: 0.01\n",
      "Epoch [12682/20000], Loss: -17.224502563476562, Learning Rate: 0.01\n",
      "Epoch [12683/20000], Loss: -17.227005004882812, Learning Rate: 0.01\n",
      "Epoch [12684/20000], Loss: -17.229400634765625, Learning Rate: 0.01\n",
      "Epoch [12685/20000], Loss: -17.231918334960938, Learning Rate: 0.01\n",
      "Epoch [12686/20000], Loss: -17.234451293945312, Learning Rate: 0.01\n",
      "Epoch [12687/20000], Loss: -17.2369384765625, Learning Rate: 0.01\n",
      "Epoch [12688/20000], Loss: -17.239334106445312, Learning Rate: 0.01\n",
      "Epoch [12689/20000], Loss: -17.241836547851562, Learning Rate: 0.01\n",
      "Epoch [12690/20000], Loss: -17.244369506835938, Learning Rate: 0.01\n",
      "Epoch [12691/20000], Loss: -17.246871948242188, Learning Rate: 0.01\n",
      "Epoch [12692/20000], Loss: -17.249237060546875, Learning Rate: 0.01\n",
      "Epoch [12693/20000], Loss: -17.251617431640625, Learning Rate: 0.01\n",
      "Epoch [12694/20000], Loss: -17.253936767578125, Learning Rate: 0.01\n",
      "Epoch [12695/20000], Loss: -17.256027221679688, Learning Rate: 0.01\n",
      "Epoch [12696/20000], Loss: -17.258285522460938, Learning Rate: 0.01\n",
      "Epoch [12697/20000], Loss: -17.260284423828125, Learning Rate: 0.01\n",
      "Epoch [12698/20000], Loss: -17.262161254882812, Learning Rate: 0.01\n",
      "Epoch [12699/20000], Loss: -17.263778686523438, Learning Rate: 0.01\n",
      "Epoch [12700/20000], Loss: -17.265151977539062, Learning Rate: 0.01\n",
      "Epoch [12701/20000], Loss: -17.266006469726562, Learning Rate: 0.01\n",
      "Epoch [12702/20000], Loss: -17.266326904296875, Learning Rate: 0.01\n",
      "Epoch [12703/20000], Loss: -17.265762329101562, Learning Rate: 0.01\n",
      "Epoch [12704/20000], Loss: -17.264068603515625, Learning Rate: 0.01\n",
      "Epoch [12705/20000], Loss: -17.260665893554688, Learning Rate: 0.01\n",
      "Epoch [12706/20000], Loss: -17.255157470703125, Learning Rate: 0.01\n",
      "Epoch [12707/20000], Loss: -17.246383666992188, Learning Rate: 0.01\n",
      "Epoch [12708/20000], Loss: -17.233383178710938, Learning Rate: 0.01\n",
      "Epoch [12709/20000], Loss: -17.21435546875, Learning Rate: 0.01\n",
      "Epoch [12710/20000], Loss: -17.186965942382812, Learning Rate: 0.01\n",
      "Epoch [12711/20000], Loss: -17.148330688476562, Learning Rate: 0.01\n",
      "Epoch [12712/20000], Loss: -17.094253540039062, Learning Rate: 0.01\n",
      "Epoch [12713/20000], Loss: -17.01995849609375, Learning Rate: 0.01\n",
      "Epoch [12714/20000], Loss: -16.9200439453125, Learning Rate: 0.01\n",
      "Epoch [12715/20000], Loss: -16.7890625, Learning Rate: 0.01\n",
      "Epoch [12716/20000], Loss: -16.624710083007812, Learning Rate: 0.01\n",
      "Epoch [12717/20000], Loss: -16.42974853515625, Learning Rate: 0.01\n",
      "Epoch [12718/20000], Loss: -16.220489501953125, Learning Rate: 0.01\n",
      "Epoch [12719/20000], Loss: -16.026565551757812, Learning Rate: 0.01\n",
      "Epoch [12720/20000], Loss: -15.900405883789062, Learning Rate: 0.01\n",
      "Epoch [12721/20000], Loss: -15.896255493164062, Learning Rate: 0.01\n",
      "Epoch [12722/20000], Loss: -16.055282592773438, Learning Rate: 0.01\n",
      "Epoch [12723/20000], Loss: -16.362945556640625, Learning Rate: 0.01\n",
      "Epoch [12724/20000], Loss: -16.74560546875, Learning Rate: 0.01\n",
      "Epoch [12725/20000], Loss: -17.088287353515625, Learning Rate: 0.01\n",
      "Epoch [12726/20000], Loss: -17.294326782226562, Learning Rate: 0.01\n",
      "Epoch [12727/20000], Loss: -17.3284912109375, Learning Rate: 0.01\n",
      "Epoch [12728/20000], Loss: -17.226028442382812, Learning Rate: 0.01\n",
      "Epoch [12729/20000], Loss: -17.066986083984375, Learning Rate: 0.01\n",
      "Epoch [12730/20000], Loss: -16.937957763671875, Learning Rate: 0.01\n",
      "Epoch [12731/20000], Loss: -16.8995361328125, Learning Rate: 0.01\n",
      "Epoch [12732/20000], Loss: -16.965042114257812, Learning Rate: 0.01\n",
      "Epoch [12733/20000], Loss: -17.100509643554688, Learning Rate: 0.01\n",
      "Epoch [12734/20000], Loss: -17.244110107421875, Learning Rate: 0.01\n",
      "Epoch [12735/20000], Loss: -17.338836669921875, Learning Rate: 0.01\n",
      "Epoch [12736/20000], Loss: -17.359130859375, Learning Rate: 0.01\n",
      "Epoch [12737/20000], Loss: -17.317779541015625, Learning Rate: 0.01\n",
      "Epoch [12738/20000], Loss: -17.252410888671875, Learning Rate: 0.01\n",
      "Epoch [12739/20000], Loss: -17.20428466796875, Learning Rate: 0.01\n",
      "Epoch [12740/20000], Loss: -17.198455810546875, Learning Rate: 0.01\n",
      "Epoch [12741/20000], Loss: -17.235580444335938, Learning Rate: 0.01\n",
      "Epoch [12742/20000], Loss: -17.295867919921875, Learning Rate: 0.01\n",
      "Epoch [12743/20000], Loss: -17.351287841796875, Learning Rate: 0.01\n",
      "Epoch [12744/20000], Loss: -17.381011962890625, Learning Rate: 0.01\n",
      "Epoch [12745/20000], Loss: -17.380172729492188, Learning Rate: 0.01\n",
      "Epoch [12746/20000], Loss: -17.358779907226562, Learning Rate: 0.01\n",
      "Epoch [12747/20000], Loss: -17.3341064453125, Learning Rate: 0.01\n",
      "Epoch [12748/20000], Loss: -17.32171630859375, Learning Rate: 0.01\n",
      "Epoch [12749/20000], Loss: -17.328033447265625, Learning Rate: 0.01\n",
      "Epoch [12750/20000], Loss: -17.349563598632812, Learning Rate: 0.01\n",
      "Epoch [12751/20000], Loss: -17.376007080078125, Learning Rate: 0.01\n",
      "Epoch [12752/20000], Loss: -17.396682739257812, Learning Rate: 0.01\n",
      "Epoch [12753/20000], Loss: -17.405593872070312, Learning Rate: 0.01\n",
      "Epoch [12754/20000], Loss: -17.402984619140625, Learning Rate: 0.01\n",
      "Epoch [12755/20000], Loss: -17.394561767578125, Learning Rate: 0.01\n",
      "Epoch [12756/20000], Loss: -17.3870849609375, Learning Rate: 0.01\n",
      "Epoch [12757/20000], Loss: -17.385787963867188, Learning Rate: 0.01\n",
      "Epoch [12758/20000], Loss: -17.3919677734375, Learning Rate: 0.01\n",
      "Epoch [12759/20000], Loss: -17.403152465820312, Learning Rate: 0.01\n",
      "Epoch [12760/20000], Loss: -17.414962768554688, Learning Rate: 0.01\n",
      "Epoch [12761/20000], Loss: -17.423675537109375, Learning Rate: 0.01\n",
      "Epoch [12762/20000], Loss: -17.427627563476562, Learning Rate: 0.01\n",
      "Epoch [12763/20000], Loss: -17.427413940429688, Learning Rate: 0.01\n",
      "Epoch [12764/20000], Loss: -17.425430297851562, Learning Rate: 0.01\n",
      "Epoch [12765/20000], Loss: -17.424362182617188, Learning Rate: 0.01\n",
      "Epoch [12766/20000], Loss: -17.4256591796875, Learning Rate: 0.01\n",
      "Epoch [12767/20000], Loss: -17.429840087890625, Learning Rate: 0.01\n",
      "Epoch [12768/20000], Loss: -17.435806274414062, Learning Rate: 0.01\n",
      "Epoch [12769/20000], Loss: -17.441787719726562, Learning Rate: 0.01\n",
      "Epoch [12770/20000], Loss: -17.446701049804688, Learning Rate: 0.01\n",
      "Epoch [12771/20000], Loss: -17.44976806640625, Learning Rate: 0.01\n",
      "Epoch [12772/20000], Loss: -17.451202392578125, Learning Rate: 0.01\n",
      "Epoch [12773/20000], Loss: -17.451980590820312, Learning Rate: 0.01\n",
      "Epoch [12774/20000], Loss: -17.452926635742188, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12775/20000], Loss: -17.45465087890625, Learning Rate: 0.01\n",
      "Epoch [12776/20000], Loss: -17.45751953125, Learning Rate: 0.01\n",
      "Epoch [12777/20000], Loss: -17.461196899414062, Learning Rate: 0.01\n",
      "Epoch [12778/20000], Loss: -17.464813232421875, Learning Rate: 0.01\n",
      "Epoch [12779/20000], Loss: -17.468170166015625, Learning Rate: 0.01\n",
      "Epoch [12780/20000], Loss: -17.470870971679688, Learning Rate: 0.01\n",
      "Epoch [12781/20000], Loss: -17.4727783203125, Learning Rate: 0.01\n",
      "Epoch [12782/20000], Loss: -17.474319458007812, Learning Rate: 0.01\n",
      "Epoch [12783/20000], Loss: -17.47564697265625, Learning Rate: 0.01\n",
      "Epoch [12784/20000], Loss: -17.477188110351562, Learning Rate: 0.01\n",
      "Epoch [12785/20000], Loss: -17.478851318359375, Learning Rate: 0.01\n",
      "Epoch [12786/20000], Loss: -17.48065185546875, Learning Rate: 0.01\n",
      "Epoch [12787/20000], Loss: -17.48248291015625, Learning Rate: 0.01\n",
      "Epoch [12788/20000], Loss: -17.483795166015625, Learning Rate: 0.01\n",
      "Epoch [12789/20000], Loss: -17.484481811523438, Learning Rate: 0.01\n",
      "Epoch [12790/20000], Loss: -17.484405517578125, Learning Rate: 0.01\n",
      "Epoch [12791/20000], Loss: -17.483169555664062, Learning Rate: 0.01\n",
      "Epoch [12792/20000], Loss: -17.481048583984375, Learning Rate: 0.01\n",
      "Epoch [12793/20000], Loss: -17.477401733398438, Learning Rate: 0.01\n",
      "Epoch [12794/20000], Loss: -17.472091674804688, Learning Rate: 0.01\n",
      "Epoch [12795/20000], Loss: -17.46441650390625, Learning Rate: 0.01\n",
      "Epoch [12796/20000], Loss: -17.453720092773438, Learning Rate: 0.01\n",
      "Epoch [12797/20000], Loss: -17.43890380859375, Learning Rate: 0.01\n",
      "Epoch [12798/20000], Loss: -17.41851806640625, Learning Rate: 0.01\n",
      "Epoch [12799/20000], Loss: -17.39056396484375, Learning Rate: 0.01\n",
      "Epoch [12800/20000], Loss: -17.353240966796875, Learning Rate: 0.01\n",
      "Epoch [12801/20000], Loss: -17.303298950195312, Learning Rate: 0.01\n",
      "Epoch [12802/20000], Loss: -17.23779296875, Learning Rate: 0.01\n",
      "Epoch [12803/20000], Loss: -17.152603149414062, Learning Rate: 0.01\n",
      "Epoch [12804/20000], Loss: -17.044418334960938, Learning Rate: 0.01\n",
      "Epoch [12805/20000], Loss: -16.909332275390625, Learning Rate: 0.01\n",
      "Epoch [12806/20000], Loss: -16.74774169921875, Learning Rate: 0.01\n",
      "Epoch [12807/20000], Loss: -16.561660766601562, Learning Rate: 0.01\n",
      "Epoch [12808/20000], Loss: -16.36505126953125, Learning Rate: 0.01\n",
      "Epoch [12809/20000], Loss: -16.1766357421875, Learning Rate: 0.01\n",
      "Epoch [12810/20000], Loss: -16.03363037109375, Learning Rate: 0.01\n",
      "Epoch [12811/20000], Loss: -15.972305297851562, Learning Rate: 0.01\n",
      "Epoch [12812/20000], Loss: -16.033905029296875, Learning Rate: 0.01\n",
      "Epoch [12813/20000], Loss: -16.228286743164062, Learning Rate: 0.01\n",
      "Epoch [12814/20000], Loss: -16.537750244140625, Learning Rate: 0.01\n",
      "Epoch [12815/20000], Loss: -16.898101806640625, Learning Rate: 0.01\n",
      "Epoch [12816/20000], Loss: -17.230499267578125, Learning Rate: 0.01\n",
      "Epoch [12817/20000], Loss: -17.463180541992188, Learning Rate: 0.01\n",
      "Epoch [12818/20000], Loss: -17.5618896484375, Learning Rate: 0.01\n",
      "Epoch [12819/20000], Loss: -17.535903930664062, Learning Rate: 0.01\n",
      "Epoch [12820/20000], Loss: -17.428329467773438, Learning Rate: 0.01\n",
      "Epoch [12821/20000], Loss: -17.297439575195312, Learning Rate: 0.01\n",
      "Epoch [12822/20000], Loss: -17.196334838867188, Learning Rate: 0.01\n",
      "Epoch [12823/20000], Loss: -17.16033935546875, Learning Rate: 0.01\n",
      "Epoch [12824/20000], Loss: -17.197616577148438, Learning Rate: 0.01\n",
      "Epoch [12825/20000], Loss: -17.291748046875, Learning Rate: 0.01\n",
      "Epoch [12826/20000], Loss: -17.408920288085938, Learning Rate: 0.01\n",
      "Epoch [12827/20000], Loss: -17.512649536132812, Learning Rate: 0.01\n",
      "Epoch [12828/20000], Loss: -17.57672119140625, Learning Rate: 0.01\n",
      "Epoch [12829/20000], Loss: -17.592514038085938, Learning Rate: 0.01\n",
      "Epoch [12830/20000], Loss: -17.56903076171875, Learning Rate: 0.01\n",
      "Epoch [12831/20000], Loss: -17.526229858398438, Learning Rate: 0.01\n",
      "Epoch [12832/20000], Loss: -17.486358642578125, Learning Rate: 0.01\n",
      "Epoch [12833/20000], Loss: -17.465728759765625, Learning Rate: 0.01\n",
      "Epoch [12834/20000], Loss: -17.471603393554688, Learning Rate: 0.01\n",
      "Epoch [12835/20000], Loss: -17.500045776367188, Learning Rate: 0.01\n",
      "Epoch [12836/20000], Loss: -17.54058837890625, Learning Rate: 0.01\n",
      "Epoch [12837/20000], Loss: -17.580062866210938, Learning Rate: 0.01\n",
      "Epoch [12838/20000], Loss: -17.608154296875, Learning Rate: 0.01\n",
      "Epoch [12839/20000], Loss: -17.619720458984375, Learning Rate: 0.01\n",
      "Epoch [12840/20000], Loss: -17.61669921875, Learning Rate: 0.01\n",
      "Epoch [12841/20000], Loss: -17.604736328125, Learning Rate: 0.01\n",
      "Epoch [12842/20000], Loss: -17.591552734375, Learning Rate: 0.01\n",
      "Epoch [12843/20000], Loss: -17.58331298828125, Learning Rate: 0.01\n",
      "Epoch [12844/20000], Loss: -17.583740234375, Learning Rate: 0.01\n",
      "Epoch [12845/20000], Loss: -17.592559814453125, Learning Rate: 0.01\n",
      "Epoch [12846/20000], Loss: -17.606979370117188, Learning Rate: 0.01\n",
      "Epoch [12847/20000], Loss: -17.622756958007812, Learning Rate: 0.01\n",
      "Epoch [12848/20000], Loss: -17.635879516601562, Learning Rate: 0.01\n",
      "Epoch [12849/20000], Loss: -17.6441650390625, Learning Rate: 0.01\n",
      "Epoch [12850/20000], Loss: -17.646957397460938, Learning Rate: 0.01\n",
      "Epoch [12851/20000], Loss: -17.645797729492188, Learning Rate: 0.01\n",
      "Epoch [12852/20000], Loss: -17.642822265625, Learning Rate: 0.01\n",
      "Epoch [12853/20000], Loss: -17.640411376953125, Learning Rate: 0.01\n",
      "Epoch [12854/20000], Loss: -17.640350341796875, Learning Rate: 0.01\n",
      "Epoch [12855/20000], Loss: -17.643112182617188, Learning Rate: 0.01\n",
      "Epoch [12856/20000], Loss: -17.648422241210938, Learning Rate: 0.01\n",
      "Epoch [12857/20000], Loss: -17.654953002929688, Learning Rate: 0.01\n",
      "Epoch [12858/20000], Loss: -17.661727905273438, Learning Rate: 0.01\n",
      "Epoch [12859/20000], Loss: -17.667388916015625, Learning Rate: 0.01\n",
      "Epoch [12860/20000], Loss: -17.671417236328125, Learning Rate: 0.01\n",
      "Epoch [12861/20000], Loss: -17.67388916015625, Learning Rate: 0.01\n",
      "Epoch [12862/20000], Loss: -17.675033569335938, Learning Rate: 0.01\n",
      "Epoch [12863/20000], Loss: -17.675857543945312, Learning Rate: 0.01\n",
      "Epoch [12864/20000], Loss: -17.67669677734375, Learning Rate: 0.01\n",
      "Epoch [12865/20000], Loss: -17.678329467773438, Learning Rate: 0.01\n",
      "Epoch [12866/20000], Loss: -17.6806640625, Learning Rate: 0.01\n",
      "Epoch [12867/20000], Loss: -17.683822631835938, Learning Rate: 0.01\n",
      "Epoch [12868/20000], Loss: -17.687454223632812, Learning Rate: 0.01\n",
      "Epoch [12869/20000], Loss: -17.691192626953125, Learning Rate: 0.01\n",
      "Epoch [12870/20000], Loss: -17.694625854492188, Learning Rate: 0.01\n",
      "Epoch [12871/20000], Loss: -17.697845458984375, Learning Rate: 0.01\n",
      "Epoch [12872/20000], Loss: -17.70050048828125, Learning Rate: 0.01\n",
      "Epoch [12873/20000], Loss: -17.702743530273438, Learning Rate: 0.01\n",
      "Epoch [12874/20000], Loss: -17.704757690429688, Learning Rate: 0.01\n",
      "Epoch [12875/20000], Loss: -17.706741333007812, Learning Rate: 0.01\n",
      "Epoch [12876/20000], Loss: -17.708724975585938, Learning Rate: 0.01\n",
      "Epoch [12877/20000], Loss: -17.71099853515625, Learning Rate: 0.01\n",
      "Epoch [12878/20000], Loss: -17.713363647460938, Learning Rate: 0.01\n",
      "Epoch [12879/20000], Loss: -17.7159423828125, Learning Rate: 0.01\n",
      "Epoch [12880/20000], Loss: -17.718719482421875, Learning Rate: 0.01\n",
      "Epoch [12881/20000], Loss: -17.721435546875, Learning Rate: 0.01\n",
      "Epoch [12882/20000], Loss: -17.724212646484375, Learning Rate: 0.01\n",
      "Epoch [12883/20000], Loss: -17.726638793945312, Learning Rate: 0.01\n",
      "Epoch [12884/20000], Loss: -17.729263305664062, Learning Rate: 0.01\n",
      "Epoch [12885/20000], Loss: -17.731658935546875, Learning Rate: 0.01\n",
      "Epoch [12886/20000], Loss: -17.734085083007812, Learning Rate: 0.01\n",
      "Epoch [12887/20000], Loss: -17.736465454101562, Learning Rate: 0.01\n",
      "Epoch [12888/20000], Loss: -17.738739013671875, Learning Rate: 0.01\n",
      "Epoch [12889/20000], Loss: -17.740951538085938, Learning Rate: 0.01\n",
      "Epoch [12890/20000], Loss: -17.743484497070312, Learning Rate: 0.01\n",
      "Epoch [12891/20000], Loss: -17.7457275390625, Learning Rate: 0.01\n",
      "Epoch [12892/20000], Loss: -17.7481689453125, Learning Rate: 0.01\n",
      "Epoch [12893/20000], Loss: -17.750595092773438, Learning Rate: 0.01\n",
      "Epoch [12894/20000], Loss: -17.753219604492188, Learning Rate: 0.01\n",
      "Epoch [12895/20000], Loss: -17.755599975585938, Learning Rate: 0.01\n",
      "Epoch [12896/20000], Loss: -17.758132934570312, Learning Rate: 0.01\n",
      "Epoch [12897/20000], Loss: -17.760650634765625, Learning Rate: 0.01\n",
      "Epoch [12898/20000], Loss: -17.763076782226562, Learning Rate: 0.01\n",
      "Epoch [12899/20000], Loss: -17.765472412109375, Learning Rate: 0.01\n",
      "Epoch [12900/20000], Loss: -17.767868041992188, Learning Rate: 0.01\n",
      "Epoch [12901/20000], Loss: -17.770217895507812, Learning Rate: 0.01\n",
      "Epoch [12902/20000], Loss: -17.772628784179688, Learning Rate: 0.01\n",
      "Epoch [12903/20000], Loss: -17.77496337890625, Learning Rate: 0.01\n",
      "Epoch [12904/20000], Loss: -17.777374267578125, Learning Rate: 0.01\n",
      "Epoch [12905/20000], Loss: -17.779754638671875, Learning Rate: 0.01\n",
      "Epoch [12906/20000], Loss: -17.782211303710938, Learning Rate: 0.01\n",
      "Epoch [12907/20000], Loss: -17.784561157226562, Learning Rate: 0.01\n",
      "Epoch [12908/20000], Loss: -17.787017822265625, Learning Rate: 0.01\n",
      "Epoch [12909/20000], Loss: -17.789413452148438, Learning Rate: 0.01\n",
      "Epoch [12910/20000], Loss: -17.791748046875, Learning Rate: 0.01\n",
      "Epoch [12911/20000], Loss: -17.794326782226562, Learning Rate: 0.01\n",
      "Epoch [12912/20000], Loss: -17.7967529296875, Learning Rate: 0.01\n",
      "Epoch [12913/20000], Loss: -17.799087524414062, Learning Rate: 0.01\n",
      "Epoch [12914/20000], Loss: -17.801467895507812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12915/20000], Loss: -17.803970336914062, Learning Rate: 0.01\n",
      "Epoch [12916/20000], Loss: -17.806365966796875, Learning Rate: 0.01\n",
      "Epoch [12917/20000], Loss: -17.8087158203125, Learning Rate: 0.01\n",
      "Epoch [12918/20000], Loss: -17.811141967773438, Learning Rate: 0.01\n",
      "Epoch [12919/20000], Loss: -17.813568115234375, Learning Rate: 0.01\n",
      "Epoch [12920/20000], Loss: -17.815933227539062, Learning Rate: 0.01\n",
      "Epoch [12921/20000], Loss: -17.818328857421875, Learning Rate: 0.01\n",
      "Epoch [12922/20000], Loss: -17.8206787109375, Learning Rate: 0.01\n",
      "Epoch [12923/20000], Loss: -17.823150634765625, Learning Rate: 0.01\n",
      "Epoch [12924/20000], Loss: -17.825531005859375, Learning Rate: 0.01\n",
      "Epoch [12925/20000], Loss: -17.827972412109375, Learning Rate: 0.01\n",
      "Epoch [12926/20000], Loss: -17.830337524414062, Learning Rate: 0.01\n",
      "Epoch [12927/20000], Loss: -17.832809448242188, Learning Rate: 0.01\n",
      "Epoch [12928/20000], Loss: -17.835174560546875, Learning Rate: 0.01\n",
      "Epoch [12929/20000], Loss: -17.837554931640625, Learning Rate: 0.01\n",
      "Epoch [12930/20000], Loss: -17.84002685546875, Learning Rate: 0.01\n",
      "Epoch [12931/20000], Loss: -17.842483520507812, Learning Rate: 0.01\n",
      "Epoch [12932/20000], Loss: -17.844772338867188, Learning Rate: 0.01\n",
      "Epoch [12933/20000], Loss: -17.84716796875, Learning Rate: 0.01\n",
      "Epoch [12934/20000], Loss: -17.849609375, Learning Rate: 0.01\n",
      "Epoch [12935/20000], Loss: -17.851974487304688, Learning Rate: 0.01\n",
      "Epoch [12936/20000], Loss: -17.854446411132812, Learning Rate: 0.01\n",
      "Epoch [12937/20000], Loss: -17.856903076171875, Learning Rate: 0.01\n",
      "Epoch [12938/20000], Loss: -17.859176635742188, Learning Rate: 0.01\n",
      "Epoch [12939/20000], Loss: -17.861648559570312, Learning Rate: 0.01\n",
      "Epoch [12940/20000], Loss: -17.86407470703125, Learning Rate: 0.01\n",
      "Epoch [12941/20000], Loss: -17.866348266601562, Learning Rate: 0.01\n",
      "Epoch [12942/20000], Loss: -17.868911743164062, Learning Rate: 0.01\n",
      "Epoch [12943/20000], Loss: -17.87127685546875, Learning Rate: 0.01\n",
      "Epoch [12944/20000], Loss: -17.8736572265625, Learning Rate: 0.01\n",
      "Epoch [12945/20000], Loss: -17.876068115234375, Learning Rate: 0.01\n",
      "Epoch [12946/20000], Loss: -17.878555297851562, Learning Rate: 0.01\n",
      "Epoch [12947/20000], Loss: -17.880889892578125, Learning Rate: 0.01\n",
      "Epoch [12948/20000], Loss: -17.883224487304688, Learning Rate: 0.01\n",
      "Epoch [12949/20000], Loss: -17.885726928710938, Learning Rate: 0.01\n",
      "Epoch [12950/20000], Loss: -17.888137817382812, Learning Rate: 0.01\n",
      "Epoch [12951/20000], Loss: -17.890396118164062, Learning Rate: 0.01\n",
      "Epoch [12952/20000], Loss: -17.892959594726562, Learning Rate: 0.01\n",
      "Epoch [12953/20000], Loss: -17.895355224609375, Learning Rate: 0.01\n",
      "Epoch [12954/20000], Loss: -17.897750854492188, Learning Rate: 0.01\n",
      "Epoch [12955/20000], Loss: -17.900146484375, Learning Rate: 0.01\n",
      "Epoch [12956/20000], Loss: -17.90240478515625, Learning Rate: 0.01\n",
      "Epoch [12957/20000], Loss: -17.904861450195312, Learning Rate: 0.01\n",
      "Epoch [12958/20000], Loss: -17.907211303710938, Learning Rate: 0.01\n",
      "Epoch [12959/20000], Loss: -17.909698486328125, Learning Rate: 0.01\n",
      "Epoch [12960/20000], Loss: -17.912155151367188, Learning Rate: 0.01\n",
      "Epoch [12961/20000], Loss: -17.914474487304688, Learning Rate: 0.01\n",
      "Epoch [12962/20000], Loss: -17.917022705078125, Learning Rate: 0.01\n",
      "Epoch [12963/20000], Loss: -17.919296264648438, Learning Rate: 0.01\n",
      "Epoch [12964/20000], Loss: -17.921829223632812, Learning Rate: 0.01\n",
      "Epoch [12965/20000], Loss: -17.92413330078125, Learning Rate: 0.01\n",
      "Epoch [12966/20000], Loss: -17.926544189453125, Learning Rate: 0.01\n",
      "Epoch [12967/20000], Loss: -17.928955078125, Learning Rate: 0.01\n",
      "Epoch [12968/20000], Loss: -17.931503295898438, Learning Rate: 0.01\n",
      "Epoch [12969/20000], Loss: -17.933807373046875, Learning Rate: 0.01\n",
      "Epoch [12970/20000], Loss: -17.936233520507812, Learning Rate: 0.01\n",
      "Epoch [12971/20000], Loss: -17.938583374023438, Learning Rate: 0.01\n",
      "Epoch [12972/20000], Loss: -17.94085693359375, Learning Rate: 0.01\n",
      "Epoch [12973/20000], Loss: -17.943359375, Learning Rate: 0.01\n",
      "Epoch [12974/20000], Loss: -17.94573974609375, Learning Rate: 0.01\n",
      "Epoch [12975/20000], Loss: -17.9481201171875, Learning Rate: 0.01\n",
      "Epoch [12976/20000], Loss: -17.950576782226562, Learning Rate: 0.01\n",
      "Epoch [12977/20000], Loss: -17.953018188476562, Learning Rate: 0.01\n",
      "Epoch [12978/20000], Loss: -17.955413818359375, Learning Rate: 0.01\n",
      "Epoch [12979/20000], Loss: -17.957763671875, Learning Rate: 0.01\n",
      "Epoch [12980/20000], Loss: -17.960205078125, Learning Rate: 0.01\n",
      "Epoch [12981/20000], Loss: -17.962509155273438, Learning Rate: 0.01\n",
      "Epoch [12982/20000], Loss: -17.96490478515625, Learning Rate: 0.01\n",
      "Epoch [12983/20000], Loss: -17.967208862304688, Learning Rate: 0.01\n",
      "Epoch [12984/20000], Loss: -17.969589233398438, Learning Rate: 0.01\n",
      "Epoch [12985/20000], Loss: -17.971893310546875, Learning Rate: 0.01\n",
      "Epoch [12986/20000], Loss: -17.974258422851562, Learning Rate: 0.01\n",
      "Epoch [12987/20000], Loss: -17.976654052734375, Learning Rate: 0.01\n",
      "Epoch [12988/20000], Loss: -17.978836059570312, Learning Rate: 0.01\n",
      "Epoch [12989/20000], Loss: -17.980941772460938, Learning Rate: 0.01\n",
      "Epoch [12990/20000], Loss: -17.98309326171875, Learning Rate: 0.01\n",
      "Epoch [12991/20000], Loss: -17.984970092773438, Learning Rate: 0.01\n",
      "Epoch [12992/20000], Loss: -17.986831665039062, Learning Rate: 0.01\n",
      "Epoch [12993/20000], Loss: -17.988449096679688, Learning Rate: 0.01\n",
      "Epoch [12994/20000], Loss: -17.989761352539062, Learning Rate: 0.01\n",
      "Epoch [12995/20000], Loss: -17.990524291992188, Learning Rate: 0.01\n",
      "Epoch [12996/20000], Loss: -17.9908447265625, Learning Rate: 0.01\n",
      "Epoch [12997/20000], Loss: -17.990188598632812, Learning Rate: 0.01\n",
      "Epoch [12998/20000], Loss: -17.98834228515625, Learning Rate: 0.01\n",
      "Epoch [12999/20000], Loss: -17.984573364257812, Learning Rate: 0.01\n",
      "Epoch [13000/20000], Loss: -17.978485107421875, Learning Rate: 0.01\n",
      "Epoch [13001/20000], Loss: -17.968658447265625, Learning Rate: 0.01\n",
      "Epoch [13002/20000], Loss: -17.953811645507812, Learning Rate: 0.01\n",
      "Epoch [13003/20000], Loss: -17.931564331054688, Learning Rate: 0.01\n",
      "Epoch [13004/20000], Loss: -17.89886474609375, Learning Rate: 0.01\n",
      "Epoch [13005/20000], Loss: -17.851333618164062, Learning Rate: 0.01\n",
      "Epoch [13006/20000], Loss: -17.78338623046875, Learning Rate: 0.01\n",
      "Epoch [13007/20000], Loss: -17.686859130859375, Learning Rate: 0.01\n",
      "Epoch [13008/20000], Loss: -17.55303955078125, Learning Rate: 0.01\n",
      "Epoch [13009/20000], Loss: -17.370651245117188, Learning Rate: 0.01\n",
      "Epoch [13010/20000], Loss: -17.133621215820312, Learning Rate: 0.01\n",
      "Epoch [13011/20000], Loss: -16.838790893554688, Learning Rate: 0.01\n",
      "Epoch [13012/20000], Loss: -16.507553100585938, Learning Rate: 0.01\n",
      "Epoch [13013/20000], Loss: -16.180130004882812, Learning Rate: 0.01\n",
      "Epoch [13014/20000], Loss: -15.945343017578125, Learning Rate: 0.01\n",
      "Epoch [13015/20000], Loss: -15.89398193359375, Learning Rate: 0.01\n",
      "Epoch [13016/20000], Loss: -16.111404418945312, Learning Rate: 0.01\n",
      "Epoch [13017/20000], Loss: -16.572479248046875, Learning Rate: 0.01\n",
      "Epoch [13018/20000], Loss: -17.153656005859375, Learning Rate: 0.01\n",
      "Epoch [13019/20000], Loss: -17.653244018554688, Learning Rate: 0.01\n",
      "Epoch [13020/20000], Loss: -17.916549682617188, Learning Rate: 0.01\n",
      "Epoch [13021/20000], Loss: -17.908203125, Learning Rate: 0.01\n",
      "Epoch [13022/20000], Loss: -17.716827392578125, Learning Rate: 0.01\n",
      "Epoch [13023/20000], Loss: -17.496978759765625, Learning Rate: 0.01\n",
      "Epoch [13024/20000], Loss: -17.387130737304688, Learning Rate: 0.01\n",
      "Epoch [13025/20000], Loss: -17.453338623046875, Learning Rate: 0.01\n",
      "Epoch [13026/20000], Loss: -17.655288696289062, Learning Rate: 0.01\n",
      "Epoch [13027/20000], Loss: -17.884262084960938, Learning Rate: 0.01\n",
      "Epoch [13028/20000], Loss: -18.027511596679688, Learning Rate: 0.01\n",
      "Epoch [13029/20000], Loss: -18.036117553710938, Learning Rate: 0.01\n",
      "Epoch [13030/20000], Loss: -17.942474365234375, Learning Rate: 0.01\n",
      "Epoch [13031/20000], Loss: -17.827987670898438, Learning Rate: 0.01\n",
      "Epoch [13032/20000], Loss: -17.772201538085938, Learning Rate: 0.01\n",
      "Epoch [13033/20000], Loss: -17.808135986328125, Learning Rate: 0.01\n",
      "Epoch [13034/20000], Loss: -17.912948608398438, Learning Rate: 0.01\n",
      "Epoch [13035/20000], Loss: -18.025741577148438, Learning Rate: 0.01\n",
      "Epoch [13036/20000], Loss: -18.09136962890625, Learning Rate: 0.01\n",
      "Epoch [13037/20000], Loss: -18.089828491210938, Learning Rate: 0.01\n",
      "Epoch [13038/20000], Loss: -18.04254150390625, Learning Rate: 0.01\n",
      "Epoch [13039/20000], Loss: -17.991317749023438, Learning Rate: 0.01\n",
      "Epoch [13040/20000], Loss: -17.971633911132812, Learning Rate: 0.01\n",
      "Epoch [13041/20000], Loss: -17.99395751953125, Learning Rate: 0.01\n",
      "Epoch [13042/20000], Loss: -18.042129516601562, Learning Rate: 0.01\n",
      "Epoch [13043/20000], Loss: -18.088058471679688, Learning Rate: 0.01\n",
      "Epoch [13044/20000], Loss: -18.110198974609375, Learning Rate: 0.01\n",
      "Epoch [13045/20000], Loss: -18.104904174804688, Learning Rate: 0.01\n",
      "Epoch [13046/20000], Loss: -18.084548950195312, Learning Rate: 0.01\n",
      "Epoch [13047/20000], Loss: -18.067947387695312, Learning Rate: 0.01\n",
      "Epoch [13048/20000], Loss: -18.0677490234375, Learning Rate: 0.01\n",
      "Epoch [13049/20000], Loss: -18.084381103515625, Learning Rate: 0.01\n",
      "Epoch [13050/20000], Loss: -18.108413696289062, Learning Rate: 0.01\n",
      "Epoch [13051/20000], Loss: -18.127792358398438, Learning Rate: 0.01\n",
      "Epoch [13052/20000], Loss: -18.134796142578125, Learning Rate: 0.01\n",
      "Epoch [13053/20000], Loss: -18.130279541015625, Learning Rate: 0.01\n",
      "Epoch [13054/20000], Loss: -18.121017456054688, Learning Rate: 0.01\n",
      "Epoch [13055/20000], Loss: -18.115386962890625, Learning Rate: 0.01\n",
      "Epoch [13056/20000], Loss: -18.117752075195312, Learning Rate: 0.01\n",
      "Epoch [13057/20000], Loss: -18.127349853515625, Learning Rate: 0.01\n",
      "Epoch [13058/20000], Loss: -18.139633178710938, Learning Rate: 0.01\n",
      "Epoch [13059/20000], Loss: -18.14947509765625, Learning Rate: 0.01\n",
      "Epoch [13060/20000], Loss: -18.15380859375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13061/20000], Loss: -18.1533203125, Learning Rate: 0.01\n",
      "Epoch [13062/20000], Loss: -18.150970458984375, Learning Rate: 0.01\n",
      "Epoch [13063/20000], Loss: -18.1500244140625, Learning Rate: 0.01\n",
      "Epoch [13064/20000], Loss: -18.15240478515625, Learning Rate: 0.01\n",
      "Epoch [13065/20000], Loss: -18.157669067382812, Learning Rate: 0.01\n",
      "Epoch [13066/20000], Loss: -18.164031982421875, Learning Rate: 0.01\n",
      "Epoch [13067/20000], Loss: -18.16937255859375, Learning Rate: 0.01\n",
      "Epoch [13068/20000], Loss: -18.172393798828125, Learning Rate: 0.01\n",
      "Epoch [13069/20000], Loss: -18.173492431640625, Learning Rate: 0.01\n",
      "Epoch [13070/20000], Loss: -18.173919677734375, Learning Rate: 0.01\n",
      "Epoch [13071/20000], Loss: -18.17474365234375, Learning Rate: 0.01\n",
      "Epoch [13072/20000], Loss: -18.17694091796875, Learning Rate: 0.01\n",
      "Epoch [13073/20000], Loss: -18.180496215820312, Learning Rate: 0.01\n",
      "Epoch [13074/20000], Loss: -18.184646606445312, Learning Rate: 0.01\n",
      "Epoch [13075/20000], Loss: -18.18841552734375, Learning Rate: 0.01\n",
      "Epoch [13076/20000], Loss: -18.191390991210938, Learning Rate: 0.01\n",
      "Epoch [13077/20000], Loss: -18.1934814453125, Learning Rate: 0.01\n",
      "Epoch [13078/20000], Loss: -18.195068359375, Learning Rate: 0.01\n",
      "Epoch [13079/20000], Loss: -18.19659423828125, Learning Rate: 0.01\n",
      "Epoch [13080/20000], Loss: -18.198577880859375, Learning Rate: 0.01\n",
      "Epoch [13081/20000], Loss: -18.2010498046875, Learning Rate: 0.01\n",
      "Epoch [13082/20000], Loss: -18.204071044921875, Learning Rate: 0.01\n",
      "Epoch [13083/20000], Loss: -18.207015991210938, Learning Rate: 0.01\n",
      "Epoch [13084/20000], Loss: -18.209854125976562, Learning Rate: 0.01\n",
      "Epoch [13085/20000], Loss: -18.212188720703125, Learning Rate: 0.01\n",
      "Epoch [13086/20000], Loss: -18.214309692382812, Learning Rate: 0.01\n",
      "Epoch [13087/20000], Loss: -18.2161865234375, Learning Rate: 0.01\n",
      "Epoch [13088/20000], Loss: -18.218399047851562, Learning Rate: 0.01\n",
      "Epoch [13089/20000], Loss: -18.220626831054688, Learning Rate: 0.01\n",
      "Epoch [13090/20000], Loss: -18.223159790039062, Learning Rate: 0.01\n",
      "Epoch [13091/20000], Loss: -18.225799560546875, Learning Rate: 0.01\n",
      "Epoch [13092/20000], Loss: -18.228363037109375, Learning Rate: 0.01\n",
      "Epoch [13093/20000], Loss: -18.230819702148438, Learning Rate: 0.01\n",
      "Epoch [13094/20000], Loss: -18.233123779296875, Learning Rate: 0.01\n",
      "Epoch [13095/20000], Loss: -18.235427856445312, Learning Rate: 0.01\n",
      "Epoch [13096/20000], Loss: -18.237533569335938, Learning Rate: 0.01\n",
      "Epoch [13097/20000], Loss: -18.23968505859375, Learning Rate: 0.01\n",
      "Epoch [13098/20000], Loss: -18.242019653320312, Learning Rate: 0.01\n",
      "Epoch [13099/20000], Loss: -18.24432373046875, Learning Rate: 0.01\n",
      "Epoch [13100/20000], Loss: -18.24676513671875, Learning Rate: 0.01\n",
      "Epoch [13101/20000], Loss: -18.24920654296875, Learning Rate: 0.01\n",
      "Epoch [13102/20000], Loss: -18.251602172851562, Learning Rate: 0.01\n",
      "Epoch [13103/20000], Loss: -18.253921508789062, Learning Rate: 0.01\n",
      "Epoch [13104/20000], Loss: -18.256240844726562, Learning Rate: 0.01\n",
      "Epoch [13105/20000], Loss: -18.258377075195312, Learning Rate: 0.01\n",
      "Epoch [13106/20000], Loss: -18.260696411132812, Learning Rate: 0.01\n",
      "Epoch [13107/20000], Loss: -18.263015747070312, Learning Rate: 0.01\n",
      "Epoch [13108/20000], Loss: -18.26531982421875, Learning Rate: 0.01\n",
      "Epoch [13109/20000], Loss: -18.267684936523438, Learning Rate: 0.01\n",
      "Epoch [13110/20000], Loss: -18.270111083984375, Learning Rate: 0.01\n",
      "Epoch [13111/20000], Loss: -18.27239990234375, Learning Rate: 0.01\n",
      "Epoch [13112/20000], Loss: -18.274688720703125, Learning Rate: 0.01\n",
      "Epoch [13113/20000], Loss: -18.27691650390625, Learning Rate: 0.01\n",
      "Epoch [13114/20000], Loss: -18.279205322265625, Learning Rate: 0.01\n",
      "Epoch [13115/20000], Loss: -18.281509399414062, Learning Rate: 0.01\n",
      "Epoch [13116/20000], Loss: -18.283950805664062, Learning Rate: 0.01\n",
      "Epoch [13117/20000], Loss: -18.2861328125, Learning Rate: 0.01\n",
      "Epoch [13118/20000], Loss: -18.2884521484375, Learning Rate: 0.01\n",
      "Epoch [13119/20000], Loss: -18.290817260742188, Learning Rate: 0.01\n",
      "Epoch [13120/20000], Loss: -18.293106079101562, Learning Rate: 0.01\n",
      "Epoch [13121/20000], Loss: -18.295394897460938, Learning Rate: 0.01\n",
      "Epoch [13122/20000], Loss: -18.297683715820312, Learning Rate: 0.01\n",
      "Epoch [13123/20000], Loss: -18.300048828125, Learning Rate: 0.01\n",
      "Epoch [13124/20000], Loss: -18.302383422851562, Learning Rate: 0.01\n",
      "Epoch [13125/20000], Loss: -18.304611206054688, Learning Rate: 0.01\n",
      "Epoch [13126/20000], Loss: -18.306976318359375, Learning Rate: 0.01\n",
      "Epoch [13127/20000], Loss: -18.309249877929688, Learning Rate: 0.01\n",
      "Epoch [13128/20000], Loss: -18.311538696289062, Learning Rate: 0.01\n",
      "Epoch [13129/20000], Loss: -18.313766479492188, Learning Rate: 0.01\n",
      "Epoch [13130/20000], Loss: -18.316162109375, Learning Rate: 0.01\n",
      "Epoch [13131/20000], Loss: -18.318466186523438, Learning Rate: 0.01\n",
      "Epoch [13132/20000], Loss: -18.320724487304688, Learning Rate: 0.01\n",
      "Epoch [13133/20000], Loss: -18.323089599609375, Learning Rate: 0.01\n",
      "Epoch [13134/20000], Loss: -18.325347900390625, Learning Rate: 0.01\n",
      "Epoch [13135/20000], Loss: -18.327651977539062, Learning Rate: 0.01\n",
      "Epoch [13136/20000], Loss: -18.33001708984375, Learning Rate: 0.01\n",
      "Epoch [13137/20000], Loss: -18.332260131835938, Learning Rate: 0.01\n",
      "Epoch [13138/20000], Loss: -18.334625244140625, Learning Rate: 0.01\n",
      "Epoch [13139/20000], Loss: -18.33685302734375, Learning Rate: 0.01\n",
      "Epoch [13140/20000], Loss: -18.339279174804688, Learning Rate: 0.01\n",
      "Epoch [13141/20000], Loss: -18.341583251953125, Learning Rate: 0.01\n",
      "Epoch [13142/20000], Loss: -18.343734741210938, Learning Rate: 0.01\n",
      "Epoch [13143/20000], Loss: -18.346099853515625, Learning Rate: 0.01\n",
      "Epoch [13144/20000], Loss: -18.348419189453125, Learning Rate: 0.01\n",
      "Epoch [13145/20000], Loss: -18.3507080078125, Learning Rate: 0.01\n",
      "Epoch [13146/20000], Loss: -18.353118896484375, Learning Rate: 0.01\n",
      "Epoch [13147/20000], Loss: -18.355194091796875, Learning Rate: 0.01\n",
      "Epoch [13148/20000], Loss: -18.3575439453125, Learning Rate: 0.01\n",
      "Epoch [13149/20000], Loss: -18.359832763671875, Learning Rate: 0.01\n",
      "Epoch [13150/20000], Loss: -18.362167358398438, Learning Rate: 0.01\n",
      "Epoch [13151/20000], Loss: -18.364471435546875, Learning Rate: 0.01\n",
      "Epoch [13152/20000], Loss: -18.366806030273438, Learning Rate: 0.01\n",
      "Epoch [13153/20000], Loss: -18.369140625, Learning Rate: 0.01\n",
      "Epoch [13154/20000], Loss: -18.371429443359375, Learning Rate: 0.01\n",
      "Epoch [13155/20000], Loss: -18.373779296875, Learning Rate: 0.01\n",
      "Epoch [13156/20000], Loss: -18.37603759765625, Learning Rate: 0.01\n",
      "Epoch [13157/20000], Loss: -18.378341674804688, Learning Rate: 0.01\n",
      "Epoch [13158/20000], Loss: -18.380630493164062, Learning Rate: 0.01\n",
      "Epoch [13159/20000], Loss: -18.382919311523438, Learning Rate: 0.01\n",
      "Epoch [13160/20000], Loss: -18.385147094726562, Learning Rate: 0.01\n",
      "Epoch [13161/20000], Loss: -18.387496948242188, Learning Rate: 0.01\n",
      "Epoch [13162/20000], Loss: -18.389755249023438, Learning Rate: 0.01\n",
      "Epoch [13163/20000], Loss: -18.39208984375, Learning Rate: 0.01\n",
      "Epoch [13164/20000], Loss: -18.39434814453125, Learning Rate: 0.01\n",
      "Epoch [13165/20000], Loss: -18.396621704101562, Learning Rate: 0.01\n",
      "Epoch [13166/20000], Loss: -18.398956298828125, Learning Rate: 0.01\n",
      "Epoch [13167/20000], Loss: -18.401229858398438, Learning Rate: 0.01\n",
      "Epoch [13168/20000], Loss: -18.403518676757812, Learning Rate: 0.01\n",
      "Epoch [13169/20000], Loss: -18.405792236328125, Learning Rate: 0.01\n",
      "Epoch [13170/20000], Loss: -18.4080810546875, Learning Rate: 0.01\n",
      "Epoch [13171/20000], Loss: -18.41033935546875, Learning Rate: 0.01\n",
      "Epoch [13172/20000], Loss: -18.412612915039062, Learning Rate: 0.01\n",
      "Epoch [13173/20000], Loss: -18.414825439453125, Learning Rate: 0.01\n",
      "Epoch [13174/20000], Loss: -18.4171142578125, Learning Rate: 0.01\n",
      "Epoch [13175/20000], Loss: -18.41925048828125, Learning Rate: 0.01\n",
      "Epoch [13176/20000], Loss: -18.421493530273438, Learning Rate: 0.01\n",
      "Epoch [13177/20000], Loss: -18.42364501953125, Learning Rate: 0.01\n",
      "Epoch [13178/20000], Loss: -18.42572021484375, Learning Rate: 0.01\n",
      "Epoch [13179/20000], Loss: -18.427688598632812, Learning Rate: 0.01\n",
      "Epoch [13180/20000], Loss: -18.42974853515625, Learning Rate: 0.01\n",
      "Epoch [13181/20000], Loss: -18.431564331054688, Learning Rate: 0.01\n",
      "Epoch [13182/20000], Loss: -18.433135986328125, Learning Rate: 0.01\n",
      "Epoch [13183/20000], Loss: -18.434646606445312, Learning Rate: 0.01\n",
      "Epoch [13184/20000], Loss: -18.43597412109375, Learning Rate: 0.01\n",
      "Epoch [13185/20000], Loss: -18.436935424804688, Learning Rate: 0.01\n",
      "Epoch [13186/20000], Loss: -18.437332153320312, Learning Rate: 0.01\n",
      "Epoch [13187/20000], Loss: -18.43707275390625, Learning Rate: 0.01\n",
      "Epoch [13188/20000], Loss: -18.436050415039062, Learning Rate: 0.01\n",
      "Epoch [13189/20000], Loss: -18.43389892578125, Learning Rate: 0.01\n",
      "Epoch [13190/20000], Loss: -18.430191040039062, Learning Rate: 0.01\n",
      "Epoch [13191/20000], Loss: -18.424301147460938, Learning Rate: 0.01\n",
      "Epoch [13192/20000], Loss: -18.415603637695312, Learning Rate: 0.01\n",
      "Epoch [13193/20000], Loss: -18.402969360351562, Learning Rate: 0.01\n",
      "Epoch [13194/20000], Loss: -18.38507080078125, Learning Rate: 0.01\n",
      "Epoch [13195/20000], Loss: -18.360092163085938, Learning Rate: 0.01\n",
      "Epoch [13196/20000], Loss: -18.325424194335938, Learning Rate: 0.01\n",
      "Epoch [13197/20000], Loss: -18.27850341796875, Learning Rate: 0.01\n",
      "Epoch [13198/20000], Loss: -18.215316772460938, Learning Rate: 0.01\n",
      "Epoch [13199/20000], Loss: -18.132293701171875, Learning Rate: 0.01\n",
      "Epoch [13200/20000], Loss: -18.024948120117188, Learning Rate: 0.01\n",
      "Epoch [13201/20000], Loss: -17.891983032226562, Learning Rate: 0.01\n",
      "Epoch [13202/20000], Loss: -17.733154296875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13203/20000], Loss: -17.558670043945312, Learning Rate: 0.01\n",
      "Epoch [13204/20000], Loss: -17.3837890625, Learning Rate: 0.01\n",
      "Epoch [13205/20000], Loss: -17.243133544921875, Learning Rate: 0.01\n",
      "Epoch [13206/20000], Loss: -17.171340942382812, Learning Rate: 0.01\n",
      "Epoch [13207/20000], Loss: -17.211212158203125, Learning Rate: 0.01\n",
      "Epoch [13208/20000], Loss: -17.373382568359375, Learning Rate: 0.01\n",
      "Epoch [13209/20000], Loss: -17.641143798828125, Learning Rate: 0.01\n",
      "Epoch [13210/20000], Loss: -17.950103759765625, Learning Rate: 0.01\n",
      "Epoch [13211/20000], Loss: -18.22442626953125, Learning Rate: 0.01\n",
      "Epoch [13212/20000], Loss: -18.400222778320312, Learning Rate: 0.01\n",
      "Epoch [13213/20000], Loss: -18.455398559570312, Learning Rate: 0.01\n",
      "Epoch [13214/20000], Loss: -18.410995483398438, Learning Rate: 0.01\n",
      "Epoch [13215/20000], Loss: -18.316238403320312, Learning Rate: 0.01\n",
      "Epoch [13216/20000], Loss: -18.227264404296875, Learning Rate: 0.01\n",
      "Epoch [13217/20000], Loss: -18.185256958007812, Learning Rate: 0.01\n",
      "Epoch [13218/20000], Loss: -18.206024169921875, Learning Rate: 0.01\n",
      "Epoch [13219/20000], Loss: -18.276535034179688, Learning Rate: 0.01\n",
      "Epoch [13220/20000], Loss: -18.365798950195312, Learning Rate: 0.01\n",
      "Epoch [13221/20000], Loss: -18.43951416015625, Learning Rate: 0.01\n",
      "Epoch [13222/20000], Loss: -18.476364135742188, Learning Rate: 0.01\n",
      "Epoch [13223/20000], Loss: -18.474868774414062, Learning Rate: 0.01\n",
      "Epoch [13224/20000], Loss: -18.449752807617188, Learning Rate: 0.01\n",
      "Epoch [13225/20000], Loss: -18.423141479492188, Learning Rate: 0.01\n",
      "Epoch [13226/20000], Loss: -18.4132080078125, Learning Rate: 0.01\n",
      "Epoch [13227/20000], Loss: -18.427047729492188, Learning Rate: 0.01\n",
      "Epoch [13228/20000], Loss: -18.459548950195312, Learning Rate: 0.01\n",
      "Epoch [13229/20000], Loss: -18.497543334960938, Learning Rate: 0.01\n",
      "Epoch [13230/20000], Loss: -18.526824951171875, Learning Rate: 0.01\n",
      "Epoch [13231/20000], Loss: -18.538864135742188, Learning Rate: 0.01\n",
      "Epoch [13232/20000], Loss: -18.534011840820312, Learning Rate: 0.01\n",
      "Epoch [13233/20000], Loss: -18.519546508789062, Learning Rate: 0.01\n",
      "Epoch [13234/20000], Loss: -18.505508422851562, Learning Rate: 0.01\n",
      "Epoch [13235/20000], Loss: -18.500076293945312, Learning Rate: 0.01\n",
      "Epoch [13236/20000], Loss: -18.506576538085938, Learning Rate: 0.01\n",
      "Epoch [13237/20000], Loss: -18.522872924804688, Learning Rate: 0.01\n",
      "Epoch [13238/20000], Loss: -18.5428466796875, Learning Rate: 0.01\n",
      "Epoch [13239/20000], Loss: -18.55987548828125, Learning Rate: 0.01\n",
      "Epoch [13240/20000], Loss: -18.56964111328125, Learning Rate: 0.01\n",
      "Epoch [13241/20000], Loss: -18.571502685546875, Learning Rate: 0.01\n",
      "Epoch [13242/20000], Loss: -18.567703247070312, Learning Rate: 0.01\n",
      "Epoch [13243/20000], Loss: -18.562393188476562, Learning Rate: 0.01\n",
      "Epoch [13244/20000], Loss: -18.559097290039062, Learning Rate: 0.01\n",
      "Epoch [13245/20000], Loss: -18.56005859375, Learning Rate: 0.01\n",
      "Epoch [13246/20000], Loss: -18.565109252929688, Learning Rate: 0.01\n",
      "Epoch [13247/20000], Loss: -18.572769165039062, Learning Rate: 0.01\n",
      "Epoch [13248/20000], Loss: -18.58056640625, Learning Rate: 0.01\n",
      "Epoch [13249/20000], Loss: -18.586593627929688, Learning Rate: 0.01\n",
      "Epoch [13250/20000], Loss: -18.590194702148438, Learning Rate: 0.01\n",
      "Epoch [13251/20000], Loss: -18.591598510742188, Learning Rate: 0.01\n",
      "Epoch [13252/20000], Loss: -18.5921630859375, Learning Rate: 0.01\n",
      "Epoch [13253/20000], Loss: -18.592849731445312, Learning Rate: 0.01\n",
      "Epoch [13254/20000], Loss: -18.594482421875, Learning Rate: 0.01\n",
      "Epoch [13255/20000], Loss: -18.59759521484375, Learning Rate: 0.01\n",
      "Epoch [13256/20000], Loss: -18.601425170898438, Learning Rate: 0.01\n",
      "Epoch [13257/20000], Loss: -18.605514526367188, Learning Rate: 0.01\n",
      "Epoch [13258/20000], Loss: -18.608963012695312, Learning Rate: 0.01\n",
      "Epoch [13259/20000], Loss: -18.611679077148438, Learning Rate: 0.01\n",
      "Epoch [13260/20000], Loss: -18.613662719726562, Learning Rate: 0.01\n",
      "Epoch [13261/20000], Loss: -18.615005493164062, Learning Rate: 0.01\n",
      "Epoch [13262/20000], Loss: -18.616241455078125, Learning Rate: 0.01\n",
      "Epoch [13263/20000], Loss: -18.618011474609375, Learning Rate: 0.01\n",
      "Epoch [13264/20000], Loss: -18.620223999023438, Learning Rate: 0.01\n",
      "Epoch [13265/20000], Loss: -18.623077392578125, Learning Rate: 0.01\n",
      "Epoch [13266/20000], Loss: -18.626068115234375, Learning Rate: 0.01\n",
      "Epoch [13267/20000], Loss: -18.629180908203125, Learning Rate: 0.01\n",
      "Epoch [13268/20000], Loss: -18.632003784179688, Learning Rate: 0.01\n",
      "Epoch [13269/20000], Loss: -18.634552001953125, Learning Rate: 0.01\n",
      "Epoch [13270/20000], Loss: -18.636688232421875, Learning Rate: 0.01\n",
      "Epoch [13271/20000], Loss: -18.6385498046875, Learning Rate: 0.01\n",
      "Epoch [13272/20000], Loss: -18.640548706054688, Learning Rate: 0.01\n",
      "Epoch [13273/20000], Loss: -18.642471313476562, Learning Rate: 0.01\n",
      "Epoch [13274/20000], Loss: -18.64459228515625, Learning Rate: 0.01\n",
      "Epoch [13275/20000], Loss: -18.646835327148438, Learning Rate: 0.01\n",
      "Epoch [13276/20000], Loss: -18.649276733398438, Learning Rate: 0.01\n",
      "Epoch [13277/20000], Loss: -18.651718139648438, Learning Rate: 0.01\n",
      "Epoch [13278/20000], Loss: -18.654098510742188, Learning Rate: 0.01\n",
      "Epoch [13279/20000], Loss: -18.656356811523438, Learning Rate: 0.01\n",
      "Epoch [13280/20000], Loss: -18.658676147460938, Learning Rate: 0.01\n",
      "Epoch [13281/20000], Loss: -18.660812377929688, Learning Rate: 0.01\n",
      "Epoch [13282/20000], Loss: -18.662765502929688, Learning Rate: 0.01\n",
      "Epoch [13283/20000], Loss: -18.664886474609375, Learning Rate: 0.01\n",
      "Epoch [13284/20000], Loss: -18.667098999023438, Learning Rate: 0.01\n",
      "Epoch [13285/20000], Loss: -18.669387817382812, Learning Rate: 0.01\n",
      "Epoch [13286/20000], Loss: -18.671615600585938, Learning Rate: 0.01\n",
      "Epoch [13287/20000], Loss: -18.673934936523438, Learning Rate: 0.01\n",
      "Epoch [13288/20000], Loss: -18.676239013671875, Learning Rate: 0.01\n",
      "Epoch [13289/20000], Loss: -18.67852783203125, Learning Rate: 0.01\n",
      "Epoch [13290/20000], Loss: -18.680801391601562, Learning Rate: 0.01\n",
      "Epoch [13291/20000], Loss: -18.682815551757812, Learning Rate: 0.01\n",
      "Epoch [13292/20000], Loss: -18.685134887695312, Learning Rate: 0.01\n",
      "Epoch [13293/20000], Loss: -18.687210083007812, Learning Rate: 0.01\n",
      "Epoch [13294/20000], Loss: -18.689468383789062, Learning Rate: 0.01\n",
      "Epoch [13295/20000], Loss: -18.691665649414062, Learning Rate: 0.01\n",
      "Epoch [13296/20000], Loss: -18.693832397460938, Learning Rate: 0.01\n",
      "Epoch [13297/20000], Loss: -18.696014404296875, Learning Rate: 0.01\n",
      "Epoch [13298/20000], Loss: -18.6982421875, Learning Rate: 0.01\n",
      "Epoch [13299/20000], Loss: -18.700454711914062, Learning Rate: 0.01\n",
      "Epoch [13300/20000], Loss: -18.70263671875, Learning Rate: 0.01\n",
      "Epoch [13301/20000], Loss: -18.704910278320312, Learning Rate: 0.01\n",
      "Epoch [13302/20000], Loss: -18.70703125, Learning Rate: 0.01\n",
      "Epoch [13303/20000], Loss: -18.709320068359375, Learning Rate: 0.01\n",
      "Epoch [13304/20000], Loss: -18.711456298828125, Learning Rate: 0.01\n",
      "Epoch [13305/20000], Loss: -18.713546752929688, Learning Rate: 0.01\n",
      "Epoch [13306/20000], Loss: -18.715774536132812, Learning Rate: 0.01\n",
      "Epoch [13307/20000], Loss: -18.717971801757812, Learning Rate: 0.01\n",
      "Epoch [13308/20000], Loss: -18.720138549804688, Learning Rate: 0.01\n",
      "Epoch [13309/20000], Loss: -18.7222900390625, Learning Rate: 0.01\n",
      "Epoch [13310/20000], Loss: -18.724563598632812, Learning Rate: 0.01\n",
      "Epoch [13311/20000], Loss: -18.726669311523438, Learning Rate: 0.01\n",
      "Epoch [13312/20000], Loss: -18.728912353515625, Learning Rate: 0.01\n",
      "Epoch [13313/20000], Loss: -18.731124877929688, Learning Rate: 0.01\n",
      "Epoch [13314/20000], Loss: -18.733261108398438, Learning Rate: 0.01\n",
      "Epoch [13315/20000], Loss: -18.735427856445312, Learning Rate: 0.01\n",
      "Epoch [13316/20000], Loss: -18.737579345703125, Learning Rate: 0.01\n",
      "Epoch [13317/20000], Loss: -18.73980712890625, Learning Rate: 0.01\n",
      "Epoch [13318/20000], Loss: -18.741867065429688, Learning Rate: 0.01\n",
      "Epoch [13319/20000], Loss: -18.744125366210938, Learning Rate: 0.01\n",
      "Epoch [13320/20000], Loss: -18.746322631835938, Learning Rate: 0.01\n",
      "Epoch [13321/20000], Loss: -18.748489379882812, Learning Rate: 0.01\n",
      "Epoch [13322/20000], Loss: -18.750625610351562, Learning Rate: 0.01\n",
      "Epoch [13323/20000], Loss: -18.752777099609375, Learning Rate: 0.01\n",
      "Epoch [13324/20000], Loss: -18.75494384765625, Learning Rate: 0.01\n",
      "Epoch [13325/20000], Loss: -18.757064819335938, Learning Rate: 0.01\n",
      "Epoch [13326/20000], Loss: -18.759262084960938, Learning Rate: 0.01\n",
      "Epoch [13327/20000], Loss: -18.7613525390625, Learning Rate: 0.01\n",
      "Epoch [13328/20000], Loss: -18.763504028320312, Learning Rate: 0.01\n",
      "Epoch [13329/20000], Loss: -18.765533447265625, Learning Rate: 0.01\n",
      "Epoch [13330/20000], Loss: -18.767654418945312, Learning Rate: 0.01\n",
      "Epoch [13331/20000], Loss: -18.769546508789062, Learning Rate: 0.01\n",
      "Epoch [13332/20000], Loss: -18.771682739257812, Learning Rate: 0.01\n",
      "Epoch [13333/20000], Loss: -18.773468017578125, Learning Rate: 0.01\n",
      "Epoch [13334/20000], Loss: -18.775344848632812, Learning Rate: 0.01\n",
      "Epoch [13335/20000], Loss: -18.777008056640625, Learning Rate: 0.01\n",
      "Epoch [13336/20000], Loss: -18.778640747070312, Learning Rate: 0.01\n",
      "Epoch [13337/20000], Loss: -18.779998779296875, Learning Rate: 0.01\n",
      "Epoch [13338/20000], Loss: -18.7811279296875, Learning Rate: 0.01\n",
      "Epoch [13339/20000], Loss: -18.781936645507812, Learning Rate: 0.01\n",
      "Epoch [13340/20000], Loss: -18.782257080078125, Learning Rate: 0.01\n",
      "Epoch [13341/20000], Loss: -18.782028198242188, Learning Rate: 0.01\n",
      "Epoch [13342/20000], Loss: -18.78070068359375, Learning Rate: 0.01\n",
      "Epoch [13343/20000], Loss: -18.778411865234375, Learning Rate: 0.01\n",
      "Epoch [13344/20000], Loss: -18.7744140625, Learning Rate: 0.01\n",
      "Epoch [13345/20000], Loss: -18.768295288085938, Learning Rate: 0.01\n",
      "Epoch [13346/20000], Loss: -18.759033203125, Learning Rate: 0.01\n",
      "Epoch [13347/20000], Loss: -18.745712280273438, Learning Rate: 0.01\n",
      "Epoch [13348/20000], Loss: -18.726715087890625, Learning Rate: 0.01\n",
      "Epoch [13349/20000], Loss: -18.700210571289062, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13350/20000], Loss: -18.663482666015625, Learning Rate: 0.01\n",
      "Epoch [13351/20000], Loss: -18.613327026367188, Learning Rate: 0.01\n",
      "Epoch [13352/20000], Loss: -18.545425415039062, Learning Rate: 0.01\n",
      "Epoch [13353/20000], Loss: -18.45611572265625, Learning Rate: 0.01\n",
      "Epoch [13354/20000], Loss: -18.340774536132812, Learning Rate: 0.01\n",
      "Epoch [13355/20000], Loss: -18.198623657226562, Learning Rate: 0.01\n",
      "Epoch [13356/20000], Loss: -18.031417846679688, Learning Rate: 0.01\n",
      "Epoch [13357/20000], Loss: -17.853225708007812, Learning Rate: 0.01\n",
      "Epoch [13358/20000], Loss: -17.685028076171875, Learning Rate: 0.01\n",
      "Epoch [13359/20000], Loss: -17.568649291992188, Learning Rate: 0.01\n",
      "Epoch [13360/20000], Loss: -17.542495727539062, Learning Rate: 0.01\n",
      "Epoch [13361/20000], Loss: -17.644729614257812, Learning Rate: 0.01\n",
      "Epoch [13362/20000], Loss: -17.869827270507812, Learning Rate: 0.01\n",
      "Epoch [13363/20000], Loss: -18.175979614257812, Learning Rate: 0.01\n",
      "Epoch [13364/20000], Loss: -18.480941772460938, Learning Rate: 0.01\n",
      "Epoch [13365/20000], Loss: -18.706451416015625, Learning Rate: 0.01\n",
      "Epoch [13366/20000], Loss: -18.807754516601562, Learning Rate: 0.01\n",
      "Epoch [13367/20000], Loss: -18.789779663085938, Learning Rate: 0.01\n",
      "Epoch [13368/20000], Loss: -18.696975708007812, Learning Rate: 0.01\n",
      "Epoch [13369/20000], Loss: -18.5899658203125, Learning Rate: 0.01\n",
      "Epoch [13370/20000], Loss: -18.521148681640625, Learning Rate: 0.01\n",
      "Epoch [13371/20000], Loss: -18.51763916015625, Learning Rate: 0.01\n",
      "Epoch [13372/20000], Loss: -18.57635498046875, Learning Rate: 0.01\n",
      "Epoch [13373/20000], Loss: -18.668563842773438, Learning Rate: 0.01\n",
      "Epoch [13374/20000], Loss: -18.756790161132812, Learning Rate: 0.01\n",
      "Epoch [13375/20000], Loss: -18.81219482421875, Learning Rate: 0.01\n",
      "Epoch [13376/20000], Loss: -18.82568359375, Learning Rate: 0.01\n",
      "Epoch [13377/20000], Loss: -18.807891845703125, Learning Rate: 0.01\n",
      "Epoch [13378/20000], Loss: -18.779693603515625, Learning Rate: 0.01\n",
      "Epoch [13379/20000], Loss: -18.761795043945312, Learning Rate: 0.01\n",
      "Epoch [13380/20000], Loss: -18.76483154296875, Learning Rate: 0.01\n",
      "Epoch [13381/20000], Loss: -18.787612915039062, Learning Rate: 0.01\n",
      "Epoch [13382/20000], Loss: -18.819198608398438, Learning Rate: 0.01\n",
      "Epoch [13383/20000], Loss: -18.846923828125, Learning Rate: 0.01\n",
      "Epoch [13384/20000], Loss: -18.861709594726562, Learning Rate: 0.01\n",
      "Epoch [13385/20000], Loss: -18.862594604492188, Learning Rate: 0.01\n",
      "Epoch [13386/20000], Loss: -18.854873657226562, Learning Rate: 0.01\n",
      "Epoch [13387/20000], Loss: -18.846969604492188, Learning Rate: 0.01\n",
      "Epoch [13388/20000], Loss: -18.845611572265625, Learning Rate: 0.01\n",
      "Epoch [13389/20000], Loss: -18.85308837890625, Learning Rate: 0.01\n",
      "Epoch [13390/20000], Loss: -18.867401123046875, Learning Rate: 0.01\n",
      "Epoch [13391/20000], Loss: -18.883163452148438, Learning Rate: 0.01\n",
      "Epoch [13392/20000], Loss: -18.895095825195312, Learning Rate: 0.01\n",
      "Epoch [13393/20000], Loss: -18.9005126953125, Learning Rate: 0.01\n",
      "Epoch [13394/20000], Loss: -18.899749755859375, Learning Rate: 0.01\n",
      "Epoch [13395/20000], Loss: -18.895599365234375, Learning Rate: 0.01\n",
      "Epoch [13396/20000], Loss: -18.891754150390625, Learning Rate: 0.01\n",
      "Epoch [13397/20000], Loss: -18.891586303710938, Learning Rate: 0.01\n",
      "Epoch [13398/20000], Loss: -18.895767211914062, Learning Rate: 0.01\n",
      "Epoch [13399/20000], Loss: -18.903457641601562, Learning Rate: 0.01\n",
      "Epoch [13400/20000], Loss: -18.9124755859375, Learning Rate: 0.01\n",
      "Epoch [13401/20000], Loss: -18.920120239257812, Learning Rate: 0.01\n",
      "Epoch [13402/20000], Loss: -18.924972534179688, Learning Rate: 0.01\n",
      "Epoch [13403/20000], Loss: -18.926849365234375, Learning Rate: 0.01\n",
      "Epoch [13404/20000], Loss: -18.926467895507812, Learning Rate: 0.01\n",
      "Epoch [13405/20000], Loss: -18.92535400390625, Learning Rate: 0.01\n",
      "Epoch [13406/20000], Loss: -18.925155639648438, Learning Rate: 0.01\n",
      "Epoch [13407/20000], Loss: -18.92669677734375, Learning Rate: 0.01\n",
      "Epoch [13408/20000], Loss: -18.929977416992188, Learning Rate: 0.01\n",
      "Epoch [13409/20000], Loss: -18.934432983398438, Learning Rate: 0.01\n",
      "Epoch [13410/20000], Loss: -18.939163208007812, Learning Rate: 0.01\n",
      "Epoch [13411/20000], Loss: -18.943405151367188, Learning Rate: 0.01\n",
      "Epoch [13412/20000], Loss: -18.946487426757812, Learning Rate: 0.01\n",
      "Epoch [13413/20000], Loss: -18.948486328125, Learning Rate: 0.01\n",
      "Epoch [13414/20000], Loss: -18.949783325195312, Learning Rate: 0.01\n",
      "Epoch [13415/20000], Loss: -18.950820922851562, Learning Rate: 0.01\n",
      "Epoch [13416/20000], Loss: -18.952102661132812, Learning Rate: 0.01\n",
      "Epoch [13417/20000], Loss: -18.95391845703125, Learning Rate: 0.01\n",
      "Epoch [13418/20000], Loss: -18.956329345703125, Learning Rate: 0.01\n",
      "Epoch [13419/20000], Loss: -18.959030151367188, Learning Rate: 0.01\n",
      "Epoch [13420/20000], Loss: -18.962005615234375, Learning Rate: 0.01\n",
      "Epoch [13421/20000], Loss: -18.96478271484375, Learning Rate: 0.01\n",
      "Epoch [13422/20000], Loss: -18.967239379882812, Learning Rate: 0.01\n",
      "Epoch [13423/20000], Loss: -18.969345092773438, Learning Rate: 0.01\n",
      "Epoch [13424/20000], Loss: -18.971298217773438, Learning Rate: 0.01\n",
      "Epoch [13425/20000], Loss: -18.973114013671875, Learning Rate: 0.01\n",
      "Epoch [13426/20000], Loss: -18.975067138671875, Learning Rate: 0.01\n",
      "Epoch [13427/20000], Loss: -18.977020263671875, Learning Rate: 0.01\n",
      "Epoch [13428/20000], Loss: -18.979171752929688, Learning Rate: 0.01\n",
      "Epoch [13429/20000], Loss: -18.981369018554688, Learning Rate: 0.01\n",
      "Epoch [13430/20000], Loss: -18.983612060546875, Learning Rate: 0.01\n",
      "Epoch [13431/20000], Loss: -18.98602294921875, Learning Rate: 0.01\n",
      "Epoch [13432/20000], Loss: -18.98828125, Learning Rate: 0.01\n",
      "Epoch [13433/20000], Loss: -18.990280151367188, Learning Rate: 0.01\n",
      "Epoch [13434/20000], Loss: -18.992523193359375, Learning Rate: 0.01\n",
      "Epoch [13435/20000], Loss: -18.994430541992188, Learning Rate: 0.01\n",
      "Epoch [13436/20000], Loss: -18.996490478515625, Learning Rate: 0.01\n",
      "Epoch [13437/20000], Loss: -18.99853515625, Learning Rate: 0.01\n",
      "Epoch [13438/20000], Loss: -19.000625610351562, Learning Rate: 0.01\n",
      "Epoch [13439/20000], Loss: -19.002655029296875, Learning Rate: 0.01\n",
      "Epoch [13440/20000], Loss: -19.004867553710938, Learning Rate: 0.01\n",
      "Epoch [13441/20000], Loss: -19.007034301757812, Learning Rate: 0.01\n",
      "Epoch [13442/20000], Loss: -19.009109497070312, Learning Rate: 0.01\n",
      "Epoch [13443/20000], Loss: -19.01123046875, Learning Rate: 0.01\n",
      "Epoch [13444/20000], Loss: -19.013351440429688, Learning Rate: 0.01\n",
      "Epoch [13445/20000], Loss: -19.015472412109375, Learning Rate: 0.01\n",
      "Epoch [13446/20000], Loss: -19.017486572265625, Learning Rate: 0.01\n",
      "Epoch [13447/20000], Loss: -19.019500732421875, Learning Rate: 0.01\n",
      "Epoch [13448/20000], Loss: -19.021636962890625, Learning Rate: 0.01\n",
      "Epoch [13449/20000], Loss: -19.023666381835938, Learning Rate: 0.01\n",
      "Epoch [13450/20000], Loss: -19.025741577148438, Learning Rate: 0.01\n",
      "Epoch [13451/20000], Loss: -19.027816772460938, Learning Rate: 0.01\n",
      "Epoch [13452/20000], Loss: -19.029891967773438, Learning Rate: 0.01\n",
      "Epoch [13453/20000], Loss: -19.031997680664062, Learning Rate: 0.01\n",
      "Epoch [13454/20000], Loss: -19.034133911132812, Learning Rate: 0.01\n",
      "Epoch [13455/20000], Loss: -19.036285400390625, Learning Rate: 0.01\n",
      "Epoch [13456/20000], Loss: -19.0382080078125, Learning Rate: 0.01\n",
      "Epoch [13457/20000], Loss: -19.04034423828125, Learning Rate: 0.01\n",
      "Epoch [13458/20000], Loss: -19.042404174804688, Learning Rate: 0.01\n",
      "Epoch [13459/20000], Loss: -19.044540405273438, Learning Rate: 0.01\n",
      "Epoch [13460/20000], Loss: -19.046585083007812, Learning Rate: 0.01\n",
      "Epoch [13461/20000], Loss: -19.048553466796875, Learning Rate: 0.01\n",
      "Epoch [13462/20000], Loss: -19.050628662109375, Learning Rate: 0.01\n",
      "Epoch [13463/20000], Loss: -19.052825927734375, Learning Rate: 0.01\n",
      "Epoch [13464/20000], Loss: -19.054885864257812, Learning Rate: 0.01\n",
      "Epoch [13465/20000], Loss: -19.056991577148438, Learning Rate: 0.01\n",
      "Epoch [13466/20000], Loss: -19.059005737304688, Learning Rate: 0.01\n",
      "Epoch [13467/20000], Loss: -19.061065673828125, Learning Rate: 0.01\n",
      "Epoch [13468/20000], Loss: -19.063125610351562, Learning Rate: 0.01\n",
      "Epoch [13469/20000], Loss: -19.065200805664062, Learning Rate: 0.01\n",
      "Epoch [13470/20000], Loss: -19.067214965820312, Learning Rate: 0.01\n",
      "Epoch [13471/20000], Loss: -19.069381713867188, Learning Rate: 0.01\n",
      "Epoch [13472/20000], Loss: -19.071395874023438, Learning Rate: 0.01\n",
      "Epoch [13473/20000], Loss: -19.073471069335938, Learning Rate: 0.01\n",
      "Epoch [13474/20000], Loss: -19.075546264648438, Learning Rate: 0.01\n",
      "Epoch [13475/20000], Loss: -19.077606201171875, Learning Rate: 0.01\n",
      "Epoch [13476/20000], Loss: -19.079666137695312, Learning Rate: 0.01\n",
      "Epoch [13477/20000], Loss: -19.081756591796875, Learning Rate: 0.01\n",
      "Epoch [13478/20000], Loss: -19.083847045898438, Learning Rate: 0.01\n",
      "Epoch [13479/20000], Loss: -19.085891723632812, Learning Rate: 0.01\n",
      "Epoch [13480/20000], Loss: -19.087875366210938, Learning Rate: 0.01\n",
      "Epoch [13481/20000], Loss: -19.089889526367188, Learning Rate: 0.01\n",
      "Epoch [13482/20000], Loss: -19.092025756835938, Learning Rate: 0.01\n",
      "Epoch [13483/20000], Loss: -19.094100952148438, Learning Rate: 0.01\n",
      "Epoch [13484/20000], Loss: -19.096176147460938, Learning Rate: 0.01\n",
      "Epoch [13485/20000], Loss: -19.098190307617188, Learning Rate: 0.01\n",
      "Epoch [13486/20000], Loss: -19.100234985351562, Learning Rate: 0.01\n",
      "Epoch [13487/20000], Loss: -19.102401733398438, Learning Rate: 0.01\n",
      "Epoch [13488/20000], Loss: -19.104476928710938, Learning Rate: 0.01\n",
      "Epoch [13489/20000], Loss: -19.10638427734375, Learning Rate: 0.01\n",
      "Epoch [13490/20000], Loss: -19.108489990234375, Learning Rate: 0.01\n",
      "Epoch [13491/20000], Loss: -19.110610961914062, Learning Rate: 0.01\n",
      "Epoch [13492/20000], Loss: -19.11273193359375, Learning Rate: 0.01\n",
      "Epoch [13493/20000], Loss: -19.114700317382812, Learning Rate: 0.01\n",
      "Epoch [13494/20000], Loss: -19.11669921875, Learning Rate: 0.01\n",
      "Epoch [13495/20000], Loss: -19.118820190429688, Learning Rate: 0.01\n",
      "Epoch [13496/20000], Loss: -19.120819091796875, Learning Rate: 0.01\n",
      "Epoch [13497/20000], Loss: -19.122955322265625, Learning Rate: 0.01\n",
      "Epoch [13498/20000], Loss: -19.124908447265625, Learning Rate: 0.01\n",
      "Epoch [13499/20000], Loss: -19.12701416015625, Learning Rate: 0.01\n",
      "Epoch [13500/20000], Loss: -19.12908935546875, Learning Rate: 0.01\n",
      "Epoch [13501/20000], Loss: -19.131011962890625, Learning Rate: 0.01\n",
      "Epoch [13502/20000], Loss: -19.133056640625, Learning Rate: 0.01\n",
      "Epoch [13503/20000], Loss: -19.135208129882812, Learning Rate: 0.01\n",
      "Epoch [13504/20000], Loss: -19.137252807617188, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13505/20000], Loss: -19.139266967773438, Learning Rate: 0.01\n",
      "Epoch [13506/20000], Loss: -19.141281127929688, Learning Rate: 0.01\n",
      "Epoch [13507/20000], Loss: -19.143356323242188, Learning Rate: 0.01\n",
      "Epoch [13508/20000], Loss: -19.1453857421875, Learning Rate: 0.01\n",
      "Epoch [13509/20000], Loss: -19.147445678710938, Learning Rate: 0.01\n",
      "Epoch [13510/20000], Loss: -19.149520874023438, Learning Rate: 0.01\n",
      "Epoch [13511/20000], Loss: -19.151535034179688, Learning Rate: 0.01\n",
      "Epoch [13512/20000], Loss: -19.153533935546875, Learning Rate: 0.01\n",
      "Epoch [13513/20000], Loss: -19.155670166015625, Learning Rate: 0.01\n",
      "Epoch [13514/20000], Loss: -19.15771484375, Learning Rate: 0.01\n",
      "Epoch [13515/20000], Loss: -19.159805297851562, Learning Rate: 0.01\n",
      "Epoch [13516/20000], Loss: -19.1617431640625, Learning Rate: 0.01\n",
      "Epoch [13517/20000], Loss: -19.16387939453125, Learning Rate: 0.01\n",
      "Epoch [13518/20000], Loss: -19.165939331054688, Learning Rate: 0.01\n",
      "Epoch [13519/20000], Loss: -19.167892456054688, Learning Rate: 0.01\n",
      "Epoch [13520/20000], Loss: -19.169937133789062, Learning Rate: 0.01\n",
      "Epoch [13521/20000], Loss: -19.171981811523438, Learning Rate: 0.01\n",
      "Epoch [13522/20000], Loss: -19.174041748046875, Learning Rate: 0.01\n",
      "Epoch [13523/20000], Loss: -19.176010131835938, Learning Rate: 0.01\n",
      "Epoch [13524/20000], Loss: -19.178115844726562, Learning Rate: 0.01\n",
      "Epoch [13525/20000], Loss: -19.18017578125, Learning Rate: 0.01\n",
      "Epoch [13526/20000], Loss: -19.18212890625, Learning Rate: 0.01\n",
      "Epoch [13527/20000], Loss: -19.184234619140625, Learning Rate: 0.01\n",
      "Epoch [13528/20000], Loss: -19.186233520507812, Learning Rate: 0.01\n",
      "Epoch [13529/20000], Loss: -19.188339233398438, Learning Rate: 0.01\n",
      "Epoch [13530/20000], Loss: -19.190353393554688, Learning Rate: 0.01\n",
      "Epoch [13531/20000], Loss: -19.1923828125, Learning Rate: 0.01\n",
      "Epoch [13532/20000], Loss: -19.194351196289062, Learning Rate: 0.01\n",
      "Epoch [13533/20000], Loss: -19.196487426757812, Learning Rate: 0.01\n",
      "Epoch [13534/20000], Loss: -19.19842529296875, Learning Rate: 0.01\n",
      "Epoch [13535/20000], Loss: -19.200515747070312, Learning Rate: 0.01\n",
      "Epoch [13536/20000], Loss: -19.202545166015625, Learning Rate: 0.01\n",
      "Epoch [13537/20000], Loss: -19.204544067382812, Learning Rate: 0.01\n",
      "Epoch [13538/20000], Loss: -19.206573486328125, Learning Rate: 0.01\n",
      "Epoch [13539/20000], Loss: -19.208648681640625, Learning Rate: 0.01\n",
      "Epoch [13540/20000], Loss: -19.210601806640625, Learning Rate: 0.01\n",
      "Epoch [13541/20000], Loss: -19.212554931640625, Learning Rate: 0.01\n",
      "Epoch [13542/20000], Loss: -19.214630126953125, Learning Rate: 0.01\n",
      "Epoch [13543/20000], Loss: -19.216659545898438, Learning Rate: 0.01\n",
      "Epoch [13544/20000], Loss: -19.218719482421875, Learning Rate: 0.01\n",
      "Epoch [13545/20000], Loss: -19.220672607421875, Learning Rate: 0.01\n",
      "Epoch [13546/20000], Loss: -19.222732543945312, Learning Rate: 0.01\n",
      "Epoch [13547/20000], Loss: -19.224655151367188, Learning Rate: 0.01\n",
      "Epoch [13548/20000], Loss: -19.226638793945312, Learning Rate: 0.01\n",
      "Epoch [13549/20000], Loss: -19.228729248046875, Learning Rate: 0.01\n",
      "Epoch [13550/20000], Loss: -19.230606079101562, Learning Rate: 0.01\n",
      "Epoch [13551/20000], Loss: -19.23248291015625, Learning Rate: 0.01\n",
      "Epoch [13552/20000], Loss: -19.234420776367188, Learning Rate: 0.01\n",
      "Epoch [13553/20000], Loss: -19.236297607421875, Learning Rate: 0.01\n",
      "Epoch [13554/20000], Loss: -19.238067626953125, Learning Rate: 0.01\n",
      "Epoch [13555/20000], Loss: -19.239852905273438, Learning Rate: 0.01\n",
      "Epoch [13556/20000], Loss: -19.24139404296875, Learning Rate: 0.01\n",
      "Epoch [13557/20000], Loss: -19.242828369140625, Learning Rate: 0.01\n",
      "Epoch [13558/20000], Loss: -19.243972778320312, Learning Rate: 0.01\n",
      "Epoch [13559/20000], Loss: -19.244735717773438, Learning Rate: 0.01\n",
      "Epoch [13560/20000], Loss: -19.245162963867188, Learning Rate: 0.01\n",
      "Epoch [13561/20000], Loss: -19.244918823242188, Learning Rate: 0.01\n",
      "Epoch [13562/20000], Loss: -19.243698120117188, Learning Rate: 0.01\n",
      "Epoch [13563/20000], Loss: -19.241104125976562, Learning Rate: 0.01\n",
      "Epoch [13564/20000], Loss: -19.236831665039062, Learning Rate: 0.01\n",
      "Epoch [13565/20000], Loss: -19.2296142578125, Learning Rate: 0.01\n",
      "Epoch [13566/20000], Loss: -19.21868896484375, Learning Rate: 0.01\n",
      "Epoch [13567/20000], Loss: -19.2022705078125, Learning Rate: 0.01\n",
      "Epoch [13568/20000], Loss: -19.178131103515625, Learning Rate: 0.01\n",
      "Epoch [13569/20000], Loss: -19.142929077148438, Learning Rate: 0.01\n",
      "Epoch [13570/20000], Loss: -19.092483520507812, Learning Rate: 0.01\n",
      "Epoch [13571/20000], Loss: -19.020492553710938, Learning Rate: 0.01\n",
      "Epoch [13572/20000], Loss: -18.920089721679688, Learning Rate: 0.01\n",
      "Epoch [13573/20000], Loss: -18.782028198242188, Learning Rate: 0.01\n",
      "Epoch [13574/20000], Loss: -18.599105834960938, Learning Rate: 0.01\n",
      "Epoch [13575/20000], Loss: -18.365570068359375, Learning Rate: 0.01\n",
      "Epoch [13576/20000], Loss: -18.090286254882812, Learning Rate: 0.01\n",
      "Epoch [13577/20000], Loss: -17.796127319335938, Learning Rate: 0.01\n",
      "Epoch [13578/20000], Loss: -17.54461669921875, Learning Rate: 0.01\n",
      "Epoch [13579/20000], Loss: -17.4124755859375, Learning Rate: 0.01\n",
      "Epoch [13580/20000], Loss: -17.49359130859375, Learning Rate: 0.01\n",
      "Epoch [13581/20000], Loss: -17.814239501953125, Learning Rate: 0.01\n",
      "Epoch [13582/20000], Loss: -18.316162109375, Learning Rate: 0.01\n",
      "Epoch [13583/20000], Loss: -18.834487915039062, Learning Rate: 0.01\n",
      "Epoch [13584/20000], Loss: -19.1954345703125, Learning Rate: 0.01\n",
      "Epoch [13585/20000], Loss: -19.301681518554688, Learning Rate: 0.01\n",
      "Epoch [13586/20000], Loss: -19.179779052734375, Learning Rate: 0.01\n",
      "Epoch [13587/20000], Loss: -18.948593139648438, Learning Rate: 0.01\n",
      "Epoch [13588/20000], Loss: -18.752151489257812, Learning Rate: 0.01\n",
      "Epoch [13589/20000], Loss: -18.696487426757812, Learning Rate: 0.01\n",
      "Epoch [13590/20000], Loss: -18.803085327148438, Learning Rate: 0.01\n",
      "Epoch [13591/20000], Loss: -19.010543823242188, Learning Rate: 0.01\n",
      "Epoch [13592/20000], Loss: -19.210006713867188, Learning Rate: 0.01\n",
      "Epoch [13593/20000], Loss: -19.313217163085938, Learning Rate: 0.01\n",
      "Epoch [13594/20000], Loss: -19.29638671875, Learning Rate: 0.01\n",
      "Epoch [13595/20000], Loss: -19.20257568359375, Learning Rate: 0.01\n",
      "Epoch [13596/20000], Loss: -19.10675048828125, Learning Rate: 0.01\n",
      "Epoch [13597/20000], Loss: -19.070098876953125, Learning Rate: 0.01\n",
      "Epoch [13598/20000], Loss: -19.11102294921875, Learning Rate: 0.01\n",
      "Epoch [13599/20000], Loss: -19.20111083984375, Learning Rate: 0.01\n",
      "Epoch [13600/20000], Loss: -19.289398193359375, Learning Rate: 0.01\n",
      "Epoch [13601/20000], Loss: -19.334091186523438, Learning Rate: 0.01\n",
      "Epoch [13602/20000], Loss: -19.324996948242188, Learning Rate: 0.01\n",
      "Epoch [13603/20000], Loss: -19.283248901367188, Learning Rate: 0.01\n",
      "Epoch [13604/20000], Loss: -19.243240356445312, Learning Rate: 0.01\n",
      "Epoch [13605/20000], Loss: -19.231903076171875, Learning Rate: 0.01\n",
      "Epoch [13606/20000], Loss: -19.254440307617188, Learning Rate: 0.01\n",
      "Epoch [13607/20000], Loss: -19.296279907226562, Learning Rate: 0.01\n",
      "Epoch [13608/20000], Loss: -19.334396362304688, Learning Rate: 0.01\n",
      "Epoch [13609/20000], Loss: -19.3516845703125, Learning Rate: 0.01\n",
      "Epoch [13610/20000], Loss: -19.34600830078125, Learning Rate: 0.01\n",
      "Epoch [13611/20000], Loss: -19.327743530273438, Learning Rate: 0.01\n",
      "Epoch [13612/20000], Loss: -19.312347412109375, Learning Rate: 0.01\n",
      "Epoch [13613/20000], Loss: -19.310287475585938, Learning Rate: 0.01\n",
      "Epoch [13614/20000], Loss: -19.322891235351562, Learning Rate: 0.01\n",
      "Epoch [13615/20000], Loss: -19.342575073242188, Learning Rate: 0.01\n",
      "Epoch [13616/20000], Loss: -19.359695434570312, Learning Rate: 0.01\n",
      "Epoch [13617/20000], Loss: -19.367507934570312, Learning Rate: 0.01\n",
      "Epoch [13618/20000], Loss: -19.365386962890625, Learning Rate: 0.01\n",
      "Epoch [13619/20000], Loss: -19.35858154296875, Learning Rate: 0.01\n",
      "Epoch [13620/20000], Loss: -19.353118896484375, Learning Rate: 0.01\n",
      "Epoch [13621/20000], Loss: -19.353591918945312, Learning Rate: 0.01\n",
      "Epoch [13622/20000], Loss: -19.360092163085938, Learning Rate: 0.01\n",
      "Epoch [13623/20000], Loss: -19.369857788085938, Learning Rate: 0.01\n",
      "Epoch [13624/20000], Loss: -19.378341674804688, Learning Rate: 0.01\n",
      "Epoch [13625/20000], Loss: -19.38287353515625, Learning Rate: 0.01\n",
      "Epoch [13626/20000], Loss: -19.383285522460938, Learning Rate: 0.01\n",
      "Epoch [13627/20000], Loss: -19.381484985351562, Learning Rate: 0.01\n",
      "Epoch [13628/20000], Loss: -19.38006591796875, Learning Rate: 0.01\n",
      "Epoch [13629/20000], Loss: -19.381072998046875, Learning Rate: 0.01\n",
      "Epoch [13630/20000], Loss: -19.38470458984375, Learning Rate: 0.01\n",
      "Epoch [13631/20000], Loss: -19.389801025390625, Learning Rate: 0.01\n",
      "Epoch [13632/20000], Loss: -19.394638061523438, Learning Rate: 0.01\n",
      "Epoch [13633/20000], Loss: -19.398117065429688, Learning Rate: 0.01\n",
      "Epoch [13634/20000], Loss: -19.399765014648438, Learning Rate: 0.01\n",
      "Epoch [13635/20000], Loss: -19.400238037109375, Learning Rate: 0.01\n",
      "Epoch [13636/20000], Loss: -19.400527954101562, Learning Rate: 0.01\n",
      "Epoch [13637/20000], Loss: -19.4017333984375, Learning Rate: 0.01\n",
      "Epoch [13638/20000], Loss: -19.404067993164062, Learning Rate: 0.01\n",
      "Epoch [13639/20000], Loss: -19.407058715820312, Learning Rate: 0.01\n",
      "Epoch [13640/20000], Loss: -19.410507202148438, Learning Rate: 0.01\n",
      "Epoch [13641/20000], Loss: -19.413177490234375, Learning Rate: 0.01\n",
      "Epoch [13642/20000], Loss: -19.415298461914062, Learning Rate: 0.01\n",
      "Epoch [13643/20000], Loss: -19.416702270507812, Learning Rate: 0.01\n",
      "Epoch [13644/20000], Loss: -19.41796875, Learning Rate: 0.01\n",
      "Epoch [13645/20000], Loss: -19.419326782226562, Learning Rate: 0.01\n",
      "Epoch [13646/20000], Loss: -19.421188354492188, Learning Rate: 0.01\n",
      "Epoch [13647/20000], Loss: -19.423370361328125, Learning Rate: 0.01\n",
      "Epoch [13648/20000], Loss: -19.425888061523438, Learning Rate: 0.01\n",
      "Epoch [13649/20000], Loss: -19.428253173828125, Learning Rate: 0.01\n",
      "Epoch [13650/20000], Loss: -19.430435180664062, Learning Rate: 0.01\n",
      "Epoch [13651/20000], Loss: -19.43231201171875, Learning Rate: 0.01\n",
      "Epoch [13652/20000], Loss: -19.433868408203125, Learning Rate: 0.01\n",
      "Epoch [13653/20000], Loss: -19.43548583984375, Learning Rate: 0.01\n",
      "Epoch [13654/20000], Loss: -19.437271118164062, Learning Rate: 0.01\n",
      "Epoch [13655/20000], Loss: -19.43914794921875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13656/20000], Loss: -19.441162109375, Learning Rate: 0.01\n",
      "Epoch [13657/20000], Loss: -19.443283081054688, Learning Rate: 0.01\n",
      "Epoch [13658/20000], Loss: -19.445297241210938, Learning Rate: 0.01\n",
      "Epoch [13659/20000], Loss: -19.447311401367188, Learning Rate: 0.01\n",
      "Epoch [13660/20000], Loss: -19.449264526367188, Learning Rate: 0.01\n",
      "Epoch [13661/20000], Loss: -19.450973510742188, Learning Rate: 0.01\n",
      "Epoch [13662/20000], Loss: -19.452606201171875, Learning Rate: 0.01\n",
      "Epoch [13663/20000], Loss: -19.45440673828125, Learning Rate: 0.01\n",
      "Epoch [13664/20000], Loss: -19.456283569335938, Learning Rate: 0.01\n",
      "Epoch [13665/20000], Loss: -19.45806884765625, Learning Rate: 0.01\n",
      "Epoch [13666/20000], Loss: -19.459991455078125, Learning Rate: 0.01\n",
      "Epoch [13667/20000], Loss: -19.461715698242188, Learning Rate: 0.01\n",
      "Epoch [13668/20000], Loss: -19.463409423828125, Learning Rate: 0.01\n",
      "Epoch [13669/20000], Loss: -19.465179443359375, Learning Rate: 0.01\n",
      "Epoch [13670/20000], Loss: -19.466644287109375, Learning Rate: 0.01\n",
      "Epoch [13671/20000], Loss: -19.46795654296875, Learning Rate: 0.01\n",
      "Epoch [13672/20000], Loss: -19.469207763671875, Learning Rate: 0.01\n",
      "Epoch [13673/20000], Loss: -19.47027587890625, Learning Rate: 0.01\n",
      "Epoch [13674/20000], Loss: -19.471054077148438, Learning Rate: 0.01\n",
      "Epoch [13675/20000], Loss: -19.471420288085938, Learning Rate: 0.01\n",
      "Epoch [13676/20000], Loss: -19.471435546875, Learning Rate: 0.01\n",
      "Epoch [13677/20000], Loss: -19.470596313476562, Learning Rate: 0.01\n",
      "Epoch [13678/20000], Loss: -19.468841552734375, Learning Rate: 0.01\n",
      "Epoch [13679/20000], Loss: -19.465484619140625, Learning Rate: 0.01\n",
      "Epoch [13680/20000], Loss: -19.46038818359375, Learning Rate: 0.01\n",
      "Epoch [13681/20000], Loss: -19.45269775390625, Learning Rate: 0.01\n",
      "Epoch [13682/20000], Loss: -19.441299438476562, Learning Rate: 0.01\n",
      "Epoch [13683/20000], Loss: -19.425064086914062, Learning Rate: 0.01\n",
      "Epoch [13684/20000], Loss: -19.402099609375, Learning Rate: 0.01\n",
      "Epoch [13685/20000], Loss: -19.370147705078125, Learning Rate: 0.01\n",
      "Epoch [13686/20000], Loss: -19.326324462890625, Learning Rate: 0.01\n",
      "Epoch [13687/20000], Loss: -19.267684936523438, Learning Rate: 0.01\n",
      "Epoch [13688/20000], Loss: -19.190780639648438, Learning Rate: 0.01\n",
      "Epoch [13689/20000], Loss: -19.094268798828125, Learning Rate: 0.01\n",
      "Epoch [13690/20000], Loss: -18.9786376953125, Learning Rate: 0.01\n",
      "Epoch [13691/20000], Loss: -18.852752685546875, Learning Rate: 0.01\n",
      "Epoch [13692/20000], Loss: -18.731536865234375, Learning Rate: 0.01\n",
      "Epoch [13693/20000], Loss: -18.644561767578125, Learning Rate: 0.01\n",
      "Epoch [13694/20000], Loss: -18.621902465820312, Learning Rate: 0.01\n",
      "Epoch [13695/20000], Loss: -18.693603515625, Learning Rate: 0.01\n",
      "Epoch [13696/20000], Loss: -18.85791015625, Learning Rate: 0.01\n",
      "Epoch [13697/20000], Loss: -19.083450317382812, Learning Rate: 0.01\n",
      "Epoch [13698/20000], Loss: -19.306015014648438, Learning Rate: 0.01\n",
      "Epoch [13699/20000], Loss: -19.464126586914062, Learning Rate: 0.01\n",
      "Epoch [13700/20000], Loss: -19.523468017578125, Learning Rate: 0.01\n",
      "Epoch [13701/20000], Loss: -19.490921020507812, Learning Rate: 0.01\n",
      "Epoch [13702/20000], Loss: -19.404800415039062, Learning Rate: 0.01\n",
      "Epoch [13703/20000], Loss: -19.315414428710938, Learning Rate: 0.01\n",
      "Epoch [13704/20000], Loss: -19.265243530273438, Learning Rate: 0.01\n",
      "Epoch [13705/20000], Loss: -19.2740478515625, Learning Rate: 0.01\n",
      "Epoch [13706/20000], Loss: -19.336105346679688, Learning Rate: 0.01\n",
      "Epoch [13707/20000], Loss: -19.42205810546875, Learning Rate: 0.01\n",
      "Epoch [13708/20000], Loss: -19.497512817382812, Learning Rate: 0.01\n",
      "Epoch [13709/20000], Loss: -19.537017822265625, Learning Rate: 0.01\n",
      "Epoch [13710/20000], Loss: -19.535385131835938, Learning Rate: 0.01\n",
      "Epoch [13711/20000], Loss: -19.50555419921875, Learning Rate: 0.01\n",
      "Epoch [13712/20000], Loss: -19.469711303710938, Learning Rate: 0.01\n",
      "Epoch [13713/20000], Loss: -19.44818115234375, Learning Rate: 0.01\n",
      "Epoch [13714/20000], Loss: -19.450927734375, Learning Rate: 0.01\n",
      "Epoch [13715/20000], Loss: -19.475479125976562, Learning Rate: 0.01\n",
      "Epoch [13716/20000], Loss: -19.510025024414062, Learning Rate: 0.01\n",
      "Epoch [13717/20000], Loss: -19.540252685546875, Learning Rate: 0.01\n",
      "Epoch [13718/20000], Loss: -19.556060791015625, Learning Rate: 0.01\n",
      "Epoch [13719/20000], Loss: -19.555938720703125, Learning Rate: 0.01\n",
      "Epoch [13720/20000], Loss: -19.5447998046875, Learning Rate: 0.01\n",
      "Epoch [13721/20000], Loss: -19.532089233398438, Learning Rate: 0.01\n",
      "Epoch [13722/20000], Loss: -19.525238037109375, Learning Rate: 0.01\n",
      "Epoch [13723/20000], Loss: -19.527969360351562, Learning Rate: 0.01\n",
      "Epoch [13724/20000], Loss: -19.53900146484375, Learning Rate: 0.01\n",
      "Epoch [13725/20000], Loss: -19.5535888671875, Learning Rate: 0.01\n",
      "Epoch [13726/20000], Loss: -19.566162109375, Learning Rate: 0.01\n",
      "Epoch [13727/20000], Loss: -19.573333740234375, Learning Rate: 0.01\n",
      "Epoch [13728/20000], Loss: -19.574432373046875, Learning Rate: 0.01\n",
      "Epoch [13729/20000], Loss: -19.571395874023438, Learning Rate: 0.01\n",
      "Epoch [13730/20000], Loss: -19.567611694335938, Learning Rate: 0.01\n",
      "Epoch [13731/20000], Loss: -19.565765380859375, Learning Rate: 0.01\n",
      "Epoch [13732/20000], Loss: -19.567764282226562, Learning Rate: 0.01\n",
      "Epoch [13733/20000], Loss: -19.572769165039062, Learning Rate: 0.01\n",
      "Epoch [13734/20000], Loss: -19.579376220703125, Learning Rate: 0.01\n",
      "Epoch [13735/20000], Loss: -19.585464477539062, Learning Rate: 0.01\n",
      "Epoch [13736/20000], Loss: -19.58978271484375, Learning Rate: 0.01\n",
      "Epoch [13737/20000], Loss: -19.591720581054688, Learning Rate: 0.01\n",
      "Epoch [13738/20000], Loss: -19.592025756835938, Learning Rate: 0.01\n",
      "Epoch [13739/20000], Loss: -19.591705322265625, Learning Rate: 0.01\n",
      "Epoch [13740/20000], Loss: -19.591888427734375, Learning Rate: 0.01\n",
      "Epoch [13741/20000], Loss: -19.593154907226562, Learning Rate: 0.01\n",
      "Epoch [13742/20000], Loss: -19.59576416015625, Learning Rate: 0.01\n",
      "Epoch [13743/20000], Loss: -19.599166870117188, Learning Rate: 0.01\n",
      "Epoch [13744/20000], Loss: -19.602706909179688, Learning Rate: 0.01\n",
      "Epoch [13745/20000], Loss: -19.605682373046875, Learning Rate: 0.01\n",
      "Epoch [13746/20000], Loss: -19.608047485351562, Learning Rate: 0.01\n",
      "Epoch [13747/20000], Loss: -19.609512329101562, Learning Rate: 0.01\n",
      "Epoch [13748/20000], Loss: -19.610687255859375, Learning Rate: 0.01\n",
      "Epoch [13749/20000], Loss: -19.611602783203125, Learning Rate: 0.01\n",
      "Epoch [13750/20000], Loss: -19.612945556640625, Learning Rate: 0.01\n",
      "Epoch [13751/20000], Loss: -19.614639282226562, Learning Rate: 0.01\n",
      "Epoch [13752/20000], Loss: -19.616867065429688, Learning Rate: 0.01\n",
      "Epoch [13753/20000], Loss: -19.61920166015625, Learning Rate: 0.01\n",
      "Epoch [13754/20000], Loss: -19.621627807617188, Learning Rate: 0.01\n",
      "Epoch [13755/20000], Loss: -19.623825073242188, Learning Rate: 0.01\n",
      "Epoch [13756/20000], Loss: -19.625778198242188, Learning Rate: 0.01\n",
      "Epoch [13757/20000], Loss: -19.627426147460938, Learning Rate: 0.01\n",
      "Epoch [13758/20000], Loss: -19.628921508789062, Learning Rate: 0.01\n",
      "Epoch [13759/20000], Loss: -19.630523681640625, Learning Rate: 0.01\n",
      "Epoch [13760/20000], Loss: -19.632003784179688, Learning Rate: 0.01\n",
      "Epoch [13761/20000], Loss: -19.633758544921875, Learning Rate: 0.01\n",
      "Epoch [13762/20000], Loss: -19.635589599609375, Learning Rate: 0.01\n",
      "Epoch [13763/20000], Loss: -19.637527465820312, Learning Rate: 0.01\n",
      "Epoch [13764/20000], Loss: -19.63958740234375, Learning Rate: 0.01\n",
      "Epoch [13765/20000], Loss: -19.641448974609375, Learning Rate: 0.01\n",
      "Epoch [13766/20000], Loss: -19.643280029296875, Learning Rate: 0.01\n",
      "Epoch [13767/20000], Loss: -19.645111083984375, Learning Rate: 0.01\n",
      "Epoch [13768/20000], Loss: -19.646804809570312, Learning Rate: 0.01\n",
      "Epoch [13769/20000], Loss: -19.648406982421875, Learning Rate: 0.01\n",
      "Epoch [13770/20000], Loss: -19.650070190429688, Learning Rate: 0.01\n",
      "Epoch [13771/20000], Loss: -19.651779174804688, Learning Rate: 0.01\n",
      "Epoch [13772/20000], Loss: -19.653564453125, Learning Rate: 0.01\n",
      "Epoch [13773/20000], Loss: -19.6552734375, Learning Rate: 0.01\n",
      "Epoch [13774/20000], Loss: -19.657135009765625, Learning Rate: 0.01\n",
      "Epoch [13775/20000], Loss: -19.658935546875, Learning Rate: 0.01\n",
      "Epoch [13776/20000], Loss: -19.6607666015625, Learning Rate: 0.01\n",
      "Epoch [13777/20000], Loss: -19.662460327148438, Learning Rate: 0.01\n",
      "Epoch [13778/20000], Loss: -19.664154052734375, Learning Rate: 0.01\n",
      "Epoch [13779/20000], Loss: -19.665924072265625, Learning Rate: 0.01\n",
      "Epoch [13780/20000], Loss: -19.667739868164062, Learning Rate: 0.01\n",
      "Epoch [13781/20000], Loss: -19.669464111328125, Learning Rate: 0.01\n",
      "Epoch [13782/20000], Loss: -19.671142578125, Learning Rate: 0.01\n",
      "Epoch [13783/20000], Loss: -19.6728515625, Learning Rate: 0.01\n",
      "Epoch [13784/20000], Loss: -19.674713134765625, Learning Rate: 0.01\n",
      "Epoch [13785/20000], Loss: -19.676406860351562, Learning Rate: 0.01\n",
      "Epoch [13786/20000], Loss: -19.678131103515625, Learning Rate: 0.01\n",
      "Epoch [13787/20000], Loss: -19.679977416992188, Learning Rate: 0.01\n",
      "Epoch [13788/20000], Loss: -19.681686401367188, Learning Rate: 0.01\n",
      "Epoch [13789/20000], Loss: -19.683380126953125, Learning Rate: 0.01\n",
      "Epoch [13790/20000], Loss: -19.68511962890625, Learning Rate: 0.01\n",
      "Epoch [13791/20000], Loss: -19.686859130859375, Learning Rate: 0.01\n",
      "Epoch [13792/20000], Loss: -19.688583374023438, Learning Rate: 0.01\n",
      "Epoch [13793/20000], Loss: -19.6903076171875, Learning Rate: 0.01\n",
      "Epoch [13794/20000], Loss: -19.692047119140625, Learning Rate: 0.01\n",
      "Epoch [13795/20000], Loss: -19.693740844726562, Learning Rate: 0.01\n",
      "Epoch [13796/20000], Loss: -19.695465087890625, Learning Rate: 0.01\n",
      "Epoch [13797/20000], Loss: -19.697235107421875, Learning Rate: 0.01\n",
      "Epoch [13798/20000], Loss: -19.698883056640625, Learning Rate: 0.01\n",
      "Epoch [13799/20000], Loss: -19.70068359375, Learning Rate: 0.01\n",
      "Epoch [13800/20000], Loss: -19.702407836914062, Learning Rate: 0.01\n",
      "Epoch [13801/20000], Loss: -19.704086303710938, Learning Rate: 0.01\n",
      "Epoch [13802/20000], Loss: -19.705841064453125, Learning Rate: 0.01\n",
      "Epoch [13803/20000], Loss: -19.7076416015625, Learning Rate: 0.01\n",
      "Epoch [13804/20000], Loss: -19.709228515625, Learning Rate: 0.01\n",
      "Epoch [13805/20000], Loss: -19.711013793945312, Learning Rate: 0.01\n",
      "Epoch [13806/20000], Loss: -19.712722778320312, Learning Rate: 0.01\n",
      "Epoch [13807/20000], Loss: -19.714447021484375, Learning Rate: 0.01\n",
      "Epoch [13808/20000], Loss: -19.716201782226562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13809/20000], Loss: -19.717880249023438, Learning Rate: 0.01\n",
      "Epoch [13810/20000], Loss: -19.7196044921875, Learning Rate: 0.01\n",
      "Epoch [13811/20000], Loss: -19.721282958984375, Learning Rate: 0.01\n",
      "Epoch [13812/20000], Loss: -19.723037719726562, Learning Rate: 0.01\n",
      "Epoch [13813/20000], Loss: -19.724777221679688, Learning Rate: 0.01\n",
      "Epoch [13814/20000], Loss: -19.726409912109375, Learning Rate: 0.01\n",
      "Epoch [13815/20000], Loss: -19.728118896484375, Learning Rate: 0.01\n",
      "Epoch [13816/20000], Loss: -19.729904174804688, Learning Rate: 0.01\n",
      "Epoch [13817/20000], Loss: -19.7315673828125, Learning Rate: 0.01\n",
      "Epoch [13818/20000], Loss: -19.733352661132812, Learning Rate: 0.01\n",
      "Epoch [13819/20000], Loss: -19.735015869140625, Learning Rate: 0.01\n",
      "Epoch [13820/20000], Loss: -19.73675537109375, Learning Rate: 0.01\n",
      "Epoch [13821/20000], Loss: -19.738388061523438, Learning Rate: 0.01\n",
      "Epoch [13822/20000], Loss: -19.740142822265625, Learning Rate: 0.01\n",
      "Epoch [13823/20000], Loss: -19.74188232421875, Learning Rate: 0.01\n",
      "Epoch [13824/20000], Loss: -19.7435302734375, Learning Rate: 0.01\n",
      "Epoch [13825/20000], Loss: -19.745269775390625, Learning Rate: 0.01\n",
      "Epoch [13826/20000], Loss: -19.746871948242188, Learning Rate: 0.01\n",
      "Epoch [13827/20000], Loss: -19.74853515625, Learning Rate: 0.01\n",
      "Epoch [13828/20000], Loss: -19.750289916992188, Learning Rate: 0.01\n",
      "Epoch [13829/20000], Loss: -19.752029418945312, Learning Rate: 0.01\n",
      "Epoch [13830/20000], Loss: -19.753768920898438, Learning Rate: 0.01\n",
      "Epoch [13831/20000], Loss: -19.755447387695312, Learning Rate: 0.01\n",
      "Epoch [13832/20000], Loss: -19.757080078125, Learning Rate: 0.01\n",
      "Epoch [13833/20000], Loss: -19.758743286132812, Learning Rate: 0.01\n",
      "Epoch [13834/20000], Loss: -19.760528564453125, Learning Rate: 0.01\n",
      "Epoch [13835/20000], Loss: -19.762222290039062, Learning Rate: 0.01\n",
      "Epoch [13836/20000], Loss: -19.763916015625, Learning Rate: 0.01\n",
      "Epoch [13837/20000], Loss: -19.765579223632812, Learning Rate: 0.01\n",
      "Epoch [13838/20000], Loss: -19.76727294921875, Learning Rate: 0.01\n",
      "Epoch [13839/20000], Loss: -19.768951416015625, Learning Rate: 0.01\n",
      "Epoch [13840/20000], Loss: -19.770660400390625, Learning Rate: 0.01\n",
      "Epoch [13841/20000], Loss: -19.772384643554688, Learning Rate: 0.01\n",
      "Epoch [13842/20000], Loss: -19.774002075195312, Learning Rate: 0.01\n",
      "Epoch [13843/20000], Loss: -19.775772094726562, Learning Rate: 0.01\n",
      "Epoch [13844/20000], Loss: -19.777435302734375, Learning Rate: 0.01\n",
      "Epoch [13845/20000], Loss: -19.779159545898438, Learning Rate: 0.01\n",
      "Epoch [13846/20000], Loss: -19.780838012695312, Learning Rate: 0.01\n",
      "Epoch [13847/20000], Loss: -19.782440185546875, Learning Rate: 0.01\n",
      "Epoch [13848/20000], Loss: -19.784194946289062, Learning Rate: 0.01\n",
      "Epoch [13849/20000], Loss: -19.785888671875, Learning Rate: 0.01\n",
      "Epoch [13850/20000], Loss: -19.787490844726562, Learning Rate: 0.01\n",
      "Epoch [13851/20000], Loss: -19.789260864257812, Learning Rate: 0.01\n",
      "Epoch [13852/20000], Loss: -19.790878295898438, Learning Rate: 0.01\n",
      "Epoch [13853/20000], Loss: -19.79254150390625, Learning Rate: 0.01\n",
      "Epoch [13854/20000], Loss: -19.794281005859375, Learning Rate: 0.01\n",
      "Epoch [13855/20000], Loss: -19.79595947265625, Learning Rate: 0.01\n",
      "Epoch [13856/20000], Loss: -19.797622680664062, Learning Rate: 0.01\n",
      "Epoch [13857/20000], Loss: -19.799240112304688, Learning Rate: 0.01\n",
      "Epoch [13858/20000], Loss: -19.800979614257812, Learning Rate: 0.01\n",
      "Epoch [13859/20000], Loss: -19.802597045898438, Learning Rate: 0.01\n",
      "Epoch [13860/20000], Loss: -19.804367065429688, Learning Rate: 0.01\n",
      "Epoch [13861/20000], Loss: -19.806076049804688, Learning Rate: 0.01\n",
      "Epoch [13862/20000], Loss: -19.807586669921875, Learning Rate: 0.01\n",
      "Epoch [13863/20000], Loss: -19.809326171875, Learning Rate: 0.01\n",
      "Epoch [13864/20000], Loss: -19.811065673828125, Learning Rate: 0.01\n",
      "Epoch [13865/20000], Loss: -19.81268310546875, Learning Rate: 0.01\n",
      "Epoch [13866/20000], Loss: -19.814346313476562, Learning Rate: 0.01\n",
      "Epoch [13867/20000], Loss: -19.815994262695312, Learning Rate: 0.01\n",
      "Epoch [13868/20000], Loss: -19.817626953125, Learning Rate: 0.01\n",
      "Epoch [13869/20000], Loss: -19.819320678710938, Learning Rate: 0.01\n",
      "Epoch [13870/20000], Loss: -19.821029663085938, Learning Rate: 0.01\n",
      "Epoch [13871/20000], Loss: -19.822723388671875, Learning Rate: 0.01\n",
      "Epoch [13872/20000], Loss: -19.8243408203125, Learning Rate: 0.01\n",
      "Epoch [13873/20000], Loss: -19.825897216796875, Learning Rate: 0.01\n",
      "Epoch [13874/20000], Loss: -19.827667236328125, Learning Rate: 0.01\n",
      "Epoch [13875/20000], Loss: -19.829345703125, Learning Rate: 0.01\n",
      "Epoch [13876/20000], Loss: -19.8309326171875, Learning Rate: 0.01\n",
      "Epoch [13877/20000], Loss: -19.83258056640625, Learning Rate: 0.01\n",
      "Epoch [13878/20000], Loss: -19.834182739257812, Learning Rate: 0.01\n",
      "Epoch [13879/20000], Loss: -19.835830688476562, Learning Rate: 0.01\n",
      "Epoch [13880/20000], Loss: -19.837356567382812, Learning Rate: 0.01\n",
      "Epoch [13881/20000], Loss: -19.838973999023438, Learning Rate: 0.01\n",
      "Epoch [13882/20000], Loss: -19.840530395507812, Learning Rate: 0.01\n",
      "Epoch [13883/20000], Loss: -19.842071533203125, Learning Rate: 0.01\n",
      "Epoch [13884/20000], Loss: -19.843505859375, Learning Rate: 0.01\n",
      "Epoch [13885/20000], Loss: -19.844818115234375, Learning Rate: 0.01\n",
      "Epoch [13886/20000], Loss: -19.8460693359375, Learning Rate: 0.01\n",
      "Epoch [13887/20000], Loss: -19.847061157226562, Learning Rate: 0.01\n",
      "Epoch [13888/20000], Loss: -19.847793579101562, Learning Rate: 0.01\n",
      "Epoch [13889/20000], Loss: -19.848251342773438, Learning Rate: 0.01\n",
      "Epoch [13890/20000], Loss: -19.847946166992188, Learning Rate: 0.01\n",
      "Epoch [13891/20000], Loss: -19.8468017578125, Learning Rate: 0.01\n",
      "Epoch [13892/20000], Loss: -19.844467163085938, Learning Rate: 0.01\n",
      "Epoch [13893/20000], Loss: -19.8402099609375, Learning Rate: 0.01\n",
      "Epoch [13894/20000], Loss: -19.833206176757812, Learning Rate: 0.01\n",
      "Epoch [13895/20000], Loss: -19.82220458984375, Learning Rate: 0.01\n",
      "Epoch [13896/20000], Loss: -19.804794311523438, Learning Rate: 0.01\n",
      "Epoch [13897/20000], Loss: -19.77874755859375, Learning Rate: 0.01\n",
      "Epoch [13898/20000], Loss: -19.739578247070312, Learning Rate: 0.01\n",
      "Epoch [13899/20000], Loss: -19.6817626953125, Learning Rate: 0.01\n",
      "Epoch [13900/20000], Loss: -19.598159790039062, Learning Rate: 0.01\n",
      "Epoch [13901/20000], Loss: -19.480117797851562, Learning Rate: 0.01\n",
      "Epoch [13902/20000], Loss: -19.320098876953125, Learning Rate: 0.01\n",
      "Epoch [13903/20000], Loss: -19.115921020507812, Learning Rate: 0.01\n",
      "Epoch [13904/20000], Loss: -18.880294799804688, Learning Rate: 0.01\n",
      "Epoch [13905/20000], Loss: -18.651885986328125, Learning Rate: 0.01\n",
      "Epoch [13906/20000], Loss: -18.501953125, Learning Rate: 0.01\n",
      "Epoch [13907/20000], Loss: -18.516067504882812, Learning Rate: 0.01\n",
      "Epoch [13908/20000], Loss: -18.744064331054688, Learning Rate: 0.01\n",
      "Epoch [13909/20000], Loss: -19.138320922851562, Learning Rate: 0.01\n",
      "Epoch [13910/20000], Loss: -19.551132202148438, Learning Rate: 0.01\n",
      "Epoch [13911/20000], Loss: -19.81524658203125, Learning Rate: 0.01\n",
      "Epoch [13912/20000], Loss: -19.851669311523438, Learning Rate: 0.01\n",
      "Epoch [13913/20000], Loss: -19.707138061523438, Learning Rate: 0.01\n",
      "Epoch [13914/20000], Loss: -19.511398315429688, Learning Rate: 0.01\n",
      "Epoch [13915/20000], Loss: -19.39825439453125, Learning Rate: 0.01\n",
      "Epoch [13916/20000], Loss: -19.438491821289062, Learning Rate: 0.01\n",
      "Epoch [13917/20000], Loss: -19.602951049804688, Learning Rate: 0.01\n",
      "Epoch [13918/20000], Loss: -19.789840698242188, Learning Rate: 0.01\n",
      "Epoch [13919/20000], Loss: -19.895095825195312, Learning Rate: 0.01\n",
      "Epoch [13920/20000], Loss: -19.881362915039062, Learning Rate: 0.01\n",
      "Epoch [13921/20000], Loss: -19.791259765625, Learning Rate: 0.01\n",
      "Epoch [13922/20000], Loss: -19.705123901367188, Learning Rate: 0.01\n",
      "Epoch [13923/20000], Loss: -19.686386108398438, Learning Rate: 0.01\n",
      "Epoch [13924/20000], Loss: -19.743209838867188, Learning Rate: 0.01\n",
      "Epoch [13925/20000], Loss: -19.833282470703125, Learning Rate: 0.01\n",
      "Epoch [13926/20000], Loss: -19.89892578125, Learning Rate: 0.01\n",
      "Epoch [13927/20000], Loss: -19.908538818359375, Learning Rate: 0.01\n",
      "Epoch [13928/20000], Loss: -19.872711181640625, Learning Rate: 0.01\n",
      "Epoch [13929/20000], Loss: -19.82965087890625, Learning Rate: 0.01\n",
      "Epoch [13930/20000], Loss: -19.814605712890625, Learning Rate: 0.01\n",
      "Epoch [13931/20000], Loss: -19.837371826171875, Learning Rate: 0.01\n",
      "Epoch [13932/20000], Loss: -19.880752563476562, Learning Rate: 0.01\n",
      "Epoch [13933/20000], Loss: -19.916412353515625, Learning Rate: 0.01\n",
      "Epoch [13934/20000], Loss: -19.926162719726562, Learning Rate: 0.01\n",
      "Epoch [13935/20000], Loss: -19.912094116210938, Learning Rate: 0.01\n",
      "Epoch [13936/20000], Loss: -19.891250610351562, Learning Rate: 0.01\n",
      "Epoch [13937/20000], Loss: -19.881423950195312, Learning Rate: 0.01\n",
      "Epoch [13938/20000], Loss: -19.889801025390625, Learning Rate: 0.01\n",
      "Epoch [13939/20000], Loss: -19.910079956054688, Learning Rate: 0.01\n",
      "Epoch [13940/20000], Loss: -19.92901611328125, Learning Rate: 0.01\n",
      "Epoch [13941/20000], Loss: -19.937057495117188, Learning Rate: 0.01\n",
      "Epoch [13942/20000], Loss: -19.933090209960938, Learning Rate: 0.01\n",
      "Epoch [13943/20000], Loss: -19.924041748046875, Learning Rate: 0.01\n",
      "Epoch [13944/20000], Loss: -19.918807983398438, Learning Rate: 0.01\n",
      "Epoch [13945/20000], Loss: -19.921920776367188, Learning Rate: 0.01\n",
      "Epoch [13946/20000], Loss: -19.931503295898438, Learning Rate: 0.01\n",
      "Epoch [13947/20000], Loss: -19.942031860351562, Learning Rate: 0.01\n",
      "Epoch [13948/20000], Loss: -19.948394775390625, Learning Rate: 0.01\n",
      "Epoch [13949/20000], Loss: -19.94879150390625, Learning Rate: 0.01\n",
      "Epoch [13950/20000], Loss: -19.945770263671875, Learning Rate: 0.01\n",
      "Epoch [13951/20000], Loss: -19.943145751953125, Learning Rate: 0.01\n",
      "Epoch [13952/20000], Loss: -19.943939208984375, Learning Rate: 0.01\n",
      "Epoch [13953/20000], Loss: -19.948257446289062, Learning Rate: 0.01\n",
      "Epoch [13954/20000], Loss: -19.953964233398438, Learning Rate: 0.01\n",
      "Epoch [13955/20000], Loss: -19.958694458007812, Learning Rate: 0.01\n",
      "Epoch [13956/20000], Loss: -19.960784912109375, Learning Rate: 0.01\n",
      "Epoch [13957/20000], Loss: -19.960586547851562, Learning Rate: 0.01\n",
      "Epoch [13958/20000], Loss: -19.959884643554688, Learning Rate: 0.01\n",
      "Epoch [13959/20000], Loss: -19.96026611328125, Learning Rate: 0.01\n",
      "Epoch [13960/20000], Loss: -19.96234130859375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13961/20000], Loss: -19.965667724609375, Learning Rate: 0.01\n",
      "Epoch [13962/20000], Loss: -19.969100952148438, Learning Rate: 0.01\n",
      "Epoch [13963/20000], Loss: -19.971649169921875, Learning Rate: 0.01\n",
      "Epoch [13964/20000], Loss: -19.972915649414062, Learning Rate: 0.01\n",
      "Epoch [13965/20000], Loss: -19.973587036132812, Learning Rate: 0.01\n",
      "Epoch [13966/20000], Loss: -19.974197387695312, Learning Rate: 0.01\n",
      "Epoch [13967/20000], Loss: -19.9754638671875, Learning Rate: 0.01\n",
      "Epoch [13968/20000], Loss: -19.977371215820312, Learning Rate: 0.01\n",
      "Epoch [13969/20000], Loss: -19.979812622070312, Learning Rate: 0.01\n",
      "Epoch [13970/20000], Loss: -19.982040405273438, Learning Rate: 0.01\n",
      "Epoch [13971/20000], Loss: -19.983963012695312, Learning Rate: 0.01\n",
      "Epoch [13972/20000], Loss: -19.985260009765625, Learning Rate: 0.01\n",
      "Epoch [13973/20000], Loss: -19.986312866210938, Learning Rate: 0.01\n",
      "Epoch [13974/20000], Loss: -19.987533569335938, Learning Rate: 0.01\n",
      "Epoch [13975/20000], Loss: -19.988967895507812, Learning Rate: 0.01\n",
      "Epoch [13976/20000], Loss: -19.990692138671875, Learning Rate: 0.01\n",
      "Epoch [13977/20000], Loss: -19.992446899414062, Learning Rate: 0.01\n",
      "Epoch [13978/20000], Loss: -19.994338989257812, Learning Rate: 0.01\n",
      "Epoch [13979/20000], Loss: -19.99609375, Learning Rate: 0.01\n",
      "Epoch [13980/20000], Loss: -19.997421264648438, Learning Rate: 0.01\n",
      "Epoch [13981/20000], Loss: -19.998748779296875, Learning Rate: 0.01\n",
      "Epoch [13982/20000], Loss: -20.000137329101562, Learning Rate: 0.01\n",
      "Epoch [13983/20000], Loss: -20.0015869140625, Learning Rate: 0.01\n",
      "Epoch [13984/20000], Loss: -20.003128051757812, Learning Rate: 0.01\n",
      "Epoch [13985/20000], Loss: -20.004898071289062, Learning Rate: 0.01\n",
      "Epoch [13986/20000], Loss: -20.006561279296875, Learning Rate: 0.01\n",
      "Epoch [13987/20000], Loss: -20.008056640625, Learning Rate: 0.01\n",
      "Epoch [13988/20000], Loss: -20.009536743164062, Learning Rate: 0.01\n",
      "Epoch [13989/20000], Loss: -20.010971069335938, Learning Rate: 0.01\n",
      "Epoch [13990/20000], Loss: -20.0123291015625, Learning Rate: 0.01\n",
      "Epoch [13991/20000], Loss: -20.013931274414062, Learning Rate: 0.01\n",
      "Epoch [13992/20000], Loss: -20.015396118164062, Learning Rate: 0.01\n",
      "Epoch [13993/20000], Loss: -20.016952514648438, Learning Rate: 0.01\n",
      "Epoch [13994/20000], Loss: -20.01849365234375, Learning Rate: 0.01\n",
      "Epoch [13995/20000], Loss: -20.020034790039062, Learning Rate: 0.01\n",
      "Epoch [13996/20000], Loss: -20.021575927734375, Learning Rate: 0.01\n",
      "Epoch [13997/20000], Loss: -20.022964477539062, Learning Rate: 0.01\n",
      "Epoch [13998/20000], Loss: -20.024444580078125, Learning Rate: 0.01\n",
      "Epoch [13999/20000], Loss: -20.0260009765625, Learning Rate: 0.01\n",
      "Epoch [14000/20000], Loss: -20.027481079101562, Learning Rate: 0.01\n",
      "Epoch [14001/20000], Loss: -20.028945922851562, Learning Rate: 0.01\n",
      "Epoch [14002/20000], Loss: -20.030441284179688, Learning Rate: 0.01\n",
      "Epoch [14003/20000], Loss: -20.03204345703125, Learning Rate: 0.01\n",
      "Epoch [14004/20000], Loss: -20.03350830078125, Learning Rate: 0.01\n",
      "Epoch [14005/20000], Loss: -20.034957885742188, Learning Rate: 0.01\n",
      "Epoch [14006/20000], Loss: -20.03656005859375, Learning Rate: 0.01\n",
      "Epoch [14007/20000], Loss: -20.037979125976562, Learning Rate: 0.01\n",
      "Epoch [14008/20000], Loss: -20.039505004882812, Learning Rate: 0.01\n",
      "Epoch [14009/20000], Loss: -20.040908813476562, Learning Rate: 0.01\n",
      "Epoch [14010/20000], Loss: -20.042404174804688, Learning Rate: 0.01\n",
      "Epoch [14011/20000], Loss: -20.043869018554688, Learning Rate: 0.01\n",
      "Epoch [14012/20000], Loss: -20.045333862304688, Learning Rate: 0.01\n",
      "Epoch [14013/20000], Loss: -20.046829223632812, Learning Rate: 0.01\n",
      "Epoch [14014/20000], Loss: -20.04833984375, Learning Rate: 0.01\n",
      "Epoch [14015/20000], Loss: -20.04974365234375, Learning Rate: 0.01\n",
      "Epoch [14016/20000], Loss: -20.05126953125, Learning Rate: 0.01\n",
      "Epoch [14017/20000], Loss: -20.052749633789062, Learning Rate: 0.01\n",
      "Epoch [14018/20000], Loss: -20.054168701171875, Learning Rate: 0.01\n",
      "Epoch [14019/20000], Loss: -20.05560302734375, Learning Rate: 0.01\n",
      "Epoch [14020/20000], Loss: -20.05718994140625, Learning Rate: 0.01\n",
      "Epoch [14021/20000], Loss: -20.058563232421875, Learning Rate: 0.01\n",
      "Epoch [14022/20000], Loss: -20.05999755859375, Learning Rate: 0.01\n",
      "Epoch [14023/20000], Loss: -20.061447143554688, Learning Rate: 0.01\n",
      "Epoch [14024/20000], Loss: -20.06292724609375, Learning Rate: 0.01\n",
      "Epoch [14025/20000], Loss: -20.064361572265625, Learning Rate: 0.01\n",
      "Epoch [14026/20000], Loss: -20.06573486328125, Learning Rate: 0.01\n",
      "Epoch [14027/20000], Loss: -20.06719970703125, Learning Rate: 0.01\n",
      "Epoch [14028/20000], Loss: -20.068511962890625, Learning Rate: 0.01\n",
      "Epoch [14029/20000], Loss: -20.069808959960938, Learning Rate: 0.01\n",
      "Epoch [14030/20000], Loss: -20.071090698242188, Learning Rate: 0.01\n",
      "Epoch [14031/20000], Loss: -20.072372436523438, Learning Rate: 0.01\n",
      "Epoch [14032/20000], Loss: -20.07354736328125, Learning Rate: 0.01\n",
      "Epoch [14033/20000], Loss: -20.074691772460938, Learning Rate: 0.01\n",
      "Epoch [14034/20000], Loss: -20.07562255859375, Learning Rate: 0.01\n",
      "Epoch [14035/20000], Loss: -20.076492309570312, Learning Rate: 0.01\n",
      "Epoch [14036/20000], Loss: -20.07696533203125, Learning Rate: 0.01\n",
      "Epoch [14037/20000], Loss: -20.077316284179688, Learning Rate: 0.01\n",
      "Epoch [14038/20000], Loss: -20.077056884765625, Learning Rate: 0.01\n",
      "Epoch [14039/20000], Loss: -20.076461791992188, Learning Rate: 0.01\n",
      "Epoch [14040/20000], Loss: -20.074935913085938, Learning Rate: 0.01\n",
      "Epoch [14041/20000], Loss: -20.072555541992188, Learning Rate: 0.01\n",
      "Epoch [14042/20000], Loss: -20.068527221679688, Learning Rate: 0.01\n",
      "Epoch [14043/20000], Loss: -20.062530517578125, Learning Rate: 0.01\n",
      "Epoch [14044/20000], Loss: -20.053939819335938, Learning Rate: 0.01\n",
      "Epoch [14045/20000], Loss: -20.041595458984375, Learning Rate: 0.01\n",
      "Epoch [14046/20000], Loss: -20.024139404296875, Learning Rate: 0.01\n",
      "Epoch [14047/20000], Loss: -19.999725341796875, Learning Rate: 0.01\n",
      "Epoch [14048/20000], Loss: -19.966140747070312, Learning Rate: 0.01\n",
      "Epoch [14049/20000], Loss: -19.920440673828125, Learning Rate: 0.01\n",
      "Epoch [14050/20000], Loss: -19.859222412109375, Learning Rate: 0.01\n",
      "Epoch [14051/20000], Loss: -19.778549194335938, Learning Rate: 0.01\n",
      "Epoch [14052/20000], Loss: -19.676742553710938, Learning Rate: 0.01\n",
      "Epoch [14053/20000], Loss: -19.551773071289062, Learning Rate: 0.01\n",
      "Epoch [14054/20000], Loss: -19.411056518554688, Learning Rate: 0.01\n",
      "Epoch [14055/20000], Loss: -19.264801025390625, Learning Rate: 0.01\n",
      "Epoch [14056/20000], Loss: -19.141998291015625, Learning Rate: 0.01\n",
      "Epoch [14057/20000], Loss: -19.071762084960938, Learning Rate: 0.01\n",
      "Epoch [14058/20000], Loss: -19.09539794921875, Learning Rate: 0.01\n",
      "Epoch [14059/20000], Loss: -19.225296020507812, Learning Rate: 0.01\n",
      "Epoch [14060/20000], Loss: -19.45147705078125, Learning Rate: 0.01\n",
      "Epoch [14061/20000], Loss: -19.715560913085938, Learning Rate: 0.01\n",
      "Epoch [14062/20000], Loss: -19.947296142578125, Learning Rate: 0.01\n",
      "Epoch [14063/20000], Loss: -20.086578369140625, Learning Rate: 0.01\n",
      "Epoch [14064/20000], Loss: -20.113845825195312, Learning Rate: 0.01\n",
      "Epoch [14065/20000], Loss: -20.051681518554688, Learning Rate: 0.01\n",
      "Epoch [14066/20000], Loss: -19.948959350585938, Learning Rate: 0.01\n",
      "Epoch [14067/20000], Loss: -19.860137939453125, Learning Rate: 0.01\n",
      "Epoch [14068/20000], Loss: -19.823471069335938, Learning Rate: 0.01\n",
      "Epoch [14069/20000], Loss: -19.852218627929688, Learning Rate: 0.01\n",
      "Epoch [14070/20000], Loss: -19.930038452148438, Learning Rate: 0.01\n",
      "Epoch [14071/20000], Loss: -20.02313232421875, Learning Rate: 0.01\n",
      "Epoch [14072/20000], Loss: -20.095901489257812, Learning Rate: 0.01\n",
      "Epoch [14073/20000], Loss: -20.127182006835938, Learning Rate: 0.01\n",
      "Epoch [14074/20000], Loss: -20.1173095703125, Learning Rate: 0.01\n",
      "Epoch [14075/20000], Loss: -20.08319091796875, Learning Rate: 0.01\n",
      "Epoch [14076/20000], Loss: -20.048385620117188, Learning Rate: 0.01\n",
      "Epoch [14077/20000], Loss: -20.031387329101562, Learning Rate: 0.01\n",
      "Epoch [14078/20000], Loss: -20.03955078125, Learning Rate: 0.01\n",
      "Epoch [14079/20000], Loss: -20.067245483398438, Learning Rate: 0.01\n",
      "Epoch [14080/20000], Loss: -20.101608276367188, Learning Rate: 0.01\n",
      "Epoch [14081/20000], Loss: -20.128952026367188, Learning Rate: 0.01\n",
      "Epoch [14082/20000], Loss: -20.14117431640625, Learning Rate: 0.01\n",
      "Epoch [14083/20000], Loss: -20.138687133789062, Learning Rate: 0.01\n",
      "Epoch [14084/20000], Loss: -20.127670288085938, Learning Rate: 0.01\n",
      "Epoch [14085/20000], Loss: -20.116943359375, Learning Rate: 0.01\n",
      "Epoch [14086/20000], Loss: -20.112899780273438, Learning Rate: 0.01\n",
      "Epoch [14087/20000], Loss: -20.117752075195312, Learning Rate: 0.01\n",
      "Epoch [14088/20000], Loss: -20.12890625, Learning Rate: 0.01\n",
      "Epoch [14089/20000], Loss: -20.141815185546875, Learning Rate: 0.01\n",
      "Epoch [14090/20000], Loss: -20.15179443359375, Learning Rate: 0.01\n",
      "Epoch [14091/20000], Loss: -20.156448364257812, Learning Rate: 0.01\n",
      "Epoch [14092/20000], Loss: -20.155990600585938, Learning Rate: 0.01\n",
      "Epoch [14093/20000], Loss: -20.153030395507812, Learning Rate: 0.01\n",
      "Epoch [14094/20000], Loss: -20.150604248046875, Learning Rate: 0.01\n",
      "Epoch [14095/20000], Loss: -20.150665283203125, Learning Rate: 0.01\n",
      "Epoch [14096/20000], Loss: -20.153717041015625, Learning Rate: 0.01\n",
      "Epoch [14097/20000], Loss: -20.159042358398438, Learning Rate: 0.01\n",
      "Epoch [14098/20000], Loss: -20.164718627929688, Learning Rate: 0.01\n",
      "Epoch [14099/20000], Loss: -20.1690673828125, Learning Rate: 0.01\n",
      "Epoch [14100/20000], Loss: -20.171340942382812, Learning Rate: 0.01\n",
      "Epoch [14101/20000], Loss: -20.171905517578125, Learning Rate: 0.01\n",
      "Epoch [14102/20000], Loss: -20.171417236328125, Learning Rate: 0.01\n",
      "Epoch [14103/20000], Loss: -20.171310424804688, Learning Rate: 0.01\n",
      "Epoch [14104/20000], Loss: -20.172103881835938, Learning Rate: 0.01\n",
      "Epoch [14105/20000], Loss: -20.174179077148438, Learning Rate: 0.01\n",
      "Epoch [14106/20000], Loss: -20.177169799804688, Learning Rate: 0.01\n",
      "Epoch [14107/20000], Loss: -20.18048095703125, Learning Rate: 0.01\n",
      "Epoch [14108/20000], Loss: -20.1832275390625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14109/20000], Loss: -20.1851806640625, Learning Rate: 0.01\n",
      "Epoch [14110/20000], Loss: -20.186264038085938, Learning Rate: 0.01\n",
      "Epoch [14111/20000], Loss: -20.18682861328125, Learning Rate: 0.01\n",
      "Epoch [14112/20000], Loss: -20.187362670898438, Learning Rate: 0.01\n",
      "Epoch [14113/20000], Loss: -20.1881103515625, Learning Rate: 0.01\n",
      "Epoch [14114/20000], Loss: -20.189498901367188, Learning Rate: 0.01\n",
      "Epoch [14115/20000], Loss: -20.19122314453125, Learning Rate: 0.01\n",
      "Epoch [14116/20000], Loss: -20.193435668945312, Learning Rate: 0.01\n",
      "Epoch [14117/20000], Loss: -20.195556640625, Learning Rate: 0.01\n",
      "Epoch [14118/20000], Loss: -20.1973876953125, Learning Rate: 0.01\n",
      "Epoch [14119/20000], Loss: -20.198989868164062, Learning Rate: 0.01\n",
      "Epoch [14120/20000], Loss: -20.200210571289062, Learning Rate: 0.01\n",
      "Epoch [14121/20000], Loss: -20.201202392578125, Learning Rate: 0.01\n",
      "Epoch [14122/20000], Loss: -20.202117919921875, Learning Rate: 0.01\n",
      "Epoch [14123/20000], Loss: -20.203323364257812, Learning Rate: 0.01\n",
      "Epoch [14124/20000], Loss: -20.204559326171875, Learning Rate: 0.01\n",
      "Epoch [14125/20000], Loss: -20.205978393554688, Learning Rate: 0.01\n",
      "Epoch [14126/20000], Loss: -20.207611083984375, Learning Rate: 0.01\n",
      "Epoch [14127/20000], Loss: -20.209213256835938, Learning Rate: 0.01\n",
      "Epoch [14128/20000], Loss: -20.210861206054688, Learning Rate: 0.01\n",
      "Epoch [14129/20000], Loss: -20.212188720703125, Learning Rate: 0.01\n",
      "Epoch [14130/20000], Loss: -20.213623046875, Learning Rate: 0.01\n",
      "Epoch [14131/20000], Loss: -20.21484375, Learning Rate: 0.01\n",
      "Epoch [14132/20000], Loss: -20.216079711914062, Learning Rate: 0.01\n",
      "Epoch [14133/20000], Loss: -20.2174072265625, Learning Rate: 0.01\n",
      "Epoch [14134/20000], Loss: -20.218704223632812, Learning Rate: 0.01\n",
      "Epoch [14135/20000], Loss: -20.219940185546875, Learning Rate: 0.01\n",
      "Epoch [14136/20000], Loss: -20.221389770507812, Learning Rate: 0.01\n",
      "Epoch [14137/20000], Loss: -20.222747802734375, Learning Rate: 0.01\n",
      "Epoch [14138/20000], Loss: -20.22412109375, Learning Rate: 0.01\n",
      "Epoch [14139/20000], Loss: -20.22540283203125, Learning Rate: 0.01\n",
      "Epoch [14140/20000], Loss: -20.22674560546875, Learning Rate: 0.01\n",
      "Epoch [14141/20000], Loss: -20.228179931640625, Learning Rate: 0.01\n",
      "Epoch [14142/20000], Loss: -20.229446411132812, Learning Rate: 0.01\n",
      "Epoch [14143/20000], Loss: -20.230682373046875, Learning Rate: 0.01\n",
      "Epoch [14144/20000], Loss: -20.232086181640625, Learning Rate: 0.01\n",
      "Epoch [14145/20000], Loss: -20.233322143554688, Learning Rate: 0.01\n",
      "Epoch [14146/20000], Loss: -20.234603881835938, Learning Rate: 0.01\n",
      "Epoch [14147/20000], Loss: -20.2359619140625, Learning Rate: 0.01\n",
      "Epoch [14148/20000], Loss: -20.237319946289062, Learning Rate: 0.01\n",
      "Epoch [14149/20000], Loss: -20.238662719726562, Learning Rate: 0.01\n",
      "Epoch [14150/20000], Loss: -20.239959716796875, Learning Rate: 0.01\n",
      "Epoch [14151/20000], Loss: -20.241180419921875, Learning Rate: 0.01\n",
      "Epoch [14152/20000], Loss: -20.2425537109375, Learning Rate: 0.01\n",
      "Epoch [14153/20000], Loss: -20.243911743164062, Learning Rate: 0.01\n",
      "Epoch [14154/20000], Loss: -20.245193481445312, Learning Rate: 0.01\n",
      "Epoch [14155/20000], Loss: -20.246475219726562, Learning Rate: 0.01\n",
      "Epoch [14156/20000], Loss: -20.247787475585938, Learning Rate: 0.01\n",
      "Epoch [14157/20000], Loss: -20.24908447265625, Learning Rate: 0.01\n",
      "Epoch [14158/20000], Loss: -20.2503662109375, Learning Rate: 0.01\n",
      "Epoch [14159/20000], Loss: -20.25164794921875, Learning Rate: 0.01\n",
      "Epoch [14160/20000], Loss: -20.252944946289062, Learning Rate: 0.01\n",
      "Epoch [14161/20000], Loss: -20.254318237304688, Learning Rate: 0.01\n",
      "Epoch [14162/20000], Loss: -20.255523681640625, Learning Rate: 0.01\n",
      "Epoch [14163/20000], Loss: -20.256866455078125, Learning Rate: 0.01\n",
      "Epoch [14164/20000], Loss: -20.258163452148438, Learning Rate: 0.01\n",
      "Epoch [14165/20000], Loss: -20.259414672851562, Learning Rate: 0.01\n",
      "Epoch [14166/20000], Loss: -20.260787963867188, Learning Rate: 0.01\n",
      "Epoch [14167/20000], Loss: -20.262008666992188, Learning Rate: 0.01\n",
      "Epoch [14168/20000], Loss: -20.263381958007812, Learning Rate: 0.01\n",
      "Epoch [14169/20000], Loss: -20.26470947265625, Learning Rate: 0.01\n",
      "Epoch [14170/20000], Loss: -20.265914916992188, Learning Rate: 0.01\n",
      "Epoch [14171/20000], Loss: -20.26715087890625, Learning Rate: 0.01\n",
      "Epoch [14172/20000], Loss: -20.268447875976562, Learning Rate: 0.01\n",
      "Epoch [14173/20000], Loss: -20.269744873046875, Learning Rate: 0.01\n",
      "Epoch [14174/20000], Loss: -20.271041870117188, Learning Rate: 0.01\n",
      "Epoch [14175/20000], Loss: -20.272323608398438, Learning Rate: 0.01\n",
      "Epoch [14176/20000], Loss: -20.273651123046875, Learning Rate: 0.01\n",
      "Epoch [14177/20000], Loss: -20.274856567382812, Learning Rate: 0.01\n",
      "Epoch [14178/20000], Loss: -20.276199340820312, Learning Rate: 0.01\n",
      "Epoch [14179/20000], Loss: -20.277542114257812, Learning Rate: 0.01\n",
      "Epoch [14180/20000], Loss: -20.278732299804688, Learning Rate: 0.01\n",
      "Epoch [14181/20000], Loss: -20.280075073242188, Learning Rate: 0.01\n",
      "Epoch [14182/20000], Loss: -20.281280517578125, Learning Rate: 0.01\n",
      "Epoch [14183/20000], Loss: -20.282608032226562, Learning Rate: 0.01\n",
      "Epoch [14184/20000], Loss: -20.283798217773438, Learning Rate: 0.01\n",
      "Epoch [14185/20000], Loss: -20.285140991210938, Learning Rate: 0.01\n",
      "Epoch [14186/20000], Loss: -20.286407470703125, Learning Rate: 0.01\n",
      "Epoch [14187/20000], Loss: -20.287643432617188, Learning Rate: 0.01\n",
      "Epoch [14188/20000], Loss: -20.288909912109375, Learning Rate: 0.01\n",
      "Epoch [14189/20000], Loss: -20.290084838867188, Learning Rate: 0.01\n",
      "Epoch [14190/20000], Loss: -20.29144287109375, Learning Rate: 0.01\n",
      "Epoch [14191/20000], Loss: -20.292694091796875, Learning Rate: 0.01\n",
      "Epoch [14192/20000], Loss: -20.293960571289062, Learning Rate: 0.01\n",
      "Epoch [14193/20000], Loss: -20.295196533203125, Learning Rate: 0.01\n",
      "Epoch [14194/20000], Loss: -20.29644775390625, Learning Rate: 0.01\n",
      "Epoch [14195/20000], Loss: -20.29766845703125, Learning Rate: 0.01\n",
      "Epoch [14196/20000], Loss: -20.298858642578125, Learning Rate: 0.01\n",
      "Epoch [14197/20000], Loss: -20.300094604492188, Learning Rate: 0.01\n",
      "Epoch [14198/20000], Loss: -20.301101684570312, Learning Rate: 0.01\n",
      "Epoch [14199/20000], Loss: -20.302337646484375, Learning Rate: 0.01\n",
      "Epoch [14200/20000], Loss: -20.303436279296875, Learning Rate: 0.01\n",
      "Epoch [14201/20000], Loss: -20.304412841796875, Learning Rate: 0.01\n",
      "Epoch [14202/20000], Loss: -20.3052978515625, Learning Rate: 0.01\n",
      "Epoch [14203/20000], Loss: -20.30609130859375, Learning Rate: 0.01\n",
      "Epoch [14204/20000], Loss: -20.306655883789062, Learning Rate: 0.01\n",
      "Epoch [14205/20000], Loss: -20.306991577148438, Learning Rate: 0.01\n",
      "Epoch [14206/20000], Loss: -20.30682373046875, Learning Rate: 0.01\n",
      "Epoch [14207/20000], Loss: -20.306228637695312, Learning Rate: 0.01\n",
      "Epoch [14208/20000], Loss: -20.304794311523438, Learning Rate: 0.01\n",
      "Epoch [14209/20000], Loss: -20.302200317382812, Learning Rate: 0.01\n",
      "Epoch [14210/20000], Loss: -20.297805786132812, Learning Rate: 0.01\n",
      "Epoch [14211/20000], Loss: -20.291046142578125, Learning Rate: 0.01\n",
      "Epoch [14212/20000], Loss: -20.280776977539062, Learning Rate: 0.01\n",
      "Epoch [14213/20000], Loss: -20.265304565429688, Learning Rate: 0.01\n",
      "Epoch [14214/20000], Loss: -20.242462158203125, Learning Rate: 0.01\n",
      "Epoch [14215/20000], Loss: -20.209075927734375, Learning Rate: 0.01\n",
      "Epoch [14216/20000], Loss: -20.16046142578125, Learning Rate: 0.01\n",
      "Epoch [14217/20000], Loss: -20.091888427734375, Learning Rate: 0.01\n",
      "Epoch [14218/20000], Loss: -19.996612548828125, Learning Rate: 0.01\n",
      "Epoch [14219/20000], Loss: -19.869598388671875, Learning Rate: 0.01\n",
      "Epoch [14220/20000], Loss: -19.709213256835938, Learning Rate: 0.01\n",
      "Epoch [14221/20000], Loss: -19.525436401367188, Learning Rate: 0.01\n",
      "Epoch [14222/20000], Loss: -19.344467163085938, Learning Rate: 0.01\n",
      "Epoch [14223/20000], Loss: -19.218307495117188, Learning Rate: 0.01\n",
      "Epoch [14224/20000], Loss: -19.207550048828125, Learning Rate: 0.01\n",
      "Epoch [14225/20000], Loss: -19.358428955078125, Learning Rate: 0.01\n",
      "Epoch [14226/20000], Loss: -19.649337768554688, Learning Rate: 0.01\n",
      "Epoch [14227/20000], Loss: -19.986495971679688, Learning Rate: 0.01\n",
      "Epoch [14228/20000], Loss: -20.242202758789062, Learning Rate: 0.01\n",
      "Epoch [14229/20000], Loss: -20.334197998046875, Learning Rate: 0.01\n",
      "Epoch [14230/20000], Loss: -20.267196655273438, Learning Rate: 0.01\n",
      "Epoch [14231/20000], Loss: -20.118637084960938, Learning Rate: 0.01\n",
      "Epoch [14232/20000], Loss: -19.990066528320312, Learning Rate: 0.01\n",
      "Epoch [14233/20000], Loss: -19.956024169921875, Learning Rate: 0.01\n",
      "Epoch [14234/20000], Loss: -20.03277587890625, Learning Rate: 0.01\n",
      "Epoch [14235/20000], Loss: -20.171798706054688, Learning Rate: 0.01\n",
      "Epoch [14236/20000], Loss: -20.29620361328125, Learning Rate: 0.01\n",
      "Epoch [14237/20000], Loss: -20.34747314453125, Learning Rate: 0.01\n",
      "Epoch [14238/20000], Loss: -20.31878662109375, Learning Rate: 0.01\n",
      "Epoch [14239/20000], Loss: -20.249267578125, Learning Rate: 0.01\n",
      "Epoch [14240/20000], Loss: -20.193328857421875, Learning Rate: 0.01\n",
      "Epoch [14241/20000], Loss: -20.18756103515625, Learning Rate: 0.01\n",
      "Epoch [14242/20000], Loss: -20.232742309570312, Learning Rate: 0.01\n",
      "Epoch [14243/20000], Loss: -20.298797607421875, Learning Rate: 0.01\n",
      "Epoch [14244/20000], Loss: -20.348159790039062, Learning Rate: 0.01\n",
      "Epoch [14245/20000], Loss: -20.359481811523438, Learning Rate: 0.01\n",
      "Epoch [14246/20000], Loss: -20.33758544921875, Learning Rate: 0.01\n",
      "Epoch [14247/20000], Loss: -20.305328369140625, Learning Rate: 0.01\n",
      "Epoch [14248/20000], Loss: -20.287368774414062, Learning Rate: 0.01\n",
      "Epoch [14249/20000], Loss: -20.294525146484375, Learning Rate: 0.01\n",
      "Epoch [14250/20000], Loss: -20.321044921875, Learning Rate: 0.01\n",
      "Epoch [14251/20000], Loss: -20.350341796875, Learning Rate: 0.01\n",
      "Epoch [14252/20000], Loss: -20.3673095703125, Learning Rate: 0.01\n",
      "Epoch [14253/20000], Loss: -20.36669921875, Learning Rate: 0.01\n",
      "Epoch [14254/20000], Loss: -20.354446411132812, Learning Rate: 0.01\n",
      "Epoch [14255/20000], Loss: -20.342041015625, Learning Rate: 0.01\n",
      "Epoch [14256/20000], Loss: -20.338699340820312, Learning Rate: 0.01\n",
      "Epoch [14257/20000], Loss: -20.346359252929688, Learning Rate: 0.01\n",
      "Epoch [14258/20000], Loss: -20.360260009765625, Learning Rate: 0.01\n",
      "Epoch [14259/20000], Loss: -20.372726440429688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14260/20000], Loss: -20.37841796875, Learning Rate: 0.01\n",
      "Epoch [14261/20000], Loss: -20.376754760742188, Learning Rate: 0.01\n",
      "Epoch [14262/20000], Loss: -20.371429443359375, Learning Rate: 0.01\n",
      "Epoch [14263/20000], Loss: -20.367279052734375, Learning Rate: 0.01\n",
      "Epoch [14264/20000], Loss: -20.367416381835938, Learning Rate: 0.01\n",
      "Epoch [14265/20000], Loss: -20.372116088867188, Learning Rate: 0.01\n",
      "Epoch [14266/20000], Loss: -20.379196166992188, Learning Rate: 0.01\n",
      "Epoch [14267/20000], Loss: -20.384994506835938, Learning Rate: 0.01\n",
      "Epoch [14268/20000], Loss: -20.38787841796875, Learning Rate: 0.01\n",
      "Epoch [14269/20000], Loss: -20.3876953125, Learning Rate: 0.01\n",
      "Epoch [14270/20000], Loss: -20.386077880859375, Learning Rate: 0.01\n",
      "Epoch [14271/20000], Loss: -20.384857177734375, Learning Rate: 0.01\n",
      "Epoch [14272/20000], Loss: -20.385665893554688, Learning Rate: 0.01\n",
      "Epoch [14273/20000], Loss: -20.388351440429688, Learning Rate: 0.01\n",
      "Epoch [14274/20000], Loss: -20.391891479492188, Learning Rate: 0.01\n",
      "Epoch [14275/20000], Loss: -20.395111083984375, Learning Rate: 0.01\n",
      "Epoch [14276/20000], Loss: -20.397186279296875, Learning Rate: 0.01\n",
      "Epoch [14277/20000], Loss: -20.397933959960938, Learning Rate: 0.01\n",
      "Epoch [14278/20000], Loss: -20.3978271484375, Learning Rate: 0.01\n",
      "Epoch [14279/20000], Loss: -20.3980712890625, Learning Rate: 0.01\n",
      "Epoch [14280/20000], Loss: -20.398818969726562, Learning Rate: 0.01\n",
      "Epoch [14281/20000], Loss: -20.400222778320312, Learning Rate: 0.01\n",
      "Epoch [14282/20000], Loss: -20.4024658203125, Learning Rate: 0.01\n",
      "Epoch [14283/20000], Loss: -20.40447998046875, Learning Rate: 0.01\n",
      "Epoch [14284/20000], Loss: -20.406356811523438, Learning Rate: 0.01\n",
      "Epoch [14285/20000], Loss: -20.407501220703125, Learning Rate: 0.01\n",
      "Epoch [14286/20000], Loss: -20.408309936523438, Learning Rate: 0.01\n",
      "Epoch [14287/20000], Loss: -20.409042358398438, Learning Rate: 0.01\n",
      "Epoch [14288/20000], Loss: -20.409744262695312, Learning Rate: 0.01\n",
      "Epoch [14289/20000], Loss: -20.410919189453125, Learning Rate: 0.01\n",
      "Epoch [14290/20000], Loss: -20.412322998046875, Learning Rate: 0.01\n",
      "Epoch [14291/20000], Loss: -20.413787841796875, Learning Rate: 0.01\n",
      "Epoch [14292/20000], Loss: -20.4154052734375, Learning Rate: 0.01\n",
      "Epoch [14293/20000], Loss: -20.416732788085938, Learning Rate: 0.01\n",
      "Epoch [14294/20000], Loss: -20.417861938476562, Learning Rate: 0.01\n",
      "Epoch [14295/20000], Loss: -20.418853759765625, Learning Rate: 0.01\n",
      "Epoch [14296/20000], Loss: -20.419754028320312, Learning Rate: 0.01\n",
      "Epoch [14297/20000], Loss: -20.420761108398438, Learning Rate: 0.01\n",
      "Epoch [14298/20000], Loss: -20.421829223632812, Learning Rate: 0.01\n",
      "Epoch [14299/20000], Loss: -20.42303466796875, Learning Rate: 0.01\n",
      "Epoch [14300/20000], Loss: -20.42437744140625, Learning Rate: 0.01\n",
      "Epoch [14301/20000], Loss: -20.425628662109375, Learning Rate: 0.01\n",
      "Epoch [14302/20000], Loss: -20.426864624023438, Learning Rate: 0.01\n",
      "Epoch [14303/20000], Loss: -20.428009033203125, Learning Rate: 0.01\n",
      "Epoch [14304/20000], Loss: -20.4290771484375, Learning Rate: 0.01\n",
      "Epoch [14305/20000], Loss: -20.430099487304688, Learning Rate: 0.01\n",
      "Epoch [14306/20000], Loss: -20.431198120117188, Learning Rate: 0.01\n",
      "Epoch [14307/20000], Loss: -20.432342529296875, Learning Rate: 0.01\n",
      "Epoch [14308/20000], Loss: -20.433486938476562, Learning Rate: 0.01\n",
      "Epoch [14309/20000], Loss: -20.434646606445312, Learning Rate: 0.01\n",
      "Epoch [14310/20000], Loss: -20.435745239257812, Learning Rate: 0.01\n",
      "Epoch [14311/20000], Loss: -20.436920166015625, Learning Rate: 0.01\n",
      "Epoch [14312/20000], Loss: -20.43804931640625, Learning Rate: 0.01\n",
      "Epoch [14313/20000], Loss: -20.439178466796875, Learning Rate: 0.01\n",
      "Epoch [14314/20000], Loss: -20.440353393554688, Learning Rate: 0.01\n",
      "Epoch [14315/20000], Loss: -20.441452026367188, Learning Rate: 0.01\n",
      "Epoch [14316/20000], Loss: -20.442550659179688, Learning Rate: 0.01\n",
      "Epoch [14317/20000], Loss: -20.443603515625, Learning Rate: 0.01\n",
      "Epoch [14318/20000], Loss: -20.444671630859375, Learning Rate: 0.01\n",
      "Epoch [14319/20000], Loss: -20.44586181640625, Learning Rate: 0.01\n",
      "Epoch [14320/20000], Loss: -20.447097778320312, Learning Rate: 0.01\n",
      "Epoch [14321/20000], Loss: -20.448135375976562, Learning Rate: 0.01\n",
      "Epoch [14322/20000], Loss: -20.449249267578125, Learning Rate: 0.01\n",
      "Epoch [14323/20000], Loss: -20.450393676757812, Learning Rate: 0.01\n",
      "Epoch [14324/20000], Loss: -20.45147705078125, Learning Rate: 0.01\n",
      "Epoch [14325/20000], Loss: -20.452606201171875, Learning Rate: 0.01\n",
      "Epoch [14326/20000], Loss: -20.453689575195312, Learning Rate: 0.01\n",
      "Epoch [14327/20000], Loss: -20.454833984375, Learning Rate: 0.01\n",
      "Epoch [14328/20000], Loss: -20.455825805664062, Learning Rate: 0.01\n",
      "Epoch [14329/20000], Loss: -20.45697021484375, Learning Rate: 0.01\n",
      "Epoch [14330/20000], Loss: -20.458145141601562, Learning Rate: 0.01\n",
      "Epoch [14331/20000], Loss: -20.459274291992188, Learning Rate: 0.01\n",
      "Epoch [14332/20000], Loss: -20.460311889648438, Learning Rate: 0.01\n",
      "Epoch [14333/20000], Loss: -20.461395263671875, Learning Rate: 0.01\n",
      "Epoch [14334/20000], Loss: -20.462448120117188, Learning Rate: 0.01\n",
      "Epoch [14335/20000], Loss: -20.463653564453125, Learning Rate: 0.01\n",
      "Epoch [14336/20000], Loss: -20.464752197265625, Learning Rate: 0.01\n",
      "Epoch [14337/20000], Loss: -20.465835571289062, Learning Rate: 0.01\n",
      "Epoch [14338/20000], Loss: -20.466873168945312, Learning Rate: 0.01\n",
      "Epoch [14339/20000], Loss: -20.468032836914062, Learning Rate: 0.01\n",
      "Epoch [14340/20000], Loss: -20.469039916992188, Learning Rate: 0.01\n",
      "Epoch [14341/20000], Loss: -20.470184326171875, Learning Rate: 0.01\n",
      "Epoch [14342/20000], Loss: -20.47125244140625, Learning Rate: 0.01\n",
      "Epoch [14343/20000], Loss: -20.472396850585938, Learning Rate: 0.01\n",
      "Epoch [14344/20000], Loss: -20.473358154296875, Learning Rate: 0.01\n",
      "Epoch [14345/20000], Loss: -20.474578857421875, Learning Rate: 0.01\n",
      "Epoch [14346/20000], Loss: -20.47564697265625, Learning Rate: 0.01\n",
      "Epoch [14347/20000], Loss: -20.476730346679688, Learning Rate: 0.01\n",
      "Epoch [14348/20000], Loss: -20.477859497070312, Learning Rate: 0.01\n",
      "Epoch [14349/20000], Loss: -20.478988647460938, Learning Rate: 0.01\n",
      "Epoch [14350/20000], Loss: -20.480026245117188, Learning Rate: 0.01\n",
      "Epoch [14351/20000], Loss: -20.48114013671875, Learning Rate: 0.01\n",
      "Epoch [14352/20000], Loss: -20.482147216796875, Learning Rate: 0.01\n",
      "Epoch [14353/20000], Loss: -20.483230590820312, Learning Rate: 0.01\n",
      "Epoch [14354/20000], Loss: -20.484344482421875, Learning Rate: 0.01\n",
      "Epoch [14355/20000], Loss: -20.485336303710938, Learning Rate: 0.01\n",
      "Epoch [14356/20000], Loss: -20.48651123046875, Learning Rate: 0.01\n",
      "Epoch [14357/20000], Loss: -20.487548828125, Learning Rate: 0.01\n",
      "Epoch [14358/20000], Loss: -20.488662719726562, Learning Rate: 0.01\n",
      "Epoch [14359/20000], Loss: -20.489715576171875, Learning Rate: 0.01\n",
      "Epoch [14360/20000], Loss: -20.490692138671875, Learning Rate: 0.01\n",
      "Epoch [14361/20000], Loss: -20.491836547851562, Learning Rate: 0.01\n",
      "Epoch [14362/20000], Loss: -20.492904663085938, Learning Rate: 0.01\n",
      "Epoch [14363/20000], Loss: -20.493850708007812, Learning Rate: 0.01\n",
      "Epoch [14364/20000], Loss: -20.495025634765625, Learning Rate: 0.01\n",
      "Epoch [14365/20000], Loss: -20.496139526367188, Learning Rate: 0.01\n",
      "Epoch [14366/20000], Loss: -20.497085571289062, Learning Rate: 0.01\n",
      "Epoch [14367/20000], Loss: -20.4981689453125, Learning Rate: 0.01\n",
      "Epoch [14368/20000], Loss: -20.4991455078125, Learning Rate: 0.01\n",
      "Epoch [14369/20000], Loss: -20.5001220703125, Learning Rate: 0.01\n",
      "Epoch [14370/20000], Loss: -20.500946044921875, Learning Rate: 0.01\n",
      "Epoch [14371/20000], Loss: -20.501968383789062, Learning Rate: 0.01\n",
      "Epoch [14372/20000], Loss: -20.50274658203125, Learning Rate: 0.01\n",
      "Epoch [14373/20000], Loss: -20.50360107421875, Learning Rate: 0.01\n",
      "Epoch [14374/20000], Loss: -20.5042724609375, Learning Rate: 0.01\n",
      "Epoch [14375/20000], Loss: -20.5047607421875, Learning Rate: 0.01\n",
      "Epoch [14376/20000], Loss: -20.50506591796875, Learning Rate: 0.01\n",
      "Epoch [14377/20000], Loss: -20.50506591796875, Learning Rate: 0.01\n",
      "Epoch [14378/20000], Loss: -20.504653930664062, Learning Rate: 0.01\n",
      "Epoch [14379/20000], Loss: -20.503646850585938, Learning Rate: 0.01\n",
      "Epoch [14380/20000], Loss: -20.501708984375, Learning Rate: 0.01\n",
      "Epoch [14381/20000], Loss: -20.49859619140625, Learning Rate: 0.01\n",
      "Epoch [14382/20000], Loss: -20.493682861328125, Learning Rate: 0.01\n",
      "Epoch [14383/20000], Loss: -20.486328125, Learning Rate: 0.01\n",
      "Epoch [14384/20000], Loss: -20.475509643554688, Learning Rate: 0.01\n",
      "Epoch [14385/20000], Loss: -20.4593505859375, Learning Rate: 0.01\n",
      "Epoch [14386/20000], Loss: -20.435989379882812, Learning Rate: 0.01\n",
      "Epoch [14387/20000], Loss: -20.4024658203125, Learning Rate: 0.01\n",
      "Epoch [14388/20000], Loss: -20.355270385742188, Learning Rate: 0.01\n",
      "Epoch [14389/20000], Loss: -20.289016723632812, Learning Rate: 0.01\n",
      "Epoch [14390/20000], Loss: -20.199356079101562, Learning Rate: 0.01\n",
      "Epoch [14391/20000], Loss: -20.080078125, Learning Rate: 0.01\n",
      "Epoch [14392/20000], Loss: -19.932235717773438, Learning Rate: 0.01\n",
      "Epoch [14393/20000], Loss: -19.75885009765625, Learning Rate: 0.01\n",
      "Epoch [14394/20000], Loss: -19.585769653320312, Learning Rate: 0.01\n",
      "Epoch [14395/20000], Loss: -19.444976806640625, Learning Rate: 0.01\n",
      "Epoch [14396/20000], Loss: -19.398880004882812, Learning Rate: 0.01\n",
      "Epoch [14397/20000], Loss: -19.483489990234375, Learning Rate: 0.01\n",
      "Epoch [14398/20000], Loss: -19.714431762695312, Learning Rate: 0.01\n",
      "Epoch [14399/20000], Loss: -20.025787353515625, Learning Rate: 0.01\n",
      "Epoch [14400/20000], Loss: -20.317855834960938, Learning Rate: 0.01\n",
      "Epoch [14401/20000], Loss: -20.49176025390625, Learning Rate: 0.01\n",
      "Epoch [14402/20000], Loss: -20.511734008789062, Learning Rate: 0.01\n",
      "Epoch [14403/20000], Loss: -20.412857055664062, Learning Rate: 0.01\n",
      "Epoch [14404/20000], Loss: -20.274490356445312, Learning Rate: 0.01\n",
      "Epoch [14405/20000], Loss: -20.180908203125, Learning Rate: 0.01\n",
      "Epoch [14406/20000], Loss: -20.179702758789062, Learning Rate: 0.01\n",
      "Epoch [14407/20000], Loss: -20.270187377929688, Learning Rate: 0.01\n",
      "Epoch [14408/20000], Loss: -20.400421142578125, Learning Rate: 0.01\n",
      "Epoch [14409/20000], Loss: -20.506088256835938, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14410/20000], Loss: -20.543853759765625, Learning Rate: 0.01\n",
      "Epoch [14411/20000], Loss: -20.513641357421875, Learning Rate: 0.01\n",
      "Epoch [14412/20000], Loss: -20.450881958007812, Learning Rate: 0.01\n",
      "Epoch [14413/20000], Loss: -20.400543212890625, Learning Rate: 0.01\n",
      "Epoch [14414/20000], Loss: -20.393463134765625, Learning Rate: 0.01\n",
      "Epoch [14415/20000], Loss: -20.430145263671875, Learning Rate: 0.01\n",
      "Epoch [14416/20000], Loss: -20.487777709960938, Learning Rate: 0.01\n",
      "Epoch [14417/20000], Loss: -20.534774780273438, Learning Rate: 0.01\n",
      "Epoch [14418/20000], Loss: -20.55126953125, Learning Rate: 0.01\n",
      "Epoch [14419/20000], Loss: -20.537567138671875, Learning Rate: 0.01\n",
      "Epoch [14420/20000], Loss: -20.510757446289062, Learning Rate: 0.01\n",
      "Epoch [14421/20000], Loss: -20.49114990234375, Learning Rate: 0.01\n",
      "Epoch [14422/20000], Loss: -20.491226196289062, Learning Rate: 0.01\n",
      "Epoch [14423/20000], Loss: -20.51007080078125, Learning Rate: 0.01\n",
      "Epoch [14424/20000], Loss: -20.535888671875, Learning Rate: 0.01\n",
      "Epoch [14425/20000], Loss: -20.555389404296875, Learning Rate: 0.01\n",
      "Epoch [14426/20000], Loss: -20.560928344726562, Learning Rate: 0.01\n",
      "Epoch [14427/20000], Loss: -20.5538330078125, Learning Rate: 0.01\n",
      "Epoch [14428/20000], Loss: -20.542068481445312, Learning Rate: 0.01\n",
      "Epoch [14429/20000], Loss: -20.534530639648438, Learning Rate: 0.01\n",
      "Epoch [14430/20000], Loss: -20.535964965820312, Learning Rate: 0.01\n",
      "Epoch [14431/20000], Loss: -20.54522705078125, Learning Rate: 0.01\n",
      "Epoch [14432/20000], Loss: -20.557174682617188, Learning Rate: 0.01\n",
      "Epoch [14433/20000], Loss: -20.565948486328125, Learning Rate: 0.01\n",
      "Epoch [14434/20000], Loss: -20.568466186523438, Learning Rate: 0.01\n",
      "Epoch [14435/20000], Loss: -20.565933227539062, Learning Rate: 0.01\n",
      "Epoch [14436/20000], Loss: -20.561508178710938, Learning Rate: 0.01\n",
      "Epoch [14437/20000], Loss: -20.558868408203125, Learning Rate: 0.01\n",
      "Epoch [14438/20000], Loss: -20.560165405273438, Learning Rate: 0.01\n",
      "Epoch [14439/20000], Loss: -20.564773559570312, Learning Rate: 0.01\n",
      "Epoch [14440/20000], Loss: -20.57037353515625, Learning Rate: 0.01\n",
      "Epoch [14441/20000], Loss: -20.5748291015625, Learning Rate: 0.01\n",
      "Epoch [14442/20000], Loss: -20.576889038085938, Learning Rate: 0.01\n",
      "Epoch [14443/20000], Loss: -20.5765380859375, Learning Rate: 0.01\n",
      "Epoch [14444/20000], Loss: -20.575180053710938, Learning Rate: 0.01\n",
      "Epoch [14445/20000], Loss: -20.574447631835938, Learning Rate: 0.01\n",
      "Epoch [14446/20000], Loss: -20.57525634765625, Learning Rate: 0.01\n",
      "Epoch [14447/20000], Loss: -20.577529907226562, Learning Rate: 0.01\n",
      "Epoch [14448/20000], Loss: -20.580535888671875, Learning Rate: 0.01\n",
      "Epoch [14449/20000], Loss: -20.582962036132812, Learning Rate: 0.01\n",
      "Epoch [14450/20000], Loss: -20.584716796875, Learning Rate: 0.01\n",
      "Epoch [14451/20000], Loss: -20.585342407226562, Learning Rate: 0.01\n",
      "Epoch [14452/20000], Loss: -20.585372924804688, Learning Rate: 0.01\n",
      "Epoch [14453/20000], Loss: -20.585403442382812, Learning Rate: 0.01\n",
      "Epoch [14454/20000], Loss: -20.586090087890625, Learning Rate: 0.01\n",
      "Epoch [14455/20000], Loss: -20.587310791015625, Learning Rate: 0.01\n",
      "Epoch [14456/20000], Loss: -20.589080810546875, Learning Rate: 0.01\n",
      "Epoch [14457/20000], Loss: -20.590866088867188, Learning Rate: 0.01\n",
      "Epoch [14458/20000], Loss: -20.592193603515625, Learning Rate: 0.01\n",
      "Epoch [14459/20000], Loss: -20.59332275390625, Learning Rate: 0.01\n",
      "Epoch [14460/20000], Loss: -20.593856811523438, Learning Rate: 0.01\n",
      "Epoch [14461/20000], Loss: -20.594436645507812, Learning Rate: 0.01\n",
      "Epoch [14462/20000], Loss: -20.59503173828125, Learning Rate: 0.01\n",
      "Epoch [14463/20000], Loss: -20.595962524414062, Learning Rate: 0.01\n",
      "Epoch [14464/20000], Loss: -20.597183227539062, Learning Rate: 0.01\n",
      "Epoch [14465/20000], Loss: -20.598419189453125, Learning Rate: 0.01\n",
      "Epoch [14466/20000], Loss: -20.599578857421875, Learning Rate: 0.01\n",
      "Epoch [14467/20000], Loss: -20.600692749023438, Learning Rate: 0.01\n",
      "Epoch [14468/20000], Loss: -20.60162353515625, Learning Rate: 0.01\n",
      "Epoch [14469/20000], Loss: -20.602386474609375, Learning Rate: 0.01\n",
      "Epoch [14470/20000], Loss: -20.60302734375, Learning Rate: 0.01\n",
      "Epoch [14471/20000], Loss: -20.603897094726562, Learning Rate: 0.01\n",
      "Epoch [14472/20000], Loss: -20.604721069335938, Learning Rate: 0.01\n",
      "Epoch [14473/20000], Loss: -20.605606079101562, Learning Rate: 0.01\n",
      "Epoch [14474/20000], Loss: -20.60650634765625, Learning Rate: 0.01\n",
      "Epoch [14475/20000], Loss: -20.607376098632812, Learning Rate: 0.01\n",
      "Epoch [14476/20000], Loss: -20.608154296875, Learning Rate: 0.01\n",
      "Epoch [14477/20000], Loss: -20.608795166015625, Learning Rate: 0.01\n",
      "Epoch [14478/20000], Loss: -20.609054565429688, Learning Rate: 0.01\n",
      "Epoch [14479/20000], Loss: -20.6092529296875, Learning Rate: 0.01\n",
      "Epoch [14480/20000], Loss: -20.609176635742188, Learning Rate: 0.01\n",
      "Epoch [14481/20000], Loss: -20.608779907226562, Learning Rate: 0.01\n",
      "Epoch [14482/20000], Loss: -20.607864379882812, Learning Rate: 0.01\n",
      "Epoch [14483/20000], Loss: -20.606216430664062, Learning Rate: 0.01\n",
      "Epoch [14484/20000], Loss: -20.603530883789062, Learning Rate: 0.01\n",
      "Epoch [14485/20000], Loss: -20.599197387695312, Learning Rate: 0.01\n",
      "Epoch [14486/20000], Loss: -20.592727661132812, Learning Rate: 0.01\n",
      "Epoch [14487/20000], Loss: -20.582977294921875, Learning Rate: 0.01\n",
      "Epoch [14488/20000], Loss: -20.568756103515625, Learning Rate: 0.01\n",
      "Epoch [14489/20000], Loss: -20.548004150390625, Learning Rate: 0.01\n",
      "Epoch [14490/20000], Loss: -20.518020629882812, Learning Rate: 0.01\n",
      "Epoch [14491/20000], Loss: -20.475296020507812, Learning Rate: 0.01\n",
      "Epoch [14492/20000], Loss: -20.415359497070312, Learning Rate: 0.01\n",
      "Epoch [14493/20000], Loss: -20.33270263671875, Learning Rate: 0.01\n",
      "Epoch [14494/20000], Loss: -20.2232666015625, Learning Rate: 0.01\n",
      "Epoch [14495/20000], Loss: -20.084640502929688, Learning Rate: 0.01\n",
      "Epoch [14496/20000], Loss: -19.924636840820312, Learning Rate: 0.01\n",
      "Epoch [14497/20000], Loss: -19.762466430664062, Learning Rate: 0.01\n",
      "Epoch [14498/20000], Loss: -19.640228271484375, Learning Rate: 0.01\n",
      "Epoch [14499/20000], Loss: -19.6080322265625, Learning Rate: 0.01\n",
      "Epoch [14500/20000], Loss: -19.712158203125, Learning Rate: 0.01\n",
      "Epoch [14501/20000], Loss: -19.946029663085938, Learning Rate: 0.01\n",
      "Epoch [14502/20000], Loss: -20.2447509765625, Learning Rate: 0.01\n",
      "Epoch [14503/20000], Loss: -20.499725341796875, Learning Rate: 0.01\n",
      "Epoch [14504/20000], Loss: -20.626205444335938, Learning Rate: 0.01\n",
      "Epoch [14505/20000], Loss: -20.606201171875, Learning Rate: 0.01\n",
      "Epoch [14506/20000], Loss: -20.490066528320312, Learning Rate: 0.01\n",
      "Epoch [14507/20000], Loss: -20.360885620117188, Learning Rate: 0.01\n",
      "Epoch [14508/20000], Loss: -20.292999267578125, Learning Rate: 0.01\n",
      "Epoch [14509/20000], Loss: -20.320831298828125, Learning Rate: 0.01\n",
      "Epoch [14510/20000], Loss: -20.424148559570312, Learning Rate: 0.01\n",
      "Epoch [14511/20000], Loss: -20.5460205078125, Learning Rate: 0.01\n",
      "Epoch [14512/20000], Loss: -20.626358032226562, Learning Rate: 0.01\n",
      "Epoch [14513/20000], Loss: -20.63739013671875, Learning Rate: 0.01\n",
      "Epoch [14514/20000], Loss: -20.593307495117188, Learning Rate: 0.01\n",
      "Epoch [14515/20000], Loss: -20.534500122070312, Learning Rate: 0.01\n",
      "Epoch [14516/20000], Loss: -20.501190185546875, Learning Rate: 0.01\n",
      "Epoch [14517/20000], Loss: -20.511825561523438, Learning Rate: 0.01\n",
      "Epoch [14518/20000], Loss: -20.5572509765625, Learning Rate: 0.01\n",
      "Epoch [14519/20000], Loss: -20.610092163085938, Learning Rate: 0.01\n",
      "Epoch [14520/20000], Loss: -20.643508911132812, Learning Rate: 0.01\n",
      "Epoch [14521/20000], Loss: -20.64617919921875, Learning Rate: 0.01\n",
      "Epoch [14522/20000], Loss: -20.625930786132812, Learning Rate: 0.01\n",
      "Epoch [14523/20000], Loss: -20.601409912109375, Learning Rate: 0.01\n",
      "Epoch [14524/20000], Loss: -20.589492797851562, Learning Rate: 0.01\n",
      "Epoch [14525/20000], Loss: -20.596878051757812, Learning Rate: 0.01\n",
      "Epoch [14526/20000], Loss: -20.618072509765625, Learning Rate: 0.01\n",
      "Epoch [14527/20000], Loss: -20.641021728515625, Learning Rate: 0.01\n",
      "Epoch [14528/20000], Loss: -20.654495239257812, Learning Rate: 0.01\n",
      "Epoch [14529/20000], Loss: -20.65509033203125, Learning Rate: 0.01\n",
      "Epoch [14530/20000], Loss: -20.646453857421875, Learning Rate: 0.01\n",
      "Epoch [14531/20000], Loss: -20.636688232421875, Learning Rate: 0.01\n",
      "Epoch [14532/20000], Loss: -20.632461547851562, Learning Rate: 0.01\n",
      "Epoch [14533/20000], Loss: -20.636474609375, Learning Rate: 0.01\n",
      "Epoch [14534/20000], Loss: -20.646011352539062, Learning Rate: 0.01\n",
      "Epoch [14535/20000], Loss: -20.65631103515625, Learning Rate: 0.01\n",
      "Epoch [14536/20000], Loss: -20.662704467773438, Learning Rate: 0.01\n",
      "Epoch [14537/20000], Loss: -20.663818359375, Learning Rate: 0.01\n",
      "Epoch [14538/20000], Loss: -20.660980224609375, Learning Rate: 0.01\n",
      "Epoch [14539/20000], Loss: -20.6572265625, Learning Rate: 0.01\n",
      "Epoch [14540/20000], Loss: -20.655426025390625, Learning Rate: 0.01\n",
      "Epoch [14541/20000], Loss: -20.657012939453125, Learning Rate: 0.01\n",
      "Epoch [14542/20000], Loss: -20.66119384765625, Learning Rate: 0.01\n",
      "Epoch [14543/20000], Loss: -20.666244506835938, Learning Rate: 0.01\n",
      "Epoch [14544/20000], Loss: -20.669967651367188, Learning Rate: 0.01\n",
      "Epoch [14545/20000], Loss: -20.671646118164062, Learning Rate: 0.01\n",
      "Epoch [14546/20000], Loss: -20.67138671875, Learning Rate: 0.01\n",
      "Epoch [14547/20000], Loss: -20.670379638671875, Learning Rate: 0.01\n",
      "Epoch [14548/20000], Loss: -20.669662475585938, Learning Rate: 0.01\n",
      "Epoch [14549/20000], Loss: -20.670257568359375, Learning Rate: 0.01\n",
      "Epoch [14550/20000], Loss: -20.671920776367188, Learning Rate: 0.01\n",
      "Epoch [14551/20000], Loss: -20.674346923828125, Learning Rate: 0.01\n",
      "Epoch [14552/20000], Loss: -20.67681884765625, Learning Rate: 0.01\n",
      "Epoch [14553/20000], Loss: -20.678543090820312, Learning Rate: 0.01\n",
      "Epoch [14554/20000], Loss: -20.679473876953125, Learning Rate: 0.01\n",
      "Epoch [14555/20000], Loss: -20.679702758789062, Learning Rate: 0.01\n",
      "Epoch [14556/20000], Loss: -20.679824829101562, Learning Rate: 0.01\n",
      "Epoch [14557/20000], Loss: -20.680206298828125, Learning Rate: 0.01\n",
      "Epoch [14558/20000], Loss: -20.680953979492188, Learning Rate: 0.01\n",
      "Epoch [14559/20000], Loss: -20.68212890625, Learning Rate: 0.01\n",
      "Epoch [14560/20000], Loss: -20.683639526367188, Learning Rate: 0.01\n",
      "Epoch [14561/20000], Loss: -20.685134887695312, Learning Rate: 0.01\n",
      "Epoch [14562/20000], Loss: -20.68634033203125, Learning Rate: 0.01\n",
      "Epoch [14563/20000], Loss: -20.687179565429688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14564/20000], Loss: -20.687850952148438, Learning Rate: 0.01\n",
      "Epoch [14565/20000], Loss: -20.688400268554688, Learning Rate: 0.01\n",
      "Epoch [14566/20000], Loss: -20.68896484375, Learning Rate: 0.01\n",
      "Epoch [14567/20000], Loss: -20.689834594726562, Learning Rate: 0.01\n",
      "Epoch [14568/20000], Loss: -20.690673828125, Learning Rate: 0.01\n",
      "Epoch [14569/20000], Loss: -20.69171142578125, Learning Rate: 0.01\n",
      "Epoch [14570/20000], Loss: -20.692794799804688, Learning Rate: 0.01\n",
      "Epoch [14571/20000], Loss: -20.693817138671875, Learning Rate: 0.01\n",
      "Epoch [14572/20000], Loss: -20.694808959960938, Learning Rate: 0.01\n",
      "Epoch [14573/20000], Loss: -20.695602416992188, Learning Rate: 0.01\n",
      "Epoch [14574/20000], Loss: -20.696395874023438, Learning Rate: 0.01\n",
      "Epoch [14575/20000], Loss: -20.69708251953125, Learning Rate: 0.01\n",
      "Epoch [14576/20000], Loss: -20.697830200195312, Learning Rate: 0.01\n",
      "Epoch [14577/20000], Loss: -20.698654174804688, Learning Rate: 0.01\n",
      "Epoch [14578/20000], Loss: -20.699478149414062, Learning Rate: 0.01\n",
      "Epoch [14579/20000], Loss: -20.700424194335938, Learning Rate: 0.01\n",
      "Epoch [14580/20000], Loss: -20.701202392578125, Learning Rate: 0.01\n",
      "Epoch [14581/20000], Loss: -20.7021484375, Learning Rate: 0.01\n",
      "Epoch [14582/20000], Loss: -20.703048706054688, Learning Rate: 0.01\n",
      "Epoch [14583/20000], Loss: -20.703842163085938, Learning Rate: 0.01\n",
      "Epoch [14584/20000], Loss: -20.7047119140625, Learning Rate: 0.01\n",
      "Epoch [14585/20000], Loss: -20.70550537109375, Learning Rate: 0.01\n",
      "Epoch [14586/20000], Loss: -20.706298828125, Learning Rate: 0.01\n",
      "Epoch [14587/20000], Loss: -20.707077026367188, Learning Rate: 0.01\n",
      "Epoch [14588/20000], Loss: -20.707931518554688, Learning Rate: 0.01\n",
      "Epoch [14589/20000], Loss: -20.708740234375, Learning Rate: 0.01\n",
      "Epoch [14590/20000], Loss: -20.709579467773438, Learning Rate: 0.01\n",
      "Epoch [14591/20000], Loss: -20.710342407226562, Learning Rate: 0.01\n",
      "Epoch [14592/20000], Loss: -20.711349487304688, Learning Rate: 0.01\n",
      "Epoch [14593/20000], Loss: -20.712066650390625, Learning Rate: 0.01\n",
      "Epoch [14594/20000], Loss: -20.712921142578125, Learning Rate: 0.01\n",
      "Epoch [14595/20000], Loss: -20.713623046875, Learning Rate: 0.01\n",
      "Epoch [14596/20000], Loss: -20.714599609375, Learning Rate: 0.01\n",
      "Epoch [14597/20000], Loss: -20.71539306640625, Learning Rate: 0.01\n",
      "Epoch [14598/20000], Loss: -20.716140747070312, Learning Rate: 0.01\n",
      "Epoch [14599/20000], Loss: -20.717041015625, Learning Rate: 0.01\n",
      "Epoch [14600/20000], Loss: -20.717864990234375, Learning Rate: 0.01\n",
      "Epoch [14601/20000], Loss: -20.71856689453125, Learning Rate: 0.01\n",
      "Epoch [14602/20000], Loss: -20.71929931640625, Learning Rate: 0.01\n",
      "Epoch [14603/20000], Loss: -20.720077514648438, Learning Rate: 0.01\n",
      "Epoch [14604/20000], Loss: -20.720993041992188, Learning Rate: 0.01\n",
      "Epoch [14605/20000], Loss: -20.721832275390625, Learning Rate: 0.01\n",
      "Epoch [14606/20000], Loss: -20.722747802734375, Learning Rate: 0.01\n",
      "Epoch [14607/20000], Loss: -20.723403930664062, Learning Rate: 0.01\n",
      "Epoch [14608/20000], Loss: -20.724319458007812, Learning Rate: 0.01\n",
      "Epoch [14609/20000], Loss: -20.725021362304688, Learning Rate: 0.01\n",
      "Epoch [14610/20000], Loss: -20.725936889648438, Learning Rate: 0.01\n",
      "Epoch [14611/20000], Loss: -20.726730346679688, Learning Rate: 0.01\n",
      "Epoch [14612/20000], Loss: -20.727554321289062, Learning Rate: 0.01\n",
      "Epoch [14613/20000], Loss: -20.728240966796875, Learning Rate: 0.01\n",
      "Epoch [14614/20000], Loss: -20.729110717773438, Learning Rate: 0.01\n",
      "Epoch [14615/20000], Loss: -20.729904174804688, Learning Rate: 0.01\n",
      "Epoch [14616/20000], Loss: -20.730728149414062, Learning Rate: 0.01\n",
      "Epoch [14617/20000], Loss: -20.7314453125, Learning Rate: 0.01\n",
      "Epoch [14618/20000], Loss: -20.732284545898438, Learning Rate: 0.01\n",
      "Epoch [14619/20000], Loss: -20.733062744140625, Learning Rate: 0.01\n",
      "Epoch [14620/20000], Loss: -20.733871459960938, Learning Rate: 0.01\n",
      "Epoch [14621/20000], Loss: -20.734573364257812, Learning Rate: 0.01\n",
      "Epoch [14622/20000], Loss: -20.735443115234375, Learning Rate: 0.01\n",
      "Epoch [14623/20000], Loss: -20.7362060546875, Learning Rate: 0.01\n",
      "Epoch [14624/20000], Loss: -20.737060546875, Learning Rate: 0.01\n",
      "Epoch [14625/20000], Loss: -20.737823486328125, Learning Rate: 0.01\n",
      "Epoch [14626/20000], Loss: -20.73858642578125, Learning Rate: 0.01\n",
      "Epoch [14627/20000], Loss: -20.73944091796875, Learning Rate: 0.01\n",
      "Epoch [14628/20000], Loss: -20.74017333984375, Learning Rate: 0.01\n",
      "Epoch [14629/20000], Loss: -20.740966796875, Learning Rate: 0.01\n",
      "Epoch [14630/20000], Loss: -20.741806030273438, Learning Rate: 0.01\n",
      "Epoch [14631/20000], Loss: -20.7425537109375, Learning Rate: 0.01\n",
      "Epoch [14632/20000], Loss: -20.74334716796875, Learning Rate: 0.01\n",
      "Epoch [14633/20000], Loss: -20.744094848632812, Learning Rate: 0.01\n",
      "Epoch [14634/20000], Loss: -20.744918823242188, Learning Rate: 0.01\n",
      "Epoch [14635/20000], Loss: -20.745681762695312, Learning Rate: 0.01\n",
      "Epoch [14636/20000], Loss: -20.746490478515625, Learning Rate: 0.01\n",
      "Epoch [14637/20000], Loss: -20.747238159179688, Learning Rate: 0.01\n",
      "Epoch [14638/20000], Loss: -20.748031616210938, Learning Rate: 0.01\n",
      "Epoch [14639/20000], Loss: -20.748764038085938, Learning Rate: 0.01\n",
      "Epoch [14640/20000], Loss: -20.749603271484375, Learning Rate: 0.01\n",
      "Epoch [14641/20000], Loss: -20.750381469726562, Learning Rate: 0.01\n",
      "Epoch [14642/20000], Loss: -20.751144409179688, Learning Rate: 0.01\n",
      "Epoch [14643/20000], Loss: -20.751953125, Learning Rate: 0.01\n",
      "Epoch [14644/20000], Loss: -20.752731323242188, Learning Rate: 0.01\n",
      "Epoch [14645/20000], Loss: -20.7535400390625, Learning Rate: 0.01\n",
      "Epoch [14646/20000], Loss: -20.754287719726562, Learning Rate: 0.01\n",
      "Epoch [14647/20000], Loss: -20.755035400390625, Learning Rate: 0.01\n",
      "Epoch [14648/20000], Loss: -20.755859375, Learning Rate: 0.01\n",
      "Epoch [14649/20000], Loss: -20.75665283203125, Learning Rate: 0.01\n",
      "Epoch [14650/20000], Loss: -20.757354736328125, Learning Rate: 0.01\n",
      "Epoch [14651/20000], Loss: -20.75811767578125, Learning Rate: 0.01\n",
      "Epoch [14652/20000], Loss: -20.7589111328125, Learning Rate: 0.01\n",
      "Epoch [14653/20000], Loss: -20.759674072265625, Learning Rate: 0.01\n",
      "Epoch [14654/20000], Loss: -20.760452270507812, Learning Rate: 0.01\n",
      "Epoch [14655/20000], Loss: -20.761154174804688, Learning Rate: 0.01\n",
      "Epoch [14656/20000], Loss: -20.762008666992188, Learning Rate: 0.01\n",
      "Epoch [14657/20000], Loss: -20.762725830078125, Learning Rate: 0.01\n",
      "Epoch [14658/20000], Loss: -20.763504028320312, Learning Rate: 0.01\n",
      "Epoch [14659/20000], Loss: -20.7642822265625, Learning Rate: 0.01\n",
      "Epoch [14660/20000], Loss: -20.765090942382812, Learning Rate: 0.01\n",
      "Epoch [14661/20000], Loss: -20.765777587890625, Learning Rate: 0.01\n",
      "Epoch [14662/20000], Loss: -20.766632080078125, Learning Rate: 0.01\n",
      "Epoch [14663/20000], Loss: -20.767303466796875, Learning Rate: 0.01\n",
      "Epoch [14664/20000], Loss: -20.768051147460938, Learning Rate: 0.01\n",
      "Epoch [14665/20000], Loss: -20.768753051757812, Learning Rate: 0.01\n",
      "Epoch [14666/20000], Loss: -20.769561767578125, Learning Rate: 0.01\n",
      "Epoch [14667/20000], Loss: -20.7701416015625, Learning Rate: 0.01\n",
      "Epoch [14668/20000], Loss: -20.77081298828125, Learning Rate: 0.01\n",
      "Epoch [14669/20000], Loss: -20.771560668945312, Learning Rate: 0.01\n",
      "Epoch [14670/20000], Loss: -20.772171020507812, Learning Rate: 0.01\n",
      "Epoch [14671/20000], Loss: -20.772674560546875, Learning Rate: 0.01\n",
      "Epoch [14672/20000], Loss: -20.773193359375, Learning Rate: 0.01\n",
      "Epoch [14673/20000], Loss: -20.773529052734375, Learning Rate: 0.01\n",
      "Epoch [14674/20000], Loss: -20.773574829101562, Learning Rate: 0.01\n",
      "Epoch [14675/20000], Loss: -20.773345947265625, Learning Rate: 0.01\n",
      "Epoch [14676/20000], Loss: -20.772796630859375, Learning Rate: 0.01\n",
      "Epoch [14677/20000], Loss: -20.771408081054688, Learning Rate: 0.01\n",
      "Epoch [14678/20000], Loss: -20.769088745117188, Learning Rate: 0.01\n",
      "Epoch [14679/20000], Loss: -20.765121459960938, Learning Rate: 0.01\n",
      "Epoch [14680/20000], Loss: -20.758712768554688, Learning Rate: 0.01\n",
      "Epoch [14681/20000], Loss: -20.748611450195312, Learning Rate: 0.01\n",
      "Epoch [14682/20000], Loss: -20.732864379882812, Learning Rate: 0.01\n",
      "Epoch [14683/20000], Loss: -20.708633422851562, Learning Rate: 0.01\n",
      "Epoch [14684/20000], Loss: -20.671188354492188, Learning Rate: 0.01\n",
      "Epoch [14685/20000], Loss: -20.614547729492188, Learning Rate: 0.01\n",
      "Epoch [14686/20000], Loss: -20.529098510742188, Learning Rate: 0.01\n",
      "Epoch [14687/20000], Loss: -20.4049072265625, Learning Rate: 0.01\n",
      "Epoch [14688/20000], Loss: -20.228851318359375, Learning Rate: 0.01\n",
      "Epoch [14689/20000], Loss: -19.99847412109375, Learning Rate: 0.01\n",
      "Epoch [14690/20000], Loss: -19.721908569335938, Learning Rate: 0.01\n",
      "Epoch [14691/20000], Loss: -19.458038330078125, Learning Rate: 0.01\n",
      "Epoch [14692/20000], Loss: -19.292266845703125, Learning Rate: 0.01\n",
      "Epoch [14693/20000], Loss: -19.357315063476562, Learning Rate: 0.01\n",
      "Epoch [14694/20000], Loss: -19.684677124023438, Learning Rate: 0.01\n",
      "Epoch [14695/20000], Loss: -20.184371948242188, Learning Rate: 0.01\n",
      "Epoch [14696/20000], Loss: -20.617141723632812, Learning Rate: 0.01\n",
      "Epoch [14697/20000], Loss: -20.791030883789062, Learning Rate: 0.01\n",
      "Epoch [14698/20000], Loss: -20.682022094726562, Learning Rate: 0.01\n",
      "Epoch [14699/20000], Loss: -20.433380126953125, Learning Rate: 0.01\n",
      "Epoch [14700/20000], Loss: -20.2467041015625, Learning Rate: 0.01\n",
      "Epoch [14701/20000], Loss: -20.251876831054688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14702/20000], Loss: -20.44110107421875, Learning Rate: 0.01\n",
      "Epoch [14703/20000], Loss: -20.672271728515625, Learning Rate: 0.01\n",
      "Epoch [14704/20000], Loss: -20.794082641601562, Learning Rate: 0.01\n",
      "Epoch [14705/20000], Loss: -20.755599975585938, Learning Rate: 0.01\n",
      "Epoch [14706/20000], Loss: -20.629180908203125, Learning Rate: 0.01\n",
      "Epoch [14707/20000], Loss: -20.536651611328125, Learning Rate: 0.01\n",
      "Epoch [14708/20000], Loss: -20.55120849609375, Learning Rate: 0.01\n",
      "Epoch [14709/20000], Loss: -20.655441284179688, Learning Rate: 0.01\n",
      "Epoch [14710/20000], Loss: -20.764144897460938, Learning Rate: 0.01\n",
      "Epoch [14711/20000], Loss: -20.803543090820312, Learning Rate: 0.01\n",
      "Epoch [14712/20000], Loss: -20.7650146484375, Learning Rate: 0.01\n",
      "Epoch [14713/20000], Loss: -20.700210571289062, Learning Rate: 0.01\n",
      "Epoch [14714/20000], Loss: -20.670440673828125, Learning Rate: 0.01\n",
      "Epoch [14715/20000], Loss: -20.69781494140625, Learning Rate: 0.01\n",
      "Epoch [14716/20000], Loss: -20.756942749023438, Learning Rate: 0.01\n",
      "Epoch [14717/20000], Loss: -20.8013916015625, Learning Rate: 0.01\n",
      "Epoch [14718/20000], Loss: -20.804611206054688, Learning Rate: 0.01\n",
      "Epoch [14719/20000], Loss: -20.776199340820312, Learning Rate: 0.01\n",
      "Epoch [14720/20000], Loss: -20.747817993164062, Learning Rate: 0.01\n",
      "Epoch [14721/20000], Loss: -20.74456787109375, Learning Rate: 0.01\n",
      "Epoch [14722/20000], Loss: -20.767471313476562, Learning Rate: 0.01\n",
      "Epoch [14723/20000], Loss: -20.79736328125, Learning Rate: 0.01\n",
      "Epoch [14724/20000], Loss: -20.812744140625, Learning Rate: 0.01\n",
      "Epoch [14725/20000], Loss: -20.807540893554688, Learning Rate: 0.01\n",
      "Epoch [14726/20000], Loss: -20.791732788085938, Learning Rate: 0.01\n",
      "Epoch [14727/20000], Loss: -20.781356811523438, Learning Rate: 0.01\n",
      "Epoch [14728/20000], Loss: -20.785049438476562, Learning Rate: 0.01\n",
      "Epoch [14729/20000], Loss: -20.799087524414062, Learning Rate: 0.01\n",
      "Epoch [14730/20000], Loss: -20.813003540039062, Learning Rate: 0.01\n",
      "Epoch [14731/20000], Loss: -20.8179931640625, Learning Rate: 0.01\n",
      "Epoch [14732/20000], Loss: -20.813522338867188, Learning Rate: 0.01\n",
      "Epoch [14733/20000], Loss: -20.806106567382812, Learning Rate: 0.01\n",
      "Epoch [14734/20000], Loss: -20.802825927734375, Learning Rate: 0.01\n",
      "Epoch [14735/20000], Loss: -20.806427001953125, Learning Rate: 0.01\n",
      "Epoch [14736/20000], Loss: -20.814117431640625, Learning Rate: 0.01\n",
      "Epoch [14737/20000], Loss: -20.82073974609375, Learning Rate: 0.01\n",
      "Epoch [14738/20000], Loss: -20.822708129882812, Learning Rate: 0.01\n",
      "Epoch [14739/20000], Loss: -20.820526123046875, Learning Rate: 0.01\n",
      "Epoch [14740/20000], Loss: -20.817337036132812, Learning Rate: 0.01\n",
      "Epoch [14741/20000], Loss: -20.816482543945312, Learning Rate: 0.01\n",
      "Epoch [14742/20000], Loss: -20.818695068359375, Learning Rate: 0.01\n",
      "Epoch [14743/20000], Loss: -20.822860717773438, Learning Rate: 0.01\n",
      "Epoch [14744/20000], Loss: -20.8262939453125, Learning Rate: 0.01\n",
      "Epoch [14745/20000], Loss: -20.827651977539062, Learning Rate: 0.01\n",
      "Epoch [14746/20000], Loss: -20.827011108398438, Learning Rate: 0.01\n",
      "Epoch [14747/20000], Loss: -20.8258056640625, Learning Rate: 0.01\n",
      "Epoch [14748/20000], Loss: -20.825576782226562, Learning Rate: 0.01\n",
      "Epoch [14749/20000], Loss: -20.826904296875, Learning Rate: 0.01\n",
      "Epoch [14750/20000], Loss: -20.829193115234375, Learning Rate: 0.01\n",
      "Epoch [14751/20000], Loss: -20.831344604492188, Learning Rate: 0.01\n",
      "Epoch [14752/20000], Loss: -20.832443237304688, Learning Rate: 0.01\n",
      "Epoch [14753/20000], Loss: -20.832687377929688, Learning Rate: 0.01\n",
      "Epoch [14754/20000], Loss: -20.832534790039062, Learning Rate: 0.01\n",
      "Epoch [14755/20000], Loss: -20.832534790039062, Learning Rate: 0.01\n",
      "Epoch [14756/20000], Loss: -20.833297729492188, Learning Rate: 0.01\n",
      "Epoch [14757/20000], Loss: -20.8345947265625, Learning Rate: 0.01\n",
      "Epoch [14758/20000], Loss: -20.836090087890625, Learning Rate: 0.01\n",
      "Epoch [14759/20000], Loss: -20.837173461914062, Learning Rate: 0.01\n",
      "Epoch [14760/20000], Loss: -20.837799072265625, Learning Rate: 0.01\n",
      "Epoch [14761/20000], Loss: -20.838088989257812, Learning Rate: 0.01\n",
      "Epoch [14762/20000], Loss: -20.838409423828125, Learning Rate: 0.01\n",
      "Epoch [14763/20000], Loss: -20.838973999023438, Learning Rate: 0.01\n",
      "Epoch [14764/20000], Loss: -20.839645385742188, Learning Rate: 0.01\n",
      "Epoch [14765/20000], Loss: -20.840713500976562, Learning Rate: 0.01\n",
      "Epoch [14766/20000], Loss: -20.841690063476562, Learning Rate: 0.01\n",
      "Epoch [14767/20000], Loss: -20.842514038085938, Learning Rate: 0.01\n",
      "Epoch [14768/20000], Loss: -20.843185424804688, Learning Rate: 0.01\n",
      "Epoch [14769/20000], Loss: -20.843597412109375, Learning Rate: 0.01\n",
      "Epoch [14770/20000], Loss: -20.84417724609375, Learning Rate: 0.01\n",
      "Epoch [14771/20000], Loss: -20.844741821289062, Learning Rate: 0.01\n",
      "Epoch [14772/20000], Loss: -20.8455810546875, Learning Rate: 0.01\n",
      "Epoch [14773/20000], Loss: -20.8463134765625, Learning Rate: 0.01\n",
      "Epoch [14774/20000], Loss: -20.847213745117188, Learning Rate: 0.01\n",
      "Epoch [14775/20000], Loss: -20.847869873046875, Learning Rate: 0.01\n",
      "Epoch [14776/20000], Loss: -20.848480224609375, Learning Rate: 0.01\n",
      "Epoch [14777/20000], Loss: -20.849090576171875, Learning Rate: 0.01\n",
      "Epoch [14778/20000], Loss: -20.849594116210938, Learning Rate: 0.01\n",
      "Epoch [14779/20000], Loss: -20.850250244140625, Learning Rate: 0.01\n",
      "Epoch [14780/20000], Loss: -20.8509521484375, Learning Rate: 0.01\n",
      "Epoch [14781/20000], Loss: -20.85174560546875, Learning Rate: 0.01\n",
      "Epoch [14782/20000], Loss: -20.852371215820312, Learning Rate: 0.01\n",
      "Epoch [14783/20000], Loss: -20.853103637695312, Learning Rate: 0.01\n",
      "Epoch [14784/20000], Loss: -20.853805541992188, Learning Rate: 0.01\n",
      "Epoch [14785/20000], Loss: -20.85443115234375, Learning Rate: 0.01\n",
      "Epoch [14786/20000], Loss: -20.854995727539062, Learning Rate: 0.01\n",
      "Epoch [14787/20000], Loss: -20.8555908203125, Learning Rate: 0.01\n",
      "Epoch [14788/20000], Loss: -20.8563232421875, Learning Rate: 0.01\n",
      "Epoch [14789/20000], Loss: -20.856948852539062, Learning Rate: 0.01\n",
      "Epoch [14790/20000], Loss: -20.857711791992188, Learning Rate: 0.01\n",
      "Epoch [14791/20000], Loss: -20.858367919921875, Learning Rate: 0.01\n",
      "Epoch [14792/20000], Loss: -20.858993530273438, Learning Rate: 0.01\n",
      "Epoch [14793/20000], Loss: -20.859573364257812, Learning Rate: 0.01\n",
      "Epoch [14794/20000], Loss: -20.860275268554688, Learning Rate: 0.01\n",
      "Epoch [14795/20000], Loss: -20.860931396484375, Learning Rate: 0.01\n",
      "Epoch [14796/20000], Loss: -20.861557006835938, Learning Rate: 0.01\n",
      "Epoch [14797/20000], Loss: -20.86224365234375, Learning Rate: 0.01\n",
      "Epoch [14798/20000], Loss: -20.86285400390625, Learning Rate: 0.01\n",
      "Epoch [14799/20000], Loss: -20.863540649414062, Learning Rate: 0.01\n",
      "Epoch [14800/20000], Loss: -20.8642578125, Learning Rate: 0.01\n",
      "Epoch [14801/20000], Loss: -20.864822387695312, Learning Rate: 0.01\n",
      "Epoch [14802/20000], Loss: -20.865478515625, Learning Rate: 0.01\n",
      "Epoch [14803/20000], Loss: -20.8660888671875, Learning Rate: 0.01\n",
      "Epoch [14804/20000], Loss: -20.866836547851562, Learning Rate: 0.01\n",
      "Epoch [14805/20000], Loss: -20.86737060546875, Learning Rate: 0.01\n",
      "Epoch [14806/20000], Loss: -20.8680419921875, Learning Rate: 0.01\n",
      "Epoch [14807/20000], Loss: -20.868682861328125, Learning Rate: 0.01\n",
      "Epoch [14808/20000], Loss: -20.869308471679688, Learning Rate: 0.01\n",
      "Epoch [14809/20000], Loss: -20.8699951171875, Learning Rate: 0.01\n",
      "Epoch [14810/20000], Loss: -20.870529174804688, Learning Rate: 0.01\n",
      "Epoch [14811/20000], Loss: -20.871261596679688, Learning Rate: 0.01\n",
      "Epoch [14812/20000], Loss: -20.871871948242188, Learning Rate: 0.01\n",
      "Epoch [14813/20000], Loss: -20.872589111328125, Learning Rate: 0.01\n",
      "Epoch [14814/20000], Loss: -20.873077392578125, Learning Rate: 0.01\n",
      "Epoch [14815/20000], Loss: -20.873748779296875, Learning Rate: 0.01\n",
      "Epoch [14816/20000], Loss: -20.87432861328125, Learning Rate: 0.01\n",
      "Epoch [14817/20000], Loss: -20.875, Learning Rate: 0.01\n",
      "Epoch [14818/20000], Loss: -20.875701904296875, Learning Rate: 0.01\n",
      "Epoch [14819/20000], Loss: -20.876251220703125, Learning Rate: 0.01\n",
      "Epoch [14820/20000], Loss: -20.876937866210938, Learning Rate: 0.01\n",
      "Epoch [14821/20000], Loss: -20.8775634765625, Learning Rate: 0.01\n",
      "Epoch [14822/20000], Loss: -20.878204345703125, Learning Rate: 0.01\n",
      "Epoch [14823/20000], Loss: -20.878768920898438, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14824/20000], Loss: -20.879501342773438, Learning Rate: 0.01\n",
      "Epoch [14825/20000], Loss: -20.880050659179688, Learning Rate: 0.01\n",
      "Epoch [14826/20000], Loss: -20.880706787109375, Learning Rate: 0.01\n",
      "Epoch [14827/20000], Loss: -20.881317138671875, Learning Rate: 0.01\n",
      "Epoch [14828/20000], Loss: -20.881912231445312, Learning Rate: 0.01\n",
      "Epoch [14829/20000], Loss: -20.882553100585938, Learning Rate: 0.01\n",
      "Epoch [14830/20000], Loss: -20.8831787109375, Learning Rate: 0.01\n",
      "Epoch [14831/20000], Loss: -20.88385009765625, Learning Rate: 0.01\n",
      "Epoch [14832/20000], Loss: -20.884445190429688, Learning Rate: 0.01\n",
      "Epoch [14833/20000], Loss: -20.885055541992188, Learning Rate: 0.01\n",
      "Epoch [14834/20000], Loss: -20.885574340820312, Learning Rate: 0.01\n",
      "Epoch [14835/20000], Loss: -20.886199951171875, Learning Rate: 0.01\n",
      "Epoch [14836/20000], Loss: -20.88671875, Learning Rate: 0.01\n",
      "Epoch [14837/20000], Loss: -20.887222290039062, Learning Rate: 0.01\n",
      "Epoch [14838/20000], Loss: -20.887725830078125, Learning Rate: 0.01\n",
      "Epoch [14839/20000], Loss: -20.888214111328125, Learning Rate: 0.01\n",
      "Epoch [14840/20000], Loss: -20.8885498046875, Learning Rate: 0.01\n",
      "Epoch [14841/20000], Loss: -20.888748168945312, Learning Rate: 0.01\n",
      "Epoch [14842/20000], Loss: -20.888870239257812, Learning Rate: 0.01\n",
      "Epoch [14843/20000], Loss: -20.888641357421875, Learning Rate: 0.01\n",
      "Epoch [14844/20000], Loss: -20.88800048828125, Learning Rate: 0.01\n",
      "Epoch [14845/20000], Loss: -20.886962890625, Learning Rate: 0.01\n",
      "Epoch [14846/20000], Loss: -20.884963989257812, Learning Rate: 0.01\n",
      "Epoch [14847/20000], Loss: -20.881729125976562, Learning Rate: 0.01\n",
      "Epoch [14848/20000], Loss: -20.876693725585938, Learning Rate: 0.01\n",
      "Epoch [14849/20000], Loss: -20.868927001953125, Learning Rate: 0.01\n",
      "Epoch [14850/20000], Loss: -20.857086181640625, Learning Rate: 0.01\n",
      "Epoch [14851/20000], Loss: -20.839187622070312, Learning Rate: 0.01\n",
      "Epoch [14852/20000], Loss: -20.8121337890625, Learning Rate: 0.01\n",
      "Epoch [14853/20000], Loss: -20.7720947265625, Learning Rate: 0.01\n",
      "Epoch [14854/20000], Loss: -20.713104248046875, Learning Rate: 0.01\n",
      "Epoch [14855/20000], Loss: -20.628265380859375, Learning Rate: 0.01\n",
      "Epoch [14856/20000], Loss: -20.508743286132812, Learning Rate: 0.01\n",
      "Epoch [14857/20000], Loss: -20.349945068359375, Learning Rate: 0.01\n",
      "Epoch [14858/20000], Loss: -20.150146484375, Learning Rate: 0.01\n",
      "Epoch [14859/20000], Loss: -19.932907104492188, Learning Rate: 0.01\n",
      "Epoch [14860/20000], Loss: -19.737945556640625, Learning Rate: 0.01\n",
      "Epoch [14861/20000], Loss: -19.650955200195312, Learning Rate: 0.01\n",
      "Epoch [14862/20000], Loss: -19.7352294921875, Learning Rate: 0.01\n",
      "Epoch [14863/20000], Loss: -20.019424438476562, Learning Rate: 0.01\n",
      "Epoch [14864/20000], Loss: -20.406967163085938, Learning Rate: 0.01\n",
      "Epoch [14865/20000], Loss: -20.742355346679688, Learning Rate: 0.01\n",
      "Epoch [14866/20000], Loss: -20.891326904296875, Learning Rate: 0.01\n",
      "Epoch [14867/20000], Loss: -20.833602905273438, Learning Rate: 0.01\n",
      "Epoch [14868/20000], Loss: -20.659469604492188, Learning Rate: 0.01\n",
      "Epoch [14869/20000], Loss: -20.503143310546875, Learning Rate: 0.01\n",
      "Epoch [14870/20000], Loss: -20.4698486328125, Learning Rate: 0.01\n",
      "Epoch [14871/20000], Loss: -20.573974609375, Learning Rate: 0.01\n",
      "Epoch [14872/20000], Loss: -20.745574951171875, Learning Rate: 0.01\n",
      "Epoch [14873/20000], Loss: -20.875839233398438, Learning Rate: 0.01\n",
      "Epoch [14874/20000], Loss: -20.899932861328125, Learning Rate: 0.01\n",
      "Epoch [14875/20000], Loss: -20.832748413085938, Learning Rate: 0.01\n",
      "Epoch [14876/20000], Loss: -20.744659423828125, Learning Rate: 0.01\n",
      "Epoch [14877/20000], Loss: -20.707443237304688, Learning Rate: 0.01\n",
      "Epoch [14878/20000], Loss: -20.744979858398438, Learning Rate: 0.01\n",
      "Epoch [14879/20000], Loss: -20.827423095703125, Learning Rate: 0.01\n",
      "Epoch [14880/20000], Loss: -20.896804809570312, Learning Rate: 0.01\n",
      "Epoch [14881/20000], Loss: -20.914520263671875, Learning Rate: 0.01\n",
      "Epoch [14882/20000], Loss: -20.883514404296875, Learning Rate: 0.01\n",
      "Epoch [14883/20000], Loss: -20.839324951171875, Learning Rate: 0.01\n",
      "Epoch [14884/20000], Loss: -20.819534301757812, Learning Rate: 0.01\n",
      "Epoch [14885/20000], Loss: -20.836837768554688, Learning Rate: 0.01\n",
      "Epoch [14886/20000], Loss: -20.876373291015625, Learning Rate: 0.01\n",
      "Epoch [14887/20000], Loss: -20.90948486328125, Learning Rate: 0.01\n",
      "Epoch [14888/20000], Loss: -20.917587280273438, Learning Rate: 0.01\n",
      "Epoch [14889/20000], Loss: -20.902618408203125, Learning Rate: 0.01\n",
      "Epoch [14890/20000], Loss: -20.882492065429688, Learning Rate: 0.01\n",
      "Epoch [14891/20000], Loss: -20.874542236328125, Learning Rate: 0.01\n",
      "Epoch [14892/20000], Loss: -20.884414672851562, Learning Rate: 0.01\n",
      "Epoch [14893/20000], Loss: -20.903976440429688, Learning Rate: 0.01\n",
      "Epoch [14894/20000], Loss: -20.919815063476562, Learning Rate: 0.01\n",
      "Epoch [14895/20000], Loss: -20.923080444335938, Learning Rate: 0.01\n",
      "Epoch [14896/20000], Loss: -20.915664672851562, Learning Rate: 0.01\n",
      "Epoch [14897/20000], Loss: -20.905960083007812, Learning Rate: 0.01\n",
      "Epoch [14898/20000], Loss: -20.902206420898438, Learning Rate: 0.01\n",
      "Epoch [14899/20000], Loss: -20.907241821289062, Learning Rate: 0.01\n",
      "Epoch [14900/20000], Loss: -20.916900634765625, Learning Rate: 0.01\n",
      "Epoch [14901/20000], Loss: -20.924957275390625, Learning Rate: 0.01\n",
      "Epoch [14902/20000], Loss: -20.92730712890625, Learning Rate: 0.01\n",
      "Epoch [14903/20000], Loss: -20.924423217773438, Learning Rate: 0.01\n",
      "Epoch [14904/20000], Loss: -20.920074462890625, Learning Rate: 0.01\n",
      "Epoch [14905/20000], Loss: -20.918472290039062, Learning Rate: 0.01\n",
      "Epoch [14906/20000], Loss: -20.920791625976562, Learning Rate: 0.01\n",
      "Epoch [14907/20000], Loss: -20.925582885742188, Learning Rate: 0.01\n",
      "Epoch [14908/20000], Loss: -20.929962158203125, Learning Rate: 0.01\n",
      "Epoch [14909/20000], Loss: -20.931625366210938, Learning Rate: 0.01\n",
      "Epoch [14910/20000], Loss: -20.9307861328125, Learning Rate: 0.01\n",
      "Epoch [14911/20000], Loss: -20.928909301757812, Learning Rate: 0.01\n",
      "Epoch [14912/20000], Loss: -20.928070068359375, Learning Rate: 0.01\n",
      "Epoch [14913/20000], Loss: -20.928985595703125, Learning Rate: 0.01\n",
      "Epoch [14914/20000], Loss: -20.931350708007812, Learning Rate: 0.01\n",
      "Epoch [14915/20000], Loss: -20.933837890625, Learning Rate: 0.01\n",
      "Epoch [14916/20000], Loss: -20.935333251953125, Learning Rate: 0.01\n",
      "Epoch [14917/20000], Loss: -20.935592651367188, Learning Rate: 0.01\n",
      "Epoch [14918/20000], Loss: -20.93505859375, Learning Rate: 0.01\n",
      "Epoch [14919/20000], Loss: -20.934722900390625, Learning Rate: 0.01\n",
      "Epoch [14920/20000], Loss: -20.935134887695312, Learning Rate: 0.01\n",
      "Epoch [14921/20000], Loss: -20.93646240234375, Learning Rate: 0.01\n",
      "Epoch [14922/20000], Loss: -20.937896728515625, Learning Rate: 0.01\n",
      "Epoch [14923/20000], Loss: -20.939178466796875, Learning Rate: 0.01\n",
      "Epoch [14924/20000], Loss: -20.9398193359375, Learning Rate: 0.01\n",
      "Epoch [14925/20000], Loss: -20.93988037109375, Learning Rate: 0.01\n",
      "Epoch [14926/20000], Loss: -20.939971923828125, Learning Rate: 0.01\n",
      "Epoch [14927/20000], Loss: -20.940292358398438, Learning Rate: 0.01\n",
      "Epoch [14928/20000], Loss: -20.94091796875, Learning Rate: 0.01\n",
      "Epoch [14929/20000], Loss: -20.94189453125, Learning Rate: 0.01\n",
      "Epoch [14930/20000], Loss: -20.942718505859375, Learning Rate: 0.01\n",
      "Epoch [14931/20000], Loss: -20.9434814453125, Learning Rate: 0.01\n",
      "Epoch [14932/20000], Loss: -20.944046020507812, Learning Rate: 0.01\n",
      "Epoch [14933/20000], Loss: -20.94439697265625, Learning Rate: 0.01\n",
      "Epoch [14934/20000], Loss: -20.944686889648438, Learning Rate: 0.01\n",
      "Epoch [14935/20000], Loss: -20.945083618164062, Learning Rate: 0.01\n",
      "Epoch [14936/20000], Loss: -20.945724487304688, Learning Rate: 0.01\n",
      "Epoch [14937/20000], Loss: -20.946487426757812, Learning Rate: 0.01\n",
      "Epoch [14938/20000], Loss: -20.9471435546875, Learning Rate: 0.01\n",
      "Epoch [14939/20000], Loss: -20.947799682617188, Learning Rate: 0.01\n",
      "Epoch [14940/20000], Loss: -20.9482421875, Learning Rate: 0.01\n",
      "Epoch [14941/20000], Loss: -20.948684692382812, Learning Rate: 0.01\n",
      "Epoch [14942/20000], Loss: -20.949142456054688, Learning Rate: 0.01\n",
      "Epoch [14943/20000], Loss: -20.949691772460938, Learning Rate: 0.01\n",
      "Epoch [14944/20000], Loss: -20.950225830078125, Learning Rate: 0.01\n",
      "Epoch [14945/20000], Loss: -20.950759887695312, Learning Rate: 0.01\n",
      "Epoch [14946/20000], Loss: -20.951339721679688, Learning Rate: 0.01\n",
      "Epoch [14947/20000], Loss: -20.951950073242188, Learning Rate: 0.01\n",
      "Epoch [14948/20000], Loss: -20.952377319335938, Learning Rate: 0.01\n",
      "Epoch [14949/20000], Loss: -20.952865600585938, Learning Rate: 0.01\n",
      "Epoch [14950/20000], Loss: -20.953262329101562, Learning Rate: 0.01\n",
      "Epoch [14951/20000], Loss: -20.95379638671875, Learning Rate: 0.01\n",
      "Epoch [14952/20000], Loss: -20.954345703125, Learning Rate: 0.01\n",
      "Epoch [14953/20000], Loss: -20.954696655273438, Learning Rate: 0.01\n",
      "Epoch [14954/20000], Loss: -20.955169677734375, Learning Rate: 0.01\n",
      "Epoch [14955/20000], Loss: -20.955551147460938, Learning Rate: 0.01\n",
      "Epoch [14956/20000], Loss: -20.9559326171875, Learning Rate: 0.01\n",
      "Epoch [14957/20000], Loss: -20.956100463867188, Learning Rate: 0.01\n",
      "Epoch [14958/20000], Loss: -20.956253051757812, Learning Rate: 0.01\n",
      "Epoch [14959/20000], Loss: -20.956130981445312, Learning Rate: 0.01\n",
      "Epoch [14960/20000], Loss: -20.955810546875, Learning Rate: 0.01\n",
      "Epoch [14961/20000], Loss: -20.955368041992188, Learning Rate: 0.01\n",
      "Epoch [14962/20000], Loss: -20.95416259765625, Learning Rate: 0.01\n",
      "Epoch [14963/20000], Loss: -20.952392578125, Learning Rate: 0.01\n",
      "Epoch [14964/20000], Loss: -20.949600219726562, Learning Rate: 0.01\n",
      "Epoch [14965/20000], Loss: -20.945114135742188, Learning Rate: 0.01\n",
      "Epoch [14966/20000], Loss: -20.938430786132812, Learning Rate: 0.01\n",
      "Epoch [14967/20000], Loss: -20.928314208984375, Learning Rate: 0.01\n",
      "Epoch [14968/20000], Loss: -20.913040161132812, Learning Rate: 0.01\n",
      "Epoch [14969/20000], Loss: -20.890396118164062, Learning Rate: 0.01\n",
      "Epoch [14970/20000], Loss: -20.856491088867188, Learning Rate: 0.01\n",
      "Epoch [14971/20000], Loss: -20.806808471679688, Learning Rate: 0.01\n",
      "Epoch [14972/20000], Loss: -20.7342529296875, Learning Rate: 0.01\n",
      "Epoch [14973/20000], Loss: -20.631591796875, Learning Rate: 0.01\n",
      "Epoch [14974/20000], Loss: -20.489639282226562, Learning Rate: 0.01\n",
      "Epoch [14975/20000], Loss: -20.307205200195312, Learning Rate: 0.01\n",
      "Epoch [14976/20000], Loss: -20.089889526367188, Learning Rate: 0.01\n",
      "Epoch [14977/20000], Loss: -19.877639770507812, Learning Rate: 0.01\n",
      "Epoch [14978/20000], Loss: -19.727188110351562, Learning Rate: 0.01\n",
      "Epoch [14979/20000], Loss: -19.734527587890625, Learning Rate: 0.01\n",
      "Epoch [14980/20000], Loss: -19.938873291015625, Learning Rate: 0.01\n",
      "Epoch [14981/20000], Loss: -20.309967041015625, Learning Rate: 0.01\n",
      "Epoch [14982/20000], Loss: -20.694992065429688, Learning Rate: 0.01\n",
      "Epoch [14983/20000], Loss: -20.934066772460938, Learning Rate: 0.01\n",
      "Epoch [14984/20000], Loss: -20.951995849609375, Learning Rate: 0.01\n",
      "Epoch [14985/20000], Loss: -20.801239013671875, Learning Rate: 0.01\n",
      "Epoch [14986/20000], Loss: -20.612411499023438, Learning Rate: 0.01\n",
      "Epoch [14987/20000], Loss: -20.512664794921875, Learning Rate: 0.01\n",
      "Epoch [14988/20000], Loss: -20.5638427734375, Learning Rate: 0.01\n",
      "Epoch [14989/20000], Loss: -20.724777221679688, Learning Rate: 0.01\n",
      "Epoch [14990/20000], Loss: -20.8929443359375, Learning Rate: 0.01\n",
      "Epoch [14991/20000], Loss: -20.972488403320312, Learning Rate: 0.01\n",
      "Epoch [14992/20000], Loss: -20.9407958984375, Learning Rate: 0.01\n",
      "Epoch [14993/20000], Loss: -20.849227905273438, Learning Rate: 0.01\n",
      "Epoch [14994/20000], Loss: -20.776947021484375, Learning Rate: 0.01\n",
      "Epoch [14995/20000], Loss: -20.776168823242188, Learning Rate: 0.01\n",
      "Epoch [14996/20000], Loss: -20.841659545898438, Learning Rate: 0.01\n",
      "Epoch [14997/20000], Loss: -20.92535400390625, Learning Rate: 0.01\n",
      "Epoch [14998/20000], Loss: -20.973968505859375, Learning Rate: 0.01\n",
      "Epoch [14999/20000], Loss: -20.967575073242188, Learning Rate: 0.01\n",
      "Epoch [15000/20000], Loss: -20.92596435546875, Learning Rate: 0.01\n",
      "Epoch [15001/20000], Loss: -20.888168334960938, Learning Rate: 0.01\n",
      "Epoch [15002/20000], Loss: -20.883087158203125, Learning Rate: 0.01\n",
      "Epoch [15003/20000], Loss: -20.911422729492188, Learning Rate: 0.01\n",
      "Epoch [15004/20000], Loss: -20.951690673828125, Learning Rate: 0.01\n",
      "Epoch [15005/20000], Loss: -20.977920532226562, Learning Rate: 0.01\n",
      "Epoch [15006/20000], Loss: -20.978240966796875, Learning Rate: 0.01\n",
      "Epoch [15007/20000], Loss: -20.96014404296875, Learning Rate: 0.01\n",
      "Epoch [15008/20000], Loss: -20.94140625, Learning Rate: 0.01\n",
      "Epoch [15009/20000], Loss: -20.9365234375, Learning Rate: 0.01\n",
      "Epoch [15010/20000], Loss: -20.948074340820312, Learning Rate: 0.01\n",
      "Epoch [15011/20000], Loss: -20.967071533203125, Learning Rate: 0.01\n",
      "Epoch [15012/20000], Loss: -20.981475830078125, Learning Rate: 0.01\n",
      "Epoch [15013/20000], Loss: -20.98431396484375, Learning Rate: 0.01\n",
      "Epoch [15014/20000], Loss: -20.977676391601562, Learning Rate: 0.01\n",
      "Epoch [15015/20000], Loss: -20.968719482421875, Learning Rate: 0.01\n",
      "Epoch [15016/20000], Loss: -20.9647216796875, Learning Rate: 0.01\n",
      "Epoch [15017/20000], Loss: -20.968353271484375, Learning Rate: 0.01\n",
      "Epoch [15018/20000], Loss: -20.976882934570312, Learning Rate: 0.01\n",
      "Epoch [15019/20000], Loss: -20.984970092773438, Learning Rate: 0.01\n",
      "Epoch [15020/20000], Loss: -20.988525390625, Learning Rate: 0.01\n",
      "Epoch [15021/20000], Loss: -20.987136840820312, Learning Rate: 0.01\n",
      "Epoch [15022/20000], Loss: -20.983322143554688, Learning Rate: 0.01\n",
      "Epoch [15023/20000], Loss: -20.980621337890625, Learning Rate: 0.01\n",
      "Epoch [15024/20000], Loss: -20.98101806640625, Learning Rate: 0.01\n",
      "Epoch [15025/20000], Loss: -20.984222412109375, Learning Rate: 0.01\n",
      "Epoch [15026/20000], Loss: -20.988418579101562, Learning Rate: 0.01\n",
      "Epoch [15027/20000], Loss: -20.991500854492188, Learning Rate: 0.01\n",
      "Epoch [15028/20000], Loss: -20.992340087890625, Learning Rate: 0.01\n",
      "Epoch [15029/20000], Loss: -20.991500854492188, Learning Rate: 0.01\n",
      "Epoch [15030/20000], Loss: -20.990310668945312, Learning Rate: 0.01\n",
      "Epoch [15031/20000], Loss: -20.989654541015625, Learning Rate: 0.01\n",
      "Epoch [15032/20000], Loss: -20.990585327148438, Learning Rate: 0.01\n",
      "Epoch [15033/20000], Loss: -20.992324829101562, Learning Rate: 0.01\n",
      "Epoch [15034/20000], Loss: -20.9942626953125, Learning Rate: 0.01\n",
      "Epoch [15035/20000], Loss: -20.9957275390625, Learning Rate: 0.01\n",
      "Epoch [15036/20000], Loss: -20.996231079101562, Learning Rate: 0.01\n",
      "Epoch [15037/20000], Loss: -20.996185302734375, Learning Rate: 0.01\n",
      "Epoch [15038/20000], Loss: -20.99591064453125, Learning Rate: 0.01\n",
      "Epoch [15039/20000], Loss: -20.996047973632812, Learning Rate: 0.01\n",
      "Epoch [15040/20000], Loss: -20.99664306640625, Learning Rate: 0.01\n",
      "Epoch [15041/20000], Loss: -20.997650146484375, Learning Rate: 0.01\n",
      "Epoch [15042/20000], Loss: -20.99871826171875, Learning Rate: 0.01\n",
      "Epoch [15043/20000], Loss: -20.999526977539062, Learning Rate: 0.01\n",
      "Epoch [15044/20000], Loss: -21.000198364257812, Learning Rate: 0.01\n",
      "Epoch [15045/20000], Loss: -21.000473022460938, Learning Rate: 0.01\n",
      "Epoch [15046/20000], Loss: -21.000640869140625, Learning Rate: 0.01\n",
      "Epoch [15047/20000], Loss: -21.000930786132812, Learning Rate: 0.01\n",
      "Epoch [15048/20000], Loss: -21.001419067382812, Learning Rate: 0.01\n",
      "Epoch [15049/20000], Loss: -21.002044677734375, Learning Rate: 0.01\n",
      "Epoch [15050/20000], Loss: -21.002716064453125, Learning Rate: 0.01\n",
      "Epoch [15051/20000], Loss: -21.003250122070312, Learning Rate: 0.01\n",
      "Epoch [15052/20000], Loss: -21.003860473632812, Learning Rate: 0.01\n",
      "Epoch [15053/20000], Loss: -21.004364013671875, Learning Rate: 0.01\n",
      "Epoch [15054/20000], Loss: -21.004806518554688, Learning Rate: 0.01\n",
      "Epoch [15055/20000], Loss: -21.005111694335938, Learning Rate: 0.01\n",
      "Epoch [15056/20000], Loss: -21.005523681640625, Learning Rate: 0.01\n",
      "Epoch [15057/20000], Loss: -21.006103515625, Learning Rate: 0.01\n",
      "Epoch [15058/20000], Loss: -21.006546020507812, Learning Rate: 0.01\n",
      "Epoch [15059/20000], Loss: -21.007064819335938, Learning Rate: 0.01\n",
      "Epoch [15060/20000], Loss: -21.007522583007812, Learning Rate: 0.01\n",
      "Epoch [15061/20000], Loss: -21.008071899414062, Learning Rate: 0.01\n",
      "Epoch [15062/20000], Loss: -21.008544921875, Learning Rate: 0.01\n",
      "Epoch [15063/20000], Loss: -21.008895874023438, Learning Rate: 0.01\n",
      "Epoch [15064/20000], Loss: -21.009414672851562, Learning Rate: 0.01\n",
      "Epoch [15065/20000], Loss: -21.009872436523438, Learning Rate: 0.01\n",
      "Epoch [15066/20000], Loss: -21.010208129882812, Learning Rate: 0.01\n",
      "Epoch [15067/20000], Loss: -21.010757446289062, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15068/20000], Loss: -21.01116943359375, Learning Rate: 0.01\n",
      "Epoch [15069/20000], Loss: -21.011672973632812, Learning Rate: 0.01\n",
      "Epoch [15070/20000], Loss: -21.012222290039062, Learning Rate: 0.01\n",
      "Epoch [15071/20000], Loss: -21.012496948242188, Learning Rate: 0.01\n",
      "Epoch [15072/20000], Loss: -21.013046264648438, Learning Rate: 0.01\n",
      "Epoch [15073/20000], Loss: -21.013504028320312, Learning Rate: 0.01\n",
      "Epoch [15074/20000], Loss: -21.013931274414062, Learning Rate: 0.01\n",
      "Epoch [15075/20000], Loss: -21.01446533203125, Learning Rate: 0.01\n",
      "Epoch [15076/20000], Loss: -21.01483154296875, Learning Rate: 0.01\n",
      "Epoch [15077/20000], Loss: -21.015304565429688, Learning Rate: 0.01\n",
      "Epoch [15078/20000], Loss: -21.015762329101562, Learning Rate: 0.01\n",
      "Epoch [15079/20000], Loss: -21.016220092773438, Learning Rate: 0.01\n",
      "Epoch [15080/20000], Loss: -21.016616821289062, Learning Rate: 0.01\n",
      "Epoch [15081/20000], Loss: -21.017059326171875, Learning Rate: 0.01\n",
      "Epoch [15082/20000], Loss: -21.017501831054688, Learning Rate: 0.01\n",
      "Epoch [15083/20000], Loss: -21.0179443359375, Learning Rate: 0.01\n",
      "Epoch [15084/20000], Loss: -21.018463134765625, Learning Rate: 0.01\n",
      "Epoch [15085/20000], Loss: -21.018905639648438, Learning Rate: 0.01\n",
      "Epoch [15086/20000], Loss: -21.019393920898438, Learning Rate: 0.01\n",
      "Epoch [15087/20000], Loss: -21.019760131835938, Learning Rate: 0.01\n",
      "Epoch [15088/20000], Loss: -21.020095825195312, Learning Rate: 0.01\n",
      "Epoch [15089/20000], Loss: -21.020645141601562, Learning Rate: 0.01\n",
      "Epoch [15090/20000], Loss: -21.021041870117188, Learning Rate: 0.01\n",
      "Epoch [15091/20000], Loss: -21.021484375, Learning Rate: 0.01\n",
      "Epoch [15092/20000], Loss: -21.021987915039062, Learning Rate: 0.01\n",
      "Epoch [15093/20000], Loss: -21.022369384765625, Learning Rate: 0.01\n",
      "Epoch [15094/20000], Loss: -21.022705078125, Learning Rate: 0.01\n",
      "Epoch [15095/20000], Loss: -21.023300170898438, Learning Rate: 0.01\n",
      "Epoch [15096/20000], Loss: -21.023666381835938, Learning Rate: 0.01\n",
      "Epoch [15097/20000], Loss: -21.02410888671875, Learning Rate: 0.01\n",
      "Epoch [15098/20000], Loss: -21.024581909179688, Learning Rate: 0.01\n",
      "Epoch [15099/20000], Loss: -21.02496337890625, Learning Rate: 0.01\n",
      "Epoch [15100/20000], Loss: -21.025344848632812, Learning Rate: 0.01\n",
      "Epoch [15101/20000], Loss: -21.025802612304688, Learning Rate: 0.01\n",
      "Epoch [15102/20000], Loss: -21.026321411132812, Learning Rate: 0.01\n",
      "Epoch [15103/20000], Loss: -21.02679443359375, Learning Rate: 0.01\n",
      "Epoch [15104/20000], Loss: -21.027099609375, Learning Rate: 0.01\n",
      "Epoch [15105/20000], Loss: -21.027633666992188, Learning Rate: 0.01\n",
      "Epoch [15106/20000], Loss: -21.02801513671875, Learning Rate: 0.01\n",
      "Epoch [15107/20000], Loss: -21.028427124023438, Learning Rate: 0.01\n",
      "Epoch [15108/20000], Loss: -21.028915405273438, Learning Rate: 0.01\n",
      "Epoch [15109/20000], Loss: -21.029342651367188, Learning Rate: 0.01\n",
      "Epoch [15110/20000], Loss: -21.029769897460938, Learning Rate: 0.01\n",
      "Epoch [15111/20000], Loss: -21.030197143554688, Learning Rate: 0.01\n",
      "Epoch [15112/20000], Loss: -21.030609130859375, Learning Rate: 0.01\n",
      "Epoch [15113/20000], Loss: -21.031082153320312, Learning Rate: 0.01\n",
      "Epoch [15114/20000], Loss: -21.031509399414062, Learning Rate: 0.01\n",
      "Epoch [15115/20000], Loss: -21.0318603515625, Learning Rate: 0.01\n",
      "Epoch [15116/20000], Loss: -21.032257080078125, Learning Rate: 0.01\n",
      "Epoch [15117/20000], Loss: -21.032730102539062, Learning Rate: 0.01\n",
      "Epoch [15118/20000], Loss: -21.033111572265625, Learning Rate: 0.01\n",
      "Epoch [15119/20000], Loss: -21.03363037109375, Learning Rate: 0.01\n",
      "Epoch [15120/20000], Loss: -21.033981323242188, Learning Rate: 0.01\n",
      "Epoch [15121/20000], Loss: -21.034393310546875, Learning Rate: 0.01\n",
      "Epoch [15122/20000], Loss: -21.034896850585938, Learning Rate: 0.01\n",
      "Epoch [15123/20000], Loss: -21.035186767578125, Learning Rate: 0.01\n",
      "Epoch [15124/20000], Loss: -21.035675048828125, Learning Rate: 0.01\n",
      "Epoch [15125/20000], Loss: -21.036102294921875, Learning Rate: 0.01\n",
      "Epoch [15126/20000], Loss: -21.0364990234375, Learning Rate: 0.01\n",
      "Epoch [15127/20000], Loss: -21.0369873046875, Learning Rate: 0.01\n",
      "Epoch [15128/20000], Loss: -21.037322998046875, Learning Rate: 0.01\n",
      "Epoch [15129/20000], Loss: -21.037841796875, Learning Rate: 0.01\n",
      "Epoch [15130/20000], Loss: -21.038238525390625, Learning Rate: 0.01\n",
      "Epoch [15131/20000], Loss: -21.038650512695312, Learning Rate: 0.01\n",
      "Epoch [15132/20000], Loss: -21.039093017578125, Learning Rate: 0.01\n",
      "Epoch [15133/20000], Loss: -21.039505004882812, Learning Rate: 0.01\n",
      "Epoch [15134/20000], Loss: -21.0399169921875, Learning Rate: 0.01\n",
      "Epoch [15135/20000], Loss: -21.040374755859375, Learning Rate: 0.01\n",
      "Epoch [15136/20000], Loss: -21.040802001953125, Learning Rate: 0.01\n",
      "Epoch [15137/20000], Loss: -21.041183471679688, Learning Rate: 0.01\n",
      "Epoch [15138/20000], Loss: -21.04156494140625, Learning Rate: 0.01\n",
      "Epoch [15139/20000], Loss: -21.042037963867188, Learning Rate: 0.01\n",
      "Epoch [15140/20000], Loss: -21.042465209960938, Learning Rate: 0.01\n",
      "Epoch [15141/20000], Loss: -21.042831420898438, Learning Rate: 0.01\n",
      "Epoch [15142/20000], Loss: -21.043258666992188, Learning Rate: 0.01\n",
      "Epoch [15143/20000], Loss: -21.043716430664062, Learning Rate: 0.01\n",
      "Epoch [15144/20000], Loss: -21.044021606445312, Learning Rate: 0.01\n",
      "Epoch [15145/20000], Loss: -21.04449462890625, Learning Rate: 0.01\n",
      "Epoch [15146/20000], Loss: -21.0447998046875, Learning Rate: 0.01\n",
      "Epoch [15147/20000], Loss: -21.045303344726562, Learning Rate: 0.01\n",
      "Epoch [15148/20000], Loss: -21.045654296875, Learning Rate: 0.01\n",
      "Epoch [15149/20000], Loss: -21.046142578125, Learning Rate: 0.01\n",
      "Epoch [15150/20000], Loss: -21.046524047851562, Learning Rate: 0.01\n",
      "Epoch [15151/20000], Loss: -21.0469970703125, Learning Rate: 0.01\n",
      "Epoch [15152/20000], Loss: -21.047317504882812, Learning Rate: 0.01\n",
      "Epoch [15153/20000], Loss: -21.047683715820312, Learning Rate: 0.01\n",
      "Epoch [15154/20000], Loss: -21.048187255859375, Learning Rate: 0.01\n",
      "Epoch [15155/20000], Loss: -21.048568725585938, Learning Rate: 0.01\n",
      "Epoch [15156/20000], Loss: -21.0489501953125, Learning Rate: 0.01\n",
      "Epoch [15157/20000], Loss: -21.049331665039062, Learning Rate: 0.01\n",
      "Epoch [15158/20000], Loss: -21.0496826171875, Learning Rate: 0.01\n",
      "Epoch [15159/20000], Loss: -21.050125122070312, Learning Rate: 0.01\n",
      "Epoch [15160/20000], Loss: -21.050399780273438, Learning Rate: 0.01\n",
      "Epoch [15161/20000], Loss: -21.050643920898438, Learning Rate: 0.01\n",
      "Epoch [15162/20000], Loss: -21.0509033203125, Learning Rate: 0.01\n",
      "Epoch [15163/20000], Loss: -21.05108642578125, Learning Rate: 0.01\n",
      "Epoch [15164/20000], Loss: -21.051040649414062, Learning Rate: 0.01\n",
      "Epoch [15165/20000], Loss: -21.050796508789062, Learning Rate: 0.01\n",
      "Epoch [15166/20000], Loss: -21.050155639648438, Learning Rate: 0.01\n",
      "Epoch [15167/20000], Loss: -21.049026489257812, Learning Rate: 0.01\n",
      "Epoch [15168/20000], Loss: -21.046722412109375, Learning Rate: 0.01\n",
      "Epoch [15169/20000], Loss: -21.042755126953125, Learning Rate: 0.01\n",
      "Epoch [15170/20000], Loss: -21.036346435546875, Learning Rate: 0.01\n",
      "Epoch [15171/20000], Loss: -21.02557373046875, Learning Rate: 0.01\n",
      "Epoch [15172/20000], Loss: -21.0076904296875, Learning Rate: 0.01\n",
      "Epoch [15173/20000], Loss: -20.97821044921875, Learning Rate: 0.01\n",
      "Epoch [15174/20000], Loss: -20.930023193359375, Learning Rate: 0.01\n",
      "Epoch [15175/20000], Loss: -20.85107421875, Learning Rate: 0.01\n",
      "Epoch [15176/20000], Loss: -20.725143432617188, Learning Rate: 0.01\n",
      "Epoch [15177/20000], Loss: -20.526107788085938, Learning Rate: 0.01\n",
      "Epoch [15178/20000], Loss: -20.231460571289062, Learning Rate: 0.01\n",
      "Epoch [15179/20000], Loss: -19.819107055664062, Learning Rate: 0.01\n",
      "Epoch [15180/20000], Loss: -19.339797973632812, Learning Rate: 0.01\n",
      "Epoch [15181/20000], Loss: -18.904144287109375, Learning Rate: 0.01\n",
      "Epoch [15182/20000], Loss: -18.802383422851562, Learning Rate: 0.01\n",
      "Epoch [15183/20000], Loss: -19.197265625, Learning Rate: 0.01\n",
      "Epoch [15184/20000], Loss: -20.03375244140625, Learning Rate: 0.01\n",
      "Epoch [15185/20000], Loss: -20.802566528320312, Learning Rate: 0.01\n",
      "Epoch [15186/20000], Loss: -21.053131103515625, Learning Rate: 0.01\n",
      "Epoch [15187/20000], Loss: -20.753768920898438, Learning Rate: 0.01\n",
      "Epoch [15188/20000], Loss: -20.287933349609375, Learning Rate: 0.01\n",
      "Epoch [15189/20000], Loss: -20.114212036132812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15190/20000], Loss: -20.376907348632812, Learning Rate: 0.01\n",
      "Epoch [15191/20000], Loss: -20.828811645507812, Learning Rate: 0.01\n",
      "Epoch [15192/20000], Loss: -21.059524536132812, Learning Rate: 0.01\n",
      "Epoch [15193/20000], Loss: -20.932052612304688, Learning Rate: 0.01\n",
      "Epoch [15194/20000], Loss: -20.66705322265625, Learning Rate: 0.01\n",
      "Epoch [15195/20000], Loss: -20.57196044921875, Learning Rate: 0.01\n",
      "Epoch [15196/20000], Loss: -20.74090576171875, Learning Rate: 0.01\n",
      "Epoch [15197/20000], Loss: -20.982345581054688, Learning Rate: 0.01\n",
      "Epoch [15198/20000], Loss: -21.063980102539062, Learning Rate: 0.01\n",
      "Epoch [15199/20000], Loss: -20.953414916992188, Learning Rate: 0.01\n",
      "Epoch [15200/20000], Loss: -20.817276000976562, Learning Rate: 0.01\n",
      "Epoch [15201/20000], Loss: -20.816879272460938, Learning Rate: 0.01\n",
      "Epoch [15202/20000], Loss: -20.942001342773438, Learning Rate: 0.01\n",
      "Epoch [15203/20000], Loss: -21.054031372070312, Learning Rate: 0.01\n",
      "Epoch [15204/20000], Loss: -21.05084228515625, Learning Rate: 0.01\n",
      "Epoch [15205/20000], Loss: -20.968826293945312, Learning Rate: 0.01\n",
      "Epoch [15206/20000], Loss: -20.918777465820312, Learning Rate: 0.01\n",
      "Epoch [15207/20000], Loss: -20.955718994140625, Learning Rate: 0.01\n",
      "Epoch [15208/20000], Loss: -21.033462524414062, Learning Rate: 0.01\n",
      "Epoch [15209/20000], Loss: -21.070358276367188, Learning Rate: 0.01\n",
      "Epoch [15210/20000], Loss: -21.042266845703125, Learning Rate: 0.01\n",
      "Epoch [15211/20000], Loss: -20.996902465820312, Learning Rate: 0.01\n",
      "Epoch [15212/20000], Loss: -20.990158081054688, Learning Rate: 0.01\n",
      "Epoch [15213/20000], Loss: -21.026962280273438, Learning Rate: 0.01\n",
      "Epoch [15214/20000], Loss: -21.065338134765625, Learning Rate: 0.01\n",
      "Epoch [15215/20000], Loss: -21.0694580078125, Learning Rate: 0.01\n",
      "Epoch [15216/20000], Loss: -21.045318603515625, Learning Rate: 0.01\n",
      "Epoch [15217/20000], Loss: -21.026626586914062, Learning Rate: 0.01\n",
      "Epoch [15218/20000], Loss: -21.034652709960938, Learning Rate: 0.01\n",
      "Epoch [15219/20000], Loss: -21.058914184570312, Learning Rate: 0.01\n",
      "Epoch [15220/20000], Loss: -21.074142456054688, Learning Rate: 0.01\n",
      "Epoch [15221/20000], Loss: -21.068954467773438, Learning Rate: 0.01\n",
      "Epoch [15222/20000], Loss: -21.054458618164062, Learning Rate: 0.01\n",
      "Epoch [15223/20000], Loss: -21.049240112304688, Learning Rate: 0.01\n",
      "Epoch [15224/20000], Loss: -21.058425903320312, Learning Rate: 0.01\n",
      "Epoch [15225/20000], Loss: -21.07177734375, Learning Rate: 0.01\n",
      "Epoch [15226/20000], Loss: -21.076751708984375, Learning Rate: 0.01\n",
      "Epoch [15227/20000], Loss: -21.071334838867188, Learning Rate: 0.01\n",
      "Epoch [15228/20000], Loss: -21.064178466796875, Learning Rate: 0.01\n",
      "Epoch [15229/20000], Loss: -21.0638427734375, Learning Rate: 0.01\n",
      "Epoch [15230/20000], Loss: -21.07049560546875, Learning Rate: 0.01\n",
      "Epoch [15231/20000], Loss: -21.0772705078125, Learning Rate: 0.01\n",
      "Epoch [15232/20000], Loss: -21.078643798828125, Learning Rate: 0.01\n",
      "Epoch [15233/20000], Loss: -21.075042724609375, Learning Rate: 0.01\n",
      "Epoch [15234/20000], Loss: -21.072006225585938, Learning Rate: 0.01\n",
      "Epoch [15235/20000], Loss: -21.0728759765625, Learning Rate: 0.01\n",
      "Epoch [15236/20000], Loss: -21.076873779296875, Learning Rate: 0.01\n",
      "Epoch [15237/20000], Loss: -21.080474853515625, Learning Rate: 0.01\n",
      "Epoch [15238/20000], Loss: -21.08087158203125, Learning Rate: 0.01\n",
      "Epoch [15239/20000], Loss: -21.078903198242188, Learning Rate: 0.01\n",
      "Epoch [15240/20000], Loss: -21.077606201171875, Learning Rate: 0.01\n",
      "Epoch [15241/20000], Loss: -21.078414916992188, Learning Rate: 0.01\n",
      "Epoch [15242/20000], Loss: -21.080963134765625, Learning Rate: 0.01\n",
      "Epoch [15243/20000], Loss: -21.082763671875, Learning Rate: 0.01\n",
      "Epoch [15244/20000], Loss: -21.083099365234375, Learning Rate: 0.01\n",
      "Epoch [15245/20000], Loss: -21.082290649414062, Learning Rate: 0.01\n",
      "Epoch [15246/20000], Loss: -21.081741333007812, Learning Rate: 0.01\n",
      "Epoch [15247/20000], Loss: -21.082305908203125, Learning Rate: 0.01\n",
      "Epoch [15248/20000], Loss: -21.083847045898438, Learning Rate: 0.01\n",
      "Epoch [15249/20000], Loss: -21.08502197265625, Learning Rate: 0.01\n",
      "Epoch [15250/20000], Loss: -21.085433959960938, Learning Rate: 0.01\n",
      "Epoch [15251/20000], Loss: -21.085220336914062, Learning Rate: 0.01\n",
      "Epoch [15252/20000], Loss: -21.085052490234375, Learning Rate: 0.01\n",
      "Epoch [15253/20000], Loss: -21.08538818359375, Learning Rate: 0.01\n",
      "Epoch [15254/20000], Loss: -21.086273193359375, Learning Rate: 0.01\n",
      "Epoch [15255/20000], Loss: -21.087158203125, Learning Rate: 0.01\n",
      "Epoch [15256/20000], Loss: -21.087677001953125, Learning Rate: 0.01\n",
      "Epoch [15257/20000], Loss: -21.087738037109375, Learning Rate: 0.01\n",
      "Epoch [15258/20000], Loss: -21.087677001953125, Learning Rate: 0.01\n",
      "Epoch [15259/20000], Loss: -21.088043212890625, Learning Rate: 0.01\n",
      "Epoch [15260/20000], Loss: -21.088668823242188, Learning Rate: 0.01\n",
      "Epoch [15261/20000], Loss: -21.089279174804688, Learning Rate: 0.01\n",
      "Epoch [15262/20000], Loss: -21.089813232421875, Learning Rate: 0.01\n",
      "Epoch [15263/20000], Loss: -21.090057373046875, Learning Rate: 0.01\n",
      "Epoch [15264/20000], Loss: -21.090194702148438, Learning Rate: 0.01\n",
      "Epoch [15265/20000], Loss: -21.090530395507812, Learning Rate: 0.01\n",
      "Epoch [15266/20000], Loss: -21.0909423828125, Learning Rate: 0.01\n",
      "Epoch [15267/20000], Loss: -21.091293334960938, Learning Rate: 0.01\n",
      "Epoch [15268/20000], Loss: -21.0919189453125, Learning Rate: 0.01\n",
      "Epoch [15269/20000], Loss: -21.092239379882812, Learning Rate: 0.01\n",
      "Epoch [15270/20000], Loss: -21.092453002929688, Learning Rate: 0.01\n",
      "Epoch [15271/20000], Loss: -21.092788696289062, Learning Rate: 0.01\n",
      "Epoch [15272/20000], Loss: -21.093154907226562, Learning Rate: 0.01\n",
      "Epoch [15273/20000], Loss: -21.093505859375, Learning Rate: 0.01\n",
      "Epoch [15274/20000], Loss: -21.093978881835938, Learning Rate: 0.01\n",
      "Epoch [15275/20000], Loss: -21.0943603515625, Learning Rate: 0.01\n",
      "Epoch [15276/20000], Loss: -21.094635009765625, Learning Rate: 0.01\n",
      "Epoch [15277/20000], Loss: -21.094940185546875, Learning Rate: 0.01\n",
      "Epoch [15278/20000], Loss: -21.095260620117188, Learning Rate: 0.01\n",
      "Epoch [15279/20000], Loss: -21.0955810546875, Learning Rate: 0.01\n",
      "Epoch [15280/20000], Loss: -21.095993041992188, Learning Rate: 0.01\n",
      "Epoch [15281/20000], Loss: -21.096450805664062, Learning Rate: 0.01\n",
      "Epoch [15282/20000], Loss: -21.09674072265625, Learning Rate: 0.01\n",
      "Epoch [15283/20000], Loss: -21.09710693359375, Learning Rate: 0.01\n",
      "Epoch [15284/20000], Loss: -21.097381591796875, Learning Rate: 0.01\n",
      "Epoch [15285/20000], Loss: -21.097671508789062, Learning Rate: 0.01\n",
      "Epoch [15286/20000], Loss: -21.098037719726562, Learning Rate: 0.01\n",
      "Epoch [15287/20000], Loss: -21.09832763671875, Learning Rate: 0.01\n",
      "Epoch [15288/20000], Loss: -21.098770141601562, Learning Rate: 0.01\n",
      "Epoch [15289/20000], Loss: -21.09912109375, Learning Rate: 0.01\n",
      "Epoch [15290/20000], Loss: -21.099502563476562, Learning Rate: 0.01\n",
      "Epoch [15291/20000], Loss: -21.09979248046875, Learning Rate: 0.01\n",
      "Epoch [15292/20000], Loss: -21.100051879882812, Learning Rate: 0.01\n",
      "Epoch [15293/20000], Loss: -21.100418090820312, Learning Rate: 0.01\n",
      "Epoch [15294/20000], Loss: -21.100845336914062, Learning Rate: 0.01\n",
      "Epoch [15295/20000], Loss: -21.10107421875, Learning Rate: 0.01\n",
      "Epoch [15296/20000], Loss: -21.101516723632812, Learning Rate: 0.01\n",
      "Epoch [15297/20000], Loss: -21.101898193359375, Learning Rate: 0.01\n",
      "Epoch [15298/20000], Loss: -21.102188110351562, Learning Rate: 0.01\n",
      "Epoch [15299/20000], Loss: -21.102447509765625, Learning Rate: 0.01\n",
      "Epoch [15300/20000], Loss: -21.102798461914062, Learning Rate: 0.01\n",
      "Epoch [15301/20000], Loss: -21.1031494140625, Learning Rate: 0.01\n",
      "Epoch [15302/20000], Loss: -21.103515625, Learning Rate: 0.01\n",
      "Epoch [15303/20000], Loss: -21.103912353515625, Learning Rate: 0.01\n",
      "Epoch [15304/20000], Loss: -21.104232788085938, Learning Rate: 0.01\n",
      "Epoch [15305/20000], Loss: -21.104446411132812, Learning Rate: 0.01\n",
      "Epoch [15306/20000], Loss: -21.104843139648438, Learning Rate: 0.01\n",
      "Epoch [15307/20000], Loss: -21.105209350585938, Learning Rate: 0.01\n",
      "Epoch [15308/20000], Loss: -21.105621337890625, Learning Rate: 0.01\n",
      "Epoch [15309/20000], Loss: -21.105758666992188, Learning Rate: 0.01\n",
      "Epoch [15310/20000], Loss: -21.106216430664062, Learning Rate: 0.01\n",
      "Epoch [15311/20000], Loss: -21.106536865234375, Learning Rate: 0.01\n",
      "Epoch [15312/20000], Loss: -21.106857299804688, Learning Rate: 0.01\n",
      "Epoch [15313/20000], Loss: -21.107254028320312, Learning Rate: 0.01\n",
      "Epoch [15314/20000], Loss: -21.107559204101562, Learning Rate: 0.01\n",
      "Epoch [15315/20000], Loss: -21.107803344726562, Learning Rate: 0.01\n",
      "Epoch [15316/20000], Loss: -21.108200073242188, Learning Rate: 0.01\n",
      "Epoch [15317/20000], Loss: -21.108505249023438, Learning Rate: 0.01\n",
      "Epoch [15318/20000], Loss: -21.10882568359375, Learning Rate: 0.01\n",
      "Epoch [15319/20000], Loss: -21.10919189453125, Learning Rate: 0.01\n",
      "Epoch [15320/20000], Loss: -21.109527587890625, Learning Rate: 0.01\n",
      "Epoch [15321/20000], Loss: -21.109848022460938, Learning Rate: 0.01\n",
      "Epoch [15322/20000], Loss: -21.110244750976562, Learning Rate: 0.01\n",
      "Epoch [15323/20000], Loss: -21.110504150390625, Learning Rate: 0.01\n",
      "Epoch [15324/20000], Loss: -21.110824584960938, Learning Rate: 0.01\n",
      "Epoch [15325/20000], Loss: -21.111114501953125, Learning Rate: 0.01\n",
      "Epoch [15326/20000], Loss: -21.111480712890625, Learning Rate: 0.01\n",
      "Epoch [15327/20000], Loss: -21.11181640625, Learning Rate: 0.01\n",
      "Epoch [15328/20000], Loss: -21.112167358398438, Learning Rate: 0.01\n",
      "Epoch [15329/20000], Loss: -21.112396240234375, Learning Rate: 0.01\n",
      "Epoch [15330/20000], Loss: -21.112838745117188, Learning Rate: 0.01\n",
      "Epoch [15331/20000], Loss: -21.113006591796875, Learning Rate: 0.01\n",
      "Epoch [15332/20000], Loss: -21.1134033203125, Learning Rate: 0.01\n",
      "Epoch [15333/20000], Loss: -21.1136474609375, Learning Rate: 0.01\n",
      "Epoch [15334/20000], Loss: -21.11395263671875, Learning Rate: 0.01\n",
      "Epoch [15335/20000], Loss: -21.114273071289062, Learning Rate: 0.01\n",
      "Epoch [15336/20000], Loss: -21.11456298828125, Learning Rate: 0.01\n",
      "Epoch [15337/20000], Loss: -21.114791870117188, Learning Rate: 0.01\n",
      "Epoch [15338/20000], Loss: -21.114974975585938, Learning Rate: 0.01\n",
      "Epoch [15339/20000], Loss: -21.115203857421875, Learning Rate: 0.01\n",
      "Epoch [15340/20000], Loss: -21.115341186523438, Learning Rate: 0.01\n",
      "Epoch [15341/20000], Loss: -21.115447998046875, Learning Rate: 0.01\n",
      "Epoch [15342/20000], Loss: -21.11553955078125, Learning Rate: 0.01\n",
      "Epoch [15343/20000], Loss: -21.115264892578125, Learning Rate: 0.01\n",
      "Epoch [15344/20000], Loss: -21.114990234375, Learning Rate: 0.01\n",
      "Epoch [15345/20000], Loss: -21.114364624023438, Learning Rate: 0.01\n",
      "Epoch [15346/20000], Loss: -21.11334228515625, Learning Rate: 0.01\n",
      "Epoch [15347/20000], Loss: -21.111679077148438, Learning Rate: 0.01\n",
      "Epoch [15348/20000], Loss: -21.10906982421875, Learning Rate: 0.01\n",
      "Epoch [15349/20000], Loss: -21.105178833007812, Learning Rate: 0.01\n",
      "Epoch [15350/20000], Loss: -21.099197387695312, Learning Rate: 0.01\n",
      "Epoch [15351/20000], Loss: -21.090377807617188, Learning Rate: 0.01\n",
      "Epoch [15352/20000], Loss: -21.077301025390625, Learning Rate: 0.01\n",
      "Epoch [15353/20000], Loss: -21.057846069335938, Learning Rate: 0.01\n",
      "Epoch [15354/20000], Loss: -21.029220581054688, Learning Rate: 0.01\n",
      "Epoch [15355/20000], Loss: -20.987380981445312, Learning Rate: 0.01\n",
      "Epoch [15356/20000], Loss: -20.927261352539062, Learning Rate: 0.01\n",
      "Epoch [15357/20000], Loss: -20.842483520507812, Learning Rate: 0.01\n",
      "Epoch [15358/20000], Loss: -20.728469848632812, Learning Rate: 0.01\n",
      "Epoch [15359/20000], Loss: -20.5819091796875, Learning Rate: 0.01\n",
      "Epoch [15360/20000], Loss: -20.414291381835938, Learning Rate: 0.01\n",
      "Epoch [15361/20000], Loss: -20.248779296875, Learning Rate: 0.01\n",
      "Epoch [15362/20000], Loss: -20.142074584960938, Learning Rate: 0.01\n",
      "Epoch [15363/20000], Loss: -20.148208618164062, Learning Rate: 0.01\n",
      "Epoch [15364/20000], Loss: -20.312026977539062, Learning Rate: 0.01\n",
      "Epoch [15365/20000], Loss: -20.593414306640625, Learning Rate: 0.01\n",
      "Epoch [15366/20000], Loss: -20.891387939453125, Learning Rate: 0.01\n",
      "Epoch [15367/20000], Loss: -21.08294677734375, Learning Rate: 0.01\n",
      "Epoch [15368/20000], Loss: -21.110595703125, Learning Rate: 0.01\n",
      "Epoch [15369/20000], Loss: -21.007354736328125, Learning Rate: 0.01\n",
      "Epoch [15370/20000], Loss: -20.865921020507812, Learning Rate: 0.01\n",
      "Epoch [15371/20000], Loss: -20.7830810546875, Learning Rate: 0.01\n",
      "Epoch [15372/20000], Loss: -20.806121826171875, Learning Rate: 0.01\n",
      "Epoch [15373/20000], Loss: -20.916976928710938, Learning Rate: 0.01\n",
      "Epoch [15374/20000], Loss: -21.042861938476562, Learning Rate: 0.01\n",
      "Epoch [15375/20000], Loss: -21.113311767578125, Learning Rate: 0.01\n",
      "Epoch [15376/20000], Loss: -21.10333251953125, Learning Rate: 0.01\n",
      "Epoch [15377/20000], Loss: -21.042755126953125, Learning Rate: 0.01\n",
      "Epoch [15378/20000], Loss: -20.987457275390625, Learning Rate: 0.01\n",
      "Epoch [15379/20000], Loss: -20.978195190429688, Learning Rate: 0.01\n",
      "Epoch [15380/20000], Loss: -21.019180297851562, Learning Rate: 0.01\n",
      "Epoch [15381/20000], Loss: -21.079238891601562, Learning Rate: 0.01\n",
      "Epoch [15382/20000], Loss: -21.11981201171875, Learning Rate: 0.01\n",
      "Epoch [15383/20000], Loss: -21.121551513671875, Learning Rate: 0.01\n",
      "Epoch [15384/20000], Loss: -21.094284057617188, Learning Rate: 0.01\n",
      "Epoch [15385/20000], Loss: -21.0650634765625, Learning Rate: 0.01\n",
      "Epoch [15386/20000], Loss: -21.056838989257812, Learning Rate: 0.01\n",
      "Epoch [15387/20000], Loss: -21.0745849609375, Learning Rate: 0.01\n",
      "Epoch [15388/20000], Loss: -21.104721069335938, Learning Rate: 0.01\n",
      "Epoch [15389/20000], Loss: -21.127334594726562, Learning Rate: 0.01\n",
      "Epoch [15390/20000], Loss: -21.131210327148438, Learning Rate: 0.01\n",
      "Epoch [15391/20000], Loss: -21.119155883789062, Learning Rate: 0.01\n",
      "Epoch [15392/20000], Loss: -21.10369873046875, Learning Rate: 0.01\n",
      "Epoch [15393/20000], Loss: -21.096923828125, Learning Rate: 0.01\n",
      "Epoch [15394/20000], Loss: -21.103134155273438, Learning Rate: 0.01\n",
      "Epoch [15395/20000], Loss: -21.117095947265625, Learning Rate: 0.01\n",
      "Epoch [15396/20000], Loss: -21.129608154296875, Learning Rate: 0.01\n",
      "Epoch [15397/20000], Loss: -21.134048461914062, Learning Rate: 0.01\n",
      "Epoch [15398/20000], Loss: -21.13031005859375, Learning Rate: 0.01\n",
      "Epoch [15399/20000], Loss: -21.123397827148438, Learning Rate: 0.01\n",
      "Epoch [15400/20000], Loss: -21.119140625, Learning Rate: 0.01\n",
      "Epoch [15401/20000], Loss: -21.120712280273438, Learning Rate: 0.01\n",
      "Epoch [15402/20000], Loss: -21.126434326171875, Learning Rate: 0.01\n",
      "Epoch [15403/20000], Loss: -21.132659912109375, Learning Rate: 0.01\n",
      "Epoch [15404/20000], Loss: -21.135910034179688, Learning Rate: 0.01\n",
      "Epoch [15405/20000], Loss: -21.135238647460938, Learning Rate: 0.01\n",
      "Epoch [15406/20000], Loss: -21.132400512695312, Learning Rate: 0.01\n",
      "Epoch [15407/20000], Loss: -21.129974365234375, Learning Rate: 0.01\n",
      "Epoch [15408/20000], Loss: -21.12994384765625, Learning Rate: 0.01\n",
      "Epoch [15409/20000], Loss: -21.132278442382812, Learning Rate: 0.01\n",
      "Epoch [15410/20000], Loss: -21.135665893554688, Learning Rate: 0.01\n",
      "Epoch [15411/20000], Loss: -21.138153076171875, Learning Rate: 0.01\n",
      "Epoch [15412/20000], Loss: -21.138778686523438, Learning Rate: 0.01\n",
      "Epoch [15413/20000], Loss: -21.13800048828125, Learning Rate: 0.01\n",
      "Epoch [15414/20000], Loss: -21.136703491210938, Learning Rate: 0.01\n",
      "Epoch [15415/20000], Loss: -21.136032104492188, Learning Rate: 0.01\n",
      "Epoch [15416/20000], Loss: -21.136581420898438, Learning Rate: 0.01\n",
      "Epoch [15417/20000], Loss: -21.137939453125, Learning Rate: 0.01\n",
      "Epoch [15418/20000], Loss: -21.139572143554688, Learning Rate: 0.01\n",
      "Epoch [15419/20000], Loss: -21.140609741210938, Learning Rate: 0.01\n",
      "Epoch [15420/20000], Loss: -21.140869140625, Learning Rate: 0.01\n",
      "Epoch [15421/20000], Loss: -21.140640258789062, Learning Rate: 0.01\n",
      "Epoch [15422/20000], Loss: -21.140304565429688, Learning Rate: 0.01\n",
      "Epoch [15423/20000], Loss: -21.1402587890625, Learning Rate: 0.01\n",
      "Epoch [15424/20000], Loss: -21.140792846679688, Learning Rate: 0.01\n",
      "Epoch [15425/20000], Loss: -21.141586303710938, Learning Rate: 0.01\n",
      "Epoch [15426/20000], Loss: -21.1424560546875, Learning Rate: 0.01\n",
      "Epoch [15427/20000], Loss: -21.143035888671875, Learning Rate: 0.01\n",
      "Epoch [15428/20000], Loss: -21.14324951171875, Learning Rate: 0.01\n",
      "Epoch [15429/20000], Loss: -21.143280029296875, Learning Rate: 0.01\n",
      "Epoch [15430/20000], Loss: -21.143280029296875, Learning Rate: 0.01\n",
      "Epoch [15431/20000], Loss: -21.1434326171875, Learning Rate: 0.01\n",
      "Epoch [15432/20000], Loss: -21.143753051757812, Learning Rate: 0.01\n",
      "Epoch [15433/20000], Loss: -21.144180297851562, Learning Rate: 0.01\n",
      "Epoch [15434/20000], Loss: -21.14471435546875, Learning Rate: 0.01\n",
      "Epoch [15435/20000], Loss: -21.145126342773438, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15436/20000], Loss: -21.145462036132812, Learning Rate: 0.01\n",
      "Epoch [15437/20000], Loss: -21.1456298828125, Learning Rate: 0.01\n",
      "Epoch [15438/20000], Loss: -21.14581298828125, Learning Rate: 0.01\n",
      "Epoch [15439/20000], Loss: -21.146041870117188, Learning Rate: 0.01\n",
      "Epoch [15440/20000], Loss: -21.146255493164062, Learning Rate: 0.01\n",
      "Epoch [15441/20000], Loss: -21.146636962890625, Learning Rate: 0.01\n",
      "Epoch [15442/20000], Loss: -21.147079467773438, Learning Rate: 0.01\n",
      "Epoch [15443/20000], Loss: -21.147323608398438, Learning Rate: 0.01\n",
      "Epoch [15444/20000], Loss: -21.147705078125, Learning Rate: 0.01\n",
      "Epoch [15445/20000], Loss: -21.147933959960938, Learning Rate: 0.01\n",
      "Epoch [15446/20000], Loss: -21.148208618164062, Learning Rate: 0.01\n",
      "Epoch [15447/20000], Loss: -21.148330688476562, Learning Rate: 0.01\n",
      "Epoch [15448/20000], Loss: -21.14862060546875, Learning Rate: 0.01\n",
      "Epoch [15449/20000], Loss: -21.148895263671875, Learning Rate: 0.01\n",
      "Epoch [15450/20000], Loss: -21.149093627929688, Learning Rate: 0.01\n",
      "Epoch [15451/20000], Loss: -21.149505615234375, Learning Rate: 0.01\n",
      "Epoch [15452/20000], Loss: -21.149749755859375, Learning Rate: 0.01\n",
      "Epoch [15453/20000], Loss: -21.150115966796875, Learning Rate: 0.01\n",
      "Epoch [15454/20000], Loss: -21.150405883789062, Learning Rate: 0.01\n",
      "Epoch [15455/20000], Loss: -21.15057373046875, Learning Rate: 0.01\n",
      "Epoch [15456/20000], Loss: -21.150833129882812, Learning Rate: 0.01\n",
      "Epoch [15457/20000], Loss: -21.151046752929688, Learning Rate: 0.01\n",
      "Epoch [15458/20000], Loss: -21.151336669921875, Learning Rate: 0.01\n",
      "Epoch [15459/20000], Loss: -21.151519775390625, Learning Rate: 0.01\n",
      "Epoch [15460/20000], Loss: -21.151779174804688, Learning Rate: 0.01\n",
      "Epoch [15461/20000], Loss: -21.152130126953125, Learning Rate: 0.01\n",
      "Epoch [15462/20000], Loss: -21.152297973632812, Learning Rate: 0.01\n",
      "Epoch [15463/20000], Loss: -21.152603149414062, Learning Rate: 0.01\n",
      "Epoch [15464/20000], Loss: -21.152740478515625, Learning Rate: 0.01\n",
      "Epoch [15465/20000], Loss: -21.152862548828125, Learning Rate: 0.01\n",
      "Epoch [15466/20000], Loss: -21.15301513671875, Learning Rate: 0.01\n",
      "Epoch [15467/20000], Loss: -21.152938842773438, Learning Rate: 0.01\n",
      "Epoch [15468/20000], Loss: -21.152969360351562, Learning Rate: 0.01\n",
      "Epoch [15469/20000], Loss: -21.152816772460938, Learning Rate: 0.01\n",
      "Epoch [15470/20000], Loss: -21.1524658203125, Learning Rate: 0.01\n",
      "Epoch [15471/20000], Loss: -21.15185546875, Learning Rate: 0.01\n",
      "Epoch [15472/20000], Loss: -21.150848388671875, Learning Rate: 0.01\n",
      "Epoch [15473/20000], Loss: -21.149337768554688, Learning Rate: 0.01\n",
      "Epoch [15474/20000], Loss: -21.146835327148438, Learning Rate: 0.01\n",
      "Epoch [15475/20000], Loss: -21.143264770507812, Learning Rate: 0.01\n",
      "Epoch [15476/20000], Loss: -21.137588500976562, Learning Rate: 0.01\n",
      "Epoch [15477/20000], Loss: -21.128982543945312, Learning Rate: 0.01\n",
      "Epoch [15478/20000], Loss: -21.115966796875, Learning Rate: 0.01\n",
      "Epoch [15479/20000], Loss: -21.096343994140625, Learning Rate: 0.01\n",
      "Epoch [15480/20000], Loss: -21.066650390625, Learning Rate: 0.01\n",
      "Epoch [15481/20000], Loss: -21.021957397460938, Learning Rate: 0.01\n",
      "Epoch [15482/20000], Loss: -20.954864501953125, Learning Rate: 0.01\n",
      "Epoch [15483/20000], Loss: -20.85699462890625, Learning Rate: 0.01\n",
      "Epoch [15484/20000], Loss: -20.71527099609375, Learning Rate: 0.01\n",
      "Epoch [15485/20000], Loss: -20.523818969726562, Learning Rate: 0.01\n",
      "Epoch [15486/20000], Loss: -20.276443481445312, Learning Rate: 0.01\n",
      "Epoch [15487/20000], Loss: -20.006988525390625, Learning Rate: 0.01\n",
      "Epoch [15488/20000], Loss: -19.765045166015625, Learning Rate: 0.01\n",
      "Epoch [15489/20000], Loss: -19.677230834960938, Learning Rate: 0.01\n",
      "Epoch [15490/20000], Loss: -19.81878662109375, Learning Rate: 0.01\n",
      "Epoch [15491/20000], Loss: -20.219345092773438, Learning Rate: 0.01\n",
      "Epoch [15492/20000], Loss: -20.709121704101562, Learning Rate: 0.01\n",
      "Epoch [15493/20000], Loss: -21.069564819335938, Learning Rate: 0.01\n",
      "Epoch [15494/20000], Loss: -21.1517333984375, Learning Rate: 0.01\n",
      "Epoch [15495/20000], Loss: -20.9884033203125, Learning Rate: 0.01\n",
      "Epoch [15496/20000], Loss: -20.745925903320312, Learning Rate: 0.01\n",
      "Epoch [15497/20000], Loss: -20.604934692382812, Learning Rate: 0.01\n",
      "Epoch [15498/20000], Loss: -20.663925170898438, Learning Rate: 0.01\n",
      "Epoch [15499/20000], Loss: -20.868759155273438, Learning Rate: 0.01\n",
      "Epoch [15500/20000], Loss: -21.077011108398438, Learning Rate: 0.01\n",
      "Epoch [15501/20000], Loss: -21.158584594726562, Learning Rate: 0.01\n",
      "Epoch [15502/20000], Loss: -21.095718383789062, Learning Rate: 0.01\n",
      "Epoch [15503/20000], Loss: -20.973922729492188, Learning Rate: 0.01\n",
      "Epoch [15504/20000], Loss: -20.902603149414062, Learning Rate: 0.01\n",
      "Epoch [15505/20000], Loss: -20.936416625976562, Learning Rate: 0.01\n",
      "Epoch [15506/20000], Loss: -21.040420532226562, Learning Rate: 0.01\n",
      "Epoch [15507/20000], Loss: -21.134872436523438, Learning Rate: 0.01\n",
      "Epoch [15508/20000], Loss: -21.159133911132812, Learning Rate: 0.01\n",
      "Epoch [15509/20000], Loss: -21.1162109375, Learning Rate: 0.01\n",
      "Epoch [15510/20000], Loss: -21.057769775390625, Learning Rate: 0.01\n",
      "Epoch [15511/20000], Loss: -21.036056518554688, Learning Rate: 0.01\n",
      "Epoch [15512/20000], Loss: -21.066452026367188, Learning Rate: 0.01\n",
      "Epoch [15513/20000], Loss: -21.121109008789062, Learning Rate: 0.01\n",
      "Epoch [15514/20000], Loss: -21.159286499023438, Learning Rate: 0.01\n",
      "Epoch [15515/20000], Loss: -21.159408569335938, Learning Rate: 0.01\n",
      "Epoch [15516/20000], Loss: -21.132293701171875, Learning Rate: 0.01\n",
      "Epoch [15517/20000], Loss: -21.106582641601562, Learning Rate: 0.01\n",
      "Epoch [15518/20000], Loss: -21.103973388671875, Learning Rate: 0.01\n",
      "Epoch [15519/20000], Loss: -21.124893188476562, Learning Rate: 0.01\n",
      "Epoch [15520/20000], Loss: -21.152114868164062, Learning Rate: 0.01\n",
      "Epoch [15521/20000], Loss: -21.166610717773438, Learning Rate: 0.01\n",
      "Epoch [15522/20000], Loss: -21.162307739257812, Learning Rate: 0.01\n",
      "Epoch [15523/20000], Loss: -21.147628784179688, Learning Rate: 0.01\n",
      "Epoch [15524/20000], Loss: -21.136810302734375, Learning Rate: 0.01\n",
      "Epoch [15525/20000], Loss: -21.138137817382812, Learning Rate: 0.01\n",
      "Epoch [15526/20000], Loss: -21.149932861328125, Learning Rate: 0.01\n",
      "Epoch [15527/20000], Loss: -21.163192749023438, Learning Rate: 0.01\n",
      "Epoch [15528/20000], Loss: -21.169357299804688, Learning Rate: 0.01\n",
      "Epoch [15529/20000], Loss: -21.166534423828125, Learning Rate: 0.01\n",
      "Epoch [15530/20000], Loss: -21.159347534179688, Learning Rate: 0.01\n",
      "Epoch [15531/20000], Loss: -21.154342651367188, Learning Rate: 0.01\n",
      "Epoch [15532/20000], Loss: -21.155166625976562, Learning Rate: 0.01\n",
      "Epoch [15533/20000], Loss: -21.160964965820312, Learning Rate: 0.01\n",
      "Epoch [15534/20000], Loss: -21.167617797851562, Learning Rate: 0.01\n",
      "Epoch [15535/20000], Loss: -21.171066284179688, Learning Rate: 0.01\n",
      "Epoch [15536/20000], Loss: -21.170166015625, Learning Rate: 0.01\n",
      "Epoch [15537/20000], Loss: -21.166961669921875, Learning Rate: 0.01\n",
      "Epoch [15538/20000], Loss: -21.164306640625, Learning Rate: 0.01\n",
      "Epoch [15539/20000], Loss: -21.1644287109375, Learning Rate: 0.01\n",
      "Epoch [15540/20000], Loss: -21.166915893554688, Learning Rate: 0.01\n",
      "Epoch [15541/20000], Loss: -21.170272827148438, Learning Rate: 0.01\n",
      "Epoch [15542/20000], Loss: -21.172531127929688, Learning Rate: 0.01\n",
      "Epoch [15543/20000], Loss: -21.172760009765625, Learning Rate: 0.01\n",
      "Epoch [15544/20000], Loss: -21.1717529296875, Learning Rate: 0.01\n",
      "Epoch [15545/20000], Loss: -21.17041015625, Learning Rate: 0.01\n",
      "Epoch [15546/20000], Loss: -21.169952392578125, Learning Rate: 0.01\n",
      "Epoch [15547/20000], Loss: -21.170791625976562, Learning Rate: 0.01\n",
      "Epoch [15548/20000], Loss: -21.172393798828125, Learning Rate: 0.01\n",
      "Epoch [15549/20000], Loss: -21.173904418945312, Learning Rate: 0.01\n",
      "Epoch [15550/20000], Loss: -21.174697875976562, Learning Rate: 0.01\n",
      "Epoch [15551/20000], Loss: -21.1746826171875, Learning Rate: 0.01\n",
      "Epoch [15552/20000], Loss: -21.1741943359375, Learning Rate: 0.01\n",
      "Epoch [15553/20000], Loss: -21.173828125, Learning Rate: 0.01\n",
      "Epoch [15554/20000], Loss: -21.173965454101562, Learning Rate: 0.01\n",
      "Epoch [15555/20000], Loss: -21.174530029296875, Learning Rate: 0.01\n",
      "Epoch [15556/20000], Loss: -21.175399780273438, Learning Rate: 0.01\n",
      "Epoch [15557/20000], Loss: -21.1761474609375, Learning Rate: 0.01\n",
      "Epoch [15558/20000], Loss: -21.176589965820312, Learning Rate: 0.01\n",
      "Epoch [15559/20000], Loss: -21.176666259765625, Learning Rate: 0.01\n",
      "Epoch [15560/20000], Loss: -21.176605224609375, Learning Rate: 0.01\n",
      "Epoch [15561/20000], Loss: -21.1766357421875, Learning Rate: 0.01\n",
      "Epoch [15562/20000], Loss: -21.176773071289062, Learning Rate: 0.01\n",
      "Epoch [15563/20000], Loss: -21.177139282226562, Learning Rate: 0.01\n",
      "Epoch [15564/20000], Loss: -21.177642822265625, Learning Rate: 0.01\n",
      "Epoch [15565/20000], Loss: -21.178085327148438, Learning Rate: 0.01\n",
      "Epoch [15566/20000], Loss: -21.178421020507812, Learning Rate: 0.01\n",
      "Epoch [15567/20000], Loss: -21.178634643554688, Learning Rate: 0.01\n",
      "Epoch [15568/20000], Loss: -21.178787231445312, Learning Rate: 0.01\n",
      "Epoch [15569/20000], Loss: -21.178863525390625, Learning Rate: 0.01\n",
      "Epoch [15570/20000], Loss: -21.179061889648438, Learning Rate: 0.01\n",
      "Epoch [15571/20000], Loss: -21.179336547851562, Learning Rate: 0.01\n",
      "Epoch [15572/20000], Loss: -21.179580688476562, Learning Rate: 0.01\n",
      "Epoch [15573/20000], Loss: -21.17987060546875, Learning Rate: 0.01\n",
      "Epoch [15574/20000], Loss: -21.180191040039062, Learning Rate: 0.01\n",
      "Epoch [15575/20000], Loss: -21.180496215820312, Learning Rate: 0.01\n",
      "Epoch [15576/20000], Loss: -21.180770874023438, Learning Rate: 0.01\n",
      "Epoch [15577/20000], Loss: -21.180908203125, Learning Rate: 0.01\n",
      "Epoch [15578/20000], Loss: -21.181137084960938, Learning Rate: 0.01\n",
      "Epoch [15579/20000], Loss: -21.181259155273438, Learning Rate: 0.01\n",
      "Epoch [15580/20000], Loss: -21.181472778320312, Learning Rate: 0.01\n",
      "Epoch [15581/20000], Loss: -21.1817626953125, Learning Rate: 0.01\n",
      "Epoch [15582/20000], Loss: -21.182037353515625, Learning Rate: 0.01\n",
      "Epoch [15583/20000], Loss: -21.182281494140625, Learning Rate: 0.01\n",
      "Epoch [15584/20000], Loss: -21.1824951171875, Learning Rate: 0.01\n",
      "Epoch [15585/20000], Loss: -21.182723999023438, Learning Rate: 0.01\n",
      "Epoch [15586/20000], Loss: -21.18304443359375, Learning Rate: 0.01\n",
      "Epoch [15587/20000], Loss: -21.183181762695312, Learning Rate: 0.01\n",
      "Epoch [15588/20000], Loss: -21.183425903320312, Learning Rate: 0.01\n",
      "Epoch [15589/20000], Loss: -21.183609008789062, Learning Rate: 0.01\n",
      "Epoch [15590/20000], Loss: -21.18377685546875, Learning Rate: 0.01\n",
      "Epoch [15591/20000], Loss: -21.18414306640625, Learning Rate: 0.01\n",
      "Epoch [15592/20000], Loss: -21.184310913085938, Learning Rate: 0.01\n",
      "Epoch [15593/20000], Loss: -21.184600830078125, Learning Rate: 0.01\n",
      "Epoch [15594/20000], Loss: -21.184814453125, Learning Rate: 0.01\n",
      "Epoch [15595/20000], Loss: -21.18499755859375, Learning Rate: 0.01\n",
      "Epoch [15596/20000], Loss: -21.18524169921875, Learning Rate: 0.01\n",
      "Epoch [15597/20000], Loss: -21.185379028320312, Learning Rate: 0.01\n",
      "Epoch [15598/20000], Loss: -21.185638427734375, Learning Rate: 0.01\n",
      "Epoch [15599/20000], Loss: -21.185928344726562, Learning Rate: 0.01\n",
      "Epoch [15600/20000], Loss: -21.18609619140625, Learning Rate: 0.01\n",
      "Epoch [15601/20000], Loss: -21.18634033203125, Learning Rate: 0.01\n",
      "Epoch [15602/20000], Loss: -21.186538696289062, Learning Rate: 0.01\n",
      "Epoch [15603/20000], Loss: -21.186782836914062, Learning Rate: 0.01\n",
      "Epoch [15604/20000], Loss: -21.186981201171875, Learning Rate: 0.01\n",
      "Epoch [15605/20000], Loss: -21.187210083007812, Learning Rate: 0.01\n",
      "Epoch [15606/20000], Loss: -21.18743896484375, Learning Rate: 0.01\n",
      "Epoch [15607/20000], Loss: -21.187637329101562, Learning Rate: 0.01\n",
      "Epoch [15608/20000], Loss: -21.187896728515625, Learning Rate: 0.01\n",
      "Epoch [15609/20000], Loss: -21.188186645507812, Learning Rate: 0.01\n",
      "Epoch [15610/20000], Loss: -21.188323974609375, Learning Rate: 0.01\n",
      "Epoch [15611/20000], Loss: -21.188552856445312, Learning Rate: 0.01\n",
      "Epoch [15612/20000], Loss: -21.188796997070312, Learning Rate: 0.01\n",
      "Epoch [15613/20000], Loss: -21.18902587890625, Learning Rate: 0.01\n",
      "Epoch [15614/20000], Loss: -21.189239501953125, Learning Rate: 0.01\n",
      "Epoch [15615/20000], Loss: -21.189407348632812, Learning Rate: 0.01\n",
      "Epoch [15616/20000], Loss: -21.189620971679688, Learning Rate: 0.01\n",
      "Epoch [15617/20000], Loss: -21.189910888671875, Learning Rate: 0.01\n",
      "Epoch [15618/20000], Loss: -21.190048217773438, Learning Rate: 0.01\n",
      "Epoch [15619/20000], Loss: -21.19024658203125, Learning Rate: 0.01\n",
      "Epoch [15620/20000], Loss: -21.190475463867188, Learning Rate: 0.01\n",
      "Epoch [15621/20000], Loss: -21.190765380859375, Learning Rate: 0.01\n",
      "Epoch [15622/20000], Loss: -21.190948486328125, Learning Rate: 0.01\n",
      "Epoch [15623/20000], Loss: -21.191207885742188, Learning Rate: 0.01\n",
      "Epoch [15624/20000], Loss: -21.191452026367188, Learning Rate: 0.01\n",
      "Epoch [15625/20000], Loss: -21.191558837890625, Learning Rate: 0.01\n",
      "Epoch [15626/20000], Loss: -21.191864013671875, Learning Rate: 0.01\n",
      "Epoch [15627/20000], Loss: -21.191986083984375, Learning Rate: 0.01\n",
      "Epoch [15628/20000], Loss: -21.192367553710938, Learning Rate: 0.01\n",
      "Epoch [15629/20000], Loss: -21.19244384765625, Learning Rate: 0.01\n",
      "Epoch [15630/20000], Loss: -21.192703247070312, Learning Rate: 0.01\n",
      "Epoch [15631/20000], Loss: -21.192977905273438, Learning Rate: 0.01\n",
      "Epoch [15632/20000], Loss: -21.193115234375, Learning Rate: 0.01\n",
      "Epoch [15633/20000], Loss: -21.193344116210938, Learning Rate: 0.01\n",
      "Epoch [15634/20000], Loss: -21.193603515625, Learning Rate: 0.01\n",
      "Epoch [15635/20000], Loss: -21.193756103515625, Learning Rate: 0.01\n",
      "Epoch [15636/20000], Loss: -21.194000244140625, Learning Rate: 0.01\n",
      "Epoch [15637/20000], Loss: -21.19415283203125, Learning Rate: 0.01\n",
      "Epoch [15638/20000], Loss: -21.1943359375, Learning Rate: 0.01\n",
      "Epoch [15639/20000], Loss: -21.194625854492188, Learning Rate: 0.01\n",
      "Epoch [15640/20000], Loss: -21.194854736328125, Learning Rate: 0.01\n",
      "Epoch [15641/20000], Loss: -21.195037841796875, Learning Rate: 0.01\n",
      "Epoch [15642/20000], Loss: -21.1951904296875, Learning Rate: 0.01\n",
      "Epoch [15643/20000], Loss: -21.1954345703125, Learning Rate: 0.01\n",
      "Epoch [15644/20000], Loss: -21.195709228515625, Learning Rate: 0.01\n",
      "Epoch [15645/20000], Loss: -21.195877075195312, Learning Rate: 0.01\n",
      "Epoch [15646/20000], Loss: -21.196060180664062, Learning Rate: 0.01\n",
      "Epoch [15647/20000], Loss: -21.196334838867188, Learning Rate: 0.01\n",
      "Epoch [15648/20000], Loss: -21.196456909179688, Learning Rate: 0.01\n",
      "Epoch [15649/20000], Loss: -21.196685791015625, Learning Rate: 0.01\n",
      "Epoch [15650/20000], Loss: -21.196929931640625, Learning Rate: 0.01\n",
      "Epoch [15651/20000], Loss: -21.197021484375, Learning Rate: 0.01\n",
      "Epoch [15652/20000], Loss: -21.197189331054688, Learning Rate: 0.01\n",
      "Epoch [15653/20000], Loss: -21.19732666015625, Learning Rate: 0.01\n",
      "Epoch [15654/20000], Loss: -21.197418212890625, Learning Rate: 0.01\n",
      "Epoch [15655/20000], Loss: -21.197494506835938, Learning Rate: 0.01\n",
      "Epoch [15656/20000], Loss: -21.197540283203125, Learning Rate: 0.01\n",
      "Epoch [15657/20000], Loss: -21.19732666015625, Learning Rate: 0.01\n",
      "Epoch [15658/20000], Loss: -21.19696044921875, Learning Rate: 0.01\n",
      "Epoch [15659/20000], Loss: -21.196395874023438, Learning Rate: 0.01\n",
      "Epoch [15660/20000], Loss: -21.19549560546875, Learning Rate: 0.01\n",
      "Epoch [15661/20000], Loss: -21.1939697265625, Learning Rate: 0.01\n",
      "Epoch [15662/20000], Loss: -21.191452026367188, Learning Rate: 0.01\n",
      "Epoch [15663/20000], Loss: -21.187591552734375, Learning Rate: 0.01\n",
      "Epoch [15664/20000], Loss: -21.181533813476562, Learning Rate: 0.01\n",
      "Epoch [15665/20000], Loss: -21.171951293945312, Learning Rate: 0.01\n",
      "Epoch [15666/20000], Loss: -21.157089233398438, Learning Rate: 0.01\n",
      "Epoch [15667/20000], Loss: -21.133621215820312, Learning Rate: 0.01\n",
      "Epoch [15668/20000], Loss: -21.097442626953125, Learning Rate: 0.01\n",
      "Epoch [15669/20000], Loss: -21.041305541992188, Learning Rate: 0.01\n",
      "Epoch [15670/20000], Loss: -20.956451416015625, Learning Rate: 0.01\n",
      "Epoch [15671/20000], Loss: -20.828460693359375, Learning Rate: 0.01\n",
      "Epoch [15672/20000], Loss: -20.648422241210938, Learning Rate: 0.01\n",
      "Epoch [15673/20000], Loss: -20.401596069335938, Learning Rate: 0.01\n",
      "Epoch [15674/20000], Loss: -20.11639404296875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15675/20000], Loss: -19.826202392578125, Learning Rate: 0.01\n",
      "Epoch [15676/20000], Loss: -19.673538208007812, Learning Rate: 0.01\n",
      "Epoch [15677/20000], Loss: -19.732742309570312, Learning Rate: 0.01\n",
      "Epoch [15678/20000], Loss: -20.07958984375, Learning Rate: 0.01\n",
      "Epoch [15679/20000], Loss: -20.530380249023438, Learning Rate: 0.01\n",
      "Epoch [15680/20000], Loss: -20.863571166992188, Learning Rate: 0.01\n",
      "Epoch [15681/20000], Loss: -20.91033935546875, Learning Rate: 0.01\n",
      "Epoch [15682/20000], Loss: -20.7220458984375, Learning Rate: 0.01\n",
      "Epoch [15683/20000], Loss: -20.509170532226562, Learning Rate: 0.01\n",
      "Epoch [15684/20000], Loss: -20.484878540039062, Learning Rate: 0.01\n",
      "Epoch [15685/20000], Loss: -20.712432861328125, Learning Rate: 0.01\n",
      "Epoch [15686/20000], Loss: -21.02618408203125, Learning Rate: 0.01\n",
      "Epoch [15687/20000], Loss: -21.198623657226562, Learning Rate: 0.01\n",
      "Epoch [15688/20000], Loss: -21.141189575195312, Learning Rate: 0.01\n",
      "Epoch [15689/20000], Loss: -20.967208862304688, Learning Rate: 0.01\n",
      "Epoch [15690/20000], Loss: -20.864212036132812, Learning Rate: 0.01\n",
      "Epoch [15691/20000], Loss: -20.91943359375, Learning Rate: 0.01\n",
      "Epoch [15692/20000], Loss: -21.06591796875, Learning Rate: 0.01\n",
      "Epoch [15693/20000], Loss: -21.160934448242188, Learning Rate: 0.01\n",
      "Epoch [15694/20000], Loss: -21.138214111328125, Learning Rate: 0.01\n",
      "Epoch [15695/20000], Loss: -21.056198120117188, Learning Rate: 0.01\n",
      "Epoch [15696/20000], Loss: -21.019271850585938, Learning Rate: 0.01\n",
      "Epoch [15697/20000], Loss: -21.072189331054688, Learning Rate: 0.01\n",
      "Epoch [15698/20000], Loss: -21.16156005859375, Learning Rate: 0.01\n",
      "Epoch [15699/20000], Loss: -21.2071533203125, Learning Rate: 0.01\n",
      "Epoch [15700/20000], Loss: -21.181640625, Learning Rate: 0.01\n",
      "Epoch [15701/20000], Loss: -21.12811279296875, Learning Rate: 0.01\n",
      "Epoch [15702/20000], Loss: -21.10699462890625, Learning Rate: 0.01\n",
      "Epoch [15703/20000], Loss: -21.134750366210938, Learning Rate: 0.01\n",
      "Epoch [15704/20000], Loss: -21.178375244140625, Learning Rate: 0.01\n",
      "Epoch [15705/20000], Loss: -21.196060180664062, Learning Rate: 0.01\n",
      "Epoch [15706/20000], Loss: -21.180007934570312, Learning Rate: 0.01\n",
      "Epoch [15707/20000], Loss: -21.15753173828125, Learning Rate: 0.01\n",
      "Epoch [15708/20000], Loss: -21.156661987304688, Learning Rate: 0.01\n",
      "Epoch [15709/20000], Loss: -21.179428100585938, Learning Rate: 0.01\n",
      "Epoch [15710/20000], Loss: -21.203689575195312, Learning Rate: 0.01\n",
      "Epoch [15711/20000], Loss: -21.209136962890625, Learning Rate: 0.01\n",
      "Epoch [15712/20000], Loss: -21.19647216796875, Learning Rate: 0.01\n",
      "Epoch [15713/20000], Loss: -21.182601928710938, Learning Rate: 0.01\n",
      "Epoch [15714/20000], Loss: -21.182083129882812, Learning Rate: 0.01\n",
      "Epoch [15715/20000], Loss: -21.193572998046875, Learning Rate: 0.01\n",
      "Epoch [15716/20000], Loss: -21.204864501953125, Learning Rate: 0.01\n",
      "Epoch [15717/20000], Loss: -21.206192016601562, Learning Rate: 0.01\n",
      "Epoch [15718/20000], Loss: -21.199462890625, Learning Rate: 0.01\n",
      "Epoch [15719/20000], Loss: -21.193939208984375, Learning Rate: 0.01\n",
      "Epoch [15720/20000], Loss: -21.195938110351562, Learning Rate: 0.01\n",
      "Epoch [15721/20000], Loss: -21.203750610351562, Learning Rate: 0.01\n",
      "Epoch [15722/20000], Loss: -21.210464477539062, Learning Rate: 0.01\n",
      "Epoch [15723/20000], Loss: -21.2113037109375, Learning Rate: 0.01\n",
      "Epoch [15724/20000], Loss: -21.207687377929688, Learning Rate: 0.01\n",
      "Epoch [15725/20000], Loss: -21.204544067382812, Learning Rate: 0.01\n",
      "Epoch [15726/20000], Loss: -21.205291748046875, Learning Rate: 0.01\n",
      "Epoch [15727/20000], Loss: -21.2091064453125, Learning Rate: 0.01\n",
      "Epoch [15728/20000], Loss: -21.21209716796875, Learning Rate: 0.01\n",
      "Epoch [15729/20000], Loss: -21.212188720703125, Learning Rate: 0.01\n",
      "Epoch [15730/20000], Loss: -21.21002197265625, Learning Rate: 0.01\n",
      "Epoch [15731/20000], Loss: -21.20831298828125, Learning Rate: 0.01\n",
      "Epoch [15732/20000], Loss: -21.208770751953125, Learning Rate: 0.01\n",
      "Epoch [15733/20000], Loss: -21.2108154296875, Learning Rate: 0.01\n",
      "Epoch [15734/20000], Loss: -21.212783813476562, Learning Rate: 0.01\n",
      "Epoch [15735/20000], Loss: -21.21331787109375, Learning Rate: 0.01\n",
      "Epoch [15736/20000], Loss: -21.212509155273438, Learning Rate: 0.01\n",
      "Epoch [15737/20000], Loss: -21.211944580078125, Learning Rate: 0.01\n",
      "Epoch [15738/20000], Loss: -21.212081909179688, Learning Rate: 0.01\n",
      "Epoch [15739/20000], Loss: -21.213272094726562, Learning Rate: 0.01\n",
      "Epoch [15740/20000], Loss: -21.214599609375, Learning Rate: 0.01\n",
      "Epoch [15741/20000], Loss: -21.215225219726562, Learning Rate: 0.01\n",
      "Epoch [15742/20000], Loss: -21.214996337890625, Learning Rate: 0.01\n",
      "Epoch [15743/20000], Loss: -21.214675903320312, Learning Rate: 0.01\n",
      "Epoch [15744/20000], Loss: -21.2147216796875, Learning Rate: 0.01\n",
      "Epoch [15745/20000], Loss: -21.2152099609375, Learning Rate: 0.01\n",
      "Epoch [15746/20000], Loss: -21.216033935546875, Learning Rate: 0.01\n",
      "Epoch [15747/20000], Loss: -21.216506958007812, Learning Rate: 0.01\n",
      "Epoch [15748/20000], Loss: -21.216598510742188, Learning Rate: 0.01\n",
      "Epoch [15749/20000], Loss: -21.216415405273438, Learning Rate: 0.01\n",
      "Epoch [15750/20000], Loss: -21.216339111328125, Learning Rate: 0.01\n",
      "Epoch [15751/20000], Loss: -21.216644287109375, Learning Rate: 0.01\n",
      "Epoch [15752/20000], Loss: -21.21710205078125, Learning Rate: 0.01\n",
      "Epoch [15753/20000], Loss: -21.21746826171875, Learning Rate: 0.01\n",
      "Epoch [15754/20000], Loss: -21.217727661132812, Learning Rate: 0.01\n",
      "Epoch [15755/20000], Loss: -21.21771240234375, Learning Rate: 0.01\n",
      "Epoch [15756/20000], Loss: -21.217803955078125, Learning Rate: 0.01\n",
      "Epoch [15757/20000], Loss: -21.217910766601562, Learning Rate: 0.01\n",
      "Epoch [15758/20000], Loss: -21.21807861328125, Learning Rate: 0.01\n",
      "Epoch [15759/20000], Loss: -21.21844482421875, Learning Rate: 0.01\n",
      "Epoch [15760/20000], Loss: -21.218673706054688, Learning Rate: 0.01\n",
      "Epoch [15761/20000], Loss: -21.21875, Learning Rate: 0.01\n",
      "Epoch [15762/20000], Loss: -21.218856811523438, Learning Rate: 0.01\n",
      "Epoch [15763/20000], Loss: -21.218902587890625, Learning Rate: 0.01\n",
      "Epoch [15764/20000], Loss: -21.218978881835938, Learning Rate: 0.01\n",
      "Epoch [15765/20000], Loss: -21.219223022460938, Learning Rate: 0.01\n",
      "Epoch [15766/20000], Loss: -21.21923828125, Learning Rate: 0.01\n",
      "Epoch [15767/20000], Loss: -21.219375610351562, Learning Rate: 0.01\n",
      "Epoch [15768/20000], Loss: -21.219375610351562, Learning Rate: 0.01\n",
      "Epoch [15769/20000], Loss: -21.219223022460938, Learning Rate: 0.01\n",
      "Epoch [15770/20000], Loss: -21.21893310546875, Learning Rate: 0.01\n",
      "Epoch [15771/20000], Loss: -21.218475341796875, Learning Rate: 0.01\n",
      "Epoch [15772/20000], Loss: -21.218002319335938, Learning Rate: 0.01\n",
      "Epoch [15773/20000], Loss: -21.216964721679688, Learning Rate: 0.01\n",
      "Epoch [15774/20000], Loss: -21.215591430664062, Learning Rate: 0.01\n",
      "Epoch [15775/20000], Loss: -21.213287353515625, Learning Rate: 0.01\n",
      "Epoch [15776/20000], Loss: -21.209854125976562, Learning Rate: 0.01\n",
      "Epoch [15777/20000], Loss: -21.204879760742188, Learning Rate: 0.01\n",
      "Epoch [15778/20000], Loss: -21.197174072265625, Learning Rate: 0.01\n",
      "Epoch [15779/20000], Loss: -21.185775756835938, Learning Rate: 0.01\n",
      "Epoch [15780/20000], Loss: -21.168609619140625, Learning Rate: 0.01\n",
      "Epoch [15781/20000], Loss: -21.142669677734375, Learning Rate: 0.01\n",
      "Epoch [15782/20000], Loss: -21.10382080078125, Learning Rate: 0.01\n",
      "Epoch [15783/20000], Loss: -21.045974731445312, Learning Rate: 0.01\n",
      "Epoch [15784/20000], Loss: -20.960250854492188, Learning Rate: 0.01\n",
      "Epoch [15785/20000], Loss: -20.837860107421875, Learning Rate: 0.01\n",
      "Epoch [15786/20000], Loss: -20.6663818359375, Learning Rate: 0.01\n",
      "Epoch [15787/20000], Loss: -20.4461669921875, Learning Rate: 0.01\n",
      "Epoch [15788/20000], Loss: -20.184555053710938, Learning Rate: 0.01\n",
      "Epoch [15789/20000], Loss: -19.94091796875, Learning Rate: 0.01\n",
      "Epoch [15790/20000], Loss: -19.789749145507812, Learning Rate: 0.01\n",
      "Epoch [15791/20000], Loss: -19.857162475585938, Learning Rate: 0.01\n",
      "Epoch [15792/20000], Loss: -20.163864135742188, Learning Rate: 0.01\n",
      "Epoch [15793/20000], Loss: -20.633132934570312, Learning Rate: 0.01\n",
      "Epoch [15794/20000], Loss: -21.04461669921875, Learning Rate: 0.01\n",
      "Epoch [15795/20000], Loss: -21.221832275390625, Learning Rate: 0.01\n",
      "Epoch [15796/20000], Loss: -21.134628295898438, Learning Rate: 0.01\n",
      "Epoch [15797/20000], Loss: -20.906005859375, Learning Rate: 0.01\n",
      "Epoch [15798/20000], Loss: -20.719467163085938, Learning Rate: 0.01\n",
      "Epoch [15799/20000], Loss: -20.70001220703125, Learning Rate: 0.01\n",
      "Epoch [15800/20000], Loss: -20.858306884765625, Learning Rate: 0.01\n",
      "Epoch [15801/20000], Loss: -21.075286865234375, Learning Rate: 0.01\n",
      "Epoch [15802/20000], Loss: -21.211517333984375, Learning Rate: 0.01\n",
      "Epoch [15803/20000], Loss: -21.201980590820312, Learning Rate: 0.01\n",
      "Epoch [15804/20000], Loss: -21.094131469726562, Learning Rate: 0.01\n",
      "Epoch [15805/20000], Loss: -20.993927001953125, Learning Rate: 0.01\n",
      "Epoch [15806/20000], Loss: -20.980133056640625, Learning Rate: 0.01\n",
      "Epoch [15807/20000], Loss: -21.058395385742188, Learning Rate: 0.01\n",
      "Epoch [15808/20000], Loss: -21.163482666015625, Learning Rate: 0.01\n",
      "Epoch [15809/20000], Loss: -21.222702026367188, Learning Rate: 0.01\n",
      "Epoch [15810/20000], Loss: -21.208847045898438, Learning Rate: 0.01\n",
      "Epoch [15811/20000], Loss: -21.152542114257812, Learning Rate: 0.01\n",
      "Epoch [15812/20000], Loss: -21.1085205078125, Learning Rate: 0.01\n",
      "Epoch [15813/20000], Loss: -21.111557006835938, Learning Rate: 0.01\n",
      "Epoch [15814/20000], Loss: -21.155746459960938, Learning Rate: 0.01\n",
      "Epoch [15815/20000], Loss: -21.20513916015625, Learning Rate: 0.01\n",
      "Epoch [15816/20000], Loss: -21.2269287109375, Learning Rate: 0.01\n",
      "Epoch [15817/20000], Loss: -21.214202880859375, Learning Rate: 0.01\n",
      "Epoch [15818/20000], Loss: -21.185699462890625, Learning Rate: 0.01\n",
      "Epoch [15819/20000], Loss: -21.168014526367188, Learning Rate: 0.01\n",
      "Epoch [15820/20000], Loss: -21.173995971679688, Learning Rate: 0.01\n",
      "Epoch [15821/20000], Loss: -21.19732666015625, Learning Rate: 0.01\n",
      "Epoch [15822/20000], Loss: -21.220138549804688, Learning Rate: 0.01\n",
      "Epoch [15823/20000], Loss: -21.22845458984375, Learning Rate: 0.01\n",
      "Epoch [15824/20000], Loss: -21.220916748046875, Learning Rate: 0.01\n",
      "Epoch [15825/20000], Loss: -21.207351684570312, Learning Rate: 0.01\n",
      "Epoch [15826/20000], Loss: -21.199783325195312, Learning Rate: 0.01\n",
      "Epoch [15827/20000], Loss: -21.203216552734375, Learning Rate: 0.01\n",
      "Epoch [15828/20000], Loss: -21.214630126953125, Learning Rate: 0.01\n",
      "Epoch [15829/20000], Loss: -21.225601196289062, Learning Rate: 0.01\n",
      "Epoch [15830/20000], Loss: -21.229904174804688, Learning Rate: 0.01\n",
      "Epoch [15831/20000], Loss: -21.226715087890625, Learning Rate: 0.01\n",
      "Epoch [15832/20000], Loss: -21.220367431640625, Learning Rate: 0.01\n",
      "Epoch [15833/20000], Loss: -21.216400146484375, Learning Rate: 0.01\n",
      "Epoch [15834/20000], Loss: -21.217544555664062, Learning Rate: 0.01\n",
      "Epoch [15835/20000], Loss: -21.22247314453125, Learning Rate: 0.01\n",
      "Epoch [15836/20000], Loss: -21.228103637695312, Learning Rate: 0.01\n",
      "Epoch [15837/20000], Loss: -21.2310791015625, Learning Rate: 0.01\n",
      "Epoch [15838/20000], Loss: -21.23046875, Learning Rate: 0.01\n",
      "Epoch [15839/20000], Loss: -21.227767944335938, Learning Rate: 0.01\n",
      "Epoch [15840/20000], Loss: -21.22552490234375, Learning Rate: 0.01\n",
      "Epoch [15841/20000], Loss: -21.225143432617188, Learning Rate: 0.01\n",
      "Epoch [15842/20000], Loss: -21.22705078125, Learning Rate: 0.01\n",
      "Epoch [15843/20000], Loss: -21.229721069335938, Learning Rate: 0.01\n",
      "Epoch [15844/20000], Loss: -21.231842041015625, Learning Rate: 0.01\n",
      "Epoch [15845/20000], Loss: -21.232498168945312, Learning Rate: 0.01\n",
      "Epoch [15846/20000], Loss: -21.23175048828125, Learning Rate: 0.01\n",
      "Epoch [15847/20000], Loss: -21.230560302734375, Learning Rate: 0.01\n",
      "Epoch [15848/20000], Loss: -21.229812622070312, Learning Rate: 0.01\n",
      "Epoch [15849/20000], Loss: -21.23016357421875, Learning Rate: 0.01\n",
      "Epoch [15850/20000], Loss: -21.231231689453125, Learning Rate: 0.01\n",
      "Epoch [15851/20000], Loss: -21.232589721679688, Learning Rate: 0.01\n",
      "Epoch [15852/20000], Loss: -21.233535766601562, Learning Rate: 0.01\n",
      "Epoch [15853/20000], Loss: -21.23370361328125, Learning Rate: 0.01\n",
      "Epoch [15854/20000], Loss: -21.2335205078125, Learning Rate: 0.01\n",
      "Epoch [15855/20000], Loss: -21.233047485351562, Learning Rate: 0.01\n",
      "Epoch [15856/20000], Loss: -21.232772827148438, Learning Rate: 0.01\n",
      "Epoch [15857/20000], Loss: -21.233062744140625, Learning Rate: 0.01\n",
      "Epoch [15858/20000], Loss: -21.233566284179688, Learning Rate: 0.01\n",
      "Epoch [15859/20000], Loss: -21.234237670898438, Learning Rate: 0.01\n",
      "Epoch [15860/20000], Loss: -21.234786987304688, Learning Rate: 0.01\n",
      "Epoch [15861/20000], Loss: -21.235092163085938, Learning Rate: 0.01\n",
      "Epoch [15862/20000], Loss: -21.235107421875, Learning Rate: 0.01\n",
      "Epoch [15863/20000], Loss: -21.235061645507812, Learning Rate: 0.01\n",
      "Epoch [15864/20000], Loss: -21.234970092773438, Learning Rate: 0.01\n",
      "Epoch [15865/20000], Loss: -21.235000610351562, Learning Rate: 0.01\n",
      "Epoch [15866/20000], Loss: -21.235275268554688, Learning Rate: 0.01\n",
      "Epoch [15867/20000], Loss: -21.235626220703125, Learning Rate: 0.01\n",
      "Epoch [15868/20000], Loss: -21.23602294921875, Learning Rate: 0.01\n",
      "Epoch [15869/20000], Loss: -21.236282348632812, Learning Rate: 0.01\n",
      "Epoch [15870/20000], Loss: -21.23638916015625, Learning Rate: 0.01\n",
      "Epoch [15871/20000], Loss: -21.236480712890625, Learning Rate: 0.01\n",
      "Epoch [15872/20000], Loss: -21.236557006835938, Learning Rate: 0.01\n",
      "Epoch [15873/20000], Loss: -21.236587524414062, Learning Rate: 0.01\n",
      "Epoch [15874/20000], Loss: -21.2366943359375, Learning Rate: 0.01\n",
      "Epoch [15875/20000], Loss: -21.2369384765625, Learning Rate: 0.01\n",
      "Epoch [15876/20000], Loss: -21.237045288085938, Learning Rate: 0.01\n",
      "Epoch [15877/20000], Loss: -21.237274169921875, Learning Rate: 0.01\n",
      "Epoch [15878/20000], Loss: -21.237564086914062, Learning Rate: 0.01\n",
      "Epoch [15879/20000], Loss: -21.237747192382812, Learning Rate: 0.01\n",
      "Epoch [15880/20000], Loss: -21.237930297851562, Learning Rate: 0.01\n",
      "Epoch [15881/20000], Loss: -21.237991333007812, Learning Rate: 0.01\n",
      "Epoch [15882/20000], Loss: -21.238128662109375, Learning Rate: 0.01\n",
      "Epoch [15883/20000], Loss: -21.238235473632812, Learning Rate: 0.01\n",
      "Epoch [15884/20000], Loss: -21.238479614257812, Learning Rate: 0.01\n",
      "Epoch [15885/20000], Loss: -21.238479614257812, Learning Rate: 0.01\n",
      "Epoch [15886/20000], Loss: -21.238677978515625, Learning Rate: 0.01\n",
      "Epoch [15887/20000], Loss: -21.23883056640625, Learning Rate: 0.01\n",
      "Epoch [15888/20000], Loss: -21.238998413085938, Learning Rate: 0.01\n",
      "Epoch [15889/20000], Loss: -21.239212036132812, Learning Rate: 0.01\n",
      "Epoch [15890/20000], Loss: -21.239410400390625, Learning Rate: 0.01\n",
      "Epoch [15891/20000], Loss: -21.239532470703125, Learning Rate: 0.01\n",
      "Epoch [15892/20000], Loss: -21.239639282226562, Learning Rate: 0.01\n",
      "Epoch [15893/20000], Loss: -21.239715576171875, Learning Rate: 0.01\n",
      "Epoch [15894/20000], Loss: -21.239913940429688, Learning Rate: 0.01\n",
      "Epoch [15895/20000], Loss: -21.24005126953125, Learning Rate: 0.01\n",
      "Epoch [15896/20000], Loss: -21.240234375, Learning Rate: 0.01\n",
      "Epoch [15897/20000], Loss: -21.240371704101562, Learning Rate: 0.01\n",
      "Epoch [15898/20000], Loss: -21.240585327148438, Learning Rate: 0.01\n",
      "Epoch [15899/20000], Loss: -21.240692138671875, Learning Rate: 0.01\n",
      "Epoch [15900/20000], Loss: -21.240859985351562, Learning Rate: 0.01\n",
      "Epoch [15901/20000], Loss: -21.240982055664062, Learning Rate: 0.01\n",
      "Epoch [15902/20000], Loss: -21.241104125976562, Learning Rate: 0.01\n",
      "Epoch [15903/20000], Loss: -21.24127197265625, Learning Rate: 0.01\n",
      "Epoch [15904/20000], Loss: -21.24139404296875, Learning Rate: 0.01\n",
      "Epoch [15905/20000], Loss: -21.241485595703125, Learning Rate: 0.01\n",
      "Epoch [15906/20000], Loss: -21.241729736328125, Learning Rate: 0.01\n",
      "Epoch [15907/20000], Loss: -21.241775512695312, Learning Rate: 0.01\n",
      "Epoch [15908/20000], Loss: -21.241928100585938, Learning Rate: 0.01\n",
      "Epoch [15909/20000], Loss: -21.242095947265625, Learning Rate: 0.01\n",
      "Epoch [15910/20000], Loss: -21.242263793945312, Learning Rate: 0.01\n",
      "Epoch [15911/20000], Loss: -21.24237060546875, Learning Rate: 0.01\n",
      "Epoch [15912/20000], Loss: -21.242599487304688, Learning Rate: 0.01\n",
      "Epoch [15913/20000], Loss: -21.242721557617188, Learning Rate: 0.01\n",
      "Epoch [15914/20000], Loss: -21.242889404296875, Learning Rate: 0.01\n",
      "Epoch [15915/20000], Loss: -21.24298095703125, Learning Rate: 0.01\n",
      "Epoch [15916/20000], Loss: -21.24310302734375, Learning Rate: 0.01\n",
      "Epoch [15917/20000], Loss: -21.243255615234375, Learning Rate: 0.01\n",
      "Epoch [15918/20000], Loss: -21.243423461914062, Learning Rate: 0.01\n",
      "Epoch [15919/20000], Loss: -21.243545532226562, Learning Rate: 0.01\n",
      "Epoch [15920/20000], Loss: -21.243637084960938, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15921/20000], Loss: -21.243850708007812, Learning Rate: 0.01\n",
      "Epoch [15922/20000], Loss: -21.244064331054688, Learning Rate: 0.01\n",
      "Epoch [15923/20000], Loss: -21.244125366210938, Learning Rate: 0.01\n",
      "Epoch [15924/20000], Loss: -21.244186401367188, Learning Rate: 0.01\n",
      "Epoch [15925/20000], Loss: -21.244369506835938, Learning Rate: 0.01\n",
      "Epoch [15926/20000], Loss: -21.24456787109375, Learning Rate: 0.01\n",
      "Epoch [15927/20000], Loss: -21.244674682617188, Learning Rate: 0.01\n",
      "Epoch [15928/20000], Loss: -21.244781494140625, Learning Rate: 0.01\n",
      "Epoch [15929/20000], Loss: -21.244949340820312, Learning Rate: 0.01\n",
      "Epoch [15930/20000], Loss: -21.24505615234375, Learning Rate: 0.01\n",
      "Epoch [15931/20000], Loss: -21.245223999023438, Learning Rate: 0.01\n",
      "Epoch [15932/20000], Loss: -21.245346069335938, Learning Rate: 0.01\n",
      "Epoch [15933/20000], Loss: -21.2454833984375, Learning Rate: 0.01\n",
      "Epoch [15934/20000], Loss: -21.245712280273438, Learning Rate: 0.01\n",
      "Epoch [15935/20000], Loss: -21.24578857421875, Learning Rate: 0.01\n",
      "Epoch [15936/20000], Loss: -21.245941162109375, Learning Rate: 0.01\n",
      "Epoch [15937/20000], Loss: -21.24609375, Learning Rate: 0.01\n",
      "Epoch [15938/20000], Loss: -21.246307373046875, Learning Rate: 0.01\n",
      "Epoch [15939/20000], Loss: -21.246322631835938, Learning Rate: 0.01\n",
      "Epoch [15940/20000], Loss: -21.246490478515625, Learning Rate: 0.01\n",
      "Epoch [15941/20000], Loss: -21.246566772460938, Learning Rate: 0.01\n",
      "Epoch [15942/20000], Loss: -21.246749877929688, Learning Rate: 0.01\n",
      "Epoch [15943/20000], Loss: -21.246871948242188, Learning Rate: 0.01\n",
      "Epoch [15944/20000], Loss: -21.246978759765625, Learning Rate: 0.01\n",
      "Epoch [15945/20000], Loss: -21.2471923828125, Learning Rate: 0.01\n",
      "Epoch [15946/20000], Loss: -21.247222900390625, Learning Rate: 0.01\n",
      "Epoch [15947/20000], Loss: -21.24737548828125, Learning Rate: 0.01\n",
      "Epoch [15948/20000], Loss: -21.24761962890625, Learning Rate: 0.01\n",
      "Epoch [15949/20000], Loss: -21.247711181640625, Learning Rate: 0.01\n",
      "Epoch [15950/20000], Loss: -21.247772216796875, Learning Rate: 0.01\n",
      "Epoch [15951/20000], Loss: -21.2479248046875, Learning Rate: 0.01\n",
      "Epoch [15952/20000], Loss: -21.248092651367188, Learning Rate: 0.01\n",
      "Epoch [15953/20000], Loss: -21.248199462890625, Learning Rate: 0.01\n",
      "Epoch [15954/20000], Loss: -21.248275756835938, Learning Rate: 0.01\n",
      "Epoch [15955/20000], Loss: -21.248367309570312, Learning Rate: 0.01\n",
      "Epoch [15956/20000], Loss: -21.24835205078125, Learning Rate: 0.01\n",
      "Epoch [15957/20000], Loss: -21.2484130859375, Learning Rate: 0.01\n",
      "Epoch [15958/20000], Loss: -21.24835205078125, Learning Rate: 0.01\n",
      "Epoch [15959/20000], Loss: -21.248260498046875, Learning Rate: 0.01\n",
      "Epoch [15960/20000], Loss: -21.248092651367188, Learning Rate: 0.01\n",
      "Epoch [15961/20000], Loss: -21.247482299804688, Learning Rate: 0.01\n",
      "Epoch [15962/20000], Loss: -21.246749877929688, Learning Rate: 0.01\n",
      "Epoch [15963/20000], Loss: -21.245651245117188, Learning Rate: 0.01\n",
      "Epoch [15964/20000], Loss: -21.243698120117188, Learning Rate: 0.01\n",
      "Epoch [15965/20000], Loss: -21.240676879882812, Learning Rate: 0.01\n",
      "Epoch [15966/20000], Loss: -21.235946655273438, Learning Rate: 0.01\n",
      "Epoch [15967/20000], Loss: -21.22833251953125, Learning Rate: 0.01\n",
      "Epoch [15968/20000], Loss: -21.216598510742188, Learning Rate: 0.01\n",
      "Epoch [15969/20000], Loss: -21.19775390625, Learning Rate: 0.01\n",
      "Epoch [15970/20000], Loss: -21.167999267578125, Learning Rate: 0.01\n",
      "Epoch [15971/20000], Loss: -21.1207275390625, Learning Rate: 0.01\n",
      "Epoch [15972/20000], Loss: -21.046630859375, Learning Rate: 0.01\n",
      "Epoch [15973/20000], Loss: -20.930740356445312, Learning Rate: 0.01\n",
      "Epoch [15974/20000], Loss: -20.756561279296875, Learning Rate: 0.01\n",
      "Epoch [15975/20000], Loss: -20.499465942382812, Learning Rate: 0.01\n",
      "Epoch [15976/20000], Loss: -20.15802001953125, Learning Rate: 0.01\n",
      "Epoch [15977/20000], Loss: -19.742095947265625, Learning Rate: 0.01\n",
      "Epoch [15978/20000], Loss: -19.377731323242188, Learning Rate: 0.01\n",
      "Epoch [15979/20000], Loss: -19.208770751953125, Learning Rate: 0.01\n",
      "Epoch [15980/20000], Loss: -19.470779418945312, Learning Rate: 0.01\n",
      "Epoch [15981/20000], Loss: -20.102203369140625, Learning Rate: 0.01\n",
      "Epoch [15982/20000], Loss: -20.828887939453125, Learning Rate: 0.01\n",
      "Epoch [15983/20000], Loss: -21.22650146484375, Learning Rate: 0.01\n",
      "Epoch [15984/20000], Loss: -21.141082763671875, Learning Rate: 0.01\n",
      "Epoch [15985/20000], Loss: -20.767257690429688, Learning Rate: 0.01\n",
      "Epoch [15986/20000], Loss: -20.462905883789062, Learning Rate: 0.01\n",
      "Epoch [15987/20000], Loss: -20.493988037109375, Learning Rate: 0.01\n",
      "Epoch [15988/20000], Loss: -20.81365966796875, Learning Rate: 0.01\n",
      "Epoch [15989/20000], Loss: -21.1517333984375, Learning Rate: 0.01\n",
      "Epoch [15990/20000], Loss: -21.246139526367188, Learning Rate: 0.01\n",
      "Epoch [15991/20000], Loss: -21.089019775390625, Learning Rate: 0.01\n",
      "Epoch [15992/20000], Loss: -20.888442993164062, Learning Rate: 0.01\n",
      "Epoch [15993/20000], Loss: -20.849334716796875, Learning Rate: 0.01\n",
      "Epoch [15994/20000], Loss: -21.001800537109375, Learning Rate: 0.01\n",
      "Epoch [15995/20000], Loss: -21.18994140625, Learning Rate: 0.01\n",
      "Epoch [15996/20000], Loss: -21.2523193359375, Learning Rate: 0.01\n",
      "Epoch [15997/20000], Loss: -21.169921875, Learning Rate: 0.01\n",
      "Epoch [15998/20000], Loss: -21.059356689453125, Learning Rate: 0.01\n",
      "Epoch [15999/20000], Loss: -21.041244506835938, Learning Rate: 0.01\n",
      "Epoch [16000/20000], Loss: -21.125564575195312, Learning Rate: 0.01\n",
      "Epoch [16001/20000], Loss: -21.22528076171875, Learning Rate: 0.01\n",
      "Epoch [16002/20000], Loss: -21.252197265625, Learning Rate: 0.01\n",
      "Epoch [16003/20000], Loss: -21.203155517578125, Learning Rate: 0.01\n",
      "Epoch [16004/20000], Loss: -21.145965576171875, Learning Rate: 0.01\n",
      "Epoch [16005/20000], Loss: -21.142257690429688, Learning Rate: 0.01\n",
      "Epoch [16006/20000], Loss: -21.191787719726562, Learning Rate: 0.01\n",
      "Epoch [16007/20000], Loss: -21.243179321289062, Learning Rate: 0.01\n",
      "Epoch [16008/20000], Loss: -21.25274658203125, Learning Rate: 0.01\n",
      "Epoch [16009/20000], Loss: -21.2239990234375, Learning Rate: 0.01\n",
      "Epoch [16010/20000], Loss: -21.194503784179688, Learning Rate: 0.01\n",
      "Epoch [16011/20000], Loss: -21.1956787109375, Learning Rate: 0.01\n",
      "Epoch [16012/20000], Loss: -21.223587036132812, Learning Rate: 0.01\n",
      "Epoch [16013/20000], Loss: -21.250320434570312, Learning Rate: 0.01\n",
      "Epoch [16014/20000], Loss: -21.25439453125, Learning Rate: 0.01\n",
      "Epoch [16015/20000], Loss: -21.238494873046875, Learning Rate: 0.01\n",
      "Epoch [16016/20000], Loss: -21.223159790039062, Learning Rate: 0.01\n",
      "Epoch [16017/20000], Loss: -21.223846435546875, Learning Rate: 0.01\n",
      "Epoch [16018/20000], Loss: -21.238937377929688, Learning Rate: 0.01\n",
      "Epoch [16019/20000], Loss: -21.25341796875, Learning Rate: 0.01\n",
      "Epoch [16020/20000], Loss: -21.255996704101562, Learning Rate: 0.01\n",
      "Epoch [16021/20000], Loss: -21.247894287109375, Learning Rate: 0.01\n",
      "Epoch [16022/20000], Loss: -21.239456176757812, Learning Rate: 0.01\n",
      "Epoch [16023/20000], Loss: -21.2392578125, Learning Rate: 0.01\n",
      "Epoch [16024/20000], Loss: -21.246734619140625, Learning Rate: 0.01\n",
      "Epoch [16025/20000], Loss: -21.254959106445312, Learning Rate: 0.01\n",
      "Epoch [16026/20000], Loss: -21.25738525390625, Learning Rate: 0.01\n",
      "Epoch [16027/20000], Loss: -21.253799438476562, Learning Rate: 0.01\n",
      "Epoch [16028/20000], Loss: -21.249069213867188, Learning Rate: 0.01\n",
      "Epoch [16029/20000], Loss: -21.248031616210938, Learning Rate: 0.01\n",
      "Epoch [16030/20000], Loss: -21.251312255859375, Learning Rate: 0.01\n",
      "Epoch [16031/20000], Loss: -21.255950927734375, Learning Rate: 0.01\n",
      "Epoch [16032/20000], Loss: -21.258224487304688, Learning Rate: 0.01\n",
      "Epoch [16033/20000], Loss: -21.257156372070312, Learning Rate: 0.01\n",
      "Epoch [16034/20000], Loss: -21.25457763671875, Learning Rate: 0.01\n",
      "Epoch [16035/20000], Loss: -21.253265380859375, Learning Rate: 0.01\n",
      "Epoch [16036/20000], Loss: -21.254348754882812, Learning Rate: 0.01\n",
      "Epoch [16037/20000], Loss: -21.256790161132812, Learning Rate: 0.01\n",
      "Epoch [16038/20000], Loss: -21.2586669921875, Learning Rate: 0.01\n",
      "Epoch [16039/20000], Loss: -21.258926391601562, Learning Rate: 0.01\n",
      "Epoch [16040/20000], Loss: -21.257827758789062, Learning Rate: 0.01\n",
      "Epoch [16041/20000], Loss: -21.2568359375, Learning Rate: 0.01\n",
      "Epoch [16042/20000], Loss: -21.256805419921875, Learning Rate: 0.01\n",
      "Epoch [16043/20000], Loss: -21.25775146484375, Learning Rate: 0.01\n",
      "Epoch [16044/20000], Loss: -21.259063720703125, Learning Rate: 0.01\n",
      "Epoch [16045/20000], Loss: -21.259796142578125, Learning Rate: 0.01\n",
      "Epoch [16046/20000], Loss: -21.259597778320312, Learning Rate: 0.01\n",
      "Epoch [16047/20000], Loss: -21.259140014648438, Learning Rate: 0.01\n",
      "Epoch [16048/20000], Loss: -21.258819580078125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16049/20000], Loss: -21.258987426757812, Learning Rate: 0.01\n",
      "Epoch [16050/20000], Loss: -21.2596435546875, Learning Rate: 0.01\n",
      "Epoch [16051/20000], Loss: -21.260299682617188, Learning Rate: 0.01\n",
      "Epoch [16052/20000], Loss: -21.260650634765625, Learning Rate: 0.01\n",
      "Epoch [16053/20000], Loss: -21.2606201171875, Learning Rate: 0.01\n",
      "Epoch [16054/20000], Loss: -21.260406494140625, Learning Rate: 0.01\n",
      "Epoch [16055/20000], Loss: -21.260345458984375, Learning Rate: 0.01\n",
      "Epoch [16056/20000], Loss: -21.260513305664062, Learning Rate: 0.01\n",
      "Epoch [16057/20000], Loss: -21.260955810546875, Learning Rate: 0.01\n",
      "Epoch [16058/20000], Loss: -21.261276245117188, Learning Rate: 0.01\n",
      "Epoch [16059/20000], Loss: -21.261459350585938, Learning Rate: 0.01\n",
      "Epoch [16060/20000], Loss: -21.261459350585938, Learning Rate: 0.01\n",
      "Epoch [16061/20000], Loss: -21.261520385742188, Learning Rate: 0.01\n",
      "Epoch [16062/20000], Loss: -21.261459350585938, Learning Rate: 0.01\n",
      "Epoch [16063/20000], Loss: -21.261627197265625, Learning Rate: 0.01\n",
      "Epoch [16064/20000], Loss: -21.261825561523438, Learning Rate: 0.01\n",
      "Epoch [16065/20000], Loss: -21.262054443359375, Learning Rate: 0.01\n",
      "Epoch [16066/20000], Loss: -21.262298583984375, Learning Rate: 0.01\n",
      "Epoch [16067/20000], Loss: -21.262435913085938, Learning Rate: 0.01\n",
      "Epoch [16068/20000], Loss: -21.262451171875, Learning Rate: 0.01\n",
      "Epoch [16069/20000], Loss: -21.262527465820312, Learning Rate: 0.01\n",
      "Epoch [16070/20000], Loss: -21.262619018554688, Learning Rate: 0.01\n",
      "Epoch [16071/20000], Loss: -21.262710571289062, Learning Rate: 0.01\n",
      "Epoch [16072/20000], Loss: -21.262908935546875, Learning Rate: 0.01\n",
      "Epoch [16073/20000], Loss: -21.26312255859375, Learning Rate: 0.01\n",
      "Epoch [16074/20000], Loss: -21.263229370117188, Learning Rate: 0.01\n",
      "Epoch [16075/20000], Loss: -21.263336181640625, Learning Rate: 0.01\n",
      "Epoch [16076/20000], Loss: -21.263458251953125, Learning Rate: 0.01\n",
      "Epoch [16077/20000], Loss: -21.263519287109375, Learning Rate: 0.01\n",
      "Epoch [16078/20000], Loss: -21.263626098632812, Learning Rate: 0.01\n",
      "Epoch [16079/20000], Loss: -21.263763427734375, Learning Rate: 0.01\n",
      "Epoch [16080/20000], Loss: -21.263961791992188, Learning Rate: 0.01\n",
      "Epoch [16081/20000], Loss: -21.264068603515625, Learning Rate: 0.01\n",
      "Epoch [16082/20000], Loss: -21.26416015625, Learning Rate: 0.01\n",
      "Epoch [16083/20000], Loss: -21.264266967773438, Learning Rate: 0.01\n",
      "Epoch [16084/20000], Loss: -21.264358520507812, Learning Rate: 0.01\n",
      "Epoch [16085/20000], Loss: -21.264434814453125, Learning Rate: 0.01\n",
      "Epoch [16086/20000], Loss: -21.264617919921875, Learning Rate: 0.01\n",
      "Epoch [16087/20000], Loss: -21.264633178710938, Learning Rate: 0.01\n",
      "Epoch [16088/20000], Loss: -21.264801025390625, Learning Rate: 0.01\n",
      "Epoch [16089/20000], Loss: -21.264907836914062, Learning Rate: 0.01\n",
      "Epoch [16090/20000], Loss: -21.265151977539062, Learning Rate: 0.01\n",
      "Epoch [16091/20000], Loss: -21.265151977539062, Learning Rate: 0.01\n",
      "Epoch [16092/20000], Loss: -21.26531982421875, Learning Rate: 0.01\n",
      "Epoch [16093/20000], Loss: -21.265472412109375, Learning Rate: 0.01\n",
      "Epoch [16094/20000], Loss: -21.265472412109375, Learning Rate: 0.01\n",
      "Epoch [16095/20000], Loss: -21.26568603515625, Learning Rate: 0.01\n",
      "Epoch [16096/20000], Loss: -21.265731811523438, Learning Rate: 0.01\n",
      "Epoch [16097/20000], Loss: -21.265838623046875, Learning Rate: 0.01\n",
      "Epoch [16098/20000], Loss: -21.265869140625, Learning Rate: 0.01\n",
      "Epoch [16099/20000], Loss: -21.265945434570312, Learning Rate: 0.01\n",
      "Epoch [16100/20000], Loss: -21.26611328125, Learning Rate: 0.01\n",
      "Epoch [16101/20000], Loss: -21.266189575195312, Learning Rate: 0.01\n",
      "Epoch [16102/20000], Loss: -21.266265869140625, Learning Rate: 0.01\n",
      "Epoch [16103/20000], Loss: -21.266143798828125, Learning Rate: 0.01\n",
      "Epoch [16104/20000], Loss: -21.266128540039062, Learning Rate: 0.01\n",
      "Epoch [16105/20000], Loss: -21.266006469726562, Learning Rate: 0.01\n",
      "Epoch [16106/20000], Loss: -21.26593017578125, Learning Rate: 0.01\n",
      "Epoch [16107/20000], Loss: -21.265533447265625, Learning Rate: 0.01\n",
      "Epoch [16108/20000], Loss: -21.265045166015625, Learning Rate: 0.01\n",
      "Epoch [16109/20000], Loss: -21.26416015625, Learning Rate: 0.01\n",
      "Epoch [16110/20000], Loss: -21.262939453125, Learning Rate: 0.01\n",
      "Epoch [16111/20000], Loss: -21.260848999023438, Learning Rate: 0.01\n",
      "Epoch [16112/20000], Loss: -21.25775146484375, Learning Rate: 0.01\n",
      "Epoch [16113/20000], Loss: -21.252914428710938, Learning Rate: 0.01\n",
      "Epoch [16114/20000], Loss: -21.245498657226562, Learning Rate: 0.01\n",
      "Epoch [16115/20000], Loss: -21.234130859375, Learning Rate: 0.01\n",
      "Epoch [16116/20000], Loss: -21.216827392578125, Learning Rate: 0.01\n",
      "Epoch [16117/20000], Loss: -21.190078735351562, Learning Rate: 0.01\n",
      "Epoch [16118/20000], Loss: -21.149581909179688, Learning Rate: 0.01\n",
      "Epoch [16119/20000], Loss: -21.088333129882812, Learning Rate: 0.01\n",
      "Epoch [16120/20000], Loss: -20.999160766601562, Learning Rate: 0.01\n",
      "Epoch [16121/20000], Loss: -20.871856689453125, Learning Rate: 0.01\n",
      "Epoch [16122/20000], Loss: -20.705780029296875, Learning Rate: 0.01\n",
      "Epoch [16123/20000], Loss: -20.504989624023438, Learning Rate: 0.01\n",
      "Epoch [16124/20000], Loss: -20.317611694335938, Learning Rate: 0.01\n",
      "Epoch [16125/20000], Loss: -20.202774047851562, Learning Rate: 0.01\n",
      "Epoch [16126/20000], Loss: -20.264236450195312, Learning Rate: 0.01\n",
      "Epoch [16127/20000], Loss: -20.511688232421875, Learning Rate: 0.01\n",
      "Epoch [16128/20000], Loss: -20.873748779296875, Learning Rate: 0.01\n",
      "Epoch [16129/20000], Loss: -21.1668701171875, Learning Rate: 0.01\n",
      "Epoch [16130/20000], Loss: -21.261825561523438, Learning Rate: 0.01\n",
      "Epoch [16131/20000], Loss: -21.160202026367188, Learning Rate: 0.01\n",
      "Epoch [16132/20000], Loss: -20.981353759765625, Learning Rate: 0.01\n",
      "Epoch [16133/20000], Loss: -20.872650146484375, Learning Rate: 0.01\n",
      "Epoch [16134/20000], Loss: -20.910491943359375, Learning Rate: 0.01\n",
      "Epoch [16135/20000], Loss: -21.06329345703125, Learning Rate: 0.01\n",
      "Epoch [16136/20000], Loss: -21.212646484375, Learning Rate: 0.01\n",
      "Epoch [16137/20000], Loss: -21.261474609375, Learning Rate: 0.01\n",
      "Epoch [16138/20000], Loss: -21.203292846679688, Learning Rate: 0.01\n",
      "Epoch [16139/20000], Loss: -21.11285400390625, Learning Rate: 0.01\n",
      "Epoch [16140/20000], Loss: -21.076019287109375, Learning Rate: 0.01\n",
      "Epoch [16141/20000], Loss: -21.121063232421875, Learning Rate: 0.01\n",
      "Epoch [16142/20000], Loss: -21.207000732421875, Learning Rate: 0.01\n",
      "Epoch [16143/20000], Loss: -21.264755249023438, Learning Rate: 0.01\n",
      "Epoch [16144/20000], Loss: -21.25982666015625, Learning Rate: 0.01\n",
      "Epoch [16145/20000], Loss: -21.213241577148438, Learning Rate: 0.01\n",
      "Epoch [16146/20000], Loss: -21.174850463867188, Learning Rate: 0.01\n",
      "Epoch [16147/20000], Loss: -21.1785888671875, Learning Rate: 0.01\n",
      "Epoch [16148/20000], Loss: -21.217864990234375, Learning Rate: 0.01\n",
      "Epoch [16149/20000], Loss: -21.258438110351562, Learning Rate: 0.01\n",
      "Epoch [16150/20000], Loss: -21.270828247070312, Learning Rate: 0.01\n",
      "Epoch [16151/20000], Loss: -21.2540283203125, Learning Rate: 0.01\n",
      "Epoch [16152/20000], Loss: -21.229995727539062, Learning Rate: 0.01\n",
      "Epoch [16153/20000], Loss: -21.222137451171875, Learning Rate: 0.01\n",
      "Epoch [16154/20000], Loss: -21.23602294921875, Learning Rate: 0.01\n",
      "Epoch [16155/20000], Loss: -21.258209228515625, Learning Rate: 0.01\n",
      "Epoch [16156/20000], Loss: -21.271041870117188, Learning Rate: 0.01\n",
      "Epoch [16157/20000], Loss: -21.26763916015625, Learning Rate: 0.01\n",
      "Epoch [16158/20000], Loss: -21.255233764648438, Learning Rate: 0.01\n",
      "Epoch [16159/20000], Loss: -21.246780395507812, Learning Rate: 0.01\n",
      "Epoch [16160/20000], Loss: -21.249618530273438, Learning Rate: 0.01\n",
      "Epoch [16161/20000], Loss: -21.260589599609375, Learning Rate: 0.01\n",
      "Epoch [16162/20000], Loss: -21.270599365234375, Learning Rate: 0.01\n",
      "Epoch [16163/20000], Loss: -21.273147583007812, Learning Rate: 0.01\n",
      "Epoch [16164/20000], Loss: -21.268341064453125, Learning Rate: 0.01\n",
      "Epoch [16165/20000], Loss: -21.262252807617188, Learning Rate: 0.01\n",
      "Epoch [16166/20000], Loss: -21.260299682617188, Learning Rate: 0.01\n",
      "Epoch [16167/20000], Loss: -21.26373291015625, Learning Rate: 0.01\n",
      "Epoch [16168/20000], Loss: -21.269500732421875, Learning Rate: 0.01\n",
      "Epoch [16169/20000], Loss: -21.273361206054688, Learning Rate: 0.01\n",
      "Epoch [16170/20000], Loss: -21.27301025390625, Learning Rate: 0.01\n",
      "Epoch [16171/20000], Loss: -21.270187377929688, Learning Rate: 0.01\n",
      "Epoch [16172/20000], Loss: -21.267745971679688, Learning Rate: 0.01\n",
      "Epoch [16173/20000], Loss: -21.267837524414062, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16174/20000], Loss: -21.27020263671875, Learning Rate: 0.01\n",
      "Epoch [16175/20000], Loss: -21.273117065429688, Learning Rate: 0.01\n",
      "Epoch [16176/20000], Loss: -21.274566650390625, Learning Rate: 0.01\n",
      "Epoch [16177/20000], Loss: -21.274215698242188, Learning Rate: 0.01\n",
      "Epoch [16178/20000], Loss: -21.272720336914062, Learning Rate: 0.01\n",
      "Epoch [16179/20000], Loss: -21.271514892578125, Learning Rate: 0.01\n",
      "Epoch [16180/20000], Loss: -21.271881103515625, Learning Rate: 0.01\n",
      "Epoch [16181/20000], Loss: -21.273147583007812, Learning Rate: 0.01\n",
      "Epoch [16182/20000], Loss: -21.274520874023438, Learning Rate: 0.01\n",
      "Epoch [16183/20000], Loss: -21.275283813476562, Learning Rate: 0.01\n",
      "Epoch [16184/20000], Loss: -21.275054931640625, Learning Rate: 0.01\n",
      "Epoch [16185/20000], Loss: -21.274383544921875, Learning Rate: 0.01\n",
      "Epoch [16186/20000], Loss: -21.273910522460938, Learning Rate: 0.01\n",
      "Epoch [16187/20000], Loss: -21.27410888671875, Learning Rate: 0.01\n",
      "Epoch [16188/20000], Loss: -21.274703979492188, Learning Rate: 0.01\n",
      "Epoch [16189/20000], Loss: -21.275436401367188, Learning Rate: 0.01\n",
      "Epoch [16190/20000], Loss: -21.275924682617188, Learning Rate: 0.01\n",
      "Epoch [16191/20000], Loss: -21.275985717773438, Learning Rate: 0.01\n",
      "Epoch [16192/20000], Loss: -21.275772094726562, Learning Rate: 0.01\n",
      "Epoch [16193/20000], Loss: -21.275558471679688, Learning Rate: 0.01\n",
      "Epoch [16194/20000], Loss: -21.275604248046875, Learning Rate: 0.01\n",
      "Epoch [16195/20000], Loss: -21.275909423828125, Learning Rate: 0.01\n",
      "Epoch [16196/20000], Loss: -21.276260375976562, Learning Rate: 0.01\n",
      "Epoch [16197/20000], Loss: -21.276611328125, Learning Rate: 0.01\n",
      "Epoch [16198/20000], Loss: -21.276763916015625, Learning Rate: 0.01\n",
      "Epoch [16199/20000], Loss: -21.276763916015625, Learning Rate: 0.01\n",
      "Epoch [16200/20000], Loss: -21.276687622070312, Learning Rate: 0.01\n",
      "Epoch [16201/20000], Loss: -21.276641845703125, Learning Rate: 0.01\n",
      "Epoch [16202/20000], Loss: -21.276779174804688, Learning Rate: 0.01\n",
      "Epoch [16203/20000], Loss: -21.276962280273438, Learning Rate: 0.01\n",
      "Epoch [16204/20000], Loss: -21.277175903320312, Learning Rate: 0.01\n",
      "Epoch [16205/20000], Loss: -21.277450561523438, Learning Rate: 0.01\n",
      "Epoch [16206/20000], Loss: -21.27752685546875, Learning Rate: 0.01\n",
      "Epoch [16207/20000], Loss: -21.277557373046875, Learning Rate: 0.01\n",
      "Epoch [16208/20000], Loss: -21.27752685546875, Learning Rate: 0.01\n",
      "Epoch [16209/20000], Loss: -21.277679443359375, Learning Rate: 0.01\n",
      "Epoch [16210/20000], Loss: -21.277694702148438, Learning Rate: 0.01\n",
      "Epoch [16211/20000], Loss: -21.277862548828125, Learning Rate: 0.01\n",
      "Epoch [16212/20000], Loss: -21.277969360351562, Learning Rate: 0.01\n",
      "Epoch [16213/20000], Loss: -21.278152465820312, Learning Rate: 0.01\n",
      "Epoch [16214/20000], Loss: -21.278167724609375, Learning Rate: 0.01\n",
      "Epoch [16215/20000], Loss: -21.2783203125, Learning Rate: 0.01\n",
      "Epoch [16216/20000], Loss: -21.278350830078125, Learning Rate: 0.01\n",
      "Epoch [16217/20000], Loss: -21.278396606445312, Learning Rate: 0.01\n",
      "Epoch [16218/20000], Loss: -21.278518676757812, Learning Rate: 0.01\n",
      "Epoch [16219/20000], Loss: -21.278579711914062, Learning Rate: 0.01\n",
      "Epoch [16220/20000], Loss: -21.2786865234375, Learning Rate: 0.01\n",
      "Epoch [16221/20000], Loss: -21.278778076171875, Learning Rate: 0.01\n",
      "Epoch [16222/20000], Loss: -21.27880859375, Learning Rate: 0.01\n",
      "Epoch [16223/20000], Loss: -21.278762817382812, Learning Rate: 0.01\n",
      "Epoch [16224/20000], Loss: -21.278793334960938, Learning Rate: 0.01\n",
      "Epoch [16225/20000], Loss: -21.278717041015625, Learning Rate: 0.01\n",
      "Epoch [16226/20000], Loss: -21.27838134765625, Learning Rate: 0.01\n",
      "Epoch [16227/20000], Loss: -21.278121948242188, Learning Rate: 0.01\n",
      "Epoch [16228/20000], Loss: -21.27752685546875, Learning Rate: 0.01\n",
      "Epoch [16229/20000], Loss: -21.276687622070312, Learning Rate: 0.01\n",
      "Epoch [16230/20000], Loss: -21.275344848632812, Learning Rate: 0.01\n",
      "Epoch [16231/20000], Loss: -21.27325439453125, Learning Rate: 0.01\n",
      "Epoch [16232/20000], Loss: -21.269989013671875, Learning Rate: 0.01\n",
      "Epoch [16233/20000], Loss: -21.264755249023438, Learning Rate: 0.01\n",
      "Epoch [16234/20000], Loss: -21.256622314453125, Learning Rate: 0.01\n",
      "Epoch [16235/20000], Loss: -21.243759155273438, Learning Rate: 0.01\n",
      "Epoch [16236/20000], Loss: -21.223617553710938, Learning Rate: 0.01\n",
      "Epoch [16237/20000], Loss: -21.191497802734375, Learning Rate: 0.01\n",
      "Epoch [16238/20000], Loss: -21.141128540039062, Learning Rate: 0.01\n",
      "Epoch [16239/20000], Loss: -21.061111450195312, Learning Rate: 0.01\n",
      "Epoch [16240/20000], Loss: -20.938522338867188, Learning Rate: 0.01\n",
      "Epoch [16241/20000], Loss: -20.750625610351562, Learning Rate: 0.01\n",
      "Epoch [16242/20000], Loss: -20.48358154296875, Learning Rate: 0.01\n",
      "Epoch [16243/20000], Loss: -20.11785888671875, Learning Rate: 0.01\n",
      "Epoch [16244/20000], Loss: -19.707275390625, Learning Rate: 0.01\n",
      "Epoch [16245/20000], Loss: -19.328125, Learning Rate: 0.01\n",
      "Epoch [16246/20000], Loss: -19.226303100585938, Learning Rate: 0.01\n",
      "Epoch [16247/20000], Loss: -19.516281127929688, Learning Rate: 0.01\n",
      "Epoch [16248/20000], Loss: -20.211456298828125, Learning Rate: 0.01\n",
      "Epoch [16249/20000], Loss: -20.9251708984375, Learning Rate: 0.01\n",
      "Epoch [16250/20000], Loss: -21.27294921875, Learning Rate: 0.01\n",
      "Epoch [16251/20000], Loss: -21.133010864257812, Learning Rate: 0.01\n",
      "Epoch [16252/20000], Loss: -20.737960815429688, Learning Rate: 0.01\n",
      "Epoch [16253/20000], Loss: -20.465591430664062, Learning Rate: 0.01\n",
      "Epoch [16254/20000], Loss: -20.535171508789062, Learning Rate: 0.01\n",
      "Epoch [16255/20000], Loss: -20.887420654296875, Learning Rate: 0.01\n",
      "Epoch [16256/20000], Loss: -21.209609985351562, Learning Rate: 0.01\n",
      "Epoch [16257/20000], Loss: -21.266159057617188, Learning Rate: 0.01\n",
      "Epoch [16258/20000], Loss: -21.085433959960938, Learning Rate: 0.01\n",
      "Epoch [16259/20000], Loss: -20.89263916015625, Learning Rate: 0.01\n",
      "Epoch [16260/20000], Loss: -20.886016845703125, Learning Rate: 0.01\n",
      "Epoch [16261/20000], Loss: -21.05615234375, Learning Rate: 0.01\n",
      "Epoch [16262/20000], Loss: -21.236923217773438, Learning Rate: 0.01\n",
      "Epoch [16263/20000], Loss: -21.2750244140625, Learning Rate: 0.01\n",
      "Epoch [16264/20000], Loss: -21.1773681640625, Learning Rate: 0.01\n",
      "Epoch [16265/20000], Loss: -21.074356079101562, Learning Rate: 0.01\n",
      "Epoch [16266/20000], Loss: -21.074569702148438, Learning Rate: 0.01\n",
      "Epoch [16267/20000], Loss: -21.171310424804688, Learning Rate: 0.01\n",
      "Epoch [16268/20000], Loss: -21.264389038085938, Learning Rate: 0.01\n",
      "Epoch [16269/20000], Loss: -21.275711059570312, Learning Rate: 0.01\n",
      "Epoch [16270/20000], Loss: -21.218307495117188, Learning Rate: 0.01\n",
      "Epoch [16271/20000], Loss: -21.166336059570312, Learning Rate: 0.01\n",
      "Epoch [16272/20000], Loss: -21.175079345703125, Learning Rate: 0.01\n",
      "Epoch [16273/20000], Loss: -21.23016357421875, Learning Rate: 0.01\n",
      "Epoch [16274/20000], Loss: -21.2767333984375, Learning Rate: 0.01\n",
      "Epoch [16275/20000], Loss: -21.277130126953125, Learning Rate: 0.01\n",
      "Epoch [16276/20000], Loss: -21.244125366210938, Learning Rate: 0.01\n",
      "Epoch [16277/20000], Loss: -21.218719482421875, Learning Rate: 0.01\n",
      "Epoch [16278/20000], Loss: -21.22674560546875, Learning Rate: 0.01\n",
      "Epoch [16279/20000], Loss: -21.257659912109375, Learning Rate: 0.01\n",
      "Epoch [16280/20000], Loss: -21.281234741210938, Learning Rate: 0.01\n",
      "Epoch [16281/20000], Loss: -21.279632568359375, Learning Rate: 0.01\n",
      "Epoch [16282/20000], Loss: -21.261550903320312, Learning Rate: 0.01\n",
      "Epoch [16283/20000], Loss: -21.24847412109375, Learning Rate: 0.01\n",
      "Epoch [16284/20000], Loss: -21.253433227539062, Learning Rate: 0.01\n",
      "Epoch [16285/20000], Loss: -21.270065307617188, Learning Rate: 0.01\n",
      "Epoch [16286/20000], Loss: -21.28289794921875, Learning Rate: 0.01\n",
      "Epoch [16287/20000], Loss: -21.282318115234375, Learning Rate: 0.01\n",
      "Epoch [16288/20000], Loss: -21.272796630859375, Learning Rate: 0.01\n",
      "Epoch [16289/20000], Loss: -21.265579223632812, Learning Rate: 0.01\n",
      "Epoch [16290/20000], Loss: -21.267562866210938, Learning Rate: 0.01\n",
      "Epoch [16291/20000], Loss: -21.276107788085938, Learning Rate: 0.01\n",
      "Epoch [16292/20000], Loss: -21.283493041992188, Learning Rate: 0.01\n",
      "Epoch [16293/20000], Loss: -21.284072875976562, Learning Rate: 0.01\n",
      "Epoch [16294/20000], Loss: -21.279571533203125, Learning Rate: 0.01\n",
      "Epoch [16295/20000], Loss: -21.27520751953125, Learning Rate: 0.01\n",
      "Epoch [16296/20000], Loss: -21.275344848632812, Learning Rate: 0.01\n",
      "Epoch [16297/20000], Loss: -21.279403686523438, Learning Rate: 0.01\n",
      "Epoch [16298/20000], Loss: -21.283843994140625, Learning Rate: 0.01\n",
      "Epoch [16299/20000], Loss: -21.285247802734375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16300/20000], Loss: -21.283447265625, Learning Rate: 0.01\n",
      "Epoch [16301/20000], Loss: -21.280899047851562, Learning Rate: 0.01\n",
      "Epoch [16302/20000], Loss: -21.2801513671875, Learning Rate: 0.01\n",
      "Epoch [16303/20000], Loss: -21.28173828125, Learning Rate: 0.01\n",
      "Epoch [16304/20000], Loss: -21.284194946289062, Learning Rate: 0.01\n",
      "Epoch [16305/20000], Loss: -21.285690307617188, Learning Rate: 0.01\n",
      "Epoch [16306/20000], Loss: -21.285446166992188, Learning Rate: 0.01\n",
      "Epoch [16307/20000], Loss: -21.2841796875, Learning Rate: 0.01\n",
      "Epoch [16308/20000], Loss: -21.283218383789062, Learning Rate: 0.01\n",
      "Epoch [16309/20000], Loss: -21.283599853515625, Learning Rate: 0.01\n",
      "Epoch [16310/20000], Loss: -21.28472900390625, Learning Rate: 0.01\n",
      "Epoch [16311/20000], Loss: -21.285888671875, Learning Rate: 0.01\n",
      "Epoch [16312/20000], Loss: -21.286361694335938, Learning Rate: 0.01\n",
      "Epoch [16313/20000], Loss: -21.285980224609375, Learning Rate: 0.01\n",
      "Epoch [16314/20000], Loss: -21.285415649414062, Learning Rate: 0.01\n",
      "Epoch [16315/20000], Loss: -21.28509521484375, Learning Rate: 0.01\n",
      "Epoch [16316/20000], Loss: -21.285476684570312, Learning Rate: 0.01\n",
      "Epoch [16317/20000], Loss: -21.286148071289062, Learning Rate: 0.01\n",
      "Epoch [16318/20000], Loss: -21.286697387695312, Learning Rate: 0.01\n",
      "Epoch [16319/20000], Loss: -21.28692626953125, Learning Rate: 0.01\n",
      "Epoch [16320/20000], Loss: -21.286758422851562, Learning Rate: 0.01\n",
      "Epoch [16321/20000], Loss: -21.286483764648438, Learning Rate: 0.01\n",
      "Epoch [16322/20000], Loss: -21.2864990234375, Learning Rate: 0.01\n",
      "Epoch [16323/20000], Loss: -21.286712646484375, Learning Rate: 0.01\n",
      "Epoch [16324/20000], Loss: -21.287017822265625, Learning Rate: 0.01\n",
      "Epoch [16325/20000], Loss: -21.287261962890625, Learning Rate: 0.01\n",
      "Epoch [16326/20000], Loss: -21.287490844726562, Learning Rate: 0.01\n",
      "Epoch [16327/20000], Loss: -21.287521362304688, Learning Rate: 0.01\n",
      "Epoch [16328/20000], Loss: -21.287429809570312, Learning Rate: 0.01\n",
      "Epoch [16329/20000], Loss: -21.287460327148438, Learning Rate: 0.01\n",
      "Epoch [16330/20000], Loss: -21.28753662109375, Learning Rate: 0.01\n",
      "Epoch [16331/20000], Loss: -21.28765869140625, Learning Rate: 0.01\n",
      "Epoch [16332/20000], Loss: -21.287933349609375, Learning Rate: 0.01\n",
      "Epoch [16333/20000], Loss: -21.288101196289062, Learning Rate: 0.01\n",
      "Epoch [16334/20000], Loss: -21.288055419921875, Learning Rate: 0.01\n",
      "Epoch [16335/20000], Loss: -21.288131713867188, Learning Rate: 0.01\n",
      "Epoch [16336/20000], Loss: -21.288162231445312, Learning Rate: 0.01\n",
      "Epoch [16337/20000], Loss: -21.288223266601562, Learning Rate: 0.01\n",
      "Epoch [16338/20000], Loss: -21.288314819335938, Learning Rate: 0.01\n",
      "Epoch [16339/20000], Loss: -21.288421630859375, Learning Rate: 0.01\n",
      "Epoch [16340/20000], Loss: -21.288558959960938, Learning Rate: 0.01\n",
      "Epoch [16341/20000], Loss: -21.288742065429688, Learning Rate: 0.01\n",
      "Epoch [16342/20000], Loss: -21.288833618164062, Learning Rate: 0.01\n",
      "Epoch [16343/20000], Loss: -21.288772583007812, Learning Rate: 0.01\n",
      "Epoch [16344/20000], Loss: -21.288909912109375, Learning Rate: 0.01\n",
      "Epoch [16345/20000], Loss: -21.288925170898438, Learning Rate: 0.01\n",
      "Epoch [16346/20000], Loss: -21.289093017578125, Learning Rate: 0.01\n",
      "Epoch [16347/20000], Loss: -21.28912353515625, Learning Rate: 0.01\n",
      "Epoch [16348/20000], Loss: -21.28924560546875, Learning Rate: 0.01\n",
      "Epoch [16349/20000], Loss: -21.289321899414062, Learning Rate: 0.01\n",
      "Epoch [16350/20000], Loss: -21.2894287109375, Learning Rate: 0.01\n",
      "Epoch [16351/20000], Loss: -21.28948974609375, Learning Rate: 0.01\n",
      "Epoch [16352/20000], Loss: -21.289596557617188, Learning Rate: 0.01\n",
      "Epoch [16353/20000], Loss: -21.289642333984375, Learning Rate: 0.01\n",
      "Epoch [16354/20000], Loss: -21.2896728515625, Learning Rate: 0.01\n",
      "Epoch [16355/20000], Loss: -21.289825439453125, Learning Rate: 0.01\n",
      "Epoch [16356/20000], Loss: -21.289886474609375, Learning Rate: 0.01\n",
      "Epoch [16357/20000], Loss: -21.289993286132812, Learning Rate: 0.01\n",
      "Epoch [16358/20000], Loss: -21.2901611328125, Learning Rate: 0.01\n",
      "Epoch [16359/20000], Loss: -21.29022216796875, Learning Rate: 0.01\n",
      "Epoch [16360/20000], Loss: -21.290176391601562, Learning Rate: 0.01\n",
      "Epoch [16361/20000], Loss: -21.290206909179688, Learning Rate: 0.01\n",
      "Epoch [16362/20000], Loss: -21.290328979492188, Learning Rate: 0.01\n",
      "Epoch [16363/20000], Loss: -21.290420532226562, Learning Rate: 0.01\n",
      "Epoch [16364/20000], Loss: -21.290390014648438, Learning Rate: 0.01\n",
      "Epoch [16365/20000], Loss: -21.290573120117188, Learning Rate: 0.01\n",
      "Epoch [16366/20000], Loss: -21.290664672851562, Learning Rate: 0.01\n",
      "Epoch [16367/20000], Loss: -21.29071044921875, Learning Rate: 0.01\n",
      "Epoch [16368/20000], Loss: -21.290802001953125, Learning Rate: 0.01\n",
      "Epoch [16369/20000], Loss: -21.29095458984375, Learning Rate: 0.01\n",
      "Epoch [16370/20000], Loss: -21.291000366210938, Learning Rate: 0.01\n",
      "Epoch [16371/20000], Loss: -21.291000366210938, Learning Rate: 0.01\n",
      "Epoch [16372/20000], Loss: -21.291107177734375, Learning Rate: 0.01\n",
      "Epoch [16373/20000], Loss: -21.291213989257812, Learning Rate: 0.01\n",
      "Epoch [16374/20000], Loss: -21.29132080078125, Learning Rate: 0.01\n",
      "Epoch [16375/20000], Loss: -21.291366577148438, Learning Rate: 0.01\n",
      "Epoch [16376/20000], Loss: -21.291412353515625, Learning Rate: 0.01\n",
      "Epoch [16377/20000], Loss: -21.291534423828125, Learning Rate: 0.01\n",
      "Epoch [16378/20000], Loss: -21.2916259765625, Learning Rate: 0.01\n",
      "Epoch [16379/20000], Loss: -21.29168701171875, Learning Rate: 0.01\n",
      "Epoch [16380/20000], Loss: -21.291793823242188, Learning Rate: 0.01\n",
      "Epoch [16381/20000], Loss: -21.291854858398438, Learning Rate: 0.01\n",
      "Epoch [16382/20000], Loss: -21.291915893554688, Learning Rate: 0.01\n",
      "Epoch [16383/20000], Loss: -21.2919921875, Learning Rate: 0.01\n",
      "Epoch [16384/20000], Loss: -21.292037963867188, Learning Rate: 0.01\n",
      "Epoch [16385/20000], Loss: -21.2921142578125, Learning Rate: 0.01\n",
      "Epoch [16386/20000], Loss: -21.292205810546875, Learning Rate: 0.01\n",
      "Epoch [16387/20000], Loss: -21.292282104492188, Learning Rate: 0.01\n",
      "Epoch [16388/20000], Loss: -21.292282104492188, Learning Rate: 0.01\n",
      "Epoch [16389/20000], Loss: -21.292434692382812, Learning Rate: 0.01\n",
      "Epoch [16390/20000], Loss: -21.29254150390625, Learning Rate: 0.01\n",
      "Epoch [16391/20000], Loss: -21.292495727539062, Learning Rate: 0.01\n",
      "Epoch [16392/20000], Loss: -21.292694091796875, Learning Rate: 0.01\n",
      "Epoch [16393/20000], Loss: -21.292739868164062, Learning Rate: 0.01\n",
      "Epoch [16394/20000], Loss: -21.292831420898438, Learning Rate: 0.01\n",
      "Epoch [16395/20000], Loss: -21.292953491210938, Learning Rate: 0.01\n",
      "Epoch [16396/20000], Loss: -21.292938232421875, Learning Rate: 0.01\n",
      "Epoch [16397/20000], Loss: -21.293060302734375, Learning Rate: 0.01\n",
      "Epoch [16398/20000], Loss: -21.2930908203125, Learning Rate: 0.01\n",
      "Epoch [16399/20000], Loss: -21.293258666992188, Learning Rate: 0.01\n",
      "Epoch [16400/20000], Loss: -21.293167114257812, Learning Rate: 0.01\n",
      "Epoch [16401/20000], Loss: -21.293258666992188, Learning Rate: 0.01\n",
      "Epoch [16402/20000], Loss: -21.293472290039062, Learning Rate: 0.01\n",
      "Epoch [16403/20000], Loss: -21.29345703125, Learning Rate: 0.01\n",
      "Epoch [16404/20000], Loss: -21.293563842773438, Learning Rate: 0.01\n",
      "Epoch [16405/20000], Loss: -21.293594360351562, Learning Rate: 0.01\n",
      "Epoch [16406/20000], Loss: -21.293701171875, Learning Rate: 0.01\n",
      "Epoch [16407/20000], Loss: -21.293701171875, Learning Rate: 0.01\n",
      "Epoch [16408/20000], Loss: -21.2938232421875, Learning Rate: 0.01\n",
      "Epoch [16409/20000], Loss: -21.294021606445312, Learning Rate: 0.01\n",
      "Epoch [16410/20000], Loss: -21.293975830078125, Learning Rate: 0.01\n",
      "Epoch [16411/20000], Loss: -21.294052124023438, Learning Rate: 0.01\n",
      "Epoch [16412/20000], Loss: -21.294113159179688, Learning Rate: 0.01\n",
      "Epoch [16413/20000], Loss: -21.294235229492188, Learning Rate: 0.01\n",
      "Epoch [16414/20000], Loss: -21.29425048828125, Learning Rate: 0.01\n",
      "Epoch [16415/20000], Loss: -21.294326782226562, Learning Rate: 0.01\n",
      "Epoch [16416/20000], Loss: -21.294357299804688, Learning Rate: 0.01\n",
      "Epoch [16417/20000], Loss: -21.294448852539062, Learning Rate: 0.01\n",
      "Epoch [16418/20000], Loss: -21.294525146484375, Learning Rate: 0.01\n",
      "Epoch [16419/20000], Loss: -21.294448852539062, Learning Rate: 0.01\n",
      "Epoch [16420/20000], Loss: -21.294509887695312, Learning Rate: 0.01\n",
      "Epoch [16421/20000], Loss: -21.294540405273438, Learning Rate: 0.01\n",
      "Epoch [16422/20000], Loss: -21.294342041015625, Learning Rate: 0.01\n",
      "Epoch [16423/20000], Loss: -21.294235229492188, Learning Rate: 0.01\n",
      "Epoch [16424/20000], Loss: -21.29388427734375, Learning Rate: 0.01\n",
      "Epoch [16425/20000], Loss: -21.2933349609375, Learning Rate: 0.01\n",
      "Epoch [16426/20000], Loss: -21.292404174804688, Learning Rate: 0.01\n",
      "Epoch [16427/20000], Loss: -21.290908813476562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16428/20000], Loss: -21.288589477539062, Learning Rate: 0.01\n",
      "Epoch [16429/20000], Loss: -21.284591674804688, Learning Rate: 0.01\n",
      "Epoch [16430/20000], Loss: -21.27825927734375, Learning Rate: 0.01\n",
      "Epoch [16431/20000], Loss: -21.26812744140625, Learning Rate: 0.01\n",
      "Epoch [16432/20000], Loss: -21.2513427734375, Learning Rate: 0.01\n",
      "Epoch [16433/20000], Loss: -21.224258422851562, Learning Rate: 0.01\n",
      "Epoch [16434/20000], Loss: -21.1805419921875, Learning Rate: 0.01\n",
      "Epoch [16435/20000], Loss: -21.110382080078125, Learning Rate: 0.01\n",
      "Epoch [16436/20000], Loss: -21.00146484375, Learning Rate: 0.01\n",
      "Epoch [16437/20000], Loss: -20.835952758789062, Learning Rate: 0.01\n",
      "Epoch [16438/20000], Loss: -20.605728149414062, Learning Rate: 0.01\n",
      "Epoch [16439/20000], Loss: -20.315277099609375, Learning Rate: 0.01\n",
      "Epoch [16440/20000], Loss: -20.03814697265625, Learning Rate: 0.01\n",
      "Epoch [16441/20000], Loss: -19.89166259765625, Learning Rate: 0.01\n",
      "Epoch [16442/20000], Loss: -20.0457763671875, Learning Rate: 0.01\n",
      "Epoch [16443/20000], Loss: -20.489639282226562, Learning Rate: 0.01\n",
      "Epoch [16444/20000], Loss: -21.009902954101562, Learning Rate: 0.01\n",
      "Epoch [16445/20000], Loss: -21.285690307617188, Learning Rate: 0.01\n",
      "Epoch [16446/20000], Loss: -21.201126098632812, Learning Rate: 0.01\n",
      "Epoch [16447/20000], Loss: -20.92034912109375, Learning Rate: 0.01\n",
      "Epoch [16448/20000], Loss: -20.72515869140625, Learning Rate: 0.01\n",
      "Epoch [16449/20000], Loss: -20.796188354492188, Learning Rate: 0.01\n",
      "Epoch [16450/20000], Loss: -21.05572509765625, Learning Rate: 0.01\n",
      "Epoch [16451/20000], Loss: -21.26837158203125, Learning Rate: 0.01\n",
      "Epoch [16452/20000], Loss: -21.26898193359375, Learning Rate: 0.01\n",
      "Epoch [16453/20000], Loss: -21.117416381835938, Learning Rate: 0.01\n",
      "Epoch [16454/20000], Loss: -21.003829956054688, Learning Rate: 0.01\n",
      "Epoch [16455/20000], Loss: -21.046722412109375, Learning Rate: 0.01\n",
      "Epoch [16456/20000], Loss: -21.193359375, Learning Rate: 0.01\n",
      "Epoch [16457/20000], Loss: -21.292373657226562, Learning Rate: 0.01\n",
      "Epoch [16458/20000], Loss: -21.266189575195312, Learning Rate: 0.01\n",
      "Epoch [16459/20000], Loss: -21.176254272460938, Learning Rate: 0.01\n",
      "Epoch [16460/20000], Loss: -21.135406494140625, Learning Rate: 0.01\n",
      "Epoch [16461/20000], Loss: -21.186309814453125, Learning Rate: 0.01\n",
      "Epoch [16462/20000], Loss: -21.267486572265625, Learning Rate: 0.01\n",
      "Epoch [16463/20000], Loss: -21.297378540039062, Learning Rate: 0.01\n",
      "Epoch [16464/20000], Loss: -21.261428833007812, Learning Rate: 0.01\n",
      "Epoch [16465/20000], Loss: -21.21533203125, Learning Rate: 0.01\n",
      "Epoch [16466/20000], Loss: -21.214401245117188, Learning Rate: 0.01\n",
      "Epoch [16467/20000], Loss: -21.25604248046875, Learning Rate: 0.01\n",
      "Epoch [16468/20000], Loss: -21.293533325195312, Learning Rate: 0.01\n",
      "Epoch [16469/20000], Loss: -21.292739868164062, Learning Rate: 0.01\n",
      "Epoch [16470/20000], Loss: -21.265289306640625, Learning Rate: 0.01\n",
      "Epoch [16471/20000], Loss: -21.24798583984375, Learning Rate: 0.01\n",
      "Epoch [16472/20000], Loss: -21.25958251953125, Learning Rate: 0.01\n",
      "Epoch [16473/20000], Loss: -21.285476684570312, Learning Rate: 0.01\n",
      "Epoch [16474/20000], Loss: -21.298599243164062, Learning Rate: 0.01\n",
      "Epoch [16475/20000], Loss: -21.289993286132812, Learning Rate: 0.01\n",
      "Epoch [16476/20000], Loss: -21.274673461914062, Learning Rate: 0.01\n",
      "Epoch [16477/20000], Loss: -21.271224975585938, Learning Rate: 0.01\n",
      "Epoch [16478/20000], Loss: -21.282577514648438, Learning Rate: 0.01\n",
      "Epoch [16479/20000], Loss: -21.295700073242188, Learning Rate: 0.01\n",
      "Epoch [16480/20000], Loss: -21.298370361328125, Learning Rate: 0.01\n",
      "Epoch [16481/20000], Loss: -21.290985107421875, Learning Rate: 0.01\n",
      "Epoch [16482/20000], Loss: -21.283935546875, Learning Rate: 0.01\n",
      "Epoch [16483/20000], Loss: -21.285064697265625, Learning Rate: 0.01\n",
      "Epoch [16484/20000], Loss: -21.29254150390625, Learning Rate: 0.01\n",
      "Epoch [16485/20000], Loss: -21.298583984375, Learning Rate: 0.01\n",
      "Epoch [16486/20000], Loss: -21.298080444335938, Learning Rate: 0.01\n",
      "Epoch [16487/20000], Loss: -21.293472290039062, Learning Rate: 0.01\n",
      "Epoch [16488/20000], Loss: -21.290451049804688, Learning Rate: 0.01\n",
      "Epoch [16489/20000], Loss: -21.292068481445312, Learning Rate: 0.01\n",
      "Epoch [16490/20000], Loss: -21.29638671875, Learning Rate: 0.01\n",
      "Epoch [16491/20000], Loss: -21.298858642578125, Learning Rate: 0.01\n",
      "Epoch [16492/20000], Loss: -21.297744750976562, Learning Rate: 0.01\n",
      "Epoch [16493/20000], Loss: -21.294784545898438, Learning Rate: 0.01\n",
      "Epoch [16494/20000], Loss: -21.293136596679688, Learning Rate: 0.01\n",
      "Epoch [16495/20000], Loss: -21.293731689453125, Learning Rate: 0.01\n",
      "Epoch [16496/20000], Loss: -21.294952392578125, Learning Rate: 0.01\n",
      "Epoch [16497/20000], Loss: -21.294662475585938, Learning Rate: 0.01\n",
      "Epoch [16498/20000], Loss: -21.291824340820312, Learning Rate: 0.01\n",
      "Epoch [16499/20000], Loss: -21.287078857421875, Learning Rate: 0.01\n",
      "Epoch [16500/20000], Loss: -21.281478881835938, Learning Rate: 0.01\n",
      "Epoch [16501/20000], Loss: -21.27471923828125, Learning Rate: 0.01\n",
      "Epoch [16502/20000], Loss: -21.264739990234375, Learning Rate: 0.01\n",
      "Epoch [16503/20000], Loss: -21.248199462890625, Learning Rate: 0.01\n",
      "Epoch [16504/20000], Loss: -21.222183227539062, Learning Rate: 0.01\n",
      "Epoch [16505/20000], Loss: -21.182113647460938, Learning Rate: 0.01\n",
      "Epoch [16506/20000], Loss: -21.122940063476562, Learning Rate: 0.01\n",
      "Epoch [16507/20000], Loss: -21.034347534179688, Learning Rate: 0.01\n",
      "Epoch [16508/20000], Loss: -20.9063720703125, Learning Rate: 0.01\n",
      "Epoch [16509/20000], Loss: -20.721038818359375, Learning Rate: 0.01\n",
      "Epoch [16510/20000], Loss: -20.478042602539062, Learning Rate: 0.01\n",
      "Epoch [16511/20000], Loss: -20.172500610351562, Learning Rate: 0.01\n",
      "Epoch [16512/20000], Loss: -19.8763427734375, Learning Rate: 0.01\n",
      "Epoch [16513/20000], Loss: -19.65521240234375, Learning Rate: 0.01\n",
      "Epoch [16514/20000], Loss: -19.689483642578125, Learning Rate: 0.01\n",
      "Epoch [16515/20000], Loss: -20.004074096679688, Learning Rate: 0.01\n",
      "Epoch [16516/20000], Loss: -20.554336547851562, Learning Rate: 0.01\n",
      "Epoch [16517/20000], Loss: -21.06146240234375, Learning Rate: 0.01\n",
      "Epoch [16518/20000], Loss: -21.2955322265625, Learning Rate: 0.01\n",
      "Epoch [16519/20000], Loss: -21.1998291015625, Learning Rate: 0.01\n",
      "Epoch [16520/20000], Loss: -20.923065185546875, Learning Rate: 0.01\n",
      "Epoch [16521/20000], Loss: -20.701370239257812, Learning Rate: 0.01\n",
      "Epoch [16522/20000], Loss: -20.690078735351562, Learning Rate: 0.01\n",
      "Epoch [16523/20000], Loss: -20.8983154296875, Learning Rate: 0.01\n",
      "Epoch [16524/20000], Loss: -21.159103393554688, Learning Rate: 0.01\n",
      "Epoch [16525/20000], Loss: -21.29693603515625, Learning Rate: 0.01\n",
      "Epoch [16526/20000], Loss: -21.2501220703125, Learning Rate: 0.01\n",
      "Epoch [16527/20000], Loss: -21.104476928710938, Learning Rate: 0.01\n",
      "Epoch [16528/20000], Loss: -21.00335693359375, Learning Rate: 0.01\n",
      "Epoch [16529/20000], Loss: -21.027069091796875, Learning Rate: 0.01\n",
      "Epoch [16530/20000], Loss: -21.150146484375, Learning Rate: 0.01\n",
      "Epoch [16531/20000], Loss: -21.268112182617188, Learning Rate: 0.01\n",
      "Epoch [16532/20000], Loss: -21.299942016601562, Learning Rate: 0.01\n",
      "Epoch [16533/20000], Loss: -21.245880126953125, Learning Rate: 0.01\n",
      "Epoch [16534/20000], Loss: -21.172454833984375, Learning Rate: 0.01\n",
      "Epoch [16535/20000], Loss: -21.148971557617188, Learning Rate: 0.01\n",
      "Epoch [16536/20000], Loss: -21.190338134765625, Learning Rate: 0.01\n",
      "Epoch [16537/20000], Loss: -21.258758544921875, Learning Rate: 0.01\n",
      "Epoch [16538/20000], Loss: -21.299606323242188, Learning Rate: 0.01\n",
      "Epoch [16539/20000], Loss: -21.291000366210938, Learning Rate: 0.01\n",
      "Epoch [16540/20000], Loss: -21.253311157226562, Learning Rate: 0.01\n",
      "Epoch [16541/20000], Loss: -21.22540283203125, Learning Rate: 0.01\n",
      "Epoch [16542/20000], Loss: -21.231124877929688, Learning Rate: 0.01\n",
      "Epoch [16543/20000], Loss: -21.262237548828125, Learning Rate: 0.01\n",
      "Epoch [16544/20000], Loss: -21.293243408203125, Learning Rate: 0.01\n",
      "Epoch [16545/20000], Loss: -21.302139282226562, Learning Rate: 0.01\n",
      "Epoch [16546/20000], Loss: -21.288818359375, Learning Rate: 0.01\n",
      "Epoch [16547/20000], Loss: -21.269805908203125, Learning Rate: 0.01\n",
      "Epoch [16548/20000], Loss: -21.26226806640625, Learning Rate: 0.01\n",
      "Epoch [16549/20000], Loss: -21.271591186523438, Learning Rate: 0.01\n",
      "Epoch [16550/20000], Loss: -21.288925170898438, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16551/20000], Loss: -21.301239013671875, Learning Rate: 0.01\n",
      "Epoch [16552/20000], Loss: -21.3013916015625, Learning Rate: 0.01\n",
      "Epoch [16553/20000], Loss: -21.292755126953125, Learning Rate: 0.01\n",
      "Epoch [16554/20000], Loss: -21.28436279296875, Learning Rate: 0.01\n",
      "Epoch [16555/20000], Loss: -21.283050537109375, Learning Rate: 0.01\n",
      "Epoch [16556/20000], Loss: -21.289276123046875, Learning Rate: 0.01\n",
      "Epoch [16557/20000], Loss: -21.297821044921875, Learning Rate: 0.01\n",
      "Epoch [16558/20000], Loss: -21.302902221679688, Learning Rate: 0.01\n",
      "Epoch [16559/20000], Loss: -21.302047729492188, Learning Rate: 0.01\n",
      "Epoch [16560/20000], Loss: -21.297561645507812, Learning Rate: 0.01\n",
      "Epoch [16561/20000], Loss: -21.293838500976562, Learning Rate: 0.01\n",
      "Epoch [16562/20000], Loss: -21.293594360351562, Learning Rate: 0.01\n",
      "Epoch [16563/20000], Loss: -21.296707153320312, Learning Rate: 0.01\n",
      "Epoch [16564/20000], Loss: -21.300765991210938, Learning Rate: 0.01\n",
      "Epoch [16565/20000], Loss: -21.3033447265625, Learning Rate: 0.01\n",
      "Epoch [16566/20000], Loss: -21.303146362304688, Learning Rate: 0.01\n",
      "Epoch [16567/20000], Loss: -21.301239013671875, Learning Rate: 0.01\n",
      "Epoch [16568/20000], Loss: -21.299270629882812, Learning Rate: 0.01\n",
      "Epoch [16569/20000], Loss: -21.298873901367188, Learning Rate: 0.01\n",
      "Epoch [16570/20000], Loss: -21.300201416015625, Learning Rate: 0.01\n",
      "Epoch [16571/20000], Loss: -21.3021240234375, Learning Rate: 0.01\n",
      "Epoch [16572/20000], Loss: -21.303695678710938, Learning Rate: 0.01\n",
      "Epoch [16573/20000], Loss: -21.303970336914062, Learning Rate: 0.01\n",
      "Epoch [16574/20000], Loss: -21.3033447265625, Learning Rate: 0.01\n",
      "Epoch [16575/20000], Loss: -21.302352905273438, Learning Rate: 0.01\n",
      "Epoch [16576/20000], Loss: -21.301803588867188, Learning Rate: 0.01\n",
      "Epoch [16577/20000], Loss: -21.302154541015625, Learning Rate: 0.01\n",
      "Epoch [16578/20000], Loss: -21.303009033203125, Learning Rate: 0.01\n",
      "Epoch [16579/20000], Loss: -21.303939819335938, Learning Rate: 0.01\n",
      "Epoch [16580/20000], Loss: -21.304458618164062, Learning Rate: 0.01\n",
      "Epoch [16581/20000], Loss: -21.30438232421875, Learning Rate: 0.01\n",
      "Epoch [16582/20000], Loss: -21.304031372070312, Learning Rate: 0.01\n",
      "Epoch [16583/20000], Loss: -21.30377197265625, Learning Rate: 0.01\n",
      "Epoch [16584/20000], Loss: -21.303573608398438, Learning Rate: 0.01\n",
      "Epoch [16585/20000], Loss: -21.303802490234375, Learning Rate: 0.01\n",
      "Epoch [16586/20000], Loss: -21.304306030273438, Learning Rate: 0.01\n",
      "Epoch [16587/20000], Loss: -21.304702758789062, Learning Rate: 0.01\n",
      "Epoch [16588/20000], Loss: -21.304931640625, Learning Rate: 0.01\n",
      "Epoch [16589/20000], Loss: -21.304901123046875, Learning Rate: 0.01\n",
      "Epoch [16590/20000], Loss: -21.304855346679688, Learning Rate: 0.01\n",
      "Epoch [16591/20000], Loss: -21.304672241210938, Learning Rate: 0.01\n",
      "Epoch [16592/20000], Loss: -21.304702758789062, Learning Rate: 0.01\n",
      "Epoch [16593/20000], Loss: -21.304794311523438, Learning Rate: 0.01\n",
      "Epoch [16594/20000], Loss: -21.304962158203125, Learning Rate: 0.01\n",
      "Epoch [16595/20000], Loss: -21.305084228515625, Learning Rate: 0.01\n",
      "Epoch [16596/20000], Loss: -21.305343627929688, Learning Rate: 0.01\n",
      "Epoch [16597/20000], Loss: -21.305404663085938, Learning Rate: 0.01\n",
      "Epoch [16598/20000], Loss: -21.305419921875, Learning Rate: 0.01\n",
      "Epoch [16599/20000], Loss: -21.305389404296875, Learning Rate: 0.01\n",
      "Epoch [16600/20000], Loss: -21.305374145507812, Learning Rate: 0.01\n",
      "Epoch [16601/20000], Loss: -21.305374145507812, Learning Rate: 0.01\n",
      "Epoch [16602/20000], Loss: -21.305419921875, Learning Rate: 0.01\n",
      "Epoch [16603/20000], Loss: -21.30560302734375, Learning Rate: 0.01\n",
      "Epoch [16604/20000], Loss: -21.305679321289062, Learning Rate: 0.01\n",
      "Epoch [16605/20000], Loss: -21.3057861328125, Learning Rate: 0.01\n",
      "Epoch [16606/20000], Loss: -21.305892944335938, Learning Rate: 0.01\n",
      "Epoch [16607/20000], Loss: -21.305923461914062, Learning Rate: 0.01\n",
      "Epoch [16608/20000], Loss: -21.305923461914062, Learning Rate: 0.01\n",
      "Epoch [16609/20000], Loss: -21.305999755859375, Learning Rate: 0.01\n",
      "Epoch [16610/20000], Loss: -21.306045532226562, Learning Rate: 0.01\n",
      "Epoch [16611/20000], Loss: -21.30609130859375, Learning Rate: 0.01\n",
      "Epoch [16612/20000], Loss: -21.30615234375, Learning Rate: 0.01\n",
      "Epoch [16613/20000], Loss: -21.306182861328125, Learning Rate: 0.01\n",
      "Epoch [16614/20000], Loss: -21.306304931640625, Learning Rate: 0.01\n",
      "Epoch [16615/20000], Loss: -21.306411743164062, Learning Rate: 0.01\n",
      "Epoch [16616/20000], Loss: -21.306365966796875, Learning Rate: 0.01\n",
      "Epoch [16617/20000], Loss: -21.30645751953125, Learning Rate: 0.01\n",
      "Epoch [16618/20000], Loss: -21.306594848632812, Learning Rate: 0.01\n",
      "Epoch [16619/20000], Loss: -21.306549072265625, Learning Rate: 0.01\n",
      "Epoch [16620/20000], Loss: -21.306640625, Learning Rate: 0.01\n",
      "Epoch [16621/20000], Loss: -21.306686401367188, Learning Rate: 0.01\n",
      "Epoch [16622/20000], Loss: -21.306732177734375, Learning Rate: 0.01\n",
      "Epoch [16623/20000], Loss: -21.306777954101562, Learning Rate: 0.01\n",
      "Epoch [16624/20000], Loss: -21.306854248046875, Learning Rate: 0.01\n",
      "Epoch [16625/20000], Loss: -21.30694580078125, Learning Rate: 0.01\n",
      "Epoch [16626/20000], Loss: -21.307037353515625, Learning Rate: 0.01\n",
      "Epoch [16627/20000], Loss: -21.307037353515625, Learning Rate: 0.01\n",
      "Epoch [16628/20000], Loss: -21.307037353515625, Learning Rate: 0.01\n",
      "Epoch [16629/20000], Loss: -21.307144165039062, Learning Rate: 0.01\n",
      "Epoch [16630/20000], Loss: -21.307205200195312, Learning Rate: 0.01\n",
      "Epoch [16631/20000], Loss: -21.307220458984375, Learning Rate: 0.01\n",
      "Epoch [16632/20000], Loss: -21.307327270507812, Learning Rate: 0.01\n",
      "Epoch [16633/20000], Loss: -21.30731201171875, Learning Rate: 0.01\n",
      "Epoch [16634/20000], Loss: -21.307449340820312, Learning Rate: 0.01\n",
      "Epoch [16635/20000], Loss: -21.307479858398438, Learning Rate: 0.01\n",
      "Epoch [16636/20000], Loss: -21.307449340820312, Learning Rate: 0.01\n",
      "Epoch [16637/20000], Loss: -21.307510375976562, Learning Rate: 0.01\n",
      "Epoch [16638/20000], Loss: -21.307662963867188, Learning Rate: 0.01\n",
      "Epoch [16639/20000], Loss: -21.307662963867188, Learning Rate: 0.01\n",
      "Epoch [16640/20000], Loss: -21.307693481445312, Learning Rate: 0.01\n",
      "Epoch [16641/20000], Loss: -21.307769775390625, Learning Rate: 0.01\n",
      "Epoch [16642/20000], Loss: -21.307846069335938, Learning Rate: 0.01\n",
      "Epoch [16643/20000], Loss: -21.307830810546875, Learning Rate: 0.01\n",
      "Epoch [16644/20000], Loss: -21.307998657226562, Learning Rate: 0.01\n",
      "Epoch [16645/20000], Loss: -21.307998657226562, Learning Rate: 0.01\n",
      "Epoch [16646/20000], Loss: -21.308090209960938, Learning Rate: 0.01\n",
      "Epoch [16647/20000], Loss: -21.308059692382812, Learning Rate: 0.01\n",
      "Epoch [16648/20000], Loss: -21.308135986328125, Learning Rate: 0.01\n",
      "Epoch [16649/20000], Loss: -21.30816650390625, Learning Rate: 0.01\n",
      "Epoch [16650/20000], Loss: -21.30828857421875, Learning Rate: 0.01\n",
      "Epoch [16651/20000], Loss: -21.308273315429688, Learning Rate: 0.01\n",
      "Epoch [16652/20000], Loss: -21.308380126953125, Learning Rate: 0.01\n",
      "Epoch [16653/20000], Loss: -21.308380126953125, Learning Rate: 0.01\n",
      "Epoch [16654/20000], Loss: -21.308395385742188, Learning Rate: 0.01\n",
      "Epoch [16655/20000], Loss: -21.308502197265625, Learning Rate: 0.01\n",
      "Epoch [16656/20000], Loss: -21.30859375, Learning Rate: 0.01\n",
      "Epoch [16657/20000], Loss: -21.308578491210938, Learning Rate: 0.01\n",
      "Epoch [16658/20000], Loss: -21.308609008789062, Learning Rate: 0.01\n",
      "Epoch [16659/20000], Loss: -21.30877685546875, Learning Rate: 0.01\n",
      "Epoch [16660/20000], Loss: -21.308807373046875, Learning Rate: 0.01\n",
      "Epoch [16661/20000], Loss: -21.308853149414062, Learning Rate: 0.01\n",
      "Epoch [16662/20000], Loss: -21.308929443359375, Learning Rate: 0.01\n",
      "Epoch [16663/20000], Loss: -21.308929443359375, Learning Rate: 0.01\n",
      "Epoch [16664/20000], Loss: -21.308975219726562, Learning Rate: 0.01\n",
      "Epoch [16665/20000], Loss: -21.309036254882812, Learning Rate: 0.01\n",
      "Epoch [16666/20000], Loss: -21.309158325195312, Learning Rate: 0.01\n",
      "Epoch [16667/20000], Loss: -21.309127807617188, Learning Rate: 0.01\n",
      "Epoch [16668/20000], Loss: -21.309249877929688, Learning Rate: 0.01\n",
      "Epoch [16669/20000], Loss: -21.309280395507812, Learning Rate: 0.01\n",
      "Epoch [16670/20000], Loss: -21.309219360351562, Learning Rate: 0.01\n",
      "Epoch [16671/20000], Loss: -21.309326171875, Learning Rate: 0.01\n",
      "Epoch [16672/20000], Loss: -21.309417724609375, Learning Rate: 0.01\n",
      "Epoch [16673/20000], Loss: -21.309539794921875, Learning Rate: 0.01\n",
      "Epoch [16674/20000], Loss: -21.3094482421875, Learning Rate: 0.01\n",
      "Epoch [16675/20000], Loss: -21.309524536132812, Learning Rate: 0.01\n",
      "Epoch [16676/20000], Loss: -21.3095703125, Learning Rate: 0.01\n",
      "Epoch [16677/20000], Loss: -21.30963134765625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16678/20000], Loss: -21.309585571289062, Learning Rate: 0.01\n",
      "Epoch [16679/20000], Loss: -21.309722900390625, Learning Rate: 0.01\n",
      "Epoch [16680/20000], Loss: -21.309677124023438, Learning Rate: 0.01\n",
      "Epoch [16681/20000], Loss: -21.309707641601562, Learning Rate: 0.01\n",
      "Epoch [16682/20000], Loss: -21.30963134765625, Learning Rate: 0.01\n",
      "Epoch [16683/20000], Loss: -21.309555053710938, Learning Rate: 0.01\n",
      "Epoch [16684/20000], Loss: -21.309432983398438, Learning Rate: 0.01\n",
      "Epoch [16685/20000], Loss: -21.309371948242188, Learning Rate: 0.01\n",
      "Epoch [16686/20000], Loss: -21.30889892578125, Learning Rate: 0.01\n",
      "Epoch [16687/20000], Loss: -21.308319091796875, Learning Rate: 0.01\n",
      "Epoch [16688/20000], Loss: -21.3074951171875, Learning Rate: 0.01\n",
      "Epoch [16689/20000], Loss: -21.306060791015625, Learning Rate: 0.01\n",
      "Epoch [16690/20000], Loss: -21.303878784179688, Learning Rate: 0.01\n",
      "Epoch [16691/20000], Loss: -21.300506591796875, Learning Rate: 0.01\n",
      "Epoch [16692/20000], Loss: -21.295059204101562, Learning Rate: 0.01\n",
      "Epoch [16693/20000], Loss: -21.286468505859375, Learning Rate: 0.01\n",
      "Epoch [16694/20000], Loss: -21.272750854492188, Learning Rate: 0.01\n",
      "Epoch [16695/20000], Loss: -21.250503540039062, Learning Rate: 0.01\n",
      "Epoch [16696/20000], Loss: -21.215286254882812, Learning Rate: 0.01\n",
      "Epoch [16697/20000], Loss: -21.158279418945312, Learning Rate: 0.01\n",
      "Epoch [16698/20000], Loss: -21.068130493164062, Learning Rate: 0.01\n",
      "Epoch [16699/20000], Loss: -20.92498779296875, Learning Rate: 0.01\n",
      "Epoch [16700/20000], Loss: -20.709259033203125, Learning Rate: 0.01\n",
      "Epoch [16701/20000], Loss: -20.388046264648438, Learning Rate: 0.01\n",
      "Epoch [16702/20000], Loss: -19.970840454101562, Learning Rate: 0.01\n",
      "Epoch [16703/20000], Loss: -19.475082397460938, Learning Rate: 0.01\n",
      "Epoch [16704/20000], Loss: -19.1004638671875, Learning Rate: 0.01\n",
      "Epoch [16705/20000], Loss: -19.022125244140625, Learning Rate: 0.01\n",
      "Epoch [16706/20000], Loss: -19.506103515625, Learning Rate: 0.01\n",
      "Epoch [16707/20000], Loss: -20.338104248046875, Learning Rate: 0.01\n",
      "Epoch [16708/20000], Loss: -21.085845947265625, Learning Rate: 0.01\n",
      "Epoch [16709/20000], Loss: -21.303421020507812, Learning Rate: 0.01\n",
      "Epoch [16710/20000], Loss: -20.991592407226562, Learning Rate: 0.01\n",
      "Epoch [16711/20000], Loss: -20.537490844726562, Learning Rate: 0.01\n",
      "Epoch [16712/20000], Loss: -20.36212158203125, Learning Rate: 0.01\n",
      "Epoch [16713/20000], Loss: -20.631454467773438, Learning Rate: 0.01\n",
      "Epoch [16714/20000], Loss: -21.062850952148438, Learning Rate: 0.01\n",
      "Epoch [16715/20000], Loss: -21.301971435546875, Learning Rate: 0.01\n",
      "Epoch [16716/20000], Loss: -21.235015869140625, Learning Rate: 0.01\n",
      "Epoch [16717/20000], Loss: -20.96099853515625, Learning Rate: 0.01\n",
      "Epoch [16718/20000], Loss: -20.597457885742188, Learning Rate: 0.01\n",
      "Epoch [16719/20000], Loss: -20.477310180664062, Learning Rate: 0.01\n",
      "Epoch [16720/20000], Loss: -20.792678833007812, Learning Rate: 0.01\n",
      "Epoch [16721/20000], Loss: -21.17474365234375, Learning Rate: 0.01\n",
      "Epoch [16722/20000], Loss: -21.301223754882812, Learning Rate: 0.01\n",
      "Epoch [16723/20000], Loss: -21.1197509765625, Learning Rate: 0.01\n",
      "Epoch [16724/20000], Loss: -20.814697265625, Learning Rate: 0.01\n",
      "Epoch [16725/20000], Loss: -20.571762084960938, Learning Rate: 0.01\n",
      "Epoch [16726/20000], Loss: -20.634841918945312, Learning Rate: 0.01\n",
      "Epoch [16727/20000], Loss: -20.976776123046875, Learning Rate: 0.01\n",
      "Epoch [16728/20000], Loss: -21.271530151367188, Learning Rate: 0.01\n",
      "Epoch [16729/20000], Loss: -21.259613037109375, Learning Rate: 0.01\n",
      "Epoch [16730/20000], Loss: -21.046630859375, Learning Rate: 0.01\n",
      "Epoch [16731/20000], Loss: -20.94488525390625, Learning Rate: 0.01\n",
      "Epoch [16732/20000], Loss: -21.085250854492188, Learning Rate: 0.01\n",
      "Epoch [16733/20000], Loss: -21.27093505859375, Learning Rate: 0.01\n",
      "Epoch [16734/20000], Loss: -21.297149658203125, Learning Rate: 0.01\n",
      "Epoch [16735/20000], Loss: -21.178543090820312, Learning Rate: 0.01\n",
      "Epoch [16736/20000], Loss: -21.086013793945312, Learning Rate: 0.01\n",
      "Epoch [16737/20000], Loss: -21.125762939453125, Learning Rate: 0.01\n",
      "Epoch [16738/20000], Loss: -21.242050170898438, Learning Rate: 0.01\n",
      "Epoch [16739/20000], Loss: -21.30645751953125, Learning Rate: 0.01\n",
      "Epoch [16740/20000], Loss: -21.270462036132812, Learning Rate: 0.01\n",
      "Epoch [16741/20000], Loss: -21.2022705078125, Learning Rate: 0.01\n",
      "Epoch [16742/20000], Loss: -21.192153930664062, Learning Rate: 0.01\n",
      "Epoch [16743/20000], Loss: -21.248733520507812, Learning Rate: 0.01\n",
      "Epoch [16744/20000], Loss: -21.303848266601562, Learning Rate: 0.01\n",
      "Epoch [16745/20000], Loss: -21.30120849609375, Learning Rate: 0.01\n",
      "Epoch [16746/20000], Loss: -21.26031494140625, Learning Rate: 0.01\n",
      "Epoch [16747/20000], Loss: -21.241378784179688, Learning Rate: 0.01\n",
      "Epoch [16748/20000], Loss: -21.266708374023438, Learning Rate: 0.01\n",
      "Epoch [16749/20000], Loss: -21.30328369140625, Learning Rate: 0.01\n",
      "Epoch [16750/20000], Loss: -21.309478759765625, Learning Rate: 0.01\n",
      "Epoch [16751/20000], Loss: -21.287750244140625, Learning Rate: 0.01\n",
      "Epoch [16752/20000], Loss: -21.272506713867188, Learning Rate: 0.01\n",
      "Epoch [16753/20000], Loss: -21.283233642578125, Learning Rate: 0.01\n",
      "Epoch [16754/20000], Loss: -21.304550170898438, Learning Rate: 0.01\n",
      "Epoch [16755/20000], Loss: -21.310867309570312, Learning Rate: 0.01\n",
      "Epoch [16756/20000], Loss: -21.299484252929688, Learning Rate: 0.01\n",
      "Epoch [16757/20000], Loss: -21.289337158203125, Learning Rate: 0.01\n",
      "Epoch [16758/20000], Loss: -21.293746948242188, Learning Rate: 0.01\n",
      "Epoch [16759/20000], Loss: -21.306488037109375, Learning Rate: 0.01\n",
      "Epoch [16760/20000], Loss: -21.312530517578125, Learning Rate: 0.01\n",
      "Epoch [16761/20000], Loss: -21.30718994140625, Learning Rate: 0.01\n",
      "Epoch [16762/20000], Loss: -21.299636840820312, Learning Rate: 0.01\n",
      "Epoch [16763/20000], Loss: -21.2999267578125, Learning Rate: 0.01\n",
      "Epoch [16764/20000], Loss: -21.306884765625, Learning Rate: 0.01\n",
      "Epoch [16765/20000], Loss: -21.312286376953125, Learning Rate: 0.01\n",
      "Epoch [16766/20000], Loss: -21.310943603515625, Learning Rate: 0.01\n",
      "Epoch [16767/20000], Loss: -21.306488037109375, Learning Rate: 0.01\n",
      "Epoch [16768/20000], Loss: -21.305160522460938, Learning Rate: 0.01\n",
      "Epoch [16769/20000], Loss: -21.308319091796875, Learning Rate: 0.01\n",
      "Epoch [16770/20000], Loss: -21.3121337890625, Learning Rate: 0.01\n",
      "Epoch [16771/20000], Loss: -21.3126220703125, Learning Rate: 0.01\n",
      "Epoch [16772/20000], Loss: -21.310287475585938, Learning Rate: 0.01\n",
      "Epoch [16773/20000], Loss: -21.308441162109375, Learning Rate: 0.01\n",
      "Epoch [16774/20000], Loss: -21.30938720703125, Learning Rate: 0.01\n",
      "Epoch [16775/20000], Loss: -21.311904907226562, Learning Rate: 0.01\n",
      "Epoch [16776/20000], Loss: -21.31329345703125, Learning Rate: 0.01\n",
      "Epoch [16777/20000], Loss: -21.312637329101562, Learning Rate: 0.01\n",
      "Epoch [16778/20000], Loss: -21.311264038085938, Learning Rate: 0.01\n",
      "Epoch [16779/20000], Loss: -21.31097412109375, Learning Rate: 0.01\n",
      "Epoch [16780/20000], Loss: -21.312042236328125, Learning Rate: 0.01\n",
      "Epoch [16781/20000], Loss: -21.31329345703125, Learning Rate: 0.01\n",
      "Epoch [16782/20000], Loss: -21.313446044921875, Learning Rate: 0.01\n",
      "Epoch [16783/20000], Loss: -21.312744140625, Learning Rate: 0.01\n",
      "Epoch [16784/20000], Loss: -21.312210083007812, Learning Rate: 0.01\n",
      "Epoch [16785/20000], Loss: -21.312469482421875, Learning Rate: 0.01\n",
      "Epoch [16786/20000], Loss: -21.3133544921875, Learning Rate: 0.01\n",
      "Epoch [16787/20000], Loss: -21.31378173828125, Learning Rate: 0.01\n",
      "Epoch [16788/20000], Loss: -21.313735961914062, Learning Rate: 0.01\n",
      "Epoch [16789/20000], Loss: -21.313278198242188, Learning Rate: 0.01\n",
      "Epoch [16790/20000], Loss: -21.313232421875, Learning Rate: 0.01\n",
      "Epoch [16791/20000], Loss: -21.313430786132812, Learning Rate: 0.01\n",
      "Epoch [16792/20000], Loss: -21.313873291015625, Learning Rate: 0.01\n",
      "Epoch [16793/20000], Loss: -21.314132690429688, Learning Rate: 0.01\n",
      "Epoch [16794/20000], Loss: -21.313980102539062, Learning Rate: 0.01\n",
      "Epoch [16795/20000], Loss: -21.313766479492188, Learning Rate: 0.01\n",
      "Epoch [16796/20000], Loss: -21.313796997070312, Learning Rate: 0.01\n",
      "Epoch [16797/20000], Loss: -21.313980102539062, Learning Rate: 0.01\n",
      "Epoch [16798/20000], Loss: -21.314346313476562, Learning Rate: 0.01\n",
      "Epoch [16799/20000], Loss: -21.314407348632812, Learning Rate: 0.01\n",
      "Epoch [16800/20000], Loss: -21.314361572265625, Learning Rate: 0.01\n",
      "Epoch [16801/20000], Loss: -21.314254760742188, Learning Rate: 0.01\n",
      "Epoch [16802/20000], Loss: -21.314300537109375, Learning Rate: 0.01\n",
      "Epoch [16803/20000], Loss: -21.314422607421875, Learning Rate: 0.01\n",
      "Epoch [16804/20000], Loss: -21.314666748046875, Learning Rate: 0.01\n",
      "Epoch [16805/20000], Loss: -21.314712524414062, Learning Rate: 0.01\n",
      "Epoch [16806/20000], Loss: -21.314590454101562, Learning Rate: 0.01\n",
      "Epoch [16807/20000], Loss: -21.31463623046875, Learning Rate: 0.01\n",
      "Epoch [16808/20000], Loss: -21.314682006835938, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16809/20000], Loss: -21.314804077148438, Learning Rate: 0.01\n",
      "Epoch [16810/20000], Loss: -21.314865112304688, Learning Rate: 0.01\n",
      "Epoch [16811/20000], Loss: -21.314971923828125, Learning Rate: 0.01\n",
      "Epoch [16812/20000], Loss: -21.31488037109375, Learning Rate: 0.01\n",
      "Epoch [16813/20000], Loss: -21.314895629882812, Learning Rate: 0.01\n",
      "Epoch [16814/20000], Loss: -21.31494140625, Learning Rate: 0.01\n",
      "Epoch [16815/20000], Loss: -21.31500244140625, Learning Rate: 0.01\n",
      "Epoch [16816/20000], Loss: -21.315170288085938, Learning Rate: 0.01\n",
      "Epoch [16817/20000], Loss: -21.315216064453125, Learning Rate: 0.01\n",
      "Epoch [16818/20000], Loss: -21.315231323242188, Learning Rate: 0.01\n",
      "Epoch [16819/20000], Loss: -21.31524658203125, Learning Rate: 0.01\n",
      "Epoch [16820/20000], Loss: -21.315261840820312, Learning Rate: 0.01\n",
      "Epoch [16821/20000], Loss: -21.31536865234375, Learning Rate: 0.01\n",
      "Epoch [16822/20000], Loss: -21.315444946289062, Learning Rate: 0.01\n",
      "Epoch [16823/20000], Loss: -21.315460205078125, Learning Rate: 0.01\n",
      "Epoch [16824/20000], Loss: -21.315460205078125, Learning Rate: 0.01\n",
      "Epoch [16825/20000], Loss: -21.315567016601562, Learning Rate: 0.01\n",
      "Epoch [16826/20000], Loss: -21.315582275390625, Learning Rate: 0.01\n",
      "Epoch [16827/20000], Loss: -21.315628051757812, Learning Rate: 0.01\n",
      "Epoch [16828/20000], Loss: -21.31573486328125, Learning Rate: 0.01\n",
      "Epoch [16829/20000], Loss: -21.315750122070312, Learning Rate: 0.01\n",
      "Epoch [16830/20000], Loss: -21.3157958984375, Learning Rate: 0.01\n",
      "Epoch [16831/20000], Loss: -21.3157958984375, Learning Rate: 0.01\n",
      "Epoch [16832/20000], Loss: -21.315902709960938, Learning Rate: 0.01\n",
      "Epoch [16833/20000], Loss: -21.315841674804688, Learning Rate: 0.01\n",
      "Epoch [16834/20000], Loss: -21.31591796875, Learning Rate: 0.01\n",
      "Epoch [16835/20000], Loss: -21.316055297851562, Learning Rate: 0.01\n",
      "Epoch [16836/20000], Loss: -21.315963745117188, Learning Rate: 0.01\n",
      "Epoch [16837/20000], Loss: -21.316024780273438, Learning Rate: 0.01\n",
      "Epoch [16838/20000], Loss: -21.31610107421875, Learning Rate: 0.01\n",
      "Epoch [16839/20000], Loss: -21.316085815429688, Learning Rate: 0.01\n",
      "Epoch [16840/20000], Loss: -21.316253662109375, Learning Rate: 0.01\n",
      "Epoch [16841/20000], Loss: -21.316253662109375, Learning Rate: 0.01\n",
      "Epoch [16842/20000], Loss: -21.3162841796875, Learning Rate: 0.01\n",
      "Epoch [16843/20000], Loss: -21.3162841796875, Learning Rate: 0.01\n",
      "Epoch [16844/20000], Loss: -21.316329956054688, Learning Rate: 0.01\n",
      "Epoch [16845/20000], Loss: -21.31640625, Learning Rate: 0.01\n",
      "Epoch [16846/20000], Loss: -21.31646728515625, Learning Rate: 0.01\n",
      "Epoch [16847/20000], Loss: -21.316497802734375, Learning Rate: 0.01\n",
      "Epoch [16848/20000], Loss: -21.316543579101562, Learning Rate: 0.01\n",
      "Epoch [16849/20000], Loss: -21.31658935546875, Learning Rate: 0.01\n",
      "Epoch [16850/20000], Loss: -21.316543579101562, Learning Rate: 0.01\n",
      "Epoch [16851/20000], Loss: -21.31658935546875, Learning Rate: 0.01\n",
      "Epoch [16852/20000], Loss: -21.316619873046875, Learning Rate: 0.01\n",
      "Epoch [16853/20000], Loss: -21.316635131835938, Learning Rate: 0.01\n",
      "Epoch [16854/20000], Loss: -21.316665649414062, Learning Rate: 0.01\n",
      "Epoch [16855/20000], Loss: -21.316543579101562, Learning Rate: 0.01\n",
      "Epoch [16856/20000], Loss: -21.316513061523438, Learning Rate: 0.01\n",
      "Epoch [16857/20000], Loss: -21.31634521484375, Learning Rate: 0.01\n",
      "Epoch [16858/20000], Loss: -21.316085815429688, Learning Rate: 0.01\n",
      "Epoch [16859/20000], Loss: -21.315689086914062, Learning Rate: 0.01\n",
      "Epoch [16860/20000], Loss: -21.31512451171875, Learning Rate: 0.01\n",
      "Epoch [16861/20000], Loss: -21.314132690429688, Learning Rate: 0.01\n",
      "Epoch [16862/20000], Loss: -21.312698364257812, Learning Rate: 0.01\n",
      "Epoch [16863/20000], Loss: -21.3104248046875, Learning Rate: 0.01\n",
      "Epoch [16864/20000], Loss: -21.306732177734375, Learning Rate: 0.01\n",
      "Epoch [16865/20000], Loss: -21.301162719726562, Learning Rate: 0.01\n",
      "Epoch [16866/20000], Loss: -21.292221069335938, Learning Rate: 0.01\n",
      "Epoch [16867/20000], Loss: -21.278106689453125, Learning Rate: 0.01\n",
      "Epoch [16868/20000], Loss: -21.255935668945312, Learning Rate: 0.01\n",
      "Epoch [16869/20000], Loss: -21.220733642578125, Learning Rate: 0.01\n",
      "Epoch [16870/20000], Loss: -21.16619873046875, Learning Rate: 0.01\n",
      "Epoch [16871/20000], Loss: -21.082138061523438, Learning Rate: 0.01\n",
      "Epoch [16872/20000], Loss: -20.958328247070312, Learning Rate: 0.01\n",
      "Epoch [16873/20000], Loss: -20.783172607421875, Learning Rate: 0.01\n",
      "Epoch [16874/20000], Loss: -20.564926147460938, Learning Rate: 0.01\n",
      "Epoch [16875/20000], Loss: -20.331878662109375, Learning Rate: 0.01\n",
      "Epoch [16876/20000], Loss: -20.180892944335938, Learning Rate: 0.01\n",
      "Epoch [16877/20000], Loss: -20.21002197265625, Learning Rate: 0.01\n",
      "Epoch [16878/20000], Loss: -20.492431640625, Learning Rate: 0.01\n",
      "Epoch [16879/20000], Loss: -20.908401489257812, Learning Rate: 0.01\n",
      "Epoch [16880/20000], Loss: -21.238143920898438, Learning Rate: 0.01\n",
      "Epoch [16881/20000], Loss: -21.308868408203125, Learning Rate: 0.01\n",
      "Epoch [16882/20000], Loss: -21.147415161132812, Learning Rate: 0.01\n",
      "Epoch [16883/20000], Loss: -20.934295654296875, Learning Rate: 0.01\n",
      "Epoch [16884/20000], Loss: -20.855392456054688, Learning Rate: 0.01\n",
      "Epoch [16885/20000], Loss: -20.975875854492188, Learning Rate: 0.01\n",
      "Epoch [16886/20000], Loss: -21.18426513671875, Learning Rate: 0.01\n",
      "Epoch [16887/20000], Loss: -21.311111450195312, Learning Rate: 0.01\n",
      "Epoch [16888/20000], Loss: -21.279129028320312, Learning Rate: 0.01\n",
      "Epoch [16889/20000], Loss: -21.16015625, Learning Rate: 0.01\n",
      "Epoch [16890/20000], Loss: -21.087234497070312, Learning Rate: 0.01\n",
      "Epoch [16891/20000], Loss: -21.128189086914062, Learning Rate: 0.01\n",
      "Epoch [16892/20000], Loss: -21.237823486328125, Learning Rate: 0.01\n",
      "Epoch [16893/20000], Loss: -21.312637329101562, Learning Rate: 0.01\n",
      "Epoch [16894/20000], Loss: -21.29949951171875, Learning Rate: 0.01\n",
      "Epoch [16895/20000], Loss: -21.235031127929688, Learning Rate: 0.01\n",
      "Epoch [16896/20000], Loss: -21.195358276367188, Learning Rate: 0.01\n",
      "Epoch [16897/20000], Loss: -21.219772338867188, Learning Rate: 0.01\n",
      "Epoch [16898/20000], Loss: -21.2791748046875, Learning Rate: 0.01\n",
      "Epoch [16899/20000], Loss: -21.316726684570312, Learning Rate: 0.01\n",
      "Epoch [16900/20000], Loss: -21.305908203125, Learning Rate: 0.01\n",
      "Epoch [16901/20000], Loss: -21.270339965820312, Learning Rate: 0.01\n",
      "Epoch [16902/20000], Loss: -21.251876831054688, Learning Rate: 0.01\n",
      "Epoch [16903/20000], Loss: -21.268020629882812, Learning Rate: 0.01\n",
      "Epoch [16904/20000], Loss: -21.300369262695312, Learning Rate: 0.01\n",
      "Epoch [16905/20000], Loss: -21.318313598632812, Learning Rate: 0.01\n",
      "Epoch [16906/20000], Loss: -21.310256958007812, Learning Rate: 0.01\n",
      "Epoch [16907/20000], Loss: -21.291000366210938, Learning Rate: 0.01\n",
      "Epoch [16908/20000], Loss: -21.282638549804688, Learning Rate: 0.01\n",
      "Epoch [16909/20000], Loss: -21.292617797851562, Learning Rate: 0.01\n",
      "Epoch [16910/20000], Loss: -21.3099365234375, Learning Rate: 0.01\n",
      "Epoch [16911/20000], Loss: -21.318801879882812, Learning Rate: 0.01\n",
      "Epoch [16912/20000], Loss: -21.313812255859375, Learning Rate: 0.01\n",
      "Epoch [16913/20000], Loss: -21.303558349609375, Learning Rate: 0.01\n",
      "Epoch [16914/20000], Loss: -21.299407958984375, Learning Rate: 0.01\n",
      "Epoch [16915/20000], Loss: -21.304840087890625, Learning Rate: 0.01\n",
      "Epoch [16916/20000], Loss: -21.314178466796875, Learning Rate: 0.01\n",
      "Epoch [16917/20000], Loss: -21.318984985351562, Learning Rate: 0.01\n",
      "Epoch [16918/20000], Loss: -21.316696166992188, Learning Rate: 0.01\n",
      "Epoch [16919/20000], Loss: -21.31121826171875, Learning Rate: 0.01\n",
      "Epoch [16920/20000], Loss: -21.308639526367188, Learning Rate: 0.01\n",
      "Epoch [16921/20000], Loss: -21.311187744140625, Learning Rate: 0.01\n",
      "Epoch [16922/20000], Loss: -21.31610107421875, Learning Rate: 0.01\n",
      "Epoch [16923/20000], Loss: -21.319168090820312, Learning Rate: 0.01\n",
      "Epoch [16924/20000], Loss: -21.318450927734375, Learning Rate: 0.01\n",
      "Epoch [16925/20000], Loss: -21.315658569335938, Learning Rate: 0.01\n",
      "Epoch [16926/20000], Loss: -21.313873291015625, Learning Rate: 0.01\n",
      "Epoch [16927/20000], Loss: -21.314682006835938, Learning Rate: 0.01\n",
      "Epoch [16928/20000], Loss: -21.317138671875, Learning Rate: 0.01\n",
      "Epoch [16929/20000], Loss: -21.319244384765625, Learning Rate: 0.01\n",
      "Epoch [16930/20000], Loss: -21.319351196289062, Learning Rate: 0.01\n",
      "Epoch [16931/20000], Loss: -21.318115234375, Learning Rate: 0.01\n",
      "Epoch [16932/20000], Loss: -21.316879272460938, Learning Rate: 0.01\n",
      "Epoch [16933/20000], Loss: -21.316864013671875, Learning Rate: 0.01\n",
      "Epoch [16934/20000], Loss: -21.317977905273438, Learning Rate: 0.01\n",
      "Epoch [16935/20000], Loss: -21.319290161132812, Learning Rate: 0.01\n",
      "Epoch [16936/20000], Loss: -21.319778442382812, Learning Rate: 0.01\n",
      "Epoch [16937/20000], Loss: -21.319549560546875, Learning Rate: 0.01\n",
      "Epoch [16938/20000], Loss: -21.318710327148438, Learning Rate: 0.01\n",
      "Epoch [16939/20000], Loss: -21.318389892578125, Learning Rate: 0.01\n",
      "Epoch [16940/20000], Loss: -21.318710327148438, Learning Rate: 0.01\n",
      "Epoch [16941/20000], Loss: -21.319351196289062, Learning Rate: 0.01\n",
      "Epoch [16942/20000], Loss: -21.319915771484375, Learning Rate: 0.01\n",
      "Epoch [16943/20000], Loss: -21.320068359375, Learning Rate: 0.01\n",
      "Epoch [16944/20000], Loss: -21.31976318359375, Learning Rate: 0.01\n",
      "Epoch [16945/20000], Loss: -21.319503784179688, Learning Rate: 0.01\n",
      "Epoch [16946/20000], Loss: -21.319366455078125, Learning Rate: 0.01\n",
      "Epoch [16947/20000], Loss: -21.319610595703125, Learning Rate: 0.01\n",
      "Epoch [16948/20000], Loss: -21.320068359375, Learning Rate: 0.01\n",
      "Epoch [16949/20000], Loss: -21.320343017578125, Learning Rate: 0.01\n",
      "Epoch [16950/20000], Loss: -21.320297241210938, Learning Rate: 0.01\n",
      "Epoch [16951/20000], Loss: -21.320144653320312, Learning Rate: 0.01\n",
      "Epoch [16952/20000], Loss: -21.320068359375, Learning Rate: 0.01\n",
      "Epoch [16953/20000], Loss: -21.32000732421875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16954/20000], Loss: -21.3201904296875, Learning Rate: 0.01\n",
      "Epoch [16955/20000], Loss: -21.320358276367188, Learning Rate: 0.01\n",
      "Epoch [16956/20000], Loss: -21.320526123046875, Learning Rate: 0.01\n",
      "Epoch [16957/20000], Loss: -21.320602416992188, Learning Rate: 0.01\n",
      "Epoch [16958/20000], Loss: -21.320587158203125, Learning Rate: 0.01\n",
      "Epoch [16959/20000], Loss: -21.320526123046875, Learning Rate: 0.01\n",
      "Epoch [16960/20000], Loss: -21.320510864257812, Learning Rate: 0.01\n",
      "Epoch [16961/20000], Loss: -21.320556640625, Learning Rate: 0.01\n",
      "Epoch [16962/20000], Loss: -21.3206787109375, Learning Rate: 0.01\n",
      "Epoch [16963/20000], Loss: -21.32080078125, Learning Rate: 0.01\n",
      "Epoch [16964/20000], Loss: -21.320816040039062, Learning Rate: 0.01\n",
      "Epoch [16965/20000], Loss: -21.320907592773438, Learning Rate: 0.01\n",
      "Epoch [16966/20000], Loss: -21.320816040039062, Learning Rate: 0.01\n",
      "Epoch [16967/20000], Loss: -21.320877075195312, Learning Rate: 0.01\n",
      "Epoch [16968/20000], Loss: -21.3209228515625, Learning Rate: 0.01\n",
      "Epoch [16969/20000], Loss: -21.320938110351562, Learning Rate: 0.01\n",
      "Epoch [16970/20000], Loss: -21.321014404296875, Learning Rate: 0.01\n",
      "Epoch [16971/20000], Loss: -21.321090698242188, Learning Rate: 0.01\n",
      "Epoch [16972/20000], Loss: -21.321075439453125, Learning Rate: 0.01\n",
      "Epoch [16973/20000], Loss: -21.3211669921875, Learning Rate: 0.01\n",
      "Epoch [16974/20000], Loss: -21.32122802734375, Learning Rate: 0.01\n",
      "Epoch [16975/20000], Loss: -21.32122802734375, Learning Rate: 0.01\n",
      "Epoch [16976/20000], Loss: -21.32122802734375, Learning Rate: 0.01\n",
      "Epoch [16977/20000], Loss: -21.3212890625, Learning Rate: 0.01\n",
      "Epoch [16978/20000], Loss: -21.32135009765625, Learning Rate: 0.01\n",
      "Epoch [16979/20000], Loss: -21.321395874023438, Learning Rate: 0.01\n",
      "Epoch [16980/20000], Loss: -21.32147216796875, Learning Rate: 0.01\n",
      "Epoch [16981/20000], Loss: -21.321441650390625, Learning Rate: 0.01\n",
      "Epoch [16982/20000], Loss: -21.321456909179688, Learning Rate: 0.01\n",
      "Epoch [16983/20000], Loss: -21.321456909179688, Learning Rate: 0.01\n",
      "Epoch [16984/20000], Loss: -21.321548461914062, Learning Rate: 0.01\n",
      "Epoch [16985/20000], Loss: -21.321685791015625, Learning Rate: 0.01\n",
      "Epoch [16986/20000], Loss: -21.321670532226562, Learning Rate: 0.01\n",
      "Epoch [16987/20000], Loss: -21.3216552734375, Learning Rate: 0.01\n",
      "Epoch [16988/20000], Loss: -21.321670532226562, Learning Rate: 0.01\n",
      "Epoch [16989/20000], Loss: -21.3216552734375, Learning Rate: 0.01\n",
      "Epoch [16990/20000], Loss: -21.321792602539062, Learning Rate: 0.01\n",
      "Epoch [16991/20000], Loss: -21.321746826171875, Learning Rate: 0.01\n",
      "Epoch [16992/20000], Loss: -21.321792602539062, Learning Rate: 0.01\n",
      "Epoch [16993/20000], Loss: -21.32177734375, Learning Rate: 0.01\n",
      "Epoch [16994/20000], Loss: -21.321807861328125, Learning Rate: 0.01\n",
      "Epoch [16995/20000], Loss: -21.321884155273438, Learning Rate: 0.01\n",
      "Epoch [16996/20000], Loss: -21.321884155273438, Learning Rate: 0.01\n",
      "Epoch [16997/20000], Loss: -21.321914672851562, Learning Rate: 0.01\n",
      "Epoch [16998/20000], Loss: -21.321914672851562, Learning Rate: 0.01\n",
      "Epoch [16999/20000], Loss: -21.321853637695312, Learning Rate: 0.01\n",
      "Epoch [17000/20000], Loss: -21.32183837890625, Learning Rate: 0.01\n",
      "Epoch [17001/20000], Loss: -21.321670532226562, Learning Rate: 0.01\n",
      "Epoch [17002/20000], Loss: -21.321548461914062, Learning Rate: 0.01\n",
      "Epoch [17003/20000], Loss: -21.321273803710938, Learning Rate: 0.01\n",
      "Epoch [17004/20000], Loss: -21.320770263671875, Learning Rate: 0.01\n",
      "Epoch [17005/20000], Loss: -21.32000732421875, Learning Rate: 0.01\n",
      "Epoch [17006/20000], Loss: -21.318954467773438, Learning Rate: 0.01\n",
      "Epoch [17007/20000], Loss: -21.317047119140625, Learning Rate: 0.01\n",
      "Epoch [17008/20000], Loss: -21.314163208007812, Learning Rate: 0.01\n",
      "Epoch [17009/20000], Loss: -21.309478759765625, Learning Rate: 0.01\n",
      "Epoch [17010/20000], Loss: -21.30206298828125, Learning Rate: 0.01\n",
      "Epoch [17011/20000], Loss: -21.290283203125, Learning Rate: 0.01\n",
      "Epoch [17012/20000], Loss: -21.271011352539062, Learning Rate: 0.01\n",
      "Epoch [17013/20000], Loss: -21.24017333984375, Learning Rate: 0.01\n",
      "Epoch [17014/20000], Loss: -21.189727783203125, Learning Rate: 0.01\n",
      "Epoch [17015/20000], Loss: -21.109329223632812, Learning Rate: 0.01\n",
      "Epoch [17016/20000], Loss: -20.979476928710938, Learning Rate: 0.01\n",
      "Epoch [17017/20000], Loss: -20.780258178710938, Learning Rate: 0.01\n",
      "Epoch [17018/20000], Loss: -20.475296020507812, Learning Rate: 0.01\n",
      "Epoch [17019/20000], Loss: -20.063644409179688, Learning Rate: 0.01\n",
      "Epoch [17020/20000], Loss: -19.542205810546875, Learning Rate: 0.01\n",
      "Epoch [17021/20000], Loss: -19.09136962890625, Learning Rate: 0.01\n",
      "Epoch [17022/20000], Loss: -18.882354736328125, Learning Rate: 0.01\n",
      "Epoch [17023/20000], Loss: -19.261428833007812, Learning Rate: 0.01\n",
      "Epoch [17024/20000], Loss: -20.087203979492188, Learning Rate: 0.01\n",
      "Epoch [17025/20000], Loss: -20.959060668945312, Learning Rate: 0.01\n",
      "Epoch [17026/20000], Loss: -21.319076538085938, Learning Rate: 0.01\n",
      "Epoch [17027/20000], Loss: -21.063827514648438, Learning Rate: 0.01\n",
      "Epoch [17028/20000], Loss: -20.566421508789062, Learning Rate: 0.01\n",
      "Epoch [17029/20000], Loss: -20.314697265625, Learning Rate: 0.01\n",
      "Epoch [17030/20000], Loss: -20.555160522460938, Learning Rate: 0.01\n",
      "Epoch [17031/20000], Loss: -21.029129028320312, Learning Rate: 0.01\n",
      "Epoch [17032/20000], Loss: -21.30987548828125, Learning Rate: 0.01\n",
      "Epoch [17033/20000], Loss: -21.204132080078125, Learning Rate: 0.01\n",
      "Epoch [17034/20000], Loss: -20.921051025390625, Learning Rate: 0.01\n",
      "Epoch [17035/20000], Loss: -20.806411743164062, Learning Rate: 0.01\n",
      "Epoch [17036/20000], Loss: -20.968856811523438, Learning Rate: 0.01\n",
      "Epoch [17037/20000], Loss: -21.226486206054688, Learning Rate: 0.01\n",
      "Epoch [17038/20000], Loss: -21.317947387695312, Learning Rate: 0.01\n",
      "Epoch [17039/20000], Loss: -21.201934814453125, Learning Rate: 0.01\n",
      "Epoch [17040/20000], Loss: -21.058609008789062, Learning Rate: 0.01\n",
      "Epoch [17041/20000], Loss: -21.059249877929688, Learning Rate: 0.01\n",
      "Epoch [17042/20000], Loss: -21.195892333984375, Learning Rate: 0.01\n",
      "Epoch [17043/20000], Loss: -21.310943603515625, Learning Rate: 0.01\n",
      "Epoch [17044/20000], Loss: -21.2994384765625, Learning Rate: 0.01\n",
      "Epoch [17045/20000], Loss: -21.209213256835938, Learning Rate: 0.01\n",
      "Epoch [17046/20000], Loss: -21.161056518554688, Learning Rate: 0.01\n",
      "Epoch [17047/20000], Loss: -21.209121704101562, Learning Rate: 0.01\n",
      "Epoch [17048/20000], Loss: -21.291732788085938, Learning Rate: 0.01\n",
      "Epoch [17049/20000], Loss: -21.322265625, Learning Rate: 0.01\n",
      "Epoch [17050/20000], Loss: -21.284683227539062, Learning Rate: 0.01\n",
      "Epoch [17051/20000], Loss: -21.238174438476562, Learning Rate: 0.01\n",
      "Epoch [17052/20000], Loss: -21.239761352539062, Learning Rate: 0.01\n",
      "Epoch [17053/20000], Loss: -21.283294677734375, Learning Rate: 0.01\n",
      "Epoch [17054/20000], Loss: -21.319473266601562, Learning Rate: 0.01\n",
      "Epoch [17055/20000], Loss: -21.315231323242188, Learning Rate: 0.01\n",
      "Epoch [17056/20000], Loss: -21.286590576171875, Learning Rate: 0.01\n",
      "Epoch [17057/20000], Loss: -21.2716064453125, Learning Rate: 0.01\n",
      "Epoch [17058/20000], Loss: -21.286270141601562, Learning Rate: 0.01\n",
      "Epoch [17059/20000], Loss: -21.31243896484375, Learning Rate: 0.01\n",
      "Epoch [17060/20000], Loss: -21.322906494140625, Learning Rate: 0.01\n",
      "Epoch [17061/20000], Loss: -21.31201171875, Learning Rate: 0.01\n",
      "Epoch [17062/20000], Loss: -21.296966552734375, Learning Rate: 0.01\n",
      "Epoch [17063/20000], Loss: -21.295654296875, Learning Rate: 0.01\n",
      "Epoch [17064/20000], Loss: -21.308547973632812, Learning Rate: 0.01\n",
      "Epoch [17065/20000], Loss: -21.3211669921875, Learning Rate: 0.01\n",
      "Epoch [17066/20000], Loss: -21.32208251953125, Learning Rate: 0.01\n",
      "Epoch [17067/20000], Loss: -21.313812255859375, Learning Rate: 0.01\n",
      "Epoch [17068/20000], Loss: -21.307281494140625, Learning Rate: 0.01\n",
      "Epoch [17069/20000], Loss: -21.309707641601562, Learning Rate: 0.01\n",
      "Epoch [17070/20000], Loss: -21.317703247070312, Learning Rate: 0.01\n",
      "Epoch [17071/20000], Loss: -21.32318115234375, Learning Rate: 0.01\n",
      "Epoch [17072/20000], Loss: -21.321929931640625, Learning Rate: 0.01\n",
      "Epoch [17073/20000], Loss: -21.316986083984375, Learning Rate: 0.01\n",
      "Epoch [17074/20000], Loss: -21.314498901367188, Learning Rate: 0.01\n",
      "Epoch [17075/20000], Loss: -21.316818237304688, Learning Rate: 0.01\n",
      "Epoch [17076/20000], Loss: -21.321304321289062, Learning Rate: 0.01\n",
      "Epoch [17077/20000], Loss: -21.323593139648438, Learning Rate: 0.01\n",
      "Epoch [17078/20000], Loss: -21.322418212890625, Learning Rate: 0.01\n",
      "Epoch [17079/20000], Loss: -21.31988525390625, Learning Rate: 0.01\n",
      "Epoch [17080/20000], Loss: -21.318817138671875, Learning Rate: 0.01\n",
      "Epoch [17081/20000], Loss: -21.3203125, Learning Rate: 0.01\n",
      "Epoch [17082/20000], Loss: -21.32269287109375, Learning Rate: 0.01\n",
      "Epoch [17083/20000], Loss: -21.32391357421875, Learning Rate: 0.01\n",
      "Epoch [17084/20000], Loss: -21.32318115234375, Learning Rate: 0.01\n",
      "Epoch [17085/20000], Loss: -21.32183837890625, Learning Rate: 0.01\n",
      "Epoch [17086/20000], Loss: -21.321182250976562, Learning Rate: 0.01\n",
      "Epoch [17087/20000], Loss: -21.322021484375, Learning Rate: 0.01\n",
      "Epoch [17088/20000], Loss: -21.32330322265625, Learning Rate: 0.01\n",
      "Epoch [17089/20000], Loss: -21.323989868164062, Learning Rate: 0.01\n",
      "Epoch [17090/20000], Loss: -21.323776245117188, Learning Rate: 0.01\n",
      "Epoch [17091/20000], Loss: -21.323013305664062, Learning Rate: 0.01\n",
      "Epoch [17092/20000], Loss: -21.322769165039062, Learning Rate: 0.01\n",
      "Epoch [17093/20000], Loss: -21.323013305664062, Learning Rate: 0.01\n",
      "Epoch [17094/20000], Loss: -21.32373046875, Learning Rate: 0.01\n",
      "Epoch [17095/20000], Loss: -21.32415771484375, Learning Rate: 0.01\n",
      "Epoch [17096/20000], Loss: -21.324264526367188, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17097/20000], Loss: -21.32391357421875, Learning Rate: 0.01\n",
      "Epoch [17098/20000], Loss: -21.323623657226562, Learning Rate: 0.01\n",
      "Epoch [17099/20000], Loss: -21.32373046875, Learning Rate: 0.01\n",
      "Epoch [17100/20000], Loss: -21.3240966796875, Learning Rate: 0.01\n",
      "Epoch [17101/20000], Loss: -21.324371337890625, Learning Rate: 0.01\n",
      "Epoch [17102/20000], Loss: -21.324432373046875, Learning Rate: 0.01\n",
      "Epoch [17103/20000], Loss: -21.324417114257812, Learning Rate: 0.01\n",
      "Epoch [17104/20000], Loss: -21.324188232421875, Learning Rate: 0.01\n",
      "Epoch [17105/20000], Loss: -21.324127197265625, Learning Rate: 0.01\n",
      "Epoch [17106/20000], Loss: -21.324356079101562, Learning Rate: 0.01\n",
      "Epoch [17107/20000], Loss: -21.324569702148438, Learning Rate: 0.01\n",
      "Epoch [17108/20000], Loss: -21.32464599609375, Learning Rate: 0.01\n",
      "Epoch [17109/20000], Loss: -21.324691772460938, Learning Rate: 0.01\n",
      "Epoch [17110/20000], Loss: -21.32464599609375, Learning Rate: 0.01\n",
      "Epoch [17111/20000], Loss: -21.3245849609375, Learning Rate: 0.01\n",
      "Epoch [17112/20000], Loss: -21.324569702148438, Learning Rate: 0.01\n",
      "Epoch [17113/20000], Loss: -21.324752807617188, Learning Rate: 0.01\n",
      "Epoch [17114/20000], Loss: -21.324783325195312, Learning Rate: 0.01\n",
      "Epoch [17115/20000], Loss: -21.324813842773438, Learning Rate: 0.01\n",
      "Epoch [17116/20000], Loss: -21.324905395507812, Learning Rate: 0.01\n",
      "Epoch [17117/20000], Loss: -21.324874877929688, Learning Rate: 0.01\n",
      "Epoch [17118/20000], Loss: -21.324813842773438, Learning Rate: 0.01\n",
      "Epoch [17119/20000], Loss: -21.324859619140625, Learning Rate: 0.01\n",
      "Epoch [17120/20000], Loss: -21.325027465820312, Learning Rate: 0.01\n",
      "Epoch [17121/20000], Loss: -21.324981689453125, Learning Rate: 0.01\n",
      "Epoch [17122/20000], Loss: -21.325027465820312, Learning Rate: 0.01\n",
      "Epoch [17123/20000], Loss: -21.325103759765625, Learning Rate: 0.01\n",
      "Epoch [17124/20000], Loss: -21.325119018554688, Learning Rate: 0.01\n",
      "Epoch [17125/20000], Loss: -21.32513427734375, Learning Rate: 0.01\n",
      "Epoch [17126/20000], Loss: -21.325119018554688, Learning Rate: 0.01\n",
      "Epoch [17127/20000], Loss: -21.325164794921875, Learning Rate: 0.01\n",
      "Epoch [17128/20000], Loss: -21.325286865234375, Learning Rate: 0.01\n",
      "Epoch [17129/20000], Loss: -21.3253173828125, Learning Rate: 0.01\n",
      "Epoch [17130/20000], Loss: -21.3253173828125, Learning Rate: 0.01\n",
      "Epoch [17131/20000], Loss: -21.325439453125, Learning Rate: 0.01\n",
      "Epoch [17132/20000], Loss: -21.325393676757812, Learning Rate: 0.01\n",
      "Epoch [17133/20000], Loss: -21.325286865234375, Learning Rate: 0.01\n",
      "Epoch [17134/20000], Loss: -21.325439453125, Learning Rate: 0.01\n",
      "Epoch [17135/20000], Loss: -21.325531005859375, Learning Rate: 0.01\n",
      "Epoch [17136/20000], Loss: -21.32550048828125, Learning Rate: 0.01\n",
      "Epoch [17137/20000], Loss: -21.325576782226562, Learning Rate: 0.01\n",
      "Epoch [17138/20000], Loss: -21.325515747070312, Learning Rate: 0.01\n",
      "Epoch [17139/20000], Loss: -21.325637817382812, Learning Rate: 0.01\n",
      "Epoch [17140/20000], Loss: -21.325698852539062, Learning Rate: 0.01\n",
      "Epoch [17141/20000], Loss: -21.325637817382812, Learning Rate: 0.01\n",
      "Epoch [17142/20000], Loss: -21.325714111328125, Learning Rate: 0.01\n",
      "Epoch [17143/20000], Loss: -21.325714111328125, Learning Rate: 0.01\n",
      "Epoch [17144/20000], Loss: -21.325729370117188, Learning Rate: 0.01\n",
      "Epoch [17145/20000], Loss: -21.325759887695312, Learning Rate: 0.01\n",
      "Epoch [17146/20000], Loss: -21.325790405273438, Learning Rate: 0.01\n",
      "Epoch [17147/20000], Loss: -21.325897216796875, Learning Rate: 0.01\n",
      "Epoch [17148/20000], Loss: -21.32586669921875, Learning Rate: 0.01\n",
      "Epoch [17149/20000], Loss: -21.325973510742188, Learning Rate: 0.01\n",
      "Epoch [17150/20000], Loss: -21.32598876953125, Learning Rate: 0.01\n",
      "Epoch [17151/20000], Loss: -21.326065063476562, Learning Rate: 0.01\n",
      "Epoch [17152/20000], Loss: -21.3260498046875, Learning Rate: 0.01\n",
      "Epoch [17153/20000], Loss: -21.326065063476562, Learning Rate: 0.01\n",
      "Epoch [17154/20000], Loss: -21.326080322265625, Learning Rate: 0.01\n",
      "Epoch [17155/20000], Loss: -21.326095581054688, Learning Rate: 0.01\n",
      "Epoch [17156/20000], Loss: -21.32611083984375, Learning Rate: 0.01\n",
      "Epoch [17157/20000], Loss: -21.326156616210938, Learning Rate: 0.01\n",
      "Epoch [17158/20000], Loss: -21.326187133789062, Learning Rate: 0.01\n",
      "Epoch [17159/20000], Loss: -21.326263427734375, Learning Rate: 0.01\n",
      "Epoch [17160/20000], Loss: -21.326248168945312, Learning Rate: 0.01\n",
      "Epoch [17161/20000], Loss: -21.32623291015625, Learning Rate: 0.01\n",
      "Epoch [17162/20000], Loss: -21.326263427734375, Learning Rate: 0.01\n",
      "Epoch [17163/20000], Loss: -21.3262939453125, Learning Rate: 0.01\n",
      "Epoch [17164/20000], Loss: -21.3262939453125, Learning Rate: 0.01\n",
      "Epoch [17165/20000], Loss: -21.326339721679688, Learning Rate: 0.01\n",
      "Epoch [17166/20000], Loss: -21.3262939453125, Learning Rate: 0.01\n",
      "Epoch [17167/20000], Loss: -21.3262939453125, Learning Rate: 0.01\n",
      "Epoch [17168/20000], Loss: -21.32623291015625, Learning Rate: 0.01\n",
      "Epoch [17169/20000], Loss: -21.326171875, Learning Rate: 0.01\n",
      "Epoch [17170/20000], Loss: -21.325973510742188, Learning Rate: 0.01\n",
      "Epoch [17171/20000], Loss: -21.325820922851562, Learning Rate: 0.01\n",
      "Epoch [17172/20000], Loss: -21.325439453125, Learning Rate: 0.01\n",
      "Epoch [17173/20000], Loss: -21.32489013671875, Learning Rate: 0.01\n",
      "Epoch [17174/20000], Loss: -21.323974609375, Learning Rate: 0.01\n",
      "Epoch [17175/20000], Loss: -21.322418212890625, Learning Rate: 0.01\n",
      "Epoch [17176/20000], Loss: -21.320114135742188, Learning Rate: 0.01\n",
      "Epoch [17177/20000], Loss: -21.316329956054688, Learning Rate: 0.01\n",
      "Epoch [17178/20000], Loss: -21.310287475585938, Learning Rate: 0.01\n",
      "Epoch [17179/20000], Loss: -21.3004150390625, Learning Rate: 0.01\n",
      "Epoch [17180/20000], Loss: -21.28466796875, Learning Rate: 0.01\n",
      "Epoch [17181/20000], Loss: -21.259109497070312, Learning Rate: 0.01\n",
      "Epoch [17182/20000], Loss: -21.218048095703125, Learning Rate: 0.01\n",
      "Epoch [17183/20000], Loss: -21.152389526367188, Learning Rate: 0.01\n",
      "Epoch [17184/20000], Loss: -21.050094604492188, Learning Rate: 0.01\n",
      "Epoch [17185/20000], Loss: -20.895721435546875, Learning Rate: 0.01\n",
      "Epoch [17186/20000], Loss: -20.680313110351562, Learning Rate: 0.01\n",
      "Epoch [17187/20000], Loss: -20.410476684570312, Learning Rate: 0.01\n",
      "Epoch [17188/20000], Loss: -20.15142822265625, Learning Rate: 0.01\n",
      "Epoch [17189/20000], Loss: -20.01824951171875, Learning Rate: 0.01\n",
      "Epoch [17190/20000], Loss: -20.160049438476562, Learning Rate: 0.01\n",
      "Epoch [17191/20000], Loss: -20.573898315429688, Learning Rate: 0.01\n",
      "Epoch [17192/20000], Loss: -21.0528564453125, Learning Rate: 0.01\n",
      "Epoch [17193/20000], Loss: -21.304443359375, Learning Rate: 0.01\n",
      "Epoch [17194/20000], Loss: -21.224716186523438, Learning Rate: 0.01\n",
      "Epoch [17195/20000], Loss: -20.970108032226562, Learning Rate: 0.01\n",
      "Epoch [17196/20000], Loss: -20.80047607421875, Learning Rate: 0.01\n",
      "Epoch [17197/20000], Loss: -20.876327514648438, Learning Rate: 0.01\n",
      "Epoch [17198/20000], Loss: -21.116836547851562, Learning Rate: 0.01\n",
      "Epoch [17199/20000], Loss: -21.301284790039062, Learning Rate: 0.01\n",
      "Epoch [17200/20000], Loss: -21.288177490234375, Learning Rate: 0.01\n",
      "Epoch [17201/20000], Loss: -21.144271850585938, Learning Rate: 0.01\n",
      "Epoch [17202/20000], Loss: -21.048568725585938, Learning Rate: 0.01\n",
      "Epoch [17203/20000], Loss: -21.100265502929688, Learning Rate: 0.01\n",
      "Epoch [17204/20000], Loss: -21.2396240234375, Learning Rate: 0.01\n",
      "Epoch [17205/20000], Loss: -21.324310302734375, Learning Rate: 0.01\n",
      "Epoch [17206/20000], Loss: -21.291839599609375, Learning Rate: 0.01\n",
      "Epoch [17207/20000], Loss: -21.2083740234375, Learning Rate: 0.01\n",
      "Epoch [17208/20000], Loss: -21.177688598632812, Learning Rate: 0.01\n",
      "Epoch [17209/20000], Loss: -21.230331420898438, Learning Rate: 0.01\n",
      "Epoch [17210/20000], Loss: -21.302993774414062, Learning Rate: 0.01\n",
      "Epoch [17211/20000], Loss: -21.323013305664062, Learning Rate: 0.01\n",
      "Epoch [17212/20000], Loss: -21.2855224609375, Learning Rate: 0.01\n",
      "Epoch [17213/20000], Loss: -21.246246337890625, Learning Rate: 0.01\n",
      "Epoch [17214/20000], Loss: -21.251983642578125, Learning Rate: 0.01\n",
      "Epoch [17215/20000], Loss: -21.293319702148438, Learning Rate: 0.01\n",
      "Epoch [17216/20000], Loss: -21.325027465820312, Learning Rate: 0.01\n",
      "Epoch [17217/20000], Loss: -21.31976318359375, Learning Rate: 0.01\n",
      "Epoch [17218/20000], Loss: -21.293304443359375, Learning Rate: 0.01\n",
      "Epoch [17219/20000], Loss: -21.280197143554688, Learning Rate: 0.01\n",
      "Epoch [17220/20000], Loss: -21.293899536132812, Learning Rate: 0.01\n",
      "Epoch [17221/20000], Loss: -21.317413330078125, Learning Rate: 0.01\n",
      "Epoch [17222/20000], Loss: -21.326568603515625, Learning Rate: 0.01\n",
      "Epoch [17223/20000], Loss: -21.316574096679688, Learning Rate: 0.01\n",
      "Epoch [17224/20000], Loss: -21.303176879882812, Learning Rate: 0.01\n",
      "Epoch [17225/20000], Loss: -21.3023681640625, Learning Rate: 0.01\n",
      "Epoch [17226/20000], Loss: -21.31427001953125, Learning Rate: 0.01\n",
      "Epoch [17227/20000], Loss: -21.325775146484375, Learning Rate: 0.01\n",
      "Epoch [17228/20000], Loss: -21.32659912109375, Learning Rate: 0.01\n",
      "Epoch [17229/20000], Loss: -21.31903076171875, Learning Rate: 0.01\n",
      "Epoch [17230/20000], Loss: -21.313140869140625, Learning Rate: 0.01\n",
      "Epoch [17231/20000], Loss: -21.315399169921875, Learning Rate: 0.01\n",
      "Epoch [17232/20000], Loss: -21.322616577148438, Learning Rate: 0.01\n",
      "Epoch [17233/20000], Loss: -21.327468872070312, Learning Rate: 0.01\n",
      "Epoch [17234/20000], Loss: -21.326141357421875, Learning Rate: 0.01\n",
      "Epoch [17235/20000], Loss: -21.321640014648438, Learning Rate: 0.01\n",
      "Epoch [17236/20000], Loss: -21.31951904296875, Learning Rate: 0.01\n",
      "Epoch [17237/20000], Loss: -21.321624755859375, Learning Rate: 0.01\n",
      "Epoch [17238/20000], Loss: -21.325759887695312, Learning Rate: 0.01\n",
      "Epoch [17239/20000], Loss: -21.327972412109375, Learning Rate: 0.01\n",
      "Epoch [17240/20000], Loss: -21.3267822265625, Learning Rate: 0.01\n",
      "Epoch [17241/20000], Loss: -21.32440185546875, Learning Rate: 0.01\n",
      "Epoch [17242/20000], Loss: -21.323577880859375, Learning Rate: 0.01\n",
      "Epoch [17243/20000], Loss: -21.3250732421875, Learning Rate: 0.01\n",
      "Epoch [17244/20000], Loss: -21.327163696289062, Learning Rate: 0.01\n",
      "Epoch [17245/20000], Loss: -21.328201293945312, Learning Rate: 0.01\n",
      "Epoch [17246/20000], Loss: -21.327529907226562, Learning Rate: 0.01\n",
      "Epoch [17247/20000], Loss: -21.326278686523438, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17248/20000], Loss: -21.325836181640625, Learning Rate: 0.01\n",
      "Epoch [17249/20000], Loss: -21.326553344726562, Learning Rate: 0.01\n",
      "Epoch [17250/20000], Loss: -21.327682495117188, Learning Rate: 0.01\n",
      "Epoch [17251/20000], Loss: -21.32830810546875, Learning Rate: 0.01\n",
      "Epoch [17252/20000], Loss: -21.327972412109375, Learning Rate: 0.01\n",
      "Epoch [17253/20000], Loss: -21.327301025390625, Learning Rate: 0.01\n",
      "Epoch [17254/20000], Loss: -21.32708740234375, Learning Rate: 0.01\n",
      "Epoch [17255/20000], Loss: -21.327407836914062, Learning Rate: 0.01\n",
      "Epoch [17256/20000], Loss: -21.32794189453125, Learning Rate: 0.01\n",
      "Epoch [17257/20000], Loss: -21.328414916992188, Learning Rate: 0.01\n",
      "Epoch [17258/20000], Loss: -21.328399658203125, Learning Rate: 0.01\n",
      "Epoch [17259/20000], Loss: -21.328094482421875, Learning Rate: 0.01\n",
      "Epoch [17260/20000], Loss: -21.327880859375, Learning Rate: 0.01\n",
      "Epoch [17261/20000], Loss: -21.3280029296875, Learning Rate: 0.01\n",
      "Epoch [17262/20000], Loss: -21.328231811523438, Learning Rate: 0.01\n",
      "Epoch [17263/20000], Loss: -21.328475952148438, Learning Rate: 0.01\n",
      "Epoch [17264/20000], Loss: -21.328582763671875, Learning Rate: 0.01\n",
      "Epoch [17265/20000], Loss: -21.328475952148438, Learning Rate: 0.01\n",
      "Epoch [17266/20000], Loss: -21.328277587890625, Learning Rate: 0.01\n",
      "Epoch [17267/20000], Loss: -21.3282470703125, Learning Rate: 0.01\n",
      "Epoch [17268/20000], Loss: -21.328414916992188, Learning Rate: 0.01\n",
      "Epoch [17269/20000], Loss: -21.32867431640625, Learning Rate: 0.01\n",
      "Epoch [17270/20000], Loss: -21.328643798828125, Learning Rate: 0.01\n",
      "Epoch [17271/20000], Loss: -21.328598022460938, Learning Rate: 0.01\n",
      "Epoch [17272/20000], Loss: -21.328384399414062, Learning Rate: 0.01\n",
      "Epoch [17273/20000], Loss: -21.328292846679688, Learning Rate: 0.01\n",
      "Epoch [17274/20000], Loss: -21.328170776367188, Learning Rate: 0.01\n",
      "Epoch [17275/20000], Loss: -21.328018188476562, Learning Rate: 0.01\n",
      "Epoch [17276/20000], Loss: -21.327804565429688, Learning Rate: 0.01\n",
      "Epoch [17277/20000], Loss: -21.327560424804688, Learning Rate: 0.01\n",
      "Epoch [17278/20000], Loss: -21.326919555664062, Learning Rate: 0.01\n",
      "Epoch [17279/20000], Loss: -21.326187133789062, Learning Rate: 0.01\n",
      "Epoch [17280/20000], Loss: -21.325042724609375, Learning Rate: 0.01\n",
      "Epoch [17281/20000], Loss: -21.323516845703125, Learning Rate: 0.01\n",
      "Epoch [17282/20000], Loss: -21.321304321289062, Learning Rate: 0.01\n",
      "Epoch [17283/20000], Loss: -21.317916870117188, Learning Rate: 0.01\n",
      "Epoch [17284/20000], Loss: -21.312896728515625, Learning Rate: 0.01\n",
      "Epoch [17285/20000], Loss: -21.305435180664062, Learning Rate: 0.01\n",
      "Epoch [17286/20000], Loss: -21.294387817382812, Learning Rate: 0.01\n",
      "Epoch [17287/20000], Loss: -21.277618408203125, Learning Rate: 0.01\n",
      "Epoch [17288/20000], Loss: -21.252639770507812, Learning Rate: 0.01\n",
      "Epoch [17289/20000], Loss: -21.21453857421875, Learning Rate: 0.01\n",
      "Epoch [17290/20000], Loss: -21.157913208007812, Learning Rate: 0.01\n",
      "Epoch [17291/20000], Loss: -21.072372436523438, Learning Rate: 0.01\n",
      "Epoch [17292/20000], Loss: -20.949295043945312, Learning Rate: 0.01\n",
      "Epoch [17293/20000], Loss: -20.770401000976562, Learning Rate: 0.01\n",
      "Epoch [17294/20000], Loss: -20.536346435546875, Learning Rate: 0.01\n",
      "Epoch [17295/20000], Loss: -20.23687744140625, Learning Rate: 0.01\n",
      "Epoch [17296/20000], Loss: -19.9403076171875, Learning Rate: 0.01\n",
      "Epoch [17297/20000], Loss: -19.698257446289062, Learning Rate: 0.01\n",
      "Epoch [17298/20000], Loss: -19.696640014648438, Learning Rate: 0.01\n",
      "Epoch [17299/20000], Loss: -19.964874267578125, Learning Rate: 0.01\n",
      "Epoch [17300/20000], Loss: -20.497039794921875, Learning Rate: 0.01\n",
      "Epoch [17301/20000], Loss: -21.024673461914062, Learning Rate: 0.01\n",
      "Epoch [17302/20000], Loss: -21.310012817382812, Learning Rate: 0.01\n",
      "Epoch [17303/20000], Loss: -21.261154174804688, Learning Rate: 0.01\n",
      "Epoch [17304/20000], Loss: -21.000259399414062, Learning Rate: 0.01\n",
      "Epoch [17305/20000], Loss: -20.758377075195312, Learning Rate: 0.01\n",
      "Epoch [17306/20000], Loss: -20.707611083984375, Learning Rate: 0.01\n",
      "Epoch [17307/20000], Loss: -20.887725830078125, Learning Rate: 0.01\n",
      "Epoch [17308/20000], Loss: -21.14923095703125, Learning Rate: 0.01\n",
      "Epoch [17309/20000], Loss: -21.313751220703125, Learning Rate: 0.01\n",
      "Epoch [17310/20000], Loss: -21.295455932617188, Learning Rate: 0.01\n",
      "Epoch [17311/20000], Loss: -21.159072875976562, Learning Rate: 0.01\n",
      "Epoch [17312/20000], Loss: -21.044677734375, Learning Rate: 0.01\n",
      "Epoch [17313/20000], Loss: -21.045745849609375, Learning Rate: 0.01\n",
      "Epoch [17314/20000], Loss: -21.156158447265625, Learning Rate: 0.01\n",
      "Epoch [17315/20000], Loss: -21.279220581054688, Learning Rate: 0.01\n",
      "Epoch [17316/20000], Loss: -21.327346801757812, Learning Rate: 0.01\n",
      "Epoch [17317/20000], Loss: -21.286224365234375, Learning Rate: 0.01\n",
      "Epoch [17318/20000], Loss: -21.213241577148438, Learning Rate: 0.01\n",
      "Epoch [17319/20000], Loss: -21.179306030273438, Learning Rate: 0.01\n",
      "Epoch [17320/20000], Loss: -21.209381103515625, Learning Rate: 0.01\n",
      "Epoch [17321/20000], Loss: -21.274444580078125, Learning Rate: 0.01\n",
      "Epoch [17322/20000], Loss: -21.321380615234375, Learning Rate: 0.01\n",
      "Epoch [17323/20000], Loss: -21.32183837890625, Learning Rate: 0.01\n",
      "Epoch [17324/20000], Loss: -21.288711547851562, Learning Rate: 0.01\n",
      "Epoch [17325/20000], Loss: -21.258209228515625, Learning Rate: 0.01\n",
      "Epoch [17326/20000], Loss: -21.25726318359375, Learning Rate: 0.01\n",
      "Epoch [17327/20000], Loss: -21.283584594726562, Learning Rate: 0.01\n",
      "Epoch [17328/20000], Loss: -21.314544677734375, Learning Rate: 0.01\n",
      "Epoch [17329/20000], Loss: -21.32781982421875, Learning Rate: 0.01\n",
      "Epoch [17330/20000], Loss: -21.319076538085938, Learning Rate: 0.01\n",
      "Epoch [17331/20000], Loss: -21.3013916015625, Learning Rate: 0.01\n",
      "Epoch [17332/20000], Loss: -21.291519165039062, Learning Rate: 0.01\n",
      "Epoch [17333/20000], Loss: -21.297149658203125, Learning Rate: 0.01\n",
      "Epoch [17334/20000], Loss: -21.312255859375, Learning Rate: 0.01\n",
      "Epoch [17335/20000], Loss: -21.325164794921875, Learning Rate: 0.01\n",
      "Epoch [17336/20000], Loss: -21.328018188476562, Learning Rate: 0.01\n",
      "Epoch [17337/20000], Loss: -21.321853637695312, Learning Rate: 0.01\n",
      "Epoch [17338/20000], Loss: -21.313949584960938, Learning Rate: 0.01\n",
      "Epoch [17339/20000], Loss: -21.31103515625, Learning Rate: 0.01\n",
      "Epoch [17340/20000], Loss: -21.3150634765625, Learning Rate: 0.01\n",
      "Epoch [17341/20000], Loss: -21.322341918945312, Learning Rate: 0.01\n",
      "Epoch [17342/20000], Loss: -21.327850341796875, Learning Rate: 0.01\n",
      "Epoch [17343/20000], Loss: -21.328659057617188, Learning Rate: 0.01\n",
      "Epoch [17344/20000], Loss: -21.325637817382812, Learning Rate: 0.01\n",
      "Epoch [17345/20000], Loss: -21.32208251953125, Learning Rate: 0.01\n",
      "Epoch [17346/20000], Loss: -21.320907592773438, Learning Rate: 0.01\n",
      "Epoch [17347/20000], Loss: -21.322784423828125, Learning Rate: 0.01\n",
      "Epoch [17348/20000], Loss: -21.326080322265625, Learning Rate: 0.01\n",
      "Epoch [17349/20000], Loss: -21.328750610351562, Learning Rate: 0.01\n",
      "Epoch [17350/20000], Loss: -21.329345703125, Learning Rate: 0.01\n",
      "Epoch [17351/20000], Loss: -21.328292846679688, Learning Rate: 0.01\n",
      "Epoch [17352/20000], Loss: -21.326766967773438, Learning Rate: 0.01\n",
      "Epoch [17353/20000], Loss: -21.325912475585938, Learning Rate: 0.01\n",
      "Epoch [17354/20000], Loss: -21.326461791992188, Learning Rate: 0.01\n",
      "Epoch [17355/20000], Loss: -21.327743530273438, Learning Rate: 0.01\n",
      "Epoch [17356/20000], Loss: -21.329116821289062, Learning Rate: 0.01\n",
      "Epoch [17357/20000], Loss: -21.32977294921875, Learning Rate: 0.01\n",
      "Epoch [17358/20000], Loss: -21.329605102539062, Learning Rate: 0.01\n",
      "Epoch [17359/20000], Loss: -21.3289794921875, Learning Rate: 0.01\n",
      "Epoch [17360/20000], Loss: -21.328567504882812, Learning Rate: 0.01\n",
      "Epoch [17361/20000], Loss: -21.328521728515625, Learning Rate: 0.01\n",
      "Epoch [17362/20000], Loss: -21.328948974609375, Learning Rate: 0.01\n",
      "Epoch [17363/20000], Loss: -21.3294677734375, Learning Rate: 0.01\n",
      "Epoch [17364/20000], Loss: -21.329940795898438, Learning Rate: 0.01\n",
      "Epoch [17365/20000], Loss: -21.330230712890625, Learning Rate: 0.01\n",
      "Epoch [17366/20000], Loss: -21.3302001953125, Learning Rate: 0.01\n",
      "Epoch [17367/20000], Loss: -21.329971313476562, Learning Rate: 0.01\n",
      "Epoch [17368/20000], Loss: -21.329833984375, Learning Rate: 0.01\n",
      "Epoch [17369/20000], Loss: -21.329818725585938, Learning Rate: 0.01\n",
      "Epoch [17370/20000], Loss: -21.330047607421875, Learning Rate: 0.01\n",
      "Epoch [17371/20000], Loss: -21.330230712890625, Learning Rate: 0.01\n",
      "Epoch [17372/20000], Loss: -21.330398559570312, Learning Rate: 0.01\n",
      "Epoch [17373/20000], Loss: -21.330459594726562, Learning Rate: 0.01\n",
      "Epoch [17374/20000], Loss: -21.3304443359375, Learning Rate: 0.01\n",
      "Epoch [17375/20000], Loss: -21.330474853515625, Learning Rate: 0.01\n",
      "Epoch [17376/20000], Loss: -21.3304443359375, Learning Rate: 0.01\n",
      "Epoch [17377/20000], Loss: -21.330474853515625, Learning Rate: 0.01\n",
      "Epoch [17378/20000], Loss: -21.3304443359375, Learning Rate: 0.01\n",
      "Epoch [17379/20000], Loss: -21.330596923828125, Learning Rate: 0.01\n",
      "Epoch [17380/20000], Loss: -21.330642700195312, Learning Rate: 0.01\n",
      "Epoch [17381/20000], Loss: -21.330734252929688, Learning Rate: 0.01\n",
      "Epoch [17382/20000], Loss: -21.330856323242188, Learning Rate: 0.01\n",
      "Epoch [17383/20000], Loss: -21.330856323242188, Learning Rate: 0.01\n",
      "Epoch [17384/20000], Loss: -21.330917358398438, Learning Rate: 0.01\n",
      "Epoch [17385/20000], Loss: -21.330856323242188, Learning Rate: 0.01\n",
      "Epoch [17386/20000], Loss: -21.330947875976562, Learning Rate: 0.01\n",
      "Epoch [17387/20000], Loss: -21.330902099609375, Learning Rate: 0.01\n",
      "Epoch [17388/20000], Loss: -21.330963134765625, Learning Rate: 0.01\n",
      "Epoch [17389/20000], Loss: -21.330978393554688, Learning Rate: 0.01\n",
      "Epoch [17390/20000], Loss: -21.331085205078125, Learning Rate: 0.01\n",
      "Epoch [17391/20000], Loss: -21.331069946289062, Learning Rate: 0.01\n",
      "Epoch [17392/20000], Loss: -21.331130981445312, Learning Rate: 0.01\n",
      "Epoch [17393/20000], Loss: -21.331192016601562, Learning Rate: 0.01\n",
      "Epoch [17394/20000], Loss: -21.33123779296875, Learning Rate: 0.01\n",
      "Epoch [17395/20000], Loss: -21.331146240234375, Learning Rate: 0.01\n",
      "Epoch [17396/20000], Loss: -21.331222534179688, Learning Rate: 0.01\n",
      "Epoch [17397/20000], Loss: -21.331253051757812, Learning Rate: 0.01\n",
      "Epoch [17398/20000], Loss: -21.331222534179688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17399/20000], Loss: -21.331314086914062, Learning Rate: 0.01\n",
      "Epoch [17400/20000], Loss: -21.331298828125, Learning Rate: 0.01\n",
      "Epoch [17401/20000], Loss: -21.331329345703125, Learning Rate: 0.01\n",
      "Epoch [17402/20000], Loss: -21.331451416015625, Learning Rate: 0.01\n",
      "Epoch [17403/20000], Loss: -21.331436157226562, Learning Rate: 0.01\n",
      "Epoch [17404/20000], Loss: -21.331466674804688, Learning Rate: 0.01\n",
      "Epoch [17405/20000], Loss: -21.331497192382812, Learning Rate: 0.01\n",
      "Epoch [17406/20000], Loss: -21.331466674804688, Learning Rate: 0.01\n",
      "Epoch [17407/20000], Loss: -21.331497192382812, Learning Rate: 0.01\n",
      "Epoch [17408/20000], Loss: -21.331588745117188, Learning Rate: 0.01\n",
      "Epoch [17409/20000], Loss: -21.331512451171875, Learning Rate: 0.01\n",
      "Epoch [17410/20000], Loss: -21.331512451171875, Learning Rate: 0.01\n",
      "Epoch [17411/20000], Loss: -21.331649780273438, Learning Rate: 0.01\n",
      "Epoch [17412/20000], Loss: -21.331680297851562, Learning Rate: 0.01\n",
      "Epoch [17413/20000], Loss: -21.331619262695312, Learning Rate: 0.01\n",
      "Epoch [17414/20000], Loss: -21.331588745117188, Learning Rate: 0.01\n",
      "Epoch [17415/20000], Loss: -21.3316650390625, Learning Rate: 0.01\n",
      "Epoch [17416/20000], Loss: -21.331771850585938, Learning Rate: 0.01\n",
      "Epoch [17417/20000], Loss: -21.331710815429688, Learning Rate: 0.01\n",
      "Epoch [17418/20000], Loss: -21.331695556640625, Learning Rate: 0.01\n",
      "Epoch [17419/20000], Loss: -21.331756591796875, Learning Rate: 0.01\n",
      "Epoch [17420/20000], Loss: -21.331863403320312, Learning Rate: 0.01\n",
      "Epoch [17421/20000], Loss: -21.331802368164062, Learning Rate: 0.01\n",
      "Epoch [17422/20000], Loss: -21.33184814453125, Learning Rate: 0.01\n",
      "Epoch [17423/20000], Loss: -21.331863403320312, Learning Rate: 0.01\n",
      "Epoch [17424/20000], Loss: -21.331893920898438, Learning Rate: 0.01\n",
      "Epoch [17425/20000], Loss: -21.332000732421875, Learning Rate: 0.01\n",
      "Epoch [17426/20000], Loss: -21.33197021484375, Learning Rate: 0.01\n",
      "Epoch [17427/20000], Loss: -21.331954956054688, Learning Rate: 0.01\n",
      "Epoch [17428/20000], Loss: -21.33203125, Learning Rate: 0.01\n",
      "Epoch [17429/20000], Loss: -21.331954956054688, Learning Rate: 0.01\n",
      "Epoch [17430/20000], Loss: -21.33203125, Learning Rate: 0.01\n",
      "Epoch [17431/20000], Loss: -21.33209228515625, Learning Rate: 0.01\n",
      "Epoch [17432/20000], Loss: -21.332122802734375, Learning Rate: 0.01\n",
      "Epoch [17433/20000], Loss: -21.33203125, Learning Rate: 0.01\n",
      "Epoch [17434/20000], Loss: -21.332183837890625, Learning Rate: 0.01\n",
      "Epoch [17435/20000], Loss: -21.332229614257812, Learning Rate: 0.01\n",
      "Epoch [17436/20000], Loss: -21.33221435546875, Learning Rate: 0.01\n",
      "Epoch [17437/20000], Loss: -21.332183837890625, Learning Rate: 0.01\n",
      "Epoch [17438/20000], Loss: -21.332183837890625, Learning Rate: 0.01\n",
      "Epoch [17439/20000], Loss: -21.332199096679688, Learning Rate: 0.01\n",
      "Epoch [17440/20000], Loss: -21.332244873046875, Learning Rate: 0.01\n",
      "Epoch [17441/20000], Loss: -21.332244873046875, Learning Rate: 0.01\n",
      "Epoch [17442/20000], Loss: -21.332382202148438, Learning Rate: 0.01\n",
      "Epoch [17443/20000], Loss: -21.33233642578125, Learning Rate: 0.01\n",
      "Epoch [17444/20000], Loss: -21.332290649414062, Learning Rate: 0.01\n",
      "Epoch [17445/20000], Loss: -21.332305908203125, Learning Rate: 0.01\n",
      "Epoch [17446/20000], Loss: -21.332412719726562, Learning Rate: 0.01\n",
      "Epoch [17447/20000], Loss: -21.332412719726562, Learning Rate: 0.01\n",
      "Epoch [17448/20000], Loss: -21.33245849609375, Learning Rate: 0.01\n",
      "Epoch [17449/20000], Loss: -21.3323974609375, Learning Rate: 0.01\n",
      "Epoch [17450/20000], Loss: -21.332412719726562, Learning Rate: 0.01\n",
      "Epoch [17451/20000], Loss: -21.332412719726562, Learning Rate: 0.01\n",
      "Epoch [17452/20000], Loss: -21.332412719726562, Learning Rate: 0.01\n",
      "Epoch [17453/20000], Loss: -21.332366943359375, Learning Rate: 0.01\n",
      "Epoch [17454/20000], Loss: -21.332305908203125, Learning Rate: 0.01\n",
      "Epoch [17455/20000], Loss: -21.33221435546875, Learning Rate: 0.01\n",
      "Epoch [17456/20000], Loss: -21.331954956054688, Learning Rate: 0.01\n",
      "Epoch [17457/20000], Loss: -21.331619262695312, Learning Rate: 0.01\n",
      "Epoch [17458/20000], Loss: -21.331192016601562, Learning Rate: 0.01\n",
      "Epoch [17459/20000], Loss: -21.330474853515625, Learning Rate: 0.01\n",
      "Epoch [17460/20000], Loss: -21.329666137695312, Learning Rate: 0.01\n",
      "Epoch [17461/20000], Loss: -21.328125, Learning Rate: 0.01\n",
      "Epoch [17462/20000], Loss: -21.325912475585938, Learning Rate: 0.01\n",
      "Epoch [17463/20000], Loss: -21.322402954101562, Learning Rate: 0.01\n",
      "Epoch [17464/20000], Loss: -21.316925048828125, Learning Rate: 0.01\n",
      "Epoch [17465/20000], Loss: -21.308502197265625, Learning Rate: 0.01\n",
      "Epoch [17466/20000], Loss: -21.295333862304688, Learning Rate: 0.01\n",
      "Epoch [17467/20000], Loss: -21.274566650390625, Learning Rate: 0.01\n",
      "Epoch [17468/20000], Loss: -21.241607666015625, Learning Rate: 0.01\n",
      "Epoch [17469/20000], Loss: -21.190032958984375, Learning Rate: 0.01\n",
      "Epoch [17470/20000], Loss: -21.108413696289062, Learning Rate: 0.01\n",
      "Epoch [17471/20000], Loss: -20.983383178710938, Learning Rate: 0.01\n",
      "Epoch [17472/20000], Loss: -20.79193115234375, Learning Rate: 0.01\n",
      "Epoch [17473/20000], Loss: -20.520889282226562, Learning Rate: 0.01\n",
      "Epoch [17474/20000], Loss: -20.150314331054688, Learning Rate: 0.01\n",
      "Epoch [17475/20000], Loss: -19.7374267578125, Learning Rate: 0.01\n",
      "Epoch [17476/20000], Loss: -19.355804443359375, Learning Rate: 0.01\n",
      "Epoch [17477/20000], Loss: -19.253097534179688, Learning Rate: 0.01\n",
      "Epoch [17478/20000], Loss: -19.5233154296875, Learning Rate: 0.01\n",
      "Epoch [17479/20000], Loss: -20.181640625, Learning Rate: 0.01\n",
      "Epoch [17480/20000], Loss: -20.842697143554688, Learning Rate: 0.01\n",
      "Epoch [17481/20000], Loss: -21.159942626953125, Learning Rate: 0.01\n",
      "Epoch [17482/20000], Loss: -21.031967163085938, Learning Rate: 0.01\n",
      "Epoch [17483/20000], Loss: -20.705780029296875, Learning Rate: 0.01\n",
      "Epoch [17484/20000], Loss: -20.54315185546875, Learning Rate: 0.01\n",
      "Epoch [17485/20000], Loss: -20.710037231445312, Learning Rate: 0.01\n",
      "Epoch [17486/20000], Loss: -21.056381225585938, Learning Rate: 0.01\n",
      "Epoch [17487/20000], Loss: -21.257431030273438, Learning Rate: 0.01\n",
      "Epoch [17488/20000], Loss: -21.175994873046875, Learning Rate: 0.01\n",
      "Epoch [17489/20000], Loss: -20.9658203125, Learning Rate: 0.01\n",
      "Epoch [17490/20000], Loss: -20.888427734375, Learning Rate: 0.01\n",
      "Epoch [17491/20000], Loss: -21.037002563476562, Learning Rate: 0.01\n",
      "Epoch [17492/20000], Loss: -21.254196166992188, Learning Rate: 0.01\n",
      "Epoch [17493/20000], Loss: -21.327239990234375, Learning Rate: 0.01\n",
      "Epoch [17494/20000], Loss: -21.223175048828125, Learning Rate: 0.01\n",
      "Epoch [17495/20000], Loss: -21.097732543945312, Learning Rate: 0.01\n",
      "Epoch [17496/20000], Loss: -21.100753784179688, Learning Rate: 0.01\n",
      "Epoch [17497/20000], Loss: -21.219528198242188, Learning Rate: 0.01\n",
      "Epoch [17498/20000], Loss: -21.317840576171875, Learning Rate: 0.01\n",
      "Epoch [17499/20000], Loss: -21.306228637695312, Learning Rate: 0.01\n",
      "Epoch [17500/20000], Loss: -21.231292724609375, Learning Rate: 0.01\n",
      "Epoch [17501/20000], Loss: -21.19873046875, Learning Rate: 0.01\n",
      "Epoch [17502/20000], Loss: -21.244857788085938, Learning Rate: 0.01\n",
      "Epoch [17503/20000], Loss: -21.310012817382812, Learning Rate: 0.01\n",
      "Epoch [17504/20000], Loss: -21.323593139648438, Learning Rate: 0.01\n",
      "Epoch [17505/20000], Loss: -21.285079956054688, Learning Rate: 0.01\n",
      "Epoch [17506/20000], Loss: -21.253219604492188, Learning Rate: 0.01\n",
      "Epoch [17507/20000], Loss: -21.267684936523438, Learning Rate: 0.01\n",
      "Epoch [17508/20000], Loss: -21.309783935546875, Learning Rate: 0.01\n",
      "Epoch [17509/20000], Loss: -21.332412719726562, Learning Rate: 0.01\n",
      "Epoch [17510/20000], Loss: -21.318252563476562, Learning Rate: 0.01\n",
      "Epoch [17511/20000], Loss: -21.292739868164062, Learning Rate: 0.01\n",
      "Epoch [17512/20000], Loss: -21.288330078125, Learning Rate: 0.01\n",
      "Epoch [17513/20000], Loss: -21.307891845703125, Learning Rate: 0.01\n",
      "Epoch [17514/20000], Loss: -21.32763671875, Learning Rate: 0.01\n",
      "Epoch [17515/20000], Loss: -21.328338623046875, Learning Rate: 0.01\n",
      "Epoch [17516/20000], Loss: -21.315338134765625, Learning Rate: 0.01\n",
      "Epoch [17517/20000], Loss: -21.307525634765625, Learning Rate: 0.01\n",
      "Epoch [17518/20000], Loss: -21.314224243164062, Learning Rate: 0.01\n",
      "Epoch [17519/20000], Loss: -21.326919555664062, Learning Rate: 0.01\n",
      "Epoch [17520/20000], Loss: -21.332046508789062, Learning Rate: 0.01\n",
      "Epoch [17521/20000], Loss: -21.3262939453125, Learning Rate: 0.01\n",
      "Epoch [17522/20000], Loss: -21.318756103515625, Learning Rate: 0.01\n",
      "Epoch [17523/20000], Loss: -21.318313598632812, Learning Rate: 0.01\n",
      "Epoch [17524/20000], Loss: -21.325149536132812, Learning Rate: 0.01\n",
      "Epoch [17525/20000], Loss: -21.331634521484375, Learning Rate: 0.01\n",
      "Epoch [17526/20000], Loss: -21.332061767578125, Learning Rate: 0.01\n",
      "Epoch [17527/20000], Loss: -21.327865600585938, Learning Rate: 0.01\n",
      "Epoch [17528/20000], Loss: -21.324752807617188, Learning Rate: 0.01\n",
      "Epoch [17529/20000], Loss: -21.326278686523438, Learning Rate: 0.01\n",
      "Epoch [17530/20000], Loss: -21.33026123046875, Learning Rate: 0.01\n",
      "Epoch [17531/20000], Loss: -21.33258056640625, Learning Rate: 0.01\n",
      "Epoch [17532/20000], Loss: -21.33154296875, Learning Rate: 0.01\n",
      "Epoch [17533/20000], Loss: -21.329147338867188, Learning Rate: 0.01\n",
      "Epoch [17534/20000], Loss: -21.328323364257812, Learning Rate: 0.01\n",
      "Epoch [17535/20000], Loss: -21.329971313476562, Learning Rate: 0.01\n",
      "Epoch [17536/20000], Loss: -21.332290649414062, Learning Rate: 0.01\n",
      "Epoch [17537/20000], Loss: -21.333145141601562, Learning Rate: 0.01\n",
      "Epoch [17538/20000], Loss: -21.332229614257812, Learning Rate: 0.01\n",
      "Epoch [17539/20000], Loss: -21.33099365234375, Learning Rate: 0.01\n",
      "Epoch [17540/20000], Loss: -21.33074951171875, Learning Rate: 0.01\n",
      "Epoch [17541/20000], Loss: -21.331680297851562, Learning Rate: 0.01\n",
      "Epoch [17542/20000], Loss: -21.332855224609375, Learning Rate: 0.01\n",
      "Epoch [17543/20000], Loss: -21.333145141601562, Learning Rate: 0.01\n",
      "Epoch [17544/20000], Loss: -21.332687377929688, Learning Rate: 0.01\n",
      "Epoch [17545/20000], Loss: -21.33203125, Learning Rate: 0.01\n",
      "Epoch [17546/20000], Loss: -21.332000732421875, Learning Rate: 0.01\n",
      "Epoch [17547/20000], Loss: -21.332534790039062, Learning Rate: 0.01\n",
      "Epoch [17548/20000], Loss: -21.333267211914062, Learning Rate: 0.01\n",
      "Epoch [17549/20000], Loss: -21.333480834960938, Learning Rate: 0.01\n",
      "Epoch [17550/20000], Loss: -21.333206176757812, Learning Rate: 0.01\n",
      "Epoch [17551/20000], Loss: -21.332855224609375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17552/20000], Loss: -21.332855224609375, Learning Rate: 0.01\n",
      "Epoch [17553/20000], Loss: -21.333084106445312, Learning Rate: 0.01\n",
      "Epoch [17554/20000], Loss: -21.333389282226562, Learning Rate: 0.01\n",
      "Epoch [17555/20000], Loss: -21.33355712890625, Learning Rate: 0.01\n",
      "Epoch [17556/20000], Loss: -21.333450317382812, Learning Rate: 0.01\n",
      "Epoch [17557/20000], Loss: -21.333343505859375, Learning Rate: 0.01\n",
      "Epoch [17558/20000], Loss: -21.33319091796875, Learning Rate: 0.01\n",
      "Epoch [17559/20000], Loss: -21.333358764648438, Learning Rate: 0.01\n",
      "Epoch [17560/20000], Loss: -21.333648681640625, Learning Rate: 0.01\n",
      "Epoch [17561/20000], Loss: -21.333709716796875, Learning Rate: 0.01\n",
      "Epoch [17562/20000], Loss: -21.333694458007812, Learning Rate: 0.01\n",
      "Epoch [17563/20000], Loss: -21.333709716796875, Learning Rate: 0.01\n",
      "Epoch [17564/20000], Loss: -21.333663940429688, Learning Rate: 0.01\n",
      "Epoch [17565/20000], Loss: -21.333587646484375, Learning Rate: 0.01\n",
      "Epoch [17566/20000], Loss: -21.3338623046875, Learning Rate: 0.01\n",
      "Epoch [17567/20000], Loss: -21.333847045898438, Learning Rate: 0.01\n",
      "Epoch [17568/20000], Loss: -21.333953857421875, Learning Rate: 0.01\n",
      "Epoch [17569/20000], Loss: -21.3338623046875, Learning Rate: 0.01\n",
      "Epoch [17570/20000], Loss: -21.33392333984375, Learning Rate: 0.01\n",
      "Epoch [17571/20000], Loss: -21.333938598632812, Learning Rate: 0.01\n",
      "Epoch [17572/20000], Loss: -21.33392333984375, Learning Rate: 0.01\n",
      "Epoch [17573/20000], Loss: -21.33404541015625, Learning Rate: 0.01\n",
      "Epoch [17574/20000], Loss: -21.333984375, Learning Rate: 0.01\n",
      "Epoch [17575/20000], Loss: -21.3341064453125, Learning Rate: 0.01\n",
      "Epoch [17576/20000], Loss: -21.33404541015625, Learning Rate: 0.01\n",
      "Epoch [17577/20000], Loss: -21.334075927734375, Learning Rate: 0.01\n",
      "Epoch [17578/20000], Loss: -21.334075927734375, Learning Rate: 0.01\n",
      "Epoch [17579/20000], Loss: -21.334060668945312, Learning Rate: 0.01\n",
      "Epoch [17580/20000], Loss: -21.334152221679688, Learning Rate: 0.01\n",
      "Epoch [17581/20000], Loss: -21.334182739257812, Learning Rate: 0.01\n",
      "Epoch [17582/20000], Loss: -21.334228515625, Learning Rate: 0.01\n",
      "Epoch [17583/20000], Loss: -21.334259033203125, Learning Rate: 0.01\n",
      "Epoch [17584/20000], Loss: -21.334274291992188, Learning Rate: 0.01\n",
      "Epoch [17585/20000], Loss: -21.334259033203125, Learning Rate: 0.01\n",
      "Epoch [17586/20000], Loss: -21.334304809570312, Learning Rate: 0.01\n",
      "Epoch [17587/20000], Loss: -21.334320068359375, Learning Rate: 0.01\n",
      "Epoch [17588/20000], Loss: -21.334320068359375, Learning Rate: 0.01\n",
      "Epoch [17589/20000], Loss: -21.334335327148438, Learning Rate: 0.01\n",
      "Epoch [17590/20000], Loss: -21.334381103515625, Learning Rate: 0.01\n",
      "Epoch [17591/20000], Loss: -21.33441162109375, Learning Rate: 0.01\n",
      "Epoch [17592/20000], Loss: -21.334320068359375, Learning Rate: 0.01\n",
      "Epoch [17593/20000], Loss: -21.334320068359375, Learning Rate: 0.01\n",
      "Epoch [17594/20000], Loss: -21.334259033203125, Learning Rate: 0.01\n",
      "Epoch [17595/20000], Loss: -21.334197998046875, Learning Rate: 0.01\n",
      "Epoch [17596/20000], Loss: -21.334091186523438, Learning Rate: 0.01\n",
      "Epoch [17597/20000], Loss: -21.333969116210938, Learning Rate: 0.01\n",
      "Epoch [17598/20000], Loss: -21.333770751953125, Learning Rate: 0.01\n",
      "Epoch [17599/20000], Loss: -21.333480834960938, Learning Rate: 0.01\n",
      "Epoch [17600/20000], Loss: -21.333053588867188, Learning Rate: 0.01\n",
      "Epoch [17601/20000], Loss: -21.332275390625, Learning Rate: 0.01\n",
      "Epoch [17602/20000], Loss: -21.3311767578125, Learning Rate: 0.01\n",
      "Epoch [17603/20000], Loss: -21.32958984375, Learning Rate: 0.01\n",
      "Epoch [17604/20000], Loss: -21.327178955078125, Learning Rate: 0.01\n",
      "Epoch [17605/20000], Loss: -21.32354736328125, Learning Rate: 0.01\n",
      "Epoch [17606/20000], Loss: -21.317886352539062, Learning Rate: 0.01\n",
      "Epoch [17607/20000], Loss: -21.3092041015625, Learning Rate: 0.01\n",
      "Epoch [17608/20000], Loss: -21.29583740234375, Learning Rate: 0.01\n",
      "Epoch [17609/20000], Loss: -21.274887084960938, Learning Rate: 0.01\n",
      "Epoch [17610/20000], Loss: -21.24249267578125, Learning Rate: 0.01\n",
      "Epoch [17611/20000], Loss: -21.191665649414062, Learning Rate: 0.01\n",
      "Epoch [17612/20000], Loss: -21.114151000976562, Learning Rate: 0.01\n",
      "Epoch [17613/20000], Loss: -20.993789672851562, Learning Rate: 0.01\n",
      "Epoch [17614/20000], Loss: -20.819564819335938, Learning Rate: 0.01\n",
      "Epoch [17615/20000], Loss: -20.565109252929688, Learning Rate: 0.01\n",
      "Epoch [17616/20000], Loss: -20.24798583984375, Learning Rate: 0.01\n",
      "Epoch [17617/20000], Loss: -19.869369506835938, Learning Rate: 0.01\n",
      "Epoch [17618/20000], Loss: -19.58746337890625, Learning Rate: 0.01\n",
      "Epoch [17619/20000], Loss: -19.491424560546875, Learning Rate: 0.01\n",
      "Epoch [17620/20000], Loss: -19.807632446289062, Learning Rate: 0.01\n",
      "Epoch [17621/20000], Loss: -20.396514892578125, Learning Rate: 0.01\n",
      "Epoch [17622/20000], Loss: -21.01837158203125, Learning Rate: 0.01\n",
      "Epoch [17623/20000], Loss: -21.320343017578125, Learning Rate: 0.01\n",
      "Epoch [17624/20000], Loss: -21.212631225585938, Learning Rate: 0.01\n",
      "Epoch [17625/20000], Loss: -20.885086059570312, Learning Rate: 0.01\n",
      "Epoch [17626/20000], Loss: -20.634078979492188, Learning Rate: 0.01\n",
      "Epoch [17627/20000], Loss: -20.676239013671875, Learning Rate: 0.01\n",
      "Epoch [17628/20000], Loss: -20.949127197265625, Learning Rate: 0.01\n",
      "Epoch [17629/20000], Loss: -21.2353515625, Learning Rate: 0.01\n",
      "Epoch [17630/20000], Loss: -21.320632934570312, Learning Rate: 0.01\n",
      "Epoch [17631/20000], Loss: -21.198272705078125, Learning Rate: 0.01\n",
      "Epoch [17632/20000], Loss: -21.033599853515625, Learning Rate: 0.01\n",
      "Epoch [17633/20000], Loss: -20.989700317382812, Learning Rate: 0.01\n",
      "Epoch [17634/20000], Loss: -21.103118896484375, Learning Rate: 0.01\n",
      "Epoch [17635/20000], Loss: -21.255783081054688, Learning Rate: 0.01\n",
      "Epoch [17636/20000], Loss: -21.321044921875, Learning Rate: 0.01\n",
      "Epoch [17637/20000], Loss: -21.271209716796875, Learning Rate: 0.01\n",
      "Epoch [17638/20000], Loss: -21.185638427734375, Learning Rate: 0.01\n",
      "Epoch [17639/20000], Loss: -21.159927368164062, Learning Rate: 0.01\n",
      "Epoch [17640/20000], Loss: -21.212432861328125, Learning Rate: 0.01\n",
      "Epoch [17641/20000], Loss: -21.289199829101562, Learning Rate: 0.01\n",
      "Epoch [17642/20000], Loss: -21.321792602539062, Learning Rate: 0.01\n",
      "Epoch [17643/20000], Loss: -21.297027587890625, Learning Rate: 0.01\n",
      "Epoch [17644/20000], Loss: -21.256103515625, Learning Rate: 0.01\n",
      "Epoch [17645/20000], Loss: -21.245086669921875, Learning Rate: 0.01\n",
      "Epoch [17646/20000], Loss: -21.273391723632812, Learning Rate: 0.01\n",
      "Epoch [17647/20000], Loss: -21.310501098632812, Learning Rate: 0.01\n",
      "Epoch [17648/20000], Loss: -21.324615478515625, Learning Rate: 0.01\n",
      "Epoch [17649/20000], Loss: -21.311309814453125, Learning Rate: 0.01\n",
      "Epoch [17650/20000], Loss: -21.291824340820312, Learning Rate: 0.01\n",
      "Epoch [17651/20000], Loss: -21.288787841796875, Learning Rate: 0.01\n",
      "Epoch [17652/20000], Loss: -21.304000854492188, Learning Rate: 0.01\n",
      "Epoch [17653/20000], Loss: -21.322341918945312, Learning Rate: 0.01\n",
      "Epoch [17654/20000], Loss: -21.32830810546875, Learning Rate: 0.01\n",
      "Epoch [17655/20000], Loss: -21.3206787109375, Learning Rate: 0.01\n",
      "Epoch [17656/20000], Loss: -21.311141967773438, Learning Rate: 0.01\n",
      "Epoch [17657/20000], Loss: -21.31024169921875, Learning Rate: 0.01\n",
      "Epoch [17658/20000], Loss: -21.31884765625, Learning Rate: 0.01\n",
      "Epoch [17659/20000], Loss: -21.328598022460938, Learning Rate: 0.01\n",
      "Epoch [17660/20000], Loss: -21.331634521484375, Learning Rate: 0.01\n",
      "Epoch [17661/20000], Loss: -21.327438354492188, Learning Rate: 0.01\n",
      "Epoch [17662/20000], Loss: -21.321853637695312, Learning Rate: 0.01\n",
      "Epoch [17663/20000], Loss: -21.321029663085938, Learning Rate: 0.01\n",
      "Epoch [17664/20000], Loss: -21.325637817382812, Learning Rate: 0.01\n",
      "Epoch [17665/20000], Loss: -21.331497192382812, Learning Rate: 0.01\n",
      "Epoch [17666/20000], Loss: -21.333984375, Learning Rate: 0.01\n",
      "Epoch [17667/20000], Loss: -21.332000732421875, Learning Rate: 0.01\n",
      "Epoch [17668/20000], Loss: -21.3284912109375, Learning Rate: 0.01\n",
      "Epoch [17669/20000], Loss: -21.327011108398438, Learning Rate: 0.01\n",
      "Epoch [17670/20000], Loss: -21.328872680664062, Learning Rate: 0.01\n",
      "Epoch [17671/20000], Loss: -21.332412719726562, Learning Rate: 0.01\n",
      "Epoch [17672/20000], Loss: -21.334732055664062, Learning Rate: 0.01\n",
      "Epoch [17673/20000], Loss: -21.334625244140625, Learning Rate: 0.01\n",
      "Epoch [17674/20000], Loss: -21.332763671875, Learning Rate: 0.01\n",
      "Epoch [17675/20000], Loss: -21.331069946289062, Learning Rate: 0.01\n",
      "Epoch [17676/20000], Loss: -21.331100463867188, Learning Rate: 0.01\n",
      "Epoch [17677/20000], Loss: -21.332717895507812, Learning Rate: 0.01\n",
      "Epoch [17678/20000], Loss: -21.3345947265625, Learning Rate: 0.01\n",
      "Epoch [17679/20000], Loss: -21.33544921875, Learning Rate: 0.01\n",
      "Epoch [17680/20000], Loss: -21.334976196289062, Learning Rate: 0.01\n",
      "Epoch [17681/20000], Loss: -21.333938598632812, Learning Rate: 0.01\n",
      "Epoch [17682/20000], Loss: -21.33319091796875, Learning Rate: 0.01\n",
      "Epoch [17683/20000], Loss: -21.333450317382812, Learning Rate: 0.01\n",
      "Epoch [17684/20000], Loss: -21.334274291992188, Learning Rate: 0.01\n",
      "Epoch [17685/20000], Loss: -21.335113525390625, Learning Rate: 0.01\n",
      "Epoch [17686/20000], Loss: -21.335556030273438, Learning Rate: 0.01\n",
      "Epoch [17687/20000], Loss: -21.335250854492188, Learning Rate: 0.01\n",
      "Epoch [17688/20000], Loss: -21.334808349609375, Learning Rate: 0.01\n",
      "Epoch [17689/20000], Loss: -21.334579467773438, Learning Rate: 0.01\n",
      "Epoch [17690/20000], Loss: -21.334609985351562, Learning Rate: 0.01\n",
      "Epoch [17691/20000], Loss: -21.334991455078125, Learning Rate: 0.01\n",
      "Epoch [17692/20000], Loss: -21.335372924804688, Learning Rate: 0.01\n",
      "Epoch [17693/20000], Loss: -21.335601806640625, Learning Rate: 0.01\n",
      "Epoch [17694/20000], Loss: -21.335601806640625, Learning Rate: 0.01\n",
      "Epoch [17695/20000], Loss: -21.335403442382812, Learning Rate: 0.01\n",
      "Epoch [17696/20000], Loss: -21.335220336914062, Learning Rate: 0.01\n",
      "Epoch [17697/20000], Loss: -21.33526611328125, Learning Rate: 0.01\n",
      "Epoch [17698/20000], Loss: -21.335479736328125, Learning Rate: 0.01\n",
      "Epoch [17699/20000], Loss: -21.335617065429688, Learning Rate: 0.01\n",
      "Epoch [17700/20000], Loss: -21.335800170898438, Learning Rate: 0.01\n",
      "Epoch [17701/20000], Loss: -21.335800170898438, Learning Rate: 0.01\n",
      "Epoch [17702/20000], Loss: -21.335678100585938, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17703/20000], Loss: -21.335662841796875, Learning Rate: 0.01\n",
      "Epoch [17704/20000], Loss: -21.335693359375, Learning Rate: 0.01\n",
      "Epoch [17705/20000], Loss: -21.335784912109375, Learning Rate: 0.01\n",
      "Epoch [17706/20000], Loss: -21.335830688476562, Learning Rate: 0.01\n",
      "Epoch [17707/20000], Loss: -21.3359375, Learning Rate: 0.01\n",
      "Epoch [17708/20000], Loss: -21.33599853515625, Learning Rate: 0.01\n",
      "Epoch [17709/20000], Loss: -21.335983276367188, Learning Rate: 0.01\n",
      "Epoch [17710/20000], Loss: -21.335906982421875, Learning Rate: 0.01\n",
      "Epoch [17711/20000], Loss: -21.335952758789062, Learning Rate: 0.01\n",
      "Epoch [17712/20000], Loss: -21.335891723632812, Learning Rate: 0.01\n",
      "Epoch [17713/20000], Loss: -21.335952758789062, Learning Rate: 0.01\n",
      "Epoch [17714/20000], Loss: -21.3360595703125, Learning Rate: 0.01\n",
      "Epoch [17715/20000], Loss: -21.336074829101562, Learning Rate: 0.01\n",
      "Epoch [17716/20000], Loss: -21.33612060546875, Learning Rate: 0.01\n",
      "Epoch [17717/20000], Loss: -21.336135864257812, Learning Rate: 0.01\n",
      "Epoch [17718/20000], Loss: -21.336181640625, Learning Rate: 0.01\n",
      "Epoch [17719/20000], Loss: -21.336151123046875, Learning Rate: 0.01\n",
      "Epoch [17720/20000], Loss: -21.336135864257812, Learning Rate: 0.01\n",
      "Epoch [17721/20000], Loss: -21.336135864257812, Learning Rate: 0.01\n",
      "Epoch [17722/20000], Loss: -21.336166381835938, Learning Rate: 0.01\n",
      "Epoch [17723/20000], Loss: -21.336227416992188, Learning Rate: 0.01\n",
      "Epoch [17724/20000], Loss: -21.336273193359375, Learning Rate: 0.01\n",
      "Epoch [17725/20000], Loss: -21.336318969726562, Learning Rate: 0.01\n",
      "Epoch [17726/20000], Loss: -21.336334228515625, Learning Rate: 0.01\n",
      "Epoch [17727/20000], Loss: -21.336349487304688, Learning Rate: 0.01\n",
      "Epoch [17728/20000], Loss: -21.336349487304688, Learning Rate: 0.01\n",
      "Epoch [17729/20000], Loss: -21.336334228515625, Learning Rate: 0.01\n",
      "Epoch [17730/20000], Loss: -21.336380004882812, Learning Rate: 0.01\n",
      "Epoch [17731/20000], Loss: -21.336410522460938, Learning Rate: 0.01\n",
      "Epoch [17732/20000], Loss: -21.336441040039062, Learning Rate: 0.01\n",
      "Epoch [17733/20000], Loss: -21.33648681640625, Learning Rate: 0.01\n",
      "Epoch [17734/20000], Loss: -21.336502075195312, Learning Rate: 0.01\n",
      "Epoch [17735/20000], Loss: -21.33648681640625, Learning Rate: 0.01\n",
      "Epoch [17736/20000], Loss: -21.336517333984375, Learning Rate: 0.01\n",
      "Epoch [17737/20000], Loss: -21.336532592773438, Learning Rate: 0.01\n",
      "Epoch [17738/20000], Loss: -21.336593627929688, Learning Rate: 0.01\n",
      "Epoch [17739/20000], Loss: -21.336532592773438, Learning Rate: 0.01\n",
      "Epoch [17740/20000], Loss: -21.336517333984375, Learning Rate: 0.01\n",
      "Epoch [17741/20000], Loss: -21.336624145507812, Learning Rate: 0.01\n",
      "Epoch [17742/20000], Loss: -21.336669921875, Learning Rate: 0.01\n",
      "Epoch [17743/20000], Loss: -21.336654663085938, Learning Rate: 0.01\n",
      "Epoch [17744/20000], Loss: -21.336715698242188, Learning Rate: 0.01\n",
      "Epoch [17745/20000], Loss: -21.336685180664062, Learning Rate: 0.01\n",
      "Epoch [17746/20000], Loss: -21.336700439453125, Learning Rate: 0.01\n",
      "Epoch [17747/20000], Loss: -21.336685180664062, Learning Rate: 0.01\n",
      "Epoch [17748/20000], Loss: -21.336822509765625, Learning Rate: 0.01\n",
      "Epoch [17749/20000], Loss: -21.336807250976562, Learning Rate: 0.01\n",
      "Epoch [17750/20000], Loss: -21.3367919921875, Learning Rate: 0.01\n",
      "Epoch [17751/20000], Loss: -21.336807250976562, Learning Rate: 0.01\n",
      "Epoch [17752/20000], Loss: -21.336837768554688, Learning Rate: 0.01\n",
      "Epoch [17753/20000], Loss: -21.336883544921875, Learning Rate: 0.01\n",
      "Epoch [17754/20000], Loss: -21.336868286132812, Learning Rate: 0.01\n",
      "Epoch [17755/20000], Loss: -21.3369140625, Learning Rate: 0.01\n",
      "Epoch [17756/20000], Loss: -21.336898803710938, Learning Rate: 0.01\n",
      "Epoch [17757/20000], Loss: -21.33697509765625, Learning Rate: 0.01\n",
      "Epoch [17758/20000], Loss: -21.336944580078125, Learning Rate: 0.01\n",
      "Epoch [17759/20000], Loss: -21.336944580078125, Learning Rate: 0.01\n",
      "Epoch [17760/20000], Loss: -21.336959838867188, Learning Rate: 0.01\n",
      "Epoch [17761/20000], Loss: -21.337020874023438, Learning Rate: 0.01\n",
      "Epoch [17762/20000], Loss: -21.33697509765625, Learning Rate: 0.01\n",
      "Epoch [17763/20000], Loss: -21.337020874023438, Learning Rate: 0.01\n",
      "Epoch [17764/20000], Loss: -21.337020874023438, Learning Rate: 0.01\n",
      "Epoch [17765/20000], Loss: -21.337051391601562, Learning Rate: 0.01\n",
      "Epoch [17766/20000], Loss: -21.337020874023438, Learning Rate: 0.01\n",
      "Epoch [17767/20000], Loss: -21.337127685546875, Learning Rate: 0.01\n",
      "Epoch [17768/20000], Loss: -21.337081909179688, Learning Rate: 0.01\n",
      "Epoch [17769/20000], Loss: -21.337112426757812, Learning Rate: 0.01\n",
      "Epoch [17770/20000], Loss: -21.33709716796875, Learning Rate: 0.01\n",
      "Epoch [17771/20000], Loss: -21.337127685546875, Learning Rate: 0.01\n",
      "Epoch [17772/20000], Loss: -21.337173461914062, Learning Rate: 0.01\n",
      "Epoch [17773/20000], Loss: -21.337127685546875, Learning Rate: 0.01\n",
      "Epoch [17774/20000], Loss: -21.337173461914062, Learning Rate: 0.01\n",
      "Epoch [17775/20000], Loss: -21.337188720703125, Learning Rate: 0.01\n",
      "Epoch [17776/20000], Loss: -21.337112426757812, Learning Rate: 0.01\n",
      "Epoch [17777/20000], Loss: -21.337142944335938, Learning Rate: 0.01\n",
      "Epoch [17778/20000], Loss: -21.337081909179688, Learning Rate: 0.01\n",
      "Epoch [17779/20000], Loss: -21.337005615234375, Learning Rate: 0.01\n",
      "Epoch [17780/20000], Loss: -21.336883544921875, Learning Rate: 0.01\n",
      "Epoch [17781/20000], Loss: -21.33673095703125, Learning Rate: 0.01\n",
      "Epoch [17782/20000], Loss: -21.3363037109375, Learning Rate: 0.01\n",
      "Epoch [17783/20000], Loss: -21.33587646484375, Learning Rate: 0.01\n",
      "Epoch [17784/20000], Loss: -21.335098266601562, Learning Rate: 0.01\n",
      "Epoch [17785/20000], Loss: -21.33392333984375, Learning Rate: 0.01\n",
      "Epoch [17786/20000], Loss: -21.3319091796875, Learning Rate: 0.01\n",
      "Epoch [17787/20000], Loss: -21.32879638671875, Learning Rate: 0.01\n",
      "Epoch [17788/20000], Loss: -21.323745727539062, Learning Rate: 0.01\n",
      "Epoch [17789/20000], Loss: -21.315673828125, Learning Rate: 0.01\n",
      "Epoch [17790/20000], Loss: -21.302520751953125, Learning Rate: 0.01\n",
      "Epoch [17791/20000], Loss: -21.281143188476562, Learning Rate: 0.01\n",
      "Epoch [17792/20000], Loss: -21.24609375, Learning Rate: 0.01\n",
      "Epoch [17793/20000], Loss: -21.189407348632812, Learning Rate: 0.01\n",
      "Epoch [17794/20000], Loss: -21.09686279296875, Learning Rate: 0.01\n",
      "Epoch [17795/20000], Loss: -20.950775146484375, Learning Rate: 0.01\n",
      "Epoch [17796/20000], Loss: -20.721649169921875, Learning Rate: 0.01\n",
      "Epoch [17797/20000], Loss: -20.391525268554688, Learning Rate: 0.01\n",
      "Epoch [17798/20000], Loss: -19.940628051757812, Learning Rate: 0.01\n",
      "Epoch [17799/20000], Loss: -19.457534790039062, Learning Rate: 0.01\n",
      "Epoch [17800/20000], Loss: -19.064987182617188, Learning Rate: 0.01\n",
      "Epoch [17801/20000], Loss: -19.084564208984375, Learning Rate: 0.01\n",
      "Epoch [17802/20000], Loss: -19.5618896484375, Learning Rate: 0.01\n",
      "Epoch [17803/20000], Loss: -20.35552978515625, Learning Rate: 0.01\n",
      "Epoch [17804/20000], Loss: -20.940933227539062, Learning Rate: 0.01\n",
      "Epoch [17805/20000], Loss: -21.037460327148438, Learning Rate: 0.01\n",
      "Epoch [17806/20000], Loss: -20.770233154296875, Learning Rate: 0.01\n",
      "Epoch [17807/20000], Loss: -20.5487060546875, Learning Rate: 0.01\n",
      "Epoch [17808/20000], Loss: -20.651412963867188, Learning Rate: 0.01\n",
      "Epoch [17809/20000], Loss: -20.9520263671875, Learning Rate: 0.01\n",
      "Epoch [17810/20000], Loss: -21.125015258789062, Learning Rate: 0.01\n",
      "Epoch [17811/20000], Loss: -21.02777099609375, Learning Rate: 0.01\n",
      "Epoch [17812/20000], Loss: -20.865478515625, Learning Rate: 0.01\n",
      "Epoch [17813/20000], Loss: -20.897705078125, Learning Rate: 0.01\n",
      "Epoch [17814/20000], Loss: -21.128662109375, Learning Rate: 0.01\n",
      "Epoch [17815/20000], Loss: -21.298110961914062, Learning Rate: 0.01\n",
      "Epoch [17816/20000], Loss: -21.233642578125, Learning Rate: 0.01\n",
      "Epoch [17817/20000], Loss: -21.060531616210938, Learning Rate: 0.01\n",
      "Epoch [17818/20000], Loss: -21.015579223632812, Learning Rate: 0.01\n",
      "Epoch [17819/20000], Loss: -21.158981323242188, Learning Rate: 0.01\n",
      "Epoch [17820/20000], Loss: -21.311355590820312, Learning Rate: 0.01\n",
      "Epoch [17821/20000], Loss: -21.31353759765625, Learning Rate: 0.01\n",
      "Epoch [17822/20000], Loss: -21.211990356445312, Learning Rate: 0.01\n",
      "Epoch [17823/20000], Loss: -21.163528442382812, Learning Rate: 0.01\n",
      "Epoch [17824/20000], Loss: -21.223907470703125, Learning Rate: 0.01\n",
      "Epoch [17825/20000], Loss: -21.300140380859375, Learning Rate: 0.01\n",
      "Epoch [17826/20000], Loss: -21.302520751953125, Learning Rate: 0.01\n",
      "Epoch [17827/20000], Loss: -21.256973266601562, Learning Rate: 0.01\n",
      "Epoch [17828/20000], Loss: -21.246627807617188, Learning Rate: 0.01\n",
      "Epoch [17829/20000], Loss: -21.289291381835938, Learning Rate: 0.01\n",
      "Epoch [17830/20000], Loss: -21.325424194335938, Learning Rate: 0.01\n",
      "Epoch [17831/20000], Loss: -21.311355590820312, Learning Rate: 0.01\n",
      "Epoch [17832/20000], Loss: -21.276397705078125, Learning Rate: 0.01\n",
      "Epoch [17833/20000], Loss: -21.273696899414062, Learning Rate: 0.01\n",
      "Epoch [17834/20000], Loss: -21.307418823242188, Learning Rate: 0.01\n",
      "Epoch [17835/20000], Loss: -21.335098266601562, Learning Rate: 0.01\n",
      "Epoch [17836/20000], Loss: -21.328445434570312, Learning Rate: 0.01\n",
      "Epoch [17837/20000], Loss: -21.305679321289062, Learning Rate: 0.01\n",
      "Epoch [17838/20000], Loss: -21.2998046875, Learning Rate: 0.01\n",
      "Epoch [17839/20000], Loss: -21.315567016601562, Learning Rate: 0.01\n",
      "Epoch [17840/20000], Loss: -21.330307006835938, Learning Rate: 0.01\n",
      "Epoch [17841/20000], Loss: -21.328384399414062, Learning Rate: 0.01\n",
      "Epoch [17842/20000], Loss: -21.318893432617188, Learning Rate: 0.01\n",
      "Epoch [17843/20000], Loss: -21.318344116210938, Learning Rate: 0.01\n",
      "Epoch [17844/20000], Loss: -21.327911376953125, Learning Rate: 0.01\n",
      "Epoch [17845/20000], Loss: -21.3345947265625, Learning Rate: 0.01\n",
      "Epoch [17846/20000], Loss: -21.3310546875, Learning Rate: 0.01\n",
      "Epoch [17847/20000], Loss: -21.32379150390625, Learning Rate: 0.01\n",
      "Epoch [17848/20000], Loss: -21.323318481445312, Learning Rate: 0.01\n",
      "Epoch [17849/20000], Loss: -21.3302001953125, Learning Rate: 0.01\n",
      "Epoch [17850/20000], Loss: -21.33642578125, Learning Rate: 0.01\n",
      "Epoch [17851/20000], Loss: -21.33575439453125, Learning Rate: 0.01\n",
      "Epoch [17852/20000], Loss: -21.331222534179688, Learning Rate: 0.01\n",
      "Epoch [17853/20000], Loss: -21.329360961914062, Learning Rate: 0.01\n",
      "Epoch [17854/20000], Loss: -21.331939697265625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17855/20000], Loss: -21.335296630859375, Learning Rate: 0.01\n",
      "Epoch [17856/20000], Loss: -21.335525512695312, Learning Rate: 0.01\n",
      "Epoch [17857/20000], Loss: -21.333663940429688, Learning Rate: 0.01\n",
      "Epoch [17858/20000], Loss: -21.33294677734375, Learning Rate: 0.01\n",
      "Epoch [17859/20000], Loss: -21.334579467773438, Learning Rate: 0.01\n",
      "Epoch [17860/20000], Loss: -21.336669921875, Learning Rate: 0.01\n",
      "Epoch [17861/20000], Loss: -21.336959838867188, Learning Rate: 0.01\n",
      "Epoch [17862/20000], Loss: -21.335433959960938, Learning Rate: 0.01\n",
      "Epoch [17863/20000], Loss: -21.3343505859375, Learning Rate: 0.01\n",
      "Epoch [17864/20000], Loss: -21.334869384765625, Learning Rate: 0.01\n",
      "Epoch [17865/20000], Loss: -21.336349487304688, Learning Rate: 0.01\n",
      "Epoch [17866/20000], Loss: -21.337295532226562, Learning Rate: 0.01\n",
      "Epoch [17867/20000], Loss: -21.3369140625, Learning Rate: 0.01\n",
      "Epoch [17868/20000], Loss: -21.336196899414062, Learning Rate: 0.01\n",
      "Epoch [17869/20000], Loss: -21.33612060546875, Learning Rate: 0.01\n",
      "Epoch [17870/20000], Loss: -21.336761474609375, Learning Rate: 0.01\n",
      "Epoch [17871/20000], Loss: -21.337295532226562, Learning Rate: 0.01\n",
      "Epoch [17872/20000], Loss: -21.337142944335938, Learning Rate: 0.01\n",
      "Epoch [17873/20000], Loss: -21.336746215820312, Learning Rate: 0.01\n",
      "Epoch [17874/20000], Loss: -21.336563110351562, Learning Rate: 0.01\n",
      "Epoch [17875/20000], Loss: -21.33685302734375, Learning Rate: 0.01\n",
      "Epoch [17876/20000], Loss: -21.337417602539062, Learning Rate: 0.01\n",
      "Epoch [17877/20000], Loss: -21.337646484375, Learning Rate: 0.01\n",
      "Epoch [17878/20000], Loss: -21.337509155273438, Learning Rate: 0.01\n",
      "Epoch [17879/20000], Loss: -21.33721923828125, Learning Rate: 0.01\n",
      "Epoch [17880/20000], Loss: -21.337295532226562, Learning Rate: 0.01\n",
      "Epoch [17881/20000], Loss: -21.337432861328125, Learning Rate: 0.01\n",
      "Epoch [17882/20000], Loss: -21.337646484375, Learning Rate: 0.01\n",
      "Epoch [17883/20000], Loss: -21.337646484375, Learning Rate: 0.01\n",
      "Epoch [17884/20000], Loss: -21.337570190429688, Learning Rate: 0.01\n",
      "Epoch [17885/20000], Loss: -21.337493896484375, Learning Rate: 0.01\n",
      "Epoch [17886/20000], Loss: -21.337554931640625, Learning Rate: 0.01\n",
      "Epoch [17887/20000], Loss: -21.337753295898438, Learning Rate: 0.01\n",
      "Epoch [17888/20000], Loss: -21.337905883789062, Learning Rate: 0.01\n",
      "Epoch [17889/20000], Loss: -21.337875366210938, Learning Rate: 0.01\n",
      "Epoch [17890/20000], Loss: -21.33782958984375, Learning Rate: 0.01\n",
      "Epoch [17891/20000], Loss: -21.337814331054688, Learning Rate: 0.01\n",
      "Epoch [17892/20000], Loss: -21.337844848632812, Learning Rate: 0.01\n",
      "Epoch [17893/20000], Loss: -21.337905883789062, Learning Rate: 0.01\n",
      "Epoch [17894/20000], Loss: -21.337875366210938, Learning Rate: 0.01\n",
      "Epoch [17895/20000], Loss: -21.337997436523438, Learning Rate: 0.01\n",
      "Epoch [17896/20000], Loss: -21.338058471679688, Learning Rate: 0.01\n",
      "Epoch [17897/20000], Loss: -21.337982177734375, Learning Rate: 0.01\n",
      "Epoch [17898/20000], Loss: -21.337997436523438, Learning Rate: 0.01\n",
      "Epoch [17899/20000], Loss: -21.33819580078125, Learning Rate: 0.01\n",
      "Epoch [17900/20000], Loss: -21.338165283203125, Learning Rate: 0.01\n",
      "Epoch [17901/20000], Loss: -21.338119506835938, Learning Rate: 0.01\n",
      "Epoch [17902/20000], Loss: -21.338104248046875, Learning Rate: 0.01\n",
      "Epoch [17903/20000], Loss: -21.338165283203125, Learning Rate: 0.01\n",
      "Epoch [17904/20000], Loss: -21.338180541992188, Learning Rate: 0.01\n",
      "Epoch [17905/20000], Loss: -21.338119506835938, Learning Rate: 0.01\n",
      "Epoch [17906/20000], Loss: -21.338211059570312, Learning Rate: 0.01\n",
      "Epoch [17907/20000], Loss: -21.338165283203125, Learning Rate: 0.01\n",
      "Epoch [17908/20000], Loss: -21.338287353515625, Learning Rate: 0.01\n",
      "Epoch [17909/20000], Loss: -21.338226318359375, Learning Rate: 0.01\n",
      "Epoch [17910/20000], Loss: -21.338302612304688, Learning Rate: 0.01\n",
      "Epoch [17911/20000], Loss: -21.338348388671875, Learning Rate: 0.01\n",
      "Epoch [17912/20000], Loss: -21.338287353515625, Learning Rate: 0.01\n",
      "Epoch [17913/20000], Loss: -21.338333129882812, Learning Rate: 0.01\n",
      "Epoch [17914/20000], Loss: -21.338363647460938, Learning Rate: 0.01\n",
      "Epoch [17915/20000], Loss: -21.33837890625, Learning Rate: 0.01\n",
      "Epoch [17916/20000], Loss: -21.33843994140625, Learning Rate: 0.01\n",
      "Epoch [17917/20000], Loss: -21.3385009765625, Learning Rate: 0.01\n",
      "Epoch [17918/20000], Loss: -21.338394165039062, Learning Rate: 0.01\n",
      "Epoch [17919/20000], Loss: -21.338455200195312, Learning Rate: 0.01\n",
      "Epoch [17920/20000], Loss: -21.338546752929688, Learning Rate: 0.01\n",
      "Epoch [17921/20000], Loss: -21.338516235351562, Learning Rate: 0.01\n",
      "Epoch [17922/20000], Loss: -21.338531494140625, Learning Rate: 0.01\n",
      "Epoch [17923/20000], Loss: -21.338531494140625, Learning Rate: 0.01\n",
      "Epoch [17924/20000], Loss: -21.338531494140625, Learning Rate: 0.01\n",
      "Epoch [17925/20000], Loss: -21.338577270507812, Learning Rate: 0.01\n",
      "Epoch [17926/20000], Loss: -21.338546752929688, Learning Rate: 0.01\n",
      "Epoch [17927/20000], Loss: -21.338653564453125, Learning Rate: 0.01\n",
      "Epoch [17928/20000], Loss: -21.338653564453125, Learning Rate: 0.01\n",
      "Epoch [17929/20000], Loss: -21.338668823242188, Learning Rate: 0.01\n",
      "Epoch [17930/20000], Loss: -21.33868408203125, Learning Rate: 0.01\n",
      "Epoch [17931/20000], Loss: -21.33868408203125, Learning Rate: 0.01\n",
      "Epoch [17932/20000], Loss: -21.338729858398438, Learning Rate: 0.01\n",
      "Epoch [17933/20000], Loss: -21.338729858398438, Learning Rate: 0.01\n",
      "Epoch [17934/20000], Loss: -21.33880615234375, Learning Rate: 0.01\n",
      "Epoch [17935/20000], Loss: -21.338775634765625, Learning Rate: 0.01\n",
      "Epoch [17936/20000], Loss: -21.338775634765625, Learning Rate: 0.01\n",
      "Epoch [17937/20000], Loss: -21.338775634765625, Learning Rate: 0.01\n",
      "Epoch [17938/20000], Loss: -21.338897705078125, Learning Rate: 0.01\n",
      "Epoch [17939/20000], Loss: -21.338912963867188, Learning Rate: 0.01\n",
      "Epoch [17940/20000], Loss: -21.338836669921875, Learning Rate: 0.01\n",
      "Epoch [17941/20000], Loss: -21.338943481445312, Learning Rate: 0.01\n",
      "Epoch [17942/20000], Loss: -21.338958740234375, Learning Rate: 0.01\n",
      "Epoch [17943/20000], Loss: -21.338943481445312, Learning Rate: 0.01\n",
      "Epoch [17944/20000], Loss: -21.33892822265625, Learning Rate: 0.01\n",
      "Epoch [17945/20000], Loss: -21.339004516601562, Learning Rate: 0.01\n",
      "Epoch [17946/20000], Loss: -21.339080810546875, Learning Rate: 0.01\n",
      "Epoch [17947/20000], Loss: -21.339035034179688, Learning Rate: 0.01\n",
      "Epoch [17948/20000], Loss: -21.33905029296875, Learning Rate: 0.01\n",
      "Epoch [17949/20000], Loss: -21.339065551757812, Learning Rate: 0.01\n",
      "Epoch [17950/20000], Loss: -21.339080810546875, Learning Rate: 0.01\n",
      "Epoch [17951/20000], Loss: -21.339096069335938, Learning Rate: 0.01\n",
      "Epoch [17952/20000], Loss: -21.339157104492188, Learning Rate: 0.01\n",
      "Epoch [17953/20000], Loss: -21.339187622070312, Learning Rate: 0.01\n",
      "Epoch [17954/20000], Loss: -21.339187622070312, Learning Rate: 0.01\n",
      "Epoch [17955/20000], Loss: -21.33917236328125, Learning Rate: 0.01\n",
      "Epoch [17956/20000], Loss: -21.339111328125, Learning Rate: 0.01\n",
      "Epoch [17957/20000], Loss: -21.339202880859375, Learning Rate: 0.01\n",
      "Epoch [17958/20000], Loss: -21.339248657226562, Learning Rate: 0.01\n",
      "Epoch [17959/20000], Loss: -21.339187622070312, Learning Rate: 0.01\n",
      "Epoch [17960/20000], Loss: -21.339248657226562, Learning Rate: 0.01\n",
      "Epoch [17961/20000], Loss: -21.339248657226562, Learning Rate: 0.01\n",
      "Epoch [17962/20000], Loss: -21.339263916015625, Learning Rate: 0.01\n",
      "Epoch [17963/20000], Loss: -21.33929443359375, Learning Rate: 0.01\n",
      "Epoch [17964/20000], Loss: -21.339202880859375, Learning Rate: 0.01\n",
      "Epoch [17965/20000], Loss: -21.339218139648438, Learning Rate: 0.01\n",
      "Epoch [17966/20000], Loss: -21.339096069335938, Learning Rate: 0.01\n",
      "Epoch [17967/20000], Loss: -21.3388671875, Learning Rate: 0.01\n",
      "Epoch [17968/20000], Loss: -21.338729858398438, Learning Rate: 0.01\n",
      "Epoch [17969/20000], Loss: -21.33819580078125, Learning Rate: 0.01\n",
      "Epoch [17970/20000], Loss: -21.337249755859375, Learning Rate: 0.01\n",
      "Epoch [17971/20000], Loss: -21.335617065429688, Learning Rate: 0.01\n",
      "Epoch [17972/20000], Loss: -21.332763671875, Learning Rate: 0.01\n",
      "Epoch [17973/20000], Loss: -21.327499389648438, Learning Rate: 0.01\n",
      "Epoch [17974/20000], Loss: -21.3179931640625, Learning Rate: 0.01\n",
      "Epoch [17975/20000], Loss: -21.300735473632812, Learning Rate: 0.01\n",
      "Epoch [17976/20000], Loss: -21.26922607421875, Learning Rate: 0.01\n",
      "Epoch [17977/20000], Loss: -21.2115478515625, Learning Rate: 0.01\n",
      "Epoch [17978/20000], Loss: -21.107666015625, Learning Rate: 0.01\n",
      "Epoch [17979/20000], Loss: -20.9176025390625, Learning Rate: 0.01\n",
      "Epoch [17980/20000], Loss: -20.589752197265625, Learning Rate: 0.01\n",
      "Epoch [17981/20000], Loss: -20.027206420898438, Learning Rate: 0.01\n",
      "Epoch [17982/20000], Loss: -19.208602905273438, Learning Rate: 0.01\n",
      "Epoch [17983/20000], Loss: -18.140182495117188, Learning Rate: 0.01\n",
      "Epoch [17984/20000], Loss: -17.4361572265625, Learning Rate: 0.01\n",
      "Epoch [17985/20000], Loss: -17.630783081054688, Learning Rate: 0.01\n",
      "Epoch [17986/20000], Loss: -19.2265625, Learning Rate: 0.01\n",
      "Epoch [17987/20000], Loss: -20.897796630859375, Learning Rate: 0.01\n",
      "Epoch [17988/20000], Loss: -21.267715454101562, Learning Rate: 0.01\n",
      "Epoch [17989/20000], Loss: -20.3514404296875, Learning Rate: 0.01\n",
      "Epoch [17990/20000], Loss: -19.515518188476562, Learning Rate: 0.01\n",
      "Epoch [17991/20000], Loss: -19.91229248046875, Learning Rate: 0.01\n",
      "Epoch [17992/20000], Loss: -20.946060180664062, Learning Rate: 0.01\n",
      "Epoch [17993/20000], Loss: -21.305694580078125, Learning Rate: 0.01\n",
      "Epoch [17994/20000], Loss: -20.73931884765625, Learning Rate: 0.01\n",
      "Epoch [17995/20000], Loss: -20.283447265625, Learning Rate: 0.01\n",
      "Epoch [17996/20000], Loss: -20.666488647460938, Learning Rate: 0.01\n",
      "Epoch [17997/20000], Loss: -21.252914428710938, Learning Rate: 0.01\n",
      "Epoch [17998/20000], Loss: -21.22515869140625, Learning Rate: 0.01\n",
      "Epoch [17999/20000], Loss: -20.804901123046875, Learning Rate: 0.01\n",
      "Epoch [18000/20000], Loss: -20.749771118164062, Learning Rate: 0.01\n",
      "Epoch [18001/20000], Loss: -21.139190673828125, Learning Rate: 0.01\n",
      "Epoch [18002/20000], Loss: -21.334732055664062, Learning Rate: 0.01\n",
      "Epoch [18003/20000], Loss: -21.114761352539062, Learning Rate: 0.01\n",
      "Epoch [18004/20000], Loss: -20.94085693359375, Learning Rate: 0.01\n",
      "Epoch [18005/20000], Loss: -21.108978271484375, Learning Rate: 0.01\n",
      "Epoch [18006/20000], Loss: -21.3255615234375, Learning Rate: 0.01\n",
      "Epoch [18007/20000], Loss: -21.265335083007812, Learning Rate: 0.01\n",
      "Epoch [18008/20000], Loss: -21.10186767578125, Learning Rate: 0.01\n",
      "Epoch [18009/20000], Loss: -21.139663696289062, Learning Rate: 0.01\n",
      "Epoch [18010/20000], Loss: -21.299407958984375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18011/20000], Loss: -21.322036743164062, Learning Rate: 0.01\n",
      "Epoch [18012/20000], Loss: -21.214447021484375, Learning Rate: 0.01\n",
      "Epoch [18013/20000], Loss: -21.188507080078125, Learning Rate: 0.01\n",
      "Epoch [18014/20000], Loss: -21.285202026367188, Learning Rate: 0.01\n",
      "Epoch [18015/20000], Loss: -21.33721923828125, Learning Rate: 0.01\n",
      "Epoch [18016/20000], Loss: -21.280593872070312, Learning Rate: 0.01\n",
      "Epoch [18017/20000], Loss: -21.237075805664062, Learning Rate: 0.01\n",
      "Epoch [18018/20000], Loss: -21.282821655273438, Learning Rate: 0.01\n",
      "Epoch [18019/20000], Loss: -21.336212158203125, Learning Rate: 0.01\n",
      "Epoch [18020/20000], Loss: -21.317489624023438, Learning Rate: 0.01\n",
      "Epoch [18021/20000], Loss: -21.27642822265625, Learning Rate: 0.01\n",
      "Epoch [18022/20000], Loss: -21.288711547851562, Learning Rate: 0.01\n",
      "Epoch [18023/20000], Loss: -21.329864501953125, Learning Rate: 0.01\n",
      "Epoch [18024/20000], Loss: -21.333908081054688, Learning Rate: 0.01\n",
      "Epoch [18025/20000], Loss: -21.305923461914062, Learning Rate: 0.01\n",
      "Epoch [18026/20000], Loss: -21.299972534179688, Learning Rate: 0.01\n",
      "Epoch [18027/20000], Loss: -21.324752807617188, Learning Rate: 0.01\n",
      "Epoch [18028/20000], Loss: -21.33837890625, Learning Rate: 0.01\n",
      "Epoch [18029/20000], Loss: -21.324462890625, Learning Rate: 0.01\n",
      "Epoch [18030/20000], Loss: -21.312484741210938, Learning Rate: 0.01\n",
      "Epoch [18031/20000], Loss: -21.323257446289062, Learning Rate: 0.01\n",
      "Epoch [18032/20000], Loss: -21.337615966796875, Learning Rate: 0.01\n",
      "Epoch [18033/20000], Loss: -21.3343505859375, Learning Rate: 0.01\n",
      "Epoch [18034/20000], Loss: -21.323501586914062, Learning Rate: 0.01\n",
      "Epoch [18035/20000], Loss: -21.324783325195312, Learning Rate: 0.01\n",
      "Epoch [18036/20000], Loss: -21.335220336914062, Learning Rate: 0.01\n",
      "Epoch [18037/20000], Loss: -21.338348388671875, Learning Rate: 0.01\n",
      "Epoch [18038/20000], Loss: -21.331832885742188, Learning Rate: 0.01\n",
      "Epoch [18039/20000], Loss: -21.328384399414062, Learning Rate: 0.01\n",
      "Epoch [18040/20000], Loss: -21.333572387695312, Learning Rate: 0.01\n",
      "Epoch [18041/20000], Loss: -21.338699340820312, Learning Rate: 0.01\n",
      "Epoch [18042/20000], Loss: -21.336578369140625, Learning Rate: 0.01\n",
      "Epoch [18043/20000], Loss: -21.3326416015625, Learning Rate: 0.01\n",
      "Epoch [18044/20000], Loss: -21.333572387695312, Learning Rate: 0.01\n",
      "Epoch [18045/20000], Loss: -21.33770751953125, Learning Rate: 0.01\n",
      "Epoch [18046/20000], Loss: -21.338668823242188, Learning Rate: 0.01\n",
      "Epoch [18047/20000], Loss: -21.336105346679688, Learning Rate: 0.01\n",
      "Epoch [18048/20000], Loss: -21.334762573242188, Learning Rate: 0.01\n",
      "Epoch [18049/20000], Loss: -21.33685302734375, Learning Rate: 0.01\n",
      "Epoch [18050/20000], Loss: -21.3388671875, Learning Rate: 0.01\n",
      "Epoch [18051/20000], Loss: -21.33819580078125, Learning Rate: 0.01\n",
      "Epoch [18052/20000], Loss: -21.336578369140625, Learning Rate: 0.01\n",
      "Epoch [18053/20000], Loss: -21.336746215820312, Learning Rate: 0.01\n",
      "Epoch [18054/20000], Loss: -21.338424682617188, Learning Rate: 0.01\n",
      "Epoch [18055/20000], Loss: -21.339111328125, Learning Rate: 0.01\n",
      "Epoch [18056/20000], Loss: -21.338165283203125, Learning Rate: 0.01\n",
      "Epoch [18057/20000], Loss: -21.337554931640625, Learning Rate: 0.01\n",
      "Epoch [18058/20000], Loss: -21.338088989257812, Learning Rate: 0.01\n",
      "Epoch [18059/20000], Loss: -21.33905029296875, Learning Rate: 0.01\n",
      "Epoch [18060/20000], Loss: -21.339004516601562, Learning Rate: 0.01\n",
      "Epoch [18061/20000], Loss: -21.33831787109375, Learning Rate: 0.01\n",
      "Epoch [18062/20000], Loss: -21.338241577148438, Learning Rate: 0.01\n",
      "Epoch [18063/20000], Loss: -21.338821411132812, Learning Rate: 0.01\n",
      "Epoch [18064/20000], Loss: -21.339218139648438, Learning Rate: 0.01\n",
      "Epoch [18065/20000], Loss: -21.33905029296875, Learning Rate: 0.01\n",
      "Epoch [18066/20000], Loss: -21.338638305664062, Learning Rate: 0.01\n",
      "Epoch [18067/20000], Loss: -21.338668823242188, Learning Rate: 0.01\n",
      "Epoch [18068/20000], Loss: -21.339111328125, Learning Rate: 0.01\n",
      "Epoch [18069/20000], Loss: -21.339248657226562, Learning Rate: 0.01\n",
      "Epoch [18070/20000], Loss: -21.339080810546875, Learning Rate: 0.01\n",
      "Epoch [18071/20000], Loss: -21.338943481445312, Learning Rate: 0.01\n",
      "Epoch [18072/20000], Loss: -21.339080810546875, Learning Rate: 0.01\n",
      "Epoch [18073/20000], Loss: -21.339263916015625, Learning Rate: 0.01\n",
      "Epoch [18074/20000], Loss: -21.339401245117188, Learning Rate: 0.01\n",
      "Epoch [18075/20000], Loss: -21.339202880859375, Learning Rate: 0.01\n",
      "Epoch [18076/20000], Loss: -21.339187622070312, Learning Rate: 0.01\n",
      "Epoch [18077/20000], Loss: -21.339279174804688, Learning Rate: 0.01\n",
      "Epoch [18078/20000], Loss: -21.339401245117188, Learning Rate: 0.01\n",
      "Epoch [18079/20000], Loss: -21.33941650390625, Learning Rate: 0.01\n",
      "Epoch [18080/20000], Loss: -21.339385986328125, Learning Rate: 0.01\n",
      "Epoch [18081/20000], Loss: -21.339401245117188, Learning Rate: 0.01\n",
      "Epoch [18082/20000], Loss: -21.33941650390625, Learning Rate: 0.01\n",
      "Epoch [18083/20000], Loss: -21.33953857421875, Learning Rate: 0.01\n",
      "Epoch [18084/20000], Loss: -21.339508056640625, Learning Rate: 0.01\n",
      "Epoch [18085/20000], Loss: -21.339492797851562, Learning Rate: 0.01\n",
      "Epoch [18086/20000], Loss: -21.339553833007812, Learning Rate: 0.01\n",
      "Epoch [18087/20000], Loss: -21.339630126953125, Learning Rate: 0.01\n",
      "Epoch [18088/20000], Loss: -21.339569091796875, Learning Rate: 0.01\n",
      "Epoch [18089/20000], Loss: -21.339599609375, Learning Rate: 0.01\n",
      "Epoch [18090/20000], Loss: -21.339614868164062, Learning Rate: 0.01\n",
      "Epoch [18091/20000], Loss: -21.339599609375, Learning Rate: 0.01\n",
      "Epoch [18092/20000], Loss: -21.33966064453125, Learning Rate: 0.01\n",
      "Epoch [18093/20000], Loss: -21.339706420898438, Learning Rate: 0.01\n",
      "Epoch [18094/20000], Loss: -21.339736938476562, Learning Rate: 0.01\n",
      "Epoch [18095/20000], Loss: -21.339630126953125, Learning Rate: 0.01\n",
      "Epoch [18096/20000], Loss: -21.339736938476562, Learning Rate: 0.01\n",
      "Epoch [18097/20000], Loss: -21.339736938476562, Learning Rate: 0.01\n",
      "Epoch [18098/20000], Loss: -21.33978271484375, Learning Rate: 0.01\n",
      "Epoch [18099/20000], Loss: -21.339767456054688, Learning Rate: 0.01\n",
      "Epoch [18100/20000], Loss: -21.339752197265625, Learning Rate: 0.01\n",
      "Epoch [18101/20000], Loss: -21.339797973632812, Learning Rate: 0.01\n",
      "Epoch [18102/20000], Loss: -21.339813232421875, Learning Rate: 0.01\n",
      "Epoch [18103/20000], Loss: -21.339859008789062, Learning Rate: 0.01\n",
      "Epoch [18104/20000], Loss: -21.33984375, Learning Rate: 0.01\n",
      "Epoch [18105/20000], Loss: -21.339813232421875, Learning Rate: 0.01\n",
      "Epoch [18106/20000], Loss: -21.339813232421875, Learning Rate: 0.01\n",
      "Epoch [18107/20000], Loss: -21.339859008789062, Learning Rate: 0.01\n",
      "Epoch [18108/20000], Loss: -21.339889526367188, Learning Rate: 0.01\n",
      "Epoch [18109/20000], Loss: -21.339920043945312, Learning Rate: 0.01\n",
      "Epoch [18110/20000], Loss: -21.339950561523438, Learning Rate: 0.01\n",
      "Epoch [18111/20000], Loss: -21.3399658203125, Learning Rate: 0.01\n",
      "Epoch [18112/20000], Loss: -21.339981079101562, Learning Rate: 0.01\n",
      "Epoch [18113/20000], Loss: -21.339950561523438, Learning Rate: 0.01\n",
      "Epoch [18114/20000], Loss: -21.340011596679688, Learning Rate: 0.01\n",
      "Epoch [18115/20000], Loss: -21.339981079101562, Learning Rate: 0.01\n",
      "Epoch [18116/20000], Loss: -21.340042114257812, Learning Rate: 0.01\n",
      "Epoch [18117/20000], Loss: -21.34002685546875, Learning Rate: 0.01\n",
      "Epoch [18118/20000], Loss: -21.34002685546875, Learning Rate: 0.01\n",
      "Epoch [18119/20000], Loss: -21.340042114257812, Learning Rate: 0.01\n",
      "Epoch [18120/20000], Loss: -21.340118408203125, Learning Rate: 0.01\n",
      "Epoch [18121/20000], Loss: -21.340103149414062, Learning Rate: 0.01\n",
      "Epoch [18122/20000], Loss: -21.340103149414062, Learning Rate: 0.01\n",
      "Epoch [18123/20000], Loss: -21.340103149414062, Learning Rate: 0.01\n",
      "Epoch [18124/20000], Loss: -21.340179443359375, Learning Rate: 0.01\n",
      "Epoch [18125/20000], Loss: -21.3402099609375, Learning Rate: 0.01\n",
      "Epoch [18126/20000], Loss: -21.340225219726562, Learning Rate: 0.01\n",
      "Epoch [18127/20000], Loss: -21.340164184570312, Learning Rate: 0.01\n",
      "Epoch [18128/20000], Loss: -21.3402099609375, Learning Rate: 0.01\n",
      "Epoch [18129/20000], Loss: -21.340225219726562, Learning Rate: 0.01\n",
      "Epoch [18130/20000], Loss: -21.34027099609375, Learning Rate: 0.01\n",
      "Epoch [18131/20000], Loss: -21.340179443359375, Learning Rate: 0.01\n",
      "Epoch [18132/20000], Loss: -21.3402099609375, Learning Rate: 0.01\n",
      "Epoch [18133/20000], Loss: -21.340255737304688, Learning Rate: 0.01\n",
      "Epoch [18134/20000], Loss: -21.34027099609375, Learning Rate: 0.01\n",
      "Epoch [18135/20000], Loss: -21.34033203125, Learning Rate: 0.01\n",
      "Epoch [18136/20000], Loss: -21.340240478515625, Learning Rate: 0.01\n",
      "Epoch [18137/20000], Loss: -21.340362548828125, Learning Rate: 0.01\n",
      "Epoch [18138/20000], Loss: -21.340316772460938, Learning Rate: 0.01\n",
      "Epoch [18139/20000], Loss: -21.34039306640625, Learning Rate: 0.01\n",
      "Epoch [18140/20000], Loss: -21.340408325195312, Learning Rate: 0.01\n",
      "Epoch [18141/20000], Loss: -21.3404541015625, Learning Rate: 0.01\n",
      "Epoch [18142/20000], Loss: -21.3404541015625, Learning Rate: 0.01\n",
      "Epoch [18143/20000], Loss: -21.340499877929688, Learning Rate: 0.01\n",
      "Epoch [18144/20000], Loss: -21.340423583984375, Learning Rate: 0.01\n",
      "Epoch [18145/20000], Loss: -21.340499877929688, Learning Rate: 0.01\n",
      "Epoch [18146/20000], Loss: -21.340545654296875, Learning Rate: 0.01\n",
      "Epoch [18147/20000], Loss: -21.340484619140625, Learning Rate: 0.01\n",
      "Epoch [18148/20000], Loss: -21.340469360351562, Learning Rate: 0.01\n",
      "Epoch [18149/20000], Loss: -21.340530395507812, Learning Rate: 0.01\n",
      "Epoch [18150/20000], Loss: -21.340469360351562, Learning Rate: 0.01\n",
      "Epoch [18151/20000], Loss: -21.340576171875, Learning Rate: 0.01\n",
      "Epoch [18152/20000], Loss: -21.340576171875, Learning Rate: 0.01\n",
      "Epoch [18153/20000], Loss: -21.340560913085938, Learning Rate: 0.01\n",
      "Epoch [18154/20000], Loss: -21.340576171875, Learning Rate: 0.01\n",
      "Epoch [18155/20000], Loss: -21.340606689453125, Learning Rate: 0.01\n",
      "Epoch [18156/20000], Loss: -21.340621948242188, Learning Rate: 0.01\n",
      "Epoch [18157/20000], Loss: -21.340652465820312, Learning Rate: 0.01\n",
      "Epoch [18158/20000], Loss: -21.3406982421875, Learning Rate: 0.01\n",
      "Epoch [18159/20000], Loss: -21.340682983398438, Learning Rate: 0.01\n",
      "Epoch [18160/20000], Loss: -21.340667724609375, Learning Rate: 0.01\n",
      "Epoch [18161/20000], Loss: -21.3406982421875, Learning Rate: 0.01\n",
      "Epoch [18162/20000], Loss: -21.34075927734375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18163/20000], Loss: -21.34075927734375, Learning Rate: 0.01\n",
      "Epoch [18164/20000], Loss: -21.34075927734375, Learning Rate: 0.01\n",
      "Epoch [18165/20000], Loss: -21.340713500976562, Learning Rate: 0.01\n",
      "Epoch [18166/20000], Loss: -21.34075927734375, Learning Rate: 0.01\n",
      "Epoch [18167/20000], Loss: -21.340850830078125, Learning Rate: 0.01\n",
      "Epoch [18168/20000], Loss: -21.340744018554688, Learning Rate: 0.01\n",
      "Epoch [18169/20000], Loss: -21.340835571289062, Learning Rate: 0.01\n",
      "Epoch [18170/20000], Loss: -21.340850830078125, Learning Rate: 0.01\n",
      "Epoch [18171/20000], Loss: -21.340835571289062, Learning Rate: 0.01\n",
      "Epoch [18172/20000], Loss: -21.34088134765625, Learning Rate: 0.01\n",
      "Epoch [18173/20000], Loss: -21.34088134765625, Learning Rate: 0.01\n",
      "Epoch [18174/20000], Loss: -21.3408203125, Learning Rate: 0.01\n",
      "Epoch [18175/20000], Loss: -21.3409423828125, Learning Rate: 0.01\n",
      "Epoch [18176/20000], Loss: -21.340911865234375, Learning Rate: 0.01\n",
      "Epoch [18177/20000], Loss: -21.340896606445312, Learning Rate: 0.01\n",
      "Epoch [18178/20000], Loss: -21.340957641601562, Learning Rate: 0.01\n",
      "Epoch [18179/20000], Loss: -21.340927124023438, Learning Rate: 0.01\n",
      "Epoch [18180/20000], Loss: -21.340927124023438, Learning Rate: 0.01\n",
      "Epoch [18181/20000], Loss: -21.340805053710938, Learning Rate: 0.01\n",
      "Epoch [18182/20000], Loss: -21.340835571289062, Learning Rate: 0.01\n",
      "Epoch [18183/20000], Loss: -21.3406982421875, Learning Rate: 0.01\n",
      "Epoch [18184/20000], Loss: -21.34051513671875, Learning Rate: 0.01\n",
      "Epoch [18185/20000], Loss: -21.340240478515625, Learning Rate: 0.01\n",
      "Epoch [18186/20000], Loss: -21.339828491210938, Learning Rate: 0.01\n",
      "Epoch [18187/20000], Loss: -21.339126586914062, Learning Rate: 0.01\n",
      "Epoch [18188/20000], Loss: -21.337966918945312, Learning Rate: 0.01\n",
      "Epoch [18189/20000], Loss: -21.335983276367188, Learning Rate: 0.01\n",
      "Epoch [18190/20000], Loss: -21.332778930664062, Learning Rate: 0.01\n",
      "Epoch [18191/20000], Loss: -21.327377319335938, Learning Rate: 0.01\n",
      "Epoch [18192/20000], Loss: -21.31842041015625, Learning Rate: 0.01\n",
      "Epoch [18193/20000], Loss: -21.303359985351562, Learning Rate: 0.01\n",
      "Epoch [18194/20000], Loss: -21.277984619140625, Learning Rate: 0.01\n",
      "Epoch [18195/20000], Loss: -21.235382080078125, Learning Rate: 0.01\n",
      "Epoch [18196/20000], Loss: -21.164627075195312, Learning Rate: 0.01\n",
      "Epoch [18197/20000], Loss: -21.048416137695312, Learning Rate: 0.01\n",
      "Epoch [18198/20000], Loss: -20.865158081054688, Learning Rate: 0.01\n",
      "Epoch [18199/20000], Loss: -20.59283447265625, Learning Rate: 0.01\n",
      "Epoch [18200/20000], Loss: -20.237213134765625, Learning Rate: 0.01\n",
      "Epoch [18201/20000], Loss: -19.864593505859375, Learning Rate: 0.01\n",
      "Epoch [18202/20000], Loss: -19.660736083984375, Learning Rate: 0.01\n",
      "Epoch [18203/20000], Loss: -19.818496704101562, Learning Rate: 0.01\n",
      "Epoch [18204/20000], Loss: -20.367721557617188, Learning Rate: 0.01\n",
      "Epoch [18205/20000], Loss: -20.969711303710938, Learning Rate: 0.01\n",
      "Epoch [18206/20000], Loss: -21.246826171875, Learning Rate: 0.01\n",
      "Epoch [18207/20000], Loss: -21.120697021484375, Learning Rate: 0.01\n",
      "Epoch [18208/20000], Loss: -20.854904174804688, Learning Rate: 0.01\n",
      "Epoch [18209/20000], Loss: -20.7525634765625, Learning Rate: 0.01\n",
      "Epoch [18210/20000], Loss: -20.887130737304688, Learning Rate: 0.01\n",
      "Epoch [18211/20000], Loss: -21.084228515625, Learning Rate: 0.01\n",
      "Epoch [18212/20000], Loss: -21.153488159179688, Learning Rate: 0.01\n",
      "Epoch [18213/20000], Loss: -21.111892700195312, Learning Rate: 0.01\n",
      "Epoch [18214/20000], Loss: -21.09222412109375, Learning Rate: 0.01\n",
      "Epoch [18215/20000], Loss: -21.156204223632812, Learning Rate: 0.01\n",
      "Epoch [18216/20000], Loss: -21.221084594726562, Learning Rate: 0.01\n",
      "Epoch [18217/20000], Loss: -21.202728271484375, Learning Rate: 0.01\n",
      "Epoch [18218/20000], Loss: -21.1422119140625, Learning Rate: 0.01\n",
      "Epoch [18219/20000], Loss: -21.142425537109375, Learning Rate: 0.01\n",
      "Epoch [18220/20000], Loss: -21.231964111328125, Learning Rate: 0.01\n",
      "Epoch [18221/20000], Loss: -21.316864013671875, Learning Rate: 0.01\n",
      "Epoch [18222/20000], Loss: -21.30999755859375, Learning Rate: 0.01\n",
      "Epoch [18223/20000], Loss: -21.236038208007812, Learning Rate: 0.01\n",
      "Epoch [18224/20000], Loss: -21.19317626953125, Learning Rate: 0.01\n",
      "Epoch [18225/20000], Loss: -21.234603881835938, Learning Rate: 0.01\n",
      "Epoch [18226/20000], Loss: -21.308837890625, Learning Rate: 0.01\n",
      "Epoch [18227/20000], Loss: -21.338882446289062, Learning Rate: 0.01\n",
      "Epoch [18228/20000], Loss: -21.309463500976562, Learning Rate: 0.01\n",
      "Epoch [18229/20000], Loss: -21.271347045898438, Learning Rate: 0.01\n",
      "Epoch [18230/20000], Loss: -21.271072387695312, Learning Rate: 0.01\n",
      "Epoch [18231/20000], Loss: -21.300689697265625, Learning Rate: 0.01\n",
      "Epoch [18232/20000], Loss: -21.323348999023438, Learning Rate: 0.01\n",
      "Epoch [18233/20000], Loss: -21.321823120117188, Learning Rate: 0.01\n",
      "Epoch [18234/20000], Loss: -21.31146240234375, Learning Rate: 0.01\n",
      "Epoch [18235/20000], Loss: -21.31195068359375, Learning Rate: 0.01\n",
      "Epoch [18236/20000], Loss: -21.32122802734375, Learning Rate: 0.01\n",
      "Epoch [18237/20000], Loss: -21.325607299804688, Learning Rate: 0.01\n",
      "Epoch [18238/20000], Loss: -21.320541381835938, Learning Rate: 0.01\n",
      "Epoch [18239/20000], Loss: -21.316635131835938, Learning Rate: 0.01\n",
      "Epoch [18240/20000], Loss: -21.322921752929688, Learning Rate: 0.01\n",
      "Epoch [18241/20000], Loss: -21.334228515625, Learning Rate: 0.01\n",
      "Epoch [18242/20000], Loss: -21.338775634765625, Learning Rate: 0.01\n",
      "Epoch [18243/20000], Loss: -21.332656860351562, Learning Rate: 0.01\n",
      "Epoch [18244/20000], Loss: -21.324462890625, Learning Rate: 0.01\n",
      "Epoch [18245/20000], Loss: -21.324066162109375, Learning Rate: 0.01\n",
      "Epoch [18246/20000], Loss: -21.331802368164062, Learning Rate: 0.01\n",
      "Epoch [18247/20000], Loss: -21.339324951171875, Learning Rate: 0.01\n",
      "Epoch [18248/20000], Loss: -21.340103149414062, Learning Rate: 0.01\n",
      "Epoch [18249/20000], Loss: -21.335922241210938, Learning Rate: 0.01\n",
      "Epoch [18250/20000], Loss: -21.332595825195312, Learning Rate: 0.01\n",
      "Epoch [18251/20000], Loss: -21.333526611328125, Learning Rate: 0.01\n",
      "Epoch [18252/20000], Loss: -21.33660888671875, Learning Rate: 0.01\n",
      "Epoch [18253/20000], Loss: -21.338424682617188, Learning Rate: 0.01\n",
      "Epoch [18254/20000], Loss: -21.338165283203125, Learning Rate: 0.01\n",
      "Epoch [18255/20000], Loss: -21.337600708007812, Learning Rate: 0.01\n",
      "Epoch [18256/20000], Loss: -21.338104248046875, Learning Rate: 0.01\n",
      "Epoch [18257/20000], Loss: -21.339324951171875, Learning Rate: 0.01\n",
      "Epoch [18258/20000], Loss: -21.339736938476562, Learning Rate: 0.01\n",
      "Epoch [18259/20000], Loss: -21.338882446289062, Learning Rate: 0.01\n",
      "Epoch [18260/20000], Loss: -21.337921142578125, Learning Rate: 0.01\n",
      "Epoch [18261/20000], Loss: -21.338226318359375, Learning Rate: 0.01\n",
      "Epoch [18262/20000], Loss: -21.339752197265625, Learning Rate: 0.01\n",
      "Epoch [18263/20000], Loss: -21.341033935546875, Learning Rate: 0.01\n",
      "Epoch [18264/20000], Loss: -21.34112548828125, Learning Rate: 0.01\n",
      "Epoch [18265/20000], Loss: -21.340240478515625, Learning Rate: 0.01\n",
      "Epoch [18266/20000], Loss: -21.33953857421875, Learning Rate: 0.01\n",
      "Epoch [18267/20000], Loss: -21.339523315429688, Learning Rate: 0.01\n",
      "Epoch [18268/20000], Loss: -21.3402099609375, Learning Rate: 0.01\n",
      "Epoch [18269/20000], Loss: -21.340866088867188, Learning Rate: 0.01\n",
      "Epoch [18270/20000], Loss: -21.340988159179688, Learning Rate: 0.01\n",
      "Epoch [18271/20000], Loss: -21.340835571289062, Learning Rate: 0.01\n",
      "Epoch [18272/20000], Loss: -21.340744018554688, Learning Rate: 0.01\n",
      "Epoch [18273/20000], Loss: -21.340896606445312, Learning Rate: 0.01\n",
      "Epoch [18274/20000], Loss: -21.341049194335938, Learning Rate: 0.01\n",
      "Epoch [18275/20000], Loss: -21.341064453125, Learning Rate: 0.01\n",
      "Epoch [18276/20000], Loss: -21.341018676757812, Learning Rate: 0.01\n",
      "Epoch [18277/20000], Loss: -21.340835571289062, Learning Rate: 0.01\n",
      "Epoch [18278/20000], Loss: -21.340988159179688, Learning Rate: 0.01\n",
      "Epoch [18279/20000], Loss: -21.341201782226562, Learning Rate: 0.01\n",
      "Epoch [18280/20000], Loss: -21.341323852539062, Learning Rate: 0.01\n",
      "Epoch [18281/20000], Loss: -21.341461181640625, Learning Rate: 0.01\n",
      "Epoch [18282/20000], Loss: -21.34149169921875, Learning Rate: 0.01\n",
      "Epoch [18283/20000], Loss: -21.341323852539062, Learning Rate: 0.01\n",
      "Epoch [18284/20000], Loss: -21.34124755859375, Learning Rate: 0.01\n",
      "Epoch [18285/20000], Loss: -21.3414306640625, Learning Rate: 0.01\n",
      "Epoch [18286/20000], Loss: -21.341461181640625, Learning Rate: 0.01\n",
      "Epoch [18287/20000], Loss: -21.341598510742188, Learning Rate: 0.01\n",
      "Epoch [18288/20000], Loss: -21.341506958007812, Learning Rate: 0.01\n",
      "Epoch [18289/20000], Loss: -21.341583251953125, Learning Rate: 0.01\n",
      "Epoch [18290/20000], Loss: -21.341537475585938, Learning Rate: 0.01\n",
      "Epoch [18291/20000], Loss: -21.341598510742188, Learning Rate: 0.01\n",
      "Epoch [18292/20000], Loss: -21.341720581054688, Learning Rate: 0.01\n",
      "Epoch [18293/20000], Loss: -21.341705322265625, Learning Rate: 0.01\n",
      "Epoch [18294/20000], Loss: -21.341690063476562, Learning Rate: 0.01\n",
      "Epoch [18295/20000], Loss: -21.341751098632812, Learning Rate: 0.01\n",
      "Epoch [18296/20000], Loss: -21.341766357421875, Learning Rate: 0.01\n",
      "Epoch [18297/20000], Loss: -21.341690063476562, Learning Rate: 0.01\n",
      "Epoch [18298/20000], Loss: -21.341812133789062, Learning Rate: 0.01\n",
      "Epoch [18299/20000], Loss: -21.341888427734375, Learning Rate: 0.01\n",
      "Epoch [18300/20000], Loss: -21.341796875, Learning Rate: 0.01\n",
      "Epoch [18301/20000], Loss: -21.341888427734375, Learning Rate: 0.01\n",
      "Epoch [18302/20000], Loss: -21.341873168945312, Learning Rate: 0.01\n",
      "Epoch [18303/20000], Loss: -21.341842651367188, Learning Rate: 0.01\n",
      "Epoch [18304/20000], Loss: -21.341903686523438, Learning Rate: 0.01\n",
      "Epoch [18305/20000], Loss: -21.341888427734375, Learning Rate: 0.01\n",
      "Epoch [18306/20000], Loss: -21.34185791015625, Learning Rate: 0.01\n",
      "Epoch [18307/20000], Loss: -21.341949462890625, Learning Rate: 0.01\n",
      "Epoch [18308/20000], Loss: -21.341873168945312, Learning Rate: 0.01\n",
      "Epoch [18309/20000], Loss: -21.34197998046875, Learning Rate: 0.01\n",
      "Epoch [18310/20000], Loss: -21.341934204101562, Learning Rate: 0.01\n",
      "Epoch [18311/20000], Loss: -21.341964721679688, Learning Rate: 0.01\n",
      "Epoch [18312/20000], Loss: -21.342025756835938, Learning Rate: 0.01\n",
      "Epoch [18313/20000], Loss: -21.342041015625, Learning Rate: 0.01\n",
      "Epoch [18314/20000], Loss: -21.341995239257812, Learning Rate: 0.01\n",
      "Epoch [18315/20000], Loss: -21.342041015625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18316/20000], Loss: -21.342025756835938, Learning Rate: 0.01\n",
      "Epoch [18317/20000], Loss: -21.342071533203125, Learning Rate: 0.01\n",
      "Epoch [18318/20000], Loss: -21.342117309570312, Learning Rate: 0.01\n",
      "Epoch [18319/20000], Loss: -21.342117309570312, Learning Rate: 0.01\n",
      "Epoch [18320/20000], Loss: -21.342147827148438, Learning Rate: 0.01\n",
      "Epoch [18321/20000], Loss: -21.342208862304688, Learning Rate: 0.01\n",
      "Epoch [18322/20000], Loss: -21.3421630859375, Learning Rate: 0.01\n",
      "Epoch [18323/20000], Loss: -21.342239379882812, Learning Rate: 0.01\n",
      "Epoch [18324/20000], Loss: -21.342193603515625, Learning Rate: 0.01\n",
      "Epoch [18325/20000], Loss: -21.34222412109375, Learning Rate: 0.01\n",
      "Epoch [18326/20000], Loss: -21.342193603515625, Learning Rate: 0.01\n",
      "Epoch [18327/20000], Loss: -21.342315673828125, Learning Rate: 0.01\n",
      "Epoch [18328/20000], Loss: -21.342300415039062, Learning Rate: 0.01\n",
      "Epoch [18329/20000], Loss: -21.34222412109375, Learning Rate: 0.01\n",
      "Epoch [18330/20000], Loss: -21.342315673828125, Learning Rate: 0.01\n",
      "Epoch [18331/20000], Loss: -21.342391967773438, Learning Rate: 0.01\n",
      "Epoch [18332/20000], Loss: -21.34234619140625, Learning Rate: 0.01\n",
      "Epoch [18333/20000], Loss: -21.342330932617188, Learning Rate: 0.01\n",
      "Epoch [18334/20000], Loss: -21.342361450195312, Learning Rate: 0.01\n",
      "Epoch [18335/20000], Loss: -21.342330932617188, Learning Rate: 0.01\n",
      "Epoch [18336/20000], Loss: -21.342391967773438, Learning Rate: 0.01\n",
      "Epoch [18337/20000], Loss: -21.3424072265625, Learning Rate: 0.01\n",
      "Epoch [18338/20000], Loss: -21.342391967773438, Learning Rate: 0.01\n",
      "Epoch [18339/20000], Loss: -21.342361450195312, Learning Rate: 0.01\n",
      "Epoch [18340/20000], Loss: -21.34246826171875, Learning Rate: 0.01\n",
      "Epoch [18341/20000], Loss: -21.342422485351562, Learning Rate: 0.01\n",
      "Epoch [18342/20000], Loss: -21.342376708984375, Learning Rate: 0.01\n",
      "Epoch [18343/20000], Loss: -21.342437744140625, Learning Rate: 0.01\n",
      "Epoch [18344/20000], Loss: -21.342514038085938, Learning Rate: 0.01\n",
      "Epoch [18345/20000], Loss: -21.342498779296875, Learning Rate: 0.01\n",
      "Epoch [18346/20000], Loss: -21.342483520507812, Learning Rate: 0.01\n",
      "Epoch [18347/20000], Loss: -21.342529296875, Learning Rate: 0.01\n",
      "Epoch [18348/20000], Loss: -21.342575073242188, Learning Rate: 0.01\n",
      "Epoch [18349/20000], Loss: -21.342605590820312, Learning Rate: 0.01\n",
      "Epoch [18350/20000], Loss: -21.342605590820312, Learning Rate: 0.01\n",
      "Epoch [18351/20000], Loss: -21.342605590820312, Learning Rate: 0.01\n",
      "Epoch [18352/20000], Loss: -21.342605590820312, Learning Rate: 0.01\n",
      "Epoch [18353/20000], Loss: -21.3426513671875, Learning Rate: 0.01\n",
      "Epoch [18354/20000], Loss: -21.34271240234375, Learning Rate: 0.01\n",
      "Epoch [18355/20000], Loss: -21.3426513671875, Learning Rate: 0.01\n",
      "Epoch [18356/20000], Loss: -21.3426513671875, Learning Rate: 0.01\n",
      "Epoch [18357/20000], Loss: -21.3426513671875, Learning Rate: 0.01\n",
      "Epoch [18358/20000], Loss: -21.342620849609375, Learning Rate: 0.01\n",
      "Epoch [18359/20000], Loss: -21.342620849609375, Learning Rate: 0.01\n",
      "Epoch [18360/20000], Loss: -21.342559814453125, Learning Rate: 0.01\n",
      "Epoch [18361/20000], Loss: -21.342453002929688, Learning Rate: 0.01\n",
      "Epoch [18362/20000], Loss: -21.3424072265625, Learning Rate: 0.01\n",
      "Epoch [18363/20000], Loss: -21.342269897460938, Learning Rate: 0.01\n",
      "Epoch [18364/20000], Loss: -21.341964721679688, Learning Rate: 0.01\n",
      "Epoch [18365/20000], Loss: -21.341506958007812, Learning Rate: 0.01\n",
      "Epoch [18366/20000], Loss: -21.340835571289062, Learning Rate: 0.01\n",
      "Epoch [18367/20000], Loss: -21.339767456054688, Learning Rate: 0.01\n",
      "Epoch [18368/20000], Loss: -21.3380126953125, Learning Rate: 0.01\n",
      "Epoch [18369/20000], Loss: -21.335189819335938, Learning Rate: 0.01\n",
      "Epoch [18370/20000], Loss: -21.330581665039062, Learning Rate: 0.01\n",
      "Epoch [18371/20000], Loss: -21.323089599609375, Learning Rate: 0.01\n",
      "Epoch [18372/20000], Loss: -21.310821533203125, Learning Rate: 0.01\n",
      "Epoch [18373/20000], Loss: -21.290496826171875, Learning Rate: 0.01\n",
      "Epoch [18374/20000], Loss: -21.25689697265625, Learning Rate: 0.01\n",
      "Epoch [18375/20000], Loss: -21.201019287109375, Learning Rate: 0.01\n",
      "Epoch [18376/20000], Loss: -21.1099853515625, Learning Rate: 0.01\n",
      "Epoch [18377/20000], Loss: -20.95928955078125, Learning Rate: 0.01\n",
      "Epoch [18378/20000], Loss: -20.723739624023438, Learning Rate: 0.01\n",
      "Epoch [18379/20000], Loss: -20.35638427734375, Learning Rate: 0.01\n",
      "Epoch [18380/20000], Loss: -19.857986450195312, Learning Rate: 0.01\n",
      "Epoch [18381/20000], Loss: -19.2298583984375, Learning Rate: 0.01\n",
      "Epoch [18382/20000], Loss: -18.72747802734375, Learning Rate: 0.01\n",
      "Epoch [18383/20000], Loss: -18.5802001953125, Learning Rate: 0.01\n",
      "Epoch [18384/20000], Loss: -19.1884765625, Learning Rate: 0.01\n",
      "Epoch [18385/20000], Loss: -20.244857788085938, Learning Rate: 0.01\n",
      "Epoch [18386/20000], Loss: -21.13787841796875, Learning Rate: 0.01\n",
      "Epoch [18387/20000], Loss: -21.286087036132812, Learning Rate: 0.01\n",
      "Epoch [18388/20000], Loss: -20.793106079101562, Learning Rate: 0.01\n",
      "Epoch [18389/20000], Loss: -20.274810791015625, Learning Rate: 0.01\n",
      "Epoch [18390/20000], Loss: -20.250167846679688, Learning Rate: 0.01\n",
      "Epoch [18391/20000], Loss: -20.750442504882812, Learning Rate: 0.01\n",
      "Epoch [18392/20000], Loss: -21.235671997070312, Learning Rate: 0.01\n",
      "Epoch [18393/20000], Loss: -21.275115966796875, Learning Rate: 0.01\n",
      "Epoch [18394/20000], Loss: -20.9630126953125, Learning Rate: 0.01\n",
      "Epoch [18395/20000], Loss: -20.736312866210938, Learning Rate: 0.01\n",
      "Epoch [18396/20000], Loss: -20.871688842773438, Learning Rate: 0.01\n",
      "Epoch [18397/20000], Loss: -21.18310546875, Learning Rate: 0.01\n",
      "Epoch [18398/20000], Loss: -21.324203491210938, Learning Rate: 0.01\n",
      "Epoch [18399/20000], Loss: -21.195358276367188, Learning Rate: 0.01\n",
      "Epoch [18400/20000], Loss: -21.020553588867188, Learning Rate: 0.01\n",
      "Epoch [18401/20000], Loss: -21.037704467773438, Learning Rate: 0.01\n",
      "Epoch [18402/20000], Loss: -21.21148681640625, Learning Rate: 0.01\n",
      "Epoch [18403/20000], Loss: -21.3310546875, Learning Rate: 0.01\n",
      "Epoch [18404/20000], Loss: -21.28179931640625, Learning Rate: 0.01\n",
      "Epoch [18405/20000], Loss: -21.167877197265625, Learning Rate: 0.01\n",
      "Epoch [18406/20000], Loss: -21.151199340820312, Learning Rate: 0.01\n",
      "Epoch [18407/20000], Loss: -21.24603271484375, Learning Rate: 0.01\n",
      "Epoch [18408/20000], Loss: -21.333038330078125, Learning Rate: 0.01\n",
      "Epoch [18409/20000], Loss: -21.3204345703125, Learning Rate: 0.01\n",
      "Epoch [18410/20000], Loss: -21.250381469726562, Learning Rate: 0.01\n",
      "Epoch [18411/20000], Loss: -21.224472045898438, Learning Rate: 0.01\n",
      "Epoch [18412/20000], Loss: -21.272064208984375, Learning Rate: 0.01\n",
      "Epoch [18413/20000], Loss: -21.331161499023438, Learning Rate: 0.01\n",
      "Epoch [18414/20000], Loss: -21.336395263671875, Learning Rate: 0.01\n",
      "Epoch [18415/20000], Loss: -21.29742431640625, Learning Rate: 0.01\n",
      "Epoch [18416/20000], Loss: -21.273040771484375, Learning Rate: 0.01\n",
      "Epoch [18417/20000], Loss: -21.292739868164062, Learning Rate: 0.01\n",
      "Epoch [18418/20000], Loss: -21.329299926757812, Learning Rate: 0.01\n",
      "Epoch [18419/20000], Loss: -21.340652465820312, Learning Rate: 0.01\n",
      "Epoch [18420/20000], Loss: -21.322021484375, Learning Rate: 0.01\n",
      "Epoch [18421/20000], Loss: -21.303665161132812, Learning Rate: 0.01\n",
      "Epoch [18422/20000], Loss: -21.3092041015625, Learning Rate: 0.01\n",
      "Epoch [18423/20000], Loss: -21.329986572265625, Learning Rate: 0.01\n",
      "Epoch [18424/20000], Loss: -21.341262817382812, Learning Rate: 0.01\n",
      "Epoch [18425/20000], Loss: -21.333984375, Learning Rate: 0.01\n",
      "Epoch [18426/20000], Loss: -21.321609497070312, Learning Rate: 0.01\n",
      "Epoch [18427/20000], Loss: -21.320632934570312, Learning Rate: 0.01\n",
      "Epoch [18428/20000], Loss: -21.331558227539062, Learning Rate: 0.01\n",
      "Epoch [18429/20000], Loss: -21.341079711914062, Learning Rate: 0.01\n",
      "Epoch [18430/20000], Loss: -21.339981079101562, Learning Rate: 0.01\n",
      "Epoch [18431/20000], Loss: -21.332244873046875, Learning Rate: 0.01\n",
      "Epoch [18432/20000], Loss: -21.3287353515625, Learning Rate: 0.01\n",
      "Epoch [18433/20000], Loss: -21.33319091796875, Learning Rate: 0.01\n",
      "Epoch [18434/20000], Loss: -21.340057373046875, Learning Rate: 0.01\n",
      "Epoch [18435/20000], Loss: -21.342147827148438, Learning Rate: 0.01\n",
      "Epoch [18436/20000], Loss: -21.338729858398438, Learning Rate: 0.01\n",
      "Epoch [18437/20000], Loss: -21.334991455078125, Learning Rate: 0.01\n",
      "Epoch [18438/20000], Loss: -21.335418701171875, Learning Rate: 0.01\n",
      "Epoch [18439/20000], Loss: -21.339111328125, Learning Rate: 0.01\n",
      "Epoch [18440/20000], Loss: -21.34210205078125, Learning Rate: 0.01\n",
      "Epoch [18441/20000], Loss: -21.341659545898438, Learning Rate: 0.01\n",
      "Epoch [18442/20000], Loss: -21.33935546875, Learning Rate: 0.01\n",
      "Epoch [18443/20000], Loss: -21.338088989257812, Learning Rate: 0.01\n",
      "Epoch [18444/20000], Loss: -21.33935546875, Learning Rate: 0.01\n",
      "Epoch [18445/20000], Loss: -21.341476440429688, Learning Rate: 0.01\n",
      "Epoch [18446/20000], Loss: -21.342300415039062, Learning Rate: 0.01\n",
      "Epoch [18447/20000], Loss: -21.34161376953125, Learning Rate: 0.01\n",
      "Epoch [18448/20000], Loss: -21.340377807617188, Learning Rate: 0.01\n",
      "Epoch [18449/20000], Loss: -21.34014892578125, Learning Rate: 0.01\n",
      "Epoch [18450/20000], Loss: -21.341232299804688, Learning Rate: 0.01\n",
      "Epoch [18451/20000], Loss: -21.342254638671875, Learning Rate: 0.01\n",
      "Epoch [18452/20000], Loss: -21.342483520507812, Learning Rate: 0.01\n",
      "Epoch [18453/20000], Loss: -21.341888427734375, Learning Rate: 0.01\n",
      "Epoch [18454/20000], Loss: -21.341262817382812, Learning Rate: 0.01\n",
      "Epoch [18455/20000], Loss: -21.341461181640625, Learning Rate: 0.01\n",
      "Epoch [18456/20000], Loss: -21.341964721679688, Learning Rate: 0.01\n",
      "Epoch [18457/20000], Loss: -21.342498779296875, Learning Rate: 0.01\n",
      "Epoch [18458/20000], Loss: -21.3426513671875, Learning Rate: 0.01\n",
      "Epoch [18459/20000], Loss: -21.3421630859375, Learning Rate: 0.01\n",
      "Epoch [18460/20000], Loss: -21.341964721679688, Learning Rate: 0.01\n",
      "Epoch [18461/20000], Loss: -21.342041015625, Learning Rate: 0.01\n",
      "Epoch [18462/20000], Loss: -21.3424072265625, Learning Rate: 0.01\n",
      "Epoch [18463/20000], Loss: -21.342605590820312, Learning Rate: 0.01\n",
      "Epoch [18464/20000], Loss: -21.34271240234375, Learning Rate: 0.01\n",
      "Epoch [18465/20000], Loss: -21.342529296875, Learning Rate: 0.01\n",
      "Epoch [18466/20000], Loss: -21.342376708984375, Learning Rate: 0.01\n",
      "Epoch [18467/20000], Loss: -21.342453002929688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18468/20000], Loss: -21.342605590820312, Learning Rate: 0.01\n",
      "Epoch [18469/20000], Loss: -21.3427734375, Learning Rate: 0.01\n",
      "Epoch [18470/20000], Loss: -21.342803955078125, Learning Rate: 0.01\n",
      "Epoch [18471/20000], Loss: -21.342758178710938, Learning Rate: 0.01\n",
      "Epoch [18472/20000], Loss: -21.34271240234375, Learning Rate: 0.01\n",
      "Epoch [18473/20000], Loss: -21.342666625976562, Learning Rate: 0.01\n",
      "Epoch [18474/20000], Loss: -21.342788696289062, Learning Rate: 0.01\n",
      "Epoch [18475/20000], Loss: -21.342849731445312, Learning Rate: 0.01\n",
      "Epoch [18476/20000], Loss: -21.342864990234375, Learning Rate: 0.01\n",
      "Epoch [18477/20000], Loss: -21.3428955078125, Learning Rate: 0.01\n",
      "Epoch [18478/20000], Loss: -21.34283447265625, Learning Rate: 0.01\n",
      "Epoch [18479/20000], Loss: -21.342819213867188, Learning Rate: 0.01\n",
      "Epoch [18480/20000], Loss: -21.3428955078125, Learning Rate: 0.01\n",
      "Epoch [18481/20000], Loss: -21.34295654296875, Learning Rate: 0.01\n",
      "Epoch [18482/20000], Loss: -21.342987060546875, Learning Rate: 0.01\n",
      "Epoch [18483/20000], Loss: -21.343002319335938, Learning Rate: 0.01\n",
      "Epoch [18484/20000], Loss: -21.342987060546875, Learning Rate: 0.01\n",
      "Epoch [18485/20000], Loss: -21.342987060546875, Learning Rate: 0.01\n",
      "Epoch [18486/20000], Loss: -21.342987060546875, Learning Rate: 0.01\n",
      "Epoch [18487/20000], Loss: -21.343048095703125, Learning Rate: 0.01\n",
      "Epoch [18488/20000], Loss: -21.3431396484375, Learning Rate: 0.01\n",
      "Epoch [18489/20000], Loss: -21.343124389648438, Learning Rate: 0.01\n",
      "Epoch [18490/20000], Loss: -21.343154907226562, Learning Rate: 0.01\n",
      "Epoch [18491/20000], Loss: -21.343124389648438, Learning Rate: 0.01\n",
      "Epoch [18492/20000], Loss: -21.343093872070312, Learning Rate: 0.01\n",
      "Epoch [18493/20000], Loss: -21.3431396484375, Learning Rate: 0.01\n",
      "Epoch [18494/20000], Loss: -21.343215942382812, Learning Rate: 0.01\n",
      "Epoch [18495/20000], Loss: -21.343215942382812, Learning Rate: 0.01\n",
      "Epoch [18496/20000], Loss: -21.343215942382812, Learning Rate: 0.01\n",
      "Epoch [18497/20000], Loss: -21.34320068359375, Learning Rate: 0.01\n",
      "Epoch [18498/20000], Loss: -21.343231201171875, Learning Rate: 0.01\n",
      "Epoch [18499/20000], Loss: -21.34326171875, Learning Rate: 0.01\n",
      "Epoch [18500/20000], Loss: -21.343215942382812, Learning Rate: 0.01\n",
      "Epoch [18501/20000], Loss: -21.343338012695312, Learning Rate: 0.01\n",
      "Epoch [18502/20000], Loss: -21.34326171875, Learning Rate: 0.01\n",
      "Epoch [18503/20000], Loss: -21.343307495117188, Learning Rate: 0.01\n",
      "Epoch [18504/20000], Loss: -21.343338012695312, Learning Rate: 0.01\n",
      "Epoch [18505/20000], Loss: -21.343338012695312, Learning Rate: 0.01\n",
      "Epoch [18506/20000], Loss: -21.3433837890625, Learning Rate: 0.01\n",
      "Epoch [18507/20000], Loss: -21.343414306640625, Learning Rate: 0.01\n",
      "Epoch [18508/20000], Loss: -21.343399047851562, Learning Rate: 0.01\n",
      "Epoch [18509/20000], Loss: -21.343399047851562, Learning Rate: 0.01\n",
      "Epoch [18510/20000], Loss: -21.3433837890625, Learning Rate: 0.01\n",
      "Epoch [18511/20000], Loss: -21.343399047851562, Learning Rate: 0.01\n",
      "Epoch [18512/20000], Loss: -21.343475341796875, Learning Rate: 0.01\n",
      "Epoch [18513/20000], Loss: -21.3433837890625, Learning Rate: 0.01\n",
      "Epoch [18514/20000], Loss: -21.34344482421875, Learning Rate: 0.01\n",
      "Epoch [18515/20000], Loss: -21.343490600585938, Learning Rate: 0.01\n",
      "Epoch [18516/20000], Loss: -21.343505859375, Learning Rate: 0.01\n",
      "Epoch [18517/20000], Loss: -21.343460083007812, Learning Rate: 0.01\n",
      "Epoch [18518/20000], Loss: -21.343460083007812, Learning Rate: 0.01\n",
      "Epoch [18519/20000], Loss: -21.343490600585938, Learning Rate: 0.01\n",
      "Epoch [18520/20000], Loss: -21.343551635742188, Learning Rate: 0.01\n",
      "Epoch [18521/20000], Loss: -21.343521118164062, Learning Rate: 0.01\n",
      "Epoch [18522/20000], Loss: -21.34356689453125, Learning Rate: 0.01\n",
      "Epoch [18523/20000], Loss: -21.343658447265625, Learning Rate: 0.01\n",
      "Epoch [18524/20000], Loss: -21.3436279296875, Learning Rate: 0.01\n",
      "Epoch [18525/20000], Loss: -21.3436279296875, Learning Rate: 0.01\n",
      "Epoch [18526/20000], Loss: -21.343597412109375, Learning Rate: 0.01\n",
      "Epoch [18527/20000], Loss: -21.343673706054688, Learning Rate: 0.01\n",
      "Epoch [18528/20000], Loss: -21.343673706054688, Learning Rate: 0.01\n",
      "Epoch [18529/20000], Loss: -21.343658447265625, Learning Rate: 0.01\n",
      "Epoch [18530/20000], Loss: -21.3436279296875, Learning Rate: 0.01\n",
      "Epoch [18531/20000], Loss: -21.343658447265625, Learning Rate: 0.01\n",
      "Epoch [18532/20000], Loss: -21.34368896484375, Learning Rate: 0.01\n",
      "Epoch [18533/20000], Loss: -21.343734741210938, Learning Rate: 0.01\n",
      "Epoch [18534/20000], Loss: -21.343780517578125, Learning Rate: 0.01\n",
      "Epoch [18535/20000], Loss: -21.343734741210938, Learning Rate: 0.01\n",
      "Epoch [18536/20000], Loss: -21.343765258789062, Learning Rate: 0.01\n",
      "Epoch [18537/20000], Loss: -21.34381103515625, Learning Rate: 0.01\n",
      "Epoch [18538/20000], Loss: -21.3438720703125, Learning Rate: 0.01\n",
      "Epoch [18539/20000], Loss: -21.343734741210938, Learning Rate: 0.01\n",
      "Epoch [18540/20000], Loss: -21.343841552734375, Learning Rate: 0.01\n",
      "Epoch [18541/20000], Loss: -21.343841552734375, Learning Rate: 0.01\n",
      "Epoch [18542/20000], Loss: -21.343826293945312, Learning Rate: 0.01\n",
      "Epoch [18543/20000], Loss: -21.343841552734375, Learning Rate: 0.01\n",
      "Epoch [18544/20000], Loss: -21.343887329101562, Learning Rate: 0.01\n",
      "Epoch [18545/20000], Loss: -21.343902587890625, Learning Rate: 0.01\n",
      "Epoch [18546/20000], Loss: -21.3438720703125, Learning Rate: 0.01\n",
      "Epoch [18547/20000], Loss: -21.343917846679688, Learning Rate: 0.01\n",
      "Epoch [18548/20000], Loss: -21.343963623046875, Learning Rate: 0.01\n",
      "Epoch [18549/20000], Loss: -21.343948364257812, Learning Rate: 0.01\n",
      "Epoch [18550/20000], Loss: -21.343948364257812, Learning Rate: 0.01\n",
      "Epoch [18551/20000], Loss: -21.343948364257812, Learning Rate: 0.01\n",
      "Epoch [18552/20000], Loss: -21.344024658203125, Learning Rate: 0.01\n",
      "Epoch [18553/20000], Loss: -21.343994140625, Learning Rate: 0.01\n",
      "Epoch [18554/20000], Loss: -21.344009399414062, Learning Rate: 0.01\n",
      "Epoch [18555/20000], Loss: -21.343963623046875, Learning Rate: 0.01\n",
      "Epoch [18556/20000], Loss: -21.343994140625, Learning Rate: 0.01\n",
      "Epoch [18557/20000], Loss: -21.343963623046875, Learning Rate: 0.01\n",
      "Epoch [18558/20000], Loss: -21.344070434570312, Learning Rate: 0.01\n",
      "Epoch [18559/20000], Loss: -21.34405517578125, Learning Rate: 0.01\n",
      "Epoch [18560/20000], Loss: -21.343994140625, Learning Rate: 0.01\n",
      "Epoch [18561/20000], Loss: -21.343994140625, Learning Rate: 0.01\n",
      "Epoch [18562/20000], Loss: -21.344009399414062, Learning Rate: 0.01\n",
      "Epoch [18563/20000], Loss: -21.34393310546875, Learning Rate: 0.01\n",
      "Epoch [18564/20000], Loss: -21.343643188476562, Learning Rate: 0.01\n",
      "Epoch [18565/20000], Loss: -21.343353271484375, Learning Rate: 0.01\n",
      "Epoch [18566/20000], Loss: -21.3428955078125, Learning Rate: 0.01\n",
      "Epoch [18567/20000], Loss: -21.342010498046875, Learning Rate: 0.01\n",
      "Epoch [18568/20000], Loss: -21.340423583984375, Learning Rate: 0.01\n",
      "Epoch [18569/20000], Loss: -21.337646484375, Learning Rate: 0.01\n",
      "Epoch [18570/20000], Loss: -21.332748413085938, Learning Rate: 0.01\n",
      "Epoch [18571/20000], Loss: -21.323959350585938, Learning Rate: 0.01\n",
      "Epoch [18572/20000], Loss: -21.308486938476562, Learning Rate: 0.01\n",
      "Epoch [18573/20000], Loss: -21.280670166015625, Learning Rate: 0.01\n",
      "Epoch [18574/20000], Loss: -21.231353759765625, Learning Rate: 0.01\n",
      "Epoch [18575/20000], Loss: -21.143966674804688, Learning Rate: 0.01\n",
      "Epoch [18576/20000], Loss: -20.993988037109375, Learning Rate: 0.01\n",
      "Epoch [18577/20000], Loss: -20.7447509765625, Learning Rate: 0.01\n",
      "Epoch [18578/20000], Loss: -20.371261596679688, Learning Rate: 0.01\n",
      "Epoch [18579/20000], Loss: -19.888641357421875, Learning Rate: 0.01\n",
      "Epoch [18580/20000], Loss: -19.4818115234375, Learning Rate: 0.01\n",
      "Epoch [18581/20000], Loss: -19.4520263671875, Learning Rate: 0.01\n",
      "Epoch [18582/20000], Loss: -20.047805786132812, Learning Rate: 0.01\n",
      "Epoch [18583/20000], Loss: -20.9049072265625, Learning Rate: 0.01\n",
      "Epoch [18584/20000], Loss: -21.340850830078125, Learning Rate: 0.01\n",
      "Epoch [18585/20000], Loss: -21.088729858398438, Learning Rate: 0.01\n",
      "Epoch [18586/20000], Loss: -20.594406127929688, Learning Rate: 0.01\n",
      "Epoch [18587/20000], Loss: -20.493240356445312, Learning Rate: 0.01\n",
      "Epoch [18588/20000], Loss: -20.895919799804688, Learning Rate: 0.01\n",
      "Epoch [18589/20000], Loss: -21.302490234375, Learning Rate: 0.01\n",
      "Epoch [18590/20000], Loss: -21.260086059570312, Learning Rate: 0.01\n",
      "Epoch [18591/20000], Loss: -20.950973510742188, Learning Rate: 0.01\n",
      "Epoch [18592/20000], Loss: -20.863754272460938, Learning Rate: 0.01\n",
      "Epoch [18593/20000], Loss: -21.109725952148438, Learning Rate: 0.01\n",
      "Epoch [18594/20000], Loss: -21.333541870117188, Learning Rate: 0.01\n",
      "Epoch [18595/20000], Loss: -21.262954711914062, Learning Rate: 0.01\n",
      "Epoch [18596/20000], Loss: -21.078125, Learning Rate: 0.01\n",
      "Epoch [18597/20000], Loss: -21.083786010742188, Learning Rate: 0.01\n",
      "Epoch [18598/20000], Loss: -21.258636474609375, Learning Rate: 0.01\n",
      "Epoch [18599/20000], Loss: -21.342910766601562, Learning Rate: 0.01\n",
      "Epoch [18600/20000], Loss: -21.25048828125, Learning Rate: 0.01\n",
      "Epoch [18601/20000], Loss: -21.164138793945312, Learning Rate: 0.01\n",
      "Epoch [18602/20000], Loss: -21.223068237304688, Learning Rate: 0.01\n",
      "Epoch [18603/20000], Loss: -21.327194213867188, Learning Rate: 0.01\n",
      "Epoch [18604/20000], Loss: -21.32806396484375, Learning Rate: 0.01\n",
      "Epoch [18605/20000], Loss: -21.254714965820312, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18606/20000], Loss: -21.238113403320312, Learning Rate: 0.01\n",
      "Epoch [18607/20000], Loss: -21.301025390625, Learning Rate: 0.01\n",
      "Epoch [18608/20000], Loss: -21.343597412109375, Learning Rate: 0.01\n",
      "Epoch [18609/20000], Loss: -21.3140869140625, Learning Rate: 0.01\n",
      "Epoch [18610/20000], Loss: -21.27606201171875, Learning Rate: 0.01\n",
      "Epoch [18611/20000], Loss: -21.2926025390625, Learning Rate: 0.01\n",
      "Epoch [18612/20000], Loss: -21.33441162109375, Learning Rate: 0.01\n",
      "Epoch [18613/20000], Loss: -21.339813232421875, Learning Rate: 0.01\n",
      "Epoch [18614/20000], Loss: -21.311965942382812, Learning Rate: 0.01\n",
      "Epoch [18615/20000], Loss: -21.3017578125, Learning Rate: 0.01\n",
      "Epoch [18616/20000], Loss: -21.324081420898438, Learning Rate: 0.01\n",
      "Epoch [18617/20000], Loss: -21.343276977539062, Learning Rate: 0.01\n",
      "Epoch [18618/20000], Loss: -21.334686279296875, Learning Rate: 0.01\n",
      "Epoch [18619/20000], Loss: -21.318450927734375, Learning Rate: 0.01\n",
      "Epoch [18620/20000], Loss: -21.321807861328125, Learning Rate: 0.01\n",
      "Epoch [18621/20000], Loss: -21.338104248046875, Learning Rate: 0.01\n",
      "Epoch [18622/20000], Loss: -21.343276977539062, Learning Rate: 0.01\n",
      "Epoch [18623/20000], Loss: -21.333572387695312, Learning Rate: 0.01\n",
      "Epoch [18624/20000], Loss: -21.327133178710938, Learning Rate: 0.01\n",
      "Epoch [18625/20000], Loss: -21.333938598632812, Learning Rate: 0.01\n",
      "Epoch [18626/20000], Loss: -21.342880249023438, Learning Rate: 0.01\n",
      "Epoch [18627/20000], Loss: -21.342010498046875, Learning Rate: 0.01\n",
      "Epoch [18628/20000], Loss: -21.335342407226562, Learning Rate: 0.01\n",
      "Epoch [18629/20000], Loss: -21.334228515625, Learning Rate: 0.01\n",
      "Epoch [18630/20000], Loss: -21.339935302734375, Learning Rate: 0.01\n",
      "Epoch [18631/20000], Loss: -21.343948364257812, Learning Rate: 0.01\n",
      "Epoch [18632/20000], Loss: -21.341537475585938, Learning Rate: 0.01\n",
      "Epoch [18633/20000], Loss: -21.337860107421875, Learning Rate: 0.01\n",
      "Epoch [18634/20000], Loss: -21.3387451171875, Learning Rate: 0.01\n",
      "Epoch [18635/20000], Loss: -21.342559814453125, Learning Rate: 0.01\n",
      "Epoch [18636/20000], Loss: -21.34393310546875, Learning Rate: 0.01\n",
      "Epoch [18637/20000], Loss: -21.341827392578125, Learning Rate: 0.01\n",
      "Epoch [18638/20000], Loss: -21.34014892578125, Learning Rate: 0.01\n",
      "Epoch [18639/20000], Loss: -21.341323852539062, Learning Rate: 0.01\n",
      "Epoch [18640/20000], Loss: -21.34356689453125, Learning Rate: 0.01\n",
      "Epoch [18641/20000], Loss: -21.3438720703125, Learning Rate: 0.01\n",
      "Epoch [18642/20000], Loss: -21.342422485351562, Learning Rate: 0.01\n",
      "Epoch [18643/20000], Loss: -21.341720581054688, Learning Rate: 0.01\n",
      "Epoch [18644/20000], Loss: -21.342727661132812, Learning Rate: 0.01\n",
      "Epoch [18645/20000], Loss: -21.344024658203125, Learning Rate: 0.01\n",
      "Epoch [18646/20000], Loss: -21.343902587890625, Learning Rate: 0.01\n",
      "Epoch [18647/20000], Loss: -21.34307861328125, Learning Rate: 0.01\n",
      "Epoch [18648/20000], Loss: -21.342849731445312, Learning Rate: 0.01\n",
      "Epoch [18649/20000], Loss: -21.343490600585938, Learning Rate: 0.01\n",
      "Epoch [18650/20000], Loss: -21.344161987304688, Learning Rate: 0.01\n",
      "Epoch [18651/20000], Loss: -21.3441162109375, Learning Rate: 0.01\n",
      "Epoch [18652/20000], Loss: -21.343582153320312, Learning Rate: 0.01\n",
      "Epoch [18653/20000], Loss: -21.34344482421875, Learning Rate: 0.01\n",
      "Epoch [18654/20000], Loss: -21.343902587890625, Learning Rate: 0.01\n",
      "Epoch [18655/20000], Loss: -21.34429931640625, Learning Rate: 0.01\n",
      "Epoch [18656/20000], Loss: -21.34423828125, Learning Rate: 0.01\n",
      "Epoch [18657/20000], Loss: -21.343917846679688, Learning Rate: 0.01\n",
      "Epoch [18658/20000], Loss: -21.343902587890625, Learning Rate: 0.01\n",
      "Epoch [18659/20000], Loss: -21.344085693359375, Learning Rate: 0.01\n",
      "Epoch [18660/20000], Loss: -21.344345092773438, Learning Rate: 0.01\n",
      "Epoch [18661/20000], Loss: -21.344467163085938, Learning Rate: 0.01\n",
      "Epoch [18662/20000], Loss: -21.34417724609375, Learning Rate: 0.01\n",
      "Epoch [18663/20000], Loss: -21.344100952148438, Learning Rate: 0.01\n",
      "Epoch [18664/20000], Loss: -21.34423828125, Learning Rate: 0.01\n",
      "Epoch [18665/20000], Loss: -21.344436645507812, Learning Rate: 0.01\n",
      "Epoch [18666/20000], Loss: -21.34442138671875, Learning Rate: 0.01\n",
      "Epoch [18667/20000], Loss: -21.344406127929688, Learning Rate: 0.01\n",
      "Epoch [18668/20000], Loss: -21.344390869140625, Learning Rate: 0.01\n",
      "Epoch [18669/20000], Loss: -21.344406127929688, Learning Rate: 0.01\n",
      "Epoch [18670/20000], Loss: -21.344512939453125, Learning Rate: 0.01\n",
      "Epoch [18671/20000], Loss: -21.34454345703125, Learning Rate: 0.01\n",
      "Epoch [18672/20000], Loss: -21.344375610351562, Learning Rate: 0.01\n",
      "Epoch [18673/20000], Loss: -21.3443603515625, Learning Rate: 0.01\n",
      "Epoch [18674/20000], Loss: -21.344284057617188, Learning Rate: 0.01\n",
      "Epoch [18675/20000], Loss: -21.344329833984375, Learning Rate: 0.01\n",
      "Epoch [18676/20000], Loss: -21.34423828125, Learning Rate: 0.01\n",
      "Epoch [18677/20000], Loss: -21.344024658203125, Learning Rate: 0.01\n",
      "Epoch [18678/20000], Loss: -21.343643188476562, Learning Rate: 0.01\n",
      "Epoch [18679/20000], Loss: -21.342971801757812, Learning Rate: 0.01\n",
      "Epoch [18680/20000], Loss: -21.34222412109375, Learning Rate: 0.01\n",
      "Epoch [18681/20000], Loss: -21.340896606445312, Learning Rate: 0.01\n",
      "Epoch [18682/20000], Loss: -21.338607788085938, Learning Rate: 0.01\n",
      "Epoch [18683/20000], Loss: -21.33502197265625, Learning Rate: 0.01\n",
      "Epoch [18684/20000], Loss: -21.329238891601562, Learning Rate: 0.01\n",
      "Epoch [18685/20000], Loss: -21.319854736328125, Learning Rate: 0.01\n",
      "Epoch [18686/20000], Loss: -21.304412841796875, Learning Rate: 0.01\n",
      "Epoch [18687/20000], Loss: -21.2791748046875, Learning Rate: 0.01\n",
      "Epoch [18688/20000], Loss: -21.237274169921875, Learning Rate: 0.01\n",
      "Epoch [18689/20000], Loss: -21.168960571289062, Learning Rate: 0.01\n",
      "Epoch [18690/20000], Loss: -21.05621337890625, Learning Rate: 0.01\n",
      "Epoch [18691/20000], Loss: -20.878341674804688, Learning Rate: 0.01\n",
      "Epoch [18692/20000], Loss: -20.596771240234375, Learning Rate: 0.01\n",
      "Epoch [18693/20000], Loss: -20.199386596679688, Learning Rate: 0.01\n",
      "Epoch [18694/20000], Loss: -19.663177490234375, Learning Rate: 0.01\n",
      "Epoch [18695/20000], Loss: -19.145050048828125, Learning Rate: 0.01\n",
      "Epoch [18696/20000], Loss: -18.807891845703125, Learning Rate: 0.01\n",
      "Epoch [18697/20000], Loss: -19.073440551757812, Learning Rate: 0.01\n",
      "Epoch [18698/20000], Loss: -19.874969482421875, Learning Rate: 0.01\n",
      "Epoch [18699/20000], Loss: -20.85125732421875, Learning Rate: 0.01\n",
      "Epoch [18700/20000], Loss: -21.332504272460938, Learning Rate: 0.01\n",
      "Epoch [18701/20000], Loss: -21.117843627929688, Learning Rate: 0.01\n",
      "Epoch [18702/20000], Loss: -20.579071044921875, Learning Rate: 0.01\n",
      "Epoch [18703/20000], Loss: -20.276473999023438, Learning Rate: 0.01\n",
      "Epoch [18704/20000], Loss: -20.520706176757812, Learning Rate: 0.01\n",
      "Epoch [18705/20000], Loss: -21.034561157226562, Learning Rate: 0.01\n",
      "Epoch [18706/20000], Loss: -21.333404541015625, Learning Rate: 0.01\n",
      "Epoch [18707/20000], Loss: -21.202774047851562, Learning Rate: 0.01\n",
      "Epoch [18708/20000], Loss: -20.893463134765625, Learning Rate: 0.01\n",
      "Epoch [18709/20000], Loss: -20.794723510742188, Learning Rate: 0.01\n",
      "Epoch [18710/20000], Loss: -20.99853515625, Learning Rate: 0.01\n",
      "Epoch [18711/20000], Loss: -21.270065307617188, Learning Rate: 0.01\n",
      "Epoch [18712/20000], Loss: -21.329254150390625, Learning Rate: 0.01\n",
      "Epoch [18713/20000], Loss: -21.17742919921875, Learning Rate: 0.01\n",
      "Epoch [18714/20000], Loss: -21.042160034179688, Learning Rate: 0.01\n",
      "Epoch [18715/20000], Loss: -21.086029052734375, Learning Rate: 0.01\n",
      "Epoch [18716/20000], Loss: -21.247817993164062, Learning Rate: 0.01\n",
      "Epoch [18717/20000], Loss: -21.340545654296875, Learning Rate: 0.01\n",
      "Epoch [18718/20000], Loss: -21.289077758789062, Learning Rate: 0.01\n",
      "Epoch [18719/20000], Loss: -21.191268920898438, Learning Rate: 0.01\n",
      "Epoch [18720/20000], Loss: -21.175811767578125, Learning Rate: 0.01\n",
      "Epoch [18721/20000], Loss: -21.256195068359375, Learning Rate: 0.01\n",
      "Epoch [18722/20000], Loss: -21.333145141601562, Learning Rate: 0.01\n",
      "Epoch [18723/20000], Loss: -21.330154418945312, Learning Rate: 0.01\n",
      "Epoch [18724/20000], Loss: -21.273361206054688, Learning Rate: 0.01\n",
      "Epoch [18725/20000], Loss: -21.242843627929688, Learning Rate: 0.01\n",
      "Epoch [18726/20000], Loss: -21.2744140625, Learning Rate: 0.01\n",
      "Epoch [18727/20000], Loss: -21.326416015625, Learning Rate: 0.01\n",
      "Epoch [18728/20000], Loss: -21.342453002929688, Learning Rate: 0.01\n",
      "Epoch [18729/20000], Loss: -21.316116333007812, Learning Rate: 0.01\n",
      "Epoch [18730/20000], Loss: -21.288299560546875, Learning Rate: 0.01\n",
      "Epoch [18731/20000], Loss: -21.293777465820312, Learning Rate: 0.01\n",
      "Epoch [18732/20000], Loss: -21.323318481445312, Learning Rate: 0.01\n",
      "Epoch [18733/20000], Loss: -21.343231201171875, Learning Rate: 0.01\n",
      "Epoch [18734/20000], Loss: -21.336212158203125, Learning Rate: 0.01\n",
      "Epoch [18735/20000], Loss: -21.317398071289062, Learning Rate: 0.01\n",
      "Epoch [18736/20000], Loss: -21.311416625976562, Learning Rate: 0.01\n",
      "Epoch [18737/20000], Loss: -21.32421875, Learning Rate: 0.01\n",
      "Epoch [18738/20000], Loss: -21.340286254882812, Learning Rate: 0.01\n",
      "Epoch [18739/20000], Loss: -21.343429565429688, Learning Rate: 0.01\n",
      "Epoch [18740/20000], Loss: -21.33416748046875, Learning Rate: 0.01\n",
      "Epoch [18741/20000], Loss: -21.3258056640625, Learning Rate: 0.01\n",
      "Epoch [18742/20000], Loss: -21.328140258789062, Learning Rate: 0.01\n",
      "Epoch [18743/20000], Loss: -21.337692260742188, Learning Rate: 0.01\n",
      "Epoch [18744/20000], Loss: -21.344024658203125, Learning Rate: 0.01\n",
      "Epoch [18745/20000], Loss: -21.342117309570312, Learning Rate: 0.01\n",
      "Epoch [18746/20000], Loss: -21.33612060546875, Learning Rate: 0.01\n",
      "Epoch [18747/20000], Loss: -21.333633422851562, Learning Rate: 0.01\n",
      "Epoch [18748/20000], Loss: -21.33709716796875, Learning Rate: 0.01\n",
      "Epoch [18749/20000], Loss: -21.342483520507812, Learning Rate: 0.01\n",
      "Epoch [18750/20000], Loss: -21.344482421875, Learning Rate: 0.01\n",
      "Epoch [18751/20000], Loss: -21.342178344726562, Learning Rate: 0.01\n",
      "Epoch [18752/20000], Loss: -21.339019775390625, Learning Rate: 0.01\n",
      "Epoch [18753/20000], Loss: -21.338714599609375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18754/20000], Loss: -21.34130859375, Learning Rate: 0.01\n",
      "Epoch [18755/20000], Loss: -21.34393310546875, Learning Rate: 0.01\n",
      "Epoch [18756/20000], Loss: -21.344375610351562, Learning Rate: 0.01\n",
      "Epoch [18757/20000], Loss: -21.34283447265625, Learning Rate: 0.01\n",
      "Epoch [18758/20000], Loss: -21.34136962890625, Learning Rate: 0.01\n",
      "Epoch [18759/20000], Loss: -21.341583251953125, Learning Rate: 0.01\n",
      "Epoch [18760/20000], Loss: -21.343002319335938, Learning Rate: 0.01\n",
      "Epoch [18761/20000], Loss: -21.344406127929688, Learning Rate: 0.01\n",
      "Epoch [18762/20000], Loss: -21.344451904296875, Learning Rate: 0.01\n",
      "Epoch [18763/20000], Loss: -21.343643188476562, Learning Rate: 0.01\n",
      "Epoch [18764/20000], Loss: -21.342849731445312, Learning Rate: 0.01\n",
      "Epoch [18765/20000], Loss: -21.34307861328125, Learning Rate: 0.01\n",
      "Epoch [18766/20000], Loss: -21.3438720703125, Learning Rate: 0.01\n",
      "Epoch [18767/20000], Loss: -21.344558715820312, Learning Rate: 0.01\n",
      "Epoch [18768/20000], Loss: -21.344680786132812, Learning Rate: 0.01\n",
      "Epoch [18769/20000], Loss: -21.344223022460938, Learning Rate: 0.01\n",
      "Epoch [18770/20000], Loss: -21.343765258789062, Learning Rate: 0.01\n",
      "Epoch [18771/20000], Loss: -21.343826293945312, Learning Rate: 0.01\n",
      "Epoch [18772/20000], Loss: -21.344268798828125, Learning Rate: 0.01\n",
      "Epoch [18773/20000], Loss: -21.344680786132812, Learning Rate: 0.01\n",
      "Epoch [18774/20000], Loss: -21.344772338867188, Learning Rate: 0.01\n",
      "Epoch [18775/20000], Loss: -21.34454345703125, Learning Rate: 0.01\n",
      "Epoch [18776/20000], Loss: -21.344375610351562, Learning Rate: 0.01\n",
      "Epoch [18777/20000], Loss: -21.344329833984375, Learning Rate: 0.01\n",
      "Epoch [18778/20000], Loss: -21.344467163085938, Learning Rate: 0.01\n",
      "Epoch [18779/20000], Loss: -21.3447265625, Learning Rate: 0.01\n",
      "Epoch [18780/20000], Loss: -21.344924926757812, Learning Rate: 0.01\n",
      "Epoch [18781/20000], Loss: -21.344833374023438, Learning Rate: 0.01\n",
      "Epoch [18782/20000], Loss: -21.344696044921875, Learning Rate: 0.01\n",
      "Epoch [18783/20000], Loss: -21.344650268554688, Learning Rate: 0.01\n",
      "Epoch [18784/20000], Loss: -21.344650268554688, Learning Rate: 0.01\n",
      "Epoch [18785/20000], Loss: -21.344818115234375, Learning Rate: 0.01\n",
      "Epoch [18786/20000], Loss: -21.344970703125, Learning Rate: 0.01\n",
      "Epoch [18787/20000], Loss: -21.344940185546875, Learning Rate: 0.01\n",
      "Epoch [18788/20000], Loss: -21.34490966796875, Learning Rate: 0.01\n",
      "Epoch [18789/20000], Loss: -21.3448486328125, Learning Rate: 0.01\n",
      "Epoch [18790/20000], Loss: -21.344894409179688, Learning Rate: 0.01\n",
      "Epoch [18791/20000], Loss: -21.344940185546875, Learning Rate: 0.01\n",
      "Epoch [18792/20000], Loss: -21.345001220703125, Learning Rate: 0.01\n",
      "Epoch [18793/20000], Loss: -21.34503173828125, Learning Rate: 0.01\n",
      "Epoch [18794/20000], Loss: -21.34503173828125, Learning Rate: 0.01\n",
      "Epoch [18795/20000], Loss: -21.3450927734375, Learning Rate: 0.01\n",
      "Epoch [18796/20000], Loss: -21.344970703125, Learning Rate: 0.01\n",
      "Epoch [18797/20000], Loss: -21.345046997070312, Learning Rate: 0.01\n",
      "Epoch [18798/20000], Loss: -21.345108032226562, Learning Rate: 0.01\n",
      "Epoch [18799/20000], Loss: -21.345077514648438, Learning Rate: 0.01\n",
      "Epoch [18800/20000], Loss: -21.345138549804688, Learning Rate: 0.01\n",
      "Epoch [18801/20000], Loss: -21.345138549804688, Learning Rate: 0.01\n",
      "Epoch [18802/20000], Loss: -21.3450927734375, Learning Rate: 0.01\n",
      "Epoch [18803/20000], Loss: -21.34515380859375, Learning Rate: 0.01\n",
      "Epoch [18804/20000], Loss: -21.34515380859375, Learning Rate: 0.01\n",
      "Epoch [18805/20000], Loss: -21.345169067382812, Learning Rate: 0.01\n",
      "Epoch [18806/20000], Loss: -21.345245361328125, Learning Rate: 0.01\n",
      "Epoch [18807/20000], Loss: -21.345260620117188, Learning Rate: 0.01\n",
      "Epoch [18808/20000], Loss: -21.34521484375, Learning Rate: 0.01\n",
      "Epoch [18809/20000], Loss: -21.3453369140625, Learning Rate: 0.01\n",
      "Epoch [18810/20000], Loss: -21.345245361328125, Learning Rate: 0.01\n",
      "Epoch [18811/20000], Loss: -21.34527587890625, Learning Rate: 0.01\n",
      "Epoch [18812/20000], Loss: -21.34527587890625, Learning Rate: 0.01\n",
      "Epoch [18813/20000], Loss: -21.3453369140625, Learning Rate: 0.01\n",
      "Epoch [18814/20000], Loss: -21.345306396484375, Learning Rate: 0.01\n",
      "Epoch [18815/20000], Loss: -21.3453369140625, Learning Rate: 0.01\n",
      "Epoch [18816/20000], Loss: -21.345306396484375, Learning Rate: 0.01\n",
      "Epoch [18817/20000], Loss: -21.345306396484375, Learning Rate: 0.01\n",
      "Epoch [18818/20000], Loss: -21.345443725585938, Learning Rate: 0.01\n",
      "Epoch [18819/20000], Loss: -21.345367431640625, Learning Rate: 0.01\n",
      "Epoch [18820/20000], Loss: -21.345443725585938, Learning Rate: 0.01\n",
      "Epoch [18821/20000], Loss: -21.345413208007812, Learning Rate: 0.01\n",
      "Epoch [18822/20000], Loss: -21.34539794921875, Learning Rate: 0.01\n",
      "Epoch [18823/20000], Loss: -21.345413208007812, Learning Rate: 0.01\n",
      "Epoch [18824/20000], Loss: -21.345474243164062, Learning Rate: 0.01\n",
      "Epoch [18825/20000], Loss: -21.345458984375, Learning Rate: 0.01\n",
      "Epoch [18826/20000], Loss: -21.345428466796875, Learning Rate: 0.01\n",
      "Epoch [18827/20000], Loss: -21.34552001953125, Learning Rate: 0.01\n",
      "Epoch [18828/20000], Loss: -21.345596313476562, Learning Rate: 0.01\n",
      "Epoch [18829/20000], Loss: -21.345565795898438, Learning Rate: 0.01\n",
      "Epoch [18830/20000], Loss: -21.345626831054688, Learning Rate: 0.01\n",
      "Epoch [18831/20000], Loss: -21.345550537109375, Learning Rate: 0.01\n",
      "Epoch [18832/20000], Loss: -21.345535278320312, Learning Rate: 0.01\n",
      "Epoch [18833/20000], Loss: -21.345611572265625, Learning Rate: 0.01\n",
      "Epoch [18834/20000], Loss: -21.345626831054688, Learning Rate: 0.01\n",
      "Epoch [18835/20000], Loss: -21.345565795898438, Learning Rate: 0.01\n",
      "Epoch [18836/20000], Loss: -21.345626831054688, Learning Rate: 0.01\n",
      "Epoch [18837/20000], Loss: -21.3455810546875, Learning Rate: 0.01\n",
      "Epoch [18838/20000], Loss: -21.34564208984375, Learning Rate: 0.01\n",
      "Epoch [18839/20000], Loss: -21.345611572265625, Learning Rate: 0.01\n",
      "Epoch [18840/20000], Loss: -21.345657348632812, Learning Rate: 0.01\n",
      "Epoch [18841/20000], Loss: -21.345672607421875, Learning Rate: 0.01\n",
      "Epoch [18842/20000], Loss: -21.345703125, Learning Rate: 0.01\n",
      "Epoch [18843/20000], Loss: -21.345779418945312, Learning Rate: 0.01\n",
      "Epoch [18844/20000], Loss: -21.345703125, Learning Rate: 0.01\n",
      "Epoch [18845/20000], Loss: -21.345687866210938, Learning Rate: 0.01\n",
      "Epoch [18846/20000], Loss: -21.345809936523438, Learning Rate: 0.01\n",
      "Epoch [18847/20000], Loss: -21.345779418945312, Learning Rate: 0.01\n",
      "Epoch [18848/20000], Loss: -21.34576416015625, Learning Rate: 0.01\n",
      "Epoch [18849/20000], Loss: -21.345748901367188, Learning Rate: 0.01\n",
      "Epoch [18850/20000], Loss: -21.345809936523438, Learning Rate: 0.01\n",
      "Epoch [18851/20000], Loss: -21.345779418945312, Learning Rate: 0.01\n",
      "Epoch [18852/20000], Loss: -21.345840454101562, Learning Rate: 0.01\n",
      "Epoch [18853/20000], Loss: -21.345794677734375, Learning Rate: 0.01\n",
      "Epoch [18854/20000], Loss: -21.345840454101562, Learning Rate: 0.01\n",
      "Epoch [18855/20000], Loss: -21.345855712890625, Learning Rate: 0.01\n",
      "Epoch [18856/20000], Loss: -21.345901489257812, Learning Rate: 0.01\n",
      "Epoch [18857/20000], Loss: -21.345855712890625, Learning Rate: 0.01\n",
      "Epoch [18858/20000], Loss: -21.345901489257812, Learning Rate: 0.01\n",
      "Epoch [18859/20000], Loss: -21.345916748046875, Learning Rate: 0.01\n",
      "Epoch [18860/20000], Loss: -21.345870971679688, Learning Rate: 0.01\n",
      "Epoch [18861/20000], Loss: -21.345947265625, Learning Rate: 0.01\n",
      "Epoch [18862/20000], Loss: -21.345962524414062, Learning Rate: 0.01\n",
      "Epoch [18863/20000], Loss: -21.345901489257812, Learning Rate: 0.01\n",
      "Epoch [18864/20000], Loss: -21.345962524414062, Learning Rate: 0.01\n",
      "Epoch [18865/20000], Loss: -21.345977783203125, Learning Rate: 0.01\n",
      "Epoch [18866/20000], Loss: -21.34600830078125, Learning Rate: 0.01\n",
      "Epoch [18867/20000], Loss: -21.346023559570312, Learning Rate: 0.01\n",
      "Epoch [18868/20000], Loss: -21.34600830078125, Learning Rate: 0.01\n",
      "Epoch [18869/20000], Loss: -21.346084594726562, Learning Rate: 0.01\n",
      "Epoch [18870/20000], Loss: -21.346099853515625, Learning Rate: 0.01\n",
      "Epoch [18871/20000], Loss: -21.346084594726562, Learning Rate: 0.01\n",
      "Epoch [18872/20000], Loss: -21.3460693359375, Learning Rate: 0.01\n",
      "Epoch [18873/20000], Loss: -21.345977783203125, Learning Rate: 0.01\n",
      "Epoch [18874/20000], Loss: -21.345993041992188, Learning Rate: 0.01\n",
      "Epoch [18875/20000], Loss: -21.3460693359375, Learning Rate: 0.01\n",
      "Epoch [18876/20000], Loss: -21.346099853515625, Learning Rate: 0.01\n",
      "Epoch [18877/20000], Loss: -21.346145629882812, Learning Rate: 0.01\n",
      "Epoch [18878/20000], Loss: -21.346160888671875, Learning Rate: 0.01\n",
      "Epoch [18879/20000], Loss: -21.34619140625, Learning Rate: 0.01\n",
      "Epoch [18880/20000], Loss: -21.34619140625, Learning Rate: 0.01\n",
      "Epoch [18881/20000], Loss: -21.34613037109375, Learning Rate: 0.01\n",
      "Epoch [18882/20000], Loss: -21.34619140625, Learning Rate: 0.01\n",
      "Epoch [18883/20000], Loss: -21.346176147460938, Learning Rate: 0.01\n",
      "Epoch [18884/20000], Loss: -21.34619140625, Learning Rate: 0.01\n",
      "Epoch [18885/20000], Loss: -21.346176147460938, Learning Rate: 0.01\n",
      "Epoch [18886/20000], Loss: -21.34619140625, Learning Rate: 0.01\n",
      "Epoch [18887/20000], Loss: -21.346206665039062, Learning Rate: 0.01\n",
      "Epoch [18888/20000], Loss: -21.346267700195312, Learning Rate: 0.01\n",
      "Epoch [18889/20000], Loss: -21.346267700195312, Learning Rate: 0.01\n",
      "Epoch [18890/20000], Loss: -21.346343994140625, Learning Rate: 0.01\n",
      "Epoch [18891/20000], Loss: -21.346343994140625, Learning Rate: 0.01\n",
      "Epoch [18892/20000], Loss: -21.346298217773438, Learning Rate: 0.01\n",
      "Epoch [18893/20000], Loss: -21.34625244140625, Learning Rate: 0.01\n",
      "Epoch [18894/20000], Loss: -21.346343994140625, Learning Rate: 0.01\n",
      "Epoch [18895/20000], Loss: -21.34637451171875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18896/20000], Loss: -21.346389770507812, Learning Rate: 0.01\n",
      "Epoch [18897/20000], Loss: -21.346481323242188, Learning Rate: 0.01\n",
      "Epoch [18898/20000], Loss: -21.346389770507812, Learning Rate: 0.01\n",
      "Epoch [18899/20000], Loss: -21.346405029296875, Learning Rate: 0.01\n",
      "Epoch [18900/20000], Loss: -21.34649658203125, Learning Rate: 0.01\n",
      "Epoch [18901/20000], Loss: -21.34637451171875, Learning Rate: 0.01\n",
      "Epoch [18902/20000], Loss: -21.346450805664062, Learning Rate: 0.01\n",
      "Epoch [18903/20000], Loss: -21.346450805664062, Learning Rate: 0.01\n",
      "Epoch [18904/20000], Loss: -21.34649658203125, Learning Rate: 0.01\n",
      "Epoch [18905/20000], Loss: -21.346481323242188, Learning Rate: 0.01\n",
      "Epoch [18906/20000], Loss: -21.346481323242188, Learning Rate: 0.01\n",
      "Epoch [18907/20000], Loss: -21.346511840820312, Learning Rate: 0.01\n",
      "Epoch [18908/20000], Loss: -21.346511840820312, Learning Rate: 0.01\n",
      "Epoch [18909/20000], Loss: -21.34649658203125, Learning Rate: 0.01\n",
      "Epoch [18910/20000], Loss: -21.3465576171875, Learning Rate: 0.01\n",
      "Epoch [18911/20000], Loss: -21.34649658203125, Learning Rate: 0.01\n",
      "Epoch [18912/20000], Loss: -21.346435546875, Learning Rate: 0.01\n",
      "Epoch [18913/20000], Loss: -21.346481323242188, Learning Rate: 0.01\n",
      "Epoch [18914/20000], Loss: -21.346435546875, Learning Rate: 0.01\n",
      "Epoch [18915/20000], Loss: -21.346450805664062, Learning Rate: 0.01\n",
      "Epoch [18916/20000], Loss: -21.346328735351562, Learning Rate: 0.01\n",
      "Epoch [18917/20000], Loss: -21.346160888671875, Learning Rate: 0.01\n",
      "Epoch [18918/20000], Loss: -21.34588623046875, Learning Rate: 0.01\n",
      "Epoch [18919/20000], Loss: -21.345550537109375, Learning Rate: 0.01\n",
      "Epoch [18920/20000], Loss: -21.345062255859375, Learning Rate: 0.01\n",
      "Epoch [18921/20000], Loss: -21.344192504882812, Learning Rate: 0.01\n",
      "Epoch [18922/20000], Loss: -21.342803955078125, Learning Rate: 0.01\n",
      "Epoch [18923/20000], Loss: -21.340682983398438, Learning Rate: 0.01\n",
      "Epoch [18924/20000], Loss: -21.337203979492188, Learning Rate: 0.01\n",
      "Epoch [18925/20000], Loss: -21.331436157226562, Learning Rate: 0.01\n",
      "Epoch [18926/20000], Loss: -21.322235107421875, Learning Rate: 0.01\n",
      "Epoch [18927/20000], Loss: -21.30694580078125, Learning Rate: 0.01\n",
      "Epoch [18928/20000], Loss: -21.282089233398438, Learning Rate: 0.01\n",
      "Epoch [18929/20000], Loss: -21.24090576171875, Learning Rate: 0.01\n",
      "Epoch [18930/20000], Loss: -21.173721313476562, Learning Rate: 0.01\n",
      "Epoch [18931/20000], Loss: -21.062515258789062, Learning Rate: 0.01\n",
      "Epoch [18932/20000], Loss: -20.88629150390625, Learning Rate: 0.01\n",
      "Epoch [18933/20000], Loss: -20.605316162109375, Learning Rate: 0.01\n",
      "Epoch [18934/20000], Loss: -20.200469970703125, Learning Rate: 0.01\n",
      "Epoch [18935/20000], Loss: -19.640029907226562, Learning Rate: 0.01\n",
      "Epoch [18936/20000], Loss: -19.05584716796875, Learning Rate: 0.01\n",
      "Epoch [18937/20000], Loss: -18.605499267578125, Learning Rate: 0.01\n",
      "Epoch [18938/20000], Loss: -18.7454833984375, Learning Rate: 0.01\n",
      "Epoch [18939/20000], Loss: -19.5037841796875, Learning Rate: 0.01\n",
      "Epoch [18940/20000], Loss: -20.602066040039062, Learning Rate: 0.01\n",
      "Epoch [18941/20000], Loss: -21.2822265625, Learning Rate: 0.01\n",
      "Epoch [18942/20000], Loss: -21.188888549804688, Learning Rate: 0.01\n",
      "Epoch [18943/20000], Loss: -20.6185302734375, Learning Rate: 0.01\n",
      "Epoch [18944/20000], Loss: -20.196517944335938, Learning Rate: 0.01\n",
      "Epoch [18945/20000], Loss: -20.363296508789062, Learning Rate: 0.01\n",
      "Epoch [18946/20000], Loss: -20.914581298828125, Learning Rate: 0.01\n",
      "Epoch [18947/20000], Loss: -21.314559936523438, Learning Rate: 0.01\n",
      "Epoch [18948/20000], Loss: -21.234390258789062, Learning Rate: 0.01\n",
      "Epoch [18949/20000], Loss: -20.889434814453125, Learning Rate: 0.01\n",
      "Epoch [18950/20000], Loss: -20.731674194335938, Learning Rate: 0.01\n",
      "Epoch [18951/20000], Loss: -20.92681884765625, Learning Rate: 0.01\n",
      "Epoch [18952/20000], Loss: -21.243423461914062, Learning Rate: 0.01\n",
      "Epoch [18953/20000], Loss: -21.339736938476562, Learning Rate: 0.01\n",
      "Epoch [18954/20000], Loss: -21.179107666015625, Learning Rate: 0.01\n",
      "Epoch [18955/20000], Loss: -21.01361083984375, Learning Rate: 0.01\n",
      "Epoch [18956/20000], Loss: -21.048980712890625, Learning Rate: 0.01\n",
      "Epoch [18957/20000], Loss: -21.231887817382812, Learning Rate: 0.01\n",
      "Epoch [18958/20000], Loss: -21.343765258789062, Learning Rate: 0.01\n",
      "Epoch [18959/20000], Loss: -21.288055419921875, Learning Rate: 0.01\n",
      "Epoch [18960/20000], Loss: -21.175247192382812, Learning Rate: 0.01\n",
      "Epoch [18961/20000], Loss: -21.15728759765625, Learning Rate: 0.01\n",
      "Epoch [18962/20000], Loss: -21.2503662109375, Learning Rate: 0.01\n",
      "Epoch [18963/20000], Loss: -21.336471557617188, Learning Rate: 0.01\n",
      "Epoch [18964/20000], Loss: -21.328094482421875, Learning Rate: 0.01\n",
      "Epoch [18965/20000], Loss: -21.261627197265625, Learning Rate: 0.01\n",
      "Epoch [18966/20000], Loss: -21.231689453125, Learning Rate: 0.01\n",
      "Epoch [18967/20000], Loss: -21.273468017578125, Learning Rate: 0.01\n",
      "Epoch [18968/20000], Loss: -21.3316650390625, Learning Rate: 0.01\n",
      "Epoch [18969/20000], Loss: -21.342819213867188, Learning Rate: 0.01\n",
      "Epoch [18970/20000], Loss: -21.30804443359375, Learning Rate: 0.01\n",
      "Epoch [18971/20000], Loss: -21.28009033203125, Learning Rate: 0.01\n",
      "Epoch [18972/20000], Loss: -21.29351806640625, Learning Rate: 0.01\n",
      "Epoch [18973/20000], Loss: -21.328857421875, Learning Rate: 0.01\n",
      "Epoch [18974/20000], Loss: -21.3458251953125, Learning Rate: 0.01\n",
      "Epoch [18975/20000], Loss: -21.331878662109375, Learning Rate: 0.01\n",
      "Epoch [18976/20000], Loss: -21.311248779296875, Learning Rate: 0.01\n",
      "Epoch [18977/20000], Loss: -21.31048583984375, Learning Rate: 0.01\n",
      "Epoch [18978/20000], Loss: -21.32891845703125, Learning Rate: 0.01\n",
      "Epoch [18979/20000], Loss: -21.344528198242188, Learning Rate: 0.01\n",
      "Epoch [18980/20000], Loss: -21.3424072265625, Learning Rate: 0.01\n",
      "Epoch [18981/20000], Loss: -21.329971313476562, Learning Rate: 0.01\n",
      "Epoch [18982/20000], Loss: -21.324020385742188, Learning Rate: 0.01\n",
      "Epoch [18983/20000], Loss: -21.330978393554688, Learning Rate: 0.01\n",
      "Epoch [18984/20000], Loss: -21.342239379882812, Learning Rate: 0.01\n",
      "Epoch [18985/20000], Loss: -21.34588623046875, Learning Rate: 0.01\n",
      "Epoch [18986/20000], Loss: -21.340469360351562, Learning Rate: 0.01\n",
      "Epoch [18987/20000], Loss: -21.334243774414062, Learning Rate: 0.01\n",
      "Epoch [18988/20000], Loss: -21.33465576171875, Learning Rate: 0.01\n",
      "Epoch [18989/20000], Loss: -21.34075927734375, Learning Rate: 0.01\n",
      "Epoch [18990/20000], Loss: -21.345611572265625, Learning Rate: 0.01\n",
      "Epoch [18991/20000], Loss: -21.345001220703125, Learning Rate: 0.01\n",
      "Epoch [18992/20000], Loss: -21.341079711914062, Learning Rate: 0.01\n",
      "Epoch [18993/20000], Loss: -21.339019775390625, Learning Rate: 0.01\n",
      "Epoch [18994/20000], Loss: -21.340866088867188, Learning Rate: 0.01\n",
      "Epoch [18995/20000], Loss: -21.344482421875, Learning Rate: 0.01\n",
      "Epoch [18996/20000], Loss: -21.346145629882812, Learning Rate: 0.01\n",
      "Epoch [18997/20000], Loss: -21.344970703125, Learning Rate: 0.01\n",
      "Epoch [18998/20000], Loss: -21.3427734375, Learning Rate: 0.01\n",
      "Epoch [18999/20000], Loss: -21.342254638671875, Learning Rate: 0.01\n",
      "Epoch [19000/20000], Loss: -21.34381103515625, Learning Rate: 0.01\n",
      "Epoch [19001/20000], Loss: -21.3455810546875, Learning Rate: 0.01\n",
      "Epoch [19002/20000], Loss: -21.346160888671875, Learning Rate: 0.01\n",
      "Epoch [19003/20000], Loss: -21.345367431640625, Learning Rate: 0.01\n",
      "Epoch [19004/20000], Loss: -21.34423828125, Learning Rate: 0.01\n",
      "Epoch [19005/20000], Loss: -21.344161987304688, Learning Rate: 0.01\n",
      "Epoch [19006/20000], Loss: -21.345062255859375, Learning Rate: 0.01\n",
      "Epoch [19007/20000], Loss: -21.34613037109375, Learning Rate: 0.01\n",
      "Epoch [19008/20000], Loss: -21.346343994140625, Learning Rate: 0.01\n",
      "Epoch [19009/20000], Loss: -21.345794677734375, Learning Rate: 0.01\n",
      "Epoch [19010/20000], Loss: -21.345230102539062, Learning Rate: 0.01\n",
      "Epoch [19011/20000], Loss: -21.34521484375, Learning Rate: 0.01\n",
      "Epoch [19012/20000], Loss: -21.345733642578125, Learning Rate: 0.01\n",
      "Epoch [19013/20000], Loss: -21.346237182617188, Learning Rate: 0.01\n",
      "Epoch [19014/20000], Loss: -21.346405029296875, Learning Rate: 0.01\n",
      "Epoch [19015/20000], Loss: -21.346084594726562, Learning Rate: 0.01\n",
      "Epoch [19016/20000], Loss: -21.345809936523438, Learning Rate: 0.01\n",
      "Epoch [19017/20000], Loss: -21.345840454101562, Learning Rate: 0.01\n",
      "Epoch [19018/20000], Loss: -21.346023559570312, Learning Rate: 0.01\n",
      "Epoch [19019/20000], Loss: -21.346328735351562, Learning Rate: 0.01\n",
      "Epoch [19020/20000], Loss: -21.346450805664062, Learning Rate: 0.01\n",
      "Epoch [19021/20000], Loss: -21.346389770507812, Learning Rate: 0.01\n",
      "Epoch [19022/20000], Loss: -21.346267700195312, Learning Rate: 0.01\n",
      "Epoch [19023/20000], Loss: -21.346176147460938, Learning Rate: 0.01\n",
      "Epoch [19024/20000], Loss: -21.3463134765625, Learning Rate: 0.01\n",
      "Epoch [19025/20000], Loss: -21.346435546875, Learning Rate: 0.01\n",
      "Epoch [19026/20000], Loss: -21.346542358398438, Learning Rate: 0.01\n",
      "Epoch [19027/20000], Loss: -21.346527099609375, Learning Rate: 0.01\n",
      "Epoch [19028/20000], Loss: -21.346511840820312, Learning Rate: 0.01\n",
      "Epoch [19029/20000], Loss: -21.346420288085938, Learning Rate: 0.01\n",
      "Epoch [19030/20000], Loss: -21.346466064453125, Learning Rate: 0.01\n",
      "Epoch [19031/20000], Loss: -21.346527099609375, Learning Rate: 0.01\n",
      "Epoch [19032/20000], Loss: -21.346542358398438, Learning Rate: 0.01\n",
      "Epoch [19033/20000], Loss: -21.346633911132812, Learning Rate: 0.01\n",
      "Epoch [19034/20000], Loss: -21.34661865234375, Learning Rate: 0.01\n",
      "Epoch [19035/20000], Loss: -21.346633911132812, Learning Rate: 0.01\n",
      "Epoch [19036/20000], Loss: -21.346572875976562, Learning Rate: 0.01\n",
      "Epoch [19037/20000], Loss: -21.346603393554688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19038/20000], Loss: -21.346694946289062, Learning Rate: 0.01\n",
      "Epoch [19039/20000], Loss: -21.346755981445312, Learning Rate: 0.01\n",
      "Epoch [19040/20000], Loss: -21.346847534179688, Learning Rate: 0.01\n",
      "Epoch [19041/20000], Loss: -21.346710205078125, Learning Rate: 0.01\n",
      "Epoch [19042/20000], Loss: -21.346694946289062, Learning Rate: 0.01\n",
      "Epoch [19043/20000], Loss: -21.34674072265625, Learning Rate: 0.01\n",
      "Epoch [19044/20000], Loss: -21.346725463867188, Learning Rate: 0.01\n",
      "Epoch [19045/20000], Loss: -21.3468017578125, Learning Rate: 0.01\n",
      "Epoch [19046/20000], Loss: -21.34686279296875, Learning Rate: 0.01\n",
      "Epoch [19047/20000], Loss: -21.346878051757812, Learning Rate: 0.01\n",
      "Epoch [19048/20000], Loss: -21.346847534179688, Learning Rate: 0.01\n",
      "Epoch [19049/20000], Loss: -21.346832275390625, Learning Rate: 0.01\n",
      "Epoch [19050/20000], Loss: -21.346832275390625, Learning Rate: 0.01\n",
      "Epoch [19051/20000], Loss: -21.34686279296875, Learning Rate: 0.01\n",
      "Epoch [19052/20000], Loss: -21.346923828125, Learning Rate: 0.01\n",
      "Epoch [19053/20000], Loss: -21.34686279296875, Learning Rate: 0.01\n",
      "Epoch [19054/20000], Loss: -21.346923828125, Learning Rate: 0.01\n",
      "Epoch [19055/20000], Loss: -21.346954345703125, Learning Rate: 0.01\n",
      "Epoch [19056/20000], Loss: -21.346954345703125, Learning Rate: 0.01\n",
      "Epoch [19057/20000], Loss: -21.346939086914062, Learning Rate: 0.01\n",
      "Epoch [19058/20000], Loss: -21.347015380859375, Learning Rate: 0.01\n",
      "Epoch [19059/20000], Loss: -21.34698486328125, Learning Rate: 0.01\n",
      "Epoch [19060/20000], Loss: -21.347000122070312, Learning Rate: 0.01\n",
      "Epoch [19061/20000], Loss: -21.347000122070312, Learning Rate: 0.01\n",
      "Epoch [19062/20000], Loss: -21.347030639648438, Learning Rate: 0.01\n",
      "Epoch [19063/20000], Loss: -21.347015380859375, Learning Rate: 0.01\n",
      "Epoch [19064/20000], Loss: -21.347015380859375, Learning Rate: 0.01\n",
      "Epoch [19065/20000], Loss: -21.3470458984375, Learning Rate: 0.01\n",
      "Epoch [19066/20000], Loss: -21.347091674804688, Learning Rate: 0.01\n",
      "Epoch [19067/20000], Loss: -21.347000122070312, Learning Rate: 0.01\n",
      "Epoch [19068/20000], Loss: -21.347061157226562, Learning Rate: 0.01\n",
      "Epoch [19069/20000], Loss: -21.34698486328125, Learning Rate: 0.01\n",
      "Epoch [19070/20000], Loss: -21.346954345703125, Learning Rate: 0.01\n",
      "Epoch [19071/20000], Loss: -21.34686279296875, Learning Rate: 0.01\n",
      "Epoch [19072/20000], Loss: -21.346847534179688, Learning Rate: 0.01\n",
      "Epoch [19073/20000], Loss: -21.346603393554688, Learning Rate: 0.01\n",
      "Epoch [19074/20000], Loss: -21.346328735351562, Learning Rate: 0.01\n",
      "Epoch [19075/20000], Loss: -21.345932006835938, Learning Rate: 0.01\n",
      "Epoch [19076/20000], Loss: -21.345260620117188, Learning Rate: 0.01\n",
      "Epoch [19077/20000], Loss: -21.344268798828125, Learning Rate: 0.01\n",
      "Epoch [19078/20000], Loss: -21.342636108398438, Learning Rate: 0.01\n",
      "Epoch [19079/20000], Loss: -21.340087890625, Learning Rate: 0.01\n",
      "Epoch [19080/20000], Loss: -21.335922241210938, Learning Rate: 0.01\n",
      "Epoch [19081/20000], Loss: -21.329193115234375, Learning Rate: 0.01\n",
      "Epoch [19082/20000], Loss: -21.318435668945312, Learning Rate: 0.01\n",
      "Epoch [19083/20000], Loss: -21.300857543945312, Learning Rate: 0.01\n",
      "Epoch [19084/20000], Loss: -21.272262573242188, Learning Rate: 0.01\n",
      "Epoch [19085/20000], Loss: -21.226394653320312, Learning Rate: 0.01\n",
      "Epoch [19086/20000], Loss: -21.152450561523438, Learning Rate: 0.01\n",
      "Epoch [19087/20000], Loss: -21.038421630859375, Learning Rate: 0.01\n",
      "Epoch [19088/20000], Loss: -20.866195678710938, Learning Rate: 0.01\n",
      "Epoch [19089/20000], Loss: -20.634048461914062, Learning Rate: 0.01\n",
      "Epoch [19090/20000], Loss: -20.351516723632812, Learning Rate: 0.01\n",
      "Epoch [19091/20000], Loss: -20.117416381835938, Learning Rate: 0.01\n",
      "Epoch [19092/20000], Loss: -20.042922973632812, Learning Rate: 0.01\n",
      "Epoch [19093/20000], Loss: -20.282455444335938, Learning Rate: 0.01\n",
      "Epoch [19094/20000], Loss: -20.737472534179688, Learning Rate: 0.01\n",
      "Epoch [19095/20000], Loss: -21.165817260742188, Learning Rate: 0.01\n",
      "Epoch [19096/20000], Loss: -21.306915283203125, Learning Rate: 0.01\n",
      "Epoch [19097/20000], Loss: -21.156707763671875, Learning Rate: 0.01\n",
      "Epoch [19098/20000], Loss: -20.928314208984375, Learning Rate: 0.01\n",
      "Epoch [19099/20000], Loss: -20.851730346679688, Learning Rate: 0.01\n",
      "Epoch [19100/20000], Loss: -20.994705200195312, Learning Rate: 0.01\n",
      "Epoch [19101/20000], Loss: -21.201919555664062, Learning Rate: 0.01\n",
      "Epoch [19102/20000], Loss: -21.28668212890625, Learning Rate: 0.01\n",
      "Epoch [19103/20000], Loss: -21.211090087890625, Learning Rate: 0.01\n",
      "Epoch [19104/20000], Loss: -21.105636596679688, Learning Rate: 0.01\n",
      "Epoch [19105/20000], Loss: -21.106643676757812, Learning Rate: 0.01\n",
      "Epoch [19106/20000], Loss: -21.216079711914062, Learning Rate: 0.01\n",
      "Epoch [19107/20000], Loss: -21.317398071289062, Learning Rate: 0.01\n",
      "Epoch [19108/20000], Loss: -21.31475830078125, Learning Rate: 0.01\n",
      "Epoch [19109/20000], Loss: -21.236251831054688, Learning Rate: 0.01\n",
      "Epoch [19110/20000], Loss: -21.1842041015625, Learning Rate: 0.01\n",
      "Epoch [19111/20000], Loss: -21.218978881835938, Learning Rate: 0.01\n",
      "Epoch [19112/20000], Loss: -21.300216674804688, Learning Rate: 0.01\n",
      "Epoch [19113/20000], Loss: -21.345840454101562, Learning Rate: 0.01\n",
      "Epoch [19114/20000], Loss: -21.323837280273438, Learning Rate: 0.01\n",
      "Epoch [19115/20000], Loss: -21.276565551757812, Learning Rate: 0.01\n",
      "Epoch [19116/20000], Loss: -21.262176513671875, Learning Rate: 0.01\n",
      "Epoch [19117/20000], Loss: -21.2918701171875, Learning Rate: 0.01\n",
      "Epoch [19118/20000], Loss: -21.328628540039062, Learning Rate: 0.01\n",
      "Epoch [19119/20000], Loss: -21.336166381835938, Learning Rate: 0.01\n",
      "Epoch [19120/20000], Loss: -21.31732177734375, Learning Rate: 0.01\n",
      "Epoch [19121/20000], Loss: -21.302597045898438, Learning Rate: 0.01\n",
      "Epoch [19122/20000], Loss: -21.311294555664062, Learning Rate: 0.01\n",
      "Epoch [19123/20000], Loss: -21.33282470703125, Learning Rate: 0.01\n",
      "Epoch [19124/20000], Loss: -21.343841552734375, Learning Rate: 0.01\n",
      "Epoch [19125/20000], Loss: -21.335617065429688, Learning Rate: 0.01\n",
      "Epoch [19126/20000], Loss: -21.321197509765625, Learning Rate: 0.01\n",
      "Epoch [19127/20000], Loss: -21.318313598632812, Learning Rate: 0.01\n",
      "Epoch [19128/20000], Loss: -21.329559326171875, Learning Rate: 0.01\n",
      "Epoch [19129/20000], Loss: -21.342971801757812, Learning Rate: 0.01\n",
      "Epoch [19130/20000], Loss: -21.3465576171875, Learning Rate: 0.01\n",
      "Epoch [19131/20000], Loss: -21.339950561523438, Learning Rate: 0.01\n",
      "Epoch [19132/20000], Loss: -21.333099365234375, Learning Rate: 0.01\n",
      "Epoch [19133/20000], Loss: -21.333343505859375, Learning Rate: 0.01\n",
      "Epoch [19134/20000], Loss: -21.339508056640625, Learning Rate: 0.01\n",
      "Epoch [19135/20000], Loss: -21.344635009765625, Learning Rate: 0.01\n",
      "Epoch [19136/20000], Loss: -21.344314575195312, Learning Rate: 0.01\n",
      "Epoch [19137/20000], Loss: -21.340667724609375, Learning Rate: 0.01\n",
      "Epoch [19138/20000], Loss: -21.338653564453125, Learning Rate: 0.01\n",
      "Epoch [19139/20000], Loss: -21.3409423828125, Learning Rate: 0.01\n",
      "Epoch [19140/20000], Loss: -21.345001220703125, Learning Rate: 0.01\n",
      "Epoch [19141/20000], Loss: -21.347213745117188, Learning Rate: 0.01\n",
      "Epoch [19142/20000], Loss: -21.345993041992188, Learning Rate: 0.01\n",
      "Epoch [19143/20000], Loss: -21.343368530273438, Learning Rate: 0.01\n",
      "Epoch [19144/20000], Loss: -21.342193603515625, Learning Rate: 0.01\n",
      "Epoch [19145/20000], Loss: -21.34344482421875, Learning Rate: 0.01\n",
      "Epoch [19146/20000], Loss: -21.3455810546875, Learning Rate: 0.01\n",
      "Epoch [19147/20000], Loss: -21.346832275390625, Learning Rate: 0.01\n",
      "Epoch [19148/20000], Loss: -21.346435546875, Learning Rate: 0.01\n",
      "Epoch [19149/20000], Loss: -21.3453369140625, Learning Rate: 0.01\n",
      "Epoch [19150/20000], Loss: -21.344985961914062, Learning Rate: 0.01\n",
      "Epoch [19151/20000], Loss: -21.345718383789062, Learning Rate: 0.01\n",
      "Epoch [19152/20000], Loss: -21.346847534179688, Learning Rate: 0.01\n",
      "Epoch [19153/20000], Loss: -21.347305297851562, Learning Rate: 0.01\n",
      "Epoch [19154/20000], Loss: -21.346939086914062, Learning Rate: 0.01\n",
      "Epoch [19155/20000], Loss: -21.346206665039062, Learning Rate: 0.01\n",
      "Epoch [19156/20000], Loss: -21.345947265625, Learning Rate: 0.01\n",
      "Epoch [19157/20000], Loss: -21.346237182617188, Learning Rate: 0.01\n",
      "Epoch [19158/20000], Loss: -21.346908569335938, Learning Rate: 0.01\n",
      "Epoch [19159/20000], Loss: -21.34735107421875, Learning Rate: 0.01\n",
      "Epoch [19160/20000], Loss: -21.347335815429688, Learning Rate: 0.01\n",
      "Epoch [19161/20000], Loss: -21.347076416015625, Learning Rate: 0.01\n",
      "Epoch [19162/20000], Loss: -21.346832275390625, Learning Rate: 0.01\n",
      "Epoch [19163/20000], Loss: -21.34698486328125, Learning Rate: 0.01\n",
      "Epoch [19164/20000], Loss: -21.347244262695312, Learning Rate: 0.01\n",
      "Epoch [19165/20000], Loss: -21.347579956054688, Learning Rate: 0.01\n",
      "Epoch [19166/20000], Loss: -21.347579956054688, Learning Rate: 0.01\n",
      "Epoch [19167/20000], Loss: -21.34747314453125, Learning Rate: 0.01\n",
      "Epoch [19168/20000], Loss: -21.347305297851562, Learning Rate: 0.01\n",
      "Epoch [19169/20000], Loss: -21.347274780273438, Learning Rate: 0.01\n",
      "Epoch [19170/20000], Loss: -21.347335815429688, Learning Rate: 0.01\n",
      "Epoch [19171/20000], Loss: -21.347549438476562, Learning Rate: 0.01\n",
      "Epoch [19172/20000], Loss: -21.347564697265625, Learning Rate: 0.01\n",
      "Epoch [19173/20000], Loss: -21.347640991210938, Learning Rate: 0.01\n",
      "Epoch [19174/20000], Loss: -21.34759521484375, Learning Rate: 0.01\n",
      "Epoch [19175/20000], Loss: -21.3475341796875, Learning Rate: 0.01\n",
      "Epoch [19176/20000], Loss: -21.347549438476562, Learning Rate: 0.01\n",
      "Epoch [19177/20000], Loss: -21.347625732421875, Learning Rate: 0.01\n",
      "Epoch [19178/20000], Loss: -21.3477783203125, Learning Rate: 0.01\n",
      "Epoch [19179/20000], Loss: -21.347747802734375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19180/20000], Loss: -21.347747802734375, Learning Rate: 0.01\n",
      "Epoch [19181/20000], Loss: -21.347747802734375, Learning Rate: 0.01\n",
      "Epoch [19182/20000], Loss: -21.347686767578125, Learning Rate: 0.01\n",
      "Epoch [19183/20000], Loss: -21.347747802734375, Learning Rate: 0.01\n",
      "Epoch [19184/20000], Loss: -21.347686767578125, Learning Rate: 0.01\n",
      "Epoch [19185/20000], Loss: -21.3477783203125, Learning Rate: 0.01\n",
      "Epoch [19186/20000], Loss: -21.347854614257812, Learning Rate: 0.01\n",
      "Epoch [19187/20000], Loss: -21.347747802734375, Learning Rate: 0.01\n",
      "Epoch [19188/20000], Loss: -21.347808837890625, Learning Rate: 0.01\n",
      "Epoch [19189/20000], Loss: -21.347915649414062, Learning Rate: 0.01\n",
      "Epoch [19190/20000], Loss: -21.347869873046875, Learning Rate: 0.01\n",
      "Epoch [19191/20000], Loss: -21.34783935546875, Learning Rate: 0.01\n",
      "Epoch [19192/20000], Loss: -21.347900390625, Learning Rate: 0.01\n",
      "Epoch [19193/20000], Loss: -21.347915649414062, Learning Rate: 0.01\n",
      "Epoch [19194/20000], Loss: -21.347900390625, Learning Rate: 0.01\n",
      "Epoch [19195/20000], Loss: -21.347930908203125, Learning Rate: 0.01\n",
      "Epoch [19196/20000], Loss: -21.347930908203125, Learning Rate: 0.01\n",
      "Epoch [19197/20000], Loss: -21.3480224609375, Learning Rate: 0.01\n",
      "Epoch [19198/20000], Loss: -21.347930908203125, Learning Rate: 0.01\n",
      "Epoch [19199/20000], Loss: -21.3480224609375, Learning Rate: 0.01\n",
      "Epoch [19200/20000], Loss: -21.347930908203125, Learning Rate: 0.01\n",
      "Epoch [19201/20000], Loss: -21.347976684570312, Learning Rate: 0.01\n",
      "Epoch [19202/20000], Loss: -21.34796142578125, Learning Rate: 0.01\n",
      "Epoch [19203/20000], Loss: -21.34796142578125, Learning Rate: 0.01\n",
      "Epoch [19204/20000], Loss: -21.347869873046875, Learning Rate: 0.01\n",
      "Epoch [19205/20000], Loss: -21.347869873046875, Learning Rate: 0.01\n",
      "Epoch [19206/20000], Loss: -21.347732543945312, Learning Rate: 0.01\n",
      "Epoch [19207/20000], Loss: -21.347763061523438, Learning Rate: 0.01\n",
      "Epoch [19208/20000], Loss: -21.347564697265625, Learning Rate: 0.01\n",
      "Epoch [19209/20000], Loss: -21.347381591796875, Learning Rate: 0.01\n",
      "Epoch [19210/20000], Loss: -21.347091674804688, Learning Rate: 0.01\n",
      "Epoch [19211/20000], Loss: -21.3466796875, Learning Rate: 0.01\n",
      "Epoch [19212/20000], Loss: -21.346038818359375, Learning Rate: 0.01\n",
      "Epoch [19213/20000], Loss: -21.345046997070312, Learning Rate: 0.01\n",
      "Epoch [19214/20000], Loss: -21.343658447265625, Learning Rate: 0.01\n",
      "Epoch [19215/20000], Loss: -21.341400146484375, Learning Rate: 0.01\n",
      "Epoch [19216/20000], Loss: -21.338180541992188, Learning Rate: 0.01\n",
      "Epoch [19217/20000], Loss: -21.333145141601562, Learning Rate: 0.01\n",
      "Epoch [19218/20000], Loss: -21.32550048828125, Learning Rate: 0.01\n",
      "Epoch [19219/20000], Loss: -21.3135986328125, Learning Rate: 0.01\n",
      "Epoch [19220/20000], Loss: -21.29522705078125, Learning Rate: 0.01\n",
      "Epoch [19221/20000], Loss: -21.266616821289062, Learning Rate: 0.01\n",
      "Epoch [19222/20000], Loss: -21.222274780273438, Learning Rate: 0.01\n",
      "Epoch [19223/20000], Loss: -21.152984619140625, Learning Rate: 0.01\n",
      "Epoch [19224/20000], Loss: -21.048477172851562, Learning Rate: 0.01\n",
      "Epoch [19225/20000], Loss: -20.888824462890625, Learning Rate: 0.01\n",
      "Epoch [19226/20000], Loss: -20.664535522460938, Learning Rate: 0.01\n",
      "Epoch [19227/20000], Loss: -20.352676391601562, Learning Rate: 0.01\n",
      "Epoch [19228/20000], Loss: -19.99713134765625, Learning Rate: 0.01\n",
      "Epoch [19229/20000], Loss: -19.636993408203125, Learning Rate: 0.01\n",
      "Epoch [19230/20000], Loss: -19.4825439453125, Learning Rate: 0.01\n",
      "Epoch [19231/20000], Loss: -19.627212524414062, Learning Rate: 0.01\n",
      "Epoch [19232/20000], Loss: -20.169479370117188, Learning Rate: 0.01\n",
      "Epoch [19233/20000], Loss: -20.82684326171875, Learning Rate: 0.01\n",
      "Epoch [19234/20000], Loss: -21.271224975585938, Learning Rate: 0.01\n",
      "Epoch [19235/20000], Loss: -21.297821044921875, Learning Rate: 0.01\n",
      "Epoch [19236/20000], Loss: -21.0101318359375, Learning Rate: 0.01\n",
      "Epoch [19237/20000], Loss: -20.703155517578125, Learning Rate: 0.01\n",
      "Epoch [19238/20000], Loss: -20.623382568359375, Learning Rate: 0.01\n",
      "Epoch [19239/20000], Loss: -20.837890625, Learning Rate: 0.01\n",
      "Epoch [19240/20000], Loss: -21.149017333984375, Learning Rate: 0.01\n",
      "Epoch [19241/20000], Loss: -21.321685791015625, Learning Rate: 0.01\n",
      "Epoch [19242/20000], Loss: -21.265274047851562, Learning Rate: 0.01\n",
      "Epoch [19243/20000], Loss: -21.094314575195312, Learning Rate: 0.01\n",
      "Epoch [19244/20000], Loss: -20.996826171875, Learning Rate: 0.01\n",
      "Epoch [19245/20000], Loss: -21.056396484375, Learning Rate: 0.01\n",
      "Epoch [19246/20000], Loss: -21.2115478515625, Learning Rate: 0.01\n",
      "Epoch [19247/20000], Loss: -21.319549560546875, Learning Rate: 0.01\n",
      "Epoch [19248/20000], Loss: -21.307708740234375, Learning Rate: 0.01\n",
      "Epoch [19249/20000], Loss: -21.2227783203125, Learning Rate: 0.01\n",
      "Epoch [19250/20000], Loss: -21.165298461914062, Learning Rate: 0.01\n",
      "Epoch [19251/20000], Loss: -21.193954467773438, Learning Rate: 0.01\n",
      "Epoch [19252/20000], Loss: -21.274490356445312, Learning Rate: 0.01\n",
      "Epoch [19253/20000], Loss: -21.3330078125, Learning Rate: 0.01\n",
      "Epoch [19254/20000], Loss: -21.3267822265625, Learning Rate: 0.01\n",
      "Epoch [19255/20000], Loss: -21.279617309570312, Learning Rate: 0.01\n",
      "Epoch [19256/20000], Loss: -21.247848510742188, Learning Rate: 0.01\n",
      "Epoch [19257/20000], Loss: -21.262481689453125, Learning Rate: 0.01\n",
      "Epoch [19258/20000], Loss: -21.30792236328125, Learning Rate: 0.01\n",
      "Epoch [19259/20000], Loss: -21.341949462890625, Learning Rate: 0.01\n",
      "Epoch [19260/20000], Loss: -21.340240478515625, Learning Rate: 0.01\n",
      "Epoch [19261/20000], Loss: -21.313735961914062, Learning Rate: 0.01\n",
      "Epoch [19262/20000], Loss: -21.292999267578125, Learning Rate: 0.01\n",
      "Epoch [19263/20000], Loss: -21.298080444335938, Learning Rate: 0.01\n",
      "Epoch [19264/20000], Loss: -21.322158813476562, Learning Rate: 0.01\n",
      "Epoch [19265/20000], Loss: -21.343704223632812, Learning Rate: 0.01\n",
      "Epoch [19266/20000], Loss: -21.346664428710938, Learning Rate: 0.01\n",
      "Epoch [19267/20000], Loss: -21.333847045898438, Learning Rate: 0.01\n",
      "Epoch [19268/20000], Loss: -21.320587158203125, Learning Rate: 0.01\n",
      "Epoch [19269/20000], Loss: -21.31939697265625, Learning Rate: 0.01\n",
      "Epoch [19270/20000], Loss: -21.330078125, Learning Rate: 0.01\n",
      "Epoch [19271/20000], Loss: -21.342697143554688, Learning Rate: 0.01\n",
      "Epoch [19272/20000], Loss: -21.34759521484375, Learning Rate: 0.01\n",
      "Epoch [19273/20000], Loss: -21.343170166015625, Learning Rate: 0.01\n",
      "Epoch [19274/20000], Loss: -21.33587646484375, Learning Rate: 0.01\n",
      "Epoch [19275/20000], Loss: -21.332962036132812, Learning Rate: 0.01\n",
      "Epoch [19276/20000], Loss: -21.33648681640625, Learning Rate: 0.01\n",
      "Epoch [19277/20000], Loss: -21.3428955078125, Learning Rate: 0.01\n",
      "Epoch [19278/20000], Loss: -21.347000122070312, Learning Rate: 0.01\n",
      "Epoch [19279/20000], Loss: -21.346527099609375, Learning Rate: 0.01\n",
      "Epoch [19280/20000], Loss: -21.343109130859375, Learning Rate: 0.01\n",
      "Epoch [19281/20000], Loss: -21.340560913085938, Learning Rate: 0.01\n",
      "Epoch [19282/20000], Loss: -21.341033935546875, Learning Rate: 0.01\n",
      "Epoch [19283/20000], Loss: -21.34393310546875, Learning Rate: 0.01\n",
      "Epoch [19284/20000], Loss: -21.346908569335938, Learning Rate: 0.01\n",
      "Epoch [19285/20000], Loss: -21.34783935546875, Learning Rate: 0.01\n",
      "Epoch [19286/20000], Loss: -21.346710205078125, Learning Rate: 0.01\n",
      "Epoch [19287/20000], Loss: -21.344879150390625, Learning Rate: 0.01\n",
      "Epoch [19288/20000], Loss: -21.344070434570312, Learning Rate: 0.01\n",
      "Epoch [19289/20000], Loss: -21.344802856445312, Learning Rate: 0.01\n",
      "Epoch [19290/20000], Loss: -21.346481323242188, Learning Rate: 0.01\n",
      "Epoch [19291/20000], Loss: -21.347915649414062, Learning Rate: 0.01\n",
      "Epoch [19292/20000], Loss: -21.348175048828125, Learning Rate: 0.01\n",
      "Epoch [19293/20000], Loss: -21.347457885742188, Learning Rate: 0.01\n",
      "Epoch [19294/20000], Loss: -21.3465576171875, Learning Rate: 0.01\n",
      "Epoch [19295/20000], Loss: -21.346221923828125, Learning Rate: 0.01\n",
      "Epoch [19296/20000], Loss: -21.346588134765625, Learning Rate: 0.01\n",
      "Epoch [19297/20000], Loss: -21.3472900390625, Learning Rate: 0.01\n",
      "Epoch [19298/20000], Loss: -21.348037719726562, Learning Rate: 0.01\n",
      "Epoch [19299/20000], Loss: -21.348220825195312, Learning Rate: 0.01\n",
      "Epoch [19300/20000], Loss: -21.347991943359375, Learning Rate: 0.01\n",
      "Epoch [19301/20000], Loss: -21.347579956054688, Learning Rate: 0.01\n",
      "Epoch [19302/20000], Loss: -21.347366333007812, Learning Rate: 0.01\n",
      "Epoch [19303/20000], Loss: -21.3475341796875, Learning Rate: 0.01\n",
      "Epoch [19304/20000], Loss: -21.347854614257812, Learning Rate: 0.01\n",
      "Epoch [19305/20000], Loss: -21.34820556640625, Learning Rate: 0.01\n",
      "Epoch [19306/20000], Loss: -21.34832763671875, Learning Rate: 0.01\n",
      "Epoch [19307/20000], Loss: -21.348251342773438, Learning Rate: 0.01\n",
      "Epoch [19308/20000], Loss: -21.348114013671875, Learning Rate: 0.01\n",
      "Epoch [19309/20000], Loss: -21.347976684570312, Learning Rate: 0.01\n",
      "Epoch [19310/20000], Loss: -21.347946166992188, Learning Rate: 0.01\n",
      "Epoch [19311/20000], Loss: -21.34814453125, Learning Rate: 0.01\n",
      "Epoch [19312/20000], Loss: -21.348190307617188, Learning Rate: 0.01\n",
      "Epoch [19313/20000], Loss: -21.348419189453125, Learning Rate: 0.01\n",
      "Epoch [19314/20000], Loss: -21.3485107421875, Learning Rate: 0.01\n",
      "Epoch [19315/20000], Loss: -21.348464965820312, Learning Rate: 0.01\n",
      "Epoch [19316/20000], Loss: -21.348403930664062, Learning Rate: 0.01\n",
      "Epoch [19317/20000], Loss: -21.34832763671875, Learning Rate: 0.01\n",
      "Epoch [19318/20000], Loss: -21.3482666015625, Learning Rate: 0.01\n",
      "Epoch [19319/20000], Loss: -21.348373413085938, Learning Rate: 0.01\n",
      "Epoch [19320/20000], Loss: -21.348480224609375, Learning Rate: 0.01\n",
      "Epoch [19321/20000], Loss: -21.348556518554688, Learning Rate: 0.01\n",
      "Epoch [19322/20000], Loss: -21.348587036132812, Learning Rate: 0.01\n",
      "Epoch [19323/20000], Loss: -21.348602294921875, Learning Rate: 0.01\n",
      "Epoch [19324/20000], Loss: -21.34857177734375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19325/20000], Loss: -21.348480224609375, Learning Rate: 0.01\n",
      "Epoch [19326/20000], Loss: -21.348526000976562, Learning Rate: 0.01\n",
      "Epoch [19327/20000], Loss: -21.348556518554688, Learning Rate: 0.01\n",
      "Epoch [19328/20000], Loss: -21.348587036132812, Learning Rate: 0.01\n",
      "Epoch [19329/20000], Loss: -21.348617553710938, Learning Rate: 0.01\n",
      "Epoch [19330/20000], Loss: -21.348678588867188, Learning Rate: 0.01\n",
      "Epoch [19331/20000], Loss: -21.3486328125, Learning Rate: 0.01\n",
      "Epoch [19332/20000], Loss: -21.348663330078125, Learning Rate: 0.01\n",
      "Epoch [19333/20000], Loss: -21.348602294921875, Learning Rate: 0.01\n",
      "Epoch [19334/20000], Loss: -21.34869384765625, Learning Rate: 0.01\n",
      "Epoch [19335/20000], Loss: -21.348648071289062, Learning Rate: 0.01\n",
      "Epoch [19336/20000], Loss: -21.34869384765625, Learning Rate: 0.01\n",
      "Epoch [19337/20000], Loss: -21.348678588867188, Learning Rate: 0.01\n",
      "Epoch [19338/20000], Loss: -21.348663330078125, Learning Rate: 0.01\n",
      "Epoch [19339/20000], Loss: -21.348724365234375, Learning Rate: 0.01\n",
      "Epoch [19340/20000], Loss: -21.348709106445312, Learning Rate: 0.01\n",
      "Epoch [19341/20000], Loss: -21.348770141601562, Learning Rate: 0.01\n",
      "Epoch [19342/20000], Loss: -21.348800659179688, Learning Rate: 0.01\n",
      "Epoch [19343/20000], Loss: -21.3487548828125, Learning Rate: 0.01\n",
      "Epoch [19344/20000], Loss: -21.348800659179688, Learning Rate: 0.01\n",
      "Epoch [19345/20000], Loss: -21.348800659179688, Learning Rate: 0.01\n",
      "Epoch [19346/20000], Loss: -21.34881591796875, Learning Rate: 0.01\n",
      "Epoch [19347/20000], Loss: -21.34881591796875, Learning Rate: 0.01\n",
      "Epoch [19348/20000], Loss: -21.348846435546875, Learning Rate: 0.01\n",
      "Epoch [19349/20000], Loss: -21.348922729492188, Learning Rate: 0.01\n",
      "Epoch [19350/20000], Loss: -21.348892211914062, Learning Rate: 0.01\n",
      "Epoch [19351/20000], Loss: -21.348846435546875, Learning Rate: 0.01\n",
      "Epoch [19352/20000], Loss: -21.34893798828125, Learning Rate: 0.01\n",
      "Epoch [19353/20000], Loss: -21.348953247070312, Learning Rate: 0.01\n",
      "Epoch [19354/20000], Loss: -21.348907470703125, Learning Rate: 0.01\n",
      "Epoch [19355/20000], Loss: -21.348968505859375, Learning Rate: 0.01\n",
      "Epoch [19356/20000], Loss: -21.348876953125, Learning Rate: 0.01\n",
      "Epoch [19357/20000], Loss: -21.348907470703125, Learning Rate: 0.01\n",
      "Epoch [19358/20000], Loss: -21.348846435546875, Learning Rate: 0.01\n",
      "Epoch [19359/20000], Loss: -21.348876953125, Learning Rate: 0.01\n",
      "Epoch [19360/20000], Loss: -21.348770141601562, Learning Rate: 0.01\n",
      "Epoch [19361/20000], Loss: -21.348739624023438, Learning Rate: 0.01\n",
      "Epoch [19362/20000], Loss: -21.348678588867188, Learning Rate: 0.01\n",
      "Epoch [19363/20000], Loss: -21.34857177734375, Learning Rate: 0.01\n",
      "Epoch [19364/20000], Loss: -21.348373413085938, Learning Rate: 0.01\n",
      "Epoch [19365/20000], Loss: -21.348068237304688, Learning Rate: 0.01\n",
      "Epoch [19366/20000], Loss: -21.347640991210938, Learning Rate: 0.01\n",
      "Epoch [19367/20000], Loss: -21.34698486328125, Learning Rate: 0.01\n",
      "Epoch [19368/20000], Loss: -21.346023559570312, Learning Rate: 0.01\n",
      "Epoch [19369/20000], Loss: -21.34454345703125, Learning Rate: 0.01\n",
      "Epoch [19370/20000], Loss: -21.342361450195312, Learning Rate: 0.01\n",
      "Epoch [19371/20000], Loss: -21.338714599609375, Learning Rate: 0.01\n",
      "Epoch [19372/20000], Loss: -21.333297729492188, Learning Rate: 0.01\n",
      "Epoch [19373/20000], Loss: -21.3248291015625, Learning Rate: 0.01\n",
      "Epoch [19374/20000], Loss: -21.311508178710938, Learning Rate: 0.01\n",
      "Epoch [19375/20000], Loss: -21.290542602539062, Learning Rate: 0.01\n",
      "Epoch [19376/20000], Loss: -21.2579345703125, Learning Rate: 0.01\n",
      "Epoch [19377/20000], Loss: -21.2071533203125, Learning Rate: 0.01\n",
      "Epoch [19378/20000], Loss: -21.129226684570312, Learning Rate: 0.01\n",
      "Epoch [19379/20000], Loss: -21.013839721679688, Learning Rate: 0.01\n",
      "Epoch [19380/20000], Loss: -20.850494384765625, Learning Rate: 0.01\n",
      "Epoch [19381/20000], Loss: -20.640701293945312, Learning Rate: 0.01\n",
      "Epoch [19382/20000], Loss: -20.4091796875, Learning Rate: 0.01\n",
      "Epoch [19383/20000], Loss: -20.23077392578125, Learning Rate: 0.01\n",
      "Epoch [19384/20000], Loss: -20.199859619140625, Learning Rate: 0.01\n",
      "Epoch [19385/20000], Loss: -20.39495849609375, Learning Rate: 0.01\n",
      "Epoch [19386/20000], Loss: -20.687698364257812, Learning Rate: 0.01\n",
      "Epoch [19387/20000], Loss: -20.931121826171875, Learning Rate: 0.01\n",
      "Epoch [19388/20000], Loss: -21.066421508789062, Learning Rate: 0.01\n",
      "Epoch [19389/20000], Loss: -21.074493408203125, Learning Rate: 0.01\n",
      "Epoch [19390/20000], Loss: -20.975723266601562, Learning Rate: 0.01\n",
      "Epoch [19391/20000], Loss: -20.876052856445312, Learning Rate: 0.01\n",
      "Epoch [19392/20000], Loss: -20.857833862304688, Learning Rate: 0.01\n",
      "Epoch [19393/20000], Loss: -20.989654541015625, Learning Rate: 0.01\n",
      "Epoch [19394/20000], Loss: -21.184158325195312, Learning Rate: 0.01\n",
      "Epoch [19395/20000], Loss: -21.323013305664062, Learning Rate: 0.01\n",
      "Epoch [19396/20000], Loss: -21.334060668945312, Learning Rate: 0.01\n",
      "Epoch [19397/20000], Loss: -21.239303588867188, Learning Rate: 0.01\n",
      "Epoch [19398/20000], Loss: -21.120941162109375, Learning Rate: 0.01\n",
      "Epoch [19399/20000], Loss: -21.046615600585938, Learning Rate: 0.01\n",
      "Epoch [19400/20000], Loss: -21.049270629882812, Learning Rate: 0.01\n",
      "Epoch [19401/20000], Loss: -21.08349609375, Learning Rate: 0.01\n",
      "Epoch [19402/20000], Loss: -21.110595703125, Learning Rate: 0.01\n",
      "Epoch [19403/20000], Loss: -21.09613037109375, Learning Rate: 0.01\n",
      "Epoch [19404/20000], Loss: -21.073959350585938, Learning Rate: 0.01\n",
      "Epoch [19405/20000], Loss: -21.097442626953125, Learning Rate: 0.01\n",
      "Epoch [19406/20000], Loss: -21.183929443359375, Learning Rate: 0.01\n",
      "Epoch [19407/20000], Loss: -21.290847778320312, Learning Rate: 0.01\n",
      "Epoch [19408/20000], Loss: -21.346893310546875, Learning Rate: 0.01\n",
      "Epoch [19409/20000], Loss: -21.328338623046875, Learning Rate: 0.01\n",
      "Epoch [19410/20000], Loss: -21.269363403320312, Learning Rate: 0.01\n",
      "Epoch [19411/20000], Loss: -21.222549438476562, Learning Rate: 0.01\n",
      "Epoch [19412/20000], Loss: -21.213409423828125, Learning Rate: 0.01\n",
      "Epoch [19413/20000], Loss: -21.233856201171875, Learning Rate: 0.01\n",
      "Epoch [19414/20000], Loss: -21.2554931640625, Learning Rate: 0.01\n",
      "Epoch [19415/20000], Loss: -21.267364501953125, Learning Rate: 0.01\n",
      "Epoch [19416/20000], Loss: -21.273483276367188, Learning Rate: 0.01\n",
      "Epoch [19417/20000], Loss: -21.288131713867188, Learning Rate: 0.01\n",
      "Epoch [19418/20000], Loss: -21.313186645507812, Learning Rate: 0.01\n",
      "Epoch [19419/20000], Loss: -21.33795166015625, Learning Rate: 0.01\n",
      "Epoch [19420/20000], Loss: -21.3480224609375, Learning Rate: 0.01\n",
      "Epoch [19421/20000], Loss: -21.33953857421875, Learning Rate: 0.01\n",
      "Epoch [19422/20000], Loss: -21.321945190429688, Learning Rate: 0.01\n",
      "Epoch [19423/20000], Loss: -21.308990478515625, Learning Rate: 0.01\n",
      "Epoch [19424/20000], Loss: -21.30792236328125, Learning Rate: 0.01\n",
      "Epoch [19425/20000], Loss: -21.315185546875, Learning Rate: 0.01\n",
      "Epoch [19426/20000], Loss: -21.323516845703125, Learning Rate: 0.01\n",
      "Epoch [19427/20000], Loss: -21.328277587890625, Learning Rate: 0.01\n",
      "Epoch [19428/20000], Loss: -21.330886840820312, Learning Rate: 0.01\n",
      "Epoch [19429/20000], Loss: -21.334518432617188, Learning Rate: 0.01\n",
      "Epoch [19430/20000], Loss: -21.340377807617188, Learning Rate: 0.01\n",
      "Epoch [19431/20000], Loss: -21.345932006835938, Learning Rate: 0.01\n",
      "Epoch [19432/20000], Loss: -21.347671508789062, Learning Rate: 0.01\n",
      "Epoch [19433/20000], Loss: -21.3447265625, Learning Rate: 0.01\n",
      "Epoch [19434/20000], Loss: -21.339447021484375, Learning Rate: 0.01\n",
      "Epoch [19435/20000], Loss: -21.335952758789062, Learning Rate: 0.01\n",
      "Epoch [19436/20000], Loss: -21.335922241210938, Learning Rate: 0.01\n",
      "Epoch [19437/20000], Loss: -21.338714599609375, Learning Rate: 0.01\n",
      "Epoch [19438/20000], Loss: -21.342178344726562, Learning Rate: 0.01\n",
      "Epoch [19439/20000], Loss: -21.344268798828125, Learning Rate: 0.01\n",
      "Epoch [19440/20000], Loss: -21.345046997070312, Learning Rate: 0.01\n",
      "Epoch [19441/20000], Loss: -21.345550537109375, Learning Rate: 0.01\n",
      "Epoch [19442/20000], Loss: -21.346405029296875, Learning Rate: 0.01\n",
      "Epoch [19443/20000], Loss: -21.34759521484375, Learning Rate: 0.01\n",
      "Epoch [19444/20000], Loss: -21.348388671875, Learning Rate: 0.01\n",
      "Epoch [19445/20000], Loss: -21.347991943359375, Learning Rate: 0.01\n",
      "Epoch [19446/20000], Loss: -21.346923828125, Learning Rate: 0.01\n",
      "Epoch [19447/20000], Loss: -21.345565795898438, Learning Rate: 0.01\n",
      "Epoch [19448/20000], Loss: -21.344924926757812, Learning Rate: 0.01\n",
      "Epoch [19449/20000], Loss: -21.345306396484375, Learning Rate: 0.01\n",
      "Epoch [19450/20000], Loss: -21.346145629882812, Learning Rate: 0.01\n",
      "Epoch [19451/20000], Loss: -21.3470458984375, Learning Rate: 0.01\n",
      "Epoch [19452/20000], Loss: -21.347564697265625, Learning Rate: 0.01\n",
      "Epoch [19453/20000], Loss: -21.347854614257812, Learning Rate: 0.01\n",
      "Epoch [19454/20000], Loss: -21.348052978515625, Learning Rate: 0.01\n",
      "Epoch [19455/20000], Loss: -21.348312377929688, Learning Rate: 0.01\n",
      "Epoch [19456/20000], Loss: -21.348648071289062, Learning Rate: 0.01\n",
      "Epoch [19457/20000], Loss: -21.349075317382812, Learning Rate: 0.01\n",
      "Epoch [19458/20000], Loss: -21.349227905273438, Learning Rate: 0.01\n",
      "Epoch [19459/20000], Loss: -21.349151611328125, Learning Rate: 0.01\n",
      "Epoch [19460/20000], Loss: -21.348800659179688, Learning Rate: 0.01\n",
      "Epoch [19461/20000], Loss: -21.3485107421875, Learning Rate: 0.01\n",
      "Epoch [19462/20000], Loss: -21.348403930664062, Learning Rate: 0.01\n",
      "Epoch [19463/20000], Loss: -21.348480224609375, Learning Rate: 0.01\n",
      "Epoch [19464/20000], Loss: -21.3485107421875, Learning Rate: 0.01\n",
      "Epoch [19465/20000], Loss: -21.348556518554688, Learning Rate: 0.01\n",
      "Epoch [19466/20000], Loss: -21.348526000976562, Learning Rate: 0.01\n",
      "Epoch [19467/20000], Loss: -21.348480224609375, Learning Rate: 0.01\n",
      "Epoch [19468/20000], Loss: -21.3485107421875, Learning Rate: 0.01\n",
      "Epoch [19469/20000], Loss: -21.348541259765625, Learning Rate: 0.01\n",
      "Epoch [19470/20000], Loss: -21.348587036132812, Learning Rate: 0.01\n",
      "Epoch [19471/20000], Loss: -21.348831176757812, Learning Rate: 0.01\n",
      "Epoch [19472/20000], Loss: -21.348876953125, Learning Rate: 0.01\n",
      "Epoch [19473/20000], Loss: -21.348953247070312, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19474/20000], Loss: -21.349014282226562, Learning Rate: 0.01\n",
      "Epoch [19475/20000], Loss: -21.348968505859375, Learning Rate: 0.01\n",
      "Epoch [19476/20000], Loss: -21.349044799804688, Learning Rate: 0.01\n",
      "Epoch [19477/20000], Loss: -21.349075317382812, Learning Rate: 0.01\n",
      "Epoch [19478/20000], Loss: -21.349090576171875, Learning Rate: 0.01\n",
      "Epoch [19479/20000], Loss: -21.34912109375, Learning Rate: 0.01\n",
      "Epoch [19480/20000], Loss: -21.349136352539062, Learning Rate: 0.01\n",
      "Epoch [19481/20000], Loss: -21.349212646484375, Learning Rate: 0.01\n",
      "Epoch [19482/20000], Loss: -21.349044799804688, Learning Rate: 0.01\n",
      "Epoch [19483/20000], Loss: -21.349044799804688, Learning Rate: 0.01\n",
      "Epoch [19484/20000], Loss: -21.3489990234375, Learning Rate: 0.01\n",
      "Epoch [19485/20000], Loss: -21.348861694335938, Learning Rate: 0.01\n",
      "Epoch [19486/20000], Loss: -21.348770141601562, Learning Rate: 0.01\n",
      "Epoch [19487/20000], Loss: -21.348663330078125, Learning Rate: 0.01\n",
      "Epoch [19488/20000], Loss: -21.348419189453125, Learning Rate: 0.01\n",
      "Epoch [19489/20000], Loss: -21.3480224609375, Learning Rate: 0.01\n",
      "Epoch [19490/20000], Loss: -21.347579956054688, Learning Rate: 0.01\n",
      "Epoch [19491/20000], Loss: -21.346923828125, Learning Rate: 0.01\n",
      "Epoch [19492/20000], Loss: -21.346038818359375, Learning Rate: 0.01\n",
      "Epoch [19493/20000], Loss: -21.344711303710938, Learning Rate: 0.01\n",
      "Epoch [19494/20000], Loss: -21.342864990234375, Learning Rate: 0.01\n",
      "Epoch [19495/20000], Loss: -21.340316772460938, Learning Rate: 0.01\n",
      "Epoch [19496/20000], Loss: -21.336532592773438, Learning Rate: 0.01\n",
      "Epoch [19497/20000], Loss: -21.331161499023438, Learning Rate: 0.01\n",
      "Epoch [19498/20000], Loss: -21.323150634765625, Learning Rate: 0.01\n",
      "Epoch [19499/20000], Loss: -21.311553955078125, Learning Rate: 0.01\n",
      "Epoch [19500/20000], Loss: -21.294265747070312, Learning Rate: 0.01\n",
      "Epoch [19501/20000], Loss: -21.269180297851562, Learning Rate: 0.01\n",
      "Epoch [19502/20000], Loss: -21.231552124023438, Learning Rate: 0.01\n",
      "Epoch [19503/20000], Loss: -21.1773681640625, Learning Rate: 0.01\n",
      "Epoch [19504/20000], Loss: -21.097610473632812, Learning Rate: 0.01\n",
      "Epoch [19505/20000], Loss: -20.987579345703125, Learning Rate: 0.01\n",
      "Epoch [19506/20000], Loss: -20.834304809570312, Learning Rate: 0.01\n",
      "Epoch [19507/20000], Loss: -20.64508056640625, Learning Rate: 0.01\n",
      "Epoch [19508/20000], Loss: -20.418228149414062, Learning Rate: 0.01\n",
      "Epoch [19509/20000], Loss: -20.21173095703125, Learning Rate: 0.01\n",
      "Epoch [19510/20000], Loss: -20.062713623046875, Learning Rate: 0.01\n",
      "Epoch [19511/20000], Loss: -20.088165283203125, Learning Rate: 0.01\n",
      "Epoch [19512/20000], Loss: -20.291259765625, Learning Rate: 0.01\n",
      "Epoch [19513/20000], Loss: -20.666030883789062, Learning Rate: 0.01\n",
      "Epoch [19514/20000], Loss: -21.0491943359375, Learning Rate: 0.01\n",
      "Epoch [19515/20000], Loss: -21.296112060546875, Learning Rate: 0.01\n",
      "Epoch [19516/20000], Loss: -21.32977294921875, Learning Rate: 0.01\n",
      "Epoch [19517/20000], Loss: -21.194656372070312, Learning Rate: 0.01\n",
      "Epoch [19518/20000], Loss: -21.011581420898438, Learning Rate: 0.01\n",
      "Epoch [19519/20000], Loss: -20.898712158203125, Learning Rate: 0.01\n",
      "Epoch [19520/20000], Loss: -20.926620483398438, Learning Rate: 0.01\n",
      "Epoch [19521/20000], Loss: -21.063140869140625, Learning Rate: 0.01\n",
      "Epoch [19522/20000], Loss: -21.226959228515625, Learning Rate: 0.01\n",
      "Epoch [19523/20000], Loss: -21.324569702148438, Learning Rate: 0.01\n",
      "Epoch [19524/20000], Loss: -21.322219848632812, Learning Rate: 0.01\n",
      "Epoch [19525/20000], Loss: -21.251785278320312, Learning Rate: 0.01\n",
      "Epoch [19526/20000], Loss: -21.177871704101562, Learning Rate: 0.01\n",
      "Epoch [19527/20000], Loss: -21.155380249023438, Learning Rate: 0.01\n",
      "Epoch [19528/20000], Loss: -21.192657470703125, Learning Rate: 0.01\n",
      "Epoch [19529/20000], Loss: -21.261917114257812, Learning Rate: 0.01\n",
      "Epoch [19530/20000], Loss: -21.31793212890625, Learning Rate: 0.01\n",
      "Epoch [19531/20000], Loss: -21.333969116210938, Learning Rate: 0.01\n",
      "Epoch [19532/20000], Loss: -21.313583374023438, Learning Rate: 0.01\n",
      "Epoch [19533/20000], Loss: -21.281723022460938, Learning Rate: 0.01\n",
      "Epoch [19534/20000], Loss: -21.264862060546875, Learning Rate: 0.01\n",
      "Epoch [19535/20000], Loss: -21.273040771484375, Learning Rate: 0.01\n",
      "Epoch [19536/20000], Loss: -21.29876708984375, Learning Rate: 0.01\n",
      "Epoch [19537/20000], Loss: -21.324447631835938, Learning Rate: 0.01\n",
      "Epoch [19538/20000], Loss: -21.33648681640625, Learning Rate: 0.01\n",
      "Epoch [19539/20000], Loss: -21.3328857421875, Learning Rate: 0.01\n",
      "Epoch [19540/20000], Loss: -21.321685791015625, Learning Rate: 0.01\n",
      "Epoch [19541/20000], Loss: -21.313491821289062, Learning Rate: 0.01\n",
      "Epoch [19542/20000], Loss: -21.31414794921875, Learning Rate: 0.01\n",
      "Epoch [19543/20000], Loss: -21.322845458984375, Learning Rate: 0.01\n",
      "Epoch [19544/20000], Loss: -21.333419799804688, Learning Rate: 0.01\n",
      "Epoch [19545/20000], Loss: -21.34014892578125, Learning Rate: 0.01\n",
      "Epoch [19546/20000], Loss: -21.348678588867188, Learning Rate: 0.005\n",
      "Epoch [19547/20000], Loss: -21.342041015625, Learning Rate: 0.005\n",
      "Epoch [19548/20000], Loss: -21.348678588867188, Learning Rate: 0.005\n",
      "Epoch [19549/20000], Loss: -21.343643188476562, Learning Rate: 0.005\n",
      "Epoch [19550/20000], Loss: -21.34881591796875, Learning Rate: 0.005\n",
      "Epoch [19551/20000], Loss: -21.344970703125, Learning Rate: 0.005\n",
      "Epoch [19552/20000], Loss: -21.3487548828125, Learning Rate: 0.005\n",
      "Epoch [19553/20000], Loss: -21.346023559570312, Learning Rate: 0.005\n",
      "Epoch [19554/20000], Loss: -21.348800659179688, Learning Rate: 0.005\n",
      "Epoch [19555/20000], Loss: -21.346786499023438, Learning Rate: 0.005\n",
      "Epoch [19556/20000], Loss: -21.348892211914062, Learning Rate: 0.005\n",
      "Epoch [19557/20000], Loss: -21.347518920898438, Learning Rate: 0.005\n",
      "Epoch [19558/20000], Loss: -21.348922729492188, Learning Rate: 0.005\n",
      "Epoch [19559/20000], Loss: -21.3480224609375, Learning Rate: 0.005\n",
      "Epoch [19560/20000], Loss: -21.349029541015625, Learning Rate: 0.005\n",
      "Epoch [19561/20000], Loss: -21.348342895507812, Learning Rate: 0.005\n",
      "Epoch [19562/20000], Loss: -21.349105834960938, Learning Rate: 0.005\n",
      "Epoch [19563/20000], Loss: -21.348739624023438, Learning Rate: 0.005\n",
      "Epoch [19564/20000], Loss: -21.349136352539062, Learning Rate: 0.005\n",
      "Epoch [19565/20000], Loss: -21.348953247070312, Learning Rate: 0.005\n",
      "Epoch [19566/20000], Loss: -21.349227905273438, Learning Rate: 0.005\n",
      "Epoch [19567/20000], Loss: -21.34918212890625, Learning Rate: 0.005\n",
      "Epoch [19568/20000], Loss: -21.349319458007812, Learning Rate: 0.005\n",
      "Epoch [19569/20000], Loss: -21.34930419921875, Learning Rate: 0.005\n",
      "Epoch [19570/20000], Loss: -21.349395751953125, Learning Rate: 0.005\n",
      "Epoch [19571/20000], Loss: -21.349395751953125, Learning Rate: 0.005\n",
      "Epoch [19572/20000], Loss: -21.349411010742188, Learning Rate: 0.005\n",
      "Epoch [19573/20000], Loss: -21.34954833984375, Learning Rate: 0.005\n",
      "Epoch [19574/20000], Loss: -21.349517822265625, Learning Rate: 0.005\n",
      "Epoch [19575/20000], Loss: -21.349594116210938, Learning Rate: 0.005\n",
      "Epoch [19576/20000], Loss: -21.349594116210938, Learning Rate: 0.005\n",
      "Epoch [19577/20000], Loss: -21.349716186523438, Learning Rate: 0.005\n",
      "Epoch [19578/20000], Loss: -21.349639892578125, Learning Rate: 0.005\n",
      "Epoch [19579/20000], Loss: -21.349746704101562, Learning Rate: 0.005\n",
      "Epoch [19580/20000], Loss: -21.349716186523438, Learning Rate: 0.005\n",
      "Epoch [19581/20000], Loss: -21.3497314453125, Learning Rate: 0.005\n",
      "Epoch [19582/20000], Loss: -21.349716186523438, Learning Rate: 0.005\n",
      "Epoch [19583/20000], Loss: -21.349822998046875, Learning Rate: 0.005\n",
      "Epoch [19584/20000], Loss: -21.34979248046875, Learning Rate: 0.005\n",
      "Epoch [19585/20000], Loss: -21.349853515625, Learning Rate: 0.005\n",
      "Epoch [19586/20000], Loss: -21.34979248046875, Learning Rate: 0.005\n",
      "Epoch [19587/20000], Loss: -21.349884033203125, Learning Rate: 0.005\n",
      "Epoch [19588/20000], Loss: -21.349777221679688, Learning Rate: 0.005\n",
      "Epoch [19589/20000], Loss: -21.349853515625, Learning Rate: 0.005\n",
      "Epoch [19590/20000], Loss: -21.349822998046875, Learning Rate: 0.005\n",
      "Epoch [19591/20000], Loss: -21.349899291992188, Learning Rate: 0.005\n",
      "Epoch [19592/20000], Loss: -21.349853515625, Learning Rate: 0.005\n",
      "Epoch [19593/20000], Loss: -21.34991455078125, Learning Rate: 0.005\n",
      "Epoch [19594/20000], Loss: -21.349899291992188, Learning Rate: 0.005\n",
      "Epoch [19595/20000], Loss: -21.349899291992188, Learning Rate: 0.005\n",
      "Epoch [19596/20000], Loss: -21.34991455078125, Learning Rate: 0.005\n",
      "Epoch [19597/20000], Loss: -21.3499755859375, Learning Rate: 0.005\n",
      "Epoch [19598/20000], Loss: -21.349899291992188, Learning Rate: 0.005\n",
      "Epoch [19599/20000], Loss: -21.349990844726562, Learning Rate: 0.005\n",
      "Epoch [19600/20000], Loss: -21.349899291992188, Learning Rate: 0.005\n",
      "Epoch [19601/20000], Loss: -21.349990844726562, Learning Rate: 0.005\n",
      "Epoch [19602/20000], Loss: -21.349945068359375, Learning Rate: 0.005\n",
      "Epoch [19603/20000], Loss: -21.350021362304688, Learning Rate: 0.005\n",
      "Epoch [19604/20000], Loss: -21.3499755859375, Learning Rate: 0.005\n",
      "Epoch [19605/20000], Loss: -21.349990844726562, Learning Rate: 0.005\n",
      "Epoch [19606/20000], Loss: -21.349945068359375, Learning Rate: 0.005\n",
      "Epoch [19607/20000], Loss: -21.350006103515625, Learning Rate: 0.005\n",
      "Epoch [19608/20000], Loss: -21.349960327148438, Learning Rate: 0.005\n",
      "Epoch [19609/20000], Loss: -21.349990844726562, Learning Rate: 0.005\n",
      "Epoch [19610/20000], Loss: -21.35003662109375, Learning Rate: 0.005\n",
      "Epoch [19611/20000], Loss: -21.3499755859375, Learning Rate: 0.005\n",
      "Epoch [19612/20000], Loss: -21.350006103515625, Learning Rate: 0.005\n",
      "Epoch [19613/20000], Loss: -21.350006103515625, Learning Rate: 0.005\n",
      "Epoch [19614/20000], Loss: -21.350021362304688, Learning Rate: 0.005\n",
      "Epoch [19615/20000], Loss: -21.350051879882812, Learning Rate: 0.005\n",
      "Epoch [19616/20000], Loss: -21.350021362304688, Learning Rate: 0.005\n",
      "Epoch [19617/20000], Loss: -21.350067138671875, Learning Rate: 0.005\n",
      "Epoch [19618/20000], Loss: -21.349990844726562, Learning Rate: 0.005\n",
      "Epoch [19619/20000], Loss: -21.350082397460938, Learning Rate: 0.005\n",
      "Epoch [19620/20000], Loss: -21.350051879882812, Learning Rate: 0.005\n",
      "Epoch [19621/20000], Loss: -21.35009765625, Learning Rate: 0.005\n",
      "Epoch [19622/20000], Loss: -21.350082397460938, Learning Rate: 0.005\n",
      "Epoch [19623/20000], Loss: -21.350051879882812, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19624/20000], Loss: -21.350021362304688, Learning Rate: 0.005\n",
      "Epoch [19625/20000], Loss: -21.350082397460938, Learning Rate: 0.005\n",
      "Epoch [19626/20000], Loss: -21.35009765625, Learning Rate: 0.005\n",
      "Epoch [19627/20000], Loss: -21.350128173828125, Learning Rate: 0.005\n",
      "Epoch [19628/20000], Loss: -21.350128173828125, Learning Rate: 0.005\n",
      "Epoch [19629/20000], Loss: -21.350051879882812, Learning Rate: 0.005\n",
      "Epoch [19630/20000], Loss: -21.350143432617188, Learning Rate: 0.005\n",
      "Epoch [19631/20000], Loss: -21.350128173828125, Learning Rate: 0.005\n",
      "Epoch [19632/20000], Loss: -21.350128173828125, Learning Rate: 0.005\n",
      "Epoch [19633/20000], Loss: -21.350082397460938, Learning Rate: 0.005\n",
      "Epoch [19634/20000], Loss: -21.350173950195312, Learning Rate: 0.005\n",
      "Epoch [19635/20000], Loss: -21.350173950195312, Learning Rate: 0.005\n",
      "Epoch [19636/20000], Loss: -21.350143432617188, Learning Rate: 0.005\n",
      "Epoch [19637/20000], Loss: -21.350128173828125, Learning Rate: 0.005\n",
      "Epoch [19638/20000], Loss: -21.350173950195312, Learning Rate: 0.005\n",
      "Epoch [19639/20000], Loss: -21.350173950195312, Learning Rate: 0.005\n",
      "Epoch [19640/20000], Loss: -21.350189208984375, Learning Rate: 0.005\n",
      "Epoch [19641/20000], Loss: -21.350128173828125, Learning Rate: 0.005\n",
      "Epoch [19642/20000], Loss: -21.350112915039062, Learning Rate: 0.005\n",
      "Epoch [19643/20000], Loss: -21.3502197265625, Learning Rate: 0.005\n",
      "Epoch [19644/20000], Loss: -21.350204467773438, Learning Rate: 0.005\n",
      "Epoch [19645/20000], Loss: -21.350189208984375, Learning Rate: 0.005\n",
      "Epoch [19646/20000], Loss: -21.350204467773438, Learning Rate: 0.005\n",
      "Epoch [19647/20000], Loss: -21.350204467773438, Learning Rate: 0.005\n",
      "Epoch [19648/20000], Loss: -21.35015869140625, Learning Rate: 0.005\n",
      "Epoch [19649/20000], Loss: -21.350234985351562, Learning Rate: 0.005\n",
      "Epoch [19650/20000], Loss: -21.35015869140625, Learning Rate: 0.005\n",
      "Epoch [19651/20000], Loss: -21.350234985351562, Learning Rate: 0.005\n",
      "Epoch [19652/20000], Loss: -21.350189208984375, Learning Rate: 0.005\n",
      "Epoch [19653/20000], Loss: -21.3502197265625, Learning Rate: 0.005\n",
      "Epoch [19654/20000], Loss: -21.3502197265625, Learning Rate: 0.005\n",
      "Epoch [19655/20000], Loss: -21.350265502929688, Learning Rate: 0.005\n",
      "Epoch [19656/20000], Loss: -21.350234985351562, Learning Rate: 0.005\n",
      "Epoch [19657/20000], Loss: -21.35028076171875, Learning Rate: 0.005\n",
      "Epoch [19658/20000], Loss: -21.350296020507812, Learning Rate: 0.005\n",
      "Epoch [19659/20000], Loss: -21.350296020507812, Learning Rate: 0.005\n",
      "Epoch [19660/20000], Loss: -21.350265502929688, Learning Rate: 0.005\n",
      "Epoch [19661/20000], Loss: -21.350311279296875, Learning Rate: 0.005\n",
      "Epoch [19662/20000], Loss: -21.35028076171875, Learning Rate: 0.005\n",
      "Epoch [19663/20000], Loss: -21.350311279296875, Learning Rate: 0.005\n",
      "Epoch [19664/20000], Loss: -21.350341796875, Learning Rate: 0.005\n",
      "Epoch [19665/20000], Loss: -21.350357055664062, Learning Rate: 0.005\n",
      "Epoch [19666/20000], Loss: -21.350296020507812, Learning Rate: 0.005\n",
      "Epoch [19667/20000], Loss: -21.350341796875, Learning Rate: 0.005\n",
      "Epoch [19668/20000], Loss: -21.350326538085938, Learning Rate: 0.005\n",
      "Epoch [19669/20000], Loss: -21.350265502929688, Learning Rate: 0.005\n",
      "Epoch [19670/20000], Loss: -21.350357055664062, Learning Rate: 0.005\n",
      "Epoch [19671/20000], Loss: -21.350357055664062, Learning Rate: 0.005\n",
      "Epoch [19672/20000], Loss: -21.350357055664062, Learning Rate: 0.005\n",
      "Epoch [19673/20000], Loss: -21.35040283203125, Learning Rate: 0.005\n",
      "Epoch [19674/20000], Loss: -21.350326538085938, Learning Rate: 0.005\n",
      "Epoch [19675/20000], Loss: -21.350433349609375, Learning Rate: 0.005\n",
      "Epoch [19676/20000], Loss: -21.350387573242188, Learning Rate: 0.005\n",
      "Epoch [19677/20000], Loss: -21.350433349609375, Learning Rate: 0.005\n",
      "Epoch [19678/20000], Loss: -21.350387573242188, Learning Rate: 0.005\n",
      "Epoch [19679/20000], Loss: -21.350418090820312, Learning Rate: 0.005\n",
      "Epoch [19680/20000], Loss: -21.350433349609375, Learning Rate: 0.005\n",
      "Epoch [19681/20000], Loss: -21.350418090820312, Learning Rate: 0.005\n",
      "Epoch [19682/20000], Loss: -21.350418090820312, Learning Rate: 0.005\n",
      "Epoch [19683/20000], Loss: -21.350448608398438, Learning Rate: 0.005\n",
      "Epoch [19684/20000], Loss: -21.35040283203125, Learning Rate: 0.005\n",
      "Epoch [19685/20000], Loss: -21.3504638671875, Learning Rate: 0.005\n",
      "Epoch [19686/20000], Loss: -21.350418090820312, Learning Rate: 0.005\n",
      "Epoch [19687/20000], Loss: -21.350494384765625, Learning Rate: 0.005\n",
      "Epoch [19688/20000], Loss: -21.350433349609375, Learning Rate: 0.005\n",
      "Epoch [19689/20000], Loss: -21.3504638671875, Learning Rate: 0.005\n",
      "Epoch [19690/20000], Loss: -21.3504638671875, Learning Rate: 0.005\n",
      "Epoch [19691/20000], Loss: -21.350479125976562, Learning Rate: 0.005\n",
      "Epoch [19692/20000], Loss: -21.350494384765625, Learning Rate: 0.005\n",
      "Epoch [19693/20000], Loss: -21.350509643554688, Learning Rate: 0.005\n",
      "Epoch [19694/20000], Loss: -21.350555419921875, Learning Rate: 0.005\n",
      "Epoch [19695/20000], Loss: -21.350509643554688, Learning Rate: 0.005\n",
      "Epoch [19696/20000], Loss: -21.350494384765625, Learning Rate: 0.005\n",
      "Epoch [19697/20000], Loss: -21.350570678710938, Learning Rate: 0.005\n",
      "Epoch [19698/20000], Loss: -21.350555419921875, Learning Rate: 0.005\n",
      "Epoch [19699/20000], Loss: -21.350555419921875, Learning Rate: 0.005\n",
      "Epoch [19700/20000], Loss: -21.350555419921875, Learning Rate: 0.005\n",
      "Epoch [19701/20000], Loss: -21.35052490234375, Learning Rate: 0.005\n",
      "Epoch [19702/20000], Loss: -21.350540161132812, Learning Rate: 0.005\n",
      "Epoch [19703/20000], Loss: -21.350555419921875, Learning Rate: 0.005\n",
      "Epoch [19704/20000], Loss: -21.3505859375, Learning Rate: 0.005\n",
      "Epoch [19705/20000], Loss: -21.350479125976562, Learning Rate: 0.005\n",
      "Epoch [19706/20000], Loss: -21.3505859375, Learning Rate: 0.005\n",
      "Epoch [19707/20000], Loss: -21.350570678710938, Learning Rate: 0.005\n",
      "Epoch [19708/20000], Loss: -21.3505859375, Learning Rate: 0.005\n",
      "Epoch [19709/20000], Loss: -21.3505859375, Learning Rate: 0.005\n",
      "Epoch [19710/20000], Loss: -21.350601196289062, Learning Rate: 0.005\n",
      "Epoch [19711/20000], Loss: -21.3505859375, Learning Rate: 0.005\n",
      "Epoch [19712/20000], Loss: -21.350631713867188, Learning Rate: 0.005\n",
      "Epoch [19713/20000], Loss: -21.35064697265625, Learning Rate: 0.005\n",
      "Epoch [19714/20000], Loss: -21.35064697265625, Learning Rate: 0.005\n",
      "Epoch [19715/20000], Loss: -21.350631713867188, Learning Rate: 0.005\n",
      "Epoch [19716/20000], Loss: -21.350616455078125, Learning Rate: 0.005\n",
      "Epoch [19717/20000], Loss: -21.350631713867188, Learning Rate: 0.005\n",
      "Epoch [19718/20000], Loss: -21.350601196289062, Learning Rate: 0.005\n",
      "Epoch [19719/20000], Loss: -21.35064697265625, Learning Rate: 0.005\n",
      "Epoch [19720/20000], Loss: -21.350616455078125, Learning Rate: 0.005\n",
      "Epoch [19721/20000], Loss: -21.350662231445312, Learning Rate: 0.005\n",
      "Epoch [19722/20000], Loss: -21.35064697265625, Learning Rate: 0.005\n",
      "Epoch [19723/20000], Loss: -21.350662231445312, Learning Rate: 0.005\n",
      "Epoch [19724/20000], Loss: -21.350662231445312, Learning Rate: 0.005\n",
      "Epoch [19725/20000], Loss: -21.3505859375, Learning Rate: 0.005\n",
      "Epoch [19726/20000], Loss: -21.350723266601562, Learning Rate: 0.005\n",
      "Epoch [19727/20000], Loss: -21.350631713867188, Learning Rate: 0.005\n",
      "Epoch [19728/20000], Loss: -21.350616455078125, Learning Rate: 0.005\n",
      "Epoch [19729/20000], Loss: -21.350662231445312, Learning Rate: 0.005\n",
      "Epoch [19730/20000], Loss: -21.3507080078125, Learning Rate: 0.005\n",
      "Epoch [19731/20000], Loss: -21.350692749023438, Learning Rate: 0.005\n",
      "Epoch [19732/20000], Loss: -21.3507080078125, Learning Rate: 0.005\n",
      "Epoch [19733/20000], Loss: -21.350723266601562, Learning Rate: 0.005\n",
      "Epoch [19734/20000], Loss: -21.350723266601562, Learning Rate: 0.005\n",
      "Epoch [19735/20000], Loss: -21.350784301757812, Learning Rate: 0.005\n",
      "Epoch [19736/20000], Loss: -21.350784301757812, Learning Rate: 0.005\n",
      "Epoch [19737/20000], Loss: -21.350784301757812, Learning Rate: 0.005\n",
      "Epoch [19738/20000], Loss: -21.350738525390625, Learning Rate: 0.005\n",
      "Epoch [19739/20000], Loss: -21.350738525390625, Learning Rate: 0.005\n",
      "Epoch [19740/20000], Loss: -21.350845336914062, Learning Rate: 0.005\n",
      "Epoch [19741/20000], Loss: -21.350784301757812, Learning Rate: 0.005\n",
      "Epoch [19742/20000], Loss: -21.35076904296875, Learning Rate: 0.005\n",
      "Epoch [19743/20000], Loss: -21.350753784179688, Learning Rate: 0.005\n",
      "Epoch [19744/20000], Loss: -21.35076904296875, Learning Rate: 0.005\n",
      "Epoch [19745/20000], Loss: -21.350814819335938, Learning Rate: 0.005\n",
      "Epoch [19746/20000], Loss: -21.350753784179688, Learning Rate: 0.005\n",
      "Epoch [19747/20000], Loss: -21.350799560546875, Learning Rate: 0.005\n",
      "Epoch [19748/20000], Loss: -21.350845336914062, Learning Rate: 0.005\n",
      "Epoch [19749/20000], Loss: -21.350814819335938, Learning Rate: 0.005\n",
      "Epoch [19750/20000], Loss: -21.350784301757812, Learning Rate: 0.005\n",
      "Epoch [19751/20000], Loss: -21.350845336914062, Learning Rate: 0.005\n",
      "Epoch [19752/20000], Loss: -21.350784301757812, Learning Rate: 0.005\n",
      "Epoch [19753/20000], Loss: -21.350845336914062, Learning Rate: 0.005\n",
      "Epoch [19754/20000], Loss: -21.350845336914062, Learning Rate: 0.005\n",
      "Epoch [19755/20000], Loss: -21.350860595703125, Learning Rate: 0.005\n",
      "Epoch [19756/20000], Loss: -21.350875854492188, Learning Rate: 0.005\n",
      "Epoch [19757/20000], Loss: -21.350784301757812, Learning Rate: 0.005\n",
      "Epoch [19758/20000], Loss: -21.350799560546875, Learning Rate: 0.005\n",
      "Epoch [19759/20000], Loss: -21.350860595703125, Learning Rate: 0.005\n",
      "Epoch [19760/20000], Loss: -21.350845336914062, Learning Rate: 0.005\n",
      "Epoch [19761/20000], Loss: -21.350845336914062, Learning Rate: 0.005\n",
      "Epoch [19762/20000], Loss: -21.350814819335938, Learning Rate: 0.005\n",
      "Epoch [19763/20000], Loss: -21.350860595703125, Learning Rate: 0.005\n",
      "Epoch [19764/20000], Loss: -21.350830078125, Learning Rate: 0.005\n",
      "Epoch [19765/20000], Loss: -21.350845336914062, Learning Rate: 0.005\n",
      "Epoch [19766/20000], Loss: -21.350875854492188, Learning Rate: 0.005\n",
      "Epoch [19767/20000], Loss: -21.350830078125, Learning Rate: 0.005\n",
      "Epoch [19768/20000], Loss: -21.350860595703125, Learning Rate: 0.005\n",
      "Epoch [19769/20000], Loss: -21.35089111328125, Learning Rate: 0.005\n",
      "Epoch [19770/20000], Loss: -21.350875854492188, Learning Rate: 0.005\n",
      "Epoch [19771/20000], Loss: -21.35089111328125, Learning Rate: 0.005\n",
      "Epoch [19772/20000], Loss: -21.350921630859375, Learning Rate: 0.005\n",
      "Epoch [19773/20000], Loss: -21.350921630859375, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19774/20000], Loss: -21.350906372070312, Learning Rate: 0.005\n",
      "Epoch [19775/20000], Loss: -21.35089111328125, Learning Rate: 0.005\n",
      "Epoch [19776/20000], Loss: -21.350967407226562, Learning Rate: 0.005\n",
      "Epoch [19777/20000], Loss: -21.350982666015625, Learning Rate: 0.005\n",
      "Epoch [19778/20000], Loss: -21.350982666015625, Learning Rate: 0.005\n",
      "Epoch [19779/20000], Loss: -21.350967407226562, Learning Rate: 0.005\n",
      "Epoch [19780/20000], Loss: -21.350921630859375, Learning Rate: 0.005\n",
      "Epoch [19781/20000], Loss: -21.350982666015625, Learning Rate: 0.005\n",
      "Epoch [19782/20000], Loss: -21.350997924804688, Learning Rate: 0.005\n",
      "Epoch [19783/20000], Loss: -21.350967407226562, Learning Rate: 0.005\n",
      "Epoch [19784/20000], Loss: -21.350967407226562, Learning Rate: 0.005\n",
      "Epoch [19785/20000], Loss: -21.350982666015625, Learning Rate: 0.005\n",
      "Epoch [19786/20000], Loss: -21.350936889648438, Learning Rate: 0.005\n",
      "Epoch [19787/20000], Loss: -21.350936889648438, Learning Rate: 0.005\n",
      "Epoch [19788/20000], Loss: -21.350967407226562, Learning Rate: 0.005\n",
      "Epoch [19789/20000], Loss: -21.351028442382812, Learning Rate: 0.005\n",
      "Epoch [19790/20000], Loss: -21.35101318359375, Learning Rate: 0.005\n",
      "Epoch [19791/20000], Loss: -21.35101318359375, Learning Rate: 0.005\n",
      "Epoch [19792/20000], Loss: -21.351028442382812, Learning Rate: 0.005\n",
      "Epoch [19793/20000], Loss: -21.35101318359375, Learning Rate: 0.005\n",
      "Epoch [19794/20000], Loss: -21.350967407226562, Learning Rate: 0.005\n",
      "Epoch [19795/20000], Loss: -21.351058959960938, Learning Rate: 0.005\n",
      "Epoch [19796/20000], Loss: -21.351043701171875, Learning Rate: 0.005\n",
      "Epoch [19797/20000], Loss: -21.35107421875, Learning Rate: 0.005\n",
      "Epoch [19798/20000], Loss: -21.351043701171875, Learning Rate: 0.005\n",
      "Epoch [19799/20000], Loss: -21.35113525390625, Learning Rate: 0.005\n",
      "Epoch [19800/20000], Loss: -21.351058959960938, Learning Rate: 0.005\n",
      "Epoch [19801/20000], Loss: -21.351043701171875, Learning Rate: 0.005\n",
      "Epoch [19802/20000], Loss: -21.351043701171875, Learning Rate: 0.005\n",
      "Epoch [19803/20000], Loss: -21.351043701171875, Learning Rate: 0.005\n",
      "Epoch [19804/20000], Loss: -21.351089477539062, Learning Rate: 0.005\n",
      "Epoch [19805/20000], Loss: -21.351104736328125, Learning Rate: 0.005\n",
      "Epoch [19806/20000], Loss: -21.351150512695312, Learning Rate: 0.005\n",
      "Epoch [19807/20000], Loss: -21.351150512695312, Learning Rate: 0.005\n",
      "Epoch [19808/20000], Loss: -21.35107421875, Learning Rate: 0.005\n",
      "Epoch [19809/20000], Loss: -21.351028442382812, Learning Rate: 0.005\n",
      "Epoch [19810/20000], Loss: -21.351089477539062, Learning Rate: 0.005\n",
      "Epoch [19811/20000], Loss: -21.351119995117188, Learning Rate: 0.005\n",
      "Epoch [19812/20000], Loss: -21.351165771484375, Learning Rate: 0.005\n",
      "Epoch [19813/20000], Loss: -21.351165771484375, Learning Rate: 0.005\n",
      "Epoch [19814/20000], Loss: -21.351165771484375, Learning Rate: 0.005\n",
      "Epoch [19815/20000], Loss: -21.35113525390625, Learning Rate: 0.005\n",
      "Epoch [19816/20000], Loss: -21.351165771484375, Learning Rate: 0.005\n",
      "Epoch [19817/20000], Loss: -21.351165771484375, Learning Rate: 0.005\n",
      "Epoch [19818/20000], Loss: -21.351119995117188, Learning Rate: 0.005\n",
      "Epoch [19819/20000], Loss: -21.351150512695312, Learning Rate: 0.005\n",
      "Epoch [19820/20000], Loss: -21.351226806640625, Learning Rate: 0.005\n",
      "Epoch [19821/20000], Loss: -21.351181030273438, Learning Rate: 0.005\n",
      "Epoch [19822/20000], Loss: -21.351165771484375, Learning Rate: 0.005\n",
      "Epoch [19823/20000], Loss: -21.3511962890625, Learning Rate: 0.005\n",
      "Epoch [19824/20000], Loss: -21.351211547851562, Learning Rate: 0.005\n",
      "Epoch [19825/20000], Loss: -21.351211547851562, Learning Rate: 0.005\n",
      "Epoch [19826/20000], Loss: -21.351181030273438, Learning Rate: 0.005\n",
      "Epoch [19827/20000], Loss: -21.351287841796875, Learning Rate: 0.005\n",
      "Epoch [19828/20000], Loss: -21.351181030273438, Learning Rate: 0.005\n",
      "Epoch [19829/20000], Loss: -21.351333618164062, Learning Rate: 0.005\n",
      "Epoch [19830/20000], Loss: -21.351303100585938, Learning Rate: 0.005\n",
      "Epoch [19831/20000], Loss: -21.351303100585938, Learning Rate: 0.005\n",
      "Epoch [19832/20000], Loss: -21.351303100585938, Learning Rate: 0.005\n",
      "Epoch [19833/20000], Loss: -21.351272583007812, Learning Rate: 0.005\n",
      "Epoch [19834/20000], Loss: -21.351272583007812, Learning Rate: 0.005\n",
      "Epoch [19835/20000], Loss: -21.351287841796875, Learning Rate: 0.005\n",
      "Epoch [19836/20000], Loss: -21.35137939453125, Learning Rate: 0.005\n",
      "Epoch [19837/20000], Loss: -21.351272583007812, Learning Rate: 0.005\n",
      "Epoch [19838/20000], Loss: -21.351303100585938, Learning Rate: 0.005\n",
      "Epoch [19839/20000], Loss: -21.351318359375, Learning Rate: 0.005\n",
      "Epoch [19840/20000], Loss: -21.351333618164062, Learning Rate: 0.005\n",
      "Epoch [19841/20000], Loss: -21.35137939453125, Learning Rate: 0.005\n",
      "Epoch [19842/20000], Loss: -21.351394653320312, Learning Rate: 0.005\n",
      "Epoch [19843/20000], Loss: -21.351333618164062, Learning Rate: 0.005\n",
      "Epoch [19844/20000], Loss: -21.351333618164062, Learning Rate: 0.005\n",
      "Epoch [19845/20000], Loss: -21.351333618164062, Learning Rate: 0.005\n",
      "Epoch [19846/20000], Loss: -21.351348876953125, Learning Rate: 0.005\n",
      "Epoch [19847/20000], Loss: -21.351318359375, Learning Rate: 0.005\n",
      "Epoch [19848/20000], Loss: -21.35137939453125, Learning Rate: 0.005\n",
      "Epoch [19849/20000], Loss: -21.351425170898438, Learning Rate: 0.005\n",
      "Epoch [19850/20000], Loss: -21.351348876953125, Learning Rate: 0.005\n",
      "Epoch [19851/20000], Loss: -21.351425170898438, Learning Rate: 0.005\n",
      "Epoch [19852/20000], Loss: -21.35137939453125, Learning Rate: 0.005\n",
      "Epoch [19853/20000], Loss: -21.351394653320312, Learning Rate: 0.005\n",
      "Epoch [19854/20000], Loss: -21.351364135742188, Learning Rate: 0.005\n",
      "Epoch [19855/20000], Loss: -21.351486206054688, Learning Rate: 0.005\n",
      "Epoch [19856/20000], Loss: -21.351455688476562, Learning Rate: 0.005\n",
      "Epoch [19857/20000], Loss: -21.35137939453125, Learning Rate: 0.005\n",
      "Epoch [19858/20000], Loss: -21.351425170898438, Learning Rate: 0.005\n",
      "Epoch [19859/20000], Loss: -21.351409912109375, Learning Rate: 0.005\n",
      "Epoch [19860/20000], Loss: -21.351455688476562, Learning Rate: 0.005\n",
      "Epoch [19861/20000], Loss: -21.351425170898438, Learning Rate: 0.005\n",
      "Epoch [19862/20000], Loss: -21.3514404296875, Learning Rate: 0.005\n",
      "Epoch [19863/20000], Loss: -21.351470947265625, Learning Rate: 0.005\n",
      "Epoch [19864/20000], Loss: -21.351425170898438, Learning Rate: 0.005\n",
      "Epoch [19865/20000], Loss: -21.351486206054688, Learning Rate: 0.005\n",
      "Epoch [19866/20000], Loss: -21.351455688476562, Learning Rate: 0.005\n",
      "Epoch [19867/20000], Loss: -21.351455688476562, Learning Rate: 0.005\n",
      "Epoch [19868/20000], Loss: -21.351470947265625, Learning Rate: 0.005\n",
      "Epoch [19869/20000], Loss: -21.351547241210938, Learning Rate: 0.005\n",
      "Epoch [19870/20000], Loss: -21.35150146484375, Learning Rate: 0.005\n",
      "Epoch [19871/20000], Loss: -21.35150146484375, Learning Rate: 0.005\n",
      "Epoch [19872/20000], Loss: -21.351547241210938, Learning Rate: 0.005\n",
      "Epoch [19873/20000], Loss: -21.351455688476562, Learning Rate: 0.005\n",
      "Epoch [19874/20000], Loss: -21.351531982421875, Learning Rate: 0.005\n",
      "Epoch [19875/20000], Loss: -21.351593017578125, Learning Rate: 0.005\n",
      "Epoch [19876/20000], Loss: -21.35150146484375, Learning Rate: 0.005\n",
      "Epoch [19877/20000], Loss: -21.351593017578125, Learning Rate: 0.005\n",
      "Epoch [19878/20000], Loss: -21.3515625, Learning Rate: 0.005\n",
      "Epoch [19879/20000], Loss: -21.351577758789062, Learning Rate: 0.005\n",
      "Epoch [19880/20000], Loss: -21.351593017578125, Learning Rate: 0.005\n",
      "Epoch [19881/20000], Loss: -21.351531982421875, Learning Rate: 0.005\n",
      "Epoch [19882/20000], Loss: -21.3515625, Learning Rate: 0.005\n",
      "Epoch [19883/20000], Loss: -21.351547241210938, Learning Rate: 0.005\n",
      "Epoch [19884/20000], Loss: -21.351516723632812, Learning Rate: 0.005\n",
      "Epoch [19885/20000], Loss: -21.351577758789062, Learning Rate: 0.005\n",
      "Epoch [19886/20000], Loss: -21.3515625, Learning Rate: 0.005\n",
      "Epoch [19887/20000], Loss: -21.3515625, Learning Rate: 0.005\n",
      "Epoch [19888/20000], Loss: -21.351531982421875, Learning Rate: 0.005\n",
      "Epoch [19889/20000], Loss: -21.351516723632812, Learning Rate: 0.005\n",
      "Epoch [19890/20000], Loss: -21.351547241210938, Learning Rate: 0.005\n",
      "Epoch [19891/20000], Loss: -21.351531982421875, Learning Rate: 0.005\n",
      "Epoch [19892/20000], Loss: -21.351608276367188, Learning Rate: 0.005\n",
      "Epoch [19893/20000], Loss: -21.351638793945312, Learning Rate: 0.005\n",
      "Epoch [19894/20000], Loss: -21.35162353515625, Learning Rate: 0.005\n",
      "Epoch [19895/20000], Loss: -21.351577758789062, Learning Rate: 0.005\n",
      "Epoch [19896/20000], Loss: -21.351730346679688, Learning Rate: 0.005\n",
      "Epoch [19897/20000], Loss: -21.351638793945312, Learning Rate: 0.005\n",
      "Epoch [19898/20000], Loss: -21.351638793945312, Learning Rate: 0.005\n",
      "Epoch [19899/20000], Loss: -21.351593017578125, Learning Rate: 0.005\n",
      "Epoch [19900/20000], Loss: -21.351593017578125, Learning Rate: 0.005\n",
      "Epoch [19901/20000], Loss: -21.35162353515625, Learning Rate: 0.005\n",
      "Epoch [19902/20000], Loss: -21.351715087890625, Learning Rate: 0.005\n",
      "Epoch [19903/20000], Loss: -21.3516845703125, Learning Rate: 0.005\n",
      "Epoch [19904/20000], Loss: -21.351669311523438, Learning Rate: 0.005\n",
      "Epoch [19905/20000], Loss: -21.351715087890625, Learning Rate: 0.005\n",
      "Epoch [19906/20000], Loss: -21.351715087890625, Learning Rate: 0.005\n",
      "Epoch [19907/20000], Loss: -21.351715087890625, Learning Rate: 0.005\n",
      "Epoch [19908/20000], Loss: -21.351715087890625, Learning Rate: 0.005\n",
      "Epoch [19909/20000], Loss: -21.351638793945312, Learning Rate: 0.005\n",
      "Epoch [19910/20000], Loss: -21.351776123046875, Learning Rate: 0.005\n",
      "Epoch [19911/20000], Loss: -21.35174560546875, Learning Rate: 0.005\n",
      "Epoch [19912/20000], Loss: -21.351730346679688, Learning Rate: 0.005\n",
      "Epoch [19913/20000], Loss: -21.351760864257812, Learning Rate: 0.005\n",
      "Epoch [19914/20000], Loss: -21.35174560546875, Learning Rate: 0.005\n",
      "Epoch [19915/20000], Loss: -21.351776123046875, Learning Rate: 0.005\n",
      "Epoch [19916/20000], Loss: -21.351776123046875, Learning Rate: 0.005\n",
      "Epoch [19917/20000], Loss: -21.351791381835938, Learning Rate: 0.005\n",
      "Epoch [19918/20000], Loss: -21.351776123046875, Learning Rate: 0.005\n",
      "Epoch [19919/20000], Loss: -21.351791381835938, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19920/20000], Loss: -21.351821899414062, Learning Rate: 0.005\n",
      "Epoch [19921/20000], Loss: -21.351791381835938, Learning Rate: 0.005\n",
      "Epoch [19922/20000], Loss: -21.351776123046875, Learning Rate: 0.005\n",
      "Epoch [19923/20000], Loss: -21.35174560546875, Learning Rate: 0.005\n",
      "Epoch [19924/20000], Loss: -21.351760864257812, Learning Rate: 0.005\n",
      "Epoch [19925/20000], Loss: -21.351806640625, Learning Rate: 0.005\n",
      "Epoch [19926/20000], Loss: -21.351821899414062, Learning Rate: 0.005\n",
      "Epoch [19927/20000], Loss: -21.351730346679688, Learning Rate: 0.005\n",
      "Epoch [19928/20000], Loss: -21.351821899414062, Learning Rate: 0.005\n",
      "Epoch [19929/20000], Loss: -21.351806640625, Learning Rate: 0.005\n",
      "Epoch [19930/20000], Loss: -21.351882934570312, Learning Rate: 0.005\n",
      "Epoch [19931/20000], Loss: -21.351837158203125, Learning Rate: 0.005\n",
      "Epoch [19932/20000], Loss: -21.351821899414062, Learning Rate: 0.005\n",
      "Epoch [19933/20000], Loss: -21.351837158203125, Learning Rate: 0.005\n",
      "Epoch [19934/20000], Loss: -21.35186767578125, Learning Rate: 0.005\n",
      "Epoch [19935/20000], Loss: -21.351821899414062, Learning Rate: 0.005\n",
      "Epoch [19936/20000], Loss: -21.35186767578125, Learning Rate: 0.005\n",
      "Epoch [19937/20000], Loss: -21.351898193359375, Learning Rate: 0.005\n",
      "Epoch [19938/20000], Loss: -21.351837158203125, Learning Rate: 0.005\n",
      "Epoch [19939/20000], Loss: -21.351898193359375, Learning Rate: 0.005\n",
      "Epoch [19940/20000], Loss: -21.351898193359375, Learning Rate: 0.005\n",
      "Epoch [19941/20000], Loss: -21.351852416992188, Learning Rate: 0.005\n",
      "Epoch [19942/20000], Loss: -21.351959228515625, Learning Rate: 0.005\n",
      "Epoch [19943/20000], Loss: -21.35186767578125, Learning Rate: 0.005\n",
      "Epoch [19944/20000], Loss: -21.3519287109375, Learning Rate: 0.005\n",
      "Epoch [19945/20000], Loss: -21.351913452148438, Learning Rate: 0.005\n",
      "Epoch [19946/20000], Loss: -21.351943969726562, Learning Rate: 0.005\n",
      "Epoch [19947/20000], Loss: -21.351943969726562, Learning Rate: 0.005\n",
      "Epoch [19948/20000], Loss: -21.351959228515625, Learning Rate: 0.005\n",
      "Epoch [19949/20000], Loss: -21.351898193359375, Learning Rate: 0.005\n",
      "Epoch [19950/20000], Loss: -21.352020263671875, Learning Rate: 0.005\n",
      "Epoch [19951/20000], Loss: -21.351959228515625, Learning Rate: 0.005\n",
      "Epoch [19952/20000], Loss: -21.351974487304688, Learning Rate: 0.005\n",
      "Epoch [19953/20000], Loss: -21.351959228515625, Learning Rate: 0.005\n",
      "Epoch [19954/20000], Loss: -21.35198974609375, Learning Rate: 0.005\n",
      "Epoch [19955/20000], Loss: -21.352020263671875, Learning Rate: 0.005\n",
      "Epoch [19956/20000], Loss: -21.352005004882812, Learning Rate: 0.005\n",
      "Epoch [19957/20000], Loss: -21.351959228515625, Learning Rate: 0.005\n",
      "Epoch [19958/20000], Loss: -21.35205078125, Learning Rate: 0.005\n",
      "Epoch [19959/20000], Loss: -21.351943969726562, Learning Rate: 0.005\n",
      "Epoch [19960/20000], Loss: -21.35198974609375, Learning Rate: 0.005\n",
      "Epoch [19961/20000], Loss: -21.352005004882812, Learning Rate: 0.005\n",
      "Epoch [19962/20000], Loss: -21.352066040039062, Learning Rate: 0.005\n",
      "Epoch [19963/20000], Loss: -21.352096557617188, Learning Rate: 0.005\n",
      "Epoch [19964/20000], Loss: -21.352035522460938, Learning Rate: 0.005\n",
      "Epoch [19965/20000], Loss: -21.352005004882812, Learning Rate: 0.005\n",
      "Epoch [19966/20000], Loss: -21.352020263671875, Learning Rate: 0.005\n",
      "Epoch [19967/20000], Loss: -21.352035522460938, Learning Rate: 0.005\n",
      "Epoch [19968/20000], Loss: -21.352020263671875, Learning Rate: 0.005\n",
      "Epoch [19969/20000], Loss: -21.352096557617188, Learning Rate: 0.005\n",
      "Epoch [19970/20000], Loss: -21.352096557617188, Learning Rate: 0.005\n",
      "Epoch [19971/20000], Loss: -21.352096557617188, Learning Rate: 0.005\n",
      "Epoch [19972/20000], Loss: -21.35211181640625, Learning Rate: 0.005\n",
      "Epoch [19973/20000], Loss: -21.352066040039062, Learning Rate: 0.005\n",
      "Epoch [19974/20000], Loss: -21.352035522460938, Learning Rate: 0.005\n",
      "Epoch [19975/20000], Loss: -21.352142333984375, Learning Rate: 0.005\n",
      "Epoch [19976/20000], Loss: -21.352157592773438, Learning Rate: 0.005\n",
      "Epoch [19977/20000], Loss: -21.352096557617188, Learning Rate: 0.005\n",
      "Epoch [19978/20000], Loss: -21.352127075195312, Learning Rate: 0.005\n",
      "Epoch [19979/20000], Loss: -21.352188110351562, Learning Rate: 0.005\n",
      "Epoch [19980/20000], Loss: -21.352081298828125, Learning Rate: 0.005\n",
      "Epoch [19981/20000], Loss: -21.352127075195312, Learning Rate: 0.005\n",
      "Epoch [19982/20000], Loss: -21.3521728515625, Learning Rate: 0.005\n",
      "Epoch [19983/20000], Loss: -21.352127075195312, Learning Rate: 0.005\n",
      "Epoch [19984/20000], Loss: -21.352157592773438, Learning Rate: 0.005\n",
      "Epoch [19985/20000], Loss: -21.3521728515625, Learning Rate: 0.005\n",
      "Epoch [19986/20000], Loss: -21.352218627929688, Learning Rate: 0.005\n",
      "Epoch [19987/20000], Loss: -21.352218627929688, Learning Rate: 0.005\n",
      "Epoch [19988/20000], Loss: -21.352203369140625, Learning Rate: 0.005\n",
      "Epoch [19989/20000], Loss: -21.352142333984375, Learning Rate: 0.005\n",
      "Epoch [19990/20000], Loss: -21.352127075195312, Learning Rate: 0.005\n",
      "Epoch [19991/20000], Loss: -21.352157592773438, Learning Rate: 0.005\n",
      "Epoch [19992/20000], Loss: -21.352188110351562, Learning Rate: 0.005\n",
      "Epoch [19993/20000], Loss: -21.352264404296875, Learning Rate: 0.005\n",
      "Epoch [19994/20000], Loss: -21.352264404296875, Learning Rate: 0.005\n",
      "Epoch [19995/20000], Loss: -21.352264404296875, Learning Rate: 0.005\n",
      "Epoch [19996/20000], Loss: -21.352218627929688, Learning Rate: 0.005\n",
      "Epoch [19997/20000], Loss: -21.35223388671875, Learning Rate: 0.005\n",
      "Epoch [19998/20000], Loss: -21.352264404296875, Learning Rate: 0.005\n",
      "Epoch [19999/20000], Loss: -21.352279663085938, Learning Rate: 0.005\n",
      "Epoch [20000/20000], Loss: -21.352249145507812, Learning Rate: 0.005\n"
     ]
    }
   ],
   "source": [
    "from Inference.PointEstimate import AdamGradientDescent\n",
    "def _MAP(nbiter, std_init,logposterior, dim, device='cpu'):\n",
    "        optimizer = AdamGradientDescent(logposterior, nbiter, .01, .00000001, 50, .5, device, True)\n",
    "\n",
    "        theta0 = torch.empty((1, dim), device=device).normal_(0., std=std_init)\n",
    "        best_theta, best_score, score = optimizer.run(theta0)\n",
    "\n",
    "        return best_theta.detach().clone()\n",
    "\n",
    "theta=_MAP(20000,1., logposterior, param_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mylogpdf(x):\n",
    "    return logposterior(x)\n",
    "def potential(x):\n",
    "    theta=torch.Tensor(x).requires_grad_(True).float()\n",
    "    #print(x)\n",
    "    lp=mylogpdf(theta.unsqueeze(0))\n",
    "    lp.backward()\n",
    "    return -lp.detach().numpy(), -theta.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfc8b57591a4c3b8a42d8b173e677a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "samples = hamiltonian_monte_carlo(500, potential,\n",
    "                                  initial_position=theta.squeeze().numpy(), \n",
    "                                  initial_step_size=0.001,\n",
    "                                  path_len=.1\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan],\n",
       "        [nan, nan, nan,  ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_ens=torch.as_tensor(samples)\n",
    "theta_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2cbff9b1d642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msetup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakePlot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/GeNVI/BayesianNN/Experiments/foong/__init__.py\u001b[0m in \u001b[0;36mmakePlot\u001b[0;34m(self, theta, device)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalized_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             plt.plot(x_lin.detach().cpu().numpy(), y_pred.squeeze(0).detach().cpu().numpy(), alpha=0.8*alpha, linewidth=my_lw,\n\u001b[1;32m     70\u001b[0m                      color='springgreen', zorder=2)\n",
      "\u001b[0;32m~/GeNVI/BayesianNN/Experiments/__init__.py\u001b[0m in \u001b[0;36m_normalized_prediction\u001b[0;34m(self, X, theta, device)\u001b[0m\n\u001b[1;32m    116\u001b[0m \t\t\"\"\"\n\u001b[1;32m    117\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_scaler_y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaler_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaler_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GeNVI/BayesianNN/Models/__init__.py\u001b[0m in \u001b[0;36mmlp\u001b[0;34m(x, theta, input_dim, layerwidth, nb_layers, activation)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0minput_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         m = torch.matmul(theta[0].view(\n\u001b[0;32m---> 44\u001b[0;31m             nb_theta, 1, layerwidth, input_dim), input_x)\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayerwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup.makePlot(torch.as_tensor(samples),device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
