{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "#import Experiments.Foong_L5W50.setup as exp\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as  pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find what device to work with\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Boston Housing Dataset .pt (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('Experiments/Boston/Data/boston_housing.txt')\n",
    "X = data[:, range(data.shape[ 1 ] - 1) ]\n",
    "y = data[:, data.shape[ 1 ] - 1 ]\n",
    "\n",
    "permutation = np.random.choice(range(X.shape[0]), X.shape[0], replace = False)\n",
    "size_train = np.round(X.shape[0]*0.9)\n",
    "index_train = permutation[0:int(size_train)]\n",
    "index_test = permutation[int(size_train) : ]\n",
    "\n",
    "X_train = X[ index_train, : ]\n",
    "y_train = y[ index_train ]\n",
    "X_test = X[ index_test, : ]\n",
    "y_test = y[ index_test ]\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "torch.save(X_train, 'Experiments/Boston/Data/boston_train_X.pt')\n",
    "torch.save(y_train, 'Experiments/Boston/Data/boston_train_y.pt')\n",
    "\n",
    "torch.save(X_test, 'Experiments/Boston/Data/boston_test_X.pt')\n",
    "torch.save(y_test, 'Experiments/Boston/Data/boston_test_y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.load('Experiments/Boston/Data/boston_train_X.pt').to(device).float()\n",
    "y_train = torch.load('Experiments/Boston/Data/boston_train_y.pt').to(device).float()\n",
    "x_test = torch.load('Experiments/Boston/Data/boston_test_X.pt').to(device).float()\n",
    "y_test = torch.load('Experiments/Boston/Data/boston_test_y.pt').to(device).float()\n",
    "y_train = y_train.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Net #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=13\n",
    "nblayers = 1\n",
    "activation=nn.Tanh()\n",
    "layerwidth = 50\n",
    "param_count = (input_dim+1)*layerwidth+(nblayers-1)*(layerwidth**2+layerwidth)+layerwidth+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x,theta,input_dim=input_dim,layerwidth=layerwidth,nb_layers=nblayers,activation=activation):\n",
    "    \"\"\"\n",
    "    Feedforward neural network used as the observation model for the likelihood\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): Input of the network of size NbExemples X NbDimensions   \n",
    "        theta (Tensor):  M set of parameters of the network NbModels X NbParam\n",
    "        input_dim (Int): dimensions of NN's inputs (=NbDimensions)\n",
    "        layerwidth (Int): Number of hidden units per layer \n",
    "        nb_layers (Int): Number of layers\n",
    "        activation (Module/Function): activation function of the neural network\n",
    "\n",
    "    Returns:\n",
    "        Predictions (Tensor) with dimensions NbModels X NbExemples X NbDimensions\n",
    "\n",
    "\n",
    "    Example:\n",
    "\n",
    "    input_dim=11\n",
    "    nblayers = 2\n",
    "    activation=nn.Tanh()\n",
    "    layerwidth = 20\n",
    "    param_count = (input_dim+1)*layerwidth+(nblayers-1)*(layerwidth**2+layerwidth)+layerwidth+1\n",
    "\n",
    "    x=torch.rand(3,input_dim)\n",
    "    theta=torch.rand(5,param_count)\n",
    "    mlp(x,theta,input_dim=input_dim,layerwidth=layerwidth,nb_layers=nblayers,activation=activation)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    nb_theta=theta.shape[0]\n",
    "    nb_x=x.shape[0]\n",
    "    split_sizes=[input_dim*layerwidth]+[layerwidth]+[layerwidth**2,layerwidth]*(nb_layers-1)+[layerwidth,1]\n",
    "    theta=theta.split(split_sizes,dim=1)\n",
    "    input_x=x.view(nb_x,input_dim,1)\n",
    "    m=torch.matmul(theta[0].view(nb_theta,1,layerwidth,input_dim),input_x)\n",
    "    m=m.add(theta[1].reshape(nb_theta,1,layerwidth,1))\n",
    "    m=activation(m)\n",
    "    for i in range(nb_layers-1):\n",
    "        m=torch.matmul(theta[2*i+2].view(-1,1,layerwidth,layerwidth),m)\n",
    "        m=m.add(theta[2*i+3].reshape(-1,1,layerwidth,1))\n",
    "        m=activation(m)\n",
    "    m=torch.matmul(theta[2*(nb_layers-1)+2].view(nb_theta,1,1,layerwidth),m)\n",
    "    m=m.add(theta[2*(nb_layers-1)+3].reshape(nb_theta,1,1,1))\n",
    "    return m.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior density #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_noise = torch.tensor(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_norm(x, mu, std):\n",
    "    \"\"\"\n",
    "    Evaluation of 1D normal distribution on tensors\n",
    "\n",
    "    Parameters:\n",
    "        x (Tensor): Data tensor\n",
    "        mu (Tensor): Mean tensor of same size as x\n",
    "        std (Tensor): Positive scalar\n",
    "\n",
    "    Returns:\n",
    "        logproba (Tensor): Same size as x\n",
    "    \"\"\"\n",
    "    return -0.5 * torch.log(2*np.pi*std**2) -(0.5 * (1/(std**2))* (x-mu)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.ones(param_count).to(device)\n",
    "mu = torch.zeros(param_count).to(device)\n",
    "dim=param_count\n",
    "def logprior(x,m=mu, s=S,dim=dim):\n",
    "    n_x=x.shape[0]\n",
    "    H=S.view(dim,1,1).inverse().view(1,1,dim)\n",
    "    d=((x-mu.view(1,dim))**2).view(1,n_x,dim)\n",
    "    const=0.5*S.log().sum()+0.5*dim*torch.tensor(2*math.pi).log()\n",
    "    return -0.5*(H*d).sum(2)-const\n",
    "#test!\n",
    "#prior=torch.distributions.multivariate_normal.MultivariateNormal(mu,covariance_matrix=torch.diagflat(S))\n",
    "#x=torch.rand(10,param_count)\n",
    "#torch.allclose(logprior(x),prior.log_prob(x))\n",
    "#True!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(theta, x, y, sigma_noise):\n",
    "        y_pred = mlp(x, theta)\n",
    "        L = _log_norm(y_pred, y.unsqueeze(0).repeat(theta.shape[0], 1, 1), torch.tensor([sigma_noise], device=device))\n",
    "        return torch.sum(L, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logposterior(theta, x, y, sigma_noise):\n",
    "        return logprior(theta).add(loglikelihood(theta, x, y, sigma_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logtarget = lambda theta : logposterior(theta, x_train, y_train, sigma_noise )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypernet #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_size=1\n",
    "lat_dim=5\n",
    "init_w=.15\n",
    "init_b=.001\n",
    "activation=nn.ReLU()\n",
    "output_dim=param_count\n",
    "#x_train, y_train = exp.get_training_data(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train[:,:1].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = y_train.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNet(nn.Module):\n",
    "            def __init__(self, lat_dim, nb_neur, output_dim,  activation=nn.ReLU(), init_w=.1, init_b=.1):\n",
    "                super(HNet, self).__init__()\n",
    "                self.lat_dim = lat_dim\n",
    "                self.output_dim=output_dim\n",
    "                self.hnet=nn.Sequential(\n",
    "                        nn.Linear(lat_dim,nb_neur),\n",
    "                        activation,\n",
    "                        nn.Linear(nb_neur,output_dim)\n",
    "                        ).to(device)\n",
    "                \n",
    "                torch.nn.init.normal_(self.hnet[2].weight,mean=0., std=init_w)\n",
    "                torch.nn.init.normal_(self.hnet[2].bias,mean=0., std=init_b)\n",
    "    \n",
    "            def forward(self, n=1):\n",
    "                epsilon = torch.randn(size=(n,self.lat_dim)).to(device)\n",
    "                return self.hnet(epsilon)           \n",
    "\n",
    "class HyNetEns(nn.Module):\n",
    "    def __init__(self,nb_comp,lat_dim, output_dim, activation, init_w,init_b):\n",
    "        super(HyNetEns, self).__init__()\n",
    "        self.nb_comp=nb_comp\n",
    "        self.output_dim=output_dim\n",
    "        self.components= nn.ModuleList([HNet(lat_dim,output_dim,output_dim,activation,init_w,init_b) for i in range(nb_comp)]).to(device)   \n",
    "\n",
    "        # \"Silverman's rule of thumb\", Wand and Jones p.111 \"Kernel Smoothing\" 1995.                                 \n",
    "    def get_H(self, nb_samples):\n",
    "        theta=self.sample(nb_samples)\n",
    "        c_=(nb_samples*(self.output_dim+2))/4\n",
    "        c=torch.as_tensor(c_).pow(2/(self.output_dim+4)).to(device)      \n",
    "        H_=theta.var(1)/c\n",
    "        #H_=(theta.var()/c)*torch.ones(self.output_dim) #to try!\n",
    "        return theta, H_.clamp(torch.finfo().eps,float('inf'))\n",
    "\n",
    "    def KDE(self, theta_,theta, H_):\n",
    "        def kernel(theta1,theta2,H):\n",
    "            mvn = torch.distributions.multivariate_normal.MultivariateNormal(theta1, torch.diagflat(H))\n",
    "            return mvn.log_prob(theta2)\n",
    "        LQ=torch.Tensor(theta_.shape[0],self.nb_comp,theta.shape[1]).to(device) \n",
    "        for c in range(self.nb_comp):\n",
    "            for i in range(theta_.shape[0]):\n",
    "                LQ[i,c]=kernel(theta_[i],theta[c],H_[c])\n",
    "        N_=self.nb_comp*theta.shape[1]\n",
    "        N=torch.as_tensor(float(N_)).to(device)\n",
    "        return (LQ.logsumexp(2).logsumexp(1)-torch.log(N)).unsqueeze(1)\n",
    "\n",
    "    def sample(self, n=1):\n",
    "        return torch.stack([self.components[c](n) for c in range(self.nb_comp)])\n",
    "\n",
    "    \n",
    "    def forward(self, n=1):\n",
    "        d = torch.distributions.multinomial.Multinomial(n, torch.ones(self.nb_comp))\n",
    "        m = d.sample()\n",
    "        return torch.cat([self.components[c](int(m[c])) for c in range(len(self.components))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyper_Nets=HyNetEns(ensemble_size,lat_dim, output_dim,activation,init_w,init_b).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "theta =Hyper_Nets.sample(500).detach().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(6, 3,figsize=(8,15))\n",
    "for k in range(6):\n",
    "    for l in range(3):\n",
    "        i=np.random.choice(a=np.arange(param_count), size=2,replace=False)\n",
    "        axs[k, l].set(title=str(i[0])+' vs '+str(i[1]))\n",
    "        for c in range(Hyper_Nets.nb_comp):\n",
    "            axs[k, l].scatter(theta[c,:,i[0]],theta[c,:,i[1]],marker='.',alpha=0.2,color='C'+str(c+2))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "x_lin = torch.arange(-2.,2.0,.01).unsqueeze(1).to(device)\n",
    "# Sampling the distribution over Neural Networks 1000 times, and plotting with transparency to make it appear as a smooth distribution\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 10)\n",
    "#plt.title('Prior')\n",
    "#ax.set_ylim(-4,4)\n",
    "\n",
    "plt.scatter(x_train.cpu(), y_train.cpu())\n",
    "\n",
    "nb_samples_plt=10\n",
    "theta = Hyper_Nets.sample(nb_samples_plt).detach()\n",
    "for c in range(Hyper_Nets.nb_comp):\n",
    "    for i in range(nb_samples_plt):\n",
    "        y_pred = mlp(x_lin,theta[c,i].unsqueeze(0))\n",
    "    #    plt.plot(x_test.detach().cpu().numpy(), y_test.squeeze(0).detach().cpu().numpy(), alpha=0.05, linewidth=1, color='green')\n",
    "        plt.plot(x_lin.detach().cpu().numpy(), y_pred.squeeze(0).detach().cpu().numpy(), alpha=0.05, linewidth=1, color='C'+str(c+2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy estimation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KDE(x,y,prec):\n",
    "    \"\"\"\n",
    "    KDE    \n",
    "    \n",
    "    Parameters:\n",
    "        x (Tensor): Inputs, NbExemples X NbDimensions   \n",
    "        y (Tensor):  Batched samples, NbBatch x NbSamples X NbDimensions\n",
    "        prec (Float): scalar factor for bandwidth scaling\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "        (Tensor) KDE estimate for x based on batched diagonal \"Silverman's rule of thumb\", NbExemples X 1\n",
    "        See Wand and Jones p.111 \"Kernel Smoothing\" 1995.  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dim=x.shape[-1]\n",
    "    n_ed=x.shape[0]\n",
    "    n_comp=y.shape[0]\n",
    "    n_kde=y.shape[1]\n",
    "    c_=(n_kde*(dim+2))/4\n",
    "    c=torch.as_tensor(c_).pow(2/(dim+4)).to(device)  \n",
    "    H=prec*(y.var(1)/c).clamp(torch.finfo().eps,float('inf'))\n",
    "\n",
    "    d=((y.view(n_comp,n_kde,1,dim)-x.view(1,1,n_ed,dim))**2)\n",
    "    H_=H.view(n_comp,dim,1,1).inverse().view(n_comp,1,1,dim)\n",
    "    const=0.5*H.log().sum(1)+0.5*dim*torch.tensor(2*math.pi).log()\n",
    "    const=const.view(n_comp,1,1)\n",
    "    ln=-0.5*(H_*d).sum(3)-const\n",
    "    N=torch.as_tensor(float(n_comp*n_kde),device=device)\n",
    "    return (ln.logsumexp(0).logsumexp(0)-torch.log(N)).unsqueeze(-1)\n",
    "\n",
    "\n",
    "def NNE(theta,k=1):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        theta (Tensor): Samples, NbExemples X NbDimensions   \n",
    "        k (Int): ordinal number\n",
    "\n",
    "    Returns:\n",
    "        (Float) k-Nearest Neighbour Estimation of the entropy of theta  \n",
    "    \n",
    "    \"\"\"\n",
    "    nb_samples=theta.shape[0]\n",
    "    dim=theta.shape[1]\n",
    "    D=torch.cdist(theta,theta)\n",
    "    a = torch.topk(D, k=k+1, dim=0, largest=False, sorted=True)[0][k].clamp(torch.finfo().eps,float('inf')).to(device)\n",
    "    d=torch.as_tensor(float(dim),device=device)\n",
    "    K=torch.as_tensor(float(k),device=device)\n",
    "    N=torch.as_tensor(float(nb_samples),device=device)\n",
    "    pi=torch.as_tensor(math.pi,device=device)\n",
    "    lcd = d/2.*pi.log() - torch.lgamma(1. + d/2.0)\n",
    "    return torch.digamma(N) - torch.digamma(K) + lcd + d/nb_samples*torch.sum(torch.log(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of density estimation on Multivariate Normal\n",
    "\n",
    "dim=100\n",
    "p=torch.distributions.multivariate_normal.MultivariateNormal(torch.zeros(dim), torch.diagflat(2.*torch.ones(dim)))\n",
    "print(p.entropy())\n",
    "print(-KDE(p.sample((1,1000)).squeeze(0), p.sample((1,1000)),1.).mean())\n",
    "print(NNE(p.sample((1,1000)).squeeze(0)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KDE_prec=1.\n",
    "n_samples_KDE=500\n",
    "n_samples_NNE=1000\n",
    "n_samples_ED=10\n",
    "n_samples_LP=10\n",
    "max_iter=10000\n",
    "learning_rate=0.001\n",
    "min_lr=0.000005\n",
    "patience=1000\n",
    "lr_decay=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-KDE(Hyper_Nets(n_samples_ED),Hyper_Nets.sample(n_samples_KDE),KDE_prec).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNE(Hyper_Nets(n_samples_NNE),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "liveloss = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "optimizer = torch.optim.Adam(Hyper_Nets.parameters(), lr=learning_rate)\n",
    "        \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=lr_decay)\n",
    "\n",
    "        \n",
    "for t in range(max_iter):\n",
    "    optimizer.zero_grad()\n",
    "    logs = {}\n",
    "\n",
    "\n",
    "#KDE:    \n",
    "    ED=-KDE(Hyper_Nets(n_samples_ED),Hyper_Nets.sample(n_samples_KDE),KDE_prec).mean()\n",
    "#Nearest Neighbour Entropy estimation\n",
    "#    ED=NNE(Hyper_Nets(n_samples_NNE))\n",
    "    LP=logtarget(Hyper_Nets(n_samples_LP)).mean()\n",
    "    L =-ED-LP\n",
    "\n",
    "    L.backward()\n",
    "\n",
    "    #training_loss.append(L.detach().clone().cpu().numpy())\n",
    "\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    logs['ELBO'] = L.detach().clone().cpu().numpy()\n",
    "    logs['entropie diff'] = ED.detach().clone().cpu().numpy()\n",
    "    logs['learning rate'] = learning_rate\n",
    "    \n",
    "\n",
    "    scheduler.step(L.detach().clone().cpu().numpy())\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % 10 == 0:\n",
    "        liveloss.update(logs)\n",
    "        liveloss.draw()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if lr < min_lr:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLPD from Quinonero-Candela and al.\n",
    "# the average negative log predictive density (NLPD) of the true targets\n",
    "\n",
    "def NLPD(theta, x, y, sigma_noise):\n",
    "    y_pred = mlp(x, theta)\n",
    "    L = _log_norm(y_pred, y, torch.tensor([sigma_noise], device=device))\n",
    "    n_x = torch.as_tensor(float(x.shape[0]), device=device)\n",
    "    n_theta = torch.as_tensor(float(theta.shape[0]), device=device)\n",
    "    log_posterior_predictive = torch.logsumexp(L, 0) - torch.log(n_theta)\n",
    "    return torch.std_mean(log_posterior_predictive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpp=NLPD(Hyper_Nets(1000),x_test,y_test,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
