{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jaxlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8dc7b2c3c93e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0monp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvmap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearningV1\\lib\\site-packages\\jax\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m  \u001b[1;31m# side-effecting import sets up operator overloads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearningV1\\lib\\site-packages\\jax\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m from .util import (unzip2, unzip3, curry, partial, safe_map, safe_zip,\n\u001b[0;32m     51\u001b[0m                    WrapHashably, Hashable, prod)\n\u001b[1;32m---> 52\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxla_bridge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcanonicalize_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mabstract_arrays\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mShapedArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minterpreters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpartial_eval\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MachineLearningV1\\lib\\site-packages\\jax\\lib\\xla_bridge.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mjaxlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0m_minimum_jaxlib_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jaxlib'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We demonstrate how to use NUTS to do inference on a simple (small)\n",
    "Bayesian neural network with two hidden layers.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as onp\n",
    "\n",
    "import jax.numpy as np\n",
    "import jax.random as random\n",
    "from jax import vmap\n",
    "from jax.config import config as jax_config\n",
    "\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.handlers import sample, seed, substitute, trace\n",
    "from numpyro.hmc_util import initialize_model\n",
    "from numpyro.mcmc import mcmc\n",
    "\n",
    "matplotlib.use('Agg')  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the non-linearity we use in our neural network\n",
    "def nonlin(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a two-layer bayesian neural network with computational flow\n",
    "# given by D_X => D_H => D_H => D_Y where D_H is the number of\n",
    "# hidden units. (note we indicate tensor dimensions in the comments)\n",
    "def model(X, Y, D_H):\n",
    "    D_X, D_Y = X.shape[1], 1\n",
    "\n",
    "    # sample first layer (we put unit normal priors on all weights)\n",
    "    w1 = sample(\"w1\", dist.Normal(np.zeros((D_X, D_H)), np.ones((D_X, D_H))))  # D_X D_H\n",
    "    z1 = nonlin(np.matmul(X, w1))   # N D_H  <= first layer of activations\n",
    "\n",
    "    # sample second layer\n",
    "    w2 = sample(\"w2\", dist.Normal(np.zeros((D_H, D_H)), np.ones((D_H, D_H))))  # D_H D_H\n",
    "    z2 = nonlin(np.matmul(z1, w2))  # N D_H  <= second layer of activations\n",
    "\n",
    "    # sample final layer of weights and neural network output\n",
    "    w3 = sample(\"w3\", dist.Normal(np.zeros((D_H, D_Y)), np.ones((D_H, D_Y))))  # D_H D_Y\n",
    "    z3 = np.matmul(z2, w3)  # N D_Y  <= output of the neural network\n",
    "\n",
    "    # we put a prior on the observation noise\n",
    "    prec_obs = sample(\"prec_obs\", dist.Gamma(3.0, 1.0))\n",
    "    sigma_obs = 1.0 / np.sqrt(prec_obs)\n",
    "\n",
    "    # observe data\n",
    "    sample(\"Y\", dist.Normal(z3, sigma_obs), obs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for HMC inference\n",
    "def run_inference(model, args, rng, X, Y, D_H):\n",
    "    init_params, potential_fn, constrain_fn = initialize_model(rng, model, X, Y, D_H)\n",
    "    samples = mcmc(args.num_warmup, args.num_samples, init_params,\n",
    "                   sampler='hmc', potential_fn=potential_fn, constrain_fn=constrain_fn)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for prediction\n",
    "def predict(model, rng, samples, X, D_H):\n",
    "    model = substitute(seed(model, rng), samples)\n",
    "    # note that Y will be sampled in the model because we pass Y=None here\n",
    "    model_trace = trace(model).get_trace(X=X, Y=None, D_H=D_H)\n",
    "    return model_trace['Y']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create artificial regression dataset\n",
    "def get_data(N=50, D_X=3, sigma_obs=0.05, N_test=500):\n",
    "    D_Y = 1  # create 1d outputs\n",
    "    onp.random.seed(0)\n",
    "    X = np.linspace(-1, 1, N)\n",
    "    X = np.power(X[:, onp.newaxis], np.arange(D_X))\n",
    "    W = 0.5 * onp.random.randn(D_X)\n",
    "    Y = np.dot(X, W) + 0.5 * np.power(0.5 + X[:, 1], 2.0) * np.sin(4.0 * X[:, 1])\n",
    "    Y += sigma_obs * onp.random.randn(N)\n",
    "    Y = Y[:, onp.newaxis]\n",
    "    Y -= np.mean(Y)\n",
    "    Y /= np.std(Y)\n",
    "\n",
    "    assert X.shape == (N, D_X)\n",
    "    assert Y.shape == (N, D_Y)\n",
    "\n",
    "    X_test = np.linspace(-1.3, 1.3, N_test)\n",
    "    X_test = np.power(X_test[:, onp.newaxis], np.arange(D_X))\n",
    "\n",
    "    return X, Y, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    jax_config.update('jax_platform_name', args.device)\n",
    "    N, D_X, D_H = args.num_data, 3, args.num_hidden\n",
    "    X, Y, X_test = get_data(N=N, D_X=D_X)\n",
    "\n",
    "    # do inference\n",
    "    rng, rng_predict = random.split(random.PRNGKey(0))\n",
    "    samples = run_inference(model, args, rng, X, Y, D_H)\n",
    "\n",
    "    # predict Y_test at inputs X_test\n",
    "    vmap_args = (samples, random.split(rng_predict, args.num_samples))\n",
    "    predictions = vmap(lambda samples, rng: predict(model, rng, samples, X_test, D_H))(*vmap_args)\n",
    "    predictions = predictions[..., 0]\n",
    "\n",
    "    # compute mean prediction and confidence interval around median\n",
    "    mean_prediction = np.mean(predictions, axis=0)\n",
    "    percentiles = onp.percentile(predictions, [5.0, 95.0], axis=0)\n",
    "\n",
    "    # make plots\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "    # plot training data\n",
    "    ax.plot(X[:, 1], Y[:, 0], 'kx')\n",
    "    # plot 90% confidence level of predictions\n",
    "    ax.fill_between(X_test[:, 1], percentiles[0, :], percentiles[1, :], color='lightblue')\n",
    "    # plot mean prediction\n",
    "    ax.plot(X_test[:, 1], mean_prediction, 'blue', ls='solid', lw=2.0)\n",
    "    ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 90% CI\")\n",
    "\n",
    "    plt.savefig('bnn_plot.pdf')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Bayesian neural network example\")\n",
    "    parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=2000, type=int)\n",
    "    parser.add_argument(\"--num-warmup\", nargs='?', default=1000, type=int)\n",
    "    parser.add_argument(\"--num-data\", nargs='?', default=100, type=int)\n",
    "    parser.add_argument(\"--num-hidden\", nargs='?', default=5, type=int)\n",
    "    parser.add_argument(\"--device\", default='cpu', type=str, help='use \"cpu\" or \"gpu\".')\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
