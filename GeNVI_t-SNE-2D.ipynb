{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from livelossplot import PlotLosses\n",
    "from Inference.GeNVI_method import GeNVariationalInference, GeNetEns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from Experiments.foong import Setup\n",
    "#setup=Setup(device, layerwidth=50)\n",
    "\n",
    "from Experiments.boston import Setup\n",
    "setup=Setup(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target density #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logtarget= setup.logposterior\n",
    "param_count=setup.param_count\n",
    "model=setup._model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE-Variational Distribution #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activation = nn.Tanh()#nn.ReLU()\n",
    "init_b = .001\n",
    "\n",
    "GeN = GeNetEns(1, 2, 50, param_count, activation, 0.2, init_b, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Tools import logmvn01pdf\n",
    "\n",
    "x=torch.arange(-3.,3., 0.05).to(device)\n",
    "\n",
    "grid_X,grid_Y=torch.meshgrid(x, x)\n",
    "colors=torch.Tensor(grid_X.shape)\n",
    "\n",
    "for i in range(colors.shape[0]):\n",
    "    for j in range(colors.shape[1]):\n",
    "        xy=torch.Tensor([grid_X[i,j],grid_Y[i,j]]).unsqueeze(0).to(device)\n",
    "        colors[i,j]=logmvn01pdf(xy,device).exp().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise=(grid_X.cpu().numpy(),grid_Y.cpu().numpy(),colors.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 120])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "cmap = cm.viridis\n",
    "\n",
    "plt.pcolormesh(noise[0],noise[1],noise[2],cmap=cmap)\n",
    "plt.axis('scaled')\n",
    "plt.title('Gaussian noise')\n",
    "colors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "def makePlot(self, GeN,noise, alpha=0.05 ,device=device):\n",
    "    def get_linewidth(linewidth, axis):\n",
    "        fig = axis.get_figure()\n",
    "        ppi = 72  # matplolib points per inches\n",
    "        length = fig.bbox_inches.height * axis.get_position().height\n",
    "        value_range = np.diff(axis.get_ylim())[0]\n",
    "        return linewidth * ppi * length / value_range\n",
    "#    nb_samples_plot=theta.shape[0]\n",
    "    x_lin = torch.linspace(-2.0, 2.0).unsqueeze(1)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 10)\n",
    "    plt.xlim(-2, 2) \n",
    "    plt.ylim(-4, 6)\n",
    "    plt.grid(True, which='major', linewidth=0.5)\n",
    "    my_lw=get_linewidth(0.2,ax)\n",
    "#    alpha = (.9 / torch.tensor(float(nb_samples_plot)).sqrt()).clamp(0.05, 1.)\n",
    "    \n",
    "    colors=noise[2]\n",
    "    \n",
    "    norm=Normalize(vmin=colors.min(), vmax=colors.max())\n",
    "    m = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    \n",
    "    for i in range(colors.shape[0]):\n",
    "        for j in range(colors.shape[1]):\n",
    "            xy_noise=torch.Tensor([noise[0][i,j],noise[1][i,j]]).unsqueeze(0).to(device)\n",
    "            theta=GeN.components[0].hnet(xy_noise).detach()\n",
    "            y_pred = self._normalized_prediction(x_lin, theta, device)\n",
    "            color=m.to_rgba(colors[i,j])\n",
    "            plt.plot(x_lin.detach().cpu().numpy(), y_pred.squeeze(0).detach().cpu().numpy(), alpha=alpha, linewidth=1.0, color=color,zorder=3)\n",
    "    plt.scatter(self._X_train.cpu(), self._y_train.cpu(), marker='.',color='black',zorder=4)\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup.plot:\n",
    "    fig=makePlot(setup,GeN,noise, alpha=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20000], Loss: 1349493.625, Entropy 93.4504165649414, Learning Rate: 0.01\n",
      "Epoch [1/20000], Loss: 2218543.0, Entropy 80.54517364501953, Learning Rate: 0.01\n",
      "Epoch [2/20000], Loss: 756919.125, Entropy 58.206233978271484, Learning Rate: 0.01\n",
      "Epoch [3/20000], Loss: 442180.5, Entropy 42.06381607055664, Learning Rate: 0.01\n",
      "Epoch [4/20000], Loss: 374216.28125, Entropy 48.28152847290039, Learning Rate: 0.01\n",
      "Epoch [5/20000], Loss: 405307.6875, Entropy 34.075439453125, Learning Rate: 0.01\n",
      "Epoch [6/20000], Loss: 253751.46875, Entropy 19.20648956298828, Learning Rate: 0.01\n",
      "Epoch [7/20000], Loss: 290403.75, Entropy 20.25414276123047, Learning Rate: 0.01\n",
      "Epoch [8/20000], Loss: 224982.265625, Entropy 28.900829315185547, Learning Rate: 0.01\n",
      "Epoch [9/20000], Loss: 146088.75, Entropy 11.689711570739746, Learning Rate: 0.01\n",
      "Epoch [10/20000], Loss: 126942.5234375, Entropy 4.480998516082764, Learning Rate: 0.01\n",
      "Epoch [11/20000], Loss: 106372.65625, Entropy 3.9702978134155273, Learning Rate: 0.01\n",
      "Epoch [12/20000], Loss: 99632.65625, Entropy -7.018318176269531, Learning Rate: 0.01\n",
      "Epoch [13/20000], Loss: 101954.765625, Entropy -1.1388874053955078, Learning Rate: 0.01\n",
      "Epoch [14/20000], Loss: 65199.14453125, Entropy -16.17400550842285, Learning Rate: 0.01\n",
      "Epoch [15/20000], Loss: 49936.87109375, Entropy -28.954357147216797, Learning Rate: 0.01\n",
      "Epoch [16/20000], Loss: 39250.19921875, Entropy -29.189924240112305, Learning Rate: 0.01\n",
      "Epoch [17/20000], Loss: 38516.546875, Entropy -38.387962341308594, Learning Rate: 0.01\n",
      "Epoch [18/20000], Loss: 55695.69140625, Entropy -23.80935287475586, Learning Rate: 0.01\n",
      "Epoch [19/20000], Loss: 49370.58203125, Entropy -34.030635833740234, Learning Rate: 0.01\n",
      "Epoch [20/20000], Loss: 39304.66796875, Entropy -25.551340103149414, Learning Rate: 0.01\n",
      "Epoch [21/20000], Loss: 30769.41015625, Entropy -37.46930694580078, Learning Rate: 0.01\n",
      "Epoch [22/20000], Loss: 31267.32421875, Entropy -39.75271987915039, Learning Rate: 0.01\n",
      "Epoch [23/20000], Loss: 25749.40625, Entropy -48.04483413696289, Learning Rate: 0.01\n",
      "Epoch [24/20000], Loss: 26886.341796875, Entropy -28.181365966796875, Learning Rate: 0.01\n",
      "Epoch [25/20000], Loss: 18867.361328125, Entropy -47.48960876464844, Learning Rate: 0.01\n",
      "Epoch [26/20000], Loss: 22500.33203125, Entropy -27.810834884643555, Learning Rate: 0.01\n",
      "Epoch [27/20000], Loss: 22908.9765625, Entropy -65.26138305664062, Learning Rate: 0.01\n",
      "Epoch [28/20000], Loss: 19024.16796875, Entropy -48.60622787475586, Learning Rate: 0.01\n",
      "Epoch [29/20000], Loss: 21348.724609375, Entropy -48.44424819946289, Learning Rate: 0.01\n",
      "Epoch [30/20000], Loss: 24650.576171875, Entropy -58.29674530029297, Learning Rate: 0.01\n",
      "Epoch [31/20000], Loss: 13271.33984375, Entropy -43.69692611694336, Learning Rate: 0.01\n",
      "Epoch [32/20000], Loss: 14768.505859375, Entropy -56.88563537597656, Learning Rate: 0.01\n",
      "Epoch [33/20000], Loss: 12898.478515625, Entropy -39.1529541015625, Learning Rate: 0.01\n",
      "Epoch [34/20000], Loss: 12344.7490234375, Entropy -68.93885803222656, Learning Rate: 0.01\n",
      "Epoch [35/20000], Loss: 11907.84375, Entropy -62.21794891357422, Learning Rate: 0.01\n",
      "Epoch [36/20000], Loss: 9550.3662109375, Entropy -66.59859466552734, Learning Rate: 0.01\n",
      "Epoch [37/20000], Loss: 9875.123046875, Entropy -83.47468566894531, Learning Rate: 0.01\n",
      "Epoch [38/20000], Loss: 11380.873046875, Entropy -68.67166137695312, Learning Rate: 0.01\n",
      "Epoch [39/20000], Loss: 10212.7529296875, Entropy -76.24703979492188, Learning Rate: 0.01\n",
      "Epoch [40/20000], Loss: 9300.478515625, Entropy -62.974578857421875, Learning Rate: 0.01\n",
      "Epoch [41/20000], Loss: 7541.75537109375, Entropy -77.12688446044922, Learning Rate: 0.01\n",
      "Epoch [42/20000], Loss: 8536.8037109375, Entropy -87.20327758789062, Learning Rate: 0.01\n",
      "Epoch [43/20000], Loss: 6507.54833984375, Entropy -77.96772766113281, Learning Rate: 0.01\n",
      "Epoch [44/20000], Loss: 6894.8525390625, Entropy -92.0355453491211, Learning Rate: 0.01\n",
      "Epoch [45/20000], Loss: 6838.53759765625, Entropy -75.3517837524414, Learning Rate: 0.01\n",
      "Epoch [46/20000], Loss: 6225.21044921875, Entropy -86.91083526611328, Learning Rate: 0.01\n",
      "Epoch [47/20000], Loss: 6505.52880859375, Entropy -76.7609634399414, Learning Rate: 0.01\n",
      "Epoch [48/20000], Loss: 6141.03759765625, Entropy -73.16817474365234, Learning Rate: 0.01\n",
      "Epoch [49/20000], Loss: 5662.01904296875, Entropy -78.02079772949219, Learning Rate: 0.01\n",
      "Epoch [50/20000], Loss: 5270.70703125, Entropy -82.60818481445312, Learning Rate: 0.01\n",
      "Epoch [51/20000], Loss: 5378.07666015625, Entropy -79.17414093017578, Learning Rate: 0.01\n",
      "Epoch [52/20000], Loss: 5899.0625, Entropy -78.690673828125, Learning Rate: 0.01\n",
      "Epoch [53/20000], Loss: 6745.88720703125, Entropy -75.82383728027344, Learning Rate: 0.01\n",
      "Epoch [54/20000], Loss: 4269.13916015625, Entropy -84.74406433105469, Learning Rate: 0.01\n",
      "Epoch [55/20000], Loss: 5013.732421875, Entropy -90.8070297241211, Learning Rate: 0.01\n",
      "Epoch [56/20000], Loss: 4976.880859375, Entropy -88.06380462646484, Learning Rate: 0.01\n",
      "Epoch [57/20000], Loss: 4280.10791015625, Entropy -93.48560333251953, Learning Rate: 0.01\n",
      "Epoch [58/20000], Loss: 4363.09814453125, Entropy -99.4764404296875, Learning Rate: 0.01\n",
      "Epoch [59/20000], Loss: 4291.59765625, Entropy -85.48570251464844, Learning Rate: 0.01\n",
      "Epoch [60/20000], Loss: 4863.18359375, Entropy -96.10281372070312, Learning Rate: 0.01\n",
      "Epoch [61/20000], Loss: 4261.98828125, Entropy -89.89231872558594, Learning Rate: 0.01\n",
      "Epoch [62/20000], Loss: 4125.96728515625, Entropy -78.42218780517578, Learning Rate: 0.01\n",
      "Epoch [63/20000], Loss: 4088.254638671875, Entropy -82.25843811035156, Learning Rate: 0.01\n",
      "Epoch [64/20000], Loss: 3898.701904296875, Entropy -87.50706481933594, Learning Rate: 0.01\n",
      "Epoch [65/20000], Loss: 4072.4921875, Entropy -101.44048309326172, Learning Rate: 0.01\n",
      "Epoch [66/20000], Loss: 4372.47216796875, Entropy -97.70236206054688, Learning Rate: 0.01\n",
      "Epoch [67/20000], Loss: 3422.43212890625, Entropy -105.91131591796875, Learning Rate: 0.01\n",
      "Epoch [68/20000], Loss: 3455.01708984375, Entropy -92.25831604003906, Learning Rate: 0.01\n",
      "Epoch [69/20000], Loss: 3326.564697265625, Entropy -100.87673950195312, Learning Rate: 0.01\n",
      "Epoch [70/20000], Loss: 4223.1416015625, Entropy -106.50595092773438, Learning Rate: 0.01\n",
      "Epoch [71/20000], Loss: 3984.797607421875, Entropy -111.69937896728516, Learning Rate: 0.01\n",
      "Epoch [72/20000], Loss: 3368.73193359375, Entropy -89.82758331298828, Learning Rate: 0.01\n",
      "Epoch [73/20000], Loss: 3187.30078125, Entropy -94.31707763671875, Learning Rate: 0.01\n",
      "Epoch [74/20000], Loss: 3297.27783203125, Entropy -103.32345581054688, Learning Rate: 0.01\n",
      "Epoch [75/20000], Loss: 3880.2177734375, Entropy -84.43621063232422, Learning Rate: 0.01\n",
      "Epoch [76/20000], Loss: 3299.396240234375, Entropy -97.9945297241211, Learning Rate: 0.01\n",
      "Epoch [77/20000], Loss: 3082.21875, Entropy -103.99276733398438, Learning Rate: 0.01\n",
      "Epoch [78/20000], Loss: 3383.5390625, Entropy -104.2420654296875, Learning Rate: 0.01\n",
      "Epoch [79/20000], Loss: 3724.819580078125, Entropy -82.72140502929688, Learning Rate: 0.01\n",
      "Epoch [80/20000], Loss: 3498.783203125, Entropy -90.09266662597656, Learning Rate: 0.01\n",
      "Epoch [81/20000], Loss: 3630.44287109375, Entropy -102.91038513183594, Learning Rate: 0.01\n",
      "Epoch [82/20000], Loss: 3200.386474609375, Entropy -101.63762664794922, Learning Rate: 0.01\n",
      "Epoch [83/20000], Loss: 4164.26513671875, Entropy -103.29177856445312, Learning Rate: 0.01\n",
      "Epoch [84/20000], Loss: 3020.353759765625, Entropy -83.11027526855469, Learning Rate: 0.01\n",
      "Epoch [85/20000], Loss: 3791.440673828125, Entropy -69.8089370727539, Learning Rate: 0.01\n",
      "Epoch [86/20000], Loss: 3060.9296875, Entropy -93.32308197021484, Learning Rate: 0.01\n",
      "Epoch [87/20000], Loss: 3390.8046875, Entropy -96.26726531982422, Learning Rate: 0.01\n",
      "Epoch [88/20000], Loss: 3021.544921875, Entropy -90.8287582397461, Learning Rate: 0.01\n",
      "Epoch [89/20000], Loss: 2736.013916015625, Entropy -93.44490051269531, Learning Rate: 0.01\n",
      "Epoch [90/20000], Loss: 3068.91796875, Entropy -110.67298889160156, Learning Rate: 0.01\n",
      "Epoch [91/20000], Loss: 3112.114501953125, Entropy -100.5463638305664, Learning Rate: 0.01\n",
      "Epoch [92/20000], Loss: 3065.124755859375, Entropy -102.6181411743164, Learning Rate: 0.01\n",
      "Epoch [93/20000], Loss: 3155.17724609375, Entropy -106.86241912841797, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/20000], Loss: 2796.086669921875, Entropy -92.37513732910156, Learning Rate: 0.01\n",
      "Epoch [95/20000], Loss: 3129.62451171875, Entropy -93.6249771118164, Learning Rate: 0.01\n",
      "Epoch [96/20000], Loss: 2820.513671875, Entropy -93.10613250732422, Learning Rate: 0.01\n",
      "Epoch [97/20000], Loss: 2945.91748046875, Entropy -95.18148040771484, Learning Rate: 0.01\n",
      "Epoch [98/20000], Loss: 2658.515625, Entropy -97.22900390625, Learning Rate: 0.01\n",
      "Epoch [99/20000], Loss: 2546.417724609375, Entropy -103.56561279296875, Learning Rate: 0.01\n",
      "Epoch [100/20000], Loss: 2900.204833984375, Entropy -100.82254028320312, Learning Rate: 0.01\n",
      "Epoch [101/20000], Loss: 2510.037841796875, Entropy -97.1315689086914, Learning Rate: 0.01\n",
      "Epoch [102/20000], Loss: 2860.396728515625, Entropy -84.62399291992188, Learning Rate: 0.01\n",
      "Epoch [103/20000], Loss: 2930.959716796875, Entropy -97.21400451660156, Learning Rate: 0.01\n",
      "Epoch [104/20000], Loss: 2640.25048828125, Entropy -109.90084838867188, Learning Rate: 0.01\n",
      "Epoch [105/20000], Loss: 3210.167724609375, Entropy -109.86275482177734, Learning Rate: 0.01\n",
      "Epoch [106/20000], Loss: 2534.09326171875, Entropy -99.74431610107422, Learning Rate: 0.01\n",
      "Epoch [107/20000], Loss: 2594.419677734375, Entropy -104.21170043945312, Learning Rate: 0.01\n",
      "Epoch [108/20000], Loss: 2605.588623046875, Entropy -102.22474670410156, Learning Rate: 0.01\n",
      "Epoch [109/20000], Loss: 2631.310302734375, Entropy -98.56349182128906, Learning Rate: 0.01\n",
      "Epoch [110/20000], Loss: 2690.576171875, Entropy -123.29729461669922, Learning Rate: 0.01\n",
      "Epoch [111/20000], Loss: 2486.72119140625, Entropy -91.16654968261719, Learning Rate: 0.01\n",
      "Epoch [112/20000], Loss: 2554.43798828125, Entropy -93.66626739501953, Learning Rate: 0.01\n",
      "Epoch [113/20000], Loss: 2882.871826171875, Entropy -108.1920394897461, Learning Rate: 0.01\n",
      "Epoch [114/20000], Loss: 3036.947509765625, Entropy -103.79820251464844, Learning Rate: 0.01\n",
      "Epoch [115/20000], Loss: 2431.9951171875, Entropy -104.2352294921875, Learning Rate: 0.01\n",
      "Epoch [116/20000], Loss: 2356.7880859375, Entropy -110.3675765991211, Learning Rate: 0.01\n",
      "Epoch [117/20000], Loss: 2988.052734375, Entropy -112.20970153808594, Learning Rate: 0.01\n",
      "Epoch [118/20000], Loss: 2758.396484375, Entropy -87.15406036376953, Learning Rate: 0.01\n",
      "Epoch [119/20000], Loss: 2760.691162109375, Entropy -102.3216323852539, Learning Rate: 0.01\n",
      "Epoch [120/20000], Loss: 3372.65673828125, Entropy -101.9474105834961, Learning Rate: 0.01\n",
      "Epoch [121/20000], Loss: 2644.281005859375, Entropy -96.01190948486328, Learning Rate: 0.01\n",
      "Epoch [122/20000], Loss: 2499.149658203125, Entropy -97.78251647949219, Learning Rate: 0.01\n",
      "Epoch [123/20000], Loss: 2234.23779296875, Entropy -109.9681396484375, Learning Rate: 0.01\n",
      "Epoch [124/20000], Loss: 2756.885498046875, Entropy -105.9579849243164, Learning Rate: 0.01\n",
      "Epoch [125/20000], Loss: 2314.21337890625, Entropy -96.65657806396484, Learning Rate: 0.01\n",
      "Epoch [126/20000], Loss: 2985.70751953125, Entropy -97.36011505126953, Learning Rate: 0.01\n",
      "Epoch [127/20000], Loss: 2438.2666015625, Entropy -116.1209945678711, Learning Rate: 0.01\n",
      "Epoch [128/20000], Loss: 2433.041259765625, Entropy -87.62706756591797, Learning Rate: 0.01\n",
      "Epoch [129/20000], Loss: 2623.100341796875, Entropy -87.17033386230469, Learning Rate: 0.01\n",
      "Epoch [130/20000], Loss: 2483.93798828125, Entropy -96.09102630615234, Learning Rate: 0.01\n",
      "Epoch [131/20000], Loss: 2541.68017578125, Entropy -99.34171295166016, Learning Rate: 0.01\n",
      "Epoch [132/20000], Loss: 2667.67041015625, Entropy -120.42845153808594, Learning Rate: 0.01\n",
      "Epoch [133/20000], Loss: 2791.12939453125, Entropy -86.18241882324219, Learning Rate: 0.01\n",
      "Epoch [134/20000], Loss: 2369.91259765625, Entropy -117.32402038574219, Learning Rate: 0.01\n",
      "Epoch [135/20000], Loss: 2251.165283203125, Entropy -118.78652954101562, Learning Rate: 0.01\n",
      "Epoch [136/20000], Loss: 2470.356689453125, Entropy -92.72956085205078, Learning Rate: 0.01\n",
      "Epoch [137/20000], Loss: 2730.337158203125, Entropy -99.3541030883789, Learning Rate: 0.01\n",
      "Epoch [138/20000], Loss: 2307.89208984375, Entropy -94.83534240722656, Learning Rate: 0.01\n",
      "Epoch [139/20000], Loss: 2406.85107421875, Entropy -111.22905731201172, Learning Rate: 0.01\n",
      "Epoch [140/20000], Loss: 2810.197021484375, Entropy -108.3905258178711, Learning Rate: 0.01\n",
      "Epoch [141/20000], Loss: 2447.521240234375, Entropy -101.6488265991211, Learning Rate: 0.01\n",
      "Epoch [142/20000], Loss: 2320.5654296875, Entropy -105.358642578125, Learning Rate: 0.01\n",
      "Epoch [143/20000], Loss: 2440.9384765625, Entropy -92.22396087646484, Learning Rate: 0.01\n",
      "Epoch [144/20000], Loss: 2374.02099609375, Entropy -90.15437316894531, Learning Rate: 0.01\n",
      "Epoch [145/20000], Loss: 2221.723388671875, Entropy -103.36752319335938, Learning Rate: 0.01\n",
      "Epoch [146/20000], Loss: 2411.586669921875, Entropy -106.73686981201172, Learning Rate: 0.01\n",
      "Epoch [147/20000], Loss: 2697.61181640625, Entropy -116.3759536743164, Learning Rate: 0.01\n",
      "Epoch [148/20000], Loss: 2056.13134765625, Entropy -96.70362091064453, Learning Rate: 0.01\n",
      "Epoch [149/20000], Loss: 2063.124267578125, Entropy -105.16675567626953, Learning Rate: 0.01\n",
      "Epoch [150/20000], Loss: 2446.77734375, Entropy -115.24818420410156, Learning Rate: 0.01\n",
      "Epoch [151/20000], Loss: 2322.876708984375, Entropy -112.22489929199219, Learning Rate: 0.01\n",
      "Epoch [152/20000], Loss: 2381.395263671875, Entropy -97.4444808959961, Learning Rate: 0.01\n",
      "Epoch [153/20000], Loss: 2515.997314453125, Entropy -105.09095764160156, Learning Rate: 0.01\n",
      "Epoch [154/20000], Loss: 2380.645751953125, Entropy -112.1732406616211, Learning Rate: 0.01\n",
      "Epoch [155/20000], Loss: 2307.20751953125, Entropy -100.6000747680664, Learning Rate: 0.01\n",
      "Epoch [156/20000], Loss: 2196.518310546875, Entropy -107.30402374267578, Learning Rate: 0.01\n",
      "Epoch [157/20000], Loss: 1946.839599609375, Entropy -87.15206146240234, Learning Rate: 0.01\n",
      "Epoch [158/20000], Loss: 2094.87841796875, Entropy -101.99246215820312, Learning Rate: 0.01\n",
      "Epoch [159/20000], Loss: 2475.11181640625, Entropy -108.40483856201172, Learning Rate: 0.01\n",
      "Epoch [160/20000], Loss: 2316.97412109375, Entropy -109.55113983154297, Learning Rate: 0.01\n",
      "Epoch [161/20000], Loss: 2504.617919921875, Entropy -104.27748107910156, Learning Rate: 0.01\n",
      "Epoch [162/20000], Loss: 2112.15771484375, Entropy -110.57872772216797, Learning Rate: 0.01\n",
      "Epoch [163/20000], Loss: 1999.154541015625, Entropy -117.60017395019531, Learning Rate: 0.01\n",
      "Epoch [164/20000], Loss: 2170.96875, Entropy -95.7567138671875, Learning Rate: 0.01\n",
      "Epoch [165/20000], Loss: 2160.813232421875, Entropy -108.96798706054688, Learning Rate: 0.01\n",
      "Epoch [166/20000], Loss: 2048.14990234375, Entropy -111.97737121582031, Learning Rate: 0.01\n",
      "Epoch [167/20000], Loss: 2241.782958984375, Entropy -105.09330749511719, Learning Rate: 0.01\n",
      "Epoch [168/20000], Loss: 2192.43212890625, Entropy -111.18318939208984, Learning Rate: 0.01\n",
      "Epoch [169/20000], Loss: 2123.803955078125, Entropy -110.40193939208984, Learning Rate: 0.01\n",
      "Epoch [170/20000], Loss: 2132.91650390625, Entropy -97.3089599609375, Learning Rate: 0.01\n",
      "Epoch [171/20000], Loss: 2275.31103515625, Entropy -92.2772445678711, Learning Rate: 0.01\n",
      "Epoch [172/20000], Loss: 2452.517578125, Entropy -118.58855438232422, Learning Rate: 0.01\n",
      "Epoch [173/20000], Loss: 2466.7353515625, Entropy -111.6244888305664, Learning Rate: 0.01\n",
      "Epoch [174/20000], Loss: 2086.47021484375, Entropy -109.369384765625, Learning Rate: 0.01\n",
      "Epoch [175/20000], Loss: 2020.2271728515625, Entropy -108.27002716064453, Learning Rate: 0.01\n",
      "Epoch [176/20000], Loss: 1947.0732421875, Entropy -105.84642028808594, Learning Rate: 0.01\n",
      "Epoch [177/20000], Loss: 2173.02099609375, Entropy -106.40987396240234, Learning Rate: 0.01\n",
      "Epoch [178/20000], Loss: 1973.3448486328125, Entropy -113.34264373779297, Learning Rate: 0.01\n",
      "Epoch [179/20000], Loss: 2271.95361328125, Entropy -122.2737045288086, Learning Rate: 0.01\n",
      "Epoch [180/20000], Loss: 2168.18798828125, Entropy -106.2843246459961, Learning Rate: 0.01\n",
      "Epoch [181/20000], Loss: 2099.286865234375, Entropy -102.08605194091797, Learning Rate: 0.01\n",
      "Epoch [182/20000], Loss: 2124.081787109375, Entropy -109.38807678222656, Learning Rate: 0.01\n",
      "Epoch [183/20000], Loss: 2375.89892578125, Entropy -107.71944427490234, Learning Rate: 0.01\n",
      "Epoch [184/20000], Loss: 2177.938720703125, Entropy -105.8852310180664, Learning Rate: 0.01\n",
      "Epoch [185/20000], Loss: 1968.084716796875, Entropy -109.7865219116211, Learning Rate: 0.01\n",
      "Epoch [186/20000], Loss: 2019.927490234375, Entropy -98.24752807617188, Learning Rate: 0.01\n",
      "Epoch [187/20000], Loss: 2448.02490234375, Entropy -116.0664291381836, Learning Rate: 0.01\n",
      "Epoch [188/20000], Loss: 1839.4183349609375, Entropy -104.3624496459961, Learning Rate: 0.01\n",
      "Epoch [189/20000], Loss: 1893.710205078125, Entropy -108.59966278076172, Learning Rate: 0.01\n",
      "Epoch [190/20000], Loss: 2477.365234375, Entropy -93.18750762939453, Learning Rate: 0.01\n",
      "Epoch [191/20000], Loss: 2132.947509765625, Entropy -122.1477279663086, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [192/20000], Loss: 1889.0433349609375, Entropy -119.8416976928711, Learning Rate: 0.01\n",
      "Epoch [193/20000], Loss: 1985.5257568359375, Entropy -121.98081970214844, Learning Rate: 0.01\n",
      "Epoch [194/20000], Loss: 1980.932373046875, Entropy -98.73074340820312, Learning Rate: 0.01\n",
      "Epoch [195/20000], Loss: 2036.5128173828125, Entropy -113.2931900024414, Learning Rate: 0.01\n",
      "Epoch [196/20000], Loss: 1931.4749755859375, Entropy -107.81511688232422, Learning Rate: 0.01\n",
      "Epoch [197/20000], Loss: 2053.625732421875, Entropy -118.51766967773438, Learning Rate: 0.01\n",
      "Epoch [198/20000], Loss: 2057.249267578125, Entropy -109.62151336669922, Learning Rate: 0.01\n",
      "Epoch [199/20000], Loss: 2096.83251953125, Entropy -102.22559356689453, Learning Rate: 0.01\n",
      "Epoch [200/20000], Loss: 2051.87353515625, Entropy -119.18095397949219, Learning Rate: 0.01\n",
      "Epoch [201/20000], Loss: 1927.1358642578125, Entropy -97.14370727539062, Learning Rate: 0.01\n",
      "Epoch [202/20000], Loss: 2048.365966796875, Entropy -114.66425323486328, Learning Rate: 0.01\n",
      "Epoch [203/20000], Loss: 1931.6495361328125, Entropy -125.25778198242188, Learning Rate: 0.01\n",
      "Epoch [204/20000], Loss: 1977.927734375, Entropy -110.52703094482422, Learning Rate: 0.01\n",
      "Epoch [205/20000], Loss: 1872.90625, Entropy -109.34790802001953, Learning Rate: 0.01\n",
      "Epoch [206/20000], Loss: 2194.4775390625, Entropy -109.08699035644531, Learning Rate: 0.01\n",
      "Epoch [207/20000], Loss: 2212.490234375, Entropy -102.4864501953125, Learning Rate: 0.01\n",
      "Epoch [208/20000], Loss: 1922.848876953125, Entropy -128.06199645996094, Learning Rate: 0.01\n",
      "Epoch [209/20000], Loss: 2032.881103515625, Entropy -107.6766586303711, Learning Rate: 0.01\n",
      "Epoch [210/20000], Loss: 2008.99267578125, Entropy -107.97897338867188, Learning Rate: 0.01\n",
      "Epoch [211/20000], Loss: 1842.7978515625, Entropy -109.88136291503906, Learning Rate: 0.01\n",
      "Epoch [212/20000], Loss: 2116.046630859375, Entropy -108.3105239868164, Learning Rate: 0.01\n",
      "Epoch [213/20000], Loss: 1843.583740234375, Entropy -105.94378662109375, Learning Rate: 0.01\n",
      "Epoch [214/20000], Loss: 1806.182373046875, Entropy -115.43507385253906, Learning Rate: 0.01\n",
      "Epoch [215/20000], Loss: 1886.79052734375, Entropy -94.6540298461914, Learning Rate: 0.01\n",
      "Epoch [216/20000], Loss: 1887.1114501953125, Entropy -103.53181457519531, Learning Rate: 0.01\n",
      "Epoch [217/20000], Loss: 2107.933837890625, Entropy -122.68421936035156, Learning Rate: 0.01\n",
      "Epoch [218/20000], Loss: 1829.6826171875, Entropy -111.65302276611328, Learning Rate: 0.01\n",
      "Epoch [219/20000], Loss: 1925.4560546875, Entropy -108.9828109741211, Learning Rate: 0.01\n",
      "Epoch [220/20000], Loss: 1897.99072265625, Entropy -108.9203109741211, Learning Rate: 0.01\n",
      "Epoch [221/20000], Loss: 1971.6741943359375, Entropy -113.06362915039062, Learning Rate: 0.01\n",
      "Epoch [222/20000], Loss: 1856.899169921875, Entropy -103.59608459472656, Learning Rate: 0.01\n",
      "Epoch [223/20000], Loss: 1813.2244873046875, Entropy -99.30198669433594, Learning Rate: 0.01\n",
      "Epoch [224/20000], Loss: 2039.62939453125, Entropy -103.47806549072266, Learning Rate: 0.01\n",
      "Epoch [225/20000], Loss: 1752.076904296875, Entropy -101.37193298339844, Learning Rate: 0.01\n",
      "Epoch [226/20000], Loss: 2480.18798828125, Entropy -105.35736083984375, Learning Rate: 0.01\n",
      "Epoch [227/20000], Loss: 2062.79150390625, Entropy -106.51954650878906, Learning Rate: 0.01\n",
      "Epoch [228/20000], Loss: 1763.79833984375, Entropy -112.67276000976562, Learning Rate: 0.01\n",
      "Epoch [229/20000], Loss: 1834.79345703125, Entropy -93.37459564208984, Learning Rate: 0.01\n",
      "Epoch [230/20000], Loss: 1803.94140625, Entropy -115.83401489257812, Learning Rate: 0.01\n",
      "Epoch [231/20000], Loss: 1939.6463623046875, Entropy -95.35401916503906, Learning Rate: 0.01\n",
      "Epoch [232/20000], Loss: 1860.5140380859375, Entropy -111.37871551513672, Learning Rate: 0.01\n",
      "Epoch [233/20000], Loss: 1716.5784912109375, Entropy -108.30789184570312, Learning Rate: 0.01\n",
      "Epoch [234/20000], Loss: 1798.833251953125, Entropy -115.74825286865234, Learning Rate: 0.01\n",
      "Epoch [235/20000], Loss: 2040.1300048828125, Entropy -122.37539672851562, Learning Rate: 0.01\n",
      "Epoch [236/20000], Loss: 1699.183349609375, Entropy -105.0013427734375, Learning Rate: 0.01\n",
      "Epoch [237/20000], Loss: 1713.367919921875, Entropy -88.68468475341797, Learning Rate: 0.01\n",
      "Epoch [238/20000], Loss: 1702.9434814453125, Entropy -101.67027282714844, Learning Rate: 0.01\n",
      "Epoch [239/20000], Loss: 2082.28564453125, Entropy -118.40189361572266, Learning Rate: 0.01\n",
      "Epoch [240/20000], Loss: 1942.6964111328125, Entropy -116.94733428955078, Learning Rate: 0.01\n",
      "Epoch [241/20000], Loss: 1770.5758056640625, Entropy -95.32105255126953, Learning Rate: 0.01\n",
      "Epoch [242/20000], Loss: 2008.15673828125, Entropy -113.0175552368164, Learning Rate: 0.01\n",
      "Epoch [243/20000], Loss: 1824.4161376953125, Entropy -107.13825225830078, Learning Rate: 0.01\n",
      "Epoch [244/20000], Loss: 1891.931396484375, Entropy -105.49246978759766, Learning Rate: 0.01\n",
      "Epoch [245/20000], Loss: 1750.358642578125, Entropy -111.38851928710938, Learning Rate: 0.01\n",
      "Epoch [246/20000], Loss: 1774.685302734375, Entropy -108.24749755859375, Learning Rate: 0.01\n",
      "Epoch [247/20000], Loss: 1773.595703125, Entropy -93.68729400634766, Learning Rate: 0.01\n",
      "Epoch [248/20000], Loss: 1765.3875732421875, Entropy -115.55815887451172, Learning Rate: 0.01\n",
      "Epoch [249/20000], Loss: 1683.0484619140625, Entropy -118.79122924804688, Learning Rate: 0.01\n",
      "Epoch [250/20000], Loss: 1926.408447265625, Entropy -103.42070770263672, Learning Rate: 0.01\n",
      "Epoch [251/20000], Loss: 1769.1533203125, Entropy -114.87238311767578, Learning Rate: 0.01\n",
      "Epoch [252/20000], Loss: 1849.222412109375, Entropy -106.66031646728516, Learning Rate: 0.01\n",
      "Epoch [253/20000], Loss: 1892.375732421875, Entropy -115.44828033447266, Learning Rate: 0.01\n",
      "Epoch [254/20000], Loss: 1676.1729736328125, Entropy -103.70341491699219, Learning Rate: 0.01\n",
      "Epoch [255/20000], Loss: 1938.2657470703125, Entropy -96.85710906982422, Learning Rate: 0.01\n",
      "Epoch [256/20000], Loss: 1898.3734130859375, Entropy -102.83740234375, Learning Rate: 0.01\n",
      "Epoch [257/20000], Loss: 1819.6468505859375, Entropy -106.32083892822266, Learning Rate: 0.01\n",
      "Epoch [258/20000], Loss: 1720.4073486328125, Entropy -111.40792846679688, Learning Rate: 0.01\n",
      "Epoch [259/20000], Loss: 2055.040283203125, Entropy -114.69116973876953, Learning Rate: 0.01\n",
      "Epoch [260/20000], Loss: 1967.2100830078125, Entropy -101.69267272949219, Learning Rate: 0.01\n",
      "Epoch [261/20000], Loss: 1762.4322509765625, Entropy -108.64813995361328, Learning Rate: 0.01\n",
      "Epoch [262/20000], Loss: 1809.631103515625, Entropy -95.56919860839844, Learning Rate: 0.01\n",
      "Epoch [263/20000], Loss: 1655.5250244140625, Entropy -101.28824615478516, Learning Rate: 0.01\n",
      "Epoch [264/20000], Loss: 1763.935791015625, Entropy -105.03386688232422, Learning Rate: 0.01\n",
      "Epoch [265/20000], Loss: 1909.63037109375, Entropy -100.63031005859375, Learning Rate: 0.01\n",
      "Epoch [266/20000], Loss: 1869.984619140625, Entropy -83.5328140258789, Learning Rate: 0.01\n",
      "Epoch [267/20000], Loss: 1750.3170166015625, Entropy -123.61880493164062, Learning Rate: 0.01\n",
      "Epoch [268/20000], Loss: 1792.9405517578125, Entropy -107.79307556152344, Learning Rate: 0.01\n",
      "Epoch [269/20000], Loss: 1813.472900390625, Entropy -102.97654724121094, Learning Rate: 0.01\n",
      "Epoch [270/20000], Loss: 1878.96826171875, Entropy -108.24723052978516, Learning Rate: 0.01\n",
      "Epoch [271/20000], Loss: 1706.289794921875, Entropy -99.31854248046875, Learning Rate: 0.01\n",
      "Epoch [272/20000], Loss: 1909.9434814453125, Entropy -98.71073913574219, Learning Rate: 0.01\n",
      "Epoch [273/20000], Loss: 1743.3912353515625, Entropy -104.29288482666016, Learning Rate: 0.01\n",
      "Epoch [274/20000], Loss: 1730.048828125, Entropy -95.40774536132812, Learning Rate: 0.01\n",
      "Epoch [275/20000], Loss: 1881.2122802734375, Entropy -122.18238830566406, Learning Rate: 0.01\n",
      "Epoch [276/20000], Loss: 1760.4078369140625, Entropy -110.76158142089844, Learning Rate: 0.01\n",
      "Epoch [277/20000], Loss: 1825.052001953125, Entropy -115.52767944335938, Learning Rate: 0.01\n",
      "Epoch [278/20000], Loss: 1859.86083984375, Entropy -89.67919921875, Learning Rate: 0.01\n",
      "Epoch [279/20000], Loss: 1803.3870849609375, Entropy -98.08280944824219, Learning Rate: 0.01\n",
      "Epoch [280/20000], Loss: 1742.11572265625, Entropy -98.1951675415039, Learning Rate: 0.01\n",
      "Epoch [281/20000], Loss: 1732.8431396484375, Entropy -107.93235778808594, Learning Rate: 0.01\n",
      "Epoch [282/20000], Loss: 1746.875732421875, Entropy -106.86991882324219, Learning Rate: 0.01\n",
      "Epoch [283/20000], Loss: 1773.373779296875, Entropy -106.97520446777344, Learning Rate: 0.01\n",
      "Epoch [284/20000], Loss: 1692.574951171875, Entropy -102.22295379638672, Learning Rate: 0.01\n",
      "Epoch [285/20000], Loss: 1761.3946533203125, Entropy -101.05428314208984, Learning Rate: 0.01\n",
      "Epoch [286/20000], Loss: 1909.9130859375, Entropy -101.70438385009766, Learning Rate: 0.01\n",
      "Epoch [287/20000], Loss: 1793.3447265625, Entropy -97.83908081054688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [288/20000], Loss: 1740.1978759765625, Entropy -108.75137329101562, Learning Rate: 0.01\n",
      "Epoch [289/20000], Loss: 1695.4173583984375, Entropy -110.72959899902344, Learning Rate: 0.01\n",
      "Epoch [290/20000], Loss: 1743.0050048828125, Entropy -108.34354400634766, Learning Rate: 0.01\n",
      "Epoch [291/20000], Loss: 1664.840576171875, Entropy -93.86959838867188, Learning Rate: 0.01\n",
      "Epoch [292/20000], Loss: 1884.3155517578125, Entropy -96.97105407714844, Learning Rate: 0.01\n",
      "Epoch [293/20000], Loss: 1735.6209716796875, Entropy -106.46949005126953, Learning Rate: 0.01\n",
      "Epoch [294/20000], Loss: 1686.4476318359375, Entropy -93.39058685302734, Learning Rate: 0.01\n",
      "Epoch [295/20000], Loss: 1745.188720703125, Entropy -101.15847778320312, Learning Rate: 0.01\n",
      "Epoch [296/20000], Loss: 1669.563232421875, Entropy -119.54816436767578, Learning Rate: 0.01\n",
      "Epoch [297/20000], Loss: 1918.495361328125, Entropy -121.0810317993164, Learning Rate: 0.01\n",
      "Epoch [298/20000], Loss: 1665.3359375, Entropy -97.34999084472656, Learning Rate: 0.01\n",
      "Epoch [299/20000], Loss: 1664.53857421875, Entropy -100.9311294555664, Learning Rate: 0.01\n",
      "Epoch [300/20000], Loss: 1827.384521484375, Entropy -100.70539855957031, Learning Rate: 0.01\n",
      "Epoch [301/20000], Loss: 1733.1578369140625, Entropy -111.63379669189453, Learning Rate: 0.01\n",
      "Epoch [302/20000], Loss: 1768.5416259765625, Entropy -119.79747009277344, Learning Rate: 0.01\n",
      "Epoch [303/20000], Loss: 1667.283447265625, Entropy -103.72872161865234, Learning Rate: 0.01\n",
      "Epoch [304/20000], Loss: 1623.8916015625, Entropy -96.40554809570312, Learning Rate: 0.01\n",
      "Epoch [305/20000], Loss: 1794.349853515625, Entropy -108.13087463378906, Learning Rate: 0.01\n",
      "Epoch [306/20000], Loss: 1650.141357421875, Entropy -113.396728515625, Learning Rate: 0.01\n",
      "Epoch [307/20000], Loss: 1714.399169921875, Entropy -108.56477355957031, Learning Rate: 0.01\n",
      "Epoch [308/20000], Loss: 1750.2403564453125, Entropy -115.654296875, Learning Rate: 0.01\n",
      "Epoch [309/20000], Loss: 2029.7528076171875, Entropy -108.47396087646484, Learning Rate: 0.01\n",
      "Epoch [310/20000], Loss: 1684.9300537109375, Entropy -83.81227111816406, Learning Rate: 0.01\n",
      "Epoch [311/20000], Loss: 1727.22412109375, Entropy -102.60784912109375, Learning Rate: 0.01\n",
      "Epoch [312/20000], Loss: 1567.521240234375, Entropy -86.71569061279297, Learning Rate: 0.01\n",
      "Epoch [313/20000], Loss: 1675.8111572265625, Entropy -113.13713836669922, Learning Rate: 0.01\n",
      "Epoch [314/20000], Loss: 1650.5150146484375, Entropy -94.44693756103516, Learning Rate: 0.01\n",
      "Epoch [315/20000], Loss: 1827.1737060546875, Entropy -92.57156372070312, Learning Rate: 0.01\n",
      "Epoch [316/20000], Loss: 1722.7088623046875, Entropy -101.32966613769531, Learning Rate: 0.01\n",
      "Epoch [317/20000], Loss: 1639.2830810546875, Entropy -80.59333801269531, Learning Rate: 0.01\n",
      "Epoch [318/20000], Loss: 1650.6495361328125, Entropy -108.79070281982422, Learning Rate: 0.01\n",
      "Epoch [319/20000], Loss: 1677.224609375, Entropy -108.28140258789062, Learning Rate: 0.01\n",
      "Epoch [320/20000], Loss: 2011.119384765625, Entropy -104.74136352539062, Learning Rate: 0.01\n",
      "Epoch [321/20000], Loss: 1750.230224609375, Entropy -106.69886779785156, Learning Rate: 0.01\n",
      "Epoch [322/20000], Loss: 1654.29150390625, Entropy -111.94719696044922, Learning Rate: 0.01\n",
      "Epoch [323/20000], Loss: 1812.2166748046875, Entropy -111.34302520751953, Learning Rate: 0.01\n",
      "Epoch [324/20000], Loss: 1795.249267578125, Entropy -103.3427505493164, Learning Rate: 0.01\n",
      "Epoch [325/20000], Loss: 1676.8572998046875, Entropy -107.43521118164062, Learning Rate: 0.01\n",
      "Epoch [326/20000], Loss: 1696.272216796875, Entropy -104.26972198486328, Learning Rate: 0.01\n",
      "Epoch [327/20000], Loss: 1714.06494140625, Entropy -87.64295959472656, Learning Rate: 0.01\n",
      "Epoch [328/20000], Loss: 1752.2930908203125, Entropy -96.44995880126953, Learning Rate: 0.01\n",
      "Epoch [329/20000], Loss: 1745.064697265625, Entropy -108.88868713378906, Learning Rate: 0.01\n",
      "Epoch [330/20000], Loss: 1714.6519775390625, Entropy -102.52922821044922, Learning Rate: 0.01\n",
      "Epoch [331/20000], Loss: 1769.906982421875, Entropy -97.65510559082031, Learning Rate: 0.01\n",
      "Epoch [332/20000], Loss: 1626.0045166015625, Entropy -95.38107299804688, Learning Rate: 0.01\n",
      "Epoch [333/20000], Loss: 1658.6307373046875, Entropy -98.6952896118164, Learning Rate: 0.01\n",
      "Epoch [334/20000], Loss: 1706.2491455078125, Entropy -114.84392547607422, Learning Rate: 0.01\n",
      "Epoch [335/20000], Loss: 1632.53076171875, Entropy -110.27100372314453, Learning Rate: 0.01\n",
      "Epoch [336/20000], Loss: 1611.0478515625, Entropy -74.18220520019531, Learning Rate: 0.01\n",
      "Epoch [337/20000], Loss: 1790.94677734375, Entropy -99.28302764892578, Learning Rate: 0.01\n",
      "Epoch [338/20000], Loss: 1567.09716796875, Entropy -99.54158020019531, Learning Rate: 0.01\n",
      "Epoch [339/20000], Loss: 1664.302978515625, Entropy -116.65776824951172, Learning Rate: 0.01\n",
      "Epoch [340/20000], Loss: 1801.266357421875, Entropy -94.14718627929688, Learning Rate: 0.01\n",
      "Epoch [341/20000], Loss: 1665.44384765625, Entropy -90.66238403320312, Learning Rate: 0.01\n",
      "Epoch [342/20000], Loss: 1589.248046875, Entropy -104.60276794433594, Learning Rate: 0.01\n",
      "Epoch [343/20000], Loss: 1557.13525390625, Entropy -94.46656036376953, Learning Rate: 0.01\n",
      "Epoch [344/20000], Loss: 1759.70263671875, Entropy -118.4891586303711, Learning Rate: 0.01\n",
      "Epoch [345/20000], Loss: 1610.8048095703125, Entropy -92.38792419433594, Learning Rate: 0.01\n",
      "Epoch [346/20000], Loss: 1597.88818359375, Entropy -88.8024673461914, Learning Rate: 0.01\n",
      "Epoch [347/20000], Loss: 1670.5401611328125, Entropy -110.79541015625, Learning Rate: 0.01\n",
      "Epoch [348/20000], Loss: 1597.3126220703125, Entropy -102.56712341308594, Learning Rate: 0.01\n",
      "Epoch [349/20000], Loss: 1832.4090576171875, Entropy -105.79402160644531, Learning Rate: 0.01\n",
      "Epoch [350/20000], Loss: 1605.7379150390625, Entropy -102.75627899169922, Learning Rate: 0.01\n",
      "Epoch [351/20000], Loss: 1566.8702392578125, Entropy -74.65377044677734, Learning Rate: 0.01\n",
      "Epoch [352/20000], Loss: 1603.1004638671875, Entropy -92.56013488769531, Learning Rate: 0.01\n",
      "Epoch [353/20000], Loss: 1602.554931640625, Entropy -83.02631378173828, Learning Rate: 0.01\n",
      "Epoch [354/20000], Loss: 1545.6092529296875, Entropy -98.42337036132812, Learning Rate: 0.01\n",
      "Epoch [355/20000], Loss: 1709.2568359375, Entropy -101.88720703125, Learning Rate: 0.01\n",
      "Epoch [356/20000], Loss: 1643.3173828125, Entropy -90.56668853759766, Learning Rate: 0.01\n",
      "Epoch [357/20000], Loss: 1566.2845458984375, Entropy -102.77550506591797, Learning Rate: 0.01\n",
      "Epoch [358/20000], Loss: 1681.9356689453125, Entropy -116.5118637084961, Learning Rate: 0.01\n",
      "Epoch [359/20000], Loss: 1671.0948486328125, Entropy -97.44095611572266, Learning Rate: 0.01\n",
      "Epoch [360/20000], Loss: 1696.201416015625, Entropy -103.78548431396484, Learning Rate: 0.01\n",
      "Epoch [361/20000], Loss: 1677.212646484375, Entropy -98.17976379394531, Learning Rate: 0.01\n",
      "Epoch [362/20000], Loss: 1625.8353271484375, Entropy -104.81101989746094, Learning Rate: 0.01\n",
      "Epoch [363/20000], Loss: 1662.977294921875, Entropy -94.49288177490234, Learning Rate: 0.01\n",
      "Epoch [364/20000], Loss: 1812.7744140625, Entropy -81.92784881591797, Learning Rate: 0.01\n",
      "Epoch [365/20000], Loss: 1507.5048828125, Entropy -87.59587860107422, Learning Rate: 0.01\n",
      "Epoch [366/20000], Loss: 1642.436767578125, Entropy -99.27599334716797, Learning Rate: 0.01\n",
      "Epoch [367/20000], Loss: 1633.6693115234375, Entropy -86.24446868896484, Learning Rate: 0.01\n",
      "Epoch [368/20000], Loss: 1650.5694580078125, Entropy -99.3881607055664, Learning Rate: 0.01\n",
      "Epoch [369/20000], Loss: 1556.9969482421875, Entropy -111.59571838378906, Learning Rate: 0.01\n",
      "Epoch [370/20000], Loss: 1543.406494140625, Entropy -89.90492248535156, Learning Rate: 0.01\n",
      "Epoch [371/20000], Loss: 1500.00537109375, Entropy -109.35003662109375, Learning Rate: 0.01\n",
      "Epoch [372/20000], Loss: 1619.251953125, Entropy -95.46426391601562, Learning Rate: 0.01\n",
      "Epoch [373/20000], Loss: 1578.691162109375, Entropy -96.8585205078125, Learning Rate: 0.01\n",
      "Epoch [374/20000], Loss: 1634.5811767578125, Entropy -97.64677429199219, Learning Rate: 0.01\n",
      "Epoch [375/20000], Loss: 1561.4898681640625, Entropy -97.22171020507812, Learning Rate: 0.01\n",
      "Epoch [376/20000], Loss: 1880.7489013671875, Entropy -107.25608825683594, Learning Rate: 0.01\n",
      "Epoch [377/20000], Loss: 1729.93603515625, Entropy -104.1201171875, Learning Rate: 0.01\n",
      "Epoch [378/20000], Loss: 1541.066162109375, Entropy -94.40105438232422, Learning Rate: 0.01\n",
      "Epoch [379/20000], Loss: 1611.2491455078125, Entropy -100.90880584716797, Learning Rate: 0.01\n",
      "Epoch [380/20000], Loss: 1611.5609130859375, Entropy -100.4374771118164, Learning Rate: 0.01\n",
      "Epoch [381/20000], Loss: 1656.5269775390625, Entropy -99.10903930664062, Learning Rate: 0.01\n",
      "Epoch [382/20000], Loss: 1510.8087158203125, Entropy -106.65657043457031, Learning Rate: 0.01\n",
      "Epoch [383/20000], Loss: 1593.0045166015625, Entropy -101.1734390258789, Learning Rate: 0.01\n",
      "Epoch [384/20000], Loss: 1749.2352294921875, Entropy -86.92048645019531, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [385/20000], Loss: 1589.4449462890625, Entropy -113.67286682128906, Learning Rate: 0.01\n",
      "Epoch [386/20000], Loss: 1554.5322265625, Entropy -117.92851257324219, Learning Rate: 0.01\n",
      "Epoch [387/20000], Loss: 1625.955322265625, Entropy -95.3973388671875, Learning Rate: 0.01\n",
      "Epoch [388/20000], Loss: 1584.192626953125, Entropy -105.54466247558594, Learning Rate: 0.01\n",
      "Epoch [389/20000], Loss: 1615.345458984375, Entropy -107.48169708251953, Learning Rate: 0.01\n",
      "Epoch [390/20000], Loss: 1570.3299560546875, Entropy -111.4007568359375, Learning Rate: 0.01\n",
      "Epoch [391/20000], Loss: 1568.341552734375, Entropy -93.35980987548828, Learning Rate: 0.01\n",
      "Epoch [392/20000], Loss: 1596.945068359375, Entropy -121.47716522216797, Learning Rate: 0.01\n",
      "Epoch [393/20000], Loss: 1501.1376953125, Entropy -89.15985870361328, Learning Rate: 0.01\n",
      "Epoch [394/20000], Loss: 1637.0177001953125, Entropy -93.40767669677734, Learning Rate: 0.01\n",
      "Epoch [395/20000], Loss: 1506.5494384765625, Entropy -86.70769500732422, Learning Rate: 0.01\n",
      "Epoch [396/20000], Loss: 1591.457763671875, Entropy -101.84148406982422, Learning Rate: 0.01\n",
      "Epoch [397/20000], Loss: 1493.9761962890625, Entropy -99.96369171142578, Learning Rate: 0.01\n",
      "Epoch [398/20000], Loss: 1668.298583984375, Entropy -88.3829574584961, Learning Rate: 0.01\n",
      "Epoch [399/20000], Loss: 1582.2825927734375, Entropy -99.64341735839844, Learning Rate: 0.01\n",
      "Epoch [400/20000], Loss: 1543.381591796875, Entropy -91.66448974609375, Learning Rate: 0.01\n",
      "Epoch [401/20000], Loss: 1577.35400390625, Entropy -94.83384704589844, Learning Rate: 0.01\n",
      "Epoch [402/20000], Loss: 1676.0264892578125, Entropy -94.32817840576172, Learning Rate: 0.01\n",
      "Epoch [403/20000], Loss: 1573.7474365234375, Entropy -86.13652801513672, Learning Rate: 0.01\n",
      "Epoch [404/20000], Loss: 1841.967529296875, Entropy -109.1041488647461, Learning Rate: 0.01\n",
      "Epoch [405/20000], Loss: 1593.5887451171875, Entropy -89.38651275634766, Learning Rate: 0.01\n",
      "Epoch [406/20000], Loss: 1499.542724609375, Entropy -89.51969146728516, Learning Rate: 0.01\n",
      "Epoch [407/20000], Loss: 1703.2906494140625, Entropy -79.82340240478516, Learning Rate: 0.01\n",
      "Epoch [408/20000], Loss: 1558.051513671875, Entropy -89.21871185302734, Learning Rate: 0.01\n",
      "Epoch [409/20000], Loss: 1793.0869140625, Entropy -94.96107482910156, Learning Rate: 0.01\n",
      "Epoch [410/20000], Loss: 1616.1046142578125, Entropy -111.239990234375, Learning Rate: 0.01\n",
      "Epoch [411/20000], Loss: 1554.843017578125, Entropy -100.28384399414062, Learning Rate: 0.01\n",
      "Epoch [412/20000], Loss: 1503.424560546875, Entropy -84.36799621582031, Learning Rate: 0.01\n",
      "Epoch [413/20000], Loss: 1652.928466796875, Entropy -92.90557098388672, Learning Rate: 0.01\n",
      "Epoch [414/20000], Loss: 1479.8485107421875, Entropy -91.30508422851562, Learning Rate: 0.01\n",
      "Epoch [415/20000], Loss: 1543.3858642578125, Entropy -103.23228454589844, Learning Rate: 0.01\n",
      "Epoch [416/20000], Loss: 1466.70703125, Entropy -104.56056213378906, Learning Rate: 0.01\n",
      "Epoch [417/20000], Loss: 1607.9559326171875, Entropy -103.29415893554688, Learning Rate: 0.01\n",
      "Epoch [418/20000], Loss: 1553.5421142578125, Entropy -109.33158111572266, Learning Rate: 0.01\n",
      "Epoch [419/20000], Loss: 1550.9686279296875, Entropy -85.31574249267578, Learning Rate: 0.01\n",
      "Epoch [420/20000], Loss: 1579.859619140625, Entropy -101.59309387207031, Learning Rate: 0.01\n",
      "Epoch [421/20000], Loss: 1484.0953369140625, Entropy -85.45247650146484, Learning Rate: 0.01\n",
      "Epoch [422/20000], Loss: 1565.357421875, Entropy -84.88679504394531, Learning Rate: 0.01\n",
      "Epoch [423/20000], Loss: 1513.586669921875, Entropy -99.85269927978516, Learning Rate: 0.01\n",
      "Epoch [424/20000], Loss: 1584.1763916015625, Entropy -93.78640747070312, Learning Rate: 0.01\n",
      "Epoch [425/20000], Loss: 1565.9130859375, Entropy -81.0855712890625, Learning Rate: 0.01\n",
      "Epoch [426/20000], Loss: 1552.0556640625, Entropy -104.68950653076172, Learning Rate: 0.01\n",
      "Epoch [427/20000], Loss: 1596.8638916015625, Entropy -91.51222229003906, Learning Rate: 0.01\n",
      "Epoch [428/20000], Loss: 1479.7738037109375, Entropy -93.83323669433594, Learning Rate: 0.01\n",
      "Epoch [429/20000], Loss: 1544.50927734375, Entropy -104.02246856689453, Learning Rate: 0.01\n",
      "Epoch [430/20000], Loss: 1636.7943115234375, Entropy -82.7824478149414, Learning Rate: 0.01\n",
      "Epoch [431/20000], Loss: 1521.564453125, Entropy -122.48665618896484, Learning Rate: 0.01\n",
      "Epoch [432/20000], Loss: 1523.1727294921875, Entropy -73.42272186279297, Learning Rate: 0.01\n",
      "Epoch [433/20000], Loss: 1719.676025390625, Entropy -97.25196838378906, Learning Rate: 0.01\n",
      "Epoch [434/20000], Loss: 1699.265625, Entropy -105.2950439453125, Learning Rate: 0.01\n",
      "Epoch [435/20000], Loss: 1553.08984375, Entropy -92.41425323486328, Learning Rate: 0.01\n",
      "Epoch [436/20000], Loss: 1536.27490234375, Entropy -100.46055603027344, Learning Rate: 0.01\n",
      "Epoch [437/20000], Loss: 1467.4747314453125, Entropy -101.98472595214844, Learning Rate: 0.01\n",
      "Epoch [438/20000], Loss: 1531.2344970703125, Entropy -96.00235748291016, Learning Rate: 0.01\n",
      "Epoch [439/20000], Loss: 1525.4139404296875, Entropy -83.58499908447266, Learning Rate: 0.01\n",
      "Epoch [440/20000], Loss: 1525.6688232421875, Entropy -97.88595581054688, Learning Rate: 0.01\n",
      "Epoch [441/20000], Loss: 1483.1376953125, Entropy -104.82521057128906, Learning Rate: 0.01\n",
      "Epoch [442/20000], Loss: 1497.1861572265625, Entropy -98.07942962646484, Learning Rate: 0.01\n",
      "Epoch [443/20000], Loss: 1514.5841064453125, Entropy -94.26253509521484, Learning Rate: 0.01\n",
      "Epoch [444/20000], Loss: 1536.7222900390625, Entropy -108.05178833007812, Learning Rate: 0.01\n",
      "Epoch [445/20000], Loss: 1510.3773193359375, Entropy -120.6142807006836, Learning Rate: 0.01\n",
      "Epoch [446/20000], Loss: 1485.5416259765625, Entropy -80.55888366699219, Learning Rate: 0.01\n",
      "Epoch [447/20000], Loss: 1550.868896484375, Entropy -86.5322036743164, Learning Rate: 0.01\n",
      "Epoch [448/20000], Loss: 1493.9708251953125, Entropy -90.76828002929688, Learning Rate: 0.01\n",
      "Epoch [449/20000], Loss: 1586.2967529296875, Entropy -82.05563354492188, Learning Rate: 0.01\n",
      "Epoch [450/20000], Loss: 1528.85498046875, Entropy -86.68286895751953, Learning Rate: 0.01\n",
      "Epoch [451/20000], Loss: 1554.0601806640625, Entropy -92.9193115234375, Learning Rate: 0.01\n",
      "Epoch [452/20000], Loss: 1525.2613525390625, Entropy -88.7367172241211, Learning Rate: 0.01\n",
      "Epoch [453/20000], Loss: 1522.203125, Entropy -99.14507293701172, Learning Rate: 0.01\n",
      "Epoch [454/20000], Loss: 1524.1617431640625, Entropy -103.72643280029297, Learning Rate: 0.01\n",
      "Epoch [455/20000], Loss: 1461.4864501953125, Entropy -94.40070343017578, Learning Rate: 0.01\n",
      "Epoch [456/20000], Loss: 1489.75048828125, Entropy -92.22466278076172, Learning Rate: 0.01\n",
      "Epoch [457/20000], Loss: 1563.2271728515625, Entropy -100.62110900878906, Learning Rate: 0.01\n",
      "Epoch [458/20000], Loss: 1452.1334228515625, Entropy -98.9735336303711, Learning Rate: 0.01\n",
      "Epoch [459/20000], Loss: 1494.009521484375, Entropy -105.35491943359375, Learning Rate: 0.01\n",
      "Epoch [460/20000], Loss: 1467.795166015625, Entropy -82.88613891601562, Learning Rate: 0.01\n",
      "Epoch [461/20000], Loss: 1654.2352294921875, Entropy -118.89493560791016, Learning Rate: 0.01\n",
      "Epoch [462/20000], Loss: 1401.380859375, Entropy -79.23280334472656, Learning Rate: 0.01\n",
      "Epoch [463/20000], Loss: 1461.9732666015625, Entropy -103.36024475097656, Learning Rate: 0.01\n",
      "Epoch [464/20000], Loss: 1509.3883056640625, Entropy -98.94099426269531, Learning Rate: 0.01\n",
      "Epoch [465/20000], Loss: 1471.436767578125, Entropy -76.8546142578125, Learning Rate: 0.01\n",
      "Epoch [466/20000], Loss: 1647.939453125, Entropy -91.20222473144531, Learning Rate: 0.01\n",
      "Epoch [467/20000], Loss: 1524.7889404296875, Entropy -94.77588653564453, Learning Rate: 0.01\n",
      "Epoch [468/20000], Loss: 1525.7633056640625, Entropy -90.20842742919922, Learning Rate: 0.01\n",
      "Epoch [469/20000], Loss: 1487.224853515625, Entropy -98.74432373046875, Learning Rate: 0.01\n",
      "Epoch [470/20000], Loss: 1481.6448974609375, Entropy -95.91242218017578, Learning Rate: 0.01\n",
      "Epoch [471/20000], Loss: 1439.3170166015625, Entropy -86.6275863647461, Learning Rate: 0.01\n",
      "Epoch [472/20000], Loss: 1577.6104736328125, Entropy -104.38380432128906, Learning Rate: 0.01\n",
      "Epoch [473/20000], Loss: 1425.68701171875, Entropy -87.95458984375, Learning Rate: 0.01\n",
      "Epoch [474/20000], Loss: 1493.0465087890625, Entropy -82.77301788330078, Learning Rate: 0.01\n",
      "Epoch [475/20000], Loss: 1613.3057861328125, Entropy -84.38623046875, Learning Rate: 0.01\n",
      "Epoch [476/20000], Loss: 1499.7403564453125, Entropy -96.759033203125, Learning Rate: 0.01\n",
      "Epoch [477/20000], Loss: 1443.496826171875, Entropy -79.54788208007812, Learning Rate: 0.01\n",
      "Epoch [478/20000], Loss: 1560.5692138671875, Entropy -80.12769317626953, Learning Rate: 0.01\n",
      "Epoch [479/20000], Loss: 1564.2149658203125, Entropy -84.85597229003906, Learning Rate: 0.01\n",
      "Epoch [480/20000], Loss: 1504.7884521484375, Entropy -98.40630340576172, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [481/20000], Loss: 1478.697021484375, Entropy -81.86670684814453, Learning Rate: 0.01\n",
      "Epoch [482/20000], Loss: 1493.8563232421875, Entropy -88.4627685546875, Learning Rate: 0.01\n",
      "Epoch [483/20000], Loss: 1459.6702880859375, Entropy -86.6491470336914, Learning Rate: 0.01\n",
      "Epoch [484/20000], Loss: 1433.024169921875, Entropy -64.92041015625, Learning Rate: 0.01\n",
      "Epoch [485/20000], Loss: 1435.45068359375, Entropy -77.6884536743164, Learning Rate: 0.01\n",
      "Epoch [486/20000], Loss: 1389.0389404296875, Entropy -66.92394256591797, Learning Rate: 0.01\n",
      "Epoch [487/20000], Loss: 1782.197998046875, Entropy -71.67876434326172, Learning Rate: 0.01\n",
      "Epoch [488/20000], Loss: 1479.8956298828125, Entropy -93.50538635253906, Learning Rate: 0.01\n",
      "Epoch [489/20000], Loss: 1457.5330810546875, Entropy -88.23943328857422, Learning Rate: 0.01\n",
      "Epoch [490/20000], Loss: 1465.1741943359375, Entropy -79.49752044677734, Learning Rate: 0.01\n",
      "Epoch [491/20000], Loss: 1456.368896484375, Entropy -83.99121856689453, Learning Rate: 0.01\n",
      "Epoch [492/20000], Loss: 1441.9180908203125, Entropy -73.21200561523438, Learning Rate: 0.01\n",
      "Epoch [493/20000], Loss: 1431.49072265625, Entropy -90.39360046386719, Learning Rate: 0.01\n",
      "Epoch [494/20000], Loss: 1407.384765625, Entropy -77.68486022949219, Learning Rate: 0.01\n",
      "Epoch [495/20000], Loss: 1475.4417724609375, Entropy -99.79457092285156, Learning Rate: 0.01\n",
      "Epoch [496/20000], Loss: 1406.910888671875, Entropy -88.31148529052734, Learning Rate: 0.01\n",
      "Epoch [497/20000], Loss: 1423.0367431640625, Entropy -92.06856536865234, Learning Rate: 0.01\n",
      "Epoch [498/20000], Loss: 1544.14599609375, Entropy -90.75759887695312, Learning Rate: 0.01\n",
      "Epoch [499/20000], Loss: 1429.072265625, Entropy -85.15530395507812, Learning Rate: 0.01\n",
      "Epoch [500/20000], Loss: 1469.3719482421875, Entropy -103.14791107177734, Learning Rate: 0.01\n",
      "Epoch [501/20000], Loss: 1425.6973876953125, Entropy -81.08551788330078, Learning Rate: 0.01\n",
      "Epoch [502/20000], Loss: 1508.6195068359375, Entropy -84.99317932128906, Learning Rate: 0.01\n",
      "Epoch [503/20000], Loss: 1492.1910400390625, Entropy -84.54280090332031, Learning Rate: 0.01\n",
      "Epoch [504/20000], Loss: 1500.1177978515625, Entropy -84.11433410644531, Learning Rate: 0.01\n",
      "Epoch [505/20000], Loss: 1480.6329345703125, Entropy -104.2841567993164, Learning Rate: 0.01\n",
      "Epoch [506/20000], Loss: 1493.240966796875, Entropy -92.26284790039062, Learning Rate: 0.01\n",
      "Epoch [507/20000], Loss: 1546.0064697265625, Entropy -87.73361206054688, Learning Rate: 0.01\n",
      "Epoch [508/20000], Loss: 1497.338623046875, Entropy -94.42769622802734, Learning Rate: 0.01\n",
      "Epoch [509/20000], Loss: 1424.3421630859375, Entropy -80.77021789550781, Learning Rate: 0.01\n",
      "Epoch [510/20000], Loss: 1461.418212890625, Entropy -82.19386291503906, Learning Rate: 0.01\n",
      "Epoch [511/20000], Loss: 1602.5509033203125, Entropy -72.68607330322266, Learning Rate: 0.01\n",
      "Epoch [512/20000], Loss: 1479.5511474609375, Entropy -83.49931335449219, Learning Rate: 0.01\n",
      "Epoch [513/20000], Loss: 1451.9114990234375, Entropy -101.79714965820312, Learning Rate: 0.01\n",
      "Epoch [514/20000], Loss: 1495.88427734375, Entropy -93.87793731689453, Learning Rate: 0.01\n",
      "Epoch [515/20000], Loss: 1431.9210205078125, Entropy -90.37791442871094, Learning Rate: 0.01\n",
      "Epoch [516/20000], Loss: 1481.6495361328125, Entropy -87.40013122558594, Learning Rate: 0.01\n",
      "Epoch [517/20000], Loss: 1462.0216064453125, Entropy -80.02425384521484, Learning Rate: 0.01\n",
      "Epoch [518/20000], Loss: 1479.1434326171875, Entropy -92.2884521484375, Learning Rate: 0.01\n",
      "Epoch [519/20000], Loss: 1423.444580078125, Entropy -65.43167114257812, Learning Rate: 0.01\n",
      "Epoch [520/20000], Loss: 1486.4129638671875, Entropy -91.981689453125, Learning Rate: 0.01\n",
      "Epoch [521/20000], Loss: 1493.6690673828125, Entropy -93.42103576660156, Learning Rate: 0.01\n",
      "Epoch [522/20000], Loss: 1481.0972900390625, Entropy -100.82193756103516, Learning Rate: 0.01\n",
      "Epoch [523/20000], Loss: 1491.2120361328125, Entropy -83.4760513305664, Learning Rate: 0.01\n",
      "Epoch [524/20000], Loss: 1701.6402587890625, Entropy -88.60282897949219, Learning Rate: 0.01\n",
      "Epoch [525/20000], Loss: 1505.78271484375, Entropy -86.98198699951172, Learning Rate: 0.01\n",
      "Epoch [526/20000], Loss: 1592.205078125, Entropy -81.4194107055664, Learning Rate: 0.01\n",
      "Epoch [527/20000], Loss: 1499.2215576171875, Entropy -77.76329040527344, Learning Rate: 0.01\n",
      "Epoch [528/20000], Loss: 1468.9544677734375, Entropy -95.62837982177734, Learning Rate: 0.01\n",
      "Epoch [529/20000], Loss: 1462.586181640625, Entropy -74.1883773803711, Learning Rate: 0.01\n",
      "Epoch [530/20000], Loss: 1440.939697265625, Entropy -87.91034698486328, Learning Rate: 0.01\n",
      "Epoch [531/20000], Loss: 1471.7720947265625, Entropy -87.39191436767578, Learning Rate: 0.01\n",
      "Epoch [532/20000], Loss: 1438.150146484375, Entropy -76.70203399658203, Learning Rate: 0.01\n",
      "Epoch [533/20000], Loss: 1427.067138671875, Entropy -81.07047271728516, Learning Rate: 0.01\n",
      "Epoch [534/20000], Loss: 1424.2877197265625, Entropy -70.96216583251953, Learning Rate: 0.01\n",
      "Epoch [535/20000], Loss: 1454.1954345703125, Entropy -83.1654281616211, Learning Rate: 0.01\n",
      "Epoch [536/20000], Loss: 1441.968017578125, Entropy -89.84584045410156, Learning Rate: 0.01\n",
      "Epoch [537/20000], Loss: 1498.393310546875, Entropy -85.79655456542969, Learning Rate: 0.01\n",
      "Epoch [538/20000], Loss: 1477.6761474609375, Entropy -79.44510650634766, Learning Rate: 0.01\n",
      "Epoch [539/20000], Loss: 1610.9267578125, Entropy -87.57149505615234, Learning Rate: 0.01\n",
      "Epoch [540/20000], Loss: 1468.44921875, Entropy -75.35517120361328, Learning Rate: 0.01\n",
      "Epoch [541/20000], Loss: 1494.6630859375, Entropy -82.11226654052734, Learning Rate: 0.01\n",
      "Epoch [542/20000], Loss: 1465.7015380859375, Entropy -72.63880157470703, Learning Rate: 0.01\n",
      "Epoch [543/20000], Loss: 1513.78857421875, Entropy -75.70547485351562, Learning Rate: 0.01\n",
      "Epoch [544/20000], Loss: 1426.7147216796875, Entropy -90.06883239746094, Learning Rate: 0.01\n",
      "Epoch [545/20000], Loss: 1426.3626708984375, Entropy -75.3189926147461, Learning Rate: 0.01\n",
      "Epoch [546/20000], Loss: 1400.732177734375, Entropy -74.58074188232422, Learning Rate: 0.01\n",
      "Epoch [547/20000], Loss: 1511.9041748046875, Entropy -94.14289093017578, Learning Rate: 0.01\n",
      "Epoch [548/20000], Loss: 1384.7239990234375, Entropy -61.98289108276367, Learning Rate: 0.01\n",
      "Epoch [549/20000], Loss: 1465.5003662109375, Entropy -73.9535140991211, Learning Rate: 0.01\n",
      "Epoch [550/20000], Loss: 1455.056640625, Entropy -73.36698913574219, Learning Rate: 0.01\n",
      "Epoch [551/20000], Loss: 1467.2000732421875, Entropy -84.1324234008789, Learning Rate: 0.01\n",
      "Epoch [552/20000], Loss: 1492.669921875, Entropy -77.40472412109375, Learning Rate: 0.01\n",
      "Epoch [553/20000], Loss: 1439.7462158203125, Entropy -69.03849792480469, Learning Rate: 0.01\n",
      "Epoch [554/20000], Loss: 1458.9271240234375, Entropy -87.22222137451172, Learning Rate: 0.01\n",
      "Epoch [555/20000], Loss: 1444.5565185546875, Entropy -86.2090835571289, Learning Rate: 0.01\n",
      "Epoch [556/20000], Loss: 1409.53173828125, Entropy -77.77323150634766, Learning Rate: 0.01\n",
      "Epoch [557/20000], Loss: 1492.362548828125, Entropy -86.34735107421875, Learning Rate: 0.01\n",
      "Epoch [558/20000], Loss: 1351.48681640625, Entropy -57.08544921875, Learning Rate: 0.01\n",
      "Epoch [559/20000], Loss: 1533.789794921875, Entropy -100.46421813964844, Learning Rate: 0.01\n",
      "Epoch [560/20000], Loss: 1462.85888671875, Entropy -78.73082733154297, Learning Rate: 0.01\n",
      "Epoch [561/20000], Loss: 1449.56396484375, Entropy -73.54244232177734, Learning Rate: 0.01\n",
      "Epoch [562/20000], Loss: 1422.7301025390625, Entropy -84.93389129638672, Learning Rate: 0.01\n",
      "Epoch [563/20000], Loss: 1463.2725830078125, Entropy -83.39576721191406, Learning Rate: 0.01\n",
      "Epoch [564/20000], Loss: 1535.42822265625, Entropy -79.33291625976562, Learning Rate: 0.01\n",
      "Epoch [565/20000], Loss: 1381.6812744140625, Entropy -74.91645812988281, Learning Rate: 0.01\n",
      "Epoch [566/20000], Loss: 1444.7713623046875, Entropy -79.79891204833984, Learning Rate: 0.01\n",
      "Epoch [567/20000], Loss: 1478.4320068359375, Entropy -67.84062957763672, Learning Rate: 0.01\n",
      "Epoch [568/20000], Loss: 1446.9647216796875, Entropy -90.12248992919922, Learning Rate: 0.01\n",
      "Epoch [569/20000], Loss: 1386.9478759765625, Entropy -67.12602233886719, Learning Rate: 0.01\n",
      "Epoch [570/20000], Loss: 1397.8914794921875, Entropy -79.65306091308594, Learning Rate: 0.01\n",
      "Epoch [571/20000], Loss: 1460.86572265625, Entropy -78.4912109375, Learning Rate: 0.01\n",
      "Epoch [572/20000], Loss: 1553.09423828125, Entropy -98.3573226928711, Learning Rate: 0.01\n",
      "Epoch [573/20000], Loss: 1380.708740234375, Entropy -75.5438232421875, Learning Rate: 0.01\n",
      "Epoch [574/20000], Loss: 1374.7325439453125, Entropy -69.23070526123047, Learning Rate: 0.01\n",
      "Epoch [575/20000], Loss: 1413.7872314453125, Entropy -71.0694580078125, Learning Rate: 0.01\n",
      "Epoch [576/20000], Loss: 1463.0623779296875, Entropy -93.14524841308594, Learning Rate: 0.01\n",
      "Epoch [577/20000], Loss: 1553.2611083984375, Entropy -65.16573333740234, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [578/20000], Loss: 1552.65234375, Entropy -76.39990997314453, Learning Rate: 0.01\n",
      "Epoch [579/20000], Loss: 1529.8446044921875, Entropy -59.70209884643555, Learning Rate: 0.01\n",
      "Epoch [580/20000], Loss: 1473.6170654296875, Entropy -71.7439956665039, Learning Rate: 0.01\n",
      "Epoch [581/20000], Loss: 1498.3643798828125, Entropy -65.25812530517578, Learning Rate: 0.01\n",
      "Epoch [582/20000], Loss: 1384.6898193359375, Entropy -82.06655883789062, Learning Rate: 0.01\n",
      "Epoch [583/20000], Loss: 1483.8328857421875, Entropy -90.23171997070312, Learning Rate: 0.01\n",
      "Epoch [584/20000], Loss: 1386.8289794921875, Entropy -84.0326156616211, Learning Rate: 0.01\n",
      "Epoch [585/20000], Loss: 1496.0428466796875, Entropy -73.43184661865234, Learning Rate: 0.01\n",
      "Epoch [586/20000], Loss: 1476.63623046875, Entropy -61.19147491455078, Learning Rate: 0.01\n",
      "Epoch [587/20000], Loss: 1388.9677734375, Entropy -64.0771255493164, Learning Rate: 0.01\n",
      "Epoch [588/20000], Loss: 1366.855224609375, Entropy -73.98424530029297, Learning Rate: 0.01\n",
      "Epoch [589/20000], Loss: 1420.292236328125, Entropy -84.0079345703125, Learning Rate: 0.01\n",
      "Epoch [590/20000], Loss: 1405.721435546875, Entropy -66.27245330810547, Learning Rate: 0.01\n",
      "Epoch [591/20000], Loss: 1453.3553466796875, Entropy -67.760498046875, Learning Rate: 0.01\n",
      "Epoch [592/20000], Loss: 1382.0919189453125, Entropy -74.7900161743164, Learning Rate: 0.01\n",
      "Epoch [593/20000], Loss: 1431.98779296875, Entropy -84.93352508544922, Learning Rate: 0.01\n",
      "Epoch [594/20000], Loss: 1385.516845703125, Entropy -66.66730499267578, Learning Rate: 0.01\n",
      "Epoch [595/20000], Loss: 1405.5411376953125, Entropy -79.328857421875, Learning Rate: 0.01\n",
      "Epoch [596/20000], Loss: 1530.55517578125, Entropy -66.70142364501953, Learning Rate: 0.01\n",
      "Epoch [597/20000], Loss: 1544.77001953125, Entropy -61.46985626220703, Learning Rate: 0.01\n",
      "Epoch [598/20000], Loss: 1391.5823974609375, Entropy -67.16645050048828, Learning Rate: 0.01\n",
      "Epoch [599/20000], Loss: 1506.1337890625, Entropy -64.38199615478516, Learning Rate: 0.01\n",
      "Epoch [600/20000], Loss: 1470.1279296875, Entropy -74.45697021484375, Learning Rate: 0.01\n",
      "Epoch [601/20000], Loss: 1452.1859130859375, Entropy -70.31214141845703, Learning Rate: 0.01\n",
      "Epoch [602/20000], Loss: 1378.358154296875, Entropy -78.609375, Learning Rate: 0.01\n",
      "Epoch [603/20000], Loss: 1346.0208740234375, Entropy -72.8021240234375, Learning Rate: 0.01\n",
      "Epoch [604/20000], Loss: 1515.5140380859375, Entropy -82.76161193847656, Learning Rate: 0.01\n",
      "Epoch [605/20000], Loss: 1418.2244873046875, Entropy -54.23606491088867, Learning Rate: 0.01\n",
      "Epoch [606/20000], Loss: 1362.4959716796875, Entropy -78.87264251708984, Learning Rate: 0.01\n",
      "Epoch [607/20000], Loss: 1466.5059814453125, Entropy -75.31879425048828, Learning Rate: 0.01\n",
      "Epoch [608/20000], Loss: 1384.299072265625, Entropy -68.06697845458984, Learning Rate: 0.01\n",
      "Epoch [609/20000], Loss: 1362.0389404296875, Entropy -71.49089813232422, Learning Rate: 0.01\n",
      "Epoch [610/20000], Loss: 1418.477294921875, Entropy -62.92316436767578, Learning Rate: 0.01\n",
      "Epoch [611/20000], Loss: 1405.1099853515625, Entropy -71.58821868896484, Learning Rate: 0.01\n",
      "Epoch [612/20000], Loss: 1347.840576171875, Entropy -64.96080780029297, Learning Rate: 0.01\n",
      "Epoch [613/20000], Loss: 1504.26611328125, Entropy -77.97637939453125, Learning Rate: 0.01\n",
      "Epoch [614/20000], Loss: 1448.9180908203125, Entropy -77.76185607910156, Learning Rate: 0.01\n",
      "Epoch [615/20000], Loss: 1434.779052734375, Entropy -67.87326049804688, Learning Rate: 0.01\n",
      "Epoch [616/20000], Loss: 1416.7196044921875, Entropy -93.20460510253906, Learning Rate: 0.01\n",
      "Epoch [617/20000], Loss: 1365.3134765625, Entropy -79.91740417480469, Learning Rate: 0.01\n",
      "Epoch [618/20000], Loss: 1387.3243408203125, Entropy -62.396297454833984, Learning Rate: 0.01\n",
      "Epoch [619/20000], Loss: 1382.3056640625, Entropy -64.08187103271484, Learning Rate: 0.01\n",
      "Epoch [620/20000], Loss: 1383.6842041015625, Entropy -72.53273010253906, Learning Rate: 0.01\n",
      "Epoch [621/20000], Loss: 1410.99169921875, Entropy -64.72200775146484, Learning Rate: 0.01\n",
      "Epoch [622/20000], Loss: 1411.9935302734375, Entropy -82.43817901611328, Learning Rate: 0.01\n",
      "Epoch [623/20000], Loss: 1343.63232421875, Entropy -80.80975341796875, Learning Rate: 0.01\n",
      "Epoch [624/20000], Loss: 1452.6026611328125, Entropy -70.15570831298828, Learning Rate: 0.01\n",
      "Epoch [625/20000], Loss: 1381.50927734375, Entropy -81.35673522949219, Learning Rate: 0.01\n",
      "Epoch [626/20000], Loss: 1432.2376708984375, Entropy -85.29056549072266, Learning Rate: 0.01\n",
      "Epoch [627/20000], Loss: 1439.637451171875, Entropy -83.59056091308594, Learning Rate: 0.01\n",
      "Epoch [628/20000], Loss: 1387.259521484375, Entropy -77.68566131591797, Learning Rate: 0.01\n",
      "Epoch [629/20000], Loss: 1370.01806640625, Entropy -61.12846374511719, Learning Rate: 0.01\n",
      "Epoch [630/20000], Loss: 1419.183349609375, Entropy -61.16463851928711, Learning Rate: 0.01\n",
      "Epoch [631/20000], Loss: 1415.146728515625, Entropy -70.95582580566406, Learning Rate: 0.01\n",
      "Epoch [632/20000], Loss: 1378.92041015625, Entropy -69.30213928222656, Learning Rate: 0.01\n",
      "Epoch [633/20000], Loss: 1383.1146240234375, Entropy -63.229339599609375, Learning Rate: 0.01\n",
      "Epoch [634/20000], Loss: 1401.688232421875, Entropy -68.09390258789062, Learning Rate: 0.01\n",
      "Epoch [635/20000], Loss: 1330.8558349609375, Entropy -67.98021697998047, Learning Rate: 0.01\n",
      "Epoch [636/20000], Loss: 1353.900390625, Entropy -67.35543060302734, Learning Rate: 0.01\n",
      "Epoch [637/20000], Loss: 1408.4097900390625, Entropy -60.49778366088867, Learning Rate: 0.01\n",
      "Epoch [638/20000], Loss: 1417.85546875, Entropy -67.44985961914062, Learning Rate: 0.01\n",
      "Epoch [639/20000], Loss: 1397.271484375, Entropy -72.64697265625, Learning Rate: 0.01\n",
      "Epoch [640/20000], Loss: 1356.34228515625, Entropy -63.727840423583984, Learning Rate: 0.01\n",
      "Epoch [641/20000], Loss: 1412.7703857421875, Entropy -73.50923156738281, Learning Rate: 0.01\n",
      "Epoch [642/20000], Loss: 1386.8472900390625, Entropy -65.15038299560547, Learning Rate: 0.01\n",
      "Epoch [643/20000], Loss: 1437.138427734375, Entropy -66.15240478515625, Learning Rate: 0.01\n",
      "Epoch [644/20000], Loss: 1398.1705322265625, Entropy -75.10466003417969, Learning Rate: 0.01\n",
      "Epoch [645/20000], Loss: 1501.0531005859375, Entropy -67.94483947753906, Learning Rate: 0.01\n",
      "Epoch [646/20000], Loss: 1427.5792236328125, Entropy -71.37876892089844, Learning Rate: 0.01\n",
      "Epoch [647/20000], Loss: 1493.9151611328125, Entropy -88.49327087402344, Learning Rate: 0.01\n",
      "Epoch [648/20000], Loss: 1429.1103515625, Entropy -48.38462829589844, Learning Rate: 0.01\n",
      "Epoch [649/20000], Loss: 1376.6202392578125, Entropy -71.44180297851562, Learning Rate: 0.01\n",
      "Epoch [650/20000], Loss: 1535.9903564453125, Entropy -54.56351089477539, Learning Rate: 0.01\n",
      "Epoch [651/20000], Loss: 1479.760986328125, Entropy -64.26323699951172, Learning Rate: 0.01\n",
      "Epoch [652/20000], Loss: 1412.4976806640625, Entropy -71.80562591552734, Learning Rate: 0.01\n",
      "Epoch [653/20000], Loss: 1420.3310546875, Entropy -59.62290954589844, Learning Rate: 0.01\n",
      "Epoch [654/20000], Loss: 1373.6688232421875, Entropy -64.51864624023438, Learning Rate: 0.01\n",
      "Epoch [655/20000], Loss: 1405.77099609375, Entropy -62.005706787109375, Learning Rate: 0.01\n",
      "Epoch [656/20000], Loss: 1351.380615234375, Entropy -64.34471130371094, Learning Rate: 0.01\n",
      "Epoch [657/20000], Loss: 1395.6356201171875, Entropy -62.5793342590332, Learning Rate: 0.01\n",
      "Epoch [658/20000], Loss: 1431.0706787109375, Entropy -61.34196472167969, Learning Rate: 0.01\n",
      "Epoch [659/20000], Loss: 1398.986572265625, Entropy -56.050262451171875, Learning Rate: 0.01\n",
      "Epoch [660/20000], Loss: 1455.4039306640625, Entropy -82.06853485107422, Learning Rate: 0.01\n",
      "Epoch [661/20000], Loss: 1378.41748046875, Entropy -59.19966506958008, Learning Rate: 0.01\n",
      "Epoch [662/20000], Loss: 1433.8787841796875, Entropy -58.49251937866211, Learning Rate: 0.01\n",
      "Epoch [663/20000], Loss: 1419.413330078125, Entropy -66.3707275390625, Learning Rate: 0.01\n",
      "Epoch [664/20000], Loss: 1376.4146728515625, Entropy -68.85770416259766, Learning Rate: 0.01\n",
      "Epoch [665/20000], Loss: 1399.059326171875, Entropy -59.90414810180664, Learning Rate: 0.01\n",
      "Epoch [666/20000], Loss: 1419.509521484375, Entropy -77.67279815673828, Learning Rate: 0.01\n",
      "Epoch [667/20000], Loss: 1385.3375244140625, Entropy -49.24409103393555, Learning Rate: 0.01\n",
      "Epoch [668/20000], Loss: 1376.5074462890625, Entropy -59.16083908081055, Learning Rate: 0.01\n",
      "Epoch [669/20000], Loss: 1395.503173828125, Entropy -66.983154296875, Learning Rate: 0.01\n",
      "Epoch [670/20000], Loss: 1458.286865234375, Entropy -51.1786994934082, Learning Rate: 0.01\n",
      "Epoch [671/20000], Loss: 1311.860107421875, Entropy -67.02692413330078, Learning Rate: 0.01\n",
      "Epoch [672/20000], Loss: 1401.705810546875, Entropy -60.95446014404297, Learning Rate: 0.01\n",
      "Epoch [673/20000], Loss: 1426.982666015625, Entropy -61.59678649902344, Learning Rate: 0.01\n",
      "Epoch [674/20000], Loss: 1361.6866455078125, Entropy -66.27985382080078, Learning Rate: 0.01\n",
      "Epoch [675/20000], Loss: 1417.761474609375, Entropy -69.61910247802734, Learning Rate: 0.01\n",
      "Epoch [676/20000], Loss: 1394.95263671875, Entropy -37.97924041748047, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [677/20000], Loss: 1433.93017578125, Entropy -70.06893157958984, Learning Rate: 0.01\n",
      "Epoch [678/20000], Loss: 1378.88623046875, Entropy -68.92327880859375, Learning Rate: 0.01\n",
      "Epoch [679/20000], Loss: 1388.576904296875, Entropy -58.968631744384766, Learning Rate: 0.01\n",
      "Epoch [680/20000], Loss: 1442.722412109375, Entropy -62.37117385864258, Learning Rate: 0.01\n",
      "Epoch [681/20000], Loss: 1429.3236083984375, Entropy -57.261474609375, Learning Rate: 0.01\n",
      "Epoch [682/20000], Loss: 1416.865478515625, Entropy -64.33524322509766, Learning Rate: 0.01\n",
      "Epoch [683/20000], Loss: 1354.3731689453125, Entropy -72.14769744873047, Learning Rate: 0.01\n",
      "Epoch [684/20000], Loss: 1443.827392578125, Entropy -57.84458923339844, Learning Rate: 0.01\n",
      "Epoch [685/20000], Loss: 1393.731201171875, Entropy -68.07255554199219, Learning Rate: 0.01\n",
      "Epoch [686/20000], Loss: 1336.7196044921875, Entropy -70.30874633789062, Learning Rate: 0.01\n",
      "Epoch [687/20000], Loss: 1378.77880859375, Entropy -72.89604949951172, Learning Rate: 0.01\n",
      "Epoch [688/20000], Loss: 1381.748291015625, Entropy -53.8875732421875, Learning Rate: 0.01\n",
      "Epoch [689/20000], Loss: 1428.9351806640625, Entropy -61.1130256652832, Learning Rate: 0.01\n",
      "Epoch [690/20000], Loss: 1448.08056640625, Entropy -57.228759765625, Learning Rate: 0.01\n",
      "Epoch [691/20000], Loss: 1353.559326171875, Entropy -66.85061645507812, Learning Rate: 0.01\n",
      "Epoch [692/20000], Loss: 1362.29296875, Entropy -52.31916046142578, Learning Rate: 0.01\n",
      "Epoch [693/20000], Loss: 1448.78564453125, Entropy -65.05860900878906, Learning Rate: 0.01\n",
      "Epoch [694/20000], Loss: 1406.7171630859375, Entropy -69.25224304199219, Learning Rate: 0.01\n",
      "Epoch [695/20000], Loss: 1359.26708984375, Entropy -60.57511901855469, Learning Rate: 0.01\n",
      "Epoch [696/20000], Loss: 1357.159912109375, Entropy -58.60666275024414, Learning Rate: 0.01\n",
      "Epoch [697/20000], Loss: 1347.0211181640625, Entropy -62.01474380493164, Learning Rate: 0.01\n",
      "Epoch [698/20000], Loss: 1519.2950439453125, Entropy -66.42045593261719, Learning Rate: 0.01\n",
      "Epoch [699/20000], Loss: 1398.15380859375, Entropy -56.14228820800781, Learning Rate: 0.01\n",
      "Epoch [700/20000], Loss: 1354.679931640625, Entropy -63.33623504638672, Learning Rate: 0.01\n",
      "Epoch [701/20000], Loss: 1363.294921875, Entropy -64.0936279296875, Learning Rate: 0.01\n",
      "Epoch [702/20000], Loss: 1426.8095703125, Entropy -52.3500862121582, Learning Rate: 0.01\n",
      "Epoch [703/20000], Loss: 1386.3240966796875, Entropy -67.4951171875, Learning Rate: 0.01\n",
      "Epoch [704/20000], Loss: 1381.7449951171875, Entropy -55.890419006347656, Learning Rate: 0.01\n",
      "Epoch [705/20000], Loss: 1417.085205078125, Entropy -62.78434371948242, Learning Rate: 0.01\n",
      "Epoch [706/20000], Loss: 1382.593017578125, Entropy -57.57374954223633, Learning Rate: 0.01\n",
      "Epoch [707/20000], Loss: 1349.32568359375, Entropy -56.11443328857422, Learning Rate: 0.01\n",
      "Epoch [708/20000], Loss: 1352.74609375, Entropy -44.78037643432617, Learning Rate: 0.01\n",
      "Epoch [709/20000], Loss: 1391.0770263671875, Entropy -65.26607513427734, Learning Rate: 0.01\n",
      "Epoch [710/20000], Loss: 1371.889892578125, Entropy -57.27241134643555, Learning Rate: 0.01\n",
      "Epoch [711/20000], Loss: 1351.3564453125, Entropy -62.16481399536133, Learning Rate: 0.01\n",
      "Epoch [712/20000], Loss: 1436.1402587890625, Entropy -50.370277404785156, Learning Rate: 0.01\n",
      "Epoch [713/20000], Loss: 1347.3594970703125, Entropy -42.25748825073242, Learning Rate: 0.01\n",
      "Epoch [714/20000], Loss: 1456.566650390625, Entropy -50.33815383911133, Learning Rate: 0.01\n",
      "Epoch [715/20000], Loss: 1515.08837890625, Entropy -50.48320388793945, Learning Rate: 0.01\n",
      "Epoch [716/20000], Loss: 1350.7603759765625, Entropy -47.24855041503906, Learning Rate: 0.01\n",
      "Epoch [717/20000], Loss: 1468.053466796875, Entropy -70.92157745361328, Learning Rate: 0.01\n",
      "Epoch [718/20000], Loss: 1360.2772216796875, Entropy -55.94522476196289, Learning Rate: 0.01\n",
      "Epoch [719/20000], Loss: 1517.58203125, Entropy -67.00422668457031, Learning Rate: 0.01\n",
      "Epoch [720/20000], Loss: 1353.5999755859375, Entropy -53.36201095581055, Learning Rate: 0.01\n",
      "Epoch [721/20000], Loss: 1449.8812255859375, Entropy -66.11750793457031, Learning Rate: 0.01\n",
      "Epoch [722/20000], Loss: 1338.9725341796875, Entropy -55.141048431396484, Learning Rate: 0.01\n",
      "Epoch [723/20000], Loss: 1388.1104736328125, Entropy -48.08037185668945, Learning Rate: 0.01\n",
      "Epoch [724/20000], Loss: 1401.947021484375, Entropy -57.417030334472656, Learning Rate: 0.01\n",
      "Epoch [725/20000], Loss: 1475.1810302734375, Entropy -56.5229377746582, Learning Rate: 0.01\n",
      "Epoch [726/20000], Loss: 1471.234130859375, Entropy -52.5860481262207, Learning Rate: 0.01\n",
      "Epoch [727/20000], Loss: 1462.572509765625, Entropy -56.35154342651367, Learning Rate: 0.01\n",
      "Epoch [728/20000], Loss: 1450.049560546875, Entropy -60.74589157104492, Learning Rate: 0.01\n",
      "Epoch [729/20000], Loss: 1467.9678955078125, Entropy -69.9378662109375, Learning Rate: 0.01\n",
      "Epoch [730/20000], Loss: 1419.859375, Entropy -58.74189376831055, Learning Rate: 0.01\n",
      "Epoch [731/20000], Loss: 1417.6270751953125, Entropy -58.72705841064453, Learning Rate: 0.01\n",
      "Epoch [732/20000], Loss: 1381.904541015625, Entropy -55.47962951660156, Learning Rate: 0.01\n",
      "Epoch [733/20000], Loss: 1481.4520263671875, Entropy -55.315834045410156, Learning Rate: 0.01\n",
      "Epoch [734/20000], Loss: 1412.4190673828125, Entropy -55.09140396118164, Learning Rate: 0.01\n",
      "Epoch [735/20000], Loss: 1519.0718994140625, Entropy -74.7647476196289, Learning Rate: 0.01\n",
      "Epoch [736/20000], Loss: 1524.6156005859375, Entropy -70.59640502929688, Learning Rate: 0.01\n",
      "Epoch [737/20000], Loss: 1512.2672119140625, Entropy -52.431175231933594, Learning Rate: 0.01\n",
      "Epoch [738/20000], Loss: 1457.1668701171875, Entropy -60.22414016723633, Learning Rate: 0.01\n",
      "Epoch [739/20000], Loss: 1429.0009765625, Entropy -64.8505859375, Learning Rate: 0.01\n",
      "Epoch [740/20000], Loss: 1501.9749755859375, Entropy -54.77912139892578, Learning Rate: 0.01\n",
      "Epoch [741/20000], Loss: 1504.728271484375, Entropy -53.64909362792969, Learning Rate: 0.01\n",
      "Epoch [742/20000], Loss: 1504.8228759765625, Entropy -56.1247444152832, Learning Rate: 0.01\n",
      "Epoch [743/20000], Loss: 1437.7796630859375, Entropy -59.47648239135742, Learning Rate: 0.01\n",
      "Epoch [744/20000], Loss: 1377.8416748046875, Entropy -36.81459045410156, Learning Rate: 0.01\n",
      "Epoch [745/20000], Loss: 1368.4757080078125, Entropy -36.28605651855469, Learning Rate: 0.01\n",
      "Epoch [746/20000], Loss: 1390.37451171875, Entropy -56.50628662109375, Learning Rate: 0.01\n",
      "Epoch [747/20000], Loss: 1345.8092041015625, Entropy -51.12623596191406, Learning Rate: 0.01\n",
      "Epoch [748/20000], Loss: 1486.4661865234375, Entropy -58.67206954956055, Learning Rate: 0.01\n",
      "Epoch [749/20000], Loss: 1399.721435546875, Entropy -63.925174713134766, Learning Rate: 0.01\n",
      "Epoch [750/20000], Loss: 1449.0361328125, Entropy -52.47123718261719, Learning Rate: 0.01\n",
      "Epoch [751/20000], Loss: 1368.4307861328125, Entropy -55.14610290527344, Learning Rate: 0.01\n",
      "Epoch [752/20000], Loss: 1427.63427734375, Entropy -52.16673278808594, Learning Rate: 0.01\n",
      "Epoch [753/20000], Loss: 1394.0498046875, Entropy -54.389564514160156, Learning Rate: 0.01\n",
      "Epoch [754/20000], Loss: 1429.9306640625, Entropy -62.89048385620117, Learning Rate: 0.01\n",
      "Epoch [755/20000], Loss: 1450.697021484375, Entropy -58.27851104736328, Learning Rate: 0.01\n",
      "Epoch [756/20000], Loss: 1340.1265869140625, Entropy -42.92272186279297, Learning Rate: 0.01\n",
      "Epoch [757/20000], Loss: 1342.466552734375, Entropy -51.16730499267578, Learning Rate: 0.01\n",
      "Epoch [758/20000], Loss: 1406.326904296875, Entropy -53.40823745727539, Learning Rate: 0.01\n",
      "Epoch [759/20000], Loss: 1377.227294921875, Entropy -77.5112075805664, Learning Rate: 0.01\n",
      "Epoch [760/20000], Loss: 1394.7626953125, Entropy -52.45800018310547, Learning Rate: 0.01\n",
      "Epoch [761/20000], Loss: 1471.021484375, Entropy -43.75734329223633, Learning Rate: 0.01\n",
      "Epoch [762/20000], Loss: 1438.4359130859375, Entropy -51.78287887573242, Learning Rate: 0.01\n",
      "Epoch [763/20000], Loss: 1404.0472412109375, Entropy -51.452606201171875, Learning Rate: 0.01\n",
      "Epoch [764/20000], Loss: 1494.793701171875, Entropy -48.48942947387695, Learning Rate: 0.01\n",
      "Epoch [765/20000], Loss: 1430.600341796875, Entropy -59.60751724243164, Learning Rate: 0.01\n",
      "Epoch [766/20000], Loss: 1512.4888916015625, Entropy -51.05807876586914, Learning Rate: 0.01\n",
      "Epoch [767/20000], Loss: 1507.6595458984375, Entropy -36.98533248901367, Learning Rate: 0.01\n",
      "Epoch [768/20000], Loss: 1366.0731201171875, Entropy -49.60776138305664, Learning Rate: 0.01\n",
      "Epoch [769/20000], Loss: 1388.28662109375, Entropy -57.22793197631836, Learning Rate: 0.01\n",
      "Epoch [770/20000], Loss: 1329.5498046875, Entropy -46.40977096557617, Learning Rate: 0.01\n",
      "Epoch [771/20000], Loss: 1361.7684326171875, Entropy -46.2459716796875, Learning Rate: 0.01\n",
      "Epoch [772/20000], Loss: 1378.4918212890625, Entropy -47.80061340332031, Learning Rate: 0.01\n",
      "Epoch [773/20000], Loss: 1429.193115234375, Entropy -59.993408203125, Learning Rate: 0.01\n",
      "Epoch [774/20000], Loss: 1335.8892822265625, Entropy -52.11632537841797, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [775/20000], Loss: 1454.1904296875, Entropy -50.425418853759766, Learning Rate: 0.01\n",
      "Epoch [776/20000], Loss: 1290.65966796875, Entropy -39.566810607910156, Learning Rate: 0.01\n",
      "Epoch [777/20000], Loss: 1342.735595703125, Entropy -43.317996978759766, Learning Rate: 0.01\n",
      "Epoch [778/20000], Loss: 1336.7322998046875, Entropy -62.28507614135742, Learning Rate: 0.01\n",
      "Epoch [779/20000], Loss: 1317.492431640625, Entropy -51.00453186035156, Learning Rate: 0.01\n",
      "Epoch [780/20000], Loss: 1390.4091796875, Entropy -45.05242919921875, Learning Rate: 0.01\n",
      "Epoch [781/20000], Loss: 1385.650390625, Entropy -59.79297637939453, Learning Rate: 0.01\n",
      "Epoch [782/20000], Loss: 1346.2532958984375, Entropy -47.32205581665039, Learning Rate: 0.01\n",
      "Epoch [783/20000], Loss: 1366.3026123046875, Entropy -38.886253356933594, Learning Rate: 0.01\n",
      "Epoch [784/20000], Loss: 1357.0721435546875, Entropy -47.75766372680664, Learning Rate: 0.01\n",
      "Epoch [785/20000], Loss: 1283.4068603515625, Entropy -54.10784149169922, Learning Rate: 0.01\n",
      "Epoch [786/20000], Loss: 1579.9583740234375, Entropy -48.71918487548828, Learning Rate: 0.01\n",
      "Epoch [787/20000], Loss: 1303.6275634765625, Entropy -36.083553314208984, Learning Rate: 0.01\n",
      "Epoch [788/20000], Loss: 1297.6920166015625, Entropy -40.02141571044922, Learning Rate: 0.01\n",
      "Epoch [789/20000], Loss: 1392.37841796875, Entropy -55.52674102783203, Learning Rate: 0.01\n",
      "Epoch [790/20000], Loss: 1340.107421875, Entropy -69.27104949951172, Learning Rate: 0.01\n",
      "Epoch [791/20000], Loss: 1394.9896240234375, Entropy -55.34530258178711, Learning Rate: 0.01\n",
      "Epoch [792/20000], Loss: 1314.932861328125, Entropy -40.22795867919922, Learning Rate: 0.01\n",
      "Epoch [793/20000], Loss: 1306.6304931640625, Entropy -42.56517028808594, Learning Rate: 0.01\n",
      "Epoch [794/20000], Loss: 1398.4461669921875, Entropy -43.39773178100586, Learning Rate: 0.01\n",
      "Epoch [795/20000], Loss: 1289.1412353515625, Entropy -40.242401123046875, Learning Rate: 0.01\n",
      "Epoch [796/20000], Loss: 1365.8427734375, Entropy -38.01976013183594, Learning Rate: 0.01\n",
      "Epoch [797/20000], Loss: 1358.1292724609375, Entropy -52.93727111816406, Learning Rate: 0.01\n",
      "Epoch [798/20000], Loss: 1373.838623046875, Entropy -59.307186126708984, Learning Rate: 0.01\n",
      "Epoch [799/20000], Loss: 1304.9478759765625, Entropy -44.516815185546875, Learning Rate: 0.01\n",
      "Epoch [800/20000], Loss: 1297.8201904296875, Entropy -32.306785583496094, Learning Rate: 0.01\n",
      "Epoch [801/20000], Loss: 1291.616455078125, Entropy -47.43819046020508, Learning Rate: 0.01\n",
      "Epoch [802/20000], Loss: 1396.6241455078125, Entropy -52.905311584472656, Learning Rate: 0.01\n",
      "Epoch [803/20000], Loss: 1339.3858642578125, Entropy -55.562171936035156, Learning Rate: 0.01\n",
      "Epoch [804/20000], Loss: 1387.2867431640625, Entropy -52.45787811279297, Learning Rate: 0.01\n",
      "Epoch [805/20000], Loss: 1365.0054931640625, Entropy -40.3930549621582, Learning Rate: 0.01\n",
      "Epoch [806/20000], Loss: 1304.142822265625, Entropy -43.30679702758789, Learning Rate: 0.01\n",
      "Epoch [807/20000], Loss: 1427.541015625, Entropy -50.27452087402344, Learning Rate: 0.01\n",
      "Epoch [808/20000], Loss: 1487.246337890625, Entropy -44.05728530883789, Learning Rate: 0.01\n",
      "Epoch [809/20000], Loss: 1347.754638671875, Entropy -37.2549934387207, Learning Rate: 0.01\n",
      "Epoch [810/20000], Loss: 1391.3310546875, Entropy -54.986175537109375, Learning Rate: 0.01\n",
      "Epoch [811/20000], Loss: 1412.372802734375, Entropy -39.08797836303711, Learning Rate: 0.01\n",
      "Epoch [812/20000], Loss: 1360.983642578125, Entropy -54.221160888671875, Learning Rate: 0.01\n",
      "Epoch [813/20000], Loss: 1271.7685546875, Entropy -30.540599822998047, Learning Rate: 0.01\n",
      "Epoch [814/20000], Loss: 1342.319091796875, Entropy -39.77396011352539, Learning Rate: 0.01\n",
      "Epoch [815/20000], Loss: 1272.9862060546875, Entropy -35.74698257446289, Learning Rate: 0.01\n",
      "Epoch [816/20000], Loss: 1378.4566650390625, Entropy -61.69859313964844, Learning Rate: 0.01\n",
      "Epoch [817/20000], Loss: 1334.2491455078125, Entropy -46.40481185913086, Learning Rate: 0.01\n",
      "Epoch [818/20000], Loss: 1383.910400390625, Entropy -76.48681640625, Learning Rate: 0.01\n",
      "Epoch [819/20000], Loss: 1337.4095458984375, Entropy -40.50637435913086, Learning Rate: 0.01\n",
      "Epoch [820/20000], Loss: 1404.3519287109375, Entropy -40.06003952026367, Learning Rate: 0.01\n",
      "Epoch [821/20000], Loss: 1308.034423828125, Entropy -46.17628479003906, Learning Rate: 0.01\n",
      "Epoch [822/20000], Loss: 1364.557861328125, Entropy -41.93212127685547, Learning Rate: 0.01\n",
      "Epoch [823/20000], Loss: 1333.2701416015625, Entropy -61.50837326049805, Learning Rate: 0.01\n",
      "Epoch [824/20000], Loss: 1320.427490234375, Entropy -35.26824951171875, Learning Rate: 0.01\n",
      "Epoch [825/20000], Loss: 1350.4530029296875, Entropy -34.05496597290039, Learning Rate: 0.01\n",
      "Epoch [826/20000], Loss: 1325.5823974609375, Entropy -44.69174575805664, Learning Rate: 0.01\n",
      "Epoch [827/20000], Loss: 1422.400634765625, Entropy -60.49872589111328, Learning Rate: 0.01\n",
      "Epoch [828/20000], Loss: 1351.1107177734375, Entropy -33.37736511230469, Learning Rate: 0.01\n",
      "Epoch [829/20000], Loss: 1345.4835205078125, Entropy -50.08223342895508, Learning Rate: 0.01\n",
      "Epoch [830/20000], Loss: 1403.4912109375, Entropy -39.839744567871094, Learning Rate: 0.01\n",
      "Epoch [831/20000], Loss: 1350.451416015625, Entropy -34.99485778808594, Learning Rate: 0.01\n",
      "Epoch [832/20000], Loss: 1337.705322265625, Entropy -50.235755920410156, Learning Rate: 0.01\n",
      "Epoch [833/20000], Loss: 1327.18701171875, Entropy -38.30421829223633, Learning Rate: 0.01\n",
      "Epoch [834/20000], Loss: 1332.4549560546875, Entropy -32.53919982910156, Learning Rate: 0.01\n",
      "Epoch [835/20000], Loss: 1317.586181640625, Entropy -40.1954345703125, Learning Rate: 0.01\n",
      "Epoch [836/20000], Loss: 1414.5911865234375, Entropy -41.597801208496094, Learning Rate: 0.01\n",
      "Epoch [837/20000], Loss: 1388.19775390625, Entropy -35.222373962402344, Learning Rate: 0.01\n",
      "Epoch [838/20000], Loss: 1322.633056640625, Entropy -48.696903228759766, Learning Rate: 0.01\n",
      "Epoch [839/20000], Loss: 1323.988037109375, Entropy -38.057167053222656, Learning Rate: 0.01\n",
      "Epoch [840/20000], Loss: 1401.25390625, Entropy -36.778316497802734, Learning Rate: 0.01\n",
      "Epoch [841/20000], Loss: 1342.0111083984375, Entropy -40.93793869018555, Learning Rate: 0.01\n",
      "Epoch [842/20000], Loss: 1317.014404296875, Entropy -55.57752990722656, Learning Rate: 0.01\n",
      "Epoch [843/20000], Loss: 1389.9600830078125, Entropy -55.66874694824219, Learning Rate: 0.01\n",
      "Epoch [844/20000], Loss: 1345.1385498046875, Entropy -60.46011734008789, Learning Rate: 0.01\n",
      "Epoch [845/20000], Loss: 1321.49072265625, Entropy -36.51917266845703, Learning Rate: 0.01\n",
      "Epoch [846/20000], Loss: 1380.4415283203125, Entropy -34.85294723510742, Learning Rate: 0.01\n",
      "Epoch [847/20000], Loss: 1301.45947265625, Entropy -49.28765869140625, Learning Rate: 0.01\n",
      "Epoch [848/20000], Loss: 1323.899658203125, Entropy -42.916961669921875, Learning Rate: 0.01\n",
      "Epoch [849/20000], Loss: 1375.9068603515625, Entropy -44.1086311340332, Learning Rate: 0.01\n",
      "Epoch [850/20000], Loss: 1288.66064453125, Entropy -40.08414840698242, Learning Rate: 0.01\n",
      "Epoch [851/20000], Loss: 1282.4443359375, Entropy -37.84429168701172, Learning Rate: 0.01\n",
      "Epoch [852/20000], Loss: 1284.675048828125, Entropy -23.61667823791504, Learning Rate: 0.01\n",
      "Epoch [853/20000], Loss: 1364.3060302734375, Entropy -36.84327697753906, Learning Rate: 0.01\n",
      "Epoch [854/20000], Loss: 1350.9063720703125, Entropy -40.90135955810547, Learning Rate: 0.01\n",
      "Epoch [855/20000], Loss: 1317.0113525390625, Entropy -47.31721878051758, Learning Rate: 0.01\n",
      "Epoch [856/20000], Loss: 1384.343505859375, Entropy -39.138755798339844, Learning Rate: 0.01\n",
      "Epoch [857/20000], Loss: 1330.014892578125, Entropy -43.159812927246094, Learning Rate: 0.01\n",
      "Epoch [858/20000], Loss: 1396.005126953125, Entropy -44.14391326904297, Learning Rate: 0.01\n",
      "Epoch [859/20000], Loss: 1288.69091796875, Entropy -24.101099014282227, Learning Rate: 0.01\n",
      "Epoch [860/20000], Loss: 1359.2786865234375, Entropy -34.90141677856445, Learning Rate: 0.01\n",
      "Epoch [861/20000], Loss: 1318.97265625, Entropy -27.740482330322266, Learning Rate: 0.01\n",
      "Epoch [862/20000], Loss: 1368.968017578125, Entropy -28.174259185791016, Learning Rate: 0.01\n",
      "Epoch [863/20000], Loss: 1334.6656494140625, Entropy -56.69352340698242, Learning Rate: 0.01\n",
      "Epoch [864/20000], Loss: 1302.43017578125, Entropy -39.03828048706055, Learning Rate: 0.01\n",
      "Epoch [865/20000], Loss: 1347.304443359375, Entropy -50.87642288208008, Learning Rate: 0.01\n",
      "Epoch [866/20000], Loss: 1333.648681640625, Entropy -30.4827823638916, Learning Rate: 0.01\n",
      "Epoch [867/20000], Loss: 1377.369384765625, Entropy -37.127811431884766, Learning Rate: 0.01\n",
      "Epoch [868/20000], Loss: 1360.9837646484375, Entropy -24.10321807861328, Learning Rate: 0.01\n",
      "Epoch [869/20000], Loss: 1479.3348388671875, Entropy -45.62061309814453, Learning Rate: 0.01\n",
      "Epoch [870/20000], Loss: 1326.037353515625, Entropy -33.13429641723633, Learning Rate: 0.01\n",
      "Epoch [871/20000], Loss: 1455.9368896484375, Entropy -34.51441192626953, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [872/20000], Loss: 1593.036376953125, Entropy -35.41357421875, Learning Rate: 0.01\n",
      "Epoch [873/20000], Loss: 1379.7593994140625, Entropy -48.03203201293945, Learning Rate: 0.01\n",
      "Epoch [874/20000], Loss: 1535.795654296875, Entropy -40.83792495727539, Learning Rate: 0.01\n",
      "Epoch [875/20000], Loss: 1462.859619140625, Entropy -49.84686279296875, Learning Rate: 0.01\n",
      "Epoch [876/20000], Loss: 1349.8603515625, Entropy -24.88416862487793, Learning Rate: 0.01\n",
      "Epoch [877/20000], Loss: 1381.4755859375, Entropy -39.39445495605469, Learning Rate: 0.01\n",
      "Epoch [878/20000], Loss: 1352.5611572265625, Entropy -45.94345474243164, Learning Rate: 0.01\n",
      "Epoch [879/20000], Loss: 1301.677978515625, Entropy -30.90968132019043, Learning Rate: 0.01\n",
      "Epoch [880/20000], Loss: 1339.0465087890625, Entropy -39.00135040283203, Learning Rate: 0.01\n",
      "Epoch [881/20000], Loss: 1359.3443603515625, Entropy -39.70738983154297, Learning Rate: 0.01\n",
      "Epoch [882/20000], Loss: 1344.6221923828125, Entropy -42.90729522705078, Learning Rate: 0.01\n",
      "Epoch [883/20000], Loss: 1322.5927734375, Entropy -52.93609619140625, Learning Rate: 0.01\n",
      "Epoch [884/20000], Loss: 1291.47998046875, Entropy -25.488550186157227, Learning Rate: 0.01\n",
      "Epoch [885/20000], Loss: 1297.9593505859375, Entropy -35.5592041015625, Learning Rate: 0.01\n",
      "Epoch [886/20000], Loss: 1344.588623046875, Entropy -23.270263671875, Learning Rate: 0.01\n",
      "Epoch [887/20000], Loss: 1331.1904296875, Entropy -34.39874267578125, Learning Rate: 0.01\n",
      "Epoch [888/20000], Loss: 1363.9461669921875, Entropy -27.75967788696289, Learning Rate: 0.01\n",
      "Epoch [889/20000], Loss: 1319.6029052734375, Entropy -31.7337646484375, Learning Rate: 0.01\n",
      "Epoch [890/20000], Loss: 1285.3958740234375, Entropy -25.727691650390625, Learning Rate: 0.01\n",
      "Epoch [891/20000], Loss: 1372.776123046875, Entropy -41.17655563354492, Learning Rate: 0.01\n",
      "Epoch [892/20000], Loss: 1336.8302001953125, Entropy -34.34858703613281, Learning Rate: 0.01\n",
      "Epoch [893/20000], Loss: 1301.8538818359375, Entropy -25.9239444732666, Learning Rate: 0.01\n",
      "Epoch [894/20000], Loss: 1335.778564453125, Entropy -43.626914978027344, Learning Rate: 0.01\n",
      "Epoch [895/20000], Loss: 1414.7921142578125, Entropy -5.47833776473999, Learning Rate: 0.01\n",
      "Epoch [896/20000], Loss: 1319.6434326171875, Entropy -28.700328826904297, Learning Rate: 0.01\n",
      "Epoch [897/20000], Loss: 1313.7098388671875, Entropy -40.715049743652344, Learning Rate: 0.01\n",
      "Epoch [898/20000], Loss: 1350.0242919921875, Entropy -45.7121467590332, Learning Rate: 0.01\n",
      "Epoch [899/20000], Loss: 1345.3564453125, Entropy -38.366233825683594, Learning Rate: 0.01\n",
      "Epoch [900/20000], Loss: 1285.17333984375, Entropy -47.82019805908203, Learning Rate: 0.01\n",
      "Epoch [901/20000], Loss: 1429.839599609375, Entropy -27.983686447143555, Learning Rate: 0.01\n",
      "Epoch [902/20000], Loss: 1338.771484375, Entropy -32.6587028503418, Learning Rate: 0.01\n",
      "Epoch [903/20000], Loss: 1365.7572021484375, Entropy -20.075119018554688, Learning Rate: 0.01\n",
      "Epoch [904/20000], Loss: 1381.9200439453125, Entropy -41.13751983642578, Learning Rate: 0.01\n",
      "Epoch [905/20000], Loss: 1351.3028564453125, Entropy -39.16413879394531, Learning Rate: 0.01\n",
      "Epoch [906/20000], Loss: 1315.699951171875, Entropy -44.65665817260742, Learning Rate: 0.01\n",
      "Epoch [907/20000], Loss: 1293.3900146484375, Entropy -27.4260196685791, Learning Rate: 0.01\n",
      "Epoch [908/20000], Loss: 1344.412109375, Entropy -24.09294319152832, Learning Rate: 0.01\n",
      "Epoch [909/20000], Loss: 1362.031494140625, Entropy -54.16266632080078, Learning Rate: 0.01\n",
      "Epoch [910/20000], Loss: 1378.2452392578125, Entropy -23.008203506469727, Learning Rate: 0.01\n",
      "Epoch [911/20000], Loss: 1375.7423095703125, Entropy -26.37982940673828, Learning Rate: 0.01\n",
      "Epoch [912/20000], Loss: 1326.0472412109375, Entropy -34.65271759033203, Learning Rate: 0.01\n",
      "Epoch [913/20000], Loss: 1422.5206298828125, Entropy -24.715770721435547, Learning Rate: 0.01\n",
      "Epoch [914/20000], Loss: 1347.01708984375, Entropy -21.16216278076172, Learning Rate: 0.01\n",
      "Epoch [915/20000], Loss: 1358.206787109375, Entropy -35.477291107177734, Learning Rate: 0.01\n",
      "Epoch [916/20000], Loss: 1333.0999755859375, Entropy -32.333251953125, Learning Rate: 0.01\n",
      "Epoch [917/20000], Loss: 1384.211669921875, Entropy -31.00848960876465, Learning Rate: 0.01\n",
      "Epoch [918/20000], Loss: 1352.59814453125, Entropy -22.673831939697266, Learning Rate: 0.01\n",
      "Epoch [919/20000], Loss: 1420.6236572265625, Entropy -20.82946014404297, Learning Rate: 0.01\n",
      "Epoch [920/20000], Loss: 1372.3499755859375, Entropy -35.252227783203125, Learning Rate: 0.01\n",
      "Epoch [921/20000], Loss: 1290.2176513671875, Entropy -31.3070068359375, Learning Rate: 0.01\n",
      "Epoch [922/20000], Loss: 1424.5201416015625, Entropy -9.97268295288086, Learning Rate: 0.01\n",
      "Epoch [923/20000], Loss: 1315.8125, Entropy -24.866361618041992, Learning Rate: 0.01\n",
      "Epoch [924/20000], Loss: 1355.627197265625, Entropy -16.72024917602539, Learning Rate: 0.01\n",
      "Epoch [925/20000], Loss: 1326.6646728515625, Entropy -29.39767837524414, Learning Rate: 0.01\n",
      "Epoch [926/20000], Loss: 1320.1282958984375, Entropy -12.830729484558105, Learning Rate: 0.01\n",
      "Epoch [927/20000], Loss: 1314.46630859375, Entropy -36.8755989074707, Learning Rate: 0.01\n",
      "Epoch [928/20000], Loss: 1340.442626953125, Entropy -22.261049270629883, Learning Rate: 0.01\n",
      "Epoch [929/20000], Loss: 1356.479248046875, Entropy -29.65146827697754, Learning Rate: 0.01\n",
      "Epoch [930/20000], Loss: 1385.9202880859375, Entropy -44.24502944946289, Learning Rate: 0.01\n",
      "Epoch [931/20000], Loss: 1288.826904296875, Entropy -17.64083480834961, Learning Rate: 0.01\n",
      "Epoch [932/20000], Loss: 1282.564453125, Entropy -32.9199104309082, Learning Rate: 0.01\n",
      "Epoch [933/20000], Loss: 1265.528076171875, Entropy -22.246288299560547, Learning Rate: 0.01\n",
      "Epoch [934/20000], Loss: 1274.6849365234375, Entropy -11.293431282043457, Learning Rate: 0.01\n",
      "Epoch [935/20000], Loss: 1349.9862060546875, Entropy -22.997787475585938, Learning Rate: 0.01\n",
      "Epoch [936/20000], Loss: 1295.9947509765625, Entropy -6.634463787078857, Learning Rate: 0.01\n",
      "Epoch [937/20000], Loss: 1304.1070556640625, Entropy -18.90044403076172, Learning Rate: 0.01\n",
      "Epoch [938/20000], Loss: 1344.6947021484375, Entropy -20.00501823425293, Learning Rate: 0.01\n",
      "Epoch [939/20000], Loss: 1339.8035888671875, Entropy -25.39875602722168, Learning Rate: 0.01\n",
      "Epoch [940/20000], Loss: 1300.8316650390625, Entropy 4.242025852203369, Learning Rate: 0.01\n",
      "Epoch [941/20000], Loss: 1363.8463134765625, Entropy -24.540956497192383, Learning Rate: 0.01\n",
      "Epoch [942/20000], Loss: 1333.958740234375, Entropy -43.94816207885742, Learning Rate: 0.01\n",
      "Epoch [943/20000], Loss: 1358.453369140625, Entropy -35.87247848510742, Learning Rate: 0.01\n",
      "Epoch [944/20000], Loss: 1417.4351806640625, Entropy -33.52805709838867, Learning Rate: 0.01\n",
      "Epoch [945/20000], Loss: 1301.5745849609375, Entropy -10.981049537658691, Learning Rate: 0.01\n",
      "Epoch [946/20000], Loss: 1344.539794921875, Entropy -19.202985763549805, Learning Rate: 0.01\n",
      "Epoch [947/20000], Loss: 1274.191650390625, Entropy -23.294321060180664, Learning Rate: 0.01\n",
      "Epoch [948/20000], Loss: 1345.457763671875, Entropy -33.39073181152344, Learning Rate: 0.01\n",
      "Epoch [949/20000], Loss: 1279.124755859375, Entropy -31.129972457885742, Learning Rate: 0.01\n",
      "Epoch [950/20000], Loss: 1298.674560546875, Entropy -2.1884939670562744, Learning Rate: 0.01\n",
      "Epoch [951/20000], Loss: 1317.8929443359375, Entropy -19.89342498779297, Learning Rate: 0.01\n",
      "Epoch [952/20000], Loss: 1408.9112548828125, Entropy -30.941606521606445, Learning Rate: 0.01\n",
      "Epoch [953/20000], Loss: 1274.890869140625, Entropy -10.301861763000488, Learning Rate: 0.01\n",
      "Epoch [954/20000], Loss: 1400.397705078125, Entropy -14.18380355834961, Learning Rate: 0.01\n",
      "Epoch [955/20000], Loss: 1628.1302490234375, Entropy -9.912860870361328, Learning Rate: 0.01\n",
      "Epoch [956/20000], Loss: 1444.970703125, Entropy -29.036415100097656, Learning Rate: 0.01\n",
      "Epoch [957/20000], Loss: 1343.957763671875, Entropy -31.915950775146484, Learning Rate: 0.01\n",
      "Epoch [958/20000], Loss: 1584.458251953125, Entropy -20.758527755737305, Learning Rate: 0.01\n",
      "Epoch [959/20000], Loss: 1419.8486328125, Entropy -27.503793716430664, Learning Rate: 0.01\n",
      "Epoch [960/20000], Loss: 1337.9619140625, Entropy -21.402538299560547, Learning Rate: 0.01\n",
      "Epoch [961/20000], Loss: 1391.3131103515625, Entropy -19.31519317626953, Learning Rate: 0.01\n",
      "Epoch [962/20000], Loss: 1339.8572998046875, Entropy -15.754203796386719, Learning Rate: 0.01\n",
      "Epoch [963/20000], Loss: 1359.6416015625, Entropy -26.583463668823242, Learning Rate: 0.01\n",
      "Epoch [964/20000], Loss: 1334.4344482421875, Entropy -3.805469274520874, Learning Rate: 0.01\n",
      "Epoch [965/20000], Loss: 1316.4639892578125, Entropy -36.7081298828125, Learning Rate: 0.01\n",
      "Epoch [966/20000], Loss: 1335.3873291015625, Entropy -25.62657928466797, Learning Rate: 0.01\n",
      "Epoch [967/20000], Loss: 1308.865234375, Entropy -21.921091079711914, Learning Rate: 0.01\n",
      "Epoch [968/20000], Loss: 1330.21923828125, Entropy -27.987010955810547, Learning Rate: 0.01\n",
      "Epoch [969/20000], Loss: 1345.885986328125, Entropy -36.16289520263672, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [970/20000], Loss: 1292.5989990234375, Entropy -17.29062843322754, Learning Rate: 0.01\n",
      "Epoch [971/20000], Loss: 1383.2987060546875, Entropy -5.157846450805664, Learning Rate: 0.01\n",
      "Epoch [972/20000], Loss: 1283.3228759765625, Entropy -22.03423500061035, Learning Rate: 0.01\n",
      "Epoch [973/20000], Loss: 1354.5150146484375, Entropy -8.935020446777344, Learning Rate: 0.01\n",
      "Epoch [974/20000], Loss: 1311.238037109375, Entropy -12.417266845703125, Learning Rate: 0.01\n",
      "Epoch [975/20000], Loss: 1340.7476806640625, Entropy -24.04781723022461, Learning Rate: 0.01\n",
      "Epoch [976/20000], Loss: 1535.5989990234375, Entropy -22.941192626953125, Learning Rate: 0.01\n",
      "Epoch [977/20000], Loss: 1320.396240234375, Entropy -23.37360382080078, Learning Rate: 0.01\n",
      "Epoch [978/20000], Loss: 1418.23291015625, Entropy -19.967634201049805, Learning Rate: 0.01\n",
      "Epoch [979/20000], Loss: 1346.5181884765625, Entropy -10.91653060913086, Learning Rate: 0.01\n",
      "Epoch [980/20000], Loss: 1587.5042724609375, Entropy -22.794755935668945, Learning Rate: 0.01\n",
      "Epoch [981/20000], Loss: 1398.0836181640625, Entropy -49.307899475097656, Learning Rate: 0.01\n",
      "Epoch [982/20000], Loss: 1424.3887939453125, Entropy -17.569896697998047, Learning Rate: 0.01\n",
      "Epoch [983/20000], Loss: 1403.2091064453125, Entropy -25.615739822387695, Learning Rate: 0.01\n",
      "Epoch [984/20000], Loss: 1288.0750732421875, Entropy -21.595672607421875, Learning Rate: 0.01\n",
      "Epoch [985/20000], Loss: 1430.5987548828125, Entropy -23.460107803344727, Learning Rate: 0.01\n",
      "Epoch [986/20000], Loss: 1393.48876953125, Entropy -10.434741020202637, Learning Rate: 0.01\n",
      "Epoch [987/20000], Loss: 1479.513671875, Entropy -23.43832015991211, Learning Rate: 0.01\n",
      "Epoch [988/20000], Loss: 1429.7650146484375, Entropy -16.844100952148438, Learning Rate: 0.01\n",
      "Epoch [989/20000], Loss: 1377.6947021484375, Entropy -28.779052734375, Learning Rate: 0.01\n",
      "Epoch [990/20000], Loss: 1467.1358642578125, Entropy -44.58340072631836, Learning Rate: 0.01\n",
      "Epoch [991/20000], Loss: 1303.70751953125, Entropy -17.657604217529297, Learning Rate: 0.01\n",
      "Epoch [992/20000], Loss: 1569.035400390625, Entropy -37.033809661865234, Learning Rate: 0.01\n",
      "Epoch [993/20000], Loss: 1459.8604736328125, Entropy -2.8914904594421387, Learning Rate: 0.01\n",
      "Epoch [994/20000], Loss: 1376.8751220703125, Entropy -37.99728012084961, Learning Rate: 0.01\n",
      "Epoch [995/20000], Loss: 1407.3720703125, Entropy -14.92890453338623, Learning Rate: 0.01\n",
      "Epoch [996/20000], Loss: 1323.755615234375, Entropy -9.495556831359863, Learning Rate: 0.01\n",
      "Epoch [997/20000], Loss: 1321.587158203125, Entropy 11.849308967590332, Learning Rate: 0.01\n",
      "Epoch [998/20000], Loss: 1266.2752685546875, Entropy -20.261295318603516, Learning Rate: 0.01\n",
      "Epoch [999/20000], Loss: 1374.0775146484375, Entropy -5.247775077819824, Learning Rate: 0.01\n",
      "Epoch [1000/20000], Loss: 1329.05078125, Entropy -25.459718704223633, Learning Rate: 0.01\n",
      "Epoch [1001/20000], Loss: 1349.1624755859375, Entropy -7.07072114944458, Learning Rate: 0.01\n",
      "Epoch [1002/20000], Loss: 1301.1202392578125, Entropy -15.409876823425293, Learning Rate: 0.01\n",
      "Epoch [1003/20000], Loss: 1322.4300537109375, Entropy -22.400054931640625, Learning Rate: 0.01\n",
      "Epoch [1004/20000], Loss: 1302.5948486328125, Entropy -21.71472930908203, Learning Rate: 0.01\n",
      "Epoch [1005/20000], Loss: 1278.58642578125, Entropy -14.56884765625, Learning Rate: 0.01\n",
      "Epoch [1006/20000], Loss: 1379.290283203125, Entropy -38.293434143066406, Learning Rate: 0.01\n",
      "Epoch [1007/20000], Loss: 1316.77587890625, Entropy -8.248343467712402, Learning Rate: 0.01\n",
      "Epoch [1008/20000], Loss: 1263.519775390625, Entropy -15.243964195251465, Learning Rate: 0.01\n",
      "Epoch [1009/20000], Loss: 1289.966064453125, Entropy -1.987765908241272, Learning Rate: 0.01\n",
      "Epoch [1010/20000], Loss: 1259.2225341796875, Entropy -4.6915130615234375, Learning Rate: 0.01\n",
      "Epoch [1011/20000], Loss: 1295.0179443359375, Entropy -8.198508262634277, Learning Rate: 0.01\n",
      "Epoch [1012/20000], Loss: 1325.6483154296875, Entropy -17.278705596923828, Learning Rate: 0.01\n",
      "Epoch [1013/20000], Loss: 1325.210693359375, Entropy -25.16843032836914, Learning Rate: 0.01\n",
      "Epoch [1014/20000], Loss: 1401.2567138671875, Entropy -7.308078289031982, Learning Rate: 0.01\n",
      "Epoch [1015/20000], Loss: 1335.59375, Entropy -22.270946502685547, Learning Rate: 0.01\n",
      "Epoch [1016/20000], Loss: 1403.1806640625, Entropy -47.7822151184082, Learning Rate: 0.01\n",
      "Epoch [1017/20000], Loss: 1359.8419189453125, Entropy -30.310436248779297, Learning Rate: 0.01\n",
      "Epoch [1018/20000], Loss: 1259.7529296875, Entropy -12.294625282287598, Learning Rate: 0.01\n",
      "Epoch [1019/20000], Loss: 1348.1478271484375, Entropy -8.246603965759277, Learning Rate: 0.01\n",
      "Epoch [1020/20000], Loss: 1297.420166015625, Entropy -21.270442962646484, Learning Rate: 0.01\n",
      "Epoch [1021/20000], Loss: 1295.149658203125, Entropy -14.92577838897705, Learning Rate: 0.01\n",
      "Epoch [1022/20000], Loss: 1278.7120361328125, Entropy -8.582684516906738, Learning Rate: 0.01\n",
      "Epoch [1023/20000], Loss: 1282.121337890625, Entropy -23.685503005981445, Learning Rate: 0.01\n",
      "Epoch [1024/20000], Loss: 1298.9986572265625, Entropy -12.902172088623047, Learning Rate: 0.01\n",
      "Epoch [1025/20000], Loss: 1340.8687744140625, Entropy -20.381425857543945, Learning Rate: 0.01\n",
      "Epoch [1026/20000], Loss: 1288.3787841796875, Entropy -1.6643602848052979, Learning Rate: 0.01\n",
      "Epoch [1027/20000], Loss: 1240.9676513671875, Entropy -4.4951934814453125, Learning Rate: 0.01\n",
      "Epoch [1028/20000], Loss: 1311.8416748046875, Entropy -16.673173904418945, Learning Rate: 0.01\n",
      "Epoch [1029/20000], Loss: 1262.3746337890625, Entropy -20.678878784179688, Learning Rate: 0.01\n",
      "Epoch [1030/20000], Loss: 1353.1187744140625, Entropy -16.666059494018555, Learning Rate: 0.01\n",
      "Epoch [1031/20000], Loss: 1331.33740234375, Entropy -22.28769302368164, Learning Rate: 0.01\n",
      "Epoch [1032/20000], Loss: 1309.6866455078125, Entropy -17.139835357666016, Learning Rate: 0.01\n",
      "Epoch [1033/20000], Loss: 1250.3619384765625, Entropy -21.28377342224121, Learning Rate: 0.01\n",
      "Epoch [1034/20000], Loss: 1335.1590576171875, Entropy -21.039052963256836, Learning Rate: 0.01\n",
      "Epoch [1035/20000], Loss: 1232.1385498046875, Entropy -25.016435623168945, Learning Rate: 0.01\n",
      "Epoch [1036/20000], Loss: 1300.4085693359375, Entropy 3.295369863510132, Learning Rate: 0.01\n",
      "Epoch [1037/20000], Loss: 1310.914794921875, Entropy -22.544950485229492, Learning Rate: 0.01\n",
      "Epoch [1038/20000], Loss: 1343.7236328125, Entropy -18.087976455688477, Learning Rate: 0.01\n",
      "Epoch [1039/20000], Loss: 1237.337890625, Entropy -7.185970306396484, Learning Rate: 0.01\n",
      "Epoch [1040/20000], Loss: 1275.6376953125, Entropy -23.019779205322266, Learning Rate: 0.01\n",
      "Epoch [1041/20000], Loss: 1317.4912109375, Entropy -16.890289306640625, Learning Rate: 0.01\n",
      "Epoch [1042/20000], Loss: 1320.48779296875, Entropy -13.914551734924316, Learning Rate: 0.01\n",
      "Epoch [1043/20000], Loss: 1239.4454345703125, Entropy -7.250430583953857, Learning Rate: 0.01\n",
      "Epoch [1044/20000], Loss: 1428.7867431640625, Entropy -23.270814895629883, Learning Rate: 0.01\n",
      "Epoch [1045/20000], Loss: 1316.172119140625, Entropy -31.750072479248047, Learning Rate: 0.01\n",
      "Epoch [1046/20000], Loss: 1300.2308349609375, Entropy -7.873848915100098, Learning Rate: 0.01\n",
      "Epoch [1047/20000], Loss: 1320.292236328125, Entropy -3.5750298500061035, Learning Rate: 0.01\n",
      "Epoch [1048/20000], Loss: 1284.733642578125, Entropy -25.620134353637695, Learning Rate: 0.01\n",
      "Epoch [1049/20000], Loss: 1330.8145751953125, Entropy -19.326141357421875, Learning Rate: 0.01\n",
      "Epoch [1050/20000], Loss: 1291.7279052734375, Entropy -1.9217556715011597, Learning Rate: 0.01\n",
      "Epoch [1051/20000], Loss: 1373.8096923828125, Entropy -1.118789792060852, Learning Rate: 0.01\n",
      "Epoch [1052/20000], Loss: 1416.872802734375, Entropy 2.090566635131836, Learning Rate: 0.01\n",
      "Epoch [1053/20000], Loss: 1299.5189208984375, Entropy -2.3191399574279785, Learning Rate: 0.01\n",
      "Epoch [1054/20000], Loss: 1396.8013916015625, Entropy -11.456356048583984, Learning Rate: 0.01\n",
      "Epoch [1055/20000], Loss: 1383.800048828125, Entropy -15.738571166992188, Learning Rate: 0.01\n",
      "Epoch [1056/20000], Loss: 1473.6746826171875, Entropy -24.499155044555664, Learning Rate: 0.01\n",
      "Epoch [1057/20000], Loss: 1267.522705078125, Entropy -3.301978349685669, Learning Rate: 0.01\n",
      "Epoch [1058/20000], Loss: 1414.2301025390625, Entropy -6.182959079742432, Learning Rate: 0.01\n",
      "Epoch [1059/20000], Loss: 1314.3577880859375, Entropy -1.844063639640808, Learning Rate: 0.01\n",
      "Epoch [1060/20000], Loss: 1406.189208984375, Entropy -13.740209579467773, Learning Rate: 0.01\n",
      "Epoch [1061/20000], Loss: 1415.61083984375, Entropy -13.227992057800293, Learning Rate: 0.01\n",
      "Epoch [1062/20000], Loss: 1336.990234375, Entropy -20.476545333862305, Learning Rate: 0.01\n",
      "Epoch [1063/20000], Loss: 1396.5042724609375, Entropy -6.941751480102539, Learning Rate: 0.01\n",
      "Epoch [1064/20000], Loss: 1416.4371337890625, Entropy 11.0142822265625, Learning Rate: 0.01\n",
      "Epoch [1065/20000], Loss: 1956.6368408203125, Entropy -8.134440422058105, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1066/20000], Loss: 1526.9034423828125, Entropy -4.814748287200928, Learning Rate: 0.01\n",
      "Epoch [1067/20000], Loss: 2280.943359375, Entropy -9.88473129272461, Learning Rate: 0.01\n",
      "Epoch [1068/20000], Loss: 1367.459228515625, Entropy -11.128096580505371, Learning Rate: 0.01\n",
      "Epoch [1069/20000], Loss: 1925.586181640625, Entropy -11.601227760314941, Learning Rate: 0.01\n",
      "Epoch [1070/20000], Loss: 1523.6329345703125, Entropy -16.686384201049805, Learning Rate: 0.01\n",
      "Epoch [1071/20000], Loss: 1646.32666015625, Entropy -12.009191513061523, Learning Rate: 0.01\n",
      "Epoch [1072/20000], Loss: 1409.51953125, Entropy -0.49360498785972595, Learning Rate: 0.01\n",
      "Epoch [1073/20000], Loss: 1833.275634765625, Entropy -20.562265396118164, Learning Rate: 0.01\n",
      "Epoch [1074/20000], Loss: 1592.363037109375, Entropy -10.7252779006958, Learning Rate: 0.01\n",
      "Epoch [1075/20000], Loss: 1446.2410888671875, Entropy -14.578701972961426, Learning Rate: 0.01\n",
      "Epoch [1076/20000], Loss: 1716.265380859375, Entropy 5.051149368286133, Learning Rate: 0.01\n",
      "Epoch [1077/20000], Loss: 1721.5250244140625, Entropy -0.35345569252967834, Learning Rate: 0.01\n",
      "Epoch [1078/20000], Loss: 2007.0196533203125, Entropy -1.4283089637756348, Learning Rate: 0.01\n",
      "Epoch [1079/20000], Loss: 1401.57421875, Entropy -10.9580078125, Learning Rate: 0.01\n",
      "Epoch [1080/20000], Loss: 1828.8157958984375, Entropy -13.85584831237793, Learning Rate: 0.01\n",
      "Epoch [1081/20000], Loss: 1574.6483154296875, Entropy -22.40996551513672, Learning Rate: 0.01\n",
      "Epoch [1082/20000], Loss: 1507.6328125, Entropy 1.8214880228042603, Learning Rate: 0.01\n",
      "Epoch [1083/20000], Loss: 1435.0718994140625, Entropy -20.104713439941406, Learning Rate: 0.01\n",
      "Epoch [1084/20000], Loss: 1408.6063232421875, Entropy -20.462833404541016, Learning Rate: 0.01\n",
      "Epoch [1085/20000], Loss: 1336.1700439453125, Entropy -4.660423278808594, Learning Rate: 0.01\n",
      "Epoch [1086/20000], Loss: 1452.96533203125, Entropy -12.272558212280273, Learning Rate: 0.01\n",
      "Epoch [1087/20000], Loss: 1565.0517578125, Entropy -1.2659610509872437, Learning Rate: 0.01\n",
      "Epoch [1088/20000], Loss: 1468.3143310546875, Entropy -21.031856536865234, Learning Rate: 0.01\n",
      "Epoch [1089/20000], Loss: 1672.6236572265625, Entropy -8.919862747192383, Learning Rate: 0.01\n",
      "Epoch [1090/20000], Loss: 1354.2236328125, Entropy -4.707292556762695, Learning Rate: 0.01\n",
      "Epoch [1091/20000], Loss: 1519.08203125, Entropy -13.253090858459473, Learning Rate: 0.01\n",
      "Epoch [1092/20000], Loss: 1428.905517578125, Entropy -8.590776443481445, Learning Rate: 0.01\n",
      "Epoch [1093/20000], Loss: 1477.5172119140625, Entropy -24.345659255981445, Learning Rate: 0.01\n",
      "Epoch [1094/20000], Loss: 1459.1815185546875, Entropy -13.162175178527832, Learning Rate: 0.01\n",
      "Epoch [1095/20000], Loss: 1376.490234375, Entropy -6.984323501586914, Learning Rate: 0.01\n",
      "Epoch [1096/20000], Loss: 1549.3851318359375, Entropy -20.174415588378906, Learning Rate: 0.01\n",
      "Epoch [1097/20000], Loss: 1399.750732421875, Entropy -13.631994247436523, Learning Rate: 0.01\n",
      "Epoch [1098/20000], Loss: 1473.801513671875, Entropy -17.954687118530273, Learning Rate: 0.01\n",
      "Epoch [1099/20000], Loss: 1388.9912109375, Entropy -11.184069633483887, Learning Rate: 0.01\n",
      "Epoch [1100/20000], Loss: 1370.84326171875, Entropy -15.42556381225586, Learning Rate: 0.01\n",
      "Epoch [1101/20000], Loss: 1398.1197509765625, Entropy 1.2122410535812378, Learning Rate: 0.01\n",
      "Epoch [1102/20000], Loss: 1478.8494873046875, Entropy -17.661317825317383, Learning Rate: 0.01\n",
      "Epoch [1103/20000], Loss: 1383.095458984375, Entropy -19.360889434814453, Learning Rate: 0.01\n",
      "Epoch [1104/20000], Loss: 1326.4879150390625, Entropy 2.301384449005127, Learning Rate: 0.01\n",
      "Epoch [1105/20000], Loss: 1387.9326171875, Entropy -17.4782657623291, Learning Rate: 0.01\n",
      "Epoch [1106/20000], Loss: 1436.5699462890625, Entropy -33.236820220947266, Learning Rate: 0.01\n",
      "Epoch [1107/20000], Loss: 1399.0517578125, Entropy -9.793180465698242, Learning Rate: 0.01\n",
      "Epoch [1108/20000], Loss: 1398.736083984375, Entropy -7.922544956207275, Learning Rate: 0.01\n",
      "Epoch [1109/20000], Loss: 1365.101318359375, Entropy 2.0908257961273193, Learning Rate: 0.01\n",
      "Epoch [1110/20000], Loss: 1344.072998046875, Entropy 14.931554794311523, Learning Rate: 0.01\n",
      "Epoch [1111/20000], Loss: 1441.9822998046875, Entropy -2.6051504611968994, Learning Rate: 0.01\n",
      "Epoch [1112/20000], Loss: 1323.4366455078125, Entropy -20.305131912231445, Learning Rate: 0.01\n",
      "Epoch [1113/20000], Loss: 1322.100341796875, Entropy -2.3850576877593994, Learning Rate: 0.01\n",
      "Epoch [1114/20000], Loss: 1320.0159912109375, Entropy -0.32022902369499207, Learning Rate: 0.01\n",
      "Epoch [1115/20000], Loss: 1370.57373046875, Entropy -17.95414161682129, Learning Rate: 0.01\n",
      "Epoch [1116/20000], Loss: 1337.8441162109375, Entropy -11.95073413848877, Learning Rate: 0.01\n",
      "Epoch [1117/20000], Loss: 1261.5989990234375, Entropy -0.7227100729942322, Learning Rate: 0.01\n",
      "Epoch [1118/20000], Loss: 1384.4512939453125, Entropy -20.058216094970703, Learning Rate: 0.01\n",
      "Epoch [1119/20000], Loss: 1282.925537109375, Entropy 2.562323570251465, Learning Rate: 0.01\n",
      "Epoch [1120/20000], Loss: 1396.3675537109375, Entropy -7.468542098999023, Learning Rate: 0.01\n",
      "Epoch [1121/20000], Loss: 1355.4105224609375, Entropy 3.177032470703125, Learning Rate: 0.01\n",
      "Epoch [1122/20000], Loss: 1418.3751220703125, Entropy -25.977340698242188, Learning Rate: 0.01\n",
      "Epoch [1123/20000], Loss: 1287.7802734375, Entropy -9.274105072021484, Learning Rate: 0.01\n",
      "Epoch [1124/20000], Loss: 1378.903076171875, Entropy -1.9268158674240112, Learning Rate: 0.01\n",
      "Epoch [1125/20000], Loss: 1286.0302734375, Entropy -10.973820686340332, Learning Rate: 0.01\n",
      "Epoch [1126/20000], Loss: 1337.9156494140625, Entropy -25.061176300048828, Learning Rate: 0.01\n",
      "Epoch [1127/20000], Loss: 1279.974609375, Entropy -4.560556411743164, Learning Rate: 0.01\n",
      "Epoch [1128/20000], Loss: 1351.0693359375, Entropy -5.82078742980957, Learning Rate: 0.01\n",
      "Epoch [1129/20000], Loss: 1234.0262451171875, Entropy -0.7977177500724792, Learning Rate: 0.01\n",
      "Epoch [1130/20000], Loss: 1291.392822265625, Entropy -21.26845932006836, Learning Rate: 0.01\n",
      "Epoch [1131/20000], Loss: 1245.5911865234375, Entropy 13.820589065551758, Learning Rate: 0.01\n",
      "Epoch [1132/20000], Loss: 1359.9195556640625, Entropy -19.419946670532227, Learning Rate: 0.01\n",
      "Epoch [1133/20000], Loss: 1314.2843017578125, Entropy 10.432863235473633, Learning Rate: 0.01\n",
      "Epoch [1134/20000], Loss: 1312.9844970703125, Entropy -9.54079818725586, Learning Rate: 0.01\n",
      "Epoch [1135/20000], Loss: 1325.810791015625, Entropy -17.191997528076172, Learning Rate: 0.01\n",
      "Epoch [1136/20000], Loss: 1319.478759765625, Entropy -18.905193328857422, Learning Rate: 0.01\n",
      "Epoch [1137/20000], Loss: 1266.583251953125, Entropy 8.948101997375488, Learning Rate: 0.01\n",
      "Epoch [1138/20000], Loss: 1234.9833984375, Entropy -3.0960638523101807, Learning Rate: 0.01\n",
      "Epoch [1139/20000], Loss: 1308.71533203125, Entropy -3.7715518474578857, Learning Rate: 0.01\n",
      "Epoch [1140/20000], Loss: 1237.0875244140625, Entropy -1.9694560766220093, Learning Rate: 0.01\n",
      "Epoch [1141/20000], Loss: 1263.0858154296875, Entropy -8.482537269592285, Learning Rate: 0.01\n",
      "Epoch [1142/20000], Loss: 1231.748046875, Entropy 1.8646917343139648, Learning Rate: 0.01\n",
      "Epoch [1143/20000], Loss: 1227.0274658203125, Entropy 0.485360324382782, Learning Rate: 0.01\n",
      "Epoch [1144/20000], Loss: 1236.896728515625, Entropy -8.811690330505371, Learning Rate: 0.01\n",
      "Epoch [1145/20000], Loss: 1244.18115234375, Entropy -13.07122802734375, Learning Rate: 0.01\n",
      "Epoch [1146/20000], Loss: 1241.6470947265625, Entropy 13.696670532226562, Learning Rate: 0.01\n",
      "Epoch [1147/20000], Loss: 1273.3968505859375, Entropy -3.1257355213165283, Learning Rate: 0.01\n",
      "Epoch [1148/20000], Loss: 1285.0654296875, Entropy -6.793698787689209, Learning Rate: 0.01\n",
      "Epoch [1149/20000], Loss: 1337.37060546875, Entropy -5.968801975250244, Learning Rate: 0.01\n",
      "Epoch [1150/20000], Loss: 1260.457763671875, Entropy -28.22396469116211, Learning Rate: 0.01\n",
      "Epoch [1151/20000], Loss: 1230.117919921875, Entropy -1.648073434829712, Learning Rate: 0.01\n",
      "Epoch [1152/20000], Loss: 1237.9755859375, Entropy -4.608992099761963, Learning Rate: 0.01\n",
      "Epoch [1153/20000], Loss: 1254.0430908203125, Entropy 13.828713417053223, Learning Rate: 0.01\n",
      "Epoch [1154/20000], Loss: 1255.096923828125, Entropy -6.078210830688477, Learning Rate: 0.01\n",
      "Epoch [1155/20000], Loss: 1300.388427734375, Entropy 10.059788703918457, Learning Rate: 0.01\n",
      "Epoch [1156/20000], Loss: 1248.9940185546875, Entropy 19.66106414794922, Learning Rate: 0.01\n",
      "Epoch [1157/20000], Loss: 1262.6505126953125, Entropy 0.8118090033531189, Learning Rate: 0.01\n",
      "Epoch [1158/20000], Loss: 1292.01904296875, Entropy -7.911032676696777, Learning Rate: 0.01\n",
      "Epoch [1159/20000], Loss: 1232.4346923828125, Entropy 4.145637512207031, Learning Rate: 0.01\n",
      "Epoch [1160/20000], Loss: 1310.1131591796875, Entropy -18.951862335205078, Learning Rate: 0.01\n",
      "Epoch [1161/20000], Loss: 1219.288330078125, Entropy 3.048074245452881, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1162/20000], Loss: 1293.646240234375, Entropy -10.528366088867188, Learning Rate: 0.01\n",
      "Epoch [1163/20000], Loss: 1250.1708984375, Entropy -6.452404022216797, Learning Rate: 0.01\n",
      "Epoch [1164/20000], Loss: 1225.200927734375, Entropy -6.874165058135986, Learning Rate: 0.01\n",
      "Epoch [1165/20000], Loss: 1221.3282470703125, Entropy 7.203156471252441, Learning Rate: 0.01\n",
      "Epoch [1166/20000], Loss: 1235.6220703125, Entropy 4.336738109588623, Learning Rate: 0.01\n",
      "Epoch [1167/20000], Loss: 1300.96435546875, Entropy -10.210867881774902, Learning Rate: 0.01\n",
      "Epoch [1168/20000], Loss: 1298.1724853515625, Entropy -15.12376594543457, Learning Rate: 0.01\n",
      "Epoch [1169/20000], Loss: 1288.1019287109375, Entropy -7.296022891998291, Learning Rate: 0.01\n",
      "Epoch [1170/20000], Loss: 1241.95458984375, Entropy 8.488327026367188, Learning Rate: 0.01\n",
      "Epoch [1171/20000], Loss: 1366.618896484375, Entropy -10.190180778503418, Learning Rate: 0.01\n",
      "Epoch [1172/20000], Loss: 1253.984375, Entropy 2.380811929702759, Learning Rate: 0.01\n",
      "Epoch [1173/20000], Loss: 1248.9930419921875, Entropy 2.431718587875366, Learning Rate: 0.01\n",
      "Epoch [1174/20000], Loss: 1228.96533203125, Entropy 1.9779088497161865, Learning Rate: 0.01\n",
      "Epoch [1175/20000], Loss: 1236.6934814453125, Entropy -3.44161057472229, Learning Rate: 0.01\n",
      "Epoch [1176/20000], Loss: 1234.7557373046875, Entropy 1.073256254196167, Learning Rate: 0.01\n",
      "Epoch [1177/20000], Loss: 1266.6171875, Entropy -4.073398113250732, Learning Rate: 0.01\n",
      "Epoch [1178/20000], Loss: 1334.3502197265625, Entropy -15.842143058776855, Learning Rate: 0.01\n",
      "Epoch [1179/20000], Loss: 1224.2071533203125, Entropy 9.154837608337402, Learning Rate: 0.01\n",
      "Epoch [1180/20000], Loss: 1290.8447265625, Entropy 8.842363357543945, Learning Rate: 0.01\n",
      "Epoch [1181/20000], Loss: 1221.1058349609375, Entropy -2.0550830364227295, Learning Rate: 0.01\n",
      "Epoch [1182/20000], Loss: 1274.63232421875, Entropy -12.813695907592773, Learning Rate: 0.01\n",
      "Epoch [1183/20000], Loss: 1219.0113525390625, Entropy 0.9605602025985718, Learning Rate: 0.01\n",
      "Epoch [1184/20000], Loss: 1253.80078125, Entropy -4.335974216461182, Learning Rate: 0.01\n",
      "Epoch [1185/20000], Loss: 1213.1361083984375, Entropy 1.4796892404556274, Learning Rate: 0.01\n",
      "Epoch [1186/20000], Loss: 1270.248779296875, Entropy -5.364396095275879, Learning Rate: 0.01\n",
      "Epoch [1187/20000], Loss: 1280.093994140625, Entropy 3.1540422439575195, Learning Rate: 0.01\n",
      "Epoch [1188/20000], Loss: 1228.0767822265625, Entropy 12.093184471130371, Learning Rate: 0.01\n",
      "Epoch [1189/20000], Loss: 1254.9912109375, Entropy 3.139697313308716, Learning Rate: 0.01\n",
      "Epoch [1190/20000], Loss: 1237.0601806640625, Entropy 3.836561441421509, Learning Rate: 0.01\n",
      "Epoch [1191/20000], Loss: 1235.526123046875, Entropy -8.09357738494873, Learning Rate: 0.01\n",
      "Epoch [1192/20000], Loss: 1261.6298828125, Entropy 2.602177619934082, Learning Rate: 0.01\n",
      "Epoch [1193/20000], Loss: 1222.2779541015625, Entropy 0.44214507937431335, Learning Rate: 0.01\n",
      "Epoch [1194/20000], Loss: 1204.4815673828125, Entropy 7.392251014709473, Learning Rate: 0.01\n",
      "Epoch [1195/20000], Loss: 1327.78759765625, Entropy 6.08697509765625, Learning Rate: 0.01\n",
      "Epoch [1196/20000], Loss: 1197.76708984375, Entropy 16.214506149291992, Learning Rate: 0.01\n",
      "Epoch [1197/20000], Loss: 1244.138916015625, Entropy 4.109559535980225, Learning Rate: 0.01\n",
      "Epoch [1198/20000], Loss: 1263.0531005859375, Entropy 10.135251998901367, Learning Rate: 0.01\n",
      "Epoch [1199/20000], Loss: 1251.818359375, Entropy -1.2971410751342773, Learning Rate: 0.01\n",
      "Epoch [1200/20000], Loss: 1222.8702392578125, Entropy 6.835735321044922, Learning Rate: 0.01\n",
      "Epoch [1201/20000], Loss: 1249.643310546875, Entropy -4.911489009857178, Learning Rate: 0.01\n",
      "Epoch [1202/20000], Loss: 1211.26416015625, Entropy 15.795161247253418, Learning Rate: 0.01\n",
      "Epoch [1203/20000], Loss: 1220.085693359375, Entropy 8.765002250671387, Learning Rate: 0.01\n",
      "Epoch [1204/20000], Loss: 1258.6280517578125, Entropy 10.284963607788086, Learning Rate: 0.01\n",
      "Epoch [1205/20000], Loss: 1208.963134765625, Entropy 7.108927249908447, Learning Rate: 0.01\n",
      "Epoch [1206/20000], Loss: 1239.8427734375, Entropy 8.043896675109863, Learning Rate: 0.01\n",
      "Epoch [1207/20000], Loss: 1250.090087890625, Entropy -1.4422757625579834, Learning Rate: 0.01\n",
      "Epoch [1208/20000], Loss: 1241.466552734375, Entropy -5.530599117279053, Learning Rate: 0.01\n",
      "Epoch [1209/20000], Loss: 1316.9970703125, Entropy 22.111719131469727, Learning Rate: 0.01\n",
      "Epoch [1210/20000], Loss: 1254.994140625, Entropy 21.025157928466797, Learning Rate: 0.01\n",
      "Epoch [1211/20000], Loss: 1241.2274169921875, Entropy 4.278810977935791, Learning Rate: 0.01\n",
      "Epoch [1212/20000], Loss: 1251.7926025390625, Entropy 9.786569595336914, Learning Rate: 0.01\n",
      "Epoch [1213/20000], Loss: 1218.1761474609375, Entropy 7.605212211608887, Learning Rate: 0.01\n",
      "Epoch [1214/20000], Loss: 1234.92919921875, Entropy -1.6423105001449585, Learning Rate: 0.01\n",
      "Epoch [1215/20000], Loss: 1219.8275146484375, Entropy 7.785812854766846, Learning Rate: 0.01\n",
      "Epoch [1216/20000], Loss: 1225.77587890625, Entropy 3.7167882919311523, Learning Rate: 0.01\n",
      "Epoch [1217/20000], Loss: 1212.822265625, Entropy 17.22102165222168, Learning Rate: 0.01\n",
      "Epoch [1218/20000], Loss: 1264.2847900390625, Entropy 1.4986398220062256, Learning Rate: 0.01\n",
      "Epoch [1219/20000], Loss: 1223.359130859375, Entropy -0.5865325331687927, Learning Rate: 0.01\n",
      "Epoch [1220/20000], Loss: 1235.0467529296875, Entropy 19.802780151367188, Learning Rate: 0.01\n",
      "Epoch [1221/20000], Loss: 1251.280029296875, Entropy 7.349468231201172, Learning Rate: 0.01\n",
      "Epoch [1222/20000], Loss: 1238.3720703125, Entropy -4.512834072113037, Learning Rate: 0.01\n",
      "Epoch [1223/20000], Loss: 1205.3758544921875, Entropy 15.624605178833008, Learning Rate: 0.01\n",
      "Epoch [1224/20000], Loss: 1243.7867431640625, Entropy 6.070258140563965, Learning Rate: 0.01\n",
      "Epoch [1225/20000], Loss: 1238.93994140625, Entropy -6.220134258270264, Learning Rate: 0.01\n",
      "Epoch [1226/20000], Loss: 1276.363037109375, Entropy 26.225263595581055, Learning Rate: 0.01\n",
      "Epoch [1227/20000], Loss: 1225.33203125, Entropy 15.461703300476074, Learning Rate: 0.01\n",
      "Epoch [1228/20000], Loss: 1245.568115234375, Entropy 3.8433473110198975, Learning Rate: 0.01\n",
      "Epoch [1229/20000], Loss: 1255.3302001953125, Entropy -5.127779006958008, Learning Rate: 0.01\n",
      "Epoch [1230/20000], Loss: 1260.39501953125, Entropy 13.505037307739258, Learning Rate: 0.01\n",
      "Epoch [1231/20000], Loss: 1254.55810546875, Entropy 19.07817268371582, Learning Rate: 0.01\n",
      "Epoch [1232/20000], Loss: 1267.930908203125, Entropy 0.5902829766273499, Learning Rate: 0.01\n",
      "Epoch [1233/20000], Loss: 1283.4439697265625, Entropy 7.642058372497559, Learning Rate: 0.01\n",
      "Epoch [1234/20000], Loss: 1258.2371826171875, Entropy 7.777321815490723, Learning Rate: 0.01\n",
      "Epoch [1235/20000], Loss: 1233.562744140625, Entropy 26.74554443359375, Learning Rate: 0.01\n",
      "Epoch [1236/20000], Loss: 1247.1234130859375, Entropy 10.994490623474121, Learning Rate: 0.01\n",
      "Epoch [1237/20000], Loss: 1215.74462890625, Entropy 3.6052098274230957, Learning Rate: 0.01\n",
      "Epoch [1238/20000], Loss: 1229.22265625, Entropy 7.353135108947754, Learning Rate: 0.01\n",
      "Epoch [1239/20000], Loss: 1244.2750244140625, Entropy 8.937661170959473, Learning Rate: 0.01\n",
      "Epoch [1240/20000], Loss: 1214.6795654296875, Entropy 10.17978286743164, Learning Rate: 0.01\n",
      "Epoch [1241/20000], Loss: 1253.566162109375, Entropy 9.783669471740723, Learning Rate: 0.01\n",
      "Epoch [1242/20000], Loss: 1216.4278564453125, Entropy 7.030258655548096, Learning Rate: 0.01\n",
      "Epoch [1243/20000], Loss: 1231.128662109375, Entropy 2.215981960296631, Learning Rate: 0.01\n",
      "Epoch [1244/20000], Loss: 1232.1932373046875, Entropy 16.704925537109375, Learning Rate: 0.01\n",
      "Epoch [1245/20000], Loss: 1254.66015625, Entropy -2.989516019821167, Learning Rate: 0.01\n",
      "Epoch [1246/20000], Loss: 1202.809326171875, Entropy 6.061500072479248, Learning Rate: 0.01\n",
      "Epoch [1247/20000], Loss: 1280.9697265625, Entropy 8.620160102844238, Learning Rate: 0.01\n",
      "Epoch [1248/20000], Loss: 1227.841796875, Entropy 5.115504741668701, Learning Rate: 0.01\n",
      "Epoch [1249/20000], Loss: 1208.8135986328125, Entropy 17.087060928344727, Learning Rate: 0.01\n",
      "Epoch [1250/20000], Loss: 1297.5252685546875, Entropy 0.5583159327507019, Learning Rate: 0.01\n",
      "Epoch [1251/20000], Loss: 1210.6845703125, Entropy 20.92527961730957, Learning Rate: 0.01\n",
      "Epoch [1252/20000], Loss: 1256.0203857421875, Entropy 22.938745498657227, Learning Rate: 0.01\n",
      "Epoch [1253/20000], Loss: 1311.9869384765625, Entropy -18.768156051635742, Learning Rate: 0.01\n",
      "Epoch [1254/20000], Loss: 1209.1429443359375, Entropy -2.511819362640381, Learning Rate: 0.01\n",
      "Epoch [1255/20000], Loss: 1216.8707275390625, Entropy 0.05919569730758667, Learning Rate: 0.01\n",
      "Epoch [1256/20000], Loss: 1303.1395263671875, Entropy -16.284494400024414, Learning Rate: 0.01\n",
      "Epoch [1257/20000], Loss: 1211.8193359375, Entropy 15.592635154724121, Learning Rate: 0.01\n",
      "Epoch [1258/20000], Loss: 1192.6876220703125, Entropy 14.979584693908691, Learning Rate: 0.01\n",
      "Epoch [1259/20000], Loss: 1235.20703125, Entropy -1.1492434740066528, Learning Rate: 0.01\n",
      "Epoch [1260/20000], Loss: 1279.7220458984375, Entropy 15.78357219696045, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1261/20000], Loss: 1159.778076171875, Entropy 29.030832290649414, Learning Rate: 0.01\n",
      "Epoch [1262/20000], Loss: 1244.36328125, Entropy 17.668785095214844, Learning Rate: 0.01\n",
      "Epoch [1263/20000], Loss: 1190.1456298828125, Entropy 24.473337173461914, Learning Rate: 0.01\n",
      "Epoch [1264/20000], Loss: 1301.344970703125, Entropy 16.772184371948242, Learning Rate: 0.01\n",
      "Epoch [1265/20000], Loss: 1210.060791015625, Entropy 6.5873260498046875, Learning Rate: 0.01\n",
      "Epoch [1266/20000], Loss: 1279.427734375, Entropy 22.790380477905273, Learning Rate: 0.01\n",
      "Epoch [1267/20000], Loss: 1193.16552734375, Entropy 19.07729148864746, Learning Rate: 0.01\n",
      "Epoch [1268/20000], Loss: 1256.2572021484375, Entropy 15.03972339630127, Learning Rate: 0.01\n",
      "Epoch [1269/20000], Loss: 1266.400390625, Entropy 17.571598052978516, Learning Rate: 0.01\n",
      "Epoch [1270/20000], Loss: 1288.826171875, Entropy 12.985076904296875, Learning Rate: 0.01\n",
      "Epoch [1271/20000], Loss: 1250.217041015625, Entropy 14.022395133972168, Learning Rate: 0.01\n",
      "Epoch [1272/20000], Loss: 1316.9996337890625, Entropy 16.237812042236328, Learning Rate: 0.01\n",
      "Epoch [1273/20000], Loss: 1228.46044921875, Entropy -2.3078255653381348, Learning Rate: 0.01\n",
      "Epoch [1274/20000], Loss: 1288.5413818359375, Entropy 20.698955535888672, Learning Rate: 0.01\n",
      "Epoch [1275/20000], Loss: 1253.852783203125, Entropy 15.053712844848633, Learning Rate: 0.01\n",
      "Epoch [1276/20000], Loss: 1305.6295166015625, Entropy 18.06964874267578, Learning Rate: 0.01\n",
      "Epoch [1277/20000], Loss: 1226.2222900390625, Entropy 15.07701587677002, Learning Rate: 0.01\n",
      "Epoch [1278/20000], Loss: 1309.004638671875, Entropy 12.531644821166992, Learning Rate: 0.01\n",
      "Epoch [1279/20000], Loss: 1265.60009765625, Entropy 31.10823631286621, Learning Rate: 0.01\n",
      "Epoch [1280/20000], Loss: 1310.3114013671875, Entropy 14.6127290725708, Learning Rate: 0.01\n",
      "Epoch [1281/20000], Loss: 1220.2523193359375, Entropy 25.326383590698242, Learning Rate: 0.01\n",
      "Epoch [1282/20000], Loss: 1232.8262939453125, Entropy 19.670621871948242, Learning Rate: 0.01\n",
      "Epoch [1283/20000], Loss: 1367.2303466796875, Entropy 25.392675399780273, Learning Rate: 0.01\n",
      "Epoch [1284/20000], Loss: 1294.1678466796875, Entropy 5.183722972869873, Learning Rate: 0.01\n",
      "Epoch [1285/20000], Loss: 1343.083251953125, Entropy 20.160884857177734, Learning Rate: 0.01\n",
      "Epoch [1286/20000], Loss: 1394.6229248046875, Entropy 28.818578720092773, Learning Rate: 0.01\n",
      "Epoch [1287/20000], Loss: 1241.027587890625, Entropy 20.831928253173828, Learning Rate: 0.01\n",
      "Epoch [1288/20000], Loss: 1283.661376953125, Entropy 13.356406211853027, Learning Rate: 0.01\n",
      "Epoch [1289/20000], Loss: 1302.8897705078125, Entropy 23.7961368560791, Learning Rate: 0.01\n",
      "Epoch [1290/20000], Loss: 1305.6265869140625, Entropy 2.444597005844116, Learning Rate: 0.01\n",
      "Epoch [1291/20000], Loss: 1271.05908203125, Entropy 24.625316619873047, Learning Rate: 0.01\n",
      "Epoch [1292/20000], Loss: 1328.4154052734375, Entropy 7.468543529510498, Learning Rate: 0.01\n",
      "Epoch [1293/20000], Loss: 1251.98681640625, Entropy 17.61526870727539, Learning Rate: 0.01\n",
      "Epoch [1294/20000], Loss: 1208.101806640625, Entropy 34.16749572753906, Learning Rate: 0.01\n",
      "Epoch [1295/20000], Loss: 1341.5308837890625, Entropy 29.148988723754883, Learning Rate: 0.01\n",
      "Epoch [1296/20000], Loss: 1292.0968017578125, Entropy 34.912254333496094, Learning Rate: 0.01\n",
      "Epoch [1297/20000], Loss: 1276.217041015625, Entropy 32.71329879760742, Learning Rate: 0.01\n",
      "Epoch [1298/20000], Loss: 1210.4185791015625, Entropy 24.04066276550293, Learning Rate: 0.01\n",
      "Epoch [1299/20000], Loss: 1260.4195556640625, Entropy 15.473648071289062, Learning Rate: 0.01\n",
      "Epoch [1300/20000], Loss: 1229.726318359375, Entropy 7.071707725524902, Learning Rate: 0.01\n",
      "Epoch [1301/20000], Loss: 1267.4847412109375, Entropy 36.7762451171875, Learning Rate: 0.01\n",
      "Epoch [1302/20000], Loss: 1241.6829833984375, Entropy 34.78013229370117, Learning Rate: 0.01\n",
      "Epoch [1303/20000], Loss: 1288.6434326171875, Entropy 10.3212890625, Learning Rate: 0.01\n",
      "Epoch [1304/20000], Loss: 1247.95361328125, Entropy 21.5904483795166, Learning Rate: 0.01\n",
      "Epoch [1305/20000], Loss: 1309.5743408203125, Entropy 15.134848594665527, Learning Rate: 0.01\n",
      "Epoch [1306/20000], Loss: 1250.958984375, Entropy 19.636119842529297, Learning Rate: 0.01\n",
      "Epoch [1307/20000], Loss: 1312.00927734375, Entropy 16.206892013549805, Learning Rate: 0.01\n",
      "Epoch [1308/20000], Loss: 1264.970458984375, Entropy 26.22334098815918, Learning Rate: 0.01\n",
      "Epoch [1309/20000], Loss: 1252.928955078125, Entropy 28.1226749420166, Learning Rate: 0.01\n",
      "Epoch [1310/20000], Loss: 1226.67822265625, Entropy 20.209829330444336, Learning Rate: 0.01\n",
      "Epoch [1311/20000], Loss: 1238.07373046875, Entropy 22.80134391784668, Learning Rate: 0.01\n",
      "Epoch [1312/20000], Loss: 1221.188720703125, Entropy 53.574462890625, Learning Rate: 0.01\n",
      "Epoch [1313/20000], Loss: 1278.2637939453125, Entropy 19.30194854736328, Learning Rate: 0.01\n",
      "Epoch [1314/20000], Loss: 1245.04833984375, Entropy 28.447086334228516, Learning Rate: 0.01\n",
      "Epoch [1315/20000], Loss: 1199.2650146484375, Entropy 23.762102127075195, Learning Rate: 0.01\n",
      "Epoch [1316/20000], Loss: 1304.875, Entropy 22.888484954833984, Learning Rate: 0.01\n",
      "Epoch [1317/20000], Loss: 1232.6324462890625, Entropy 26.61414337158203, Learning Rate: 0.01\n",
      "Epoch [1318/20000], Loss: 1197.4388427734375, Entropy 33.79378890991211, Learning Rate: 0.01\n",
      "Epoch [1319/20000], Loss: 1225.797119140625, Entropy 22.470571517944336, Learning Rate: 0.01\n",
      "Epoch [1320/20000], Loss: 1307.2130126953125, Entropy 18.765485763549805, Learning Rate: 0.01\n",
      "Epoch [1321/20000], Loss: 1224.9561767578125, Entropy 29.944896697998047, Learning Rate: 0.01\n",
      "Epoch [1322/20000], Loss: 1281.3966064453125, Entropy 10.750136375427246, Learning Rate: 0.01\n",
      "Epoch [1323/20000], Loss: 1223.9896240234375, Entropy 13.636274337768555, Learning Rate: 0.01\n",
      "Epoch [1324/20000], Loss: 1285.1458740234375, Entropy 22.853656768798828, Learning Rate: 0.01\n",
      "Epoch [1325/20000], Loss: 1270.7115478515625, Entropy 23.41792106628418, Learning Rate: 0.01\n",
      "Epoch [1326/20000], Loss: 1319.0313720703125, Entropy 20.835468292236328, Learning Rate: 0.01\n",
      "Epoch [1327/20000], Loss: 1276.796142578125, Entropy 32.540565490722656, Learning Rate: 0.01\n",
      "Epoch [1328/20000], Loss: 1342.5921630859375, Entropy 43.09893035888672, Learning Rate: 0.01\n",
      "Epoch [1329/20000], Loss: 1334.9044189453125, Entropy 17.13691520690918, Learning Rate: 0.01\n",
      "Epoch [1330/20000], Loss: 1253.4716796875, Entropy 20.422143936157227, Learning Rate: 0.01\n",
      "Epoch [1331/20000], Loss: 1255.8634033203125, Entropy 49.39595413208008, Learning Rate: 0.01\n",
      "Epoch [1332/20000], Loss: 1251.2235107421875, Entropy 28.59319305419922, Learning Rate: 0.01\n",
      "Epoch [1333/20000], Loss: 1259.29443359375, Entropy 11.651651382446289, Learning Rate: 0.01\n",
      "Epoch [1334/20000], Loss: 1253.2889404296875, Entropy 28.08587074279785, Learning Rate: 0.01\n",
      "Epoch [1335/20000], Loss: 1281.885498046875, Entropy 14.494221687316895, Learning Rate: 0.01\n",
      "Epoch [1336/20000], Loss: 1348.71044921875, Entropy 23.54136085510254, Learning Rate: 0.01\n",
      "Epoch [1337/20000], Loss: 1276.0826416015625, Entropy 35.60009765625, Learning Rate: 0.01\n",
      "Epoch [1338/20000], Loss: 1228.64453125, Entropy 19.945537567138672, Learning Rate: 0.01\n",
      "Epoch [1339/20000], Loss: 1288.0263671875, Entropy 21.424001693725586, Learning Rate: 0.01\n",
      "Epoch [1340/20000], Loss: 1271.1558837890625, Entropy 19.705307006835938, Learning Rate: 0.01\n",
      "Epoch [1341/20000], Loss: 1241.7587890625, Entropy 34.60595703125, Learning Rate: 0.01\n",
      "Epoch [1342/20000], Loss: 1301.5181884765625, Entropy 24.094858169555664, Learning Rate: 0.01\n",
      "Epoch [1343/20000], Loss: 1192.971435546875, Entropy 36.88275909423828, Learning Rate: 0.01\n",
      "Epoch [1344/20000], Loss: 1256.893310546875, Entropy 35.36619567871094, Learning Rate: 0.01\n",
      "Epoch [1345/20000], Loss: 1199.30078125, Entropy 24.846210479736328, Learning Rate: 0.01\n",
      "Epoch [1346/20000], Loss: 1192.486328125, Entropy 27.193344116210938, Learning Rate: 0.01\n",
      "Epoch [1347/20000], Loss: 1264.5528564453125, Entropy 19.73919105529785, Learning Rate: 0.01\n",
      "Epoch [1348/20000], Loss: 1306.5565185546875, Entropy 16.204280853271484, Learning Rate: 0.01\n",
      "Epoch [1349/20000], Loss: 1188.1756591796875, Entropy 31.466352462768555, Learning Rate: 0.01\n",
      "Epoch [1350/20000], Loss: 1263.1007080078125, Entropy 39.17008590698242, Learning Rate: 0.01\n",
      "Epoch [1351/20000], Loss: 1289.650634765625, Entropy 16.404499053955078, Learning Rate: 0.01\n",
      "Epoch [1352/20000], Loss: 1301.3831787109375, Entropy 34.914608001708984, Learning Rate: 0.01\n",
      "Epoch [1353/20000], Loss: 1273.8280029296875, Entropy 10.385663032531738, Learning Rate: 0.01\n",
      "Epoch [1354/20000], Loss: 1286.4456787109375, Entropy 13.731816291809082, Learning Rate: 0.01\n",
      "Epoch [1355/20000], Loss: 1223.0841064453125, Entropy 25.18608283996582, Learning Rate: 0.01\n",
      "Epoch [1356/20000], Loss: 1201.022705078125, Entropy 18.122854232788086, Learning Rate: 0.01\n",
      "Epoch [1357/20000], Loss: 1232.323486328125, Entropy 37.64763259887695, Learning Rate: 0.01\n",
      "Epoch [1358/20000], Loss: 1217.6717529296875, Entropy 16.67320442199707, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1359/20000], Loss: 1207.5804443359375, Entropy 36.61781311035156, Learning Rate: 0.01\n",
      "Epoch [1360/20000], Loss: 1207.9447021484375, Entropy 28.27763557434082, Learning Rate: 0.01\n",
      "Epoch [1361/20000], Loss: 1187.1414794921875, Entropy 28.200199127197266, Learning Rate: 0.01\n",
      "Epoch [1362/20000], Loss: 1216.792724609375, Entropy 34.483909606933594, Learning Rate: 0.01\n",
      "Epoch [1363/20000], Loss: 1211.7191162109375, Entropy 45.24211883544922, Learning Rate: 0.01\n",
      "Epoch [1364/20000], Loss: 1213.096435546875, Entropy 35.247806549072266, Learning Rate: 0.01\n",
      "Epoch [1365/20000], Loss: 1187.9130859375, Entropy 29.101287841796875, Learning Rate: 0.01\n",
      "Epoch [1366/20000], Loss: 1243.967529296875, Entropy 35.146629333496094, Learning Rate: 0.01\n",
      "Epoch [1367/20000], Loss: 1327.2529296875, Entropy 33.174583435058594, Learning Rate: 0.01\n",
      "Epoch [1368/20000], Loss: 1278.9320068359375, Entropy 26.344745635986328, Learning Rate: 0.01\n",
      "Epoch [1369/20000], Loss: 1316.064697265625, Entropy 34.2836799621582, Learning Rate: 0.01\n",
      "Epoch [1370/20000], Loss: 1406.6676025390625, Entropy 22.80356788635254, Learning Rate: 0.01\n",
      "Epoch [1371/20000], Loss: 1352.2113037109375, Entropy 21.533037185668945, Learning Rate: 0.01\n",
      "Epoch [1372/20000], Loss: 1223.205322265625, Entropy 18.81753921508789, Learning Rate: 0.01\n",
      "Epoch [1373/20000], Loss: 1355.0450439453125, Entropy 31.651142120361328, Learning Rate: 0.01\n",
      "Epoch [1374/20000], Loss: 1207.1708984375, Entropy 24.697660446166992, Learning Rate: 0.01\n",
      "Epoch [1375/20000], Loss: 1296.186767578125, Entropy 34.66840362548828, Learning Rate: 0.01\n",
      "Epoch [1376/20000], Loss: 1261.1529541015625, Entropy 33.410057067871094, Learning Rate: 0.01\n",
      "Epoch [1377/20000], Loss: 1289.6036376953125, Entropy 22.818439483642578, Learning Rate: 0.01\n",
      "Epoch [1378/20000], Loss: 1211.979248046875, Entropy 50.56265640258789, Learning Rate: 0.01\n",
      "Epoch [1379/20000], Loss: 1187.955810546875, Entropy 41.42051315307617, Learning Rate: 0.01\n",
      "Epoch [1380/20000], Loss: 1222.6463623046875, Entropy 30.905736923217773, Learning Rate: 0.01\n",
      "Epoch [1381/20000], Loss: 1285.9615478515625, Entropy 40.40182876586914, Learning Rate: 0.01\n",
      "Epoch [1382/20000], Loss: 1236.365478515625, Entropy 29.58270263671875, Learning Rate: 0.01\n",
      "Epoch [1383/20000], Loss: 1248.3182373046875, Entropy 16.92989730834961, Learning Rate: 0.01\n",
      "Epoch [1384/20000], Loss: 1220.70703125, Entropy 30.242589950561523, Learning Rate: 0.01\n",
      "Epoch [1385/20000], Loss: 1198.4169921875, Entropy 38.281002044677734, Learning Rate: 0.01\n",
      "Epoch [1386/20000], Loss: 1207.841064453125, Entropy 21.720842361450195, Learning Rate: 0.01\n",
      "Epoch [1387/20000], Loss: 1297.9755859375, Entropy 23.080644607543945, Learning Rate: 0.01\n",
      "Epoch [1388/20000], Loss: 1288.283203125, Entropy 39.61648178100586, Learning Rate: 0.01\n",
      "Epoch [1389/20000], Loss: 1232.9449462890625, Entropy 46.73018264770508, Learning Rate: 0.01\n",
      "Epoch [1390/20000], Loss: 1227.508544921875, Entropy 19.833229064941406, Learning Rate: 0.01\n",
      "Epoch [1391/20000], Loss: 1211.069091796875, Entropy 29.11128807067871, Learning Rate: 0.01\n",
      "Epoch [1392/20000], Loss: 1242.307373046875, Entropy 34.87507247924805, Learning Rate: 0.01\n",
      "Epoch [1393/20000], Loss: 1205.465576171875, Entropy 25.26773452758789, Learning Rate: 0.01\n",
      "Epoch [1394/20000], Loss: 1213.7088623046875, Entropy 40.91183853149414, Learning Rate: 0.01\n",
      "Epoch [1395/20000], Loss: 1257.403076171875, Entropy 40.46991729736328, Learning Rate: 0.01\n",
      "Epoch [1396/20000], Loss: 1224.9984130859375, Entropy 27.11455535888672, Learning Rate: 0.01\n",
      "Epoch [1397/20000], Loss: 1201.89404296875, Entropy 30.15410041809082, Learning Rate: 0.01\n",
      "Epoch [1398/20000], Loss: 1200.8204345703125, Entropy 20.82779312133789, Learning Rate: 0.01\n",
      "Epoch [1399/20000], Loss: 1186.251953125, Entropy 34.40232467651367, Learning Rate: 0.01\n",
      "Epoch [1400/20000], Loss: 1239.7689208984375, Entropy 47.00832748413086, Learning Rate: 0.01\n",
      "Epoch [1401/20000], Loss: 1198.6107177734375, Entropy 32.38715744018555, Learning Rate: 0.01\n",
      "Epoch [1402/20000], Loss: 1237.4031982421875, Entropy 33.92494201660156, Learning Rate: 0.01\n",
      "Epoch [1403/20000], Loss: 1207.0643310546875, Entropy 27.12616729736328, Learning Rate: 0.01\n",
      "Epoch [1404/20000], Loss: 1232.4090576171875, Entropy 38.27485656738281, Learning Rate: 0.01\n",
      "Epoch [1405/20000], Loss: 1241.7716064453125, Entropy 33.536216735839844, Learning Rate: 0.01\n",
      "Epoch [1406/20000], Loss: 1183.1383056640625, Entropy 37.51972961425781, Learning Rate: 0.01\n",
      "Epoch [1407/20000], Loss: 1206.504638671875, Entropy 43.626953125, Learning Rate: 0.01\n",
      "Epoch [1408/20000], Loss: 1176.2530517578125, Entropy 23.07196807861328, Learning Rate: 0.01\n",
      "Epoch [1409/20000], Loss: 1190.7939453125, Entropy 45.38409423828125, Learning Rate: 0.01\n",
      "Epoch [1410/20000], Loss: 1253.516357421875, Entropy 41.41997146606445, Learning Rate: 0.01\n",
      "Epoch [1411/20000], Loss: 1182.65576171875, Entropy 28.10039520263672, Learning Rate: 0.01\n",
      "Epoch [1412/20000], Loss: 1193.2666015625, Entropy 50.86257553100586, Learning Rate: 0.01\n",
      "Epoch [1413/20000], Loss: 1226.0419921875, Entropy 44.36314392089844, Learning Rate: 0.01\n",
      "Epoch [1414/20000], Loss: 1260.2347412109375, Entropy 24.37965965270996, Learning Rate: 0.01\n",
      "Epoch [1415/20000], Loss: 1199.5963134765625, Entropy 26.478973388671875, Learning Rate: 0.01\n",
      "Epoch [1416/20000], Loss: 1268.59423828125, Entropy 43.371707916259766, Learning Rate: 0.01\n",
      "Epoch [1417/20000], Loss: 1232.8629150390625, Entropy 64.6573486328125, Learning Rate: 0.01\n",
      "Epoch [1418/20000], Loss: 1237.7601318359375, Entropy 34.30002212524414, Learning Rate: 0.01\n",
      "Epoch [1419/20000], Loss: 1229.1947021484375, Entropy 33.53230667114258, Learning Rate: 0.01\n",
      "Epoch [1420/20000], Loss: 1212.315185546875, Entropy 51.62110137939453, Learning Rate: 0.01\n",
      "Epoch [1421/20000], Loss: 1217.367919921875, Entropy 36.3564338684082, Learning Rate: 0.01\n",
      "Epoch [1422/20000], Loss: 1215.7581787109375, Entropy 31.220975875854492, Learning Rate: 0.01\n",
      "Epoch [1423/20000], Loss: 1274.1502685546875, Entropy 42.12646484375, Learning Rate: 0.01\n",
      "Epoch [1424/20000], Loss: 1194.2119140625, Entropy 44.13184356689453, Learning Rate: 0.01\n",
      "Epoch [1425/20000], Loss: 1181.71826171875, Entropy 32.74920654296875, Learning Rate: 0.01\n",
      "Epoch [1426/20000], Loss: 1268.1336669921875, Entropy 32.71837615966797, Learning Rate: 0.01\n",
      "Epoch [1427/20000], Loss: 1328.42236328125, Entropy 45.13947296142578, Learning Rate: 0.01\n",
      "Epoch [1428/20000], Loss: 1291.2928466796875, Entropy 37.61640167236328, Learning Rate: 0.01\n",
      "Epoch [1429/20000], Loss: 1284.32275390625, Entropy 42.83097457885742, Learning Rate: 0.01\n",
      "Epoch [1430/20000], Loss: 1257.031982421875, Entropy 27.113237380981445, Learning Rate: 0.01\n",
      "Epoch [1431/20000], Loss: 1236.3466796875, Entropy 48.59812545776367, Learning Rate: 0.01\n",
      "Epoch [1432/20000], Loss: 1212.275146484375, Entropy 48.237060546875, Learning Rate: 0.01\n",
      "Epoch [1433/20000], Loss: 1247.39208984375, Entropy 48.32819366455078, Learning Rate: 0.01\n",
      "Epoch [1434/20000], Loss: 1281.4578857421875, Entropy 42.921146392822266, Learning Rate: 0.01\n",
      "Epoch [1435/20000], Loss: 1204.9462890625, Entropy 33.65109634399414, Learning Rate: 0.01\n",
      "Epoch [1436/20000], Loss: 1297.93017578125, Entropy 45.03971481323242, Learning Rate: 0.01\n",
      "Epoch [1437/20000], Loss: 1309.961181640625, Entropy 40.93950271606445, Learning Rate: 0.01\n",
      "Epoch [1438/20000], Loss: 1253.25341796875, Entropy 35.62383270263672, Learning Rate: 0.01\n",
      "Epoch [1439/20000], Loss: 1193.115966796875, Entropy 50.41282653808594, Learning Rate: 0.01\n",
      "Epoch [1440/20000], Loss: 1268.76611328125, Entropy 19.22766876220703, Learning Rate: 0.01\n",
      "Epoch [1441/20000], Loss: 1264.82763671875, Entropy 52.636112213134766, Learning Rate: 0.01\n",
      "Epoch [1442/20000], Loss: 1342.8699951171875, Entropy 38.72319793701172, Learning Rate: 0.01\n",
      "Epoch [1443/20000], Loss: 1306.9022216796875, Entropy 41.306819915771484, Learning Rate: 0.01\n",
      "Epoch [1444/20000], Loss: 1182.1439208984375, Entropy 47.13081359863281, Learning Rate: 0.01\n",
      "Epoch [1445/20000], Loss: 1426.1568603515625, Entropy 57.775760650634766, Learning Rate: 0.01\n",
      "Epoch [1446/20000], Loss: 1532.389892578125, Entropy 53.59246826171875, Learning Rate: 0.01\n",
      "Epoch [1447/20000], Loss: 1323.331787109375, Entropy 44.57008743286133, Learning Rate: 0.01\n",
      "Epoch [1448/20000], Loss: 1578.7286376953125, Entropy 43.20219039916992, Learning Rate: 0.01\n",
      "Epoch [1449/20000], Loss: 1519.5960693359375, Entropy 31.37163543701172, Learning Rate: 0.01\n",
      "Epoch [1450/20000], Loss: 1458.5213623046875, Entropy 45.996673583984375, Learning Rate: 0.01\n",
      "Epoch [1451/20000], Loss: 1666.658203125, Entropy 51.591800689697266, Learning Rate: 0.01\n",
      "Epoch [1452/20000], Loss: 1349.8306884765625, Entropy 51.233646392822266, Learning Rate: 0.01\n",
      "Epoch [1453/20000], Loss: 1790.95703125, Entropy 48.541015625, Learning Rate: 0.01\n",
      "Epoch [1454/20000], Loss: 1274.388916015625, Entropy 38.88282012939453, Learning Rate: 0.01\n",
      "Epoch [1455/20000], Loss: 2164.998291015625, Entropy 44.25636672973633, Learning Rate: 0.01\n",
      "Epoch [1456/20000], Loss: 1863.747314453125, Entropy 49.77422332763672, Learning Rate: 0.01\n",
      "Epoch [1457/20000], Loss: 1629.3341064453125, Entropy 28.246002197265625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1458/20000], Loss: 1892.8712158203125, Entropy 49.68633270263672, Learning Rate: 0.01\n",
      "Epoch [1459/20000], Loss: 1431.2591552734375, Entropy 40.008934020996094, Learning Rate: 0.01\n",
      "Epoch [1460/20000], Loss: 1886.8896484375, Entropy 43.90403747558594, Learning Rate: 0.01\n",
      "Epoch [1461/20000], Loss: 1765.2440185546875, Entropy 54.41730499267578, Learning Rate: 0.01\n",
      "Epoch [1462/20000], Loss: 1440.7364501953125, Entropy 41.08789825439453, Learning Rate: 0.01\n",
      "Epoch [1463/20000], Loss: 1622.9788818359375, Entropy 34.49394607543945, Learning Rate: 0.01\n",
      "Epoch [1464/20000], Loss: 1977.2890625, Entropy 40.612918853759766, Learning Rate: 0.01\n",
      "Epoch [1465/20000], Loss: 2374.4892578125, Entropy 44.986053466796875, Learning Rate: 0.01\n",
      "Epoch [1466/20000], Loss: 4325.96240234375, Entropy 42.38377380371094, Learning Rate: 0.01\n",
      "Epoch [1467/20000], Loss: 5539.3447265625, Entropy 37.02138900756836, Learning Rate: 0.01\n",
      "Epoch [1468/20000], Loss: 2779.864501953125, Entropy 59.5277214050293, Learning Rate: 0.01\n",
      "Epoch [1469/20000], Loss: 3165.336669921875, Entropy 36.98481750488281, Learning Rate: 0.01\n",
      "Epoch [1470/20000], Loss: 6079.97314453125, Entropy 39.69857406616211, Learning Rate: 0.01\n",
      "Epoch [1471/20000], Loss: 2451.99560546875, Entropy 41.29685974121094, Learning Rate: 0.01\n",
      "Epoch [1472/20000], Loss: 6158.61376953125, Entropy 23.03999900817871, Learning Rate: 0.01\n",
      "Epoch [1473/20000], Loss: 1954.8382568359375, Entropy 42.97575759887695, Learning Rate: 0.01\n",
      "Epoch [1474/20000], Loss: 2871.371337890625, Entropy 37.222312927246094, Learning Rate: 0.01\n",
      "Epoch [1475/20000], Loss: 3931.440185546875, Entropy 20.87913703918457, Learning Rate: 0.01\n",
      "Epoch [1476/20000], Loss: 1986.132080078125, Entropy 24.767005920410156, Learning Rate: 0.01\n",
      "Epoch [1477/20000], Loss: 3091.212646484375, Entropy 36.633426666259766, Learning Rate: 0.01\n",
      "Epoch [1478/20000], Loss: 3065.755126953125, Entropy 26.320316314697266, Learning Rate: 0.01\n",
      "Epoch [1479/20000], Loss: 1773.8887939453125, Entropy 11.583539962768555, Learning Rate: 0.01\n",
      "Epoch [1480/20000], Loss: 2860.9619140625, Entropy 9.73664665222168, Learning Rate: 0.01\n",
      "Epoch [1481/20000], Loss: 1679.9195556640625, Entropy 16.13524055480957, Learning Rate: 0.01\n",
      "Epoch [1482/20000], Loss: 1779.427734375, Entropy 14.1535005569458, Learning Rate: 0.01\n",
      "Epoch [1483/20000], Loss: 1970.1300048828125, Entropy -3.249147891998291, Learning Rate: 0.01\n",
      "Epoch [1484/20000], Loss: 1724.6766357421875, Entropy 19.441448211669922, Learning Rate: 0.01\n",
      "Epoch [1485/20000], Loss: 1863.004150390625, Entropy 10.900107383728027, Learning Rate: 0.01\n",
      "Epoch [1486/20000], Loss: 1605.0858154296875, Entropy 18.768535614013672, Learning Rate: 0.01\n",
      "Epoch [1487/20000], Loss: 1591.1890869140625, Entropy 15.897379875183105, Learning Rate: 0.01\n",
      "Epoch [1488/20000], Loss: 1729.4884033203125, Entropy 8.379435539245605, Learning Rate: 0.01\n",
      "Epoch [1489/20000], Loss: 1401.7197265625, Entropy 9.958349227905273, Learning Rate: 0.01\n",
      "Epoch [1490/20000], Loss: 1885.8182373046875, Entropy 5.87444543838501, Learning Rate: 0.01\n",
      "Epoch [1491/20000], Loss: 1572.7000732421875, Entropy 0.18318815529346466, Learning Rate: 0.01\n",
      "Epoch [1492/20000], Loss: 1572.7293701171875, Entropy -12.142291069030762, Learning Rate: 0.01\n",
      "Epoch [1493/20000], Loss: 1605.6666259765625, Entropy -6.506683349609375, Learning Rate: 0.01\n",
      "Epoch [1494/20000], Loss: 1685.40966796875, Entropy 0.830044150352478, Learning Rate: 0.01\n",
      "Epoch [1495/20000], Loss: 1443.265380859375, Entropy 1.6626627445220947, Learning Rate: 0.01\n",
      "Epoch [1496/20000], Loss: 1406.2581787109375, Entropy -4.786932468414307, Learning Rate: 0.01\n",
      "Epoch [1497/20000], Loss: 1660.5001220703125, Entropy -15.48081111907959, Learning Rate: 0.01\n",
      "Epoch [1498/20000], Loss: 1617.16552734375, Entropy 24.126916885375977, Learning Rate: 0.01\n",
      "Epoch [1499/20000], Loss: 1316.6295166015625, Entropy -13.436830520629883, Learning Rate: 0.01\n",
      "Epoch [1500/20000], Loss: 1593.0567626953125, Entropy -10.434626579284668, Learning Rate: 0.01\n",
      "Epoch [1501/20000], Loss: 1535.3433837890625, Entropy -1.5878726243972778, Learning Rate: 0.01\n",
      "Epoch [1502/20000], Loss: 1293.353515625, Entropy 7.263328552246094, Learning Rate: 0.01\n",
      "Epoch [1503/20000], Loss: 1512.3914794921875, Entropy -4.719810485839844, Learning Rate: 0.01\n",
      "Epoch [1504/20000], Loss: 1503.7783203125, Entropy -3.5423948764801025, Learning Rate: 0.01\n",
      "Epoch [1505/20000], Loss: 1308.8880615234375, Entropy 12.806200981140137, Learning Rate: 0.01\n",
      "Epoch [1506/20000], Loss: 1417.4671630859375, Entropy 14.524641036987305, Learning Rate: 0.01\n",
      "Epoch [1507/20000], Loss: 1352.395751953125, Entropy 2.1500914096832275, Learning Rate: 0.01\n",
      "Epoch [1508/20000], Loss: 1326.51416015625, Entropy 0.6317592263221741, Learning Rate: 0.01\n",
      "Epoch [1509/20000], Loss: 1389.2147216796875, Entropy 4.403695583343506, Learning Rate: 0.01\n",
      "Epoch [1510/20000], Loss: 1425.06884765625, Entropy 8.2916841506958, Learning Rate: 0.01\n",
      "Epoch [1511/20000], Loss: 1314.468994140625, Entropy 1.0109138488769531, Learning Rate: 0.01\n",
      "Epoch [1512/20000], Loss: 1370.34326171875, Entropy -15.504274368286133, Learning Rate: 0.01\n",
      "Epoch [1513/20000], Loss: 1391.8712158203125, Entropy 9.224236488342285, Learning Rate: 0.01\n",
      "Epoch [1514/20000], Loss: 1308.171630859375, Entropy -6.900704383850098, Learning Rate: 0.01\n",
      "Epoch [1515/20000], Loss: 1357.270263671875, Entropy 4.83107852935791, Learning Rate: 0.01\n",
      "Epoch [1516/20000], Loss: 1317.487060546875, Entropy -11.96139907836914, Learning Rate: 0.01\n",
      "Epoch [1517/20000], Loss: 1358.5228271484375, Entropy -5.22485876083374, Learning Rate: 0.01\n",
      "Epoch [1518/20000], Loss: 1383.2733154296875, Entropy 12.567001342773438, Learning Rate: 0.01\n",
      "Epoch [1519/20000], Loss: 1255.22998046875, Entropy -5.339308261871338, Learning Rate: 0.01\n",
      "Epoch [1520/20000], Loss: 1382.1285400390625, Entropy 7.918580055236816, Learning Rate: 0.01\n",
      "Epoch [1521/20000], Loss: 1336.904052734375, Entropy 2.814063549041748, Learning Rate: 0.01\n",
      "Epoch [1522/20000], Loss: 1212.8935546875, Entropy 5.761200428009033, Learning Rate: 0.01\n",
      "Epoch [1523/20000], Loss: 1302.8280029296875, Entropy 3.1052775382995605, Learning Rate: 0.01\n",
      "Epoch [1524/20000], Loss: 1270.9814453125, Entropy -5.196045398712158, Learning Rate: 0.01\n",
      "Epoch [1525/20000], Loss: 1303.6795654296875, Entropy -0.801979660987854, Learning Rate: 0.01\n",
      "Epoch [1526/20000], Loss: 1206.1646728515625, Entropy 17.688953399658203, Learning Rate: 0.01\n",
      "Epoch [1527/20000], Loss: 1353.716064453125, Entropy 0.8706992864608765, Learning Rate: 0.01\n",
      "Epoch [1528/20000], Loss: 1255.5888671875, Entropy 23.746721267700195, Learning Rate: 0.01\n",
      "Epoch [1529/20000], Loss: 1224.58349609375, Entropy -4.630913257598877, Learning Rate: 0.01\n",
      "Epoch [1530/20000], Loss: 1248.76123046875, Entropy 6.439290523529053, Learning Rate: 0.01\n",
      "Epoch [1531/20000], Loss: 1311.3726806640625, Entropy 15.468453407287598, Learning Rate: 0.01\n",
      "Epoch [1532/20000], Loss: 1256.493408203125, Entropy -0.3639199733734131, Learning Rate: 0.01\n",
      "Epoch [1533/20000], Loss: 1272.571533203125, Entropy 8.285842895507812, Learning Rate: 0.01\n",
      "Epoch [1534/20000], Loss: 1260.4786376953125, Entropy -6.252829551696777, Learning Rate: 0.01\n",
      "Epoch [1535/20000], Loss: 1249.3612060546875, Entropy -8.129008293151855, Learning Rate: 0.01\n",
      "Epoch [1536/20000], Loss: 1234.947998046875, Entropy 5.736976146697998, Learning Rate: 0.01\n",
      "Epoch [1537/20000], Loss: 1361.16650390625, Entropy 10.400662422180176, Learning Rate: 0.01\n",
      "Epoch [1538/20000], Loss: 1282.9754638671875, Entropy -16.207122802734375, Learning Rate: 0.01\n",
      "Epoch [1539/20000], Loss: 1281.27197265625, Entropy 5.288341999053955, Learning Rate: 0.01\n",
      "Epoch [1540/20000], Loss: 1325.5906982421875, Entropy 9.572367668151855, Learning Rate: 0.01\n",
      "Epoch [1541/20000], Loss: 1214.7777099609375, Entropy 7.718616008758545, Learning Rate: 0.01\n",
      "Epoch [1542/20000], Loss: 1223.8360595703125, Entropy 19.868757247924805, Learning Rate: 0.01\n",
      "Epoch [1543/20000], Loss: 1315.2149658203125, Entropy 10.96922492980957, Learning Rate: 0.01\n",
      "Epoch [1544/20000], Loss: 1273.3338623046875, Entropy 3.6538641452789307, Learning Rate: 0.01\n",
      "Epoch [1545/20000], Loss: 1249.669189453125, Entropy 16.752103805541992, Learning Rate: 0.01\n",
      "Epoch [1546/20000], Loss: 1316.0504150390625, Entropy -11.800166130065918, Learning Rate: 0.01\n",
      "Epoch [1547/20000], Loss: 1232.9136962890625, Entropy 2.5040040016174316, Learning Rate: 0.01\n",
      "Epoch [1548/20000], Loss: 1261.024658203125, Entropy 6.50443696975708, Learning Rate: 0.01\n",
      "Epoch [1549/20000], Loss: 1280.9649658203125, Entropy -1.7841668128967285, Learning Rate: 0.01\n",
      "Epoch [1550/20000], Loss: 1230.4886474609375, Entropy 1.690429925918579, Learning Rate: 0.01\n",
      "Epoch [1551/20000], Loss: 1298.796875, Entropy -2.408965587615967, Learning Rate: 0.01\n",
      "Epoch [1552/20000], Loss: 1238.8834228515625, Entropy 4.94514274597168, Learning Rate: 0.01\n",
      "Epoch [1553/20000], Loss: 1282.2530517578125, Entropy 8.305800437927246, Learning Rate: 0.01\n",
      "Epoch [1554/20000], Loss: 1202.05615234375, Entropy 5.062495231628418, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1555/20000], Loss: 1213.5843505859375, Entropy 17.68108558654785, Learning Rate: 0.01\n",
      "Epoch [1556/20000], Loss: 1251.2725830078125, Entropy -3.0077006816864014, Learning Rate: 0.01\n",
      "Epoch [1557/20000], Loss: 1207.72802734375, Entropy 12.960092544555664, Learning Rate: 0.01\n",
      "Epoch [1558/20000], Loss: 1207.8392333984375, Entropy 20.584360122680664, Learning Rate: 0.01\n",
      "Epoch [1559/20000], Loss: 1242.7127685546875, Entropy 4.589424133300781, Learning Rate: 0.01\n",
      "Epoch [1560/20000], Loss: 1317.579833984375, Entropy -3.7667064666748047, Learning Rate: 0.01\n",
      "Epoch [1561/20000], Loss: 1229.34228515625, Entropy -2.206779956817627, Learning Rate: 0.01\n",
      "Epoch [1562/20000], Loss: 1214.1956787109375, Entropy 7.993413925170898, Learning Rate: 0.01\n",
      "Epoch [1563/20000], Loss: 1190.107666015625, Entropy 20.235774993896484, Learning Rate: 0.01\n",
      "Epoch [1564/20000], Loss: 1221.46044921875, Entropy 8.33575439453125, Learning Rate: 0.01\n",
      "Epoch [1565/20000], Loss: 1313.57861328125, Entropy 0.29139846563339233, Learning Rate: 0.01\n",
      "Epoch [1566/20000], Loss: 1221.870849609375, Entropy 21.307241439819336, Learning Rate: 0.01\n",
      "Epoch [1567/20000], Loss: 1209.4859619140625, Entropy 17.712499618530273, Learning Rate: 0.01\n",
      "Epoch [1568/20000], Loss: 1230.153076171875, Entropy 9.848122596740723, Learning Rate: 0.01\n",
      "Epoch [1569/20000], Loss: 1225.42041015625, Entropy 9.317855834960938, Learning Rate: 0.01\n",
      "Epoch [1570/20000], Loss: 1220.9794921875, Entropy 15.081928253173828, Learning Rate: 0.01\n",
      "Epoch [1571/20000], Loss: 1247.850341796875, Entropy 6.865180492401123, Learning Rate: 0.01\n",
      "Epoch [1572/20000], Loss: 1226.3858642578125, Entropy 9.148467063903809, Learning Rate: 0.01\n",
      "Epoch [1573/20000], Loss: 1201.1925048828125, Entropy 6.757214069366455, Learning Rate: 0.01\n",
      "Epoch [1574/20000], Loss: 1190.4798583984375, Entropy 1.9480271339416504, Learning Rate: 0.01\n",
      "Epoch [1575/20000], Loss: 1270.4754638671875, Entropy 16.814905166625977, Learning Rate: 0.01\n",
      "Epoch [1576/20000], Loss: 1213.440673828125, Entropy 14.760661125183105, Learning Rate: 0.01\n",
      "Epoch [1577/20000], Loss: 1215.8333740234375, Entropy 18.069250106811523, Learning Rate: 0.01\n",
      "Epoch [1578/20000], Loss: 1181.4990234375, Entropy 18.82678985595703, Learning Rate: 0.01\n",
      "Epoch [1579/20000], Loss: 1219.248046875, Entropy 11.170106887817383, Learning Rate: 0.01\n",
      "Epoch [1580/20000], Loss: 1170.6654052734375, Entropy 41.39904022216797, Learning Rate: 0.01\n",
      "Epoch [1581/20000], Loss: 1199.3223876953125, Entropy 22.07679557800293, Learning Rate: 0.01\n",
      "Epoch [1582/20000], Loss: 1286.7628173828125, Entropy 11.14651870727539, Learning Rate: 0.01\n",
      "Epoch [1583/20000], Loss: 1207.25146484375, Entropy 12.550183296203613, Learning Rate: 0.01\n",
      "Epoch [1584/20000], Loss: 1246.2890625, Entropy 26.717681884765625, Learning Rate: 0.01\n",
      "Epoch [1585/20000], Loss: 1270.0294189453125, Entropy 9.700165748596191, Learning Rate: 0.01\n",
      "Epoch [1586/20000], Loss: 1182.3106689453125, Entropy 27.932905197143555, Learning Rate: 0.01\n",
      "Epoch [1587/20000], Loss: 1237.4580078125, Entropy -4.541782855987549, Learning Rate: 0.01\n",
      "Epoch [1588/20000], Loss: 1199.573974609375, Entropy 12.067683219909668, Learning Rate: 0.01\n",
      "Epoch [1589/20000], Loss: 1208.374755859375, Entropy 11.362968444824219, Learning Rate: 0.01\n",
      "Epoch [1590/20000], Loss: 1168.52880859375, Entropy 20.981496810913086, Learning Rate: 0.01\n",
      "Epoch [1591/20000], Loss: 1208.79736328125, Entropy 31.0738525390625, Learning Rate: 0.01\n",
      "Epoch [1592/20000], Loss: 1205.7109375, Entropy 20.658687591552734, Learning Rate: 0.01\n",
      "Epoch [1593/20000], Loss: 1194.1700439453125, Entropy 22.30843162536621, Learning Rate: 0.01\n",
      "Epoch [1594/20000], Loss: 1211.62158203125, Entropy 23.628271102905273, Learning Rate: 0.01\n",
      "Epoch [1595/20000], Loss: 1182.7147216796875, Entropy 24.178924560546875, Learning Rate: 0.01\n",
      "Epoch [1596/20000], Loss: 1178.433349609375, Entropy 15.084670066833496, Learning Rate: 0.01\n",
      "Epoch [1597/20000], Loss: 1207.176025390625, Entropy 5.945048809051514, Learning Rate: 0.01\n",
      "Epoch [1598/20000], Loss: 1248.280517578125, Entropy 12.252299308776855, Learning Rate: 0.01\n",
      "Epoch [1599/20000], Loss: 1213.39697265625, Entropy 38.8074836730957, Learning Rate: 0.01\n",
      "Epoch [1600/20000], Loss: 1182.6912841796875, Entropy 16.313072204589844, Learning Rate: 0.01\n",
      "Epoch [1601/20000], Loss: 1237.2540283203125, Entropy 9.609027862548828, Learning Rate: 0.01\n",
      "Epoch [1602/20000], Loss: 1177.9884033203125, Entropy 13.72977352142334, Learning Rate: 0.01\n",
      "Epoch [1603/20000], Loss: 1208.49267578125, Entropy 4.884992599487305, Learning Rate: 0.01\n",
      "Epoch [1604/20000], Loss: 1202.406982421875, Entropy 8.18862247467041, Learning Rate: 0.01\n",
      "Epoch [1605/20000], Loss: 1198.4578857421875, Entropy 22.313514709472656, Learning Rate: 0.01\n",
      "Epoch [1606/20000], Loss: 1183.5423583984375, Entropy 6.177121639251709, Learning Rate: 0.01\n",
      "Epoch [1607/20000], Loss: 1190.9000244140625, Entropy 10.449870109558105, Learning Rate: 0.01\n",
      "Epoch [1608/20000], Loss: 1193.646728515625, Entropy 18.128299713134766, Learning Rate: 0.01\n",
      "Epoch [1609/20000], Loss: 1173.1588134765625, Entropy 19.218502044677734, Learning Rate: 0.01\n",
      "Epoch [1610/20000], Loss: 1186.5277099609375, Entropy 17.172117233276367, Learning Rate: 0.01\n",
      "Epoch [1611/20000], Loss: 1225.306396484375, Entropy -2.753448486328125, Learning Rate: 0.01\n",
      "Epoch [1612/20000], Loss: 1229.1728515625, Entropy 14.839384078979492, Learning Rate: 0.01\n",
      "Epoch [1613/20000], Loss: 1180.5858154296875, Entropy 26.72359275817871, Learning Rate: 0.01\n",
      "Epoch [1614/20000], Loss: 1279.6351318359375, Entropy 16.57461929321289, Learning Rate: 0.01\n",
      "Epoch [1615/20000], Loss: 1233.7060546875, Entropy 14.434535026550293, Learning Rate: 0.01\n",
      "Epoch [1616/20000], Loss: 1206.3514404296875, Entropy 14.675909042358398, Learning Rate: 0.01\n",
      "Epoch [1617/20000], Loss: 1197.5731201171875, Entropy 19.12891387939453, Learning Rate: 0.01\n",
      "Epoch [1618/20000], Loss: 1196.3323974609375, Entropy 25.58924674987793, Learning Rate: 0.01\n",
      "Epoch [1619/20000], Loss: 1214.5234375, Entropy 14.527531623840332, Learning Rate: 0.01\n",
      "Epoch [1620/20000], Loss: 1225.3408203125, Entropy 21.674236297607422, Learning Rate: 0.01\n",
      "Epoch [1621/20000], Loss: 1230.7889404296875, Entropy 12.355290412902832, Learning Rate: 0.01\n",
      "Epoch [1622/20000], Loss: 1194.820556640625, Entropy 21.461158752441406, Learning Rate: 0.01\n",
      "Epoch [1623/20000], Loss: 1211.1881103515625, Entropy 16.162946701049805, Learning Rate: 0.01\n",
      "Epoch [1624/20000], Loss: 1173.8353271484375, Entropy 43.867393493652344, Learning Rate: 0.01\n",
      "Epoch [1625/20000], Loss: 1185.6429443359375, Entropy 20.010122299194336, Learning Rate: 0.01\n",
      "Epoch [1626/20000], Loss: 1186.8419189453125, Entropy 22.30274772644043, Learning Rate: 0.01\n",
      "Epoch [1627/20000], Loss: 1243.0997314453125, Entropy 17.32634162902832, Learning Rate: 0.01\n",
      "Epoch [1628/20000], Loss: 1218.3822021484375, Entropy 6.799807071685791, Learning Rate: 0.01\n",
      "Epoch [1629/20000], Loss: 1195.9779052734375, Entropy 21.085229873657227, Learning Rate: 0.01\n",
      "Epoch [1630/20000], Loss: 1213.882568359375, Entropy 17.5640869140625, Learning Rate: 0.01\n",
      "Epoch [1631/20000], Loss: 1149.8701171875, Entropy 30.7333984375, Learning Rate: 0.01\n",
      "Epoch [1632/20000], Loss: 1219.22021484375, Entropy 15.264632225036621, Learning Rate: 0.01\n",
      "Epoch [1633/20000], Loss: 1233.493408203125, Entropy 16.95387840270996, Learning Rate: 0.01\n",
      "Epoch [1634/20000], Loss: 1177.063232421875, Entropy 36.13245391845703, Learning Rate: 0.01\n",
      "Epoch [1635/20000], Loss: 1175.93701171875, Entropy 39.3991584777832, Learning Rate: 0.01\n",
      "Epoch [1636/20000], Loss: 1177.7811279296875, Entropy 33.11599349975586, Learning Rate: 0.01\n",
      "Epoch [1637/20000], Loss: 1256.5968017578125, Entropy 17.896697998046875, Learning Rate: 0.01\n",
      "Epoch [1638/20000], Loss: 1188.641845703125, Entropy 32.950313568115234, Learning Rate: 0.01\n",
      "Epoch [1639/20000], Loss: 1242.91845703125, Entropy 17.582672119140625, Learning Rate: 0.01\n",
      "Epoch [1640/20000], Loss: 1204.4542236328125, Entropy 28.836977005004883, Learning Rate: 0.01\n",
      "Epoch [1641/20000], Loss: 1220.1495361328125, Entropy 34.05474090576172, Learning Rate: 0.01\n",
      "Epoch [1642/20000], Loss: 1172.8759765625, Entropy 21.30636215209961, Learning Rate: 0.01\n",
      "Epoch [1643/20000], Loss: 1162.563232421875, Entropy 33.73431396484375, Learning Rate: 0.01\n",
      "Epoch [1644/20000], Loss: 1202.68896484375, Entropy 36.96615219116211, Learning Rate: 0.01\n",
      "Epoch [1645/20000], Loss: 1207.31982421875, Entropy 15.21684455871582, Learning Rate: 0.01\n",
      "Epoch [1646/20000], Loss: 1230.285888671875, Entropy 17.137889862060547, Learning Rate: 0.01\n",
      "Epoch [1647/20000], Loss: 1275.81103515625, Entropy 16.07345962524414, Learning Rate: 0.01\n",
      "Epoch [1648/20000], Loss: 1209.052001953125, Entropy 27.982948303222656, Learning Rate: 0.01\n",
      "Epoch [1649/20000], Loss: 1200.28759765625, Entropy 28.963031768798828, Learning Rate: 0.01\n",
      "Epoch [1650/20000], Loss: 1227.881591796875, Entropy 19.9185733795166, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1651/20000], Loss: 1219.767333984375, Entropy 27.872892379760742, Learning Rate: 0.01\n",
      "Epoch [1652/20000], Loss: 1148.49072265625, Entropy 40.617515563964844, Learning Rate: 0.01\n",
      "Epoch [1653/20000], Loss: 1180.3505859375, Entropy 35.65338134765625, Learning Rate: 0.01\n",
      "Epoch [1654/20000], Loss: 1154.3365478515625, Entropy 41.28309631347656, Learning Rate: 0.01\n",
      "Epoch [1655/20000], Loss: 1194.7479248046875, Entropy 40.17394256591797, Learning Rate: 0.01\n",
      "Epoch [1656/20000], Loss: 1210.6903076171875, Entropy 25.70941925048828, Learning Rate: 0.01\n",
      "Epoch [1657/20000], Loss: 1198.0308837890625, Entropy 22.245283126831055, Learning Rate: 0.01\n",
      "Epoch [1658/20000], Loss: 1184.4317626953125, Entropy 33.755958557128906, Learning Rate: 0.01\n",
      "Epoch [1659/20000], Loss: 1229.550537109375, Entropy 30.44002914428711, Learning Rate: 0.01\n",
      "Epoch [1660/20000], Loss: 1199.9830322265625, Entropy 43.46984100341797, Learning Rate: 0.01\n",
      "Epoch [1661/20000], Loss: 1209.3126220703125, Entropy 24.233173370361328, Learning Rate: 0.01\n",
      "Epoch [1662/20000], Loss: 1194.141845703125, Entropy 41.873592376708984, Learning Rate: 0.01\n",
      "Epoch [1663/20000], Loss: 1182.8018798828125, Entropy 31.764789581298828, Learning Rate: 0.01\n",
      "Epoch [1664/20000], Loss: 1187.7938232421875, Entropy 30.852602005004883, Learning Rate: 0.01\n",
      "Epoch [1665/20000], Loss: 1198.5361328125, Entropy 33.67435836791992, Learning Rate: 0.01\n",
      "Epoch [1666/20000], Loss: 1262.7509765625, Entropy 20.874515533447266, Learning Rate: 0.01\n",
      "Epoch [1667/20000], Loss: 1207.8270263671875, Entropy 42.42972183227539, Learning Rate: 0.01\n",
      "Epoch [1668/20000], Loss: 1243.2451171875, Entropy 30.3243408203125, Learning Rate: 0.01\n",
      "Epoch [1669/20000], Loss: 1146.0364990234375, Entropy 44.85704040527344, Learning Rate: 0.01\n",
      "Epoch [1670/20000], Loss: 1145.9901123046875, Entropy 54.93726348876953, Learning Rate: 0.01\n",
      "Epoch [1671/20000], Loss: 1192.6551513671875, Entropy 27.319496154785156, Learning Rate: 0.01\n",
      "Epoch [1672/20000], Loss: 1150.8165283203125, Entropy 36.70418930053711, Learning Rate: 0.01\n",
      "Epoch [1673/20000], Loss: 1184.23388671875, Entropy 41.29750442504883, Learning Rate: 0.01\n",
      "Epoch [1674/20000], Loss: 1205.52734375, Entropy 27.01466178894043, Learning Rate: 0.01\n",
      "Epoch [1675/20000], Loss: 1190.3907470703125, Entropy 35.24477005004883, Learning Rate: 0.01\n",
      "Epoch [1676/20000], Loss: 1230.238525390625, Entropy 35.13994216918945, Learning Rate: 0.01\n",
      "Epoch [1677/20000], Loss: 1197.156982421875, Entropy 39.48140335083008, Learning Rate: 0.01\n",
      "Epoch [1678/20000], Loss: 1197.9041748046875, Entropy 41.71970748901367, Learning Rate: 0.01\n",
      "Epoch [1679/20000], Loss: 1155.6953125, Entropy 38.56300354003906, Learning Rate: 0.01\n",
      "Epoch [1680/20000], Loss: 1209.357177734375, Entropy 26.798702239990234, Learning Rate: 0.01\n",
      "Epoch [1681/20000], Loss: 1228.0321044921875, Entropy 33.370235443115234, Learning Rate: 0.01\n",
      "Epoch [1682/20000], Loss: 1169.363037109375, Entropy 34.054168701171875, Learning Rate: 0.01\n",
      "Epoch [1683/20000], Loss: 1195.4898681640625, Entropy 33.42407989501953, Learning Rate: 0.01\n",
      "Epoch [1684/20000], Loss: 1160.5113525390625, Entropy 28.247297286987305, Learning Rate: 0.01\n",
      "Epoch [1685/20000], Loss: 1194.3074951171875, Entropy 40.37124252319336, Learning Rate: 0.01\n",
      "Epoch [1686/20000], Loss: 1165.66357421875, Entropy 36.31403350830078, Learning Rate: 0.01\n",
      "Epoch [1687/20000], Loss: 1162.6253662109375, Entropy 37.46927261352539, Learning Rate: 0.01\n",
      "Epoch [1688/20000], Loss: 1162.8739013671875, Entropy 44.747947692871094, Learning Rate: 0.01\n",
      "Epoch [1689/20000], Loss: 1209.3843994140625, Entropy 45.7120246887207, Learning Rate: 0.01\n",
      "Epoch [1690/20000], Loss: 1175.5740966796875, Entropy 44.94721984863281, Learning Rate: 0.01\n",
      "Epoch [1691/20000], Loss: 1178.9964599609375, Entropy 47.695960998535156, Learning Rate: 0.01\n",
      "Epoch [1692/20000], Loss: 1209.93896484375, Entropy 30.99907112121582, Learning Rate: 0.01\n",
      "Epoch [1693/20000], Loss: 1155.9127197265625, Entropy 46.282318115234375, Learning Rate: 0.01\n",
      "Epoch [1694/20000], Loss: 1184.818359375, Entropy 47.80780029296875, Learning Rate: 0.01\n",
      "Epoch [1695/20000], Loss: 1172.05908203125, Entropy 31.574607849121094, Learning Rate: 0.01\n",
      "Epoch [1696/20000], Loss: 1143.726806640625, Entropy 39.206459045410156, Learning Rate: 0.01\n",
      "Epoch [1697/20000], Loss: 1193.13720703125, Entropy 38.46044158935547, Learning Rate: 0.01\n",
      "Epoch [1698/20000], Loss: 1189.5927734375, Entropy 36.14960479736328, Learning Rate: 0.01\n",
      "Epoch [1699/20000], Loss: 1214.71484375, Entropy 21.052080154418945, Learning Rate: 0.01\n",
      "Epoch [1700/20000], Loss: 1230.7537841796875, Entropy 21.482744216918945, Learning Rate: 0.01\n",
      "Epoch [1701/20000], Loss: 1167.5267333984375, Entropy 48.61839294433594, Learning Rate: 0.01\n",
      "Epoch [1702/20000], Loss: 1210.775390625, Entropy 35.00080871582031, Learning Rate: 0.01\n",
      "Epoch [1703/20000], Loss: 1176.254638671875, Entropy 31.21770477294922, Learning Rate: 0.01\n",
      "Epoch [1704/20000], Loss: 1155.653564453125, Entropy 46.14175796508789, Learning Rate: 0.01\n",
      "Epoch [1705/20000], Loss: 1182.2554931640625, Entropy 40.266204833984375, Learning Rate: 0.01\n",
      "Epoch [1706/20000], Loss: 1204.8726806640625, Entropy 36.2756462097168, Learning Rate: 0.01\n",
      "Epoch [1707/20000], Loss: 1213.5404052734375, Entropy 30.731290817260742, Learning Rate: 0.01\n",
      "Epoch [1708/20000], Loss: 1185.1766357421875, Entropy 58.473880767822266, Learning Rate: 0.01\n",
      "Epoch [1709/20000], Loss: 1340.8316650390625, Entropy 39.37508773803711, Learning Rate: 0.01\n",
      "Epoch [1710/20000], Loss: 1177.0345458984375, Entropy 44.7835578918457, Learning Rate: 0.01\n",
      "Epoch [1711/20000], Loss: 1149.3970947265625, Entropy 45.83798599243164, Learning Rate: 0.01\n",
      "Epoch [1712/20000], Loss: 1172.72314453125, Entropy 36.01508712768555, Learning Rate: 0.01\n",
      "Epoch [1713/20000], Loss: 1152.05615234375, Entropy 39.2800407409668, Learning Rate: 0.01\n",
      "Epoch [1714/20000], Loss: 1155.833251953125, Entropy 49.95101547241211, Learning Rate: 0.01\n",
      "Epoch [1715/20000], Loss: 1136.76025390625, Entropy 49.94361877441406, Learning Rate: 0.01\n",
      "Epoch [1716/20000], Loss: 1176.8531494140625, Entropy 47.67523956298828, Learning Rate: 0.01\n",
      "Epoch [1717/20000], Loss: 1156.4161376953125, Entropy 49.92617416381836, Learning Rate: 0.01\n",
      "Epoch [1718/20000], Loss: 1201.2132568359375, Entropy 56.999237060546875, Learning Rate: 0.01\n",
      "Epoch [1719/20000], Loss: 1198.4610595703125, Entropy 33.91733932495117, Learning Rate: 0.01\n",
      "Epoch [1720/20000], Loss: 1168.3529052734375, Entropy 60.97707748413086, Learning Rate: 0.01\n",
      "Epoch [1721/20000], Loss: 1199.45751953125, Entropy 46.53680419921875, Learning Rate: 0.01\n",
      "Epoch [1722/20000], Loss: 1143.056640625, Entropy 53.16788864135742, Learning Rate: 0.01\n",
      "Epoch [1723/20000], Loss: 1179.45751953125, Entropy 29.419456481933594, Learning Rate: 0.01\n",
      "Epoch [1724/20000], Loss: 1177.09716796875, Entropy 33.97085189819336, Learning Rate: 0.01\n",
      "Epoch [1725/20000], Loss: 1135.651123046875, Entropy 52.21784973144531, Learning Rate: 0.01\n",
      "Epoch [1726/20000], Loss: 1265.504150390625, Entropy 37.920711517333984, Learning Rate: 0.01\n",
      "Epoch [1727/20000], Loss: 1182.5614013671875, Entropy 51.014198303222656, Learning Rate: 0.01\n",
      "Epoch [1728/20000], Loss: 1122.7154541015625, Entropy 54.611785888671875, Learning Rate: 0.01\n",
      "Epoch [1729/20000], Loss: 1158.5635986328125, Entropy 38.97960662841797, Learning Rate: 0.01\n",
      "Epoch [1730/20000], Loss: 1174.6258544921875, Entropy 66.52779388427734, Learning Rate: 0.01\n",
      "Epoch [1731/20000], Loss: 1246.11572265625, Entropy 42.83571243286133, Learning Rate: 0.01\n",
      "Epoch [1732/20000], Loss: 1166.9462890625, Entropy 26.792118072509766, Learning Rate: 0.01\n",
      "Epoch [1733/20000], Loss: 1216.96240234375, Entropy 40.5757942199707, Learning Rate: 0.01\n",
      "Epoch [1734/20000], Loss: 1183.876220703125, Entropy 39.685508728027344, Learning Rate: 0.01\n",
      "Epoch [1735/20000], Loss: 1211.7772216796875, Entropy 55.03773498535156, Learning Rate: 0.01\n",
      "Epoch [1736/20000], Loss: 1198.4189453125, Entropy 46.279541015625, Learning Rate: 0.01\n",
      "Epoch [1737/20000], Loss: 1174.5682373046875, Entropy 55.53438186645508, Learning Rate: 0.01\n",
      "Epoch [1738/20000], Loss: 1184.576171875, Entropy 51.513118743896484, Learning Rate: 0.01\n",
      "Epoch [1739/20000], Loss: 1156.564208984375, Entropy 50.56602478027344, Learning Rate: 0.01\n",
      "Epoch [1740/20000], Loss: 1183.6080322265625, Entropy 63.80769348144531, Learning Rate: 0.01\n",
      "Epoch [1741/20000], Loss: 1245.9344482421875, Entropy 46.82558822631836, Learning Rate: 0.01\n",
      "Epoch [1742/20000], Loss: 1126.2691650390625, Entropy 53.59380340576172, Learning Rate: 0.01\n",
      "Epoch [1743/20000], Loss: 1144.7518310546875, Entropy 51.01499938964844, Learning Rate: 0.01\n",
      "Epoch [1744/20000], Loss: 1194.4447021484375, Entropy 32.31191635131836, Learning Rate: 0.01\n",
      "Epoch [1745/20000], Loss: 1174.3812255859375, Entropy 48.06514358520508, Learning Rate: 0.01\n",
      "Epoch [1746/20000], Loss: 1149.1937255859375, Entropy 56.342655181884766, Learning Rate: 0.01\n",
      "Epoch [1747/20000], Loss: 1164.296142578125, Entropy 38.61505126953125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1748/20000], Loss: 1166.6729736328125, Entropy 50.70547866821289, Learning Rate: 0.01\n",
      "Epoch [1749/20000], Loss: 1170.1248779296875, Entropy 60.22447967529297, Learning Rate: 0.01\n",
      "Epoch [1750/20000], Loss: 1169.5733642578125, Entropy 62.27387619018555, Learning Rate: 0.01\n",
      "Epoch [1751/20000], Loss: 1192.427734375, Entropy 39.628421783447266, Learning Rate: 0.01\n",
      "Epoch [1752/20000], Loss: 1186.372802734375, Entropy 48.8791389465332, Learning Rate: 0.01\n",
      "Epoch [1753/20000], Loss: 1151.73876953125, Entropy 44.85824203491211, Learning Rate: 0.01\n",
      "Epoch [1754/20000], Loss: 1136.4237060546875, Entropy 39.29749298095703, Learning Rate: 0.01\n",
      "Epoch [1755/20000], Loss: 1147.01123046875, Entropy 61.30915832519531, Learning Rate: 0.01\n",
      "Epoch [1756/20000], Loss: 1217.145751953125, Entropy 37.60562515258789, Learning Rate: 0.01\n",
      "Epoch [1757/20000], Loss: 1210.6728515625, Entropy 53.27460861206055, Learning Rate: 0.01\n",
      "Epoch [1758/20000], Loss: 1175.750732421875, Entropy 54.94626998901367, Learning Rate: 0.01\n",
      "Epoch [1759/20000], Loss: 1230.0078125, Entropy 60.137451171875, Learning Rate: 0.01\n",
      "Epoch [1760/20000], Loss: 1149.4041748046875, Entropy 57.88349533081055, Learning Rate: 0.01\n",
      "Epoch [1761/20000], Loss: 1167.09912109375, Entropy 61.36701965332031, Learning Rate: 0.01\n",
      "Epoch [1762/20000], Loss: 1215.9615478515625, Entropy 51.06654357910156, Learning Rate: 0.01\n",
      "Epoch [1763/20000], Loss: 1169.9234619140625, Entropy 44.06283187866211, Learning Rate: 0.01\n",
      "Epoch [1764/20000], Loss: 1156.642333984375, Entropy 59.1146125793457, Learning Rate: 0.01\n",
      "Epoch [1765/20000], Loss: 1175.5821533203125, Entropy 54.306190490722656, Learning Rate: 0.01\n",
      "Epoch [1766/20000], Loss: 1123.8902587890625, Entropy 67.28939819335938, Learning Rate: 0.01\n",
      "Epoch [1767/20000], Loss: 1153.132080078125, Entropy 63.6455078125, Learning Rate: 0.01\n",
      "Epoch [1768/20000], Loss: 1226.16748046875, Entropy 37.879966735839844, Learning Rate: 0.01\n",
      "Epoch [1769/20000], Loss: 1155.2313232421875, Entropy 55.88460922241211, Learning Rate: 0.01\n",
      "Epoch [1770/20000], Loss: 1174.33349609375, Entropy 42.913700103759766, Learning Rate: 0.01\n",
      "Epoch [1771/20000], Loss: 1177.8499755859375, Entropy 59.68201446533203, Learning Rate: 0.01\n",
      "Epoch [1772/20000], Loss: 1133.5283203125, Entropy 55.788963317871094, Learning Rate: 0.01\n",
      "Epoch [1773/20000], Loss: 1178.1173095703125, Entropy 37.327056884765625, Learning Rate: 0.01\n",
      "Epoch [1774/20000], Loss: 1182.1348876953125, Entropy 57.91586685180664, Learning Rate: 0.01\n",
      "Epoch [1775/20000], Loss: 1163.04345703125, Entropy 45.7429084777832, Learning Rate: 0.01\n",
      "Epoch [1776/20000], Loss: 1144.479736328125, Entropy 63.187137603759766, Learning Rate: 0.01\n",
      "Epoch [1777/20000], Loss: 1266.6968994140625, Entropy 47.54430389404297, Learning Rate: 0.01\n",
      "Epoch [1778/20000], Loss: 1148.9659423828125, Entropy 63.17832565307617, Learning Rate: 0.01\n",
      "Epoch [1779/20000], Loss: 1168.6837158203125, Entropy 47.84684371948242, Learning Rate: 0.01\n",
      "Epoch [1780/20000], Loss: 1146.1341552734375, Entropy 64.660400390625, Learning Rate: 0.01\n",
      "Epoch [1781/20000], Loss: 1127.6119384765625, Entropy 67.05516052246094, Learning Rate: 0.01\n",
      "Epoch [1782/20000], Loss: 1196.9078369140625, Entropy 53.12019348144531, Learning Rate: 0.01\n",
      "Epoch [1783/20000], Loss: 1209.28857421875, Entropy 56.08778381347656, Learning Rate: 0.01\n",
      "Epoch [1784/20000], Loss: 1165.4580078125, Entropy 70.053466796875, Learning Rate: 0.01\n",
      "Epoch [1785/20000], Loss: 1138.5087890625, Entropy 72.69989013671875, Learning Rate: 0.01\n",
      "Epoch [1786/20000], Loss: 1162.37841796875, Entropy 66.68266296386719, Learning Rate: 0.01\n",
      "Epoch [1787/20000], Loss: 1157.7476806640625, Entropy 57.84939956665039, Learning Rate: 0.01\n",
      "Epoch [1788/20000], Loss: 1213.1097412109375, Entropy 64.43848419189453, Learning Rate: 0.01\n",
      "Epoch [1789/20000], Loss: 1180.3006591796875, Entropy 44.39194869995117, Learning Rate: 0.01\n",
      "Epoch [1790/20000], Loss: 1137.5760498046875, Entropy 62.698055267333984, Learning Rate: 0.01\n",
      "Epoch [1791/20000], Loss: 1130.571533203125, Entropy 67.47799682617188, Learning Rate: 0.01\n",
      "Epoch [1792/20000], Loss: 1116.001708984375, Entropy 79.31106567382812, Learning Rate: 0.01\n",
      "Epoch [1793/20000], Loss: 1143.1083984375, Entropy 56.313377380371094, Learning Rate: 0.01\n",
      "Epoch [1794/20000], Loss: 1170.848876953125, Entropy 49.311859130859375, Learning Rate: 0.01\n",
      "Epoch [1795/20000], Loss: 1147.4261474609375, Entropy 72.9797592163086, Learning Rate: 0.01\n",
      "Epoch [1796/20000], Loss: 1171.9090576171875, Entropy 71.23238372802734, Learning Rate: 0.01\n",
      "Epoch [1797/20000], Loss: 1151.01513671875, Entropy 64.84767150878906, Learning Rate: 0.01\n",
      "Epoch [1798/20000], Loss: 1144.681640625, Entropy 63.70737838745117, Learning Rate: 0.01\n",
      "Epoch [1799/20000], Loss: 1178.6265869140625, Entropy 68.7111587524414, Learning Rate: 0.01\n",
      "Epoch [1800/20000], Loss: 1111.20703125, Entropy 58.40192413330078, Learning Rate: 0.01\n",
      "Epoch [1801/20000], Loss: 1253.7801513671875, Entropy 46.64635467529297, Learning Rate: 0.01\n",
      "Epoch [1802/20000], Loss: 1268.080322265625, Entropy 58.1378288269043, Learning Rate: 0.01\n",
      "Epoch [1803/20000], Loss: 1126.73388671875, Entropy 77.01123046875, Learning Rate: 0.01\n",
      "Epoch [1804/20000], Loss: 1191.1175537109375, Entropy 69.54219818115234, Learning Rate: 0.01\n",
      "Epoch [1805/20000], Loss: 1231.18408203125, Entropy 66.93119812011719, Learning Rate: 0.01\n",
      "Epoch [1806/20000], Loss: 1188.8060302734375, Entropy 67.59947967529297, Learning Rate: 0.01\n",
      "Epoch [1807/20000], Loss: 1190.0869140625, Entropy 61.78688430786133, Learning Rate: 0.01\n",
      "Epoch [1808/20000], Loss: 1140.2957763671875, Entropy 47.31377029418945, Learning Rate: 0.01\n",
      "Epoch [1809/20000], Loss: 1141.514892578125, Entropy 68.45042419433594, Learning Rate: 0.01\n",
      "Epoch [1810/20000], Loss: 1171.576904296875, Entropy 63.24137496948242, Learning Rate: 0.01\n",
      "Epoch [1811/20000], Loss: 1165.202392578125, Entropy 64.80023193359375, Learning Rate: 0.01\n",
      "Epoch [1812/20000], Loss: 1132.8271484375, Entropy 70.05204010009766, Learning Rate: 0.01\n",
      "Epoch [1813/20000], Loss: 1159.621337890625, Entropy 57.39373016357422, Learning Rate: 0.01\n",
      "Epoch [1814/20000], Loss: 1254.907958984375, Entropy 53.03711700439453, Learning Rate: 0.01\n",
      "Epoch [1815/20000], Loss: 1171.8282470703125, Entropy 57.28053665161133, Learning Rate: 0.01\n",
      "Epoch [1816/20000], Loss: 1164.6796875, Entropy 68.85369873046875, Learning Rate: 0.01\n",
      "Epoch [1817/20000], Loss: 1130.5811767578125, Entropy 71.49781036376953, Learning Rate: 0.01\n",
      "Epoch [1818/20000], Loss: 1227.5478515625, Entropy 57.03947067260742, Learning Rate: 0.01\n",
      "Epoch [1819/20000], Loss: 1175.7025146484375, Entropy 60.92280960083008, Learning Rate: 0.01\n",
      "Epoch [1820/20000], Loss: 1138.0028076171875, Entropy 76.00165557861328, Learning Rate: 0.01\n",
      "Epoch [1821/20000], Loss: 1156.760498046875, Entropy 54.341243743896484, Learning Rate: 0.01\n",
      "Epoch [1822/20000], Loss: 1159.252197265625, Entropy 63.82029724121094, Learning Rate: 0.01\n",
      "Epoch [1823/20000], Loss: 1113.2142333984375, Entropy 72.25416564941406, Learning Rate: 0.01\n",
      "Epoch [1824/20000], Loss: 1136.927734375, Entropy 72.46830749511719, Learning Rate: 0.01\n",
      "Epoch [1825/20000], Loss: 1175.31396484375, Entropy 69.6074447631836, Learning Rate: 0.01\n",
      "Epoch [1826/20000], Loss: 1163.983642578125, Entropy 36.06501007080078, Learning Rate: 0.01\n",
      "Epoch [1827/20000], Loss: 1168.79052734375, Entropy 64.89044189453125, Learning Rate: 0.01\n",
      "Epoch [1828/20000], Loss: 1202.730224609375, Entropy 60.18182373046875, Learning Rate: 0.01\n",
      "Epoch [1829/20000], Loss: 1125.5203857421875, Entropy 65.06578063964844, Learning Rate: 0.01\n",
      "Epoch [1830/20000], Loss: 1147.8206787109375, Entropy 68.78697967529297, Learning Rate: 0.01\n",
      "Epoch [1831/20000], Loss: 1142.4599609375, Entropy 55.08543014526367, Learning Rate: 0.01\n",
      "Epoch [1832/20000], Loss: 1159.011474609375, Entropy 61.05907440185547, Learning Rate: 0.01\n",
      "Epoch [1833/20000], Loss: 1154.216796875, Entropy 65.20519256591797, Learning Rate: 0.01\n",
      "Epoch [1834/20000], Loss: 1152.529052734375, Entropy 72.3857650756836, Learning Rate: 0.01\n",
      "Epoch [1835/20000], Loss: 1180.3321533203125, Entropy 45.15138626098633, Learning Rate: 0.01\n",
      "Epoch [1836/20000], Loss: 1156.2115478515625, Entropy 58.27897262573242, Learning Rate: 0.01\n",
      "Epoch [1837/20000], Loss: 1140.5714111328125, Entropy 71.310546875, Learning Rate: 0.01\n",
      "Epoch [1838/20000], Loss: 1164.7576904296875, Entropy 71.69025421142578, Learning Rate: 0.01\n",
      "Epoch [1839/20000], Loss: 1137.9637451171875, Entropy 64.39070129394531, Learning Rate: 0.01\n",
      "Epoch [1840/20000], Loss: 1187.0396728515625, Entropy 59.39411163330078, Learning Rate: 0.01\n",
      "Epoch [1841/20000], Loss: 1194.736328125, Entropy 61.19118881225586, Learning Rate: 0.01\n",
      "Epoch [1842/20000], Loss: 1237.826416015625, Entropy 69.60796356201172, Learning Rate: 0.01\n",
      "Epoch [1843/20000], Loss: 1128.01171875, Entropy 60.19877624511719, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1844/20000], Loss: 1128.9495849609375, Entropy 63.5851936340332, Learning Rate: 0.01\n",
      "Epoch [1845/20000], Loss: 1145.2496337890625, Entropy 70.31875610351562, Learning Rate: 0.01\n",
      "Epoch [1846/20000], Loss: 1149.3203125, Entropy 75.31619262695312, Learning Rate: 0.01\n",
      "Epoch [1847/20000], Loss: 1134.12353515625, Entropy 70.29447174072266, Learning Rate: 0.01\n",
      "Epoch [1848/20000], Loss: 1137.0682373046875, Entropy 53.097434997558594, Learning Rate: 0.01\n",
      "Epoch [1849/20000], Loss: 1146.1048583984375, Entropy 81.05692291259766, Learning Rate: 0.01\n",
      "Epoch [1850/20000], Loss: 1148.5416259765625, Entropy 80.19532012939453, Learning Rate: 0.01\n",
      "Epoch [1851/20000], Loss: 1139.353759765625, Entropy 71.64785766601562, Learning Rate: 0.01\n",
      "Epoch [1852/20000], Loss: 1182.16845703125, Entropy 68.33982849121094, Learning Rate: 0.01\n",
      "Epoch [1853/20000], Loss: 1149.0926513671875, Entropy 67.29313659667969, Learning Rate: 0.01\n",
      "Epoch [1854/20000], Loss: 1157.3319091796875, Entropy 81.09967803955078, Learning Rate: 0.01\n",
      "Epoch [1855/20000], Loss: 1150.317626953125, Entropy 82.17414855957031, Learning Rate: 0.01\n",
      "Epoch [1856/20000], Loss: 1157.350341796875, Entropy 62.03180694580078, Learning Rate: 0.01\n",
      "Epoch [1857/20000], Loss: 1123.5484619140625, Entropy 74.16938018798828, Learning Rate: 0.01\n",
      "Epoch [1858/20000], Loss: 1189.2176513671875, Entropy 69.21408081054688, Learning Rate: 0.01\n",
      "Epoch [1859/20000], Loss: 1166.9708251953125, Entropy 73.75237274169922, Learning Rate: 0.01\n",
      "Epoch [1860/20000], Loss: 1137.6866455078125, Entropy 79.92390441894531, Learning Rate: 0.01\n",
      "Epoch [1861/20000], Loss: 1127.981201171875, Entropy 85.13815307617188, Learning Rate: 0.01\n",
      "Epoch [1862/20000], Loss: 1172.5277099609375, Entropy 69.0503921508789, Learning Rate: 0.01\n",
      "Epoch [1863/20000], Loss: 1175.6934814453125, Entropy 71.3287353515625, Learning Rate: 0.01\n",
      "Epoch [1864/20000], Loss: 1131.1602783203125, Entropy 81.67809295654297, Learning Rate: 0.01\n",
      "Epoch [1865/20000], Loss: 1115.7183837890625, Entropy 80.36991882324219, Learning Rate: 0.01\n",
      "Epoch [1866/20000], Loss: 1183.0396728515625, Entropy 67.57103729248047, Learning Rate: 0.01\n",
      "Epoch [1867/20000], Loss: 1145.8018798828125, Entropy 80.8344497680664, Learning Rate: 0.01\n",
      "Epoch [1868/20000], Loss: 1129.057373046875, Entropy 78.98521423339844, Learning Rate: 0.01\n",
      "Epoch [1869/20000], Loss: 1122.92626953125, Entropy 68.42044830322266, Learning Rate: 0.01\n",
      "Epoch [1870/20000], Loss: 1133.0989990234375, Entropy 75.87474060058594, Learning Rate: 0.01\n",
      "Epoch [1871/20000], Loss: 1135.848876953125, Entropy 65.16194152832031, Learning Rate: 0.01\n",
      "Epoch [1872/20000], Loss: 1137.5850830078125, Entropy 70.46318817138672, Learning Rate: 0.01\n",
      "Epoch [1873/20000], Loss: 1149.4295654296875, Entropy 75.48475646972656, Learning Rate: 0.01\n",
      "Epoch [1874/20000], Loss: 1152.45263671875, Entropy 68.78679656982422, Learning Rate: 0.01\n",
      "Epoch [1875/20000], Loss: 1124.3992919921875, Entropy 68.74617004394531, Learning Rate: 0.01\n",
      "Epoch [1876/20000], Loss: 1105.0225830078125, Entropy 75.38888549804688, Learning Rate: 0.01\n",
      "Epoch [1877/20000], Loss: 1194.77880859375, Entropy 84.72681427001953, Learning Rate: 0.01\n",
      "Epoch [1878/20000], Loss: 1146.0440673828125, Entropy 87.03864288330078, Learning Rate: 0.01\n",
      "Epoch [1879/20000], Loss: 1155.38916015625, Entropy 76.50125885009766, Learning Rate: 0.01\n",
      "Epoch [1880/20000], Loss: 1135.6490478515625, Entropy 91.83616638183594, Learning Rate: 0.01\n",
      "Epoch [1881/20000], Loss: 1104.7708740234375, Entropy 83.06114959716797, Learning Rate: 0.01\n",
      "Epoch [1882/20000], Loss: 1234.38623046875, Entropy 75.7203140258789, Learning Rate: 0.01\n",
      "Epoch [1883/20000], Loss: 1166.7593994140625, Entropy 97.37564086914062, Learning Rate: 0.01\n",
      "Epoch [1884/20000], Loss: 1162.9207763671875, Entropy 79.06971740722656, Learning Rate: 0.01\n",
      "Epoch [1885/20000], Loss: 1144.396484375, Entropy 77.37867736816406, Learning Rate: 0.01\n",
      "Epoch [1886/20000], Loss: 1129.0859375, Entropy 78.7882080078125, Learning Rate: 0.01\n",
      "Epoch [1887/20000], Loss: 1104.8375244140625, Entropy 94.01902770996094, Learning Rate: 0.01\n",
      "Epoch [1888/20000], Loss: 1158.64990234375, Entropy 74.61404418945312, Learning Rate: 0.01\n",
      "Epoch [1889/20000], Loss: 1086.8712158203125, Entropy 80.17874908447266, Learning Rate: 0.01\n",
      "Epoch [1890/20000], Loss: 1130.8563232421875, Entropy 84.48179626464844, Learning Rate: 0.01\n",
      "Epoch [1891/20000], Loss: 1165.8922119140625, Entropy 81.58023071289062, Learning Rate: 0.01\n",
      "Epoch [1892/20000], Loss: 1140.5628662109375, Entropy 90.24464416503906, Learning Rate: 0.01\n",
      "Epoch [1893/20000], Loss: 1140.576171875, Entropy 63.55012512207031, Learning Rate: 0.01\n",
      "Epoch [1894/20000], Loss: 1116.7540283203125, Entropy 83.07492065429688, Learning Rate: 0.01\n",
      "Epoch [1895/20000], Loss: 1117.9932861328125, Entropy 99.80194854736328, Learning Rate: 0.01\n",
      "Epoch [1896/20000], Loss: 1116.960205078125, Entropy 96.89984893798828, Learning Rate: 0.01\n",
      "Epoch [1897/20000], Loss: 1146.173828125, Entropy 74.97521209716797, Learning Rate: 0.01\n",
      "Epoch [1898/20000], Loss: 1140.0758056640625, Entropy 93.24947357177734, Learning Rate: 0.01\n",
      "Epoch [1899/20000], Loss: 1129.7010498046875, Entropy 81.4567642211914, Learning Rate: 0.01\n",
      "Epoch [1900/20000], Loss: 1158.0428466796875, Entropy 62.94777297973633, Learning Rate: 0.01\n",
      "Epoch [1901/20000], Loss: 1118.466552734375, Entropy 104.0541763305664, Learning Rate: 0.01\n",
      "Epoch [1902/20000], Loss: 1137.21337890625, Entropy 94.93252563476562, Learning Rate: 0.01\n",
      "Epoch [1903/20000], Loss: 1124.2059326171875, Entropy 85.7201919555664, Learning Rate: 0.01\n",
      "Epoch [1904/20000], Loss: 1183.3614501953125, Entropy 86.95121002197266, Learning Rate: 0.01\n",
      "Epoch [1905/20000], Loss: 1118.892578125, Entropy 87.8680648803711, Learning Rate: 0.01\n",
      "Epoch [1906/20000], Loss: 1121.3348388671875, Entropy 86.7491683959961, Learning Rate: 0.01\n",
      "Epoch [1907/20000], Loss: 1147.7003173828125, Entropy 88.7603759765625, Learning Rate: 0.01\n",
      "Epoch [1908/20000], Loss: 1175.6697998046875, Entropy 85.37749481201172, Learning Rate: 0.01\n",
      "Epoch [1909/20000], Loss: 1099.891357421875, Entropy 97.41494750976562, Learning Rate: 0.01\n",
      "Epoch [1910/20000], Loss: 1118.0858154296875, Entropy 85.6463623046875, Learning Rate: 0.01\n",
      "Epoch [1911/20000], Loss: 1108.6497802734375, Entropy 83.12673950195312, Learning Rate: 0.01\n",
      "Epoch [1912/20000], Loss: 1159.7333984375, Entropy 78.21241760253906, Learning Rate: 0.01\n",
      "Epoch [1913/20000], Loss: 1114.5101318359375, Entropy 92.56267547607422, Learning Rate: 0.01\n",
      "Epoch [1914/20000], Loss: 1106.6685791015625, Entropy 86.87353515625, Learning Rate: 0.01\n",
      "Epoch [1915/20000], Loss: 1130.9549560546875, Entropy 85.45132446289062, Learning Rate: 0.01\n",
      "Epoch [1916/20000], Loss: 1180.4605712890625, Entropy 88.6552505493164, Learning Rate: 0.01\n",
      "Epoch [1917/20000], Loss: 1139.5125732421875, Entropy 81.84305572509766, Learning Rate: 0.01\n",
      "Epoch [1918/20000], Loss: 1129.5040283203125, Entropy 93.3931884765625, Learning Rate: 0.01\n",
      "Epoch [1919/20000], Loss: 1169.908203125, Entropy 84.75350189208984, Learning Rate: 0.01\n",
      "Epoch [1920/20000], Loss: 1153.1966552734375, Entropy 95.87549591064453, Learning Rate: 0.01\n",
      "Epoch [1921/20000], Loss: 1151.4755859375, Entropy 94.79949951171875, Learning Rate: 0.01\n",
      "Epoch [1922/20000], Loss: 1129.67626953125, Entropy 90.3292007446289, Learning Rate: 0.01\n",
      "Epoch [1923/20000], Loss: 1126.0123291015625, Entropy 89.50387573242188, Learning Rate: 0.01\n",
      "Epoch [1924/20000], Loss: 1122.93115234375, Entropy 90.68988037109375, Learning Rate: 0.01\n",
      "Epoch [1925/20000], Loss: 1140.927978515625, Entropy 103.71498107910156, Learning Rate: 0.01\n",
      "Epoch [1926/20000], Loss: 1225.2867431640625, Entropy 96.56703186035156, Learning Rate: 0.01\n",
      "Epoch [1927/20000], Loss: 1106.9888916015625, Entropy 110.32084655761719, Learning Rate: 0.01\n",
      "Epoch [1928/20000], Loss: 1157.55859375, Entropy 97.78192138671875, Learning Rate: 0.01\n",
      "Epoch [1929/20000], Loss: 1146.4298095703125, Entropy 90.6033706665039, Learning Rate: 0.01\n",
      "Epoch [1930/20000], Loss: 1132.7564697265625, Entropy 95.70295715332031, Learning Rate: 0.01\n",
      "Epoch [1931/20000], Loss: 1171.081298828125, Entropy 89.6930160522461, Learning Rate: 0.01\n",
      "Epoch [1932/20000], Loss: 1169.8807373046875, Entropy 86.39118957519531, Learning Rate: 0.01\n",
      "Epoch [1933/20000], Loss: 1113.200927734375, Entropy 87.78121185302734, Learning Rate: 0.01\n",
      "Epoch [1934/20000], Loss: 1128.810791015625, Entropy 89.65692138671875, Learning Rate: 0.01\n",
      "Epoch [1935/20000], Loss: 1145.35888671875, Entropy 90.47943115234375, Learning Rate: 0.01\n",
      "Epoch [1936/20000], Loss: 1110.5380859375, Entropy 79.01576232910156, Learning Rate: 0.01\n",
      "Epoch [1937/20000], Loss: 1183.4268798828125, Entropy 93.29126739501953, Learning Rate: 0.01\n",
      "Epoch [1938/20000], Loss: 1147.558837890625, Entropy 95.91143035888672, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1939/20000], Loss: 1166.2823486328125, Entropy 92.70791625976562, Learning Rate: 0.01\n",
      "Epoch [1940/20000], Loss: 1129.5435791015625, Entropy 102.17792510986328, Learning Rate: 0.01\n",
      "Epoch [1941/20000], Loss: 1117.332763671875, Entropy 95.86673736572266, Learning Rate: 0.01\n",
      "Epoch [1942/20000], Loss: 1134.282470703125, Entropy 100.05138397216797, Learning Rate: 0.01\n",
      "Epoch [1943/20000], Loss: 1149.061767578125, Entropy 78.12477111816406, Learning Rate: 0.01\n",
      "Epoch [1944/20000], Loss: 1146.6031494140625, Entropy 94.5950927734375, Learning Rate: 0.01\n",
      "Epoch [1945/20000], Loss: 1151.0888671875, Entropy 86.94720458984375, Learning Rate: 0.01\n",
      "Epoch [1946/20000], Loss: 1143.0123291015625, Entropy 87.30339050292969, Learning Rate: 0.01\n",
      "Epoch [1947/20000], Loss: 1143.8748779296875, Entropy 80.96192932128906, Learning Rate: 0.01\n",
      "Epoch [1948/20000], Loss: 1108.782958984375, Entropy 93.47004699707031, Learning Rate: 0.01\n",
      "Epoch [1949/20000], Loss: 1091.7730712890625, Entropy 85.64143371582031, Learning Rate: 0.01\n",
      "Epoch [1950/20000], Loss: 1122.7098388671875, Entropy 98.64261627197266, Learning Rate: 0.01\n",
      "Epoch [1951/20000], Loss: 1150.0078125, Entropy 79.49762725830078, Learning Rate: 0.01\n",
      "Epoch [1952/20000], Loss: 1119.791015625, Entropy 91.26215362548828, Learning Rate: 0.01\n",
      "Epoch [1953/20000], Loss: 1106.0233154296875, Entropy 89.49923706054688, Learning Rate: 0.01\n",
      "Epoch [1954/20000], Loss: 1124.2489013671875, Entropy 90.32359313964844, Learning Rate: 0.01\n",
      "Epoch [1955/20000], Loss: 1143.6195068359375, Entropy 99.51509857177734, Learning Rate: 0.01\n",
      "Epoch [1956/20000], Loss: 1098.604736328125, Entropy 84.81781005859375, Learning Rate: 0.01\n",
      "Epoch [1957/20000], Loss: 1074.5078125, Entropy 103.51262664794922, Learning Rate: 0.01\n",
      "Epoch [1958/20000], Loss: 1119.5673828125, Entropy 92.47869110107422, Learning Rate: 0.01\n",
      "Epoch [1959/20000], Loss: 1110.015380859375, Entropy 94.37358093261719, Learning Rate: 0.01\n",
      "Epoch [1960/20000], Loss: 1122.3509521484375, Entropy 95.44991302490234, Learning Rate: 0.01\n",
      "Epoch [1961/20000], Loss: 1148.7186279296875, Entropy 88.48699188232422, Learning Rate: 0.01\n",
      "Epoch [1962/20000], Loss: 1183.076904296875, Entropy 89.53050231933594, Learning Rate: 0.01\n",
      "Epoch [1963/20000], Loss: 1175.8529052734375, Entropy 92.30974578857422, Learning Rate: 0.01\n",
      "Epoch [1964/20000], Loss: 1108.9605712890625, Entropy 102.93220520019531, Learning Rate: 0.01\n",
      "Epoch [1965/20000], Loss: 1130.091796875, Entropy 73.23271942138672, Learning Rate: 0.01\n",
      "Epoch [1966/20000], Loss: 1181.4818115234375, Entropy 93.318603515625, Learning Rate: 0.01\n",
      "Epoch [1967/20000], Loss: 1125.6353759765625, Entropy 83.05105590820312, Learning Rate: 0.01\n",
      "Epoch [1968/20000], Loss: 1164.270751953125, Entropy 99.03955078125, Learning Rate: 0.01\n",
      "Epoch [1969/20000], Loss: 1214.854736328125, Entropy 110.34554290771484, Learning Rate: 0.01\n",
      "Epoch [1970/20000], Loss: 1083.910888671875, Entropy 89.70652770996094, Learning Rate: 0.01\n",
      "Epoch [1971/20000], Loss: 1192.194091796875, Entropy 93.66414642333984, Learning Rate: 0.01\n",
      "Epoch [1972/20000], Loss: 1133.1378173828125, Entropy 105.66597747802734, Learning Rate: 0.01\n",
      "Epoch [1973/20000], Loss: 1171.3353271484375, Entropy 103.74617767333984, Learning Rate: 0.01\n",
      "Epoch [1974/20000], Loss: 1141.044677734375, Entropy 94.42736053466797, Learning Rate: 0.01\n",
      "Epoch [1975/20000], Loss: 1145.5457763671875, Entropy 94.96614074707031, Learning Rate: 0.01\n",
      "Epoch [1976/20000], Loss: 1176.3660888671875, Entropy 101.76219177246094, Learning Rate: 0.01\n",
      "Epoch [1977/20000], Loss: 1140.377685546875, Entropy 87.07268524169922, Learning Rate: 0.01\n",
      "Epoch [1978/20000], Loss: 1177.5228271484375, Entropy 96.27254486083984, Learning Rate: 0.01\n",
      "Epoch [1979/20000], Loss: 1156.6717529296875, Entropy 94.76972198486328, Learning Rate: 0.01\n",
      "Epoch [1980/20000], Loss: 1098.7994384765625, Entropy 99.89555358886719, Learning Rate: 0.01\n",
      "Epoch [1981/20000], Loss: 1083.7449951171875, Entropy 103.91230010986328, Learning Rate: 0.01\n",
      "Epoch [1982/20000], Loss: 1132.5655517578125, Entropy 100.59458923339844, Learning Rate: 0.01\n",
      "Epoch [1983/20000], Loss: 1107.41259765625, Entropy 101.98274993896484, Learning Rate: 0.01\n",
      "Epoch [1984/20000], Loss: 1140.1796875, Entropy 94.64130401611328, Learning Rate: 0.01\n",
      "Epoch [1985/20000], Loss: 1127.25341796875, Entropy 83.05916595458984, Learning Rate: 0.01\n",
      "Epoch [1986/20000], Loss: 1109.4180908203125, Entropy 103.61206817626953, Learning Rate: 0.01\n",
      "Epoch [1987/20000], Loss: 1131.778564453125, Entropy 105.33714294433594, Learning Rate: 0.01\n",
      "Epoch [1988/20000], Loss: 1099.4200439453125, Entropy 108.17172241210938, Learning Rate: 0.01\n",
      "Epoch [1989/20000], Loss: 1093.06298828125, Entropy 122.1983642578125, Learning Rate: 0.01\n",
      "Epoch [1990/20000], Loss: 1122.849609375, Entropy 92.44155883789062, Learning Rate: 0.01\n",
      "Epoch [1991/20000], Loss: 1141.5870361328125, Entropy 97.13126373291016, Learning Rate: 0.01\n",
      "Epoch [1992/20000], Loss: 1089.162353515625, Entropy 93.88948059082031, Learning Rate: 0.01\n",
      "Epoch [1993/20000], Loss: 1159.9273681640625, Entropy 87.76797485351562, Learning Rate: 0.01\n",
      "Epoch [1994/20000], Loss: 1080.749267578125, Entropy 114.5829849243164, Learning Rate: 0.01\n",
      "Epoch [1995/20000], Loss: 1159.924072265625, Entropy 118.63900756835938, Learning Rate: 0.01\n",
      "Epoch [1996/20000], Loss: 1122.890380859375, Entropy 109.32831573486328, Learning Rate: 0.01\n",
      "Epoch [1997/20000], Loss: 1132.9874267578125, Entropy 106.73672485351562, Learning Rate: 0.01\n",
      "Epoch [1998/20000], Loss: 1172.4349365234375, Entropy 82.39424896240234, Learning Rate: 0.01\n",
      "Epoch [1999/20000], Loss: 1178.6612548828125, Entropy 86.1200180053711, Learning Rate: 0.01\n",
      "Epoch [2000/20000], Loss: 1103.256591796875, Entropy 102.19886779785156, Learning Rate: 0.01\n",
      "Epoch [2001/20000], Loss: 1126.8927001953125, Entropy 115.35565948486328, Learning Rate: 0.01\n",
      "Epoch [2002/20000], Loss: 1172.361572265625, Entropy 104.95429229736328, Learning Rate: 0.01\n",
      "Epoch [2003/20000], Loss: 1292.2984619140625, Entropy 87.3685302734375, Learning Rate: 0.01\n",
      "Epoch [2004/20000], Loss: 1142.37548828125, Entropy 97.72845458984375, Learning Rate: 0.01\n",
      "Epoch [2005/20000], Loss: 1212.2200927734375, Entropy 108.13903045654297, Learning Rate: 0.01\n",
      "Epoch [2006/20000], Loss: 1135.0633544921875, Entropy 95.14679718017578, Learning Rate: 0.01\n",
      "Epoch [2007/20000], Loss: 1254.6600341796875, Entropy 104.42720794677734, Learning Rate: 0.01\n",
      "Epoch [2008/20000], Loss: 1144.5308837890625, Entropy 113.77345275878906, Learning Rate: 0.01\n",
      "Epoch [2009/20000], Loss: 1263.97412109375, Entropy 99.9332504272461, Learning Rate: 0.01\n",
      "Epoch [2010/20000], Loss: 1248.166748046875, Entropy 90.83512115478516, Learning Rate: 0.01\n",
      "Epoch [2011/20000], Loss: 1203.1053466796875, Entropy 106.07343292236328, Learning Rate: 0.01\n",
      "Epoch [2012/20000], Loss: 1198.4520263671875, Entropy 114.06156158447266, Learning Rate: 0.01\n",
      "Epoch [2013/20000], Loss: 1226.034912109375, Entropy 90.91722106933594, Learning Rate: 0.01\n",
      "Epoch [2014/20000], Loss: 1189.694580078125, Entropy 84.8367919921875, Learning Rate: 0.01\n",
      "Epoch [2015/20000], Loss: 1114.2767333984375, Entropy 116.06432342529297, Learning Rate: 0.01\n",
      "Epoch [2016/20000], Loss: 1140.616455078125, Entropy 110.52500915527344, Learning Rate: 0.01\n",
      "Epoch [2017/20000], Loss: 1118.5628662109375, Entropy 98.4124984741211, Learning Rate: 0.01\n",
      "Epoch [2018/20000], Loss: 1142.4598388671875, Entropy 103.86652374267578, Learning Rate: 0.01\n",
      "Epoch [2019/20000], Loss: 1143.3690185546875, Entropy 94.4432601928711, Learning Rate: 0.01\n",
      "Epoch [2020/20000], Loss: 1165.9312744140625, Entropy 117.8167953491211, Learning Rate: 0.01\n",
      "Epoch [2021/20000], Loss: 1182.92431640625, Entropy 110.30453491210938, Learning Rate: 0.01\n",
      "Epoch [2022/20000], Loss: 1094.38427734375, Entropy 99.88903045654297, Learning Rate: 0.01\n",
      "Epoch [2023/20000], Loss: 1106.986328125, Entropy 109.53512573242188, Learning Rate: 0.01\n",
      "Epoch [2024/20000], Loss: 1141.026123046875, Entropy 87.72402954101562, Learning Rate: 0.01\n",
      "Epoch [2025/20000], Loss: 1125.7254638671875, Entropy 106.35173797607422, Learning Rate: 0.01\n",
      "Epoch [2026/20000], Loss: 1137.8319091796875, Entropy 105.02299499511719, Learning Rate: 0.01\n",
      "Epoch [2027/20000], Loss: 1148.6666259765625, Entropy 103.90384674072266, Learning Rate: 0.01\n",
      "Epoch [2028/20000], Loss: 1149.69775390625, Entropy 119.2378158569336, Learning Rate: 0.01\n",
      "Epoch [2029/20000], Loss: 1124.04296875, Entropy 95.04741668701172, Learning Rate: 0.01\n",
      "Epoch [2030/20000], Loss: 1091.2296142578125, Entropy 109.89911651611328, Learning Rate: 0.01\n",
      "Epoch [2031/20000], Loss: 1094.9246826171875, Entropy 106.55493927001953, Learning Rate: 0.01\n",
      "Epoch [2032/20000], Loss: 1097.1273193359375, Entropy 125.7902603149414, Learning Rate: 0.01\n",
      "Epoch [2033/20000], Loss: 1141.5166015625, Entropy 102.46173858642578, Learning Rate: 0.01\n",
      "Epoch [2034/20000], Loss: 1162.43212890625, Entropy 107.2472915649414, Learning Rate: 0.01\n",
      "Epoch [2035/20000], Loss: 1125.947509765625, Entropy 102.86776733398438, Learning Rate: 0.01\n",
      "Epoch [2036/20000], Loss: 1201.83544921875, Entropy 97.64053344726562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2037/20000], Loss: 1116.8809814453125, Entropy 109.13727569580078, Learning Rate: 0.01\n",
      "Epoch [2038/20000], Loss: 1135.9012451171875, Entropy 112.92494201660156, Learning Rate: 0.01\n",
      "Epoch [2039/20000], Loss: 1144.56103515625, Entropy 92.57994079589844, Learning Rate: 0.01\n",
      "Epoch [2040/20000], Loss: 1162.686767578125, Entropy 107.53524017333984, Learning Rate: 0.01\n",
      "Epoch [2041/20000], Loss: 1137.6651611328125, Entropy 97.59568786621094, Learning Rate: 0.01\n",
      "Epoch [2042/20000], Loss: 1112.3917236328125, Entropy 115.23779296875, Learning Rate: 0.01\n",
      "Epoch [2043/20000], Loss: 1180.7099609375, Entropy 119.86585998535156, Learning Rate: 0.01\n",
      "Epoch [2044/20000], Loss: 1183.3629150390625, Entropy 111.98944854736328, Learning Rate: 0.01\n",
      "Epoch [2045/20000], Loss: 1137.0703125, Entropy 120.58657836914062, Learning Rate: 0.01\n",
      "Epoch [2046/20000], Loss: 1201.69091796875, Entropy 95.09404754638672, Learning Rate: 0.01\n",
      "Epoch [2047/20000], Loss: 1098.1414794921875, Entropy 103.49412536621094, Learning Rate: 0.01\n",
      "Epoch [2048/20000], Loss: 1103.937744140625, Entropy 101.2410659790039, Learning Rate: 0.01\n",
      "Epoch [2049/20000], Loss: 1135.177978515625, Entropy 116.7587890625, Learning Rate: 0.01\n",
      "Epoch [2050/20000], Loss: 1086.9871826171875, Entropy 112.9745864868164, Learning Rate: 0.01\n",
      "Epoch [2051/20000], Loss: 1102.8209228515625, Entropy 121.32337951660156, Learning Rate: 0.01\n",
      "Epoch [2052/20000], Loss: 1083.3458251953125, Entropy 120.60025787353516, Learning Rate: 0.01\n",
      "Epoch [2053/20000], Loss: 1113.9345703125, Entropy 92.75506591796875, Learning Rate: 0.01\n",
      "Epoch [2054/20000], Loss: 1134.8468017578125, Entropy 104.56666564941406, Learning Rate: 0.01\n",
      "Epoch [2055/20000], Loss: 1102.899658203125, Entropy 113.96331787109375, Learning Rate: 0.01\n",
      "Epoch [2056/20000], Loss: 1111.092529296875, Entropy 133.37387084960938, Learning Rate: 0.01\n",
      "Epoch [2057/20000], Loss: 1141.1982421875, Entropy 112.03997802734375, Learning Rate: 0.01\n",
      "Epoch [2058/20000], Loss: 1166.135986328125, Entropy 106.45500946044922, Learning Rate: 0.01\n",
      "Epoch [2059/20000], Loss: 1141.9383544921875, Entropy 105.1663818359375, Learning Rate: 0.01\n",
      "Epoch [2060/20000], Loss: 1123.3868408203125, Entropy 106.0618896484375, Learning Rate: 0.01\n",
      "Epoch [2061/20000], Loss: 1168.6248779296875, Entropy 124.92642211914062, Learning Rate: 0.01\n",
      "Epoch [2062/20000], Loss: 1119.069580078125, Entropy 118.7515869140625, Learning Rate: 0.01\n",
      "Epoch [2063/20000], Loss: 1211.0684814453125, Entropy 119.60960388183594, Learning Rate: 0.01\n",
      "Epoch [2064/20000], Loss: 1113.8701171875, Entropy 122.77359008789062, Learning Rate: 0.01\n",
      "Epoch [2065/20000], Loss: 1208.0457763671875, Entropy 103.25328063964844, Learning Rate: 0.01\n",
      "Epoch [2066/20000], Loss: 1158.488037109375, Entropy 102.30615997314453, Learning Rate: 0.01\n",
      "Epoch [2067/20000], Loss: 1157.9345703125, Entropy 119.77355194091797, Learning Rate: 0.01\n",
      "Epoch [2068/20000], Loss: 1199.6767578125, Entropy 110.7284927368164, Learning Rate: 0.01\n",
      "Epoch [2069/20000], Loss: 1161.9986572265625, Entropy 104.40413665771484, Learning Rate: 0.01\n",
      "Epoch [2070/20000], Loss: 1278.3775634765625, Entropy 108.4071044921875, Learning Rate: 0.01\n",
      "Epoch [2071/20000], Loss: 1112.982177734375, Entropy 121.1330795288086, Learning Rate: 0.01\n",
      "Epoch [2072/20000], Loss: 1254.8045654296875, Entropy 96.5907211303711, Learning Rate: 0.01\n",
      "Epoch [2073/20000], Loss: 1137.0699462890625, Entropy 112.3126220703125, Learning Rate: 0.01\n",
      "Epoch [2074/20000], Loss: 1339.43212890625, Entropy 104.06513214111328, Learning Rate: 0.01\n",
      "Epoch [2075/20000], Loss: 1198.358642578125, Entropy 121.00753784179688, Learning Rate: 0.01\n",
      "Epoch [2076/20000], Loss: 1495.172119140625, Entropy 116.93704986572266, Learning Rate: 0.01\n",
      "Epoch [2077/20000], Loss: 1155.3359375, Entropy 112.965087890625, Learning Rate: 0.01\n",
      "Epoch [2078/20000], Loss: 1368.3223876953125, Entropy 120.05232238769531, Learning Rate: 0.01\n",
      "Epoch [2079/20000], Loss: 1252.001953125, Entropy 122.42396545410156, Learning Rate: 0.01\n",
      "Epoch [2080/20000], Loss: 1342.2777099609375, Entropy 109.65001678466797, Learning Rate: 0.01\n",
      "Epoch [2081/20000], Loss: 1380.4984130859375, Entropy 115.28759765625, Learning Rate: 0.01\n",
      "Epoch [2082/20000], Loss: 1332.306884765625, Entropy 114.19212341308594, Learning Rate: 0.01\n",
      "Epoch [2083/20000], Loss: 1402.6485595703125, Entropy 127.68795776367188, Learning Rate: 0.01\n",
      "Epoch [2084/20000], Loss: 1209.57080078125, Entropy 106.99526977539062, Learning Rate: 0.01\n",
      "Epoch [2085/20000], Loss: 1464.29736328125, Entropy 115.4554672241211, Learning Rate: 0.01\n",
      "Epoch [2086/20000], Loss: 1139.208251953125, Entropy 112.58845520019531, Learning Rate: 0.01\n",
      "Epoch [2087/20000], Loss: 1321.5072021484375, Entropy 97.5575180053711, Learning Rate: 0.01\n",
      "Epoch [2088/20000], Loss: 1294.56787109375, Entropy 121.9334945678711, Learning Rate: 0.01\n",
      "Epoch [2089/20000], Loss: 1412.5511474609375, Entropy 134.8616180419922, Learning Rate: 0.01\n",
      "Epoch [2090/20000], Loss: 1371.3153076171875, Entropy 104.83061981201172, Learning Rate: 0.01\n",
      "Epoch [2091/20000], Loss: 1169.7447509765625, Entropy 97.07188415527344, Learning Rate: 0.01\n",
      "Epoch [2092/20000], Loss: 1523.177001953125, Entropy 115.13634490966797, Learning Rate: 0.01\n",
      "Epoch [2093/20000], Loss: 1217.8209228515625, Entropy 116.61466217041016, Learning Rate: 0.01\n",
      "Epoch [2094/20000], Loss: 1435.59521484375, Entropy 107.64366912841797, Learning Rate: 0.01\n",
      "Epoch [2095/20000], Loss: 1230.5960693359375, Entropy 114.68482208251953, Learning Rate: 0.01\n",
      "Epoch [2096/20000], Loss: 1270.9263916015625, Entropy 111.19190216064453, Learning Rate: 0.01\n",
      "Epoch [2097/20000], Loss: 1506.3150634765625, Entropy 113.91050720214844, Learning Rate: 0.01\n",
      "Epoch [2098/20000], Loss: 1350.1502685546875, Entropy 113.2169189453125, Learning Rate: 0.01\n",
      "Epoch [2099/20000], Loss: 1272.5753173828125, Entropy 119.39216613769531, Learning Rate: 0.01\n",
      "Epoch [2100/20000], Loss: 1179.537109375, Entropy 112.54353332519531, Learning Rate: 0.01\n",
      "Epoch [2101/20000], Loss: 1376.6392822265625, Entropy 106.9305419921875, Learning Rate: 0.01\n",
      "Epoch [2102/20000], Loss: 1265.364013671875, Entropy 110.75767517089844, Learning Rate: 0.01\n",
      "Epoch [2103/20000], Loss: 1278.865234375, Entropy 96.90253448486328, Learning Rate: 0.01\n",
      "Epoch [2104/20000], Loss: 1322.2117919921875, Entropy 119.69560241699219, Learning Rate: 0.01\n",
      "Epoch [2105/20000], Loss: 1201.4774169921875, Entropy 116.2702865600586, Learning Rate: 0.01\n",
      "Epoch [2106/20000], Loss: 1303.393310546875, Entropy 95.23585510253906, Learning Rate: 0.01\n",
      "Epoch [2107/20000], Loss: 1166.523681640625, Entropy 106.54917907714844, Learning Rate: 0.01\n",
      "Epoch [2108/20000], Loss: 1286.3897705078125, Entropy 108.41849517822266, Learning Rate: 0.01\n",
      "Epoch [2109/20000], Loss: 1134.6527099609375, Entropy 108.69307708740234, Learning Rate: 0.01\n",
      "Epoch [2110/20000], Loss: 1236.6822509765625, Entropy 132.2501983642578, Learning Rate: 0.01\n",
      "Epoch [2111/20000], Loss: 1165.5352783203125, Entropy 101.40998077392578, Learning Rate: 0.01\n",
      "Epoch [2112/20000], Loss: 1226.7200927734375, Entropy 96.0630874633789, Learning Rate: 0.01\n",
      "Epoch [2113/20000], Loss: 1173.1199951171875, Entropy 101.08154296875, Learning Rate: 0.01\n",
      "Epoch [2114/20000], Loss: 1138.0123291015625, Entropy 112.33733367919922, Learning Rate: 0.01\n",
      "Epoch [2115/20000], Loss: 1138.89453125, Entropy 125.44822692871094, Learning Rate: 0.01\n",
      "Epoch [2116/20000], Loss: 1186.95458984375, Entropy 99.67206573486328, Learning Rate: 0.01\n",
      "Epoch [2117/20000], Loss: 1118.7674560546875, Entropy 114.235107421875, Learning Rate: 0.01\n",
      "Epoch [2118/20000], Loss: 1233.1842041015625, Entropy 101.80962371826172, Learning Rate: 0.01\n",
      "Epoch [2119/20000], Loss: 1115.751220703125, Entropy 114.00345611572266, Learning Rate: 0.01\n",
      "Epoch [2120/20000], Loss: 1197.4757080078125, Entropy 103.2970199584961, Learning Rate: 0.01\n",
      "Epoch [2121/20000], Loss: 1289.558349609375, Entropy 89.85236358642578, Learning Rate: 0.01\n",
      "Epoch [2122/20000], Loss: 1194.35986328125, Entropy 91.51966857910156, Learning Rate: 0.01\n",
      "Epoch [2123/20000], Loss: 1282.0863037109375, Entropy 103.7796630859375, Learning Rate: 0.01\n",
      "Epoch [2124/20000], Loss: 1229.94189453125, Entropy 106.30191040039062, Learning Rate: 0.01\n",
      "Epoch [2125/20000], Loss: 1186.0274658203125, Entropy 121.57089233398438, Learning Rate: 0.01\n",
      "Epoch [2126/20000], Loss: 1239.551025390625, Entropy 104.09198760986328, Learning Rate: 0.01\n",
      "Epoch [2127/20000], Loss: 1260.9735107421875, Entropy 102.59950256347656, Learning Rate: 0.01\n",
      "Epoch [2128/20000], Loss: 1160.6610107421875, Entropy 105.34634399414062, Learning Rate: 0.01\n",
      "Epoch [2129/20000], Loss: 1244.7496337890625, Entropy 128.7772216796875, Learning Rate: 0.01\n",
      "Epoch [2130/20000], Loss: 1183.5067138671875, Entropy 110.0936508178711, Learning Rate: 0.01\n",
      "Epoch [2131/20000], Loss: 1156.6217041015625, Entropy 104.41458129882812, Learning Rate: 0.01\n",
      "Epoch [2132/20000], Loss: 1198.3092041015625, Entropy 113.79511260986328, Learning Rate: 0.01\n",
      "Epoch [2133/20000], Loss: 1138.5191650390625, Entropy 106.52426147460938, Learning Rate: 0.01\n",
      "Epoch [2134/20000], Loss: 1186.5218505859375, Entropy 124.31415557861328, Learning Rate: 0.01\n",
      "Epoch [2135/20000], Loss: 1128.268798828125, Entropy 107.17115020751953, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2136/20000], Loss: 1164.5205078125, Entropy 118.127197265625, Learning Rate: 0.01\n",
      "Epoch [2137/20000], Loss: 1179.5111083984375, Entropy 109.88021850585938, Learning Rate: 0.01\n",
      "Epoch [2138/20000], Loss: 1159.44091796875, Entropy 133.1678009033203, Learning Rate: 0.01\n",
      "Epoch [2139/20000], Loss: 1123.0845947265625, Entropy 108.21817779541016, Learning Rate: 0.01\n",
      "Epoch [2140/20000], Loss: 1119.386474609375, Entropy 107.50467681884766, Learning Rate: 0.01\n",
      "Epoch [2141/20000], Loss: 1159.579833984375, Entropy 99.13853454589844, Learning Rate: 0.01\n",
      "Epoch [2142/20000], Loss: 1164.748046875, Entropy 109.72789764404297, Learning Rate: 0.01\n",
      "Epoch [2143/20000], Loss: 1106.41552734375, Entropy 110.63682556152344, Learning Rate: 0.01\n",
      "Epoch [2144/20000], Loss: 1107.413330078125, Entropy 125.8553466796875, Learning Rate: 0.01\n",
      "Epoch [2145/20000], Loss: 1187.0953369140625, Entropy 105.5766372680664, Learning Rate: 0.01\n",
      "Epoch [2146/20000], Loss: 1101.615234375, Entropy 113.94034576416016, Learning Rate: 0.01\n",
      "Epoch [2147/20000], Loss: 1143.628173828125, Entropy 104.07271575927734, Learning Rate: 0.01\n",
      "Epoch [2148/20000], Loss: 1109.89794921875, Entropy 119.49463653564453, Learning Rate: 0.01\n",
      "Epoch [2149/20000], Loss: 1145.667236328125, Entropy 117.03273010253906, Learning Rate: 0.01\n",
      "Epoch [2150/20000], Loss: 1155.5955810546875, Entropy 98.38614654541016, Learning Rate: 0.01\n",
      "Epoch [2151/20000], Loss: 1129.5010986328125, Entropy 130.3706817626953, Learning Rate: 0.01\n",
      "Epoch [2152/20000], Loss: 1085.8248291015625, Entropy 119.35896301269531, Learning Rate: 0.01\n",
      "Epoch [2153/20000], Loss: 1175.2069091796875, Entropy 115.50144958496094, Learning Rate: 0.01\n",
      "Epoch [2154/20000], Loss: 1083.2120361328125, Entropy 115.59458923339844, Learning Rate: 0.01\n",
      "Epoch [2155/20000], Loss: 1158.9251708984375, Entropy 118.88871002197266, Learning Rate: 0.01\n",
      "Epoch [2156/20000], Loss: 1118.3912353515625, Entropy 96.2524185180664, Learning Rate: 0.01\n",
      "Epoch [2157/20000], Loss: 1145.0145263671875, Entropy 105.46707153320312, Learning Rate: 0.01\n",
      "Epoch [2158/20000], Loss: 1089.7840576171875, Entropy 126.96573638916016, Learning Rate: 0.01\n",
      "Epoch [2159/20000], Loss: 1088.95166015625, Entropy 102.78646850585938, Learning Rate: 0.01\n",
      "Epoch [2160/20000], Loss: 1137.831298828125, Entropy 121.92813873291016, Learning Rate: 0.01\n",
      "Epoch [2161/20000], Loss: 1111.66943359375, Entropy 110.51437377929688, Learning Rate: 0.01\n",
      "Epoch [2162/20000], Loss: 1098.330322265625, Entropy 117.62940979003906, Learning Rate: 0.01\n",
      "Epoch [2163/20000], Loss: 1119.8653564453125, Entropy 111.13407135009766, Learning Rate: 0.01\n",
      "Epoch [2164/20000], Loss: 1146.8905029296875, Entropy 115.40386199951172, Learning Rate: 0.01\n",
      "Epoch [2165/20000], Loss: 1128.2803955078125, Entropy 109.78036499023438, Learning Rate: 0.01\n",
      "Epoch [2166/20000], Loss: 1097.4273681640625, Entropy 113.13844299316406, Learning Rate: 0.01\n",
      "Epoch [2167/20000], Loss: 1079.96533203125, Entropy 119.71732330322266, Learning Rate: 0.01\n",
      "Epoch [2168/20000], Loss: 1070.81201171875, Entropy 117.1184310913086, Learning Rate: 0.01\n",
      "Epoch [2169/20000], Loss: 1112.878173828125, Entropy 120.24481964111328, Learning Rate: 0.01\n",
      "Epoch [2170/20000], Loss: 1099.64404296875, Entropy 136.3914031982422, Learning Rate: 0.01\n",
      "Epoch [2171/20000], Loss: 1078.880615234375, Entropy 111.72950744628906, Learning Rate: 0.01\n",
      "Epoch [2172/20000], Loss: 1110.9954833984375, Entropy 120.94468688964844, Learning Rate: 0.01\n",
      "Epoch [2173/20000], Loss: 1107.4263916015625, Entropy 125.64788055419922, Learning Rate: 0.01\n",
      "Epoch [2174/20000], Loss: 1148.0201416015625, Entropy 108.71614074707031, Learning Rate: 0.01\n",
      "Epoch [2175/20000], Loss: 1116.4534912109375, Entropy 111.28837585449219, Learning Rate: 0.01\n",
      "Epoch [2176/20000], Loss: 1120.4658203125, Entropy 114.59327697753906, Learning Rate: 0.01\n",
      "Epoch [2177/20000], Loss: 1144.5103759765625, Entropy 103.12806701660156, Learning Rate: 0.01\n",
      "Epoch [2178/20000], Loss: 1150.395751953125, Entropy 119.5350570678711, Learning Rate: 0.01\n",
      "Epoch [2179/20000], Loss: 1137.4761962890625, Entropy 115.8323974609375, Learning Rate: 0.01\n",
      "Epoch [2180/20000], Loss: 1086.2447509765625, Entropy 113.04396057128906, Learning Rate: 0.01\n",
      "Epoch [2181/20000], Loss: 1131.8310546875, Entropy 110.70965576171875, Learning Rate: 0.01\n",
      "Epoch [2182/20000], Loss: 1094.3759765625, Entropy 127.54071044921875, Learning Rate: 0.01\n",
      "Epoch [2183/20000], Loss: 1063.1146240234375, Entropy 117.10006713867188, Learning Rate: 0.01\n",
      "Epoch [2184/20000], Loss: 1096.4901123046875, Entropy 122.2147216796875, Learning Rate: 0.01\n",
      "Epoch [2185/20000], Loss: 1097.2579345703125, Entropy 126.34202575683594, Learning Rate: 0.01\n",
      "Epoch [2186/20000], Loss: 1095.2535400390625, Entropy 111.03202819824219, Learning Rate: 0.01\n",
      "Epoch [2187/20000], Loss: 1078.718017578125, Entropy 122.34239959716797, Learning Rate: 0.01\n",
      "Epoch [2188/20000], Loss: 1103.9503173828125, Entropy 118.84825134277344, Learning Rate: 0.01\n",
      "Epoch [2189/20000], Loss: 1108.1444091796875, Entropy 112.7459716796875, Learning Rate: 0.01\n",
      "Epoch [2190/20000], Loss: 1080.301025390625, Entropy 122.79769897460938, Learning Rate: 0.01\n",
      "Epoch [2191/20000], Loss: 1066.7279052734375, Entropy 128.39793395996094, Learning Rate: 0.01\n",
      "Epoch [2192/20000], Loss: 1132.1195068359375, Entropy 119.3284912109375, Learning Rate: 0.01\n",
      "Epoch [2193/20000], Loss: 1112.33251953125, Entropy 128.60409545898438, Learning Rate: 0.01\n",
      "Epoch [2194/20000], Loss: 1075.27294921875, Entropy 121.25921630859375, Learning Rate: 0.01\n",
      "Epoch [2195/20000], Loss: 1133.5565185546875, Entropy 129.8304901123047, Learning Rate: 0.01\n",
      "Epoch [2196/20000], Loss: 1151.2027587890625, Entropy 124.07139587402344, Learning Rate: 0.01\n",
      "Epoch [2197/20000], Loss: 1124.7203369140625, Entropy 116.60192108154297, Learning Rate: 0.01\n",
      "Epoch [2198/20000], Loss: 1122.6812744140625, Entropy 118.60321807861328, Learning Rate: 0.01\n",
      "Epoch [2199/20000], Loss: 1025.974853515625, Entropy 129.33811950683594, Learning Rate: 0.01\n",
      "Epoch [2200/20000], Loss: 1119.4508056640625, Entropy 117.5571517944336, Learning Rate: 0.01\n",
      "Epoch [2201/20000], Loss: 1087.7913818359375, Entropy 118.77515411376953, Learning Rate: 0.01\n",
      "Epoch [2202/20000], Loss: 1055.1796875, Entropy 137.62786865234375, Learning Rate: 0.01\n",
      "Epoch [2203/20000], Loss: 1125.036865234375, Entropy 119.607421875, Learning Rate: 0.01\n",
      "Epoch [2204/20000], Loss: 1174.740478515625, Entropy 100.63302612304688, Learning Rate: 0.01\n",
      "Epoch [2205/20000], Loss: 1104.463623046875, Entropy 130.50347900390625, Learning Rate: 0.01\n",
      "Epoch [2206/20000], Loss: 1067.8406982421875, Entropy 127.1738510131836, Learning Rate: 0.01\n",
      "Epoch [2207/20000], Loss: 1121.33837890625, Entropy 129.15460205078125, Learning Rate: 0.01\n",
      "Epoch [2208/20000], Loss: 1146.27001953125, Entropy 129.4489288330078, Learning Rate: 0.01\n",
      "Epoch [2209/20000], Loss: 1114.423095703125, Entropy 128.58892822265625, Learning Rate: 0.01\n",
      "Epoch [2210/20000], Loss: 1078.8907470703125, Entropy 113.15413665771484, Learning Rate: 0.01\n",
      "Epoch [2211/20000], Loss: 1117.89501953125, Entropy 117.5744857788086, Learning Rate: 0.01\n",
      "Epoch [2212/20000], Loss: 1107.1539306640625, Entropy 123.55472564697266, Learning Rate: 0.01\n",
      "Epoch [2213/20000], Loss: 1114.2186279296875, Entropy 116.09848022460938, Learning Rate: 0.01\n",
      "Epoch [2214/20000], Loss: 1070.982177734375, Entropy 117.525634765625, Learning Rate: 0.01\n",
      "Epoch [2215/20000], Loss: 1154.3275146484375, Entropy 121.36060333251953, Learning Rate: 0.01\n",
      "Epoch [2216/20000], Loss: 1165.435546875, Entropy 122.13410186767578, Learning Rate: 0.01\n",
      "Epoch [2217/20000], Loss: 1168.1273193359375, Entropy 107.75275421142578, Learning Rate: 0.01\n",
      "Epoch [2218/20000], Loss: 1063.356201171875, Entropy 142.95635986328125, Learning Rate: 0.01\n",
      "Epoch [2219/20000], Loss: 1173.768310546875, Entropy 112.14418029785156, Learning Rate: 0.01\n",
      "Epoch [2220/20000], Loss: 1078.8450927734375, Entropy 123.37557220458984, Learning Rate: 0.01\n",
      "Epoch [2221/20000], Loss: 1143.187744140625, Entropy 116.16487884521484, Learning Rate: 0.01\n",
      "Epoch [2222/20000], Loss: 1098.6568603515625, Entropy 124.86775207519531, Learning Rate: 0.01\n",
      "Epoch [2223/20000], Loss: 1118.9248046875, Entropy 131.92825317382812, Learning Rate: 0.01\n",
      "Epoch [2224/20000], Loss: 1095.291015625, Entropy 118.51716613769531, Learning Rate: 0.01\n",
      "Epoch [2225/20000], Loss: 1138.09423828125, Entropy 130.8549346923828, Learning Rate: 0.01\n",
      "Epoch [2226/20000], Loss: 1121.3643798828125, Entropy 123.47258758544922, Learning Rate: 0.01\n",
      "Epoch [2227/20000], Loss: 1085.615966796875, Entropy 125.53953552246094, Learning Rate: 0.01\n",
      "Epoch [2228/20000], Loss: 1084.31689453125, Entropy 145.0240936279297, Learning Rate: 0.01\n",
      "Epoch [2229/20000], Loss: 1118.75, Entropy 132.91085815429688, Learning Rate: 0.01\n",
      "Epoch [2230/20000], Loss: 1072.9517822265625, Entropy 129.16929626464844, Learning Rate: 0.01\n",
      "Epoch [2231/20000], Loss: 1108.0269775390625, Entropy 125.36677551269531, Learning Rate: 0.01\n",
      "Epoch [2232/20000], Loss: 1121.1805419921875, Entropy 130.3126678466797, Learning Rate: 0.01\n",
      "Epoch [2233/20000], Loss: 1106.112548828125, Entropy 118.08197021484375, Learning Rate: 0.01\n",
      "Epoch [2234/20000], Loss: 1093.3525390625, Entropy 131.52293395996094, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2235/20000], Loss: 1101.638916015625, Entropy 132.7471923828125, Learning Rate: 0.01\n",
      "Epoch [2236/20000], Loss: 1131.69384765625, Entropy 123.91973114013672, Learning Rate: 0.01\n",
      "Epoch [2237/20000], Loss: 1169.3599853515625, Entropy 126.57209777832031, Learning Rate: 0.01\n",
      "Epoch [2238/20000], Loss: 1125.9603271484375, Entropy 139.4635009765625, Learning Rate: 0.01\n",
      "Epoch [2239/20000], Loss: 1082.0103759765625, Entropy 136.72708129882812, Learning Rate: 0.01\n",
      "Epoch [2240/20000], Loss: 1196.473876953125, Entropy 114.85401153564453, Learning Rate: 0.01\n",
      "Epoch [2241/20000], Loss: 1112.110107421875, Entropy 127.53715515136719, Learning Rate: 0.01\n",
      "Epoch [2242/20000], Loss: 1116.09521484375, Entropy 128.79815673828125, Learning Rate: 0.01\n",
      "Epoch [2243/20000], Loss: 1141.8475341796875, Entropy 135.2824249267578, Learning Rate: 0.01\n",
      "Epoch [2244/20000], Loss: 1083.662109375, Entropy 126.74320220947266, Learning Rate: 0.01\n",
      "Epoch [2245/20000], Loss: 1126.0782470703125, Entropy 136.9042510986328, Learning Rate: 0.01\n",
      "Epoch [2246/20000], Loss: 1080.93408203125, Entropy 133.1846923828125, Learning Rate: 0.01\n",
      "Epoch [2247/20000], Loss: 1222.896240234375, Entropy 132.8785858154297, Learning Rate: 0.01\n",
      "Epoch [2248/20000], Loss: 1110.28466796875, Entropy 147.7935791015625, Learning Rate: 0.01\n",
      "Epoch [2249/20000], Loss: 1158.219482421875, Entropy 135.16636657714844, Learning Rate: 0.01\n",
      "Epoch [2250/20000], Loss: 1110.605224609375, Entropy 114.31880950927734, Learning Rate: 0.01\n",
      "Epoch [2251/20000], Loss: 1267.8402099609375, Entropy 135.29031372070312, Learning Rate: 0.01\n",
      "Epoch [2252/20000], Loss: 1149.597900390625, Entropy 125.54853057861328, Learning Rate: 0.01\n",
      "Epoch [2253/20000], Loss: 1329.9501953125, Entropy 150.69769287109375, Learning Rate: 0.01\n",
      "Epoch [2254/20000], Loss: 1191.7275390625, Entropy 124.9755630493164, Learning Rate: 0.01\n",
      "Epoch [2255/20000], Loss: 1486.250732421875, Entropy 123.52452087402344, Learning Rate: 0.01\n",
      "Epoch [2256/20000], Loss: 1475.4842529296875, Entropy 139.2725830078125, Learning Rate: 0.01\n",
      "Epoch [2257/20000], Loss: 1513.5306396484375, Entropy 142.3446044921875, Learning Rate: 0.01\n",
      "Epoch [2258/20000], Loss: 2250.735107421875, Entropy 137.0605010986328, Learning Rate: 0.01\n",
      "Epoch [2259/20000], Loss: 1400.72998046875, Entropy 128.0465545654297, Learning Rate: 0.01\n",
      "Epoch [2260/20000], Loss: 1797.09375, Entropy 122.8749771118164, Learning Rate: 0.01\n",
      "Epoch [2261/20000], Loss: 1757.17919921875, Entropy 120.0681381225586, Learning Rate: 0.01\n",
      "Epoch [2262/20000], Loss: 1445.83203125, Entropy 146.49227905273438, Learning Rate: 0.01\n",
      "Epoch [2263/20000], Loss: 1854.82421875, Entropy 127.17644500732422, Learning Rate: 0.01\n",
      "Epoch [2264/20000], Loss: 2190.781982421875, Entropy 133.68466186523438, Learning Rate: 0.01\n",
      "Epoch [2265/20000], Loss: 2805.241943359375, Entropy 134.77496337890625, Learning Rate: 0.01\n",
      "Epoch [2266/20000], Loss: 3577.1611328125, Entropy 121.09759521484375, Learning Rate: 0.01\n",
      "Epoch [2267/20000], Loss: 1268.675537109375, Entropy 126.46177673339844, Learning Rate: 0.01\n",
      "Epoch [2268/20000], Loss: 3189.9736328125, Entropy 116.47638702392578, Learning Rate: 0.01\n",
      "Epoch [2269/20000], Loss: 1298.9503173828125, Entropy 126.10742950439453, Learning Rate: 0.01\n",
      "Epoch [2270/20000], Loss: 2303.702880859375, Entropy 130.33663940429688, Learning Rate: 0.01\n",
      "Epoch [2271/20000], Loss: 1348.8726806640625, Entropy 136.74417114257812, Learning Rate: 0.01\n",
      "Epoch [2272/20000], Loss: 1756.151123046875, Entropy 101.0450439453125, Learning Rate: 0.01\n",
      "Epoch [2273/20000], Loss: 1943.4832763671875, Entropy 105.94984436035156, Learning Rate: 0.01\n",
      "Epoch [2274/20000], Loss: 1274.257568359375, Entropy 104.39041137695312, Learning Rate: 0.01\n",
      "Epoch [2275/20000], Loss: 1838.4403076171875, Entropy 113.27948760986328, Learning Rate: 0.01\n",
      "Epoch [2276/20000], Loss: 1371.1187744140625, Entropy 100.18977355957031, Learning Rate: 0.01\n",
      "Epoch [2277/20000], Loss: 1725.887451171875, Entropy 108.29056549072266, Learning Rate: 0.01\n",
      "Epoch [2278/20000], Loss: 1670.3614501953125, Entropy 111.8056411743164, Learning Rate: 0.01\n",
      "Epoch [2279/20000], Loss: 1892.1444091796875, Entropy 103.9305648803711, Learning Rate: 0.01\n",
      "Epoch [2280/20000], Loss: 1357.291259765625, Entropy 108.20401763916016, Learning Rate: 0.01\n",
      "Epoch [2281/20000], Loss: 1534.822509765625, Entropy 96.43247985839844, Learning Rate: 0.01\n",
      "Epoch [2282/20000], Loss: 1534.564208984375, Entropy 90.9955825805664, Learning Rate: 0.01\n",
      "Epoch [2283/20000], Loss: 1303.22998046875, Entropy 96.99152374267578, Learning Rate: 0.01\n",
      "Epoch [2284/20000], Loss: 1427.4049072265625, Entropy 125.05429077148438, Learning Rate: 0.01\n",
      "Epoch [2285/20000], Loss: 1278.40185546875, Entropy 99.85836029052734, Learning Rate: 0.01\n",
      "Epoch [2286/20000], Loss: 1290.4173583984375, Entropy 110.6185302734375, Learning Rate: 0.01\n",
      "Epoch [2287/20000], Loss: 1336.7564697265625, Entropy 98.69886779785156, Learning Rate: 0.01\n",
      "Epoch [2288/20000], Loss: 1239.0823974609375, Entropy 95.85118865966797, Learning Rate: 0.01\n",
      "Epoch [2289/20000], Loss: 1276.362060546875, Entropy 106.14872741699219, Learning Rate: 0.01\n",
      "Epoch [2290/20000], Loss: 1322.536376953125, Entropy 106.15702819824219, Learning Rate: 0.01\n",
      "Epoch [2291/20000], Loss: 1218.942626953125, Entropy 98.01249694824219, Learning Rate: 0.01\n",
      "Epoch [2292/20000], Loss: 1374.6339111328125, Entropy 97.48794555664062, Learning Rate: 0.01\n",
      "Epoch [2293/20000], Loss: 1225.4912109375, Entropy 79.54307556152344, Learning Rate: 0.01\n",
      "Epoch [2294/20000], Loss: 1312.9727783203125, Entropy 100.74819946289062, Learning Rate: 0.01\n",
      "Epoch [2295/20000], Loss: 1179.934326171875, Entropy 107.99077606201172, Learning Rate: 0.01\n",
      "Epoch [2296/20000], Loss: 1211.335693359375, Entropy 111.00753784179688, Learning Rate: 0.01\n",
      "Epoch [2297/20000], Loss: 1186.9404296875, Entropy 107.09268188476562, Learning Rate: 0.01\n",
      "Epoch [2298/20000], Loss: 1141.520263671875, Entropy 100.59636688232422, Learning Rate: 0.01\n",
      "Epoch [2299/20000], Loss: 1250.7344970703125, Entropy 94.05821228027344, Learning Rate: 0.01\n",
      "Epoch [2300/20000], Loss: 1179.5673828125, Entropy 94.78255462646484, Learning Rate: 0.01\n",
      "Epoch [2301/20000], Loss: 1265.902099609375, Entropy 103.16629028320312, Learning Rate: 0.01\n",
      "Epoch [2302/20000], Loss: 1115.629150390625, Entropy 101.44554138183594, Learning Rate: 0.01\n",
      "Epoch [2303/20000], Loss: 1155.7406005859375, Entropy 86.88704681396484, Learning Rate: 0.01\n",
      "Epoch [2304/20000], Loss: 1144.6146240234375, Entropy 113.08553314208984, Learning Rate: 0.01\n",
      "Epoch [2305/20000], Loss: 1151.224853515625, Entropy 91.2741470336914, Learning Rate: 0.01\n",
      "Epoch [2306/20000], Loss: 1201.2900390625, Entropy 104.2197265625, Learning Rate: 0.01\n",
      "Epoch [2307/20000], Loss: 1140.1314697265625, Entropy 87.52173614501953, Learning Rate: 0.01\n",
      "Epoch [2308/20000], Loss: 1201.5714111328125, Entropy 100.87345123291016, Learning Rate: 0.01\n",
      "Epoch [2309/20000], Loss: 1117.4056396484375, Entropy 89.67574310302734, Learning Rate: 0.01\n",
      "Epoch [2310/20000], Loss: 1116.78857421875, Entropy 109.81236267089844, Learning Rate: 0.01\n",
      "Epoch [2311/20000], Loss: 1123.2867431640625, Entropy 104.44009399414062, Learning Rate: 0.01\n",
      "Epoch [2312/20000], Loss: 1135.7896728515625, Entropy 88.9198226928711, Learning Rate: 0.01\n",
      "Epoch [2313/20000], Loss: 1123.0506591796875, Entropy 106.96932983398438, Learning Rate: 0.01\n",
      "Epoch [2314/20000], Loss: 1161.7225341796875, Entropy 97.4169921875, Learning Rate: 0.01\n",
      "Epoch [2315/20000], Loss: 1114.709716796875, Entropy 91.00569915771484, Learning Rate: 0.01\n",
      "Epoch [2316/20000], Loss: 1146.3846435546875, Entropy 103.61825561523438, Learning Rate: 0.01\n",
      "Epoch [2317/20000], Loss: 1120.5869140625, Entropy 99.3271255493164, Learning Rate: 0.01\n",
      "Epoch [2318/20000], Loss: 1096.71533203125, Entropy 96.61053466796875, Learning Rate: 0.01\n",
      "Epoch [2319/20000], Loss: 1141.9322509765625, Entropy 108.51921844482422, Learning Rate: 0.01\n",
      "Epoch [2320/20000], Loss: 1127.4320068359375, Entropy 103.19302368164062, Learning Rate: 0.01\n",
      "Epoch [2321/20000], Loss: 1077.7747802734375, Entropy 111.7353515625, Learning Rate: 0.01\n",
      "Epoch [2322/20000], Loss: 1141.4287109375, Entropy 88.99591827392578, Learning Rate: 0.01\n",
      "Epoch [2323/20000], Loss: 1122.7464599609375, Entropy 113.04499053955078, Learning Rate: 0.01\n",
      "Epoch [2324/20000], Loss: 1155.8001708984375, Entropy 107.14681243896484, Learning Rate: 0.01\n",
      "Epoch [2325/20000], Loss: 1166.1708984375, Entropy 101.6507568359375, Learning Rate: 0.01\n",
      "Epoch [2326/20000], Loss: 1133.252197265625, Entropy 112.25062561035156, Learning Rate: 0.01\n",
      "Epoch [2327/20000], Loss: 1113.0032958984375, Entropy 112.55113220214844, Learning Rate: 0.01\n",
      "Epoch [2328/20000], Loss: 1213.9586181640625, Entropy 103.3990478515625, Learning Rate: 0.01\n",
      "Epoch [2329/20000], Loss: 1165.79248046875, Entropy 102.16419982910156, Learning Rate: 0.01\n",
      "Epoch [2330/20000], Loss: 1087.9649658203125, Entropy 107.26548767089844, Learning Rate: 0.01\n",
      "Epoch [2331/20000], Loss: 1124.3023681640625, Entropy 113.76626586914062, Learning Rate: 0.01\n",
      "Epoch [2332/20000], Loss: 1129.3016357421875, Entropy 82.87958526611328, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2333/20000], Loss: 1106.046142578125, Entropy 112.1419906616211, Learning Rate: 0.01\n",
      "Epoch [2334/20000], Loss: 1142.7210693359375, Entropy 97.21216583251953, Learning Rate: 0.01\n",
      "Epoch [2335/20000], Loss: 1162.9439697265625, Entropy 108.54441833496094, Learning Rate: 0.01\n",
      "Epoch [2336/20000], Loss: 1140.67822265625, Entropy 113.52111053466797, Learning Rate: 0.01\n",
      "Epoch [2337/20000], Loss: 1092.18212890625, Entropy 115.28301239013672, Learning Rate: 0.01\n",
      "Epoch [2338/20000], Loss: 1145.59326171875, Entropy 101.02874755859375, Learning Rate: 0.01\n",
      "Epoch [2339/20000], Loss: 1117.8414306640625, Entropy 98.52943420410156, Learning Rate: 0.01\n",
      "Epoch [2340/20000], Loss: 1122.7738037109375, Entropy 116.7001724243164, Learning Rate: 0.01\n",
      "Epoch [2341/20000], Loss: 1144.4345703125, Entropy 111.49152374267578, Learning Rate: 0.01\n",
      "Epoch [2342/20000], Loss: 1097.1173095703125, Entropy 112.46739196777344, Learning Rate: 0.01\n",
      "Epoch [2343/20000], Loss: 1099.635498046875, Entropy 110.2525863647461, Learning Rate: 0.01\n",
      "Epoch [2344/20000], Loss: 1151.4471435546875, Entropy 96.91923522949219, Learning Rate: 0.01\n",
      "Epoch [2345/20000], Loss: 1067.3681640625, Entropy 104.10836791992188, Learning Rate: 0.01\n",
      "Epoch [2346/20000], Loss: 1122.5296630859375, Entropy 107.77879333496094, Learning Rate: 0.01\n",
      "Epoch [2347/20000], Loss: 1079.400390625, Entropy 120.66243743896484, Learning Rate: 0.01\n",
      "Epoch [2348/20000], Loss: 1123.7276611328125, Entropy 115.80467224121094, Learning Rate: 0.01\n",
      "Epoch [2349/20000], Loss: 1128.7479248046875, Entropy 99.5671157836914, Learning Rate: 0.01\n",
      "Epoch [2350/20000], Loss: 1089.1109619140625, Entropy 107.11481475830078, Learning Rate: 0.01\n",
      "Epoch [2351/20000], Loss: 1130.473388671875, Entropy 105.5965805053711, Learning Rate: 0.01\n",
      "Epoch [2352/20000], Loss: 1068.64208984375, Entropy 119.02884674072266, Learning Rate: 0.01\n",
      "Epoch [2353/20000], Loss: 1097.354248046875, Entropy 105.6015625, Learning Rate: 0.01\n",
      "Epoch [2354/20000], Loss: 1108.2469482421875, Entropy 114.23595428466797, Learning Rate: 0.01\n",
      "Epoch [2355/20000], Loss: 1107.715576171875, Entropy 102.36189270019531, Learning Rate: 0.01\n",
      "Epoch [2356/20000], Loss: 1079.159912109375, Entropy 113.1549072265625, Learning Rate: 0.01\n",
      "Epoch [2357/20000], Loss: 1201.891357421875, Entropy 102.75257873535156, Learning Rate: 0.01\n",
      "Epoch [2358/20000], Loss: 1124.2449951171875, Entropy 120.2416000366211, Learning Rate: 0.01\n",
      "Epoch [2359/20000], Loss: 1142.930419921875, Entropy 114.6493911743164, Learning Rate: 0.01\n",
      "Epoch [2360/20000], Loss: 1118.056884765625, Entropy 110.4008560180664, Learning Rate: 0.01\n",
      "Epoch [2361/20000], Loss: 1112.06982421875, Entropy 115.55653381347656, Learning Rate: 0.01\n",
      "Epoch [2362/20000], Loss: 1192.5426025390625, Entropy 96.73423767089844, Learning Rate: 0.01\n",
      "Epoch [2363/20000], Loss: 1128.754638671875, Entropy 103.50000762939453, Learning Rate: 0.01\n",
      "Epoch [2364/20000], Loss: 1089.6083984375, Entropy 119.8407211303711, Learning Rate: 0.01\n",
      "Epoch [2365/20000], Loss: 1098.5565185546875, Entropy 110.6058349609375, Learning Rate: 0.01\n",
      "Epoch [2366/20000], Loss: 1149.49462890625, Entropy 124.45550537109375, Learning Rate: 0.01\n",
      "Epoch [2367/20000], Loss: 1104.581787109375, Entropy 113.92613220214844, Learning Rate: 0.01\n",
      "Epoch [2368/20000], Loss: 1103.6693115234375, Entropy 131.4456024169922, Learning Rate: 0.01\n",
      "Epoch [2369/20000], Loss: 1126.343017578125, Entropy 109.16790771484375, Learning Rate: 0.01\n",
      "Epoch [2370/20000], Loss: 1108.162353515625, Entropy 109.27003479003906, Learning Rate: 0.01\n",
      "Epoch [2371/20000], Loss: 1104.5975341796875, Entropy 118.05792999267578, Learning Rate: 0.01\n",
      "Epoch [2372/20000], Loss: 1118.07177734375, Entropy 108.55953216552734, Learning Rate: 0.01\n",
      "Epoch [2373/20000], Loss: 1178.306640625, Entropy 125.23153686523438, Learning Rate: 0.01\n",
      "Epoch [2374/20000], Loss: 1080.328369140625, Entropy 118.8209228515625, Learning Rate: 0.01\n",
      "Epoch [2375/20000], Loss: 1089.3994140625, Entropy 128.8915252685547, Learning Rate: 0.01\n",
      "Epoch [2376/20000], Loss: 1127.4412841796875, Entropy 115.02103424072266, Learning Rate: 0.01\n",
      "Epoch [2377/20000], Loss: 1138.71630859375, Entropy 119.19784545898438, Learning Rate: 0.01\n",
      "Epoch [2378/20000], Loss: 1071.556884765625, Entropy 119.87194061279297, Learning Rate: 0.01\n",
      "Epoch [2379/20000], Loss: 1092.9683837890625, Entropy 122.76890563964844, Learning Rate: 0.01\n",
      "Epoch [2380/20000], Loss: 1073.356201171875, Entropy 129.84999084472656, Learning Rate: 0.01\n",
      "Epoch [2381/20000], Loss: 1106.664794921875, Entropy 113.63291931152344, Learning Rate: 0.01\n",
      "Epoch [2382/20000], Loss: 1145.9429931640625, Entropy 106.40420532226562, Learning Rate: 0.01\n",
      "Epoch [2383/20000], Loss: 1060.949462890625, Entropy 115.75786590576172, Learning Rate: 0.01\n",
      "Epoch [2384/20000], Loss: 1155.9185791015625, Entropy 105.33403015136719, Learning Rate: 0.01\n",
      "Epoch [2385/20000], Loss: 1111.394287109375, Entropy 123.51522827148438, Learning Rate: 0.01\n",
      "Epoch [2386/20000], Loss: 1098.923828125, Entropy 130.15802001953125, Learning Rate: 0.01\n",
      "Epoch [2387/20000], Loss: 1100.9267578125, Entropy 118.97601318359375, Learning Rate: 0.01\n",
      "Epoch [2388/20000], Loss: 1067.7733154296875, Entropy 117.23851013183594, Learning Rate: 0.01\n",
      "Epoch [2389/20000], Loss: 1081.110107421875, Entropy 121.27536010742188, Learning Rate: 0.01\n",
      "Epoch [2390/20000], Loss: 1064.0048828125, Entropy 120.0559310913086, Learning Rate: 0.01\n",
      "Epoch [2391/20000], Loss: 1090.4605712890625, Entropy 112.54173278808594, Learning Rate: 0.01\n",
      "Epoch [2392/20000], Loss: 1093.251953125, Entropy 126.33071899414062, Learning Rate: 0.01\n",
      "Epoch [2393/20000], Loss: 1091.514892578125, Entropy 130.06358337402344, Learning Rate: 0.01\n",
      "Epoch [2394/20000], Loss: 1058.48486328125, Entropy 121.67687225341797, Learning Rate: 0.01\n",
      "Epoch [2395/20000], Loss: 1110.5521240234375, Entropy 123.63568878173828, Learning Rate: 0.01\n",
      "Epoch [2396/20000], Loss: 1123.86865234375, Entropy 122.75870513916016, Learning Rate: 0.01\n",
      "Epoch [2397/20000], Loss: 1089.052001953125, Entropy 117.81550598144531, Learning Rate: 0.01\n",
      "Epoch [2398/20000], Loss: 1148.496826171875, Entropy 108.0887680053711, Learning Rate: 0.01\n",
      "Epoch [2399/20000], Loss: 1089.2255859375, Entropy 114.2885513305664, Learning Rate: 0.01\n",
      "Epoch [2400/20000], Loss: 1079.755615234375, Entropy 134.9113311767578, Learning Rate: 0.01\n",
      "Epoch [2401/20000], Loss: 1109.78173828125, Entropy 124.71502685546875, Learning Rate: 0.01\n",
      "Epoch [2402/20000], Loss: 1090.220947265625, Entropy 136.76495361328125, Learning Rate: 0.01\n",
      "Epoch [2403/20000], Loss: 1083.214111328125, Entropy 127.16166687011719, Learning Rate: 0.01\n",
      "Epoch [2404/20000], Loss: 1086.1004638671875, Entropy 118.81497955322266, Learning Rate: 0.01\n",
      "Epoch [2405/20000], Loss: 1061.7513427734375, Entropy 111.35887145996094, Learning Rate: 0.01\n",
      "Epoch [2406/20000], Loss: 1108.451171875, Entropy 103.63385772705078, Learning Rate: 0.01\n",
      "Epoch [2407/20000], Loss: 1083.289794921875, Entropy 141.6628875732422, Learning Rate: 0.01\n",
      "Epoch [2408/20000], Loss: 1079.1444091796875, Entropy 125.36628723144531, Learning Rate: 0.01\n",
      "Epoch [2409/20000], Loss: 1125.7587890625, Entropy 116.01123046875, Learning Rate: 0.01\n",
      "Epoch [2410/20000], Loss: 1085.2742919921875, Entropy 148.55580139160156, Learning Rate: 0.01\n",
      "Epoch [2411/20000], Loss: 1129.5338134765625, Entropy 135.77810668945312, Learning Rate: 0.01\n",
      "Epoch [2412/20000], Loss: 1086.475341796875, Entropy 126.9515609741211, Learning Rate: 0.01\n",
      "Epoch [2413/20000], Loss: 1109.1746826171875, Entropy 127.7632064819336, Learning Rate: 0.01\n",
      "Epoch [2414/20000], Loss: 1105.7554931640625, Entropy 107.42511749267578, Learning Rate: 0.01\n",
      "Epoch [2415/20000], Loss: 1181.880126953125, Entropy 117.96489715576172, Learning Rate: 0.01\n",
      "Epoch [2416/20000], Loss: 1091.2012939453125, Entropy 130.740234375, Learning Rate: 0.01\n",
      "Epoch [2417/20000], Loss: 1066.0760498046875, Entropy 123.07343292236328, Learning Rate: 0.01\n",
      "Epoch [2418/20000], Loss: 1028.5963134765625, Entropy 135.7457733154297, Learning Rate: 0.01\n",
      "Epoch [2419/20000], Loss: 1164.1837158203125, Entropy 121.14269256591797, Learning Rate: 0.01\n",
      "Epoch [2420/20000], Loss: 1085.8428955078125, Entropy 122.28369140625, Learning Rate: 0.01\n",
      "Epoch [2421/20000], Loss: 1093.8814697265625, Entropy 118.39261627197266, Learning Rate: 0.01\n",
      "Epoch [2422/20000], Loss: 1058.2327880859375, Entropy 125.75122833251953, Learning Rate: 0.01\n",
      "Epoch [2423/20000], Loss: 1077.881103515625, Entropy 141.24935913085938, Learning Rate: 0.01\n",
      "Epoch [2424/20000], Loss: 1139.0550537109375, Entropy 136.110595703125, Learning Rate: 0.01\n",
      "Epoch [2425/20000], Loss: 1116.6434326171875, Entropy 118.13753509521484, Learning Rate: 0.01\n",
      "Epoch [2426/20000], Loss: 1082.11376953125, Entropy 138.8415069580078, Learning Rate: 0.01\n",
      "Epoch [2427/20000], Loss: 1067.554931640625, Entropy 141.8542938232422, Learning Rate: 0.01\n",
      "Epoch [2428/20000], Loss: 1089.015625, Entropy 131.47943115234375, Learning Rate: 0.01\n",
      "Epoch [2429/20000], Loss: 1103.26171875, Entropy 120.0643310546875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2430/20000], Loss: 1100.94384765625, Entropy 122.4891586303711, Learning Rate: 0.01\n",
      "Epoch [2431/20000], Loss: 1057.4661865234375, Entropy 129.23861694335938, Learning Rate: 0.01\n",
      "Epoch [2432/20000], Loss: 1086.58740234375, Entropy 123.4455795288086, Learning Rate: 0.01\n",
      "Epoch [2433/20000], Loss: 1107.76171875, Entropy 126.0019302368164, Learning Rate: 0.01\n",
      "Epoch [2434/20000], Loss: 1108.9241943359375, Entropy 128.98667907714844, Learning Rate: 0.01\n",
      "Epoch [2435/20000], Loss: 1146.8330078125, Entropy 126.80758666992188, Learning Rate: 0.01\n",
      "Epoch [2436/20000], Loss: 1066.1920166015625, Entropy 123.85425567626953, Learning Rate: 0.01\n",
      "Epoch [2437/20000], Loss: 1116.36328125, Entropy 122.60185241699219, Learning Rate: 0.01\n",
      "Epoch [2438/20000], Loss: 1127.4879150390625, Entropy 131.23887634277344, Learning Rate: 0.01\n",
      "Epoch [2439/20000], Loss: 1078.240966796875, Entropy 149.1881866455078, Learning Rate: 0.01\n",
      "Epoch [2440/20000], Loss: 1054.6597900390625, Entropy 119.91275024414062, Learning Rate: 0.01\n",
      "Epoch [2441/20000], Loss: 1217.88330078125, Entropy 130.79638671875, Learning Rate: 0.01\n",
      "Epoch [2442/20000], Loss: 1107.69140625, Entropy 143.0938720703125, Learning Rate: 0.01\n",
      "Epoch [2443/20000], Loss: 1098.701171875, Entropy 128.85525512695312, Learning Rate: 0.01\n",
      "Epoch [2444/20000], Loss: 1092.730224609375, Entropy 137.7305450439453, Learning Rate: 0.01\n",
      "Epoch [2445/20000], Loss: 1133.168701171875, Entropy 129.2587432861328, Learning Rate: 0.01\n",
      "Epoch [2446/20000], Loss: 1083.0621337890625, Entropy 138.8604278564453, Learning Rate: 0.01\n",
      "Epoch [2447/20000], Loss: 1123.56298828125, Entropy 145.4869842529297, Learning Rate: 0.01\n",
      "Epoch [2448/20000], Loss: 1075.0400390625, Entropy 144.48519897460938, Learning Rate: 0.01\n",
      "Epoch [2449/20000], Loss: 1100.884033203125, Entropy 130.16583251953125, Learning Rate: 0.01\n",
      "Epoch [2450/20000], Loss: 1053.2398681640625, Entropy 125.04468536376953, Learning Rate: 0.01\n",
      "Epoch [2451/20000], Loss: 1121.00927734375, Entropy 127.49568176269531, Learning Rate: 0.01\n",
      "Epoch [2452/20000], Loss: 1026.7938232421875, Entropy 147.9093780517578, Learning Rate: 0.01\n",
      "Epoch [2453/20000], Loss: 1103.7774658203125, Entropy 130.85293579101562, Learning Rate: 0.01\n",
      "Epoch [2454/20000], Loss: 1061.5780029296875, Entropy 145.6810760498047, Learning Rate: 0.01\n",
      "Epoch [2455/20000], Loss: 1070.700927734375, Entropy 137.35264587402344, Learning Rate: 0.01\n",
      "Epoch [2456/20000], Loss: 1056.750244140625, Entropy 148.13128662109375, Learning Rate: 0.01\n",
      "Epoch [2457/20000], Loss: 1115.1396484375, Entropy 125.5384521484375, Learning Rate: 0.01\n",
      "Epoch [2458/20000], Loss: 1111.2734375, Entropy 142.30516052246094, Learning Rate: 0.01\n",
      "Epoch [2459/20000], Loss: 1073.826904296875, Entropy 135.76068115234375, Learning Rate: 0.01\n",
      "Epoch [2460/20000], Loss: 1056.7783203125, Entropy 139.70986938476562, Learning Rate: 0.01\n",
      "Epoch [2461/20000], Loss: 1064.218017578125, Entropy 139.10720825195312, Learning Rate: 0.01\n",
      "Epoch [2462/20000], Loss: 1092.78955078125, Entropy 133.04185485839844, Learning Rate: 0.01\n",
      "Epoch [2463/20000], Loss: 1141.161376953125, Entropy 131.55149841308594, Learning Rate: 0.01\n",
      "Epoch [2464/20000], Loss: 1108.6328125, Entropy 128.78077697753906, Learning Rate: 0.01\n",
      "Epoch [2465/20000], Loss: 1101.791015625, Entropy 138.75799560546875, Learning Rate: 0.01\n",
      "Epoch [2466/20000], Loss: 1086.0731201171875, Entropy 149.99459838867188, Learning Rate: 0.01\n",
      "Epoch [2467/20000], Loss: 1149.810302734375, Entropy 145.67684936523438, Learning Rate: 0.01\n",
      "Epoch [2468/20000], Loss: 1091.86181640625, Entropy 142.27151489257812, Learning Rate: 0.01\n",
      "Epoch [2469/20000], Loss: 1104.849853515625, Entropy 141.89500427246094, Learning Rate: 0.01\n",
      "Epoch [2470/20000], Loss: 1063.376708984375, Entropy 136.2853546142578, Learning Rate: 0.01\n",
      "Epoch [2471/20000], Loss: 1124.2493896484375, Entropy 133.0404815673828, Learning Rate: 0.01\n",
      "Epoch [2472/20000], Loss: 1053.587646484375, Entropy 128.95643615722656, Learning Rate: 0.01\n",
      "Epoch [2473/20000], Loss: 1088.2955322265625, Entropy 146.15208435058594, Learning Rate: 0.01\n",
      "Epoch [2474/20000], Loss: 1096.7315673828125, Entropy 132.89846801757812, Learning Rate: 0.01\n",
      "Epoch [2475/20000], Loss: 1061.21142578125, Entropy 136.4425506591797, Learning Rate: 0.01\n",
      "Epoch [2476/20000], Loss: 1071.7962646484375, Entropy 144.14138793945312, Learning Rate: 0.01\n",
      "Epoch [2477/20000], Loss: 1081.3243408203125, Entropy 137.17527770996094, Learning Rate: 0.01\n",
      "Epoch [2478/20000], Loss: 1124.533447265625, Entropy 147.98275756835938, Learning Rate: 0.01\n",
      "Epoch [2479/20000], Loss: 1053.5833740234375, Entropy 142.62718200683594, Learning Rate: 0.01\n",
      "Epoch [2480/20000], Loss: 1129.01953125, Entropy 133.17803955078125, Learning Rate: 0.01\n",
      "Epoch [2481/20000], Loss: 1172.48291015625, Entropy 136.1603546142578, Learning Rate: 0.01\n",
      "Epoch [2482/20000], Loss: 1111.5234375, Entropy 130.80987548828125, Learning Rate: 0.01\n",
      "Epoch [2483/20000], Loss: 1080.1798095703125, Entropy 144.99839782714844, Learning Rate: 0.01\n",
      "Epoch [2484/20000], Loss: 1091.737060546875, Entropy 140.47442626953125, Learning Rate: 0.01\n",
      "Epoch [2485/20000], Loss: 1049.87353515625, Entropy 151.85922241210938, Learning Rate: 0.01\n",
      "Epoch [2486/20000], Loss: 1089.27490234375, Entropy 128.79266357421875, Learning Rate: 0.01\n",
      "Epoch [2487/20000], Loss: 1095.07080078125, Entropy 142.83741760253906, Learning Rate: 0.01\n",
      "Epoch [2488/20000], Loss: 1108.28955078125, Entropy 139.78970336914062, Learning Rate: 0.01\n",
      "Epoch [2489/20000], Loss: 1084.4315185546875, Entropy 119.770751953125, Learning Rate: 0.01\n",
      "Epoch [2490/20000], Loss: 1052.0379638671875, Entropy 158.63748168945312, Learning Rate: 0.01\n",
      "Epoch [2491/20000], Loss: 1165.1800537109375, Entropy 128.128662109375, Learning Rate: 0.01\n",
      "Epoch [2492/20000], Loss: 1078.568359375, Entropy 125.85880279541016, Learning Rate: 0.01\n",
      "Epoch [2493/20000], Loss: 1106.042236328125, Entropy 126.90806579589844, Learning Rate: 0.01\n",
      "Epoch [2494/20000], Loss: 1090.0440673828125, Entropy 160.3966522216797, Learning Rate: 0.01\n",
      "Epoch [2495/20000], Loss: 1083.142333984375, Entropy 134.2451629638672, Learning Rate: 0.01\n",
      "Epoch [2496/20000], Loss: 1082.24267578125, Entropy 133.48594665527344, Learning Rate: 0.01\n",
      "Epoch [2497/20000], Loss: 1098.1444091796875, Entropy 145.3574981689453, Learning Rate: 0.01\n",
      "Epoch [2498/20000], Loss: 1088.4429931640625, Entropy 132.4265594482422, Learning Rate: 0.01\n",
      "Epoch [2499/20000], Loss: 1077.6385498046875, Entropy 147.2076873779297, Learning Rate: 0.01\n",
      "Epoch [2500/20000], Loss: 1093.716796875, Entropy 137.491943359375, Learning Rate: 0.01\n",
      "Epoch [2501/20000], Loss: 1142.740234375, Entropy 148.30990600585938, Learning Rate: 0.01\n",
      "Epoch [2502/20000], Loss: 1035.787841796875, Entropy 147.88182067871094, Learning Rate: 0.01\n",
      "Epoch [2503/20000], Loss: 1187.68310546875, Entropy 149.2773895263672, Learning Rate: 0.01\n",
      "Epoch [2504/20000], Loss: 1069.3349609375, Entropy 161.17857360839844, Learning Rate: 0.01\n",
      "Epoch [2505/20000], Loss: 1150.536865234375, Entropy 125.68994140625, Learning Rate: 0.01\n",
      "Epoch [2506/20000], Loss: 1076.625244140625, Entropy 147.1195526123047, Learning Rate: 0.01\n",
      "Epoch [2507/20000], Loss: 1092.8310546875, Entropy 139.48951721191406, Learning Rate: 0.01\n",
      "Epoch [2508/20000], Loss: 1078.2783203125, Entropy 153.2919158935547, Learning Rate: 0.01\n",
      "Epoch [2509/20000], Loss: 1116.3741455078125, Entropy 143.71200561523438, Learning Rate: 0.01\n",
      "Epoch [2510/20000], Loss: 1110.416259765625, Entropy 141.8877716064453, Learning Rate: 0.01\n",
      "Epoch [2511/20000], Loss: 1093.9346923828125, Entropy 147.25462341308594, Learning Rate: 0.01\n",
      "Epoch [2512/20000], Loss: 1145.975341796875, Entropy 133.22914123535156, Learning Rate: 0.01\n",
      "Epoch [2513/20000], Loss: 1095.0501708984375, Entropy 153.09388732910156, Learning Rate: 0.01\n",
      "Epoch [2514/20000], Loss: 1112.037353515625, Entropy 149.99525451660156, Learning Rate: 0.01\n",
      "Epoch [2515/20000], Loss: 1102.404052734375, Entropy 148.81520080566406, Learning Rate: 0.01\n",
      "Epoch [2516/20000], Loss: 1103.8001708984375, Entropy 140.7318115234375, Learning Rate: 0.01\n",
      "Epoch [2517/20000], Loss: 1093.954345703125, Entropy 154.07550048828125, Learning Rate: 0.01\n",
      "Epoch [2518/20000], Loss: 1157.1949462890625, Entropy 161.5814971923828, Learning Rate: 0.01\n",
      "Epoch [2519/20000], Loss: 1122.1053466796875, Entropy 138.51075744628906, Learning Rate: 0.01\n",
      "Epoch [2520/20000], Loss: 1159.284912109375, Entropy 149.35379028320312, Learning Rate: 0.01\n",
      "Epoch [2521/20000], Loss: 1096.78564453125, Entropy 173.23245239257812, Learning Rate: 0.01\n",
      "Epoch [2522/20000], Loss: 1138.5035400390625, Entropy 147.03163146972656, Learning Rate: 0.01\n",
      "Epoch [2523/20000], Loss: 1073.4534912109375, Entropy 150.75942993164062, Learning Rate: 0.01\n",
      "Epoch [2524/20000], Loss: 1161.2550048828125, Entropy 132.63893127441406, Learning Rate: 0.01\n",
      "Epoch [2525/20000], Loss: 1168.2686767578125, Entropy 136.741943359375, Learning Rate: 0.01\n",
      "Epoch [2526/20000], Loss: 1094.72705078125, Entropy 144.6136932373047, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2527/20000], Loss: 1238.87548828125, Entropy 151.40296936035156, Learning Rate: 0.01\n",
      "Epoch [2528/20000], Loss: 1093.625732421875, Entropy 157.2416534423828, Learning Rate: 0.01\n",
      "Epoch [2529/20000], Loss: 1135.613525390625, Entropy 145.2501983642578, Learning Rate: 0.01\n",
      "Epoch [2530/20000], Loss: 1101.134765625, Entropy 160.21295166015625, Learning Rate: 0.01\n",
      "Epoch [2531/20000], Loss: 1141.953857421875, Entropy 150.51370239257812, Learning Rate: 0.01\n",
      "Epoch [2532/20000], Loss: 1133.9136962890625, Entropy 142.8251495361328, Learning Rate: 0.01\n",
      "Epoch [2533/20000], Loss: 1178.6865234375, Entropy 151.1887664794922, Learning Rate: 0.01\n",
      "Epoch [2534/20000], Loss: 1133.147705078125, Entropy 152.93045043945312, Learning Rate: 0.01\n",
      "Epoch [2535/20000], Loss: 1108.8406982421875, Entropy 138.4559326171875, Learning Rate: 0.01\n",
      "Epoch [2536/20000], Loss: 1148.677001953125, Entropy 157.71514892578125, Learning Rate: 0.01\n",
      "Epoch [2537/20000], Loss: 1097.7506103515625, Entropy 162.00131225585938, Learning Rate: 0.01\n",
      "Epoch [2538/20000], Loss: 1169.9630126953125, Entropy 125.10812377929688, Learning Rate: 0.01\n",
      "Epoch [2539/20000], Loss: 1079.427001953125, Entropy 152.62933349609375, Learning Rate: 0.01\n",
      "Epoch [2540/20000], Loss: 1154.92626953125, Entropy 134.35690307617188, Learning Rate: 0.01\n",
      "Epoch [2541/20000], Loss: 1180.4013671875, Entropy 152.47113037109375, Learning Rate: 0.01\n",
      "Epoch [2542/20000], Loss: 1232.1611328125, Entropy 152.97894287109375, Learning Rate: 0.01\n",
      "Epoch [2543/20000], Loss: 1094.6180419921875, Entropy 149.09312438964844, Learning Rate: 0.01\n",
      "Epoch [2544/20000], Loss: 1113.6236572265625, Entropy 146.79782104492188, Learning Rate: 0.01\n",
      "Epoch [2545/20000], Loss: 1175.1826171875, Entropy 136.17454528808594, Learning Rate: 0.01\n",
      "Epoch [2546/20000], Loss: 1124.75341796875, Entropy 154.90614318847656, Learning Rate: 0.01\n",
      "Epoch [2547/20000], Loss: 1142.78076171875, Entropy 147.0506591796875, Learning Rate: 0.01\n",
      "Epoch [2548/20000], Loss: 1159.1358642578125, Entropy 148.4987335205078, Learning Rate: 0.01\n",
      "Epoch [2549/20000], Loss: 1129.4439697265625, Entropy 170.0772247314453, Learning Rate: 0.01\n",
      "Epoch [2550/20000], Loss: 1142.9019775390625, Entropy 123.85297393798828, Learning Rate: 0.01\n",
      "Epoch [2551/20000], Loss: 1095.0823974609375, Entropy 160.5336151123047, Learning Rate: 0.01\n",
      "Epoch [2552/20000], Loss: 1045.706787109375, Entropy 177.8028106689453, Learning Rate: 0.01\n",
      "Epoch [2553/20000], Loss: 1116.2950439453125, Entropy 137.28013610839844, Learning Rate: 0.01\n",
      "Epoch [2554/20000], Loss: 1086.3656005859375, Entropy 160.4348602294922, Learning Rate: 0.01\n",
      "Epoch [2555/20000], Loss: 1146.3057861328125, Entropy 128.81497192382812, Learning Rate: 0.01\n",
      "Epoch [2556/20000], Loss: 1081.6697998046875, Entropy 155.36378479003906, Learning Rate: 0.01\n",
      "Epoch [2557/20000], Loss: 1085.4261474609375, Entropy 159.98458862304688, Learning Rate: 0.01\n",
      "Epoch [2558/20000], Loss: 1099.2939453125, Entropy 152.88929748535156, Learning Rate: 0.01\n",
      "Epoch [2559/20000], Loss: 1108.0447998046875, Entropy 149.33689880371094, Learning Rate: 0.01\n",
      "Epoch [2560/20000], Loss: 1097.152587890625, Entropy 150.40318298339844, Learning Rate: 0.01\n",
      "Epoch [2561/20000], Loss: 1104.3828125, Entropy 144.6986083984375, Learning Rate: 0.01\n",
      "Epoch [2562/20000], Loss: 1079.7298583984375, Entropy 141.01795959472656, Learning Rate: 0.01\n",
      "Epoch [2563/20000], Loss: 1121.3812255859375, Entropy 150.19436645507812, Learning Rate: 0.01\n",
      "Epoch [2564/20000], Loss: 1108.4427490234375, Entropy 181.1530303955078, Learning Rate: 0.01\n",
      "Epoch [2565/20000], Loss: 1058.5870361328125, Entropy 149.64540100097656, Learning Rate: 0.01\n",
      "Epoch [2566/20000], Loss: 1067.4781494140625, Entropy 151.7622833251953, Learning Rate: 0.01\n",
      "Epoch [2567/20000], Loss: 1083.7591552734375, Entropy 152.32162475585938, Learning Rate: 0.01\n",
      "Epoch [2568/20000], Loss: 1077.220458984375, Entropy 168.85353088378906, Learning Rate: 0.01\n",
      "Epoch [2569/20000], Loss: 1051.695068359375, Entropy 145.91958618164062, Learning Rate: 0.01\n",
      "Epoch [2570/20000], Loss: 1086.5787353515625, Entropy 163.18716430664062, Learning Rate: 0.01\n",
      "Epoch [2571/20000], Loss: 1113.572265625, Entropy 162.57676696777344, Learning Rate: 0.01\n",
      "Epoch [2572/20000], Loss: 1104.4326171875, Entropy 139.13961791992188, Learning Rate: 0.01\n",
      "Epoch [2573/20000], Loss: 1073.23193359375, Entropy 148.37188720703125, Learning Rate: 0.01\n",
      "Epoch [2574/20000], Loss: 1101.062744140625, Entropy 157.25296020507812, Learning Rate: 0.01\n",
      "Epoch [2575/20000], Loss: 1067.5654296875, Entropy 149.68991088867188, Learning Rate: 0.01\n",
      "Epoch [2576/20000], Loss: 1114.897216796875, Entropy 137.2972869873047, Learning Rate: 0.01\n",
      "Epoch [2577/20000], Loss: 1050.918212890625, Entropy 159.0258331298828, Learning Rate: 0.01\n",
      "Epoch [2578/20000], Loss: 1171.9892578125, Entropy 148.71368408203125, Learning Rate: 0.01\n",
      "Epoch [2579/20000], Loss: 1034.196044921875, Entropy 159.2748260498047, Learning Rate: 0.01\n",
      "Epoch [2580/20000], Loss: 1191.7080078125, Entropy 150.3915252685547, Learning Rate: 0.01\n",
      "Epoch [2581/20000], Loss: 1099.0958251953125, Entropy 157.94070434570312, Learning Rate: 0.01\n",
      "Epoch [2582/20000], Loss: 1107.955810546875, Entropy 139.5665740966797, Learning Rate: 0.01\n",
      "Epoch [2583/20000], Loss: 1072.6737060546875, Entropy 140.14378356933594, Learning Rate: 0.01\n",
      "Epoch [2584/20000], Loss: 1144.5189208984375, Entropy 153.63999938964844, Learning Rate: 0.01\n",
      "Epoch [2585/20000], Loss: 1095.67919921875, Entropy 163.19775390625, Learning Rate: 0.01\n",
      "Epoch [2586/20000], Loss: 1047.7606201171875, Entropy 157.82472229003906, Learning Rate: 0.01\n",
      "Epoch [2587/20000], Loss: 1188.143798828125, Entropy 162.5148468017578, Learning Rate: 0.01\n",
      "Epoch [2588/20000], Loss: 1138.268310546875, Entropy 152.6490936279297, Learning Rate: 0.01\n",
      "Epoch [2589/20000], Loss: 1139.6553955078125, Entropy 156.4479217529297, Learning Rate: 0.01\n",
      "Epoch [2590/20000], Loss: 1108.6328125, Entropy 168.10772705078125, Learning Rate: 0.01\n",
      "Epoch [2591/20000], Loss: 1062.33984375, Entropy 159.75836181640625, Learning Rate: 0.01\n",
      "Epoch [2592/20000], Loss: 1097.26904296875, Entropy 143.0892333984375, Learning Rate: 0.01\n",
      "Epoch [2593/20000], Loss: 1080.67236328125, Entropy 154.04750061035156, Learning Rate: 0.01\n",
      "Epoch [2594/20000], Loss: 1065.95166015625, Entropy 162.02964782714844, Learning Rate: 0.01\n",
      "Epoch [2595/20000], Loss: 1082.2919921875, Entropy 151.42495727539062, Learning Rate: 0.01\n",
      "Epoch [2596/20000], Loss: 1088.3277587890625, Entropy 144.2272186279297, Learning Rate: 0.01\n",
      "Epoch [2597/20000], Loss: 1078.756591796875, Entropy 147.48582458496094, Learning Rate: 0.01\n",
      "Epoch [2598/20000], Loss: 1060.4383544921875, Entropy 172.5830078125, Learning Rate: 0.01\n",
      "Epoch [2599/20000], Loss: 1053.280029296875, Entropy 155.4206085205078, Learning Rate: 0.01\n",
      "Epoch [2600/20000], Loss: 1045.660400390625, Entropy 166.48626708984375, Learning Rate: 0.01\n",
      "Epoch [2601/20000], Loss: 1077.5133056640625, Entropy 172.098388671875, Learning Rate: 0.01\n",
      "Epoch [2602/20000], Loss: 1056.405029296875, Entropy 164.4225311279297, Learning Rate: 0.01\n",
      "Epoch [2603/20000], Loss: 1084.9085693359375, Entropy 152.37063598632812, Learning Rate: 0.01\n",
      "Epoch [2604/20000], Loss: 1083.14404296875, Entropy 157.5869903564453, Learning Rate: 0.01\n",
      "Epoch [2605/20000], Loss: 1076.343994140625, Entropy 164.3008575439453, Learning Rate: 0.01\n",
      "Epoch [2606/20000], Loss: 1097.2701416015625, Entropy 170.48306274414062, Learning Rate: 0.01\n",
      "Epoch [2607/20000], Loss: 1050.2420654296875, Entropy 165.46348571777344, Learning Rate: 0.01\n",
      "Epoch [2608/20000], Loss: 1069.5439453125, Entropy 148.11236572265625, Learning Rate: 0.01\n",
      "Epoch [2609/20000], Loss: 1060.2672119140625, Entropy 157.88485717773438, Learning Rate: 0.01\n",
      "Epoch [2610/20000], Loss: 1031.373046875, Entropy 167.8427734375, Learning Rate: 0.01\n",
      "Epoch [2611/20000], Loss: 1086.6082763671875, Entropy 152.4706268310547, Learning Rate: 0.01\n",
      "Epoch [2612/20000], Loss: 1066.96875, Entropy 153.95977783203125, Learning Rate: 0.01\n",
      "Epoch [2613/20000], Loss: 1095.91943359375, Entropy 154.96600341796875, Learning Rate: 0.01\n",
      "Epoch [2614/20000], Loss: 1064.701416015625, Entropy 165.3666534423828, Learning Rate: 0.01\n",
      "Epoch [2615/20000], Loss: 1053.55908203125, Entropy 174.32952880859375, Learning Rate: 0.01\n",
      "Epoch [2616/20000], Loss: 1183.0484619140625, Entropy 166.8221893310547, Learning Rate: 0.01\n",
      "Epoch [2617/20000], Loss: 1064.91259765625, Entropy 147.46658325195312, Learning Rate: 0.01\n",
      "Epoch [2618/20000], Loss: 1113.7008056640625, Entropy 155.5323486328125, Learning Rate: 0.01\n",
      "Epoch [2619/20000], Loss: 1107.2158203125, Entropy 156.31787109375, Learning Rate: 0.01\n",
      "Epoch [2620/20000], Loss: 1059.237548828125, Entropy 159.53860473632812, Learning Rate: 0.01\n",
      "Epoch [2621/20000], Loss: 1038.5032958984375, Entropy 171.1621856689453, Learning Rate: 0.01\n",
      "Epoch [2622/20000], Loss: 1129.4697265625, Entropy 170.82797241210938, Learning Rate: 0.01\n",
      "Epoch [2623/20000], Loss: 1056.017333984375, Entropy 157.94224548339844, Learning Rate: 0.01\n",
      "Epoch [2624/20000], Loss: 1141.0357666015625, Entropy 148.7289581298828, Learning Rate: 0.01\n",
      "Epoch [2625/20000], Loss: 1072.7940673828125, Entropy 159.19357299804688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2626/20000], Loss: 1052.4124755859375, Entropy 167.9219512939453, Learning Rate: 0.01\n",
      "Epoch [2627/20000], Loss: 1100.470458984375, Entropy 168.82424926757812, Learning Rate: 0.01\n",
      "Epoch [2628/20000], Loss: 1071.090087890625, Entropy 160.7413330078125, Learning Rate: 0.01\n",
      "Epoch [2629/20000], Loss: 1106.1910400390625, Entropy 180.8211669921875, Learning Rate: 0.01\n",
      "Epoch [2630/20000], Loss: 1077.51611328125, Entropy 173.8154296875, Learning Rate: 0.01\n",
      "Epoch [2631/20000], Loss: 1089.0341796875, Entropy 169.3687286376953, Learning Rate: 0.01\n",
      "Epoch [2632/20000], Loss: 1052.045654296875, Entropy 166.927001953125, Learning Rate: 0.01\n",
      "Epoch [2633/20000], Loss: 1076.2420654296875, Entropy 160.7602081298828, Learning Rate: 0.01\n",
      "Epoch [2634/20000], Loss: 1100.372802734375, Entropy 159.05406188964844, Learning Rate: 0.01\n",
      "Epoch [2635/20000], Loss: 1092.947265625, Entropy 168.0790557861328, Learning Rate: 0.01\n",
      "Epoch [2636/20000], Loss: 1043.2550048828125, Entropy 155.6693115234375, Learning Rate: 0.01\n",
      "Epoch [2637/20000], Loss: 1029.7177734375, Entropy 170.1600341796875, Learning Rate: 0.01\n",
      "Epoch [2638/20000], Loss: 1086.6231689453125, Entropy 170.28439331054688, Learning Rate: 0.01\n",
      "Epoch [2639/20000], Loss: 1101.63232421875, Entropy 162.96728515625, Learning Rate: 0.01\n",
      "Epoch [2640/20000], Loss: 1052.3837890625, Entropy 163.78855895996094, Learning Rate: 0.01\n",
      "Epoch [2641/20000], Loss: 1087.9454345703125, Entropy 159.77854919433594, Learning Rate: 0.01\n",
      "Epoch [2642/20000], Loss: 1086.290283203125, Entropy 154.56329345703125, Learning Rate: 0.01\n",
      "Epoch [2643/20000], Loss: 1078.6502685546875, Entropy 157.2847137451172, Learning Rate: 0.01\n",
      "Epoch [2644/20000], Loss: 1070.765380859375, Entropy 175.82086181640625, Learning Rate: 0.01\n",
      "Epoch [2645/20000], Loss: 1086.3963623046875, Entropy 147.29579162597656, Learning Rate: 0.01\n",
      "Epoch [2646/20000], Loss: 1065.201904296875, Entropy 170.970703125, Learning Rate: 0.01\n",
      "Epoch [2647/20000], Loss: 1061.851318359375, Entropy 158.3190460205078, Learning Rate: 0.01\n",
      "Epoch [2648/20000], Loss: 1095.001708984375, Entropy 165.05667114257812, Learning Rate: 0.01\n",
      "Epoch [2649/20000], Loss: 1047.4444580078125, Entropy 171.8134002685547, Learning Rate: 0.01\n",
      "Epoch [2650/20000], Loss: 1096.96533203125, Entropy 163.0865936279297, Learning Rate: 0.01\n",
      "Epoch [2651/20000], Loss: 1122.680419921875, Entropy 145.94155883789062, Learning Rate: 0.01\n",
      "Epoch [2652/20000], Loss: 1064.6290283203125, Entropy 189.27796936035156, Learning Rate: 0.01\n",
      "Epoch [2653/20000], Loss: 1150.08544921875, Entropy 169.3376922607422, Learning Rate: 0.01\n",
      "Epoch [2654/20000], Loss: 1048.5335693359375, Entropy 175.57760620117188, Learning Rate: 0.01\n",
      "Epoch [2655/20000], Loss: 1127.2889404296875, Entropy 161.98252868652344, Learning Rate: 0.01\n",
      "Epoch [2656/20000], Loss: 1075.6749267578125, Entropy 165.02371215820312, Learning Rate: 0.01\n",
      "Epoch [2657/20000], Loss: 1047.4765625, Entropy 172.7418975830078, Learning Rate: 0.01\n",
      "Epoch [2658/20000], Loss: 1043.6876220703125, Entropy 182.1903533935547, Learning Rate: 0.01\n",
      "Epoch [2659/20000], Loss: 1083.6915283203125, Entropy 170.77890014648438, Learning Rate: 0.01\n",
      "Epoch [2660/20000], Loss: 1048.0908203125, Entropy 152.95689392089844, Learning Rate: 0.01\n",
      "Epoch [2661/20000], Loss: 1114.8306884765625, Entropy 171.07823181152344, Learning Rate: 0.01\n",
      "Epoch [2662/20000], Loss: 1095.8316650390625, Entropy 156.2136993408203, Learning Rate: 0.01\n",
      "Epoch [2663/20000], Loss: 1105.296630859375, Entropy 176.68885803222656, Learning Rate: 0.01\n",
      "Epoch [2664/20000], Loss: 1093.174560546875, Entropy 166.70416259765625, Learning Rate: 0.01\n",
      "Epoch [2665/20000], Loss: 1089.3262939453125, Entropy 170.3704071044922, Learning Rate: 0.01\n",
      "Epoch [2666/20000], Loss: 1078.0487060546875, Entropy 177.0399169921875, Learning Rate: 0.01\n",
      "Epoch [2667/20000], Loss: 1101.7437744140625, Entropy 165.0098419189453, Learning Rate: 0.01\n",
      "Epoch [2668/20000], Loss: 1135.44580078125, Entropy 178.57046508789062, Learning Rate: 0.01\n",
      "Epoch [2669/20000], Loss: 1062.178466796875, Entropy 172.76019287109375, Learning Rate: 0.01\n",
      "Epoch [2670/20000], Loss: 1028.636962890625, Entropy 179.82276916503906, Learning Rate: 0.01\n",
      "Epoch [2671/20000], Loss: 1109.9365234375, Entropy 184.7966766357422, Learning Rate: 0.01\n",
      "Epoch [2672/20000], Loss: 1082.9730224609375, Entropy 161.2649688720703, Learning Rate: 0.01\n",
      "Epoch [2673/20000], Loss: 1161.744140625, Entropy 165.82020568847656, Learning Rate: 0.01\n",
      "Epoch [2674/20000], Loss: 1118.8731689453125, Entropy 162.1774139404297, Learning Rate: 0.01\n",
      "Epoch [2675/20000], Loss: 1255.543212890625, Entropy 168.9054718017578, Learning Rate: 0.01\n",
      "Epoch [2676/20000], Loss: 1118.935546875, Entropy 155.62896728515625, Learning Rate: 0.01\n",
      "Epoch [2677/20000], Loss: 1284.483154296875, Entropy 170.27720642089844, Learning Rate: 0.01\n",
      "Epoch [2678/20000], Loss: 1170.9105224609375, Entropy 192.9384307861328, Learning Rate: 0.01\n",
      "Epoch [2679/20000], Loss: 1264.60400390625, Entropy 151.36561584472656, Learning Rate: 0.01\n",
      "Epoch [2680/20000], Loss: 1226.2303466796875, Entropy 169.59339904785156, Learning Rate: 0.01\n",
      "Epoch [2681/20000], Loss: 1218.7677001953125, Entropy 151.36058044433594, Learning Rate: 0.01\n",
      "Epoch [2682/20000], Loss: 1087.966064453125, Entropy 177.7732391357422, Learning Rate: 0.01\n",
      "Epoch [2683/20000], Loss: 1195.562744140625, Entropy 179.05288696289062, Learning Rate: 0.01\n",
      "Epoch [2684/20000], Loss: 1074.79833984375, Entropy 159.10348510742188, Learning Rate: 0.01\n",
      "Epoch [2685/20000], Loss: 1225.451904296875, Entropy 157.33590698242188, Learning Rate: 0.01\n",
      "Epoch [2686/20000], Loss: 1099.155517578125, Entropy 177.4664764404297, Learning Rate: 0.01\n",
      "Epoch [2687/20000], Loss: 1139.7734375, Entropy 166.33392333984375, Learning Rate: 0.01\n",
      "Epoch [2688/20000], Loss: 1099.6937255859375, Entropy 172.9349822998047, Learning Rate: 0.01\n",
      "Epoch [2689/20000], Loss: 1177.4693603515625, Entropy 152.19186401367188, Learning Rate: 0.01\n",
      "Epoch [2690/20000], Loss: 1148.0545654296875, Entropy 165.3973388671875, Learning Rate: 0.01\n",
      "Epoch [2691/20000], Loss: 1140.6158447265625, Entropy 171.89944458007812, Learning Rate: 0.01\n",
      "Epoch [2692/20000], Loss: 1065.808837890625, Entropy 167.93626403808594, Learning Rate: 0.01\n",
      "Epoch [2693/20000], Loss: 1126.215087890625, Entropy 171.45823669433594, Learning Rate: 0.01\n",
      "Epoch [2694/20000], Loss: 1058.2999267578125, Entropy 181.50559997558594, Learning Rate: 0.01\n",
      "Epoch [2695/20000], Loss: 1146.3739013671875, Entropy 172.971435546875, Learning Rate: 0.01\n",
      "Epoch [2696/20000], Loss: 1059.4085693359375, Entropy 181.03659057617188, Learning Rate: 0.01\n",
      "Epoch [2697/20000], Loss: 1041.9466552734375, Entropy 180.0477752685547, Learning Rate: 0.01\n",
      "Epoch [2698/20000], Loss: 1106.4439697265625, Entropy 171.7799530029297, Learning Rate: 0.01\n",
      "Epoch [2699/20000], Loss: 1130.2105712890625, Entropy 171.8736114501953, Learning Rate: 0.01\n",
      "Epoch [2700/20000], Loss: 1112.4208984375, Entropy 171.0645294189453, Learning Rate: 0.01\n",
      "Epoch [2701/20000], Loss: 1074.0264892578125, Entropy 166.25613403320312, Learning Rate: 0.005\n",
      "Epoch [2702/20000], Loss: 1065.137451171875, Entropy 169.1415557861328, Learning Rate: 0.005\n",
      "Epoch [2703/20000], Loss: 1127.61474609375, Entropy 176.71812438964844, Learning Rate: 0.005\n",
      "Epoch [2704/20000], Loss: 1070.883544921875, Entropy 161.01980590820312, Learning Rate: 0.005\n",
      "Epoch [2705/20000], Loss: 1111.67724609375, Entropy 183.88861083984375, Learning Rate: 0.005\n",
      "Epoch [2706/20000], Loss: 1015.944091796875, Entropy 182.676025390625, Learning Rate: 0.005\n",
      "Epoch [2707/20000], Loss: 1046.3408203125, Entropy 175.3694305419922, Learning Rate: 0.005\n",
      "Epoch [2708/20000], Loss: 1031.2593994140625, Entropy 167.27867126464844, Learning Rate: 0.005\n",
      "Epoch [2709/20000], Loss: 1041.902587890625, Entropy 177.173583984375, Learning Rate: 0.005\n",
      "Epoch [2710/20000], Loss: 1072.3984375, Entropy 177.850341796875, Learning Rate: 0.005\n",
      "Epoch [2711/20000], Loss: 1046.4437255859375, Entropy 191.05929565429688, Learning Rate: 0.005\n",
      "Epoch [2712/20000], Loss: 1082.82666015625, Entropy 169.57626342773438, Learning Rate: 0.005\n",
      "Epoch [2713/20000], Loss: 1057.4285888671875, Entropy 167.8809356689453, Learning Rate: 0.005\n",
      "Epoch [2714/20000], Loss: 1043.352783203125, Entropy 168.99658203125, Learning Rate: 0.005\n",
      "Epoch [2715/20000], Loss: 1087.1956787109375, Entropy 189.3807373046875, Learning Rate: 0.005\n",
      "Epoch [2716/20000], Loss: 1055.067138671875, Entropy 157.76953125, Learning Rate: 0.005\n",
      "Epoch [2717/20000], Loss: 1040.8013916015625, Entropy 171.59632873535156, Learning Rate: 0.005\n",
      "Epoch [2718/20000], Loss: 1142.30712890625, Entropy 166.16468811035156, Learning Rate: 0.005\n",
      "Epoch [2719/20000], Loss: 1070.913330078125, Entropy 174.90386962890625, Learning Rate: 0.005\n",
      "Epoch [2720/20000], Loss: 1017.8072509765625, Entropy 196.69178771972656, Learning Rate: 0.005\n",
      "Epoch [2721/20000], Loss: 1076.8616943359375, Entropy 175.90675354003906, Learning Rate: 0.005\n",
      "Epoch [2722/20000], Loss: 1038.877197265625, Entropy 162.50779724121094, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2723/20000], Loss: 1066.31201171875, Entropy 188.3208770751953, Learning Rate: 0.005\n",
      "Epoch [2724/20000], Loss: 1065.12158203125, Entropy 173.58917236328125, Learning Rate: 0.005\n",
      "Epoch [2725/20000], Loss: 1144.8380126953125, Entropy 153.62144470214844, Learning Rate: 0.005\n",
      "Epoch [2726/20000], Loss: 1070.2542724609375, Entropy 167.3734588623047, Learning Rate: 0.005\n",
      "Epoch [2727/20000], Loss: 1047.4698486328125, Entropy 174.375732421875, Learning Rate: 0.005\n",
      "Epoch [2728/20000], Loss: 1057.5577392578125, Entropy 169.2168731689453, Learning Rate: 0.005\n",
      "Epoch [2729/20000], Loss: 1080.577392578125, Entropy 171.7341766357422, Learning Rate: 0.005\n",
      "Epoch [2730/20000], Loss: 1124.6376953125, Entropy 177.12074279785156, Learning Rate: 0.005\n",
      "Epoch [2731/20000], Loss: 1030.0526123046875, Entropy 176.28468322753906, Learning Rate: 0.005\n",
      "Epoch [2732/20000], Loss: 1135.146728515625, Entropy 164.7709197998047, Learning Rate: 0.005\n",
      "Epoch [2733/20000], Loss: 1076.7591552734375, Entropy 173.05419921875, Learning Rate: 0.005\n",
      "Epoch [2734/20000], Loss: 1044.8076171875, Entropy 179.3065185546875, Learning Rate: 0.005\n",
      "Epoch [2735/20000], Loss: 1055.275634765625, Entropy 181.01158142089844, Learning Rate: 0.005\n",
      "Epoch [2736/20000], Loss: 1046.829345703125, Entropy 175.26730346679688, Learning Rate: 0.005\n",
      "Epoch [2737/20000], Loss: 1055.2076416015625, Entropy 177.9788360595703, Learning Rate: 0.005\n",
      "Epoch [2738/20000], Loss: 1054.5233154296875, Entropy 166.70823669433594, Learning Rate: 0.005\n",
      "Epoch [2739/20000], Loss: 1064.40087890625, Entropy 168.70941162109375, Learning Rate: 0.005\n",
      "Epoch [2740/20000], Loss: 1047.6593017578125, Entropy 175.89950561523438, Learning Rate: 0.005\n",
      "Epoch [2741/20000], Loss: 1044.8994140625, Entropy 161.34376525878906, Learning Rate: 0.005\n",
      "Epoch [2742/20000], Loss: 1019.8361206054688, Entropy 180.7303009033203, Learning Rate: 0.005\n",
      "Epoch [2743/20000], Loss: 1073.2105712890625, Entropy 182.8368682861328, Learning Rate: 0.005\n",
      "Epoch [2744/20000], Loss: 1042.35693359375, Entropy 164.765625, Learning Rate: 0.005\n",
      "Epoch [2745/20000], Loss: 1037.523193359375, Entropy 169.09754943847656, Learning Rate: 0.005\n",
      "Epoch [2746/20000], Loss: 1042.0576171875, Entropy 173.2296905517578, Learning Rate: 0.005\n",
      "Epoch [2747/20000], Loss: 1086.2030029296875, Entropy 170.30604553222656, Learning Rate: 0.005\n",
      "Epoch [2748/20000], Loss: 1043.469482421875, Entropy 181.58212280273438, Learning Rate: 0.005\n",
      "Epoch [2749/20000], Loss: 1096.378662109375, Entropy 160.35934448242188, Learning Rate: 0.005\n",
      "Epoch [2750/20000], Loss: 1055.765869140625, Entropy 160.46920776367188, Learning Rate: 0.005\n",
      "Epoch [2751/20000], Loss: 1056.963623046875, Entropy 159.5762481689453, Learning Rate: 0.005\n",
      "Epoch [2752/20000], Loss: 1062.1494140625, Entropy 168.8058624267578, Learning Rate: 0.005\n",
      "Epoch [2753/20000], Loss: 1042.925537109375, Entropy 185.20252990722656, Learning Rate: 0.005\n",
      "Epoch [2754/20000], Loss: 1112.4212646484375, Entropy 182.64356994628906, Learning Rate: 0.005\n",
      "Epoch [2755/20000], Loss: 1090.193115234375, Entropy 162.5203094482422, Learning Rate: 0.005\n",
      "Epoch [2756/20000], Loss: 1046.4913330078125, Entropy 166.51870727539062, Learning Rate: 0.005\n",
      "Epoch [2757/20000], Loss: 1027.4027099609375, Entropy 171.4041748046875, Learning Rate: 0.005\n",
      "Epoch [2758/20000], Loss: 1063.0745849609375, Entropy 175.5584716796875, Learning Rate: 0.005\n",
      "Epoch [2759/20000], Loss: 1024.1776123046875, Entropy 186.12828063964844, Learning Rate: 0.005\n",
      "Epoch [2760/20000], Loss: 1017.5360717773438, Entropy 189.6632843017578, Learning Rate: 0.005\n",
      "Epoch [2761/20000], Loss: 1070.69384765625, Entropy 166.5050811767578, Learning Rate: 0.005\n",
      "Epoch [2762/20000], Loss: 1033.4022216796875, Entropy 182.97093200683594, Learning Rate: 0.005\n",
      "Epoch [2763/20000], Loss: 1034.0419921875, Entropy 177.95480346679688, Learning Rate: 0.005\n",
      "Epoch [2764/20000], Loss: 1057.8138427734375, Entropy 172.3560333251953, Learning Rate: 0.005\n",
      "Epoch [2765/20000], Loss: 1028.30859375, Entropy 174.4617919921875, Learning Rate: 0.005\n",
      "Epoch [2766/20000], Loss: 1044.282470703125, Entropy 173.4467315673828, Learning Rate: 0.005\n",
      "Epoch [2767/20000], Loss: 1057.281494140625, Entropy 164.79046630859375, Learning Rate: 0.005\n",
      "Epoch [2768/20000], Loss: 1010.2182006835938, Entropy 186.55084228515625, Learning Rate: 0.005\n",
      "Epoch [2769/20000], Loss: 1050.519775390625, Entropy 187.49884033203125, Learning Rate: 0.005\n",
      "Epoch [2770/20000], Loss: 1113.671875, Entropy 178.34828186035156, Learning Rate: 0.005\n",
      "Epoch [2771/20000], Loss: 1022.5284423828125, Entropy 177.98793029785156, Learning Rate: 0.005\n",
      "Epoch [2772/20000], Loss: 1039.959716796875, Entropy 169.21170043945312, Learning Rate: 0.005\n",
      "Epoch [2773/20000], Loss: 1029.147216796875, Entropy 180.12210083007812, Learning Rate: 0.005\n",
      "Epoch [2774/20000], Loss: 1030.109375, Entropy 177.89288330078125, Learning Rate: 0.005\n",
      "Epoch [2775/20000], Loss: 1027.695068359375, Entropy 199.6156768798828, Learning Rate: 0.005\n",
      "Epoch [2776/20000], Loss: 1064.6351318359375, Entropy 164.68978881835938, Learning Rate: 0.005\n",
      "Epoch [2777/20000], Loss: 1034.48828125, Entropy 175.3205108642578, Learning Rate: 0.005\n",
      "Epoch [2778/20000], Loss: 1035.38623046875, Entropy 165.34420776367188, Learning Rate: 0.005\n",
      "Epoch [2779/20000], Loss: 1039.8291015625, Entropy 177.02528381347656, Learning Rate: 0.005\n",
      "Epoch [2780/20000], Loss: 1060.17626953125, Entropy 177.61831665039062, Learning Rate: 0.005\n",
      "Epoch [2781/20000], Loss: 1064.3082275390625, Entropy 163.11196899414062, Learning Rate: 0.005\n",
      "Epoch [2782/20000], Loss: 1019.9810180664062, Entropy 184.91436767578125, Learning Rate: 0.005\n",
      "Epoch [2783/20000], Loss: 990.9024047851562, Entropy 184.7781219482422, Learning Rate: 0.005\n",
      "Epoch [2784/20000], Loss: 1053.895751953125, Entropy 177.63552856445312, Learning Rate: 0.005\n",
      "Epoch [2785/20000], Loss: 999.1429443359375, Entropy 181.26792907714844, Learning Rate: 0.005\n",
      "Epoch [2786/20000], Loss: 1068.000732421875, Entropy 169.84046936035156, Learning Rate: 0.005\n",
      "Epoch [2787/20000], Loss: 1101.156982421875, Entropy 181.42745971679688, Learning Rate: 0.005\n",
      "Epoch [2788/20000], Loss: 1038.701416015625, Entropy 174.81773376464844, Learning Rate: 0.005\n",
      "Epoch [2789/20000], Loss: 1074.06787109375, Entropy 186.06829833984375, Learning Rate: 0.005\n",
      "Epoch [2790/20000], Loss: 1030.525390625, Entropy 189.57843017578125, Learning Rate: 0.005\n",
      "Epoch [2791/20000], Loss: 1002.431396484375, Entropy 185.64170837402344, Learning Rate: 0.005\n",
      "Epoch [2792/20000], Loss: 1044.04736328125, Entropy 182.18186950683594, Learning Rate: 0.005\n",
      "Epoch [2793/20000], Loss: 1050.2978515625, Entropy 159.89495849609375, Learning Rate: 0.005\n",
      "Epoch [2794/20000], Loss: 1039.958740234375, Entropy 173.1227569580078, Learning Rate: 0.005\n",
      "Epoch [2795/20000], Loss: 1030.4124755859375, Entropy 182.25718688964844, Learning Rate: 0.005\n",
      "Epoch [2796/20000], Loss: 1038.23779296875, Entropy 182.0380859375, Learning Rate: 0.005\n",
      "Epoch [2797/20000], Loss: 1052.01220703125, Entropy 184.5570526123047, Learning Rate: 0.005\n",
      "Epoch [2798/20000], Loss: 1024.086669921875, Entropy 176.9640655517578, Learning Rate: 0.005\n",
      "Epoch [2799/20000], Loss: 1077.79345703125, Entropy 196.889404296875, Learning Rate: 0.005\n",
      "Epoch [2800/20000], Loss: 1066.424560546875, Entropy 176.33197021484375, Learning Rate: 0.005\n",
      "Epoch [2801/20000], Loss: 1031.6484375, Entropy 189.52679443359375, Learning Rate: 0.005\n",
      "Epoch [2802/20000], Loss: 1024.000244140625, Entropy 190.77200317382812, Learning Rate: 0.005\n",
      "Epoch [2803/20000], Loss: 1035.4385986328125, Entropy 190.36740112304688, Learning Rate: 0.005\n",
      "Epoch [2804/20000], Loss: 1063.6566162109375, Entropy 188.80430603027344, Learning Rate: 0.005\n",
      "Epoch [2805/20000], Loss: 1099.7652587890625, Entropy 181.56919860839844, Learning Rate: 0.005\n",
      "Epoch [2806/20000], Loss: 1030.9974365234375, Entropy 185.2597198486328, Learning Rate: 0.005\n",
      "Epoch [2807/20000], Loss: 1035.1715087890625, Entropy 175.56588745117188, Learning Rate: 0.005\n",
      "Epoch [2808/20000], Loss: 1055.88525390625, Entropy 181.90440368652344, Learning Rate: 0.005\n",
      "Epoch [2809/20000], Loss: 1014.4066772460938, Entropy 184.2185821533203, Learning Rate: 0.005\n",
      "Epoch [2810/20000], Loss: 1049.6292724609375, Entropy 177.71841430664062, Learning Rate: 0.005\n",
      "Epoch [2811/20000], Loss: 1058.033203125, Entropy 174.36874389648438, Learning Rate: 0.005\n",
      "Epoch [2812/20000], Loss: 1026.6756591796875, Entropy 189.1123809814453, Learning Rate: 0.005\n",
      "Epoch [2813/20000], Loss: 1035.82958984375, Entropy 192.2622833251953, Learning Rate: 0.005\n",
      "Epoch [2814/20000], Loss: 1128.70458984375, Entropy 183.38870239257812, Learning Rate: 0.005\n",
      "Epoch [2815/20000], Loss: 1045.8817138671875, Entropy 176.07899475097656, Learning Rate: 0.005\n",
      "Epoch [2816/20000], Loss: 1088.113525390625, Entropy 158.2284698486328, Learning Rate: 0.005\n",
      "Epoch [2817/20000], Loss: 1045.428955078125, Entropy 170.70489501953125, Learning Rate: 0.005\n",
      "Epoch [2818/20000], Loss: 1040.200927734375, Entropy 185.24664306640625, Learning Rate: 0.005\n",
      "Epoch [2819/20000], Loss: 1090.6312255859375, Entropy 186.70323181152344, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2820/20000], Loss: 1063.787109375, Entropy 159.91932678222656, Learning Rate: 0.005\n",
      "Epoch [2821/20000], Loss: 995.57666015625, Entropy 190.69248962402344, Learning Rate: 0.005\n",
      "Epoch [2822/20000], Loss: 1087.29248046875, Entropy 181.85089111328125, Learning Rate: 0.005\n",
      "Epoch [2823/20000], Loss: 1013.377197265625, Entropy 190.59983825683594, Learning Rate: 0.005\n",
      "Epoch [2824/20000], Loss: 1034.8028564453125, Entropy 177.24278259277344, Learning Rate: 0.005\n",
      "Epoch [2825/20000], Loss: 1040.654541015625, Entropy 174.60406494140625, Learning Rate: 0.005\n",
      "Epoch [2826/20000], Loss: 1044.1387939453125, Entropy 192.4519500732422, Learning Rate: 0.005\n",
      "Epoch [2827/20000], Loss: 1031.3795166015625, Entropy 181.17979431152344, Learning Rate: 0.005\n",
      "Epoch [2828/20000], Loss: 1032.7286376953125, Entropy 195.92333984375, Learning Rate: 0.005\n",
      "Epoch [2829/20000], Loss: 1043.7789306640625, Entropy 190.75399780273438, Learning Rate: 0.005\n",
      "Epoch [2830/20000], Loss: 1082.0823974609375, Entropy 163.76943969726562, Learning Rate: 0.005\n",
      "Epoch [2831/20000], Loss: 1014.8505249023438, Entropy 181.8936309814453, Learning Rate: 0.005\n",
      "Epoch [2832/20000], Loss: 1014.2920532226562, Entropy 171.8348846435547, Learning Rate: 0.005\n",
      "Epoch [2833/20000], Loss: 1020.4367065429688, Entropy 204.1284942626953, Learning Rate: 0.005\n",
      "Epoch [2834/20000], Loss: 1048.186767578125, Entropy 182.14370727539062, Learning Rate: 0.005\n",
      "Epoch [2835/20000], Loss: 1033.205078125, Entropy 181.77346801757812, Learning Rate: 0.005\n",
      "Epoch [2836/20000], Loss: 999.2054443359375, Entropy 201.27804565429688, Learning Rate: 0.005\n",
      "Epoch [2837/20000], Loss: 1096.719970703125, Entropy 203.57565307617188, Learning Rate: 0.005\n",
      "Epoch [2838/20000], Loss: 1063.663818359375, Entropy 178.7635498046875, Learning Rate: 0.005\n",
      "Epoch [2839/20000], Loss: 1036.265380859375, Entropy 180.01165771484375, Learning Rate: 0.005\n",
      "Epoch [2840/20000], Loss: 1075.3131103515625, Entropy 186.5819854736328, Learning Rate: 0.005\n",
      "Epoch [2841/20000], Loss: 1024.7615966796875, Entropy 174.29933166503906, Learning Rate: 0.005\n",
      "Epoch [2842/20000], Loss: 993.702392578125, Entropy 175.01730346679688, Learning Rate: 0.005\n",
      "Epoch [2843/20000], Loss: 1011.0315551757812, Entropy 174.98468017578125, Learning Rate: 0.005\n",
      "Epoch [2844/20000], Loss: 1045.265869140625, Entropy 185.42613220214844, Learning Rate: 0.005\n",
      "Epoch [2845/20000], Loss: 1012.2013549804688, Entropy 196.5463104248047, Learning Rate: 0.005\n",
      "Epoch [2846/20000], Loss: 1055.7701416015625, Entropy 184.7311248779297, Learning Rate: 0.005\n",
      "Epoch [2847/20000], Loss: 1081.28125, Entropy 173.93753051757812, Learning Rate: 0.005\n",
      "Epoch [2848/20000], Loss: 1069.018310546875, Entropy 170.97840881347656, Learning Rate: 0.005\n",
      "Epoch [2849/20000], Loss: 1005.20068359375, Entropy 190.54214477539062, Learning Rate: 0.005\n",
      "Epoch [2850/20000], Loss: 1018.9605712890625, Entropy 203.20315551757812, Learning Rate: 0.005\n",
      "Epoch [2851/20000], Loss: 1058.7548828125, Entropy 158.77386474609375, Learning Rate: 0.005\n",
      "Epoch [2852/20000], Loss: 1023.7603759765625, Entropy 197.75306701660156, Learning Rate: 0.005\n",
      "Epoch [2853/20000], Loss: 1005.5172729492188, Entropy 189.0655059814453, Learning Rate: 0.005\n",
      "Epoch [2854/20000], Loss: 1050.8109130859375, Entropy 181.61534118652344, Learning Rate: 0.005\n",
      "Epoch [2855/20000], Loss: 1052.86572265625, Entropy 188.0216827392578, Learning Rate: 0.005\n",
      "Epoch [2856/20000], Loss: 1064.322509765625, Entropy 192.0624237060547, Learning Rate: 0.005\n",
      "Epoch [2857/20000], Loss: 1049.95703125, Entropy 183.51715087890625, Learning Rate: 0.005\n",
      "Epoch [2858/20000], Loss: 1040.741943359375, Entropy 194.56893920898438, Learning Rate: 0.005\n",
      "Epoch [2859/20000], Loss: 1054.2457275390625, Entropy 184.13343811035156, Learning Rate: 0.005\n",
      "Epoch [2860/20000], Loss: 1080.433349609375, Entropy 177.03480529785156, Learning Rate: 0.005\n",
      "Epoch [2861/20000], Loss: 1044.086181640625, Entropy 180.06277465820312, Learning Rate: 0.005\n",
      "Epoch [2862/20000], Loss: 1114.4072265625, Entropy 176.07249450683594, Learning Rate: 0.005\n",
      "Epoch [2863/20000], Loss: 995.5213012695312, Entropy 191.31695556640625, Learning Rate: 0.005\n",
      "Epoch [2864/20000], Loss: 1054.2396240234375, Entropy 161.51220703125, Learning Rate: 0.005\n",
      "Epoch [2865/20000], Loss: 1073.86376953125, Entropy 189.58868408203125, Learning Rate: 0.005\n",
      "Epoch [2866/20000], Loss: 1051.7059326171875, Entropy 191.3040313720703, Learning Rate: 0.005\n",
      "Epoch [2867/20000], Loss: 1029.278076171875, Entropy 181.05673217773438, Learning Rate: 0.005\n",
      "Epoch [2868/20000], Loss: 1062.142333984375, Entropy 177.5615997314453, Learning Rate: 0.005\n",
      "Epoch [2869/20000], Loss: 1137.9493408203125, Entropy 181.83358764648438, Learning Rate: 0.005\n",
      "Epoch [2870/20000], Loss: 1077.6541748046875, Entropy 180.7443389892578, Learning Rate: 0.005\n",
      "Epoch [2871/20000], Loss: 1034.916015625, Entropy 200.76206970214844, Learning Rate: 0.005\n",
      "Epoch [2872/20000], Loss: 1008.1491088867188, Entropy 203.01800537109375, Learning Rate: 0.005\n",
      "Epoch [2873/20000], Loss: 1112.0413818359375, Entropy 176.72787475585938, Learning Rate: 0.005\n",
      "Epoch [2874/20000], Loss: 1075.714599609375, Entropy 196.04798889160156, Learning Rate: 0.005\n",
      "Epoch [2875/20000], Loss: 1038.16943359375, Entropy 187.81344604492188, Learning Rate: 0.005\n",
      "Epoch [2876/20000], Loss: 1044.70458984375, Entropy 194.88519287109375, Learning Rate: 0.005\n",
      "Epoch [2877/20000], Loss: 1023.1455078125, Entropy 184.86013793945312, Learning Rate: 0.005\n",
      "Epoch [2878/20000], Loss: 1033.01611328125, Entropy 189.5578155517578, Learning Rate: 0.005\n",
      "Epoch [2879/20000], Loss: 1070.751953125, Entropy 187.48011779785156, Learning Rate: 0.005\n",
      "Epoch [2880/20000], Loss: 1070.688232421875, Entropy 190.7646484375, Learning Rate: 0.005\n",
      "Epoch [2881/20000], Loss: 1004.6407470703125, Entropy 198.97970581054688, Learning Rate: 0.005\n",
      "Epoch [2882/20000], Loss: 1115.70654296875, Entropy 195.67713928222656, Learning Rate: 0.005\n",
      "Epoch [2883/20000], Loss: 1022.5692749023438, Entropy 191.44134521484375, Learning Rate: 0.005\n",
      "Epoch [2884/20000], Loss: 1033.2862548828125, Entropy 189.71530151367188, Learning Rate: 0.005\n",
      "Epoch [2885/20000], Loss: 1054.647705078125, Entropy 187.3768768310547, Learning Rate: 0.005\n",
      "Epoch [2886/20000], Loss: 985.8203125, Entropy 195.67453002929688, Learning Rate: 0.005\n",
      "Epoch [2887/20000], Loss: 1006.3738403320312, Entropy 196.7892608642578, Learning Rate: 0.005\n",
      "Epoch [2888/20000], Loss: 1035.548095703125, Entropy 194.2053680419922, Learning Rate: 0.005\n",
      "Epoch [2889/20000], Loss: 1037.3857421875, Entropy 177.5619354248047, Learning Rate: 0.005\n",
      "Epoch [2890/20000], Loss: 1050.9813232421875, Entropy 197.39637756347656, Learning Rate: 0.005\n",
      "Epoch [2891/20000], Loss: 1025.37548828125, Entropy 178.15875244140625, Learning Rate: 0.005\n",
      "Epoch [2892/20000], Loss: 1087.2308349609375, Entropy 206.58261108398438, Learning Rate: 0.005\n",
      "Epoch [2893/20000], Loss: 986.6865844726562, Entropy 199.88336181640625, Learning Rate: 0.005\n",
      "Epoch [2894/20000], Loss: 1013.077392578125, Entropy 186.76126098632812, Learning Rate: 0.005\n",
      "Epoch [2895/20000], Loss: 1034.6708984375, Entropy 190.6074981689453, Learning Rate: 0.005\n",
      "Epoch [2896/20000], Loss: 1050.0849609375, Entropy 194.8153839111328, Learning Rate: 0.005\n",
      "Epoch [2897/20000], Loss: 1057.087158203125, Entropy 199.60585021972656, Learning Rate: 0.005\n",
      "Epoch [2898/20000], Loss: 1041.426025390625, Entropy 180.1984100341797, Learning Rate: 0.005\n",
      "Epoch [2899/20000], Loss: 1018.9814453125, Entropy 205.92417907714844, Learning Rate: 0.005\n",
      "Epoch [2900/20000], Loss: 1061.7725830078125, Entropy 174.96188354492188, Learning Rate: 0.005\n",
      "Epoch [2901/20000], Loss: 1052.804931640625, Entropy 184.4766387939453, Learning Rate: 0.005\n",
      "Epoch [2902/20000], Loss: 1095.7701416015625, Entropy 202.86183166503906, Learning Rate: 0.005\n",
      "Epoch [2903/20000], Loss: 1052.76220703125, Entropy 184.0030059814453, Learning Rate: 0.005\n",
      "Epoch [2904/20000], Loss: 1039.7572021484375, Entropy 194.8083953857422, Learning Rate: 0.005\n",
      "Epoch [2905/20000], Loss: 1035.946533203125, Entropy 191.09613037109375, Learning Rate: 0.005\n",
      "Epoch [2906/20000], Loss: 1027.486572265625, Entropy 189.57281494140625, Learning Rate: 0.005\n",
      "Epoch [2907/20000], Loss: 1012.5178833007812, Entropy 187.9031219482422, Learning Rate: 0.005\n",
      "Epoch [2908/20000], Loss: 1013.947998046875, Entropy 195.73739624023438, Learning Rate: 0.005\n",
      "Epoch [2909/20000], Loss: 1016.2423095703125, Entropy 181.23690795898438, Learning Rate: 0.005\n",
      "Epoch [2910/20000], Loss: 1083.0147705078125, Entropy 183.91200256347656, Learning Rate: 0.005\n",
      "Epoch [2911/20000], Loss: 1001.0714111328125, Entropy 200.93421936035156, Learning Rate: 0.005\n",
      "Epoch [2912/20000], Loss: 1015.6875, Entropy 191.93576049804688, Learning Rate: 0.005\n",
      "Epoch [2913/20000], Loss: 1027.2396240234375, Entropy 177.13441467285156, Learning Rate: 0.005\n",
      "Epoch [2914/20000], Loss: 1047.88623046875, Entropy 194.9853057861328, Learning Rate: 0.005\n",
      "Epoch [2915/20000], Loss: 1077.19189453125, Entropy 198.57284545898438, Learning Rate: 0.005\n",
      "Epoch [2916/20000], Loss: 1030.673095703125, Entropy 179.70465087890625, Learning Rate: 0.005\n",
      "Epoch [2917/20000], Loss: 1035.5970458984375, Entropy 197.24302673339844, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2918/20000], Loss: 1042.835205078125, Entropy 204.60205078125, Learning Rate: 0.005\n",
      "Epoch [2919/20000], Loss: 1011.8399658203125, Entropy 188.67312622070312, Learning Rate: 0.005\n",
      "Epoch [2920/20000], Loss: 988.0335083007812, Entropy 213.7797393798828, Learning Rate: 0.005\n",
      "Epoch [2921/20000], Loss: 1052.7159423828125, Entropy 197.3546905517578, Learning Rate: 0.005\n",
      "Epoch [2922/20000], Loss: 1011.980224609375, Entropy 200.7459716796875, Learning Rate: 0.005\n",
      "Epoch [2923/20000], Loss: 1040.5234375, Entropy 187.1424560546875, Learning Rate: 0.005\n",
      "Epoch [2924/20000], Loss: 1037.6136474609375, Entropy 208.14993286132812, Learning Rate: 0.005\n",
      "Epoch [2925/20000], Loss: 1012.1722412109375, Entropy 205.61170959472656, Learning Rate: 0.005\n",
      "Epoch [2926/20000], Loss: 1064.621826171875, Entropy 177.57347106933594, Learning Rate: 0.005\n",
      "Epoch [2927/20000], Loss: 1046.72265625, Entropy 186.8922882080078, Learning Rate: 0.005\n",
      "Epoch [2928/20000], Loss: 1061.4527587890625, Entropy 186.78005981445312, Learning Rate: 0.005\n",
      "Epoch [2929/20000], Loss: 1078.190673828125, Entropy 200.93731689453125, Learning Rate: 0.005\n",
      "Epoch [2930/20000], Loss: 1041.8680419921875, Entropy 205.4135284423828, Learning Rate: 0.005\n",
      "Epoch [2931/20000], Loss: 1041.9617919921875, Entropy 204.87608337402344, Learning Rate: 0.005\n",
      "Epoch [2932/20000], Loss: 1036.0472412109375, Entropy 200.53976440429688, Learning Rate: 0.005\n",
      "Epoch [2933/20000], Loss: 1043.146728515625, Entropy 196.41624450683594, Learning Rate: 0.005\n",
      "Epoch [2934/20000], Loss: 1041.111083984375, Entropy 195.79652404785156, Learning Rate: 0.005\n",
      "Epoch [2935/20000], Loss: 1040.8304443359375, Entropy 200.9359588623047, Learning Rate: 0.005\n",
      "Epoch [2936/20000], Loss: 1049.5723876953125, Entropy 202.307373046875, Learning Rate: 0.005\n",
      "Epoch [2937/20000], Loss: 1011.3670654296875, Entropy 197.45558166503906, Learning Rate: 0.005\n",
      "Epoch [2938/20000], Loss: 1026.739013671875, Entropy 193.28273010253906, Learning Rate: 0.005\n",
      "Epoch [2939/20000], Loss: 999.1551513671875, Entropy 209.685546875, Learning Rate: 0.005\n",
      "Epoch [2940/20000], Loss: 1037.183837890625, Entropy 185.92433166503906, Learning Rate: 0.005\n",
      "Epoch [2941/20000], Loss: 1061.979248046875, Entropy 195.8117218017578, Learning Rate: 0.005\n",
      "Epoch [2942/20000], Loss: 1018.0357055664062, Entropy 198.2819061279297, Learning Rate: 0.005\n",
      "Epoch [2943/20000], Loss: 1033.1787109375, Entropy 199.7616424560547, Learning Rate: 0.005\n",
      "Epoch [2944/20000], Loss: 978.00927734375, Entropy 196.31741333007812, Learning Rate: 0.005\n",
      "Epoch [2945/20000], Loss: 1016.724853515625, Entropy 197.44764709472656, Learning Rate: 0.005\n",
      "Epoch [2946/20000], Loss: 1049.3577880859375, Entropy 195.59217834472656, Learning Rate: 0.005\n",
      "Epoch [2947/20000], Loss: 1003.19482421875, Entropy 202.035888671875, Learning Rate: 0.005\n",
      "Epoch [2948/20000], Loss: 1056.158935546875, Entropy 211.51678466796875, Learning Rate: 0.005\n",
      "Epoch [2949/20000], Loss: 1005.6159057617188, Entropy 193.55560302734375, Learning Rate: 0.005\n",
      "Epoch [2950/20000], Loss: 1038.636474609375, Entropy 193.71783447265625, Learning Rate: 0.005\n",
      "Epoch [2951/20000], Loss: 1042.07470703125, Entropy 195.29444885253906, Learning Rate: 0.005\n",
      "Epoch [2952/20000], Loss: 1074.7353515625, Entropy 209.88206481933594, Learning Rate: 0.005\n",
      "Epoch [2953/20000], Loss: 1018.6338500976562, Entropy 201.7891387939453, Learning Rate: 0.005\n",
      "Epoch [2954/20000], Loss: 1095.651123046875, Entropy 207.18885803222656, Learning Rate: 0.005\n",
      "Epoch [2955/20000], Loss: 1062.5384521484375, Entropy 199.6936798095703, Learning Rate: 0.005\n",
      "Epoch [2956/20000], Loss: 1066.942138671875, Entropy 189.74623107910156, Learning Rate: 0.005\n",
      "Epoch [2957/20000], Loss: 1050.1082763671875, Entropy 206.6742706298828, Learning Rate: 0.005\n",
      "Epoch [2958/20000], Loss: 1033.5712890625, Entropy 186.27288818359375, Learning Rate: 0.005\n",
      "Epoch [2959/20000], Loss: 1028.8394775390625, Entropy 203.19058227539062, Learning Rate: 0.005\n",
      "Epoch [2960/20000], Loss: 1032.13427734375, Entropy 200.80218505859375, Learning Rate: 0.005\n",
      "Epoch [2961/20000], Loss: 1029.3818359375, Entropy 211.6503143310547, Learning Rate: 0.005\n",
      "Epoch [2962/20000], Loss: 1069.012451171875, Entropy 196.4824981689453, Learning Rate: 0.005\n",
      "Epoch [2963/20000], Loss: 1057.8193359375, Entropy 185.00608825683594, Learning Rate: 0.005\n",
      "Epoch [2964/20000], Loss: 1063.98486328125, Entropy 197.63499450683594, Learning Rate: 0.005\n",
      "Epoch [2965/20000], Loss: 998.44580078125, Entropy 204.26071166992188, Learning Rate: 0.005\n",
      "Epoch [2966/20000], Loss: 1058.5946044921875, Entropy 202.33448791503906, Learning Rate: 0.005\n",
      "Epoch [2967/20000], Loss: 1071.08642578125, Entropy 185.09335327148438, Learning Rate: 0.005\n",
      "Epoch [2968/20000], Loss: 991.7006225585938, Entropy 225.3300323486328, Learning Rate: 0.005\n",
      "Epoch [2969/20000], Loss: 1009.66162109375, Entropy 206.58460998535156, Learning Rate: 0.005\n",
      "Epoch [2970/20000], Loss: 1059.7568359375, Entropy 199.45758056640625, Learning Rate: 0.005\n",
      "Epoch [2971/20000], Loss: 1052.0941162109375, Entropy 200.88296508789062, Learning Rate: 0.005\n",
      "Epoch [2972/20000], Loss: 1087.676025390625, Entropy 205.25009155273438, Learning Rate: 0.005\n",
      "Epoch [2973/20000], Loss: 1060.817138671875, Entropy 173.64710998535156, Learning Rate: 0.005\n",
      "Epoch [2974/20000], Loss: 1057.5626220703125, Entropy 191.0706787109375, Learning Rate: 0.005\n",
      "Epoch [2975/20000], Loss: 1020.5883178710938, Entropy 206.00823974609375, Learning Rate: 0.005\n",
      "Epoch [2976/20000], Loss: 1102.0594482421875, Entropy 197.52981567382812, Learning Rate: 0.005\n",
      "Epoch [2977/20000], Loss: 1032.5394287109375, Entropy 189.05078125, Learning Rate: 0.005\n",
      "Epoch [2978/20000], Loss: 1065.8095703125, Entropy 188.9606475830078, Learning Rate: 0.005\n",
      "Epoch [2979/20000], Loss: 1053.662841796875, Entropy 209.56155395507812, Learning Rate: 0.005\n",
      "Epoch [2980/20000], Loss: 1042.28662109375, Entropy 199.7662811279297, Learning Rate: 0.005\n",
      "Epoch [2981/20000], Loss: 1061.272705078125, Entropy 201.10980224609375, Learning Rate: 0.005\n",
      "Epoch [2982/20000], Loss: 1047.5552978515625, Entropy 203.18405151367188, Learning Rate: 0.005\n",
      "Epoch [2983/20000], Loss: 1018.2774047851562, Entropy 187.6831817626953, Learning Rate: 0.005\n",
      "Epoch [2984/20000], Loss: 1007.03759765625, Entropy 207.52699279785156, Learning Rate: 0.005\n",
      "Epoch [2985/20000], Loss: 1019.484130859375, Entropy 224.57472229003906, Learning Rate: 0.005\n",
      "Epoch [2986/20000], Loss: 1054.5439453125, Entropy 203.1209716796875, Learning Rate: 0.005\n",
      "Epoch [2987/20000], Loss: 1004.8566284179688, Entropy 201.1788787841797, Learning Rate: 0.005\n",
      "Epoch [2988/20000], Loss: 1026.0802001953125, Entropy 199.87014770507812, Learning Rate: 0.005\n",
      "Epoch [2989/20000], Loss: 1064.5439453125, Entropy 198.86314392089844, Learning Rate: 0.005\n",
      "Epoch [2990/20000], Loss: 1050.6221923828125, Entropy 196.96742248535156, Learning Rate: 0.005\n",
      "Epoch [2991/20000], Loss: 1017.28564453125, Entropy 199.48167419433594, Learning Rate: 0.005\n",
      "Epoch [2992/20000], Loss: 1090.712158203125, Entropy 198.33013916015625, Learning Rate: 0.005\n",
      "Epoch [2993/20000], Loss: 1046.825927734375, Entropy 215.40687561035156, Learning Rate: 0.005\n",
      "Epoch [2994/20000], Loss: 1007.90234375, Entropy 191.32644653320312, Learning Rate: 0.005\n",
      "Epoch [2995/20000], Loss: 1039.7529296875, Entropy 199.92623901367188, Learning Rate: 0.005\n",
      "Epoch [2996/20000], Loss: 1088.2802734375, Entropy 178.07980346679688, Learning Rate: 0.005\n",
      "Epoch [2997/20000], Loss: 1052.55908203125, Entropy 197.9767608642578, Learning Rate: 0.005\n",
      "Epoch [2998/20000], Loss: 998.5343017578125, Entropy 199.275390625, Learning Rate: 0.005\n",
      "Epoch [2999/20000], Loss: 1035.703369140625, Entropy 197.43702697753906, Learning Rate: 0.005\n",
      "Epoch [3000/20000], Loss: 1051.7862548828125, Entropy 195.7936248779297, Learning Rate: 0.005\n",
      "Epoch [3001/20000], Loss: 1013.603759765625, Entropy 201.93397521972656, Learning Rate: 0.005\n",
      "Epoch [3002/20000], Loss: 1075.1085205078125, Entropy 199.0772247314453, Learning Rate: 0.005\n",
      "Epoch [3003/20000], Loss: 1002.7898559570312, Entropy 207.01422119140625, Learning Rate: 0.005\n",
      "Epoch [3004/20000], Loss: 1015.4755859375, Entropy 197.94485473632812, Learning Rate: 0.005\n",
      "Epoch [3005/20000], Loss: 991.624267578125, Entropy 218.56163024902344, Learning Rate: 0.005\n",
      "Epoch [3006/20000], Loss: 1046.7872314453125, Entropy 195.50796508789062, Learning Rate: 0.005\n",
      "Epoch [3007/20000], Loss: 996.5826416015625, Entropy 211.3623046875, Learning Rate: 0.005\n",
      "Epoch [3008/20000], Loss: 997.6011352539062, Entropy 218.80426025390625, Learning Rate: 0.005\n",
      "Epoch [3009/20000], Loss: 1011.550048828125, Entropy 207.66259765625, Learning Rate: 0.005\n",
      "Epoch [3010/20000], Loss: 1005.2797241210938, Entropy 200.79730224609375, Learning Rate: 0.005\n",
      "Epoch [3011/20000], Loss: 1084.4400634765625, Entropy 191.23374938964844, Learning Rate: 0.005\n",
      "Epoch [3012/20000], Loss: 1077.133056640625, Entropy 196.8602752685547, Learning Rate: 0.005\n",
      "Epoch [3013/20000], Loss: 1058.2952880859375, Entropy 193.64501953125, Learning Rate: 0.005\n",
      "Epoch [3014/20000], Loss: 1041.1646728515625, Entropy 212.40390014648438, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3015/20000], Loss: 1039.030029296875, Entropy 207.6068878173828, Learning Rate: 0.005\n",
      "Epoch [3016/20000], Loss: 1066.1680908203125, Entropy 184.1457977294922, Learning Rate: 0.005\n",
      "Epoch [3017/20000], Loss: 1027.245361328125, Entropy 196.61119079589844, Learning Rate: 0.005\n",
      "Epoch [3018/20000], Loss: 1027.4129638671875, Entropy 213.6312713623047, Learning Rate: 0.005\n",
      "Epoch [3019/20000], Loss: 1020.9251098632812, Entropy 220.4839630126953, Learning Rate: 0.005\n",
      "Epoch [3020/20000], Loss: 1045.633544921875, Entropy 189.486328125, Learning Rate: 0.005\n",
      "Epoch [3021/20000], Loss: 1021.7830810546875, Entropy 199.11288452148438, Learning Rate: 0.005\n",
      "Epoch [3022/20000], Loss: 1004.9925537109375, Entropy 202.294189453125, Learning Rate: 0.005\n",
      "Epoch [3023/20000], Loss: 1049.1114501953125, Entropy 190.94798278808594, Learning Rate: 0.005\n",
      "Epoch [3024/20000], Loss: 1022.750244140625, Entropy 208.83753967285156, Learning Rate: 0.005\n",
      "Epoch [3025/20000], Loss: 1049.6363525390625, Entropy 196.603271484375, Learning Rate: 0.005\n",
      "Epoch [3026/20000], Loss: 1025.9300537109375, Entropy 206.3041229248047, Learning Rate: 0.005\n",
      "Epoch [3027/20000], Loss: 1060.586181640625, Entropy 188.1786346435547, Learning Rate: 0.005\n",
      "Epoch [3028/20000], Loss: 1016.3757934570312, Entropy 210.35577392578125, Learning Rate: 0.005\n",
      "Epoch [3029/20000], Loss: 1059.0819091796875, Entropy 228.13186645507812, Learning Rate: 0.005\n",
      "Epoch [3030/20000], Loss: 1016.658935546875, Entropy 201.585205078125, Learning Rate: 0.005\n",
      "Epoch [3031/20000], Loss: 1046.94775390625, Entropy 190.4541015625, Learning Rate: 0.005\n",
      "Epoch [3032/20000], Loss: 1033.0718994140625, Entropy 198.2274627685547, Learning Rate: 0.005\n",
      "Epoch [3033/20000], Loss: 1033.9427490234375, Entropy 210.804931640625, Learning Rate: 0.005\n",
      "Epoch [3034/20000], Loss: 1080.0743408203125, Entropy 225.19882202148438, Learning Rate: 0.005\n",
      "Epoch [3035/20000], Loss: 996.3280639648438, Entropy 221.7548370361328, Learning Rate: 0.005\n",
      "Epoch [3036/20000], Loss: 1019.6929931640625, Entropy 210.85220336914062, Learning Rate: 0.005\n",
      "Epoch [3037/20000], Loss: 1029.3798828125, Entropy 204.16305541992188, Learning Rate: 0.005\n",
      "Epoch [3038/20000], Loss: 1015.2622680664062, Entropy 212.3091583251953, Learning Rate: 0.005\n",
      "Epoch [3039/20000], Loss: 1094.8863525390625, Entropy 200.56065368652344, Learning Rate: 0.005\n",
      "Epoch [3040/20000], Loss: 1036.8883056640625, Entropy 196.136962890625, Learning Rate: 0.005\n",
      "Epoch [3041/20000], Loss: 1045.303955078125, Entropy 186.23231506347656, Learning Rate: 0.005\n",
      "Epoch [3042/20000], Loss: 1037.372802734375, Entropy 193.7151336669922, Learning Rate: 0.005\n",
      "Epoch [3043/20000], Loss: 999.9391479492188, Entropy 200.48394775390625, Learning Rate: 0.005\n",
      "Epoch [3044/20000], Loss: 956.69580078125, Entropy 202.4686279296875, Learning Rate: 0.005\n",
      "Epoch [3045/20000], Loss: 998.5690307617188, Entropy 203.87091064453125, Learning Rate: 0.005\n",
      "Epoch [3046/20000], Loss: 999.868408203125, Entropy 208.42721557617188, Learning Rate: 0.005\n",
      "Epoch [3047/20000], Loss: 1030.837646484375, Entropy 215.72100830078125, Learning Rate: 0.005\n",
      "Epoch [3048/20000], Loss: 996.0242309570312, Entropy 208.4718475341797, Learning Rate: 0.005\n",
      "Epoch [3049/20000], Loss: 1031.3670654296875, Entropy 190.068359375, Learning Rate: 0.005\n",
      "Epoch [3050/20000], Loss: 1088.6865234375, Entropy 208.9392852783203, Learning Rate: 0.005\n",
      "Epoch [3051/20000], Loss: 1047.6737060546875, Entropy 211.9131622314453, Learning Rate: 0.005\n",
      "Epoch [3052/20000], Loss: 1023.02490234375, Entropy 203.67320251464844, Learning Rate: 0.005\n",
      "Epoch [3053/20000], Loss: 1025.234619140625, Entropy 200.216796875, Learning Rate: 0.005\n",
      "Epoch [3054/20000], Loss: 1021.7116088867188, Entropy 200.6103057861328, Learning Rate: 0.005\n",
      "Epoch [3055/20000], Loss: 975.2613525390625, Entropy 219.13546752929688, Learning Rate: 0.005\n",
      "Epoch [3056/20000], Loss: 1033.5487060546875, Entropy 212.66311645507812, Learning Rate: 0.005\n",
      "Epoch [3057/20000], Loss: 996.1727294921875, Entropy 214.920654296875, Learning Rate: 0.005\n",
      "Epoch [3058/20000], Loss: 1019.6591796875, Entropy 223.77220153808594, Learning Rate: 0.005\n",
      "Epoch [3059/20000], Loss: 1066.1600341796875, Entropy 216.1173553466797, Learning Rate: 0.005\n",
      "Epoch [3060/20000], Loss: 1031.40478515625, Entropy 221.99481201171875, Learning Rate: 0.005\n",
      "Epoch [3061/20000], Loss: 1017.37841796875, Entropy 212.9652099609375, Learning Rate: 0.005\n",
      "Epoch [3062/20000], Loss: 1026.942138671875, Entropy 221.260986328125, Learning Rate: 0.005\n",
      "Epoch [3063/20000], Loss: 1075.6888427734375, Entropy 205.58645629882812, Learning Rate: 0.005\n",
      "Epoch [3064/20000], Loss: 1027.14697265625, Entropy 189.58009338378906, Learning Rate: 0.005\n",
      "Epoch [3065/20000], Loss: 1065.0963134765625, Entropy 211.6299591064453, Learning Rate: 0.005\n",
      "Epoch [3066/20000], Loss: 1060.07861328125, Entropy 205.67884826660156, Learning Rate: 0.005\n",
      "Epoch [3067/20000], Loss: 983.6658325195312, Entropy 220.8494415283203, Learning Rate: 0.005\n",
      "Epoch [3068/20000], Loss: 1042.188232421875, Entropy 213.8839569091797, Learning Rate: 0.005\n",
      "Epoch [3069/20000], Loss: 1056.9239501953125, Entropy 196.56065368652344, Learning Rate: 0.005\n",
      "Epoch [3070/20000], Loss: 1009.905517578125, Entropy 211.66281127929688, Learning Rate: 0.005\n",
      "Epoch [3071/20000], Loss: 1032.393798828125, Entropy 199.92279052734375, Learning Rate: 0.005\n",
      "Epoch [3072/20000], Loss: 1049.3712158203125, Entropy 205.8619842529297, Learning Rate: 0.005\n",
      "Epoch [3073/20000], Loss: 1014.802490234375, Entropy 210.0379638671875, Learning Rate: 0.005\n",
      "Epoch [3074/20000], Loss: 1008.6766967773438, Entropy 219.6592559814453, Learning Rate: 0.005\n",
      "Epoch [3075/20000], Loss: 1002.1566162109375, Entropy 197.9326171875, Learning Rate: 0.005\n",
      "Epoch [3076/20000], Loss: 1021.056884765625, Entropy 198.88888549804688, Learning Rate: 0.005\n",
      "Epoch [3077/20000], Loss: 1058.35546875, Entropy 204.26443481445312, Learning Rate: 0.005\n",
      "Epoch [3078/20000], Loss: 1056.521240234375, Entropy 212.69268798828125, Learning Rate: 0.005\n",
      "Epoch [3079/20000], Loss: 978.2843017578125, Entropy 215.36558532714844, Learning Rate: 0.005\n",
      "Epoch [3080/20000], Loss: 1022.478515625, Entropy 206.27442932128906, Learning Rate: 0.005\n",
      "Epoch [3081/20000], Loss: 1048.5390625, Entropy 208.46104431152344, Learning Rate: 0.005\n",
      "Epoch [3082/20000], Loss: 1019.5994262695312, Entropy 204.8661346435547, Learning Rate: 0.005\n",
      "Epoch [3083/20000], Loss: 1067.8646240234375, Entropy 201.2806396484375, Learning Rate: 0.005\n",
      "Epoch [3084/20000], Loss: 1028.696044921875, Entropy 189.68331909179688, Learning Rate: 0.005\n",
      "Epoch [3085/20000], Loss: 1029.6368408203125, Entropy 204.79025268554688, Learning Rate: 0.005\n",
      "Epoch [3086/20000], Loss: 1061.0714111328125, Entropy 205.5904998779297, Learning Rate: 0.005\n",
      "Epoch [3087/20000], Loss: 1020.5943603515625, Entropy 217.57749938964844, Learning Rate: 0.005\n",
      "Epoch [3088/20000], Loss: 1000.9956665039062, Entropy 211.3582000732422, Learning Rate: 0.005\n",
      "Epoch [3089/20000], Loss: 1061.0809326171875, Entropy 207.2761688232422, Learning Rate: 0.005\n",
      "Epoch [3090/20000], Loss: 997.3782958984375, Entropy 202.54440307617188, Learning Rate: 0.005\n",
      "Epoch [3091/20000], Loss: 1023.13134765625, Entropy 204.39236450195312, Learning Rate: 0.005\n",
      "Epoch [3092/20000], Loss: 976.3651123046875, Entropy 223.55589294433594, Learning Rate: 0.005\n",
      "Epoch [3093/20000], Loss: 1032.1917724609375, Entropy 211.5350341796875, Learning Rate: 0.005\n",
      "Epoch [3094/20000], Loss: 1014.5668334960938, Entropy 211.5868682861328, Learning Rate: 0.005\n",
      "Epoch [3095/20000], Loss: 1006.39794921875, Entropy 211.14553833007812, Learning Rate: 0.005\n",
      "Epoch [3096/20000], Loss: 1031.471435546875, Entropy 215.4451141357422, Learning Rate: 0.005\n",
      "Epoch [3097/20000], Loss: 1017.6098022460938, Entropy 205.32562255859375, Learning Rate: 0.005\n",
      "Epoch [3098/20000], Loss: 1003.4657592773438, Entropy 216.6902313232422, Learning Rate: 0.005\n",
      "Epoch [3099/20000], Loss: 1002.3201293945312, Entropy 210.9324493408203, Learning Rate: 0.005\n",
      "Epoch [3100/20000], Loss: 1021.2085571289062, Entropy 214.9119110107422, Learning Rate: 0.005\n",
      "Epoch [3101/20000], Loss: 970.31103515625, Entropy 221.21812438964844, Learning Rate: 0.005\n",
      "Epoch [3102/20000], Loss: 1055.26513671875, Entropy 190.49658203125, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3103/20000], Loss: 1095.34619140625, Entropy 206.8393096923828, Learning Rate: 0.005\n",
      "Epoch [3104/20000], Loss: 1014.838134765625, Entropy 213.10525512695312, Learning Rate: 0.005\n",
      "Epoch [3105/20000], Loss: 1036.454345703125, Entropy 212.70538330078125, Learning Rate: 0.005\n",
      "Epoch [3106/20000], Loss: 1005.80078125, Entropy 223.01829528808594, Learning Rate: 0.005\n",
      "Epoch [3107/20000], Loss: 995.7529907226562, Entropy 201.2176971435547, Learning Rate: 0.005\n",
      "Epoch [3108/20000], Loss: 976.3582763671875, Entropy 210.5244140625, Learning Rate: 0.005\n",
      "Epoch [3109/20000], Loss: 1032.80810546875, Entropy 203.52415466308594, Learning Rate: 0.005\n",
      "Epoch [3110/20000], Loss: 1013.515625, Entropy 220.36512756347656, Learning Rate: 0.005\n",
      "Epoch [3111/20000], Loss: 1072.4788818359375, Entropy 205.961669921875, Learning Rate: 0.005\n",
      "Epoch [3112/20000], Loss: 1009.231689453125, Entropy 212.03396606445312, Learning Rate: 0.005\n",
      "Epoch [3113/20000], Loss: 1028.1397705078125, Entropy 197.2384796142578, Learning Rate: 0.005\n",
      "Epoch [3114/20000], Loss: 1021.6881713867188, Entropy 213.3427276611328, Learning Rate: 0.005\n",
      "Epoch [3115/20000], Loss: 1007.2180786132812, Entropy 217.8746795654297, Learning Rate: 0.005\n",
      "Epoch [3116/20000], Loss: 1019.2374267578125, Entropy 192.26168823242188, Learning Rate: 0.005\n",
      "Epoch [3117/20000], Loss: 1040.303955078125, Entropy 207.7336883544922, Learning Rate: 0.005\n",
      "Epoch [3118/20000], Loss: 996.6146240234375, Entropy 219.41030883789062, Learning Rate: 0.005\n",
      "Epoch [3119/20000], Loss: 979.6154174804688, Entropy 213.84918212890625, Learning Rate: 0.005\n",
      "Epoch [3120/20000], Loss: 999.64111328125, Entropy 210.78968811035156, Learning Rate: 0.005\n",
      "Epoch [3121/20000], Loss: 1029.9818115234375, Entropy 226.43124389648438, Learning Rate: 0.005\n",
      "Epoch [3122/20000], Loss: 1050.7315673828125, Entropy 195.2923126220703, Learning Rate: 0.005\n",
      "Epoch [3123/20000], Loss: 978.0338745117188, Entropy 204.3845977783203, Learning Rate: 0.005\n",
      "Epoch [3124/20000], Loss: 994.435546875, Entropy 222.020263671875, Learning Rate: 0.005\n",
      "Epoch [3125/20000], Loss: 1008.63232421875, Entropy 209.86622619628906, Learning Rate: 0.005\n",
      "Epoch [3126/20000], Loss: 1019.6578979492188, Entropy 202.9202423095703, Learning Rate: 0.005\n",
      "Epoch [3127/20000], Loss: 995.5806274414062, Entropy 217.49969482421875, Learning Rate: 0.005\n",
      "Epoch [3128/20000], Loss: 972.21044921875, Entropy 214.44239807128906, Learning Rate: 0.005\n",
      "Epoch [3129/20000], Loss: 1002.211669921875, Entropy 217.54808044433594, Learning Rate: 0.005\n",
      "Epoch [3130/20000], Loss: 1025.410400390625, Entropy 197.6021270751953, Learning Rate: 0.005\n",
      "Epoch [3131/20000], Loss: 1004.9253540039062, Entropy 210.93988037109375, Learning Rate: 0.005\n",
      "Epoch [3132/20000], Loss: 1024.0155029296875, Entropy 216.5460662841797, Learning Rate: 0.005\n",
      "Epoch [3133/20000], Loss: 1034.0897216796875, Entropy 193.1503448486328, Learning Rate: 0.005\n",
      "Epoch [3134/20000], Loss: 1041.1123046875, Entropy 225.29310607910156, Learning Rate: 0.005\n",
      "Epoch [3135/20000], Loss: 1037.5186767578125, Entropy 198.73300170898438, Learning Rate: 0.005\n",
      "Epoch [3136/20000], Loss: 1000.73681640625, Entropy 233.28550720214844, Learning Rate: 0.005\n",
      "Epoch [3137/20000], Loss: 998.0384521484375, Entropy 233.78076171875, Learning Rate: 0.005\n",
      "Epoch [3138/20000], Loss: 1014.552490234375, Entropy 209.86184692382812, Learning Rate: 0.005\n",
      "Epoch [3139/20000], Loss: 1021.0794677734375, Entropy 206.17117309570312, Learning Rate: 0.005\n",
      "Epoch [3140/20000], Loss: 1018.4403686523438, Entropy 213.6902618408203, Learning Rate: 0.005\n",
      "Epoch [3141/20000], Loss: 997.3260498046875, Entropy 220.04638671875, Learning Rate: 0.005\n",
      "Epoch [3142/20000], Loss: 987.995361328125, Entropy 227.17703247070312, Learning Rate: 0.005\n",
      "Epoch [3143/20000], Loss: 990.6724853515625, Entropy 218.23147583007812, Learning Rate: 0.005\n",
      "Epoch [3144/20000], Loss: 1061.0294189453125, Entropy 210.70851135253906, Learning Rate: 0.005\n",
      "Epoch [3145/20000], Loss: 1027.8251953125, Entropy 231.4706573486328, Learning Rate: 0.005\n",
      "Epoch [3146/20000], Loss: 1018.314208984375, Entropy 228.435302734375, Learning Rate: 0.005\n",
      "Epoch [3147/20000], Loss: 1001.314208984375, Entropy 231.16477966308594, Learning Rate: 0.005\n",
      "Epoch [3148/20000], Loss: 998.6734619140625, Entropy 226.60276794433594, Learning Rate: 0.005\n",
      "Epoch [3149/20000], Loss: 981.6508178710938, Entropy 208.8987274169922, Learning Rate: 0.005\n",
      "Epoch [3150/20000], Loss: 1012.4929809570312, Entropy 220.39447021484375, Learning Rate: 0.005\n",
      "Epoch [3151/20000], Loss: 1003.0712280273438, Entropy 235.1306610107422, Learning Rate: 0.005\n",
      "Epoch [3152/20000], Loss: 989.2242431640625, Entropy 229.33914184570312, Learning Rate: 0.005\n",
      "Epoch [3153/20000], Loss: 1033.8990478515625, Entropy 215.00010681152344, Learning Rate: 0.005\n",
      "Epoch [3154/20000], Loss: 999.158203125, Entropy 215.83505249023438, Learning Rate: 0.005\n",
      "Epoch [3155/20000], Loss: 972.2150268554688, Entropy 235.51544189453125, Learning Rate: 0.005\n",
      "Epoch [3156/20000], Loss: 1017.0120849609375, Entropy 212.54373168945312, Learning Rate: 0.005\n",
      "Epoch [3157/20000], Loss: 968.2725219726562, Entropy 219.65264892578125, Learning Rate: 0.005\n",
      "Epoch [3158/20000], Loss: 999.6918334960938, Entropy 204.93255615234375, Learning Rate: 0.005\n",
      "Epoch [3159/20000], Loss: 988.7874145507812, Entropy 215.91375732421875, Learning Rate: 0.005\n",
      "Epoch [3160/20000], Loss: 983.918212890625, Entropy 229.60536193847656, Learning Rate: 0.005\n",
      "Epoch [3161/20000], Loss: 1001.3601684570312, Entropy 233.1620330810547, Learning Rate: 0.005\n",
      "Epoch [3162/20000], Loss: 1029.6148681640625, Entropy 210.1688690185547, Learning Rate: 0.005\n",
      "Epoch [3163/20000], Loss: 1006.5662841796875, Entropy 218.66030883789062, Learning Rate: 0.005\n",
      "Epoch [3164/20000], Loss: 1016.4615478515625, Entropy 217.2236328125, Learning Rate: 0.005\n",
      "Epoch [3165/20000], Loss: 1042.451904296875, Entropy 197.54737854003906, Learning Rate: 0.005\n",
      "Epoch [3166/20000], Loss: 964.1336669921875, Entropy 236.0750732421875, Learning Rate: 0.005\n",
      "Epoch [3167/20000], Loss: 982.84716796875, Entropy 228.75233459472656, Learning Rate: 0.005\n",
      "Epoch [3168/20000], Loss: 1032.0711669921875, Entropy 214.3873748779297, Learning Rate: 0.005\n",
      "Epoch [3169/20000], Loss: 991.5765380859375, Entropy 220.15933227539062, Learning Rate: 0.005\n",
      "Epoch [3170/20000], Loss: 1000.5590209960938, Entropy 218.72100830078125, Learning Rate: 0.005\n",
      "Epoch [3171/20000], Loss: 1073.443115234375, Entropy 227.3476104736328, Learning Rate: 0.005\n",
      "Epoch [3172/20000], Loss: 1001.6498413085938, Entropy 238.16546630859375, Learning Rate: 0.005\n",
      "Epoch [3173/20000], Loss: 1041.9453125, Entropy 228.171630859375, Learning Rate: 0.005\n",
      "Epoch [3174/20000], Loss: 1031.54296875, Entropy 226.69354248046875, Learning Rate: 0.005\n",
      "Epoch [3175/20000], Loss: 1035.744873046875, Entropy 221.7587127685547, Learning Rate: 0.005\n",
      "Epoch [3176/20000], Loss: 1037.0693359375, Entropy 220.06866455078125, Learning Rate: 0.005\n",
      "Epoch [3177/20000], Loss: 1029.313720703125, Entropy 213.1853790283203, Learning Rate: 0.005\n",
      "Epoch [3178/20000], Loss: 1043.181884765625, Entropy 227.88812255859375, Learning Rate: 0.005\n",
      "Epoch [3179/20000], Loss: 1017.0410766601562, Entropy 212.4656524658203, Learning Rate: 0.005\n",
      "Epoch [3180/20000], Loss: 1004.1416625976562, Entropy 214.8876495361328, Learning Rate: 0.005\n",
      "Epoch [3181/20000], Loss: 1041.740234375, Entropy 213.94358825683594, Learning Rate: 0.005\n",
      "Epoch [3182/20000], Loss: 1033.33642578125, Entropy 227.64190673828125, Learning Rate: 0.005\n",
      "Epoch [3183/20000], Loss: 1004.763427734375, Entropy 217.49929809570312, Learning Rate: 0.005\n",
      "Epoch [3184/20000], Loss: 1045.94677734375, Entropy 223.2904052734375, Learning Rate: 0.005\n",
      "Epoch [3185/20000], Loss: 987.791259765625, Entropy 230.05162048339844, Learning Rate: 0.005\n",
      "Epoch [3186/20000], Loss: 1067.151123046875, Entropy 212.42198181152344, Learning Rate: 0.005\n",
      "Epoch [3187/20000], Loss: 1026.7493896484375, Entropy 226.8221893310547, Learning Rate: 0.005\n",
      "Epoch [3188/20000], Loss: 992.9711303710938, Entropy 218.3231658935547, Learning Rate: 0.005\n",
      "Epoch [3189/20000], Loss: 1020.979248046875, Entropy 228.82437133789062, Learning Rate: 0.005\n",
      "Epoch [3190/20000], Loss: 990.90966796875, Entropy 213.04788208007812, Learning Rate: 0.005\n",
      "Epoch [3191/20000], Loss: 1034.0528564453125, Entropy 209.02503967285156, Learning Rate: 0.005\n",
      "Epoch [3192/20000], Loss: 989.6559448242188, Entropy 239.2818145751953, Learning Rate: 0.005\n",
      "Epoch [3193/20000], Loss: 1026.25244140625, Entropy 215.48333740234375, Learning Rate: 0.005\n",
      "Epoch [3194/20000], Loss: 995.23291015625, Entropy 211.68028259277344, Learning Rate: 0.005\n",
      "Epoch [3195/20000], Loss: 1024.8145751953125, Entropy 208.6874237060547, Learning Rate: 0.005\n",
      "Epoch [3196/20000], Loss: 1022.4349365234375, Entropy 227.794921875, Learning Rate: 0.005\n",
      "Epoch [3197/20000], Loss: 1004.2224731445312, Entropy 217.8853302001953, Learning Rate: 0.005\n",
      "Epoch [3198/20000], Loss: 1037.3184814453125, Entropy 208.81971740722656, Learning Rate: 0.005\n",
      "Epoch [3199/20000], Loss: 1000.3134155273438, Entropy 219.12298583984375, Learning Rate: 0.005\n",
      "Epoch [3200/20000], Loss: 1035.756103515625, Entropy 211.77496337890625, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3201/20000], Loss: 986.9273681640625, Entropy 242.41505432128906, Learning Rate: 0.005\n",
      "Epoch [3202/20000], Loss: 1032.205322265625, Entropy 231.01319885253906, Learning Rate: 0.005\n",
      "Epoch [3203/20000], Loss: 1009.0877685546875, Entropy 229.81187438964844, Learning Rate: 0.005\n",
      "Epoch [3204/20000], Loss: 1014.890625, Entropy 248.27040100097656, Learning Rate: 0.005\n",
      "Epoch [3205/20000], Loss: 977.27685546875, Entropy 228.02749633789062, Learning Rate: 0.005\n",
      "Epoch [3206/20000], Loss: 1031.8671875, Entropy 221.2891387939453, Learning Rate: 0.005\n",
      "Epoch [3207/20000], Loss: 1006.4441528320312, Entropy 226.1577911376953, Learning Rate: 0.005\n",
      "Epoch [3208/20000], Loss: 1014.041259765625, Entropy 215.62974548339844, Learning Rate: 0.005\n",
      "Epoch [3209/20000], Loss: 1019.199462890625, Entropy 217.35548400878906, Learning Rate: 0.005\n",
      "Epoch [3210/20000], Loss: 1004.6029052734375, Entropy 216.44960021972656, Learning Rate: 0.005\n",
      "Epoch [3211/20000], Loss: 1008.04931640625, Entropy 223.84646606445312, Learning Rate: 0.005\n",
      "Epoch [3212/20000], Loss: 1023.6094360351562, Entropy 241.7805633544922, Learning Rate: 0.005\n",
      "Epoch [3213/20000], Loss: 1021.7531127929688, Entropy 218.1066131591797, Learning Rate: 0.005\n",
      "Epoch [3214/20000], Loss: 1022.9749145507812, Entropy 225.8230743408203, Learning Rate: 0.005\n",
      "Epoch [3215/20000], Loss: 1054.0906982421875, Entropy 212.37112426757812, Learning Rate: 0.005\n",
      "Epoch [3216/20000], Loss: 1018.243896484375, Entropy 230.987060546875, Learning Rate: 0.005\n",
      "Epoch [3217/20000], Loss: 1045.15869140625, Entropy 215.04998779296875, Learning Rate: 0.005\n",
      "Epoch [3218/20000], Loss: 1039.125732421875, Entropy 218.5979766845703, Learning Rate: 0.005\n",
      "Epoch [3219/20000], Loss: 979.4776611328125, Entropy 230.8934326171875, Learning Rate: 0.005\n",
      "Epoch [3220/20000], Loss: 1010.6669921875, Entropy 219.35494995117188, Learning Rate: 0.005\n",
      "Epoch [3221/20000], Loss: 1018.82177734375, Entropy 229.7928466796875, Learning Rate: 0.005\n",
      "Epoch [3222/20000], Loss: 1024.4364013671875, Entropy 232.62355041503906, Learning Rate: 0.005\n",
      "Epoch [3223/20000], Loss: 988.20166015625, Entropy 237.77659606933594, Learning Rate: 0.005\n",
      "Epoch [3224/20000], Loss: 957.802734375, Entropy 248.86294555664062, Learning Rate: 0.005\n",
      "Epoch [3225/20000], Loss: 981.9536743164062, Entropy 233.20843505859375, Learning Rate: 0.005\n",
      "Epoch [3226/20000], Loss: 1029.092041015625, Entropy 212.59571838378906, Learning Rate: 0.005\n",
      "Epoch [3227/20000], Loss: 958.0970458984375, Entropy 235.65939331054688, Learning Rate: 0.005\n",
      "Epoch [3228/20000], Loss: 1015.1353759765625, Entropy 217.03817749023438, Learning Rate: 0.005\n",
      "Epoch [3229/20000], Loss: 1002.390869140625, Entropy 240.75387573242188, Learning Rate: 0.005\n",
      "Epoch [3230/20000], Loss: 1008.5641479492188, Entropy 231.05584716796875, Learning Rate: 0.005\n",
      "Epoch [3231/20000], Loss: 997.189453125, Entropy 236.34194946289062, Learning Rate: 0.005\n",
      "Epoch [3232/20000], Loss: 1044.1219482421875, Entropy 216.2712860107422, Learning Rate: 0.005\n",
      "Epoch [3233/20000], Loss: 1002.9263916015625, Entropy 214.32054138183594, Learning Rate: 0.005\n",
      "Epoch [3234/20000], Loss: 1012.1690063476562, Entropy 228.22442626953125, Learning Rate: 0.005\n",
      "Epoch [3235/20000], Loss: 977.7833862304688, Entropy 235.71331787109375, Learning Rate: 0.005\n",
      "Epoch [3236/20000], Loss: 1010.2355346679688, Entropy 217.91644287109375, Learning Rate: 0.005\n",
      "Epoch [3237/20000], Loss: 1010.4104614257812, Entropy 210.5359344482422, Learning Rate: 0.005\n",
      "Epoch [3238/20000], Loss: 1023.7548217773438, Entropy 224.00103759765625, Learning Rate: 0.005\n",
      "Epoch [3239/20000], Loss: 982.7322998046875, Entropy 234.76451110839844, Learning Rate: 0.005\n",
      "Epoch [3240/20000], Loss: 1073.9058837890625, Entropy 206.2572479248047, Learning Rate: 0.005\n",
      "Epoch [3241/20000], Loss: 1029.860107421875, Entropy 227.7207794189453, Learning Rate: 0.005\n",
      "Epoch [3242/20000], Loss: 1007.072021484375, Entropy 224.07749938964844, Learning Rate: 0.005\n",
      "Epoch [3243/20000], Loss: 1010.5555419921875, Entropy 227.33895874023438, Learning Rate: 0.005\n",
      "Epoch [3244/20000], Loss: 1057.81884765625, Entropy 227.330078125, Learning Rate: 0.005\n",
      "Epoch [3245/20000], Loss: 1034.6572265625, Entropy 200.9241485595703, Learning Rate: 0.005\n",
      "Epoch [3246/20000], Loss: 1055.1669921875, Entropy 221.59286499023438, Learning Rate: 0.005\n",
      "Epoch [3247/20000], Loss: 1027.037353515625, Entropy 232.45765686035156, Learning Rate: 0.005\n",
      "Epoch [3248/20000], Loss: 1032.1328125, Entropy 237.33233642578125, Learning Rate: 0.005\n",
      "Epoch [3249/20000], Loss: 978.303466796875, Entropy 234.50344848632812, Learning Rate: 0.005\n",
      "Epoch [3250/20000], Loss: 1044.0137939453125, Entropy 227.1703643798828, Learning Rate: 0.005\n",
      "Epoch [3251/20000], Loss: 1021.0936279296875, Entropy 225.73130798339844, Learning Rate: 0.005\n",
      "Epoch [3252/20000], Loss: 990.4012451171875, Entropy 224.03976440429688, Learning Rate: 0.005\n",
      "Epoch [3253/20000], Loss: 984.9694213867188, Entropy 234.6512908935547, Learning Rate: 0.005\n",
      "Epoch [3254/20000], Loss: 1060.7042236328125, Entropy 229.05972290039062, Learning Rate: 0.005\n",
      "Epoch [3255/20000], Loss: 1038.50146484375, Entropy 208.71636962890625, Learning Rate: 0.005\n",
      "Epoch [3256/20000], Loss: 1021.1575317382812, Entropy 213.9357147216797, Learning Rate: 0.005\n",
      "Epoch [3257/20000], Loss: 1012.0025024414062, Entropy 214.97369384765625, Learning Rate: 0.005\n",
      "Epoch [3258/20000], Loss: 1028.5614013671875, Entropy 228.12606811523438, Learning Rate: 0.005\n",
      "Epoch [3259/20000], Loss: 1005.9732666015625, Entropy 232.65792846679688, Learning Rate: 0.005\n",
      "Epoch [3260/20000], Loss: 1008.4705200195312, Entropy 232.08551025390625, Learning Rate: 0.005\n",
      "Epoch [3261/20000], Loss: 1023.225830078125, Entropy 225.77696228027344, Learning Rate: 0.005\n",
      "Epoch [3262/20000], Loss: 1091.024658203125, Entropy 238.22610473632812, Learning Rate: 0.005\n",
      "Epoch [3263/20000], Loss: 992.9021606445312, Entropy 252.6946258544922, Learning Rate: 0.005\n",
      "Epoch [3264/20000], Loss: 1045.078369140625, Entropy 222.96075439453125, Learning Rate: 0.005\n",
      "Epoch [3265/20000], Loss: 993.5086669921875, Entropy 222.79995727539062, Learning Rate: 0.005\n",
      "Epoch [3266/20000], Loss: 994.8713989257812, Entropy 227.1259002685547, Learning Rate: 0.005\n",
      "Epoch [3267/20000], Loss: 1025.078369140625, Entropy 214.08489990234375, Learning Rate: 0.005\n",
      "Epoch [3268/20000], Loss: 1034.3321533203125, Entropy 226.4286346435547, Learning Rate: 0.005\n",
      "Epoch [3269/20000], Loss: 1054.0985107421875, Entropy 216.623046875, Learning Rate: 0.005\n",
      "Epoch [3270/20000], Loss: 980.0311279296875, Entropy 225.45242309570312, Learning Rate: 0.005\n",
      "Epoch [3271/20000], Loss: 971.33837890625, Entropy 241.77952575683594, Learning Rate: 0.005\n",
      "Epoch [3272/20000], Loss: 1010.9671630859375, Entropy 233.24549865722656, Learning Rate: 0.005\n",
      "Epoch [3273/20000], Loss: 1042.91748046875, Entropy 232.36659240722656, Learning Rate: 0.005\n",
      "Epoch [3274/20000], Loss: 1011.9866333007812, Entropy 210.09222412109375, Learning Rate: 0.005\n",
      "Epoch [3275/20000], Loss: 1033.366455078125, Entropy 215.3192901611328, Learning Rate: 0.005\n",
      "Epoch [3276/20000], Loss: 1063.126708984375, Entropy 215.75933837890625, Learning Rate: 0.005\n",
      "Epoch [3277/20000], Loss: 1028.436279296875, Entropy 243.09011840820312, Learning Rate: 0.005\n",
      "Epoch [3278/20000], Loss: 1009.7064819335938, Entropy 234.3563690185547, Learning Rate: 0.005\n",
      "Epoch [3279/20000], Loss: 1062.98388671875, Entropy 215.3619842529297, Learning Rate: 0.005\n",
      "Epoch [3280/20000], Loss: 1060.400390625, Entropy 220.346435546875, Learning Rate: 0.005\n",
      "Epoch [3281/20000], Loss: 1009.3026123046875, Entropy 223.9560546875, Learning Rate: 0.005\n",
      "Epoch [3282/20000], Loss: 949.4590454101562, Entropy 248.7537078857422, Learning Rate: 0.005\n",
      "Epoch [3283/20000], Loss: 977.117431640625, Entropy 233.93075561523438, Learning Rate: 0.005\n",
      "Epoch [3284/20000], Loss: 1038.673828125, Entropy 215.0059356689453, Learning Rate: 0.005\n",
      "Epoch [3285/20000], Loss: 997.1280517578125, Entropy 239.14260864257812, Learning Rate: 0.005\n",
      "Epoch [3286/20000], Loss: 1021.6176147460938, Entropy 247.23614501953125, Learning Rate: 0.005\n",
      "Epoch [3287/20000], Loss: 1021.7841796875, Entropy 225.18502807617188, Learning Rate: 0.005\n",
      "Epoch [3288/20000], Loss: 968.2728881835938, Entropy 231.0240936279297, Learning Rate: 0.005\n",
      "Epoch [3289/20000], Loss: 1033.6756591796875, Entropy 239.460693359375, Learning Rate: 0.005\n",
      "Epoch [3290/20000], Loss: 1012.640380859375, Entropy 230.33189392089844, Learning Rate: 0.005\n",
      "Epoch [3291/20000], Loss: 1025.00341796875, Entropy 246.0355224609375, Learning Rate: 0.005\n",
      "Epoch [3292/20000], Loss: 1020.45556640625, Entropy 228.38450622558594, Learning Rate: 0.005\n",
      "Epoch [3293/20000], Loss: 1008.1113891601562, Entropy 233.8609161376953, Learning Rate: 0.005\n",
      "Epoch [3294/20000], Loss: 1001.4651489257812, Entropy 228.3162384033203, Learning Rate: 0.005\n",
      "Epoch [3295/20000], Loss: 1062.867919921875, Entropy 221.80133056640625, Learning Rate: 0.005\n",
      "Epoch [3296/20000], Loss: 1042.4061279296875, Entropy 231.46128845214844, Learning Rate: 0.005\n",
      "Epoch [3297/20000], Loss: 1004.1731567382812, Entropy 229.0691680908203, Learning Rate: 0.005\n",
      "Epoch [3298/20000], Loss: 978.4016723632812, Entropy 235.0193634033203, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3299/20000], Loss: 968.6488037109375, Entropy 234.2215576171875, Learning Rate: 0.005\n",
      "Epoch [3300/20000], Loss: 1036.5225830078125, Entropy 247.45083618164062, Learning Rate: 0.005\n",
      "Epoch [3301/20000], Loss: 971.9586181640625, Entropy 245.28944396972656, Learning Rate: 0.005\n",
      "Epoch [3302/20000], Loss: 996.2127685546875, Entropy 221.47000122070312, Learning Rate: 0.005\n",
      "Epoch [3303/20000], Loss: 1034.55908203125, Entropy 232.10687255859375, Learning Rate: 0.005\n",
      "Epoch [3304/20000], Loss: 995.0928955078125, Entropy 240.67857360839844, Learning Rate: 0.005\n",
      "Epoch [3305/20000], Loss: 963.1109619140625, Entropy 237.29690551757812, Learning Rate: 0.005\n",
      "Epoch [3306/20000], Loss: 1005.48193359375, Entropy 235.1380615234375, Learning Rate: 0.005\n",
      "Epoch [3307/20000], Loss: 1028.1907958984375, Entropy 247.49777221679688, Learning Rate: 0.005\n",
      "Epoch [3308/20000], Loss: 989.38720703125, Entropy 239.75588989257812, Learning Rate: 0.005\n",
      "Epoch [3309/20000], Loss: 996.7625122070312, Entropy 230.34173583984375, Learning Rate: 0.005\n",
      "Epoch [3310/20000], Loss: 1002.1951904296875, Entropy 234.2215576171875, Learning Rate: 0.005\n",
      "Epoch [3311/20000], Loss: 1065.3929443359375, Entropy 241.73399353027344, Learning Rate: 0.005\n",
      "Epoch [3312/20000], Loss: 1060.40234375, Entropy 232.0740966796875, Learning Rate: 0.005\n",
      "Epoch [3313/20000], Loss: 1023.8662719726562, Entropy 225.84979248046875, Learning Rate: 0.005\n",
      "Epoch [3314/20000], Loss: 970.65576171875, Entropy 247.42996215820312, Learning Rate: 0.005\n",
      "Epoch [3315/20000], Loss: 959.3751831054688, Entropy 250.4081268310547, Learning Rate: 0.005\n",
      "Epoch [3316/20000], Loss: 1045.556640625, Entropy 239.34315490722656, Learning Rate: 0.005\n",
      "Epoch [3317/20000], Loss: 999.9351196289062, Entropy 240.73858642578125, Learning Rate: 0.005\n",
      "Epoch [3318/20000], Loss: 1003.535400390625, Entropy 240.01124572753906, Learning Rate: 0.005\n",
      "Epoch [3319/20000], Loss: 1011.8662109375, Entropy 226.9482421875, Learning Rate: 0.005\n",
      "Epoch [3320/20000], Loss: 1006.8214111328125, Entropy 247.67440795898438, Learning Rate: 0.005\n",
      "Epoch [3321/20000], Loss: 1086.8453369140625, Entropy 218.7813262939453, Learning Rate: 0.005\n",
      "Epoch [3322/20000], Loss: 1002.6658935546875, Entropy 236.10145568847656, Learning Rate: 0.005\n",
      "Epoch [3323/20000], Loss: 1043.60791015625, Entropy 248.27615356445312, Learning Rate: 0.005\n",
      "Epoch [3324/20000], Loss: 986.1748046875, Entropy 228.0091552734375, Learning Rate: 0.005\n",
      "Epoch [3325/20000], Loss: 966.6973876953125, Entropy 253.195068359375, Learning Rate: 0.005\n",
      "Epoch [3326/20000], Loss: 1002.0798950195312, Entropy 242.5791778564453, Learning Rate: 0.005\n",
      "Epoch [3327/20000], Loss: 963.3881225585938, Entropy 245.8329620361328, Learning Rate: 0.005\n",
      "Epoch [3328/20000], Loss: 1015.489990234375, Entropy 228.908203125, Learning Rate: 0.005\n",
      "Epoch [3329/20000], Loss: 1007.3387451171875, Entropy 241.43771362304688, Learning Rate: 0.005\n",
      "Epoch [3330/20000], Loss: 981.9470825195312, Entropy 233.9781951904297, Learning Rate: 0.005\n",
      "Epoch [3331/20000], Loss: 986.204345703125, Entropy 239.62355041503906, Learning Rate: 0.005\n",
      "Epoch [3332/20000], Loss: 1005.7034301757812, Entropy 235.10296630859375, Learning Rate: 0.005\n",
      "Epoch [3333/20000], Loss: 1023.1460571289062, Entropy 231.2372589111328, Learning Rate: 0.005\n",
      "Epoch [3334/20000], Loss: 999.8961791992188, Entropy 230.1931610107422, Learning Rate: 0.005\n",
      "Epoch [3335/20000], Loss: 981.7028198242188, Entropy 236.4214324951172, Learning Rate: 0.005\n",
      "Epoch [3336/20000], Loss: 1007.0625610351562, Entropy 231.5369110107422, Learning Rate: 0.005\n",
      "Epoch [3337/20000], Loss: 1030.6417236328125, Entropy 229.9879608154297, Learning Rate: 0.005\n",
      "Epoch [3338/20000], Loss: 1057.763427734375, Entropy 249.23460388183594, Learning Rate: 0.005\n",
      "Epoch [3339/20000], Loss: 1016.926025390625, Entropy 225.59152221679688, Learning Rate: 0.005\n",
      "Epoch [3340/20000], Loss: 997.9970703125, Entropy 249.47994995117188, Learning Rate: 0.005\n",
      "Epoch [3341/20000], Loss: 1048.6708984375, Entropy 224.51800537109375, Learning Rate: 0.005\n",
      "Epoch [3342/20000], Loss: 1053.688232421875, Entropy 225.8577423095703, Learning Rate: 0.005\n",
      "Epoch [3343/20000], Loss: 1001.9193115234375, Entropy 257.6814270019531, Learning Rate: 0.005\n",
      "Epoch [3344/20000], Loss: 1004.6661987304688, Entropy 236.8259735107422, Learning Rate: 0.005\n",
      "Epoch [3345/20000], Loss: 971.2239990234375, Entropy 240.24246215820312, Learning Rate: 0.005\n",
      "Epoch [3346/20000], Loss: 998.4116821289062, Entropy 235.62091064453125, Learning Rate: 0.005\n",
      "Epoch [3347/20000], Loss: 1028.982666015625, Entropy 237.3528594970703, Learning Rate: 0.005\n",
      "Epoch [3348/20000], Loss: 986.029296875, Entropy 242.17710876464844, Learning Rate: 0.005\n",
      "Epoch [3349/20000], Loss: 1032.9776611328125, Entropy 228.2120819091797, Learning Rate: 0.005\n",
      "Epoch [3350/20000], Loss: 992.3331298828125, Entropy 247.98133850097656, Learning Rate: 0.005\n",
      "Epoch [3351/20000], Loss: 995.2933349609375, Entropy 245.8896484375, Learning Rate: 0.005\n",
      "Epoch [3352/20000], Loss: 1025.2720947265625, Entropy 236.08213806152344, Learning Rate: 0.005\n",
      "Epoch [3353/20000], Loss: 1004.1908569335938, Entropy 250.94256591796875, Learning Rate: 0.005\n",
      "Epoch [3354/20000], Loss: 980.0919189453125, Entropy 243.05555725097656, Learning Rate: 0.005\n",
      "Epoch [3355/20000], Loss: 999.5181884765625, Entropy 230.03488159179688, Learning Rate: 0.005\n",
      "Epoch [3356/20000], Loss: 1003.40234375, Entropy 226.95497131347656, Learning Rate: 0.005\n",
      "Epoch [3357/20000], Loss: 1050.202392578125, Entropy 236.61550903320312, Learning Rate: 0.005\n",
      "Epoch [3358/20000], Loss: 995.956787109375, Entropy 244.41506958007812, Learning Rate: 0.005\n",
      "Epoch [3359/20000], Loss: 985.4402465820312, Entropy 236.02972412109375, Learning Rate: 0.005\n",
      "Epoch [3360/20000], Loss: 997.7205200195312, Entropy 231.0330047607422, Learning Rate: 0.005\n",
      "Epoch [3361/20000], Loss: 985.475830078125, Entropy 257.32177734375, Learning Rate: 0.005\n",
      "Epoch [3362/20000], Loss: 996.49560546875, Entropy 242.55319213867188, Learning Rate: 0.005\n",
      "Epoch [3363/20000], Loss: 1018.73046875, Entropy 240.969482421875, Learning Rate: 0.005\n",
      "Epoch [3364/20000], Loss: 996.0267944335938, Entropy 225.2025604248047, Learning Rate: 0.005\n",
      "Epoch [3365/20000], Loss: 965.3663330078125, Entropy 228.87718200683594, Learning Rate: 0.005\n",
      "Epoch [3366/20000], Loss: 1049.8331298828125, Entropy 237.35867309570312, Learning Rate: 0.005\n",
      "Epoch [3367/20000], Loss: 1027.95166015625, Entropy 234.6337127685547, Learning Rate: 0.005\n",
      "Epoch [3368/20000], Loss: 997.6145629882812, Entropy 243.78692626953125, Learning Rate: 0.005\n",
      "Epoch [3369/20000], Loss: 1029.3829345703125, Entropy 235.44471740722656, Learning Rate: 0.005\n",
      "Epoch [3370/20000], Loss: 1010.4957275390625, Entropy 246.16458129882812, Learning Rate: 0.005\n",
      "Epoch [3371/20000], Loss: 1009.44677734375, Entropy 244.19834899902344, Learning Rate: 0.005\n",
      "Epoch [3372/20000], Loss: 1010.7279052734375, Entropy 236.11949157714844, Learning Rate: 0.005\n",
      "Epoch [3373/20000], Loss: 999.7806396484375, Entropy 239.05772399902344, Learning Rate: 0.005\n",
      "Epoch [3374/20000], Loss: 1016.0094604492188, Entropy 229.2246856689453, Learning Rate: 0.005\n",
      "Epoch [3375/20000], Loss: 1024.0311279296875, Entropy 237.1669158935547, Learning Rate: 0.005\n",
      "Epoch [3376/20000], Loss: 989.7756958007812, Entropy 227.2460174560547, Learning Rate: 0.005\n",
      "Epoch [3377/20000], Loss: 1054.750244140625, Entropy 242.58255004882812, Learning Rate: 0.005\n",
      "Epoch [3378/20000], Loss: 1078.862548828125, Entropy 251.4774932861328, Learning Rate: 0.005\n",
      "Epoch [3379/20000], Loss: 1017.4929809570312, Entropy 234.6811065673828, Learning Rate: 0.005\n",
      "Epoch [3380/20000], Loss: 983.9108276367188, Entropy 246.39154052734375, Learning Rate: 0.005\n",
      "Epoch [3381/20000], Loss: 1018.4343872070312, Entropy 237.7548065185547, Learning Rate: 0.005\n",
      "Epoch [3382/20000], Loss: 1033.2054443359375, Entropy 242.6211395263672, Learning Rate: 0.005\n",
      "Epoch [3383/20000], Loss: 1055.914306640625, Entropy 245.60011291503906, Learning Rate: 0.005\n",
      "Epoch [3384/20000], Loss: 998.2628173828125, Entropy 238.36024475097656, Learning Rate: 0.005\n",
      "Epoch [3385/20000], Loss: 1030.2623291015625, Entropy 217.15695190429688, Learning Rate: 0.005\n",
      "Epoch [3386/20000], Loss: 965.6512451171875, Entropy 243.05136108398438, Learning Rate: 0.005\n",
      "Epoch [3387/20000], Loss: 985.9876708984375, Entropy 229.55686950683594, Learning Rate: 0.005\n",
      "Epoch [3388/20000], Loss: 977.1614379882812, Entropy 246.4656219482422, Learning Rate: 0.005\n",
      "Epoch [3389/20000], Loss: 959.674072265625, Entropy 238.48953247070312, Learning Rate: 0.005\n",
      "Epoch [3390/20000], Loss: 1033.7132568359375, Entropy 243.8872528076172, Learning Rate: 0.005\n",
      "Epoch [3391/20000], Loss: 1040.668212890625, Entropy 237.183349609375, Learning Rate: 0.005\n",
      "Epoch [3392/20000], Loss: 990.3097534179688, Entropy 250.7410125732422, Learning Rate: 0.005\n",
      "Epoch [3393/20000], Loss: 988.3401489257812, Entropy 233.8877716064453, Learning Rate: 0.005\n",
      "Epoch [3394/20000], Loss: 1008.5182495117188, Entropy 236.51507568359375, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3395/20000], Loss: 1025.777099609375, Entropy 249.10833740234375, Learning Rate: 0.005\n",
      "Epoch [3396/20000], Loss: 1073.263916015625, Entropy 229.4346466064453, Learning Rate: 0.005\n",
      "Epoch [3397/20000], Loss: 1093.80712890625, Entropy 237.98744201660156, Learning Rate: 0.005\n",
      "Epoch [3398/20000], Loss: 1162.2410888671875, Entropy 250.0424041748047, Learning Rate: 0.005\n",
      "Epoch [3399/20000], Loss: 1113.2518310546875, Entropy 241.14320373535156, Learning Rate: 0.005\n",
      "Epoch [3400/20000], Loss: 1053.8277587890625, Entropy 236.2283477783203, Learning Rate: 0.005\n",
      "Epoch [3401/20000], Loss: 1047.9061279296875, Entropy 244.86195373535156, Learning Rate: 0.005\n",
      "Epoch [3402/20000], Loss: 1241.367919921875, Entropy 229.73358154296875, Learning Rate: 0.005\n",
      "Epoch [3403/20000], Loss: 1346.0225830078125, Entropy 234.9978485107422, Learning Rate: 0.005\n",
      "Epoch [3404/20000], Loss: 1059.8990478515625, Entropy 247.86280822753906, Learning Rate: 0.005\n",
      "Epoch [3405/20000], Loss: 1265.1602783203125, Entropy 247.77444458007812, Learning Rate: 0.005\n",
      "Epoch [3406/20000], Loss: 1065.77783203125, Entropy 245.96014404296875, Learning Rate: 0.005\n",
      "Epoch [3407/20000], Loss: 1156.0240478515625, Entropy 238.90345764160156, Learning Rate: 0.005\n",
      "Epoch [3408/20000], Loss: 1201.1444091796875, Entropy 243.919677734375, Learning Rate: 0.005\n",
      "Epoch [3409/20000], Loss: 1165.848388671875, Entropy 243.59100341796875, Learning Rate: 0.005\n",
      "Epoch [3410/20000], Loss: 1389.7410888671875, Entropy 241.43589782714844, Learning Rate: 0.005\n",
      "Epoch [3411/20000], Loss: 1246.6595458984375, Entropy 255.15711975097656, Learning Rate: 0.005\n",
      "Epoch [3412/20000], Loss: 1243.033935546875, Entropy 239.7183074951172, Learning Rate: 0.005\n",
      "Epoch [3413/20000], Loss: 1132.468994140625, Entropy 243.4980926513672, Learning Rate: 0.005\n",
      "Epoch [3414/20000], Loss: 1076.0819091796875, Entropy 230.8270721435547, Learning Rate: 0.005\n",
      "Epoch [3415/20000], Loss: 1162.3953857421875, Entropy 240.04501342773438, Learning Rate: 0.005\n",
      "Epoch [3416/20000], Loss: 1174.880615234375, Entropy 238.97531127929688, Learning Rate: 0.005\n",
      "Epoch [3417/20000], Loss: 1370.206298828125, Entropy 241.83834838867188, Learning Rate: 0.005\n",
      "Epoch [3418/20000], Loss: 1539.151611328125, Entropy 242.58456420898438, Learning Rate: 0.005\n",
      "Epoch [3419/20000], Loss: 1069.4456787109375, Entropy 233.6092529296875, Learning Rate: 0.005\n",
      "Epoch [3420/20000], Loss: 1234.2392578125, Entropy 243.4951934814453, Learning Rate: 0.005\n",
      "Epoch [3421/20000], Loss: 1144.3084716796875, Entropy 229.16050720214844, Learning Rate: 0.005\n",
      "Epoch [3422/20000], Loss: 1138.436279296875, Entropy 235.31871032714844, Learning Rate: 0.005\n",
      "Epoch [3423/20000], Loss: 1085.880126953125, Entropy 239.23597717285156, Learning Rate: 0.005\n",
      "Epoch [3424/20000], Loss: 1118.4398193359375, Entropy 234.84214782714844, Learning Rate: 0.005\n",
      "Epoch [3425/20000], Loss: 1129.024658203125, Entropy 229.60162353515625, Learning Rate: 0.005\n",
      "Epoch [3426/20000], Loss: 1072.4112548828125, Entropy 236.09483337402344, Learning Rate: 0.005\n",
      "Epoch [3427/20000], Loss: 1117.632568359375, Entropy 236.59739685058594, Learning Rate: 0.005\n",
      "Epoch [3428/20000], Loss: 1059.7388916015625, Entropy 236.5868682861328, Learning Rate: 0.005\n",
      "Epoch [3429/20000], Loss: 1198.652099609375, Entropy 220.9510040283203, Learning Rate: 0.005\n",
      "Epoch [3430/20000], Loss: 1073.1414794921875, Entropy 228.2403106689453, Learning Rate: 0.005\n",
      "Epoch [3431/20000], Loss: 1153.0732421875, Entropy 232.33416748046875, Learning Rate: 0.005\n",
      "Epoch [3432/20000], Loss: 978.0791015625, Entropy 237.58287048339844, Learning Rate: 0.005\n",
      "Epoch [3433/20000], Loss: 1060.1824951171875, Entropy 242.4626922607422, Learning Rate: 0.005\n",
      "Epoch [3434/20000], Loss: 1089.596435546875, Entropy 230.128173828125, Learning Rate: 0.005\n",
      "Epoch [3435/20000], Loss: 1076.947509765625, Entropy 238.88128662109375, Learning Rate: 0.005\n",
      "Epoch [3436/20000], Loss: 1103.044677734375, Entropy 242.68405151367188, Learning Rate: 0.005\n",
      "Epoch [3437/20000], Loss: 1119.482421875, Entropy 251.36746215820312, Learning Rate: 0.005\n",
      "Epoch [3438/20000], Loss: 1112.5806884765625, Entropy 240.20518493652344, Learning Rate: 0.005\n",
      "Epoch [3439/20000], Loss: 1035.1007080078125, Entropy 248.60902404785156, Learning Rate: 0.005\n",
      "Epoch [3440/20000], Loss: 1090.683837890625, Entropy 230.58453369140625, Learning Rate: 0.005\n",
      "Epoch [3441/20000], Loss: 1071.2926025390625, Entropy 220.24710083007812, Learning Rate: 0.005\n",
      "Epoch [3442/20000], Loss: 1092.154296875, Entropy 239.4125518798828, Learning Rate: 0.005\n",
      "Epoch [3443/20000], Loss: 1114.83447265625, Entropy 238.9850311279297, Learning Rate: 0.005\n",
      "Epoch [3444/20000], Loss: 1095.9410400390625, Entropy 254.1711883544922, Learning Rate: 0.005\n",
      "Epoch [3445/20000], Loss: 1040.9713134765625, Entropy 233.7036590576172, Learning Rate: 0.005\n",
      "Epoch [3446/20000], Loss: 1090.10546875, Entropy 238.37417602539062, Learning Rate: 0.005\n",
      "Epoch [3447/20000], Loss: 1020.2177734375, Entropy 233.91519165039062, Learning Rate: 0.005\n",
      "Epoch [3448/20000], Loss: 1188.7301025390625, Entropy 250.7664794921875, Learning Rate: 0.005\n",
      "Epoch [3449/20000], Loss: 1046.920654296875, Entropy 244.1787109375, Learning Rate: 0.005\n",
      "Epoch [3450/20000], Loss: 1166.78466796875, Entropy 229.9862518310547, Learning Rate: 0.005\n",
      "Epoch [3451/20000], Loss: 1069.1898193359375, Entropy 237.8724822998047, Learning Rate: 0.005\n",
      "Epoch [3452/20000], Loss: 1095.2706298828125, Entropy 236.41061401367188, Learning Rate: 0.005\n",
      "Epoch [3453/20000], Loss: 1065.68310546875, Entropy 235.78826904296875, Learning Rate: 0.005\n",
      "Epoch [3454/20000], Loss: 1156.8643798828125, Entropy 228.2008514404297, Learning Rate: 0.005\n",
      "Epoch [3455/20000], Loss: 1056.3834228515625, Entropy 238.75634765625, Learning Rate: 0.005\n",
      "Epoch [3456/20000], Loss: 1039.60400390625, Entropy 214.97311401367188, Learning Rate: 0.005\n",
      "Epoch [3457/20000], Loss: 1072.7257080078125, Entropy 237.99459838867188, Learning Rate: 0.005\n",
      "Epoch [3458/20000], Loss: 1087.3173828125, Entropy 239.6854705810547, Learning Rate: 0.005\n",
      "Epoch [3459/20000], Loss: 1148.470947265625, Entropy 218.42962646484375, Learning Rate: 0.005\n",
      "Epoch [3460/20000], Loss: 1245.5496826171875, Entropy 234.1698760986328, Learning Rate: 0.005\n",
      "Epoch [3461/20000], Loss: 1158.5738525390625, Entropy 264.35205078125, Learning Rate: 0.005\n",
      "Epoch [3462/20000], Loss: 1096.29443359375, Entropy 232.3661651611328, Learning Rate: 0.005\n",
      "Epoch [3463/20000], Loss: 1005.9612426757812, Entropy 236.9413604736328, Learning Rate: 0.005\n",
      "Epoch [3464/20000], Loss: 1116.5947265625, Entropy 253.98475646972656, Learning Rate: 0.005\n",
      "Epoch [3465/20000], Loss: 1072.8056640625, Entropy 228.67921447753906, Learning Rate: 0.005\n",
      "Epoch [3466/20000], Loss: 1112.8985595703125, Entropy 220.4297637939453, Learning Rate: 0.005\n",
      "Epoch [3467/20000], Loss: 1091.5362548828125, Entropy 229.7862091064453, Learning Rate: 0.005\n",
      "Epoch [3468/20000], Loss: 1046.429931640625, Entropy 232.75714111328125, Learning Rate: 0.005\n",
      "Epoch [3469/20000], Loss: 1063.148681640625, Entropy 211.75257873535156, Learning Rate: 0.005\n",
      "Epoch [3470/20000], Loss: 1121.913818359375, Entropy 242.39251708984375, Learning Rate: 0.005\n",
      "Epoch [3471/20000], Loss: 1074.56298828125, Entropy 223.88478088378906, Learning Rate: 0.005\n",
      "Epoch [3472/20000], Loss: 1020.3267822265625, Entropy 239.42263793945312, Learning Rate: 0.005\n",
      "Epoch [3473/20000], Loss: 1017.7723388671875, Entropy 238.84426879882812, Learning Rate: 0.005\n",
      "Epoch [3474/20000], Loss: 1004.6858520507812, Entropy 247.55902099609375, Learning Rate: 0.005\n",
      "Epoch [3475/20000], Loss: 1074.6278076171875, Entropy 238.05081176757812, Learning Rate: 0.005\n",
      "Epoch [3476/20000], Loss: 1015.5250854492188, Entropy 231.14849853515625, Learning Rate: 0.005\n",
      "Epoch [3477/20000], Loss: 1085.0977783203125, Entropy 221.3182830810547, Learning Rate: 0.005\n",
      "Epoch [3478/20000], Loss: 1024.060546875, Entropy 237.3918914794922, Learning Rate: 0.005\n",
      "Epoch [3479/20000], Loss: 1078.90625, Entropy 241.60562133789062, Learning Rate: 0.005\n",
      "Epoch [3480/20000], Loss: 1035.9547119140625, Entropy 240.78271484375, Learning Rate: 0.005\n",
      "Epoch [3481/20000], Loss: 1057.4388427734375, Entropy 220.3533935546875, Learning Rate: 0.005\n",
      "Epoch [3482/20000], Loss: 1005.4771118164062, Entropy 235.7997589111328, Learning Rate: 0.005\n",
      "Epoch [3483/20000], Loss: 1063.6177978515625, Entropy 236.90284729003906, Learning Rate: 0.005\n",
      "Epoch [3484/20000], Loss: 1000.1680908203125, Entropy 239.486083984375, Learning Rate: 0.005\n",
      "Epoch [3485/20000], Loss: 1085.6737060546875, Entropy 248.16079711914062, Learning Rate: 0.005\n",
      "Epoch [3486/20000], Loss: 1049.239013671875, Entropy 237.3861541748047, Learning Rate: 0.005\n",
      "Epoch [3487/20000], Loss: 1112.588623046875, Entropy 218.20086669921875, Learning Rate: 0.005\n",
      "Epoch [3488/20000], Loss: 1135.567626953125, Entropy 224.86578369140625, Learning Rate: 0.005\n",
      "Epoch [3489/20000], Loss: 1136.336669921875, Entropy 254.9954071044922, Learning Rate: 0.005\n",
      "Epoch [3490/20000], Loss: 1116.0380859375, Entropy 233.995849609375, Learning Rate: 0.005\n",
      "Epoch [3491/20000], Loss: 1054.40283203125, Entropy 248.531005859375, Learning Rate: 0.005\n",
      "Epoch [3492/20000], Loss: 1111.840576171875, Entropy 229.52471923828125, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3493/20000], Loss: 1091.1907958984375, Entropy 230.4440155029297, Learning Rate: 0.005\n",
      "Epoch [3494/20000], Loss: 1006.5572509765625, Entropy 233.1103515625, Learning Rate: 0.005\n",
      "Epoch [3495/20000], Loss: 1327.133056640625, Entropy 231.55682373046875, Learning Rate: 0.005\n",
      "Epoch [3496/20000], Loss: 1157.338134765625, Entropy 242.65737915039062, Learning Rate: 0.005\n",
      "Epoch [3497/20000], Loss: 1295.26318359375, Entropy 227.2917938232422, Learning Rate: 0.005\n",
      "Epoch [3498/20000], Loss: 1325.3426513671875, Entropy 226.0851287841797, Learning Rate: 0.005\n",
      "Epoch [3499/20000], Loss: 1061.797607421875, Entropy 225.76617431640625, Learning Rate: 0.005\n",
      "Epoch [3500/20000], Loss: 1089.162353515625, Entropy 236.07647705078125, Learning Rate: 0.005\n",
      "Epoch [3501/20000], Loss: 1122.9686279296875, Entropy 231.5453643798828, Learning Rate: 0.005\n",
      "Epoch [3502/20000], Loss: 1149.572265625, Entropy 245.88172912597656, Learning Rate: 0.005\n",
      "Epoch [3503/20000], Loss: 1353.5230712890625, Entropy 236.1013641357422, Learning Rate: 0.005\n",
      "Epoch [3504/20000], Loss: 1215.8314208984375, Entropy 229.16921997070312, Learning Rate: 0.005\n",
      "Epoch [3505/20000], Loss: 1300.359130859375, Entropy 241.2298583984375, Learning Rate: 0.005\n",
      "Epoch [3506/20000], Loss: 1115.895751953125, Entropy 231.02175903320312, Learning Rate: 0.005\n",
      "Epoch [3507/20000], Loss: 1181.28857421875, Entropy 235.2322998046875, Learning Rate: 0.005\n",
      "Epoch [3508/20000], Loss: 1090.397216796875, Entropy 243.74310302734375, Learning Rate: 0.005\n",
      "Epoch [3509/20000], Loss: 1161.3927001953125, Entropy 236.90353393554688, Learning Rate: 0.005\n",
      "Epoch [3510/20000], Loss: 1203.1373291015625, Entropy 241.43484497070312, Learning Rate: 0.005\n",
      "Epoch [3511/20000], Loss: 1060.7239990234375, Entropy 251.58567810058594, Learning Rate: 0.005\n",
      "Epoch [3512/20000], Loss: 1582.775634765625, Entropy 228.12806701660156, Learning Rate: 0.005\n",
      "Epoch [3513/20000], Loss: 1185.6263427734375, Entropy 244.8972625732422, Learning Rate: 0.005\n",
      "Epoch [3514/20000], Loss: 1515.760498046875, Entropy 238.0782012939453, Learning Rate: 0.005\n",
      "Epoch [3515/20000], Loss: 1161.113525390625, Entropy 221.9236297607422, Learning Rate: 0.005\n",
      "Epoch [3516/20000], Loss: 1314.34326171875, Entropy 249.46188354492188, Learning Rate: 0.005\n",
      "Epoch [3517/20000], Loss: 1373.657958984375, Entropy 226.87612915039062, Learning Rate: 0.005\n",
      "Epoch [3518/20000], Loss: 1514.06640625, Entropy 221.87347412109375, Learning Rate: 0.005\n",
      "Epoch [3519/20000], Loss: 1084.6973876953125, Entropy 255.04415893554688, Learning Rate: 0.005\n",
      "Epoch [3520/20000], Loss: 1219.046142578125, Entropy 233.47894287109375, Learning Rate: 0.005\n",
      "Epoch [3521/20000], Loss: 1196.0731201171875, Entropy 224.82394409179688, Learning Rate: 0.005\n",
      "Epoch [3522/20000], Loss: 1161.3714599609375, Entropy 234.2747344970703, Learning Rate: 0.005\n",
      "Epoch [3523/20000], Loss: 1060.5595703125, Entropy 213.94146728515625, Learning Rate: 0.005\n",
      "Epoch [3524/20000], Loss: 1190.650390625, Entropy 232.2731170654297, Learning Rate: 0.005\n",
      "Epoch [3525/20000], Loss: 1023.7421875, Entropy 237.3211669921875, Learning Rate: 0.005\n",
      "Epoch [3526/20000], Loss: 1063.3096923828125, Entropy 227.375732421875, Learning Rate: 0.005\n",
      "Epoch [3527/20000], Loss: 1042.0703125, Entropy 219.89686584472656, Learning Rate: 0.005\n",
      "Epoch [3528/20000], Loss: 1032.8419189453125, Entropy 238.63595581054688, Learning Rate: 0.005\n",
      "Epoch [3529/20000], Loss: 1112.5606689453125, Entropy 228.8055419921875, Learning Rate: 0.005\n",
      "Epoch [3530/20000], Loss: 1021.056884765625, Entropy 237.68370056152344, Learning Rate: 0.005\n",
      "Epoch [3531/20000], Loss: 1026.09326171875, Entropy 236.590576171875, Learning Rate: 0.005\n",
      "Epoch [3532/20000], Loss: 1090.4876708984375, Entropy 227.84237670898438, Learning Rate: 0.005\n",
      "Epoch [3533/20000], Loss: 1015.9468383789062, Entropy 236.6895294189453, Learning Rate: 0.005\n",
      "Epoch [3534/20000], Loss: 1012.2604370117188, Entropy 238.5213165283203, Learning Rate: 0.005\n",
      "Epoch [3535/20000], Loss: 1017.6739501953125, Entropy 227.03782653808594, Learning Rate: 0.005\n",
      "Epoch [3536/20000], Loss: 1038.559326171875, Entropy 228.75048828125, Learning Rate: 0.005\n",
      "Epoch [3537/20000], Loss: 1046.9500732421875, Entropy 219.70553588867188, Learning Rate: 0.005\n",
      "Epoch [3538/20000], Loss: 1017.1387939453125, Entropy 230.94436645507812, Learning Rate: 0.005\n",
      "Epoch [3539/20000], Loss: 1014.8780517578125, Entropy 234.94970703125, Learning Rate: 0.005\n",
      "Epoch [3540/20000], Loss: 1035.4415283203125, Entropy 244.09005737304688, Learning Rate: 0.005\n",
      "Epoch [3541/20000], Loss: 1003.6590576171875, Entropy 235.44554138183594, Learning Rate: 0.005\n",
      "Epoch [3542/20000], Loss: 1047.552001953125, Entropy 228.979736328125, Learning Rate: 0.005\n",
      "Epoch [3543/20000], Loss: 1017.2042846679688, Entropy 232.2017364501953, Learning Rate: 0.005\n",
      "Epoch [3544/20000], Loss: 1054.588134765625, Entropy 237.6855010986328, Learning Rate: 0.005\n",
      "Epoch [3545/20000], Loss: 1058.4593505859375, Entropy 227.2469024658203, Learning Rate: 0.005\n",
      "Epoch [3546/20000], Loss: 1048.113525390625, Entropy 250.3280792236328, Learning Rate: 0.005\n",
      "Epoch [3547/20000], Loss: 1059.1478271484375, Entropy 235.42160034179688, Learning Rate: 0.005\n",
      "Epoch [3548/20000], Loss: 1030.32421875, Entropy 242.5515594482422, Learning Rate: 0.005\n",
      "Epoch [3549/20000], Loss: 1137.33984375, Entropy 225.385498046875, Learning Rate: 0.005\n",
      "Epoch [3550/20000], Loss: 1098.2615966796875, Entropy 233.3992462158203, Learning Rate: 0.005\n",
      "Epoch [3551/20000], Loss: 1036.3924560546875, Entropy 216.4738006591797, Learning Rate: 0.005\n",
      "Epoch [3552/20000], Loss: 1062.33740234375, Entropy 224.9281768798828, Learning Rate: 0.005\n",
      "Epoch [3553/20000], Loss: 1058.64892578125, Entropy 234.00364685058594, Learning Rate: 0.005\n",
      "Epoch [3554/20000], Loss: 1079.23486328125, Entropy 224.61444091796875, Learning Rate: 0.005\n",
      "Epoch [3555/20000], Loss: 1046.898681640625, Entropy 232.06385803222656, Learning Rate: 0.005\n",
      "Epoch [3556/20000], Loss: 1006.7743530273438, Entropy 229.72125244140625, Learning Rate: 0.005\n",
      "Epoch [3557/20000], Loss: 981.74365234375, Entropy 246.302978515625, Learning Rate: 0.005\n",
      "Epoch [3558/20000], Loss: 1038.162109375, Entropy 240.647705078125, Learning Rate: 0.005\n",
      "Epoch [3559/20000], Loss: 1070.510986328125, Entropy 224.23394775390625, Learning Rate: 0.005\n",
      "Epoch [3560/20000], Loss: 1085.7581787109375, Entropy 219.18234252929688, Learning Rate: 0.005\n",
      "Epoch [3561/20000], Loss: 1006.5406494140625, Entropy 246.75462341308594, Learning Rate: 0.005\n",
      "Epoch [3562/20000], Loss: 1009.481689453125, Entropy 238.375, Learning Rate: 0.005\n",
      "Epoch [3563/20000], Loss: 1030.7069091796875, Entropy 218.33973693847656, Learning Rate: 0.005\n",
      "Epoch [3564/20000], Loss: 1038.022705078125, Entropy 229.537841796875, Learning Rate: 0.005\n",
      "Epoch [3565/20000], Loss: 979.16552734375, Entropy 252.37779235839844, Learning Rate: 0.005\n",
      "Epoch [3566/20000], Loss: 987.264404296875, Entropy 245.26882934570312, Learning Rate: 0.005\n",
      "Epoch [3567/20000], Loss: 1021.1273803710938, Entropy 223.6522979736328, Learning Rate: 0.005\n",
      "Epoch [3568/20000], Loss: 1000.71728515625, Entropy 229.94265747070312, Learning Rate: 0.005\n",
      "Epoch [3569/20000], Loss: 1000.987548828125, Entropy 250.90914916992188, Learning Rate: 0.005\n",
      "Epoch [3570/20000], Loss: 1006.934326171875, Entropy 240.27877807617188, Learning Rate: 0.005\n",
      "Epoch [3571/20000], Loss: 1037.00732421875, Entropy 234.61351013183594, Learning Rate: 0.005\n",
      "Epoch [3572/20000], Loss: 1025.6151123046875, Entropy 243.7089385986328, Learning Rate: 0.005\n",
      "Epoch [3573/20000], Loss: 1042.586669921875, Entropy 227.79852294921875, Learning Rate: 0.005\n",
      "Epoch [3574/20000], Loss: 993.77099609375, Entropy 247.96617126464844, Learning Rate: 0.005\n",
      "Epoch [3575/20000], Loss: 1032.9659423828125, Entropy 228.9140167236328, Learning Rate: 0.005\n",
      "Epoch [3576/20000], Loss: 1036.159423828125, Entropy 230.41952514648438, Learning Rate: 0.005\n",
      "Epoch [3577/20000], Loss: 1072.2021484375, Entropy 239.47825622558594, Learning Rate: 0.005\n",
      "Epoch [3578/20000], Loss: 991.5511474609375, Entropy 248.52854919433594, Learning Rate: 0.005\n",
      "Epoch [3579/20000], Loss: 1004.5621337890625, Entropy 254.41085815429688, Learning Rate: 0.005\n",
      "Epoch [3580/20000], Loss: 1006.5639038085938, Entropy 232.3526153564453, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3581/20000], Loss: 1031.927734375, Entropy 232.8919219970703, Learning Rate: 0.005\n",
      "Epoch [3582/20000], Loss: 1003.1349487304688, Entropy 250.6833953857422, Learning Rate: 0.005\n",
      "Epoch [3583/20000], Loss: 986.66796875, Entropy 252.12747192382812, Learning Rate: 0.005\n",
      "Epoch [3584/20000], Loss: 1008.778564453125, Entropy 245.73825073242188, Learning Rate: 0.005\n",
      "Epoch [3585/20000], Loss: 1012.0218505859375, Entropy 231.96229553222656, Learning Rate: 0.005\n",
      "Epoch [3586/20000], Loss: 1036.1529541015625, Entropy 224.85800170898438, Learning Rate: 0.005\n",
      "Epoch [3587/20000], Loss: 946.63232421875, Entropy 248.79745483398438, Learning Rate: 0.005\n",
      "Epoch [3588/20000], Loss: 1029.796142578125, Entropy 235.82652282714844, Learning Rate: 0.005\n",
      "Epoch [3589/20000], Loss: 991.5128173828125, Entropy 234.71316528320312, Learning Rate: 0.005\n",
      "Epoch [3590/20000], Loss: 1077.03369140625, Entropy 239.41046142578125, Learning Rate: 0.005\n",
      "Epoch [3591/20000], Loss: 984.791015625, Entropy 230.02957153320312, Learning Rate: 0.005\n",
      "Epoch [3592/20000], Loss: 985.41259765625, Entropy 234.83851623535156, Learning Rate: 0.005\n",
      "Epoch [3593/20000], Loss: 1028.3902587890625, Entropy 239.76724243164062, Learning Rate: 0.005\n",
      "Epoch [3594/20000], Loss: 988.576416015625, Entropy 231.419677734375, Learning Rate: 0.005\n",
      "Epoch [3595/20000], Loss: 1029.0419921875, Entropy 236.0271759033203, Learning Rate: 0.005\n",
      "Epoch [3596/20000], Loss: 1028.859130859375, Entropy 226.3173828125, Learning Rate: 0.005\n",
      "Epoch [3597/20000], Loss: 998.885986328125, Entropy 245.41378784179688, Learning Rate: 0.005\n",
      "Epoch [3598/20000], Loss: 966.8072509765625, Entropy 249.16659545898438, Learning Rate: 0.005\n",
      "Epoch [3599/20000], Loss: 958.258544921875, Entropy 233.72714233398438, Learning Rate: 0.005\n",
      "Epoch [3600/20000], Loss: 978.0873413085938, Entropy 247.9089813232422, Learning Rate: 0.005\n",
      "Epoch [3601/20000], Loss: 990.6795043945312, Entropy 218.9153594970703, Learning Rate: 0.005\n",
      "Epoch [3602/20000], Loss: 970.8709716796875, Entropy 232.19358825683594, Learning Rate: 0.005\n",
      "Epoch [3603/20000], Loss: 976.0418090820312, Entropy 229.51702880859375, Learning Rate: 0.005\n",
      "Epoch [3604/20000], Loss: 1055.465087890625, Entropy 235.990966796875, Learning Rate: 0.005\n",
      "Epoch [3605/20000], Loss: 992.2669677734375, Entropy 228.9373779296875, Learning Rate: 0.005\n",
      "Epoch [3606/20000], Loss: 987.6426391601562, Entropy 236.9529266357422, Learning Rate: 0.005\n",
      "Epoch [3607/20000], Loss: 1002.4197998046875, Entropy 257.8365783691406, Learning Rate: 0.005\n",
      "Epoch [3608/20000], Loss: 1016.2203369140625, Entropy 226.41542053222656, Learning Rate: 0.005\n",
      "Epoch [3609/20000], Loss: 983.1397094726562, Entropy 243.64093017578125, Learning Rate: 0.005\n",
      "Epoch [3610/20000], Loss: 960.382080078125, Entropy 254.31263732910156, Learning Rate: 0.005\n",
      "Epoch [3611/20000], Loss: 1019.89404296875, Entropy 248.43212890625, Learning Rate: 0.005\n",
      "Epoch [3612/20000], Loss: 953.9384765625, Entropy 249.22921752929688, Learning Rate: 0.005\n",
      "Epoch [3613/20000], Loss: 984.2958374023438, Entropy 244.8137969970703, Learning Rate: 0.005\n",
      "Epoch [3614/20000], Loss: 996.47705078125, Entropy 243.630859375, Learning Rate: 0.005\n",
      "Epoch [3615/20000], Loss: 1003.3275756835938, Entropy 238.46905517578125, Learning Rate: 0.005\n",
      "Epoch [3616/20000], Loss: 980.74951171875, Entropy 255.55374145507812, Learning Rate: 0.005\n",
      "Epoch [3617/20000], Loss: 972.5453491210938, Entropy 227.28387451171875, Learning Rate: 0.005\n",
      "Epoch [3618/20000], Loss: 976.3175659179688, Entropy 237.90838623046875, Learning Rate: 0.005\n",
      "Epoch [3619/20000], Loss: 979.5382080078125, Entropy 241.97178649902344, Learning Rate: 0.005\n",
      "Epoch [3620/20000], Loss: 960.2177734375, Entropy 237.75157165527344, Learning Rate: 0.005\n",
      "Epoch [3621/20000], Loss: 961.7386474609375, Entropy 242.172119140625, Learning Rate: 0.005\n",
      "Epoch [3622/20000], Loss: 985.3549194335938, Entropy 236.7805633544922, Learning Rate: 0.005\n",
      "Epoch [3623/20000], Loss: 1041.359130859375, Entropy 228.88140869140625, Learning Rate: 0.005\n",
      "Epoch [3624/20000], Loss: 977.3187866210938, Entropy 240.69110107421875, Learning Rate: 0.005\n",
      "Epoch [3625/20000], Loss: 996.12548828125, Entropy 235.68333435058594, Learning Rate: 0.005\n",
      "Epoch [3626/20000], Loss: 1076.0706787109375, Entropy 221.32254028320312, Learning Rate: 0.005\n",
      "Epoch [3627/20000], Loss: 982.2039794921875, Entropy 253.659912109375, Learning Rate: 0.005\n",
      "Epoch [3628/20000], Loss: 1020.0481567382812, Entropy 232.55596923828125, Learning Rate: 0.005\n",
      "Epoch [3629/20000], Loss: 1012.60546875, Entropy 254.25965881347656, Learning Rate: 0.005\n",
      "Epoch [3630/20000], Loss: 972.5035400390625, Entropy 248.16624450683594, Learning Rate: 0.005\n",
      "Epoch [3631/20000], Loss: 1036.5333251953125, Entropy 228.89271545410156, Learning Rate: 0.005\n",
      "Epoch [3632/20000], Loss: 991.9127807617188, Entropy 254.5447540283203, Learning Rate: 0.005\n",
      "Epoch [3633/20000], Loss: 1027.4034423828125, Entropy 232.7290496826172, Learning Rate: 0.005\n",
      "Epoch [3634/20000], Loss: 1003.6190185546875, Entropy 231.85549926757812, Learning Rate: 0.005\n",
      "Epoch [3635/20000], Loss: 1053.9345703125, Entropy 225.276611328125, Learning Rate: 0.005\n",
      "Epoch [3636/20000], Loss: 1024.9739990234375, Entropy 229.3929901123047, Learning Rate: 0.005\n",
      "Epoch [3637/20000], Loss: 1001.47998046875, Entropy 231.82728576660156, Learning Rate: 0.005\n",
      "Epoch [3638/20000], Loss: 960.7743530273438, Entropy 232.1356658935547, Learning Rate: 0.005\n",
      "Epoch [3639/20000], Loss: 998.9241943359375, Entropy 243.76759338378906, Learning Rate: 0.005\n",
      "Epoch [3640/20000], Loss: 1001.4605712890625, Entropy 234.5244140625, Learning Rate: 0.005\n",
      "Epoch [3641/20000], Loss: 1009.4352416992188, Entropy 240.78643798828125, Learning Rate: 0.005\n",
      "Epoch [3642/20000], Loss: 1016.46484375, Entropy 231.91737365722656, Learning Rate: 0.005\n",
      "Epoch [3643/20000], Loss: 1028.449951171875, Entropy 245.15780639648438, Learning Rate: 0.005\n",
      "Epoch [3644/20000], Loss: 1021.4973754882812, Entropy 248.5852813720703, Learning Rate: 0.005\n",
      "Epoch [3645/20000], Loss: 1014.4497680664062, Entropy 234.22052001953125, Learning Rate: 0.005\n",
      "Epoch [3646/20000], Loss: 973.5982666015625, Entropy 248.70423889160156, Learning Rate: 0.005\n",
      "Epoch [3647/20000], Loss: 1066.678466796875, Entropy 244.26792907714844, Learning Rate: 0.005\n",
      "Epoch [3648/20000], Loss: 1014.20361328125, Entropy 230.32484436035156, Learning Rate: 0.005\n",
      "Epoch [3649/20000], Loss: 1010.7855224609375, Entropy 236.5408935546875, Learning Rate: 0.005\n",
      "Epoch [3650/20000], Loss: 1019.9617309570312, Entropy 240.28936767578125, Learning Rate: 0.005\n",
      "Epoch [3651/20000], Loss: 1000.5576171875, Entropy 231.4842529296875, Learning Rate: 0.005\n",
      "Epoch [3652/20000], Loss: 1021.955078125, Entropy 234.90626525878906, Learning Rate: 0.005\n",
      "Epoch [3653/20000], Loss: 992.1927490234375, Entropy 236.02085876464844, Learning Rate: 0.005\n",
      "Epoch [3654/20000], Loss: 1001.1253051757812, Entropy 239.2659149169922, Learning Rate: 0.005\n",
      "Epoch [3655/20000], Loss: 986.4078979492188, Entropy 238.0236053466797, Learning Rate: 0.005\n",
      "Epoch [3656/20000], Loss: 974.4456787109375, Entropy 248.32284545898438, Learning Rate: 0.005\n",
      "Epoch [3657/20000], Loss: 1010.54541015625, Entropy 232.19461059570312, Learning Rate: 0.005\n",
      "Epoch [3658/20000], Loss: 974.7459716796875, Entropy 258.5142822265625, Learning Rate: 0.005\n",
      "Epoch [3659/20000], Loss: 995.4541625976562, Entropy 232.9888153076172, Learning Rate: 0.005\n",
      "Epoch [3660/20000], Loss: 970.3553466796875, Entropy 243.23976135253906, Learning Rate: 0.005\n",
      "Epoch [3661/20000], Loss: 1008.4573974609375, Entropy 244.4178466796875, Learning Rate: 0.005\n",
      "Epoch [3662/20000], Loss: 968.6637573242188, Entropy 246.37823486328125, Learning Rate: 0.005\n",
      "Epoch [3663/20000], Loss: 1020.7542724609375, Entropy 245.21676635742188, Learning Rate: 0.005\n",
      "Epoch [3664/20000], Loss: 981.3802490234375, Entropy 241.84022521972656, Learning Rate: 0.005\n",
      "Epoch [3665/20000], Loss: 990.5733032226562, Entropy 245.1434783935547, Learning Rate: 0.005\n",
      "Epoch [3666/20000], Loss: 1049.1712646484375, Entropy 247.34109497070312, Learning Rate: 0.005\n",
      "Epoch [3667/20000], Loss: 1029.80908203125, Entropy 236.00189208984375, Learning Rate: 0.005\n",
      "Epoch [3668/20000], Loss: 984.82275390625, Entropy 245.70703125, Learning Rate: 0.005\n",
      "Epoch [3669/20000], Loss: 1038.040771484375, Entropy 248.56979370117188, Learning Rate: 0.005\n",
      "Epoch [3670/20000], Loss: 1048.8616943359375, Entropy 248.17581176757812, Learning Rate: 0.005\n",
      "Epoch [3671/20000], Loss: 987.4735717773438, Entropy 240.1005096435547, Learning Rate: 0.005\n",
      "Epoch [3672/20000], Loss: 1050.7509765625, Entropy 249.647216796875, Learning Rate: 0.005\n",
      "Epoch [3673/20000], Loss: 1071.02392578125, Entropy 253.855224609375, Learning Rate: 0.005\n",
      "Epoch [3674/20000], Loss: 1008.2708129882812, Entropy 246.72979736328125, Learning Rate: 0.005\n",
      "Epoch [3675/20000], Loss: 1052.0181884765625, Entropy 238.7350311279297, Learning Rate: 0.005\n",
      "Epoch [3676/20000], Loss: 960.7423095703125, Entropy 255.36038208007812, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3677/20000], Loss: 1034.625, Entropy 248.23760986328125, Learning Rate: 0.005\n",
      "Epoch [3678/20000], Loss: 972.699462890625, Entropy 239.31748962402344, Learning Rate: 0.005\n",
      "Epoch [3679/20000], Loss: 1058.8353271484375, Entropy 238.466796875, Learning Rate: 0.005\n",
      "Epoch [3680/20000], Loss: 1031.4835205078125, Entropy 236.884521484375, Learning Rate: 0.005\n",
      "Epoch [3681/20000], Loss: 1052.4708251953125, Entropy 247.72900390625, Learning Rate: 0.005\n",
      "Epoch [3682/20000], Loss: 1041.36181640625, Entropy 234.91043090820312, Learning Rate: 0.005\n",
      "Epoch [3683/20000], Loss: 1071.2916259765625, Entropy 242.19558715820312, Learning Rate: 0.005\n",
      "Epoch [3684/20000], Loss: 999.230224609375, Entropy 264.5761413574219, Learning Rate: 0.005\n",
      "Epoch [3685/20000], Loss: 1062.5694580078125, Entropy 243.95921325683594, Learning Rate: 0.005\n",
      "Epoch [3686/20000], Loss: 1017.632568359375, Entropy 245.33358764648438, Learning Rate: 0.005\n",
      "Epoch [3687/20000], Loss: 1005.6719970703125, Entropy 251.47586059570312, Learning Rate: 0.005\n",
      "Epoch [3688/20000], Loss: 982.7587280273438, Entropy 248.7761688232422, Learning Rate: 0.005\n",
      "Epoch [3689/20000], Loss: 1014.710205078125, Entropy 246.78195190429688, Learning Rate: 0.005\n",
      "Epoch [3690/20000], Loss: 1053.0244140625, Entropy 262.0639343261719, Learning Rate: 0.005\n",
      "Epoch [3691/20000], Loss: 994.9404907226562, Entropy 255.48406982421875, Learning Rate: 0.005\n",
      "Epoch [3692/20000], Loss: 1012.0140380859375, Entropy 241.79090881347656, Learning Rate: 0.005\n",
      "Epoch [3693/20000], Loss: 1029.458740234375, Entropy 259.23345947265625, Learning Rate: 0.005\n",
      "Epoch [3694/20000], Loss: 1021.7674560546875, Entropy 240.44406127929688, Learning Rate: 0.005\n",
      "Epoch [3695/20000], Loss: 1034.3653564453125, Entropy 247.3076934814453, Learning Rate: 0.005\n",
      "Epoch [3696/20000], Loss: 996.6888427734375, Entropy 249.21542358398438, Learning Rate: 0.005\n",
      "Epoch [3697/20000], Loss: 1011.6566162109375, Entropy 251.24351501464844, Learning Rate: 0.005\n",
      "Epoch [3698/20000], Loss: 1013.5549926757812, Entropy 242.60491943359375, Learning Rate: 0.005\n",
      "Epoch [3699/20000], Loss: 948.590576171875, Entropy 264.4224853515625, Learning Rate: 0.005\n",
      "Epoch [3700/20000], Loss: 1038.339111328125, Entropy 225.05511474609375, Learning Rate: 0.005\n",
      "Epoch [3701/20000], Loss: 1017.1142578125, Entropy 258.5818176269531, Learning Rate: 0.005\n",
      "Epoch [3702/20000], Loss: 1000.783935546875, Entropy 256.5375061035156, Learning Rate: 0.005\n",
      "Epoch [3703/20000], Loss: 943.0660400390625, Entropy 271.9142150878906, Learning Rate: 0.005\n",
      "Epoch [3704/20000], Loss: 957.5196533203125, Entropy 264.4877014160156, Learning Rate: 0.005\n",
      "Epoch [3705/20000], Loss: 1051.9320068359375, Entropy 246.72140502929688, Learning Rate: 0.005\n",
      "Epoch [3706/20000], Loss: 1003.6099243164062, Entropy 255.6099395751953, Learning Rate: 0.005\n",
      "Epoch [3707/20000], Loss: 983.6587524414062, Entropy 254.2270965576172, Learning Rate: 0.005\n",
      "Epoch [3708/20000], Loss: 1013.0654296875, Entropy 247.37167358398438, Learning Rate: 0.005\n",
      "Epoch [3709/20000], Loss: 1021.9926147460938, Entropy 264.09002685546875, Learning Rate: 0.005\n",
      "Epoch [3710/20000], Loss: 1034.404296875, Entropy 239.111083984375, Learning Rate: 0.005\n",
      "Epoch [3711/20000], Loss: 945.4915771484375, Entropy 246.68386840820312, Learning Rate: 0.005\n",
      "Epoch [3712/20000], Loss: 1082.9569091796875, Entropy 237.95118713378906, Learning Rate: 0.005\n",
      "Epoch [3713/20000], Loss: 1031.1900634765625, Entropy 257.891357421875, Learning Rate: 0.005\n",
      "Epoch [3714/20000], Loss: 987.3607788085938, Entropy 246.0836639404297, Learning Rate: 0.005\n",
      "Epoch [3715/20000], Loss: 946.3455810546875, Entropy 245.84751892089844, Learning Rate: 0.005\n",
      "Epoch [3716/20000], Loss: 1000.3135986328125, Entropy 266.8511657714844, Learning Rate: 0.005\n",
      "Epoch [3717/20000], Loss: 1009.9996337890625, Entropy 247.92552185058594, Learning Rate: 0.005\n",
      "Epoch [3718/20000], Loss: 998.1183471679688, Entropy 230.0282440185547, Learning Rate: 0.005\n",
      "Epoch [3719/20000], Loss: 1027.7178955078125, Entropy 243.44204711914062, Learning Rate: 0.005\n",
      "Epoch [3720/20000], Loss: 977.51806640625, Entropy 252.65817260742188, Learning Rate: 0.005\n",
      "Epoch [3721/20000], Loss: 968.7872924804688, Entropy 254.0922088623047, Learning Rate: 0.005\n",
      "Epoch [3722/20000], Loss: 981.0306396484375, Entropy 252.53517150878906, Learning Rate: 0.005\n",
      "Epoch [3723/20000], Loss: 1001.167724609375, Entropy 243.13247680664062, Learning Rate: 0.005\n",
      "Epoch [3724/20000], Loss: 983.5282592773438, Entropy 246.4165496826172, Learning Rate: 0.005\n",
      "Epoch [3725/20000], Loss: 1023.8218994140625, Entropy 239.06642150878906, Learning Rate: 0.005\n",
      "Epoch [3726/20000], Loss: 940.789794921875, Entropy 264.9393615722656, Learning Rate: 0.005\n",
      "Epoch [3727/20000], Loss: 967.42041015625, Entropy 263.5167541503906, Learning Rate: 0.005\n",
      "Epoch [3728/20000], Loss: 994.5904541015625, Entropy 239.44186401367188, Learning Rate: 0.005\n",
      "Epoch [3729/20000], Loss: 988.8427734375, Entropy 259.222412109375, Learning Rate: 0.005\n",
      "Epoch [3730/20000], Loss: 939.51806640625, Entropy 287.5220947265625, Learning Rate: 0.005\n",
      "Epoch [3731/20000], Loss: 989.8438720703125, Entropy 249.40589904785156, Learning Rate: 0.005\n",
      "Epoch [3732/20000], Loss: 951.545166015625, Entropy 251.02297973632812, Learning Rate: 0.005\n",
      "Epoch [3733/20000], Loss: 1006.023193359375, Entropy 256.5097351074219, Learning Rate: 0.005\n",
      "Epoch [3734/20000], Loss: 988.8977661132812, Entropy 248.08636474609375, Learning Rate: 0.005\n",
      "Epoch [3735/20000], Loss: 994.7352905273438, Entropy 235.99005126953125, Learning Rate: 0.005\n",
      "Epoch [3736/20000], Loss: 1009.1676635742188, Entropy 248.08905029296875, Learning Rate: 0.005\n",
      "Epoch [3737/20000], Loss: 977.6007690429688, Entropy 243.64886474609375, Learning Rate: 0.005\n",
      "Epoch [3738/20000], Loss: 944.64990234375, Entropy 273.4592590332031, Learning Rate: 0.005\n",
      "Epoch [3739/20000], Loss: 945.2691650390625, Entropy 257.9975891113281, Learning Rate: 0.005\n",
      "Epoch [3740/20000], Loss: 1003.36962890625, Entropy 260.5667419433594, Learning Rate: 0.005\n",
      "Epoch [3741/20000], Loss: 1025.74658203125, Entropy 234.57481384277344, Learning Rate: 0.005\n",
      "Epoch [3742/20000], Loss: 1027.982666015625, Entropy 245.59097290039062, Learning Rate: 0.005\n",
      "Epoch [3743/20000], Loss: 1007.6400146484375, Entropy 255.39146423339844, Learning Rate: 0.005\n",
      "Epoch [3744/20000], Loss: 1004.501220703125, Entropy 249.57089233398438, Learning Rate: 0.005\n",
      "Epoch [3745/20000], Loss: 975.9888916015625, Entropy 262.7133483886719, Learning Rate: 0.005\n",
      "Epoch [3746/20000], Loss: 1005.2653198242188, Entropy 250.2360076904297, Learning Rate: 0.005\n",
      "Epoch [3747/20000], Loss: 1114.4866943359375, Entropy 243.3856201171875, Learning Rate: 0.005\n",
      "Epoch [3748/20000], Loss: 1008.0780029296875, Entropy 259.7838134765625, Learning Rate: 0.005\n",
      "Epoch [3749/20000], Loss: 1147.23291015625, Entropy 227.59535217285156, Learning Rate: 0.005\n",
      "Epoch [3750/20000], Loss: 999.35888671875, Entropy 263.0625305175781, Learning Rate: 0.005\n",
      "Epoch [3751/20000], Loss: 1043.9249267578125, Entropy 242.22317504882812, Learning Rate: 0.005\n",
      "Epoch [3752/20000], Loss: 1021.4185791015625, Entropy 260.7284851074219, Learning Rate: 0.005\n",
      "Epoch [3753/20000], Loss: 1032.72314453125, Entropy 253.7427520751953, Learning Rate: 0.005\n",
      "Epoch [3754/20000], Loss: 1019.4551391601562, Entropy 249.4020233154297, Learning Rate: 0.005\n",
      "Epoch [3755/20000], Loss: 1055.589111328125, Entropy 245.50802612304688, Learning Rate: 0.005\n",
      "Epoch [3756/20000], Loss: 1064.323486328125, Entropy 268.00750732421875, Learning Rate: 0.005\n",
      "Epoch [3757/20000], Loss: 1011.2481079101562, Entropy 261.81158447265625, Learning Rate: 0.005\n",
      "Epoch [3758/20000], Loss: 984.4561157226562, Entropy 295.38055419921875, Learning Rate: 0.005\n",
      "Epoch [3759/20000], Loss: 1067.9451904296875, Entropy 250.62583923339844, Learning Rate: 0.005\n",
      "Epoch [3760/20000], Loss: 1073.6734619140625, Entropy 246.7880096435547, Learning Rate: 0.005\n",
      "Epoch [3761/20000], Loss: 1035.11962890625, Entropy 254.78271484375, Learning Rate: 0.005\n",
      "Epoch [3762/20000], Loss: 998.581787109375, Entropy 261.8802490234375, Learning Rate: 0.005\n",
      "Epoch [3763/20000], Loss: 1054.51806640625, Entropy 246.1573028564453, Learning Rate: 0.005\n",
      "Epoch [3764/20000], Loss: 1058.3531494140625, Entropy 265.0848693847656, Learning Rate: 0.005\n",
      "Epoch [3765/20000], Loss: 1061.51806640625, Entropy 238.0067901611328, Learning Rate: 0.005\n",
      "Epoch [3766/20000], Loss: 998.288330078125, Entropy 256.4671630859375, Learning Rate: 0.005\n",
      "Epoch [3767/20000], Loss: 988.5900268554688, Entropy 287.63092041015625, Learning Rate: 0.005\n",
      "Epoch [3768/20000], Loss: 1009.723388671875, Entropy 264.1728820800781, Learning Rate: 0.005\n",
      "Epoch [3769/20000], Loss: 1022.60302734375, Entropy 258.1374816894531, Learning Rate: 0.005\n",
      "Epoch [3770/20000], Loss: 1105.137451171875, Entropy 250.8423614501953, Learning Rate: 0.005\n",
      "Epoch [3771/20000], Loss: 1018.1585083007812, Entropy 263.70526123046875, Learning Rate: 0.005\n",
      "Epoch [3772/20000], Loss: 1066.19775390625, Entropy 259.57763671875, Learning Rate: 0.005\n",
      "Epoch [3773/20000], Loss: 1007.5390014648438, Entropy 247.4428253173828, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3774/20000], Loss: 1047.8714599609375, Entropy 256.0489196777344, Learning Rate: 0.005\n",
      "Epoch [3775/20000], Loss: 979.5447998046875, Entropy 245.977294921875, Learning Rate: 0.005\n",
      "Epoch [3776/20000], Loss: 1055.6365966796875, Entropy 255.738037109375, Learning Rate: 0.005\n",
      "Epoch [3777/20000], Loss: 1049.519287109375, Entropy 256.8616027832031, Learning Rate: 0.005\n",
      "Epoch [3778/20000], Loss: 1029.5667724609375, Entropy 249.18484497070312, Learning Rate: 0.005\n",
      "Epoch [3779/20000], Loss: 977.66748046875, Entropy 262.3380126953125, Learning Rate: 0.005\n",
      "Epoch [3780/20000], Loss: 1056.059326171875, Entropy 260.4707946777344, Learning Rate: 0.005\n",
      "Epoch [3781/20000], Loss: 1071.0655517578125, Entropy 273.46533203125, Learning Rate: 0.005\n",
      "Epoch [3782/20000], Loss: 1080.3060302734375, Entropy 268.5673522949219, Learning Rate: 0.005\n",
      "Epoch [3783/20000], Loss: 985.1643676757812, Entropy 254.5085906982422, Learning Rate: 0.005\n",
      "Epoch [3784/20000], Loss: 1072.3695068359375, Entropy 248.07156372070312, Learning Rate: 0.005\n",
      "Epoch [3785/20000], Loss: 1025.060302734375, Entropy 255.52081298828125, Learning Rate: 0.005\n",
      "Epoch [3786/20000], Loss: 1101.2158203125, Entropy 243.819580078125, Learning Rate: 0.005\n",
      "Epoch [3787/20000], Loss: 1103.5328369140625, Entropy 248.80812072753906, Learning Rate: 0.005\n",
      "Epoch [3788/20000], Loss: 999.8836059570312, Entropy 279.48065185546875, Learning Rate: 0.005\n",
      "Epoch [3789/20000], Loss: 996.67529296875, Entropy 270.1834716796875, Learning Rate: 0.005\n",
      "Epoch [3790/20000], Loss: 1140.6470947265625, Entropy 260.7291259765625, Learning Rate: 0.005\n",
      "Epoch [3791/20000], Loss: 1057.3870849609375, Entropy 249.55905151367188, Learning Rate: 0.005\n",
      "Epoch [3792/20000], Loss: 1198.383056640625, Entropy 249.88381958007812, Learning Rate: 0.005\n",
      "Epoch [3793/20000], Loss: 1072.9830322265625, Entropy 254.8987274169922, Learning Rate: 0.005\n",
      "Epoch [3794/20000], Loss: 1164.29443359375, Entropy 255.7520294189453, Learning Rate: 0.005\n",
      "Epoch [3795/20000], Loss: 1090.566162109375, Entropy 242.83168029785156, Learning Rate: 0.005\n",
      "Epoch [3796/20000], Loss: 1140.267333984375, Entropy 246.0613555908203, Learning Rate: 0.005\n",
      "Epoch [3797/20000], Loss: 1256.5582275390625, Entropy 246.3569793701172, Learning Rate: 0.005\n",
      "Epoch [3798/20000], Loss: 1043.855712890625, Entropy 246.58441162109375, Learning Rate: 0.005\n",
      "Epoch [3799/20000], Loss: 1140.975830078125, Entropy 249.4248046875, Learning Rate: 0.005\n",
      "Epoch [3800/20000], Loss: 1119.346923828125, Entropy 235.84646606445312, Learning Rate: 0.005\n",
      "Epoch [3801/20000], Loss: 1158.400390625, Entropy 260.6283264160156, Learning Rate: 0.005\n",
      "Epoch [3802/20000], Loss: 1016.67919921875, Entropy 268.6060791015625, Learning Rate: 0.005\n",
      "Epoch [3803/20000], Loss: 1271.2568359375, Entropy 262.51898193359375, Learning Rate: 0.005\n",
      "Epoch [3804/20000], Loss: 1285.9703369140625, Entropy 258.7256164550781, Learning Rate: 0.005\n",
      "Epoch [3805/20000], Loss: 1650.039306640625, Entropy 245.60311889648438, Learning Rate: 0.005\n",
      "Epoch [3806/20000], Loss: 1694.548095703125, Entropy 256.8477478027344, Learning Rate: 0.005\n",
      "Epoch [3807/20000], Loss: 1317.968017578125, Entropy 255.3363037109375, Learning Rate: 0.005\n",
      "Epoch [3808/20000], Loss: 1079.52734375, Entropy 271.9538879394531, Learning Rate: 0.005\n",
      "Epoch [3809/20000], Loss: 1581.2913818359375, Entropy 259.962646484375, Learning Rate: 0.005\n",
      "Epoch [3810/20000], Loss: 1182.9697265625, Entropy 262.03076171875, Learning Rate: 0.005\n",
      "Epoch [3811/20000], Loss: 1204.7784423828125, Entropy 250.3815155029297, Learning Rate: 0.005\n",
      "Epoch [3812/20000], Loss: 1073.1456298828125, Entropy 250.611083984375, Learning Rate: 0.005\n",
      "Epoch [3813/20000], Loss: 1123.0960693359375, Entropy 270.1328430175781, Learning Rate: 0.005\n",
      "Epoch [3814/20000], Loss: 1027.917236328125, Entropy 248.4027557373047, Learning Rate: 0.005\n",
      "Epoch [3815/20000], Loss: 1040.42578125, Entropy 275.4893798828125, Learning Rate: 0.005\n",
      "Epoch [3816/20000], Loss: 1042.4105224609375, Entropy 245.483154296875, Learning Rate: 0.005\n",
      "Epoch [3817/20000], Loss: 1140.1519775390625, Entropy 266.4832763671875, Learning Rate: 0.005\n",
      "Epoch [3818/20000], Loss: 1061.705078125, Entropy 249.3548126220703, Learning Rate: 0.005\n",
      "Epoch [3819/20000], Loss: 1012.5779418945312, Entropy 251.9158477783203, Learning Rate: 0.005\n",
      "Epoch [3820/20000], Loss: 973.5074462890625, Entropy 240.92530822753906, Learning Rate: 0.005\n",
      "Epoch [3821/20000], Loss: 970.404052734375, Entropy 262.5069885253906, Learning Rate: 0.005\n",
      "Epoch [3822/20000], Loss: 1025.1490478515625, Entropy 240.35926818847656, Learning Rate: 0.005\n",
      "Epoch [3823/20000], Loss: 1051.8837890625, Entropy 252.68597412109375, Learning Rate: 0.005\n",
      "Epoch [3824/20000], Loss: 970.2089233398438, Entropy 257.71746826171875, Learning Rate: 0.005\n",
      "Epoch [3825/20000], Loss: 1026.65673828125, Entropy 263.4540100097656, Learning Rate: 0.005\n",
      "Epoch [3826/20000], Loss: 1018.0042724609375, Entropy 247.16590881347656, Learning Rate: 0.005\n",
      "Epoch [3827/20000], Loss: 1055.3912353515625, Entropy 250.92515563964844, Learning Rate: 0.005\n",
      "Epoch [3828/20000], Loss: 946.183837890625, Entropy 263.514404296875, Learning Rate: 0.005\n",
      "Epoch [3829/20000], Loss: 1009.0393676757812, Entropy 257.22906494140625, Learning Rate: 0.005\n",
      "Epoch [3830/20000], Loss: 1034.2576904296875, Entropy 250.0694122314453, Learning Rate: 0.005\n",
      "Epoch [3831/20000], Loss: 1008.99951171875, Entropy 259.7209167480469, Learning Rate: 0.005\n",
      "Epoch [3832/20000], Loss: 1029.7647705078125, Entropy 261.8500061035156, Learning Rate: 0.005\n",
      "Epoch [3833/20000], Loss: 1001.5071411132812, Entropy 250.92596435546875, Learning Rate: 0.005\n",
      "Epoch [3834/20000], Loss: 1000.6499633789062, Entropy 245.7888946533203, Learning Rate: 0.005\n",
      "Epoch [3835/20000], Loss: 1093.1551513671875, Entropy 224.2298126220703, Learning Rate: 0.005\n",
      "Epoch [3836/20000], Loss: 1148.20947265625, Entropy 262.12548828125, Learning Rate: 0.005\n",
      "Epoch [3837/20000], Loss: 1018.136962890625, Entropy 248.17909240722656, Learning Rate: 0.005\n",
      "Epoch [3838/20000], Loss: 1164.874267578125, Entropy 244.7251434326172, Learning Rate: 0.005\n",
      "Epoch [3839/20000], Loss: 1019.5467529296875, Entropy 246.50245666503906, Learning Rate: 0.005\n",
      "Epoch [3840/20000], Loss: 1223.4244384765625, Entropy 264.4215087890625, Learning Rate: 0.005\n",
      "Epoch [3841/20000], Loss: 1060.91064453125, Entropy 239.68186950683594, Learning Rate: 0.005\n",
      "Epoch [3842/20000], Loss: 1384.0860595703125, Entropy 254.30764770507812, Learning Rate: 0.005\n",
      "Epoch [3843/20000], Loss: 1191.638427734375, Entropy 248.95330810546875, Learning Rate: 0.005\n",
      "Epoch [3844/20000], Loss: 1332.0426025390625, Entropy 259.8083190917969, Learning Rate: 0.005\n",
      "Epoch [3845/20000], Loss: 1512.5916748046875, Entropy 253.12147521972656, Learning Rate: 0.005\n",
      "Epoch [3846/20000], Loss: 1770.4468994140625, Entropy 250.1180877685547, Learning Rate: 0.005\n",
      "Epoch [3847/20000], Loss: 2474.52587890625, Entropy 235.5252685546875, Learning Rate: 0.005\n",
      "Epoch [3848/20000], Loss: 1956.1014404296875, Entropy 258.5724182128906, Learning Rate: 0.005\n",
      "Epoch [3849/20000], Loss: 3039.1884765625, Entropy 247.3677520751953, Learning Rate: 0.005\n",
      "Epoch [3850/20000], Loss: 1835.8653564453125, Entropy 263.3836364746094, Learning Rate: 0.005\n",
      "Epoch [3851/20000], Loss: 2705.96044921875, Entropy 218.351318359375, Learning Rate: 0.005\n",
      "Epoch [3852/20000], Loss: 3404.03515625, Entropy 244.1837615966797, Learning Rate: 0.005\n",
      "Epoch [3853/20000], Loss: 3653.140625, Entropy 250.2941131591797, Learning Rate: 0.005\n",
      "Epoch [3854/20000], Loss: 1780.9957275390625, Entropy 248.01991271972656, Learning Rate: 0.005\n",
      "Epoch [3855/20000], Loss: 1486.6390380859375, Entropy 242.4520263671875, Learning Rate: 0.005\n",
      "Epoch [3856/20000], Loss: 2458.5712890625, Entropy 249.12417602539062, Learning Rate: 0.005\n",
      "Epoch [3857/20000], Loss: 1298.6904296875, Entropy 263.5816650390625, Learning Rate: 0.005\n",
      "Epoch [3858/20000], Loss: 2024.989501953125, Entropy 228.38995361328125, Learning Rate: 0.005\n",
      "Epoch [3859/20000], Loss: 1621.61865234375, Entropy 222.9959259033203, Learning Rate: 0.005\n",
      "Epoch [3860/20000], Loss: 1401.639404296875, Entropy 252.88417053222656, Learning Rate: 0.005\n",
      "Epoch [3861/20000], Loss: 1697.3338623046875, Entropy 222.41549682617188, Learning Rate: 0.005\n",
      "Epoch [3862/20000], Loss: 1479.489990234375, Entropy 233.67770385742188, Learning Rate: 0.005\n",
      "Epoch [3863/20000], Loss: 1293.648681640625, Entropy 247.61154174804688, Learning Rate: 0.005\n",
      "Epoch [3864/20000], Loss: 1571.8328857421875, Entropy 220.89382934570312, Learning Rate: 0.005\n",
      "Epoch [3865/20000], Loss: 1499.574951171875, Entropy 218.9968719482422, Learning Rate: 0.005\n",
      "Epoch [3866/20000], Loss: 1167.2069091796875, Entropy 219.59580993652344, Learning Rate: 0.005\n",
      "Epoch [3867/20000], Loss: 1866.0252685546875, Entropy 217.62905883789062, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3868/20000], Loss: 1308.193359375, Entropy 198.54042053222656, Learning Rate: 0.005\n",
      "Epoch [3869/20000], Loss: 1260.71923828125, Entropy 214.3968048095703, Learning Rate: 0.005\n",
      "Epoch [3870/20000], Loss: 1529.6492919921875, Entropy 209.705322265625, Learning Rate: 0.005\n",
      "Epoch [3871/20000], Loss: 1210.6195068359375, Entropy 217.93983459472656, Learning Rate: 0.005\n",
      "Epoch [3872/20000], Loss: 1320.5830078125, Entropy 206.86746215820312, Learning Rate: 0.005\n",
      "Epoch [3873/20000], Loss: 1352.504638671875, Entropy 201.84991455078125, Learning Rate: 0.005\n",
      "Epoch [3874/20000], Loss: 1181.5556640625, Entropy 206.48731994628906, Learning Rate: 0.005\n",
      "Epoch [3875/20000], Loss: 1114.2799072265625, Entropy 218.7949981689453, Learning Rate: 0.005\n",
      "Epoch [3876/20000], Loss: 1423.9427490234375, Entropy 197.67835998535156, Learning Rate: 0.005\n",
      "Epoch [3877/20000], Loss: 1397.9127197265625, Entropy 201.23641967773438, Learning Rate: 0.005\n",
      "Epoch [3878/20000], Loss: 1183.3544921875, Entropy 233.58819580078125, Learning Rate: 0.005\n",
      "Epoch [3879/20000], Loss: 1431.85107421875, Entropy 209.7061309814453, Learning Rate: 0.005\n",
      "Epoch [3880/20000], Loss: 1140.96533203125, Entropy 207.12648010253906, Learning Rate: 0.005\n",
      "Epoch [3881/20000], Loss: 1194.718505859375, Entropy 210.9800567626953, Learning Rate: 0.005\n",
      "Epoch [3882/20000], Loss: 1331.349853515625, Entropy 208.1751708984375, Learning Rate: 0.005\n",
      "Epoch [3883/20000], Loss: 1389.197265625, Entropy 204.27371215820312, Learning Rate: 0.005\n",
      "Epoch [3884/20000], Loss: 1214.017578125, Entropy 197.62213134765625, Learning Rate: 0.005\n",
      "Epoch [3885/20000], Loss: 1737.26708984375, Entropy 205.58035278320312, Learning Rate: 0.005\n",
      "Epoch [3886/20000], Loss: 1211.0306396484375, Entropy 205.93397521972656, Learning Rate: 0.005\n",
      "Epoch [3887/20000], Loss: 1838.4219970703125, Entropy 211.96397399902344, Learning Rate: 0.005\n",
      "Epoch [3888/20000], Loss: 1200.095947265625, Entropy 200.8758544921875, Learning Rate: 0.005\n",
      "Epoch [3889/20000], Loss: 1305.87744140625, Entropy 189.3002471923828, Learning Rate: 0.005\n",
      "Epoch [3890/20000], Loss: 1226.66650390625, Entropy 205.03469848632812, Learning Rate: 0.005\n",
      "Epoch [3891/20000], Loss: 1386.048095703125, Entropy 199.13897705078125, Learning Rate: 0.005\n",
      "Epoch [3892/20000], Loss: 1152.99169921875, Entropy 196.75135803222656, Learning Rate: 0.005\n",
      "Epoch [3893/20000], Loss: 1239.1676025390625, Entropy 204.7433624267578, Learning Rate: 0.005\n",
      "Epoch [3894/20000], Loss: 1302.35595703125, Entropy 201.57940673828125, Learning Rate: 0.005\n",
      "Epoch [3895/20000], Loss: 1111.546142578125, Entropy 194.40948486328125, Learning Rate: 0.005\n",
      "Epoch [3896/20000], Loss: 1223.6942138671875, Entropy 200.6986541748047, Learning Rate: 0.005\n",
      "Epoch [3897/20000], Loss: 1193.2025146484375, Entropy 184.81881713867188, Learning Rate: 0.005\n",
      "Epoch [3898/20000], Loss: 1161.2056884765625, Entropy 172.52769470214844, Learning Rate: 0.005\n",
      "Epoch [3899/20000], Loss: 1137.2816162109375, Entropy 186.41769409179688, Learning Rate: 0.005\n",
      "Epoch [3900/20000], Loss: 1173.7359619140625, Entropy 198.2586212158203, Learning Rate: 0.005\n",
      "Epoch [3901/20000], Loss: 1174.495849609375, Entropy 205.3219451904297, Learning Rate: 0.005\n",
      "Epoch [3902/20000], Loss: 1107.2535400390625, Entropy 217.1395721435547, Learning Rate: 0.005\n",
      "Epoch [3903/20000], Loss: 1122.7938232421875, Entropy 213.03672790527344, Learning Rate: 0.005\n",
      "Epoch [3904/20000], Loss: 1142.7587890625, Entropy 194.4779815673828, Learning Rate: 0.005\n",
      "Epoch [3905/20000], Loss: 1114.8638916015625, Entropy 200.9705810546875, Learning Rate: 0.005\n",
      "Epoch [3906/20000], Loss: 1176.2152099609375, Entropy 187.1867218017578, Learning Rate: 0.005\n",
      "Epoch [3907/20000], Loss: 1184.7679443359375, Entropy 194.0312042236328, Learning Rate: 0.005\n",
      "Epoch [3908/20000], Loss: 1071.2916259765625, Entropy 210.24514770507812, Learning Rate: 0.005\n",
      "Epoch [3909/20000], Loss: 1213.4173583984375, Entropy 193.521240234375, Learning Rate: 0.005\n",
      "Epoch [3910/20000], Loss: 1096.78564453125, Entropy 191.22769165039062, Learning Rate: 0.005\n",
      "Epoch [3911/20000], Loss: 1121.1136474609375, Entropy 199.2892608642578, Learning Rate: 0.005\n",
      "Epoch [3912/20000], Loss: 1074.0313720703125, Entropy 196.533447265625, Learning Rate: 0.005\n",
      "Epoch [3913/20000], Loss: 1051.54150390625, Entropy 205.7783966064453, Learning Rate: 0.005\n",
      "Epoch [3914/20000], Loss: 1124.6875, Entropy 189.97817993164062, Learning Rate: 0.005\n",
      "Epoch [3915/20000], Loss: 1061.16650390625, Entropy 192.57424926757812, Learning Rate: 0.005\n",
      "Epoch [3916/20000], Loss: 1104.9613037109375, Entropy 199.99862670898438, Learning Rate: 0.005\n",
      "Epoch [3917/20000], Loss: 1077.8048095703125, Entropy 176.86146545410156, Learning Rate: 0.005\n",
      "Epoch [3918/20000], Loss: 1079.629638671875, Entropy 215.14593505859375, Learning Rate: 0.005\n",
      "Epoch [3919/20000], Loss: 1016.19482421875, Entropy 217.63612365722656, Learning Rate: 0.005\n",
      "Epoch [3920/20000], Loss: 1030.509521484375, Entropy 194.32476806640625, Learning Rate: 0.005\n",
      "Epoch [3921/20000], Loss: 1016.4730224609375, Entropy 181.93663024902344, Learning Rate: 0.005\n",
      "Epoch [3922/20000], Loss: 1028.830078125, Entropy 197.23043823242188, Learning Rate: 0.005\n",
      "Epoch [3923/20000], Loss: 1033.675048828125, Entropy 187.89935302734375, Learning Rate: 0.005\n",
      "Epoch [3924/20000], Loss: 1049.814208984375, Entropy 206.0819091796875, Learning Rate: 0.005\n",
      "Epoch [3925/20000], Loss: 1069.4434814453125, Entropy 195.95252990722656, Learning Rate: 0.005\n",
      "Epoch [3926/20000], Loss: 1073.515380859375, Entropy 180.1443634033203, Learning Rate: 0.005\n",
      "Epoch [3927/20000], Loss: 1030.252197265625, Entropy 187.2349395751953, Learning Rate: 0.005\n",
      "Epoch [3928/20000], Loss: 1025.6219482421875, Entropy 195.75721740722656, Learning Rate: 0.005\n",
      "Epoch [3929/20000], Loss: 1034.95068359375, Entropy 203.99073791503906, Learning Rate: 0.005\n",
      "Epoch [3930/20000], Loss: 986.1766967773438, Entropy 211.32440185546875, Learning Rate: 0.005\n",
      "Epoch [3931/20000], Loss: 1042.6512451171875, Entropy 180.2117156982422, Learning Rate: 0.005\n",
      "Epoch [3932/20000], Loss: 1025.2508544921875, Entropy 193.35940551757812, Learning Rate: 0.005\n",
      "Epoch [3933/20000], Loss: 1036.0714111328125, Entropy 181.7859649658203, Learning Rate: 0.005\n",
      "Epoch [3934/20000], Loss: 1036.786376953125, Entropy 206.10647583007812, Learning Rate: 0.005\n",
      "Epoch [3935/20000], Loss: 1062.58056640625, Entropy 186.90432739257812, Learning Rate: 0.005\n",
      "Epoch [3936/20000], Loss: 1097.04931640625, Entropy 196.7509765625, Learning Rate: 0.005\n",
      "Epoch [3937/20000], Loss: 1014.7810668945312, Entropy 191.1885528564453, Learning Rate: 0.005\n",
      "Epoch [3938/20000], Loss: 1027.2894287109375, Entropy 190.20327758789062, Learning Rate: 0.005\n",
      "Epoch [3939/20000], Loss: 1047.286376953125, Entropy 206.30145263671875, Learning Rate: 0.005\n",
      "Epoch [3940/20000], Loss: 1011.7645874023438, Entropy 214.3072967529297, Learning Rate: 0.005\n",
      "Epoch [3941/20000], Loss: 1015.5084838867188, Entropy 194.0447235107422, Learning Rate: 0.005\n",
      "Epoch [3942/20000], Loss: 1013.3012084960938, Entropy 200.8730010986328, Learning Rate: 0.005\n",
      "Epoch [3943/20000], Loss: 1015.8281860351562, Entropy 213.47332763671875, Learning Rate: 0.005\n",
      "Epoch [3944/20000], Loss: 1057.1805419921875, Entropy 197.5360107421875, Learning Rate: 0.005\n",
      "Epoch [3945/20000], Loss: 1027.6624755859375, Entropy 212.2527313232422, Learning Rate: 0.005\n",
      "Epoch [3946/20000], Loss: 997.8969116210938, Entropy 206.6697540283203, Learning Rate: 0.005\n",
      "Epoch [3947/20000], Loss: 1049.7132568359375, Entropy 199.24549865722656, Learning Rate: 0.005\n",
      "Epoch [3948/20000], Loss: 1039.6153564453125, Entropy 202.49929809570312, Learning Rate: 0.005\n",
      "Epoch [3949/20000], Loss: 1010.4141845703125, Entropy 191.366455078125, Learning Rate: 0.005\n",
      "Epoch [3950/20000], Loss: 1010.2471923828125, Entropy 209.55335998535156, Learning Rate: 0.005\n",
      "Epoch [3951/20000], Loss: 1027.8719482421875, Entropy 200.02804565429688, Learning Rate: 0.005\n",
      "Epoch [3952/20000], Loss: 1098.3052978515625, Entropy 209.12118530273438, Learning Rate: 0.005\n",
      "Epoch [3953/20000], Loss: 993.6502685546875, Entropy 190.4453125, Learning Rate: 0.005\n",
      "Epoch [3954/20000], Loss: 1036.4630126953125, Entropy 214.44322204589844, Learning Rate: 0.005\n",
      "Epoch [3955/20000], Loss: 1028.595458984375, Entropy 201.58587646484375, Learning Rate: 0.005\n",
      "Epoch [3956/20000], Loss: 1037.53564453125, Entropy 213.00840759277344, Learning Rate: 0.005\n",
      "Epoch [3957/20000], Loss: 979.6032104492188, Entropy 219.7546844482422, Learning Rate: 0.005\n",
      "Epoch [3958/20000], Loss: 1076.56201171875, Entropy 204.82745361328125, Learning Rate: 0.005\n",
      "Epoch [3959/20000], Loss: 1074.73779296875, Entropy 207.0686798095703, Learning Rate: 0.005\n",
      "Epoch [3960/20000], Loss: 1033.9998779296875, Entropy 186.73753356933594, Learning Rate: 0.005\n",
      "Epoch [3961/20000], Loss: 998.97265625, Entropy 211.29415893554688, Learning Rate: 0.005\n",
      "Epoch [3962/20000], Loss: 1041.794921875, Entropy 208.83804321289062, Learning Rate: 0.005\n",
      "Epoch [3963/20000], Loss: 1021.5230102539062, Entropy 216.1843719482422, Learning Rate: 0.005\n",
      "Epoch [3964/20000], Loss: 1044.63427734375, Entropy 208.92063903808594, Learning Rate: 0.005\n",
      "Epoch [3965/20000], Loss: 1020.0806884765625, Entropy 193.40187072753906, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3966/20000], Loss: 1022.4191284179688, Entropy 212.7880096435547, Learning Rate: 0.005\n",
      "Epoch [3967/20000], Loss: 1009.0325317382812, Entropy 218.32574462890625, Learning Rate: 0.005\n",
      "Epoch [3968/20000], Loss: 1058.828369140625, Entropy 186.49037170410156, Learning Rate: 0.005\n",
      "Epoch [3969/20000], Loss: 1027.5946044921875, Entropy 226.51451110839844, Learning Rate: 0.005\n",
      "Epoch [3970/20000], Loss: 1075.406005859375, Entropy 210.86474609375, Learning Rate: 0.005\n",
      "Epoch [3971/20000], Loss: 1010.02294921875, Entropy 220.57345581054688, Learning Rate: 0.005\n",
      "Epoch [3972/20000], Loss: 998.611572265625, Entropy 217.15292358398438, Learning Rate: 0.005\n",
      "Epoch [3973/20000], Loss: 1055.5009765625, Entropy 207.20175170898438, Learning Rate: 0.005\n",
      "Epoch [3974/20000], Loss: 1043.95751953125, Entropy 215.14083862304688, Learning Rate: 0.005\n",
      "Epoch [3975/20000], Loss: 1028.89892578125, Entropy 216.0096893310547, Learning Rate: 0.005\n",
      "Epoch [3976/20000], Loss: 1056.0826416015625, Entropy 194.0886688232422, Learning Rate: 0.005\n",
      "Epoch [3977/20000], Loss: 991.3939208984375, Entropy 219.90261840820312, Learning Rate: 0.005\n",
      "Epoch [3978/20000], Loss: 1002.1541748046875, Entropy 209.34580993652344, Learning Rate: 0.005\n",
      "Epoch [3979/20000], Loss: 1056.2301025390625, Entropy 200.85362243652344, Learning Rate: 0.005\n",
      "Epoch [3980/20000], Loss: 1001.5565795898438, Entropy 214.9711456298828, Learning Rate: 0.005\n",
      "Epoch [3981/20000], Loss: 1022.5499267578125, Entropy 218.187255859375, Learning Rate: 0.005\n",
      "Epoch [3982/20000], Loss: 1032.6746826171875, Entropy 212.177734375, Learning Rate: 0.005\n",
      "Epoch [3983/20000], Loss: 978.4390258789062, Entropy 224.6414337158203, Learning Rate: 0.005\n",
      "Epoch [3984/20000], Loss: 1003.1361083984375, Entropy 208.87571716308594, Learning Rate: 0.005\n",
      "Epoch [3985/20000], Loss: 1040.32568359375, Entropy 222.16815185546875, Learning Rate: 0.005\n",
      "Epoch [3986/20000], Loss: 1022.1474609375, Entropy 209.24093627929688, Learning Rate: 0.005\n",
      "Epoch [3987/20000], Loss: 1014.9244384765625, Entropy 212.80213928222656, Learning Rate: 0.005\n",
      "Epoch [3988/20000], Loss: 973.94482421875, Entropy 215.36013793945312, Learning Rate: 0.005\n",
      "Epoch [3989/20000], Loss: 985.039794921875, Entropy 200.05332946777344, Learning Rate: 0.005\n",
      "Epoch [3990/20000], Loss: 998.9651489257812, Entropy 215.82196044921875, Learning Rate: 0.005\n",
      "Epoch [3991/20000], Loss: 991.877197265625, Entropy 215.73399353027344, Learning Rate: 0.005\n",
      "Epoch [3992/20000], Loss: 982.576171875, Entropy 219.208740234375, Learning Rate: 0.005\n",
      "Epoch [3993/20000], Loss: 1018.0077514648438, Entropy 231.0281219482422, Learning Rate: 0.005\n",
      "Epoch [3994/20000], Loss: 1024.2352294921875, Entropy 206.69468688964844, Learning Rate: 0.005\n",
      "Epoch [3995/20000], Loss: 998.9378662109375, Entropy 218.72195434570312, Learning Rate: 0.005\n",
      "Epoch [3996/20000], Loss: 1008.4419555664062, Entropy 218.6663055419922, Learning Rate: 0.005\n",
      "Epoch [3997/20000], Loss: 1012.9419555664062, Entropy 209.6548614501953, Learning Rate: 0.005\n",
      "Epoch [3998/20000], Loss: 1024.6923828125, Entropy 213.17401123046875, Learning Rate: 0.005\n",
      "Epoch [3999/20000], Loss: 1000.87353515625, Entropy 211.23814392089844, Learning Rate: 0.005\n",
      "Epoch [4000/20000], Loss: 1005.6865844726562, Entropy 211.3164825439453, Learning Rate: 0.005\n",
      "Epoch [4001/20000], Loss: 1014.2188110351562, Entropy 212.31182861328125, Learning Rate: 0.005\n",
      "Epoch [4002/20000], Loss: 994.8223876953125, Entropy 197.23190307617188, Learning Rate: 0.005\n",
      "Epoch [4003/20000], Loss: 993.41943359375, Entropy 214.44227600097656, Learning Rate: 0.005\n",
      "Epoch [4004/20000], Loss: 1005.50146484375, Entropy 204.25413513183594, Learning Rate: 0.005\n",
      "Epoch [4005/20000], Loss: 960.8292236328125, Entropy 229.374267578125, Learning Rate: 0.005\n",
      "Epoch [4006/20000], Loss: 1016.9833984375, Entropy 214.12648010253906, Learning Rate: 0.005\n",
      "Epoch [4007/20000], Loss: 1023.2130737304688, Entropy 231.28668212890625, Learning Rate: 0.005\n",
      "Epoch [4008/20000], Loss: 992.0022583007812, Entropy 238.7399139404297, Learning Rate: 0.005\n",
      "Epoch [4009/20000], Loss: 1038.0584716796875, Entropy 212.30894470214844, Learning Rate: 0.005\n",
      "Epoch [4010/20000], Loss: 990.3232421875, Entropy 237.96360778808594, Learning Rate: 0.005\n",
      "Epoch [4011/20000], Loss: 997.5545654296875, Entropy 216.46780395507812, Learning Rate: 0.005\n",
      "Epoch [4012/20000], Loss: 1019.7047729492188, Entropy 228.0580291748047, Learning Rate: 0.005\n",
      "Epoch [4013/20000], Loss: 1063.670654296875, Entropy 224.92718505859375, Learning Rate: 0.005\n",
      "Epoch [4014/20000], Loss: 1001.3295288085938, Entropy 223.4753875732422, Learning Rate: 0.005\n",
      "Epoch [4015/20000], Loss: 986.9251708984375, Entropy 213.29347229003906, Learning Rate: 0.005\n",
      "Epoch [4016/20000], Loss: 997.766357421875, Entropy 229.1649169921875, Learning Rate: 0.005\n",
      "Epoch [4017/20000], Loss: 945.8900146484375, Entropy 235.37745666503906, Learning Rate: 0.005\n",
      "Epoch [4018/20000], Loss: 973.3087158203125, Entropy 227.22303771972656, Learning Rate: 0.005\n",
      "Epoch [4019/20000], Loss: 998.8511962890625, Entropy 197.01431274414062, Learning Rate: 0.005\n",
      "Epoch [4020/20000], Loss: 989.2230224609375, Entropy 219.46359252929688, Learning Rate: 0.005\n",
      "Epoch [4021/20000], Loss: 988.371337890625, Entropy 210.84628295898438, Learning Rate: 0.005\n",
      "Epoch [4022/20000], Loss: 1017.6495361328125, Entropy 202.154296875, Learning Rate: 0.005\n",
      "Epoch [4023/20000], Loss: 975.2774658203125, Entropy 228.390625, Learning Rate: 0.005\n",
      "Epoch [4024/20000], Loss: 1011.412353515625, Entropy 234.60630798339844, Learning Rate: 0.005\n",
      "Epoch [4025/20000], Loss: 1006.001953125, Entropy 210.78125, Learning Rate: 0.005\n",
      "Epoch [4026/20000], Loss: 1015.7111206054688, Entropy 220.30572509765625, Learning Rate: 0.005\n",
      "Epoch [4027/20000], Loss: 980.7906494140625, Entropy 233.10079956054688, Learning Rate: 0.005\n",
      "Epoch [4028/20000], Loss: 996.86865234375, Entropy 210.4495849609375, Learning Rate: 0.005\n",
      "Epoch [4029/20000], Loss: 988.501708984375, Entropy 221.45654296875, Learning Rate: 0.005\n",
      "Epoch [4030/20000], Loss: 1011.8148193359375, Entropy 229.17007446289062, Learning Rate: 0.005\n",
      "Epoch [4031/20000], Loss: 1019.8634033203125, Entropy 211.11593627929688, Learning Rate: 0.005\n",
      "Epoch [4032/20000], Loss: 1051.46728515625, Entropy 228.2371368408203, Learning Rate: 0.005\n",
      "Epoch [4033/20000], Loss: 1053.675048828125, Entropy 219.02764892578125, Learning Rate: 0.005\n",
      "Epoch [4034/20000], Loss: 955.7247314453125, Entropy 238.36526489257812, Learning Rate: 0.005\n",
      "Epoch [4035/20000], Loss: 985.114501953125, Entropy 220.70130920410156, Learning Rate: 0.005\n",
      "Epoch [4036/20000], Loss: 973.4967041015625, Entropy 235.64816284179688, Learning Rate: 0.005\n",
      "Epoch [4037/20000], Loss: 998.8607177734375, Entropy 237.31753540039062, Learning Rate: 0.005\n",
      "Epoch [4038/20000], Loss: 1009.228759765625, Entropy 201.19700622558594, Learning Rate: 0.005\n",
      "Epoch [4039/20000], Loss: 989.7586669921875, Entropy 228.250732421875, Learning Rate: 0.005\n",
      "Epoch [4040/20000], Loss: 979.0809936523438, Entropy 212.86846923828125, Learning Rate: 0.005\n",
      "Epoch [4041/20000], Loss: 982.9382934570312, Entropy 238.5348358154297, Learning Rate: 0.005\n",
      "Epoch [4042/20000], Loss: 1007.3479614257812, Entropy 226.43817138671875, Learning Rate: 0.005\n",
      "Epoch [4043/20000], Loss: 976.7741088867188, Entropy 234.1957550048828, Learning Rate: 0.005\n",
      "Epoch [4044/20000], Loss: 978.1773681640625, Entropy 237.222412109375, Learning Rate: 0.005\n",
      "Epoch [4045/20000], Loss: 1000.1113891601562, Entropy 230.56048583984375, Learning Rate: 0.005\n",
      "Epoch [4046/20000], Loss: 1013.6845703125, Entropy 221.68214416503906, Learning Rate: 0.005\n",
      "Epoch [4047/20000], Loss: 1034.022216796875, Entropy 227.34970092773438, Learning Rate: 0.005\n",
      "Epoch [4048/20000], Loss: 989.2156982421875, Entropy 224.270263671875, Learning Rate: 0.005\n",
      "Epoch [4049/20000], Loss: 1019.0917358398438, Entropy 227.2684783935547, Learning Rate: 0.005\n",
      "Epoch [4050/20000], Loss: 1037.55322265625, Entropy 237.3291473388672, Learning Rate: 0.005\n",
      "Epoch [4051/20000], Loss: 1016.6904296875, Entropy 230.20350646972656, Learning Rate: 0.005\n",
      "Epoch [4052/20000], Loss: 980.8065185546875, Entropy 226.44735717773438, Learning Rate: 0.005\n",
      "Epoch [4053/20000], Loss: 1024.698974609375, Entropy 224.31463623046875, Learning Rate: 0.005\n",
      "Epoch [4054/20000], Loss: 972.6910400390625, Entropy 233.36453247070312, Learning Rate: 0.005\n",
      "Epoch [4055/20000], Loss: 1043.451904296875, Entropy 230.59397888183594, Learning Rate: 0.005\n",
      "Epoch [4056/20000], Loss: 1037.270263671875, Entropy 217.489990234375, Learning Rate: 0.005\n",
      "Epoch [4057/20000], Loss: 1037.9140625, Entropy 205.09112548828125, Learning Rate: 0.005\n",
      "Epoch [4058/20000], Loss: 997.97314453125, Entropy 226.41159057617188, Learning Rate: 0.005\n",
      "Epoch [4059/20000], Loss: 992.2091064453125, Entropy 240.1845703125, Learning Rate: 0.005\n",
      "Epoch [4060/20000], Loss: 996.6468505859375, Entropy 230.51364135742188, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4061/20000], Loss: 1045.27197265625, Entropy 218.52261352539062, Learning Rate: 0.005\n",
      "Epoch [4062/20000], Loss: 996.2943115234375, Entropy 224.26614379882812, Learning Rate: 0.005\n",
      "Epoch [4063/20000], Loss: 982.2598876953125, Entropy 238.23475646972656, Learning Rate: 0.005\n",
      "Epoch [4064/20000], Loss: 976.1802978515625, Entropy 242.03268432617188, Learning Rate: 0.005\n",
      "Epoch [4065/20000], Loss: 1001.00634765625, Entropy 226.47830200195312, Learning Rate: 0.005\n",
      "Epoch [4066/20000], Loss: 994.355712890625, Entropy 223.76089477539062, Learning Rate: 0.005\n",
      "Epoch [4067/20000], Loss: 991.2345581054688, Entropy 233.32904052734375, Learning Rate: 0.005\n",
      "Epoch [4068/20000], Loss: 1037.507080078125, Entropy 229.44903564453125, Learning Rate: 0.005\n",
      "Epoch [4069/20000], Loss: 1067.4461669921875, Entropy 218.7996826171875, Learning Rate: 0.005\n",
      "Epoch [4070/20000], Loss: 974.4405517578125, Entropy 220.08921813964844, Learning Rate: 0.005\n",
      "Epoch [4071/20000], Loss: 998.37548828125, Entropy 240.68502807617188, Learning Rate: 0.005\n",
      "Epoch [4072/20000], Loss: 979.5211181640625, Entropy 233.64987182617188, Learning Rate: 0.005\n",
      "Epoch [4073/20000], Loss: 985.1972045898438, Entropy 230.70184326171875, Learning Rate: 0.005\n",
      "Epoch [4074/20000], Loss: 1033.508544921875, Entropy 239.81539916992188, Learning Rate: 0.005\n",
      "Epoch [4075/20000], Loss: 1003.9209594726562, Entropy 225.09686279296875, Learning Rate: 0.005\n",
      "Epoch [4076/20000], Loss: 1012.7339477539062, Entropy 214.4250946044922, Learning Rate: 0.005\n",
      "Epoch [4077/20000], Loss: 1015.0466918945312, Entropy 248.3778839111328, Learning Rate: 0.005\n",
      "Epoch [4078/20000], Loss: 1045.1942138671875, Entropy 217.54495239257812, Learning Rate: 0.005\n",
      "Epoch [4079/20000], Loss: 984.358154296875, Entropy 227.21437072753906, Learning Rate: 0.005\n",
      "Epoch [4080/20000], Loss: 1026.8817138671875, Entropy 236.2107696533203, Learning Rate: 0.005\n",
      "Epoch [4081/20000], Loss: 967.6556396484375, Entropy 233.57765197753906, Learning Rate: 0.005\n",
      "Epoch [4082/20000], Loss: 1104.3746337890625, Entropy 231.599365234375, Learning Rate: 0.005\n",
      "Epoch [4083/20000], Loss: 1038.2725830078125, Entropy 223.5510711669922, Learning Rate: 0.005\n",
      "Epoch [4084/20000], Loss: 1014.3216552734375, Entropy 244.1923828125, Learning Rate: 0.005\n",
      "Epoch [4085/20000], Loss: 963.69140625, Entropy 239.03836059570312, Learning Rate: 0.005\n",
      "Epoch [4086/20000], Loss: 988.6373291015625, Entropy 234.671875, Learning Rate: 0.005\n",
      "Epoch [4087/20000], Loss: 1032.0003662109375, Entropy 249.52659606933594, Learning Rate: 0.005\n",
      "Epoch [4088/20000], Loss: 1054.4105224609375, Entropy 250.07388305664062, Learning Rate: 0.005\n",
      "Epoch [4089/20000], Loss: 1000.511962890625, Entropy 245.33450317382812, Learning Rate: 0.005\n",
      "Epoch [4090/20000], Loss: 1097.385498046875, Entropy 230.284912109375, Learning Rate: 0.005\n",
      "Epoch [4091/20000], Loss: 1006.5872802734375, Entropy 245.83311462402344, Learning Rate: 0.005\n",
      "Epoch [4092/20000], Loss: 1020.1647338867188, Entropy 220.3408966064453, Learning Rate: 0.005\n",
      "Epoch [4093/20000], Loss: 981.0908203125, Entropy 248.69155883789062, Learning Rate: 0.005\n",
      "Epoch [4094/20000], Loss: 1052.6741943359375, Entropy 231.26998901367188, Learning Rate: 0.005\n",
      "Epoch [4095/20000], Loss: 973.3318481445312, Entropy 243.2927703857422, Learning Rate: 0.005\n",
      "Epoch [4096/20000], Loss: 1002.712646484375, Entropy 226.53065490722656, Learning Rate: 0.005\n",
      "Epoch [4097/20000], Loss: 1018.2662353515625, Entropy 232.81201171875, Learning Rate: 0.005\n",
      "Epoch [4098/20000], Loss: 1034.168701171875, Entropy 225.57952880859375, Learning Rate: 0.005\n",
      "Epoch [4099/20000], Loss: 1021.626708984375, Entropy 228.96788024902344, Learning Rate: 0.005\n",
      "Epoch [4100/20000], Loss: 982.7117309570312, Entropy 243.44024658203125, Learning Rate: 0.005\n",
      "Epoch [4101/20000], Loss: 1006.3057861328125, Entropy 225.16329956054688, Learning Rate: 0.005\n",
      "Epoch [4102/20000], Loss: 977.480712890625, Entropy 248.26625061035156, Learning Rate: 0.005\n",
      "Epoch [4103/20000], Loss: 997.8111572265625, Entropy 233.49951171875, Learning Rate: 0.005\n",
      "Epoch [4104/20000], Loss: 951.2078247070312, Entropy 235.42230224609375, Learning Rate: 0.005\n",
      "Epoch [4105/20000], Loss: 1005.77392578125, Entropy 260.7868957519531, Learning Rate: 0.005\n",
      "Epoch [4106/20000], Loss: 953.92822265625, Entropy 248.33692932128906, Learning Rate: 0.005\n",
      "Epoch [4107/20000], Loss: 981.1264038085938, Entropy 233.2842559814453, Learning Rate: 0.005\n",
      "Epoch [4108/20000], Loss: 989.6514892578125, Entropy 245.66099548339844, Learning Rate: 0.005\n",
      "Epoch [4109/20000], Loss: 978.7561645507812, Entropy 236.3511505126953, Learning Rate: 0.005\n",
      "Epoch [4110/20000], Loss: 994.2908325195312, Entropy 233.24041748046875, Learning Rate: 0.005\n",
      "Epoch [4111/20000], Loss: 979.8223266601562, Entropy 230.92120361328125, Learning Rate: 0.005\n",
      "Epoch [4112/20000], Loss: 1005.6962890625, Entropy 243.81565856933594, Learning Rate: 0.005\n",
      "Epoch [4113/20000], Loss: 1022.4614868164062, Entropy 241.8237762451172, Learning Rate: 0.005\n",
      "Epoch [4114/20000], Loss: 982.05908203125, Entropy 242.5491943359375, Learning Rate: 0.005\n",
      "Epoch [4115/20000], Loss: 987.1929321289062, Entropy 238.5873565673828, Learning Rate: 0.005\n",
      "Epoch [4116/20000], Loss: 962.79931640625, Entropy 245.16238403320312, Learning Rate: 0.005\n",
      "Epoch [4117/20000], Loss: 1008.774169921875, Entropy 238.93409729003906, Learning Rate: 0.005\n",
      "Epoch [4118/20000], Loss: 944.2200317382812, Entropy 247.03656005859375, Learning Rate: 0.005\n",
      "Epoch [4119/20000], Loss: 1071.34716796875, Entropy 237.2460174560547, Learning Rate: 0.005\n",
      "Epoch [4120/20000], Loss: 1011.439208984375, Entropy 245.53675842285156, Learning Rate: 0.005\n",
      "Epoch [4121/20000], Loss: 1025.88671875, Entropy 240.24801635742188, Learning Rate: 0.005\n",
      "Epoch [4122/20000], Loss: 988.5109252929688, Entropy 242.3213653564453, Learning Rate: 0.005\n",
      "Epoch [4123/20000], Loss: 1110.075927734375, Entropy 244.35354614257812, Learning Rate: 0.005\n",
      "Epoch [4124/20000], Loss: 1025.2730712890625, Entropy 233.43276977539062, Learning Rate: 0.005\n",
      "Epoch [4125/20000], Loss: 1012.5372314453125, Entropy 239.27798461914062, Learning Rate: 0.005\n",
      "Epoch [4126/20000], Loss: 1008.7308959960938, Entropy 230.6147003173828, Learning Rate: 0.005\n",
      "Epoch [4127/20000], Loss: 1051.206787109375, Entropy 238.46054077148438, Learning Rate: 0.005\n",
      "Epoch [4128/20000], Loss: 1046.64892578125, Entropy 235.39303588867188, Learning Rate: 0.005\n",
      "Epoch [4129/20000], Loss: 1087.0147705078125, Entropy 236.75669860839844, Learning Rate: 0.005\n",
      "Epoch [4130/20000], Loss: 1002.6676025390625, Entropy 246.69778442382812, Learning Rate: 0.005\n",
      "Epoch [4131/20000], Loss: 1045.724853515625, Entropy 232.2783660888672, Learning Rate: 0.005\n",
      "Epoch [4132/20000], Loss: 1003.8796997070312, Entropy 244.55157470703125, Learning Rate: 0.005\n",
      "Epoch [4133/20000], Loss: 1044.992919921875, Entropy 237.4325408935547, Learning Rate: 0.005\n",
      "Epoch [4134/20000], Loss: 947.8372192382812, Entropy 244.2225799560547, Learning Rate: 0.005\n",
      "Epoch [4135/20000], Loss: 1100.949462890625, Entropy 222.90057373046875, Learning Rate: 0.005\n",
      "Epoch [4136/20000], Loss: 1011.6115112304688, Entropy 237.2470245361328, Learning Rate: 0.005\n",
      "Epoch [4137/20000], Loss: 995.3786010742188, Entropy 252.6570281982422, Learning Rate: 0.005\n",
      "Epoch [4138/20000], Loss: 1050.001220703125, Entropy 234.7380828857422, Learning Rate: 0.005\n",
      "Epoch [4139/20000], Loss: 989.843017578125, Entropy 244.06765747070312, Learning Rate: 0.005\n",
      "Epoch [4140/20000], Loss: 1001.8641967773438, Entropy 255.7733917236328, Learning Rate: 0.005\n",
      "Epoch [4141/20000], Loss: 1020.64208984375, Entropy 242.52633666992188, Learning Rate: 0.005\n",
      "Epoch [4142/20000], Loss: 1025.19970703125, Entropy 223.410888671875, Learning Rate: 0.005\n",
      "Epoch [4143/20000], Loss: 985.93505859375, Entropy 239.51803588867188, Learning Rate: 0.005\n",
      "Epoch [4144/20000], Loss: 1045.212646484375, Entropy 248.275146484375, Learning Rate: 0.005\n",
      "Epoch [4145/20000], Loss: 1003.1857299804688, Entropy 239.8882293701172, Learning Rate: 0.005\n",
      "Epoch [4146/20000], Loss: 1057.657470703125, Entropy 240.33616638183594, Learning Rate: 0.005\n",
      "Epoch [4147/20000], Loss: 1027.7724609375, Entropy 236.49505615234375, Learning Rate: 0.005\n",
      "Epoch [4148/20000], Loss: 1017.6761474609375, Entropy 263.5833435058594, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4149/20000], Loss: 1004.1806640625, Entropy 231.51953125, Learning Rate: 0.005\n",
      "Epoch [4150/20000], Loss: 1182.0726318359375, Entropy 240.47323608398438, Learning Rate: 0.005\n",
      "Epoch [4151/20000], Loss: 1022.269775390625, Entropy 251.39064025878906, Learning Rate: 0.005\n",
      "Epoch [4152/20000], Loss: 1215.4766845703125, Entropy 240.22702026367188, Learning Rate: 0.005\n",
      "Epoch [4153/20000], Loss: 1084.07958984375, Entropy 244.68429565429688, Learning Rate: 0.005\n",
      "Epoch [4154/20000], Loss: 1243.277587890625, Entropy 256.4464416503906, Learning Rate: 0.005\n",
      "Epoch [4155/20000], Loss: 1063.8902587890625, Entropy 242.2842254638672, Learning Rate: 0.005\n",
      "Epoch [4156/20000], Loss: 1461.8201904296875, Entropy 246.39657592773438, Learning Rate: 0.005\n",
      "Epoch [4157/20000], Loss: 1112.8533935546875, Entropy 244.5787811279297, Learning Rate: 0.005\n",
      "Epoch [4158/20000], Loss: 1748.837158203125, Entropy 248.5261993408203, Learning Rate: 0.005\n",
      "Epoch [4159/20000], Loss: 1234.336669921875, Entropy 251.95819091796875, Learning Rate: 0.005\n",
      "Epoch [4160/20000], Loss: 2239.391357421875, Entropy 249.9693145751953, Learning Rate: 0.005\n",
      "Epoch [4161/20000], Loss: 1239.2918701171875, Entropy 232.38233947753906, Learning Rate: 0.005\n",
      "Epoch [4162/20000], Loss: 2750.973876953125, Entropy 242.35223388671875, Learning Rate: 0.005\n",
      "Epoch [4163/20000], Loss: 1503.5438232421875, Entropy 246.10086059570312, Learning Rate: 0.005\n",
      "Epoch [4164/20000], Loss: 3088.606689453125, Entropy 237.17022705078125, Learning Rate: 0.005\n",
      "Epoch [4165/20000], Loss: 1238.5491943359375, Entropy 229.08616638183594, Learning Rate: 0.005\n",
      "Epoch [4166/20000], Loss: 2035.986572265625, Entropy 224.98507690429688, Learning Rate: 0.005\n",
      "Epoch [4167/20000], Loss: 2912.063720703125, Entropy 222.97134399414062, Learning Rate: 0.005\n",
      "Epoch [4168/20000], Loss: 1316.58349609375, Entropy 217.35842895507812, Learning Rate: 0.005\n",
      "Epoch [4169/20000], Loss: 3229.98779296875, Entropy 207.33847045898438, Learning Rate: 0.005\n",
      "Epoch [4170/20000], Loss: 2247.451904296875, Entropy 225.76182556152344, Learning Rate: 0.005\n",
      "Epoch [4171/20000], Loss: 2296.6337890625, Entropy 210.84852600097656, Learning Rate: 0.005\n",
      "Epoch [4172/20000], Loss: 2474.387451171875, Entropy 209.475830078125, Learning Rate: 0.005\n",
      "Epoch [4173/20000], Loss: 1743.30322265625, Entropy 227.6026153564453, Learning Rate: 0.005\n",
      "Epoch [4174/20000], Loss: 2579.940185546875, Entropy 235.7165985107422, Learning Rate: 0.005\n",
      "Epoch [4175/20000], Loss: 1718.500244140625, Entropy 230.0166778564453, Learning Rate: 0.005\n",
      "Epoch [4176/20000], Loss: 1811.4869384765625, Entropy 203.16519165039062, Learning Rate: 0.005\n",
      "Epoch [4177/20000], Loss: 2061.730712890625, Entropy 224.193359375, Learning Rate: 0.005\n",
      "Epoch [4178/20000], Loss: 1594.1915283203125, Entropy 210.51571655273438, Learning Rate: 0.005\n",
      "Epoch [4179/20000], Loss: 1428.512939453125, Entropy 202.45855712890625, Learning Rate: 0.005\n",
      "Epoch [4180/20000], Loss: 1615.7615966796875, Entropy 204.90089416503906, Learning Rate: 0.005\n",
      "Epoch [4181/20000], Loss: 1907.5457763671875, Entropy 192.0797576904297, Learning Rate: 0.005\n",
      "Epoch [4182/20000], Loss: 1165.826904296875, Entropy 206.2078857421875, Learning Rate: 0.005\n",
      "Epoch [4183/20000], Loss: 1650.9554443359375, Entropy 207.89405822753906, Learning Rate: 0.005\n",
      "Epoch [4184/20000], Loss: 1400.577880859375, Entropy 195.7772674560547, Learning Rate: 0.005\n",
      "Epoch [4185/20000], Loss: 1163.6134033203125, Entropy 183.83311462402344, Learning Rate: 0.005\n",
      "Epoch [4186/20000], Loss: 1524.8524169921875, Entropy 180.90341186523438, Learning Rate: 0.005\n",
      "Epoch [4187/20000], Loss: 1288.33544921875, Entropy 189.47962951660156, Learning Rate: 0.005\n",
      "Epoch [4188/20000], Loss: 1215.4232177734375, Entropy 190.476318359375, Learning Rate: 0.005\n",
      "Epoch [4189/20000], Loss: 1324.7025146484375, Entropy 190.52420043945312, Learning Rate: 0.005\n",
      "Epoch [4190/20000], Loss: 1218.3306884765625, Entropy 184.77700805664062, Learning Rate: 0.005\n",
      "Epoch [4191/20000], Loss: 1184.1002197265625, Entropy 179.09481811523438, Learning Rate: 0.005\n",
      "Epoch [4192/20000], Loss: 1308.193359375, Entropy 186.19178771972656, Learning Rate: 0.005\n",
      "Epoch [4193/20000], Loss: 1240.49560546875, Entropy 188.2459259033203, Learning Rate: 0.005\n",
      "Epoch [4194/20000], Loss: 1119.5589599609375, Entropy 176.3838653564453, Learning Rate: 0.005\n",
      "Epoch [4195/20000], Loss: 1078.5914306640625, Entropy 193.6375732421875, Learning Rate: 0.005\n",
      "Epoch [4196/20000], Loss: 1195.2501220703125, Entropy 186.8445281982422, Learning Rate: 0.005\n",
      "Epoch [4197/20000], Loss: 1247.4844970703125, Entropy 183.16441345214844, Learning Rate: 0.005\n",
      "Epoch [4198/20000], Loss: 1041.37646484375, Entropy 189.20831298828125, Learning Rate: 0.005\n",
      "Epoch [4199/20000], Loss: 1170.7872314453125, Entropy 179.13682556152344, Learning Rate: 0.005\n",
      "Epoch [4200/20000], Loss: 1397.63330078125, Entropy 180.14797973632812, Learning Rate: 0.005\n",
      "Epoch [4201/20000], Loss: 1104.9039306640625, Entropy 192.21437072753906, Learning Rate: 0.005\n",
      "Epoch [4202/20000], Loss: 1186.73046875, Entropy 186.87350463867188, Learning Rate: 0.005\n",
      "Epoch [4203/20000], Loss: 1377.6085205078125, Entropy 164.25672912597656, Learning Rate: 0.005\n",
      "Epoch [4204/20000], Loss: 1108.0865478515625, Entropy 178.02023315429688, Learning Rate: 0.005\n",
      "Epoch [4205/20000], Loss: 1207.021728515625, Entropy 180.3697052001953, Learning Rate: 0.005\n",
      "Epoch [4206/20000], Loss: 1165.722900390625, Entropy 195.2598419189453, Learning Rate: 0.005\n",
      "Epoch [4207/20000], Loss: 1228.578125, Entropy 180.96978759765625, Learning Rate: 0.005\n",
      "Epoch [4208/20000], Loss: 1139.7830810546875, Entropy 180.77186584472656, Learning Rate: 0.005\n",
      "Epoch [4209/20000], Loss: 1111.451171875, Entropy 180.7119903564453, Learning Rate: 0.005\n",
      "Epoch [4210/20000], Loss: 1221.418701171875, Entropy 182.88722229003906, Learning Rate: 0.005\n",
      "Epoch [4211/20000], Loss: 1165.638916015625, Entropy 175.15870666503906, Learning Rate: 0.005\n",
      "Epoch [4212/20000], Loss: 1174.618408203125, Entropy 184.32749938964844, Learning Rate: 0.005\n",
      "Epoch [4213/20000], Loss: 1065.369873046875, Entropy 171.01458740234375, Learning Rate: 0.005\n",
      "Epoch [4214/20000], Loss: 1185.8619384765625, Entropy 160.8651123046875, Learning Rate: 0.005\n",
      "Epoch [4215/20000], Loss: 1108.3206787109375, Entropy 181.6817169189453, Learning Rate: 0.005\n",
      "Epoch [4216/20000], Loss: 1036.7630615234375, Entropy 186.69674682617188, Learning Rate: 0.005\n",
      "Epoch [4217/20000], Loss: 1104.668212890625, Entropy 194.40003967285156, Learning Rate: 0.005\n",
      "Epoch [4218/20000], Loss: 1077.073486328125, Entropy 192.54302978515625, Learning Rate: 0.005\n",
      "Epoch [4219/20000], Loss: 1050.9390869140625, Entropy 154.14613342285156, Learning Rate: 0.005\n",
      "Epoch [4220/20000], Loss: 1097.220703125, Entropy 174.98077392578125, Learning Rate: 0.005\n",
      "Epoch [4221/20000], Loss: 1089.4556884765625, Entropy 182.8563995361328, Learning Rate: 0.005\n",
      "Epoch [4222/20000], Loss: 1060.7318115234375, Entropy 170.66116333007812, Learning Rate: 0.005\n",
      "Epoch [4223/20000], Loss: 1034.1104736328125, Entropy 178.2362823486328, Learning Rate: 0.005\n",
      "Epoch [4224/20000], Loss: 1076.8243408203125, Entropy 179.43870544433594, Learning Rate: 0.005\n",
      "Epoch [4225/20000], Loss: 1050.567138671875, Entropy 177.84304809570312, Learning Rate: 0.005\n",
      "Epoch [4226/20000], Loss: 1034.9122314453125, Entropy 169.9010772705078, Learning Rate: 0.005\n",
      "Epoch [4227/20000], Loss: 1067.5360107421875, Entropy 183.9451141357422, Learning Rate: 0.005\n",
      "Epoch [4228/20000], Loss: 1065.189453125, Entropy 178.96487426757812, Learning Rate: 0.005\n",
      "Epoch [4229/20000], Loss: 1040.998291015625, Entropy 184.44227600097656, Learning Rate: 0.005\n",
      "Epoch [4230/20000], Loss: 1030.423095703125, Entropy 188.33839416503906, Learning Rate: 0.005\n",
      "Epoch [4231/20000], Loss: 1073.087890625, Entropy 165.0980682373047, Learning Rate: 0.005\n",
      "Epoch [4232/20000], Loss: 1014.5106201171875, Entropy 188.87179565429688, Learning Rate: 0.0025\n",
      "Epoch [4233/20000], Loss: 1014.6619873046875, Entropy 192.23898315429688, Learning Rate: 0.0025\n",
      "Epoch [4234/20000], Loss: 1059.4755859375, Entropy 163.61212158203125, Learning Rate: 0.0025\n",
      "Epoch [4235/20000], Loss: 1051.03857421875, Entropy 182.7001953125, Learning Rate: 0.0025\n",
      "Epoch [4236/20000], Loss: 1008.1202392578125, Entropy 195.16238403320312, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4237/20000], Loss: 1020.7508544921875, Entropy 184.9061279296875, Learning Rate: 0.0025\n",
      "Epoch [4238/20000], Loss: 983.387451171875, Entropy 180.91238403320312, Learning Rate: 0.0025\n",
      "Epoch [4239/20000], Loss: 1037.32421875, Entropy 178.53952026367188, Learning Rate: 0.0025\n",
      "Epoch [4240/20000], Loss: 1021.176513671875, Entropy 184.6883544921875, Learning Rate: 0.0025\n",
      "Epoch [4241/20000], Loss: 1043.183349609375, Entropy 169.4087677001953, Learning Rate: 0.0025\n",
      "Epoch [4242/20000], Loss: 1011.4706420898438, Entropy 189.8594512939453, Learning Rate: 0.0025\n",
      "Epoch [4243/20000], Loss: 1027.3907470703125, Entropy 176.47874450683594, Learning Rate: 0.0025\n",
      "Epoch [4244/20000], Loss: 1067.200927734375, Entropy 195.46331787109375, Learning Rate: 0.0025\n",
      "Epoch [4245/20000], Loss: 1008.71533203125, Entropy 185.80819702148438, Learning Rate: 0.0025\n",
      "Epoch [4246/20000], Loss: 1017.8330688476562, Entropy 186.5592498779297, Learning Rate: 0.0025\n",
      "Epoch [4247/20000], Loss: 1017.94580078125, Entropy 200.74220275878906, Learning Rate: 0.0025\n",
      "Epoch [4248/20000], Loss: 1037.00537109375, Entropy 174.74459838867188, Learning Rate: 0.0025\n",
      "Epoch [4249/20000], Loss: 1014.7478637695312, Entropy 184.7381134033203, Learning Rate: 0.0025\n",
      "Epoch [4250/20000], Loss: 1032.7027587890625, Entropy 172.6533203125, Learning Rate: 0.0025\n",
      "Epoch [4251/20000], Loss: 1036.43896484375, Entropy 178.23760986328125, Learning Rate: 0.0025\n",
      "Epoch [4252/20000], Loss: 1055.9874267578125, Entropy 170.97911071777344, Learning Rate: 0.0025\n",
      "Epoch [4253/20000], Loss: 1042.9534912109375, Entropy 187.0002899169922, Learning Rate: 0.0025\n",
      "Epoch [4254/20000], Loss: 1032.3763427734375, Entropy 164.2018280029297, Learning Rate: 0.0025\n",
      "Epoch [4255/20000], Loss: 1009.0155639648438, Entropy 176.3649444580078, Learning Rate: 0.0025\n",
      "Epoch [4256/20000], Loss: 1007.7764282226562, Entropy 184.5629119873047, Learning Rate: 0.0025\n",
      "Epoch [4257/20000], Loss: 1041.084716796875, Entropy 179.2736053466797, Learning Rate: 0.0025\n",
      "Epoch [4258/20000], Loss: 993.032958984375, Entropy 195.52308654785156, Learning Rate: 0.0025\n",
      "Epoch [4259/20000], Loss: 1053.4962158203125, Entropy 166.57569885253906, Learning Rate: 0.0025\n",
      "Epoch [4260/20000], Loss: 1017.8455810546875, Entropy 189.03968811035156, Learning Rate: 0.0025\n",
      "Epoch [4261/20000], Loss: 1045.90869140625, Entropy 175.7345733642578, Learning Rate: 0.0025\n",
      "Epoch [4262/20000], Loss: 1080.3450927734375, Entropy 192.1457061767578, Learning Rate: 0.0025\n",
      "Epoch [4263/20000], Loss: 996.8218994140625, Entropy 199.358154296875, Learning Rate: 0.0025\n",
      "Epoch [4264/20000], Loss: 1002.598388671875, Entropy 190.84619140625, Learning Rate: 0.0025\n",
      "Epoch [4265/20000], Loss: 992.6596069335938, Entropy 191.3090057373047, Learning Rate: 0.0025\n",
      "Epoch [4266/20000], Loss: 1065.6907958984375, Entropy 189.7425079345703, Learning Rate: 0.0025\n",
      "Epoch [4267/20000], Loss: 1004.8767700195312, Entropy 186.9806671142578, Learning Rate: 0.0025\n",
      "Epoch [4268/20000], Loss: 1007.388427734375, Entropy 194.16151428222656, Learning Rate: 0.0025\n",
      "Epoch [4269/20000], Loss: 1042.336181640625, Entropy 191.33737182617188, Learning Rate: 0.0025\n",
      "Epoch [4270/20000], Loss: 1032.684326171875, Entropy 195.60411071777344, Learning Rate: 0.0025\n",
      "Epoch [4271/20000], Loss: 1021.8450317382812, Entropy 190.93389892578125, Learning Rate: 0.0025\n",
      "Epoch [4272/20000], Loss: 987.441650390625, Entropy 194.4427490234375, Learning Rate: 0.0025\n",
      "Epoch [4273/20000], Loss: 997.63720703125, Entropy 188.86585998535156, Learning Rate: 0.0025\n",
      "Epoch [4274/20000], Loss: 1042.6041259765625, Entropy 175.9767608642578, Learning Rate: 0.0025\n",
      "Epoch [4275/20000], Loss: 1016.61083984375, Entropy 187.7880859375, Learning Rate: 0.0025\n",
      "Epoch [4276/20000], Loss: 1032.7435302734375, Entropy 195.517578125, Learning Rate: 0.0025\n",
      "Epoch [4277/20000], Loss: 1016.7443237304688, Entropy 189.79730224609375, Learning Rate: 0.0025\n",
      "Epoch [4278/20000], Loss: 999.9563598632812, Entropy 187.81890869140625, Learning Rate: 0.0025\n",
      "Epoch [4279/20000], Loss: 991.4496459960938, Entropy 200.1925811767578, Learning Rate: 0.0025\n",
      "Epoch [4280/20000], Loss: 1005.6622924804688, Entropy 188.57159423828125, Learning Rate: 0.0025\n",
      "Epoch [4281/20000], Loss: 1027.4818115234375, Entropy 189.78298950195312, Learning Rate: 0.0025\n",
      "Epoch [4282/20000], Loss: 1039.646240234375, Entropy 186.02804565429688, Learning Rate: 0.0025\n",
      "Epoch [4283/20000], Loss: 996.2333984375, Entropy 211.90750122070312, Learning Rate: 0.0025\n",
      "Epoch [4284/20000], Loss: 984.2628173828125, Entropy 193.93661499023438, Learning Rate: 0.0025\n",
      "Epoch [4285/20000], Loss: 1043.60986328125, Entropy 171.9354248046875, Learning Rate: 0.0025\n",
      "Epoch [4286/20000], Loss: 1027.820556640625, Entropy 190.49945068359375, Learning Rate: 0.0025\n",
      "Epoch [4287/20000], Loss: 1032.244873046875, Entropy 192.0941925048828, Learning Rate: 0.0025\n",
      "Epoch [4288/20000], Loss: 1021.0303955078125, Entropy 182.68020629882812, Learning Rate: 0.0025\n",
      "Epoch [4289/20000], Loss: 1055.9931640625, Entropy 192.5543670654297, Learning Rate: 0.0025\n",
      "Epoch [4290/20000], Loss: 988.382568359375, Entropy 201.4930419921875, Learning Rate: 0.0025\n",
      "Epoch [4291/20000], Loss: 1013.513427734375, Entropy 203.6248779296875, Learning Rate: 0.0025\n",
      "Epoch [4292/20000], Loss: 1032.77880859375, Entropy 183.90670776367188, Learning Rate: 0.0025\n",
      "Epoch [4293/20000], Loss: 1000.3523559570312, Entropy 190.72882080078125, Learning Rate: 0.0025\n",
      "Epoch [4294/20000], Loss: 1044.1884765625, Entropy 174.06695556640625, Learning Rate: 0.0025\n",
      "Epoch [4295/20000], Loss: 1022.972412109375, Entropy 203.444091796875, Learning Rate: 0.0025\n",
      "Epoch [4296/20000], Loss: 1009.9727783203125, Entropy 201.72866821289062, Learning Rate: 0.0025\n",
      "Epoch [4297/20000], Loss: 1052.341552734375, Entropy 182.11819458007812, Learning Rate: 0.0025\n",
      "Epoch [4298/20000], Loss: 1015.090087890625, Entropy 186.353271484375, Learning Rate: 0.0025\n",
      "Epoch [4299/20000], Loss: 1023.0964965820312, Entropy 183.25579833984375, Learning Rate: 0.0025\n",
      "Epoch [4300/20000], Loss: 1042.3302001953125, Entropy 174.1574249267578, Learning Rate: 0.0025\n",
      "Epoch [4301/20000], Loss: 1044.606201171875, Entropy 187.9621124267578, Learning Rate: 0.0025\n",
      "Epoch [4302/20000], Loss: 1043.927490234375, Entropy 181.43663024902344, Learning Rate: 0.0025\n",
      "Epoch [4303/20000], Loss: 978.8377685546875, Entropy 198.28292846679688, Learning Rate: 0.0025\n",
      "Epoch [4304/20000], Loss: 992.5612182617188, Entropy 196.93414306640625, Learning Rate: 0.0025\n",
      "Epoch [4305/20000], Loss: 980.5907592773438, Entropy 210.87933349609375, Learning Rate: 0.0025\n",
      "Epoch [4306/20000], Loss: 1018.1730346679688, Entropy 196.94073486328125, Learning Rate: 0.0025\n",
      "Epoch [4307/20000], Loss: 1021.16162109375, Entropy 187.28623962402344, Learning Rate: 0.0025\n",
      "Epoch [4308/20000], Loss: 1007.3551635742188, Entropy 191.0368194580078, Learning Rate: 0.0025\n",
      "Epoch [4309/20000], Loss: 1028.7930908203125, Entropy 203.3533477783203, Learning Rate: 0.0025\n",
      "Epoch [4310/20000], Loss: 1065.7027587890625, Entropy 200.8600311279297, Learning Rate: 0.0025\n",
      "Epoch [4311/20000], Loss: 1037.30126953125, Entropy 196.60980224609375, Learning Rate: 0.0025\n",
      "Epoch [4312/20000], Loss: 1005.0137939453125, Entropy 209.799560546875, Learning Rate: 0.0025\n",
      "Epoch [4313/20000], Loss: 1014.5195922851562, Entropy 185.4276885986328, Learning Rate: 0.0025\n",
      "Epoch [4314/20000], Loss: 986.0013427734375, Entropy 202.26145935058594, Learning Rate: 0.0025\n",
      "Epoch [4315/20000], Loss: 1001.9703369140625, Entropy 190.94456481933594, Learning Rate: 0.0025\n",
      "Epoch [4316/20000], Loss: 1008.4683227539062, Entropy 202.7450408935547, Learning Rate: 0.0025\n",
      "Epoch [4317/20000], Loss: 1052.4052734375, Entropy 182.14230346679688, Learning Rate: 0.0025\n",
      "Epoch [4318/20000], Loss: 1000.9989624023438, Entropy 201.89019775390625, Learning Rate: 0.0025\n",
      "Epoch [4319/20000], Loss: 979.5235595703125, Entropy 205.78421020507812, Learning Rate: 0.0025\n",
      "Epoch [4320/20000], Loss: 999.0416870117188, Entropy 187.3072967529297, Learning Rate: 0.0025\n",
      "Epoch [4321/20000], Loss: 973.7484741210938, Entropy 205.2737274169922, Learning Rate: 0.0025\n",
      "Epoch [4322/20000], Loss: 974.9058837890625, Entropy 211.482421875, Learning Rate: 0.0025\n",
      "Epoch [4323/20000], Loss: 1038.154541015625, Entropy 196.65875244140625, Learning Rate: 0.0025\n",
      "Epoch [4324/20000], Loss: 997.3308715820312, Entropy 197.62994384765625, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4325/20000], Loss: 1016.0621337890625, Entropy 200.21205139160156, Learning Rate: 0.0025\n",
      "Epoch [4326/20000], Loss: 993.7881469726562, Entropy 190.5340118408203, Learning Rate: 0.0025\n",
      "Epoch [4327/20000], Loss: 1015.8407592773438, Entropy 204.9382781982422, Learning Rate: 0.0025\n",
      "Epoch [4328/20000], Loss: 1001.802734375, Entropy 197.8372802734375, Learning Rate: 0.0025\n",
      "Epoch [4329/20000], Loss: 985.9127807617188, Entropy 198.2969512939453, Learning Rate: 0.0025\n",
      "Epoch [4330/20000], Loss: 970.8760986328125, Entropy 208.45278930664062, Learning Rate: 0.0025\n",
      "Epoch [4331/20000], Loss: 1015.3389282226562, Entropy 200.04205322265625, Learning Rate: 0.0025\n",
      "Epoch [4332/20000], Loss: 1059.1514892578125, Entropy 188.3898162841797, Learning Rate: 0.0025\n",
      "Epoch [4333/20000], Loss: 984.3579711914062, Entropy 199.33355712890625, Learning Rate: 0.0025\n",
      "Epoch [4334/20000], Loss: 1005.6572875976562, Entropy 193.5027313232422, Learning Rate: 0.0025\n",
      "Epoch [4335/20000], Loss: 1003.5098876953125, Entropy 205.15841674804688, Learning Rate: 0.0025\n",
      "Epoch [4336/20000], Loss: 978.3619384765625, Entropy 210.0076904296875, Learning Rate: 0.0025\n",
      "Epoch [4337/20000], Loss: 1015.5338134765625, Entropy 200.21241760253906, Learning Rate: 0.0025\n",
      "Epoch [4338/20000], Loss: 995.4596557617188, Entropy 207.59283447265625, Learning Rate: 0.0025\n",
      "Epoch [4339/20000], Loss: 1018.5771484375, Entropy 204.46473693847656, Learning Rate: 0.0025\n",
      "Epoch [4340/20000], Loss: 958.1573486328125, Entropy 219.73062133789062, Learning Rate: 0.0025\n",
      "Epoch [4341/20000], Loss: 979.3712158203125, Entropy 219.28721618652344, Learning Rate: 0.0025\n",
      "Epoch [4342/20000], Loss: 971.7180786132812, Entropy 196.6141815185547, Learning Rate: 0.0025\n",
      "Epoch [4343/20000], Loss: 1028.681884765625, Entropy 188.52618408203125, Learning Rate: 0.0025\n",
      "Epoch [4344/20000], Loss: 994.4102172851562, Entropy 197.6237030029297, Learning Rate: 0.0025\n",
      "Epoch [4345/20000], Loss: 1016.3595581054688, Entropy 209.38140869140625, Learning Rate: 0.0025\n",
      "Epoch [4346/20000], Loss: 1010.5599975585938, Entropy 199.16217041015625, Learning Rate: 0.0025\n",
      "Epoch [4347/20000], Loss: 995.8635864257812, Entropy 203.3478546142578, Learning Rate: 0.0025\n",
      "Epoch [4348/20000], Loss: 993.2852172851562, Entropy 213.5152130126953, Learning Rate: 0.0025\n",
      "Epoch [4349/20000], Loss: 1012.3875122070312, Entropy 196.3082733154297, Learning Rate: 0.0025\n",
      "Epoch [4350/20000], Loss: 1018.802734375, Entropy 186.7000732421875, Learning Rate: 0.0025\n",
      "Epoch [4351/20000], Loss: 995.4166259765625, Entropy 201.25315856933594, Learning Rate: 0.0025\n",
      "Epoch [4352/20000], Loss: 967.4346313476562, Entropy 221.0554656982422, Learning Rate: 0.0025\n",
      "Epoch [4353/20000], Loss: 1003.7947387695312, Entropy 194.73919677734375, Learning Rate: 0.0025\n",
      "Epoch [4354/20000], Loss: 1028.4964599609375, Entropy 197.19894409179688, Learning Rate: 0.0025\n",
      "Epoch [4355/20000], Loss: 982.4324951171875, Entropy 224.739013671875, Learning Rate: 0.0025\n",
      "Epoch [4356/20000], Loss: 996.04833984375, Entropy 194.47474670410156, Learning Rate: 0.0025\n",
      "Epoch [4357/20000], Loss: 1027.2650146484375, Entropy 190.77842712402344, Learning Rate: 0.0025\n",
      "Epoch [4358/20000], Loss: 982.830322265625, Entropy 217.27769470214844, Learning Rate: 0.0025\n",
      "Epoch [4359/20000], Loss: 1009.0655517578125, Entropy 212.60128784179688, Learning Rate: 0.0025\n",
      "Epoch [4360/20000], Loss: 1026.4224853515625, Entropy 190.6326446533203, Learning Rate: 0.0025\n",
      "Epoch [4361/20000], Loss: 1021.0068359375, Entropy 218.45323181152344, Learning Rate: 0.0025\n",
      "Epoch [4362/20000], Loss: 977.3681030273438, Entropy 204.65167236328125, Learning Rate: 0.0025\n",
      "Epoch [4363/20000], Loss: 1008.5302734375, Entropy 206.50491333007812, Learning Rate: 0.0025\n",
      "Epoch [4364/20000], Loss: 987.2407836914062, Entropy 207.1548614501953, Learning Rate: 0.0025\n",
      "Epoch [4365/20000], Loss: 988.8968505859375, Entropy 214.21414184570312, Learning Rate: 0.0025\n",
      "Epoch [4366/20000], Loss: 1006.4813842773438, Entropy 209.4181365966797, Learning Rate: 0.0025\n",
      "Epoch [4367/20000], Loss: 981.4677734375, Entropy 219.78746032714844, Learning Rate: 0.0025\n",
      "Epoch [4368/20000], Loss: 1032.318359375, Entropy 186.75936889648438, Learning Rate: 0.0025\n",
      "Epoch [4369/20000], Loss: 1002.9929809570312, Entropy 209.65728759765625, Learning Rate: 0.0025\n",
      "Epoch [4370/20000], Loss: 1012.2451782226562, Entropy 213.60052490234375, Learning Rate: 0.0025\n",
      "Epoch [4371/20000], Loss: 1006.410400390625, Entropy 207.59420776367188, Learning Rate: 0.0025\n",
      "Epoch [4372/20000], Loss: 996.0271606445312, Entropy 209.3324432373047, Learning Rate: 0.0025\n",
      "Epoch [4373/20000], Loss: 998.6517333984375, Entropy 205.78355407714844, Learning Rate: 0.0025\n",
      "Epoch [4374/20000], Loss: 981.2936401367188, Entropy 205.97503662109375, Learning Rate: 0.0025\n",
      "Epoch [4375/20000], Loss: 1045.9345703125, Entropy 194.46417236328125, Learning Rate: 0.0025\n",
      "Epoch [4376/20000], Loss: 975.2498779296875, Entropy 218.67762756347656, Learning Rate: 0.0025\n",
      "Epoch [4377/20000], Loss: 1023.7705688476562, Entropy 220.7045440673828, Learning Rate: 0.0025\n",
      "Epoch [4378/20000], Loss: 1017.819091796875, Entropy 195.86024475097656, Learning Rate: 0.0025\n",
      "Epoch [4379/20000], Loss: 1021.49365234375, Entropy 211.32054138183594, Learning Rate: 0.0025\n",
      "Epoch [4380/20000], Loss: 1019.4107666015625, Entropy 214.9276123046875, Learning Rate: 0.0025\n",
      "Epoch [4381/20000], Loss: 976.5126953125, Entropy 203.62913513183594, Learning Rate: 0.0025\n",
      "Epoch [4382/20000], Loss: 1003.03466796875, Entropy 218.55567932128906, Learning Rate: 0.0025\n",
      "Epoch [4383/20000], Loss: 1016.1780395507812, Entropy 212.8821563720703, Learning Rate: 0.0025\n",
      "Epoch [4384/20000], Loss: 999.126708984375, Entropy 210.30409240722656, Learning Rate: 0.0025\n",
      "Epoch [4385/20000], Loss: 1014.2067260742188, Entropy 203.7797393798828, Learning Rate: 0.0025\n",
      "Epoch [4386/20000], Loss: 964.186767578125, Entropy 215.02749633789062, Learning Rate: 0.0025\n",
      "Epoch [4387/20000], Loss: 992.248779296875, Entropy 233.55897521972656, Learning Rate: 0.0025\n",
      "Epoch [4388/20000], Loss: 990.4273681640625, Entropy 210.80712890625, Learning Rate: 0.0025\n",
      "Epoch [4389/20000], Loss: 992.855224609375, Entropy 207.11550903320312, Learning Rate: 0.0025\n",
      "Epoch [4390/20000], Loss: 989.7739868164062, Entropy 209.5955352783203, Learning Rate: 0.0025\n",
      "Epoch [4391/20000], Loss: 1029.3974609375, Entropy 212.6875762939453, Learning Rate: 0.0025\n",
      "Epoch [4392/20000], Loss: 1002.431640625, Entropy 202.90049743652344, Learning Rate: 0.0025\n",
      "Epoch [4393/20000], Loss: 963.5318603515625, Entropy 198.27162170410156, Learning Rate: 0.0025\n",
      "Epoch [4394/20000], Loss: 998.3121337890625, Entropy 213.80953979492188, Learning Rate: 0.0025\n",
      "Epoch [4395/20000], Loss: 996.5311279296875, Entropy 209.54624938964844, Learning Rate: 0.0025\n",
      "Epoch [4396/20000], Loss: 969.6012573242188, Entropy 222.12615966796875, Learning Rate: 0.0025\n",
      "Epoch [4397/20000], Loss: 965.7174682617188, Entropy 214.01690673828125, Learning Rate: 0.0025\n",
      "Epoch [4398/20000], Loss: 1015.69580078125, Entropy 221.44764709472656, Learning Rate: 0.0025\n",
      "Epoch [4399/20000], Loss: 1006.8563232421875, Entropy 195.749267578125, Learning Rate: 0.0025\n",
      "Epoch [4400/20000], Loss: 1020.5860595703125, Entropy 216.66183471679688, Learning Rate: 0.0025\n",
      "Epoch [4401/20000], Loss: 1073.4521484375, Entropy 211.53384399414062, Learning Rate: 0.0025\n",
      "Epoch [4402/20000], Loss: 980.4176025390625, Entropy 218.46875, Learning Rate: 0.0025\n",
      "Epoch [4403/20000], Loss: 998.0679321289062, Entropy 209.0266571044922, Learning Rate: 0.0025\n",
      "Epoch [4404/20000], Loss: 986.50927734375, Entropy 204.04244995117188, Learning Rate: 0.0025\n",
      "Epoch [4405/20000], Loss: 976.4205932617188, Entropy 232.1490936279297, Learning Rate: 0.0025\n",
      "Epoch [4406/20000], Loss: 1013.6217651367188, Entropy 212.26116943359375, Learning Rate: 0.0025\n",
      "Epoch [4407/20000], Loss: 1048.52490234375, Entropy 209.10113525390625, Learning Rate: 0.0025\n",
      "Epoch [4408/20000], Loss: 1002.0865478515625, Entropy 216.42459106445312, Learning Rate: 0.0025\n",
      "Epoch [4409/20000], Loss: 974.7532348632812, Entropy 213.0357208251953, Learning Rate: 0.0025\n",
      "Epoch [4410/20000], Loss: 986.2706298828125, Entropy 213.14718627929688, Learning Rate: 0.0025\n",
      "Epoch [4411/20000], Loss: 1019.076416015625, Entropy 195.42713928222656, Learning Rate: 0.0025\n",
      "Epoch [4412/20000], Loss: 1010.6032104492188, Entropy 189.84210205078125, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4413/20000], Loss: 1007.1608276367188, Entropy 234.3593292236328, Learning Rate: 0.0025\n",
      "Epoch [4414/20000], Loss: 1025.7388916015625, Entropy 212.00686645507812, Learning Rate: 0.0025\n",
      "Epoch [4415/20000], Loss: 1010.9155883789062, Entropy 225.87615966796875, Learning Rate: 0.0025\n",
      "Epoch [4416/20000], Loss: 1008.1356811523438, Entropy 209.39849853515625, Learning Rate: 0.0025\n",
      "Epoch [4417/20000], Loss: 990.4627685546875, Entropy 212.06393432617188, Learning Rate: 0.0025\n",
      "Epoch [4418/20000], Loss: 966.6605834960938, Entropy 223.2185821533203, Learning Rate: 0.0025\n",
      "Epoch [4419/20000], Loss: 1002.588623046875, Entropy 230.4754638671875, Learning Rate: 0.0025\n",
      "Epoch [4420/20000], Loss: 995.572265625, Entropy 209.65115356445312, Learning Rate: 0.0025\n",
      "Epoch [4421/20000], Loss: 954.0323486328125, Entropy 232.23558044433594, Learning Rate: 0.0025\n",
      "Epoch [4422/20000], Loss: 980.5227661132812, Entropy 229.9013214111328, Learning Rate: 0.0025\n",
      "Epoch [4423/20000], Loss: 1018.6448974609375, Entropy 225.38479614257812, Learning Rate: 0.0025\n",
      "Epoch [4424/20000], Loss: 976.9127807617188, Entropy 210.2786407470703, Learning Rate: 0.0025\n",
      "Epoch [4425/20000], Loss: 978.3890380859375, Entropy 207.568115234375, Learning Rate: 0.0025\n",
      "Epoch [4426/20000], Loss: 1034.0340576171875, Entropy 221.44541931152344, Learning Rate: 0.0025\n",
      "Epoch [4427/20000], Loss: 965.3208618164062, Entropy 234.8026885986328, Learning Rate: 0.0025\n",
      "Epoch [4428/20000], Loss: 994.2481079101562, Entropy 195.3962860107422, Learning Rate: 0.0025\n",
      "Epoch [4429/20000], Loss: 992.9695434570312, Entropy 234.5428924560547, Learning Rate: 0.0025\n",
      "Epoch [4430/20000], Loss: 997.7476196289062, Entropy 207.5787811279297, Learning Rate: 0.0025\n",
      "Epoch [4431/20000], Loss: 1018.8555908203125, Entropy 211.2503662109375, Learning Rate: 0.0025\n",
      "Epoch [4432/20000], Loss: 964.279541015625, Entropy 227.36190795898438, Learning Rate: 0.0025\n",
      "Epoch [4433/20000], Loss: 961.126708984375, Entropy 220.2996826171875, Learning Rate: 0.0025\n",
      "Epoch [4434/20000], Loss: 1031.7867431640625, Entropy 220.37620544433594, Learning Rate: 0.0025\n",
      "Epoch [4435/20000], Loss: 1036.9493408203125, Entropy 222.3363037109375, Learning Rate: 0.0025\n",
      "Epoch [4436/20000], Loss: 1010.701416015625, Entropy 221.75062561035156, Learning Rate: 0.0025\n",
      "Epoch [4437/20000], Loss: 1015.759033203125, Entropy 209.11373901367188, Learning Rate: 0.0025\n",
      "Epoch [4438/20000], Loss: 993.9575805664062, Entropy 219.7694549560547, Learning Rate: 0.0025\n",
      "Epoch [4439/20000], Loss: 939.2927856445312, Entropy 232.9021759033203, Learning Rate: 0.0025\n",
      "Epoch [4440/20000], Loss: 1011.9837036132812, Entropy 212.7073516845703, Learning Rate: 0.0025\n",
      "Epoch [4441/20000], Loss: 991.82763671875, Entropy 217.97865295410156, Learning Rate: 0.0025\n",
      "Epoch [4442/20000], Loss: 967.9464721679688, Entropy 220.80401611328125, Learning Rate: 0.0025\n",
      "Epoch [4443/20000], Loss: 1038.2581787109375, Entropy 215.5751190185547, Learning Rate: 0.0025\n",
      "Epoch [4444/20000], Loss: 970.7821655273438, Entropy 223.1190643310547, Learning Rate: 0.0025\n",
      "Epoch [4445/20000], Loss: 988.135498046875, Entropy 227.36585998535156, Learning Rate: 0.0025\n",
      "Epoch [4446/20000], Loss: 958.96484375, Entropy 226.60899353027344, Learning Rate: 0.0025\n",
      "Epoch [4447/20000], Loss: 979.8258056640625, Entropy 213.31948852539062, Learning Rate: 0.0025\n",
      "Epoch [4448/20000], Loss: 961.3912353515625, Entropy 225.68502807617188, Learning Rate: 0.0025\n",
      "Epoch [4449/20000], Loss: 971.7032470703125, Entropy 232.74166870117188, Learning Rate: 0.0025\n",
      "Epoch [4450/20000], Loss: 999.0272827148438, Entropy 235.4205780029297, Learning Rate: 0.0025\n",
      "Epoch [4451/20000], Loss: 987.4761962890625, Entropy 221.798583984375, Learning Rate: 0.0025\n",
      "Epoch [4452/20000], Loss: 974.97314453125, Entropy 236.63595581054688, Learning Rate: 0.0025\n",
      "Epoch [4453/20000], Loss: 979.314697265625, Entropy 229.69837951660156, Learning Rate: 0.0025\n",
      "Epoch [4454/20000], Loss: 972.15576171875, Entropy 238.21409606933594, Learning Rate: 0.0025\n",
      "Epoch [4455/20000], Loss: 985.589599609375, Entropy 218.70188903808594, Learning Rate: 0.0025\n",
      "Epoch [4456/20000], Loss: 975.0433349609375, Entropy 223.27659606933594, Learning Rate: 0.0025\n",
      "Epoch [4457/20000], Loss: 998.8707885742188, Entropy 233.4625701904297, Learning Rate: 0.0025\n",
      "Epoch [4458/20000], Loss: 983.3118286132812, Entropy 220.9735870361328, Learning Rate: 0.0025\n",
      "Epoch [4459/20000], Loss: 989.2977294921875, Entropy 224.13699340820312, Learning Rate: 0.0025\n",
      "Epoch [4460/20000], Loss: 1075.36669921875, Entropy 230.3719940185547, Learning Rate: 0.0025\n",
      "Epoch [4461/20000], Loss: 1032.0350341796875, Entropy 218.65383911132812, Learning Rate: 0.0025\n",
      "Epoch [4462/20000], Loss: 1019.3482666015625, Entropy 222.45706176757812, Learning Rate: 0.0025\n",
      "Epoch [4463/20000], Loss: 974.9886474609375, Entropy 220.48507690429688, Learning Rate: 0.0025\n",
      "Epoch [4464/20000], Loss: 963.6353759765625, Entropy 225.46446228027344, Learning Rate: 0.0025\n",
      "Epoch [4465/20000], Loss: 1007.60400390625, Entropy 227.38331604003906, Learning Rate: 0.0025\n",
      "Epoch [4466/20000], Loss: 1013.2230224609375, Entropy 224.52296447753906, Learning Rate: 0.0025\n",
      "Epoch [4467/20000], Loss: 961.2275390625, Entropy 228.85256958007812, Learning Rate: 0.0025\n",
      "Epoch [4468/20000], Loss: 986.4794921875, Entropy 239.08377075195312, Learning Rate: 0.0025\n",
      "Epoch [4469/20000], Loss: 1004.2467041015625, Entropy 223.23069763183594, Learning Rate: 0.0025\n",
      "Epoch [4470/20000], Loss: 963.67431640625, Entropy 230.18003845214844, Learning Rate: 0.0025\n",
      "Epoch [4471/20000], Loss: 971.357666015625, Entropy 231.94300842285156, Learning Rate: 0.0025\n",
      "Epoch [4472/20000], Loss: 1005.8895874023438, Entropy 215.32366943359375, Learning Rate: 0.0025\n",
      "Epoch [4473/20000], Loss: 966.484130859375, Entropy 217.36764526367188, Learning Rate: 0.0025\n",
      "Epoch [4474/20000], Loss: 1017.3226928710938, Entropy 236.6817169189453, Learning Rate: 0.0025\n",
      "Epoch [4475/20000], Loss: 986.3080444335938, Entropy 216.0926055908203, Learning Rate: 0.0025\n",
      "Epoch [4476/20000], Loss: 967.12646484375, Entropy 229.97666931152344, Learning Rate: 0.0025\n",
      "Epoch [4477/20000], Loss: 974.0703735351562, Entropy 217.1690216064453, Learning Rate: 0.0025\n",
      "Epoch [4478/20000], Loss: 979.85888671875, Entropy 219.31712341308594, Learning Rate: 0.0025\n",
      "Epoch [4479/20000], Loss: 981.2852783203125, Entropy 213.6859130859375, Learning Rate: 0.0025\n",
      "Epoch [4480/20000], Loss: 958.449951171875, Entropy 236.18128967285156, Learning Rate: 0.0025\n",
      "Epoch [4481/20000], Loss: 1038.71484375, Entropy 209.66319274902344, Learning Rate: 0.0025\n",
      "Epoch [4482/20000], Loss: 992.7333984375, Entropy 216.59214782714844, Learning Rate: 0.0025\n",
      "Epoch [4483/20000], Loss: 952.5921630859375, Entropy 231.48228454589844, Learning Rate: 0.0025\n",
      "Epoch [4484/20000], Loss: 987.28369140625, Entropy 218.15953063964844, Learning Rate: 0.0025\n",
      "Epoch [4485/20000], Loss: 974.904052734375, Entropy 223.27171325683594, Learning Rate: 0.0025\n",
      "Epoch [4486/20000], Loss: 996.1942138671875, Entropy 225.21945190429688, Learning Rate: 0.0025\n",
      "Epoch [4487/20000], Loss: 971.9096069335938, Entropy 223.37017822265625, Learning Rate: 0.0025\n",
      "Epoch [4488/20000], Loss: 982.98779296875, Entropy 239.04237365722656, Learning Rate: 0.0025\n",
      "Epoch [4489/20000], Loss: 946.1065673828125, Entropy 245.676513671875, Learning Rate: 0.0025\n",
      "Epoch [4490/20000], Loss: 952.8703002929688, Entropy 211.7293243408203, Learning Rate: 0.0025\n",
      "Epoch [4491/20000], Loss: 1037.453857421875, Entropy 218.39959716796875, Learning Rate: 0.0025\n",
      "Epoch [4492/20000], Loss: 1027.1376953125, Entropy 202.03610229492188, Learning Rate: 0.0025\n",
      "Epoch [4493/20000], Loss: 986.3216552734375, Entropy 226.82839965820312, Learning Rate: 0.0025\n",
      "Epoch [4494/20000], Loss: 1008.5890502929688, Entropy 213.93402099609375, Learning Rate: 0.0025\n",
      "Epoch [4495/20000], Loss: 989.999755859375, Entropy 226.06216430664062, Learning Rate: 0.0025\n",
      "Epoch [4496/20000], Loss: 1058.946044921875, Entropy 251.86846923828125, Learning Rate: 0.0025\n",
      "Epoch [4497/20000], Loss: 1040.4339599609375, Entropy 224.11624145507812, Learning Rate: 0.0025\n",
      "Epoch [4498/20000], Loss: 942.70556640625, Entropy 242.603515625, Learning Rate: 0.0025\n",
      "Epoch [4499/20000], Loss: 994.2803955078125, Entropy 218.70114135742188, Learning Rate: 0.0025\n",
      "Epoch [4500/20000], Loss: 978.2174682617188, Entropy 247.0533905029297, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4501/20000], Loss: 984.2028198242188, Entropy 240.06243896484375, Learning Rate: 0.0025\n",
      "Epoch [4502/20000], Loss: 970.3524780273438, Entropy 227.5486297607422, Learning Rate: 0.0025\n",
      "Epoch [4503/20000], Loss: 1042.7735595703125, Entropy 232.61000061035156, Learning Rate: 0.0025\n",
      "Epoch [4504/20000], Loss: 956.7379150390625, Entropy 244.74769592285156, Learning Rate: 0.0025\n",
      "Epoch [4505/20000], Loss: 1039.66552734375, Entropy 225.70762634277344, Learning Rate: 0.0025\n",
      "Epoch [4506/20000], Loss: 954.7591552734375, Entropy 221.16172790527344, Learning Rate: 0.0025\n",
      "Epoch [4507/20000], Loss: 971.3450927734375, Entropy 231.50143432617188, Learning Rate: 0.0025\n",
      "Epoch [4508/20000], Loss: 995.8964233398438, Entropy 239.7678985595703, Learning Rate: 0.0025\n",
      "Epoch [4509/20000], Loss: 956.6560668945312, Entropy 251.3565216064453, Learning Rate: 0.0025\n",
      "Epoch [4510/20000], Loss: 977.3594360351562, Entropy 226.7315216064453, Learning Rate: 0.0025\n",
      "Epoch [4511/20000], Loss: 981.985595703125, Entropy 240.43276977539062, Learning Rate: 0.0025\n",
      "Epoch [4512/20000], Loss: 979.7322998046875, Entropy 236.41468811035156, Learning Rate: 0.0025\n",
      "Epoch [4513/20000], Loss: 988.391845703125, Entropy 230.46104431152344, Learning Rate: 0.0025\n",
      "Epoch [4514/20000], Loss: 1027.7733154296875, Entropy 216.8706817626953, Learning Rate: 0.0025\n",
      "Epoch [4515/20000], Loss: 1013.8385620117188, Entropy 222.03570556640625, Learning Rate: 0.0025\n",
      "Epoch [4516/20000], Loss: 974.561767578125, Entropy 250.6820068359375, Learning Rate: 0.0025\n",
      "Epoch [4517/20000], Loss: 969.6642456054688, Entropy 240.1936798095703, Learning Rate: 0.0025\n",
      "Epoch [4518/20000], Loss: 1010.9229736328125, Entropy 223.24673461914062, Learning Rate: 0.0025\n",
      "Epoch [4519/20000], Loss: 951.6469116210938, Entropy 237.1889190673828, Learning Rate: 0.0025\n",
      "Epoch [4520/20000], Loss: 975.845458984375, Entropy 234.63917541503906, Learning Rate: 0.0025\n",
      "Epoch [4521/20000], Loss: 960.8851928710938, Entropy 225.8777618408203, Learning Rate: 0.0025\n",
      "Epoch [4522/20000], Loss: 983.349365234375, Entropy 239.02406311035156, Learning Rate: 0.0025\n",
      "Epoch [4523/20000], Loss: 971.2288818359375, Entropy 243.83889770507812, Learning Rate: 0.0025\n",
      "Epoch [4524/20000], Loss: 993.3604736328125, Entropy 228.89889526367188, Learning Rate: 0.0025\n",
      "Epoch [4525/20000], Loss: 986.8701171875, Entropy 231.05699157714844, Learning Rate: 0.0025\n",
      "Epoch [4526/20000], Loss: 986.4034423828125, Entropy 217.09921264648438, Learning Rate: 0.0025\n",
      "Epoch [4527/20000], Loss: 942.504150390625, Entropy 236.17835998535156, Learning Rate: 0.0025\n",
      "Epoch [4528/20000], Loss: 1000.052001953125, Entropy 230.20921325683594, Learning Rate: 0.0025\n",
      "Epoch [4529/20000], Loss: 983.8640747070312, Entropy 223.78436279296875, Learning Rate: 0.0025\n",
      "Epoch [4530/20000], Loss: 981.0140380859375, Entropy 226.81239318847656, Learning Rate: 0.0025\n",
      "Epoch [4531/20000], Loss: 980.6126708984375, Entropy 249.36195373535156, Learning Rate: 0.0025\n",
      "Epoch [4532/20000], Loss: 974.9423828125, Entropy 241.76039123535156, Learning Rate: 0.0025\n",
      "Epoch [4533/20000], Loss: 1031.046630859375, Entropy 230.6757354736328, Learning Rate: 0.0025\n",
      "Epoch [4534/20000], Loss: 968.5840454101562, Entropy 233.6616668701172, Learning Rate: 0.0025\n",
      "Epoch [4535/20000], Loss: 1005.4210205078125, Entropy 226.16275024414062, Learning Rate: 0.0025\n",
      "Epoch [4536/20000], Loss: 957.7448120117188, Entropy 246.09002685546875, Learning Rate: 0.0025\n",
      "Epoch [4537/20000], Loss: 977.3969116210938, Entropy 233.4969024658203, Learning Rate: 0.0025\n",
      "Epoch [4538/20000], Loss: 969.472412109375, Entropy 246.00643920898438, Learning Rate: 0.0025\n",
      "Epoch [4539/20000], Loss: 997.0780639648438, Entropy 237.8056182861328, Learning Rate: 0.0025\n",
      "Epoch [4540/20000], Loss: 964.2413330078125, Entropy 237.95558166503906, Learning Rate: 0.0025\n",
      "Epoch [4541/20000], Loss: 938.323974609375, Entropy 249.51158142089844, Learning Rate: 0.0025\n",
      "Epoch [4542/20000], Loss: 1016.1141357421875, Entropy 237.51585388183594, Learning Rate: 0.0025\n",
      "Epoch [4543/20000], Loss: 979.3062133789062, Entropy 243.34197998046875, Learning Rate: 0.0025\n",
      "Epoch [4544/20000], Loss: 986.53857421875, Entropy 236.52096557617188, Learning Rate: 0.0025\n",
      "Epoch [4545/20000], Loss: 978.2279052734375, Entropy 236.92630004882812, Learning Rate: 0.0025\n",
      "Epoch [4546/20000], Loss: 992.70556640625, Entropy 241.67613220214844, Learning Rate: 0.0025\n",
      "Epoch [4547/20000], Loss: 1021.3702392578125, Entropy 214.87991333007812, Learning Rate: 0.0025\n",
      "Epoch [4548/20000], Loss: 961.4593505859375, Entropy 240.3095703125, Learning Rate: 0.0025\n",
      "Epoch [4549/20000], Loss: 967.9727783203125, Entropy 228.02503967285156, Learning Rate: 0.0025\n",
      "Epoch [4550/20000], Loss: 987.173095703125, Entropy 222.69442749023438, Learning Rate: 0.0025\n",
      "Epoch [4551/20000], Loss: 979.4423828125, Entropy 228.87039184570312, Learning Rate: 0.0025\n",
      "Epoch [4552/20000], Loss: 1034.713134765625, Entropy 235.0310516357422, Learning Rate: 0.0025\n",
      "Epoch [4553/20000], Loss: 995.5694580078125, Entropy 240.1580810546875, Learning Rate: 0.0025\n",
      "Epoch [4554/20000], Loss: 988.48388671875, Entropy 252.31138610839844, Learning Rate: 0.0025\n",
      "Epoch [4555/20000], Loss: 951.212158203125, Entropy 238.39181518554688, Learning Rate: 0.0025\n",
      "Epoch [4556/20000], Loss: 993.6319580078125, Entropy 232.18728637695312, Learning Rate: 0.0025\n",
      "Epoch [4557/20000], Loss: 992.2818603515625, Entropy 233.84999084472656, Learning Rate: 0.0025\n",
      "Epoch [4558/20000], Loss: 996.8834838867188, Entropy 226.39349365234375, Learning Rate: 0.0025\n",
      "Epoch [4559/20000], Loss: 961.44775390625, Entropy 247.15968322753906, Learning Rate: 0.0025\n",
      "Epoch [4560/20000], Loss: 947.7303466796875, Entropy 238.7216796875, Learning Rate: 0.0025\n",
      "Epoch [4561/20000], Loss: 962.2322387695312, Entropy 232.4665985107422, Learning Rate: 0.0025\n",
      "Epoch [4562/20000], Loss: 1029.203125, Entropy 240.53155517578125, Learning Rate: 0.0025\n",
      "Epoch [4563/20000], Loss: 970.7835083007812, Entropy 234.13409423828125, Learning Rate: 0.0025\n",
      "Epoch [4564/20000], Loss: 975.047607421875, Entropy 231.86428833007812, Learning Rate: 0.0025\n",
      "Epoch [4565/20000], Loss: 999.6312866210938, Entropy 237.34466552734375, Learning Rate: 0.0025\n",
      "Epoch [4566/20000], Loss: 964.0374755859375, Entropy 249.25392150878906, Learning Rate: 0.0025\n",
      "Epoch [4567/20000], Loss: 975.9727783203125, Entropy 239.67971801757812, Learning Rate: 0.0025\n",
      "Epoch [4568/20000], Loss: 1003.869873046875, Entropy 249.60276794433594, Learning Rate: 0.0025\n",
      "Epoch [4569/20000], Loss: 957.3768310546875, Entropy 250.78526306152344, Learning Rate: 0.0025\n",
      "Epoch [4570/20000], Loss: 925.2003173828125, Entropy 252.42428588867188, Learning Rate: 0.0025\n",
      "Epoch [4571/20000], Loss: 963.069091796875, Entropy 247.34971618652344, Learning Rate: 0.0025\n",
      "Epoch [4572/20000], Loss: 1001.4287109375, Entropy 214.08518981933594, Learning Rate: 0.0025\n",
      "Epoch [4573/20000], Loss: 976.5384521484375, Entropy 226.72958374023438, Learning Rate: 0.0025\n",
      "Epoch [4574/20000], Loss: 972.3756713867188, Entropy 241.04034423828125, Learning Rate: 0.0025\n",
      "Epoch [4575/20000], Loss: 950.2568359375, Entropy 244.403564453125, Learning Rate: 0.0025\n",
      "Epoch [4576/20000], Loss: 994.430908203125, Entropy 219.01730346679688, Learning Rate: 0.0025\n",
      "Epoch [4577/20000], Loss: 972.1231689453125, Entropy 231.23452758789062, Learning Rate: 0.0025\n",
      "Epoch [4578/20000], Loss: 969.9031982421875, Entropy 231.72178649902344, Learning Rate: 0.0025\n",
      "Epoch [4579/20000], Loss: 973.8541259765625, Entropy 240.7646484375, Learning Rate: 0.0025\n",
      "Epoch [4580/20000], Loss: 991.7371826171875, Entropy 246.42996215820312, Learning Rate: 0.0025\n",
      "Epoch [4581/20000], Loss: 988.9580078125, Entropy 259.9267272949219, Learning Rate: 0.0025\n",
      "Epoch [4582/20000], Loss: 1009.3623046875, Entropy 241.07589721679688, Learning Rate: 0.0025\n",
      "Epoch [4583/20000], Loss: 988.3687133789062, Entropy 240.2313995361328, Learning Rate: 0.0025\n",
      "Epoch [4584/20000], Loss: 984.6681518554688, Entropy 226.44757080078125, Learning Rate: 0.0025\n",
      "Epoch [4585/20000], Loss: 944.1685180664062, Entropy 246.31268310546875, Learning Rate: 0.0025\n",
      "Epoch [4586/20000], Loss: 962.6983642578125, Entropy 259.0751953125, Learning Rate: 0.0025\n",
      "Epoch [4587/20000], Loss: 966.9573364257812, Entropy 240.8179473876953, Learning Rate: 0.0025\n",
      "Epoch [4588/20000], Loss: 991.8173217773438, Entropy 240.1114044189453, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4589/20000], Loss: 971.43212890625, Entropy 243.553466796875, Learning Rate: 0.0025\n",
      "Epoch [4590/20000], Loss: 960.4977416992188, Entropy 250.1396026611328, Learning Rate: 0.0025\n",
      "Epoch [4591/20000], Loss: 966.1064453125, Entropy 242.0263671875, Learning Rate: 0.0025\n",
      "Epoch [4592/20000], Loss: 1004.7628173828125, Entropy 230.88534545898438, Learning Rate: 0.0025\n",
      "Epoch [4593/20000], Loss: 957.7020263671875, Entropy 248.33859252929688, Learning Rate: 0.0025\n",
      "Epoch [4594/20000], Loss: 940.3936157226562, Entropy 254.0349884033203, Learning Rate: 0.0025\n",
      "Epoch [4595/20000], Loss: 985.7990112304688, Entropy 230.25604248046875, Learning Rate: 0.0025\n",
      "Epoch [4596/20000], Loss: 978.6553955078125, Entropy 248.25987243652344, Learning Rate: 0.0025\n",
      "Epoch [4597/20000], Loss: 969.4393920898438, Entropy 253.84014892578125, Learning Rate: 0.0025\n",
      "Epoch [4598/20000], Loss: 976.7598876953125, Entropy 247.24429321289062, Learning Rate: 0.0025\n",
      "Epoch [4599/20000], Loss: 939.3009033203125, Entropy 263.8971252441406, Learning Rate: 0.0025\n",
      "Epoch [4600/20000], Loss: 966.0513916015625, Entropy 248.02841186523438, Learning Rate: 0.0025\n",
      "Epoch [4601/20000], Loss: 963.3106689453125, Entropy 255.92835998535156, Learning Rate: 0.0025\n",
      "Epoch [4602/20000], Loss: 984.56396484375, Entropy 234.86595153808594, Learning Rate: 0.0025\n",
      "Epoch [4603/20000], Loss: 969.0357666015625, Entropy 234.95654296875, Learning Rate: 0.0025\n",
      "Epoch [4604/20000], Loss: 999.6647338867188, Entropy 235.3106231689453, Learning Rate: 0.0025\n",
      "Epoch [4605/20000], Loss: 990.4287719726562, Entropy 247.1804656982422, Learning Rate: 0.0025\n",
      "Epoch [4606/20000], Loss: 994.7747192382812, Entropy 254.0983123779297, Learning Rate: 0.0025\n",
      "Epoch [4607/20000], Loss: 983.4049682617188, Entropy 258.31951904296875, Learning Rate: 0.0025\n",
      "Epoch [4608/20000], Loss: 968.1699829101562, Entropy 239.20184326171875, Learning Rate: 0.0025\n",
      "Epoch [4609/20000], Loss: 955.4010009765625, Entropy 240.57339477539062, Learning Rate: 0.0025\n",
      "Epoch [4610/20000], Loss: 968.4103393554688, Entropy 238.0564727783203, Learning Rate: 0.0025\n",
      "Epoch [4611/20000], Loss: 961.3253784179688, Entropy 253.42510986328125, Learning Rate: 0.0025\n",
      "Epoch [4612/20000], Loss: 980.30322265625, Entropy 246.92800903320312, Learning Rate: 0.0025\n",
      "Epoch [4613/20000], Loss: 986.2994384765625, Entropy 254.77944946289062, Learning Rate: 0.0025\n",
      "Epoch [4614/20000], Loss: 996.9337158203125, Entropy 242.23367309570312, Learning Rate: 0.0025\n",
      "Epoch [4615/20000], Loss: 957.7533569335938, Entropy 251.08953857421875, Learning Rate: 0.0025\n",
      "Epoch [4616/20000], Loss: 969.37939453125, Entropy 250.91331481933594, Learning Rate: 0.0025\n",
      "Epoch [4617/20000], Loss: 926.7637939453125, Entropy 255.61073303222656, Learning Rate: 0.0025\n",
      "Epoch [4618/20000], Loss: 987.98779296875, Entropy 229.57237243652344, Learning Rate: 0.0025\n",
      "Epoch [4619/20000], Loss: 978.1326293945312, Entropy 237.5438995361328, Learning Rate: 0.0025\n",
      "Epoch [4620/20000], Loss: 964.7472534179688, Entropy 249.07855224609375, Learning Rate: 0.0025\n",
      "Epoch [4621/20000], Loss: 961.9774169921875, Entropy 255.051025390625, Learning Rate: 0.0025\n",
      "Epoch [4622/20000], Loss: 990.8502197265625, Entropy 243.58255004882812, Learning Rate: 0.0025\n",
      "Epoch [4623/20000], Loss: 978.6669921875, Entropy 239.98342895507812, Learning Rate: 0.0025\n",
      "Epoch [4624/20000], Loss: 1013.763916015625, Entropy 238.28195190429688, Learning Rate: 0.0025\n",
      "Epoch [4625/20000], Loss: 995.7435302734375, Entropy 231.281982421875, Learning Rate: 0.0025\n",
      "Epoch [4626/20000], Loss: 978.6416625976562, Entropy 237.0321807861328, Learning Rate: 0.0025\n",
      "Epoch [4627/20000], Loss: 970.2651977539062, Entropy 239.1015167236328, Learning Rate: 0.0025\n",
      "Epoch [4628/20000], Loss: 987.1888427734375, Entropy 260.3354187011719, Learning Rate: 0.0025\n",
      "Epoch [4629/20000], Loss: 977.2059936523438, Entropy 258.23822021484375, Learning Rate: 0.0025\n",
      "Epoch [4630/20000], Loss: 1019.72607421875, Entropy 249.00686645507812, Learning Rate: 0.0025\n",
      "Epoch [4631/20000], Loss: 961.4329223632812, Entropy 236.31488037109375, Learning Rate: 0.0025\n",
      "Epoch [4632/20000], Loss: 961.2808227539062, Entropy 245.8389434814453, Learning Rate: 0.0025\n",
      "Epoch [4633/20000], Loss: 974.8270263671875, Entropy 237.74093627929688, Learning Rate: 0.0025\n",
      "Epoch [4634/20000], Loss: 963.6322021484375, Entropy 238.31112670898438, Learning Rate: 0.0025\n",
      "Epoch [4635/20000], Loss: 988.4166870117188, Entropy 242.13372802734375, Learning Rate: 0.0025\n",
      "Epoch [4636/20000], Loss: 1000.6995849609375, Entropy 249.07394409179688, Learning Rate: 0.0025\n",
      "Epoch [4637/20000], Loss: 975.4415283203125, Entropy 236.96206665039062, Learning Rate: 0.0025\n",
      "Epoch [4638/20000], Loss: 1000.27490234375, Entropy 250.59022521972656, Learning Rate: 0.0025\n",
      "Epoch [4639/20000], Loss: 961.5146484375, Entropy 259.6861572265625, Learning Rate: 0.0025\n",
      "Epoch [4640/20000], Loss: 962.6409912109375, Entropy 256.1330871582031, Learning Rate: 0.0025\n",
      "Epoch [4641/20000], Loss: 980.5847778320312, Entropy 238.0637969970703, Learning Rate: 0.0025\n",
      "Epoch [4642/20000], Loss: 1008.5348510742188, Entropy 239.3738555908203, Learning Rate: 0.0025\n",
      "Epoch [4643/20000], Loss: 994.5635986328125, Entropy 263.1235046386719, Learning Rate: 0.0025\n",
      "Epoch [4644/20000], Loss: 916.3563232421875, Entropy 270.0080261230469, Learning Rate: 0.0025\n",
      "Epoch [4645/20000], Loss: 972.5158081054688, Entropy 242.7772674560547, Learning Rate: 0.0025\n",
      "Epoch [4646/20000], Loss: 956.4299926757812, Entropy 248.8273162841797, Learning Rate: 0.0025\n",
      "Epoch [4647/20000], Loss: 1022.41064453125, Entropy 255.35107421875, Learning Rate: 0.0025\n",
      "Epoch [4648/20000], Loss: 982.1721801757812, Entropy 243.0047149658203, Learning Rate: 0.0025\n",
      "Epoch [4649/20000], Loss: 991.0136108398438, Entropy 242.2303924560547, Learning Rate: 0.0025\n",
      "Epoch [4650/20000], Loss: 977.3211059570312, Entropy 239.78631591796875, Learning Rate: 0.0025\n",
      "Epoch [4651/20000], Loss: 963.2940673828125, Entropy 255.24220275878906, Learning Rate: 0.0025\n",
      "Epoch [4652/20000], Loss: 993.598388671875, Entropy 252.77366638183594, Learning Rate: 0.0025\n",
      "Epoch [4653/20000], Loss: 1026.6046142578125, Entropy 253.02305603027344, Learning Rate: 0.0025\n",
      "Epoch [4654/20000], Loss: 998.17626953125, Entropy 250.1248779296875, Learning Rate: 0.0025\n",
      "Epoch [4655/20000], Loss: 1020.4195556640625, Entropy 232.24229431152344, Learning Rate: 0.0025\n",
      "Epoch [4656/20000], Loss: 978.9093017578125, Entropy 235.74107360839844, Learning Rate: 0.0025\n",
      "Epoch [4657/20000], Loss: 969.5443115234375, Entropy 243.61924743652344, Learning Rate: 0.0025\n",
      "Epoch [4658/20000], Loss: 950.2316284179688, Entropy 258.44781494140625, Learning Rate: 0.0025\n",
      "Epoch [4659/20000], Loss: 965.7302856445312, Entropy 261.07537841796875, Learning Rate: 0.0025\n",
      "Epoch [4660/20000], Loss: 953.214111328125, Entropy 264.4920349121094, Learning Rate: 0.0025\n",
      "Epoch [4661/20000], Loss: 1000.6510009765625, Entropy 257.6481628417969, Learning Rate: 0.0025\n",
      "Epoch [4662/20000], Loss: 991.56494140625, Entropy 259.2028503417969, Learning Rate: 0.0025\n",
      "Epoch [4663/20000], Loss: 960.1432495117188, Entropy 240.0955352783203, Learning Rate: 0.0025\n",
      "Epoch [4664/20000], Loss: 965.143310546875, Entropy 268.4517517089844, Learning Rate: 0.0025\n",
      "Epoch [4665/20000], Loss: 972.8438720703125, Entropy 251.61622619628906, Learning Rate: 0.0025\n",
      "Epoch [4666/20000], Loss: 990.1725463867188, Entropy 238.85858154296875, Learning Rate: 0.0025\n",
      "Epoch [4667/20000], Loss: 974.8409423828125, Entropy 253.14218139648438, Learning Rate: 0.0025\n",
      "Epoch [4668/20000], Loss: 931.68603515625, Entropy 270.3035888671875, Learning Rate: 0.0025\n",
      "Epoch [4669/20000], Loss: 971.0573120117188, Entropy 250.85821533203125, Learning Rate: 0.0025\n",
      "Epoch [4670/20000], Loss: 982.7936401367188, Entropy 261.35101318359375, Learning Rate: 0.0025\n",
      "Epoch [4671/20000], Loss: 986.089599609375, Entropy 276.7704772949219, Learning Rate: 0.0025\n",
      "Epoch [4672/20000], Loss: 987.2326049804688, Entropy 256.76812744140625, Learning Rate: 0.0025\n",
      "Epoch [4673/20000], Loss: 964.6595458984375, Entropy 258.6867980957031, Learning Rate: 0.0025\n",
      "Epoch [4674/20000], Loss: 987.7796630859375, Entropy 243.578125, Learning Rate: 0.0025\n",
      "Epoch [4675/20000], Loss: 966.24951171875, Entropy 254.51132202148438, Learning Rate: 0.0025\n",
      "Epoch [4676/20000], Loss: 961.2913818359375, Entropy 250.60450744628906, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4677/20000], Loss: 947.31787109375, Entropy 258.3353271484375, Learning Rate: 0.0025\n",
      "Epoch [4678/20000], Loss: 976.0626220703125, Entropy 253.25320434570312, Learning Rate: 0.0025\n",
      "Epoch [4679/20000], Loss: 960.4371337890625, Entropy 254.42982482910156, Learning Rate: 0.0025\n",
      "Epoch [4680/20000], Loss: 982.8140258789062, Entropy 236.9794464111328, Learning Rate: 0.0025\n",
      "Epoch [4681/20000], Loss: 1004.8184814453125, Entropy 247.68124389648438, Learning Rate: 0.0025\n",
      "Epoch [4682/20000], Loss: 997.4906005859375, Entropy 236.48069763183594, Learning Rate: 0.0025\n",
      "Epoch [4683/20000], Loss: 946.2244873046875, Entropy 252.26988220214844, Learning Rate: 0.0025\n",
      "Epoch [4684/20000], Loss: 950.8336181640625, Entropy 276.0613708496094, Learning Rate: 0.0025\n",
      "Epoch [4685/20000], Loss: 1087.9473876953125, Entropy 246.5359649658203, Learning Rate: 0.0025\n",
      "Epoch [4686/20000], Loss: 961.1904907226562, Entropy 264.37506103515625, Learning Rate: 0.0025\n",
      "Epoch [4687/20000], Loss: 973.8355712890625, Entropy 253.28573608398438, Learning Rate: 0.0025\n",
      "Epoch [4688/20000], Loss: 939.4705810546875, Entropy 262.2080993652344, Learning Rate: 0.0025\n",
      "Epoch [4689/20000], Loss: 957.0746459960938, Entropy 250.10186767578125, Learning Rate: 0.0025\n",
      "Epoch [4690/20000], Loss: 966.871826171875, Entropy 261.8515625, Learning Rate: 0.0025\n",
      "Epoch [4691/20000], Loss: 988.7520751953125, Entropy 262.9940490722656, Learning Rate: 0.0025\n",
      "Epoch [4692/20000], Loss: 963.5701904296875, Entropy 259.9523620605469, Learning Rate: 0.0025\n",
      "Epoch [4693/20000], Loss: 961.4476318359375, Entropy 259.6576843261719, Learning Rate: 0.0025\n",
      "Epoch [4694/20000], Loss: 976.876220703125, Entropy 269.303466796875, Learning Rate: 0.0025\n",
      "Epoch [4695/20000], Loss: 969.258544921875, Entropy 265.7910461425781, Learning Rate: 0.0025\n",
      "Epoch [4696/20000], Loss: 980.1032104492188, Entropy 260.42047119140625, Learning Rate: 0.0025\n",
      "Epoch [4697/20000], Loss: 946.3013916015625, Entropy 253.93772888183594, Learning Rate: 0.0025\n",
      "Epoch [4698/20000], Loss: 967.300537109375, Entropy 244.50808715820312, Learning Rate: 0.0025\n",
      "Epoch [4699/20000], Loss: 1006.1547241210938, Entropy 257.40728759765625, Learning Rate: 0.0025\n",
      "Epoch [4700/20000], Loss: 956.4927368164062, Entropy 246.4080047607422, Learning Rate: 0.0025\n",
      "Epoch [4701/20000], Loss: 955.90673828125, Entropy 244.63113403320312, Learning Rate: 0.0025\n",
      "Epoch [4702/20000], Loss: 987.55419921875, Entropy 245.47000122070312, Learning Rate: 0.0025\n",
      "Epoch [4703/20000], Loss: 983.4337158203125, Entropy 266.6360778808594, Learning Rate: 0.0025\n",
      "Epoch [4704/20000], Loss: 947.8807983398438, Entropy 272.51824951171875, Learning Rate: 0.0025\n",
      "Epoch [4705/20000], Loss: 991.7144775390625, Entropy 258.1368713378906, Learning Rate: 0.0025\n",
      "Epoch [4706/20000], Loss: 968.236328125, Entropy 266.2380065917969, Learning Rate: 0.0025\n",
      "Epoch [4707/20000], Loss: 967.721923828125, Entropy 249.17349243164062, Learning Rate: 0.0025\n",
      "Epoch [4708/20000], Loss: 959.710205078125, Entropy 263.4397888183594, Learning Rate: 0.0025\n",
      "Epoch [4709/20000], Loss: 945.9523315429688, Entropy 274.96392822265625, Learning Rate: 0.0025\n",
      "Epoch [4710/20000], Loss: 931.8528442382812, Entropy 255.92132568359375, Learning Rate: 0.0025\n",
      "Epoch [4711/20000], Loss: 959.64501953125, Entropy 265.7628173828125, Learning Rate: 0.0025\n",
      "Epoch [4712/20000], Loss: 961.6015625, Entropy 282.2662353515625, Learning Rate: 0.0025\n",
      "Epoch [4713/20000], Loss: 953.535888671875, Entropy 273.0154724121094, Learning Rate: 0.0025\n",
      "Epoch [4714/20000], Loss: 957.7110595703125, Entropy 244.87985229492188, Learning Rate: 0.0025\n",
      "Epoch [4715/20000], Loss: 987.367431640625, Entropy 258.0564880371094, Learning Rate: 0.0025\n",
      "Epoch [4716/20000], Loss: 988.2628173828125, Entropy 260.8193359375, Learning Rate: 0.0025\n",
      "Epoch [4717/20000], Loss: 985.4522705078125, Entropy 253.99856567382812, Learning Rate: 0.0025\n",
      "Epoch [4718/20000], Loss: 939.8524169921875, Entropy 265.6611328125, Learning Rate: 0.0025\n",
      "Epoch [4719/20000], Loss: 933.8899536132812, Entropy 256.78326416015625, Learning Rate: 0.0025\n",
      "Epoch [4720/20000], Loss: 956.0107421875, Entropy 264.13232421875, Learning Rate: 0.0025\n",
      "Epoch [4721/20000], Loss: 987.264892578125, Entropy 252.47213745117188, Learning Rate: 0.0025\n",
      "Epoch [4722/20000], Loss: 987.51025390625, Entropy 240.7498779296875, Learning Rate: 0.0025\n",
      "Epoch [4723/20000], Loss: 982.080322265625, Entropy 267.9971923828125, Learning Rate: 0.0025\n",
      "Epoch [4724/20000], Loss: 928.62353515625, Entropy 274.1935119628906, Learning Rate: 0.0025\n",
      "Epoch [4725/20000], Loss: 963.337158203125, Entropy 243.61085510253906, Learning Rate: 0.0025\n",
      "Epoch [4726/20000], Loss: 978.33154296875, Entropy 269.1809997558594, Learning Rate: 0.0025\n",
      "Epoch [4727/20000], Loss: 967.1156005859375, Entropy 260.8966064453125, Learning Rate: 0.0025\n",
      "Epoch [4728/20000], Loss: 949.8358154296875, Entropy 266.5718078613281, Learning Rate: 0.0025\n",
      "Epoch [4729/20000], Loss: 1003.5439453125, Entropy 263.126953125, Learning Rate: 0.0025\n",
      "Epoch [4730/20000], Loss: 977.4053955078125, Entropy 252.38717651367188, Learning Rate: 0.0025\n",
      "Epoch [4731/20000], Loss: 1010.3583984375, Entropy 245.27609252929688, Learning Rate: 0.0025\n",
      "Epoch [4732/20000], Loss: 949.5610961914062, Entropy 271.93194580078125, Learning Rate: 0.0025\n",
      "Epoch [4733/20000], Loss: 981.7286376953125, Entropy 258.2760925292969, Learning Rate: 0.0025\n",
      "Epoch [4734/20000], Loss: 992.691162109375, Entropy 259.6600646972656, Learning Rate: 0.0025\n",
      "Epoch [4735/20000], Loss: 953.404541015625, Entropy 270.1208190917969, Learning Rate: 0.0025\n",
      "Epoch [4736/20000], Loss: 982.1219482421875, Entropy 259.1586608886719, Learning Rate: 0.0025\n",
      "Epoch [4737/20000], Loss: 947.4865112304688, Entropy 251.1088409423828, Learning Rate: 0.0025\n",
      "Epoch [4738/20000], Loss: 972.2523803710938, Entropy 242.29022216796875, Learning Rate: 0.0025\n",
      "Epoch [4739/20000], Loss: 977.9374389648438, Entropy 266.00128173828125, Learning Rate: 0.0025\n",
      "Epoch [4740/20000], Loss: 976.481689453125, Entropy 270.0067443847656, Learning Rate: 0.0025\n",
      "Epoch [4741/20000], Loss: 979.6116943359375, Entropy 257.6880187988281, Learning Rate: 0.0025\n",
      "Epoch [4742/20000], Loss: 969.41650390625, Entropy 259.9814758300781, Learning Rate: 0.0025\n",
      "Epoch [4743/20000], Loss: 973.097412109375, Entropy 268.3324890136719, Learning Rate: 0.0025\n",
      "Epoch [4744/20000], Loss: 968.4146118164062, Entropy 251.76312255859375, Learning Rate: 0.0025\n",
      "Epoch [4745/20000], Loss: 911.7491455078125, Entropy 280.2379150390625, Learning Rate: 0.0025\n",
      "Epoch [4746/20000], Loss: 931.3785400390625, Entropy 272.2218017578125, Learning Rate: 0.0025\n",
      "Epoch [4747/20000], Loss: 961.152099609375, Entropy 265.5516052246094, Learning Rate: 0.0025\n",
      "Epoch [4748/20000], Loss: 964.7646484375, Entropy 261.3669128417969, Learning Rate: 0.0025\n",
      "Epoch [4749/20000], Loss: 982.3444213867188, Entropy 260.76885986328125, Learning Rate: 0.0025\n",
      "Epoch [4750/20000], Loss: 989.93359375, Entropy 275.5146484375, Learning Rate: 0.0025\n",
      "Epoch [4751/20000], Loss: 950.6337890625, Entropy 265.1483459472656, Learning Rate: 0.0025\n",
      "Epoch [4752/20000], Loss: 972.1937255859375, Entropy 257.5693359375, Learning Rate: 0.0025\n",
      "Epoch [4753/20000], Loss: 987.6981201171875, Entropy 247.75070190429688, Learning Rate: 0.0025\n",
      "Epoch [4754/20000], Loss: 956.632568359375, Entropy 257.0972900390625, Learning Rate: 0.0025\n",
      "Epoch [4755/20000], Loss: 1018.2960205078125, Entropy 261.6634216308594, Learning Rate: 0.0025\n",
      "Epoch [4756/20000], Loss: 982.3495483398438, Entropy 266.43853759765625, Learning Rate: 0.0025\n",
      "Epoch [4757/20000], Loss: 963.6153564453125, Entropy 258.7666015625, Learning Rate: 0.0025\n",
      "Epoch [4758/20000], Loss: 987.4521484375, Entropy 250.1435546875, Learning Rate: 0.0025\n",
      "Epoch [4759/20000], Loss: 944.0684204101562, Entropy 272.18316650390625, Learning Rate: 0.0025\n",
      "Epoch [4760/20000], Loss: 909.7012939453125, Entropy 275.9016418457031, Learning Rate: 0.0025\n",
      "Epoch [4761/20000], Loss: 954.459716796875, Entropy 271.8420715332031, Learning Rate: 0.0025\n",
      "Epoch [4762/20000], Loss: 960.1791381835938, Entropy 251.5607147216797, Learning Rate: 0.0025\n",
      "Epoch [4763/20000], Loss: 966.503173828125, Entropy 266.0756530761719, Learning Rate: 0.0025\n",
      "Epoch [4764/20000], Loss: 980.4739990234375, Entropy 260.712158203125, Learning Rate: 0.0025\n",
      "Epoch [4765/20000], Loss: 993.0097045898438, Entropy 261.66912841796875, Learning Rate: 0.0025\n",
      "Epoch [4766/20000], Loss: 970.8431396484375, Entropy 261.2892761230469, Learning Rate: 0.0025\n",
      "Epoch [4767/20000], Loss: 974.52685546875, Entropy 287.1838684082031, Learning Rate: 0.0025\n",
      "Epoch [4768/20000], Loss: 963.9173583984375, Entropy 257.6505126953125, Learning Rate: 0.0025\n",
      "Epoch [4769/20000], Loss: 968.6875, Entropy 258.6570129394531, Learning Rate: 0.0025\n",
      "Epoch [4770/20000], Loss: 964.316162109375, Entropy 259.0680236816406, Learning Rate: 0.0025\n",
      "Epoch [4771/20000], Loss: 936.9196166992188, Entropy 269.49969482421875, Learning Rate: 0.0025\n",
      "Epoch [4772/20000], Loss: 935.5556640625, Entropy 262.4513244628906, Learning Rate: 0.0025\n",
      "Epoch [4773/20000], Loss: 934.999755859375, Entropy 263.8366394042969, Learning Rate: 0.0025\n",
      "Epoch [4774/20000], Loss: 967.4462280273438, Entropy 245.9842071533203, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4775/20000], Loss: 979.4923095703125, Entropy 264.3612365722656, Learning Rate: 0.0025\n",
      "Epoch [4776/20000], Loss: 953.685546875, Entropy 281.4591064453125, Learning Rate: 0.0025\n",
      "Epoch [4777/20000], Loss: 978.07421875, Entropy 263.3249206542969, Learning Rate: 0.0025\n",
      "Epoch [4778/20000], Loss: 953.225341796875, Entropy 262.0598449707031, Learning Rate: 0.0025\n",
      "Epoch [4779/20000], Loss: 970.008056640625, Entropy 250.12643432617188, Learning Rate: 0.0025\n",
      "Epoch [4780/20000], Loss: 967.2899169921875, Entropy 265.30517578125, Learning Rate: 0.0025\n",
      "Epoch [4781/20000], Loss: 1004.8763427734375, Entropy 263.1222229003906, Learning Rate: 0.0025\n",
      "Epoch [4782/20000], Loss: 989.617919921875, Entropy 275.3817443847656, Learning Rate: 0.0025\n",
      "Epoch [4783/20000], Loss: 950.154296875, Entropy 269.6104431152344, Learning Rate: 0.0025\n",
      "Epoch [4784/20000], Loss: 958.3797607421875, Entropy 273.6336669921875, Learning Rate: 0.0025\n",
      "Epoch [4785/20000], Loss: 946.7433471679688, Entropy 263.33892822265625, Learning Rate: 0.0025\n",
      "Epoch [4786/20000], Loss: 967.85302734375, Entropy 266.1838684082031, Learning Rate: 0.0025\n",
      "Epoch [4787/20000], Loss: 999.9628295898438, Entropy 261.05010986328125, Learning Rate: 0.0025\n",
      "Epoch [4788/20000], Loss: 996.0235595703125, Entropy 269.2704772949219, Learning Rate: 0.0025\n",
      "Epoch [4789/20000], Loss: 940.1217651367188, Entropy 265.86614990234375, Learning Rate: 0.0025\n",
      "Epoch [4790/20000], Loss: 966.64697265625, Entropy 263.7491760253906, Learning Rate: 0.0025\n",
      "Epoch [4791/20000], Loss: 985.99951171875, Entropy 265.545654296875, Learning Rate: 0.0025\n",
      "Epoch [4792/20000], Loss: 1000.6015014648438, Entropy 268.75628662109375, Learning Rate: 0.0025\n",
      "Epoch [4793/20000], Loss: 961.5494384765625, Entropy 269.251708984375, Learning Rate: 0.0025\n",
      "Epoch [4794/20000], Loss: 932.71630859375, Entropy 268.4025573730469, Learning Rate: 0.0025\n",
      "Epoch [4795/20000], Loss: 912.0340576171875, Entropy 264.8642272949219, Learning Rate: 0.0025\n",
      "Epoch [4796/20000], Loss: 962.1111450195312, Entropy 270.88677978515625, Learning Rate: 0.0025\n",
      "Epoch [4797/20000], Loss: 932.216064453125, Entropy 262.84033203125, Learning Rate: 0.0025\n",
      "Epoch [4798/20000], Loss: 990.3851928710938, Entropy 277.75311279296875, Learning Rate: 0.0025\n",
      "Epoch [4799/20000], Loss: 952.1099243164062, Entropy 258.48492431640625, Learning Rate: 0.0025\n",
      "Epoch [4800/20000], Loss: 992.665283203125, Entropy 259.6788024902344, Learning Rate: 0.0025\n",
      "Epoch [4801/20000], Loss: 940.0834350585938, Entropy 285.91424560546875, Learning Rate: 0.0025\n",
      "Epoch [4802/20000], Loss: 967.9647216796875, Entropy 259.3915100097656, Learning Rate: 0.0025\n",
      "Epoch [4803/20000], Loss: 947.85205078125, Entropy 267.9066467285156, Learning Rate: 0.0025\n",
      "Epoch [4804/20000], Loss: 990.8203125, Entropy 252.14454650878906, Learning Rate: 0.0025\n",
      "Epoch [4805/20000], Loss: 953.9920654296875, Entropy 264.1748046875, Learning Rate: 0.0025\n",
      "Epoch [4806/20000], Loss: 972.884765625, Entropy 273.1282958984375, Learning Rate: 0.0025\n",
      "Epoch [4807/20000], Loss: 952.47216796875, Entropy 255.56015014648438, Learning Rate: 0.0025\n",
      "Epoch [4808/20000], Loss: 953.5419921875, Entropy 278.7655029296875, Learning Rate: 0.0025\n",
      "Epoch [4809/20000], Loss: 972.021240234375, Entropy 247.73538208007812, Learning Rate: 0.0025\n",
      "Epoch [4810/20000], Loss: 981.9996337890625, Entropy 270.8112487792969, Learning Rate: 0.0025\n",
      "Epoch [4811/20000], Loss: 950.699951171875, Entropy 273.788818359375, Learning Rate: 0.0025\n",
      "Epoch [4812/20000], Loss: 1003.7076416015625, Entropy 257.1723327636719, Learning Rate: 0.0025\n",
      "Epoch [4813/20000], Loss: 956.30908203125, Entropy 265.59716796875, Learning Rate: 0.0025\n",
      "Epoch [4814/20000], Loss: 968.9983520507812, Entropy 272.27313232421875, Learning Rate: 0.0025\n",
      "Epoch [4815/20000], Loss: 959.9217529296875, Entropy 258.2844543457031, Learning Rate: 0.0025\n",
      "Epoch [4816/20000], Loss: 991.5756225585938, Entropy 269.49871826171875, Learning Rate: 0.0025\n",
      "Epoch [4817/20000], Loss: 920.8446044921875, Entropy 263.3551330566406, Learning Rate: 0.0025\n",
      "Epoch [4818/20000], Loss: 951.7428588867188, Entropy 255.8604278564453, Learning Rate: 0.0025\n",
      "Epoch [4819/20000], Loss: 954.0191650390625, Entropy 273.7779541015625, Learning Rate: 0.0025\n",
      "Epoch [4820/20000], Loss: 977.736083984375, Entropy 282.07373046875, Learning Rate: 0.0025\n",
      "Epoch [4821/20000], Loss: 958.153564453125, Entropy 262.9267272949219, Learning Rate: 0.0025\n",
      "Epoch [4822/20000], Loss: 964.7811279296875, Entropy 278.0218200683594, Learning Rate: 0.0025\n",
      "Epoch [4823/20000], Loss: 911.5306396484375, Entropy 301.1252746582031, Learning Rate: 0.0025\n",
      "Epoch [4824/20000], Loss: 967.7020263671875, Entropy 274.7253112792969, Learning Rate: 0.0025\n",
      "Epoch [4825/20000], Loss: 938.8638916015625, Entropy 277.8746337890625, Learning Rate: 0.0025\n",
      "Epoch [4826/20000], Loss: 1002.8743896484375, Entropy 265.0516662597656, Learning Rate: 0.0025\n",
      "Epoch [4827/20000], Loss: 967.6287841796875, Entropy 278.7854919433594, Learning Rate: 0.0025\n",
      "Epoch [4828/20000], Loss: 959.957763671875, Entropy 268.7961730957031, Learning Rate: 0.0025\n",
      "Epoch [4829/20000], Loss: 964.038818359375, Entropy 269.1158447265625, Learning Rate: 0.0025\n",
      "Epoch [4830/20000], Loss: 961.2186279296875, Entropy 264.9157409667969, Learning Rate: 0.0025\n",
      "Epoch [4831/20000], Loss: 957.106689453125, Entropy 258.02099609375, Learning Rate: 0.0025\n",
      "Epoch [4832/20000], Loss: 938.6231689453125, Entropy 264.9063720703125, Learning Rate: 0.0025\n",
      "Epoch [4833/20000], Loss: 916.1025390625, Entropy 283.7079772949219, Learning Rate: 0.0025\n",
      "Epoch [4834/20000], Loss: 948.0654296875, Entropy 273.9264221191406, Learning Rate: 0.0025\n",
      "Epoch [4835/20000], Loss: 943.9039306640625, Entropy 276.3504943847656, Learning Rate: 0.0025\n",
      "Epoch [4836/20000], Loss: 952.444091796875, Entropy 272.2644958496094, Learning Rate: 0.0025\n",
      "Epoch [4837/20000], Loss: 983.1336669921875, Entropy 276.1065673828125, Learning Rate: 0.0025\n",
      "Epoch [4838/20000], Loss: 948.3143310546875, Entropy 275.1034240722656, Learning Rate: 0.0025\n",
      "Epoch [4839/20000], Loss: 979.6470947265625, Entropy 267.6985778808594, Learning Rate: 0.0025\n",
      "Epoch [4840/20000], Loss: 975.1165771484375, Entropy 276.8840637207031, Learning Rate: 0.0025\n",
      "Epoch [4841/20000], Loss: 963.79052734375, Entropy 253.2821044921875, Learning Rate: 0.0025\n",
      "Epoch [4842/20000], Loss: 990.6617431640625, Entropy 266.5038757324219, Learning Rate: 0.0025\n",
      "Epoch [4843/20000], Loss: 965.7761840820312, Entropy 282.77655029296875, Learning Rate: 0.0025\n",
      "Epoch [4844/20000], Loss: 923.113037109375, Entropy 278.6895446777344, Learning Rate: 0.0025\n",
      "Epoch [4845/20000], Loss: 969.2679443359375, Entropy 270.54541015625, Learning Rate: 0.0025\n",
      "Epoch [4846/20000], Loss: 896.8833618164062, Entropy 287.73394775390625, Learning Rate: 0.0025\n",
      "Epoch [4847/20000], Loss: 934.5748901367188, Entropy 279.87506103515625, Learning Rate: 0.0025\n",
      "Epoch [4848/20000], Loss: 943.6704711914062, Entropy 272.33551025390625, Learning Rate: 0.0025\n",
      "Epoch [4849/20000], Loss: 956.3336791992188, Entropy 278.38787841796875, Learning Rate: 0.0025\n",
      "Epoch [4850/20000], Loss: 988.8099365234375, Entropy 270.7853698730469, Learning Rate: 0.0025\n",
      "Epoch [4851/20000], Loss: 955.2225341796875, Entropy 265.2104797363281, Learning Rate: 0.0025\n",
      "Epoch [4852/20000], Loss: 927.8367919921875, Entropy 287.9140625, Learning Rate: 0.0025\n",
      "Epoch [4853/20000], Loss: 935.3228759765625, Entropy 271.9484558105469, Learning Rate: 0.0025\n",
      "Epoch [4854/20000], Loss: 922.8226318359375, Entropy 282.7008056640625, Learning Rate: 0.0025\n",
      "Epoch [4855/20000], Loss: 980.797607421875, Entropy 276.8052673339844, Learning Rate: 0.0025\n",
      "Epoch [4856/20000], Loss: 950.9121704101562, Entropy 277.09588623046875, Learning Rate: 0.0025\n",
      "Epoch [4857/20000], Loss: 952.3719482421875, Entropy 276.1649475097656, Learning Rate: 0.0025\n",
      "Epoch [4858/20000], Loss: 984.7076416015625, Entropy 274.66357421875, Learning Rate: 0.0025\n",
      "Epoch [4859/20000], Loss: 1003.0770263671875, Entropy 268.0128479003906, Learning Rate: 0.0025\n",
      "Epoch [4860/20000], Loss: 952.097900390625, Entropy 270.3361511230469, Learning Rate: 0.0025\n",
      "Epoch [4861/20000], Loss: 950.7611083984375, Entropy 278.7646789550781, Learning Rate: 0.0025\n",
      "Epoch [4862/20000], Loss: 996.9771118164062, Entropy 268.96295166015625, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4863/20000], Loss: 946.60498046875, Entropy 283.8517150878906, Learning Rate: 0.0025\n",
      "Epoch [4864/20000], Loss: 937.328857421875, Entropy 274.9830322265625, Learning Rate: 0.0025\n",
      "Epoch [4865/20000], Loss: 951.4115600585938, Entropy 291.03826904296875, Learning Rate: 0.0025\n",
      "Epoch [4866/20000], Loss: 939.0263061523438, Entropy 267.79254150390625, Learning Rate: 0.0025\n",
      "Epoch [4867/20000], Loss: 936.7977294921875, Entropy 273.6209716796875, Learning Rate: 0.0025\n",
      "Epoch [4868/20000], Loss: 963.47705078125, Entropy 259.4288330078125, Learning Rate: 0.0025\n",
      "Epoch [4869/20000], Loss: 940.2784423828125, Entropy 289.92431640625, Learning Rate: 0.0025\n",
      "Epoch [4870/20000], Loss: 968.3135375976562, Entropy 266.36138916015625, Learning Rate: 0.0025\n",
      "Epoch [4871/20000], Loss: 965.8639526367188, Entropy 259.27191162109375, Learning Rate: 0.0025\n",
      "Epoch [4872/20000], Loss: 940.217041015625, Entropy 281.0844421386719, Learning Rate: 0.0025\n",
      "Epoch [4873/20000], Loss: 982.3795166015625, Entropy 260.0460205078125, Learning Rate: 0.0025\n",
      "Epoch [4874/20000], Loss: 936.0817260742188, Entropy 283.68414306640625, Learning Rate: 0.0025\n",
      "Epoch [4875/20000], Loss: 983.17041015625, Entropy 276.4132080078125, Learning Rate: 0.0025\n",
      "Epoch [4876/20000], Loss: 953.5765380859375, Entropy 283.45556640625, Learning Rate: 0.0025\n",
      "Epoch [4877/20000], Loss: 927.7083740234375, Entropy 289.7039489746094, Learning Rate: 0.0025\n",
      "Epoch [4878/20000], Loss: 988.1475219726562, Entropy 283.12176513671875, Learning Rate: 0.0025\n",
      "Epoch [4879/20000], Loss: 980.0311279296875, Entropy 280.8622741699219, Learning Rate: 0.0025\n",
      "Epoch [4880/20000], Loss: 942.1920166015625, Entropy 284.2108459472656, Learning Rate: 0.0025\n",
      "Epoch [4881/20000], Loss: 954.5581665039062, Entropy 270.39569091796875, Learning Rate: 0.0025\n",
      "Epoch [4882/20000], Loss: 1030.6400146484375, Entropy 264.3484802246094, Learning Rate: 0.0025\n",
      "Epoch [4883/20000], Loss: 963.833251953125, Entropy 274.4934387207031, Learning Rate: 0.0025\n",
      "Epoch [4884/20000], Loss: 1002.9593505859375, Entropy 268.1253662109375, Learning Rate: 0.0025\n",
      "Epoch [4885/20000], Loss: 932.4989013671875, Entropy 283.3415222167969, Learning Rate: 0.0025\n",
      "Epoch [4886/20000], Loss: 952.1698608398438, Entropy 281.12554931640625, Learning Rate: 0.0025\n",
      "Epoch [4887/20000], Loss: 932.0823974609375, Entropy 272.6435546875, Learning Rate: 0.0025\n",
      "Epoch [4888/20000], Loss: 938.3599853515625, Entropy 283.10009765625, Learning Rate: 0.0025\n",
      "Epoch [4889/20000], Loss: 1005.0809326171875, Entropy 281.9410400390625, Learning Rate: 0.0025\n",
      "Epoch [4890/20000], Loss: 987.0616455078125, Entropy 279.236328125, Learning Rate: 0.0025\n",
      "Epoch [4891/20000], Loss: 986.4554443359375, Entropy 281.9775695800781, Learning Rate: 0.0025\n",
      "Epoch [4892/20000], Loss: 967.282470703125, Entropy 261.9526672363281, Learning Rate: 0.0025\n",
      "Epoch [4893/20000], Loss: 922.6253662109375, Entropy 293.5693359375, Learning Rate: 0.0025\n",
      "Epoch [4894/20000], Loss: 963.968994140625, Entropy 271.20751953125, Learning Rate: 0.0025\n",
      "Epoch [4895/20000], Loss: 968.6737060546875, Entropy 279.6808776855469, Learning Rate: 0.0025\n",
      "Epoch [4896/20000], Loss: 990.4143676757812, Entropy 263.81744384765625, Learning Rate: 0.0025\n",
      "Epoch [4897/20000], Loss: 954.6927490234375, Entropy 284.657958984375, Learning Rate: 0.0025\n",
      "Epoch [4898/20000], Loss: 954.888916015625, Entropy 286.0854187011719, Learning Rate: 0.0025\n",
      "Epoch [4899/20000], Loss: 948.274658203125, Entropy 287.5390930175781, Learning Rate: 0.0025\n",
      "Epoch [4900/20000], Loss: 905.490966796875, Entropy 286.1826171875, Learning Rate: 0.0025\n",
      "Epoch [4901/20000], Loss: 978.5504150390625, Entropy 264.2577209472656, Learning Rate: 0.0025\n",
      "Epoch [4902/20000], Loss: 963.0986938476562, Entropy 285.82574462890625, Learning Rate: 0.0025\n",
      "Epoch [4903/20000], Loss: 932.6888427734375, Entropy 301.9045715332031, Learning Rate: 0.0025\n",
      "Epoch [4904/20000], Loss: 909.77978515625, Entropy 307.8766784667969, Learning Rate: 0.0025\n",
      "Epoch [4905/20000], Loss: 970.4511108398438, Entropy 278.24615478515625, Learning Rate: 0.0025\n",
      "Epoch [4906/20000], Loss: 934.7236328125, Entropy 296.94677734375, Learning Rate: 0.0025\n",
      "Epoch [4907/20000], Loss: 969.573974609375, Entropy 273.7362060546875, Learning Rate: 0.0025\n",
      "Epoch [4908/20000], Loss: 950.46240234375, Entropy 288.8783264160156, Learning Rate: 0.0025\n",
      "Epoch [4909/20000], Loss: 946.7705688476562, Entropy 284.89117431640625, Learning Rate: 0.0025\n",
      "Epoch [4910/20000], Loss: 920.4306640625, Entropy 298.2901916503906, Learning Rate: 0.0025\n",
      "Epoch [4911/20000], Loss: 951.8885498046875, Entropy 269.6925048828125, Learning Rate: 0.0025\n",
      "Epoch [4912/20000], Loss: 981.5098876953125, Entropy 286.0990905761719, Learning Rate: 0.0025\n",
      "Epoch [4913/20000], Loss: 983.56298828125, Entropy 288.5805969238281, Learning Rate: 0.0025\n",
      "Epoch [4914/20000], Loss: 975.7305297851562, Entropy 280.39691162109375, Learning Rate: 0.0025\n",
      "Epoch [4915/20000], Loss: 991.273681640625, Entropy 267.874755859375, Learning Rate: 0.0025\n",
      "Epoch [4916/20000], Loss: 982.7906494140625, Entropy 284.1568908691406, Learning Rate: 0.0025\n",
      "Epoch [4917/20000], Loss: 952.2097778320312, Entropy 291.92156982421875, Learning Rate: 0.0025\n",
      "Epoch [4918/20000], Loss: 946.1468505859375, Entropy 282.5024719238281, Learning Rate: 0.0025\n",
      "Epoch [4919/20000], Loss: 948.4492797851562, Entropy 289.67156982421875, Learning Rate: 0.0025\n",
      "Epoch [4920/20000], Loss: 941.6224365234375, Entropy 275.8730773925781, Learning Rate: 0.0025\n",
      "Epoch [4921/20000], Loss: 966.71337890625, Entropy 275.5634460449219, Learning Rate: 0.0025\n",
      "Epoch [4922/20000], Loss: 963.1292114257812, Entropy 279.05072021484375, Learning Rate: 0.0025\n",
      "Epoch [4923/20000], Loss: 926.935302734375, Entropy 281.86181640625, Learning Rate: 0.0025\n",
      "Epoch [4924/20000], Loss: 960.2578735351562, Entropy 282.38616943359375, Learning Rate: 0.0025\n",
      "Epoch [4925/20000], Loss: 972.7353515625, Entropy 287.2346496582031, Learning Rate: 0.0025\n",
      "Epoch [4926/20000], Loss: 942.029541015625, Entropy 295.8685607910156, Learning Rate: 0.0025\n",
      "Epoch [4927/20000], Loss: 990.8919677734375, Entropy 271.5013732910156, Learning Rate: 0.0025\n",
      "Epoch [4928/20000], Loss: 937.32177734375, Entropy 283.6277160644531, Learning Rate: 0.0025\n",
      "Epoch [4929/20000], Loss: 943.7528076171875, Entropy 292.9212341308594, Learning Rate: 0.0025\n",
      "Epoch [4930/20000], Loss: 944.47265625, Entropy 289.77294921875, Learning Rate: 0.0025\n",
      "Epoch [4931/20000], Loss: 956.682373046875, Entropy 291.5508117675781, Learning Rate: 0.0025\n",
      "Epoch [4932/20000], Loss: 973.89453125, Entropy 278.7680969238281, Learning Rate: 0.0025\n",
      "Epoch [4933/20000], Loss: 998.1639404296875, Entropy 284.0912780761719, Learning Rate: 0.0025\n",
      "Epoch [4934/20000], Loss: 979.666748046875, Entropy 269.6490783691406, Learning Rate: 0.0025\n",
      "Epoch [4935/20000], Loss: 918.1276245117188, Entropy 285.80718994140625, Learning Rate: 0.0025\n",
      "Epoch [4936/20000], Loss: 938.1817626953125, Entropy 285.2799377441406, Learning Rate: 0.0025\n",
      "Epoch [4937/20000], Loss: 942.5103759765625, Entropy 291.3454284667969, Learning Rate: 0.0025\n",
      "Epoch [4938/20000], Loss: 968.1630859375, Entropy 281.1739196777344, Learning Rate: 0.0025\n",
      "Epoch [4939/20000], Loss: 935.0380249023438, Entropy 285.96807861328125, Learning Rate: 0.0025\n",
      "Epoch [4940/20000], Loss: 992.245361328125, Entropy 281.62060546875, Learning Rate: 0.0025\n",
      "Epoch [4941/20000], Loss: 944.556884765625, Entropy 286.3095397949219, Learning Rate: 0.0025\n",
      "Epoch [4942/20000], Loss: 929.91943359375, Entropy 295.9364929199219, Learning Rate: 0.0025\n",
      "Epoch [4943/20000], Loss: 963.4498901367188, Entropy 281.81744384765625, Learning Rate: 0.0025\n",
      "Epoch [4944/20000], Loss: 946.82763671875, Entropy 287.3049621582031, Learning Rate: 0.0025\n",
      "Epoch [4945/20000], Loss: 950.5006103515625, Entropy 289.3651428222656, Learning Rate: 0.0025\n",
      "Epoch [4946/20000], Loss: 936.44677734375, Entropy 275.93798828125, Learning Rate: 0.0025\n",
      "Epoch [4947/20000], Loss: 937.18701171875, Entropy 292.8265075683594, Learning Rate: 0.0025\n",
      "Epoch [4948/20000], Loss: 935.144775390625, Entropy 288.5235595703125, Learning Rate: 0.0025\n",
      "Epoch [4949/20000], Loss: 980.7921142578125, Entropy 302.1972961425781, Learning Rate: 0.0025\n",
      "Epoch [4950/20000], Loss: 961.5863037109375, Entropy 285.3260803222656, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4951/20000], Loss: 956.6854248046875, Entropy 286.7184753417969, Learning Rate: 0.0025\n",
      "Epoch [4952/20000], Loss: 939.5250244140625, Entropy 277.5232238769531, Learning Rate: 0.0025\n",
      "Epoch [4953/20000], Loss: 976.9761962890625, Entropy 283.0689697265625, Learning Rate: 0.0025\n",
      "Epoch [4954/20000], Loss: 1028.50146484375, Entropy 293.59332275390625, Learning Rate: 0.0025\n",
      "Epoch [4955/20000], Loss: 931.0028076171875, Entropy 298.7452392578125, Learning Rate: 0.0025\n",
      "Epoch [4956/20000], Loss: 956.99072265625, Entropy 287.3954772949219, Learning Rate: 0.0025\n",
      "Epoch [4957/20000], Loss: 969.884033203125, Entropy 289.9309997558594, Learning Rate: 0.0025\n",
      "Epoch [4958/20000], Loss: 957.400390625, Entropy 288.2059631347656, Learning Rate: 0.0025\n",
      "Epoch [4959/20000], Loss: 1014.223876953125, Entropy 261.1889953613281, Learning Rate: 0.0025\n",
      "Epoch [4960/20000], Loss: 950.5105590820312, Entropy 285.29742431640625, Learning Rate: 0.0025\n",
      "Epoch [4961/20000], Loss: 980.46826171875, Entropy 287.2596435546875, Learning Rate: 0.0025\n",
      "Epoch [4962/20000], Loss: 972.5291748046875, Entropy 288.579345703125, Learning Rate: 0.0025\n",
      "Epoch [4963/20000], Loss: 952.715087890625, Entropy 284.3200378417969, Learning Rate: 0.0025\n",
      "Epoch [4964/20000], Loss: 952.76123046875, Entropy 276.01171875, Learning Rate: 0.0025\n",
      "Epoch [4965/20000], Loss: 979.4672241210938, Entropy 272.17315673828125, Learning Rate: 0.0025\n",
      "Epoch [4966/20000], Loss: 904.8530883789062, Entropy 295.54364013671875, Learning Rate: 0.0025\n",
      "Epoch [4967/20000], Loss: 972.892333984375, Entropy 299.3177185058594, Learning Rate: 0.0025\n",
      "Epoch [4968/20000], Loss: 955.04931640625, Entropy 280.0779113769531, Learning Rate: 0.0025\n",
      "Epoch [4969/20000], Loss: 931.1498413085938, Entropy 306.25543212890625, Learning Rate: 0.0025\n",
      "Epoch [4970/20000], Loss: 964.6519775390625, Entropy 279.7252197265625, Learning Rate: 0.0025\n",
      "Epoch [4971/20000], Loss: 951.5242919921875, Entropy 285.6228942871094, Learning Rate: 0.0025\n",
      "Epoch [4972/20000], Loss: 1000.4765625, Entropy 268.69287109375, Learning Rate: 0.0025\n",
      "Epoch [4973/20000], Loss: 977.7228393554688, Entropy 280.82037353515625, Learning Rate: 0.0025\n",
      "Epoch [4974/20000], Loss: 959.052001953125, Entropy 269.6570129394531, Learning Rate: 0.0025\n",
      "Epoch [4975/20000], Loss: 935.7694702148438, Entropy 278.79205322265625, Learning Rate: 0.0025\n",
      "Epoch [4976/20000], Loss: 924.3531494140625, Entropy 303.4547119140625, Learning Rate: 0.0025\n",
      "Epoch [4977/20000], Loss: 951.883544921875, Entropy 281.1305236816406, Learning Rate: 0.0025\n",
      "Epoch [4978/20000], Loss: 960.298095703125, Entropy 288.8294372558594, Learning Rate: 0.0025\n",
      "Epoch [4979/20000], Loss: 918.5989990234375, Entropy 292.0047302246094, Learning Rate: 0.0025\n",
      "Epoch [4980/20000], Loss: 938.509521484375, Entropy 290.8880310058594, Learning Rate: 0.0025\n",
      "Epoch [4981/20000], Loss: 984.8531494140625, Entropy 296.7565612792969, Learning Rate: 0.0025\n",
      "Epoch [4982/20000], Loss: 948.561767578125, Entropy 280.9095458984375, Learning Rate: 0.0025\n",
      "Epoch [4983/20000], Loss: 913.15478515625, Entropy 306.19482421875, Learning Rate: 0.0025\n",
      "Epoch [4984/20000], Loss: 974.6552734375, Entropy 288.3957824707031, Learning Rate: 0.0025\n",
      "Epoch [4985/20000], Loss: 964.7835693359375, Entropy 293.7190856933594, Learning Rate: 0.0025\n",
      "Epoch [4986/20000], Loss: 962.4096069335938, Entropy 287.73175048828125, Learning Rate: 0.0025\n",
      "Epoch [4987/20000], Loss: 1015.6514892578125, Entropy 277.5317077636719, Learning Rate: 0.0025\n",
      "Epoch [4988/20000], Loss: 969.4779052734375, Entropy 284.0343017578125, Learning Rate: 0.0025\n",
      "Epoch [4989/20000], Loss: 1017.6267700195312, Entropy 277.35882568359375, Learning Rate: 0.0025\n",
      "Epoch [4990/20000], Loss: 941.181640625, Entropy 286.148193359375, Learning Rate: 0.0025\n",
      "Epoch [4991/20000], Loss: 947.4371337890625, Entropy 294.5225830078125, Learning Rate: 0.0025\n",
      "Epoch [4992/20000], Loss: 978.01025390625, Entropy 300.585693359375, Learning Rate: 0.0025\n",
      "Epoch [4993/20000], Loss: 953.7565307617188, Entropy 301.47515869140625, Learning Rate: 0.0025\n",
      "Epoch [4994/20000], Loss: 965.432373046875, Entropy 282.2813720703125, Learning Rate: 0.0025\n",
      "Epoch [4995/20000], Loss: 922.0831298828125, Entropy 291.8643798828125, Learning Rate: 0.0025\n",
      "Epoch [4996/20000], Loss: 931.0675048828125, Entropy 298.9946594238281, Learning Rate: 0.0025\n",
      "Epoch [4997/20000], Loss: 925.4705810546875, Entropy 303.9554138183594, Learning Rate: 0.0025\n",
      "Epoch [4998/20000], Loss: 972.4564208984375, Entropy 275.1429138183594, Learning Rate: 0.0025\n",
      "Epoch [4999/20000], Loss: 943.3564453125, Entropy 284.9363098144531, Learning Rate: 0.0025\n",
      "Epoch [5000/20000], Loss: 955.3445434570312, Entropy 294.89312744140625, Learning Rate: 0.0025\n",
      "Epoch [5001/20000], Loss: 962.384521484375, Entropy 280.5646667480469, Learning Rate: 0.0025\n",
      "Epoch [5002/20000], Loss: 953.25341796875, Entropy 276.8761901855469, Learning Rate: 0.0025\n",
      "Epoch [5003/20000], Loss: 919.781005859375, Entropy 304.6625671386719, Learning Rate: 0.0025\n",
      "Epoch [5004/20000], Loss: 924.153564453125, Entropy 297.1520080566406, Learning Rate: 0.0025\n",
      "Epoch [5005/20000], Loss: 919.8173828125, Entropy 299.7820129394531, Learning Rate: 0.0025\n",
      "Epoch [5006/20000], Loss: 923.276611328125, Entropy 293.3853454589844, Learning Rate: 0.0025\n",
      "Epoch [5007/20000], Loss: 970.13525390625, Entropy 282.4924011230469, Learning Rate: 0.0025\n",
      "Epoch [5008/20000], Loss: 945.8609008789062, Entropy 301.21307373046875, Learning Rate: 0.0025\n",
      "Epoch [5009/20000], Loss: 939.21240234375, Entropy 300.4578552246094, Learning Rate: 0.0025\n",
      "Epoch [5010/20000], Loss: 939.4064331054688, Entropy 307.63446044921875, Learning Rate: 0.0025\n",
      "Epoch [5011/20000], Loss: 956.7952880859375, Entropy 293.4158020019531, Learning Rate: 0.0025\n",
      "Epoch [5012/20000], Loss: 945.93017578125, Entropy 298.106201171875, Learning Rate: 0.0025\n",
      "Epoch [5013/20000], Loss: 960.24267578125, Entropy 286.0460205078125, Learning Rate: 0.0025\n",
      "Epoch [5014/20000], Loss: 977.899169921875, Entropy 292.9898376464844, Learning Rate: 0.0025\n",
      "Epoch [5015/20000], Loss: 937.9691162109375, Entropy 298.1810607910156, Learning Rate: 0.0025\n",
      "Epoch [5016/20000], Loss: 950.7730102539062, Entropy 290.26971435546875, Learning Rate: 0.0025\n",
      "Epoch [5017/20000], Loss: 954.5015258789062, Entropy 314.84283447265625, Learning Rate: 0.0025\n",
      "Epoch [5018/20000], Loss: 939.6550903320312, Entropy 293.02398681640625, Learning Rate: 0.0025\n",
      "Epoch [5019/20000], Loss: 1004.22509765625, Entropy 283.9036560058594, Learning Rate: 0.0025\n",
      "Epoch [5020/20000], Loss: 941.2681884765625, Entropy 290.0232849121094, Learning Rate: 0.0025\n",
      "Epoch [5021/20000], Loss: 974.2469482421875, Entropy 275.6562194824219, Learning Rate: 0.0025\n",
      "Epoch [5022/20000], Loss: 954.5623779296875, Entropy 282.9931335449219, Learning Rate: 0.0025\n",
      "Epoch [5023/20000], Loss: 992.2598876953125, Entropy 279.0112609863281, Learning Rate: 0.0025\n",
      "Epoch [5024/20000], Loss: 967.5604248046875, Entropy 288.9836730957031, Learning Rate: 0.0025\n",
      "Epoch [5025/20000], Loss: 946.3929443359375, Entropy 285.7831726074219, Learning Rate: 0.0025\n",
      "Epoch [5026/20000], Loss: 958.7653198242188, Entropy 282.35943603515625, Learning Rate: 0.0025\n",
      "Epoch [5027/20000], Loss: 942.2637939453125, Entropy 297.4775695800781, Learning Rate: 0.0025\n",
      "Epoch [5028/20000], Loss: 921.4598388671875, Entropy 289.7821350097656, Learning Rate: 0.0025\n",
      "Epoch [5029/20000], Loss: 967.7689819335938, Entropy 297.12554931640625, Learning Rate: 0.0025\n",
      "Epoch [5030/20000], Loss: 967.3561401367188, Entropy 295.09259033203125, Learning Rate: 0.0025\n",
      "Epoch [5031/20000], Loss: 974.7298583984375, Entropy 288.3111572265625, Learning Rate: 0.0025\n",
      "Epoch [5032/20000], Loss: 993.1624755859375, Entropy 286.3505859375, Learning Rate: 0.0025\n",
      "Epoch [5033/20000], Loss: 933.222900390625, Entropy 295.3300476074219, Learning Rate: 0.0025\n",
      "Epoch [5034/20000], Loss: 954.931396484375, Entropy 280.3703308105469, Learning Rate: 0.0025\n",
      "Epoch [5035/20000], Loss: 931.908935546875, Entropy 293.0956726074219, Learning Rate: 0.0025\n",
      "Epoch [5036/20000], Loss: 933.682373046875, Entropy 290.9459228515625, Learning Rate: 0.0025\n",
      "Epoch [5037/20000], Loss: 924.2147216796875, Entropy 302.9221496582031, Learning Rate: 0.0025\n",
      "Epoch [5038/20000], Loss: 931.92041015625, Entropy 306.718505859375, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5039/20000], Loss: 937.1043701171875, Entropy 296.6044006347656, Learning Rate: 0.0025\n",
      "Epoch [5040/20000], Loss: 1017.9757690429688, Entropy 302.71453857421875, Learning Rate: 0.0025\n",
      "Epoch [5041/20000], Loss: 978.8797607421875, Entropy 295.8223876953125, Learning Rate: 0.0025\n",
      "Epoch [5042/20000], Loss: 943.1597900390625, Entropy 284.1046447753906, Learning Rate: 0.0025\n",
      "Epoch [5043/20000], Loss: 981.241943359375, Entropy 279.4118957519531, Learning Rate: 0.0025\n",
      "Epoch [5044/20000], Loss: 939.2529296875, Entropy 291.4411315917969, Learning Rate: 0.0025\n",
      "Epoch [5045/20000], Loss: 967.79443359375, Entropy 290.3223876953125, Learning Rate: 0.0025\n",
      "Epoch [5046/20000], Loss: 962.1354370117188, Entropy 297.33905029296875, Learning Rate: 0.0025\n",
      "Epoch [5047/20000], Loss: 932.5657958984375, Entropy 307.2432861328125, Learning Rate: 0.0025\n",
      "Epoch [5048/20000], Loss: 953.7490234375, Entropy 285.2576904296875, Learning Rate: 0.0025\n",
      "Epoch [5049/20000], Loss: 972.9818115234375, Entropy 290.4975891113281, Learning Rate: 0.0025\n",
      "Epoch [5050/20000], Loss: 921.590087890625, Entropy 303.1015625, Learning Rate: 0.0025\n",
      "Epoch [5051/20000], Loss: 921.262451171875, Entropy 307.6817321777344, Learning Rate: 0.0025\n",
      "Epoch [5052/20000], Loss: 996.4515380859375, Entropy 281.158935546875, Learning Rate: 0.0025\n",
      "Epoch [5053/20000], Loss: 1026.121826171875, Entropy 276.107421875, Learning Rate: 0.0025\n",
      "Epoch [5054/20000], Loss: 954.912109375, Entropy 308.3612060546875, Learning Rate: 0.0025\n",
      "Epoch [5055/20000], Loss: 1005.2398071289062, Entropy 294.77020263671875, Learning Rate: 0.0025\n",
      "Epoch [5056/20000], Loss: 981.0501708984375, Entropy 295.5518798828125, Learning Rate: 0.0025\n",
      "Epoch [5057/20000], Loss: 927.9503784179688, Entropy 290.00286865234375, Learning Rate: 0.0025\n",
      "Epoch [5058/20000], Loss: 935.72802734375, Entropy 288.6619567871094, Learning Rate: 0.0025\n",
      "Epoch [5059/20000], Loss: 959.8389892578125, Entropy 287.20068359375, Learning Rate: 0.0025\n",
      "Epoch [5060/20000], Loss: 947.78759765625, Entropy 289.3815612792969, Learning Rate: 0.0025\n",
      "Epoch [5061/20000], Loss: 935.7227783203125, Entropy 296.2906188964844, Learning Rate: 0.0025\n",
      "Epoch [5062/20000], Loss: 945.3878173828125, Entropy 284.750732421875, Learning Rate: 0.0025\n",
      "Epoch [5063/20000], Loss: 985.0758056640625, Entropy 284.4863586425781, Learning Rate: 0.0025\n",
      "Epoch [5064/20000], Loss: 971.6619262695312, Entropy 273.86932373046875, Learning Rate: 0.0025\n",
      "Epoch [5065/20000], Loss: 928.4287109375, Entropy 289.5704345703125, Learning Rate: 0.0025\n",
      "Epoch [5066/20000], Loss: 1000.477294921875, Entropy 295.5892639160156, Learning Rate: 0.0025\n",
      "Epoch [5067/20000], Loss: 939.845947265625, Entropy 305.9075622558594, Learning Rate: 0.0025\n",
      "Epoch [5068/20000], Loss: 988.178466796875, Entropy 288.1203918457031, Learning Rate: 0.0025\n",
      "Epoch [5069/20000], Loss: 932.5217895507812, Entropy 294.32415771484375, Learning Rate: 0.0025\n",
      "Epoch [5070/20000], Loss: 995.201171875, Entropy 294.3497314453125, Learning Rate: 0.0025\n",
      "Epoch [5071/20000], Loss: 946.656005859375, Entropy 303.2524719238281, Learning Rate: 0.0025\n",
      "Epoch [5072/20000], Loss: 972.8236083984375, Entropy 307.6332092285156, Learning Rate: 0.0025\n",
      "Epoch [5073/20000], Loss: 956.875732421875, Entropy 294.2284851074219, Learning Rate: 0.0025\n",
      "Epoch [5074/20000], Loss: 938.4552001953125, Entropy 305.814453125, Learning Rate: 0.0025\n",
      "Epoch [5075/20000], Loss: 971.9515380859375, Entropy 292.922607421875, Learning Rate: 0.0025\n",
      "Epoch [5076/20000], Loss: 962.70947265625, Entropy 292.0750427246094, Learning Rate: 0.0025\n",
      "Epoch [5077/20000], Loss: 986.68701171875, Entropy 278.7785339355469, Learning Rate: 0.0025\n",
      "Epoch [5078/20000], Loss: 974.1063232421875, Entropy 291.408935546875, Learning Rate: 0.0025\n",
      "Epoch [5079/20000], Loss: 940.0841064453125, Entropy 294.4700012207031, Learning Rate: 0.0025\n",
      "Epoch [5080/20000], Loss: 906.701904296875, Entropy 310.6966247558594, Learning Rate: 0.0025\n",
      "Epoch [5081/20000], Loss: 1029.825927734375, Entropy 284.3715515136719, Learning Rate: 0.0025\n",
      "Epoch [5082/20000], Loss: 956.3311767578125, Entropy 296.7272033691406, Learning Rate: 0.0025\n",
      "Epoch [5083/20000], Loss: 925.5890502929688, Entropy 299.64544677734375, Learning Rate: 0.0025\n",
      "Epoch [5084/20000], Loss: 950.8772583007812, Entropy 303.58453369140625, Learning Rate: 0.0025\n",
      "Epoch [5085/20000], Loss: 943.91259765625, Entropy 292.9807434082031, Learning Rate: 0.0025\n",
      "Epoch [5086/20000], Loss: 974.6365966796875, Entropy 306.6536560058594, Learning Rate: 0.0025\n",
      "Epoch [5087/20000], Loss: 891.6708374023438, Entropy 308.42132568359375, Learning Rate: 0.0025\n",
      "Epoch [5088/20000], Loss: 978.4913330078125, Entropy 296.0400695800781, Learning Rate: 0.0025\n",
      "Epoch [5089/20000], Loss: 971.6912841796875, Entropy 304.5007629394531, Learning Rate: 0.0025\n",
      "Epoch [5090/20000], Loss: 1008.5798950195312, Entropy 288.02154541015625, Learning Rate: 0.0025\n",
      "Epoch [5091/20000], Loss: 940.3021240234375, Entropy 293.3682556152344, Learning Rate: 0.0025\n",
      "Epoch [5092/20000], Loss: 953.773681640625, Entropy 286.55322265625, Learning Rate: 0.0025\n",
      "Epoch [5093/20000], Loss: 937.2059326171875, Entropy 287.5944519042969, Learning Rate: 0.0025\n",
      "Epoch [5094/20000], Loss: 979.8147583007812, Entropy 307.04925537109375, Learning Rate: 0.0025\n",
      "Epoch [5095/20000], Loss: 969.6795654296875, Entropy 304.0226745605469, Learning Rate: 0.0025\n",
      "Epoch [5096/20000], Loss: 994.6856079101562, Entropy 284.26251220703125, Learning Rate: 0.0025\n",
      "Epoch [5097/20000], Loss: 1004.5819091796875, Entropy 303.8083801269531, Learning Rate: 0.0025\n",
      "Epoch [5098/20000], Loss: 956.3665771484375, Entropy 294.6892395019531, Learning Rate: 0.0025\n",
      "Epoch [5099/20000], Loss: 1002.1656494140625, Entropy 286.6900329589844, Learning Rate: 0.0025\n",
      "Epoch [5100/20000], Loss: 946.62353515625, Entropy 305.130615234375, Learning Rate: 0.0025\n",
      "Epoch [5101/20000], Loss: 946.1868286132812, Entropy 290.83526611328125, Learning Rate: 0.0025\n",
      "Epoch [5102/20000], Loss: 905.3660888671875, Entropy 290.906005859375, Learning Rate: 0.0025\n",
      "Epoch [5103/20000], Loss: 968.5621337890625, Entropy 300.7332458496094, Learning Rate: 0.0025\n",
      "Epoch [5104/20000], Loss: 965.6669921875, Entropy 299.4046936035156, Learning Rate: 0.0025\n",
      "Epoch [5105/20000], Loss: 975.3648681640625, Entropy 280.2486267089844, Learning Rate: 0.0025\n",
      "Epoch [5106/20000], Loss: 908.7249755859375, Entropy 307.0595703125, Learning Rate: 0.0025\n",
      "Epoch [5107/20000], Loss: 983.9213256835938, Entropy 303.02471923828125, Learning Rate: 0.0025\n",
      "Epoch [5108/20000], Loss: 972.5772094726562, Entropy 283.96771240234375, Learning Rate: 0.0025\n",
      "Epoch [5109/20000], Loss: 943.284423828125, Entropy 289.4836120605469, Learning Rate: 0.0025\n",
      "Epoch [5110/20000], Loss: 953.4249267578125, Entropy 310.2687072753906, Learning Rate: 0.0025\n",
      "Epoch [5111/20000], Loss: 1010.6048583984375, Entropy 299.5875244140625, Learning Rate: 0.0025\n",
      "Epoch [5112/20000], Loss: 955.3574829101562, Entropy 298.46063232421875, Learning Rate: 0.0025\n",
      "Epoch [5113/20000], Loss: 1014.59423828125, Entropy 307.7609558105469, Learning Rate: 0.0025\n",
      "Epoch [5114/20000], Loss: 966.5988159179688, Entropy 304.38177490234375, Learning Rate: 0.0025\n",
      "Epoch [5115/20000], Loss: 1034.2216796875, Entropy 300.2054138183594, Learning Rate: 0.0025\n",
      "Epoch [5116/20000], Loss: 975.9820556640625, Entropy 307.449951171875, Learning Rate: 0.0025\n",
      "Epoch [5117/20000], Loss: 930.0717163085938, Entropy 321.22991943359375, Learning Rate: 0.0025\n",
      "Epoch [5118/20000], Loss: 974.161376953125, Entropy 298.6101379394531, Learning Rate: 0.0025\n",
      "Epoch [5119/20000], Loss: 957.1734619140625, Entropy 288.3052978515625, Learning Rate: 0.0025\n",
      "Epoch [5120/20000], Loss: 967.4144287109375, Entropy 293.0684814453125, Learning Rate: 0.0025\n",
      "Epoch [5121/20000], Loss: 993.9771728515625, Entropy 284.9415588378906, Learning Rate: 0.0025\n",
      "Epoch [5122/20000], Loss: 921.6500244140625, Entropy 301.0894775390625, Learning Rate: 0.0025\n",
      "Epoch [5123/20000], Loss: 916.831787109375, Entropy 290.69775390625, Learning Rate: 0.0025\n",
      "Epoch [5124/20000], Loss: 948.4271240234375, Entropy 297.1513671875, Learning Rate: 0.0025\n",
      "Epoch [5125/20000], Loss: 948.9957275390625, Entropy 308.0954895019531, Learning Rate: 0.0025\n",
      "Epoch [5126/20000], Loss: 961.8718872070312, Entropy 304.35247802734375, Learning Rate: 0.0025\n",
      "Epoch [5127/20000], Loss: 926.7041015625, Entropy 307.4757080078125, Learning Rate: 0.0025\n",
      "Epoch [5128/20000], Loss: 985.0127563476562, Entropy 301.32122802734375, Learning Rate: 0.0025\n",
      "Epoch [5129/20000], Loss: 958.8509521484375, Entropy 304.4315185546875, Learning Rate: 0.0025\n",
      "Epoch [5130/20000], Loss: 937.2760620117188, Entropy 301.77777099609375, Learning Rate: 0.0025\n",
      "Epoch [5131/20000], Loss: 945.7987060546875, Entropy 301.034912109375, Learning Rate: 0.0025\n",
      "Epoch [5132/20000], Loss: 938.438232421875, Entropy 296.7414855957031, Learning Rate: 0.0025\n",
      "Epoch [5133/20000], Loss: 978.8577880859375, Entropy 307.2580871582031, Learning Rate: 0.0025\n",
      "Epoch [5134/20000], Loss: 961.3720703125, Entropy 295.4523010253906, Learning Rate: 0.0025\n",
      "Epoch [5135/20000], Loss: 933.2099609375, Entropy 291.5283203125, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5136/20000], Loss: 914.2030029296875, Entropy 303.8728942871094, Learning Rate: 0.0025\n",
      "Epoch [5137/20000], Loss: 929.439453125, Entropy 313.169921875, Learning Rate: 0.0025\n",
      "Epoch [5138/20000], Loss: 935.619384765625, Entropy 296.7798156738281, Learning Rate: 0.0025\n",
      "Epoch [5139/20000], Loss: 1037.9931640625, Entropy 275.5827941894531, Learning Rate: 0.0025\n",
      "Epoch [5140/20000], Loss: 940.7747802734375, Entropy 307.9916076660156, Learning Rate: 0.0025\n",
      "Epoch [5141/20000], Loss: 997.4425048828125, Entropy 296.531494140625, Learning Rate: 0.0025\n",
      "Epoch [5142/20000], Loss: 928.06689453125, Entropy 302.5396423339844, Learning Rate: 0.0025\n",
      "Epoch [5143/20000], Loss: 888.130126953125, Entropy 321.256591796875, Learning Rate: 0.0025\n",
      "Epoch [5144/20000], Loss: 973.836181640625, Entropy 308.9086608886719, Learning Rate: 0.0025\n",
      "Epoch [5145/20000], Loss: 1028.3673095703125, Entropy 294.8404541015625, Learning Rate: 0.0025\n",
      "Epoch [5146/20000], Loss: 983.1456298828125, Entropy 296.2860107421875, Learning Rate: 0.0025\n",
      "Epoch [5147/20000], Loss: 1036.8004150390625, Entropy 305.8251037597656, Learning Rate: 0.0025\n",
      "Epoch [5148/20000], Loss: 1069.6455078125, Entropy 295.96282958984375, Learning Rate: 0.0025\n",
      "Epoch [5149/20000], Loss: 979.8045654296875, Entropy 304.4031677246094, Learning Rate: 0.0025\n",
      "Epoch [5150/20000], Loss: 1097.4281005859375, Entropy 316.1413269042969, Learning Rate: 0.0025\n",
      "Epoch [5151/20000], Loss: 998.2286376953125, Entropy 323.8447570800781, Learning Rate: 0.0025\n",
      "Epoch [5152/20000], Loss: 1112.632568359375, Entropy 308.431640625, Learning Rate: 0.0025\n",
      "Epoch [5153/20000], Loss: 1103.160888671875, Entropy 290.928466796875, Learning Rate: 0.0025\n",
      "Epoch [5154/20000], Loss: 969.21875, Entropy 313.67041015625, Learning Rate: 0.0025\n",
      "Epoch [5155/20000], Loss: 1149.09521484375, Entropy 303.13140869140625, Learning Rate: 0.0025\n",
      "Epoch [5156/20000], Loss: 1066.671630859375, Entropy 295.6784973144531, Learning Rate: 0.0025\n",
      "Epoch [5157/20000], Loss: 1046.118408203125, Entropy 307.8675231933594, Learning Rate: 0.0025\n",
      "Epoch [5158/20000], Loss: 1105.182861328125, Entropy 314.8747253417969, Learning Rate: 0.0025\n",
      "Epoch [5159/20000], Loss: 1057.73193359375, Entropy 306.88079833984375, Learning Rate: 0.0025\n",
      "Epoch [5160/20000], Loss: 1081.3271484375, Entropy 290.3346862792969, Learning Rate: 0.0025\n",
      "Epoch [5161/20000], Loss: 1015.9104614257812, Entropy 310.21539306640625, Learning Rate: 0.0025\n",
      "Epoch [5162/20000], Loss: 1058.300537109375, Entropy 295.43896484375, Learning Rate: 0.0025\n",
      "Epoch [5163/20000], Loss: 1044.0250244140625, Entropy 287.3450012207031, Learning Rate: 0.0025\n",
      "Epoch [5164/20000], Loss: 1071.8662109375, Entropy 321.39654541015625, Learning Rate: 0.0025\n",
      "Epoch [5165/20000], Loss: 933.2387084960938, Entropy 314.71990966796875, Learning Rate: 0.0025\n",
      "Epoch [5166/20000], Loss: 1036.275390625, Entropy 294.5041809082031, Learning Rate: 0.0025\n",
      "Epoch [5167/20000], Loss: 1021.8642578125, Entropy 291.1212463378906, Learning Rate: 0.0025\n",
      "Epoch [5168/20000], Loss: 1011.2416381835938, Entropy 301.45111083984375, Learning Rate: 0.0025\n",
      "Epoch [5169/20000], Loss: 1074.233642578125, Entropy 291.2500915527344, Learning Rate: 0.0025\n",
      "Epoch [5170/20000], Loss: 944.2978515625, Entropy 313.5302734375, Learning Rate: 0.0025\n",
      "Epoch [5171/20000], Loss: 1091.258056640625, Entropy 295.8094787597656, Learning Rate: 0.0025\n",
      "Epoch [5172/20000], Loss: 935.1781005859375, Entropy 301.1736755371094, Learning Rate: 0.0025\n",
      "Epoch [5173/20000], Loss: 974.0598754882812, Entropy 307.05267333984375, Learning Rate: 0.0025\n",
      "Epoch [5174/20000], Loss: 941.1939086914062, Entropy 316.46783447265625, Learning Rate: 0.0025\n",
      "Epoch [5175/20000], Loss: 983.6442260742188, Entropy 299.96734619140625, Learning Rate: 0.0025\n",
      "Epoch [5176/20000], Loss: 1005.72509765625, Entropy 296.0799865722656, Learning Rate: 0.0025\n",
      "Epoch [5177/20000], Loss: 963.90478515625, Entropy 314.9780578613281, Learning Rate: 0.0025\n",
      "Epoch [5178/20000], Loss: 976.1829833984375, Entropy 299.716064453125, Learning Rate: 0.0025\n",
      "Epoch [5179/20000], Loss: 966.1327514648438, Entropy 320.97723388671875, Learning Rate: 0.0025\n",
      "Epoch [5180/20000], Loss: 985.546875, Entropy 299.890625, Learning Rate: 0.0025\n",
      "Epoch [5181/20000], Loss: 1010.0438232421875, Entropy 295.5715637207031, Learning Rate: 0.0025\n",
      "Epoch [5182/20000], Loss: 986.90185546875, Entropy 310.8016662597656, Learning Rate: 0.0025\n",
      "Epoch [5183/20000], Loss: 1035.2864990234375, Entropy 304.1294860839844, Learning Rate: 0.0025\n",
      "Epoch [5184/20000], Loss: 1003.308837890625, Entropy 291.0924987792969, Learning Rate: 0.0025\n",
      "Epoch [5185/20000], Loss: 1001.03369140625, Entropy 303.7772216796875, Learning Rate: 0.0025\n",
      "Epoch [5186/20000], Loss: 1056.574462890625, Entropy 307.23480224609375, Learning Rate: 0.0025\n",
      "Epoch [5187/20000], Loss: 984.743408203125, Entropy 306.7015686035156, Learning Rate: 0.0025\n",
      "Epoch [5188/20000], Loss: 975.9093017578125, Entropy 294.7550048828125, Learning Rate: 0.0025\n",
      "Epoch [5189/20000], Loss: 963.1525268554688, Entropy 302.32098388671875, Learning Rate: 0.0025\n",
      "Epoch [5190/20000], Loss: 936.489501953125, Entropy 309.2179260253906, Learning Rate: 0.0025\n",
      "Epoch [5191/20000], Loss: 973.32373046875, Entropy 295.8778991699219, Learning Rate: 0.0025\n",
      "Epoch [5192/20000], Loss: 953.1982421875, Entropy 307.2200012207031, Learning Rate: 0.0025\n",
      "Epoch [5193/20000], Loss: 1008.5252685546875, Entropy 287.3946533203125, Learning Rate: 0.0025\n",
      "Epoch [5194/20000], Loss: 987.8687744140625, Entropy 306.5323486328125, Learning Rate: 0.0025\n",
      "Epoch [5195/20000], Loss: 1006.0850830078125, Entropy 298.8096008300781, Learning Rate: 0.0025\n",
      "Epoch [5196/20000], Loss: 922.849365234375, Entropy 320.6522521972656, Learning Rate: 0.0025\n",
      "Epoch [5197/20000], Loss: 989.5635986328125, Entropy 308.0735168457031, Learning Rate: 0.0025\n",
      "Epoch [5198/20000], Loss: 934.0576171875, Entropy 305.7120361328125, Learning Rate: 0.0025\n",
      "Epoch [5199/20000], Loss: 935.5460205078125, Entropy 317.3616943359375, Learning Rate: 0.0025\n",
      "Epoch [5200/20000], Loss: 900.0853271484375, Entropy 309.8735656738281, Learning Rate: 0.0025\n",
      "Epoch [5201/20000], Loss: 945.23095703125, Entropy 302.2707824707031, Learning Rate: 0.0025\n",
      "Epoch [5202/20000], Loss: 974.673095703125, Entropy 299.9779968261719, Learning Rate: 0.0025\n",
      "Epoch [5203/20000], Loss: 958.3492431640625, Entropy 310.5615539550781, Learning Rate: 0.0025\n",
      "Epoch [5204/20000], Loss: 972.89404296875, Entropy 297.7073669433594, Learning Rate: 0.0025\n",
      "Epoch [5205/20000], Loss: 966.186279296875, Entropy 299.5648498535156, Learning Rate: 0.0025\n",
      "Epoch [5206/20000], Loss: 914.1455688476562, Entropy 291.65826416015625, Learning Rate: 0.0025\n",
      "Epoch [5207/20000], Loss: 963.5455322265625, Entropy 303.5031433105469, Learning Rate: 0.0025\n",
      "Epoch [5208/20000], Loss: 970.87890625, Entropy 300.39306640625, Learning Rate: 0.0025\n",
      "Epoch [5209/20000], Loss: 977.3251953125, Entropy 295.7166442871094, Learning Rate: 0.0025\n",
      "Epoch [5210/20000], Loss: 897.021240234375, Entropy 294.6391296386719, Learning Rate: 0.0025\n",
      "Epoch [5211/20000], Loss: 1023.0101318359375, Entropy 312.7423095703125, Learning Rate: 0.0025\n",
      "Epoch [5212/20000], Loss: 938.7174072265625, Entropy 308.129638671875, Learning Rate: 0.0025\n",
      "Epoch [5213/20000], Loss: 1071.44775390625, Entropy 302.1640930175781, Learning Rate: 0.0025\n",
      "Epoch [5214/20000], Loss: 975.447021484375, Entropy 290.66650390625, Learning Rate: 0.0025\n",
      "Epoch [5215/20000], Loss: 1003.1412353515625, Entropy 309.9605407714844, Learning Rate: 0.0025\n",
      "Epoch [5216/20000], Loss: 985.4842529296875, Entropy 297.0780334472656, Learning Rate: 0.0025\n",
      "Epoch [5217/20000], Loss: 961.8596801757812, Entropy 289.80023193359375, Learning Rate: 0.0025\n",
      "Epoch [5218/20000], Loss: 956.2100830078125, Entropy 296.8408203125, Learning Rate: 0.0025\n",
      "Epoch [5219/20000], Loss: 1057.0394287109375, Entropy 300.1036376953125, Learning Rate: 0.0025\n",
      "Epoch [5220/20000], Loss: 978.979248046875, Entropy 313.5461730957031, Learning Rate: 0.0025\n",
      "Epoch [5221/20000], Loss: 1026.8111572265625, Entropy 297.4228515625, Learning Rate: 0.0025\n",
      "Epoch [5222/20000], Loss: 931.5235595703125, Entropy 297.8357849121094, Learning Rate: 0.0025\n",
      "Epoch [5223/20000], Loss: 981.861328125, Entropy 297.9841003417969, Learning Rate: 0.0025\n",
      "Epoch [5224/20000], Loss: 1046.47119140625, Entropy 297.5745544433594, Learning Rate: 0.0025\n",
      "Epoch [5225/20000], Loss: 1051.8824462890625, Entropy 298.7946472167969, Learning Rate: 0.0025\n",
      "Epoch [5226/20000], Loss: 1070.9544677734375, Entropy 300.825927734375, Learning Rate: 0.0025\n",
      "Epoch [5227/20000], Loss: 1078.66748046875, Entropy 306.39178466796875, Learning Rate: 0.0025\n",
      "Epoch [5228/20000], Loss: 964.7845458984375, Entropy 314.8940734863281, Learning Rate: 0.0025\n",
      "Epoch [5229/20000], Loss: 1066.3319091796875, Entropy 287.7936706542969, Learning Rate: 0.0025\n",
      "Epoch [5230/20000], Loss: 966.943115234375, Entropy 308.3447265625, Learning Rate: 0.0025\n",
      "Epoch [5231/20000], Loss: 999.927734375, Entropy 302.4014892578125, Learning Rate: 0.0025\n",
      "Epoch [5232/20000], Loss: 1030.588134765625, Entropy 299.43975830078125, Learning Rate: 0.0025\n",
      "Epoch [5233/20000], Loss: 979.3963623046875, Entropy 308.3229675292969, Learning Rate: 0.0025\n",
      "Epoch [5234/20000], Loss: 1005.3643798828125, Entropy 300.934814453125, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5235/20000], Loss: 1025.101806640625, Entropy 301.78033447265625, Learning Rate: 0.0025\n",
      "Epoch [5236/20000], Loss: 1018.2662353515625, Entropy 306.8372497558594, Learning Rate: 0.0025\n",
      "Epoch [5237/20000], Loss: 953.4302978515625, Entropy 310.9429016113281, Learning Rate: 0.0025\n",
      "Epoch [5238/20000], Loss: 1164.6998291015625, Entropy 300.2672119140625, Learning Rate: 0.0025\n",
      "Epoch [5239/20000], Loss: 1001.2052001953125, Entropy 305.9902038574219, Learning Rate: 0.0025\n",
      "Epoch [5240/20000], Loss: 1282.1229248046875, Entropy 301.5466613769531, Learning Rate: 0.0025\n",
      "Epoch [5241/20000], Loss: 1075.5235595703125, Entropy 296.5647888183594, Learning Rate: 0.0025\n",
      "Epoch [5242/20000], Loss: 1262.32958984375, Entropy 292.0341796875, Learning Rate: 0.0025\n",
      "Epoch [5243/20000], Loss: 1173.980712890625, Entropy 304.33587646484375, Learning Rate: 0.0025\n",
      "Epoch [5244/20000], Loss: 1059.799560546875, Entropy 297.24041748046875, Learning Rate: 0.0025\n",
      "Epoch [5245/20000], Loss: 1162.2672119140625, Entropy 292.6737976074219, Learning Rate: 0.0025\n",
      "Epoch [5246/20000], Loss: 1148.1116943359375, Entropy 301.3877868652344, Learning Rate: 0.0025\n",
      "Epoch [5247/20000], Loss: 1056.86279296875, Entropy 291.64715576171875, Learning Rate: 0.0025\n",
      "Epoch [5248/20000], Loss: 1146.744140625, Entropy 306.2664489746094, Learning Rate: 0.0025\n",
      "Epoch [5249/20000], Loss: 1054.35302734375, Entropy 293.1430358886719, Learning Rate: 0.0025\n",
      "Epoch [5250/20000], Loss: 1203.5782470703125, Entropy 290.8195495605469, Learning Rate: 0.0025\n",
      "Epoch [5251/20000], Loss: 1439.3812255859375, Entropy 317.5616760253906, Learning Rate: 0.0025\n",
      "Epoch [5252/20000], Loss: 1506.13134765625, Entropy 300.28240966796875, Learning Rate: 0.0025\n",
      "Epoch [5253/20000], Loss: 1565.1712646484375, Entropy 288.2849426269531, Learning Rate: 0.0025\n",
      "Epoch [5254/20000], Loss: 1180.3209228515625, Entropy 301.3707275390625, Learning Rate: 0.0025\n",
      "Epoch [5255/20000], Loss: 1380.889892578125, Entropy 298.0238342285156, Learning Rate: 0.0025\n",
      "Epoch [5256/20000], Loss: 1718.71240234375, Entropy 301.0959167480469, Learning Rate: 0.0025\n",
      "Epoch [5257/20000], Loss: 1331.2548828125, Entropy 284.08056640625, Learning Rate: 0.0025\n",
      "Epoch [5258/20000], Loss: 1946.91455078125, Entropy 312.42095947265625, Learning Rate: 0.0025\n",
      "Epoch [5259/20000], Loss: 1499.353515625, Entropy 309.5523986816406, Learning Rate: 0.0025\n",
      "Epoch [5260/20000], Loss: 1248.5711669921875, Entropy 293.08349609375, Learning Rate: 0.0025\n",
      "Epoch [5261/20000], Loss: 1533.903564453125, Entropy 282.9828796386719, Learning Rate: 0.0025\n",
      "Epoch [5262/20000], Loss: 1400.9423828125, Entropy 303.61468505859375, Learning Rate: 0.0025\n",
      "Epoch [5263/20000], Loss: 1208.14013671875, Entropy 312.29364013671875, Learning Rate: 0.0025\n",
      "Epoch [5264/20000], Loss: 1773.530517578125, Entropy 299.7978515625, Learning Rate: 0.0025\n",
      "Epoch [5265/20000], Loss: 939.318359375, Entropy 306.9241027832031, Learning Rate: 0.0025\n",
      "Epoch [5266/20000], Loss: 1576.2957763671875, Entropy 296.3155517578125, Learning Rate: 0.0025\n",
      "Epoch [5267/20000], Loss: 1676.2130126953125, Entropy 300.8154602050781, Learning Rate: 0.0025\n",
      "Epoch [5268/20000], Loss: 1391.9056396484375, Entropy 287.4882507324219, Learning Rate: 0.0025\n",
      "Epoch [5269/20000], Loss: 1781.9476318359375, Entropy 289.7616271972656, Learning Rate: 0.0025\n",
      "Epoch [5270/20000], Loss: 1453.628662109375, Entropy 314.11883544921875, Learning Rate: 0.0025\n",
      "Epoch [5271/20000], Loss: 1626.10205078125, Entropy 290.74517822265625, Learning Rate: 0.0025\n",
      "Epoch [5272/20000], Loss: 1072.3411865234375, Entropy 287.8954162597656, Learning Rate: 0.0025\n",
      "Epoch [5273/20000], Loss: 1315.6251220703125, Entropy 299.1792297363281, Learning Rate: 0.0025\n",
      "Epoch [5274/20000], Loss: 1179.4542236328125, Entropy 291.9238586425781, Learning Rate: 0.0025\n",
      "Epoch [5275/20000], Loss: 1197.963134765625, Entropy 284.62310791015625, Learning Rate: 0.0025\n",
      "Epoch [5276/20000], Loss: 1251.0711669921875, Entropy 288.5047607421875, Learning Rate: 0.0025\n",
      "Epoch [5277/20000], Loss: 1135.2967529296875, Entropy 285.3221130371094, Learning Rate: 0.0025\n",
      "Epoch [5278/20000], Loss: 1398.9051513671875, Entropy 268.9582214355469, Learning Rate: 0.0025\n",
      "Epoch [5279/20000], Loss: 1019.9790649414062, Entropy 263.82635498046875, Learning Rate: 0.0025\n",
      "Epoch [5280/20000], Loss: 1353.8016357421875, Entropy 278.035888671875, Learning Rate: 0.0025\n",
      "Epoch [5281/20000], Loss: 1022.21630859375, Entropy 287.9572448730469, Learning Rate: 0.0025\n",
      "Epoch [5282/20000], Loss: 1121.732177734375, Entropy 254.55584716796875, Learning Rate: 0.0025\n",
      "Epoch [5283/20000], Loss: 1175.85791015625, Entropy 267.34912109375, Learning Rate: 0.0025\n",
      "Epoch [5284/20000], Loss: 936.3641357421875, Entropy 291.5898742675781, Learning Rate: 0.0025\n",
      "Epoch [5285/20000], Loss: 1080.8858642578125, Entropy 286.4078369140625, Learning Rate: 0.0025\n",
      "Epoch [5286/20000], Loss: 1180.5850830078125, Entropy 272.11181640625, Learning Rate: 0.0025\n",
      "Epoch [5287/20000], Loss: 1037.3289794921875, Entropy 259.6209411621094, Learning Rate: 0.0025\n",
      "Epoch [5288/20000], Loss: 1127.192626953125, Entropy 287.1501770019531, Learning Rate: 0.0025\n",
      "Epoch [5289/20000], Loss: 1041.2471923828125, Entropy 287.7589111328125, Learning Rate: 0.0025\n",
      "Epoch [5290/20000], Loss: 989.8499755859375, Entropy 279.4309387207031, Learning Rate: 0.0025\n",
      "Epoch [5291/20000], Loss: 1062.42138671875, Entropy 269.6383056640625, Learning Rate: 0.0025\n",
      "Epoch [5292/20000], Loss: 998.925537109375, Entropy 271.4097595214844, Learning Rate: 0.0025\n",
      "Epoch [5293/20000], Loss: 986.152587890625, Entropy 261.6867980957031, Learning Rate: 0.0025\n",
      "Epoch [5294/20000], Loss: 1032.682373046875, Entropy 280.65350341796875, Learning Rate: 0.0025\n",
      "Epoch [5295/20000], Loss: 967.7189331054688, Entropy 294.56536865234375, Learning Rate: 0.0025\n",
      "Epoch [5296/20000], Loss: 920.0157470703125, Entropy 291.1029357910156, Learning Rate: 0.0025\n",
      "Epoch [5297/20000], Loss: 1006.6146240234375, Entropy 275.2664489746094, Learning Rate: 0.0025\n",
      "Epoch [5298/20000], Loss: 1068.0933837890625, Entropy 289.4821472167969, Learning Rate: 0.0025\n",
      "Epoch [5299/20000], Loss: 1048.663818359375, Entropy 268.01361083984375, Learning Rate: 0.0025\n",
      "Epoch [5300/20000], Loss: 1114.0185546875, Entropy 278.7154235839844, Learning Rate: 0.0025\n",
      "Epoch [5301/20000], Loss: 1012.298828125, Entropy 282.5043029785156, Learning Rate: 0.0025\n",
      "Epoch [5302/20000], Loss: 968.896484375, Entropy 266.4836120605469, Learning Rate: 0.0025\n",
      "Epoch [5303/20000], Loss: 1049.0955810546875, Entropy 279.1724548339844, Learning Rate: 0.0025\n",
      "Epoch [5304/20000], Loss: 975.9094848632812, Entropy 282.94769287109375, Learning Rate: 0.0025\n",
      "Epoch [5305/20000], Loss: 1018.6767578125, Entropy 272.7291259765625, Learning Rate: 0.0025\n",
      "Epoch [5306/20000], Loss: 1037.3448486328125, Entropy 264.9938659667969, Learning Rate: 0.0025\n",
      "Epoch [5307/20000], Loss: 956.322509765625, Entropy 291.8606872558594, Learning Rate: 0.0025\n",
      "Epoch [5308/20000], Loss: 1075.5499267578125, Entropy 274.3632507324219, Learning Rate: 0.0025\n",
      "Epoch [5309/20000], Loss: 1026.892333984375, Entropy 275.4240417480469, Learning Rate: 0.0025\n",
      "Epoch [5310/20000], Loss: 1095.056396484375, Entropy 285.71600341796875, Learning Rate: 0.0025\n",
      "Epoch [5311/20000], Loss: 1029.9949951171875, Entropy 281.3124694824219, Learning Rate: 0.0025\n",
      "Epoch [5312/20000], Loss: 975.17578125, Entropy 297.1599426269531, Learning Rate: 0.0025\n",
      "Epoch [5313/20000], Loss: 1152.8878173828125, Entropy 268.8587341308594, Learning Rate: 0.0025\n",
      "Epoch [5314/20000], Loss: 1125.095458984375, Entropy 271.9589538574219, Learning Rate: 0.0025\n",
      "Epoch [5315/20000], Loss: 1030.8291015625, Entropy 279.89373779296875, Learning Rate: 0.0025\n",
      "Epoch [5316/20000], Loss: 1363.759765625, Entropy 264.4936218261719, Learning Rate: 0.0025\n",
      "Epoch [5317/20000], Loss: 1015.468505859375, Entropy 280.1903381347656, Learning Rate: 0.0025\n",
      "Epoch [5318/20000], Loss: 1141.81689453125, Entropy 281.91998291015625, Learning Rate: 0.0025\n",
      "Epoch [5319/20000], Loss: 1282.753662109375, Entropy 278.5831604003906, Learning Rate: 0.0025\n",
      "Epoch [5320/20000], Loss: 994.4933471679688, Entropy 284.59027099609375, Learning Rate: 0.0025\n",
      "Epoch [5321/20000], Loss: 1367.572998046875, Entropy 272.6971740722656, Learning Rate: 0.0025\n",
      "Epoch [5322/20000], Loss: 1279.067138671875, Entropy 278.0721740722656, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5323/20000], Loss: 1430.662841796875, Entropy 292.81158447265625, Learning Rate: 0.0025\n",
      "Epoch [5324/20000], Loss: 1238.9078369140625, Entropy 291.3267822265625, Learning Rate: 0.0025\n",
      "Epoch [5325/20000], Loss: 1199.8310546875, Entropy 273.8606262207031, Learning Rate: 0.0025\n",
      "Epoch [5326/20000], Loss: 1351.986572265625, Entropy 276.6727294921875, Learning Rate: 0.0025\n",
      "Epoch [5327/20000], Loss: 1140.5211181640625, Entropy 272.9667053222656, Learning Rate: 0.0025\n",
      "Epoch [5328/20000], Loss: 1496.1514892578125, Entropy 258.68505859375, Learning Rate: 0.0025\n",
      "Epoch [5329/20000], Loss: 1018.326904296875, Entropy 274.6590576171875, Learning Rate: 0.0025\n",
      "Epoch [5330/20000], Loss: 1049.8035888671875, Entropy 278.7176208496094, Learning Rate: 0.0025\n",
      "Epoch [5331/20000], Loss: 1248.276611328125, Entropy 286.0745544433594, Learning Rate: 0.0025\n",
      "Epoch [5332/20000], Loss: 1016.1953735351562, Entropy 273.52301025390625, Learning Rate: 0.0025\n",
      "Epoch [5333/20000], Loss: 1170.8507080078125, Entropy 253.00662231445312, Learning Rate: 0.0025\n",
      "Epoch [5334/20000], Loss: 1050.77587890625, Entropy 268.68572998046875, Learning Rate: 0.0025\n",
      "Epoch [5335/20000], Loss: 947.6072998046875, Entropy 271.9027404785156, Learning Rate: 0.0025\n",
      "Epoch [5336/20000], Loss: 1061.3065185546875, Entropy 283.03955078125, Learning Rate: 0.0025\n",
      "Epoch [5337/20000], Loss: 978.4451904296875, Entropy 282.8168029785156, Learning Rate: 0.0025\n",
      "Epoch [5338/20000], Loss: 1046.0126953125, Entropy 274.6787109375, Learning Rate: 0.0025\n",
      "Epoch [5339/20000], Loss: 1077.726806640625, Entropy 264.55181884765625, Learning Rate: 0.0025\n",
      "Epoch [5340/20000], Loss: 989.7785034179688, Entropy 278.42901611328125, Learning Rate: 0.0025\n",
      "Epoch [5341/20000], Loss: 975.5975341796875, Entropy 270.9849853515625, Learning Rate: 0.0025\n",
      "Epoch [5342/20000], Loss: 979.653564453125, Entropy 278.2513122558594, Learning Rate: 0.0025\n",
      "Epoch [5343/20000], Loss: 1042.664306640625, Entropy 264.9681701660156, Learning Rate: 0.0025\n",
      "Epoch [5344/20000], Loss: 963.8515625, Entropy 265.9245300292969, Learning Rate: 0.0025\n",
      "Epoch [5345/20000], Loss: 982.9402465820312, Entropy 296.90289306640625, Learning Rate: 0.0025\n",
      "Epoch [5346/20000], Loss: 1051.78857421875, Entropy 263.1721496582031, Learning Rate: 0.0025\n",
      "Epoch [5347/20000], Loss: 1002.55810546875, Entropy 288.9124755859375, Learning Rate: 0.0025\n",
      "Epoch [5348/20000], Loss: 998.9520874023438, Entropy 258.07647705078125, Learning Rate: 0.0025\n",
      "Epoch [5349/20000], Loss: 1087.3778076171875, Entropy 275.5107727050781, Learning Rate: 0.0025\n",
      "Epoch [5350/20000], Loss: 974.51123046875, Entropy 268.8074035644531, Learning Rate: 0.0025\n",
      "Epoch [5351/20000], Loss: 997.2554931640625, Entropy 265.6750793457031, Learning Rate: 0.0025\n",
      "Epoch [5352/20000], Loss: 956.34033203125, Entropy 279.5332336425781, Learning Rate: 0.0025\n",
      "Epoch [5353/20000], Loss: 974.5452880859375, Entropy 278.7572937011719, Learning Rate: 0.0025\n",
      "Epoch [5354/20000], Loss: 1009.3146362304688, Entropy 276.20050048828125, Learning Rate: 0.0025\n",
      "Epoch [5355/20000], Loss: 966.456298828125, Entropy 273.9818420410156, Learning Rate: 0.0025\n",
      "Epoch [5356/20000], Loss: 994.6390380859375, Entropy 269.9667663574219, Learning Rate: 0.0025\n",
      "Epoch [5357/20000], Loss: 997.593994140625, Entropy 257.2483215332031, Learning Rate: 0.0025\n",
      "Epoch [5358/20000], Loss: 996.8232421875, Entropy 263.3602600097656, Learning Rate: 0.0025\n",
      "Epoch [5359/20000], Loss: 991.434326171875, Entropy 271.6327819824219, Learning Rate: 0.0025\n",
      "Epoch [5360/20000], Loss: 960.5084838867188, Entropy 285.12738037109375, Learning Rate: 0.0025\n",
      "Epoch [5361/20000], Loss: 1052.590576171875, Entropy 261.2143249511719, Learning Rate: 0.0025\n",
      "Epoch [5362/20000], Loss: 995.451904296875, Entropy 279.8329162597656, Learning Rate: 0.0025\n",
      "Epoch [5363/20000], Loss: 970.2010498046875, Entropy 276.4948425292969, Learning Rate: 0.0025\n",
      "Epoch [5364/20000], Loss: 1000.423095703125, Entropy 251.71212768554688, Learning Rate: 0.0025\n",
      "Epoch [5365/20000], Loss: 964.7735595703125, Entropy 280.4028015136719, Learning Rate: 0.0025\n",
      "Epoch [5366/20000], Loss: 943.8568725585938, Entropy 283.38580322265625, Learning Rate: 0.0025\n",
      "Epoch [5367/20000], Loss: 953.085693359375, Entropy 285.3538513183594, Learning Rate: 0.0025\n",
      "Epoch [5368/20000], Loss: 996.2989501953125, Entropy 283.0135803222656, Learning Rate: 0.0025\n",
      "Epoch [5369/20000], Loss: 1018.444580078125, Entropy 265.0922546386719, Learning Rate: 0.0025\n",
      "Epoch [5370/20000], Loss: 942.007568359375, Entropy 278.9097900390625, Learning Rate: 0.0025\n",
      "Epoch [5371/20000], Loss: 959.22216796875, Entropy 285.574462890625, Learning Rate: 0.0025\n",
      "Epoch [5372/20000], Loss: 941.447509765625, Entropy 293.5263977050781, Learning Rate: 0.0025\n",
      "Epoch [5373/20000], Loss: 965.181884765625, Entropy 269.1343078613281, Learning Rate: 0.0025\n",
      "Epoch [5374/20000], Loss: 963.8935546875, Entropy 282.3913269042969, Learning Rate: 0.0025\n",
      "Epoch [5375/20000], Loss: 966.326171875, Entropy 276.9971923828125, Learning Rate: 0.0025\n",
      "Epoch [5376/20000], Loss: 999.5879516601562, Entropy 274.25079345703125, Learning Rate: 0.0025\n",
      "Epoch [5377/20000], Loss: 946.48974609375, Entropy 273.8958435058594, Learning Rate: 0.0025\n",
      "Epoch [5378/20000], Loss: 972.746826171875, Entropy 260.2417297363281, Learning Rate: 0.0025\n",
      "Epoch [5379/20000], Loss: 992.7498779296875, Entropy 263.4216003417969, Learning Rate: 0.0025\n",
      "Epoch [5380/20000], Loss: 925.5003662109375, Entropy 287.827392578125, Learning Rate: 0.0025\n",
      "Epoch [5381/20000], Loss: 961.0123291015625, Entropy 283.7291564941406, Learning Rate: 0.0025\n",
      "Epoch [5382/20000], Loss: 977.8570556640625, Entropy 266.9425048828125, Learning Rate: 0.0025\n",
      "Epoch [5383/20000], Loss: 964.3630981445312, Entropy 275.02642822265625, Learning Rate: 0.0025\n",
      "Epoch [5384/20000], Loss: 936.3504638671875, Entropy 277.3466796875, Learning Rate: 0.0025\n",
      "Epoch [5385/20000], Loss: 952.0825805664062, Entropy 264.86273193359375, Learning Rate: 0.0025\n",
      "Epoch [5386/20000], Loss: 967.2562255859375, Entropy 264.4976501464844, Learning Rate: 0.0025\n",
      "Epoch [5387/20000], Loss: 946.4661865234375, Entropy 276.6620178222656, Learning Rate: 0.0025\n",
      "Epoch [5388/20000], Loss: 937.6703491210938, Entropy 285.40618896484375, Learning Rate: 0.0025\n",
      "Epoch [5389/20000], Loss: 949.7359619140625, Entropy 287.5926208496094, Learning Rate: 0.0025\n",
      "Epoch [5390/20000], Loss: 984.6195678710938, Entropy 284.09027099609375, Learning Rate: 0.0025\n",
      "Epoch [5391/20000], Loss: 968.2526245117188, Entropy 270.01763916015625, Learning Rate: 0.0025\n",
      "Epoch [5392/20000], Loss: 988.414794921875, Entropy 268.8537292480469, Learning Rate: 0.0025\n",
      "Epoch [5393/20000], Loss: 970.266357421875, Entropy 275.1370849609375, Learning Rate: 0.0025\n",
      "Epoch [5394/20000], Loss: 972.067138671875, Entropy 283.0242919921875, Learning Rate: 0.0025\n",
      "Epoch [5395/20000], Loss: 945.248291015625, Entropy 273.5984802246094, Learning Rate: 0.0025\n",
      "Epoch [5396/20000], Loss: 951.6089477539062, Entropy 271.99224853515625, Learning Rate: 0.0025\n",
      "Epoch [5397/20000], Loss: 994.9598388671875, Entropy 280.5549011230469, Learning Rate: 0.0025\n",
      "Epoch [5398/20000], Loss: 970.8035888671875, Entropy 268.4629821777344, Learning Rate: 0.0025\n",
      "Epoch [5399/20000], Loss: 984.2279663085938, Entropy 275.26092529296875, Learning Rate: 0.0025\n",
      "Epoch [5400/20000], Loss: 921.9024658203125, Entropy 282.4690246582031, Learning Rate: 0.0025\n",
      "Epoch [5401/20000], Loss: 961.86865234375, Entropy 287.0483093261719, Learning Rate: 0.0025\n",
      "Epoch [5402/20000], Loss: 969.13818359375, Entropy 284.8955383300781, Learning Rate: 0.0025\n",
      "Epoch [5403/20000], Loss: 1007.447509765625, Entropy 260.5099792480469, Learning Rate: 0.0025\n",
      "Epoch [5404/20000], Loss: 962.5419921875, Entropy 291.6763916015625, Learning Rate: 0.0025\n",
      "Epoch [5405/20000], Loss: 972.70458984375, Entropy 285.5496520996094, Learning Rate: 0.0025\n",
      "Epoch [5406/20000], Loss: 955.7938232421875, Entropy 286.2815856933594, Learning Rate: 0.0025\n",
      "Epoch [5407/20000], Loss: 922.2718505859375, Entropy 287.3585205078125, Learning Rate: 0.0025\n",
      "Epoch [5408/20000], Loss: 962.4336547851562, Entropy 287.57916259765625, Learning Rate: 0.0025\n",
      "Epoch [5409/20000], Loss: 933.3756103515625, Entropy 275.0703125, Learning Rate: 0.0025\n",
      "Epoch [5410/20000], Loss: 940.775390625, Entropy 282.4371643066406, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5411/20000], Loss: 986.3916015625, Entropy 273.7748107910156, Learning Rate: 0.0025\n",
      "Epoch [5412/20000], Loss: 1001.9744873046875, Entropy 286.0639343261719, Learning Rate: 0.0025\n",
      "Epoch [5413/20000], Loss: 923.243408203125, Entropy 295.11181640625, Learning Rate: 0.0025\n",
      "Epoch [5414/20000], Loss: 966.069091796875, Entropy 277.1365051269531, Learning Rate: 0.0025\n",
      "Epoch [5415/20000], Loss: 938.5050048828125, Entropy 288.7391052246094, Learning Rate: 0.0025\n",
      "Epoch [5416/20000], Loss: 949.1339721679688, Entropy 282.15570068359375, Learning Rate: 0.0025\n",
      "Epoch [5417/20000], Loss: 966.046142578125, Entropy 271.84912109375, Learning Rate: 0.0025\n",
      "Epoch [5418/20000], Loss: 951.378662109375, Entropy 293.9237976074219, Learning Rate: 0.0025\n",
      "Epoch [5419/20000], Loss: 986.7801513671875, Entropy 272.7190246582031, Learning Rate: 0.0025\n",
      "Epoch [5420/20000], Loss: 943.590576171875, Entropy 257.6643371582031, Learning Rate: 0.0025\n",
      "Epoch [5421/20000], Loss: 958.9931030273438, Entropy 271.65374755859375, Learning Rate: 0.0025\n",
      "Epoch [5422/20000], Loss: 961.392333984375, Entropy 286.2864074707031, Learning Rate: 0.0025\n",
      "Epoch [5423/20000], Loss: 961.4078369140625, Entropy 277.8985595703125, Learning Rate: 0.0025\n",
      "Epoch [5424/20000], Loss: 966.738525390625, Entropy 291.0867919921875, Learning Rate: 0.0025\n",
      "Epoch [5425/20000], Loss: 927.47705078125, Entropy 294.65380859375, Learning Rate: 0.0025\n",
      "Epoch [5426/20000], Loss: 986.5399169921875, Entropy 282.2307434082031, Learning Rate: 0.0025\n",
      "Epoch [5427/20000], Loss: 971.970947265625, Entropy 286.9266052246094, Learning Rate: 0.0025\n",
      "Epoch [5428/20000], Loss: 964.0269775390625, Entropy 265.9612731933594, Learning Rate: 0.0025\n",
      "Epoch [5429/20000], Loss: 982.9429321289062, Entropy 290.09381103515625, Learning Rate: 0.0025\n",
      "Epoch [5430/20000], Loss: 953.9019775390625, Entropy 278.2655029296875, Learning Rate: 0.0025\n",
      "Epoch [5431/20000], Loss: 950.3052368164062, Entropy 275.26312255859375, Learning Rate: 0.0025\n",
      "Epoch [5432/20000], Loss: 960.473388671875, Entropy 285.8351135253906, Learning Rate: 0.0025\n",
      "Epoch [5433/20000], Loss: 945.9243774414062, Entropy 297.26324462890625, Learning Rate: 0.0025\n",
      "Epoch [5434/20000], Loss: 943.406982421875, Entropy 286.7177429199219, Learning Rate: 0.0025\n",
      "Epoch [5435/20000], Loss: 962.7271728515625, Entropy 293.3077392578125, Learning Rate: 0.0025\n",
      "Epoch [5436/20000], Loss: 930.0478515625, Entropy 285.3966369628906, Learning Rate: 0.0025\n",
      "Epoch [5437/20000], Loss: 1004.709716796875, Entropy 295.1324462890625, Learning Rate: 0.0025\n",
      "Epoch [5438/20000], Loss: 957.789794921875, Entropy 283.8013610839844, Learning Rate: 0.0025\n",
      "Epoch [5439/20000], Loss: 967.2650146484375, Entropy 282.7575988769531, Learning Rate: 0.0025\n",
      "Epoch [5440/20000], Loss: 956.7422485351562, Entropy 293.48712158203125, Learning Rate: 0.0025\n",
      "Epoch [5441/20000], Loss: 977.6356201171875, Entropy 287.8031921386719, Learning Rate: 0.0025\n",
      "Epoch [5442/20000], Loss: 949.4999389648438, Entropy 310.70343017578125, Learning Rate: 0.0025\n",
      "Epoch [5443/20000], Loss: 953.744873046875, Entropy 291.696533203125, Learning Rate: 0.0025\n",
      "Epoch [5444/20000], Loss: 929.8226318359375, Entropy 288.6979675292969, Learning Rate: 0.0025\n",
      "Epoch [5445/20000], Loss: 956.2147216796875, Entropy 293.21337890625, Learning Rate: 0.0025\n",
      "Epoch [5446/20000], Loss: 972.5633544921875, Entropy 283.39208984375, Learning Rate: 0.0025\n",
      "Epoch [5447/20000], Loss: 974.05517578125, Entropy 291.9453430175781, Learning Rate: 0.0025\n",
      "Epoch [5448/20000], Loss: 947.482421875, Entropy 286.0359802246094, Learning Rate: 0.0025\n",
      "Epoch [5449/20000], Loss: 944.3271484375, Entropy 290.2485046386719, Learning Rate: 0.0025\n",
      "Epoch [5450/20000], Loss: 998.8289794921875, Entropy 268.9408264160156, Learning Rate: 0.0025\n",
      "Epoch [5451/20000], Loss: 998.1954345703125, Entropy 270.6686706542969, Learning Rate: 0.0025\n",
      "Epoch [5452/20000], Loss: 943.4238891601562, Entropy 291.11297607421875, Learning Rate: 0.0025\n",
      "Epoch [5453/20000], Loss: 982.986083984375, Entropy 289.2163391113281, Learning Rate: 0.0025\n",
      "Epoch [5454/20000], Loss: 962.3876953125, Entropy 283.6364440917969, Learning Rate: 0.0025\n",
      "Epoch [5455/20000], Loss: 962.2420654296875, Entropy 292.2353820800781, Learning Rate: 0.0025\n",
      "Epoch [5456/20000], Loss: 895.63623046875, Entropy 310.8291320800781, Learning Rate: 0.0025\n",
      "Epoch [5457/20000], Loss: 979.1680297851562, Entropy 302.58453369140625, Learning Rate: 0.0025\n",
      "Epoch [5458/20000], Loss: 958.999267578125, Entropy 296.0325012207031, Learning Rate: 0.0025\n",
      "Epoch [5459/20000], Loss: 948.1168823242188, Entropy 281.97906494140625, Learning Rate: 0.0025\n",
      "Epoch [5460/20000], Loss: 947.2678833007812, Entropy 271.50531005859375, Learning Rate: 0.0025\n",
      "Epoch [5461/20000], Loss: 973.1937866210938, Entropy 277.31512451171875, Learning Rate: 0.0025\n",
      "Epoch [5462/20000], Loss: 945.0458984375, Entropy 283.1523132324219, Learning Rate: 0.0025\n",
      "Epoch [5463/20000], Loss: 939.092041015625, Entropy 300.4941101074219, Learning Rate: 0.0025\n",
      "Epoch [5464/20000], Loss: 958.0411376953125, Entropy 277.4057312011719, Learning Rate: 0.0025\n",
      "Epoch [5465/20000], Loss: 928.3995361328125, Entropy 288.5735778808594, Learning Rate: 0.0025\n",
      "Epoch [5466/20000], Loss: 962.0952758789062, Entropy 283.82452392578125, Learning Rate: 0.0025\n",
      "Epoch [5467/20000], Loss: 949.3079833984375, Entropy 292.3055725097656, Learning Rate: 0.0025\n",
      "Epoch [5468/20000], Loss: 967.5518798828125, Entropy 296.849609375, Learning Rate: 0.0025\n",
      "Epoch [5469/20000], Loss: 956.11767578125, Entropy 280.8317565917969, Learning Rate: 0.0025\n",
      "Epoch [5470/20000], Loss: 930.0650634765625, Entropy 294.927734375, Learning Rate: 0.0025\n",
      "Epoch [5471/20000], Loss: 941.64404296875, Entropy 296.00830078125, Learning Rate: 0.0025\n",
      "Epoch [5472/20000], Loss: 971.1978759765625, Entropy 271.0404968261719, Learning Rate: 0.0025\n",
      "Epoch [5473/20000], Loss: 938.0712890625, Entropy 297.0515441894531, Learning Rate: 0.0025\n",
      "Epoch [5474/20000], Loss: 940.7748413085938, Entropy 302.44378662109375, Learning Rate: 0.0025\n",
      "Epoch [5475/20000], Loss: 979.898193359375, Entropy 288.5194396972656, Learning Rate: 0.0025\n",
      "Epoch [5476/20000], Loss: 976.89013671875, Entropy 289.0689697265625, Learning Rate: 0.0025\n",
      "Epoch [5477/20000], Loss: 921.853515625, Entropy 299.8160095214844, Learning Rate: 0.0025\n",
      "Epoch [5478/20000], Loss: 996.0302734375, Entropy 283.5123291015625, Learning Rate: 0.0025\n",
      "Epoch [5479/20000], Loss: 921.630126953125, Entropy 297.0413513183594, Learning Rate: 0.0025\n",
      "Epoch [5480/20000], Loss: 948.3953857421875, Entropy 297.19970703125, Learning Rate: 0.0025\n",
      "Epoch [5481/20000], Loss: 923.3972778320312, Entropy 289.30987548828125, Learning Rate: 0.0025\n",
      "Epoch [5482/20000], Loss: 964.2342529296875, Entropy 284.4116516113281, Learning Rate: 0.0025\n",
      "Epoch [5483/20000], Loss: 922.3325805664062, Entropy 293.10162353515625, Learning Rate: 0.0025\n",
      "Epoch [5484/20000], Loss: 950.39794921875, Entropy 292.654052734375, Learning Rate: 0.0025\n",
      "Epoch [5485/20000], Loss: 928.344970703125, Entropy 290.0833435058594, Learning Rate: 0.0025\n",
      "Epoch [5486/20000], Loss: 919.4776611328125, Entropy 287.0055236816406, Learning Rate: 0.0025\n",
      "Epoch [5487/20000], Loss: 949.96142578125, Entropy 273.8294372558594, Learning Rate: 0.0025\n",
      "Epoch [5488/20000], Loss: 918.5812377929688, Entropy 300.98492431640625, Learning Rate: 0.0025\n",
      "Epoch [5489/20000], Loss: 924.612060546875, Entropy 301.4474182128906, Learning Rate: 0.0025\n",
      "Epoch [5490/20000], Loss: 969.6162109375, Entropy 284.06005859375, Learning Rate: 0.0025\n",
      "Epoch [5491/20000], Loss: 950.84765625, Entropy 285.1173095703125, Learning Rate: 0.0025\n",
      "Epoch [5492/20000], Loss: 922.347412109375, Entropy 278.8878173828125, Learning Rate: 0.0025\n",
      "Epoch [5493/20000], Loss: 937.953369140625, Entropy 291.4152526855469, Learning Rate: 0.0025\n",
      "Epoch [5494/20000], Loss: 921.283935546875, Entropy 298.7347106933594, Learning Rate: 0.0025\n",
      "Epoch [5495/20000], Loss: 953.5494384765625, Entropy 294.0847473144531, Learning Rate: 0.0025\n",
      "Epoch [5496/20000], Loss: 921.05615234375, Entropy 295.2073059082031, Learning Rate: 0.0025\n",
      "Epoch [5497/20000], Loss: 910.512939453125, Entropy 301.6213073730469, Learning Rate: 0.0025\n",
      "Epoch [5498/20000], Loss: 938.837646484375, Entropy 307.9949951171875, Learning Rate: 0.0025\n",
      "Epoch [5499/20000], Loss: 965.5067749023438, Entropy 286.10528564453125, Learning Rate: 0.0025\n",
      "Epoch [5500/20000], Loss: 1002.7503662109375, Entropy 272.3661804199219, Learning Rate: 0.0025\n",
      "Epoch [5501/20000], Loss: 970.7515869140625, Entropy 294.9911193847656, Learning Rate: 0.0025\n",
      "Epoch [5502/20000], Loss: 1030.4857177734375, Entropy 279.0704345703125, Learning Rate: 0.0025\n",
      "Epoch [5503/20000], Loss: 954.2007446289062, Entropy 291.45831298828125, Learning Rate: 0.0025\n",
      "Epoch [5504/20000], Loss: 900.75634765625, Entropy 311.61083984375, Learning Rate: 0.0025\n",
      "Epoch [5505/20000], Loss: 934.8065185546875, Entropy 298.0086364746094, Learning Rate: 0.0025\n",
      "Epoch [5506/20000], Loss: 963.345458984375, Entropy 295.7680358886719, Learning Rate: 0.0025\n",
      "Epoch [5507/20000], Loss: 973.2318725585938, Entropy 284.29058837890625, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5508/20000], Loss: 957.6075439453125, Entropy 277.5268249511719, Learning Rate: 0.0025\n",
      "Epoch [5509/20000], Loss: 934.5741577148438, Entropy 296.59613037109375, Learning Rate: 0.0025\n",
      "Epoch [5510/20000], Loss: 941.822509765625, Entropy 313.1582336425781, Learning Rate: 0.0025\n",
      "Epoch [5511/20000], Loss: 916.628173828125, Entropy 310.0031433105469, Learning Rate: 0.0025\n",
      "Epoch [5512/20000], Loss: 959.086181640625, Entropy 304.1246643066406, Learning Rate: 0.0025\n",
      "Epoch [5513/20000], Loss: 956.26123046875, Entropy 312.4613342285156, Learning Rate: 0.0025\n",
      "Epoch [5514/20000], Loss: 974.953125, Entropy 292.0382385253906, Learning Rate: 0.0025\n",
      "Epoch [5515/20000], Loss: 918.1311645507812, Entropy 317.43719482421875, Learning Rate: 0.0025\n",
      "Epoch [5516/20000], Loss: 946.08544921875, Entropy 288.2846984863281, Learning Rate: 0.0025\n",
      "Epoch [5517/20000], Loss: 929.4041137695312, Entropy 302.18450927734375, Learning Rate: 0.0025\n",
      "Epoch [5518/20000], Loss: 955.1717529296875, Entropy 304.9481506347656, Learning Rate: 0.0025\n",
      "Epoch [5519/20000], Loss: 966.558837890625, Entropy 309.0520935058594, Learning Rate: 0.0025\n",
      "Epoch [5520/20000], Loss: 964.2855224609375, Entropy 299.6465759277344, Learning Rate: 0.0025\n",
      "Epoch [5521/20000], Loss: 926.5118408203125, Entropy 302.1505126953125, Learning Rate: 0.0025\n",
      "Epoch [5522/20000], Loss: 938.8316650390625, Entropy 293.9402770996094, Learning Rate: 0.0025\n",
      "Epoch [5523/20000], Loss: 928.1629638671875, Entropy 312.2335510253906, Learning Rate: 0.0025\n",
      "Epoch [5524/20000], Loss: 984.8426513671875, Entropy 290.2227783203125, Learning Rate: 0.0025\n",
      "Epoch [5525/20000], Loss: 961.6055297851562, Entropy 303.88433837890625, Learning Rate: 0.0025\n",
      "Epoch [5526/20000], Loss: 936.2140502929688, Entropy 289.35992431640625, Learning Rate: 0.0025\n",
      "Epoch [5527/20000], Loss: 954.308837890625, Entropy 285.9024658203125, Learning Rate: 0.0025\n",
      "Epoch [5528/20000], Loss: 917.66015625, Entropy 301.5495910644531, Learning Rate: 0.0025\n",
      "Epoch [5529/20000], Loss: 941.709228515625, Entropy 311.0633239746094, Learning Rate: 0.0025\n",
      "Epoch [5530/20000], Loss: 966.7158203125, Entropy 289.5157470703125, Learning Rate: 0.0025\n",
      "Epoch [5531/20000], Loss: 941.8770751953125, Entropy 310.3249816894531, Learning Rate: 0.0025\n",
      "Epoch [5532/20000], Loss: 964.0550537109375, Entropy 291.7356872558594, Learning Rate: 0.0025\n",
      "Epoch [5533/20000], Loss: 945.8340454101562, Entropy 316.90814208984375, Learning Rate: 0.0025\n",
      "Epoch [5534/20000], Loss: 947.0686645507812, Entropy 282.72735595703125, Learning Rate: 0.0025\n",
      "Epoch [5535/20000], Loss: 979.9569091796875, Entropy 281.7799072265625, Learning Rate: 0.0025\n",
      "Epoch [5536/20000], Loss: 916.3228759765625, Entropy 319.1996154785156, Learning Rate: 0.0025\n",
      "Epoch [5537/20000], Loss: 964.6741333007812, Entropy 298.02557373046875, Learning Rate: 0.0025\n",
      "Epoch [5538/20000], Loss: 956.8328247070312, Entropy 282.99761962890625, Learning Rate: 0.0025\n",
      "Epoch [5539/20000], Loss: 912.8085327148438, Entropy 305.29803466796875, Learning Rate: 0.0025\n",
      "Epoch [5540/20000], Loss: 948.7645263671875, Entropy 308.7810974121094, Learning Rate: 0.0025\n",
      "Epoch [5541/20000], Loss: 956.2994995117188, Entropy 305.00201416015625, Learning Rate: 0.0025\n",
      "Epoch [5542/20000], Loss: 940.141845703125, Entropy 308.0115966796875, Learning Rate: 0.0025\n",
      "Epoch [5543/20000], Loss: 984.83984375, Entropy 292.1053466796875, Learning Rate: 0.0025\n",
      "Epoch [5544/20000], Loss: 948.3310546875, Entropy 287.4677429199219, Learning Rate: 0.0025\n",
      "Epoch [5545/20000], Loss: 931.092041015625, Entropy 309.8838806152344, Learning Rate: 0.0025\n",
      "Epoch [5546/20000], Loss: 940.5116577148438, Entropy 296.35272216796875, Learning Rate: 0.0025\n",
      "Epoch [5547/20000], Loss: 931.1148681640625, Entropy 304.2999267578125, Learning Rate: 0.0025\n",
      "Epoch [5548/20000], Loss: 980.4405517578125, Entropy 285.3511962890625, Learning Rate: 0.0025\n",
      "Epoch [5549/20000], Loss: 921.814697265625, Entropy 301.4561462402344, Learning Rate: 0.0025\n",
      "Epoch [5550/20000], Loss: 958.1724243164062, Entropy 302.77349853515625, Learning Rate: 0.0025\n",
      "Epoch [5551/20000], Loss: 927.8369140625, Entropy 304.9888916015625, Learning Rate: 0.0025\n",
      "Epoch [5552/20000], Loss: 871.774658203125, Entropy 322.3887939453125, Learning Rate: 0.0025\n",
      "Epoch [5553/20000], Loss: 954.8872680664062, Entropy 293.65509033203125, Learning Rate: 0.0025\n",
      "Epoch [5554/20000], Loss: 941.522705078125, Entropy 305.7024230957031, Learning Rate: 0.0025\n",
      "Epoch [5555/20000], Loss: 914.5196533203125, Entropy 300.8421936035156, Learning Rate: 0.0025\n",
      "Epoch [5556/20000], Loss: 915.3162841796875, Entropy 311.9374084472656, Learning Rate: 0.0025\n",
      "Epoch [5557/20000], Loss: 945.2903442382812, Entropy 311.03485107421875, Learning Rate: 0.0025\n",
      "Epoch [5558/20000], Loss: 948.65478515625, Entropy 294.2323913574219, Learning Rate: 0.0025\n",
      "Epoch [5559/20000], Loss: 938.3411865234375, Entropy 297.5658264160156, Learning Rate: 0.0025\n",
      "Epoch [5560/20000], Loss: 962.6016845703125, Entropy 288.2750549316406, Learning Rate: 0.0025\n",
      "Epoch [5561/20000], Loss: 926.792236328125, Entropy 311.4647521972656, Learning Rate: 0.0025\n",
      "Epoch [5562/20000], Loss: 936.9630126953125, Entropy 305.6685791015625, Learning Rate: 0.0025\n",
      "Epoch [5563/20000], Loss: 895.0621948242188, Entropy 319.02642822265625, Learning Rate: 0.0025\n",
      "Epoch [5564/20000], Loss: 958.8827514648438, Entropy 307.14996337890625, Learning Rate: 0.0025\n",
      "Epoch [5565/20000], Loss: 930.82275390625, Entropy 303.6490783691406, Learning Rate: 0.0025\n",
      "Epoch [5566/20000], Loss: 936.40478515625, Entropy 296.1264343261719, Learning Rate: 0.0025\n",
      "Epoch [5567/20000], Loss: 1023.2820434570312, Entropy 300.47088623046875, Learning Rate: 0.0025\n",
      "Epoch [5568/20000], Loss: 961.617919921875, Entropy 309.4201965332031, Learning Rate: 0.0025\n",
      "Epoch [5569/20000], Loss: 943.3890380859375, Entropy 314.3124084472656, Learning Rate: 0.0025\n",
      "Epoch [5570/20000], Loss: 962.9092407226562, Entropy 294.39459228515625, Learning Rate: 0.0025\n",
      "Epoch [5571/20000], Loss: 942.7872314453125, Entropy 305.4554748535156, Learning Rate: 0.0025\n",
      "Epoch [5572/20000], Loss: 950.1192626953125, Entropy 321.6988525390625, Learning Rate: 0.0025\n",
      "Epoch [5573/20000], Loss: 951.86474609375, Entropy 304.3960876464844, Learning Rate: 0.0025\n",
      "Epoch [5574/20000], Loss: 928.944091796875, Entropy 301.5617980957031, Learning Rate: 0.0025\n",
      "Epoch [5575/20000], Loss: 999.037353515625, Entropy 300.6646728515625, Learning Rate: 0.0025\n",
      "Epoch [5576/20000], Loss: 900.0003662109375, Entropy 305.324462890625, Learning Rate: 0.0025\n",
      "Epoch [5577/20000], Loss: 929.9888916015625, Entropy 302.4548645019531, Learning Rate: 0.0025\n",
      "Epoch [5578/20000], Loss: 951.0680541992188, Entropy 300.68695068359375, Learning Rate: 0.0025\n",
      "Epoch [5579/20000], Loss: 925.2318725585938, Entropy 285.33111572265625, Learning Rate: 0.0025\n",
      "Epoch [5580/20000], Loss: 935.2059326171875, Entropy 291.0625305175781, Learning Rate: 0.0025\n",
      "Epoch [5581/20000], Loss: 964.9732055664062, Entropy 321.97772216796875, Learning Rate: 0.0025\n",
      "Epoch [5582/20000], Loss: 899.21533203125, Entropy 306.1285095214844, Learning Rate: 0.0025\n",
      "Epoch [5583/20000], Loss: 939.8629150390625, Entropy 305.3450622558594, Learning Rate: 0.0025\n",
      "Epoch [5584/20000], Loss: 950.1412353515625, Entropy 303.4875793457031, Learning Rate: 0.0025\n",
      "Epoch [5585/20000], Loss: 953.095703125, Entropy 301.1217956542969, Learning Rate: 0.0025\n",
      "Epoch [5586/20000], Loss: 919.52783203125, Entropy 304.3485412597656, Learning Rate: 0.0025\n",
      "Epoch [5587/20000], Loss: 939.181884765625, Entropy 294.0385437011719, Learning Rate: 0.0025\n",
      "Epoch [5588/20000], Loss: 936.18408203125, Entropy 298.6931457519531, Learning Rate: 0.0025\n",
      "Epoch [5589/20000], Loss: 950.0533447265625, Entropy 301.2112731933594, Learning Rate: 0.0025\n",
      "Epoch [5590/20000], Loss: 943.4794921875, Entropy 306.5412902832031, Learning Rate: 0.0025\n",
      "Epoch [5591/20000], Loss: 964.6270141601562, Entropy 293.37249755859375, Learning Rate: 0.0025\n",
      "Epoch [5592/20000], Loss: 923.7276611328125, Entropy 307.6584167480469, Learning Rate: 0.0025\n",
      "Epoch [5593/20000], Loss: 909.3372802734375, Entropy 308.0851745605469, Learning Rate: 0.0025\n",
      "Epoch [5594/20000], Loss: 920.3875732421875, Entropy 306.7161865234375, Learning Rate: 0.0025\n",
      "Epoch [5595/20000], Loss: 999.3275756835938, Entropy 311.22369384765625, Learning Rate: 0.0025\n",
      "Epoch [5596/20000], Loss: 942.3587646484375, Entropy 308.8761901855469, Learning Rate: 0.0025\n",
      "Epoch [5597/20000], Loss: 957.4937744140625, Entropy 304.1883239746094, Learning Rate: 0.0025\n",
      "Epoch [5598/20000], Loss: 936.53125, Entropy 316.8743591308594, Learning Rate: 0.0025\n",
      "Epoch [5599/20000], Loss: 919.4100341796875, Entropy 301.4852600097656, Learning Rate: 0.0025\n",
      "Epoch [5600/20000], Loss: 911.2750244140625, Entropy 314.8261413574219, Learning Rate: 0.0025\n",
      "Epoch [5601/20000], Loss: 983.139892578125, Entropy 299.1908874511719, Learning Rate: 0.0025\n",
      "Epoch [5602/20000], Loss: 947.928955078125, Entropy 318.1413269042969, Learning Rate: 0.0025\n",
      "Epoch [5603/20000], Loss: 942.8179931640625, Entropy 315.0719909667969, Learning Rate: 0.0025\n",
      "Epoch [5604/20000], Loss: 951.4970092773438, Entropy 304.17486572265625, Learning Rate: 0.0025\n",
      "Epoch [5605/20000], Loss: 924.759521484375, Entropy 307.1340637207031, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5606/20000], Loss: 930.4434814453125, Entropy 314.1343994140625, Learning Rate: 0.0025\n",
      "Epoch [5607/20000], Loss: 938.9426879882812, Entropy 304.52557373046875, Learning Rate: 0.0025\n",
      "Epoch [5608/20000], Loss: 954.889404296875, Entropy 300.197265625, Learning Rate: 0.0025\n",
      "Epoch [5609/20000], Loss: 938.492431640625, Entropy 312.0442810058594, Learning Rate: 0.0025\n",
      "Epoch [5610/20000], Loss: 979.490966796875, Entropy 303.2693176269531, Learning Rate: 0.0025\n",
      "Epoch [5611/20000], Loss: 949.1873168945312, Entropy 309.86676025390625, Learning Rate: 0.0025\n",
      "Epoch [5612/20000], Loss: 954.8392333984375, Entropy 302.4591979980469, Learning Rate: 0.0025\n",
      "Epoch [5613/20000], Loss: 882.739501953125, Entropy 314.7884826660156, Learning Rate: 0.0025\n",
      "Epoch [5614/20000], Loss: 949.820556640625, Entropy 295.4392395019531, Learning Rate: 0.0025\n",
      "Epoch [5615/20000], Loss: 908.7136840820312, Entropy 313.79693603515625, Learning Rate: 0.0025\n",
      "Epoch [5616/20000], Loss: 945.1279296875, Entropy 318.1457214355469, Learning Rate: 0.0025\n",
      "Epoch [5617/20000], Loss: 923.6829833984375, Entropy 319.7283020019531, Learning Rate: 0.0025\n",
      "Epoch [5618/20000], Loss: 916.8489990234375, Entropy 311.4387512207031, Learning Rate: 0.0025\n",
      "Epoch [5619/20000], Loss: 939.31103515625, Entropy 308.7581787109375, Learning Rate: 0.0025\n",
      "Epoch [5620/20000], Loss: 930.5740356445312, Entropy 311.38409423828125, Learning Rate: 0.0025\n",
      "Epoch [5621/20000], Loss: 960.1434936523438, Entropy 308.93743896484375, Learning Rate: 0.0025\n",
      "Epoch [5622/20000], Loss: 974.6407470703125, Entropy 306.2146301269531, Learning Rate: 0.0025\n",
      "Epoch [5623/20000], Loss: 917.60009765625, Entropy 312.694580078125, Learning Rate: 0.0025\n",
      "Epoch [5624/20000], Loss: 940.0778198242188, Entropy 308.84332275390625, Learning Rate: 0.0025\n",
      "Epoch [5625/20000], Loss: 918.64892578125, Entropy 314.0282287597656, Learning Rate: 0.0025\n",
      "Epoch [5626/20000], Loss: 910.7753295898438, Entropy 301.67254638671875, Learning Rate: 0.0025\n",
      "Epoch [5627/20000], Loss: 996.021484375, Entropy 293.6169738769531, Learning Rate: 0.0025\n",
      "Epoch [5628/20000], Loss: 930.05859375, Entropy 311.433837890625, Learning Rate: 0.0025\n",
      "Epoch [5629/20000], Loss: 933.1995849609375, Entropy 327.8542175292969, Learning Rate: 0.0025\n",
      "Epoch [5630/20000], Loss: 950.4637451171875, Entropy 322.69775390625, Learning Rate: 0.0025\n",
      "Epoch [5631/20000], Loss: 940.2418212890625, Entropy 307.9835510253906, Learning Rate: 0.0025\n",
      "Epoch [5632/20000], Loss: 948.10546875, Entropy 317.97314453125, Learning Rate: 0.0025\n",
      "Epoch [5633/20000], Loss: 926.6658935546875, Entropy 297.3279113769531, Learning Rate: 0.0025\n",
      "Epoch [5634/20000], Loss: 896.8455810546875, Entropy 314.9045715332031, Learning Rate: 0.0025\n",
      "Epoch [5635/20000], Loss: 899.1021728515625, Entropy 322.4181823730469, Learning Rate: 0.0025\n",
      "Epoch [5636/20000], Loss: 960.83837890625, Entropy 300.0509033203125, Learning Rate: 0.0025\n",
      "Epoch [5637/20000], Loss: 904.5164794921875, Entropy 321.9342041015625, Learning Rate: 0.0025\n",
      "Epoch [5638/20000], Loss: 941.3204956054688, Entropy 309.05255126953125, Learning Rate: 0.0025\n",
      "Epoch [5639/20000], Loss: 902.0732421875, Entropy 310.4498291015625, Learning Rate: 0.0025\n",
      "Epoch [5640/20000], Loss: 960.1527099609375, Entropy 323.0912780761719, Learning Rate: 0.0025\n",
      "Epoch [5641/20000], Loss: 942.134765625, Entropy 312.8316345214844, Learning Rate: 0.0025\n",
      "Epoch [5642/20000], Loss: 982.7791748046875, Entropy 303.1861267089844, Learning Rate: 0.0025\n",
      "Epoch [5643/20000], Loss: 965.1699829101562, Entropy 308.25299072265625, Learning Rate: 0.0025\n",
      "Epoch [5644/20000], Loss: 905.673828125, Entropy 322.5730895996094, Learning Rate: 0.0025\n",
      "Epoch [5645/20000], Loss: 956.646240234375, Entropy 307.0263977050781, Learning Rate: 0.0025\n",
      "Epoch [5646/20000], Loss: 939.1802978515625, Entropy 322.1116638183594, Learning Rate: 0.0025\n",
      "Epoch [5647/20000], Loss: 968.54931640625, Entropy 322.6219787597656, Learning Rate: 0.0025\n",
      "Epoch [5648/20000], Loss: 954.9554443359375, Entropy 319.8251953125, Learning Rate: 0.0025\n",
      "Epoch [5649/20000], Loss: 940.32861328125, Entropy 305.4791259765625, Learning Rate: 0.0025\n",
      "Epoch [5650/20000], Loss: 908.3515625, Entropy 299.8497619628906, Learning Rate: 0.0025\n",
      "Epoch [5651/20000], Loss: 955.140625, Entropy 313.9104309082031, Learning Rate: 0.0025\n",
      "Epoch [5652/20000], Loss: 942.4813232421875, Entropy 331.49853515625, Learning Rate: 0.0025\n",
      "Epoch [5653/20000], Loss: 928.1204833984375, Entropy 325.2298278808594, Learning Rate: 0.0025\n",
      "Epoch [5654/20000], Loss: 904.9642944335938, Entropy 336.15631103515625, Learning Rate: 0.0025\n",
      "Epoch [5655/20000], Loss: 892.156982421875, Entropy 307.7745361328125, Learning Rate: 0.0025\n",
      "Epoch [5656/20000], Loss: 996.5114135742188, Entropy 299.66131591796875, Learning Rate: 0.0025\n",
      "Epoch [5657/20000], Loss: 946.7005615234375, Entropy 307.8892517089844, Learning Rate: 0.0025\n",
      "Epoch [5658/20000], Loss: 934.783447265625, Entropy 324.5399169921875, Learning Rate: 0.0025\n",
      "Epoch [5659/20000], Loss: 950.203125, Entropy 321.7048034667969, Learning Rate: 0.0025\n",
      "Epoch [5660/20000], Loss: 886.737548828125, Entropy 327.0745544433594, Learning Rate: 0.0025\n",
      "Epoch [5661/20000], Loss: 999.3577880859375, Entropy 297.5062561035156, Learning Rate: 0.0025\n",
      "Epoch [5662/20000], Loss: 1001.0833129882812, Entropy 322.62261962890625, Learning Rate: 0.0025\n",
      "Epoch [5663/20000], Loss: 908.2339477539062, Entropy 327.49481201171875, Learning Rate: 0.0025\n",
      "Epoch [5664/20000], Loss: 958.5108642578125, Entropy 326.1058654785156, Learning Rate: 0.0025\n",
      "Epoch [5665/20000], Loss: 923.3551025390625, Entropy 310.3775634765625, Learning Rate: 0.0025\n",
      "Epoch [5666/20000], Loss: 921.6214599609375, Entropy 301.1068420410156, Learning Rate: 0.0025\n",
      "Epoch [5667/20000], Loss: 1002.3223266601562, Entropy 316.53216552734375, Learning Rate: 0.0025\n",
      "Epoch [5668/20000], Loss: 996.9102783203125, Entropy 322.3949279785156, Learning Rate: 0.0025\n",
      "Epoch [5669/20000], Loss: 941.1077270507812, Entropy 316.86541748046875, Learning Rate: 0.0025\n",
      "Epoch [5670/20000], Loss: 950.662841796875, Entropy 304.50927734375, Learning Rate: 0.0025\n",
      "Epoch [5671/20000], Loss: 942.101318359375, Entropy 310.9723815917969, Learning Rate: 0.0025\n",
      "Epoch [5672/20000], Loss: 955.0546875, Entropy 314.0643005371094, Learning Rate: 0.0025\n",
      "Epoch [5673/20000], Loss: 974.011962890625, Entropy 314.6933898925781, Learning Rate: 0.0025\n",
      "Epoch [5674/20000], Loss: 937.5555419921875, Entropy 326.01171875, Learning Rate: 0.0025\n",
      "Epoch [5675/20000], Loss: 944.2120971679688, Entropy 311.56402587890625, Learning Rate: 0.0025\n",
      "Epoch [5676/20000], Loss: 969.4725341796875, Entropy 315.9919128417969, Learning Rate: 0.0025\n",
      "Epoch [5677/20000], Loss: 987.6590576171875, Entropy 305.0993347167969, Learning Rate: 0.0025\n",
      "Epoch [5678/20000], Loss: 921.2800903320312, Entropy 328.95928955078125, Learning Rate: 0.0025\n",
      "Epoch [5679/20000], Loss: 924.8245849609375, Entropy 317.2715148925781, Learning Rate: 0.0025\n",
      "Epoch [5680/20000], Loss: 921.0011596679688, Entropy 320.28143310546875, Learning Rate: 0.0025\n",
      "Epoch [5681/20000], Loss: 967.7169799804688, Entropy 317.12371826171875, Learning Rate: 0.0025\n",
      "Epoch [5682/20000], Loss: 937.1019287109375, Entropy 317.2733154296875, Learning Rate: 0.0025\n",
      "Epoch [5683/20000], Loss: 914.0219116210938, Entropy 341.26129150390625, Learning Rate: 0.0025\n",
      "Epoch [5684/20000], Loss: 939.5323486328125, Entropy 323.01123046875, Learning Rate: 0.0025\n",
      "Epoch [5685/20000], Loss: 950.749267578125, Entropy 303.28515625, Learning Rate: 0.0025\n",
      "Epoch [5686/20000], Loss: 944.9622802734375, Entropy 311.825927734375, Learning Rate: 0.0025\n",
      "Epoch [5687/20000], Loss: 931.3847045898438, Entropy 314.50445556640625, Learning Rate: 0.0025\n",
      "Epoch [5688/20000], Loss: 898.05126953125, Entropy 329.2059326171875, Learning Rate: 0.0025\n",
      "Epoch [5689/20000], Loss: 926.6591796875, Entropy 315.549072265625, Learning Rate: 0.0025\n",
      "Epoch [5690/20000], Loss: 966.830810546875, Entropy 313.0723876953125, Learning Rate: 0.0025\n",
      "Epoch [5691/20000], Loss: 966.693115234375, Entropy 318.6355895996094, Learning Rate: 0.0025\n",
      "Epoch [5692/20000], Loss: 911.1066284179688, Entropy 323.44207763671875, Learning Rate: 0.0025\n",
      "Epoch [5693/20000], Loss: 911.0671997070312, Entropy 319.06121826171875, Learning Rate: 0.0025\n",
      "Epoch [5694/20000], Loss: 985.8856201171875, Entropy 305.4643249511719, Learning Rate: 0.0025\n",
      "Epoch [5695/20000], Loss: 940.7855224609375, Entropy 315.4614562988281, Learning Rate: 0.0025\n",
      "Epoch [5696/20000], Loss: 934.9844360351562, Entropy 320.97332763671875, Learning Rate: 0.0025\n",
      "Epoch [5697/20000], Loss: 962.056396484375, Entropy 327.7532653808594, Learning Rate: 0.0025\n",
      "Epoch [5698/20000], Loss: 917.285400390625, Entropy 302.8760986328125, Learning Rate: 0.0025\n",
      "Epoch [5699/20000], Loss: 926.4893188476562, Entropy 312.80242919921875, Learning Rate: 0.0025\n",
      "Epoch [5700/20000], Loss: 954.1341552734375, Entropy 330.7119445800781, Learning Rate: 0.0025\n",
      "Epoch [5701/20000], Loss: 924.0180053710938, Entropy 324.16961669921875, Learning Rate: 0.0025\n",
      "Epoch [5702/20000], Loss: 908.95849609375, Entropy 318.38818359375, Learning Rate: 0.0025\n",
      "Epoch [5703/20000], Loss: 925.541015625, Entropy 315.7331848144531, Learning Rate: 0.0025\n",
      "Epoch [5704/20000], Loss: 903.4413452148438, Entropy 340.08148193359375, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5705/20000], Loss: 902.1517333984375, Entropy 326.5683288574219, Learning Rate: 0.0025\n",
      "Epoch [5706/20000], Loss: 973.3675537109375, Entropy 320.1007385253906, Learning Rate: 0.0025\n",
      "Epoch [5707/20000], Loss: 939.1358642578125, Entropy 300.9467468261719, Learning Rate: 0.0025\n",
      "Epoch [5708/20000], Loss: 889.7095947265625, Entropy 321.9630126953125, Learning Rate: 0.0025\n",
      "Epoch [5709/20000], Loss: 934.0130615234375, Entropy 313.81787109375, Learning Rate: 0.0025\n",
      "Epoch [5710/20000], Loss: 966.7047119140625, Entropy 309.44287109375, Learning Rate: 0.0025\n",
      "Epoch [5711/20000], Loss: 935.9651489257812, Entropy 319.82904052734375, Learning Rate: 0.0025\n",
      "Epoch [5712/20000], Loss: 956.5286865234375, Entropy 323.62109375, Learning Rate: 0.0025\n",
      "Epoch [5713/20000], Loss: 957.543212890625, Entropy 312.7341613769531, Learning Rate: 0.0025\n",
      "Epoch [5714/20000], Loss: 967.4439086914062, Entropy 317.95819091796875, Learning Rate: 0.0025\n",
      "Epoch [5715/20000], Loss: 956.6195068359375, Entropy 320.7342834472656, Learning Rate: 0.0025\n",
      "Epoch [5716/20000], Loss: 942.8474731445312, Entropy 307.05096435546875, Learning Rate: 0.0025\n",
      "Epoch [5717/20000], Loss: 955.0438232421875, Entropy 306.1417236328125, Learning Rate: 0.0025\n",
      "Epoch [5718/20000], Loss: 954.5189208984375, Entropy 311.6673278808594, Learning Rate: 0.0025\n",
      "Epoch [5719/20000], Loss: 961.5255126953125, Entropy 324.9927062988281, Learning Rate: 0.0025\n",
      "Epoch [5720/20000], Loss: 937.1571044921875, Entropy 315.5523681640625, Learning Rate: 0.0025\n",
      "Epoch [5721/20000], Loss: 916.2988891601562, Entropy 323.90985107421875, Learning Rate: 0.0025\n",
      "Epoch [5722/20000], Loss: 968.7711181640625, Entropy 301.0759582519531, Learning Rate: 0.0025\n",
      "Epoch [5723/20000], Loss: 956.1448364257812, Entropy 336.52777099609375, Learning Rate: 0.0025\n",
      "Epoch [5724/20000], Loss: 958.973388671875, Entropy 323.7427978515625, Learning Rate: 0.0025\n",
      "Epoch [5725/20000], Loss: 986.7015380859375, Entropy 327.3977966308594, Learning Rate: 0.0025\n",
      "Epoch [5726/20000], Loss: 934.0208740234375, Entropy 312.6712341308594, Learning Rate: 0.0025\n",
      "Epoch [5727/20000], Loss: 925.54833984375, Entropy 322.6374816894531, Learning Rate: 0.0025\n",
      "Epoch [5728/20000], Loss: 925.31787109375, Entropy 319.0768737792969, Learning Rate: 0.0025\n",
      "Epoch [5729/20000], Loss: 970.6231689453125, Entropy 334.433349609375, Learning Rate: 0.0025\n",
      "Epoch [5730/20000], Loss: 926.0194702148438, Entropy 325.48443603515625, Learning Rate: 0.0025\n",
      "Epoch [5731/20000], Loss: 937.930908203125, Entropy 326.8304443359375, Learning Rate: 0.0025\n",
      "Epoch [5732/20000], Loss: 944.286376953125, Entropy 331.9951477050781, Learning Rate: 0.0025\n",
      "Epoch [5733/20000], Loss: 946.1063842773438, Entropy 317.85125732421875, Learning Rate: 0.0025\n",
      "Epoch [5734/20000], Loss: 1002.5902099609375, Entropy 317.5578918457031, Learning Rate: 0.0025\n",
      "Epoch [5735/20000], Loss: 972.72021484375, Entropy 326.0224304199219, Learning Rate: 0.0025\n",
      "Epoch [5736/20000], Loss: 929.7056274414062, Entropy 332.85491943359375, Learning Rate: 0.0025\n",
      "Epoch [5737/20000], Loss: 918.9881591796875, Entropy 327.7787780761719, Learning Rate: 0.0025\n",
      "Epoch [5738/20000], Loss: 948.4678344726562, Entropy 338.87115478515625, Learning Rate: 0.0025\n",
      "Epoch [5739/20000], Loss: 1066.621826171875, Entropy 332.5657043457031, Learning Rate: 0.0025\n",
      "Epoch [5740/20000], Loss: 990.5936889648438, Entropy 328.60272216796875, Learning Rate: 0.0025\n",
      "Epoch [5741/20000], Loss: 983.4332275390625, Entropy 322.1593017578125, Learning Rate: 0.0025\n",
      "Epoch [5742/20000], Loss: 1059.40673828125, Entropy 319.97076416015625, Learning Rate: 0.0025\n",
      "Epoch [5743/20000], Loss: 949.8670654296875, Entropy 335.0432434082031, Learning Rate: 0.0025\n",
      "Epoch [5744/20000], Loss: 963.6947631835938, Entropy 309.65447998046875, Learning Rate: 0.0025\n",
      "Epoch [5745/20000], Loss: 978.9855346679688, Entropy 312.08880615234375, Learning Rate: 0.0025\n",
      "Epoch [5746/20000], Loss: 936.3536987304688, Entropy 335.04632568359375, Learning Rate: 0.0025\n",
      "Epoch [5747/20000], Loss: 1047.2574462890625, Entropy 302.5550842285156, Learning Rate: 0.0025\n",
      "Epoch [5748/20000], Loss: 956.6498413085938, Entropy 330.06573486328125, Learning Rate: 0.0025\n",
      "Epoch [5749/20000], Loss: 1017.8671875, Entropy 320.2118225097656, Learning Rate: 0.0025\n",
      "Epoch [5750/20000], Loss: 949.9235229492188, Entropy 341.04107666015625, Learning Rate: 0.0025\n",
      "Epoch [5751/20000], Loss: 980.5654907226562, Entropy 324.63323974609375, Learning Rate: 0.0025\n",
      "Epoch [5752/20000], Loss: 1000.973388671875, Entropy 328.8280334472656, Learning Rate: 0.0025\n",
      "Epoch [5753/20000], Loss: 973.255126953125, Entropy 318.3848571777344, Learning Rate: 0.0025\n",
      "Epoch [5754/20000], Loss: 921.2936401367188, Entropy 340.49188232421875, Learning Rate: 0.0025\n",
      "Epoch [5755/20000], Loss: 949.9301147460938, Entropy 312.62542724609375, Learning Rate: 0.0025\n",
      "Epoch [5756/20000], Loss: 951.7471923828125, Entropy 332.5953369140625, Learning Rate: 0.0025\n",
      "Epoch [5757/20000], Loss: 943.2955322265625, Entropy 325.2189636230469, Learning Rate: 0.0025\n",
      "Epoch [5758/20000], Loss: 959.2750244140625, Entropy 326.7877197265625, Learning Rate: 0.0025\n",
      "Epoch [5759/20000], Loss: 977.812255859375, Entropy 321.3299865722656, Learning Rate: 0.0025\n",
      "Epoch [5760/20000], Loss: 938.1810913085938, Entropy 314.81927490234375, Learning Rate: 0.0025\n",
      "Epoch [5761/20000], Loss: 980.9430541992188, Entropy 322.44671630859375, Learning Rate: 0.0025\n",
      "Epoch [5762/20000], Loss: 937.258544921875, Entropy 338.0662536621094, Learning Rate: 0.0025\n",
      "Epoch [5763/20000], Loss: 970.1007690429688, Entropy 335.81964111328125, Learning Rate: 0.0025\n",
      "Epoch [5764/20000], Loss: 943.9087524414062, Entropy 320.07574462890625, Learning Rate: 0.0025\n",
      "Epoch [5765/20000], Loss: 951.1563720703125, Entropy 311.6375427246094, Learning Rate: 0.0025\n",
      "Epoch [5766/20000], Loss: 993.3327026367188, Entropy 321.62506103515625, Learning Rate: 0.0025\n",
      "Epoch [5767/20000], Loss: 941.5962524414062, Entropy 317.84356689453125, Learning Rate: 0.0025\n",
      "Epoch [5768/20000], Loss: 961.0867919921875, Entropy 310.8998107910156, Learning Rate: 0.0025\n",
      "Epoch [5769/20000], Loss: 981.35302734375, Entropy 326.9276123046875, Learning Rate: 0.0025\n",
      "Epoch [5770/20000], Loss: 942.9507446289062, Entropy 329.45037841796875, Learning Rate: 0.0025\n",
      "Epoch [5771/20000], Loss: 944.306640625, Entropy 319.1670837402344, Learning Rate: 0.0025\n",
      "Epoch [5772/20000], Loss: 960.6453857421875, Entropy 320.9084777832031, Learning Rate: 0.0025\n",
      "Epoch [5773/20000], Loss: 993.546875, Entropy 328.841552734375, Learning Rate: 0.0025\n",
      "Epoch [5774/20000], Loss: 991.8408203125, Entropy 326.5265808105469, Learning Rate: 0.0025\n",
      "Epoch [5775/20000], Loss: 945.4253540039062, Entropy 325.89776611328125, Learning Rate: 0.0025\n",
      "Epoch [5776/20000], Loss: 986.0264892578125, Entropy 320.4595031738281, Learning Rate: 0.0025\n",
      "Epoch [5777/20000], Loss: 925.487548828125, Entropy 326.7223815917969, Learning Rate: 0.0025\n",
      "Epoch [5778/20000], Loss: 1007.9130249023438, Entropy 321.21539306640625, Learning Rate: 0.0025\n",
      "Epoch [5779/20000], Loss: 957.4426879882812, Entropy 318.96160888671875, Learning Rate: 0.0025\n",
      "Epoch [5780/20000], Loss: 1085.0009765625, Entropy 312.24468994140625, Learning Rate: 0.0025\n",
      "Epoch [5781/20000], Loss: 985.2077026367188, Entropy 317.06463623046875, Learning Rate: 0.0025\n",
      "Epoch [5782/20000], Loss: 1035.0057373046875, Entropy 316.3832702636719, Learning Rate: 0.0025\n",
      "Epoch [5783/20000], Loss: 1179.937255859375, Entropy 310.14520263671875, Learning Rate: 0.0025\n",
      "Epoch [5784/20000], Loss: 941.448974609375, Entropy 322.5600891113281, Learning Rate: 0.0025\n",
      "Epoch [5785/20000], Loss: 988.5025634765625, Entropy 333.2439880371094, Learning Rate: 0.0025\n",
      "Epoch [5786/20000], Loss: 979.1748046875, Entropy 333.6473083496094, Learning Rate: 0.0025\n",
      "Epoch [5787/20000], Loss: 1074.35888671875, Entropy 308.9642028808594, Learning Rate: 0.0025\n",
      "Epoch [5788/20000], Loss: 1240.4840087890625, Entropy 330.6661682128906, Learning Rate: 0.0025\n",
      "Epoch [5789/20000], Loss: 987.3660888671875, Entropy 305.6369934082031, Learning Rate: 0.0025\n",
      "Epoch [5790/20000], Loss: 1051.3505859375, Entropy 336.8079528808594, Learning Rate: 0.0025\n",
      "Epoch [5791/20000], Loss: 1072.68798828125, Entropy 334.9410095214844, Learning Rate: 0.0025\n",
      "Epoch [5792/20000], Loss: 1004.726806640625, Entropy 318.5484313964844, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5793/20000], Loss: 1180.0325927734375, Entropy 321.9316101074219, Learning Rate: 0.0025\n",
      "Epoch [5794/20000], Loss: 1034.8175048828125, Entropy 339.1543273925781, Learning Rate: 0.0025\n",
      "Epoch [5795/20000], Loss: 1052.20458984375, Entropy 315.98199462890625, Learning Rate: 0.0025\n",
      "Epoch [5796/20000], Loss: 1042.649658203125, Entropy 330.035888671875, Learning Rate: 0.0025\n",
      "Epoch [5797/20000], Loss: 1056.0758056640625, Entropy 322.394287109375, Learning Rate: 0.0025\n",
      "Epoch [5798/20000], Loss: 1178.3934326171875, Entropy 324.8737487792969, Learning Rate: 0.0025\n",
      "Epoch [5799/20000], Loss: 1025.598876953125, Entropy 317.41888427734375, Learning Rate: 0.0025\n",
      "Epoch [5800/20000], Loss: 1049.0587158203125, Entropy 324.6934814453125, Learning Rate: 0.0025\n",
      "Epoch [5801/20000], Loss: 1066.268310546875, Entropy 319.89703369140625, Learning Rate: 0.0025\n",
      "Epoch [5802/20000], Loss: 954.0467529296875, Entropy 316.8987731933594, Learning Rate: 0.0025\n",
      "Epoch [5803/20000], Loss: 1043.8037109375, Entropy 312.04656982421875, Learning Rate: 0.0025\n",
      "Epoch [5804/20000], Loss: 1011.7850952148438, Entropy 333.44952392578125, Learning Rate: 0.0025\n",
      "Epoch [5805/20000], Loss: 1052.279541015625, Entropy 323.97509765625, Learning Rate: 0.0025\n",
      "Epoch [5806/20000], Loss: 1059.1214599609375, Entropy 337.6075744628906, Learning Rate: 0.0025\n",
      "Epoch [5807/20000], Loss: 1074.763916015625, Entropy 320.32476806640625, Learning Rate: 0.0025\n",
      "Epoch [5808/20000], Loss: 1341.8929443359375, Entropy 310.7366638183594, Learning Rate: 0.0025\n",
      "Epoch [5809/20000], Loss: 1215.7222900390625, Entropy 313.5909423828125, Learning Rate: 0.0025\n",
      "Epoch [5810/20000], Loss: 1004.7875366210938, Entropy 321.84967041015625, Learning Rate: 0.0025\n",
      "Epoch [5811/20000], Loss: 1019.4515380859375, Entropy 309.7234191894531, Learning Rate: 0.0025\n",
      "Epoch [5812/20000], Loss: 1016.0802001953125, Entropy 329.7615661621094, Learning Rate: 0.0025\n",
      "Epoch [5813/20000], Loss: 943.154541015625, Entropy 331.570068359375, Learning Rate: 0.0025\n",
      "Epoch [5814/20000], Loss: 975.764892578125, Entropy 318.6094970703125, Learning Rate: 0.0025\n",
      "Epoch [5815/20000], Loss: 1032.6024169921875, Entropy 314.6962890625, Learning Rate: 0.0025\n",
      "Epoch [5816/20000], Loss: 1140.4034423828125, Entropy 322.3874816894531, Learning Rate: 0.0025\n",
      "Epoch [5817/20000], Loss: 953.2066650390625, Entropy 300.2435302734375, Learning Rate: 0.0025\n",
      "Epoch [5818/20000], Loss: 1055.96728515625, Entropy 329.93621826171875, Learning Rate: 0.0025\n",
      "Epoch [5819/20000], Loss: 1044.4840087890625, Entropy 311.8608093261719, Learning Rate: 0.0025\n",
      "Epoch [5820/20000], Loss: 1012.8218994140625, Entropy 327.8221740722656, Learning Rate: 0.0025\n",
      "Epoch [5821/20000], Loss: 958.9931640625, Entropy 324.9600830078125, Learning Rate: 0.0025\n",
      "Epoch [5822/20000], Loss: 963.2157592773438, Entropy 332.95135498046875, Learning Rate: 0.0025\n",
      "Epoch [5823/20000], Loss: 948.3421630859375, Entropy 332.0121765136719, Learning Rate: 0.0025\n",
      "Epoch [5824/20000], Loss: 965.5012817382812, Entropy 307.72821044921875, Learning Rate: 0.0025\n",
      "Epoch [5825/20000], Loss: 964.0401611328125, Entropy 325.6124267578125, Learning Rate: 0.0025\n",
      "Epoch [5826/20000], Loss: 925.80712890625, Entropy 325.42919921875, Learning Rate: 0.0025\n",
      "Epoch [5827/20000], Loss: 957.7340087890625, Entropy 326.9776306152344, Learning Rate: 0.0025\n",
      "Epoch [5828/20000], Loss: 981.7188720703125, Entropy 300.0244445800781, Learning Rate: 0.0025\n",
      "Epoch [5829/20000], Loss: 957.4769287109375, Entropy 314.1698303222656, Learning Rate: 0.0025\n",
      "Epoch [5830/20000], Loss: 920.8238525390625, Entropy 330.8721008300781, Learning Rate: 0.0025\n",
      "Epoch [5831/20000], Loss: 970.3319702148438, Entropy 325.47711181640625, Learning Rate: 0.0025\n",
      "Epoch [5832/20000], Loss: 931.9728393554688, Entropy 318.41461181640625, Learning Rate: 0.0025\n",
      "Epoch [5833/20000], Loss: 923.8208618164062, Entropy 324.87762451171875, Learning Rate: 0.0025\n",
      "Epoch [5834/20000], Loss: 902.998046875, Entropy 326.3503723144531, Learning Rate: 0.0025\n",
      "Epoch [5835/20000], Loss: 968.1687622070312, Entropy 308.91229248046875, Learning Rate: 0.0025\n",
      "Epoch [5836/20000], Loss: 925.8475952148438, Entropy 306.35052490234375, Learning Rate: 0.0025\n",
      "Epoch [5837/20000], Loss: 914.6907958984375, Entropy 326.0799865722656, Learning Rate: 0.0025\n",
      "Epoch [5838/20000], Loss: 965.6375732421875, Entropy 317.047607421875, Learning Rate: 0.0025\n",
      "Epoch [5839/20000], Loss: 1059.70263671875, Entropy 313.1519775390625, Learning Rate: 0.0025\n",
      "Epoch [5840/20000], Loss: 938.6298828125, Entropy 328.1899108886719, Learning Rate: 0.0025\n",
      "Epoch [5841/20000], Loss: 949.2586669921875, Entropy 321.2381896972656, Learning Rate: 0.0025\n",
      "Epoch [5842/20000], Loss: 894.81201171875, Entropy 334.0344543457031, Learning Rate: 0.0025\n",
      "Epoch [5843/20000], Loss: 939.897216796875, Entropy 305.8492126464844, Learning Rate: 0.0025\n",
      "Epoch [5844/20000], Loss: 921.6371459960938, Entropy 328.11724853515625, Learning Rate: 0.0025\n",
      "Epoch [5845/20000], Loss: 943.850341796875, Entropy 323.2590637207031, Learning Rate: 0.0025\n",
      "Epoch [5846/20000], Loss: 961.8002319335938, Entropy 315.11517333984375, Learning Rate: 0.0025\n",
      "Epoch [5847/20000], Loss: 888.4696044921875, Entropy 327.5409851074219, Learning Rate: 0.0025\n",
      "Epoch [5848/20000], Loss: 911.3526000976562, Entropy 336.51092529296875, Learning Rate: 0.0025\n",
      "Epoch [5849/20000], Loss: 942.4517822265625, Entropy 314.2221374511719, Learning Rate: 0.0025\n",
      "Epoch [5850/20000], Loss: 971.5152587890625, Entropy 307.4641418457031, Learning Rate: 0.0025\n",
      "Epoch [5851/20000], Loss: 929.904541015625, Entropy 323.180908203125, Learning Rate: 0.0025\n",
      "Epoch [5852/20000], Loss: 927.456298828125, Entropy 329.5375671386719, Learning Rate: 0.0025\n",
      "Epoch [5853/20000], Loss: 907.6619873046875, Entropy 330.1383056640625, Learning Rate: 0.0025\n",
      "Epoch [5854/20000], Loss: 973.6762084960938, Entropy 303.29034423828125, Learning Rate: 0.0025\n",
      "Epoch [5855/20000], Loss: 918.233154296875, Entropy 325.0359191894531, Learning Rate: 0.0025\n",
      "Epoch [5856/20000], Loss: 965.5216064453125, Entropy 307.4929504394531, Learning Rate: 0.0025\n",
      "Epoch [5857/20000], Loss: 895.021240234375, Entropy 321.4906311035156, Learning Rate: 0.0025\n",
      "Epoch [5858/20000], Loss: 978.1216430664062, Entropy 311.90423583984375, Learning Rate: 0.0025\n",
      "Epoch [5859/20000], Loss: 913.4783935546875, Entropy 322.5638732910156, Learning Rate: 0.0025\n",
      "Epoch [5860/20000], Loss: 942.5966186523438, Entropy 311.21856689453125, Learning Rate: 0.0025\n",
      "Epoch [5861/20000], Loss: 961.098388671875, Entropy 332.7095947265625, Learning Rate: 0.0025\n",
      "Epoch [5862/20000], Loss: 906.9024658203125, Entropy 315.9753112792969, Learning Rate: 0.0025\n",
      "Epoch [5863/20000], Loss: 916.4203491210938, Entropy 341.61871337890625, Learning Rate: 0.0025\n",
      "Epoch [5864/20000], Loss: 941.0747680664062, Entropy 324.36981201171875, Learning Rate: 0.0025\n",
      "Epoch [5865/20000], Loss: 970.1129760742188, Entropy 321.78912353515625, Learning Rate: 0.0025\n",
      "Epoch [5866/20000], Loss: 894.937255859375, Entropy 335.8377990722656, Learning Rate: 0.0025\n",
      "Epoch [5867/20000], Loss: 949.2574462890625, Entropy 326.8220520019531, Learning Rate: 0.0025\n",
      "Epoch [5868/20000], Loss: 938.0240478515625, Entropy 330.958984375, Learning Rate: 0.0025\n",
      "Epoch [5869/20000], Loss: 903.5643920898438, Entropy 319.31072998046875, Learning Rate: 0.0025\n",
      "Epoch [5870/20000], Loss: 881.2871704101562, Entropy 328.62310791015625, Learning Rate: 0.0025\n",
      "Epoch [5871/20000], Loss: 928.9490966796875, Entropy 342.2512512207031, Learning Rate: 0.0025\n",
      "Epoch [5872/20000], Loss: 911.44091796875, Entropy 331.396484375, Learning Rate: 0.0025\n",
      "Epoch [5873/20000], Loss: 937.8622436523438, Entropy 309.66973876953125, Learning Rate: 0.0025\n",
      "Epoch [5874/20000], Loss: 894.5958251953125, Entropy 341.3517150878906, Learning Rate: 0.0025\n",
      "Epoch [5875/20000], Loss: 890.1793823242188, Entropy 334.64312744140625, Learning Rate: 0.0025\n",
      "Epoch [5876/20000], Loss: 969.7003173828125, Entropy 317.1085510253906, Learning Rate: 0.0025\n",
      "Epoch [5877/20000], Loss: 909.2598876953125, Entropy 334.0936279296875, Learning Rate: 0.0025\n",
      "Epoch [5878/20000], Loss: 924.1751708984375, Entropy 337.7388610839844, Learning Rate: 0.0025\n",
      "Epoch [5879/20000], Loss: 898.9080810546875, Entropy 344.1562805175781, Learning Rate: 0.0025\n",
      "Epoch [5880/20000], Loss: 882.622802734375, Entropy 332.8681640625, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5881/20000], Loss: 947.135498046875, Entropy 334.1141357421875, Learning Rate: 0.0025\n",
      "Epoch [5882/20000], Loss: 982.229248046875, Entropy 324.527587890625, Learning Rate: 0.0025\n",
      "Epoch [5883/20000], Loss: 947.7919921875, Entropy 318.66943359375, Learning Rate: 0.0025\n",
      "Epoch [5884/20000], Loss: 948.893310546875, Entropy 331.84912109375, Learning Rate: 0.0025\n",
      "Epoch [5885/20000], Loss: 940.4761352539062, Entropy 337.68121337890625, Learning Rate: 0.0025\n",
      "Epoch [5886/20000], Loss: 972.1173095703125, Entropy 335.1229553222656, Learning Rate: 0.0025\n",
      "Epoch [5887/20000], Loss: 976.752197265625, Entropy 319.3013916015625, Learning Rate: 0.0025\n",
      "Epoch [5888/20000], Loss: 969.4295654296875, Entropy 312.042236328125, Learning Rate: 0.0025\n",
      "Epoch [5889/20000], Loss: 1033.325927734375, Entropy 330.81927490234375, Learning Rate: 0.0025\n",
      "Epoch [5890/20000], Loss: 981.7015380859375, Entropy 319.751220703125, Learning Rate: 0.0025\n",
      "Epoch [5891/20000], Loss: 1074.290771484375, Entropy 313.41741943359375, Learning Rate: 0.0025\n",
      "Epoch [5892/20000], Loss: 1150.955810546875, Entropy 335.32940673828125, Learning Rate: 0.0025\n",
      "Epoch [5893/20000], Loss: 946.0054931640625, Entropy 320.3205261230469, Learning Rate: 0.0025\n",
      "Epoch [5894/20000], Loss: 1135.3809814453125, Entropy 318.5233154296875, Learning Rate: 0.0025\n",
      "Epoch [5895/20000], Loss: 1076.9783935546875, Entropy 327.6044006347656, Learning Rate: 0.0025\n",
      "Epoch [5896/20000], Loss: 985.0972900390625, Entropy 330.3201599121094, Learning Rate: 0.0025\n",
      "Epoch [5897/20000], Loss: 1044.5089111328125, Entropy 329.72265625, Learning Rate: 0.0025\n",
      "Epoch [5898/20000], Loss: 918.8303833007812, Entropy 328.48748779296875, Learning Rate: 0.0025\n",
      "Epoch [5899/20000], Loss: 1055.09228515625, Entropy 319.37664794921875, Learning Rate: 0.0025\n",
      "Epoch [5900/20000], Loss: 1064.5653076171875, Entropy 317.1385803222656, Learning Rate: 0.0025\n",
      "Epoch [5901/20000], Loss: 933.6842651367188, Entropy 329.34075927734375, Learning Rate: 0.0025\n",
      "Epoch [5902/20000], Loss: 907.9391479492188, Entropy 345.91131591796875, Learning Rate: 0.0025\n",
      "Epoch [5903/20000], Loss: 937.06298828125, Entropy 321.9498291015625, Learning Rate: 0.0025\n",
      "Epoch [5904/20000], Loss: 908.1690673828125, Entropy 326.6041259765625, Learning Rate: 0.0025\n",
      "Epoch [5905/20000], Loss: 957.348388671875, Entropy 327.0819396972656, Learning Rate: 0.0025\n",
      "Epoch [5906/20000], Loss: 924.8695068359375, Entropy 334.1748352050781, Learning Rate: 0.0025\n",
      "Epoch [5907/20000], Loss: 1014.279052734375, Entropy 323.0568542480469, Learning Rate: 0.0025\n",
      "Epoch [5908/20000], Loss: 1046.7193603515625, Entropy 312.8624267578125, Learning Rate: 0.0025\n",
      "Epoch [5909/20000], Loss: 940.5655517578125, Entropy 338.6961364746094, Learning Rate: 0.0025\n",
      "Epoch [5910/20000], Loss: 1025.706298828125, Entropy 318.50079345703125, Learning Rate: 0.0025\n",
      "Epoch [5911/20000], Loss: 985.9285278320312, Entropy 333.09954833984375, Learning Rate: 0.0025\n",
      "Epoch [5912/20000], Loss: 967.7882080078125, Entropy 320.3785095214844, Learning Rate: 0.0025\n",
      "Epoch [5913/20000], Loss: 960.181884765625, Entropy 328.6501770019531, Learning Rate: 0.0025\n",
      "Epoch [5914/20000], Loss: 982.2779541015625, Entropy 339.78515625, Learning Rate: 0.0025\n",
      "Epoch [5915/20000], Loss: 975.75439453125, Entropy 320.0845642089844, Learning Rate: 0.0025\n",
      "Epoch [5916/20000], Loss: 1058.1962890625, Entropy 313.51519775390625, Learning Rate: 0.0025\n",
      "Epoch [5917/20000], Loss: 958.2657470703125, Entropy 338.6380310058594, Learning Rate: 0.0025\n",
      "Epoch [5918/20000], Loss: 1001.919921875, Entropy 321.2677917480469, Learning Rate: 0.0025\n",
      "Epoch [5919/20000], Loss: 1010.1719970703125, Entropy 317.7649230957031, Learning Rate: 0.0025\n",
      "Epoch [5920/20000], Loss: 943.091552734375, Entropy 323.1599426269531, Learning Rate: 0.0025\n",
      "Epoch [5921/20000], Loss: 979.533935546875, Entropy 326.478271484375, Learning Rate: 0.0025\n",
      "Epoch [5922/20000], Loss: 957.32568359375, Entropy 340.5276184082031, Learning Rate: 0.0025\n",
      "Epoch [5923/20000], Loss: 925.3497924804688, Entropy 336.41156005859375, Learning Rate: 0.0025\n",
      "Epoch [5924/20000], Loss: 1040.662841796875, Entropy 338.51763916015625, Learning Rate: 0.0025\n",
      "Epoch [5925/20000], Loss: 898.7833862304688, Entropy 322.40081787109375, Learning Rate: 0.0025\n",
      "Epoch [5926/20000], Loss: 967.4353637695312, Entropy 337.05511474609375, Learning Rate: 0.0025\n",
      "Epoch [5927/20000], Loss: 1017.3404541015625, Entropy 324.7020568847656, Learning Rate: 0.0025\n",
      "Epoch [5928/20000], Loss: 973.98291015625, Entropy 325.0022888183594, Learning Rate: 0.0025\n",
      "Epoch [5929/20000], Loss: 945.5877685546875, Entropy 332.3982849121094, Learning Rate: 0.0025\n",
      "Epoch [5930/20000], Loss: 971.2099609375, Entropy 313.253662109375, Learning Rate: 0.0025\n",
      "Epoch [5931/20000], Loss: 990.0244750976562, Entropy 318.42547607421875, Learning Rate: 0.0025\n",
      "Epoch [5932/20000], Loss: 980.3482055664062, Entropy 336.83709716796875, Learning Rate: 0.0025\n",
      "Epoch [5933/20000], Loss: 1042.038818359375, Entropy 330.9145202636719, Learning Rate: 0.0025\n",
      "Epoch [5934/20000], Loss: 1113.4942626953125, Entropy 337.6075744628906, Learning Rate: 0.0025\n",
      "Epoch [5935/20000], Loss: 1030.878173828125, Entropy 326.174560546875, Learning Rate: 0.0025\n",
      "Epoch [5936/20000], Loss: 981.3090209960938, Entropy 326.22552490234375, Learning Rate: 0.0025\n",
      "Epoch [5937/20000], Loss: 923.842041015625, Entropy 335.996826171875, Learning Rate: 0.0025\n",
      "Epoch [5938/20000], Loss: 999.3441162109375, Entropy 320.1410827636719, Learning Rate: 0.0025\n",
      "Epoch [5939/20000], Loss: 1014.8599853515625, Entropy 319.2220764160156, Learning Rate: 0.0025\n",
      "Epoch [5940/20000], Loss: 1045.3612060546875, Entropy 320.8018798828125, Learning Rate: 0.0025\n",
      "Epoch [5941/20000], Loss: 1023.5736694335938, Entropy 331.21160888671875, Learning Rate: 0.0025\n",
      "Epoch [5942/20000], Loss: 959.4559326171875, Entropy 320.2688903808594, Learning Rate: 0.0025\n",
      "Epoch [5943/20000], Loss: 955.6370849609375, Entropy 324.6396484375, Learning Rate: 0.0025\n",
      "Epoch [5944/20000], Loss: 1040.00634765625, Entropy 315.9313659667969, Learning Rate: 0.0025\n",
      "Epoch [5945/20000], Loss: 1080.588623046875, Entropy 318.13653564453125, Learning Rate: 0.0025\n",
      "Epoch [5946/20000], Loss: 960.7848510742188, Entropy 345.56292724609375, Learning Rate: 0.0025\n",
      "Epoch [5947/20000], Loss: 992.6197509765625, Entropy 326.5785827636719, Learning Rate: 0.0025\n",
      "Epoch [5948/20000], Loss: 1028.006103515625, Entropy 338.9867248535156, Learning Rate: 0.0025\n",
      "Epoch [5949/20000], Loss: 958.3038330078125, Entropy 292.6851806640625, Learning Rate: 0.0025\n",
      "Epoch [5950/20000], Loss: 1005.5850219726562, Entropy 326.47528076171875, Learning Rate: 0.0025\n",
      "Epoch [5951/20000], Loss: 928.0091552734375, Entropy 346.6833190917969, Learning Rate: 0.0025\n",
      "Epoch [5952/20000], Loss: 1022.8397216796875, Entropy 329.2037353515625, Learning Rate: 0.0025\n",
      "Epoch [5953/20000], Loss: 937.08349609375, Entropy 331.2312316894531, Learning Rate: 0.0025\n",
      "Epoch [5954/20000], Loss: 964.7960205078125, Entropy 341.3944396972656, Learning Rate: 0.0025\n",
      "Epoch [5955/20000], Loss: 995.5819091796875, Entropy 313.2835388183594, Learning Rate: 0.0025\n",
      "Epoch [5956/20000], Loss: 927.5382080078125, Entropy 324.0321044921875, Learning Rate: 0.0025\n",
      "Epoch [5957/20000], Loss: 944.59033203125, Entropy 332.0435485839844, Learning Rate: 0.0025\n",
      "Epoch [5958/20000], Loss: 970.0494384765625, Entropy 318.0440979003906, Learning Rate: 0.0025\n",
      "Epoch [5959/20000], Loss: 958.7030639648438, Entropy 310.03216552734375, Learning Rate: 0.0025\n",
      "Epoch [5960/20000], Loss: 956.1414184570312, Entropy 311.59857177734375, Learning Rate: 0.0025\n",
      "Epoch [5961/20000], Loss: 1015.4241943359375, Entropy 317.9401550292969, Learning Rate: 0.0025\n",
      "Epoch [5962/20000], Loss: 984.7479248046875, Entropy 324.820068359375, Learning Rate: 0.0025\n",
      "Epoch [5963/20000], Loss: 1057.04638671875, Entropy 319.9588928222656, Learning Rate: 0.0025\n",
      "Epoch [5964/20000], Loss: 1103.56005859375, Entropy 322.96453857421875, Learning Rate: 0.0025\n",
      "Epoch [5965/20000], Loss: 1078.8648681640625, Entropy 336.9734191894531, Learning Rate: 0.0025\n",
      "Epoch [5966/20000], Loss: 1118.37939453125, Entropy 324.794921875, Learning Rate: 0.0025\n",
      "Epoch [5967/20000], Loss: 1029.640380859375, Entropy 328.11907958984375, Learning Rate: 0.0025\n",
      "Epoch [5968/20000], Loss: 1192.342041015625, Entropy 341.0208435058594, Learning Rate: 0.0025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5969/20000], Loss: 1206.266357421875, Entropy 331.48089599609375, Learning Rate: 0.0025\n",
      "Epoch [5970/20000], Loss: 1345.646728515625, Entropy 322.10272216796875, Learning Rate: 0.0025\n",
      "Epoch [5971/20000], Loss: 1253.025634765625, Entropy 332.1875305175781, Learning Rate: 0.0025\n",
      "Epoch [5972/20000], Loss: 1512.46484375, Entropy 320.72283935546875, Learning Rate: 0.0025\n",
      "Epoch [5973/20000], Loss: 1501.2138671875, Entropy 328.9554138183594, Learning Rate: 0.0025\n",
      "Epoch [5974/20000], Loss: 2152.155517578125, Entropy 327.0888366699219, Learning Rate: 0.0025\n",
      "Epoch [5975/20000], Loss: 1628.78466796875, Entropy 320.0368957519531, Learning Rate: 0.0025\n",
      "Epoch [5976/20000], Loss: 1654.4644775390625, Entropy 319.3999328613281, Learning Rate: 0.0025\n",
      "Epoch [5977/20000], Loss: 1870.795166015625, Entropy 333.2295227050781, Learning Rate: 0.0025\n",
      "Epoch [5978/20000], Loss: 1078.3212890625, Entropy 312.70513916015625, Learning Rate: 0.0025\n",
      "Epoch [5979/20000], Loss: 1722.56787109375, Entropy 309.0683898925781, Learning Rate: 0.0025\n",
      "Epoch [5980/20000], Loss: 1184.57177734375, Entropy 312.85211181640625, Learning Rate: 0.0025\n",
      "Epoch [5981/20000], Loss: 1147.289306640625, Entropy 326.05279541015625, Learning Rate: 0.0025\n",
      "Epoch [5982/20000], Loss: 1213.6405029296875, Entropy 320.6866149902344, Learning Rate: 0.0025\n",
      "Epoch [5983/20000], Loss: 1410.6998291015625, Entropy 309.1315612792969, Learning Rate: 0.0025\n",
      "Epoch [5984/20000], Loss: 1038.10546875, Entropy 316.8495178222656, Learning Rate: 0.0025\n",
      "Epoch [5985/20000], Loss: 1480.8240966796875, Entropy 299.3830261230469, Learning Rate: 0.0025\n",
      "Epoch [5986/20000], Loss: 1046.58154296875, Entropy 317.75469970703125, Learning Rate: 0.0025\n",
      "Epoch [5987/20000], Loss: 1175.4149169921875, Entropy 310.715087890625, Learning Rate: 0.0025\n",
      "Epoch [5988/20000], Loss: 1192.5030517578125, Entropy 318.2268981933594, Learning Rate: 0.0025\n",
      "Epoch [5989/20000], Loss: 990.87158203125, Entropy 314.2076416015625, Learning Rate: 0.0025\n",
      "Epoch [5990/20000], Loss: 1047.9091796875, Entropy 319.61602783203125, Learning Rate: 0.0025\n",
      "Epoch [5991/20000], Loss: 1110.29052734375, Entropy 315.40911865234375, Learning Rate: 0.0025\n",
      "Epoch [5992/20000], Loss: 1186.580078125, Entropy 312.9957275390625, Learning Rate: 0.0025\n",
      "Epoch [5993/20000], Loss: 1149.08544921875, Entropy 308.20660400390625, Learning Rate: 0.0025\n",
      "Epoch [5994/20000], Loss: 1094.279052734375, Entropy 310.5564880371094, Learning Rate: 0.0025\n",
      "Epoch [5995/20000], Loss: 1009.0275268554688, Entropy 301.44635009765625, Learning Rate: 0.0025\n",
      "Epoch [5996/20000], Loss: 1152.53857421875, Entropy 312.9603576660156, Learning Rate: 0.0025\n",
      "Epoch [5997/20000], Loss: 994.3113403320312, Entropy 309.83416748046875, Learning Rate: 0.0025\n",
      "Epoch [5998/20000], Loss: 1154.891357421875, Entropy 304.1151428222656, Learning Rate: 0.0025\n",
      "Epoch [5999/20000], Loss: 1091.6219482421875, Entropy 301.1675720214844, Learning Rate: 0.0025\n",
      "Epoch [6000/20000], Loss: 1334.2393798828125, Entropy 313.5767517089844, Learning Rate: 0.0025\n",
      "Epoch [6001/20000], Loss: 971.5294189453125, Entropy 320.2633056640625, Learning Rate: 0.0025\n",
      "Epoch [6002/20000], Loss: 1244.4180908203125, Entropy 302.6218566894531, Learning Rate: 0.0025\n",
      "Epoch [6003/20000], Loss: 1020.4200439453125, Entropy 314.8389892578125, Learning Rate: 0.0025\n",
      "Epoch [6004/20000], Loss: 1260.2572021484375, Entropy 304.5340576171875, Learning Rate: 0.0025\n",
      "Epoch [6005/20000], Loss: 1445.2978515625, Entropy 299.825439453125, Learning Rate: 0.0025\n",
      "Epoch [6006/20000], Loss: 1241.364013671875, Entropy 307.11785888671875, Learning Rate: 0.0025\n",
      "Epoch [6007/20000], Loss: 1247.0789794921875, Entropy 301.1036376953125, Learning Rate: 0.0025\n",
      "Epoch [6008/20000], Loss: 1068.73046875, Entropy 293.69207763671875, Learning Rate: 0.0025\n",
      "Epoch [6009/20000], Loss: 1051.066650390625, Entropy 301.386962890625, Learning Rate: 0.0025\n",
      "Epoch [6010/20000], Loss: 1068.783447265625, Entropy 308.385009765625, Learning Rate: 0.0025\n",
      "Epoch [6011/20000], Loss: 1203.657470703125, Entropy 287.02069091796875, Learning Rate: 0.0025\n",
      "Epoch [6012/20000], Loss: 1375.8333740234375, Entropy 293.8463439941406, Learning Rate: 0.0025\n",
      "Epoch [6013/20000], Loss: 1024.935302734375, Entropy 297.6170959472656, Learning Rate: 0.0025\n",
      "Epoch [6014/20000], Loss: 1225.302978515625, Entropy 299.83843994140625, Learning Rate: 0.0025\n",
      "Epoch [6015/20000], Loss: 1169.54638671875, Entropy 318.5001220703125, Learning Rate: 0.0025\n",
      "Epoch [6016/20000], Loss: 1124.77392578125, Entropy 300.18597412109375, Learning Rate: 0.0025\n",
      "Epoch [6017/20000], Loss: 1229.5093994140625, Entropy 298.8465270996094, Learning Rate: 0.0025\n",
      "Epoch [6018/20000], Loss: 1053.452880859375, Entropy 304.14141845703125, Learning Rate: 0.0025\n",
      "Epoch [6019/20000], Loss: 1144.2681884765625, Entropy 294.8886413574219, Learning Rate: 0.0025\n",
      "Epoch [6020/20000], Loss: 996.56201171875, Entropy 304.0224304199219, Learning Rate: 0.0025\n",
      "Epoch [6021/20000], Loss: 922.013427734375, Entropy 315.8589172363281, Learning Rate: 0.0025\n",
      "Epoch [6022/20000], Loss: 1095.8118896484375, Entropy 304.5124816894531, Learning Rate: 0.0025\n",
      "Epoch [6023/20000], Loss: 993.123779296875, Entropy 317.4562683105469, Learning Rate: 0.0025\n",
      "Epoch [6024/20000], Loss: 1061.1181640625, Entropy 291.90972900390625, Learning Rate: 0.0025\n",
      "Epoch [6025/20000], Loss: 970.8055419921875, Entropy 309.6280517578125, Learning Rate: 0.0025\n",
      "Epoch [6026/20000], Loss: 960.2428588867188, Entropy 298.24688720703125, Learning Rate: 0.0025\n",
      "Epoch [6027/20000], Loss: 972.68603515625, Entropy 280.8517150878906, Learning Rate: 0.0025\n",
      "Epoch [6028/20000], Loss: 982.6678466796875, Entropy 291.2543029785156, Learning Rate: 0.0025\n",
      "Epoch [6029/20000], Loss: 986.0426025390625, Entropy 313.1188659667969, Learning Rate: 0.0025\n",
      "Epoch [6030/20000], Loss: 965.687255859375, Entropy 293.4331970214844, Learning Rate: 0.0025\n",
      "Epoch [6031/20000], Loss: 1082.14892578125, Entropy 306.9123840332031, Learning Rate: 0.0025\n",
      "Epoch [6032/20000], Loss: 1020.5924072265625, Entropy 302.0621337890625, Learning Rate: 0.0025\n",
      "Epoch [6033/20000], Loss: 1078.98779296875, Entropy 313.22576904296875, Learning Rate: 0.0025\n",
      "Epoch [6034/20000], Loss: 1060.7060546875, Entropy 294.09771728515625, Learning Rate: 0.0025\n",
      "Epoch [6035/20000], Loss: 988.907958984375, Entropy 264.7431335449219, Learning Rate: 0.0025\n",
      "Epoch [6036/20000], Loss: 1036.5797119140625, Entropy 296.3046569824219, Learning Rate: 0.0025\n",
      "Epoch [6037/20000], Loss: 972.453125, Entropy 286.4320983886719, Learning Rate: 0.0025\n",
      "Epoch [6038/20000], Loss: 1043.97265625, Entropy 298.26287841796875, Learning Rate: 0.0025\n",
      "Epoch [6039/20000], Loss: 1025.37451171875, Entropy 288.41339111328125, Learning Rate: 0.0025\n",
      "Epoch [6040/20000], Loss: 1013.50927734375, Entropy 313.7881774902344, Learning Rate: 0.0025\n",
      "Epoch [6041/20000], Loss: 975.681884765625, Entropy 313.3942565917969, Learning Rate: 0.0025\n",
      "Epoch [6042/20000], Loss: 1069.3282470703125, Entropy 293.6360778808594, Learning Rate: 0.0025\n",
      "Epoch [6043/20000], Loss: 1259.0751953125, Entropy 293.718505859375, Learning Rate: 0.0025\n",
      "Epoch [6044/20000], Loss: 1018.5068359375, Entropy 292.4413757324219, Learning Rate: 0.0025\n",
      "Epoch [6045/20000], Loss: 1234.651611328125, Entropy 302.5647888183594, Learning Rate: 0.0025\n",
      "Epoch [6046/20000], Loss: 1188.44140625, Entropy 291.0666809082031, Learning Rate: 0.0025\n",
      "Epoch [6047/20000], Loss: 1145.217041015625, Entropy 306.75042724609375, Learning Rate: 0.0025\n",
      "Epoch [6048/20000], Loss: 1438.80126953125, Entropy 314.750244140625, Learning Rate: 0.0025\n",
      "Epoch [6049/20000], Loss: 1049.220458984375, Entropy 293.6691589355469, Learning Rate: 0.0025\n",
      "Epoch [6050/20000], Loss: 1294.49951171875, Entropy 293.54144287109375, Learning Rate: 0.0025\n",
      "Epoch [6051/20000], Loss: 1277.949951171875, Entropy 294.4828186035156, Learning Rate: 0.0025\n",
      "Epoch [6052/20000], Loss: 989.1781005859375, Entropy 293.1452331542969, Learning Rate: 0.0025\n",
      "Epoch [6053/20000], Loss: 1228.4569091796875, Entropy 285.59228515625, Learning Rate: 0.0025\n",
      "Epoch [6054/20000], Loss: 1041.49365234375, Entropy 291.69390869140625, Learning Rate: 0.00125\n",
      "Epoch [6055/20000], Loss: 940.60107421875, Entropy 311.9450378417969, Learning Rate: 0.00125\n",
      "Epoch [6056/20000], Loss: 1028.58837890625, Entropy 292.23974609375, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6057/20000], Loss: 1052.60400390625, Entropy 286.8953857421875, Learning Rate: 0.00125\n",
      "Epoch [6058/20000], Loss: 1003.2582397460938, Entropy 319.51092529296875, Learning Rate: 0.00125\n",
      "Epoch [6059/20000], Loss: 956.9722900390625, Entropy 290.1813659667969, Learning Rate: 0.00125\n",
      "Epoch [6060/20000], Loss: 920.9385375976562, Entropy 318.28277587890625, Learning Rate: 0.00125\n",
      "Epoch [6061/20000], Loss: 1008.1107177734375, Entropy 293.2888488769531, Learning Rate: 0.00125\n",
      "Epoch [6062/20000], Loss: 1008.606201171875, Entropy 299.5333251953125, Learning Rate: 0.00125\n",
      "Epoch [6063/20000], Loss: 997.65185546875, Entropy 286.6145324707031, Learning Rate: 0.00125\n",
      "Epoch [6064/20000], Loss: 945.524169921875, Entropy 278.9769287109375, Learning Rate: 0.00125\n",
      "Epoch [6065/20000], Loss: 925.26025390625, Entropy 300.24609375, Learning Rate: 0.00125\n",
      "Epoch [6066/20000], Loss: 951.9075927734375, Entropy 293.41552734375, Learning Rate: 0.00125\n",
      "Epoch [6067/20000], Loss: 959.5560302734375, Entropy 298.47314453125, Learning Rate: 0.00125\n",
      "Epoch [6068/20000], Loss: 931.5836791992188, Entropy 309.38446044921875, Learning Rate: 0.00125\n",
      "Epoch [6069/20000], Loss: 932.1561889648438, Entropy 302.41802978515625, Learning Rate: 0.00125\n",
      "Epoch [6070/20000], Loss: 1004.5184326171875, Entropy 287.2679748535156, Learning Rate: 0.00125\n",
      "Epoch [6071/20000], Loss: 981.401611328125, Entropy 294.7621154785156, Learning Rate: 0.00125\n",
      "Epoch [6072/20000], Loss: 962.8553466796875, Entropy 294.4091491699219, Learning Rate: 0.00125\n",
      "Epoch [6073/20000], Loss: 977.0247802734375, Entropy 289.359375, Learning Rate: 0.00125\n",
      "Epoch [6074/20000], Loss: 952.7884521484375, Entropy 294.3883972167969, Learning Rate: 0.00125\n",
      "Epoch [6075/20000], Loss: 955.9285888671875, Entropy 291.5220947265625, Learning Rate: 0.00125\n",
      "Epoch [6076/20000], Loss: 923.4193115234375, Entropy 313.6578674316406, Learning Rate: 0.00125\n",
      "Epoch [6077/20000], Loss: 983.3612060546875, Entropy 287.1767578125, Learning Rate: 0.00125\n",
      "Epoch [6078/20000], Loss: 987.321533203125, Entropy 283.7607421875, Learning Rate: 0.00125\n",
      "Epoch [6079/20000], Loss: 948.2188110351562, Entropy 289.38458251953125, Learning Rate: 0.00125\n",
      "Epoch [6080/20000], Loss: 958.7021484375, Entropy 301.0281677246094, Learning Rate: 0.00125\n",
      "Epoch [6081/20000], Loss: 953.2779541015625, Entropy 309.2102355957031, Learning Rate: 0.00125\n",
      "Epoch [6082/20000], Loss: 909.2573852539062, Entropy 313.95050048828125, Learning Rate: 0.00125\n",
      "Epoch [6083/20000], Loss: 931.6181640625, Entropy 318.27587890625, Learning Rate: 0.00125\n",
      "Epoch [6084/20000], Loss: 933.436767578125, Entropy 299.9881286621094, Learning Rate: 0.00125\n",
      "Epoch [6085/20000], Loss: 930.3445434570312, Entropy 302.38006591796875, Learning Rate: 0.00125\n",
      "Epoch [6086/20000], Loss: 940.3878173828125, Entropy 296.052001953125, Learning Rate: 0.00125\n",
      "Epoch [6087/20000], Loss: 945.4940795898438, Entropy 292.98834228515625, Learning Rate: 0.00125\n",
      "Epoch [6088/20000], Loss: 911.3025512695312, Entropy 300.29644775390625, Learning Rate: 0.00125\n",
      "Epoch [6089/20000], Loss: 931.794677734375, Entropy 313.3780212402344, Learning Rate: 0.00125\n",
      "Epoch [6090/20000], Loss: 958.1728515625, Entropy 294.4331359863281, Learning Rate: 0.00125\n",
      "Epoch [6091/20000], Loss: 952.9608154296875, Entropy 293.636474609375, Learning Rate: 0.00125\n",
      "Epoch [6092/20000], Loss: 954.726318359375, Entropy 293.6726989746094, Learning Rate: 0.00125\n",
      "Epoch [6093/20000], Loss: 931.288818359375, Entropy 303.7111511230469, Learning Rate: 0.00125\n",
      "Epoch [6094/20000], Loss: 986.4508666992188, Entropy 291.50347900390625, Learning Rate: 0.00125\n",
      "Epoch [6095/20000], Loss: 930.9239501953125, Entropy 309.9453125, Learning Rate: 0.00125\n",
      "Epoch [6096/20000], Loss: 954.66552734375, Entropy 297.7509460449219, Learning Rate: 0.00125\n",
      "Epoch [6097/20000], Loss: 1006.3924560546875, Entropy 297.6872863769531, Learning Rate: 0.00125\n",
      "Epoch [6098/20000], Loss: 890.1534423828125, Entropy 293.6104431152344, Learning Rate: 0.00125\n",
      "Epoch [6099/20000], Loss: 977.9059448242188, Entropy 281.47857666015625, Learning Rate: 0.00125\n",
      "Epoch [6100/20000], Loss: 969.5249633789062, Entropy 298.04168701171875, Learning Rate: 0.00125\n",
      "Epoch [6101/20000], Loss: 914.0888671875, Entropy 276.8879699707031, Learning Rate: 0.00125\n",
      "Epoch [6102/20000], Loss: 917.375732421875, Entropy 297.8836669921875, Learning Rate: 0.00125\n",
      "Epoch [6103/20000], Loss: 956.2885131835938, Entropy 294.28485107421875, Learning Rate: 0.00125\n",
      "Epoch [6104/20000], Loss: 941.9280395507812, Entropy 300.27252197265625, Learning Rate: 0.00125\n",
      "Epoch [6105/20000], Loss: 928.1566162109375, Entropy 317.1418151855469, Learning Rate: 0.00125\n",
      "Epoch [6106/20000], Loss: 929.2203369140625, Entropy 308.7126770019531, Learning Rate: 0.00125\n",
      "Epoch [6107/20000], Loss: 943.476318359375, Entropy 297.4955139160156, Learning Rate: 0.00125\n",
      "Epoch [6108/20000], Loss: 905.7886962890625, Entropy 300.6283264160156, Learning Rate: 0.00125\n",
      "Epoch [6109/20000], Loss: 934.4528198242188, Entropy 307.77825927734375, Learning Rate: 0.00125\n",
      "Epoch [6110/20000], Loss: 912.4010009765625, Entropy 304.9624938964844, Learning Rate: 0.00125\n",
      "Epoch [6111/20000], Loss: 945.2728271484375, Entropy 300.4604187011719, Learning Rate: 0.00125\n",
      "Epoch [6112/20000], Loss: 942.7789306640625, Entropy 290.4521789550781, Learning Rate: 0.00125\n",
      "Epoch [6113/20000], Loss: 965.7562866210938, Entropy 280.94927978515625, Learning Rate: 0.00125\n",
      "Epoch [6114/20000], Loss: 964.2130737304688, Entropy 295.03192138671875, Learning Rate: 0.00125\n",
      "Epoch [6115/20000], Loss: 905.6694946289062, Entropy 307.74822998046875, Learning Rate: 0.00125\n",
      "Epoch [6116/20000], Loss: 926.0424194335938, Entropy 318.22662353515625, Learning Rate: 0.00125\n",
      "Epoch [6117/20000], Loss: 933.9813232421875, Entropy 307.7173767089844, Learning Rate: 0.00125\n",
      "Epoch [6118/20000], Loss: 907.920654296875, Entropy 315.8039855957031, Learning Rate: 0.00125\n",
      "Epoch [6119/20000], Loss: 968.88330078125, Entropy 305.1636962890625, Learning Rate: 0.00125\n",
      "Epoch [6120/20000], Loss: 931.6385498046875, Entropy 303.5096740722656, Learning Rate: 0.00125\n",
      "Epoch [6121/20000], Loss: 940.4813842773438, Entropy 308.45660400390625, Learning Rate: 0.00125\n",
      "Epoch [6122/20000], Loss: 967.640869140625, Entropy 291.742431640625, Learning Rate: 0.00125\n",
      "Epoch [6123/20000], Loss: 996.3944091796875, Entropy 290.7525634765625, Learning Rate: 0.00125\n",
      "Epoch [6124/20000], Loss: 956.4263916015625, Entropy 304.597900390625, Learning Rate: 0.00125\n",
      "Epoch [6125/20000], Loss: 914.9417114257812, Entropy 301.12042236328125, Learning Rate: 0.00125\n",
      "Epoch [6126/20000], Loss: 959.70556640625, Entropy 304.8851318359375, Learning Rate: 0.00125\n",
      "Epoch [6127/20000], Loss: 924.874267578125, Entropy 302.1642150878906, Learning Rate: 0.00125\n",
      "Epoch [6128/20000], Loss: 961.905029296875, Entropy 302.1200256347656, Learning Rate: 0.00125\n",
      "Epoch [6129/20000], Loss: 960.3037109375, Entropy 287.8299865722656, Learning Rate: 0.00125\n",
      "Epoch [6130/20000], Loss: 971.1083984375, Entropy 291.7370910644531, Learning Rate: 0.00125\n",
      "Epoch [6131/20000], Loss: 872.0095825195312, Entropy 316.38800048828125, Learning Rate: 0.00125\n",
      "Epoch [6132/20000], Loss: 982.0355834960938, Entropy 291.89776611328125, Learning Rate: 0.00125\n",
      "Epoch [6133/20000], Loss: 959.6387939453125, Entropy 295.5743713378906, Learning Rate: 0.00125\n",
      "Epoch [6134/20000], Loss: 943.856201171875, Entropy 295.2392578125, Learning Rate: 0.00125\n",
      "Epoch [6135/20000], Loss: 945.467529296875, Entropy 296.8586730957031, Learning Rate: 0.00125\n",
      "Epoch [6136/20000], Loss: 906.9166259765625, Entropy 308.7093811035156, Learning Rate: 0.00125\n",
      "Epoch [6137/20000], Loss: 894.326171875, Entropy 322.8450622558594, Learning Rate: 0.00125\n",
      "Epoch [6138/20000], Loss: 979.477783203125, Entropy 301.2743225097656, Learning Rate: 0.00125\n",
      "Epoch [6139/20000], Loss: 961.1658325195312, Entropy 306.66107177734375, Learning Rate: 0.00125\n",
      "Epoch [6140/20000], Loss: 936.5006103515625, Entropy 306.9050598144531, Learning Rate: 0.00125\n",
      "Epoch [6141/20000], Loss: 900.5341796875, Entropy 306.2823486328125, Learning Rate: 0.00125\n",
      "Epoch [6142/20000], Loss: 950.849853515625, Entropy 298.1159362792969, Learning Rate: 0.00125\n",
      "Epoch [6143/20000], Loss: 921.8016357421875, Entropy 290.035888671875, Learning Rate: 0.00125\n",
      "Epoch [6144/20000], Loss: 975.1159057617188, Entropy 291.93817138671875, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6145/20000], Loss: 891.2069702148438, Entropy 314.50445556640625, Learning Rate: 0.00125\n",
      "Epoch [6146/20000], Loss: 978.8868408203125, Entropy 287.3182067871094, Learning Rate: 0.00125\n",
      "Epoch [6147/20000], Loss: 935.35986328125, Entropy 295.3333435058594, Learning Rate: 0.00125\n",
      "Epoch [6148/20000], Loss: 959.77734375, Entropy 288.8830871582031, Learning Rate: 0.00125\n",
      "Epoch [6149/20000], Loss: 962.75048828125, Entropy 304.1783447265625, Learning Rate: 0.00125\n",
      "Epoch [6150/20000], Loss: 958.7755126953125, Entropy 295.9122009277344, Learning Rate: 0.00125\n",
      "Epoch [6151/20000], Loss: 963.6096801757812, Entropy 304.91595458984375, Learning Rate: 0.00125\n",
      "Epoch [6152/20000], Loss: 936.347900390625, Entropy 297.7591857910156, Learning Rate: 0.00125\n",
      "Epoch [6153/20000], Loss: 915.8485107421875, Entropy 298.9588317871094, Learning Rate: 0.00125\n",
      "Epoch [6154/20000], Loss: 912.17626953125, Entropy 282.0325622558594, Learning Rate: 0.00125\n",
      "Epoch [6155/20000], Loss: 920.4154052734375, Entropy 313.8128662109375, Learning Rate: 0.00125\n",
      "Epoch [6156/20000], Loss: 904.2598266601562, Entropy 305.73760986328125, Learning Rate: 0.00125\n",
      "Epoch [6157/20000], Loss: 957.8840942382812, Entropy 311.51068115234375, Learning Rate: 0.00125\n",
      "Epoch [6158/20000], Loss: 967.8597412109375, Entropy 289.6900634765625, Learning Rate: 0.00125\n",
      "Epoch [6159/20000], Loss: 965.1190185546875, Entropy 287.0608215332031, Learning Rate: 0.00125\n",
      "Epoch [6160/20000], Loss: 941.1553955078125, Entropy 319.5705871582031, Learning Rate: 0.00125\n",
      "Epoch [6161/20000], Loss: 891.7442626953125, Entropy 321.5070495605469, Learning Rate: 0.00125\n",
      "Epoch [6162/20000], Loss: 962.410888671875, Entropy 302.3846435546875, Learning Rate: 0.00125\n",
      "Epoch [6163/20000], Loss: 976.625244140625, Entropy 290.1854553222656, Learning Rate: 0.00125\n",
      "Epoch [6164/20000], Loss: 928.76171875, Entropy 302.0724182128906, Learning Rate: 0.00125\n",
      "Epoch [6165/20000], Loss: 908.236083984375, Entropy 319.5966491699219, Learning Rate: 0.00125\n",
      "Epoch [6166/20000], Loss: 921.4121704101562, Entropy 321.65362548828125, Learning Rate: 0.00125\n",
      "Epoch [6167/20000], Loss: 979.5687255859375, Entropy 294.1257019042969, Learning Rate: 0.00125\n",
      "Epoch [6168/20000], Loss: 942.3001708984375, Entropy 303.7200012207031, Learning Rate: 0.00125\n",
      "Epoch [6169/20000], Loss: 950.38134765625, Entropy 290.4715270996094, Learning Rate: 0.00125\n",
      "Epoch [6170/20000], Loss: 956.2291870117188, Entropy 289.52203369140625, Learning Rate: 0.00125\n",
      "Epoch [6171/20000], Loss: 914.367431640625, Entropy 330.3035583496094, Learning Rate: 0.00125\n",
      "Epoch [6172/20000], Loss: 912.704833984375, Entropy 300.8319091796875, Learning Rate: 0.00125\n",
      "Epoch [6173/20000], Loss: 954.591796875, Entropy 277.9704284667969, Learning Rate: 0.00125\n",
      "Epoch [6174/20000], Loss: 919.8258056640625, Entropy 308.2388916015625, Learning Rate: 0.00125\n",
      "Epoch [6175/20000], Loss: 906.4661865234375, Entropy 326.3343505859375, Learning Rate: 0.00125\n",
      "Epoch [6176/20000], Loss: 908.1068725585938, Entropy 321.53094482421875, Learning Rate: 0.00125\n",
      "Epoch [6177/20000], Loss: 925.2743530273438, Entropy 311.38909912109375, Learning Rate: 0.00125\n",
      "Epoch [6178/20000], Loss: 898.539794921875, Entropy 316.2944030761719, Learning Rate: 0.00125\n",
      "Epoch [6179/20000], Loss: 918.498779296875, Entropy 285.1639709472656, Learning Rate: 0.00125\n",
      "Epoch [6180/20000], Loss: 875.9324340820312, Entropy 320.11553955078125, Learning Rate: 0.00125\n",
      "Epoch [6181/20000], Loss: 924.4671020507812, Entropy 315.00323486328125, Learning Rate: 0.00125\n",
      "Epoch [6182/20000], Loss: 929.1867065429688, Entropy 314.52276611328125, Learning Rate: 0.00125\n",
      "Epoch [6183/20000], Loss: 951.7293701171875, Entropy 323.5553894042969, Learning Rate: 0.00125\n",
      "Epoch [6184/20000], Loss: 917.34033203125, Entropy 302.1385803222656, Learning Rate: 0.00125\n",
      "Epoch [6185/20000], Loss: 982.2392578125, Entropy 304.3599853515625, Learning Rate: 0.00125\n",
      "Epoch [6186/20000], Loss: 935.1942749023438, Entropy 293.17877197265625, Learning Rate: 0.00125\n",
      "Epoch [6187/20000], Loss: 923.7411499023438, Entropy 306.02252197265625, Learning Rate: 0.00125\n",
      "Epoch [6188/20000], Loss: 985.3023071289062, Entropy 316.21246337890625, Learning Rate: 0.00125\n",
      "Epoch [6189/20000], Loss: 949.324462890625, Entropy 314.9765625, Learning Rate: 0.00125\n",
      "Epoch [6190/20000], Loss: 932.21875, Entropy 313.2711181640625, Learning Rate: 0.00125\n",
      "Epoch [6191/20000], Loss: 898.6717529296875, Entropy 322.1517028808594, Learning Rate: 0.00125\n",
      "Epoch [6192/20000], Loss: 931.3406372070312, Entropy 301.38238525390625, Learning Rate: 0.00125\n",
      "Epoch [6193/20000], Loss: 936.0519409179688, Entropy 308.11676025390625, Learning Rate: 0.00125\n",
      "Epoch [6194/20000], Loss: 927.56689453125, Entropy 305.64794921875, Learning Rate: 0.00125\n",
      "Epoch [6195/20000], Loss: 1012.2620849609375, Entropy 299.3169250488281, Learning Rate: 0.00125\n",
      "Epoch [6196/20000], Loss: 944.8157958984375, Entropy 317.2104187011719, Learning Rate: 0.00125\n",
      "Epoch [6197/20000], Loss: 881.0933837890625, Entropy 318.6799621582031, Learning Rate: 0.00125\n",
      "Epoch [6198/20000], Loss: 942.1410522460938, Entropy 293.62847900390625, Learning Rate: 0.00125\n",
      "Epoch [6199/20000], Loss: 898.4603271484375, Entropy 324.4875183105469, Learning Rate: 0.00125\n",
      "Epoch [6200/20000], Loss: 939.90478515625, Entropy 312.0290832519531, Learning Rate: 0.00125\n",
      "Epoch [6201/20000], Loss: 937.1993408203125, Entropy 319.6477966308594, Learning Rate: 0.00125\n",
      "Epoch [6202/20000], Loss: 952.6327514648438, Entropy 292.20233154296875, Learning Rate: 0.00125\n",
      "Epoch [6203/20000], Loss: 908.6232299804688, Entropy 318.31585693359375, Learning Rate: 0.00125\n",
      "Epoch [6204/20000], Loss: 895.72119140625, Entropy 317.5513916015625, Learning Rate: 0.00125\n",
      "Epoch [6205/20000], Loss: 919.045654296875, Entropy 305.5010070800781, Learning Rate: 0.00125\n",
      "Epoch [6206/20000], Loss: 900.804931640625, Entropy 325.791748046875, Learning Rate: 0.00125\n",
      "Epoch [6207/20000], Loss: 932.1726684570312, Entropy 305.44647216796875, Learning Rate: 0.00125\n",
      "Epoch [6208/20000], Loss: 911.2316284179688, Entropy 312.77313232421875, Learning Rate: 0.00125\n",
      "Epoch [6209/20000], Loss: 938.7970581054688, Entropy 287.60418701171875, Learning Rate: 0.00125\n",
      "Epoch [6210/20000], Loss: 929.691650390625, Entropy 299.5234375, Learning Rate: 0.00125\n",
      "Epoch [6211/20000], Loss: 936.343505859375, Entropy 312.8852233886719, Learning Rate: 0.00125\n",
      "Epoch [6212/20000], Loss: 963.328125, Entropy 298.8250427246094, Learning Rate: 0.00125\n",
      "Epoch [6213/20000], Loss: 920.3928833007812, Entropy 310.23382568359375, Learning Rate: 0.00125\n",
      "Epoch [6214/20000], Loss: 873.7303466796875, Entropy 324.1284484863281, Learning Rate: 0.00125\n",
      "Epoch [6215/20000], Loss: 908.6683349609375, Entropy 316.3279724121094, Learning Rate: 0.00125\n",
      "Epoch [6216/20000], Loss: 938.1387329101562, Entropy 308.29791259765625, Learning Rate: 0.00125\n",
      "Epoch [6217/20000], Loss: 976.3961181640625, Entropy 298.3426818847656, Learning Rate: 0.00125\n",
      "Epoch [6218/20000], Loss: 914.61962890625, Entropy 320.3730773925781, Learning Rate: 0.00125\n",
      "Epoch [6219/20000], Loss: 931.0279541015625, Entropy 310.74267578125, Learning Rate: 0.00125\n",
      "Epoch [6220/20000], Loss: 937.2948608398438, Entropy 298.04241943359375, Learning Rate: 0.00125\n",
      "Epoch [6221/20000], Loss: 927.3941650390625, Entropy 311.6225280761719, Learning Rate: 0.00125\n",
      "Epoch [6222/20000], Loss: 867.3169555664062, Entropy 324.31182861328125, Learning Rate: 0.00125\n",
      "Epoch [6223/20000], Loss: 934.9865112304688, Entropy 308.69049072265625, Learning Rate: 0.00125\n",
      "Epoch [6224/20000], Loss: 919.6607055664062, Entropy 315.29315185546875, Learning Rate: 0.00125\n",
      "Epoch [6225/20000], Loss: 911.3582153320312, Entropy 328.88104248046875, Learning Rate: 0.00125\n",
      "Epoch [6226/20000], Loss: 919.1655883789062, Entropy 308.30487060546875, Learning Rate: 0.00125\n",
      "Epoch [6227/20000], Loss: 938.873046875, Entropy 307.060791015625, Learning Rate: 0.00125\n",
      "Epoch [6228/20000], Loss: 892.42919921875, Entropy 326.9736328125, Learning Rate: 0.00125\n",
      "Epoch [6229/20000], Loss: 949.8551025390625, Entropy 305.888671875, Learning Rate: 0.00125\n",
      "Epoch [6230/20000], Loss: 927.1563720703125, Entropy 317.6175842285156, Learning Rate: 0.00125\n",
      "Epoch [6231/20000], Loss: 911.142578125, Entropy 312.6263122558594, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6232/20000], Loss: 906.3138427734375, Entropy 319.4595642089844, Learning Rate: 0.00125\n",
      "Epoch [6233/20000], Loss: 913.7542724609375, Entropy 332.8199462890625, Learning Rate: 0.00125\n",
      "Epoch [6234/20000], Loss: 924.228271484375, Entropy 299.0169677734375, Learning Rate: 0.00125\n",
      "Epoch [6235/20000], Loss: 941.2781372070312, Entropy 316.48760986328125, Learning Rate: 0.00125\n",
      "Epoch [6236/20000], Loss: 904.6693115234375, Entropy 314.869140625, Learning Rate: 0.00125\n",
      "Epoch [6237/20000], Loss: 933.4832763671875, Entropy 315.4336242675781, Learning Rate: 0.00125\n",
      "Epoch [6238/20000], Loss: 915.5014038085938, Entropy 332.21038818359375, Learning Rate: 0.00125\n",
      "Epoch [6239/20000], Loss: 897.5648193359375, Entropy 308.6925048828125, Learning Rate: 0.00125\n",
      "Epoch [6240/20000], Loss: 920.4468994140625, Entropy 330.4132080078125, Learning Rate: 0.00125\n",
      "Epoch [6241/20000], Loss: 915.174072265625, Entropy 324.6168212890625, Learning Rate: 0.00125\n",
      "Epoch [6242/20000], Loss: 892.4669189453125, Entropy 322.239990234375, Learning Rate: 0.00125\n",
      "Epoch [6243/20000], Loss: 900.4268798828125, Entropy 319.3375244140625, Learning Rate: 0.00125\n",
      "Epoch [6244/20000], Loss: 977.278564453125, Entropy 318.9208984375, Learning Rate: 0.00125\n",
      "Epoch [6245/20000], Loss: 906.59228515625, Entropy 318.5535583496094, Learning Rate: 0.00125\n",
      "Epoch [6246/20000], Loss: 917.8870239257812, Entropy 333.52593994140625, Learning Rate: 0.00125\n",
      "Epoch [6247/20000], Loss: 896.7876586914062, Entropy 314.45281982421875, Learning Rate: 0.00125\n",
      "Epoch [6248/20000], Loss: 924.056640625, Entropy 317.4533386230469, Learning Rate: 0.00125\n",
      "Epoch [6249/20000], Loss: 936.91748046875, Entropy 300.7862548828125, Learning Rate: 0.00125\n",
      "Epoch [6250/20000], Loss: 927.0087890625, Entropy 320.6546325683594, Learning Rate: 0.00125\n",
      "Epoch [6251/20000], Loss: 936.9829711914062, Entropy 283.45928955078125, Learning Rate: 0.00125\n",
      "Epoch [6252/20000], Loss: 912.2503662109375, Entropy 325.3641662597656, Learning Rate: 0.00125\n",
      "Epoch [6253/20000], Loss: 898.384033203125, Entropy 326.5001525878906, Learning Rate: 0.00125\n",
      "Epoch [6254/20000], Loss: 924.4471435546875, Entropy 317.3927001953125, Learning Rate: 0.00125\n",
      "Epoch [6255/20000], Loss: 911.2489013671875, Entropy 333.353515625, Learning Rate: 0.00125\n",
      "Epoch [6256/20000], Loss: 935.4945678710938, Entropy 307.54144287109375, Learning Rate: 0.00125\n",
      "Epoch [6257/20000], Loss: 892.7951049804688, Entropy 323.19232177734375, Learning Rate: 0.00125\n",
      "Epoch [6258/20000], Loss: 931.293212890625, Entropy 328.0216979980469, Learning Rate: 0.00125\n",
      "Epoch [6259/20000], Loss: 943.6980590820312, Entropy 316.80194091796875, Learning Rate: 0.00125\n",
      "Epoch [6260/20000], Loss: 943.5740356445312, Entropy 317.39837646484375, Learning Rate: 0.00125\n",
      "Epoch [6261/20000], Loss: 906.3330078125, Entropy 320.2511901855469, Learning Rate: 0.00125\n",
      "Epoch [6262/20000], Loss: 951.7574462890625, Entropy 315.1724548339844, Learning Rate: 0.00125\n",
      "Epoch [6263/20000], Loss: 982.37890625, Entropy 300.0457458496094, Learning Rate: 0.00125\n",
      "Epoch [6264/20000], Loss: 948.1386108398438, Entropy 317.49127197265625, Learning Rate: 0.00125\n",
      "Epoch [6265/20000], Loss: 964.2703857421875, Entropy 321.138427734375, Learning Rate: 0.00125\n",
      "Epoch [6266/20000], Loss: 918.9246826171875, Entropy 321.9383850097656, Learning Rate: 0.00125\n",
      "Epoch [6267/20000], Loss: 912.09423828125, Entropy 311.6177673339844, Learning Rate: 0.00125\n",
      "Epoch [6268/20000], Loss: 948.7803955078125, Entropy 301.7978820800781, Learning Rate: 0.00125\n",
      "Epoch [6269/20000], Loss: 905.733642578125, Entropy 331.85888671875, Learning Rate: 0.00125\n",
      "Epoch [6270/20000], Loss: 915.6219482421875, Entropy 310.6440734863281, Learning Rate: 0.00125\n",
      "Epoch [6271/20000], Loss: 926.221923828125, Entropy 300.181640625, Learning Rate: 0.00125\n",
      "Epoch [6272/20000], Loss: 909.7347412109375, Entropy 334.1472473144531, Learning Rate: 0.00125\n",
      "Epoch [6273/20000], Loss: 909.7957763671875, Entropy 310.7170104980469, Learning Rate: 0.00125\n",
      "Epoch [6274/20000], Loss: 908.4381103515625, Entropy 334.5347595214844, Learning Rate: 0.00125\n",
      "Epoch [6275/20000], Loss: 980.8172607421875, Entropy 292.6552429199219, Learning Rate: 0.00125\n",
      "Epoch [6276/20000], Loss: 928.3674926757812, Entropy 324.91107177734375, Learning Rate: 0.00125\n",
      "Epoch [6277/20000], Loss: 889.011962890625, Entropy 326.1013488769531, Learning Rate: 0.00125\n",
      "Epoch [6278/20000], Loss: 916.4834594726562, Entropy 324.13397216796875, Learning Rate: 0.00125\n",
      "Epoch [6279/20000], Loss: 965.1539306640625, Entropy 305.6846923828125, Learning Rate: 0.00125\n",
      "Epoch [6280/20000], Loss: 921.5889892578125, Entropy 323.6520690917969, Learning Rate: 0.00125\n",
      "Epoch [6281/20000], Loss: 881.6618041992188, Entropy 334.52154541015625, Learning Rate: 0.00125\n",
      "Epoch [6282/20000], Loss: 928.129150390625, Entropy 325.458984375, Learning Rate: 0.00125\n",
      "Epoch [6283/20000], Loss: 927.7114868164062, Entropy 324.78131103515625, Learning Rate: 0.00125\n",
      "Epoch [6284/20000], Loss: 940.8056030273438, Entropy 312.49420166015625, Learning Rate: 0.00125\n",
      "Epoch [6285/20000], Loss: 894.6773681640625, Entropy 334.095703125, Learning Rate: 0.00125\n",
      "Epoch [6286/20000], Loss: 932.96826171875, Entropy 303.3006896972656, Learning Rate: 0.00125\n",
      "Epoch [6287/20000], Loss: 919.0990600585938, Entropy 323.58294677734375, Learning Rate: 0.00125\n",
      "Epoch [6288/20000], Loss: 934.6990966796875, Entropy 335.9951477050781, Learning Rate: 0.00125\n",
      "Epoch [6289/20000], Loss: 935.237548828125, Entropy 313.5475769042969, Learning Rate: 0.00125\n",
      "Epoch [6290/20000], Loss: 944.4901123046875, Entropy 316.5149230957031, Learning Rate: 0.00125\n",
      "Epoch [6291/20000], Loss: 945.207275390625, Entropy 317.5295104980469, Learning Rate: 0.00125\n",
      "Epoch [6292/20000], Loss: 883.8303833007812, Entropy 309.24383544921875, Learning Rate: 0.00125\n",
      "Epoch [6293/20000], Loss: 941.5993041992188, Entropy 322.18475341796875, Learning Rate: 0.00125\n",
      "Epoch [6294/20000], Loss: 938.9993896484375, Entropy 311.5298767089844, Learning Rate: 0.00125\n",
      "Epoch [6295/20000], Loss: 907.93701171875, Entropy 327.1419677734375, Learning Rate: 0.00125\n",
      "Epoch [6296/20000], Loss: 923.5577392578125, Entropy 308.5865478515625, Learning Rate: 0.00125\n",
      "Epoch [6297/20000], Loss: 915.3714599609375, Entropy 308.7185974121094, Learning Rate: 0.00125\n",
      "Epoch [6298/20000], Loss: 872.7789306640625, Entropy 344.3918151855469, Learning Rate: 0.00125\n",
      "Epoch [6299/20000], Loss: 951.4717407226562, Entropy 314.94989013671875, Learning Rate: 0.00125\n",
      "Epoch [6300/20000], Loss: 880.0230712890625, Entropy 328.6461181640625, Learning Rate: 0.00125\n",
      "Epoch [6301/20000], Loss: 920.2900390625, Entropy 323.0986022949219, Learning Rate: 0.00125\n",
      "Epoch [6302/20000], Loss: 950.2394409179688, Entropy 313.50128173828125, Learning Rate: 0.00125\n",
      "Epoch [6303/20000], Loss: 910.2322998046875, Entropy 326.73583984375, Learning Rate: 0.00125\n",
      "Epoch [6304/20000], Loss: 914.4599609375, Entropy 331.1544494628906, Learning Rate: 0.00125\n",
      "Epoch [6305/20000], Loss: 881.3749389648438, Entropy 333.70513916015625, Learning Rate: 0.00125\n",
      "Epoch [6306/20000], Loss: 954.7340698242188, Entropy 303.37225341796875, Learning Rate: 0.00125\n",
      "Epoch [6307/20000], Loss: 945.8465576171875, Entropy 319.6492004394531, Learning Rate: 0.00125\n",
      "Epoch [6308/20000], Loss: 889.1409301757812, Entropy 327.25921630859375, Learning Rate: 0.00125\n",
      "Epoch [6309/20000], Loss: 899.2960205078125, Entropy 313.980712890625, Learning Rate: 0.00125\n",
      "Epoch [6310/20000], Loss: 874.9586181640625, Entropy 341.5773620605469, Learning Rate: 0.00125\n",
      "Epoch [6311/20000], Loss: 904.44384765625, Entropy 322.4190673828125, Learning Rate: 0.00125\n",
      "Epoch [6312/20000], Loss: 963.63623046875, Entropy 330.4485168457031, Learning Rate: 0.00125\n",
      "Epoch [6313/20000], Loss: 881.619140625, Entropy 325.6668395996094, Learning Rate: 0.00125\n",
      "Epoch [6314/20000], Loss: 928.4122314453125, Entropy 316.3124084472656, Learning Rate: 0.00125\n",
      "Epoch [6315/20000], Loss: 941.7427978515625, Entropy 312.4326477050781, Learning Rate: 0.00125\n",
      "Epoch [6316/20000], Loss: 922.281982421875, Entropy 316.6438293457031, Learning Rate: 0.00125\n",
      "Epoch [6317/20000], Loss: 930.273193359375, Entropy 321.582275390625, Learning Rate: 0.00125\n",
      "Epoch [6318/20000], Loss: 886.5380859375, Entropy 324.5916442871094, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6319/20000], Loss: 930.3407592773438, Entropy 307.67706298828125, Learning Rate: 0.00125\n",
      "Epoch [6320/20000], Loss: 943.4840698242188, Entropy 326.70062255859375, Learning Rate: 0.00125\n",
      "Epoch [6321/20000], Loss: 929.4521484375, Entropy 338.7796936035156, Learning Rate: 0.00125\n",
      "Epoch [6322/20000], Loss: 928.416259765625, Entropy 321.8017578125, Learning Rate: 0.00125\n",
      "Epoch [6323/20000], Loss: 923.9698486328125, Entropy 336.1252746582031, Learning Rate: 0.00125\n",
      "Epoch [6324/20000], Loss: 900.2156372070312, Entropy 311.56683349609375, Learning Rate: 0.00125\n",
      "Epoch [6325/20000], Loss: 951.9022827148438, Entropy 325.06939697265625, Learning Rate: 0.00125\n",
      "Epoch [6326/20000], Loss: 906.385498046875, Entropy 326.0699157714844, Learning Rate: 0.00125\n",
      "Epoch [6327/20000], Loss: 940.2734985351562, Entropy 329.69183349609375, Learning Rate: 0.00125\n",
      "Epoch [6328/20000], Loss: 923.169189453125, Entropy 316.3771667480469, Learning Rate: 0.00125\n",
      "Epoch [6329/20000], Loss: 917.3138427734375, Entropy 320.5979309082031, Learning Rate: 0.00125\n",
      "Epoch [6330/20000], Loss: 908.5661010742188, Entropy 325.32403564453125, Learning Rate: 0.00125\n",
      "Epoch [6331/20000], Loss: 908.40869140625, Entropy 325.2320556640625, Learning Rate: 0.00125\n",
      "Epoch [6332/20000], Loss: 923.6430053710938, Entropy 333.25482177734375, Learning Rate: 0.00125\n",
      "Epoch [6333/20000], Loss: 956.53955078125, Entropy 319.3175048828125, Learning Rate: 0.00125\n",
      "Epoch [6334/20000], Loss: 911.6755981445312, Entropy 332.01300048828125, Learning Rate: 0.00125\n",
      "Epoch [6335/20000], Loss: 874.5518798828125, Entropy 327.5228271484375, Learning Rate: 0.00125\n",
      "Epoch [6336/20000], Loss: 898.481201171875, Entropy 309.78515625, Learning Rate: 0.00125\n",
      "Epoch [6337/20000], Loss: 930.8856201171875, Entropy 327.0079345703125, Learning Rate: 0.00125\n",
      "Epoch [6338/20000], Loss: 928.6876220703125, Entropy 319.2683410644531, Learning Rate: 0.00125\n",
      "Epoch [6339/20000], Loss: 895.2719116210938, Entropy 336.73309326171875, Learning Rate: 0.00125\n",
      "Epoch [6340/20000], Loss: 927.886962890625, Entropy 331.1260986328125, Learning Rate: 0.00125\n",
      "Epoch [6341/20000], Loss: 906.3885498046875, Entropy 308.3465881347656, Learning Rate: 0.00125\n",
      "Epoch [6342/20000], Loss: 927.8556518554688, Entropy 317.35675048828125, Learning Rate: 0.00125\n",
      "Epoch [6343/20000], Loss: 915.5521850585938, Entropy 331.15826416015625, Learning Rate: 0.00125\n",
      "Epoch [6344/20000], Loss: 916.4819946289062, Entropy 324.51422119140625, Learning Rate: 0.00125\n",
      "Epoch [6345/20000], Loss: 906.4990234375, Entropy 325.9100646972656, Learning Rate: 0.00125\n",
      "Epoch [6346/20000], Loss: 953.6722412109375, Entropy 317.8166809082031, Learning Rate: 0.00125\n",
      "Epoch [6347/20000], Loss: 899.1135864257812, Entropy 330.61871337890625, Learning Rate: 0.00125\n",
      "Epoch [6348/20000], Loss: 918.5997314453125, Entropy 314.38427734375, Learning Rate: 0.00125\n",
      "Epoch [6349/20000], Loss: 934.5115356445312, Entropy 316.24017333984375, Learning Rate: 0.00125\n",
      "Epoch [6350/20000], Loss: 923.8114013671875, Entropy 312.1447448730469, Learning Rate: 0.00125\n",
      "Epoch [6351/20000], Loss: 934.488037109375, Entropy 317.7055969238281, Learning Rate: 0.00125\n",
      "Epoch [6352/20000], Loss: 917.5332641601562, Entropy 331.24713134765625, Learning Rate: 0.00125\n",
      "Epoch [6353/20000], Loss: 897.7725830078125, Entropy 331.6761779785156, Learning Rate: 0.00125\n",
      "Epoch [6354/20000], Loss: 898.2266845703125, Entropy 335.8359680175781, Learning Rate: 0.00125\n",
      "Epoch [6355/20000], Loss: 947.373779296875, Entropy 318.8226013183594, Learning Rate: 0.00125\n",
      "Epoch [6356/20000], Loss: 925.0762939453125, Entropy 329.8124084472656, Learning Rate: 0.00125\n",
      "Epoch [6357/20000], Loss: 937.3359375, Entropy 323.5640869140625, Learning Rate: 0.00125\n",
      "Epoch [6358/20000], Loss: 891.0552978515625, Entropy 322.9225769042969, Learning Rate: 0.00125\n",
      "Epoch [6359/20000], Loss: 932.2528076171875, Entropy 336.3018798828125, Learning Rate: 0.00125\n",
      "Epoch [6360/20000], Loss: 935.5585327148438, Entropy 323.50738525390625, Learning Rate: 0.00125\n",
      "Epoch [6361/20000], Loss: 940.7290649414062, Entropy 332.84869384765625, Learning Rate: 0.00125\n",
      "Epoch [6362/20000], Loss: 934.8688354492188, Entropy 316.93658447265625, Learning Rate: 0.00125\n",
      "Epoch [6363/20000], Loss: 975.0550537109375, Entropy 318.8455810546875, Learning Rate: 0.00125\n",
      "Epoch [6364/20000], Loss: 903.25830078125, Entropy 321.6559143066406, Learning Rate: 0.00125\n",
      "Epoch [6365/20000], Loss: 884.6092529296875, Entropy 342.5837097167969, Learning Rate: 0.00125\n",
      "Epoch [6366/20000], Loss: 921.346923828125, Entropy 328.7738952636719, Learning Rate: 0.00125\n",
      "Epoch [6367/20000], Loss: 940.6055908203125, Entropy 328.2203063964844, Learning Rate: 0.00125\n",
      "Epoch [6368/20000], Loss: 943.7542114257812, Entropy 324.73748779296875, Learning Rate: 0.00125\n",
      "Epoch [6369/20000], Loss: 911.1187744140625, Entropy 319.7911071777344, Learning Rate: 0.00125\n",
      "Epoch [6370/20000], Loss: 942.181884765625, Entropy 315.09814453125, Learning Rate: 0.00125\n",
      "Epoch [6371/20000], Loss: 927.8438720703125, Entropy 317.1583557128906, Learning Rate: 0.00125\n",
      "Epoch [6372/20000], Loss: 909.4492797851562, Entropy 330.74139404296875, Learning Rate: 0.00125\n",
      "Epoch [6373/20000], Loss: 896.99365234375, Entropy 325.4461975097656, Learning Rate: 0.00125\n",
      "Epoch [6374/20000], Loss: 908.519287109375, Entropy 337.0989685058594, Learning Rate: 0.00125\n",
      "Epoch [6375/20000], Loss: 895.2542724609375, Entropy 316.2664794921875, Learning Rate: 0.00125\n",
      "Epoch [6376/20000], Loss: 924.87451171875, Entropy 313.0993347167969, Learning Rate: 0.00125\n",
      "Epoch [6377/20000], Loss: 936.07373046875, Entropy 321.1170349121094, Learning Rate: 0.00125\n",
      "Epoch [6378/20000], Loss: 916.2242431640625, Entropy 333.6120910644531, Learning Rate: 0.00125\n",
      "Epoch [6379/20000], Loss: 909.5924682617188, Entropy 337.82171630859375, Learning Rate: 0.00125\n",
      "Epoch [6380/20000], Loss: 899.9380493164062, Entropy 342.41937255859375, Learning Rate: 0.00125\n",
      "Epoch [6381/20000], Loss: 935.7943115234375, Entropy 311.1479187011719, Learning Rate: 0.00125\n",
      "Epoch [6382/20000], Loss: 860.6884765625, Entropy 326.8591613769531, Learning Rate: 0.00125\n",
      "Epoch [6383/20000], Loss: 920.1159057617188, Entropy 319.21551513671875, Learning Rate: 0.00125\n",
      "Epoch [6384/20000], Loss: 954.9118041992188, Entropy 321.29290771484375, Learning Rate: 0.00125\n",
      "Epoch [6385/20000], Loss: 921.1409912109375, Entropy 331.5079345703125, Learning Rate: 0.00125\n",
      "Epoch [6386/20000], Loss: 888.7278442382812, Entropy 332.37017822265625, Learning Rate: 0.00125\n",
      "Epoch [6387/20000], Loss: 936.1961059570312, Entropy 334.27655029296875, Learning Rate: 0.00125\n",
      "Epoch [6388/20000], Loss: 969.1849975585938, Entropy 320.83538818359375, Learning Rate: 0.00125\n",
      "Epoch [6389/20000], Loss: 928.6694946289062, Entropy 335.73797607421875, Learning Rate: 0.00125\n",
      "Epoch [6390/20000], Loss: 906.363525390625, Entropy 334.4740905761719, Learning Rate: 0.00125\n",
      "Epoch [6391/20000], Loss: 925.9485473632812, Entropy 333.80133056640625, Learning Rate: 0.00125\n",
      "Epoch [6392/20000], Loss: 923.5201416015625, Entropy 325.1014099121094, Learning Rate: 0.00125\n",
      "Epoch [6393/20000], Loss: 918.8350830078125, Entropy 315.1185302734375, Learning Rate: 0.00125\n",
      "Epoch [6394/20000], Loss: 890.7896728515625, Entropy 328.3265075683594, Learning Rate: 0.00125\n",
      "Epoch [6395/20000], Loss: 987.3856201171875, Entropy 306.9214172363281, Learning Rate: 0.00125\n",
      "Epoch [6396/20000], Loss: 904.0811157226562, Entropy 329.88568115234375, Learning Rate: 0.00125\n",
      "Epoch [6397/20000], Loss: 952.725341796875, Entropy 335.9300842285156, Learning Rate: 0.00125\n",
      "Epoch [6398/20000], Loss: 895.4151000976562, Entropy 326.78143310546875, Learning Rate: 0.00125\n",
      "Epoch [6399/20000], Loss: 895.526611328125, Entropy 334.4334716796875, Learning Rate: 0.00125\n",
      "Epoch [6400/20000], Loss: 908.1441650390625, Entropy 320.4157409667969, Learning Rate: 0.00125\n",
      "Epoch [6401/20000], Loss: 942.5636596679688, Entropy 335.48272705078125, Learning Rate: 0.00125\n",
      "Epoch [6402/20000], Loss: 906.0589599609375, Entropy 335.4870300292969, Learning Rate: 0.00125\n",
      "Epoch [6403/20000], Loss: 889.568115234375, Entropy 333.9557800292969, Learning Rate: 0.00125\n",
      "Epoch [6404/20000], Loss: 978.4180297851562, Entropy 329.80487060546875, Learning Rate: 0.00125\n",
      "Epoch [6405/20000], Loss: 906.77783203125, Entropy 340.70263671875, Learning Rate: 0.00125\n",
      "Epoch [6406/20000], Loss: 944.428466796875, Entropy 326.4613342285156, Learning Rate: 0.00125\n",
      "Epoch [6407/20000], Loss: 944.2595825195312, Entropy 324.68402099609375, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6408/20000], Loss: 894.57568359375, Entropy 318.548583984375, Learning Rate: 0.00125\n",
      "Epoch [6409/20000], Loss: 937.6524658203125, Entropy 335.3658447265625, Learning Rate: 0.00125\n",
      "Epoch [6410/20000], Loss: 928.3096923828125, Entropy 327.4004211425781, Learning Rate: 0.00125\n",
      "Epoch [6411/20000], Loss: 881.0966796875, Entropy 345.6705322265625, Learning Rate: 0.00125\n",
      "Epoch [6412/20000], Loss: 932.184326171875, Entropy 317.9510803222656, Learning Rate: 0.00125\n",
      "Epoch [6413/20000], Loss: 970.29248046875, Entropy 340.4574890136719, Learning Rate: 0.00125\n",
      "Epoch [6414/20000], Loss: 942.8631591796875, Entropy 326.2311096191406, Learning Rate: 0.00125\n",
      "Epoch [6415/20000], Loss: 984.54248046875, Entropy 315.9250183105469, Learning Rate: 0.00125\n",
      "Epoch [6416/20000], Loss: 938.361083984375, Entropy 333.9516296386719, Learning Rate: 0.00125\n",
      "Epoch [6417/20000], Loss: 928.1449584960938, Entropy 333.41558837890625, Learning Rate: 0.00125\n",
      "Epoch [6418/20000], Loss: 888.20947265625, Entropy 340.6109313964844, Learning Rate: 0.00125\n",
      "Epoch [6419/20000], Loss: 961.4756469726562, Entropy 314.81793212890625, Learning Rate: 0.00125\n",
      "Epoch [6420/20000], Loss: 945.7843017578125, Entropy 327.3290100097656, Learning Rate: 0.00125\n",
      "Epoch [6421/20000], Loss: 886.878662109375, Entropy 342.824951171875, Learning Rate: 0.00125\n",
      "Epoch [6422/20000], Loss: 931.0234375, Entropy 330.5040588378906, Learning Rate: 0.00125\n",
      "Epoch [6423/20000], Loss: 919.3450927734375, Entropy 341.158203125, Learning Rate: 0.00125\n",
      "Epoch [6424/20000], Loss: 920.5823974609375, Entropy 330.4449157714844, Learning Rate: 0.00125\n",
      "Epoch [6425/20000], Loss: 926.3491821289062, Entropy 331.27264404296875, Learning Rate: 0.00125\n",
      "Epoch [6426/20000], Loss: 936.197021484375, Entropy 332.0871276855469, Learning Rate: 0.00125\n",
      "Epoch [6427/20000], Loss: 897.7811279296875, Entropy 338.636474609375, Learning Rate: 0.00125\n",
      "Epoch [6428/20000], Loss: 916.3473510742188, Entropy 335.25616455078125, Learning Rate: 0.00125\n",
      "Epoch [6429/20000], Loss: 955.75, Entropy 320.4344177246094, Learning Rate: 0.00125\n",
      "Epoch [6430/20000], Loss: 902.2185668945312, Entropy 331.61297607421875, Learning Rate: 0.00125\n",
      "Epoch [6431/20000], Loss: 889.244384765625, Entropy 328.2030334472656, Learning Rate: 0.00125\n",
      "Epoch [6432/20000], Loss: 903.062744140625, Entropy 342.1961364746094, Learning Rate: 0.00125\n",
      "Epoch [6433/20000], Loss: 918.1260986328125, Entropy 335.0849609375, Learning Rate: 0.00125\n",
      "Epoch [6434/20000], Loss: 929.531982421875, Entropy 334.583984375, Learning Rate: 0.00125\n",
      "Epoch [6435/20000], Loss: 885.52587890625, Entropy 336.5990905761719, Learning Rate: 0.00125\n",
      "Epoch [6436/20000], Loss: 946.8948974609375, Entropy 322.5047302246094, Learning Rate: 0.00125\n",
      "Epoch [6437/20000], Loss: 937.4237060546875, Entropy 335.0903015136719, Learning Rate: 0.00125\n",
      "Epoch [6438/20000], Loss: 902.5255126953125, Entropy 338.0763244628906, Learning Rate: 0.00125\n",
      "Epoch [6439/20000], Loss: 878.0191650390625, Entropy 335.4060974121094, Learning Rate: 0.00125\n",
      "Epoch [6440/20000], Loss: 922.5347900390625, Entropy 316.7425842285156, Learning Rate: 0.00125\n",
      "Epoch [6441/20000], Loss: 895.8253173828125, Entropy 338.3892822265625, Learning Rate: 0.00125\n",
      "Epoch [6442/20000], Loss: 966.857177734375, Entropy 320.810302734375, Learning Rate: 0.00125\n",
      "Epoch [6443/20000], Loss: 897.522216796875, Entropy 334.9674072265625, Learning Rate: 0.00125\n",
      "Epoch [6444/20000], Loss: 943.671875, Entropy 324.5492248535156, Learning Rate: 0.00125\n",
      "Epoch [6445/20000], Loss: 942.5343017578125, Entropy 337.0043029785156, Learning Rate: 0.00125\n",
      "Epoch [6446/20000], Loss: 946.651611328125, Entropy 323.75, Learning Rate: 0.00125\n",
      "Epoch [6447/20000], Loss: 943.7780151367188, Entropy 326.30865478515625, Learning Rate: 0.00125\n",
      "Epoch [6448/20000], Loss: 922.7578735351562, Entropy 336.96636962890625, Learning Rate: 0.00125\n",
      "Epoch [6449/20000], Loss: 935.5999145507812, Entropy 347.25823974609375, Learning Rate: 0.00125\n",
      "Epoch [6450/20000], Loss: 895.56201171875, Entropy 335.4504699707031, Learning Rate: 0.00125\n",
      "Epoch [6451/20000], Loss: 924.3042602539062, Entropy 336.42572021484375, Learning Rate: 0.00125\n",
      "Epoch [6452/20000], Loss: 911.7293701171875, Entropy 351.1997985839844, Learning Rate: 0.00125\n",
      "Epoch [6453/20000], Loss: 929.006591796875, Entropy 333.6064758300781, Learning Rate: 0.00125\n",
      "Epoch [6454/20000], Loss: 908.2071533203125, Entropy 343.3262939453125, Learning Rate: 0.00125\n",
      "Epoch [6455/20000], Loss: 918.369873046875, Entropy 310.9097900390625, Learning Rate: 0.00125\n",
      "Epoch [6456/20000], Loss: 919.2735595703125, Entropy 332.7911376953125, Learning Rate: 0.00125\n",
      "Epoch [6457/20000], Loss: 921.275390625, Entropy 338.6202392578125, Learning Rate: 0.00125\n",
      "Epoch [6458/20000], Loss: 928.0460815429688, Entropy 344.08563232421875, Learning Rate: 0.00125\n",
      "Epoch [6459/20000], Loss: 937.693115234375, Entropy 330.1131896972656, Learning Rate: 0.00125\n",
      "Epoch [6460/20000], Loss: 916.107666015625, Entropy 343.964599609375, Learning Rate: 0.00125\n",
      "Epoch [6461/20000], Loss: 903.6299438476562, Entropy 337.71453857421875, Learning Rate: 0.00125\n",
      "Epoch [6462/20000], Loss: 915.6092529296875, Entropy 339.7584228515625, Learning Rate: 0.00125\n",
      "Epoch [6463/20000], Loss: 881.2490844726562, Entropy 342.16021728515625, Learning Rate: 0.00125\n",
      "Epoch [6464/20000], Loss: 884.4966430664062, Entropy 340.02398681640625, Learning Rate: 0.00125\n",
      "Epoch [6465/20000], Loss: 921.9290771484375, Entropy 333.7440490722656, Learning Rate: 0.00125\n",
      "Epoch [6466/20000], Loss: 910.6290893554688, Entropy 334.67205810546875, Learning Rate: 0.00125\n",
      "Epoch [6467/20000], Loss: 905.9378662109375, Entropy 331.2886657714844, Learning Rate: 0.00125\n",
      "Epoch [6468/20000], Loss: 908.36767578125, Entropy 342.0040283203125, Learning Rate: 0.00125\n",
      "Epoch [6469/20000], Loss: 913.0226440429688, Entropy 323.51727294921875, Learning Rate: 0.00125\n",
      "Epoch [6470/20000], Loss: 946.2935180664062, Entropy 313.37603759765625, Learning Rate: 0.00125\n",
      "Epoch [6471/20000], Loss: 974.1883544921875, Entropy 340.4718933105469, Learning Rate: 0.00125\n",
      "Epoch [6472/20000], Loss: 909.1473388671875, Entropy 336.3433532714844, Learning Rate: 0.00125\n",
      "Epoch [6473/20000], Loss: 880.11474609375, Entropy 356.1385803222656, Learning Rate: 0.00125\n",
      "Epoch [6474/20000], Loss: 932.414306640625, Entropy 324.4062805175781, Learning Rate: 0.00125\n",
      "Epoch [6475/20000], Loss: 882.5708618164062, Entropy 353.78411865234375, Learning Rate: 0.00125\n",
      "Epoch [6476/20000], Loss: 949.1058349609375, Entropy 350.2335205078125, Learning Rate: 0.00125\n",
      "Epoch [6477/20000], Loss: 930.3600463867188, Entropy 340.27362060546875, Learning Rate: 0.00125\n",
      "Epoch [6478/20000], Loss: 929.6078491210938, Entropy 330.95526123046875, Learning Rate: 0.00125\n",
      "Epoch [6479/20000], Loss: 932.2537841796875, Entropy 333.1932067871094, Learning Rate: 0.00125\n",
      "Epoch [6480/20000], Loss: 966.358642578125, Entropy 335.1678161621094, Learning Rate: 0.00125\n",
      "Epoch [6481/20000], Loss: 892.450927734375, Entropy 356.9604187011719, Learning Rate: 0.00125\n",
      "Epoch [6482/20000], Loss: 883.458251953125, Entropy 330.0420227050781, Learning Rate: 0.00125\n",
      "Epoch [6483/20000], Loss: 897.9403076171875, Entropy 333.1329650878906, Learning Rate: 0.00125\n",
      "Epoch [6484/20000], Loss: 875.7066650390625, Entropy 354.5953369140625, Learning Rate: 0.00125\n",
      "Epoch [6485/20000], Loss: 917.7369384765625, Entropy 337.1332092285156, Learning Rate: 0.00125\n",
      "Epoch [6486/20000], Loss: 900.845947265625, Entropy 359.8821105957031, Learning Rate: 0.00125\n",
      "Epoch [6487/20000], Loss: 945.487548828125, Entropy 328.61767578125, Learning Rate: 0.00125\n",
      "Epoch [6488/20000], Loss: 909.8108520507812, Entropy 336.82196044921875, Learning Rate: 0.00125\n",
      "Epoch [6489/20000], Loss: 947.6551513671875, Entropy 332.8127746582031, Learning Rate: 0.00125\n",
      "Epoch [6490/20000], Loss: 901.08740234375, Entropy 340.603515625, Learning Rate: 0.00125\n",
      "Epoch [6491/20000], Loss: 901.0279541015625, Entropy 335.384521484375, Learning Rate: 0.00125\n",
      "Epoch [6492/20000], Loss: 912.0169677734375, Entropy 332.5756530761719, Learning Rate: 0.00125\n",
      "Epoch [6493/20000], Loss: 945.3912963867188, Entropy 347.11077880859375, Learning Rate: 0.00125\n",
      "Epoch [6494/20000], Loss: 909.1276245117188, Entropy 341.20916748046875, Learning Rate: 0.00125\n",
      "Epoch [6495/20000], Loss: 922.0118408203125, Entropy 342.6921081542969, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6496/20000], Loss: 937.0939331054688, Entropy 357.70989990234375, Learning Rate: 0.00125\n",
      "Epoch [6497/20000], Loss: 910.6904296875, Entropy 323.0650939941406, Learning Rate: 0.00125\n",
      "Epoch [6498/20000], Loss: 927.0450439453125, Entropy 322.4957275390625, Learning Rate: 0.00125\n",
      "Epoch [6499/20000], Loss: 910.6519775390625, Entropy 328.5245666503906, Learning Rate: 0.00125\n",
      "Epoch [6500/20000], Loss: 909.7459716796875, Entropy 348.5885009765625, Learning Rate: 0.00125\n",
      "Epoch [6501/20000], Loss: 892.7293701171875, Entropy 350.1690673828125, Learning Rate: 0.00125\n",
      "Epoch [6502/20000], Loss: 915.4083251953125, Entropy 345.1866760253906, Learning Rate: 0.00125\n",
      "Epoch [6503/20000], Loss: 928.1954345703125, Entropy 346.2560119628906, Learning Rate: 0.00125\n",
      "Epoch [6504/20000], Loss: 917.0233154296875, Entropy 355.1891784667969, Learning Rate: 0.00125\n",
      "Epoch [6505/20000], Loss: 938.0924072265625, Entropy 332.6749267578125, Learning Rate: 0.00125\n",
      "Epoch [6506/20000], Loss: 956.0451049804688, Entropy 325.89471435546875, Learning Rate: 0.00125\n",
      "Epoch [6507/20000], Loss: 912.2446899414062, Entropy 350.62689208984375, Learning Rate: 0.00125\n",
      "Epoch [6508/20000], Loss: 931.7501220703125, Entropy 341.7146911621094, Learning Rate: 0.00125\n",
      "Epoch [6509/20000], Loss: 920.1573486328125, Entropy 329.1880798339844, Learning Rate: 0.00125\n",
      "Epoch [6510/20000], Loss: 876.02587890625, Entropy 333.6835021972656, Learning Rate: 0.00125\n",
      "Epoch [6511/20000], Loss: 954.3141479492188, Entropy 341.62371826171875, Learning Rate: 0.00125\n",
      "Epoch [6512/20000], Loss: 926.1361083984375, Entropy 350.4485778808594, Learning Rate: 0.00125\n",
      "Epoch [6513/20000], Loss: 962.6748657226562, Entropy 328.46917724609375, Learning Rate: 0.00125\n",
      "Epoch [6514/20000], Loss: 928.5758056640625, Entropy 341.0326232910156, Learning Rate: 0.00125\n",
      "Epoch [6515/20000], Loss: 965.983642578125, Entropy 329.7797546386719, Learning Rate: 0.00125\n",
      "Epoch [6516/20000], Loss: 920.321533203125, Entropy 338.8499450683594, Learning Rate: 0.00125\n",
      "Epoch [6517/20000], Loss: 889.6630859375, Entropy 354.4864501953125, Learning Rate: 0.00125\n",
      "Epoch [6518/20000], Loss: 921.439208984375, Entropy 329.3765563964844, Learning Rate: 0.00125\n",
      "Epoch [6519/20000], Loss: 884.9277954101562, Entropy 346.19305419921875, Learning Rate: 0.00125\n",
      "Epoch [6520/20000], Loss: 918.6529541015625, Entropy 346.4366455078125, Learning Rate: 0.00125\n",
      "Epoch [6521/20000], Loss: 948.2265625, Entropy 337.5788269042969, Learning Rate: 0.00125\n",
      "Epoch [6522/20000], Loss: 897.3226318359375, Entropy 327.7102966308594, Learning Rate: 0.00125\n",
      "Epoch [6523/20000], Loss: 942.62060546875, Entropy 332.7019348144531, Learning Rate: 0.00125\n",
      "Epoch [6524/20000], Loss: 909.5446166992188, Entropy 336.35101318359375, Learning Rate: 0.00125\n",
      "Epoch [6525/20000], Loss: 952.1595458984375, Entropy 314.4335632324219, Learning Rate: 0.00125\n",
      "Epoch [6526/20000], Loss: 951.0057373046875, Entropy 345.5085144042969, Learning Rate: 0.00125\n",
      "Epoch [6527/20000], Loss: 904.1123657226562, Entropy 327.95379638671875, Learning Rate: 0.00125\n",
      "Epoch [6528/20000], Loss: 907.8326416015625, Entropy 347.8238525390625, Learning Rate: 0.00125\n",
      "Epoch [6529/20000], Loss: 925.8115234375, Entropy 340.6123352050781, Learning Rate: 0.00125\n",
      "Epoch [6530/20000], Loss: 919.0128173828125, Entropy 330.073974609375, Learning Rate: 0.00125\n",
      "Epoch [6531/20000], Loss: 890.8298950195312, Entropy 350.40826416015625, Learning Rate: 0.00125\n",
      "Epoch [6532/20000], Loss: 896.9041748046875, Entropy 345.4689025878906, Learning Rate: 0.00125\n",
      "Epoch [6533/20000], Loss: 915.1842041015625, Entropy 333.5339050292969, Learning Rate: 0.00125\n",
      "Epoch [6534/20000], Loss: 894.0659790039062, Entropy 353.81097412109375, Learning Rate: 0.00125\n",
      "Epoch [6535/20000], Loss: 890.7295532226562, Entropy 334.87640380859375, Learning Rate: 0.00125\n",
      "Epoch [6536/20000], Loss: 942.48095703125, Entropy 333.2524108886719, Learning Rate: 0.00125\n",
      "Epoch [6537/20000], Loss: 892.9741821289062, Entropy 361.74713134765625, Learning Rate: 0.00125\n",
      "Epoch [6538/20000], Loss: 918.6505126953125, Entropy 346.1238098144531, Learning Rate: 0.00125\n",
      "Epoch [6539/20000], Loss: 971.7623291015625, Entropy 342.6121826171875, Learning Rate: 0.00125\n",
      "Epoch [6540/20000], Loss: 934.894287109375, Entropy 339.3863220214844, Learning Rate: 0.00125\n",
      "Epoch [6541/20000], Loss: 917.2855224609375, Entropy 334.425537109375, Learning Rate: 0.00125\n",
      "Epoch [6542/20000], Loss: 907.7291259765625, Entropy 358.8456115722656, Learning Rate: 0.00125\n",
      "Epoch [6543/20000], Loss: 940.1105346679688, Entropy 347.02386474609375, Learning Rate: 0.00125\n",
      "Epoch [6544/20000], Loss: 914.5989379882812, Entropy 342.14593505859375, Learning Rate: 0.00125\n",
      "Epoch [6545/20000], Loss: 916.3365478515625, Entropy 340.5384216308594, Learning Rate: 0.00125\n",
      "Epoch [6546/20000], Loss: 881.8089599609375, Entropy 341.2724609375, Learning Rate: 0.00125\n",
      "Epoch [6547/20000], Loss: 934.66845703125, Entropy 329.5246887207031, Learning Rate: 0.00125\n",
      "Epoch [6548/20000], Loss: 883.3203125, Entropy 351.627685546875, Learning Rate: 0.00125\n",
      "Epoch [6549/20000], Loss: 887.3889770507812, Entropy 338.20721435546875, Learning Rate: 0.00125\n",
      "Epoch [6550/20000], Loss: 931.2918701171875, Entropy 352.95458984375, Learning Rate: 0.00125\n",
      "Epoch [6551/20000], Loss: 922.6812744140625, Entropy 338.7590637207031, Learning Rate: 0.00125\n",
      "Epoch [6552/20000], Loss: 920.072265625, Entropy 336.1310119628906, Learning Rate: 0.00125\n",
      "Epoch [6553/20000], Loss: 941.1741943359375, Entropy 346.6197509765625, Learning Rate: 0.00125\n",
      "Epoch [6554/20000], Loss: 880.9130859375, Entropy 353.4674072265625, Learning Rate: 0.00125\n",
      "Epoch [6555/20000], Loss: 890.6143798828125, Entropy 339.3970947265625, Learning Rate: 0.00125\n",
      "Epoch [6556/20000], Loss: 916.3119506835938, Entropy 338.23577880859375, Learning Rate: 0.00125\n",
      "Epoch [6557/20000], Loss: 903.5999755859375, Entropy 358.511474609375, Learning Rate: 0.00125\n",
      "Epoch [6558/20000], Loss: 910.0260620117188, Entropy 352.05010986328125, Learning Rate: 0.00125\n",
      "Epoch [6559/20000], Loss: 898.2075805664062, Entropy 350.32952880859375, Learning Rate: 0.00125\n",
      "Epoch [6560/20000], Loss: 887.16552734375, Entropy 332.9178161621094, Learning Rate: 0.00125\n",
      "Epoch [6561/20000], Loss: 928.5968627929688, Entropy 347.56890869140625, Learning Rate: 0.00125\n",
      "Epoch [6562/20000], Loss: 936.1187744140625, Entropy 339.4784240722656, Learning Rate: 0.00125\n",
      "Epoch [6563/20000], Loss: 905.667724609375, Entropy 343.9851989746094, Learning Rate: 0.00125\n",
      "Epoch [6564/20000], Loss: 943.968017578125, Entropy 351.95263671875, Learning Rate: 0.00125\n",
      "Epoch [6565/20000], Loss: 935.7037353515625, Entropy 327.2419738769531, Learning Rate: 0.00125\n",
      "Epoch [6566/20000], Loss: 877.1925048828125, Entropy 339.6961669921875, Learning Rate: 0.00125\n",
      "Epoch [6567/20000], Loss: 914.229736328125, Entropy 352.9519348144531, Learning Rate: 0.00125\n",
      "Epoch [6568/20000], Loss: 937.1144409179688, Entropy 347.67962646484375, Learning Rate: 0.00125\n",
      "Epoch [6569/20000], Loss: 927.3759155273438, Entropy 358.05413818359375, Learning Rate: 0.00125\n",
      "Epoch [6570/20000], Loss: 918.1275024414062, Entropy 341.33636474609375, Learning Rate: 0.00125\n",
      "Epoch [6571/20000], Loss: 926.0814208984375, Entropy 332.2575378417969, Learning Rate: 0.00125\n",
      "Epoch [6572/20000], Loss: 930.8065185546875, Entropy 343.0928039550781, Learning Rate: 0.00125\n",
      "Epoch [6573/20000], Loss: 907.6649169921875, Entropy 352.0188903808594, Learning Rate: 0.00125\n",
      "Epoch [6574/20000], Loss: 967.4457397460938, Entropy 350.85797119140625, Learning Rate: 0.00125\n",
      "Epoch [6575/20000], Loss: 959.66748046875, Entropy 333.9201354980469, Learning Rate: 0.00125\n",
      "Epoch [6576/20000], Loss: 891.860595703125, Entropy 341.3429870605469, Learning Rate: 0.00125\n",
      "Epoch [6577/20000], Loss: 933.8088989257812, Entropy 333.70672607421875, Learning Rate: 0.00125\n",
      "Epoch [6578/20000], Loss: 913.4127197265625, Entropy 341.1283264160156, Learning Rate: 0.00125\n",
      "Epoch [6579/20000], Loss: 889.321533203125, Entropy 354.75146484375, Learning Rate: 0.00125\n",
      "Epoch [6580/20000], Loss: 902.3783569335938, Entropy 350.22784423828125, Learning Rate: 0.00125\n",
      "Epoch [6581/20000], Loss: 890.9632568359375, Entropy 346.2605285644531, Learning Rate: 0.00125\n",
      "Epoch [6582/20000], Loss: 886.0302734375, Entropy 345.1078796386719, Learning Rate: 0.00125\n",
      "Epoch [6583/20000], Loss: 906.0523681640625, Entropy 340.5525817871094, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6584/20000], Loss: 885.462890625, Entropy 359.4332580566406, Learning Rate: 0.00125\n",
      "Epoch [6585/20000], Loss: 946.6295166015625, Entropy 344.6865539550781, Learning Rate: 0.00125\n",
      "Epoch [6586/20000], Loss: 930.180908203125, Entropy 344.7983093261719, Learning Rate: 0.00125\n",
      "Epoch [6587/20000], Loss: 957.0789794921875, Entropy 330.50537109375, Learning Rate: 0.00125\n",
      "Epoch [6588/20000], Loss: 875.8895263671875, Entropy 370.2018737792969, Learning Rate: 0.00125\n",
      "Epoch [6589/20000], Loss: 958.8568115234375, Entropy 353.6427917480469, Learning Rate: 0.00125\n",
      "Epoch [6590/20000], Loss: 941.5794677734375, Entropy 326.2220153808594, Learning Rate: 0.00125\n",
      "Epoch [6591/20000], Loss: 891.2576293945312, Entropy 348.12811279296875, Learning Rate: 0.00125\n",
      "Epoch [6592/20000], Loss: 901.7549438476562, Entropy 341.38641357421875, Learning Rate: 0.00125\n",
      "Epoch [6593/20000], Loss: 917.3861083984375, Entropy 360.648193359375, Learning Rate: 0.00125\n",
      "Epoch [6594/20000], Loss: 937.2987060546875, Entropy 352.0244140625, Learning Rate: 0.00125\n",
      "Epoch [6595/20000], Loss: 926.3109741210938, Entropy 350.70428466796875, Learning Rate: 0.00125\n",
      "Epoch [6596/20000], Loss: 920.6564331054688, Entropy 343.73724365234375, Learning Rate: 0.00125\n",
      "Epoch [6597/20000], Loss: 957.4312744140625, Entropy 337.6564636230469, Learning Rate: 0.00125\n",
      "Epoch [6598/20000], Loss: 929.4478759765625, Entropy 327.8038330078125, Learning Rate: 0.00125\n",
      "Epoch [6599/20000], Loss: 921.245361328125, Entropy 348.82373046875, Learning Rate: 0.00125\n",
      "Epoch [6600/20000], Loss: 922.8844604492188, Entropy 343.99896240234375, Learning Rate: 0.00125\n",
      "Epoch [6601/20000], Loss: 897.6346435546875, Entropy 336.7935791015625, Learning Rate: 0.00125\n",
      "Epoch [6602/20000], Loss: 929.58984375, Entropy 358.967529296875, Learning Rate: 0.00125\n",
      "Epoch [6603/20000], Loss: 925.002197265625, Entropy 335.8478088378906, Learning Rate: 0.00125\n",
      "Epoch [6604/20000], Loss: 891.3236694335938, Entropy 355.54534912109375, Learning Rate: 0.00125\n",
      "Epoch [6605/20000], Loss: 897.0089111328125, Entropy 346.180419921875, Learning Rate: 0.00125\n",
      "Epoch [6606/20000], Loss: 934.9222412109375, Entropy 351.5152282714844, Learning Rate: 0.00125\n",
      "Epoch [6607/20000], Loss: 936.2267456054688, Entropy 327.39349365234375, Learning Rate: 0.00125\n",
      "Epoch [6608/20000], Loss: 902.4805908203125, Entropy 349.5585021972656, Learning Rate: 0.00125\n",
      "Epoch [6609/20000], Loss: 881.7854614257812, Entropy 344.81536865234375, Learning Rate: 0.00125\n",
      "Epoch [6610/20000], Loss: 915.1776733398438, Entropy 321.88873291015625, Learning Rate: 0.00125\n",
      "Epoch [6611/20000], Loss: 929.9915771484375, Entropy 343.9115295410156, Learning Rate: 0.00125\n",
      "Epoch [6612/20000], Loss: 955.982421875, Entropy 335.5003967285156, Learning Rate: 0.00125\n",
      "Epoch [6613/20000], Loss: 905.0745849609375, Entropy 338.5256042480469, Learning Rate: 0.00125\n",
      "Epoch [6614/20000], Loss: 883.031494140625, Entropy 341.5205078125, Learning Rate: 0.00125\n",
      "Epoch [6615/20000], Loss: 936.6176147460938, Entropy 349.31475830078125, Learning Rate: 0.00125\n",
      "Epoch [6616/20000], Loss: 904.709228515625, Entropy 340.7061767578125, Learning Rate: 0.00125\n",
      "Epoch [6617/20000], Loss: 902.4605712890625, Entropy 344.1721496582031, Learning Rate: 0.00125\n",
      "Epoch [6618/20000], Loss: 902.3202514648438, Entropy 350.61773681640625, Learning Rate: 0.00125\n",
      "Epoch [6619/20000], Loss: 905.4415283203125, Entropy 342.8650817871094, Learning Rate: 0.00125\n",
      "Epoch [6620/20000], Loss: 877.3037109375, Entropy 359.506591796875, Learning Rate: 0.00125\n",
      "Epoch [6621/20000], Loss: 927.3699951171875, Entropy 339.6402587890625, Learning Rate: 0.00125\n",
      "Epoch [6622/20000], Loss: 896.6666259765625, Entropy 354.8779602050781, Learning Rate: 0.00125\n",
      "Epoch [6623/20000], Loss: 917.2341918945312, Entropy 350.86932373046875, Learning Rate: 0.00125\n",
      "Epoch [6624/20000], Loss: 902.407958984375, Entropy 339.91748046875, Learning Rate: 0.00125\n",
      "Epoch [6625/20000], Loss: 942.0117797851562, Entropy 351.35101318359375, Learning Rate: 0.00125\n",
      "Epoch [6626/20000], Loss: 917.3373413085938, Entropy 335.58587646484375, Learning Rate: 0.00125\n",
      "Epoch [6627/20000], Loss: 974.802490234375, Entropy 336.2314758300781, Learning Rate: 0.00125\n",
      "Epoch [6628/20000], Loss: 935.2963256835938, Entropy 349.91241455078125, Learning Rate: 0.00125\n",
      "Epoch [6629/20000], Loss: 921.0069580078125, Entropy 342.891357421875, Learning Rate: 0.00125\n",
      "Epoch [6630/20000], Loss: 884.4691162109375, Entropy 370.6009216308594, Learning Rate: 0.00125\n",
      "Epoch [6631/20000], Loss: 875.5889282226562, Entropy 358.60687255859375, Learning Rate: 0.00125\n",
      "Epoch [6632/20000], Loss: 884.8690185546875, Entropy 346.5483093261719, Learning Rate: 0.00125\n",
      "Epoch [6633/20000], Loss: 876.8709106445312, Entropy 358.27276611328125, Learning Rate: 0.00125\n",
      "Epoch [6634/20000], Loss: 906.2830810546875, Entropy 363.6539306640625, Learning Rate: 0.00125\n",
      "Epoch [6635/20000], Loss: 881.1397705078125, Entropy 356.4886169433594, Learning Rate: 0.00125\n",
      "Epoch [6636/20000], Loss: 909.24072265625, Entropy 352.3448486328125, Learning Rate: 0.00125\n",
      "Epoch [6637/20000], Loss: 946.096435546875, Entropy 364.7439880371094, Learning Rate: 0.00125\n",
      "Epoch [6638/20000], Loss: 916.8228759765625, Entropy 348.506591796875, Learning Rate: 0.00125\n",
      "Epoch [6639/20000], Loss: 919.4639892578125, Entropy 347.2196044921875, Learning Rate: 0.00125\n",
      "Epoch [6640/20000], Loss: 897.7719116210938, Entropy 356.70208740234375, Learning Rate: 0.00125\n",
      "Epoch [6641/20000], Loss: 933.9432983398438, Entropy 334.27459716796875, Learning Rate: 0.00125\n",
      "Epoch [6642/20000], Loss: 924.3067626953125, Entropy 325.3854675292969, Learning Rate: 0.00125\n",
      "Epoch [6643/20000], Loss: 901.21337890625, Entropy 354.0765686035156, Learning Rate: 0.00125\n",
      "Epoch [6644/20000], Loss: 906.9847412109375, Entropy 357.9625244140625, Learning Rate: 0.00125\n",
      "Epoch [6645/20000], Loss: 904.119873046875, Entropy 363.4270324707031, Learning Rate: 0.00125\n",
      "Epoch [6646/20000], Loss: 921.454345703125, Entropy 368.7425842285156, Learning Rate: 0.00125\n",
      "Epoch [6647/20000], Loss: 893.3291015625, Entropy 353.4242858886719, Learning Rate: 0.00125\n",
      "Epoch [6648/20000], Loss: 881.5906982421875, Entropy 350.1125183105469, Learning Rate: 0.00125\n",
      "Epoch [6649/20000], Loss: 929.177490234375, Entropy 358.2037353515625, Learning Rate: 0.00125\n",
      "Epoch [6650/20000], Loss: 908.031494140625, Entropy 359.1271057128906, Learning Rate: 0.00125\n",
      "Epoch [6651/20000], Loss: 904.3028564453125, Entropy 352.9224548339844, Learning Rate: 0.00125\n",
      "Epoch [6652/20000], Loss: 901.90673828125, Entropy 341.6318359375, Learning Rate: 0.00125\n",
      "Epoch [6653/20000], Loss: 915.1505737304688, Entropy 350.33050537109375, Learning Rate: 0.00125\n",
      "Epoch [6654/20000], Loss: 947.0044555664062, Entropy 340.16961669921875, Learning Rate: 0.00125\n",
      "Epoch [6655/20000], Loss: 906.5016479492188, Entropy 352.65155029296875, Learning Rate: 0.00125\n",
      "Epoch [6656/20000], Loss: 875.0794677734375, Entropy 363.1807861328125, Learning Rate: 0.00125\n",
      "Epoch [6657/20000], Loss: 930.3837890625, Entropy 359.3263854980469, Learning Rate: 0.00125\n",
      "Epoch [6658/20000], Loss: 892.83251953125, Entropy 360.4826965332031, Learning Rate: 0.00125\n",
      "Epoch [6659/20000], Loss: 944.624755859375, Entropy 339.5273742675781, Learning Rate: 0.00125\n",
      "Epoch [6660/20000], Loss: 906.00048828125, Entropy 351.76123046875, Learning Rate: 0.00125\n",
      "Epoch [6661/20000], Loss: 955.5386962890625, Entropy 342.5740661621094, Learning Rate: 0.00125\n",
      "Epoch [6662/20000], Loss: 861.8768310546875, Entropy 366.125732421875, Learning Rate: 0.00125\n",
      "Epoch [6663/20000], Loss: 906.038330078125, Entropy 354.2374267578125, Learning Rate: 0.00125\n",
      "Epoch [6664/20000], Loss: 922.0379638671875, Entropy 347.3118591308594, Learning Rate: 0.00125\n",
      "Epoch [6665/20000], Loss: 881.8919677734375, Entropy 357.4385986328125, Learning Rate: 0.00125\n",
      "Epoch [6666/20000], Loss: 923.07275390625, Entropy 337.9985046386719, Learning Rate: 0.00125\n",
      "Epoch [6667/20000], Loss: 901.251708984375, Entropy 364.8179626464844, Learning Rate: 0.00125\n",
      "Epoch [6668/20000], Loss: 904.6834716796875, Entropy 355.2980651855469, Learning Rate: 0.00125\n",
      "Epoch [6669/20000], Loss: 966.52587890625, Entropy 347.8714904785156, Learning Rate: 0.00125\n",
      "Epoch [6670/20000], Loss: 910.23486328125, Entropy 353.7596740722656, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6671/20000], Loss: 979.2582397460938, Entropy 351.19927978515625, Learning Rate: 0.00125\n",
      "Epoch [6672/20000], Loss: 863.6124267578125, Entropy 367.4747619628906, Learning Rate: 0.00125\n",
      "Epoch [6673/20000], Loss: 953.5184326171875, Entropy 365.6109313964844, Learning Rate: 0.00125\n",
      "Epoch [6674/20000], Loss: 922.6009521484375, Entropy 346.8058166503906, Learning Rate: 0.00125\n",
      "Epoch [6675/20000], Loss: 921.0946044921875, Entropy 370.7110595703125, Learning Rate: 0.00125\n",
      "Epoch [6676/20000], Loss: 874.351806640625, Entropy 351.3359375, Learning Rate: 0.00125\n",
      "Epoch [6677/20000], Loss: 954.6004028320312, Entropy 345.14410400390625, Learning Rate: 0.00125\n",
      "Epoch [6678/20000], Loss: 922.9097900390625, Entropy 356.21484375, Learning Rate: 0.00125\n",
      "Epoch [6679/20000], Loss: 886.483154296875, Entropy 347.429443359375, Learning Rate: 0.00125\n",
      "Epoch [6680/20000], Loss: 887.8839111328125, Entropy 365.409912109375, Learning Rate: 0.00125\n",
      "Epoch [6681/20000], Loss: 925.6696166992188, Entropy 351.37042236328125, Learning Rate: 0.00125\n",
      "Epoch [6682/20000], Loss: 905.0917358398438, Entropy 357.07061767578125, Learning Rate: 0.00125\n",
      "Epoch [6683/20000], Loss: 924.7738037109375, Entropy 355.4342956542969, Learning Rate: 0.00125\n",
      "Epoch [6684/20000], Loss: 903.368408203125, Entropy 358.4313049316406, Learning Rate: 0.00125\n",
      "Epoch [6685/20000], Loss: 925.7042236328125, Entropy 363.4095153808594, Learning Rate: 0.00125\n",
      "Epoch [6686/20000], Loss: 1084.517822265625, Entropy 355.8568420410156, Learning Rate: 0.00125\n",
      "Epoch [6687/20000], Loss: 967.1873779296875, Entropy 361.3707580566406, Learning Rate: 0.00125\n",
      "Epoch [6688/20000], Loss: 1027.935546875, Entropy 357.2840270996094, Learning Rate: 0.00125\n",
      "Epoch [6689/20000], Loss: 951.0440673828125, Entropy 364.3381652832031, Learning Rate: 0.00125\n",
      "Epoch [6690/20000], Loss: 1033.603515625, Entropy 337.39801025390625, Learning Rate: 0.00125\n",
      "Epoch [6691/20000], Loss: 886.5030517578125, Entropy 370.1781921386719, Learning Rate: 0.00125\n",
      "Epoch [6692/20000], Loss: 1004.7102661132812, Entropy 361.97515869140625, Learning Rate: 0.00125\n",
      "Epoch [6693/20000], Loss: 946.054931640625, Entropy 357.3058166503906, Learning Rate: 0.00125\n",
      "Epoch [6694/20000], Loss: 1004.0240478515625, Entropy 349.63525390625, Learning Rate: 0.00125\n",
      "Epoch [6695/20000], Loss: 1009.4600219726562, Entropy 348.84796142578125, Learning Rate: 0.00125\n",
      "Epoch [6696/20000], Loss: 982.757080078125, Entropy 340.5666809082031, Learning Rate: 0.00125\n",
      "Epoch [6697/20000], Loss: 972.734130859375, Entropy 342.7933654785156, Learning Rate: 0.00125\n",
      "Epoch [6698/20000], Loss: 1042.993408203125, Entropy 356.0909729003906, Learning Rate: 0.00125\n",
      "Epoch [6699/20000], Loss: 862.4158325195312, Entropy 363.68890380859375, Learning Rate: 0.00125\n",
      "Epoch [6700/20000], Loss: 921.521484375, Entropy 354.6011962890625, Learning Rate: 0.00125\n",
      "Epoch [6701/20000], Loss: 976.1751708984375, Entropy 351.2402648925781, Learning Rate: 0.00125\n",
      "Epoch [6702/20000], Loss: 946.2001953125, Entropy 340.800537109375, Learning Rate: 0.00125\n",
      "Epoch [6703/20000], Loss: 912.1207275390625, Entropy 358.3832092285156, Learning Rate: 0.00125\n",
      "Epoch [6704/20000], Loss: 964.2987060546875, Entropy 345.2504577636719, Learning Rate: 0.00125\n",
      "Epoch [6705/20000], Loss: 905.9810791015625, Entropy 359.9795227050781, Learning Rate: 0.00125\n",
      "Epoch [6706/20000], Loss: 924.6823120117188, Entropy 329.20013427734375, Learning Rate: 0.00125\n",
      "Epoch [6707/20000], Loss: 897.0516357421875, Entropy 358.9281311035156, Learning Rate: 0.00125\n",
      "Epoch [6708/20000], Loss: 946.1095581054688, Entropy 333.66131591796875, Learning Rate: 0.00125\n",
      "Epoch [6709/20000], Loss: 952.62255859375, Entropy 359.5998229980469, Learning Rate: 0.00125\n",
      "Epoch [6710/20000], Loss: 936.131591796875, Entropy 337.2240905761719, Learning Rate: 0.00125\n",
      "Epoch [6711/20000], Loss: 908.3573608398438, Entropy 349.72271728515625, Learning Rate: 0.00125\n",
      "Epoch [6712/20000], Loss: 1000.37060546875, Entropy 358.5250549316406, Learning Rate: 0.00125\n",
      "Epoch [6713/20000], Loss: 927.2939453125, Entropy 362.3824462890625, Learning Rate: 0.00125\n",
      "Epoch [6714/20000], Loss: 1014.669921875, Entropy 343.25, Learning Rate: 0.00125\n",
      "Epoch [6715/20000], Loss: 941.635498046875, Entropy 352.3106994628906, Learning Rate: 0.00125\n",
      "Epoch [6716/20000], Loss: 941.5894165039062, Entropy 342.29388427734375, Learning Rate: 0.00125\n",
      "Epoch [6717/20000], Loss: 939.6776123046875, Entropy 353.9994201660156, Learning Rate: 0.00125\n",
      "Epoch [6718/20000], Loss: 937.947021484375, Entropy 357.0121154785156, Learning Rate: 0.00125\n",
      "Epoch [6719/20000], Loss: 974.812255859375, Entropy 336.4450378417969, Learning Rate: 0.00125\n",
      "Epoch [6720/20000], Loss: 968.3868408203125, Entropy 339.6808166503906, Learning Rate: 0.00125\n",
      "Epoch [6721/20000], Loss: 928.7110595703125, Entropy 348.9475402832031, Learning Rate: 0.00125\n",
      "Epoch [6722/20000], Loss: 1006.3118286132812, Entropy 348.97625732421875, Learning Rate: 0.00125\n",
      "Epoch [6723/20000], Loss: 875.813720703125, Entropy 365.7819519042969, Learning Rate: 0.00125\n",
      "Epoch [6724/20000], Loss: 997.7249755859375, Entropy 352.0314025878906, Learning Rate: 0.00125\n",
      "Epoch [6725/20000], Loss: 950.5143432617188, Entropy 353.01568603515625, Learning Rate: 0.00125\n",
      "Epoch [6726/20000], Loss: 1013.4664306640625, Entropy 327.6407470703125, Learning Rate: 0.00125\n",
      "Epoch [6727/20000], Loss: 913.5390625, Entropy 350.6699523925781, Learning Rate: 0.00125\n",
      "Epoch [6728/20000], Loss: 957.468994140625, Entropy 343.7056579589844, Learning Rate: 0.00125\n",
      "Epoch [6729/20000], Loss: 878.3513793945312, Entropy 359.31951904296875, Learning Rate: 0.00125\n",
      "Epoch [6730/20000], Loss: 950.079345703125, Entropy 361.9318542480469, Learning Rate: 0.00125\n",
      "Epoch [6731/20000], Loss: 937.996337890625, Entropy 345.3854675292969, Learning Rate: 0.00125\n",
      "Epoch [6732/20000], Loss: 926.2807006835938, Entropy 348.49969482421875, Learning Rate: 0.00125\n",
      "Epoch [6733/20000], Loss: 948.4361572265625, Entropy 344.271240234375, Learning Rate: 0.00125\n",
      "Epoch [6734/20000], Loss: 915.510498046875, Entropy 358.8595275878906, Learning Rate: 0.00125\n",
      "Epoch [6735/20000], Loss: 878.2095336914062, Entropy 353.32135009765625, Learning Rate: 0.00125\n",
      "Epoch [6736/20000], Loss: 911.6768188476562, Entropy 350.27239990234375, Learning Rate: 0.00125\n",
      "Epoch [6737/20000], Loss: 886.534912109375, Entropy 365.9815673828125, Learning Rate: 0.00125\n",
      "Epoch [6738/20000], Loss: 948.7010498046875, Entropy 351.8033447265625, Learning Rate: 0.00125\n",
      "Epoch [6739/20000], Loss: 966.3902587890625, Entropy 360.6261901855469, Learning Rate: 0.00125\n",
      "Epoch [6740/20000], Loss: 932.7338256835938, Entropy 348.48834228515625, Learning Rate: 0.00125\n",
      "Epoch [6741/20000], Loss: 908.412841796875, Entropy 339.7149658203125, Learning Rate: 0.00125\n",
      "Epoch [6742/20000], Loss: 922.3640747070312, Entropy 347.43902587890625, Learning Rate: 0.00125\n",
      "Epoch [6743/20000], Loss: 873.59912109375, Entropy 355.7512512207031, Learning Rate: 0.00125\n",
      "Epoch [6744/20000], Loss: 877.50341796875, Entropy 352.9288330078125, Learning Rate: 0.00125\n",
      "Epoch [6745/20000], Loss: 913.9652709960938, Entropy 338.87567138671875, Learning Rate: 0.00125\n",
      "Epoch [6746/20000], Loss: 900.267822265625, Entropy 360.2566833496094, Learning Rate: 0.00125\n",
      "Epoch [6747/20000], Loss: 927.044189453125, Entropy 370.2967529296875, Learning Rate: 0.00125\n",
      "Epoch [6748/20000], Loss: 912.3189697265625, Entropy 353.0986633300781, Learning Rate: 0.00125\n",
      "Epoch [6749/20000], Loss: 881.7634887695312, Entropy 357.91302490234375, Learning Rate: 0.00125\n",
      "Epoch [6750/20000], Loss: 928.8536376953125, Entropy 354.7239074707031, Learning Rate: 0.00125\n",
      "Epoch [6751/20000], Loss: 898.416015625, Entropy 353.05810546875, Learning Rate: 0.00125\n",
      "Epoch [6752/20000], Loss: 922.2307739257812, Entropy 355.80023193359375, Learning Rate: 0.00125\n",
      "Epoch [6753/20000], Loss: 924.89990234375, Entropy 350.4736633300781, Learning Rate: 0.00125\n",
      "Epoch [6754/20000], Loss: 920.705078125, Entropy 358.9007263183594, Learning Rate: 0.00125\n",
      "Epoch [6755/20000], Loss: 940.6915283203125, Entropy 343.12255859375, Learning Rate: 0.00125\n",
      "Epoch [6756/20000], Loss: 945.9718627929688, Entropy 353.65850830078125, Learning Rate: 0.00125\n",
      "Epoch [6757/20000], Loss: 932.3236083984375, Entropy 354.863037109375, Learning Rate: 0.00125\n",
      "Epoch [6758/20000], Loss: 926.003173828125, Entropy 351.6688232421875, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6759/20000], Loss: 953.0262451171875, Entropy 344.4778747558594, Learning Rate: 0.00125\n",
      "Epoch [6760/20000], Loss: 927.6444702148438, Entropy 342.08294677734375, Learning Rate: 0.00125\n",
      "Epoch [6761/20000], Loss: 918.6634521484375, Entropy 357.536865234375, Learning Rate: 0.00125\n",
      "Epoch [6762/20000], Loss: 924.7066650390625, Entropy 358.3488464355469, Learning Rate: 0.00125\n",
      "Epoch [6763/20000], Loss: 899.8140258789062, Entropy 351.14544677734375, Learning Rate: 0.00125\n",
      "Epoch [6764/20000], Loss: 968.4814453125, Entropy 342.2060852050781, Learning Rate: 0.00125\n",
      "Epoch [6765/20000], Loss: 864.16943359375, Entropy 359.5235900878906, Learning Rate: 0.00125\n",
      "Epoch [6766/20000], Loss: 884.1018676757812, Entropy 368.57513427734375, Learning Rate: 0.00125\n",
      "Epoch [6767/20000], Loss: 943.4486083984375, Entropy 337.8388977050781, Learning Rate: 0.00125\n",
      "Epoch [6768/20000], Loss: 866.5792236328125, Entropy 365.3962097167969, Learning Rate: 0.00125\n",
      "Epoch [6769/20000], Loss: 920.4280395507812, Entropy 356.38275146484375, Learning Rate: 0.00125\n",
      "Epoch [6770/20000], Loss: 887.2859497070312, Entropy 357.08197021484375, Learning Rate: 0.00125\n",
      "Epoch [6771/20000], Loss: 935.1436157226562, Entropy 358.35443115234375, Learning Rate: 0.00125\n",
      "Epoch [6772/20000], Loss: 882.77392578125, Entropy 357.5626525878906, Learning Rate: 0.00125\n",
      "Epoch [6773/20000], Loss: 933.47998046875, Entropy 346.0027770996094, Learning Rate: 0.00125\n",
      "Epoch [6774/20000], Loss: 960.2775268554688, Entropy 363.81365966796875, Learning Rate: 0.00125\n",
      "Epoch [6775/20000], Loss: 920.0888671875, Entropy 343.7962341308594, Learning Rate: 0.00125\n",
      "Epoch [6776/20000], Loss: 929.752197265625, Entropy 362.3707580566406, Learning Rate: 0.00125\n",
      "Epoch [6777/20000], Loss: 890.49853515625, Entropy 354.4329528808594, Learning Rate: 0.00125\n",
      "Epoch [6778/20000], Loss: 936.1990356445312, Entropy 351.93133544921875, Learning Rate: 0.00125\n",
      "Epoch [6779/20000], Loss: 905.7825927734375, Entropy 342.2242126464844, Learning Rate: 0.00125\n",
      "Epoch [6780/20000], Loss: 923.691650390625, Entropy 344.56396484375, Learning Rate: 0.00125\n",
      "Epoch [6781/20000], Loss: 939.229248046875, Entropy 347.8482666015625, Learning Rate: 0.00125\n",
      "Epoch [6782/20000], Loss: 887.6050415039062, Entropy 351.96160888671875, Learning Rate: 0.00125\n",
      "Epoch [6783/20000], Loss: 968.4202880859375, Entropy 365.6806945800781, Learning Rate: 0.00125\n",
      "Epoch [6784/20000], Loss: 942.7785034179688, Entropy 345.45989990234375, Learning Rate: 0.00125\n",
      "Epoch [6785/20000], Loss: 940.7337036132812, Entropy 353.27349853515625, Learning Rate: 0.00125\n",
      "Epoch [6786/20000], Loss: 927.26220703125, Entropy 346.7681579589844, Learning Rate: 0.00125\n",
      "Epoch [6787/20000], Loss: 900.394287109375, Entropy 356.3423767089844, Learning Rate: 0.00125\n",
      "Epoch [6788/20000], Loss: 912.4517822265625, Entropy 355.3905334472656, Learning Rate: 0.00125\n",
      "Epoch [6789/20000], Loss: 985.9612426757812, Entropy 353.10968017578125, Learning Rate: 0.00125\n",
      "Epoch [6790/20000], Loss: 922.942626953125, Entropy 364.4092102050781, Learning Rate: 0.00125\n",
      "Epoch [6791/20000], Loss: 869.33544921875, Entropy 355.392333984375, Learning Rate: 0.00125\n",
      "Epoch [6792/20000], Loss: 933.9520263671875, Entropy 350.6871032714844, Learning Rate: 0.00125\n",
      "Epoch [6793/20000], Loss: 939.45654296875, Entropy 354.5177307128906, Learning Rate: 0.00125\n",
      "Epoch [6794/20000], Loss: 923.3994750976562, Entropy 349.35015869140625, Learning Rate: 0.00125\n",
      "Epoch [6795/20000], Loss: 937.7963256835938, Entropy 349.55609130859375, Learning Rate: 0.00125\n",
      "Epoch [6796/20000], Loss: 924.6764526367188, Entropy 349.59576416015625, Learning Rate: 0.00125\n",
      "Epoch [6797/20000], Loss: 877.6514892578125, Entropy 354.0461730957031, Learning Rate: 0.00125\n",
      "Epoch [6798/20000], Loss: 941.691162109375, Entropy 358.5216979980469, Learning Rate: 0.00125\n",
      "Epoch [6799/20000], Loss: 940.4406127929688, Entropy 345.99468994140625, Learning Rate: 0.00125\n",
      "Epoch [6800/20000], Loss: 925.30712890625, Entropy 356.8174133300781, Learning Rate: 0.00125\n",
      "Epoch [6801/20000], Loss: 877.337646484375, Entropy 352.7853698730469, Learning Rate: 0.00125\n",
      "Epoch [6802/20000], Loss: 904.569580078125, Entropy 364.0001525878906, Learning Rate: 0.00125\n",
      "Epoch [6803/20000], Loss: 931.1026000976562, Entropy 349.66204833984375, Learning Rate: 0.00125\n",
      "Epoch [6804/20000], Loss: 896.4419555664062, Entropy 347.98077392578125, Learning Rate: 0.00125\n",
      "Epoch [6805/20000], Loss: 918.4661865234375, Entropy 336.7073059082031, Learning Rate: 0.00125\n",
      "Epoch [6806/20000], Loss: 881.9639892578125, Entropy 351.6392517089844, Learning Rate: 0.00125\n",
      "Epoch [6807/20000], Loss: 927.6121215820312, Entropy 367.36920166015625, Learning Rate: 0.00125\n",
      "Epoch [6808/20000], Loss: 899.5078125, Entropy 351.58984375, Learning Rate: 0.00125\n",
      "Epoch [6809/20000], Loss: 950.7657470703125, Entropy 347.2727966308594, Learning Rate: 0.00125\n",
      "Epoch [6810/20000], Loss: 891.8944702148438, Entropy 359.42083740234375, Learning Rate: 0.00125\n",
      "Epoch [6811/20000], Loss: 881.8726196289062, Entropy 365.36614990234375, Learning Rate: 0.00125\n",
      "Epoch [6812/20000], Loss: 918.216064453125, Entropy 343.8671875, Learning Rate: 0.00125\n",
      "Epoch [6813/20000], Loss: 877.4209594726562, Entropy 359.39178466796875, Learning Rate: 0.00125\n",
      "Epoch [6814/20000], Loss: 929.5413208007812, Entropy 332.91375732421875, Learning Rate: 0.00125\n",
      "Epoch [6815/20000], Loss: 924.3450927734375, Entropy 358.5453186035156, Learning Rate: 0.00125\n",
      "Epoch [6816/20000], Loss: 915.2899169921875, Entropy 346.5389099121094, Learning Rate: 0.00125\n",
      "Epoch [6817/20000], Loss: 904.231689453125, Entropy 355.1037292480469, Learning Rate: 0.00125\n",
      "Epoch [6818/20000], Loss: 890.8477172851562, Entropy 373.15032958984375, Learning Rate: 0.00125\n",
      "Epoch [6819/20000], Loss: 902.53173828125, Entropy 353.2702331542969, Learning Rate: 0.00125\n",
      "Epoch [6820/20000], Loss: 915.7049560546875, Entropy 352.2250061035156, Learning Rate: 0.00125\n",
      "Epoch [6821/20000], Loss: 883.9193115234375, Entropy 344.3289794921875, Learning Rate: 0.00125\n",
      "Epoch [6822/20000], Loss: 899.25146484375, Entropy 373.417724609375, Learning Rate: 0.00125\n",
      "Epoch [6823/20000], Loss: 970.7417602539062, Entropy 341.51385498046875, Learning Rate: 0.00125\n",
      "Epoch [6824/20000], Loss: 909.1700439453125, Entropy 363.9635925292969, Learning Rate: 0.00125\n",
      "Epoch [6825/20000], Loss: 922.1976928710938, Entropy 352.48895263671875, Learning Rate: 0.00125\n",
      "Epoch [6826/20000], Loss: 876.00146484375, Entropy 366.8559265136719, Learning Rate: 0.00125\n",
      "Epoch [6827/20000], Loss: 900.6888427734375, Entropy 350.9675598144531, Learning Rate: 0.00125\n",
      "Epoch [6828/20000], Loss: 907.734619140625, Entropy 364.5105285644531, Learning Rate: 0.00125\n",
      "Epoch [6829/20000], Loss: 943.4073486328125, Entropy 364.6477966308594, Learning Rate: 0.00125\n",
      "Epoch [6830/20000], Loss: 946.4534912109375, Entropy 335.6017150878906, Learning Rate: 0.00125\n",
      "Epoch [6831/20000], Loss: 881.341796875, Entropy 352.6458435058594, Learning Rate: 0.00125\n",
      "Epoch [6832/20000], Loss: 914.7240600585938, Entropy 346.44891357421875, Learning Rate: 0.00125\n",
      "Epoch [6833/20000], Loss: 860.42822265625, Entropy 362.0509948730469, Learning Rate: 0.00125\n",
      "Epoch [6834/20000], Loss: 892.6918334960938, Entropy 362.57855224609375, Learning Rate: 0.00125\n",
      "Epoch [6835/20000], Loss: 873.3524780273438, Entropy 365.71734619140625, Learning Rate: 0.00125\n",
      "Epoch [6836/20000], Loss: 894.6976318359375, Entropy 356.9215393066406, Learning Rate: 0.00125\n",
      "Epoch [6837/20000], Loss: 871.5911254882812, Entropy 363.37591552734375, Learning Rate: 0.00125\n",
      "Epoch [6838/20000], Loss: 861.6651611328125, Entropy 364.4785461425781, Learning Rate: 0.00125\n",
      "Epoch [6839/20000], Loss: 917.8846435546875, Entropy 361.1177673339844, Learning Rate: 0.00125\n",
      "Epoch [6840/20000], Loss: 898.2794799804688, Entropy 357.01312255859375, Learning Rate: 0.00125\n",
      "Epoch [6841/20000], Loss: 877.6204833984375, Entropy 364.8785705566406, Learning Rate: 0.00125\n",
      "Epoch [6842/20000], Loss: 926.2847900390625, Entropy 354.2512512207031, Learning Rate: 0.00125\n",
      "Epoch [6843/20000], Loss: 875.728759765625, Entropy 372.3362731933594, Learning Rate: 0.00125\n",
      "Epoch [6844/20000], Loss: 899.075927734375, Entropy 366.7370300292969, Learning Rate: 0.00125\n",
      "Epoch [6845/20000], Loss: 895.4515380859375, Entropy 363.1542053222656, Learning Rate: 0.00125\n",
      "Epoch [6846/20000], Loss: 858.3927001953125, Entropy 375.4488830566406, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6847/20000], Loss: 931.00927734375, Entropy 378.1603698730469, Learning Rate: 0.00125\n",
      "Epoch [6848/20000], Loss: 986.1580200195312, Entropy 370.60479736328125, Learning Rate: 0.00125\n",
      "Epoch [6849/20000], Loss: 872.278076171875, Entropy 371.346435546875, Learning Rate: 0.00125\n",
      "Epoch [6850/20000], Loss: 903.7940063476562, Entropy 363.47406005859375, Learning Rate: 0.00125\n",
      "Epoch [6851/20000], Loss: 878.2281494140625, Entropy 364.5553894042969, Learning Rate: 0.00125\n",
      "Epoch [6852/20000], Loss: 892.757568359375, Entropy 366.5751953125, Learning Rate: 0.00125\n",
      "Epoch [6853/20000], Loss: 940.4971923828125, Entropy 358.1928405761719, Learning Rate: 0.00125\n",
      "Epoch [6854/20000], Loss: 883.3915405273438, Entropy 350.20635986328125, Learning Rate: 0.00125\n",
      "Epoch [6855/20000], Loss: 916.1378173828125, Entropy 357.4047546386719, Learning Rate: 0.00125\n",
      "Epoch [6856/20000], Loss: 895.2993774414062, Entropy 353.31365966796875, Learning Rate: 0.00125\n",
      "Epoch [6857/20000], Loss: 937.0391235351562, Entropy 363.68194580078125, Learning Rate: 0.00125\n",
      "Epoch [6858/20000], Loss: 948.212890625, Entropy 352.9474792480469, Learning Rate: 0.00125\n",
      "Epoch [6859/20000], Loss: 899.820068359375, Entropy 362.5895080566406, Learning Rate: 0.00125\n",
      "Epoch [6860/20000], Loss: 916.7962646484375, Entropy 348.6435852050781, Learning Rate: 0.00125\n",
      "Epoch [6861/20000], Loss: 875.1595458984375, Entropy 357.3640441894531, Learning Rate: 0.00125\n",
      "Epoch [6862/20000], Loss: 959.1728515625, Entropy 352.29931640625, Learning Rate: 0.00125\n",
      "Epoch [6863/20000], Loss: 876.314697265625, Entropy 352.2742614746094, Learning Rate: 0.00125\n",
      "Epoch [6864/20000], Loss: 924.8201904296875, Entropy 355.4075012207031, Learning Rate: 0.00125\n",
      "Epoch [6865/20000], Loss: 895.5457763671875, Entropy 358.5870361328125, Learning Rate: 0.00125\n",
      "Epoch [6866/20000], Loss: 890.1725463867188, Entropy 355.81768798828125, Learning Rate: 0.00125\n",
      "Epoch [6867/20000], Loss: 929.0208129882812, Entropy 358.15570068359375, Learning Rate: 0.00125\n",
      "Epoch [6868/20000], Loss: 880.045166015625, Entropy 363.759521484375, Learning Rate: 0.00125\n",
      "Epoch [6869/20000], Loss: 908.2342529296875, Entropy 371.0125732421875, Learning Rate: 0.00125\n",
      "Epoch [6870/20000], Loss: 985.7374267578125, Entropy 331.8268737792969, Learning Rate: 0.00125\n",
      "Epoch [6871/20000], Loss: 1007.93896484375, Entropy 367.4879150390625, Learning Rate: 0.00125\n",
      "Epoch [6872/20000], Loss: 986.5552978515625, Entropy 348.2771911621094, Learning Rate: 0.00125\n",
      "Epoch [6873/20000], Loss: 926.0548706054688, Entropy 360.95953369140625, Learning Rate: 0.00125\n",
      "Epoch [6874/20000], Loss: 942.5670166015625, Entropy 363.4305419921875, Learning Rate: 0.00125\n",
      "Epoch [6875/20000], Loss: 924.75927734375, Entropy 357.2447509765625, Learning Rate: 0.00125\n",
      "Epoch [6876/20000], Loss: 914.4884643554688, Entropy 358.90618896484375, Learning Rate: 0.00125\n",
      "Epoch [6877/20000], Loss: 909.1685791015625, Entropy 386.9761657714844, Learning Rate: 0.00125\n",
      "Epoch [6878/20000], Loss: 930.69189453125, Entropy 363.84765625, Learning Rate: 0.00125\n",
      "Epoch [6879/20000], Loss: 898.873046875, Entropy 372.1617736816406, Learning Rate: 0.00125\n",
      "Epoch [6880/20000], Loss: 907.5747680664062, Entropy 365.85296630859375, Learning Rate: 0.00125\n",
      "Epoch [6881/20000], Loss: 981.8765869140625, Entropy 357.3111877441406, Learning Rate: 0.00125\n",
      "Epoch [6882/20000], Loss: 920.2797241210938, Entropy 343.57147216796875, Learning Rate: 0.00125\n",
      "Epoch [6883/20000], Loss: 883.0675048828125, Entropy 368.8404541015625, Learning Rate: 0.00125\n",
      "Epoch [6884/20000], Loss: 905.419921875, Entropy 363.6932067871094, Learning Rate: 0.00125\n",
      "Epoch [6885/20000], Loss: 959.6817626953125, Entropy 360.2066650390625, Learning Rate: 0.00125\n",
      "Epoch [6886/20000], Loss: 872.0203857421875, Entropy 371.07861328125, Learning Rate: 0.00125\n",
      "Epoch [6887/20000], Loss: 918.0140380859375, Entropy 359.8606262207031, Learning Rate: 0.00125\n",
      "Epoch [6888/20000], Loss: 927.5919799804688, Entropy 370.03533935546875, Learning Rate: 0.00125\n",
      "Epoch [6889/20000], Loss: 898.4217529296875, Entropy 369.8168640136719, Learning Rate: 0.00125\n",
      "Epoch [6890/20000], Loss: 953.0776977539062, Entropy 365.64691162109375, Learning Rate: 0.00125\n",
      "Epoch [6891/20000], Loss: 885.4283447265625, Entropy 365.9709167480469, Learning Rate: 0.00125\n",
      "Epoch [6892/20000], Loss: 885.47314453125, Entropy 363.861083984375, Learning Rate: 0.00125\n",
      "Epoch [6893/20000], Loss: 864.8173828125, Entropy 362.0448303222656, Learning Rate: 0.00125\n",
      "Epoch [6894/20000], Loss: 948.9891357421875, Entropy 357.92333984375, Learning Rate: 0.00125\n",
      "Epoch [6895/20000], Loss: 934.6043701171875, Entropy 362.0140686035156, Learning Rate: 0.00125\n",
      "Epoch [6896/20000], Loss: 949.2923583984375, Entropy 370.4198303222656, Learning Rate: 0.00125\n",
      "Epoch [6897/20000], Loss: 883.9071044921875, Entropy 368.98583984375, Learning Rate: 0.00125\n",
      "Epoch [6898/20000], Loss: 900.1331176757812, Entropy 366.97076416015625, Learning Rate: 0.00125\n",
      "Epoch [6899/20000], Loss: 954.160400390625, Entropy 358.7236328125, Learning Rate: 0.00125\n",
      "Epoch [6900/20000], Loss: 899.0335693359375, Entropy 361.5428466796875, Learning Rate: 0.00125\n",
      "Epoch [6901/20000], Loss: 887.3363037109375, Entropy 365.0779724121094, Learning Rate: 0.00125\n",
      "Epoch [6902/20000], Loss: 887.646484375, Entropy 353.2315673828125, Learning Rate: 0.00125\n",
      "Epoch [6903/20000], Loss: 935.416259765625, Entropy 352.5538330078125, Learning Rate: 0.00125\n",
      "Epoch [6904/20000], Loss: 919.3829345703125, Entropy 362.0540466308594, Learning Rate: 0.00125\n",
      "Epoch [6905/20000], Loss: 886.4379272460938, Entropy 344.80169677734375, Learning Rate: 0.00125\n",
      "Epoch [6906/20000], Loss: 907.4024658203125, Entropy 377.3569030761719, Learning Rate: 0.00125\n",
      "Epoch [6907/20000], Loss: 906.4548950195312, Entropy 369.84075927734375, Learning Rate: 0.00125\n",
      "Epoch [6908/20000], Loss: 899.7172241210938, Entropy 364.02679443359375, Learning Rate: 0.00125\n",
      "Epoch [6909/20000], Loss: 902.0433349609375, Entropy 368.6268005371094, Learning Rate: 0.00125\n",
      "Epoch [6910/20000], Loss: 868.181884765625, Entropy 365.0342102050781, Learning Rate: 0.00125\n",
      "Epoch [6911/20000], Loss: 922.70703125, Entropy 373.23681640625, Learning Rate: 0.00125\n",
      "Epoch [6912/20000], Loss: 898.8604736328125, Entropy 378.1551513671875, Learning Rate: 0.00125\n",
      "Epoch [6913/20000], Loss: 923.4087524414062, Entropy 358.16998291015625, Learning Rate: 0.00125\n",
      "Epoch [6914/20000], Loss: 925.575927734375, Entropy 375.7293701171875, Learning Rate: 0.00125\n",
      "Epoch [6915/20000], Loss: 896.7242431640625, Entropy 361.0615234375, Learning Rate: 0.00125\n",
      "Epoch [6916/20000], Loss: 864.3258056640625, Entropy 370.7613830566406, Learning Rate: 0.00125\n",
      "Epoch [6917/20000], Loss: 901.6220703125, Entropy 362.1477966308594, Learning Rate: 0.00125\n",
      "Epoch [6918/20000], Loss: 888.4769287109375, Entropy 352.8753356933594, Learning Rate: 0.00125\n",
      "Epoch [6919/20000], Loss: 924.3856201171875, Entropy 358.9097595214844, Learning Rate: 0.00125\n",
      "Epoch [6920/20000], Loss: 939.8480224609375, Entropy 353.6619873046875, Learning Rate: 0.00125\n",
      "Epoch [6921/20000], Loss: 867.760498046875, Entropy 375.27978515625, Learning Rate: 0.00125\n",
      "Epoch [6922/20000], Loss: 879.2249755859375, Entropy 364.6444091796875, Learning Rate: 0.00125\n",
      "Epoch [6923/20000], Loss: 897.882080078125, Entropy 361.7543029785156, Learning Rate: 0.00125\n",
      "Epoch [6924/20000], Loss: 872.0552978515625, Entropy 382.0456237792969, Learning Rate: 0.00125\n",
      "Epoch [6925/20000], Loss: 882.2678833007812, Entropy 371.76312255859375, Learning Rate: 0.00125\n",
      "Epoch [6926/20000], Loss: 889.5980224609375, Entropy 365.7062072753906, Learning Rate: 0.00125\n",
      "Epoch [6927/20000], Loss: 908.7181396484375, Entropy 359.363037109375, Learning Rate: 0.00125\n",
      "Epoch [6928/20000], Loss: 893.84814453125, Entropy 370.8358459472656, Learning Rate: 0.00125\n",
      "Epoch [6929/20000], Loss: 856.7574462890625, Entropy 377.3916320800781, Learning Rate: 0.00125\n",
      "Epoch [6930/20000], Loss: 888.1598510742188, Entropy 345.28240966796875, Learning Rate: 0.00125\n",
      "Epoch [6931/20000], Loss: 898.8963623046875, Entropy 355.443115234375, Learning Rate: 0.00125\n",
      "Epoch [6932/20000], Loss: 988.5306396484375, Entropy 355.3268127441406, Learning Rate: 0.00125\n",
      "Epoch [6933/20000], Loss: 988.493408203125, Entropy 357.9200744628906, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6934/20000], Loss: 878.357177734375, Entropy 347.0135803222656, Learning Rate: 0.00125\n",
      "Epoch [6935/20000], Loss: 931.0711669921875, Entropy 362.1667175292969, Learning Rate: 0.00125\n",
      "Epoch [6936/20000], Loss: 914.3161010742188, Entropy 377.14849853515625, Learning Rate: 0.00125\n",
      "Epoch [6937/20000], Loss: 941.309326171875, Entropy 376.7427978515625, Learning Rate: 0.00125\n",
      "Epoch [6938/20000], Loss: 902.5216674804688, Entropy 361.66595458984375, Learning Rate: 0.00125\n",
      "Epoch [6939/20000], Loss: 894.0828247070312, Entropy 379.79486083984375, Learning Rate: 0.00125\n",
      "Epoch [6940/20000], Loss: 939.89111328125, Entropy 378.9166564941406, Learning Rate: 0.00125\n",
      "Epoch [6941/20000], Loss: 938.2022705078125, Entropy 348.4992370605469, Learning Rate: 0.00125\n",
      "Epoch [6942/20000], Loss: 881.0917358398438, Entropy 358.84210205078125, Learning Rate: 0.00125\n",
      "Epoch [6943/20000], Loss: 892.014892578125, Entropy 367.8759765625, Learning Rate: 0.00125\n",
      "Epoch [6944/20000], Loss: 932.5537719726562, Entropy 370.38897705078125, Learning Rate: 0.00125\n",
      "Epoch [6945/20000], Loss: 875.2015991210938, Entropy 380.62945556640625, Learning Rate: 0.00125\n",
      "Epoch [6946/20000], Loss: 846.6380615234375, Entropy 378.2783508300781, Learning Rate: 0.00125\n",
      "Epoch [6947/20000], Loss: 902.750732421875, Entropy 360.1317138671875, Learning Rate: 0.00125\n",
      "Epoch [6948/20000], Loss: 886.4678955078125, Entropy 374.6820373535156, Learning Rate: 0.00125\n",
      "Epoch [6949/20000], Loss: 874.2915649414062, Entropy 373.36077880859375, Learning Rate: 0.00125\n",
      "Epoch [6950/20000], Loss: 973.311767578125, Entropy 358.3207092285156, Learning Rate: 0.00125\n",
      "Epoch [6951/20000], Loss: 931.8295288085938, Entropy 363.15960693359375, Learning Rate: 0.00125\n",
      "Epoch [6952/20000], Loss: 897.3985595703125, Entropy 369.1854553222656, Learning Rate: 0.00125\n",
      "Epoch [6953/20000], Loss: 917.560791015625, Entropy 375.5688171386719, Learning Rate: 0.00125\n",
      "Epoch [6954/20000], Loss: 896.8453979492188, Entropy 368.49420166015625, Learning Rate: 0.00125\n",
      "Epoch [6955/20000], Loss: 870.4852294921875, Entropy 354.67578125, Learning Rate: 0.00125\n",
      "Epoch [6956/20000], Loss: 910.58642578125, Entropy 374.3857421875, Learning Rate: 0.00125\n",
      "Epoch [6957/20000], Loss: 898.8162841796875, Entropy 371.4718933105469, Learning Rate: 0.00125\n",
      "Epoch [6958/20000], Loss: 956.4739990234375, Entropy 358.0440673828125, Learning Rate: 0.00125\n",
      "Epoch [6959/20000], Loss: 876.9030151367188, Entropy 365.17034912109375, Learning Rate: 0.00125\n",
      "Epoch [6960/20000], Loss: 894.0828857421875, Entropy 363.3371887207031, Learning Rate: 0.00125\n",
      "Epoch [6961/20000], Loss: 932.45263671875, Entropy 365.6151428222656, Learning Rate: 0.00125\n",
      "Epoch [6962/20000], Loss: 897.4478149414062, Entropy 369.85455322265625, Learning Rate: 0.00125\n",
      "Epoch [6963/20000], Loss: 865.3114013671875, Entropy 378.8153076171875, Learning Rate: 0.00125\n",
      "Epoch [6964/20000], Loss: 935.2954711914062, Entropy 385.10772705078125, Learning Rate: 0.00125\n",
      "Epoch [6965/20000], Loss: 940.2830200195312, Entropy 348.65679931640625, Learning Rate: 0.00125\n",
      "Epoch [6966/20000], Loss: 874.96923828125, Entropy 386.7291564941406, Learning Rate: 0.00125\n",
      "Epoch [6967/20000], Loss: 915.864501953125, Entropy 367.4556884765625, Learning Rate: 0.00125\n",
      "Epoch [6968/20000], Loss: 975.844482421875, Entropy 362.8254699707031, Learning Rate: 0.00125\n",
      "Epoch [6969/20000], Loss: 910.7010498046875, Entropy 375.7699890136719, Learning Rate: 0.00125\n",
      "Epoch [6970/20000], Loss: 892.5123291015625, Entropy 368.1734924316406, Learning Rate: 0.00125\n",
      "Epoch [6971/20000], Loss: 952.866943359375, Entropy 370.2395935058594, Learning Rate: 0.00125\n",
      "Epoch [6972/20000], Loss: 916.45263671875, Entropy 363.85107421875, Learning Rate: 0.00125\n",
      "Epoch [6973/20000], Loss: 913.11572265625, Entropy 362.8723449707031, Learning Rate: 0.00125\n",
      "Epoch [6974/20000], Loss: 876.0061645507812, Entropy 386.55364990234375, Learning Rate: 0.00125\n",
      "Epoch [6975/20000], Loss: 860.05810546875, Entropy 385.9851379394531, Learning Rate: 0.00125\n",
      "Epoch [6976/20000], Loss: 858.3450927734375, Entropy 379.5880432128906, Learning Rate: 0.00125\n",
      "Epoch [6977/20000], Loss: 893.707763671875, Entropy 375.5079650878906, Learning Rate: 0.00125\n",
      "Epoch [6978/20000], Loss: 1006.5885009765625, Entropy 365.8941345214844, Learning Rate: 0.00125\n",
      "Epoch [6979/20000], Loss: 960.63916015625, Entropy 372.5509948730469, Learning Rate: 0.00125\n",
      "Epoch [6980/20000], Loss: 874.7688598632812, Entropy 368.66436767578125, Learning Rate: 0.00125\n",
      "Epoch [6981/20000], Loss: 894.4514770507812, Entropy 366.01031494140625, Learning Rate: 0.00125\n",
      "Epoch [6982/20000], Loss: 874.9069213867188, Entropy 366.13140869140625, Learning Rate: 0.00125\n",
      "Epoch [6983/20000], Loss: 921.0333251953125, Entropy 367.5899963378906, Learning Rate: 0.00125\n",
      "Epoch [6984/20000], Loss: 937.3678588867188, Entropy 371.39398193359375, Learning Rate: 0.00125\n",
      "Epoch [6985/20000], Loss: 887.7015380859375, Entropy 371.1750793457031, Learning Rate: 0.00125\n",
      "Epoch [6986/20000], Loss: 865.1763916015625, Entropy 367.1780700683594, Learning Rate: 0.00125\n",
      "Epoch [6987/20000], Loss: 933.1549072265625, Entropy 363.55029296875, Learning Rate: 0.00125\n",
      "Epoch [6988/20000], Loss: 900.56982421875, Entropy 369.4747619628906, Learning Rate: 0.00125\n",
      "Epoch [6989/20000], Loss: 920.6113891601562, Entropy 348.19720458984375, Learning Rate: 0.00125\n",
      "Epoch [6990/20000], Loss: 911.3107299804688, Entropy 362.68475341796875, Learning Rate: 0.00125\n",
      "Epoch [6991/20000], Loss: 951.4794921875, Entropy 370.5617980957031, Learning Rate: 0.00125\n",
      "Epoch [6992/20000], Loss: 903.746337890625, Entropy 374.595458984375, Learning Rate: 0.00125\n",
      "Epoch [6993/20000], Loss: 914.5289916992188, Entropy 358.64459228515625, Learning Rate: 0.00125\n",
      "Epoch [6994/20000], Loss: 872.723876953125, Entropy 374.1034240722656, Learning Rate: 0.00125\n",
      "Epoch [6995/20000], Loss: 880.2926635742188, Entropy 359.00482177734375, Learning Rate: 0.00125\n",
      "Epoch [6996/20000], Loss: 932.1820068359375, Entropy 369.4080505371094, Learning Rate: 0.00125\n",
      "Epoch [6997/20000], Loss: 904.3448486328125, Entropy 357.6119384765625, Learning Rate: 0.00125\n",
      "Epoch [6998/20000], Loss: 876.7252807617188, Entropy 376.56866455078125, Learning Rate: 0.00125\n",
      "Epoch [6999/20000], Loss: 928.50146484375, Entropy 367.4881286621094, Learning Rate: 0.00125\n",
      "Epoch [7000/20000], Loss: 891.41064453125, Entropy 366.5963134765625, Learning Rate: 0.00125\n",
      "Epoch [7001/20000], Loss: 882.1272583007812, Entropy 366.81768798828125, Learning Rate: 0.00125\n",
      "Epoch [7002/20000], Loss: 887.0098266601562, Entropy 377.30426025390625, Learning Rate: 0.00125\n",
      "Epoch [7003/20000], Loss: 920.8049926757812, Entropy 374.81195068359375, Learning Rate: 0.00125\n",
      "Epoch [7004/20000], Loss: 875.6755981445312, Entropy 369.19427490234375, Learning Rate: 0.00125\n",
      "Epoch [7005/20000], Loss: 916.6873168945312, Entropy 386.54046630859375, Learning Rate: 0.00125\n",
      "Epoch [7006/20000], Loss: 900.992431640625, Entropy 366.7606506347656, Learning Rate: 0.00125\n",
      "Epoch [7007/20000], Loss: 867.7608642578125, Entropy 374.7035217285156, Learning Rate: 0.00125\n",
      "Epoch [7008/20000], Loss: 882.1845092773438, Entropy 383.62713623046875, Learning Rate: 0.00125\n",
      "Epoch [7009/20000], Loss: 868.6028442382812, Entropy 372.83050537109375, Learning Rate: 0.00125\n",
      "Epoch [7010/20000], Loss: 924.927490234375, Entropy 365.2365417480469, Learning Rate: 0.00125\n",
      "Epoch [7011/20000], Loss: 860.8182983398438, Entropy 370.13397216796875, Learning Rate: 0.00125\n",
      "Epoch [7012/20000], Loss: 924.6138916015625, Entropy 367.2554626464844, Learning Rate: 0.00125\n",
      "Epoch [7013/20000], Loss: 872.5964965820312, Entropy 388.41546630859375, Learning Rate: 0.00125\n",
      "Epoch [7014/20000], Loss: 844.59619140625, Entropy 390.0481262207031, Learning Rate: 0.00125\n",
      "Epoch [7015/20000], Loss: 886.2406616210938, Entropy 386.91693115234375, Learning Rate: 0.00125\n",
      "Epoch [7016/20000], Loss: 966.6531982421875, Entropy 368.6629943847656, Learning Rate: 0.00125\n",
      "Epoch [7017/20000], Loss: 902.5099487304688, Entropy 371.79693603515625, Learning Rate: 0.00125\n",
      "Epoch [7018/20000], Loss: 882.3441162109375, Entropy 375.3396911621094, Learning Rate: 0.00125\n",
      "Epoch [7019/20000], Loss: 902.5354614257812, Entropy 369.57916259765625, Learning Rate: 0.00125\n",
      "Epoch [7020/20000], Loss: 931.6043090820312, Entropy 365.80023193359375, Learning Rate: 0.00125\n",
      "Epoch [7021/20000], Loss: 861.16259765625, Entropy 406.2838134765625, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7022/20000], Loss: 909.9873657226562, Entropy 363.96917724609375, Learning Rate: 0.00125\n",
      "Epoch [7023/20000], Loss: 889.2351684570312, Entropy 386.50701904296875, Learning Rate: 0.00125\n",
      "Epoch [7024/20000], Loss: 911.2310791015625, Entropy 373.1124572753906, Learning Rate: 0.00125\n",
      "Epoch [7025/20000], Loss: 871.5047607421875, Entropy 372.7682189941406, Learning Rate: 0.00125\n",
      "Epoch [7026/20000], Loss: 900.9258422851562, Entropy 378.99749755859375, Learning Rate: 0.00125\n",
      "Epoch [7027/20000], Loss: 905.0650634765625, Entropy 363.4698791503906, Learning Rate: 0.00125\n",
      "Epoch [7028/20000], Loss: 885.2242431640625, Entropy 392.1273498535156, Learning Rate: 0.00125\n",
      "Epoch [7029/20000], Loss: 935.0803833007812, Entropy 362.42315673828125, Learning Rate: 0.00125\n",
      "Epoch [7030/20000], Loss: 882.4413452148438, Entropy 385.30560302734375, Learning Rate: 0.00125\n",
      "Epoch [7031/20000], Loss: 894.283447265625, Entropy 367.242919921875, Learning Rate: 0.00125\n",
      "Epoch [7032/20000], Loss: 938.6038818359375, Entropy 383.0423889160156, Learning Rate: 0.00125\n",
      "Epoch [7033/20000], Loss: 918.0131225585938, Entropy 361.38226318359375, Learning Rate: 0.00125\n",
      "Epoch [7034/20000], Loss: 911.93603515625, Entropy 372.6228942871094, Learning Rate: 0.00125\n",
      "Epoch [7035/20000], Loss: 934.3160400390625, Entropy 372.9928283691406, Learning Rate: 0.00125\n",
      "Epoch [7036/20000], Loss: 918.5087280273438, Entropy 376.11346435546875, Learning Rate: 0.00125\n",
      "Epoch [7037/20000], Loss: 917.0681762695312, Entropy 380.81744384765625, Learning Rate: 0.00125\n",
      "Epoch [7038/20000], Loss: 881.8863525390625, Entropy 373.5813293457031, Learning Rate: 0.00125\n",
      "Epoch [7039/20000], Loss: 859.2867431640625, Entropy 396.2134704589844, Learning Rate: 0.00125\n",
      "Epoch [7040/20000], Loss: 906.0336303710938, Entropy 388.82769775390625, Learning Rate: 0.00125\n",
      "Epoch [7041/20000], Loss: 929.4491577148438, Entropy 357.50921630859375, Learning Rate: 0.00125\n",
      "Epoch [7042/20000], Loss: 867.6819458007812, Entropy 387.32672119140625, Learning Rate: 0.00125\n",
      "Epoch [7043/20000], Loss: 881.4168701171875, Entropy 363.7886962890625, Learning Rate: 0.00125\n",
      "Epoch [7044/20000], Loss: 965.2691650390625, Entropy 373.4870910644531, Learning Rate: 0.00125\n",
      "Epoch [7045/20000], Loss: 912.80078125, Entropy 370.938232421875, Learning Rate: 0.00125\n",
      "Epoch [7046/20000], Loss: 927.5411376953125, Entropy 366.2857666015625, Learning Rate: 0.00125\n",
      "Epoch [7047/20000], Loss: 896.6682739257812, Entropy 382.47808837890625, Learning Rate: 0.00125\n",
      "Epoch [7048/20000], Loss: 904.5545654296875, Entropy 372.5640869140625, Learning Rate: 0.00125\n",
      "Epoch [7049/20000], Loss: 888.9011840820312, Entropy 379.96295166015625, Learning Rate: 0.00125\n",
      "Epoch [7050/20000], Loss: 897.3062744140625, Entropy 377.7372131347656, Learning Rate: 0.00125\n",
      "Epoch [7051/20000], Loss: 892.1611328125, Entropy 367.7038879394531, Learning Rate: 0.00125\n",
      "Epoch [7052/20000], Loss: 886.833984375, Entropy 380.7158508300781, Learning Rate: 0.00125\n",
      "Epoch [7053/20000], Loss: 911.7457275390625, Entropy 371.7139892578125, Learning Rate: 0.00125\n",
      "Epoch [7054/20000], Loss: 943.1381225585938, Entropy 377.76654052734375, Learning Rate: 0.00125\n",
      "Epoch [7055/20000], Loss: 900.9285888671875, Entropy 370.68359375, Learning Rate: 0.00125\n",
      "Epoch [7056/20000], Loss: 950.1922607421875, Entropy 374.572265625, Learning Rate: 0.00125\n",
      "Epoch [7057/20000], Loss: 912.9542236328125, Entropy 361.1319580078125, Learning Rate: 0.00125\n",
      "Epoch [7058/20000], Loss: 909.41796875, Entropy 369.4109802246094, Learning Rate: 0.00125\n",
      "Epoch [7059/20000], Loss: 937.092529296875, Entropy 381.3049011230469, Learning Rate: 0.00125\n",
      "Epoch [7060/20000], Loss: 845.9886474609375, Entropy 369.6523742675781, Learning Rate: 0.00125\n",
      "Epoch [7061/20000], Loss: 912.467529296875, Entropy 371.7367248535156, Learning Rate: 0.00125\n",
      "Epoch [7062/20000], Loss: 953.0947265625, Entropy 372.2221374511719, Learning Rate: 0.00125\n",
      "Epoch [7063/20000], Loss: 867.3175048828125, Entropy 383.1935119628906, Learning Rate: 0.00125\n",
      "Epoch [7064/20000], Loss: 930.715087890625, Entropy 365.5111389160156, Learning Rate: 0.00125\n",
      "Epoch [7065/20000], Loss: 891.1036987304688, Entropy 386.23065185546875, Learning Rate: 0.00125\n",
      "Epoch [7066/20000], Loss: 878.1179809570312, Entropy 375.02679443359375, Learning Rate: 0.00125\n",
      "Epoch [7067/20000], Loss: 949.1441650390625, Entropy 380.0882873535156, Learning Rate: 0.00125\n",
      "Epoch [7068/20000], Loss: 870.257568359375, Entropy 368.1637268066406, Learning Rate: 0.00125\n",
      "Epoch [7069/20000], Loss: 910.2266845703125, Entropy 358.32568359375, Learning Rate: 0.00125\n",
      "Epoch [7070/20000], Loss: 873.1627197265625, Entropy 376.9631042480469, Learning Rate: 0.00125\n",
      "Epoch [7071/20000], Loss: 912.1346435546875, Entropy 370.8292236328125, Learning Rate: 0.00125\n",
      "Epoch [7072/20000], Loss: 914.59814453125, Entropy 372.9909973144531, Learning Rate: 0.00125\n",
      "Epoch [7073/20000], Loss: 917.76708984375, Entropy 373.2413330078125, Learning Rate: 0.00125\n",
      "Epoch [7074/20000], Loss: 945.6897583007812, Entropy 394.04913330078125, Learning Rate: 0.00125\n",
      "Epoch [7075/20000], Loss: 898.6373291015625, Entropy 374.612060546875, Learning Rate: 0.00125\n",
      "Epoch [7076/20000], Loss: 914.8175048828125, Entropy 379.9235534667969, Learning Rate: 0.00125\n",
      "Epoch [7077/20000], Loss: 908.2850341796875, Entropy 367.9940490722656, Learning Rate: 0.00125\n",
      "Epoch [7078/20000], Loss: 938.41162109375, Entropy 378.2492980957031, Learning Rate: 0.00125\n",
      "Epoch [7079/20000], Loss: 894.5888671875, Entropy 385.5138244628906, Learning Rate: 0.00125\n",
      "Epoch [7080/20000], Loss: 926.71240234375, Entropy 380.3650817871094, Learning Rate: 0.00125\n",
      "Epoch [7081/20000], Loss: 911.7833251953125, Entropy 362.6545104980469, Learning Rate: 0.00125\n",
      "Epoch [7082/20000], Loss: 898.5638427734375, Entropy 387.4527282714844, Learning Rate: 0.00125\n",
      "Epoch [7083/20000], Loss: 911.7857055664062, Entropy 369.83233642578125, Learning Rate: 0.00125\n",
      "Epoch [7084/20000], Loss: 911.89111328125, Entropy 372.6728820800781, Learning Rate: 0.00125\n",
      "Epoch [7085/20000], Loss: 881.7030029296875, Entropy 392.3455505371094, Learning Rate: 0.00125\n",
      "Epoch [7086/20000], Loss: 931.3864135742188, Entropy 376.66461181640625, Learning Rate: 0.00125\n",
      "Epoch [7087/20000], Loss: 909.8335571289062, Entropy 374.85638427734375, Learning Rate: 0.00125\n",
      "Epoch [7088/20000], Loss: 913.7413330078125, Entropy 374.6466369628906, Learning Rate: 0.00125\n",
      "Epoch [7089/20000], Loss: 871.9457397460938, Entropy 356.77178955078125, Learning Rate: 0.00125\n",
      "Epoch [7090/20000], Loss: 900.6428833007812, Entropy 374.09783935546875, Learning Rate: 0.00125\n",
      "Epoch [7091/20000], Loss: 887.1944580078125, Entropy 375.5739440917969, Learning Rate: 0.00125\n",
      "Epoch [7092/20000], Loss: 857.42626953125, Entropy 377.9537658691406, Learning Rate: 0.00125\n",
      "Epoch [7093/20000], Loss: 889.9417724609375, Entropy 379.6451416015625, Learning Rate: 0.00125\n",
      "Epoch [7094/20000], Loss: 918.4757080078125, Entropy 377.9693603515625, Learning Rate: 0.00125\n",
      "Epoch [7095/20000], Loss: 942.362060546875, Entropy 381.7957763671875, Learning Rate: 0.00125\n",
      "Epoch [7096/20000], Loss: 909.75390625, Entropy 372.4259948730469, Learning Rate: 0.00125\n",
      "Epoch [7097/20000], Loss: 950.4632568359375, Entropy 381.8051452636719, Learning Rate: 0.00125\n",
      "Epoch [7098/20000], Loss: 890.5678100585938, Entropy 388.68548583984375, Learning Rate: 0.00125\n",
      "Epoch [7099/20000], Loss: 908.5897216796875, Entropy 376.3309020996094, Learning Rate: 0.00125\n",
      "Epoch [7100/20000], Loss: 929.0897216796875, Entropy 368.73583984375, Learning Rate: 0.00125\n",
      "Epoch [7101/20000], Loss: 914.260009765625, Entropy 390.87451171875, Learning Rate: 0.00125\n",
      "Epoch [7102/20000], Loss: 870.3142700195312, Entropy 374.18475341796875, Learning Rate: 0.00125\n",
      "Epoch [7103/20000], Loss: 896.3228759765625, Entropy 377.6925048828125, Learning Rate: 0.00125\n",
      "Epoch [7104/20000], Loss: 896.1112060546875, Entropy 378.8182067871094, Learning Rate: 0.00125\n",
      "Epoch [7105/20000], Loss: 949.1082763671875, Entropy 383.9354553222656, Learning Rate: 0.00125\n",
      "Epoch [7106/20000], Loss: 900.8021850585938, Entropy 362.73822021484375, Learning Rate: 0.00125\n",
      "Epoch [7107/20000], Loss: 910.4271240234375, Entropy 376.8906555175781, Learning Rate: 0.00125\n",
      "Epoch [7108/20000], Loss: 894.9443359375, Entropy 379.5928039550781, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7109/20000], Loss: 892.6627197265625, Entropy 382.9559326171875, Learning Rate: 0.00125\n",
      "Epoch [7110/20000], Loss: 880.5355224609375, Entropy 387.2301025390625, Learning Rate: 0.00125\n",
      "Epoch [7111/20000], Loss: 879.5939331054688, Entropy 368.44036865234375, Learning Rate: 0.00125\n",
      "Epoch [7112/20000], Loss: 874.83154296875, Entropy 384.3839111328125, Learning Rate: 0.00125\n",
      "Epoch [7113/20000], Loss: 933.8209838867188, Entropy 374.87982177734375, Learning Rate: 0.00125\n",
      "Epoch [7114/20000], Loss: 896.6373291015625, Entropy 384.0558776855469, Learning Rate: 0.00125\n",
      "Epoch [7115/20000], Loss: 934.0446166992188, Entropy 373.15338134765625, Learning Rate: 0.00125\n",
      "Epoch [7116/20000], Loss: 862.8236083984375, Entropy 390.4051513671875, Learning Rate: 0.00125\n",
      "Epoch [7117/20000], Loss: 945.9641723632812, Entropy 376.44720458984375, Learning Rate: 0.00125\n",
      "Epoch [7118/20000], Loss: 926.6677856445312, Entropy 372.48846435546875, Learning Rate: 0.00125\n",
      "Epoch [7119/20000], Loss: 902.6298828125, Entropy 380.8625793457031, Learning Rate: 0.00125\n",
      "Epoch [7120/20000], Loss: 851.256591796875, Entropy 388.4360046386719, Learning Rate: 0.00125\n",
      "Epoch [7121/20000], Loss: 899.816162109375, Entropy 362.1028137207031, Learning Rate: 0.00125\n",
      "Epoch [7122/20000], Loss: 909.817626953125, Entropy 369.0202331542969, Learning Rate: 0.00125\n",
      "Epoch [7123/20000], Loss: 916.9749145507812, Entropy 378.67523193359375, Learning Rate: 0.00125\n",
      "Epoch [7124/20000], Loss: 899.6112060546875, Entropy 374.9485168457031, Learning Rate: 0.00125\n",
      "Epoch [7125/20000], Loss: 891.2015380859375, Entropy 385.5383605957031, Learning Rate: 0.00125\n",
      "Epoch [7126/20000], Loss: 873.5979614257812, Entropy 383.43597412109375, Learning Rate: 0.00125\n",
      "Epoch [7127/20000], Loss: 886.5361328125, Entropy 374.7654724121094, Learning Rate: 0.00125\n",
      "Epoch [7128/20000], Loss: 945.3502197265625, Entropy 376.2156066894531, Learning Rate: 0.00125\n",
      "Epoch [7129/20000], Loss: 892.04296875, Entropy 373.7477111816406, Learning Rate: 0.00125\n",
      "Epoch [7130/20000], Loss: 953.0030517578125, Entropy 388.7806091308594, Learning Rate: 0.00125\n",
      "Epoch [7131/20000], Loss: 937.1873779296875, Entropy 374.7239074707031, Learning Rate: 0.00125\n",
      "Epoch [7132/20000], Loss: 887.0891723632812, Entropy 381.85162353515625, Learning Rate: 0.00125\n",
      "Epoch [7133/20000], Loss: 896.3739013671875, Entropy 375.41162109375, Learning Rate: 0.00125\n",
      "Epoch [7134/20000], Loss: 908.388427734375, Entropy 379.1929626464844, Learning Rate: 0.00125\n",
      "Epoch [7135/20000], Loss: 921.627197265625, Entropy 381.7286682128906, Learning Rate: 0.00125\n",
      "Epoch [7136/20000], Loss: 892.6341552734375, Entropy 394.2957763671875, Learning Rate: 0.00125\n",
      "Epoch [7137/20000], Loss: 880.0521240234375, Entropy 388.2110900878906, Learning Rate: 0.00125\n",
      "Epoch [7138/20000], Loss: 896.9506225585938, Entropy 377.03265380859375, Learning Rate: 0.00125\n",
      "Epoch [7139/20000], Loss: 922.758544921875, Entropy 385.9316711425781, Learning Rate: 0.00125\n",
      "Epoch [7140/20000], Loss: 856.1864013671875, Entropy 369.5362854003906, Learning Rate: 0.00125\n",
      "Epoch [7141/20000], Loss: 897.10546875, Entropy 363.3036193847656, Learning Rate: 0.00125\n",
      "Epoch [7142/20000], Loss: 892.2158203125, Entropy 368.3679504394531, Learning Rate: 0.00125\n",
      "Epoch [7143/20000], Loss: 888.9572143554688, Entropy 375.02630615234375, Learning Rate: 0.00125\n",
      "Epoch [7144/20000], Loss: 870.3770751953125, Entropy 374.852294921875, Learning Rate: 0.00125\n",
      "Epoch [7145/20000], Loss: 920.4521484375, Entropy 370.9634704589844, Learning Rate: 0.00125\n",
      "Epoch [7146/20000], Loss: 874.9078369140625, Entropy 369.2830505371094, Learning Rate: 0.00125\n",
      "Epoch [7147/20000], Loss: 963.275146484375, Entropy 373.0249938964844, Learning Rate: 0.00125\n",
      "Epoch [7148/20000], Loss: 898.0049438476562, Entropy 380.71881103515625, Learning Rate: 0.00125\n",
      "Epoch [7149/20000], Loss: 927.5986938476562, Entropy 380.64312744140625, Learning Rate: 0.00125\n",
      "Epoch [7150/20000], Loss: 881.4202880859375, Entropy 369.6999206542969, Learning Rate: 0.00125\n",
      "Epoch [7151/20000], Loss: 874.794921875, Entropy 373.2298889160156, Learning Rate: 0.00125\n",
      "Epoch [7152/20000], Loss: 874.478759765625, Entropy 380.3621826171875, Learning Rate: 0.00125\n",
      "Epoch [7153/20000], Loss: 899.140380859375, Entropy 393.0193786621094, Learning Rate: 0.00125\n",
      "Epoch [7154/20000], Loss: 939.3388671875, Entropy 377.2209777832031, Learning Rate: 0.00125\n",
      "Epoch [7155/20000], Loss: 966.68701171875, Entropy 383.5691223144531, Learning Rate: 0.00125\n",
      "Epoch [7156/20000], Loss: 931.0183715820312, Entropy 359.86163330078125, Learning Rate: 0.00125\n",
      "Epoch [7157/20000], Loss: 888.800048828125, Entropy 371.9889831542969, Learning Rate: 0.00125\n",
      "Epoch [7158/20000], Loss: 920.9793701171875, Entropy 376.7134704589844, Learning Rate: 0.00125\n",
      "Epoch [7159/20000], Loss: 864.9156494140625, Entropy 366.491943359375, Learning Rate: 0.00125\n",
      "Epoch [7160/20000], Loss: 913.3988037109375, Entropy 381.7652893066406, Learning Rate: 0.00125\n",
      "Epoch [7161/20000], Loss: 920.0625, Entropy 370.3267822265625, Learning Rate: 0.00125\n",
      "Epoch [7162/20000], Loss: 938.3140258789062, Entropy 374.16827392578125, Learning Rate: 0.00125\n",
      "Epoch [7163/20000], Loss: 905.18798828125, Entropy 361.5792541503906, Learning Rate: 0.00125\n",
      "Epoch [7164/20000], Loss: 932.4790649414062, Entropy 375.50225830078125, Learning Rate: 0.00125\n",
      "Epoch [7165/20000], Loss: 866.773681640625, Entropy 377.9687805175781, Learning Rate: 0.00125\n",
      "Epoch [7166/20000], Loss: 924.9932861328125, Entropy 361.8420715332031, Learning Rate: 0.00125\n",
      "Epoch [7167/20000], Loss: 918.3885498046875, Entropy 377.8055419921875, Learning Rate: 0.00125\n",
      "Epoch [7168/20000], Loss: 904.0057373046875, Entropy 366.91162109375, Learning Rate: 0.00125\n",
      "Epoch [7169/20000], Loss: 935.5390625, Entropy 382.8701477050781, Learning Rate: 0.00125\n",
      "Epoch [7170/20000], Loss: 944.0130615234375, Entropy 371.1253967285156, Learning Rate: 0.00125\n",
      "Epoch [7171/20000], Loss: 876.9005126953125, Entropy 383.7150573730469, Learning Rate: 0.00125\n",
      "Epoch [7172/20000], Loss: 862.2303466796875, Entropy 376.2367858886719, Learning Rate: 0.00125\n",
      "Epoch [7173/20000], Loss: 869.2335815429688, Entropy 373.56439208984375, Learning Rate: 0.00125\n",
      "Epoch [7174/20000], Loss: 900.9365234375, Entropy 375.6935729980469, Learning Rate: 0.00125\n",
      "Epoch [7175/20000], Loss: 863.2807006835938, Entropy 378.72479248046875, Learning Rate: 0.00125\n",
      "Epoch [7176/20000], Loss: 942.74658203125, Entropy 370.3092956542969, Learning Rate: 0.00125\n",
      "Epoch [7177/20000], Loss: 973.4143676757812, Entropy 373.43060302734375, Learning Rate: 0.00125\n",
      "Epoch [7178/20000], Loss: 925.3759765625, Entropy 369.214599609375, Learning Rate: 0.00125\n",
      "Epoch [7179/20000], Loss: 967.1612548828125, Entropy 370.6180419921875, Learning Rate: 0.00125\n",
      "Epoch [7180/20000], Loss: 867.7784423828125, Entropy 377.8168640136719, Learning Rate: 0.00125\n",
      "Epoch [7181/20000], Loss: 885.3565673828125, Entropy 367.3188171386719, Learning Rate: 0.00125\n",
      "Epoch [7182/20000], Loss: 940.4183349609375, Entropy 391.2586669921875, Learning Rate: 0.00125\n",
      "Epoch [7183/20000], Loss: 905.9163818359375, Entropy 371.3522033691406, Learning Rate: 0.00125\n",
      "Epoch [7184/20000], Loss: 884.7778930664062, Entropy 385.17572021484375, Learning Rate: 0.00125\n",
      "Epoch [7185/20000], Loss: 887.7935791015625, Entropy 390.0830383300781, Learning Rate: 0.00125\n",
      "Epoch [7186/20000], Loss: 861.6759643554688, Entropy 387.57318115234375, Learning Rate: 0.00125\n",
      "Epoch [7187/20000], Loss: 913.5614013671875, Entropy 383.91015625, Learning Rate: 0.00125\n",
      "Epoch [7188/20000], Loss: 865.8004150390625, Entropy 394.1488342285156, Learning Rate: 0.00125\n",
      "Epoch [7189/20000], Loss: 878.289306640625, Entropy 363.9689025878906, Learning Rate: 0.00125\n",
      "Epoch [7190/20000], Loss: 900.0101318359375, Entropy 372.1114807128906, Learning Rate: 0.00125\n",
      "Epoch [7191/20000], Loss: 892.7677001953125, Entropy 383.4395751953125, Learning Rate: 0.00125\n",
      "Epoch [7192/20000], Loss: 874.761962890625, Entropy 378.08447265625, Learning Rate: 0.00125\n",
      "Epoch [7193/20000], Loss: 887.1788330078125, Entropy 373.2502136230469, Learning Rate: 0.00125\n",
      "Epoch [7194/20000], Loss: 883.1727294921875, Entropy 359.244140625, Learning Rate: 0.00125\n",
      "Epoch [7195/20000], Loss: 933.6456909179688, Entropy 367.37054443359375, Learning Rate: 0.00125\n",
      "Epoch [7196/20000], Loss: 927.96826171875, Entropy 376.7153015136719, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7197/20000], Loss: 897.50537109375, Entropy 392.1009826660156, Learning Rate: 0.00125\n",
      "Epoch [7198/20000], Loss: 907.830078125, Entropy 386.4878845214844, Learning Rate: 0.00125\n",
      "Epoch [7199/20000], Loss: 888.2635498046875, Entropy 380.2119445800781, Learning Rate: 0.00125\n",
      "Epoch [7200/20000], Loss: 926.2154541015625, Entropy 368.1877746582031, Learning Rate: 0.00125\n",
      "Epoch [7201/20000], Loss: 877.4090576171875, Entropy 382.8713073730469, Learning Rate: 0.00125\n",
      "Epoch [7202/20000], Loss: 925.6171264648438, Entropy 379.68389892578125, Learning Rate: 0.00125\n",
      "Epoch [7203/20000], Loss: 917.459716796875, Entropy 380.0420227050781, Learning Rate: 0.00125\n",
      "Epoch [7204/20000], Loss: 865.060546875, Entropy 392.273193359375, Learning Rate: 0.00125\n",
      "Epoch [7205/20000], Loss: 888.4315795898438, Entropy 392.78131103515625, Learning Rate: 0.00125\n",
      "Epoch [7206/20000], Loss: 892.58154296875, Entropy 384.6922607421875, Learning Rate: 0.00125\n",
      "Epoch [7207/20000], Loss: 905.272216796875, Entropy 371.87060546875, Learning Rate: 0.00125\n",
      "Epoch [7208/20000], Loss: 908.89111328125, Entropy 371.7754211425781, Learning Rate: 0.00125\n",
      "Epoch [7209/20000], Loss: 936.7805786132812, Entropy 380.61444091796875, Learning Rate: 0.00125\n",
      "Epoch [7210/20000], Loss: 962.8334350585938, Entropy 397.56585693359375, Learning Rate: 0.00125\n",
      "Epoch [7211/20000], Loss: 903.5720825195312, Entropy 380.48431396484375, Learning Rate: 0.00125\n",
      "Epoch [7212/20000], Loss: 904.9849853515625, Entropy 392.48828125, Learning Rate: 0.00125\n",
      "Epoch [7213/20000], Loss: 881.0678100585938, Entropy 390.14288330078125, Learning Rate: 0.00125\n",
      "Epoch [7214/20000], Loss: 905.188720703125, Entropy 368.7362060546875, Learning Rate: 0.00125\n",
      "Epoch [7215/20000], Loss: 954.8843994140625, Entropy 366.0902404785156, Learning Rate: 0.00125\n",
      "Epoch [7216/20000], Loss: 888.2395629882812, Entropy 379.03997802734375, Learning Rate: 0.00125\n",
      "Epoch [7217/20000], Loss: 873.014404296875, Entropy 378.8387451171875, Learning Rate: 0.00125\n",
      "Epoch [7218/20000], Loss: 895.956298828125, Entropy 376.5281066894531, Learning Rate: 0.00125\n",
      "Epoch [7219/20000], Loss: 867.9569091796875, Entropy 379.4533386230469, Learning Rate: 0.00125\n",
      "Epoch [7220/20000], Loss: 878.6256103515625, Entropy 379.0519714355469, Learning Rate: 0.00125\n",
      "Epoch [7221/20000], Loss: 858.3241577148438, Entropy 395.18841552734375, Learning Rate: 0.00125\n",
      "Epoch [7222/20000], Loss: 905.1513671875, Entropy 376.8938293457031, Learning Rate: 0.00125\n",
      "Epoch [7223/20000], Loss: 888.8487548828125, Entropy 373.3171691894531, Learning Rate: 0.00125\n",
      "Epoch [7224/20000], Loss: 962.60791015625, Entropy 390.8519592285156, Learning Rate: 0.00125\n",
      "Epoch [7225/20000], Loss: 842.0274047851562, Entropy 397.45635986328125, Learning Rate: 0.00125\n",
      "Epoch [7226/20000], Loss: 897.5562744140625, Entropy 402.8730773925781, Learning Rate: 0.00125\n",
      "Epoch [7227/20000], Loss: 968.5340576171875, Entropy 359.6033630371094, Learning Rate: 0.00125\n",
      "Epoch [7228/20000], Loss: 875.4375, Entropy 390.3824768066406, Learning Rate: 0.00125\n",
      "Epoch [7229/20000], Loss: 860.2120361328125, Entropy 393.7762756347656, Learning Rate: 0.00125\n",
      "Epoch [7230/20000], Loss: 852.9813842773438, Entropy 388.92242431640625, Learning Rate: 0.00125\n",
      "Epoch [7231/20000], Loss: 940.1402587890625, Entropy 367.8954162597656, Learning Rate: 0.00125\n",
      "Epoch [7232/20000], Loss: 870.9649658203125, Entropy 388.0230407714844, Learning Rate: 0.00125\n",
      "Epoch [7233/20000], Loss: 883.470947265625, Entropy 382.849609375, Learning Rate: 0.00125\n",
      "Epoch [7234/20000], Loss: 878.84912109375, Entropy 379.4211730957031, Learning Rate: 0.00125\n",
      "Epoch [7235/20000], Loss: 879.86083984375, Entropy 387.2342529296875, Learning Rate: 0.00125\n",
      "Epoch [7236/20000], Loss: 894.8594970703125, Entropy 395.9278869628906, Learning Rate: 0.00125\n",
      "Epoch [7237/20000], Loss: 902.067138671875, Entropy 386.6547546386719, Learning Rate: 0.00125\n",
      "Epoch [7238/20000], Loss: 880.7381591796875, Entropy 373.3556213378906, Learning Rate: 0.00125\n",
      "Epoch [7239/20000], Loss: 884.4496459960938, Entropy 390.24835205078125, Learning Rate: 0.00125\n",
      "Epoch [7240/20000], Loss: 880.9883422851562, Entropy 387.89837646484375, Learning Rate: 0.00125\n",
      "Epoch [7241/20000], Loss: 886.4647216796875, Entropy 388.9150390625, Learning Rate: 0.00125\n",
      "Epoch [7242/20000], Loss: 889.90478515625, Entropy 381.454833984375, Learning Rate: 0.00125\n",
      "Epoch [7243/20000], Loss: 856.1217041015625, Entropy 392.9304504394531, Learning Rate: 0.00125\n",
      "Epoch [7244/20000], Loss: 859.5712890625, Entropy 392.7760314941406, Learning Rate: 0.00125\n",
      "Epoch [7245/20000], Loss: 852.4263916015625, Entropy 392.2518615722656, Learning Rate: 0.00125\n",
      "Epoch [7246/20000], Loss: 886.4215087890625, Entropy 385.1505432128906, Learning Rate: 0.00125\n",
      "Epoch [7247/20000], Loss: 875.23583984375, Entropy 375.1318664550781, Learning Rate: 0.00125\n",
      "Epoch [7248/20000], Loss: 875.9267578125, Entropy 395.7615661621094, Learning Rate: 0.00125\n",
      "Epoch [7249/20000], Loss: 882.4485473632812, Entropy 382.87933349609375, Learning Rate: 0.00125\n",
      "Epoch [7250/20000], Loss: 877.95263671875, Entropy 396.90673828125, Learning Rate: 0.00125\n",
      "Epoch [7251/20000], Loss: 866.5111083984375, Entropy 381.8713073730469, Learning Rate: 0.00125\n",
      "Epoch [7252/20000], Loss: 928.8966064453125, Entropy 399.617919921875, Learning Rate: 0.00125\n",
      "Epoch [7253/20000], Loss: 917.3902587890625, Entropy 389.4690856933594, Learning Rate: 0.00125\n",
      "Epoch [7254/20000], Loss: 855.329833984375, Entropy 382.4167785644531, Learning Rate: 0.00125\n",
      "Epoch [7255/20000], Loss: 890.8848876953125, Entropy 378.5121154785156, Learning Rate: 0.00125\n",
      "Epoch [7256/20000], Loss: 935.065673828125, Entropy 385.4604187011719, Learning Rate: 0.00125\n",
      "Epoch [7257/20000], Loss: 938.18798828125, Entropy 389.8641357421875, Learning Rate: 0.00125\n",
      "Epoch [7258/20000], Loss: 914.768798828125, Entropy 397.7593994140625, Learning Rate: 0.00125\n",
      "Epoch [7259/20000], Loss: 880.390625, Entropy 388.7943420410156, Learning Rate: 0.00125\n",
      "Epoch [7260/20000], Loss: 932.273193359375, Entropy 383.6883544921875, Learning Rate: 0.00125\n",
      "Epoch [7261/20000], Loss: 917.94580078125, Entropy 393.0998229980469, Learning Rate: 0.00125\n",
      "Epoch [7262/20000], Loss: 857.0665283203125, Entropy 387.1767578125, Learning Rate: 0.00125\n",
      "Epoch [7263/20000], Loss: 872.4945678710938, Entropy 391.04156494140625, Learning Rate: 0.00125\n",
      "Epoch [7264/20000], Loss: 961.1947631835938, Entropy 369.77203369140625, Learning Rate: 0.00125\n",
      "Epoch [7265/20000], Loss: 921.9251098632812, Entropy 394.30023193359375, Learning Rate: 0.00125\n",
      "Epoch [7266/20000], Loss: 946.611328125, Entropy 404.2510070800781, Learning Rate: 0.00125\n",
      "Epoch [7267/20000], Loss: 1116.8548583984375, Entropy 385.7751159667969, Learning Rate: 0.00125\n",
      "Epoch [7268/20000], Loss: 996.4931640625, Entropy 394.7156066894531, Learning Rate: 0.00125\n",
      "Epoch [7269/20000], Loss: 972.736572265625, Entropy 379.1268310546875, Learning Rate: 0.00125\n",
      "Epoch [7270/20000], Loss: 1030.0670166015625, Entropy 385.43701171875, Learning Rate: 0.00125\n",
      "Epoch [7271/20000], Loss: 925.5341186523438, Entropy 380.75677490234375, Learning Rate: 0.00125\n",
      "Epoch [7272/20000], Loss: 1023.0625, Entropy 386.6971740722656, Learning Rate: 0.00125\n",
      "Epoch [7273/20000], Loss: 939.7904052734375, Entropy 383.2112731933594, Learning Rate: 0.00125\n",
      "Epoch [7274/20000], Loss: 914.0479736328125, Entropy 387.7704162597656, Learning Rate: 0.00125\n",
      "Epoch [7275/20000], Loss: 979.5387573242188, Entropy 373.25213623046875, Learning Rate: 0.00125\n",
      "Epoch [7276/20000], Loss: 1013.3304443359375, Entropy 385.4097900390625, Learning Rate: 0.00125\n",
      "Epoch [7277/20000], Loss: 986.3553466796875, Entropy 396.6636657714844, Learning Rate: 0.00125\n",
      "Epoch [7278/20000], Loss: 957.0084228515625, Entropy 380.5643310546875, Learning Rate: 0.00125\n",
      "Epoch [7279/20000], Loss: 926.5084228515625, Entropy 383.2087097167969, Learning Rate: 0.00125\n",
      "Epoch [7280/20000], Loss: 947.8277587890625, Entropy 382.5743713378906, Learning Rate: 0.00125\n",
      "Epoch [7281/20000], Loss: 917.6942138671875, Entropy 369.7799987792969, Learning Rate: 0.00125\n",
      "Epoch [7282/20000], Loss: 886.4310302734375, Entropy 369.55419921875, Learning Rate: 0.00125\n",
      "Epoch [7283/20000], Loss: 974.2371215820312, Entropy 378.25836181640625, Learning Rate: 0.00125\n",
      "Epoch [7284/20000], Loss: 938.4667358398438, Entropy 386.33233642578125, Learning Rate: 0.00125\n",
      "Epoch [7285/20000], Loss: 949.4668579101562, Entropy 391.84686279296875, Learning Rate: 0.00125\n",
      "Epoch [7286/20000], Loss: 912.6055908203125, Entropy 388.2603454589844, Learning Rate: 0.00125\n",
      "Epoch [7287/20000], Loss: 952.2509765625, Entropy 382.7345886230469, Learning Rate: 0.00125\n",
      "Epoch [7288/20000], Loss: 924.902587890625, Entropy 382.5225830078125, Learning Rate: 0.00125\n",
      "Epoch [7289/20000], Loss: 954.9454345703125, Entropy 394.5299987792969, Learning Rate: 0.00125\n",
      "Epoch [7290/20000], Loss: 941.7342529296875, Entropy 395.2273254394531, Learning Rate: 0.00125\n",
      "Epoch [7291/20000], Loss: 916.536376953125, Entropy 388.285888671875, Learning Rate: 0.00125\n",
      "Epoch [7292/20000], Loss: 881.720458984375, Entropy 401.275390625, Learning Rate: 0.00125\n",
      "Epoch [7293/20000], Loss: 914.2999267578125, Entropy 394.5124206542969, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7294/20000], Loss: 903.5222778320312, Entropy 370.93365478515625, Learning Rate: 0.00125\n",
      "Epoch [7295/20000], Loss: 886.08984375, Entropy 373.4871826171875, Learning Rate: 0.00125\n",
      "Epoch [7296/20000], Loss: 946.7083740234375, Entropy 380.6978759765625, Learning Rate: 0.00125\n",
      "Epoch [7297/20000], Loss: 937.0648193359375, Entropy 394.04638671875, Learning Rate: 0.00125\n",
      "Epoch [7298/20000], Loss: 931.1495361328125, Entropy 384.6480407714844, Learning Rate: 0.00125\n",
      "Epoch [7299/20000], Loss: 979.6094970703125, Entropy 389.3695373535156, Learning Rate: 0.00125\n",
      "Epoch [7300/20000], Loss: 879.8414306640625, Entropy 380.4691467285156, Learning Rate: 0.00125\n",
      "Epoch [7301/20000], Loss: 918.3543701171875, Entropy 381.0730285644531, Learning Rate: 0.00125\n",
      "Epoch [7302/20000], Loss: 884.86181640625, Entropy 383.5845642089844, Learning Rate: 0.00125\n",
      "Epoch [7303/20000], Loss: 853.6888427734375, Entropy 388.0544738769531, Learning Rate: 0.00125\n",
      "Epoch [7304/20000], Loss: 953.8701171875, Entropy 387.8751525878906, Learning Rate: 0.00125\n",
      "Epoch [7305/20000], Loss: 902.0314331054688, Entropy 388.18170166015625, Learning Rate: 0.00125\n",
      "Epoch [7306/20000], Loss: 859.4298095703125, Entropy 389.5138854980469, Learning Rate: 0.00125\n",
      "Epoch [7307/20000], Loss: 898.6605224609375, Entropy 389.9110107421875, Learning Rate: 0.00125\n",
      "Epoch [7308/20000], Loss: 873.324951171875, Entropy 403.9434814453125, Learning Rate: 0.00125\n",
      "Epoch [7309/20000], Loss: 889.0120849609375, Entropy 409.7167053222656, Learning Rate: 0.00125\n",
      "Epoch [7310/20000], Loss: 848.3801879882812, Entropy 392.99615478515625, Learning Rate: 0.00125\n",
      "Epoch [7311/20000], Loss: 861.096435546875, Entropy 383.3242492675781, Learning Rate: 0.00125\n",
      "Epoch [7312/20000], Loss: 909.4334716796875, Entropy 375.2159729003906, Learning Rate: 0.00125\n",
      "Epoch [7313/20000], Loss: 915.0516967773438, Entropy 379.55865478515625, Learning Rate: 0.00125\n",
      "Epoch [7314/20000], Loss: 938.08837890625, Entropy 395.38232421875, Learning Rate: 0.00125\n",
      "Epoch [7315/20000], Loss: 917.9322509765625, Entropy 372.8849182128906, Learning Rate: 0.00125\n",
      "Epoch [7316/20000], Loss: 989.3604125976562, Entropy 384.24737548828125, Learning Rate: 0.00125\n",
      "Epoch [7317/20000], Loss: 962.48486328125, Entropy 376.6068420410156, Learning Rate: 0.00125\n",
      "Epoch [7318/20000], Loss: 907.6298828125, Entropy 404.0182189941406, Learning Rate: 0.00125\n",
      "Epoch [7319/20000], Loss: 890.3260498046875, Entropy 373.4977111816406, Learning Rate: 0.00125\n",
      "Epoch [7320/20000], Loss: 960.2849731445312, Entropy 405.50103759765625, Learning Rate: 0.00125\n",
      "Epoch [7321/20000], Loss: 886.9292602539062, Entropy 377.55328369140625, Learning Rate: 0.00125\n",
      "Epoch [7322/20000], Loss: 936.965087890625, Entropy 380.95947265625, Learning Rate: 0.00125\n",
      "Epoch [7323/20000], Loss: 921.35302734375, Entropy 375.1298828125, Learning Rate: 0.00125\n",
      "Epoch [7324/20000], Loss: 867.8263549804688, Entropy 405.47601318359375, Learning Rate: 0.00125\n",
      "Epoch [7325/20000], Loss: 930.50634765625, Entropy 383.4065856933594, Learning Rate: 0.00125\n",
      "Epoch [7326/20000], Loss: 999.2996826171875, Entropy 383.9826354980469, Learning Rate: 0.00125\n",
      "Epoch [7327/20000], Loss: 873.8026733398438, Entropy 394.93060302734375, Learning Rate: 0.00125\n",
      "Epoch [7328/20000], Loss: 897.5472412109375, Entropy 381.4814758300781, Learning Rate: 0.00125\n",
      "Epoch [7329/20000], Loss: 860.73291015625, Entropy 394.1075439453125, Learning Rate: 0.00125\n",
      "Epoch [7330/20000], Loss: 914.0955200195312, Entropy 378.95257568359375, Learning Rate: 0.00125\n",
      "Epoch [7331/20000], Loss: 919.1961669921875, Entropy 373.1446838378906, Learning Rate: 0.00125\n",
      "Epoch [7332/20000], Loss: 912.0093383789062, Entropy 379.53558349609375, Learning Rate: 0.00125\n",
      "Epoch [7333/20000], Loss: 902.4423828125, Entropy 392.13134765625, Learning Rate: 0.00125\n",
      "Epoch [7334/20000], Loss: 931.0029296875, Entropy 376.7843017578125, Learning Rate: 0.00125\n",
      "Epoch [7335/20000], Loss: 884.3228759765625, Entropy 395.84765625, Learning Rate: 0.00125\n",
      "Epoch [7336/20000], Loss: 871.1311645507812, Entropy 404.21514892578125, Learning Rate: 0.00125\n",
      "Epoch [7337/20000], Loss: 896.4613037109375, Entropy 373.3439025878906, Learning Rate: 0.00125\n",
      "Epoch [7338/20000], Loss: 865.7305908203125, Entropy 395.1373291015625, Learning Rate: 0.00125\n",
      "Epoch [7339/20000], Loss: 886.185302734375, Entropy 390.25830078125, Learning Rate: 0.00125\n",
      "Epoch [7340/20000], Loss: 893.2604370117188, Entropy 382.18023681640625, Learning Rate: 0.00125\n",
      "Epoch [7341/20000], Loss: 891.8682861328125, Entropy 393.9685974121094, Learning Rate: 0.00125\n",
      "Epoch [7342/20000], Loss: 865.0139770507812, Entropy 386.95916748046875, Learning Rate: 0.00125\n",
      "Epoch [7343/20000], Loss: 931.287109375, Entropy 403.0638122558594, Learning Rate: 0.00125\n",
      "Epoch [7344/20000], Loss: 990.1998291015625, Entropy 390.3830261230469, Learning Rate: 0.00125\n",
      "Epoch [7345/20000], Loss: 930.524658203125, Entropy 401.5704650878906, Learning Rate: 0.00125\n",
      "Epoch [7346/20000], Loss: 884.0177001953125, Entropy 392.8560485839844, Learning Rate: 0.00125\n",
      "Epoch [7347/20000], Loss: 928.4747314453125, Entropy 389.9583435058594, Learning Rate: 0.00125\n",
      "Epoch [7348/20000], Loss: 892.5709228515625, Entropy 391.4967956542969, Learning Rate: 0.00125\n",
      "Epoch [7349/20000], Loss: 940.57568359375, Entropy 388.6483459472656, Learning Rate: 0.00125\n",
      "Epoch [7350/20000], Loss: 907.7413330078125, Entropy 388.1266174316406, Learning Rate: 0.00125\n",
      "Epoch [7351/20000], Loss: 937.7841796875, Entropy 382.5455017089844, Learning Rate: 0.00125\n",
      "Epoch [7352/20000], Loss: 913.9158935546875, Entropy 384.3475646972656, Learning Rate: 0.00125\n",
      "Epoch [7353/20000], Loss: 901.1312255859375, Entropy 404.0049133300781, Learning Rate: 0.00125\n",
      "Epoch [7354/20000], Loss: 883.543212890625, Entropy 384.4464111328125, Learning Rate: 0.00125\n",
      "Epoch [7355/20000], Loss: 973.0107421875, Entropy 386.1435546875, Learning Rate: 0.00125\n",
      "Epoch [7356/20000], Loss: 958.424072265625, Entropy 382.0553894042969, Learning Rate: 0.00125\n",
      "Epoch [7357/20000], Loss: 896.326171875, Entropy 392.4407043457031, Learning Rate: 0.00125\n",
      "Epoch [7358/20000], Loss: 874.4928588867188, Entropy 378.94769287109375, Learning Rate: 0.00125\n",
      "Epoch [7359/20000], Loss: 930.3087158203125, Entropy 367.1717224121094, Learning Rate: 0.00125\n",
      "Epoch [7360/20000], Loss: 922.7842407226562, Entropy 406.86077880859375, Learning Rate: 0.00125\n",
      "Epoch [7361/20000], Loss: 908.6358642578125, Entropy 401.9273986816406, Learning Rate: 0.00125\n",
      "Epoch [7362/20000], Loss: 931.8463745117188, Entropy 388.85406494140625, Learning Rate: 0.00125\n",
      "Epoch [7363/20000], Loss: 948.8903198242188, Entropy 407.29840087890625, Learning Rate: 0.00125\n",
      "Epoch [7364/20000], Loss: 974.3140869140625, Entropy 385.2059631347656, Learning Rate: 0.00125\n",
      "Epoch [7365/20000], Loss: 946.502685546875, Entropy 379.1725769042969, Learning Rate: 0.00125\n",
      "Epoch [7366/20000], Loss: 1042.7342529296875, Entropy 381.9881286621094, Learning Rate: 0.00125\n",
      "Epoch [7367/20000], Loss: 928.8724365234375, Entropy 394.62060546875, Learning Rate: 0.00125\n",
      "Epoch [7368/20000], Loss: 1021.4295654296875, Entropy 368.4744567871094, Learning Rate: 0.00125\n",
      "Epoch [7369/20000], Loss: 949.941162109375, Entropy 369.3969421386719, Learning Rate: 0.00125\n",
      "Epoch [7370/20000], Loss: 941.427734375, Entropy 389.3737487792969, Learning Rate: 0.00125\n",
      "Epoch [7371/20000], Loss: 951.5362548828125, Entropy 381.4403076171875, Learning Rate: 0.00125\n",
      "Epoch [7372/20000], Loss: 902.6552734375, Entropy 381.6896057128906, Learning Rate: 0.00125\n",
      "Epoch [7373/20000], Loss: 942.1631469726562, Entropy 382.28131103515625, Learning Rate: 0.00125\n",
      "Epoch [7374/20000], Loss: 889.6764526367188, Entropy 385.69927978515625, Learning Rate: 0.00125\n",
      "Epoch [7375/20000], Loss: 862.1328125, Entropy 387.9304504394531, Learning Rate: 0.00125\n",
      "Epoch [7376/20000], Loss: 959.0366821289062, Entropy 368.55218505859375, Learning Rate: 0.00125\n",
      "Epoch [7377/20000], Loss: 924.2224731445312, Entropy 382.73760986328125, Learning Rate: 0.00125\n",
      "Epoch [7378/20000], Loss: 870.68896484375, Entropy 404.8703308105469, Learning Rate: 0.00125\n",
      "Epoch [7379/20000], Loss: 884.30322265625, Entropy 392.1956481933594, Learning Rate: 0.00125\n",
      "Epoch [7380/20000], Loss: 969.0770263671875, Entropy 379.580810546875, Learning Rate: 0.00125\n",
      "Epoch [7381/20000], Loss: 903.4854736328125, Entropy 398.8861389160156, Learning Rate: 0.00125\n",
      "Epoch [7382/20000], Loss: 920.8905029296875, Entropy 375.7455749511719, Learning Rate: 0.00125\n",
      "Epoch [7383/20000], Loss: 919.3880615234375, Entropy 372.2466735839844, Learning Rate: 0.00125\n",
      "Epoch [7384/20000], Loss: 953.2318115234375, Entropy 391.4251403808594, Learning Rate: 0.00125\n",
      "Epoch [7385/20000], Loss: 988.036376953125, Entropy 373.6590881347656, Learning Rate: 0.00125\n",
      "Epoch [7386/20000], Loss: 893.8460693359375, Entropy 384.1149597167969, Learning Rate: 0.00125\n",
      "Epoch [7387/20000], Loss: 933.846435546875, Entropy 388.0990295410156, Learning Rate: 0.00125\n",
      "Epoch [7388/20000], Loss: 896.8609619140625, Entropy 380.8551025390625, Learning Rate: 0.00125\n",
      "Epoch [7389/20000], Loss: 965.2898559570312, Entropy 384.09796142578125, Learning Rate: 0.00125\n",
      "Epoch [7390/20000], Loss: 966.474853515625, Entropy 373.7656555175781, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7391/20000], Loss: 956.228515625, Entropy 390.2095947265625, Learning Rate: 0.00125\n",
      "Epoch [7392/20000], Loss: 931.98193359375, Entropy 373.2459411621094, Learning Rate: 0.00125\n",
      "Epoch [7393/20000], Loss: 911.7577514648438, Entropy 380.99554443359375, Learning Rate: 0.00125\n",
      "Epoch [7394/20000], Loss: 908.515380859375, Entropy 401.4008483886719, Learning Rate: 0.00125\n",
      "Epoch [7395/20000], Loss: 938.571533203125, Entropy 375.0492248535156, Learning Rate: 0.00125\n",
      "Epoch [7396/20000], Loss: 927.8841552734375, Entropy 388.9930725097656, Learning Rate: 0.00125\n",
      "Epoch [7397/20000], Loss: 894.7255249023438, Entropy 377.45806884765625, Learning Rate: 0.00125\n",
      "Epoch [7398/20000], Loss: 937.609375, Entropy 385.7184753417969, Learning Rate: 0.00125\n",
      "Epoch [7399/20000], Loss: 915.1722412109375, Entropy 401.6533508300781, Learning Rate: 0.00125\n",
      "Epoch [7400/20000], Loss: 882.9560546875, Entropy 405.9195251464844, Learning Rate: 0.00125\n",
      "Epoch [7401/20000], Loss: 1004.9520263671875, Entropy 401.0011291503906, Learning Rate: 0.00125\n",
      "Epoch [7402/20000], Loss: 863.115478515625, Entropy 400.9425048828125, Learning Rate: 0.00125\n",
      "Epoch [7403/20000], Loss: 994.97705078125, Entropy 398.0335998535156, Learning Rate: 0.00125\n",
      "Epoch [7404/20000], Loss: 898.0826416015625, Entropy 388.3297119140625, Learning Rate: 0.00125\n",
      "Epoch [7405/20000], Loss: 975.2757568359375, Entropy 382.8860168457031, Learning Rate: 0.00125\n",
      "Epoch [7406/20000], Loss: 863.5697021484375, Entropy 395.831787109375, Learning Rate: 0.00125\n",
      "Epoch [7407/20000], Loss: 921.6046142578125, Entropy 378.3171691894531, Learning Rate: 0.00125\n",
      "Epoch [7408/20000], Loss: 943.6849975585938, Entropy 397.30596923828125, Learning Rate: 0.00125\n",
      "Epoch [7409/20000], Loss: 948.18701171875, Entropy 379.98046875, Learning Rate: 0.00125\n",
      "Epoch [7410/20000], Loss: 993.7161865234375, Entropy 382.1320495605469, Learning Rate: 0.00125\n",
      "Epoch [7411/20000], Loss: 881.0146484375, Entropy 389.2596435546875, Learning Rate: 0.00125\n",
      "Epoch [7412/20000], Loss: 926.5147094726562, Entropy 376.70989990234375, Learning Rate: 0.00125\n",
      "Epoch [7413/20000], Loss: 881.1820678710938, Entropy 387.65350341796875, Learning Rate: 0.00125\n",
      "Epoch [7414/20000], Loss: 902.06982421875, Entropy 390.3132629394531, Learning Rate: 0.00125\n",
      "Epoch [7415/20000], Loss: 939.3412475585938, Entropy 371.89874267578125, Learning Rate: 0.00125\n",
      "Epoch [7416/20000], Loss: 902.6243896484375, Entropy 378.8621520996094, Learning Rate: 0.00125\n",
      "Epoch [7417/20000], Loss: 922.296142578125, Entropy 368.3464050292969, Learning Rate: 0.00125\n",
      "Epoch [7418/20000], Loss: 934.4754638671875, Entropy 376.8973083496094, Learning Rate: 0.00125\n",
      "Epoch [7419/20000], Loss: 894.3092041015625, Entropy 403.1681213378906, Learning Rate: 0.00125\n",
      "Epoch [7420/20000], Loss: 936.3388671875, Entropy 397.8280334472656, Learning Rate: 0.00125\n",
      "Epoch [7421/20000], Loss: 937.8880615234375, Entropy 368.3974914550781, Learning Rate: 0.00125\n",
      "Epoch [7422/20000], Loss: 883.3636474609375, Entropy 395.4801330566406, Learning Rate: 0.00125\n",
      "Epoch [7423/20000], Loss: 876.902587890625, Entropy 389.9506530761719, Learning Rate: 0.00125\n",
      "Epoch [7424/20000], Loss: 868.499267578125, Entropy 391.451171875, Learning Rate: 0.00125\n",
      "Epoch [7425/20000], Loss: 967.1137084960938, Entropy 385.85162353515625, Learning Rate: 0.00125\n",
      "Epoch [7426/20000], Loss: 902.611083984375, Entropy 382.9905090332031, Learning Rate: 0.00125\n",
      "Epoch [7427/20000], Loss: 997.2861938476562, Entropy 385.35467529296875, Learning Rate: 0.00125\n",
      "Epoch [7428/20000], Loss: 863.2084350585938, Entropy 392.52886962890625, Learning Rate: 0.00125\n",
      "Epoch [7429/20000], Loss: 900.4034423828125, Entropy 388.6260070800781, Learning Rate: 0.00125\n",
      "Epoch [7430/20000], Loss: 893.260986328125, Entropy 391.0386657714844, Learning Rate: 0.00125\n",
      "Epoch [7431/20000], Loss: 945.287109375, Entropy 390.9836730957031, Learning Rate: 0.00125\n",
      "Epoch [7432/20000], Loss: 885.4395751953125, Entropy 385.2876892089844, Learning Rate: 0.00125\n",
      "Epoch [7433/20000], Loss: 849.50390625, Entropy 400.5196533203125, Learning Rate: 0.00125\n",
      "Epoch [7434/20000], Loss: 895.2701416015625, Entropy 384.2456970214844, Learning Rate: 0.00125\n",
      "Epoch [7435/20000], Loss: 891.9090576171875, Entropy 391.4330139160156, Learning Rate: 0.00125\n",
      "Epoch [7436/20000], Loss: 935.2130126953125, Entropy 391.8953552246094, Learning Rate: 0.00125\n",
      "Epoch [7437/20000], Loss: 878.95703125, Entropy 392.8506164550781, Learning Rate: 0.00125\n",
      "Epoch [7438/20000], Loss: 869.38330078125, Entropy 397.9261779785156, Learning Rate: 0.00125\n",
      "Epoch [7439/20000], Loss: 928.26123046875, Entropy 390.4916687011719, Learning Rate: 0.00125\n",
      "Epoch [7440/20000], Loss: 848.1781005859375, Entropy 379.7488098144531, Learning Rate: 0.00125\n",
      "Epoch [7441/20000], Loss: 922.0894775390625, Entropy 380.7452392578125, Learning Rate: 0.00125\n",
      "Epoch [7442/20000], Loss: 945.2567138671875, Entropy 383.2585144042969, Learning Rate: 0.00125\n",
      "Epoch [7443/20000], Loss: 893.2614135742188, Entropy 377.72711181640625, Learning Rate: 0.00125\n",
      "Epoch [7444/20000], Loss: 881.19580078125, Entropy 368.5273742675781, Learning Rate: 0.00125\n",
      "Epoch [7445/20000], Loss: 859.3563842773438, Entropy 383.22003173828125, Learning Rate: 0.00125\n",
      "Epoch [7446/20000], Loss: 847.7205810546875, Entropy 398.9599914550781, Learning Rate: 0.00125\n",
      "Epoch [7447/20000], Loss: 958.7981567382812, Entropy 383.51654052734375, Learning Rate: 0.00125\n",
      "Epoch [7448/20000], Loss: 826.031005859375, Entropy 409.5632019042969, Learning Rate: 0.00125\n",
      "Epoch [7449/20000], Loss: 880.9200439453125, Entropy 380.0669860839844, Learning Rate: 0.00125\n",
      "Epoch [7450/20000], Loss: 853.225830078125, Entropy 395.37890625, Learning Rate: 0.00125\n",
      "Epoch [7451/20000], Loss: 896.4938354492188, Entropy 390.74920654296875, Learning Rate: 0.00125\n",
      "Epoch [7452/20000], Loss: 916.245849609375, Entropy 379.305419921875, Learning Rate: 0.00125\n",
      "Epoch [7453/20000], Loss: 874.5284423828125, Entropy 388.5904541015625, Learning Rate: 0.00125\n",
      "Epoch [7454/20000], Loss: 879.1098022460938, Entropy 396.75531005859375, Learning Rate: 0.00125\n",
      "Epoch [7455/20000], Loss: 897.701904296875, Entropy 390.2789611816406, Learning Rate: 0.00125\n",
      "Epoch [7456/20000], Loss: 967.41943359375, Entropy 370.9072570800781, Learning Rate: 0.00125\n",
      "Epoch [7457/20000], Loss: 907.96337890625, Entropy 385.0528869628906, Learning Rate: 0.00125\n",
      "Epoch [7458/20000], Loss: 895.9182739257812, Entropy 390.03900146484375, Learning Rate: 0.00125\n",
      "Epoch [7459/20000], Loss: 861.8798828125, Entropy 389.1121826171875, Learning Rate: 0.00125\n",
      "Epoch [7460/20000], Loss: 849.925048828125, Entropy 405.1910705566406, Learning Rate: 0.00125\n",
      "Epoch [7461/20000], Loss: 867.6632080078125, Entropy 385.5091247558594, Learning Rate: 0.00125\n",
      "Epoch [7462/20000], Loss: 904.1309814453125, Entropy 388.3003845214844, Learning Rate: 0.00125\n",
      "Epoch [7463/20000], Loss: 904.1619873046875, Entropy 383.2199401855469, Learning Rate: 0.00125\n",
      "Epoch [7464/20000], Loss: 910.59326171875, Entropy 373.5453796386719, Learning Rate: 0.00125\n",
      "Epoch [7465/20000], Loss: 872.484375, Entropy 394.4142761230469, Learning Rate: 0.00125\n",
      "Epoch [7466/20000], Loss: 906.689453125, Entropy 390.7706604003906, Learning Rate: 0.00125\n",
      "Epoch [7467/20000], Loss: 860.8536376953125, Entropy 389.0685119628906, Learning Rate: 0.00125\n",
      "Epoch [7468/20000], Loss: 847.1126708984375, Entropy 398.004638671875, Learning Rate: 0.00125\n",
      "Epoch [7469/20000], Loss: 870.709716796875, Entropy 391.2052307128906, Learning Rate: 0.00125\n",
      "Epoch [7470/20000], Loss: 925.0341186523438, Entropy 375.63299560546875, Learning Rate: 0.00125\n",
      "Epoch [7471/20000], Loss: 877.0706787109375, Entropy 399.092529296875, Learning Rate: 0.00125\n",
      "Epoch [7472/20000], Loss: 876.467529296875, Entropy 390.13720703125, Learning Rate: 0.00125\n",
      "Epoch [7473/20000], Loss: 879.96142578125, Entropy 386.5105895996094, Learning Rate: 0.00125\n",
      "Epoch [7474/20000], Loss: 888.300048828125, Entropy 398.2299499511719, Learning Rate: 0.00125\n",
      "Epoch [7475/20000], Loss: 896.3616943359375, Entropy 393.7021484375, Learning Rate: 0.00125\n",
      "Epoch [7476/20000], Loss: 901.632568359375, Entropy 385.9630432128906, Learning Rate: 0.00125\n",
      "Epoch [7477/20000], Loss: 901.98974609375, Entropy 382.9203796386719, Learning Rate: 0.00125\n",
      "Epoch [7478/20000], Loss: 869.0570678710938, Entropy 375.42022705078125, Learning Rate: 0.00125\n",
      "Epoch [7479/20000], Loss: 868.984619140625, Entropy 383.9005432128906, Learning Rate: 0.00125\n",
      "Epoch [7480/20000], Loss: 861.2255859375, Entropy 405.5682678222656, Learning Rate: 0.00125\n",
      "Epoch [7481/20000], Loss: 924.2078247070312, Entropy 381.43218994140625, Learning Rate: 0.00125\n",
      "Epoch [7482/20000], Loss: 919.6016845703125, Entropy 411.8736572265625, Learning Rate: 0.00125\n",
      "Epoch [7483/20000], Loss: 934.8524169921875, Entropy 384.9566955566406, Learning Rate: 0.00125\n",
      "Epoch [7484/20000], Loss: 892.8045043945312, Entropy 386.06561279296875, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7485/20000], Loss: 836.934814453125, Entropy 399.0980529785156, Learning Rate: 0.00125\n",
      "Epoch [7486/20000], Loss: 946.1619262695312, Entropy 389.32928466796875, Learning Rate: 0.00125\n",
      "Epoch [7487/20000], Loss: 879.786376953125, Entropy 400.8663330078125, Learning Rate: 0.00125\n",
      "Epoch [7488/20000], Loss: 899.098388671875, Entropy 391.2242126464844, Learning Rate: 0.00125\n",
      "Epoch [7489/20000], Loss: 914.7772827148438, Entropy 390.30377197265625, Learning Rate: 0.00125\n",
      "Epoch [7490/20000], Loss: 860.8245849609375, Entropy 404.6819763183594, Learning Rate: 0.00125\n",
      "Epoch [7491/20000], Loss: 806.4155883789062, Entropy 406.33001708984375, Learning Rate: 0.00125\n",
      "Epoch [7492/20000], Loss: 871.6416015625, Entropy 389.5426330566406, Learning Rate: 0.00125\n",
      "Epoch [7493/20000], Loss: 931.9295654296875, Entropy 400.0303039550781, Learning Rate: 0.00125\n",
      "Epoch [7494/20000], Loss: 887.1937255859375, Entropy 384.6847839355469, Learning Rate: 0.00125\n",
      "Epoch [7495/20000], Loss: 901.811767578125, Entropy 380.2023010253906, Learning Rate: 0.00125\n",
      "Epoch [7496/20000], Loss: 859.8611450195312, Entropy 381.13580322265625, Learning Rate: 0.00125\n",
      "Epoch [7497/20000], Loss: 906.7863159179688, Entropy 403.35601806640625, Learning Rate: 0.00125\n",
      "Epoch [7498/20000], Loss: 889.1385498046875, Entropy 383.9681091308594, Learning Rate: 0.00125\n",
      "Epoch [7499/20000], Loss: 875.8327026367188, Entropy 391.66253662109375, Learning Rate: 0.00125\n",
      "Epoch [7500/20000], Loss: 883.0609130859375, Entropy 402.9007263183594, Learning Rate: 0.00125\n",
      "Epoch [7501/20000], Loss: 921.429931640625, Entropy 390.265625, Learning Rate: 0.00125\n",
      "Epoch [7502/20000], Loss: 877.6126708984375, Entropy 392.3494567871094, Learning Rate: 0.00125\n",
      "Epoch [7503/20000], Loss: 879.5711669921875, Entropy 386.1182861328125, Learning Rate: 0.00125\n",
      "Epoch [7504/20000], Loss: 861.171875, Entropy 389.6847839355469, Learning Rate: 0.00125\n",
      "Epoch [7505/20000], Loss: 935.3338623046875, Entropy 392.2102355957031, Learning Rate: 0.00125\n",
      "Epoch [7506/20000], Loss: 875.7322387695312, Entropy 395.30511474609375, Learning Rate: 0.00125\n",
      "Epoch [7507/20000], Loss: 867.7254028320312, Entropy 400.95159912109375, Learning Rate: 0.00125\n",
      "Epoch [7508/20000], Loss: 844.349609375, Entropy 405.7254638671875, Learning Rate: 0.00125\n",
      "Epoch [7509/20000], Loss: 870.3444213867188, Entropy 399.44964599609375, Learning Rate: 0.00125\n",
      "Epoch [7510/20000], Loss: 844.7572631835938, Entropy 393.81597900390625, Learning Rate: 0.00125\n",
      "Epoch [7511/20000], Loss: 910.67578125, Entropy 391.1229248046875, Learning Rate: 0.00125\n",
      "Epoch [7512/20000], Loss: 913.4798583984375, Entropy 393.40966796875, Learning Rate: 0.00125\n",
      "Epoch [7513/20000], Loss: 862.5802001953125, Entropy 407.3282470703125, Learning Rate: 0.00125\n",
      "Epoch [7514/20000], Loss: 916.1927490234375, Entropy 394.2560729980469, Learning Rate: 0.00125\n",
      "Epoch [7515/20000], Loss: 891.8742065429688, Entropy 394.65875244140625, Learning Rate: 0.00125\n",
      "Epoch [7516/20000], Loss: 902.48583984375, Entropy 387.4880676269531, Learning Rate: 0.00125\n",
      "Epoch [7517/20000], Loss: 909.310302734375, Entropy 386.3778076171875, Learning Rate: 0.00125\n",
      "Epoch [7518/20000], Loss: 892.3965454101562, Entropy 385.88116455078125, Learning Rate: 0.00125\n",
      "Epoch [7519/20000], Loss: 871.770263671875, Entropy 409.2488708496094, Learning Rate: 0.00125\n",
      "Epoch [7520/20000], Loss: 897.605712890625, Entropy 383.8638610839844, Learning Rate: 0.00125\n",
      "Epoch [7521/20000], Loss: 943.901611328125, Entropy 381.8731689453125, Learning Rate: 0.00125\n",
      "Epoch [7522/20000], Loss: 866.971435546875, Entropy 401.5961608886719, Learning Rate: 0.00125\n",
      "Epoch [7523/20000], Loss: 895.263427734375, Entropy 399.025390625, Learning Rate: 0.00125\n",
      "Epoch [7524/20000], Loss: 877.983642578125, Entropy 392.9733581542969, Learning Rate: 0.00125\n",
      "Epoch [7525/20000], Loss: 856.5684204101562, Entropy 398.72857666015625, Learning Rate: 0.00125\n",
      "Epoch [7526/20000], Loss: 884.7535400390625, Entropy 399.5045166015625, Learning Rate: 0.00125\n",
      "Epoch [7527/20000], Loss: 854.5243530273438, Entropy 390.63970947265625, Learning Rate: 0.00125\n",
      "Epoch [7528/20000], Loss: 868.0155639648438, Entropy 384.40765380859375, Learning Rate: 0.00125\n",
      "Epoch [7529/20000], Loss: 943.37353515625, Entropy 387.06005859375, Learning Rate: 0.00125\n",
      "Epoch [7530/20000], Loss: 855.69287109375, Entropy 398.2878723144531, Learning Rate: 0.00125\n",
      "Epoch [7531/20000], Loss: 942.004150390625, Entropy 360.1421813964844, Learning Rate: 0.00125\n",
      "Epoch [7532/20000], Loss: 859.7921142578125, Entropy 400.8609313964844, Learning Rate: 0.00125\n",
      "Epoch [7533/20000], Loss: 939.2655639648438, Entropy 396.66546630859375, Learning Rate: 0.00125\n",
      "Epoch [7534/20000], Loss: 908.8707885742188, Entropy 407.46575927734375, Learning Rate: 0.00125\n",
      "Epoch [7535/20000], Loss: 888.3818359375, Entropy 391.4556579589844, Learning Rate: 0.00125\n",
      "Epoch [7536/20000], Loss: 916.6688232421875, Entropy 390.0305480957031, Learning Rate: 0.00125\n",
      "Epoch [7537/20000], Loss: 912.1093139648438, Entropy 394.77239990234375, Learning Rate: 0.00125\n",
      "Epoch [7538/20000], Loss: 909.1192626953125, Entropy 405.9068908691406, Learning Rate: 0.00125\n",
      "Epoch [7539/20000], Loss: 901.3770141601562, Entropy 382.79876708984375, Learning Rate: 0.00125\n",
      "Epoch [7540/20000], Loss: 898.8736572265625, Entropy 390.6418151855469, Learning Rate: 0.00125\n",
      "Epoch [7541/20000], Loss: 888.84326171875, Entropy 380.1528015136719, Learning Rate: 0.00125\n",
      "Epoch [7542/20000], Loss: 892.922119140625, Entropy 392.4068603515625, Learning Rate: 0.00125\n",
      "Epoch [7543/20000], Loss: 825.2203369140625, Entropy 392.5358581542969, Learning Rate: 0.00125\n",
      "Epoch [7544/20000], Loss: 913.9595947265625, Entropy 402.4513854980469, Learning Rate: 0.00125\n",
      "Epoch [7545/20000], Loss: 875.1390380859375, Entropy 412.4602966308594, Learning Rate: 0.00125\n",
      "Epoch [7546/20000], Loss: 898.167724609375, Entropy 406.1250305175781, Learning Rate: 0.00125\n",
      "Epoch [7547/20000], Loss: 903.6611328125, Entropy 391.5076599121094, Learning Rate: 0.00125\n",
      "Epoch [7548/20000], Loss: 877.441162109375, Entropy 403.967529296875, Learning Rate: 0.00125\n",
      "Epoch [7549/20000], Loss: 937.3959350585938, Entropy 372.70892333984375, Learning Rate: 0.00125\n",
      "Epoch [7550/20000], Loss: 934.0203247070312, Entropy 399.00421142578125, Learning Rate: 0.00125\n",
      "Epoch [7551/20000], Loss: 852.9495849609375, Entropy 406.9047546386719, Learning Rate: 0.00125\n",
      "Epoch [7552/20000], Loss: 906.5313720703125, Entropy 398.1724853515625, Learning Rate: 0.00125\n",
      "Epoch [7553/20000], Loss: 949.3219604492188, Entropy 400.55267333984375, Learning Rate: 0.00125\n",
      "Epoch [7554/20000], Loss: 913.1680908203125, Entropy 389.68115234375, Learning Rate: 0.00125\n",
      "Epoch [7555/20000], Loss: 903.8486328125, Entropy 387.2820739746094, Learning Rate: 0.00125\n",
      "Epoch [7556/20000], Loss: 911.1026611328125, Entropy 382.501953125, Learning Rate: 0.00125\n",
      "Epoch [7557/20000], Loss: 883.2720336914062, Entropy 397.28521728515625, Learning Rate: 0.00125\n",
      "Epoch [7558/20000], Loss: 918.579833984375, Entropy 389.8603515625, Learning Rate: 0.00125\n",
      "Epoch [7559/20000], Loss: 906.65673828125, Entropy 390.94873046875, Learning Rate: 0.00125\n",
      "Epoch [7560/20000], Loss: 864.9384155273438, Entropy 406.46710205078125, Learning Rate: 0.00125\n",
      "Epoch [7561/20000], Loss: 890.7874755859375, Entropy 409.0506286621094, Learning Rate: 0.00125\n",
      "Epoch [7562/20000], Loss: 887.591064453125, Entropy 380.6516418457031, Learning Rate: 0.00125\n",
      "Epoch [7563/20000], Loss: 902.579345703125, Entropy 395.5592956542969, Learning Rate: 0.00125\n",
      "Epoch [7564/20000], Loss: 880.0906372070312, Entropy 385.44183349609375, Learning Rate: 0.00125\n",
      "Epoch [7565/20000], Loss: 895.5897827148438, Entropy 384.49334716796875, Learning Rate: 0.00125\n",
      "Epoch [7566/20000], Loss: 908.2564697265625, Entropy 411.1528015136719, Learning Rate: 0.00125\n",
      "Epoch [7567/20000], Loss: 909.262939453125, Entropy 406.5129089355469, Learning Rate: 0.00125\n",
      "Epoch [7568/20000], Loss: 923.6150512695312, Entropy 369.71710205078125, Learning Rate: 0.00125\n",
      "Epoch [7569/20000], Loss: 905.0517578125, Entropy 384.5533447265625, Learning Rate: 0.00125\n",
      "Epoch [7570/20000], Loss: 919.4664306640625, Entropy 391.7228698730469, Learning Rate: 0.00125\n",
      "Epoch [7571/20000], Loss: 923.066162109375, Entropy 397.5679931640625, Learning Rate: 0.00125\n",
      "Epoch [7572/20000], Loss: 939.467041015625, Entropy 398.1825256347656, Learning Rate: 0.00125\n",
      "Epoch [7573/20000], Loss: 876.8306884765625, Entropy 399.3028869628906, Learning Rate: 0.00125\n",
      "Epoch [7574/20000], Loss: 847.1983032226562, Entropy 414.19915771484375, Learning Rate: 0.00125\n",
      "Epoch [7575/20000], Loss: 916.9822998046875, Entropy 404.1484375, Learning Rate: 0.00125\n",
      "Epoch [7576/20000], Loss: 859.95263671875, Entropy 411.3435974121094, Learning Rate: 0.00125\n",
      "Epoch [7577/20000], Loss: 891.029052734375, Entropy 381.4996032714844, Learning Rate: 0.00125\n",
      "Epoch [7578/20000], Loss: 908.6932373046875, Entropy 385.0453186035156, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7579/20000], Loss: 883.5965576171875, Entropy 404.0498352050781, Learning Rate: 0.00125\n",
      "Epoch [7580/20000], Loss: 934.1124267578125, Entropy 390.7779235839844, Learning Rate: 0.00125\n",
      "Epoch [7581/20000], Loss: 907.9307861328125, Entropy 394.3344421386719, Learning Rate: 0.00125\n",
      "Epoch [7582/20000], Loss: 918.4752197265625, Entropy 389.7613830566406, Learning Rate: 0.00125\n",
      "Epoch [7583/20000], Loss: 911.0180053710938, Entropy 376.14178466796875, Learning Rate: 0.00125\n",
      "Epoch [7584/20000], Loss: 898.1051025390625, Entropy 388.2121887207031, Learning Rate: 0.00125\n",
      "Epoch [7585/20000], Loss: 920.1036376953125, Entropy 392.2900390625, Learning Rate: 0.00125\n",
      "Epoch [7586/20000], Loss: 884.990234375, Entropy 396.4044494628906, Learning Rate: 0.00125\n",
      "Epoch [7587/20000], Loss: 850.93310546875, Entropy 399.5753173828125, Learning Rate: 0.00125\n",
      "Epoch [7588/20000], Loss: 950.8154907226562, Entropy 392.08099365234375, Learning Rate: 0.00125\n",
      "Epoch [7589/20000], Loss: 921.9230346679688, Entropy 398.27728271484375, Learning Rate: 0.00125\n",
      "Epoch [7590/20000], Loss: 886.36572265625, Entropy 404.1639099121094, Learning Rate: 0.00125\n",
      "Epoch [7591/20000], Loss: 895.6666259765625, Entropy 396.1446838378906, Learning Rate: 0.00125\n",
      "Epoch [7592/20000], Loss: 878.9475708007812, Entropy 398.20245361328125, Learning Rate: 0.00125\n",
      "Epoch [7593/20000], Loss: 896.4208984375, Entropy 401.3429260253906, Learning Rate: 0.00125\n",
      "Epoch [7594/20000], Loss: 938.9769897460938, Entropy 396.99859619140625, Learning Rate: 0.00125\n",
      "Epoch [7595/20000], Loss: 869.55712890625, Entropy 400.794921875, Learning Rate: 0.00125\n",
      "Epoch [7596/20000], Loss: 940.0848388671875, Entropy 409.3810729980469, Learning Rate: 0.00125\n",
      "Epoch [7597/20000], Loss: 913.014892578125, Entropy 399.1834411621094, Learning Rate: 0.00125\n",
      "Epoch [7598/20000], Loss: 906.8243408203125, Entropy 398.7451477050781, Learning Rate: 0.00125\n",
      "Epoch [7599/20000], Loss: 893.9478759765625, Entropy 386.8062438964844, Learning Rate: 0.00125\n",
      "Epoch [7600/20000], Loss: 911.24755859375, Entropy 410.4627990722656, Learning Rate: 0.00125\n",
      "Epoch [7601/20000], Loss: 890.7273559570312, Entropy 408.32086181640625, Learning Rate: 0.00125\n",
      "Epoch [7602/20000], Loss: 946.1546630859375, Entropy 397.4170227050781, Learning Rate: 0.00125\n",
      "Epoch [7603/20000], Loss: 891.659423828125, Entropy 401.9309387207031, Learning Rate: 0.00125\n",
      "Epoch [7604/20000], Loss: 932.9893798828125, Entropy 388.63671875, Learning Rate: 0.00125\n",
      "Epoch [7605/20000], Loss: 957.5865478515625, Entropy 395.3331298828125, Learning Rate: 0.00125\n",
      "Epoch [7606/20000], Loss: 889.646484375, Entropy 396.2785339355469, Learning Rate: 0.00125\n",
      "Epoch [7607/20000], Loss: 885.252685546875, Entropy 402.62646484375, Learning Rate: 0.00125\n",
      "Epoch [7608/20000], Loss: 956.412353515625, Entropy 388.5185852050781, Learning Rate: 0.00125\n",
      "Epoch [7609/20000], Loss: 913.1106567382812, Entropy 386.99603271484375, Learning Rate: 0.00125\n",
      "Epoch [7610/20000], Loss: 1004.624755859375, Entropy 388.9588317871094, Learning Rate: 0.00125\n",
      "Epoch [7611/20000], Loss: 915.2107543945312, Entropy 385.25640869140625, Learning Rate: 0.00125\n",
      "Epoch [7612/20000], Loss: 868.926025390625, Entropy 398.5572509765625, Learning Rate: 0.00125\n",
      "Epoch [7613/20000], Loss: 900.1841430664062, Entropy 392.62921142578125, Learning Rate: 0.00125\n",
      "Epoch [7614/20000], Loss: 888.57470703125, Entropy 387.4847106933594, Learning Rate: 0.00125\n",
      "Epoch [7615/20000], Loss: 943.9779663085938, Entropy 403.19671630859375, Learning Rate: 0.00125\n",
      "Epoch [7616/20000], Loss: 979.70751953125, Entropy 389.996826171875, Learning Rate: 0.00125\n",
      "Epoch [7617/20000], Loss: 874.1793212890625, Entropy 397.5457763671875, Learning Rate: 0.00125\n",
      "Epoch [7618/20000], Loss: 907.8370361328125, Entropy 396.9156188964844, Learning Rate: 0.00125\n",
      "Epoch [7619/20000], Loss: 940.9161376953125, Entropy 405.9510192871094, Learning Rate: 0.00125\n",
      "Epoch [7620/20000], Loss: 923.1983032226562, Entropy 387.90631103515625, Learning Rate: 0.00125\n",
      "Epoch [7621/20000], Loss: 927.7574462890625, Entropy 389.3396911621094, Learning Rate: 0.00125\n",
      "Epoch [7622/20000], Loss: 965.5577392578125, Entropy 400.4513244628906, Learning Rate: 0.00125\n",
      "Epoch [7623/20000], Loss: 964.2237548828125, Entropy 407.1770935058594, Learning Rate: 0.00125\n",
      "Epoch [7624/20000], Loss: 934.601318359375, Entropy 386.8188171386719, Learning Rate: 0.00125\n",
      "Epoch [7625/20000], Loss: 977.0526123046875, Entropy 389.7604675292969, Learning Rate: 0.00125\n",
      "Epoch [7626/20000], Loss: 934.307861328125, Entropy 400.5740966796875, Learning Rate: 0.00125\n",
      "Epoch [7627/20000], Loss: 988.260498046875, Entropy 399.033203125, Learning Rate: 0.00125\n",
      "Epoch [7628/20000], Loss: 913.607177734375, Entropy 385.3190612792969, Learning Rate: 0.00125\n",
      "Epoch [7629/20000], Loss: 987.166748046875, Entropy 413.0602722167969, Learning Rate: 0.00125\n",
      "Epoch [7630/20000], Loss: 928.61767578125, Entropy 401.0972900390625, Learning Rate: 0.00125\n",
      "Epoch [7631/20000], Loss: 899.9713134765625, Entropy 401.4795227050781, Learning Rate: 0.00125\n",
      "Epoch [7632/20000], Loss: 896.8652954101562, Entropy 397.45758056640625, Learning Rate: 0.00125\n",
      "Epoch [7633/20000], Loss: 919.4344482421875, Entropy 409.3096008300781, Learning Rate: 0.00125\n",
      "Epoch [7634/20000], Loss: 909.888916015625, Entropy 408.5682067871094, Learning Rate: 0.00125\n",
      "Epoch [7635/20000], Loss: 850.9197998046875, Entropy 412.4245300292969, Learning Rate: 0.00125\n",
      "Epoch [7636/20000], Loss: 848.825927734375, Entropy 407.43994140625, Learning Rate: 0.00125\n",
      "Epoch [7637/20000], Loss: 944.5164794921875, Entropy 395.34912109375, Learning Rate: 0.00125\n",
      "Epoch [7638/20000], Loss: 914.5059814453125, Entropy 399.7151794433594, Learning Rate: 0.00125\n",
      "Epoch [7639/20000], Loss: 894.560302734375, Entropy 399.617919921875, Learning Rate: 0.00125\n",
      "Epoch [7640/20000], Loss: 908.0111083984375, Entropy 413.8127746582031, Learning Rate: 0.00125\n",
      "Epoch [7641/20000], Loss: 863.3490600585938, Entropy 404.30694580078125, Learning Rate: 0.00125\n",
      "Epoch [7642/20000], Loss: 901.051513671875, Entropy 389.7247619628906, Learning Rate: 0.00125\n",
      "Epoch [7643/20000], Loss: 893.130615234375, Entropy 406.6589050292969, Learning Rate: 0.00125\n",
      "Epoch [7644/20000], Loss: 899.0755615234375, Entropy 393.3292236328125, Learning Rate: 0.00125\n",
      "Epoch [7645/20000], Loss: 862.662841796875, Entropy 391.2759704589844, Learning Rate: 0.00125\n",
      "Epoch [7646/20000], Loss: 880.6038818359375, Entropy 405.8835754394531, Learning Rate: 0.00125\n",
      "Epoch [7647/20000], Loss: 896.5750732421875, Entropy 395.7242736816406, Learning Rate: 0.00125\n",
      "Epoch [7648/20000], Loss: 890.4132080078125, Entropy 395.6509704589844, Learning Rate: 0.00125\n",
      "Epoch [7649/20000], Loss: 971.0902709960938, Entropy 395.33233642578125, Learning Rate: 0.00125\n",
      "Epoch [7650/20000], Loss: 947.36669921875, Entropy 396.5187683105469, Learning Rate: 0.00125\n",
      "Epoch [7651/20000], Loss: 936.6563720703125, Entropy 397.1929626464844, Learning Rate: 0.00125\n",
      "Epoch [7652/20000], Loss: 873.7493286132812, Entropy 401.00628662109375, Learning Rate: 0.00125\n",
      "Epoch [7653/20000], Loss: 919.48486328125, Entropy 406.3819885253906, Learning Rate: 0.00125\n",
      "Epoch [7654/20000], Loss: 923.9552001953125, Entropy 393.6838073730469, Learning Rate: 0.00125\n",
      "Epoch [7655/20000], Loss: 884.826416015625, Entropy 406.0278015136719, Learning Rate: 0.00125\n",
      "Epoch [7656/20000], Loss: 928.806640625, Entropy 389.5305480957031, Learning Rate: 0.00125\n",
      "Epoch [7657/20000], Loss: 934.0123901367188, Entropy 383.53033447265625, Learning Rate: 0.00125\n",
      "Epoch [7658/20000], Loss: 903.2371826171875, Entropy 399.0741271972656, Learning Rate: 0.00125\n",
      "Epoch [7659/20000], Loss: 890.12255859375, Entropy 390.16943359375, Learning Rate: 0.00125\n",
      "Epoch [7660/20000], Loss: 919.4241943359375, Entropy 409.3207702636719, Learning Rate: 0.00125\n",
      "Epoch [7661/20000], Loss: 885.44091796875, Entropy 407.6427917480469, Learning Rate: 0.00125\n",
      "Epoch [7662/20000], Loss: 962.3165283203125, Entropy 388.7077941894531, Learning Rate: 0.00125\n",
      "Epoch [7663/20000], Loss: 958.871337890625, Entropy 398.1579895019531, Learning Rate: 0.00125\n",
      "Epoch [7664/20000], Loss: 900.327392578125, Entropy 385.7460021972656, Learning Rate: 0.00125\n",
      "Epoch [7665/20000], Loss: 924.399658203125, Entropy 395.8941955566406, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7666/20000], Loss: 899.8072509765625, Entropy 399.7384338378906, Learning Rate: 0.00125\n",
      "Epoch [7667/20000], Loss: 962.002685546875, Entropy 398.7071838378906, Learning Rate: 0.00125\n",
      "Epoch [7668/20000], Loss: 902.5317993164062, Entropy 397.59320068359375, Learning Rate: 0.00125\n",
      "Epoch [7669/20000], Loss: 927.851318359375, Entropy 394.3768005371094, Learning Rate: 0.00125\n",
      "Epoch [7670/20000], Loss: 858.4840698242188, Entropy 410.78265380859375, Learning Rate: 0.00125\n",
      "Epoch [7671/20000], Loss: 879.9140625, Entropy 402.194091796875, Learning Rate: 0.00125\n",
      "Epoch [7672/20000], Loss: 856.2340087890625, Entropy 403.7552185058594, Learning Rate: 0.00125\n",
      "Epoch [7673/20000], Loss: 918.9205322265625, Entropy 402.4248352050781, Learning Rate: 0.00125\n",
      "Epoch [7674/20000], Loss: 873.144287109375, Entropy 407.7578125, Learning Rate: 0.00125\n",
      "Epoch [7675/20000], Loss: 968.510498046875, Entropy 397.5134582519531, Learning Rate: 0.00125\n",
      "Epoch [7676/20000], Loss: 857.14990234375, Entropy 416.2002258300781, Learning Rate: 0.00125\n",
      "Epoch [7677/20000], Loss: 905.23779296875, Entropy 404.3088073730469, Learning Rate: 0.00125\n",
      "Epoch [7678/20000], Loss: 920.2064819335938, Entropy 386.37225341796875, Learning Rate: 0.00125\n",
      "Epoch [7679/20000], Loss: 891.7259521484375, Entropy 407.6329650878906, Learning Rate: 0.00125\n",
      "Epoch [7680/20000], Loss: 946.2020263671875, Entropy 403.1556701660156, Learning Rate: 0.00125\n",
      "Epoch [7681/20000], Loss: 894.4268188476562, Entropy 399.14007568359375, Learning Rate: 0.00125\n",
      "Epoch [7682/20000], Loss: 900.8473510742188, Entropy 410.84515380859375, Learning Rate: 0.00125\n",
      "Epoch [7683/20000], Loss: 956.0423583984375, Entropy 381.9198303222656, Learning Rate: 0.00125\n",
      "Epoch [7684/20000], Loss: 894.219970703125, Entropy 401.3960876464844, Learning Rate: 0.00125\n",
      "Epoch [7685/20000], Loss: 903.081298828125, Entropy 410.0875549316406, Learning Rate: 0.00125\n",
      "Epoch [7686/20000], Loss: 885.8917236328125, Entropy 390.1492919921875, Learning Rate: 0.00125\n",
      "Epoch [7687/20000], Loss: 894.16845703125, Entropy 405.5168762207031, Learning Rate: 0.00125\n",
      "Epoch [7688/20000], Loss: 866.171875, Entropy 395.3829650878906, Learning Rate: 0.00125\n",
      "Epoch [7689/20000], Loss: 915.6593017578125, Entropy 397.2981872558594, Learning Rate: 0.00125\n",
      "Epoch [7690/20000], Loss: 900.243408203125, Entropy 394.1600036621094, Learning Rate: 0.00125\n",
      "Epoch [7691/20000], Loss: 990.3353271484375, Entropy 387.80224609375, Learning Rate: 0.00125\n",
      "Epoch [7692/20000], Loss: 880.2662353515625, Entropy 384.6535949707031, Learning Rate: 0.00125\n",
      "Epoch [7693/20000], Loss: 998.651123046875, Entropy 387.4146728515625, Learning Rate: 0.00125\n",
      "Epoch [7694/20000], Loss: 914.0250854492188, Entropy 377.86773681640625, Learning Rate: 0.00125\n",
      "Epoch [7695/20000], Loss: 897.527099609375, Entropy 396.46337890625, Learning Rate: 0.00125\n",
      "Epoch [7696/20000], Loss: 911.7587890625, Entropy 384.4254455566406, Learning Rate: 0.00125\n",
      "Epoch [7697/20000], Loss: 949.330810546875, Entropy 401.3757629394531, Learning Rate: 0.00125\n",
      "Epoch [7698/20000], Loss: 931.900634765625, Entropy 393.8578186035156, Learning Rate: 0.00125\n",
      "Epoch [7699/20000], Loss: 916.9415893554688, Entropy 412.66119384765625, Learning Rate: 0.00125\n",
      "Epoch [7700/20000], Loss: 862.8187255859375, Entropy 407.8215026855469, Learning Rate: 0.00125\n",
      "Epoch [7701/20000], Loss: 888.3138427734375, Entropy 404.9808654785156, Learning Rate: 0.00125\n",
      "Epoch [7702/20000], Loss: 893.6021728515625, Entropy 392.2062072753906, Learning Rate: 0.00125\n",
      "Epoch [7703/20000], Loss: 910.0606689453125, Entropy 392.1452331542969, Learning Rate: 0.00125\n",
      "Epoch [7704/20000], Loss: 925.493896484375, Entropy 394.1617736816406, Learning Rate: 0.00125\n",
      "Epoch [7705/20000], Loss: 911.053466796875, Entropy 390.1067199707031, Learning Rate: 0.00125\n",
      "Epoch [7706/20000], Loss: 855.8826904296875, Entropy 418.0930480957031, Learning Rate: 0.00125\n",
      "Epoch [7707/20000], Loss: 899.8775024414062, Entropy 406.66522216796875, Learning Rate: 0.00125\n",
      "Epoch [7708/20000], Loss: 854.7962646484375, Entropy 412.6185302734375, Learning Rate: 0.00125\n",
      "Epoch [7709/20000], Loss: 881.3341064453125, Entropy 402.4635925292969, Learning Rate: 0.00125\n",
      "Epoch [7710/20000], Loss: 934.2054443359375, Entropy 396.0124206542969, Learning Rate: 0.00125\n",
      "Epoch [7711/20000], Loss: 902.6263427734375, Entropy 401.1409606933594, Learning Rate: 0.00125\n",
      "Epoch [7712/20000], Loss: 946.9107666015625, Entropy 388.7950744628906, Learning Rate: 0.00125\n",
      "Epoch [7713/20000], Loss: 854.151611328125, Entropy 408.7766418457031, Learning Rate: 0.00125\n",
      "Epoch [7714/20000], Loss: 978.2738037109375, Entropy 395.01708984375, Learning Rate: 0.00125\n",
      "Epoch [7715/20000], Loss: 978.2049560546875, Entropy 394.7252197265625, Learning Rate: 0.00125\n",
      "Epoch [7716/20000], Loss: 957.6094970703125, Entropy 409.5256652832031, Learning Rate: 0.00125\n",
      "Epoch [7717/20000], Loss: 935.5595703125, Entropy 404.2532043457031, Learning Rate: 0.00125\n",
      "Epoch [7718/20000], Loss: 981.8727416992188, Entropy 388.99237060546875, Learning Rate: 0.00125\n",
      "Epoch [7719/20000], Loss: 894.308837890625, Entropy 409.12744140625, Learning Rate: 0.00125\n",
      "Epoch [7720/20000], Loss: 943.1904296875, Entropy 393.2605285644531, Learning Rate: 0.00125\n",
      "Epoch [7721/20000], Loss: 928.9247436523438, Entropy 392.61773681640625, Learning Rate: 0.00125\n",
      "Epoch [7722/20000], Loss: 928.28271484375, Entropy 391.5233459472656, Learning Rate: 0.00125\n",
      "Epoch [7723/20000], Loss: 891.1382446289062, Entropy 416.49664306640625, Learning Rate: 0.00125\n",
      "Epoch [7724/20000], Loss: 892.8307495117188, Entropy 383.14007568359375, Learning Rate: 0.00125\n",
      "Epoch [7725/20000], Loss: 935.870849609375, Entropy 396.0319519042969, Learning Rate: 0.00125\n",
      "Epoch [7726/20000], Loss: 870.1907958984375, Entropy 416.3604736328125, Learning Rate: 0.00125\n",
      "Epoch [7727/20000], Loss: 935.9887084960938, Entropy 401.61749267578125, Learning Rate: 0.00125\n",
      "Epoch [7728/20000], Loss: 872.552001953125, Entropy 407.3905334472656, Learning Rate: 0.00125\n",
      "Epoch [7729/20000], Loss: 978.7442626953125, Entropy 412.7984313964844, Learning Rate: 0.00125\n",
      "Epoch [7730/20000], Loss: 1010.6491088867188, Entropy 402.75482177734375, Learning Rate: 0.00125\n",
      "Epoch [7731/20000], Loss: 973.4293212890625, Entropy 394.490234375, Learning Rate: 0.00125\n",
      "Epoch [7732/20000], Loss: 1081.320556640625, Entropy 390.7800598144531, Learning Rate: 0.00125\n",
      "Epoch [7733/20000], Loss: 965.0113525390625, Entropy 386.1120300292969, Learning Rate: 0.00125\n",
      "Epoch [7734/20000], Loss: 1042.406005859375, Entropy 408.0287170410156, Learning Rate: 0.00125\n",
      "Epoch [7735/20000], Loss: 982.9876708984375, Entropy 391.9254455566406, Learning Rate: 0.00125\n",
      "Epoch [7736/20000], Loss: 1160.2613525390625, Entropy 382.0910949707031, Learning Rate: 0.00125\n",
      "Epoch [7737/20000], Loss: 915.4750366210938, Entropy 411.02862548828125, Learning Rate: 0.00125\n",
      "Epoch [7738/20000], Loss: 1199.2548828125, Entropy 402.8209228515625, Learning Rate: 0.00125\n",
      "Epoch [7739/20000], Loss: 901.2210693359375, Entropy 401.7415466308594, Learning Rate: 0.00125\n",
      "Epoch [7740/20000], Loss: 1098.88427734375, Entropy 418.32623291015625, Learning Rate: 0.00125\n",
      "Epoch [7741/20000], Loss: 919.9103393554688, Entropy 400.42437744140625, Learning Rate: 0.00125\n",
      "Epoch [7742/20000], Loss: 1254.502685546875, Entropy 409.79888916015625, Learning Rate: 0.00125\n",
      "Epoch [7743/20000], Loss: 1091.88232421875, Entropy 398.28167724609375, Learning Rate: 0.00125\n",
      "Epoch [7744/20000], Loss: 1388.5992431640625, Entropy 395.9022521972656, Learning Rate: 0.00125\n",
      "Epoch [7745/20000], Loss: 1560.955810546875, Entropy 386.1705322265625, Learning Rate: 0.00125\n",
      "Epoch [7746/20000], Loss: 1420.1007080078125, Entropy 419.2026062011719, Learning Rate: 0.00125\n",
      "Epoch [7747/20000], Loss: 1619.9443359375, Entropy 387.24517822265625, Learning Rate: 0.00125\n",
      "Epoch [7748/20000], Loss: 1394.3345947265625, Entropy 397.2493591308594, Learning Rate: 0.00125\n",
      "Epoch [7749/20000], Loss: 1534.370849609375, Entropy 402.21380615234375, Learning Rate: 0.00125\n",
      "Epoch [7750/20000], Loss: 1028.88671875, Entropy 403.82440185546875, Learning Rate: 0.00125\n",
      "Epoch [7751/20000], Loss: 1478.035888671875, Entropy 392.5706481933594, Learning Rate: 0.00125\n",
      "Epoch [7752/20000], Loss: 1085.416748046875, Entropy 384.80194091796875, Learning Rate: 0.00125\n",
      "Epoch [7753/20000], Loss: 1060.9857177734375, Entropy 395.4569396972656, Learning Rate: 0.00125\n",
      "Epoch [7754/20000], Loss: 1486.7734375, Entropy 395.0096435546875, Learning Rate: 0.00125\n",
      "Epoch [7755/20000], Loss: 1531.00634765625, Entropy 382.7443542480469, Learning Rate: 0.00125\n",
      "Epoch [7756/20000], Loss: 1065.87109375, Entropy 385.6976013183594, Learning Rate: 0.00125\n",
      "Epoch [7757/20000], Loss: 1035.197265625, Entropy 372.2985534667969, Learning Rate: 0.00125\n",
      "Epoch [7758/20000], Loss: 1463.463623046875, Entropy 383.8044128417969, Learning Rate: 0.00125\n",
      "Epoch [7759/20000], Loss: 902.1210327148438, Entropy 391.14874267578125, Learning Rate: 0.00125\n",
      "Epoch [7760/20000], Loss: 1371.9486083984375, Entropy 384.5499267578125, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7761/20000], Loss: 986.777587890625, Entropy 405.55908203125, Learning Rate: 0.00125\n",
      "Epoch [7762/20000], Loss: 1188.879638671875, Entropy 377.0810546875, Learning Rate: 0.00125\n",
      "Epoch [7763/20000], Loss: 1248.7578125, Entropy 376.6586608886719, Learning Rate: 0.00125\n",
      "Epoch [7764/20000], Loss: 1194.66796875, Entropy 374.60980224609375, Learning Rate: 0.00125\n",
      "Epoch [7765/20000], Loss: 1035.677001953125, Entropy 395.7176818847656, Learning Rate: 0.00125\n",
      "Epoch [7766/20000], Loss: 992.34033203125, Entropy 384.7434997558594, Learning Rate: 0.00125\n",
      "Epoch [7767/20000], Loss: 1229.4935302734375, Entropy 388.2857360839844, Learning Rate: 0.00125\n",
      "Epoch [7768/20000], Loss: 1010.7432861328125, Entropy 393.7171325683594, Learning Rate: 0.00125\n",
      "Epoch [7769/20000], Loss: 1071.167724609375, Entropy 378.79913330078125, Learning Rate: 0.00125\n",
      "Epoch [7770/20000], Loss: 882.6405029296875, Entropy 391.9704895019531, Learning Rate: 0.00125\n",
      "Epoch [7771/20000], Loss: 1017.3914794921875, Entropy 379.9122314453125, Learning Rate: 0.00125\n",
      "Epoch [7772/20000], Loss: 937.8702392578125, Entropy 374.7391662597656, Learning Rate: 0.00125\n",
      "Epoch [7773/20000], Loss: 949.837158203125, Entropy 376.5060729980469, Learning Rate: 0.00125\n",
      "Epoch [7774/20000], Loss: 976.0035400390625, Entropy 382.3263854980469, Learning Rate: 0.00125\n",
      "Epoch [7775/20000], Loss: 938.75634765625, Entropy 381.7204284667969, Learning Rate: 0.00125\n",
      "Epoch [7776/20000], Loss: 915.935546875, Entropy 365.6441345214844, Learning Rate: 0.00125\n",
      "Epoch [7777/20000], Loss: 997.1091918945312, Entropy 377.77191162109375, Learning Rate: 0.00125\n",
      "Epoch [7778/20000], Loss: 954.8218994140625, Entropy 376.4581604003906, Learning Rate: 0.00125\n",
      "Epoch [7779/20000], Loss: 1037.352294921875, Entropy 385.058349609375, Learning Rate: 0.00125\n",
      "Epoch [7780/20000], Loss: 884.844970703125, Entropy 392.3922424316406, Learning Rate: 0.00125\n",
      "Epoch [7781/20000], Loss: 974.8984375, Entropy 366.5677185058594, Learning Rate: 0.00125\n",
      "Epoch [7782/20000], Loss: 951.0316772460938, Entropy 383.49078369140625, Learning Rate: 0.00125\n",
      "Epoch [7783/20000], Loss: 959.2484130859375, Entropy 360.4406433105469, Learning Rate: 0.00125\n",
      "Epoch [7784/20000], Loss: 1031.6787109375, Entropy 369.11834716796875, Learning Rate: 0.00125\n",
      "Epoch [7785/20000], Loss: 940.6904296875, Entropy 387.6026611328125, Learning Rate: 0.00125\n",
      "Epoch [7786/20000], Loss: 1031.1339111328125, Entropy 381.8337097167969, Learning Rate: 0.00125\n",
      "Epoch [7787/20000], Loss: 1030.9583740234375, Entropy 372.8428955078125, Learning Rate: 0.00125\n",
      "Epoch [7788/20000], Loss: 1010.1377563476562, Entropy 378.45611572265625, Learning Rate: 0.00125\n",
      "Epoch [7789/20000], Loss: 1016.60205078125, Entropy 374.7848205566406, Learning Rate: 0.00125\n",
      "Epoch [7790/20000], Loss: 900.6060791015625, Entropy 389.1593322753906, Learning Rate: 0.00125\n",
      "Epoch [7791/20000], Loss: 1183.1380615234375, Entropy 373.6847839355469, Learning Rate: 0.00125\n",
      "Epoch [7792/20000], Loss: 963.9266967773438, Entropy 360.58453369140625, Learning Rate: 0.00125\n",
      "Epoch [7793/20000], Loss: 1120.548828125, Entropy 367.3043518066406, Learning Rate: 0.00125\n",
      "Epoch [7794/20000], Loss: 921.2921752929688, Entropy 380.19329833984375, Learning Rate: 0.00125\n",
      "Epoch [7795/20000], Loss: 957.3245239257812, Entropy 369.70355224609375, Learning Rate: 0.00125\n",
      "Epoch [7796/20000], Loss: 1141.058837890625, Entropy 386.2083435058594, Learning Rate: 0.00125\n",
      "Epoch [7797/20000], Loss: 948.958251953125, Entropy 361.4071044921875, Learning Rate: 0.00125\n",
      "Epoch [7798/20000], Loss: 1114.625, Entropy 375.01611328125, Learning Rate: 0.00125\n",
      "Epoch [7799/20000], Loss: 974.9324951171875, Entropy 384.8503723144531, Learning Rate: 0.00125\n",
      "Epoch [7800/20000], Loss: 1291.3447265625, Entropy 379.75531005859375, Learning Rate: 0.00125\n",
      "Epoch [7801/20000], Loss: 1108.0133056640625, Entropy 382.2557373046875, Learning Rate: 0.00125\n",
      "Epoch [7802/20000], Loss: 1188.431884765625, Entropy 369.6883239746094, Learning Rate: 0.00125\n",
      "Epoch [7803/20000], Loss: 942.92578125, Entropy 388.3338928222656, Learning Rate: 0.00125\n",
      "Epoch [7804/20000], Loss: 1090.0872802734375, Entropy 373.2145690917969, Learning Rate: 0.00125\n",
      "Epoch [7805/20000], Loss: 1038.6810302734375, Entropy 374.7650451660156, Learning Rate: 0.00125\n",
      "Epoch [7806/20000], Loss: 1130.2398681640625, Entropy 360.72412109375, Learning Rate: 0.00125\n",
      "Epoch [7807/20000], Loss: 953.1546630859375, Entropy 362.2550964355469, Learning Rate: 0.00125\n",
      "Epoch [7808/20000], Loss: 881.386962890625, Entropy 381.970703125, Learning Rate: 0.00125\n",
      "Epoch [7809/20000], Loss: 934.7880859375, Entropy 383.4777526855469, Learning Rate: 0.00125\n",
      "Epoch [7810/20000], Loss: 994.5325927734375, Entropy 385.76708984375, Learning Rate: 0.00125\n",
      "Epoch [7811/20000], Loss: 945.6915283203125, Entropy 368.84716796875, Learning Rate: 0.00125\n",
      "Epoch [7812/20000], Loss: 1184.7716064453125, Entropy 385.8842468261719, Learning Rate: 0.00125\n",
      "Epoch [7813/20000], Loss: 895.7147827148438, Entropy 376.02935791015625, Learning Rate: 0.00125\n",
      "Epoch [7814/20000], Loss: 1064.93017578125, Entropy 372.00006103515625, Learning Rate: 0.00125\n",
      "Epoch [7815/20000], Loss: 976.857177734375, Entropy 370.965576171875, Learning Rate: 0.00125\n",
      "Epoch [7816/20000], Loss: 959.630859375, Entropy 392.1286315917969, Learning Rate: 0.00125\n",
      "Epoch [7817/20000], Loss: 1024.09423828125, Entropy 376.7975769042969, Learning Rate: 0.00125\n",
      "Epoch [7818/20000], Loss: 966.9691162109375, Entropy 389.3786315917969, Learning Rate: 0.00125\n",
      "Epoch [7819/20000], Loss: 982.7586669921875, Entropy 368.3495178222656, Learning Rate: 0.00125\n",
      "Epoch [7820/20000], Loss: 1037.31201171875, Entropy 369.0604553222656, Learning Rate: 0.00125\n",
      "Epoch [7821/20000], Loss: 999.3336181640625, Entropy 380.8359375, Learning Rate: 0.00125\n",
      "Epoch [7822/20000], Loss: 959.9501342773438, Entropy 374.22515869140625, Learning Rate: 0.00125\n",
      "Epoch [7823/20000], Loss: 910.142333984375, Entropy 371.9318542480469, Learning Rate: 0.00125\n",
      "Epoch [7824/20000], Loss: 1040.9998779296875, Entropy 373.6866760253906, Learning Rate: 0.00125\n",
      "Epoch [7825/20000], Loss: 876.1428833007812, Entropy 374.75335693359375, Learning Rate: 0.00125\n",
      "Epoch [7826/20000], Loss: 959.5870361328125, Entropy 382.0536193847656, Learning Rate: 0.00125\n",
      "Epoch [7827/20000], Loss: 961.7894897460938, Entropy 369.58123779296875, Learning Rate: 0.00125\n",
      "Epoch [7828/20000], Loss: 941.675048828125, Entropy 380.3483581542969, Learning Rate: 0.00125\n",
      "Epoch [7829/20000], Loss: 1007.534423828125, Entropy 369.3890686035156, Learning Rate: 0.00125\n",
      "Epoch [7830/20000], Loss: 895.9705810546875, Entropy 375.21044921875, Learning Rate: 0.00125\n",
      "Epoch [7831/20000], Loss: 902.9653930664062, Entropy 375.35015869140625, Learning Rate: 0.00125\n",
      "Epoch [7832/20000], Loss: 925.3109130859375, Entropy 381.4514465332031, Learning Rate: 0.00125\n",
      "Epoch [7833/20000], Loss: 935.650390625, Entropy 375.6441345214844, Learning Rate: 0.00125\n",
      "Epoch [7834/20000], Loss: 867.4851684570312, Entropy 377.30621337890625, Learning Rate: 0.00125\n",
      "Epoch [7835/20000], Loss: 976.6995849609375, Entropy 390.9063720703125, Learning Rate: 0.00125\n",
      "Epoch [7836/20000], Loss: 926.860107421875, Entropy 365.6886291503906, Learning Rate: 0.00125\n",
      "Epoch [7837/20000], Loss: 948.8382568359375, Entropy 385.7195129394531, Learning Rate: 0.00125\n",
      "Epoch [7838/20000], Loss: 951.4453125, Entropy 346.91552734375, Learning Rate: 0.00125\n",
      "Epoch [7839/20000], Loss: 933.375244140625, Entropy 378.6263732910156, Learning Rate: 0.00125\n",
      "Epoch [7840/20000], Loss: 924.20849609375, Entropy 375.1109619140625, Learning Rate: 0.00125\n",
      "Epoch [7841/20000], Loss: 987.6593017578125, Entropy 368.2415466308594, Learning Rate: 0.00125\n",
      "Epoch [7842/20000], Loss: 893.8155517578125, Entropy 375.2577209472656, Learning Rate: 0.00125\n",
      "Epoch [7843/20000], Loss: 883.789306640625, Entropy 387.0763244628906, Learning Rate: 0.00125\n",
      "Epoch [7844/20000], Loss: 993.0897216796875, Entropy 354.1140441894531, Learning Rate: 0.00125\n",
      "Epoch [7845/20000], Loss: 958.7435302734375, Entropy 362.9803466796875, Learning Rate: 0.00125\n",
      "Epoch [7846/20000], Loss: 948.0032958984375, Entropy 378.63671875, Learning Rate: 0.00125\n",
      "Epoch [7847/20000], Loss: 903.0355224609375, Entropy 375.6461486816406, Learning Rate: 0.00125\n",
      "Epoch [7848/20000], Loss: 917.8101806640625, Entropy 369.6866455078125, Learning Rate: 0.00125\n",
      "Epoch [7849/20000], Loss: 907.7127685546875, Entropy 374.9568176269531, Learning Rate: 0.00125\n",
      "Epoch [7850/20000], Loss: 896.12158203125, Entropy 373.8513488769531, Learning Rate: 0.00125\n",
      "Epoch [7851/20000], Loss: 942.7620849609375, Entropy 375.835693359375, Learning Rate: 0.00125\n",
      "Epoch [7852/20000], Loss: 915.2286376953125, Entropy 359.4947509765625, Learning Rate: 0.00125\n",
      "Epoch [7853/20000], Loss: 946.3402709960938, Entropy 358.44781494140625, Learning Rate: 0.00125\n",
      "Epoch [7854/20000], Loss: 894.27685546875, Entropy 376.1708068847656, Learning Rate: 0.00125\n",
      "Epoch [7855/20000], Loss: 971.97021484375, Entropy 363.4128112792969, Learning Rate: 0.00125\n",
      "Epoch [7856/20000], Loss: 987.06396484375, Entropy 364.6448669433594, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7857/20000], Loss: 947.006591796875, Entropy 369.6385192871094, Learning Rate: 0.00125\n",
      "Epoch [7858/20000], Loss: 934.5594482421875, Entropy 362.6636657714844, Learning Rate: 0.00125\n",
      "Epoch [7859/20000], Loss: 928.029052734375, Entropy 377.3520202636719, Learning Rate: 0.00125\n",
      "Epoch [7860/20000], Loss: 916.44775390625, Entropy 375.1496887207031, Learning Rate: 0.00125\n",
      "Epoch [7861/20000], Loss: 932.5098266601562, Entropy 380.19451904296875, Learning Rate: 0.00125\n",
      "Epoch [7862/20000], Loss: 898.9886474609375, Entropy 369.9887390136719, Learning Rate: 0.00125\n",
      "Epoch [7863/20000], Loss: 919.022216796875, Entropy 372.0067138671875, Learning Rate: 0.00125\n",
      "Epoch [7864/20000], Loss: 905.5589599609375, Entropy 374.3369140625, Learning Rate: 0.00125\n",
      "Epoch [7865/20000], Loss: 998.6961669921875, Entropy 375.4072570800781, Learning Rate: 0.00125\n",
      "Epoch [7866/20000], Loss: 892.7909545898438, Entropy 374.76776123046875, Learning Rate: 0.00125\n",
      "Epoch [7867/20000], Loss: 924.8580322265625, Entropy 349.9022521972656, Learning Rate: 0.00125\n",
      "Epoch [7868/20000], Loss: 868.7317504882812, Entropy 389.46295166015625, Learning Rate: 0.00125\n",
      "Epoch [7869/20000], Loss: 875.1962890625, Entropy 363.0506896972656, Learning Rate: 0.00125\n",
      "Epoch [7870/20000], Loss: 903.6771240234375, Entropy 379.9359436035156, Learning Rate: 0.00125\n",
      "Epoch [7871/20000], Loss: 960.68505859375, Entropy 372.9181823730469, Learning Rate: 0.00125\n",
      "Epoch [7872/20000], Loss: 920.9111328125, Entropy 379.8078308105469, Learning Rate: 0.00125\n",
      "Epoch [7873/20000], Loss: 916.1297607421875, Entropy 379.39404296875, Learning Rate: 0.00125\n",
      "Epoch [7874/20000], Loss: 905.151123046875, Entropy 394.7523498535156, Learning Rate: 0.00125\n",
      "Epoch [7875/20000], Loss: 896.20751953125, Entropy 386.1890563964844, Learning Rate: 0.00125\n",
      "Epoch [7876/20000], Loss: 921.4857177734375, Entropy 367.2742004394531, Learning Rate: 0.00125\n",
      "Epoch [7877/20000], Loss: 891.052490234375, Entropy 383.50390625, Learning Rate: 0.00125\n",
      "Epoch [7878/20000], Loss: 932.4085693359375, Entropy 375.5287170410156, Learning Rate: 0.00125\n",
      "Epoch [7879/20000], Loss: 932.7969970703125, Entropy 367.8182678222656, Learning Rate: 0.00125\n",
      "Epoch [7880/20000], Loss: 876.9878540039062, Entropy 382.22796630859375, Learning Rate: 0.00125\n",
      "Epoch [7881/20000], Loss: 862.3155517578125, Entropy 391.7451477050781, Learning Rate: 0.00125\n",
      "Epoch [7882/20000], Loss: 904.9251098632812, Entropy 364.42303466796875, Learning Rate: 0.00125\n",
      "Epoch [7883/20000], Loss: 937.6922607421875, Entropy 383.112060546875, Learning Rate: 0.00125\n",
      "Epoch [7884/20000], Loss: 912.490234375, Entropy 380.2730407714844, Learning Rate: 0.00125\n",
      "Epoch [7885/20000], Loss: 924.9058227539062, Entropy 377.49212646484375, Learning Rate: 0.00125\n",
      "Epoch [7886/20000], Loss: 917.7144775390625, Entropy 379.0401916503906, Learning Rate: 0.00125\n",
      "Epoch [7887/20000], Loss: 942.4078369140625, Entropy 366.2311706542969, Learning Rate: 0.00125\n",
      "Epoch [7888/20000], Loss: 890.506591796875, Entropy 382.7607727050781, Learning Rate: 0.00125\n",
      "Epoch [7889/20000], Loss: 862.0248413085938, Entropy 386.03021240234375, Learning Rate: 0.00125\n",
      "Epoch [7890/20000], Loss: 929.172119140625, Entropy 364.7406921386719, Learning Rate: 0.00125\n",
      "Epoch [7891/20000], Loss: 926.1929321289062, Entropy 375.25738525390625, Learning Rate: 0.00125\n",
      "Epoch [7892/20000], Loss: 902.0792846679688, Entropy 371.71820068359375, Learning Rate: 0.00125\n",
      "Epoch [7893/20000], Loss: 928.4879150390625, Entropy 371.7320556640625, Learning Rate: 0.00125\n",
      "Epoch [7894/20000], Loss: 954.587890625, Entropy 369.2967224121094, Learning Rate: 0.00125\n",
      "Epoch [7895/20000], Loss: 920.9541625976562, Entropy 381.99700927734375, Learning Rate: 0.00125\n",
      "Epoch [7896/20000], Loss: 892.2569580078125, Entropy 378.0142822265625, Learning Rate: 0.00125\n",
      "Epoch [7897/20000], Loss: 894.6857299804688, Entropy 369.59466552734375, Learning Rate: 0.00125\n",
      "Epoch [7898/20000], Loss: 874.6348876953125, Entropy 388.7752380371094, Learning Rate: 0.00125\n",
      "Epoch [7899/20000], Loss: 882.870849609375, Entropy 374.287109375, Learning Rate: 0.00125\n",
      "Epoch [7900/20000], Loss: 953.5111083984375, Entropy 366.1313171386719, Learning Rate: 0.00125\n",
      "Epoch [7901/20000], Loss: 887.08154296875, Entropy 375.1954650878906, Learning Rate: 0.00125\n",
      "Epoch [7902/20000], Loss: 942.7691650390625, Entropy 377.68115234375, Learning Rate: 0.00125\n",
      "Epoch [7903/20000], Loss: 913.352294921875, Entropy 363.5627136230469, Learning Rate: 0.00125\n",
      "Epoch [7904/20000], Loss: 977.23193359375, Entropy 381.2008056640625, Learning Rate: 0.00125\n",
      "Epoch [7905/20000], Loss: 913.6046142578125, Entropy 378.2084655761719, Learning Rate: 0.00125\n",
      "Epoch [7906/20000], Loss: 960.3299560546875, Entropy 373.6920471191406, Learning Rate: 0.00125\n",
      "Epoch [7907/20000], Loss: 918.758544921875, Entropy 370.1188049316406, Learning Rate: 0.00125\n",
      "Epoch [7908/20000], Loss: 886.15380859375, Entropy 394.776123046875, Learning Rate: 0.00125\n",
      "Epoch [7909/20000], Loss: 869.303955078125, Entropy 386.8037414550781, Learning Rate: 0.00125\n",
      "Epoch [7910/20000], Loss: 866.0914306640625, Entropy 392.1085205078125, Learning Rate: 0.00125\n",
      "Epoch [7911/20000], Loss: 922.79833984375, Entropy 366.9796142578125, Learning Rate: 0.00125\n",
      "Epoch [7912/20000], Loss: 959.9998779296875, Entropy 367.3113708496094, Learning Rate: 0.00125\n",
      "Epoch [7913/20000], Loss: 909.469482421875, Entropy 382.8140563964844, Learning Rate: 0.00125\n",
      "Epoch [7914/20000], Loss: 946.968017578125, Entropy 377.9825744628906, Learning Rate: 0.00125\n",
      "Epoch [7915/20000], Loss: 921.015380859375, Entropy 383.4045715332031, Learning Rate: 0.00125\n",
      "Epoch [7916/20000], Loss: 933.3041381835938, Entropy 384.33453369140625, Learning Rate: 0.00125\n",
      "Epoch [7917/20000], Loss: 926.373779296875, Entropy 376.0275573730469, Learning Rate: 0.00125\n",
      "Epoch [7918/20000], Loss: 919.5493774414062, Entropy 376.92572021484375, Learning Rate: 0.00125\n",
      "Epoch [7919/20000], Loss: 898.538818359375, Entropy 384.0701904296875, Learning Rate: 0.00125\n",
      "Epoch [7920/20000], Loss: 901.4732666015625, Entropy 376.7707824707031, Learning Rate: 0.00125\n",
      "Epoch [7921/20000], Loss: 931.0478515625, Entropy 373.7082214355469, Learning Rate: 0.00125\n",
      "Epoch [7922/20000], Loss: 909.920166015625, Entropy 382.9025573730469, Learning Rate: 0.00125\n",
      "Epoch [7923/20000], Loss: 911.480712890625, Entropy 385.5395202636719, Learning Rate: 0.00125\n",
      "Epoch [7924/20000], Loss: 859.93310546875, Entropy 372.5755920410156, Learning Rate: 0.00125\n",
      "Epoch [7925/20000], Loss: 916.7449951171875, Entropy 380.4148254394531, Learning Rate: 0.00125\n",
      "Epoch [7926/20000], Loss: 868.2006225585938, Entropy 395.40045166015625, Learning Rate: 0.00125\n",
      "Epoch [7927/20000], Loss: 915.1624755859375, Entropy 359.95654296875, Learning Rate: 0.00125\n",
      "Epoch [7928/20000], Loss: 837.842041015625, Entropy 395.54052734375, Learning Rate: 0.00125\n",
      "Epoch [7929/20000], Loss: 896.4891357421875, Entropy 381.5574035644531, Learning Rate: 0.00125\n",
      "Epoch [7930/20000], Loss: 918.8616943359375, Entropy 367.7662353515625, Learning Rate: 0.00125\n",
      "Epoch [7931/20000], Loss: 938.0465698242188, Entropy 379.17718505859375, Learning Rate: 0.00125\n",
      "Epoch [7932/20000], Loss: 881.3524780273438, Entropy 385.38507080078125, Learning Rate: 0.00125\n",
      "Epoch [7933/20000], Loss: 926.8900756835938, Entropy 370.43939208984375, Learning Rate: 0.00125\n",
      "Epoch [7934/20000], Loss: 878.4886474609375, Entropy 385.1734313964844, Learning Rate: 0.00125\n",
      "Epoch [7935/20000], Loss: 870.12255859375, Entropy 370.7558898925781, Learning Rate: 0.00125\n",
      "Epoch [7936/20000], Loss: 871.997802734375, Entropy 388.7978820800781, Learning Rate: 0.00125\n",
      "Epoch [7937/20000], Loss: 911.835693359375, Entropy 381.0389099121094, Learning Rate: 0.00125\n",
      "Epoch [7938/20000], Loss: 914.6666259765625, Entropy 389.2563171386719, Learning Rate: 0.00125\n",
      "Epoch [7939/20000], Loss: 867.3493041992188, Entropy 394.16937255859375, Learning Rate: 0.00125\n",
      "Epoch [7940/20000], Loss: 881.73828125, Entropy 402.279052734375, Learning Rate: 0.00125\n",
      "Epoch [7941/20000], Loss: 874.5783081054688, Entropy 379.93280029296875, Learning Rate: 0.00125\n",
      "Epoch [7942/20000], Loss: 928.3011474609375, Entropy 389.1363220214844, Learning Rate: 0.00125\n",
      "Epoch [7943/20000], Loss: 880.9639282226562, Entropy 376.29888916015625, Learning Rate: 0.00125\n",
      "Epoch [7944/20000], Loss: 868.690185546875, Entropy 386.1084289550781, Learning Rate: 0.00125\n",
      "Epoch [7945/20000], Loss: 897.8424072265625, Entropy 368.311279296875, Learning Rate: 0.00125\n",
      "Epoch [7946/20000], Loss: 893.14404296875, Entropy 363.6366271972656, Learning Rate: 0.00125\n",
      "Epoch [7947/20000], Loss: 870.8373413085938, Entropy 397.55023193359375, Learning Rate: 0.00125\n",
      "Epoch [7948/20000], Loss: 938.8085327148438, Entropy 385.30108642578125, Learning Rate: 0.00125\n",
      "Epoch [7949/20000], Loss: 903.14208984375, Entropy 374.25927734375, Learning Rate: 0.00125\n",
      "Epoch [7950/20000], Loss: 870.0562744140625, Entropy 391.3387451171875, Learning Rate: 0.00125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7951/20000], Loss: 913.1707763671875, Entropy 381.4576416015625, Learning Rate: 0.00125\n",
      "Epoch [7952/20000], Loss: 883.978759765625, Entropy 385.3175048828125, Learning Rate: 0.00125\n",
      "Epoch [7953/20000], Loss: 924.6170043945312, Entropy 385.33404541015625, Learning Rate: 0.00125\n",
      "Epoch [7954/20000], Loss: 884.5000610351562, Entropy 382.91558837890625, Learning Rate: 0.00125\n",
      "Epoch [7955/20000], Loss: 864.8421630859375, Entropy 377.8177185058594, Learning Rate: 0.00125\n",
      "Epoch [7956/20000], Loss: 886.3109741210938, Entropy 378.04608154296875, Learning Rate: 0.00125\n",
      "Epoch [7957/20000], Loss: 942.8170166015625, Entropy 385.4299011230469, Learning Rate: 0.00125\n",
      "Epoch [7958/20000], Loss: 844.19775390625, Entropy 402.294677734375, Learning Rate: 0.00125\n",
      "Epoch [7959/20000], Loss: 917.5980224609375, Entropy 384.394287109375, Learning Rate: 0.00125\n",
      "Epoch [7960/20000], Loss: 888.8782958984375, Entropy 398.1741638183594, Learning Rate: 0.00125\n",
      "Epoch [7961/20000], Loss: 849.2659301757812, Entropy 394.93511962890625, Learning Rate: 0.00125\n",
      "Epoch [7962/20000], Loss: 876.2423095703125, Entropy 389.181640625, Learning Rate: 0.00125\n",
      "Epoch [7963/20000], Loss: 938.0009765625, Entropy 384.1326599121094, Learning Rate: 0.00125\n",
      "Epoch [7964/20000], Loss: 889.8665771484375, Entropy 382.25927734375, Learning Rate: 0.00125\n",
      "Epoch [7965/20000], Loss: 949.8790893554688, Entropy 369.04229736328125, Learning Rate: 0.00125\n",
      "Epoch [7966/20000], Loss: 848.1444091796875, Entropy 378.9836120605469, Learning Rate: 0.00125\n",
      "Epoch [7967/20000], Loss: 888.55419921875, Entropy 382.7014465332031, Learning Rate: 0.00125\n",
      "Epoch [7968/20000], Loss: 890.9312744140625, Entropy 390.8112487792969, Learning Rate: 0.00125\n",
      "Epoch [7969/20000], Loss: 887.614013671875, Entropy 390.2326354980469, Learning Rate: 0.00125\n",
      "Epoch [7970/20000], Loss: 879.4437255859375, Entropy 379.9256896972656, Learning Rate: 0.00125\n",
      "Epoch [7971/20000], Loss: 892.5147094726562, Entropy 382.19134521484375, Learning Rate: 0.00125\n",
      "Epoch [7972/20000], Loss: 874.1256103515625, Entropy 377.0643310546875, Learning Rate: 0.00125\n",
      "Epoch [7973/20000], Loss: 883.4473266601562, Entropy 370.61322021484375, Learning Rate: 0.00125\n",
      "Epoch [7974/20000], Loss: 908.748779296875, Entropy 383.7210998535156, Learning Rate: 0.00125\n",
      "Epoch [7975/20000], Loss: 896.540283203125, Entropy 375.5226135253906, Learning Rate: 0.00125\n",
      "Epoch [7976/20000], Loss: 902.6941528320312, Entropy 372.87200927734375, Learning Rate: 0.00125\n",
      "Epoch [7977/20000], Loss: 852.61474609375, Entropy 394.4752197265625, Learning Rate: 0.00125\n",
      "Epoch [7978/20000], Loss: 944.4930419921875, Entropy 372.7741394042969, Learning Rate: 0.00125\n",
      "Epoch [7979/20000], Loss: 875.232177734375, Entropy 380.6260986328125, Learning Rate: 0.00125\n",
      "Epoch [7980/20000], Loss: 896.1806640625, Entropy 391.0453186035156, Learning Rate: 0.00125\n",
      "Epoch [7981/20000], Loss: 912.7062377929688, Entropy 392.18194580078125, Learning Rate: 0.00125\n",
      "Epoch [7982/20000], Loss: 921.0762329101562, Entropy 373.72100830078125, Learning Rate: 0.00125\n",
      "Epoch [7983/20000], Loss: 921.4495239257812, Entropy 381.68304443359375, Learning Rate: 0.00125\n",
      "Epoch [7984/20000], Loss: 874.7005615234375, Entropy 377.6184997558594, Learning Rate: 0.00125\n",
      "Epoch [7985/20000], Loss: 948.386474609375, Entropy 380.8676452636719, Learning Rate: 0.00125\n",
      "Epoch [7986/20000], Loss: 892.864013671875, Entropy 366.5978088378906, Learning Rate: 0.00125\n",
      "Epoch [7987/20000], Loss: 902.8306274414062, Entropy 375.85955810546875, Learning Rate: 0.00125\n",
      "Epoch [7988/20000], Loss: 874.9063720703125, Entropy 381.5572814941406, Learning Rate: 0.00125\n",
      "Epoch [7989/20000], Loss: 870.0687866210938, Entropy 394.04522705078125, Learning Rate: 0.00125\n",
      "Epoch [7990/20000], Loss: 881.7591552734375, Entropy 395.8468017578125, Learning Rate: 0.00125\n",
      "Epoch [7991/20000], Loss: 933.8375244140625, Entropy 383.0821838378906, Learning Rate: 0.00125\n",
      "Epoch [7992/20000], Loss: 847.8740234375, Entropy 400.9524841308594, Learning Rate: 0.00125\n",
      "Epoch [7993/20000], Loss: 880.81298828125, Entropy 379.8600769042969, Learning Rate: 0.000625\n",
      "Epoch [7994/20000], Loss: 900.99951171875, Entropy 388.7317810058594, Learning Rate: 0.000625\n",
      "Epoch [7995/20000], Loss: 899.647216796875, Entropy 366.8482971191406, Learning Rate: 0.000625\n",
      "Epoch [7996/20000], Loss: 864.7228393554688, Entropy 380.54193115234375, Learning Rate: 0.000625\n",
      "Epoch [7997/20000], Loss: 910.462646484375, Entropy 394.8592529296875, Learning Rate: 0.000625\n",
      "Epoch [7998/20000], Loss: 915.19384765625, Entropy 370.1844482421875, Learning Rate: 0.000625\n",
      "Epoch [7999/20000], Loss: 908.3067626953125, Entropy 372.0566711425781, Learning Rate: 0.000625\n",
      "Epoch [8000/20000], Loss: 880.6646728515625, Entropy 393.1941833496094, Learning Rate: 0.000625\n",
      "Epoch [8001/20000], Loss: 904.8265991210938, Entropy 392.08526611328125, Learning Rate: 0.000625\n",
      "Epoch [8002/20000], Loss: 889.2359008789062, Entropy 387.22357177734375, Learning Rate: 0.000625\n",
      "Epoch [8003/20000], Loss: 862.5234375, Entropy 388.6988220214844, Learning Rate: 0.000625\n",
      "Epoch [8004/20000], Loss: 910.0185546875, Entropy 411.3252258300781, Learning Rate: 0.000625\n",
      "Epoch [8005/20000], Loss: 853.9754638671875, Entropy 399.6239013671875, Learning Rate: 0.000625\n",
      "Epoch [8006/20000], Loss: 881.724853515625, Entropy 387.1311950683594, Learning Rate: 0.000625\n",
      "Epoch [8007/20000], Loss: 921.7239379882812, Entropy 379.69842529296875, Learning Rate: 0.000625\n",
      "Epoch [8008/20000], Loss: 943.987060546875, Entropy 370.8958435058594, Learning Rate: 0.000625\n",
      "Epoch [8009/20000], Loss: 936.392333984375, Entropy 386.5851135253906, Learning Rate: 0.000625\n",
      "Epoch [8010/20000], Loss: 842.0485229492188, Entropy 404.52056884765625, Learning Rate: 0.000625\n",
      "Epoch [8011/20000], Loss: 843.5134887695312, Entropy 393.86273193359375, Learning Rate: 0.000625\n",
      "Epoch [8012/20000], Loss: 856.8113403320312, Entropy 389.83013916015625, Learning Rate: 0.000625\n",
      "Epoch [8013/20000], Loss: 866.4893798828125, Entropy 381.1949157714844, Learning Rate: 0.000625\n",
      "Epoch [8014/20000], Loss: 873.6825561523438, Entropy 391.99761962890625, Learning Rate: 0.000625\n",
      "Epoch [8015/20000], Loss: 878.2202758789062, Entropy 386.45550537109375, Learning Rate: 0.000625\n",
      "Epoch [8016/20000], Loss: 858.2176513671875, Entropy 405.545654296875, Learning Rate: 0.000625\n",
      "Epoch [8017/20000], Loss: 925.7684326171875, Entropy 381.2760925292969, Learning Rate: 0.000625\n",
      "Epoch [8018/20000], Loss: 931.2926635742188, Entropy 379.88421630859375, Learning Rate: 0.000625\n",
      "Epoch [8019/20000], Loss: 896.4139404296875, Entropy 383.4263916015625, Learning Rate: 0.000625\n",
      "Epoch [8020/20000], Loss: 912.99365234375, Entropy 382.8435974121094, Learning Rate: 0.000625\n",
      "Epoch [8021/20000], Loss: 898.13037109375, Entropy 376.60009765625, Learning Rate: 0.000625\n",
      "Epoch [8022/20000], Loss: 869.0445556640625, Entropy 388.96337890625, Learning Rate: 0.000625\n",
      "Epoch [8023/20000], Loss: 910.758056640625, Entropy 378.36669921875, Learning Rate: 0.000625\n",
      "Epoch [8024/20000], Loss: 927.0531616210938, Entropy 378.53070068359375, Learning Rate: 0.000625\n",
      "Epoch [8025/20000], Loss: 859.7861328125, Entropy 394.7747497558594, Learning Rate: 0.000625\n",
      "Epoch [8026/20000], Loss: 859.0360107421875, Entropy 392.7641296386719, Learning Rate: 0.000625\n",
      "Epoch [8027/20000], Loss: 883.9102783203125, Entropy 408.614990234375, Learning Rate: 0.000625\n",
      "Epoch [8028/20000], Loss: 892.6962890625, Entropy 382.0935974121094, Learning Rate: 0.000625\n",
      "Epoch [8029/20000], Loss: 898.8167724609375, Entropy 384.29443359375, Learning Rate: 0.000625\n",
      "Epoch [8030/20000], Loss: 883.547607421875, Entropy 402.4340515136719, Learning Rate: 0.000625\n",
      "Epoch [8031/20000], Loss: 893.1869506835938, Entropy 387.42108154296875, Learning Rate: 0.000625\n",
      "Epoch [8032/20000], Loss: 865.415283203125, Entropy 391.7386779785156, Learning Rate: 0.000625\n",
      "Epoch [8033/20000], Loss: 877.161376953125, Entropy 394.2853088378906, Learning Rate: 0.000625\n",
      "Epoch [8034/20000], Loss: 849.9856567382812, Entropy 387.58697509765625, Learning Rate: 0.000625\n",
      "Epoch [8035/20000], Loss: 965.426025390625, Entropy 382.5080261230469, Learning Rate: 0.000625\n",
      "Epoch [8036/20000], Loss: 928.7334594726562, Entropy 372.22515869140625, Learning Rate: 0.000625\n",
      "Epoch [8037/20000], Loss: 918.35107421875, Entropy 388.6318664550781, Learning Rate: 0.000625\n",
      "Epoch [8038/20000], Loss: 841.9583129882812, Entropy 401.05523681640625, Learning Rate: 0.000625\n",
      "Epoch [8039/20000], Loss: 916.953369140625, Entropy 385.5740051269531, Learning Rate: 0.000625\n",
      "Epoch [8040/20000], Loss: 898.246826171875, Entropy 391.0625, Learning Rate: 0.000625\n",
      "Epoch [8041/20000], Loss: 904.678955078125, Entropy 372.4385070800781, Learning Rate: 0.000625\n",
      "Epoch [8042/20000], Loss: 905.806640625, Entropy 391.6597595214844, Learning Rate: 0.000625\n",
      "Epoch [8043/20000], Loss: 856.626220703125, Entropy 406.00927734375, Learning Rate: 0.000625\n",
      "Epoch [8044/20000], Loss: 853.5126953125, Entropy 383.7629699707031, Learning Rate: 0.000625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8045/20000], Loss: 866.416015625, Entropy 396.3687438964844, Learning Rate: 0.000625\n",
      "Epoch [8046/20000], Loss: 898.0454711914062, Entropy 401.03179931640625, Learning Rate: 0.000625\n",
      "Epoch [8047/20000], Loss: 879.09326171875, Entropy 387.8712463378906, Learning Rate: 0.000625\n",
      "Epoch [8048/20000], Loss: 817.4857788085938, Entropy 381.62945556640625, Learning Rate: 0.000625\n",
      "Epoch [8049/20000], Loss: 916.846435546875, Entropy 391.4357604980469, Learning Rate: 0.000625\n",
      "Epoch [8050/20000], Loss: 879.64453125, Entropy 388.9283447265625, Learning Rate: 0.000625\n",
      "Epoch [8051/20000], Loss: 906.6290283203125, Entropy 386.0927734375, Learning Rate: 0.000625\n",
      "Epoch [8052/20000], Loss: 923.6649169921875, Entropy 370.86181640625, Learning Rate: 0.000625\n",
      "Epoch [8053/20000], Loss: 902.5185546875, Entropy 397.1629638671875, Learning Rate: 0.000625\n",
      "Epoch [8054/20000], Loss: 878.1895751953125, Entropy 378.3938293457031, Learning Rate: 0.000625\n",
      "Epoch [8055/20000], Loss: 869.4721069335938, Entropy 381.45819091796875, Learning Rate: 0.000625\n",
      "Epoch [8056/20000], Loss: 871.8282470703125, Entropy 386.6715393066406, Learning Rate: 0.000625\n",
      "Epoch [8057/20000], Loss: 886.9683837890625, Entropy 387.8205261230469, Learning Rate: 0.000625\n",
      "Epoch [8058/20000], Loss: 885.1356201171875, Entropy 391.2288513183594, Learning Rate: 0.000625\n",
      "Epoch [8059/20000], Loss: 850.708251953125, Entropy 392.9979553222656, Learning Rate: 0.000625\n",
      "Epoch [8060/20000], Loss: 881.4194946289062, Entropy 376.73980712890625, Learning Rate: 0.000625\n",
      "Epoch [8061/20000], Loss: 861.7647705078125, Entropy 387.7865295410156, Learning Rate: 0.000625\n",
      "Epoch [8062/20000], Loss: 893.03857421875, Entropy 383.4998474121094, Learning Rate: 0.000625\n",
      "Epoch [8063/20000], Loss: 910.5660400390625, Entropy 363.7432861328125, Learning Rate: 0.000625\n",
      "Epoch [8064/20000], Loss: 904.5444946289062, Entropy 385.90460205078125, Learning Rate: 0.000625\n",
      "Epoch [8065/20000], Loss: 912.6882934570312, Entropy 390.25201416015625, Learning Rate: 0.000625\n",
      "Epoch [8066/20000], Loss: 870.17041015625, Entropy 408.8699035644531, Learning Rate: 0.000625\n",
      "Epoch [8067/20000], Loss: 901.359375, Entropy 395.388427734375, Learning Rate: 0.000625\n",
      "Epoch [8068/20000], Loss: 911.2872924804688, Entropy 385.82452392578125, Learning Rate: 0.000625\n",
      "Epoch [8069/20000], Loss: 898.8609619140625, Entropy 393.65478515625, Learning Rate: 0.000625\n",
      "Epoch [8070/20000], Loss: 917.3465576171875, Entropy 382.6632080078125, Learning Rate: 0.000625\n",
      "Epoch [8071/20000], Loss: 860.79443359375, Entropy 375.2218017578125, Learning Rate: 0.000625\n",
      "Epoch [8072/20000], Loss: 885.2535400390625, Entropy 390.3739013671875, Learning Rate: 0.000625\n",
      "Epoch [8073/20000], Loss: 878.1148681640625, Entropy 390.0188293457031, Learning Rate: 0.000625\n",
      "Epoch [8074/20000], Loss: 829.0636596679688, Entropy 404.79034423828125, Learning Rate: 0.000625\n",
      "Epoch [8075/20000], Loss: 905.3175048828125, Entropy 407.278076171875, Learning Rate: 0.000625\n",
      "Epoch [8076/20000], Loss: 884.5477294921875, Entropy 389.10302734375, Learning Rate: 0.000625\n",
      "Epoch [8077/20000], Loss: 886.305908203125, Entropy 383.9879455566406, Learning Rate: 0.000625\n",
      "Epoch [8078/20000], Loss: 909.2991333007812, Entropy 379.87017822265625, Learning Rate: 0.000625\n",
      "Epoch [8079/20000], Loss: 852.2601318359375, Entropy 404.5787658691406, Learning Rate: 0.000625\n",
      "Epoch [8080/20000], Loss: 843.0662231445312, Entropy 403.29351806640625, Learning Rate: 0.000625\n",
      "Epoch [8081/20000], Loss: 883.9359130859375, Entropy 395.7374267578125, Learning Rate: 0.000625\n",
      "Epoch [8082/20000], Loss: 876.96240234375, Entropy 400.4653015136719, Learning Rate: 0.000625\n",
      "Epoch [8083/20000], Loss: 870.9031982421875, Entropy 392.8975830078125, Learning Rate: 0.000625\n",
      "Epoch [8084/20000], Loss: 854.101806640625, Entropy 400.0740966796875, Learning Rate: 0.000625\n",
      "Epoch [8085/20000], Loss: 876.7850341796875, Entropy 399.1069030761719, Learning Rate: 0.000625\n",
      "Epoch [8086/20000], Loss: 846.9321899414062, Entropy 398.83599853515625, Learning Rate: 0.000625\n",
      "Epoch [8087/20000], Loss: 863.25830078125, Entropy 388.636474609375, Learning Rate: 0.000625\n",
      "Epoch [8088/20000], Loss: 922.9423828125, Entropy 374.6350402832031, Learning Rate: 0.000625\n",
      "Epoch [8089/20000], Loss: 891.9351806640625, Entropy 377.1753845214844, Learning Rate: 0.000625\n",
      "Epoch [8090/20000], Loss: 868.9326171875, Entropy 384.1371154785156, Learning Rate: 0.000625\n",
      "Epoch [8091/20000], Loss: 879.9551391601562, Entropy 395.86090087890625, Learning Rate: 0.000625\n",
      "Epoch [8092/20000], Loss: 877.55810546875, Entropy 388.8620300292969, Learning Rate: 0.000625\n",
      "Epoch [8093/20000], Loss: 861.8065185546875, Entropy 394.3844299316406, Learning Rate: 0.000625\n",
      "Epoch [8094/20000], Loss: 918.0604248046875, Entropy 399.8630676269531, Learning Rate: 0.000625\n",
      "Epoch [8095/20000], Loss: 860.428466796875, Entropy 403.1925964355469, Learning Rate: 0.000625\n",
      "Epoch [8096/20000], Loss: 909.7239990234375, Entropy 403.4859924316406, Learning Rate: 0.000625\n",
      "Epoch [8097/20000], Loss: 858.660888671875, Entropy 401.5177307128906, Learning Rate: 0.000625\n",
      "Epoch [8098/20000], Loss: 883.4927978515625, Entropy 383.0193786621094, Learning Rate: 0.000625\n",
      "Epoch [8099/20000], Loss: 858.541748046875, Entropy 388.7135009765625, Learning Rate: 0.000625\n",
      "Epoch [8100/20000], Loss: 923.884765625, Entropy 392.2223205566406, Learning Rate: 0.000625\n",
      "Epoch [8101/20000], Loss: 875.075927734375, Entropy 403.4514465332031, Learning Rate: 0.000625\n",
      "Epoch [8102/20000], Loss: 889.7152099609375, Entropy 393.44384765625, Learning Rate: 0.000625\n",
      "Epoch [8103/20000], Loss: 880.1605224609375, Entropy 388.3670654296875, Learning Rate: 0.000625\n",
      "Epoch [8104/20000], Loss: 870.0164794921875, Entropy 395.5018005371094, Learning Rate: 0.000625\n",
      "Epoch [8105/20000], Loss: 920.4672241210938, Entropy 387.99420166015625, Learning Rate: 0.000625\n",
      "Epoch [8106/20000], Loss: 862.6676635742188, Entropy 401.63043212890625, Learning Rate: 0.000625\n",
      "Epoch [8107/20000], Loss: 854.9107666015625, Entropy 399.1210021972656, Learning Rate: 0.000625\n",
      "Epoch [8108/20000], Loss: 850.0665283203125, Entropy 411.4991760253906, Learning Rate: 0.000625\n",
      "Epoch [8109/20000], Loss: 884.6857299804688, Entropy 392.88531494140625, Learning Rate: 0.000625\n",
      "Epoch [8110/20000], Loss: 868.416748046875, Entropy 400.7223205566406, Learning Rate: 0.000625\n",
      "Epoch [8111/20000], Loss: 904.2703247070312, Entropy 399.01617431640625, Learning Rate: 0.000625\n",
      "Epoch [8112/20000], Loss: 872.9173583984375, Entropy 392.671630859375, Learning Rate: 0.000625\n",
      "Epoch [8113/20000], Loss: 838.5906982421875, Entropy 385.521484375, Learning Rate: 0.000625\n",
      "Epoch [8114/20000], Loss: 859.8607177734375, Entropy 400.2264404296875, Learning Rate: 0.000625\n",
      "Epoch [8115/20000], Loss: 890.9722290039062, Entropy 414.36578369140625, Learning Rate: 0.000625\n",
      "Epoch [8116/20000], Loss: 864.9228515625, Entropy 406.9720153808594, Learning Rate: 0.000625\n",
      "Epoch [8117/20000], Loss: 927.8965454101562, Entropy 394.56097412109375, Learning Rate: 0.000625\n",
      "Epoch [8118/20000], Loss: 896.450927734375, Entropy 406.3046875, Learning Rate: 0.000625\n",
      "Epoch [8119/20000], Loss: 891.6131591796875, Entropy 388.9047546386719, Learning Rate: 0.000625\n",
      "Epoch [8120/20000], Loss: 880.48876953125, Entropy 389.8355712890625, Learning Rate: 0.000625\n",
      "Epoch [8121/20000], Loss: 874.7401123046875, Entropy 393.2674865722656, Learning Rate: 0.000625\n",
      "Epoch [8122/20000], Loss: 855.51611328125, Entropy 413.766357421875, Learning Rate: 0.000625\n",
      "Epoch [8123/20000], Loss: 890.8134765625, Entropy 386.6318664550781, Learning Rate: 0.000625\n",
      "Epoch [8124/20000], Loss: 857.181396484375, Entropy 398.296875, Learning Rate: 0.000625\n",
      "Epoch [8125/20000], Loss: 881.6220092773438, Entropy 401.32464599609375, Learning Rate: 0.000625\n",
      "Epoch [8126/20000], Loss: 863.1666259765625, Entropy 400.0406494140625, Learning Rate: 0.000625\n",
      "Epoch [8127/20000], Loss: 932.3237915039062, Entropy 386.93560791015625, Learning Rate: 0.000625\n",
      "Epoch [8128/20000], Loss: 893.2667236328125, Entropy 400.1533203125, Learning Rate: 0.000625\n",
      "Epoch [8129/20000], Loss: 902.7796020507812, Entropy 403.50506591796875, Learning Rate: 0.000625\n",
      "Epoch [8130/20000], Loss: 897.5244140625, Entropy 384.3485412597656, Learning Rate: 0.000625\n",
      "Epoch [8131/20000], Loss: 873.542724609375, Entropy 401.5963439941406, Learning Rate: 0.000625\n",
      "Epoch [8132/20000], Loss: 899.3096923828125, Entropy 399.7373046875, Learning Rate: 0.000625\n",
      "Epoch [8133/20000], Loss: 879.121826171875, Entropy 400.8574523925781, Learning Rate: 0.000625\n",
      "Epoch [8134/20000], Loss: 845.8527221679688, Entropy 388.23040771484375, Learning Rate: 0.000625\n",
      "Epoch [8135/20000], Loss: 834.8226318359375, Entropy 398.1063232421875, Learning Rate: 0.000625\n",
      "Epoch [8136/20000], Loss: 924.20361328125, Entropy 406.70263671875, Learning Rate: 0.000625\n",
      "Epoch [8137/20000], Loss: 856.183349609375, Entropy 410.0016784667969, Learning Rate: 0.000625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8138/20000], Loss: 825.74462890625, Entropy 398.0926208496094, Learning Rate: 0.000625\n",
      "Epoch [8139/20000], Loss: 845.179443359375, Entropy 412.5914001464844, Learning Rate: 0.000625\n",
      "Epoch [8140/20000], Loss: 865.0850219726562, Entropy 394.58489990234375, Learning Rate: 0.000625\n",
      "Epoch [8141/20000], Loss: 906.7317504882812, Entropy 383.06732177734375, Learning Rate: 0.000625\n",
      "Epoch [8142/20000], Loss: 885.1668090820312, Entropy 384.50201416015625, Learning Rate: 0.000625\n",
      "Epoch [8143/20000], Loss: 880.5086059570312, Entropy 393.03118896484375, Learning Rate: 0.000625\n",
      "Epoch [8144/20000], Loss: 932.3350830078125, Entropy 392.5488586425781, Learning Rate: 0.000625\n",
      "Epoch [8145/20000], Loss: 930.8472290039062, Entropy 398.57366943359375, Learning Rate: 0.000625\n",
      "Epoch [8146/20000], Loss: 893.2396240234375, Entropy 391.3674011230469, Learning Rate: 0.000625\n",
      "Epoch [8147/20000], Loss: 899.1710205078125, Entropy 395.7172546386719, Learning Rate: 0.000625\n",
      "Epoch [8148/20000], Loss: 863.8485107421875, Entropy 396.7614440917969, Learning Rate: 0.000625\n",
      "Epoch [8149/20000], Loss: 879.627685546875, Entropy 403.1814880371094, Learning Rate: 0.000625\n",
      "Epoch [8150/20000], Loss: 970.6256103515625, Entropy 391.0727233886719, Learning Rate: 0.000625\n",
      "Epoch [8151/20000], Loss: 906.2069702148438, Entropy 393.51422119140625, Learning Rate: 0.000625\n",
      "Epoch [8152/20000], Loss: 894.480224609375, Entropy 392.3014831542969, Learning Rate: 0.000625\n",
      "Epoch [8153/20000], Loss: 882.9202880859375, Entropy 388.1813659667969, Learning Rate: 0.000625\n",
      "Epoch [8154/20000], Loss: 866.5732421875, Entropy 389.1393737792969, Learning Rate: 0.000625\n",
      "Epoch [8155/20000], Loss: 840.4462890625, Entropy 380.7707824707031, Learning Rate: 0.000625\n",
      "Epoch [8156/20000], Loss: 915.789794921875, Entropy 410.0724182128906, Learning Rate: 0.000625\n",
      "Epoch [8157/20000], Loss: 898.2991943359375, Entropy 386.7160949707031, Learning Rate: 0.000625\n",
      "Epoch [8158/20000], Loss: 870.0487060546875, Entropy 388.3701171875, Learning Rate: 0.000625\n",
      "Epoch [8159/20000], Loss: 921.0448608398438, Entropy 389.32904052734375, Learning Rate: 0.000625\n",
      "Epoch [8160/20000], Loss: 865.262451171875, Entropy 404.0260009765625, Learning Rate: 0.000625\n",
      "Epoch [8161/20000], Loss: 840.99365234375, Entropy 409.5847473144531, Learning Rate: 0.000625\n",
      "Epoch [8162/20000], Loss: 908.3223876953125, Entropy 393.8727111816406, Learning Rate: 0.000625\n",
      "Epoch [8163/20000], Loss: 898.2291870117188, Entropy 406.84185791015625, Learning Rate: 0.000625\n",
      "Epoch [8164/20000], Loss: 893.5277099609375, Entropy 395.5234375, Learning Rate: 0.000625\n",
      "Epoch [8165/20000], Loss: 895.658447265625, Entropy 399.3930358886719, Learning Rate: 0.000625\n",
      "Epoch [8166/20000], Loss: 846.4193725585938, Entropy 394.72406005859375, Learning Rate: 0.000625\n",
      "Epoch [8167/20000], Loss: 890.7254638671875, Entropy 392.8609924316406, Learning Rate: 0.000625\n",
      "Epoch [8168/20000], Loss: 872.116455078125, Entropy 405.36083984375, Learning Rate: 0.000625\n",
      "Epoch [8169/20000], Loss: 865.4857177734375, Entropy 406.8822326660156, Learning Rate: 0.000625\n",
      "Epoch [8170/20000], Loss: 943.7810668945312, Entropy 402.42340087890625, Learning Rate: 0.000625\n",
      "Epoch [8171/20000], Loss: 906.9503784179688, Entropy 397.82025146484375, Learning Rate: 0.000625\n",
      "Epoch [8172/20000], Loss: 973.2073974609375, Entropy 373.2898254394531, Learning Rate: 0.000625\n",
      "Epoch [8173/20000], Loss: 896.7017822265625, Entropy 410.3330383300781, Learning Rate: 0.000625\n",
      "Epoch [8174/20000], Loss: 891.6357421875, Entropy 401.9704895019531, Learning Rate: 0.000625\n",
      "Epoch [8175/20000], Loss: 842.6597900390625, Entropy 403.6867370605469, Learning Rate: 0.000625\n",
      "Epoch [8176/20000], Loss: 855.675048828125, Entropy 404.2702331542969, Learning Rate: 0.000625\n",
      "Epoch [8177/20000], Loss: 857.9215087890625, Entropy 405.62841796875, Learning Rate: 0.000625\n",
      "Epoch [8178/20000], Loss: 907.1544189453125, Entropy 399.6014099121094, Learning Rate: 0.000625\n",
      "Epoch [8179/20000], Loss: 851.4448852539062, Entropy 411.29412841796875, Learning Rate: 0.000625\n",
      "Epoch [8180/20000], Loss: 886.3709716796875, Entropy 387.0583190917969, Learning Rate: 0.000625\n",
      "Epoch [8181/20000], Loss: 872.6198120117188, Entropy 392.92828369140625, Learning Rate: 0.000625\n",
      "Epoch [8182/20000], Loss: 886.083984375, Entropy 397.1690673828125, Learning Rate: 0.000625\n",
      "Epoch [8183/20000], Loss: 894.8978271484375, Entropy 406.8977355957031, Learning Rate: 0.000625\n",
      "Epoch [8184/20000], Loss: 847.400146484375, Entropy 414.7769470214844, Learning Rate: 0.000625\n",
      "Epoch [8185/20000], Loss: 840.9412231445312, Entropy 404.32135009765625, Learning Rate: 0.000625\n",
      "Epoch [8186/20000], Loss: 883.4960327148438, Entropy 398.28851318359375, Learning Rate: 0.000625\n",
      "Epoch [8187/20000], Loss: 887.38232421875, Entropy 403.0067443847656, Learning Rate: 0.000625\n",
      "Epoch [8188/20000], Loss: 907.420654296875, Entropy 383.6524963378906, Learning Rate: 0.000625\n",
      "Epoch [8189/20000], Loss: 885.2529296875, Entropy 377.292724609375, Learning Rate: 0.000625\n",
      "Epoch [8190/20000], Loss: 868.2872314453125, Entropy 399.5633239746094, Learning Rate: 0.000625\n",
      "Epoch [8191/20000], Loss: 907.7015380859375, Entropy 382.9225158691406, Learning Rate: 0.000625\n",
      "Epoch [8192/20000], Loss: 894.5784301757812, Entropy 381.59576416015625, Learning Rate: 0.000625\n",
      "Epoch [8193/20000], Loss: 843.27392578125, Entropy 392.704833984375, Learning Rate: 0.000625\n",
      "Epoch [8194/20000], Loss: 859.852294921875, Entropy 400.5294494628906, Learning Rate: 0.000625\n",
      "Epoch [8195/20000], Loss: 855.4918212890625, Entropy 407.6184387207031, Learning Rate: 0.000625\n",
      "Epoch [8196/20000], Loss: 908.3447875976562, Entropy 391.04022216796875, Learning Rate: 0.000625\n",
      "Epoch [8197/20000], Loss: 889.2764892578125, Entropy 394.1486511230469, Learning Rate: 0.000625\n",
      "Epoch [8198/20000], Loss: 940.37060546875, Entropy 394.44873046875, Learning Rate: 0.000625\n",
      "Epoch [8199/20000], Loss: 866.5722045898438, Entropy 392.48443603515625, Learning Rate: 0.000625\n",
      "Epoch [8200/20000], Loss: 852.2557983398438, Entropy 401.00250244140625, Learning Rate: 0.000625\n",
      "Epoch [8201/20000], Loss: 848.8812866210938, Entropy 396.07574462890625, Learning Rate: 0.000625\n",
      "Epoch [8202/20000], Loss: 874.253173828125, Entropy 386.76513671875, Learning Rate: 0.000625\n",
      "Epoch [8203/20000], Loss: 860.31201171875, Entropy 415.9596252441406, Learning Rate: 0.000625\n",
      "Epoch [8204/20000], Loss: 853.6796264648438, Entropy 400.52984619140625, Learning Rate: 0.000625\n",
      "Epoch [8205/20000], Loss: 875.489013671875, Entropy 383.4952697753906, Learning Rate: 0.000625\n",
      "Epoch [8206/20000], Loss: 872.1262817382812, Entropy 387.63897705078125, Learning Rate: 0.000625\n",
      "Epoch [8207/20000], Loss: 887.2861328125, Entropy 388.3441467285156, Learning Rate: 0.000625\n",
      "Epoch [8208/20000], Loss: 899.231201171875, Entropy 385.3990173339844, Learning Rate: 0.000625\n",
      "Epoch [8209/20000], Loss: 930.780029296875, Entropy 383.3355712890625, Learning Rate: 0.000625\n",
      "Epoch [8210/20000], Loss: 870.985107421875, Entropy 392.7322082519531, Learning Rate: 0.000625\n",
      "Epoch [8211/20000], Loss: 853.5763549804688, Entropy 384.47125244140625, Learning Rate: 0.000625\n",
      "Epoch [8212/20000], Loss: 914.670654296875, Entropy 394.7706298828125, Learning Rate: 0.000625\n",
      "Epoch [8213/20000], Loss: 849.475341796875, Entropy 395.6681213378906, Learning Rate: 0.000625\n",
      "Epoch [8214/20000], Loss: 965.5849609375, Entropy 406.8443603515625, Learning Rate: 0.000625\n",
      "Epoch [8215/20000], Loss: 879.4718017578125, Entropy 394.5043029785156, Learning Rate: 0.000625\n",
      "Epoch [8216/20000], Loss: 928.1226196289062, Entropy 388.04779052734375, Learning Rate: 0.000625\n",
      "Epoch [8217/20000], Loss: 913.4138793945312, Entropy 383.66534423828125, Learning Rate: 0.000625\n",
      "Epoch [8218/20000], Loss: 873.4378662109375, Entropy 394.4851379394531, Learning Rate: 0.000625\n",
      "Epoch [8219/20000], Loss: 892.8427734375, Entropy 393.969482421875, Learning Rate: 0.000625\n",
      "Epoch [8220/20000], Loss: 887.6234741210938, Entropy 397.85736083984375, Learning Rate: 0.000625\n",
      "Epoch [8221/20000], Loss: 858.2086181640625, Entropy 411.5567932128906, Learning Rate: 0.000625\n",
      "Epoch [8222/20000], Loss: 900.97607421875, Entropy 408.9279479980469, Learning Rate: 0.000625\n",
      "Epoch [8223/20000], Loss: 873.749755859375, Entropy 405.9005126953125, Learning Rate: 0.000625\n",
      "Epoch [8224/20000], Loss: 878.9094848632812, Entropy 395.63763427734375, Learning Rate: 0.000625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8225/20000], Loss: 865.4398193359375, Entropy 398.6014099121094, Learning Rate: 0.000625\n",
      "Epoch [8226/20000], Loss: 843.8265380859375, Entropy 404.9661560058594, Learning Rate: 0.000625\n",
      "Epoch [8227/20000], Loss: 845.1094360351562, Entropy 420.93682861328125, Learning Rate: 0.000625\n",
      "Epoch [8228/20000], Loss: 894.6578979492188, Entropy 396.13092041015625, Learning Rate: 0.000625\n",
      "Epoch [8229/20000], Loss: 923.639892578125, Entropy 397.5221862792969, Learning Rate: 0.000625\n",
      "Epoch [8230/20000], Loss: 888.5107421875, Entropy 398.0902404785156, Learning Rate: 0.000625\n",
      "Epoch [8231/20000], Loss: 868.361328125, Entropy 388.5298767089844, Learning Rate: 0.000625\n",
      "Epoch [8232/20000], Loss: 866.1616821289062, Entropy 397.29718017578125, Learning Rate: 0.000625\n",
      "Epoch [8233/20000], Loss: 864.4697265625, Entropy 402.6559753417969, Learning Rate: 0.000625\n",
      "Epoch [8234/20000], Loss: 881.5425415039062, Entropy 406.46209716796875, Learning Rate: 0.000625\n",
      "Epoch [8235/20000], Loss: 842.81005859375, Entropy 415.6180114746094, Learning Rate: 0.000625\n",
      "Epoch [8236/20000], Loss: 877.3445434570312, Entropy 421.86822509765625, Learning Rate: 0.000625\n",
      "Epoch [8237/20000], Loss: 896.1590576171875, Entropy 382.5945129394531, Learning Rate: 0.000625\n",
      "Epoch [8238/20000], Loss: 903.2559814453125, Entropy 395.6851501464844, Learning Rate: 0.000625\n",
      "Epoch [8239/20000], Loss: 868.4859619140625, Entropy 399.2756042480469, Learning Rate: 0.000625\n",
      "Epoch [8240/20000], Loss: 924.28076171875, Entropy 413.2390441894531, Learning Rate: 0.000625\n",
      "Epoch [8241/20000], Loss: 866.6419677734375, Entropy 410.6805725097656, Learning Rate: 0.000625\n",
      "Epoch [8242/20000], Loss: 896.0302734375, Entropy 386.9552307128906, Learning Rate: 0.000625\n",
      "Epoch [8243/20000], Loss: 879.5364990234375, Entropy 391.9518737792969, Learning Rate: 0.000625\n",
      "Epoch [8244/20000], Loss: 888.8521118164062, Entropy 402.57342529296875, Learning Rate: 0.000625\n",
      "Epoch [8245/20000], Loss: 898.4481201171875, Entropy 389.3074951171875, Learning Rate: 0.000625\n",
      "Epoch [8246/20000], Loss: 875.056640625, Entropy 394.1258850097656, Learning Rate: 0.000625\n",
      "Epoch [8247/20000], Loss: 864.6715087890625, Entropy 405.710693359375, Learning Rate: 0.000625\n",
      "Epoch [8248/20000], Loss: 846.1453857421875, Entropy 403.2135925292969, Learning Rate: 0.000625\n",
      "Epoch [8249/20000], Loss: 874.7066650390625, Entropy 398.4426574707031, Learning Rate: 0.000625\n",
      "Epoch [8250/20000], Loss: 910.8609619140625, Entropy 387.0802307128906, Learning Rate: 0.000625\n",
      "Epoch [8251/20000], Loss: 871.96728515625, Entropy 393.4911193847656, Learning Rate: 0.000625\n",
      "Epoch [8252/20000], Loss: 877.491455078125, Entropy 385.7909240722656, Learning Rate: 0.000625\n",
      "Epoch [8253/20000], Loss: 831.08935546875, Entropy 417.1140441894531, Learning Rate: 0.000625\n",
      "Epoch [8254/20000], Loss: 880.185546875, Entropy 404.1512756347656, Learning Rate: 0.000625\n",
      "Epoch [8255/20000], Loss: 921.29345703125, Entropy 391.45458984375, Learning Rate: 0.000625\n",
      "Epoch [8256/20000], Loss: 806.4710083007812, Entropy 409.26312255859375, Learning Rate: 0.000625\n",
      "Epoch [8257/20000], Loss: 907.69140625, Entropy 399.3927917480469, Learning Rate: 0.000625\n",
      "Epoch [8258/20000], Loss: 882.2518920898438, Entropy 391.99700927734375, Learning Rate: 0.000625\n",
      "Epoch [8259/20000], Loss: 892.3147583007812, Entropy 404.05462646484375, Learning Rate: 0.000625\n",
      "Epoch [8260/20000], Loss: 861.02587890625, Entropy 410.5692138671875, Learning Rate: 0.000625\n",
      "Epoch [8261/20000], Loss: 841.9674072265625, Entropy 411.6834411621094, Learning Rate: 0.000625\n",
      "Epoch [8262/20000], Loss: 870.41064453125, Entropy 407.0699157714844, Learning Rate: 0.000625\n",
      "Epoch [8263/20000], Loss: 871.131103515625, Entropy 404.84814453125, Learning Rate: 0.000625\n",
      "Epoch [8264/20000], Loss: 876.769287109375, Entropy 387.2021789550781, Learning Rate: 0.000625\n",
      "Epoch [8265/20000], Loss: 893.5328369140625, Entropy 395.96630859375, Learning Rate: 0.000625\n",
      "Epoch [8266/20000], Loss: 932.5501708984375, Entropy 391.228515625, Learning Rate: 0.000625\n",
      "Epoch [8267/20000], Loss: 860.09619140625, Entropy 393.3723449707031, Learning Rate: 0.000625\n",
      "Epoch [8268/20000], Loss: 886.40966796875, Entropy 390.6111755371094, Learning Rate: 0.000625\n",
      "Epoch [8269/20000], Loss: 859.862060546875, Entropy 390.5359802246094, Learning Rate: 0.000625\n",
      "Epoch [8270/20000], Loss: 921.7794799804688, Entropy 377.14544677734375, Learning Rate: 0.000625\n",
      "Epoch [8271/20000], Loss: 852.755859375, Entropy 392.7119140625, Learning Rate: 0.000625\n",
      "Epoch [8272/20000], Loss: 905.44384765625, Entropy 391.7147521972656, Learning Rate: 0.000625\n",
      "Epoch [8273/20000], Loss: 858.6778564453125, Entropy 395.2269592285156, Learning Rate: 0.000625\n",
      "Epoch [8274/20000], Loss: 871.424560546875, Entropy 406.39794921875, Learning Rate: 0.000625\n",
      "Epoch [8275/20000], Loss: 855.2806396484375, Entropy 402.876953125, Learning Rate: 0.000625\n",
      "Epoch [8276/20000], Loss: 849.1654052734375, Entropy 396.1648254394531, Learning Rate: 0.000625\n",
      "Epoch [8277/20000], Loss: 889.798583984375, Entropy 396.66796875, Learning Rate: 0.000625\n",
      "Epoch [8278/20000], Loss: 853.7470703125, Entropy 409.6299133300781, Learning Rate: 0.000625\n",
      "Epoch [8279/20000], Loss: 922.14501953125, Entropy 379.3819580078125, Learning Rate: 0.000625\n",
      "Epoch [8280/20000], Loss: 860.8295288085938, Entropy 391.10272216796875, Learning Rate: 0.000625\n",
      "Epoch [8281/20000], Loss: 867.27099609375, Entropy 405.6832580566406, Learning Rate: 0.000625\n",
      "Epoch [8282/20000], Loss: 944.8125, Entropy 388.6246032714844, Learning Rate: 0.000625\n",
      "Epoch [8283/20000], Loss: 878.4578857421875, Entropy 397.1352233886719, Learning Rate: 0.000625\n",
      "Epoch [8284/20000], Loss: 869.36181640625, Entropy 397.6902160644531, Learning Rate: 0.000625\n",
      "Epoch [8285/20000], Loss: 862.435791015625, Entropy 396.1369934082031, Learning Rate: 0.000625\n",
      "Epoch [8286/20000], Loss: 867.0014038085938, Entropy 411.37164306640625, Learning Rate: 0.000625\n",
      "Epoch [8287/20000], Loss: 933.73583984375, Entropy 389.25537109375, Learning Rate: 0.000625\n",
      "Epoch [8288/20000], Loss: 826.2244873046875, Entropy 395.6112365722656, Learning Rate: 0.000625\n",
      "Epoch [8289/20000], Loss: 900.348876953125, Entropy 387.2853088378906, Learning Rate: 0.000625\n",
      "Epoch [8290/20000], Loss: 877.9746704101562, Entropy 393.08734130859375, Learning Rate: 0.000625\n",
      "Epoch [8291/20000], Loss: 912.0350952148438, Entropy 393.53546142578125, Learning Rate: 0.000625\n",
      "Epoch [8292/20000], Loss: 913.5555419921875, Entropy 400.8016662597656, Learning Rate: 0.000625\n",
      "Epoch [8293/20000], Loss: 887.1966552734375, Entropy 393.8701171875, Learning Rate: 0.000625\n",
      "Epoch [8294/20000], Loss: 905.8818359375, Entropy 389.3371887207031, Learning Rate: 0.000625\n",
      "Epoch [8295/20000], Loss: 925.7813720703125, Entropy 396.7939758300781, Learning Rate: 0.000625\n",
      "Epoch [8296/20000], Loss: 895.205810546875, Entropy 399.8982849121094, Learning Rate: 0.000625\n",
      "Epoch [8297/20000], Loss: 881.903564453125, Entropy 384.5699768066406, Learning Rate: 0.000625\n",
      "Epoch [8298/20000], Loss: 842.02392578125, Entropy 405.302734375, Learning Rate: 0.000625\n",
      "Epoch [8299/20000], Loss: 848.4834594726562, Entropy 399.73077392578125, Learning Rate: 0.000625\n",
      "Epoch [8300/20000], Loss: 896.2077026367188, Entropy 414.90570068359375, Learning Rate: 0.000625\n",
      "Epoch [8301/20000], Loss: 896.4259643554688, Entropy 405.32122802734375, Learning Rate: 0.000625\n",
      "Epoch [8302/20000], Loss: 910.09716796875, Entropy 399.7320251464844, Learning Rate: 0.000625\n",
      "Epoch [8303/20000], Loss: 853.0326538085938, Entropy 405.76715087890625, Learning Rate: 0.000625\n",
      "Epoch [8304/20000], Loss: 868.8072509765625, Entropy 414.5789489746094, Learning Rate: 0.000625\n",
      "Epoch [8305/20000], Loss: 863.3685913085938, Entropy 382.01861572265625, Learning Rate: 0.000625\n",
      "Epoch [8306/20000], Loss: 855.2225341796875, Entropy 402.1033020019531, Learning Rate: 0.000625\n",
      "Epoch [8307/20000], Loss: 911.014892578125, Entropy 388.5181884765625, Learning Rate: 0.000625\n",
      "Epoch [8308/20000], Loss: 896.8251953125, Entropy 396.9033203125, Learning Rate: 0.000625\n",
      "Epoch [8309/20000], Loss: 889.73681640625, Entropy 409.2746276855469, Learning Rate: 0.000625\n",
      "Epoch [8310/20000], Loss: 943.1561279296875, Entropy 413.249755859375, Learning Rate: 0.000625\n",
      "Epoch [8311/20000], Loss: 879.18798828125, Entropy 406.0832824707031, Learning Rate: 0.000625\n",
      "Epoch [8312/20000], Loss: 855.8313598632812, Entropy 414.69342041015625, Learning Rate: 0.000625\n",
      "Epoch [8313/20000], Loss: 893.2144775390625, Entropy 404.5586242675781, Learning Rate: 0.000625\n",
      "Epoch [8314/20000], Loss: 885.1260986328125, Entropy 392.447021484375, Learning Rate: 0.000625\n",
      "Epoch [8315/20000], Loss: 888.6495361328125, Entropy 407.743896484375, Learning Rate: 0.000625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8316/20000], Loss: 828.48095703125, Entropy 430.4542541503906, Learning Rate: 0.000625\n",
      "Epoch [8317/20000], Loss: 862.800048828125, Entropy 409.9999084472656, Learning Rate: 0.000625\n",
      "Epoch [8318/20000], Loss: 840.5185546875, Entropy 425.5774841308594, Learning Rate: 0.000625\n",
      "Epoch [8319/20000], Loss: 909.094970703125, Entropy 416.8171081542969, Learning Rate: 0.000625\n",
      "Epoch [8320/20000], Loss: 918.6336669921875, Entropy 400.8619079589844, Learning Rate: 0.000625\n",
      "Epoch [8321/20000], Loss: 866.2071533203125, Entropy 402.7048645019531, Learning Rate: 0.000625\n",
      "Epoch [8322/20000], Loss: 903.2241821289062, Entropy 404.68939208984375, Learning Rate: 0.000625\n",
      "Epoch [8323/20000], Loss: 894.53076171875, Entropy 406.3419494628906, Learning Rate: 0.000625\n",
      "Epoch [8324/20000], Loss: 907.6878662109375, Entropy 402.0154724121094, Learning Rate: 0.000625\n",
      "Epoch [8325/20000], Loss: 892.0331420898438, Entropy 422.34906005859375, Learning Rate: 0.000625\n",
      "Epoch [8326/20000], Loss: 870.2485961914062, Entropy 405.26812744140625, Learning Rate: 0.000625\n",
      "Epoch [8327/20000], Loss: 869.763427734375, Entropy 389.7463073730469, Learning Rate: 0.000625\n",
      "Epoch [8328/20000], Loss: 898.5418701171875, Entropy 404.1928405761719, Learning Rate: 0.000625\n",
      "Epoch [8329/20000], Loss: 909.2069702148438, Entropy 392.08599853515625, Learning Rate: 0.000625\n",
      "Epoch [8330/20000], Loss: 875.18408203125, Entropy 406.2537536621094, Learning Rate: 0.000625\n",
      "Epoch [8331/20000], Loss: 857.7605590820312, Entropy 419.66265869140625, Learning Rate: 0.000625\n",
      "Epoch [8332/20000], Loss: 894.9189453125, Entropy 412.66748046875, Learning Rate: 0.000625\n",
      "Epoch [8333/20000], Loss: 890.5370483398438, Entropy 409.73687744140625, Learning Rate: 0.000625\n",
      "Epoch [8334/20000], Loss: 888.6493530273438, Entropy 387.52667236328125, Learning Rate: 0.000625\n",
      "Epoch [8335/20000], Loss: 891.5542602539062, Entropy 393.77679443359375, Learning Rate: 0.000625\n",
      "Epoch [8336/20000], Loss: 886.838134765625, Entropy 408.6156311035156, Learning Rate: 0.000625\n",
      "Epoch [8337/20000], Loss: 852.53466796875, Entropy 414.9982604980469, Learning Rate: 0.000625\n",
      "Epoch [8338/20000], Loss: 849.3695068359375, Entropy 412.2652282714844, Learning Rate: 0.000625\n",
      "Epoch [8339/20000], Loss: 855.94091796875, Entropy 407.2628173828125, Learning Rate: 0.000625\n",
      "Epoch [8340/20000], Loss: 859.0728759765625, Entropy 402.0093994140625, Learning Rate: 0.000625\n",
      "Epoch [8341/20000], Loss: 878.0032958984375, Entropy 403.7843933105469, Learning Rate: 0.000625\n",
      "Epoch [8342/20000], Loss: 883.2784423828125, Entropy 387.4422607421875, Learning Rate: 0.000625\n",
      "Epoch [8343/20000], Loss: 877.0179443359375, Entropy 402.5199890136719, Learning Rate: 0.000625\n",
      "Epoch [8344/20000], Loss: 871.6682739257812, Entropy 409.49554443359375, Learning Rate: 0.000625\n",
      "Epoch [8345/20000], Loss: 845.183837890625, Entropy 414.2067565917969, Learning Rate: 0.000625\n",
      "Epoch [8346/20000], Loss: 897.483642578125, Entropy 405.5037536621094, Learning Rate: 0.000625\n",
      "Epoch [8347/20000], Loss: 929.11865234375, Entropy 397.5774841308594, Learning Rate: 0.000625\n",
      "Epoch [8348/20000], Loss: 838.7979736328125, Entropy 418.7532653808594, Learning Rate: 0.000625\n",
      "Epoch [8349/20000], Loss: 864.4041748046875, Entropy 391.8067932128906, Learning Rate: 0.000625\n",
      "Epoch [8350/20000], Loss: 855.9019775390625, Entropy 417.7286682128906, Learning Rate: 0.000625\n",
      "Epoch [8351/20000], Loss: 919.3656005859375, Entropy 419.0770263671875, Learning Rate: 0.000625\n",
      "Epoch [8352/20000], Loss: 893.112060546875, Entropy 398.64599609375, Learning Rate: 0.000625\n",
      "Epoch [8353/20000], Loss: 873.5914916992188, Entropy 411.73577880859375, Learning Rate: 0.000625\n",
      "Epoch [8354/20000], Loss: 900.3475341796875, Entropy 384.9914855957031, Learning Rate: 0.000625\n",
      "Epoch [8355/20000], Loss: 881.2518310546875, Entropy 406.13623046875, Learning Rate: 0.000625\n",
      "Epoch [8356/20000], Loss: 892.977294921875, Entropy 399.9031982421875, Learning Rate: 0.000625\n",
      "Epoch [8357/20000], Loss: 897.4326171875, Entropy 403.36279296875, Learning Rate: 0.000625\n",
      "Epoch [8358/20000], Loss: 937.585693359375, Entropy 404.654296875, Learning Rate: 0.000625\n",
      "Epoch [8359/20000], Loss: 856.757080078125, Entropy 401.7481994628906, Learning Rate: 0.000625\n",
      "Epoch [8360/20000], Loss: 880.5791015625, Entropy 399.7483825683594, Learning Rate: 0.000625\n",
      "Epoch [8361/20000], Loss: 862.8583984375, Entropy 395.7837829589844, Learning Rate: 0.000625\n",
      "Epoch [8362/20000], Loss: 903.9835205078125, Entropy 413.9412536621094, Learning Rate: 0.000625\n",
      "Epoch [8363/20000], Loss: 890.12158203125, Entropy 409.8545227050781, Learning Rate: 0.000625\n",
      "Epoch [8364/20000], Loss: 897.7391357421875, Entropy 387.1087951660156, Learning Rate: 0.000625\n",
      "Epoch [8365/20000], Loss: 822.6693725585938, Entropy 422.55804443359375, Learning Rate: 0.000625\n",
      "Epoch [8366/20000], Loss: 861.0440063476562, Entropy 400.80560302734375, Learning Rate: 0.000625\n",
      "Epoch [8367/20000], Loss: 918.9107055664062, Entropy 396.14459228515625, Learning Rate: 0.000625\n",
      "Epoch [8368/20000], Loss: 888.640625, Entropy 410.1293640136719, Learning Rate: 0.000625\n",
      "Epoch [8369/20000], Loss: 856.2084350585938, Entropy 424.03619384765625, Learning Rate: 0.000625\n",
      "Epoch [8370/20000], Loss: 897.8588256835938, Entropy 402.47882080078125, Learning Rate: 0.000625\n",
      "Epoch [8371/20000], Loss: 883.5595703125, Entropy 395.4425964355469, Learning Rate: 0.000625\n",
      "Epoch [8372/20000], Loss: 887.7482299804688, Entropy 408.82257080078125, Learning Rate: 0.000625\n",
      "Epoch [8373/20000], Loss: 899.9437255859375, Entropy 405.1175842285156, Learning Rate: 0.000625\n",
      "Epoch [8374/20000], Loss: 853.4776611328125, Entropy 417.9633483886719, Learning Rate: 0.000625\n",
      "Epoch [8375/20000], Loss: 832.3194580078125, Entropy 406.1950378417969, Learning Rate: 0.000625\n",
      "Epoch [8376/20000], Loss: 881.154296875, Entropy 400.6653137207031, Learning Rate: 0.000625\n",
      "Epoch [8377/20000], Loss: 850.9593505859375, Entropy 401.2911376953125, Learning Rate: 0.000625\n",
      "Epoch [8378/20000], Loss: 874.4890747070312, Entropy 398.01275634765625, Learning Rate: 0.000625\n",
      "Epoch [8379/20000], Loss: 901.955078125, Entropy 403.993896484375, Learning Rate: 0.000625\n",
      "Epoch [8380/20000], Loss: 934.9659423828125, Entropy 399.2039794921875, Learning Rate: 0.000625\n",
      "Epoch [8381/20000], Loss: 876.306884765625, Entropy 408.4093322753906, Learning Rate: 0.000625\n",
      "Epoch [8382/20000], Loss: 912.4539184570312, Entropy 394.34100341796875, Learning Rate: 0.000625\n",
      "Epoch [8383/20000], Loss: 879.1659545898438, Entropy 400.84710693359375, Learning Rate: 0.000625\n",
      "Epoch [8384/20000], Loss: 877.341064453125, Entropy 405.1634216308594, Learning Rate: 0.000625\n",
      "Epoch [8385/20000], Loss: 860.8675537109375, Entropy 408.7083435058594, Learning Rate: 0.000625\n",
      "Epoch [8386/20000], Loss: 871.91796875, Entropy 423.1942443847656, Learning Rate: 0.000625\n",
      "Epoch [8387/20000], Loss: 905.3211669921875, Entropy 400.6210021972656, Learning Rate: 0.000625\n",
      "Epoch [8388/20000], Loss: 864.138916015625, Entropy 408.44482421875, Learning Rate: 0.000625\n",
      "Epoch [8389/20000], Loss: 891.009765625, Entropy 400.8316955566406, Learning Rate: 0.000625\n",
      "Epoch [8390/20000], Loss: 857.0668334960938, Entropy 426.09674072265625, Learning Rate: 0.000625\n",
      "Epoch [8391/20000], Loss: 882.5042724609375, Entropy 405.4835205078125, Learning Rate: 0.000625\n",
      "Epoch [8392/20000], Loss: 882.1633911132812, Entropy 399.05426025390625, Learning Rate: 0.000625\n",
      "Epoch [8393/20000], Loss: 821.7777099609375, Entropy 403.6730041503906, Learning Rate: 0.000625\n",
      "Epoch [8394/20000], Loss: 913.653076171875, Entropy 410.9384765625, Learning Rate: 0.000625\n",
      "Epoch [8395/20000], Loss: 893.852294921875, Entropy 405.1925048828125, Learning Rate: 0.000625\n",
      "Epoch [8396/20000], Loss: 896.1280517578125, Entropy 415.232177734375, Learning Rate: 0.000625\n",
      "Epoch [8397/20000], Loss: 890.0606689453125, Entropy 383.7321472167969, Learning Rate: 0.000625\n",
      "Epoch [8398/20000], Loss: 864.1220092773438, Entropy 394.37164306640625, Learning Rate: 0.000625\n",
      "Epoch [8399/20000], Loss: 846.27978515625, Entropy 415.3621520996094, Learning Rate: 0.000625\n",
      "Epoch [8400/20000], Loss: 859.3760986328125, Entropy 405.5660095214844, Learning Rate: 0.000625\n",
      "Epoch [8401/20000], Loss: 884.090576171875, Entropy 418.9464111328125, Learning Rate: 0.000625\n",
      "Epoch [8402/20000], Loss: 883.5829467773438, Entropy 408.29730224609375, Learning Rate: 0.000625\n",
      "Epoch [8403/20000], Loss: 824.7867431640625, Entropy 400.6356201171875, Learning Rate: 0.000625\n",
      "Epoch [8404/20000], Loss: 882.123046875, Entropy 413.5472106933594, Learning Rate: 0.000625\n",
      "Epoch [8405/20000], Loss: 907.2362670898438, Entropy 396.62420654296875, Learning Rate: 0.000625\n",
      "Epoch [8406/20000], Loss: 881.8558349609375, Entropy 400.1404724121094, Learning Rate: 0.000625\n",
      "Epoch [8407/20000], Loss: 851.2491455078125, Entropy 398.9972229003906, Learning Rate: 0.000625\n",
      "Epoch [8408/20000], Loss: 901.7050170898438, Entropy 405.52264404296875, Learning Rate: 0.000625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8409/20000], Loss: 862.0126953125, Entropy 408.8376159667969, Learning Rate: 0.000625\n",
      "Epoch [8410/20000], Loss: 841.1135864257812, Entropy 401.79217529296875, Learning Rate: 0.000625\n",
      "Epoch [8411/20000], Loss: 873.663330078125, Entropy 414.4701843261719, Learning Rate: 0.000625\n",
      "Epoch [8412/20000], Loss: 878.0855712890625, Entropy 416.3078918457031, Learning Rate: 0.000625\n",
      "Epoch [8413/20000], Loss: 870.569091796875, Entropy 411.2428283691406, Learning Rate: 0.000625\n",
      "Epoch [8414/20000], Loss: 872.5493774414062, Entropy 393.24163818359375, Learning Rate: 0.000625\n",
      "Epoch [8415/20000], Loss: 878.7830200195312, Entropy 409.62457275390625, Learning Rate: 0.000625\n",
      "Epoch [8416/20000], Loss: 931.5006103515625, Entropy 393.974365234375, Learning Rate: 0.000625\n",
      "Epoch [8417/20000], Loss: 893.998046875, Entropy 395.0373840332031, Learning Rate: 0.000625\n",
      "Epoch [8418/20000], Loss: 898.86181640625, Entropy 391.5877685546875, Learning Rate: 0.000625\n",
      "Epoch [8419/20000], Loss: 865.6224365234375, Entropy 417.8622131347656, Learning Rate: 0.000625\n",
      "Epoch [8420/20000], Loss: 885.0333251953125, Entropy 414.9075927734375, Learning Rate: 0.000625\n",
      "Epoch [8421/20000], Loss: 882.43408203125, Entropy 394.3446044921875, Learning Rate: 0.000625\n",
      "Epoch [8422/20000], Loss: 918.2470703125, Entropy 408.1385803222656, Learning Rate: 0.000625\n",
      "Epoch [8423/20000], Loss: 889.4349365234375, Entropy 411.8197021484375, Learning Rate: 0.000625\n",
      "Epoch [8424/20000], Loss: 855.4102172851562, Entropy 407.53240966796875, Learning Rate: 0.000625\n",
      "Epoch [8425/20000], Loss: 845.1927490234375, Entropy 417.1299133300781, Learning Rate: 0.000625\n",
      "Epoch [8426/20000], Loss: 897.000244140625, Entropy 409.5836181640625, Learning Rate: 0.000625\n",
      "Epoch [8427/20000], Loss: 929.0860595703125, Entropy 405.1214904785156, Learning Rate: 0.000625\n",
      "Epoch [8428/20000], Loss: 845.013671875, Entropy 421.1236572265625, Learning Rate: 0.000625\n",
      "Epoch [8429/20000], Loss: 902.759521484375, Entropy 414.6546630859375, Learning Rate: 0.000625\n",
      "Epoch [8430/20000], Loss: 913.630859375, Entropy 387.9390563964844, Learning Rate: 0.000625\n",
      "Epoch [8431/20000], Loss: 880.3233642578125, Entropy 400.5900573730469, Learning Rate: 0.000625\n",
      "Epoch [8432/20000], Loss: 905.3291015625, Entropy 397.5162353515625, Learning Rate: 0.000625\n",
      "Epoch [8433/20000], Loss: 885.2037353515625, Entropy 411.5379638671875, Learning Rate: 0.000625\n",
      "Epoch [8434/20000], Loss: 930.1331787109375, Entropy 413.9111633300781, Learning Rate: 0.000625\n",
      "Epoch [8435/20000], Loss: 818.21044921875, Entropy 412.886474609375, Learning Rate: 0.000625\n",
      "Epoch [8436/20000], Loss: 913.1876220703125, Entropy 394.5415954589844, Learning Rate: 0.000625\n",
      "Epoch [8437/20000], Loss: 889.2078857421875, Entropy 418.2772521972656, Learning Rate: 0.000625\n",
      "Epoch [8438/20000], Loss: 826.1929931640625, Entropy 415.3206787109375, Learning Rate: 0.000625\n",
      "Epoch [8439/20000], Loss: 900.4220581054688, Entropy 408.37615966796875, Learning Rate: 0.000625\n",
      "Epoch [8440/20000], Loss: 848.7840576171875, Entropy 416.1521911621094, Learning Rate: 0.000625\n",
      "Epoch [8441/20000], Loss: 841.3605346679688, Entropy 413.85101318359375, Learning Rate: 0.000625\n",
      "Epoch [8442/20000], Loss: 903.6151123046875, Entropy 401.9526062011719, Learning Rate: 0.000625\n",
      "Epoch [8443/20000], Loss: 857.9819946289062, Entropy 420.76275634765625, Learning Rate: 0.000625\n",
      "Epoch [8444/20000], Loss: 904.5411987304688, Entropy 412.40911865234375, Learning Rate: 0.000625\n",
      "Epoch [8445/20000], Loss: 884.7713623046875, Entropy 402.52880859375, Learning Rate: 0.000625\n",
      "Epoch [8446/20000], Loss: 895.6177368164062, Entropy 404.82122802734375, Learning Rate: 0.000625\n",
      "Epoch [8447/20000], Loss: 900.124755859375, Entropy 405.19189453125, Learning Rate: 0.000625\n",
      "Epoch [8448/20000], Loss: 965.9793701171875, Entropy 395.5649108886719, Learning Rate: 0.000625\n",
      "Epoch [8449/20000], Loss: 828.649169921875, Entropy 420.0470886230469, Learning Rate: 0.000625\n",
      "Epoch [8450/20000], Loss: 867.9816284179688, Entropy 410.36566162109375, Learning Rate: 0.000625\n",
      "Epoch [8451/20000], Loss: 894.8525390625, Entropy 419.3880310058594, Learning Rate: 0.000625\n",
      "Epoch [8452/20000], Loss: 878.6365356445312, Entropy 419.62615966796875, Learning Rate: 0.000625\n",
      "Epoch [8453/20000], Loss: 928.449462890625, Entropy 404.5841979980469, Learning Rate: 0.000625\n",
      "Epoch [8454/20000], Loss: 910.3338623046875, Entropy 401.8414001464844, Learning Rate: 0.000625\n",
      "Epoch [8455/20000], Loss: 854.2410888671875, Entropy 411.7703552246094, Learning Rate: 0.000625\n",
      "Epoch [8456/20000], Loss: 903.1158447265625, Entropy 389.3249816894531, Learning Rate: 0.000625\n",
      "Epoch [8457/20000], Loss: 852.6580810546875, Entropy 400.5803527832031, Learning Rate: 0.000625\n",
      "Epoch [8458/20000], Loss: 830.5225830078125, Entropy 408.2091369628906, Learning Rate: 0.000625\n",
      "Epoch [8459/20000], Loss: 939.067626953125, Entropy 402.83544921875, Learning Rate: 0.000625\n",
      "Epoch [8460/20000], Loss: 909.556640625, Entropy 410.73193359375, Learning Rate: 0.000625\n",
      "Epoch [8461/20000], Loss: 848.236328125, Entropy 416.0841979980469, Learning Rate: 0.000625\n",
      "Epoch [8462/20000], Loss: 911.2702026367188, Entropy 408.09881591796875, Learning Rate: 0.000625\n",
      "Epoch [8463/20000], Loss: 899.9075927734375, Entropy 408.77685546875, Learning Rate: 0.000625\n",
      "Epoch [8464/20000], Loss: 900.8753662109375, Entropy 404.0671691894531, Learning Rate: 0.000625\n",
      "Epoch [8465/20000], Loss: 851.3656005859375, Entropy 408.1008605957031, Learning Rate: 0.000625\n",
      "Epoch [8466/20000], Loss: 922.519287109375, Entropy 410.3396301269531, Learning Rate: 0.000625\n",
      "Epoch [8467/20000], Loss: 906.8034057617188, Entropy 397.62164306640625, Learning Rate: 0.000625\n",
      "Epoch [8468/20000], Loss: 870.5396728515625, Entropy 410.0721435546875, Learning Rate: 0.000625\n",
      "Epoch [8469/20000], Loss: 897.0770263671875, Entropy 412.7619323730469, Learning Rate: 0.000625\n",
      "Epoch [8470/20000], Loss: 870.2222900390625, Entropy 392.1505432128906, Learning Rate: 0.000625\n",
      "Epoch [8471/20000], Loss: 881.534423828125, Entropy 407.9020080566406, Learning Rate: 0.000625\n",
      "Epoch [8472/20000], Loss: 880.42724609375, Entropy 392.4625549316406, Learning Rate: 0.000625\n",
      "Epoch [8473/20000], Loss: 933.41162109375, Entropy 402.43115234375, Learning Rate: 0.000625\n",
      "Epoch [8474/20000], Loss: 901.6853637695312, Entropy 425.97894287109375, Learning Rate: 0.000625\n",
      "Epoch [8475/20000], Loss: 908.7142944335938, Entropy 406.14349365234375, Learning Rate: 0.000625\n",
      "Epoch [8476/20000], Loss: 924.846923828125, Entropy 396.3935852050781, Learning Rate: 0.000625\n",
      "Epoch [8477/20000], Loss: 820.7935791015625, Entropy 403.3070983886719, Learning Rate: 0.000625\n",
      "Epoch [8478/20000], Loss: 870.1051025390625, Entropy 410.6673278808594, Learning Rate: 0.000625\n",
      "Epoch [8479/20000], Loss: 896.572998046875, Entropy 409.4594421386719, Learning Rate: 0.000625\n",
      "Epoch [8480/20000], Loss: 920.7514038085938, Entropy 399.96978759765625, Learning Rate: 0.000625\n",
      "Epoch [8481/20000], Loss: 892.518798828125, Entropy 409.6440124511719, Learning Rate: 0.000625\n",
      "Epoch [8482/20000], Loss: 869.0592041015625, Entropy 414.1098327636719, Learning Rate: 0.000625\n",
      "Epoch [8483/20000], Loss: 911.3693237304688, Entropy 411.76959228515625, Learning Rate: 0.000625\n",
      "Epoch [8484/20000], Loss: 872.963623046875, Entropy 412.5574951171875, Learning Rate: 0.000625\n",
      "Epoch [8485/20000], Loss: 900.398681640625, Entropy 396.4709167480469, Learning Rate: 0.000625\n",
      "Epoch [8486/20000], Loss: 894.2142333984375, Entropy 402.2516174316406, Learning Rate: 0.000625\n",
      "Epoch [8487/20000], Loss: 890.431884765625, Entropy 413.5466613769531, Learning Rate: 0.000625\n",
      "Epoch [8488/20000], Loss: 836.56640625, Entropy 408.5699157714844, Learning Rate: 0.000625\n",
      "Epoch [8489/20000], Loss: 874.097900390625, Entropy 420.8135070800781, Learning Rate: 0.000625\n",
      "Epoch [8490/20000], Loss: 864.7835693359375, Entropy 409.5289611816406, Learning Rate: 0.000625\n",
      "Epoch [8491/20000], Loss: 897.1266479492188, Entropy 423.44171142578125, Learning Rate: 0.000625\n",
      "Epoch [8492/20000], Loss: 925.91796875, Entropy 393.3606872558594, Learning Rate: 0.000625\n",
      "Epoch [8493/20000], Loss: 853.548828125, Entropy 414.0091552734375, Learning Rate: 0.000625\n",
      "Epoch [8494/20000], Loss: 892.6895751953125, Entropy 395.3160705566406, Learning Rate: 0.0003125\n",
      "Epoch [8495/20000], Loss: 847.5089111328125, Entropy 405.0018005371094, Learning Rate: 0.0003125\n",
      "Epoch [8496/20000], Loss: 861.9833984375, Entropy 411.8171691894531, Learning Rate: 0.0003125\n",
      "Epoch [8497/20000], Loss: 840.84423828125, Entropy 416.4073486328125, Learning Rate: 0.0003125\n",
      "Epoch [8498/20000], Loss: 883.5587768554688, Entropy 419.88787841796875, Learning Rate: 0.0003125\n",
      "Epoch [8499/20000], Loss: 881.6370239257812, Entropy 408.33221435546875, Learning Rate: 0.0003125\n",
      "Epoch [8500/20000], Loss: 855.144287109375, Entropy 414.80712890625, Learning Rate: 0.0003125\n",
      "Epoch [8501/20000], Loss: 915.1422119140625, Entropy 403.9655456542969, Learning Rate: 0.0003125\n",
      "Epoch [8502/20000], Loss: 855.49755859375, Entropy 420.7370300292969, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8503/20000], Loss: 857.84814453125, Entropy 415.9425354003906, Learning Rate: 0.0003125\n",
      "Epoch [8504/20000], Loss: 858.8501586914062, Entropy 406.48162841796875, Learning Rate: 0.0003125\n",
      "Epoch [8505/20000], Loss: 858.8189086914062, Entropy 404.40277099609375, Learning Rate: 0.0003125\n",
      "Epoch [8506/20000], Loss: 877.5760498046875, Entropy 423.0799255371094, Learning Rate: 0.0003125\n",
      "Epoch [8507/20000], Loss: 865.1650390625, Entropy 410.5024108886719, Learning Rate: 0.0003125\n",
      "Epoch [8508/20000], Loss: 847.68701171875, Entropy 417.9058532714844, Learning Rate: 0.0003125\n",
      "Epoch [8509/20000], Loss: 885.0126953125, Entropy 422.47216796875, Learning Rate: 0.0003125\n",
      "Epoch [8510/20000], Loss: 855.11376953125, Entropy 391.3758544921875, Learning Rate: 0.0003125\n",
      "Epoch [8511/20000], Loss: 891.4912109375, Entropy 412.027099609375, Learning Rate: 0.0003125\n",
      "Epoch [8512/20000], Loss: 920.1761474609375, Entropy 423.8835754394531, Learning Rate: 0.0003125\n",
      "Epoch [8513/20000], Loss: 849.5557861328125, Entropy 415.0071716308594, Learning Rate: 0.0003125\n",
      "Epoch [8514/20000], Loss: 856.4754638671875, Entropy 417.8340148925781, Learning Rate: 0.0003125\n",
      "Epoch [8515/20000], Loss: 878.6544189453125, Entropy 399.5682067871094, Learning Rate: 0.0003125\n",
      "Epoch [8516/20000], Loss: 859.91162109375, Entropy 418.1044921875, Learning Rate: 0.0003125\n",
      "Epoch [8517/20000], Loss: 904.7489013671875, Entropy 403.0035705566406, Learning Rate: 0.0003125\n",
      "Epoch [8518/20000], Loss: 878.724853515625, Entropy 423.2615051269531, Learning Rate: 0.0003125\n",
      "Epoch [8519/20000], Loss: 894.119384765625, Entropy 382.3274230957031, Learning Rate: 0.0003125\n",
      "Epoch [8520/20000], Loss: 912.153076171875, Entropy 429.2922668457031, Learning Rate: 0.0003125\n",
      "Epoch [8521/20000], Loss: 919.085693359375, Entropy 409.1964111328125, Learning Rate: 0.0003125\n",
      "Epoch [8522/20000], Loss: 872.4754638671875, Entropy 407.2510986328125, Learning Rate: 0.0003125\n",
      "Epoch [8523/20000], Loss: 868.185546875, Entropy 402.5802917480469, Learning Rate: 0.0003125\n",
      "Epoch [8524/20000], Loss: 876.1912231445312, Entropy 419.76568603515625, Learning Rate: 0.0003125\n",
      "Epoch [8525/20000], Loss: 881.4454345703125, Entropy 417.9256591796875, Learning Rate: 0.0003125\n",
      "Epoch [8526/20000], Loss: 866.7986450195312, Entropy 428.50494384765625, Learning Rate: 0.0003125\n",
      "Epoch [8527/20000], Loss: 867.9268798828125, Entropy 407.6524963378906, Learning Rate: 0.0003125\n",
      "Epoch [8528/20000], Loss: 881.592041015625, Entropy 421.4796142578125, Learning Rate: 0.0003125\n",
      "Epoch [8529/20000], Loss: 841.984375, Entropy 408.1916809082031, Learning Rate: 0.0003125\n",
      "Epoch [8530/20000], Loss: 893.5838623046875, Entropy 428.8392028808594, Learning Rate: 0.0003125\n",
      "Epoch [8531/20000], Loss: 888.91357421875, Entropy 409.1776428222656, Learning Rate: 0.0003125\n",
      "Epoch [8532/20000], Loss: 873.7359008789062, Entropy 395.03216552734375, Learning Rate: 0.0003125\n",
      "Epoch [8533/20000], Loss: 885.4908447265625, Entropy 406.1890563964844, Learning Rate: 0.0003125\n",
      "Epoch [8534/20000], Loss: 914.9654541015625, Entropy 403.9132080078125, Learning Rate: 0.0003125\n",
      "Epoch [8535/20000], Loss: 886.01220703125, Entropy 416.42822265625, Learning Rate: 0.0003125\n",
      "Epoch [8536/20000], Loss: 895.5140991210938, Entropy 409.07452392578125, Learning Rate: 0.0003125\n",
      "Epoch [8537/20000], Loss: 833.928955078125, Entropy 405.6413879394531, Learning Rate: 0.0003125\n",
      "Epoch [8538/20000], Loss: 858.2108154296875, Entropy 413.22998046875, Learning Rate: 0.0003125\n",
      "Epoch [8539/20000], Loss: 862.20458984375, Entropy 390.1521911621094, Learning Rate: 0.0003125\n",
      "Epoch [8540/20000], Loss: 888.6262817382812, Entropy 420.79583740234375, Learning Rate: 0.0003125\n",
      "Epoch [8541/20000], Loss: 894.46435546875, Entropy 413.79345703125, Learning Rate: 0.0003125\n",
      "Epoch [8542/20000], Loss: 873.6123046875, Entropy 415.33349609375, Learning Rate: 0.0003125\n",
      "Epoch [8543/20000], Loss: 902.6624755859375, Entropy 396.2841796875, Learning Rate: 0.0003125\n",
      "Epoch [8544/20000], Loss: 891.1053466796875, Entropy 401.4344177246094, Learning Rate: 0.0003125\n",
      "Epoch [8545/20000], Loss: 920.1578979492188, Entropy 411.87445068359375, Learning Rate: 0.0003125\n",
      "Epoch [8546/20000], Loss: 883.8133544921875, Entropy 423.2738952636719, Learning Rate: 0.0003125\n",
      "Epoch [8547/20000], Loss: 842.72998046875, Entropy 411.1042175292969, Learning Rate: 0.0003125\n",
      "Epoch [8548/20000], Loss: 892.77294921875, Entropy 417.3961181640625, Learning Rate: 0.0003125\n",
      "Epoch [8549/20000], Loss: 893.7279052734375, Entropy 417.6383361816406, Learning Rate: 0.0003125\n",
      "Epoch [8550/20000], Loss: 838.2821044921875, Entropy 415.4166259765625, Learning Rate: 0.0003125\n",
      "Epoch [8551/20000], Loss: 887.629638671875, Entropy 405.0978088378906, Learning Rate: 0.0003125\n",
      "Epoch [8552/20000], Loss: 834.3018798828125, Entropy 429.734375, Learning Rate: 0.0003125\n",
      "Epoch [8553/20000], Loss: 911.667724609375, Entropy 399.1353759765625, Learning Rate: 0.0003125\n",
      "Epoch [8554/20000], Loss: 883.5009155273438, Entropy 395.60797119140625, Learning Rate: 0.0003125\n",
      "Epoch [8555/20000], Loss: 903.277099609375, Entropy 411.2027282714844, Learning Rate: 0.0003125\n",
      "Epoch [8556/20000], Loss: 902.893798828125, Entropy 404.7010192871094, Learning Rate: 0.0003125\n",
      "Epoch [8557/20000], Loss: 873.4475708007812, Entropy 410.39886474609375, Learning Rate: 0.0003125\n",
      "Epoch [8558/20000], Loss: 894.70068359375, Entropy 405.4367980957031, Learning Rate: 0.0003125\n",
      "Epoch [8559/20000], Loss: 877.4720458984375, Entropy 412.6724853515625, Learning Rate: 0.0003125\n",
      "Epoch [8560/20000], Loss: 869.40869140625, Entropy 411.6096496582031, Learning Rate: 0.0003125\n",
      "Epoch [8561/20000], Loss: 847.468017578125, Entropy 420.3158874511719, Learning Rate: 0.0003125\n",
      "Epoch [8562/20000], Loss: 874.067138671875, Entropy 403.1748352050781, Learning Rate: 0.0003125\n",
      "Epoch [8563/20000], Loss: 841.21484375, Entropy 410.1511535644531, Learning Rate: 0.0003125\n",
      "Epoch [8564/20000], Loss: 864.147705078125, Entropy 406.4689025878906, Learning Rate: 0.0003125\n",
      "Epoch [8565/20000], Loss: 905.5845336914062, Entropy 405.42791748046875, Learning Rate: 0.0003125\n",
      "Epoch [8566/20000], Loss: 861.3944091796875, Entropy 407.4889831542969, Learning Rate: 0.0003125\n",
      "Epoch [8567/20000], Loss: 868.2875366210938, Entropy 422.17864990234375, Learning Rate: 0.0003125\n",
      "Epoch [8568/20000], Loss: 841.167724609375, Entropy 415.6740417480469, Learning Rate: 0.0003125\n",
      "Epoch [8569/20000], Loss: 902.526123046875, Entropy 416.7311706542969, Learning Rate: 0.0003125\n",
      "Epoch [8570/20000], Loss: 866.828125, Entropy 409.7018737792969, Learning Rate: 0.0003125\n",
      "Epoch [8571/20000], Loss: 893.5277099609375, Entropy 414.654296875, Learning Rate: 0.0003125\n",
      "Epoch [8572/20000], Loss: 884.702392578125, Entropy 424.90966796875, Learning Rate: 0.0003125\n",
      "Epoch [8573/20000], Loss: 922.6828002929688, Entropy 405.56182861328125, Learning Rate: 0.0003125\n",
      "Epoch [8574/20000], Loss: 828.1996459960938, Entropy 418.34124755859375, Learning Rate: 0.0003125\n",
      "Epoch [8575/20000], Loss: 912.1631469726562, Entropy 408.90838623046875, Learning Rate: 0.0003125\n",
      "Epoch [8576/20000], Loss: 870.3294677734375, Entropy 410.8671875, Learning Rate: 0.0003125\n",
      "Epoch [8577/20000], Loss: 858.4727783203125, Entropy 429.4792175292969, Learning Rate: 0.0003125\n",
      "Epoch [8578/20000], Loss: 877.6412353515625, Entropy 395.9131164550781, Learning Rate: 0.0003125\n",
      "Epoch [8579/20000], Loss: 842.6014404296875, Entropy 411.8995361328125, Learning Rate: 0.0003125\n",
      "Epoch [8580/20000], Loss: 811.6486206054688, Entropy 414.06109619140625, Learning Rate: 0.0003125\n",
      "Epoch [8581/20000], Loss: 844.6552734375, Entropy 410.8050537109375, Learning Rate: 0.0003125\n",
      "Epoch [8582/20000], Loss: 941.1961059570312, Entropy 401.37030029296875, Learning Rate: 0.0003125\n",
      "Epoch [8583/20000], Loss: 899.7080078125, Entropy 395.9378967285156, Learning Rate: 0.0003125\n",
      "Epoch [8584/20000], Loss: 887.3448486328125, Entropy 419.1003723144531, Learning Rate: 0.0003125\n",
      "Epoch [8585/20000], Loss: 910.5567626953125, Entropy 404.6100769042969, Learning Rate: 0.0003125\n",
      "Epoch [8586/20000], Loss: 889.7239990234375, Entropy 404.624755859375, Learning Rate: 0.0003125\n",
      "Epoch [8587/20000], Loss: 862.6162109375, Entropy 414.0941162109375, Learning Rate: 0.0003125\n",
      "Epoch [8588/20000], Loss: 892.294189453125, Entropy 386.5785217285156, Learning Rate: 0.0003125\n",
      "Epoch [8589/20000], Loss: 872.07958984375, Entropy 410.7853698730469, Learning Rate: 0.0003125\n",
      "Epoch [8590/20000], Loss: 853.2474365234375, Entropy 392.0807800292969, Learning Rate: 0.0003125\n",
      "Epoch [8591/20000], Loss: 874.62646484375, Entropy 430.0369567871094, Learning Rate: 0.0003125\n",
      "Epoch [8592/20000], Loss: 868.2628173828125, Entropy 418.6197814941406, Learning Rate: 0.0003125\n",
      "Epoch [8593/20000], Loss: 895.6806640625, Entropy 413.79443359375, Learning Rate: 0.0003125\n",
      "Epoch [8594/20000], Loss: 989.1514892578125, Entropy 418.4022216796875, Learning Rate: 0.0003125\n",
      "Epoch [8595/20000], Loss: 918.970947265625, Entropy 395.1960144042969, Learning Rate: 0.0003125\n",
      "Epoch [8596/20000], Loss: 942.9376220703125, Entropy 387.7076110839844, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8597/20000], Loss: 883.1442260742188, Entropy 414.71600341796875, Learning Rate: 0.0003125\n",
      "Epoch [8598/20000], Loss: 905.564453125, Entropy 408.2471923828125, Learning Rate: 0.0003125\n",
      "Epoch [8599/20000], Loss: 836.3804931640625, Entropy 421.4239807128906, Learning Rate: 0.0003125\n",
      "Epoch [8600/20000], Loss: 878.931884765625, Entropy 416.9274597167969, Learning Rate: 0.0003125\n",
      "Epoch [8601/20000], Loss: 901.7593994140625, Entropy 413.2319030761719, Learning Rate: 0.0003125\n",
      "Epoch [8602/20000], Loss: 948.7010498046875, Entropy 413.90576171875, Learning Rate: 0.0003125\n",
      "Epoch [8603/20000], Loss: 869.0919189453125, Entropy 420.5632019042969, Learning Rate: 0.0003125\n",
      "Epoch [8604/20000], Loss: 861.7169189453125, Entropy 419.5402526855469, Learning Rate: 0.0003125\n",
      "Epoch [8605/20000], Loss: 881.6573486328125, Entropy 406.6060485839844, Learning Rate: 0.0003125\n",
      "Epoch [8606/20000], Loss: 932.380859375, Entropy 402.5922546386719, Learning Rate: 0.0003125\n",
      "Epoch [8607/20000], Loss: 860.746337890625, Entropy 408.1198425292969, Learning Rate: 0.0003125\n",
      "Epoch [8608/20000], Loss: 875.2595825195312, Entropy 399.96441650390625, Learning Rate: 0.0003125\n",
      "Epoch [8609/20000], Loss: 921.1575927734375, Entropy 399.7861633300781, Learning Rate: 0.0003125\n",
      "Epoch [8610/20000], Loss: 883.2919921875, Entropy 404.9402160644531, Learning Rate: 0.0003125\n",
      "Epoch [8611/20000], Loss: 860.4300537109375, Entropy 392.5640563964844, Learning Rate: 0.0003125\n",
      "Epoch [8612/20000], Loss: 920.359619140625, Entropy 408.5534362792969, Learning Rate: 0.0003125\n",
      "Epoch [8613/20000], Loss: 828.9942626953125, Entropy 431.9685974121094, Learning Rate: 0.0003125\n",
      "Epoch [8614/20000], Loss: 858.8212890625, Entropy 399.1028137207031, Learning Rate: 0.0003125\n",
      "Epoch [8615/20000], Loss: 878.4376220703125, Entropy 404.3863220214844, Learning Rate: 0.0003125\n",
      "Epoch [8616/20000], Loss: 851.8311767578125, Entropy 417.3438720703125, Learning Rate: 0.0003125\n",
      "Epoch [8617/20000], Loss: 849.261474609375, Entropy 419.3502197265625, Learning Rate: 0.0003125\n",
      "Epoch [8618/20000], Loss: 919.27978515625, Entropy 403.9930725097656, Learning Rate: 0.0003125\n",
      "Epoch [8619/20000], Loss: 876.6490478515625, Entropy 429.4397277832031, Learning Rate: 0.0003125\n",
      "Epoch [8620/20000], Loss: 860.6547241210938, Entropy 414.02496337890625, Learning Rate: 0.0003125\n",
      "Epoch [8621/20000], Loss: 865.0523681640625, Entropy 426.6026611328125, Learning Rate: 0.0003125\n",
      "Epoch [8622/20000], Loss: 843.4722900390625, Entropy 419.0904235839844, Learning Rate: 0.0003125\n",
      "Epoch [8623/20000], Loss: 852.820068359375, Entropy 416.7806091308594, Learning Rate: 0.0003125\n",
      "Epoch [8624/20000], Loss: 847.0128784179688, Entropy 424.74664306640625, Learning Rate: 0.0003125\n",
      "Epoch [8625/20000], Loss: 896.0025634765625, Entropy 407.6155090332031, Learning Rate: 0.0003125\n",
      "Epoch [8626/20000], Loss: 864.030029296875, Entropy 412.5351867675781, Learning Rate: 0.0003125\n",
      "Epoch [8627/20000], Loss: 878.1180419921875, Entropy 414.6048583984375, Learning Rate: 0.0003125\n",
      "Epoch [8628/20000], Loss: 875.953125, Entropy 402.7709045410156, Learning Rate: 0.0003125\n",
      "Epoch [8629/20000], Loss: 925.64404296875, Entropy 407.8316955566406, Learning Rate: 0.0003125\n",
      "Epoch [8630/20000], Loss: 862.6065673828125, Entropy 414.6629943847656, Learning Rate: 0.0003125\n",
      "Epoch [8631/20000], Loss: 900.8255615234375, Entropy 406.3850402832031, Learning Rate: 0.0003125\n",
      "Epoch [8632/20000], Loss: 873.1023559570312, Entropy 424.16656494140625, Learning Rate: 0.0003125\n",
      "Epoch [8633/20000], Loss: 881.46240234375, Entropy 426.1814880371094, Learning Rate: 0.0003125\n",
      "Epoch [8634/20000], Loss: 878.2333984375, Entropy 409.6940612792969, Learning Rate: 0.0003125\n",
      "Epoch [8635/20000], Loss: 879.7496337890625, Entropy 418.0784606933594, Learning Rate: 0.0003125\n",
      "Epoch [8636/20000], Loss: 862.3846435546875, Entropy 412.1676330566406, Learning Rate: 0.0003125\n",
      "Epoch [8637/20000], Loss: 869.287841796875, Entropy 403.8146667480469, Learning Rate: 0.0003125\n",
      "Epoch [8638/20000], Loss: 923.927001953125, Entropy 433.0411682128906, Learning Rate: 0.0003125\n",
      "Epoch [8639/20000], Loss: 854.4522705078125, Entropy 408.1392822265625, Learning Rate: 0.0003125\n",
      "Epoch [8640/20000], Loss: 858.8597412109375, Entropy 418.1153564453125, Learning Rate: 0.0003125\n",
      "Epoch [8641/20000], Loss: 906.091796875, Entropy 398.57763671875, Learning Rate: 0.0003125\n",
      "Epoch [8642/20000], Loss: 873.564208984375, Entropy 415.9672546386719, Learning Rate: 0.0003125\n",
      "Epoch [8643/20000], Loss: 862.366455078125, Entropy 398.0838317871094, Learning Rate: 0.0003125\n",
      "Epoch [8644/20000], Loss: 859.3145751953125, Entropy 412.349609375, Learning Rate: 0.0003125\n",
      "Epoch [8645/20000], Loss: 850.3905029296875, Entropy 402.9563903808594, Learning Rate: 0.0003125\n",
      "Epoch [8646/20000], Loss: 900.521484375, Entropy 419.6156921386719, Learning Rate: 0.0003125\n",
      "Epoch [8647/20000], Loss: 846.0162963867188, Entropy 418.13775634765625, Learning Rate: 0.0003125\n",
      "Epoch [8648/20000], Loss: 874.3287353515625, Entropy 416.4856872558594, Learning Rate: 0.0003125\n",
      "Epoch [8649/20000], Loss: 891.814697265625, Entropy 405.3660583496094, Learning Rate: 0.0003125\n",
      "Epoch [8650/20000], Loss: 922.861572265625, Entropy 409.2704162597656, Learning Rate: 0.0003125\n",
      "Epoch [8651/20000], Loss: 886.65869140625, Entropy 403.0196533203125, Learning Rate: 0.0003125\n",
      "Epoch [8652/20000], Loss: 839.476318359375, Entropy 409.8846130371094, Learning Rate: 0.0003125\n",
      "Epoch [8653/20000], Loss: 889.7373046875, Entropy 408.689208984375, Learning Rate: 0.0003125\n",
      "Epoch [8654/20000], Loss: 890.2491455078125, Entropy 403.0803527832031, Learning Rate: 0.0003125\n",
      "Epoch [8655/20000], Loss: 893.7499389648438, Entropy 396.25933837890625, Learning Rate: 0.0003125\n",
      "Epoch [8656/20000], Loss: 843.4183349609375, Entropy 408.5396423339844, Learning Rate: 0.0003125\n",
      "Epoch [8657/20000], Loss: 865.5010375976562, Entropy 412.24554443359375, Learning Rate: 0.0003125\n",
      "Epoch [8658/20000], Loss: 938.3361206054688, Entropy 407.72882080078125, Learning Rate: 0.0003125\n",
      "Epoch [8659/20000], Loss: 860.0415649414062, Entropy 428.22943115234375, Learning Rate: 0.0003125\n",
      "Epoch [8660/20000], Loss: 868.3223876953125, Entropy 410.9083557128906, Learning Rate: 0.0003125\n",
      "Epoch [8661/20000], Loss: 919.5135498046875, Entropy 412.8140563964844, Learning Rate: 0.0003125\n",
      "Epoch [8662/20000], Loss: 852.7330322265625, Entropy 416.8243713378906, Learning Rate: 0.0003125\n",
      "Epoch [8663/20000], Loss: 889.35009765625, Entropy 399.5418701171875, Learning Rate: 0.0003125\n",
      "Epoch [8664/20000], Loss: 902.0616455078125, Entropy 396.1705322265625, Learning Rate: 0.0003125\n",
      "Epoch [8665/20000], Loss: 877.6851806640625, Entropy 402.7110595703125, Learning Rate: 0.0003125\n",
      "Epoch [8666/20000], Loss: 873.9989013671875, Entropy 411.8975830078125, Learning Rate: 0.0003125\n",
      "Epoch [8667/20000], Loss: 874.8612060546875, Entropy 422.4378662109375, Learning Rate: 0.0003125\n",
      "Epoch [8668/20000], Loss: 862.0703125, Entropy 413.6210021972656, Learning Rate: 0.0003125\n",
      "Epoch [8669/20000], Loss: 878.7332763671875, Entropy 408.990234375, Learning Rate: 0.0003125\n",
      "Epoch [8670/20000], Loss: 939.707275390625, Entropy 402.3070983886719, Learning Rate: 0.0003125\n",
      "Epoch [8671/20000], Loss: 902.5831298828125, Entropy 411.8318176269531, Learning Rate: 0.0003125\n",
      "Epoch [8672/20000], Loss: 896.379638671875, Entropy 420.8564758300781, Learning Rate: 0.0003125\n",
      "Epoch [8673/20000], Loss: 886.372314453125, Entropy 406.2807312011719, Learning Rate: 0.0003125\n",
      "Epoch [8674/20000], Loss: 903.1173095703125, Entropy 422.9928894042969, Learning Rate: 0.0003125\n",
      "Epoch [8675/20000], Loss: 856.510498046875, Entropy 425.3249206542969, Learning Rate: 0.0003125\n",
      "Epoch [8676/20000], Loss: 874.349365234375, Entropy 428.1163330078125, Learning Rate: 0.0003125\n",
      "Epoch [8677/20000], Loss: 881.9782104492188, Entropy 404.54302978515625, Learning Rate: 0.0003125\n",
      "Epoch [8678/20000], Loss: 868.8311767578125, Entropy 415.2956237792969, Learning Rate: 0.0003125\n",
      "Epoch [8679/20000], Loss: 885.47412109375, Entropy 403.0577392578125, Learning Rate: 0.0003125\n",
      "Epoch [8680/20000], Loss: 846.8297119140625, Entropy 402.12939453125, Learning Rate: 0.0003125\n",
      "Epoch [8681/20000], Loss: 869.178466796875, Entropy 409.310302734375, Learning Rate: 0.0003125\n",
      "Epoch [8682/20000], Loss: 864.7764892578125, Entropy 408.4908142089844, Learning Rate: 0.0003125\n",
      "Epoch [8683/20000], Loss: 892.0950927734375, Entropy 419.3687744140625, Learning Rate: 0.0003125\n",
      "Epoch [8684/20000], Loss: 846.396728515625, Entropy 399.828125, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8685/20000], Loss: 862.2702026367188, Entropy 420.77056884765625, Learning Rate: 0.0003125\n",
      "Epoch [8686/20000], Loss: 865.976318359375, Entropy 419.1097106933594, Learning Rate: 0.0003125\n",
      "Epoch [8687/20000], Loss: 857.890380859375, Entropy 413.6053161621094, Learning Rate: 0.0003125\n",
      "Epoch [8688/20000], Loss: 878.2056884765625, Entropy 404.4385986328125, Learning Rate: 0.0003125\n",
      "Epoch [8689/20000], Loss: 886.5287475585938, Entropy 415.73480224609375, Learning Rate: 0.0003125\n",
      "Epoch [8690/20000], Loss: 926.2291870117188, Entropy 410.67987060546875, Learning Rate: 0.0003125\n",
      "Epoch [8691/20000], Loss: 892.0872802734375, Entropy 407.3072509765625, Learning Rate: 0.0003125\n",
      "Epoch [8692/20000], Loss: 889.509765625, Entropy 417.8755798339844, Learning Rate: 0.0003125\n",
      "Epoch [8693/20000], Loss: 873.899169921875, Entropy 416.2453308105469, Learning Rate: 0.0003125\n",
      "Epoch [8694/20000], Loss: 886.4774169921875, Entropy 436.4161682128906, Learning Rate: 0.0003125\n",
      "Epoch [8695/20000], Loss: 834.3349609375, Entropy 405.13037109375, Learning Rate: 0.0003125\n",
      "Epoch [8696/20000], Loss: 913.73388671875, Entropy 404.9051818847656, Learning Rate: 0.0003125\n",
      "Epoch [8697/20000], Loss: 911.551513671875, Entropy 412.0668640136719, Learning Rate: 0.0003125\n",
      "Epoch [8698/20000], Loss: 855.0504760742188, Entropy 418.57525634765625, Learning Rate: 0.0003125\n",
      "Epoch [8699/20000], Loss: 890.17138671875, Entropy 420.5953369140625, Learning Rate: 0.0003125\n",
      "Epoch [8700/20000], Loss: 853.983642578125, Entropy 424.3562316894531, Learning Rate: 0.0003125\n",
      "Epoch [8701/20000], Loss: 854.702880859375, Entropy 422.0544738769531, Learning Rate: 0.0003125\n",
      "Epoch [8702/20000], Loss: 870.8697509765625, Entropy 405.1026611328125, Learning Rate: 0.0003125\n",
      "Epoch [8703/20000], Loss: 893.4064331054688, Entropy 414.57366943359375, Learning Rate: 0.0003125\n",
      "Epoch [8704/20000], Loss: 904.3271484375, Entropy 393.7872314453125, Learning Rate: 0.0003125\n",
      "Epoch [8705/20000], Loss: 864.9628295898438, Entropy 415.20416259765625, Learning Rate: 0.0003125\n",
      "Epoch [8706/20000], Loss: 887.55615234375, Entropy 399.5008239746094, Learning Rate: 0.0003125\n",
      "Epoch [8707/20000], Loss: 872.9647827148438, Entropy 414.01409912109375, Learning Rate: 0.0003125\n",
      "Epoch [8708/20000], Loss: 859.7039184570312, Entropy 412.91021728515625, Learning Rate: 0.0003125\n",
      "Epoch [8709/20000], Loss: 831.568359375, Entropy 426.4718933105469, Learning Rate: 0.0003125\n",
      "Epoch [8710/20000], Loss: 935.5565185546875, Entropy 410.73876953125, Learning Rate: 0.0003125\n",
      "Epoch [8711/20000], Loss: 893.268310546875, Entropy 416.9594421386719, Learning Rate: 0.0003125\n",
      "Epoch [8712/20000], Loss: 828.1439208984375, Entropy 417.2663879394531, Learning Rate: 0.0003125\n",
      "Epoch [8713/20000], Loss: 865.6192016601562, Entropy 392.44439697265625, Learning Rate: 0.0003125\n",
      "Epoch [8714/20000], Loss: 852.2734375, Entropy 416.3616943359375, Learning Rate: 0.0003125\n",
      "Epoch [8715/20000], Loss: 869.9468994140625, Entropy 415.2194519042969, Learning Rate: 0.0003125\n",
      "Epoch [8716/20000], Loss: 850.7472534179688, Entropy 425.20306396484375, Learning Rate: 0.0003125\n",
      "Epoch [8717/20000], Loss: 915.92822265625, Entropy 402.04638671875, Learning Rate: 0.0003125\n",
      "Epoch [8718/20000], Loss: 896.3775024414062, Entropy 400.01373291015625, Learning Rate: 0.0003125\n",
      "Epoch [8719/20000], Loss: 885.908447265625, Entropy 415.9791259765625, Learning Rate: 0.0003125\n",
      "Epoch [8720/20000], Loss: 922.5736083984375, Entropy 413.298583984375, Learning Rate: 0.0003125\n",
      "Epoch [8721/20000], Loss: 857.8324584960938, Entropy 408.90911865234375, Learning Rate: 0.0003125\n",
      "Epoch [8722/20000], Loss: 853.1103515625, Entropy 409.9079895019531, Learning Rate: 0.0003125\n",
      "Epoch [8723/20000], Loss: 918.1029052734375, Entropy 415.1427917480469, Learning Rate: 0.0003125\n",
      "Epoch [8724/20000], Loss: 868.708251953125, Entropy 407.8531799316406, Learning Rate: 0.0003125\n",
      "Epoch [8725/20000], Loss: 897.0919189453125, Entropy 426.2303161621094, Learning Rate: 0.0003125\n",
      "Epoch [8726/20000], Loss: 864.7288208007812, Entropy 411.60552978515625, Learning Rate: 0.0003125\n",
      "Epoch [8727/20000], Loss: 843.40478515625, Entropy 401.8217468261719, Learning Rate: 0.0003125\n",
      "Epoch [8728/20000], Loss: 810.2664794921875, Entropy 419.7578125, Learning Rate: 0.0003125\n",
      "Epoch [8729/20000], Loss: 862.1617431640625, Entropy 399.783203125, Learning Rate: 0.0003125\n",
      "Epoch [8730/20000], Loss: 867.854248046875, Entropy 418.3049011230469, Learning Rate: 0.0003125\n",
      "Epoch [8731/20000], Loss: 853.8721313476562, Entropy 408.15045166015625, Learning Rate: 0.0003125\n",
      "Epoch [8732/20000], Loss: 843.1658325195312, Entropy 418.78131103515625, Learning Rate: 0.0003125\n",
      "Epoch [8733/20000], Loss: 857.9013671875, Entropy 409.1587829589844, Learning Rate: 0.0003125\n",
      "Epoch [8734/20000], Loss: 848.3975830078125, Entropy 415.9256286621094, Learning Rate: 0.0003125\n",
      "Epoch [8735/20000], Loss: 903.322998046875, Entropy 415.1728515625, Learning Rate: 0.0003125\n",
      "Epoch [8736/20000], Loss: 919.3883666992188, Entropy 400.89068603515625, Learning Rate: 0.0003125\n",
      "Epoch [8737/20000], Loss: 882.537353515625, Entropy 410.1124267578125, Learning Rate: 0.0003125\n",
      "Epoch [8738/20000], Loss: 870.467529296875, Entropy 416.9773864746094, Learning Rate: 0.0003125\n",
      "Epoch [8739/20000], Loss: 816.9494018554688, Entropy 413.51983642578125, Learning Rate: 0.0003125\n",
      "Epoch [8740/20000], Loss: 882.6343994140625, Entropy 402.34130859375, Learning Rate: 0.0003125\n",
      "Epoch [8741/20000], Loss: 870.5997314453125, Entropy 415.7268981933594, Learning Rate: 0.0003125\n",
      "Epoch [8742/20000], Loss: 862.0372314453125, Entropy 414.8558349609375, Learning Rate: 0.0003125\n",
      "Epoch [8743/20000], Loss: 828.8582763671875, Entropy 414.2391357421875, Learning Rate: 0.0003125\n",
      "Epoch [8744/20000], Loss: 862.7611083984375, Entropy 429.3031005859375, Learning Rate: 0.0003125\n",
      "Epoch [8745/20000], Loss: 888.4080810546875, Entropy 416.3885803222656, Learning Rate: 0.0003125\n",
      "Epoch [8746/20000], Loss: 829.474853515625, Entropy 406.4414367675781, Learning Rate: 0.0003125\n",
      "Epoch [8747/20000], Loss: 859.3114013671875, Entropy 412.79052734375, Learning Rate: 0.0003125\n",
      "Epoch [8748/20000], Loss: 863.6212158203125, Entropy 411.4064025878906, Learning Rate: 0.0003125\n",
      "Epoch [8749/20000], Loss: 841.25341796875, Entropy 424.1468505859375, Learning Rate: 0.0003125\n",
      "Epoch [8750/20000], Loss: 866.0354614257812, Entropy 414.24468994140625, Learning Rate: 0.0003125\n",
      "Epoch [8751/20000], Loss: 868.952880859375, Entropy 426.0662536621094, Learning Rate: 0.0003125\n",
      "Epoch [8752/20000], Loss: 849.8958129882812, Entropy 421.68060302734375, Learning Rate: 0.0003125\n",
      "Epoch [8753/20000], Loss: 880.892578125, Entropy 419.0398864746094, Learning Rate: 0.0003125\n",
      "Epoch [8754/20000], Loss: 868.40869140625, Entropy 414.47216796875, Learning Rate: 0.0003125\n",
      "Epoch [8755/20000], Loss: 902.5370483398438, Entropy 406.32061767578125, Learning Rate: 0.0003125\n",
      "Epoch [8756/20000], Loss: 858.10546875, Entropy 424.2758483886719, Learning Rate: 0.0003125\n",
      "Epoch [8757/20000], Loss: 866.2989501953125, Entropy 424.15185546875, Learning Rate: 0.0003125\n",
      "Epoch [8758/20000], Loss: 871.334716796875, Entropy 423.4892883300781, Learning Rate: 0.0003125\n",
      "Epoch [8759/20000], Loss: 848.292236328125, Entropy 433.158935546875, Learning Rate: 0.0003125\n",
      "Epoch [8760/20000], Loss: 881.1165771484375, Entropy 421.9885559082031, Learning Rate: 0.0003125\n",
      "Epoch [8761/20000], Loss: 892.3756103515625, Entropy 416.2155456542969, Learning Rate: 0.0003125\n",
      "Epoch [8762/20000], Loss: 882.4549560546875, Entropy 403.9758605957031, Learning Rate: 0.0003125\n",
      "Epoch [8763/20000], Loss: 897.3966064453125, Entropy 418.6826477050781, Learning Rate: 0.0003125\n",
      "Epoch [8764/20000], Loss: 886.9693603515625, Entropy 406.982177734375, Learning Rate: 0.0003125\n",
      "Epoch [8765/20000], Loss: 886.528076171875, Entropy 414.4795227050781, Learning Rate: 0.0003125\n",
      "Epoch [8766/20000], Loss: 898.5610961914062, Entropy 412.44464111328125, Learning Rate: 0.0003125\n",
      "Epoch [8767/20000], Loss: 850.7947998046875, Entropy 401.7102966308594, Learning Rate: 0.0003125\n",
      "Epoch [8768/20000], Loss: 858.8277587890625, Entropy 402.0901794433594, Learning Rate: 0.0003125\n",
      "Epoch [8769/20000], Loss: 904.1409301757812, Entropy 412.65057373046875, Learning Rate: 0.0003125\n",
      "Epoch [8770/20000], Loss: 868.2479248046875, Entropy 426.6896057128906, Learning Rate: 0.0003125\n",
      "Epoch [8771/20000], Loss: 864.3947143554688, Entropy 428.46124267578125, Learning Rate: 0.0003125\n",
      "Epoch [8772/20000], Loss: 827.810546875, Entropy 421.7934265136719, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8773/20000], Loss: 870.5859375, Entropy 407.9313659667969, Learning Rate: 0.0003125\n",
      "Epoch [8774/20000], Loss: 922.35693359375, Entropy 413.1174011230469, Learning Rate: 0.0003125\n",
      "Epoch [8775/20000], Loss: 918.81201171875, Entropy 404.6823425292969, Learning Rate: 0.0003125\n",
      "Epoch [8776/20000], Loss: 887.1018676757812, Entropy 419.61578369140625, Learning Rate: 0.0003125\n",
      "Epoch [8777/20000], Loss: 875.2730712890625, Entropy 400.93701171875, Learning Rate: 0.0003125\n",
      "Epoch [8778/20000], Loss: 899.952392578125, Entropy 418.0813293457031, Learning Rate: 0.0003125\n",
      "Epoch [8779/20000], Loss: 805.5699462890625, Entropy 438.9964599609375, Learning Rate: 0.0003125\n",
      "Epoch [8780/20000], Loss: 914.8009033203125, Entropy 409.1661376953125, Learning Rate: 0.0003125\n",
      "Epoch [8781/20000], Loss: 866.2935791015625, Entropy 426.7197570800781, Learning Rate: 0.0003125\n",
      "Epoch [8782/20000], Loss: 827.0335693359375, Entropy 421.4679260253906, Learning Rate: 0.0003125\n",
      "Epoch [8783/20000], Loss: 871.451904296875, Entropy 415.2635803222656, Learning Rate: 0.0003125\n",
      "Epoch [8784/20000], Loss: 828.6661376953125, Entropy 431.9019470214844, Learning Rate: 0.0003125\n",
      "Epoch [8785/20000], Loss: 868.5628051757812, Entropy 424.62396240234375, Learning Rate: 0.0003125\n",
      "Epoch [8786/20000], Loss: 883.2658081054688, Entropy 425.47015380859375, Learning Rate: 0.0003125\n",
      "Epoch [8787/20000], Loss: 825.855224609375, Entropy 424.2680358886719, Learning Rate: 0.0003125\n",
      "Epoch [8788/20000], Loss: 837.856201171875, Entropy 408.9017639160156, Learning Rate: 0.0003125\n",
      "Epoch [8789/20000], Loss: 934.5172119140625, Entropy 407.8288879394531, Learning Rate: 0.0003125\n",
      "Epoch [8790/20000], Loss: 842.04150390625, Entropy 416.9085388183594, Learning Rate: 0.0003125\n",
      "Epoch [8791/20000], Loss: 855.329345703125, Entropy 417.9918212890625, Learning Rate: 0.0003125\n",
      "Epoch [8792/20000], Loss: 874.993896484375, Entropy 427.5066223144531, Learning Rate: 0.0003125\n",
      "Epoch [8793/20000], Loss: 850.1898193359375, Entropy 435.2130432128906, Learning Rate: 0.0003125\n",
      "Epoch [8794/20000], Loss: 827.5799560546875, Entropy 420.9385070800781, Learning Rate: 0.0003125\n",
      "Epoch [8795/20000], Loss: 902.7088623046875, Entropy 393.8341369628906, Learning Rate: 0.0003125\n",
      "Epoch [8796/20000], Loss: 872.188232421875, Entropy 406.0520935058594, Learning Rate: 0.0003125\n",
      "Epoch [8797/20000], Loss: 923.3572387695312, Entropy 408.37945556640625, Learning Rate: 0.0003125\n",
      "Epoch [8798/20000], Loss: 858.9425659179688, Entropy 422.96929931640625, Learning Rate: 0.0003125\n",
      "Epoch [8799/20000], Loss: 877.44677734375, Entropy 415.8219299316406, Learning Rate: 0.0003125\n",
      "Epoch [8800/20000], Loss: 858.663330078125, Entropy 417.5791320800781, Learning Rate: 0.0003125\n",
      "Epoch [8801/20000], Loss: 832.0999755859375, Entropy 442.6402893066406, Learning Rate: 0.0003125\n",
      "Epoch [8802/20000], Loss: 887.8803100585938, Entropy 411.83343505859375, Learning Rate: 0.0003125\n",
      "Epoch [8803/20000], Loss: 891.2230224609375, Entropy 418.0435791015625, Learning Rate: 0.0003125\n",
      "Epoch [8804/20000], Loss: 901.9039306640625, Entropy 411.9807434082031, Learning Rate: 0.0003125\n",
      "Epoch [8805/20000], Loss: 898.433349609375, Entropy 412.8553466796875, Learning Rate: 0.0003125\n",
      "Epoch [8806/20000], Loss: 878.6422119140625, Entropy 427.7906799316406, Learning Rate: 0.0003125\n",
      "Epoch [8807/20000], Loss: 882.0556640625, Entropy 434.9224853515625, Learning Rate: 0.0003125\n",
      "Epoch [8808/20000], Loss: 879.96484375, Entropy 428.1832580566406, Learning Rate: 0.0003125\n",
      "Epoch [8809/20000], Loss: 881.5853271484375, Entropy 409.8799133300781, Learning Rate: 0.0003125\n",
      "Epoch [8810/20000], Loss: 842.1658935546875, Entropy 408.5728759765625, Learning Rate: 0.0003125\n",
      "Epoch [8811/20000], Loss: 841.209716796875, Entropy 425.000732421875, Learning Rate: 0.0003125\n",
      "Epoch [8812/20000], Loss: 887.212890625, Entropy 426.2694396972656, Learning Rate: 0.0003125\n",
      "Epoch [8813/20000], Loss: 845.8760375976562, Entropy 423.19000244140625, Learning Rate: 0.0003125\n",
      "Epoch [8814/20000], Loss: 820.7332153320312, Entropy 423.41156005859375, Learning Rate: 0.0003125\n",
      "Epoch [8815/20000], Loss: 886.3419799804688, Entropy 425.91143798828125, Learning Rate: 0.0003125\n",
      "Epoch [8816/20000], Loss: 858.8389892578125, Entropy 426.8099060058594, Learning Rate: 0.0003125\n",
      "Epoch [8817/20000], Loss: 855.0889282226562, Entropy 414.42999267578125, Learning Rate: 0.0003125\n",
      "Epoch [8818/20000], Loss: 888.3062133789062, Entropy 408.27740478515625, Learning Rate: 0.0003125\n",
      "Epoch [8819/20000], Loss: 855.4468994140625, Entropy 422.9190979003906, Learning Rate: 0.0003125\n",
      "Epoch [8820/20000], Loss: 937.7716064453125, Entropy 406.4371337890625, Learning Rate: 0.0003125\n",
      "Epoch [8821/20000], Loss: 843.3757934570312, Entropy 428.69671630859375, Learning Rate: 0.0003125\n",
      "Epoch [8822/20000], Loss: 862.522705078125, Entropy 417.7933044433594, Learning Rate: 0.0003125\n",
      "Epoch [8823/20000], Loss: 874.1664428710938, Entropy 402.81146240234375, Learning Rate: 0.0003125\n",
      "Epoch [8824/20000], Loss: 848.5140380859375, Entropy 420.8356628417969, Learning Rate: 0.0003125\n",
      "Epoch [8825/20000], Loss: 870.3584594726562, Entropy 440.94671630859375, Learning Rate: 0.0003125\n",
      "Epoch [8826/20000], Loss: 888.4290161132812, Entropy 441.04888916015625, Learning Rate: 0.0003125\n",
      "Epoch [8827/20000], Loss: 875.3159790039062, Entropy 410.49041748046875, Learning Rate: 0.0003125\n",
      "Epoch [8828/20000], Loss: 947.276123046875, Entropy 403.4544372558594, Learning Rate: 0.0003125\n",
      "Epoch [8829/20000], Loss: 851.069091796875, Entropy 423.8796691894531, Learning Rate: 0.0003125\n",
      "Epoch [8830/20000], Loss: 848.3245849609375, Entropy 418.1347961425781, Learning Rate: 0.0003125\n",
      "Epoch [8831/20000], Loss: 874.304931640625, Entropy 417.9177551269531, Learning Rate: 0.0003125\n",
      "Epoch [8832/20000], Loss: 907.2923583984375, Entropy 410.4479064941406, Learning Rate: 0.0003125\n",
      "Epoch [8833/20000], Loss: 843.0880737304688, Entropy 407.95623779296875, Learning Rate: 0.0003125\n",
      "Epoch [8834/20000], Loss: 859.4727783203125, Entropy 425.9645080566406, Learning Rate: 0.0003125\n",
      "Epoch [8835/20000], Loss: 903.1815185546875, Entropy 417.9781188964844, Learning Rate: 0.0003125\n",
      "Epoch [8836/20000], Loss: 889.247802734375, Entropy 412.6788330078125, Learning Rate: 0.0003125\n",
      "Epoch [8837/20000], Loss: 834.834228515625, Entropy 415.3712158203125, Learning Rate: 0.0003125\n",
      "Epoch [8838/20000], Loss: 867.1754150390625, Entropy 413.6063537597656, Learning Rate: 0.0003125\n",
      "Epoch [8839/20000], Loss: 867.0527954101562, Entropy 426.17584228515625, Learning Rate: 0.0003125\n",
      "Epoch [8840/20000], Loss: 819.4249877929688, Entropy 418.64837646484375, Learning Rate: 0.0003125\n",
      "Epoch [8841/20000], Loss: 838.9642944335938, Entropy 421.68182373046875, Learning Rate: 0.0003125\n",
      "Epoch [8842/20000], Loss: 842.6182861328125, Entropy 414.3313903808594, Learning Rate: 0.0003125\n",
      "Epoch [8843/20000], Loss: 834.0183715820312, Entropy 421.10015869140625, Learning Rate: 0.0003125\n",
      "Epoch [8844/20000], Loss: 852.8740234375, Entropy 409.8506164550781, Learning Rate: 0.0003125\n",
      "Epoch [8845/20000], Loss: 857.1279296875, Entropy 415.4328308105469, Learning Rate: 0.0003125\n",
      "Epoch [8846/20000], Loss: 895.908935546875, Entropy 418.2569580078125, Learning Rate: 0.0003125\n",
      "Epoch [8847/20000], Loss: 876.952880859375, Entropy 412.4987487792969, Learning Rate: 0.0003125\n",
      "Epoch [8848/20000], Loss: 804.7041625976562, Entropy 428.79937744140625, Learning Rate: 0.0003125\n",
      "Epoch [8849/20000], Loss: 827.693603515625, Entropy 415.3166198730469, Learning Rate: 0.0003125\n",
      "Epoch [8850/20000], Loss: 922.32275390625, Entropy 420.0646667480469, Learning Rate: 0.0003125\n",
      "Epoch [8851/20000], Loss: 880.982666015625, Entropy 418.9744567871094, Learning Rate: 0.0003125\n",
      "Epoch [8852/20000], Loss: 898.9639892578125, Entropy 404.2960205078125, Learning Rate: 0.0003125\n",
      "Epoch [8853/20000], Loss: 877.0467529296875, Entropy 416.5426940917969, Learning Rate: 0.0003125\n",
      "Epoch [8854/20000], Loss: 918.0924072265625, Entropy 412.025390625, Learning Rate: 0.0003125\n",
      "Epoch [8855/20000], Loss: 822.2918701171875, Entropy 438.8371887207031, Learning Rate: 0.0003125\n",
      "Epoch [8856/20000], Loss: 857.786865234375, Entropy 418.3788146972656, Learning Rate: 0.0003125\n",
      "Epoch [8857/20000], Loss: 860.076904296875, Entropy 412.700927734375, Learning Rate: 0.0003125\n",
      "Epoch [8858/20000], Loss: 873.75927734375, Entropy 417.8531494140625, Learning Rate: 0.0003125\n",
      "Epoch [8859/20000], Loss: 842.5933837890625, Entropy 432.0437927246094, Learning Rate: 0.0003125\n",
      "Epoch [8860/20000], Loss: 882.7060546875, Entropy 411.085693359375, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8861/20000], Loss: 905.390869140625, Entropy 414.0166320800781, Learning Rate: 0.0003125\n",
      "Epoch [8862/20000], Loss: 882.4296875, Entropy 433.4627990722656, Learning Rate: 0.0003125\n",
      "Epoch [8863/20000], Loss: 861.144287109375, Entropy 435.2286682128906, Learning Rate: 0.0003125\n",
      "Epoch [8864/20000], Loss: 905.3275756835938, Entropy 399.44757080078125, Learning Rate: 0.0003125\n",
      "Epoch [8865/20000], Loss: 882.810302734375, Entropy 421.9806213378906, Learning Rate: 0.0003125\n",
      "Epoch [8866/20000], Loss: 881.3424682617188, Entropy 402.02740478515625, Learning Rate: 0.0003125\n",
      "Epoch [8867/20000], Loss: 859.246337890625, Entropy 422.7681884765625, Learning Rate: 0.0003125\n",
      "Epoch [8868/20000], Loss: 861.61669921875, Entropy 419.3164367675781, Learning Rate: 0.0003125\n",
      "Epoch [8869/20000], Loss: 946.6563720703125, Entropy 410.6951599121094, Learning Rate: 0.0003125\n",
      "Epoch [8870/20000], Loss: 878.4989013671875, Entropy 403.0179748535156, Learning Rate: 0.0003125\n",
      "Epoch [8871/20000], Loss: 899.91650390625, Entropy 411.9791259765625, Learning Rate: 0.0003125\n",
      "Epoch [8872/20000], Loss: 898.9192504882812, Entropy 410.39678955078125, Learning Rate: 0.0003125\n",
      "Epoch [8873/20000], Loss: 894.8992919921875, Entropy 423.0024108886719, Learning Rate: 0.0003125\n",
      "Epoch [8874/20000], Loss: 868.934814453125, Entropy 427.1974182128906, Learning Rate: 0.0003125\n",
      "Epoch [8875/20000], Loss: 852.1807250976562, Entropy 409.64874267578125, Learning Rate: 0.0003125\n",
      "Epoch [8876/20000], Loss: 880.8333740234375, Entropy 420.5181579589844, Learning Rate: 0.0003125\n",
      "Epoch [8877/20000], Loss: 911.95703125, Entropy 414.8621826171875, Learning Rate: 0.0003125\n",
      "Epoch [8878/20000], Loss: 834.5010986328125, Entropy 415.2672424316406, Learning Rate: 0.0003125\n",
      "Epoch [8879/20000], Loss: 873.2188720703125, Entropy 415.2527160644531, Learning Rate: 0.0003125\n",
      "Epoch [8880/20000], Loss: 837.244384765625, Entropy 429.2490539550781, Learning Rate: 0.0003125\n",
      "Epoch [8881/20000], Loss: 889.3853149414062, Entropy 426.45306396484375, Learning Rate: 0.0003125\n",
      "Epoch [8882/20000], Loss: 900.11083984375, Entropy 426.0803527832031, Learning Rate: 0.0003125\n",
      "Epoch [8883/20000], Loss: 867.6171264648438, Entropy 404.06280517578125, Learning Rate: 0.0003125\n",
      "Epoch [8884/20000], Loss: 884.9424438476562, Entropy 404.51678466796875, Learning Rate: 0.0003125\n",
      "Epoch [8885/20000], Loss: 863.4114990234375, Entropy 412.0852355957031, Learning Rate: 0.0003125\n",
      "Epoch [8886/20000], Loss: 886.20947265625, Entropy 409.8503723144531, Learning Rate: 0.0003125\n",
      "Epoch [8887/20000], Loss: 892.9605712890625, Entropy 410.5360412597656, Learning Rate: 0.0003125\n",
      "Epoch [8888/20000], Loss: 890.0064697265625, Entropy 410.1335754394531, Learning Rate: 0.0003125\n",
      "Epoch [8889/20000], Loss: 859.4361572265625, Entropy 410.5081787109375, Learning Rate: 0.0003125\n",
      "Epoch [8890/20000], Loss: 864.2271728515625, Entropy 417.7052307128906, Learning Rate: 0.0003125\n",
      "Epoch [8891/20000], Loss: 901.125244140625, Entropy 397.995849609375, Learning Rate: 0.0003125\n",
      "Epoch [8892/20000], Loss: 900.1690063476562, Entropy 426.65460205078125, Learning Rate: 0.0003125\n",
      "Epoch [8893/20000], Loss: 940.882568359375, Entropy 403.4002380371094, Learning Rate: 0.0003125\n",
      "Epoch [8894/20000], Loss: 890.749755859375, Entropy 415.2091979980469, Learning Rate: 0.0003125\n",
      "Epoch [8895/20000], Loss: 832.985595703125, Entropy 429.5680847167969, Learning Rate: 0.0003125\n",
      "Epoch [8896/20000], Loss: 885.1431884765625, Entropy 411.7557373046875, Learning Rate: 0.0003125\n",
      "Epoch [8897/20000], Loss: 925.0944213867188, Entropy 411.41802978515625, Learning Rate: 0.0003125\n",
      "Epoch [8898/20000], Loss: 914.5750732421875, Entropy 410.5946044921875, Learning Rate: 0.0003125\n",
      "Epoch [8899/20000], Loss: 866.5048828125, Entropy 416.55810546875, Learning Rate: 0.0003125\n",
      "Epoch [8900/20000], Loss: 835.2728881835938, Entropy 425.68804931640625, Learning Rate: 0.0003125\n",
      "Epoch [8901/20000], Loss: 922.9939575195312, Entropy 426.31268310546875, Learning Rate: 0.0003125\n",
      "Epoch [8902/20000], Loss: 833.6072998046875, Entropy 426.3944396972656, Learning Rate: 0.0003125\n",
      "Epoch [8903/20000], Loss: 870.3917236328125, Entropy 415.9576416015625, Learning Rate: 0.0003125\n",
      "Epoch [8904/20000], Loss: 869.9688720703125, Entropy 425.9634704589844, Learning Rate: 0.0003125\n",
      "Epoch [8905/20000], Loss: 881.6707153320312, Entropy 422.27532958984375, Learning Rate: 0.0003125\n",
      "Epoch [8906/20000], Loss: 908.06103515625, Entropy 407.7509765625, Learning Rate: 0.0003125\n",
      "Epoch [8907/20000], Loss: 869.25732421875, Entropy 423.9742126464844, Learning Rate: 0.0003125\n",
      "Epoch [8908/20000], Loss: 897.5830688476562, Entropy 415.58648681640625, Learning Rate: 0.0003125\n",
      "Epoch [8909/20000], Loss: 883.2506103515625, Entropy 431.5758361816406, Learning Rate: 0.0003125\n",
      "Epoch [8910/20000], Loss: 871.6251220703125, Entropy 419.6051025390625, Learning Rate: 0.0003125\n",
      "Epoch [8911/20000], Loss: 901.0064697265625, Entropy 427.1561279296875, Learning Rate: 0.0003125\n",
      "Epoch [8912/20000], Loss: 873.0643310546875, Entropy 421.669921875, Learning Rate: 0.0003125\n",
      "Epoch [8913/20000], Loss: 898.77294921875, Entropy 412.7214660644531, Learning Rate: 0.0003125\n",
      "Epoch [8914/20000], Loss: 868.7059326171875, Entropy 419.57666015625, Learning Rate: 0.0003125\n",
      "Epoch [8915/20000], Loss: 877.0875244140625, Entropy 406.935302734375, Learning Rate: 0.0003125\n",
      "Epoch [8916/20000], Loss: 918.6971435546875, Entropy 433.913818359375, Learning Rate: 0.0003125\n",
      "Epoch [8917/20000], Loss: 881.2216796875, Entropy 428.114990234375, Learning Rate: 0.0003125\n",
      "Epoch [8918/20000], Loss: 874.2093505859375, Entropy 406.673828125, Learning Rate: 0.0003125\n",
      "Epoch [8919/20000], Loss: 872.406494140625, Entropy 417.3541259765625, Learning Rate: 0.0003125\n",
      "Epoch [8920/20000], Loss: 886.369140625, Entropy 420.2919006347656, Learning Rate: 0.0003125\n",
      "Epoch [8921/20000], Loss: 925.3826904296875, Entropy 422.7663269042969, Learning Rate: 0.0003125\n",
      "Epoch [8922/20000], Loss: 864.21533203125, Entropy 420.9615173339844, Learning Rate: 0.0003125\n",
      "Epoch [8923/20000], Loss: 933.7772216796875, Entropy 414.8025207519531, Learning Rate: 0.0003125\n",
      "Epoch [8924/20000], Loss: 877.9703369140625, Entropy 417.3516845703125, Learning Rate: 0.0003125\n",
      "Epoch [8925/20000], Loss: 875.074951171875, Entropy 405.7822265625, Learning Rate: 0.0003125\n",
      "Epoch [8926/20000], Loss: 866.6602172851562, Entropy 436.81793212890625, Learning Rate: 0.0003125\n",
      "Epoch [8927/20000], Loss: 841.0345458984375, Entropy 424.9609680175781, Learning Rate: 0.0003125\n",
      "Epoch [8928/20000], Loss: 868.1064453125, Entropy 412.2244567871094, Learning Rate: 0.0003125\n",
      "Epoch [8929/20000], Loss: 876.7706298828125, Entropy 418.5994567871094, Learning Rate: 0.0003125\n",
      "Epoch [8930/20000], Loss: 888.664306640625, Entropy 401.567138671875, Learning Rate: 0.0003125\n",
      "Epoch [8931/20000], Loss: 844.9856567382812, Entropy 430.86651611328125, Learning Rate: 0.0003125\n",
      "Epoch [8932/20000], Loss: 861.173828125, Entropy 422.1882629394531, Learning Rate: 0.0003125\n",
      "Epoch [8933/20000], Loss: 989.6785278320312, Entropy 421.98077392578125, Learning Rate: 0.0003125\n",
      "Epoch [8934/20000], Loss: 894.766357421875, Entropy 413.6858825683594, Learning Rate: 0.0003125\n",
      "Epoch [8935/20000], Loss: 912.3495483398438, Entropy 424.48577880859375, Learning Rate: 0.0003125\n",
      "Epoch [8936/20000], Loss: 829.1165771484375, Entropy 417.0943603515625, Learning Rate: 0.0003125\n",
      "Epoch [8937/20000], Loss: 906.66357421875, Entropy 417.0171813964844, Learning Rate: 0.0003125\n",
      "Epoch [8938/20000], Loss: 899.7766723632812, Entropy 413.69366455078125, Learning Rate: 0.0003125\n",
      "Epoch [8939/20000], Loss: 875.01806640625, Entropy 419.2437744140625, Learning Rate: 0.0003125\n",
      "Epoch [8940/20000], Loss: 881.6719970703125, Entropy 425.5400695800781, Learning Rate: 0.0003125\n",
      "Epoch [8941/20000], Loss: 838.2115478515625, Entropy 428.6094970703125, Learning Rate: 0.0003125\n",
      "Epoch [8942/20000], Loss: 856.650390625, Entropy 435.3296813964844, Learning Rate: 0.0003125\n",
      "Epoch [8943/20000], Loss: 868.1425170898438, Entropy 425.02459716796875, Learning Rate: 0.0003125\n",
      "Epoch [8944/20000], Loss: 906.2032470703125, Entropy 424.2701110839844, Learning Rate: 0.0003125\n",
      "Epoch [8945/20000], Loss: 850.0403442382812, Entropy 420.28155517578125, Learning Rate: 0.0003125\n",
      "Epoch [8946/20000], Loss: 885.13134765625, Entropy 402.8061218261719, Learning Rate: 0.0003125\n",
      "Epoch [8947/20000], Loss: 887.9754638671875, Entropy 405.7834167480469, Learning Rate: 0.0003125\n",
      "Epoch [8948/20000], Loss: 843.4054565429688, Entropy 421.94451904296875, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8949/20000], Loss: 878.3876953125, Entropy 422.1329345703125, Learning Rate: 0.0003125\n",
      "Epoch [8950/20000], Loss: 884.797119140625, Entropy 421.9458923339844, Learning Rate: 0.0003125\n",
      "Epoch [8951/20000], Loss: 853.3765869140625, Entropy 427.7156066894531, Learning Rate: 0.0003125\n",
      "Epoch [8952/20000], Loss: 899.69580078125, Entropy 415.9891357421875, Learning Rate: 0.0003125\n",
      "Epoch [8953/20000], Loss: 887.781982421875, Entropy 425.9992370605469, Learning Rate: 0.0003125\n",
      "Epoch [8954/20000], Loss: 893.83447265625, Entropy 425.3464660644531, Learning Rate: 0.0003125\n",
      "Epoch [8955/20000], Loss: 849.3955078125, Entropy 411.5791320800781, Learning Rate: 0.0003125\n",
      "Epoch [8956/20000], Loss: 904.43896484375, Entropy 407.9828796386719, Learning Rate: 0.0003125\n",
      "Epoch [8957/20000], Loss: 893.0289306640625, Entropy 423.65576171875, Learning Rate: 0.0003125\n",
      "Epoch [8958/20000], Loss: 847.595703125, Entropy 413.017578125, Learning Rate: 0.0003125\n",
      "Epoch [8959/20000], Loss: 829.71630859375, Entropy 406.4856262207031, Learning Rate: 0.0003125\n",
      "Epoch [8960/20000], Loss: 861.781982421875, Entropy 412.7947082519531, Learning Rate: 0.0003125\n",
      "Epoch [8961/20000], Loss: 923.8035888671875, Entropy 420.5072021484375, Learning Rate: 0.0003125\n",
      "Epoch [8962/20000], Loss: 889.8375244140625, Entropy 406.951171875, Learning Rate: 0.0003125\n",
      "Epoch [8963/20000], Loss: 859.0411376953125, Entropy 425.0896911621094, Learning Rate: 0.0003125\n",
      "Epoch [8964/20000], Loss: 871.092529296875, Entropy 418.0749206542969, Learning Rate: 0.0003125\n",
      "Epoch [8965/20000], Loss: 885.8463745117188, Entropy 428.82672119140625, Learning Rate: 0.0003125\n",
      "Epoch [8966/20000], Loss: 870.524658203125, Entropy 419.8404541015625, Learning Rate: 0.0003125\n",
      "Epoch [8967/20000], Loss: 843.912841796875, Entropy 421.9527282714844, Learning Rate: 0.0003125\n",
      "Epoch [8968/20000], Loss: 883.7620849609375, Entropy 426.2738037109375, Learning Rate: 0.0003125\n",
      "Epoch [8969/20000], Loss: 924.9056396484375, Entropy 423.146484375, Learning Rate: 0.0003125\n",
      "Epoch [8970/20000], Loss: 882.6751708984375, Entropy 413.5007019042969, Learning Rate: 0.0003125\n",
      "Epoch [8971/20000], Loss: 828.2603149414062, Entropy 426.45819091796875, Learning Rate: 0.0003125\n",
      "Epoch [8972/20000], Loss: 868.6785278320312, Entropy 435.14483642578125, Learning Rate: 0.0003125\n",
      "Epoch [8973/20000], Loss: 859.9804077148438, Entropy 420.78131103515625, Learning Rate: 0.0003125\n",
      "Epoch [8974/20000], Loss: 895.9083862304688, Entropy 415.76788330078125, Learning Rate: 0.0003125\n",
      "Epoch [8975/20000], Loss: 817.2081298828125, Entropy 416.1929626464844, Learning Rate: 0.0003125\n",
      "Epoch [8976/20000], Loss: 869.4542846679688, Entropy 414.41168212890625, Learning Rate: 0.0003125\n",
      "Epoch [8977/20000], Loss: 881.6842651367188, Entropy 425.76129150390625, Learning Rate: 0.0003125\n",
      "Epoch [8978/20000], Loss: 843.04150390625, Entropy 424.1227722167969, Learning Rate: 0.0003125\n",
      "Epoch [8979/20000], Loss: 845.5564575195312, Entropy 420.28802490234375, Learning Rate: 0.0003125\n",
      "Epoch [8980/20000], Loss: 816.042236328125, Entropy 425.7408142089844, Learning Rate: 0.0003125\n",
      "Epoch [8981/20000], Loss: 887.9227294921875, Entropy 435.5766296386719, Learning Rate: 0.0003125\n",
      "Epoch [8982/20000], Loss: 860.9110107421875, Entropy 415.7012939453125, Learning Rate: 0.0003125\n",
      "Epoch [8983/20000], Loss: 844.8645629882812, Entropy 427.97320556640625, Learning Rate: 0.0003125\n",
      "Epoch [8984/20000], Loss: 847.9783935546875, Entropy 429.3890380859375, Learning Rate: 0.0003125\n",
      "Epoch [8985/20000], Loss: 814.7513427734375, Entropy 432.1054992675781, Learning Rate: 0.0003125\n",
      "Epoch [8986/20000], Loss: 865.7324829101562, Entropy 418.80035400390625, Learning Rate: 0.0003125\n",
      "Epoch [8987/20000], Loss: 840.415771484375, Entropy 426.7804260253906, Learning Rate: 0.0003125\n",
      "Epoch [8988/20000], Loss: 847.2369384765625, Entropy 433.5354309082031, Learning Rate: 0.0003125\n",
      "Epoch [8989/20000], Loss: 857.3521118164062, Entropy 417.63970947265625, Learning Rate: 0.0003125\n",
      "Epoch [8990/20000], Loss: 857.5026245117188, Entropy 410.18951416015625, Learning Rate: 0.0003125\n",
      "Epoch [8991/20000], Loss: 904.8963623046875, Entropy 414.6673583984375, Learning Rate: 0.0003125\n",
      "Epoch [8992/20000], Loss: 897.4547729492188, Entropy 414.95318603515625, Learning Rate: 0.0003125\n",
      "Epoch [8993/20000], Loss: 849.0016479492188, Entropy 420.50628662109375, Learning Rate: 0.0003125\n",
      "Epoch [8994/20000], Loss: 892.4368896484375, Entropy 415.70458984375, Learning Rate: 0.0003125\n",
      "Epoch [8995/20000], Loss: 859.5444946289062, Entropy 411.44744873046875, Learning Rate: 0.0003125\n",
      "Epoch [8996/20000], Loss: 862.9134521484375, Entropy 415.1819152832031, Learning Rate: 0.0003125\n",
      "Epoch [8997/20000], Loss: 893.6277465820312, Entropy 409.71514892578125, Learning Rate: 0.0003125\n",
      "Epoch [8998/20000], Loss: 830.9437255859375, Entropy 422.0848083496094, Learning Rate: 0.0003125\n",
      "Epoch [8999/20000], Loss: 913.7484130859375, Entropy 418.5357666015625, Learning Rate: 0.0003125\n",
      "Epoch [9000/20000], Loss: 848.3909301757812, Entropy 427.72406005859375, Learning Rate: 0.0003125\n",
      "Epoch [9001/20000], Loss: 883.3661499023438, Entropy 429.17022705078125, Learning Rate: 0.0003125\n",
      "Epoch [9002/20000], Loss: 860.826171875, Entropy 421.3189697265625, Learning Rate: 0.0003125\n",
      "Epoch [9003/20000], Loss: 878.1640625, Entropy 418.8274841308594, Learning Rate: 0.0003125\n",
      "Epoch [9004/20000], Loss: 928.564697265625, Entropy 426.1852722167969, Learning Rate: 0.0003125\n",
      "Epoch [9005/20000], Loss: 858.9833984375, Entropy 435.7620544433594, Learning Rate: 0.0003125\n",
      "Epoch [9006/20000], Loss: 890.6090087890625, Entropy 430.1180419921875, Learning Rate: 0.0003125\n",
      "Epoch [9007/20000], Loss: 900.0115356445312, Entropy 419.87884521484375, Learning Rate: 0.0003125\n",
      "Epoch [9008/20000], Loss: 903.52783203125, Entropy 406.566162109375, Learning Rate: 0.0003125\n",
      "Epoch [9009/20000], Loss: 844.8057861328125, Entropy 424.0796813964844, Learning Rate: 0.0003125\n",
      "Epoch [9010/20000], Loss: 879.4794921875, Entropy 411.2219543457031, Learning Rate: 0.0003125\n",
      "Epoch [9011/20000], Loss: 928.3533325195312, Entropy 421.84661865234375, Learning Rate: 0.0003125\n",
      "Epoch [9012/20000], Loss: 843.818603515625, Entropy 436.79248046875, Learning Rate: 0.0003125\n",
      "Epoch [9013/20000], Loss: 879.83935546875, Entropy 408.9264831542969, Learning Rate: 0.0003125\n",
      "Epoch [9014/20000], Loss: 894.1715087890625, Entropy 407.4152526855469, Learning Rate: 0.0003125\n",
      "Epoch [9015/20000], Loss: 863.287841796875, Entropy 440.65576171875, Learning Rate: 0.0003125\n",
      "Epoch [9016/20000], Loss: 895.5028076171875, Entropy 399.5220947265625, Learning Rate: 0.0003125\n",
      "Epoch [9017/20000], Loss: 859.89306640625, Entropy 430.4076843261719, Learning Rate: 0.0003125\n",
      "Epoch [9018/20000], Loss: 846.220947265625, Entropy 438.7002258300781, Learning Rate: 0.0003125\n",
      "Epoch [9019/20000], Loss: 900.6263427734375, Entropy 420.4980773925781, Learning Rate: 0.0003125\n",
      "Epoch [9020/20000], Loss: 867.823974609375, Entropy 419.1866455078125, Learning Rate: 0.0003125\n",
      "Epoch [9021/20000], Loss: 864.7630615234375, Entropy 431.4513854980469, Learning Rate: 0.0003125\n",
      "Epoch [9022/20000], Loss: 862.0659790039062, Entropy 403.50286865234375, Learning Rate: 0.0003125\n",
      "Epoch [9023/20000], Loss: 851.0971069335938, Entropy 427.08612060546875, Learning Rate: 0.0003125\n",
      "Epoch [9024/20000], Loss: 841.5899658203125, Entropy 415.95947265625, Learning Rate: 0.0003125\n",
      "Epoch [9025/20000], Loss: 902.002197265625, Entropy 422.90625, Learning Rate: 0.0003125\n",
      "Epoch [9026/20000], Loss: 864.135986328125, Entropy 423.4870300292969, Learning Rate: 0.0003125\n",
      "Epoch [9027/20000], Loss: 888.471923828125, Entropy 421.8443298339844, Learning Rate: 0.0003125\n",
      "Epoch [9028/20000], Loss: 889.8050537109375, Entropy 412.9996032714844, Learning Rate: 0.0003125\n",
      "Epoch [9029/20000], Loss: 861.8640747070312, Entropy 420.67193603515625, Learning Rate: 0.0003125\n",
      "Epoch [9030/20000], Loss: 795.3807373046875, Entropy 438.0281982421875, Learning Rate: 0.0003125\n",
      "Epoch [9031/20000], Loss: 836.2350463867188, Entropy 416.60784912109375, Learning Rate: 0.0003125\n",
      "Epoch [9032/20000], Loss: 867.9632568359375, Entropy 412.5486755371094, Learning Rate: 0.0003125\n",
      "Epoch [9033/20000], Loss: 859.2263793945312, Entropy 417.71282958984375, Learning Rate: 0.0003125\n",
      "Epoch [9034/20000], Loss: 846.6507568359375, Entropy 422.4365539550781, Learning Rate: 0.0003125\n",
      "Epoch [9035/20000], Loss: 907.2893676757812, Entropy 414.10565185546875, Learning Rate: 0.0003125\n",
      "Epoch [9036/20000], Loss: 864.622314453125, Entropy 424.6791687011719, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9037/20000], Loss: 860.68798828125, Entropy 432.5957336425781, Learning Rate: 0.0003125\n",
      "Epoch [9038/20000], Loss: 801.9795532226562, Entropy 440.58331298828125, Learning Rate: 0.0003125\n",
      "Epoch [9039/20000], Loss: 899.4053344726562, Entropy 421.03570556640625, Learning Rate: 0.0003125\n",
      "Epoch [9040/20000], Loss: 876.1796875, Entropy 438.6902160644531, Learning Rate: 0.0003125\n",
      "Epoch [9041/20000], Loss: 902.45556640625, Entropy 413.1275939941406, Learning Rate: 0.0003125\n",
      "Epoch [9042/20000], Loss: 881.9293212890625, Entropy 427.5897521972656, Learning Rate: 0.0003125\n",
      "Epoch [9043/20000], Loss: 842.75732421875, Entropy 432.3927917480469, Learning Rate: 0.0003125\n",
      "Epoch [9044/20000], Loss: 866.026611328125, Entropy 424.164794921875, Learning Rate: 0.0003125\n",
      "Epoch [9045/20000], Loss: 897.9157104492188, Entropy 421.37103271484375, Learning Rate: 0.0003125\n",
      "Epoch [9046/20000], Loss: 861.154541015625, Entropy 418.2151184082031, Learning Rate: 0.0003125\n",
      "Epoch [9047/20000], Loss: 922.0809326171875, Entropy 430.31982421875, Learning Rate: 0.0003125\n",
      "Epoch [9048/20000], Loss: 869.68115234375, Entropy 424.5858459472656, Learning Rate: 0.0003125\n",
      "Epoch [9049/20000], Loss: 901.8944702148438, Entropy 407.78753662109375, Learning Rate: 0.0003125\n",
      "Epoch [9050/20000], Loss: 869.9189453125, Entropy 416.1982421875, Learning Rate: 0.0003125\n",
      "Epoch [9051/20000], Loss: 889.9612426757812, Entropy 410.78948974609375, Learning Rate: 0.0003125\n",
      "Epoch [9052/20000], Loss: 870.461669921875, Entropy 417.13818359375, Learning Rate: 0.0003125\n",
      "Epoch [9053/20000], Loss: 901.885009765625, Entropy 405.1025390625, Learning Rate: 0.0003125\n",
      "Epoch [9054/20000], Loss: 892.3414306640625, Entropy 419.7635498046875, Learning Rate: 0.0003125\n",
      "Epoch [9055/20000], Loss: 875.5042724609375, Entropy 413.1429443359375, Learning Rate: 0.0003125\n",
      "Epoch [9056/20000], Loss: 863.6898803710938, Entropy 426.32855224609375, Learning Rate: 0.0003125\n",
      "Epoch [9057/20000], Loss: 827.6893310546875, Entropy 437.5797424316406, Learning Rate: 0.0003125\n",
      "Epoch [9058/20000], Loss: 891.9224243164062, Entropy 413.80755615234375, Learning Rate: 0.0003125\n",
      "Epoch [9059/20000], Loss: 858.0258178710938, Entropy 422.51678466796875, Learning Rate: 0.0003125\n",
      "Epoch [9060/20000], Loss: 867.788818359375, Entropy 432.9770202636719, Learning Rate: 0.0003125\n",
      "Epoch [9061/20000], Loss: 883.9329833984375, Entropy 423.1281433105469, Learning Rate: 0.0003125\n",
      "Epoch [9062/20000], Loss: 856.492431640625, Entropy 411.5110778808594, Learning Rate: 0.0003125\n",
      "Epoch [9063/20000], Loss: 834.55712890625, Entropy 426.4806213378906, Learning Rate: 0.0003125\n",
      "Epoch [9064/20000], Loss: 850.085693359375, Entropy 430.9566955566406, Learning Rate: 0.0003125\n",
      "Epoch [9065/20000], Loss: 873.2643432617188, Entropy 424.58331298828125, Learning Rate: 0.0003125\n",
      "Epoch [9066/20000], Loss: 847.982177734375, Entropy 416.5673828125, Learning Rate: 0.0003125\n",
      "Epoch [9067/20000], Loss: 849.99267578125, Entropy 433.464599609375, Learning Rate: 0.0003125\n",
      "Epoch [9068/20000], Loss: 837.8958740234375, Entropy 445.8984680175781, Learning Rate: 0.0003125\n",
      "Epoch [9069/20000], Loss: 941.0693359375, Entropy 412.331787109375, Learning Rate: 0.0003125\n",
      "Epoch [9070/20000], Loss: 877.386962890625, Entropy 431.0293273925781, Learning Rate: 0.0003125\n",
      "Epoch [9071/20000], Loss: 901.1217651367188, Entropy 422.79583740234375, Learning Rate: 0.0003125\n",
      "Epoch [9072/20000], Loss: 828.2597045898438, Entropy 437.22552490234375, Learning Rate: 0.0003125\n",
      "Epoch [9073/20000], Loss: 899.273681640625, Entropy 426.1417236328125, Learning Rate: 0.0003125\n",
      "Epoch [9074/20000], Loss: 908.0419311523438, Entropy 423.55865478515625, Learning Rate: 0.0003125\n",
      "Epoch [9075/20000], Loss: 846.8521728515625, Entropy 431.7462463378906, Learning Rate: 0.0003125\n",
      "Epoch [9076/20000], Loss: 864.1414794921875, Entropy 429.8107604980469, Learning Rate: 0.0003125\n",
      "Epoch [9077/20000], Loss: 860.7457275390625, Entropy 427.1150207519531, Learning Rate: 0.0003125\n",
      "Epoch [9078/20000], Loss: 833.4803466796875, Entropy 432.6992492675781, Learning Rate: 0.0003125\n",
      "Epoch [9079/20000], Loss: 874.0429077148438, Entropy 433.48333740234375, Learning Rate: 0.0003125\n",
      "Epoch [9080/20000], Loss: 864.5053100585938, Entropy 409.98529052734375, Learning Rate: 0.0003125\n",
      "Epoch [9081/20000], Loss: 862.1890869140625, Entropy 423.7469482421875, Learning Rate: 0.0003125\n",
      "Epoch [9082/20000], Loss: 849.357421875, Entropy 415.1481018066406, Learning Rate: 0.0003125\n",
      "Epoch [9083/20000], Loss: 890.767333984375, Entropy 409.84716796875, Learning Rate: 0.0003125\n",
      "Epoch [9084/20000], Loss: 850.634765625, Entropy 425.8843688964844, Learning Rate: 0.0003125\n",
      "Epoch [9085/20000], Loss: 905.4071655273438, Entropy 417.31182861328125, Learning Rate: 0.0003125\n",
      "Epoch [9086/20000], Loss: 862.056640625, Entropy 434.7654724121094, Learning Rate: 0.0003125\n",
      "Epoch [9087/20000], Loss: 823.6275024414062, Entropy 425.36187744140625, Learning Rate: 0.0003125\n",
      "Epoch [9088/20000], Loss: 882.3133544921875, Entropy 420.4178771972656, Learning Rate: 0.0003125\n",
      "Epoch [9089/20000], Loss: 887.2836303710938, Entropy 425.06195068359375, Learning Rate: 0.0003125\n",
      "Epoch [9090/20000], Loss: 849.59765625, Entropy 411.5945129394531, Learning Rate: 0.0003125\n",
      "Epoch [9091/20000], Loss: 857.5621337890625, Entropy 417.7213134765625, Learning Rate: 0.0003125\n",
      "Epoch [9092/20000], Loss: 868.8837890625, Entropy 415.2869567871094, Learning Rate: 0.0003125\n",
      "Epoch [9093/20000], Loss: 890.799072265625, Entropy 412.7704162597656, Learning Rate: 0.0003125\n",
      "Epoch [9094/20000], Loss: 875.8364868164062, Entropy 442.02655029296875, Learning Rate: 0.0003125\n",
      "Epoch [9095/20000], Loss: 814.9931640625, Entropy 427.0861511230469, Learning Rate: 0.0003125\n",
      "Epoch [9096/20000], Loss: 862.9486083984375, Entropy 443.4129638671875, Learning Rate: 0.0003125\n",
      "Epoch [9097/20000], Loss: 851.356689453125, Entropy 421.4214782714844, Learning Rate: 0.0003125\n",
      "Epoch [9098/20000], Loss: 885.255615234375, Entropy 420.6238098144531, Learning Rate: 0.0003125\n",
      "Epoch [9099/20000], Loss: 864.5691528320312, Entropy 435.29034423828125, Learning Rate: 0.0003125\n",
      "Epoch [9100/20000], Loss: 829.7916870117188, Entropy 431.39300537109375, Learning Rate: 0.0003125\n",
      "Epoch [9101/20000], Loss: 915.1055908203125, Entropy 405.8966979980469, Learning Rate: 0.0003125\n",
      "Epoch [9102/20000], Loss: 858.5408935546875, Entropy 427.667724609375, Learning Rate: 0.0003125\n",
      "Epoch [9103/20000], Loss: 830.714111328125, Entropy 430.9026794433594, Learning Rate: 0.0003125\n",
      "Epoch [9104/20000], Loss: 866.272216796875, Entropy 417.2922668457031, Learning Rate: 0.0003125\n",
      "Epoch [9105/20000], Loss: 884.87841796875, Entropy 404.6776428222656, Learning Rate: 0.0003125\n",
      "Epoch [9106/20000], Loss: 890.8035888671875, Entropy 429.8292236328125, Learning Rate: 0.0003125\n",
      "Epoch [9107/20000], Loss: 880.1255493164062, Entropy 434.09295654296875, Learning Rate: 0.0003125\n",
      "Epoch [9108/20000], Loss: 866.076416015625, Entropy 402.1112365722656, Learning Rate: 0.0003125\n",
      "Epoch [9109/20000], Loss: 842.0169677734375, Entropy 435.5061950683594, Learning Rate: 0.0003125\n",
      "Epoch [9110/20000], Loss: 844.188232421875, Entropy 438.45458984375, Learning Rate: 0.0003125\n",
      "Epoch [9111/20000], Loss: 842.416259765625, Entropy 438.6479187011719, Learning Rate: 0.0003125\n",
      "Epoch [9112/20000], Loss: 836.4382934570312, Entropy 435.80413818359375, Learning Rate: 0.0003125\n",
      "Epoch [9113/20000], Loss: 898.1546020507812, Entropy 424.67633056640625, Learning Rate: 0.0003125\n",
      "Epoch [9114/20000], Loss: 861.571533203125, Entropy 416.6165771484375, Learning Rate: 0.0003125\n",
      "Epoch [9115/20000], Loss: 875.3115234375, Entropy 433.2548828125, Learning Rate: 0.0003125\n",
      "Epoch [9116/20000], Loss: 920.2789306640625, Entropy 422.7801208496094, Learning Rate: 0.0003125\n",
      "Epoch [9117/20000], Loss: 856.96533203125, Entropy 418.4250793457031, Learning Rate: 0.0003125\n",
      "Epoch [9118/20000], Loss: 912.633056640625, Entropy 426.28662109375, Learning Rate: 0.0003125\n",
      "Epoch [9119/20000], Loss: 862.0120849609375, Entropy 437.8952941894531, Learning Rate: 0.0003125\n",
      "Epoch [9120/20000], Loss: 941.928955078125, Entropy 411.8625793457031, Learning Rate: 0.0003125\n",
      "Epoch [9121/20000], Loss: 868.86474609375, Entropy 428.3525390625, Learning Rate: 0.0003125\n",
      "Epoch [9122/20000], Loss: 871.9794311523438, Entropy 419.77960205078125, Learning Rate: 0.0003125\n",
      "Epoch [9123/20000], Loss: 880.6858520507812, Entropy 425.11151123046875, Learning Rate: 0.0003125\n",
      "Epoch [9124/20000], Loss: 880.4242553710938, Entropy 431.60382080078125, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9125/20000], Loss: 876.9345092773438, Entropy 413.79315185546875, Learning Rate: 0.0003125\n",
      "Epoch [9126/20000], Loss: 864.1531982421875, Entropy 440.5752258300781, Learning Rate: 0.0003125\n",
      "Epoch [9127/20000], Loss: 836.9527587890625, Entropy 419.7997131347656, Learning Rate: 0.0003125\n",
      "Epoch [9128/20000], Loss: 852.32470703125, Entropy 419.0422668457031, Learning Rate: 0.0003125\n",
      "Epoch [9129/20000], Loss: 865.9444580078125, Entropy 432.8686218261719, Learning Rate: 0.0003125\n",
      "Epoch [9130/20000], Loss: 853.0889282226562, Entropy 421.80572509765625, Learning Rate: 0.0003125\n",
      "Epoch [9131/20000], Loss: 878.0369873046875, Entropy 426.4495849609375, Learning Rate: 0.0003125\n",
      "Epoch [9132/20000], Loss: 865.6095581054688, Entropy 441.43511962890625, Learning Rate: 0.0003125\n",
      "Epoch [9133/20000], Loss: 898.6177368164062, Entropy 413.76849365234375, Learning Rate: 0.0003125\n",
      "Epoch [9134/20000], Loss: 863.0328369140625, Entropy 431.8348083496094, Learning Rate: 0.0003125\n",
      "Epoch [9135/20000], Loss: 867.4937744140625, Entropy 437.8736572265625, Learning Rate: 0.0003125\n",
      "Epoch [9136/20000], Loss: 846.193603515625, Entropy 431.2109375, Learning Rate: 0.0003125\n",
      "Epoch [9137/20000], Loss: 916.4833984375, Entropy 425.3894348144531, Learning Rate: 0.0003125\n",
      "Epoch [9138/20000], Loss: 847.4595947265625, Entropy 413.7180480957031, Learning Rate: 0.0003125\n",
      "Epoch [9139/20000], Loss: 874.482666015625, Entropy 422.965576171875, Learning Rate: 0.0003125\n",
      "Epoch [9140/20000], Loss: 917.422607421875, Entropy 415.549560546875, Learning Rate: 0.0003125\n",
      "Epoch [9141/20000], Loss: 871.6168823242188, Entropy 420.98883056640625, Learning Rate: 0.0003125\n",
      "Epoch [9142/20000], Loss: 896.2545166015625, Entropy 426.9464416503906, Learning Rate: 0.0003125\n",
      "Epoch [9143/20000], Loss: 904.62841796875, Entropy 413.6181945800781, Learning Rate: 0.0003125\n",
      "Epoch [9144/20000], Loss: 893.6856689453125, Entropy 413.4498291015625, Learning Rate: 0.0003125\n",
      "Epoch [9145/20000], Loss: 895.8123779296875, Entropy 409.5115661621094, Learning Rate: 0.0003125\n",
      "Epoch [9146/20000], Loss: 906.8284912109375, Entropy 413.1056213378906, Learning Rate: 0.0003125\n",
      "Epoch [9147/20000], Loss: 882.383544921875, Entropy 417.8391418457031, Learning Rate: 0.0003125\n",
      "Epoch [9148/20000], Loss: 904.0648193359375, Entropy 433.3896789550781, Learning Rate: 0.0003125\n",
      "Epoch [9149/20000], Loss: 902.5428466796875, Entropy 403.7024230957031, Learning Rate: 0.0003125\n",
      "Epoch [9150/20000], Loss: 811.300048828125, Entropy 425.8617858886719, Learning Rate: 0.0003125\n",
      "Epoch [9151/20000], Loss: 907.108642578125, Entropy 428.1422424316406, Learning Rate: 0.0003125\n",
      "Epoch [9152/20000], Loss: 836.0748291015625, Entropy 432.2886962890625, Learning Rate: 0.0003125\n",
      "Epoch [9153/20000], Loss: 819.99462890625, Entropy 437.4001159667969, Learning Rate: 0.0003125\n",
      "Epoch [9154/20000], Loss: 867.8375244140625, Entropy 430.0079345703125, Learning Rate: 0.0003125\n",
      "Epoch [9155/20000], Loss: 833.455810546875, Entropy 425.9632568359375, Learning Rate: 0.0003125\n",
      "Epoch [9156/20000], Loss: 923.3705444335938, Entropy 424.31280517578125, Learning Rate: 0.0003125\n",
      "Epoch [9157/20000], Loss: 813.9471435546875, Entropy 416.4002685546875, Learning Rate: 0.0003125\n",
      "Epoch [9158/20000], Loss: 842.9714965820312, Entropy 428.58721923828125, Learning Rate: 0.0003125\n",
      "Epoch [9159/20000], Loss: 909.9383544921875, Entropy 440.3210754394531, Learning Rate: 0.0003125\n",
      "Epoch [9160/20000], Loss: 838.2863159179688, Entropy 428.10797119140625, Learning Rate: 0.0003125\n",
      "Epoch [9161/20000], Loss: 809.0997314453125, Entropy 445.0166015625, Learning Rate: 0.0003125\n",
      "Epoch [9162/20000], Loss: 866.091796875, Entropy 432.3069152832031, Learning Rate: 0.0003125\n",
      "Epoch [9163/20000], Loss: 902.6580810546875, Entropy 412.7918395996094, Learning Rate: 0.0003125\n",
      "Epoch [9164/20000], Loss: 874.178466796875, Entropy 425.5470886230469, Learning Rate: 0.0003125\n",
      "Epoch [9165/20000], Loss: 867.7677001953125, Entropy 439.3650817871094, Learning Rate: 0.0003125\n",
      "Epoch [9166/20000], Loss: 871.6524658203125, Entropy 412.1069030761719, Learning Rate: 0.0003125\n",
      "Epoch [9167/20000], Loss: 867.8934326171875, Entropy 428.3420715332031, Learning Rate: 0.0003125\n",
      "Epoch [9168/20000], Loss: 884.8800048828125, Entropy 407.9986572265625, Learning Rate: 0.0003125\n",
      "Epoch [9169/20000], Loss: 864.0838623046875, Entropy 426.6022033691406, Learning Rate: 0.0003125\n",
      "Epoch [9170/20000], Loss: 864.794189453125, Entropy 414.9111328125, Learning Rate: 0.0003125\n",
      "Epoch [9171/20000], Loss: 883.46923828125, Entropy 430.51513671875, Learning Rate: 0.0003125\n",
      "Epoch [9172/20000], Loss: 935.7608642578125, Entropy 414.9168701171875, Learning Rate: 0.0003125\n",
      "Epoch [9173/20000], Loss: 854.9951171875, Entropy 427.6095275878906, Learning Rate: 0.0003125\n",
      "Epoch [9174/20000], Loss: 875.2615966796875, Entropy 408.1898498535156, Learning Rate: 0.0003125\n",
      "Epoch [9175/20000], Loss: 897.1693115234375, Entropy 433.5328369140625, Learning Rate: 0.0003125\n",
      "Epoch [9176/20000], Loss: 866.2874755859375, Entropy 428.172607421875, Learning Rate: 0.0003125\n",
      "Epoch [9177/20000], Loss: 854.72705078125, Entropy 427.095458984375, Learning Rate: 0.0003125\n",
      "Epoch [9178/20000], Loss: 864.0848388671875, Entropy 423.1784973144531, Learning Rate: 0.0003125\n",
      "Epoch [9179/20000], Loss: 924.2523803710938, Entropy 420.94256591796875, Learning Rate: 0.0003125\n",
      "Epoch [9180/20000], Loss: 845.6266479492188, Entropy 427.85882568359375, Learning Rate: 0.0003125\n",
      "Epoch [9181/20000], Loss: 883.140625, Entropy 413.7652893066406, Learning Rate: 0.0003125\n",
      "Epoch [9182/20000], Loss: 841.2398071289062, Entropy 439.29510498046875, Learning Rate: 0.0003125\n",
      "Epoch [9183/20000], Loss: 871.2149658203125, Entropy 429.84326171875, Learning Rate: 0.0003125\n",
      "Epoch [9184/20000], Loss: 857.6099243164062, Entropy 412.37994384765625, Learning Rate: 0.0003125\n",
      "Epoch [9185/20000], Loss: 836.225341796875, Entropy 420.5635070800781, Learning Rate: 0.0003125\n",
      "Epoch [9186/20000], Loss: 912.861083984375, Entropy 409.650146484375, Learning Rate: 0.0003125\n",
      "Epoch [9187/20000], Loss: 878.5068359375, Entropy 419.029052734375, Learning Rate: 0.0003125\n",
      "Epoch [9188/20000], Loss: 868.4742431640625, Entropy 417.4338684082031, Learning Rate: 0.0003125\n",
      "Epoch [9189/20000], Loss: 890.802978515625, Entropy 414.7853698730469, Learning Rate: 0.0003125\n",
      "Epoch [9190/20000], Loss: 903.88232421875, Entropy 405.90869140625, Learning Rate: 0.0003125\n",
      "Epoch [9191/20000], Loss: 873.2962646484375, Entropy 421.4029541015625, Learning Rate: 0.0003125\n",
      "Epoch [9192/20000], Loss: 860.41259765625, Entropy 416.6759948730469, Learning Rate: 0.0003125\n",
      "Epoch [9193/20000], Loss: 853.5420532226562, Entropy 434.61334228515625, Learning Rate: 0.0003125\n",
      "Epoch [9194/20000], Loss: 906.03173828125, Entropy 420.8874206542969, Learning Rate: 0.0003125\n",
      "Epoch [9195/20000], Loss: 911.2403564453125, Entropy 412.4640808105469, Learning Rate: 0.0003125\n",
      "Epoch [9196/20000], Loss: 897.0132446289062, Entropy 423.56146240234375, Learning Rate: 0.0003125\n",
      "Epoch [9197/20000], Loss: 860.2824096679688, Entropy 430.62445068359375, Learning Rate: 0.0003125\n",
      "Epoch [9198/20000], Loss: 881.654296875, Entropy 422.4098205566406, Learning Rate: 0.0003125\n",
      "Epoch [9199/20000], Loss: 867.3726806640625, Entropy 424.5772705078125, Learning Rate: 0.0003125\n",
      "Epoch [9200/20000], Loss: 897.763427734375, Entropy 429.7362060546875, Learning Rate: 0.0003125\n",
      "Epoch [9201/20000], Loss: 843.6754150390625, Entropy 439.40625, Learning Rate: 0.0003125\n",
      "Epoch [9202/20000], Loss: 898.6280517578125, Entropy 402.0272216796875, Learning Rate: 0.0003125\n",
      "Epoch [9203/20000], Loss: 917.3685302734375, Entropy 412.3049621582031, Learning Rate: 0.0003125\n",
      "Epoch [9204/20000], Loss: 823.569580078125, Entropy 428.2439270019531, Learning Rate: 0.0003125\n",
      "Epoch [9205/20000], Loss: 892.3994750976562, Entropy 426.41741943359375, Learning Rate: 0.0003125\n",
      "Epoch [9206/20000], Loss: 880.63623046875, Entropy 428.4964599609375, Learning Rate: 0.0003125\n",
      "Epoch [9207/20000], Loss: 828.2291870117188, Entropy 434.85687255859375, Learning Rate: 0.0003125\n",
      "Epoch [9208/20000], Loss: 887.8941650390625, Entropy 429.083740234375, Learning Rate: 0.0003125\n",
      "Epoch [9209/20000], Loss: 885.7415771484375, Entropy 423.5592956542969, Learning Rate: 0.0003125\n",
      "Epoch [9210/20000], Loss: 881.5009765625, Entropy 436.5871887207031, Learning Rate: 0.0003125\n",
      "Epoch [9211/20000], Loss: 843.9295654296875, Entropy 423.7057189941406, Learning Rate: 0.0003125\n",
      "Epoch [9212/20000], Loss: 868.4654541015625, Entropy 415.4279479980469, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9213/20000], Loss: 873.7288208007812, Entropy 399.07086181640625, Learning Rate: 0.0003125\n",
      "Epoch [9214/20000], Loss: 835.56689453125, Entropy 446.4045104980469, Learning Rate: 0.0003125\n",
      "Epoch [9215/20000], Loss: 870.1947021484375, Entropy 432.0558776855469, Learning Rate: 0.0003125\n",
      "Epoch [9216/20000], Loss: 866.26513671875, Entropy 417.3871765136719, Learning Rate: 0.0003125\n",
      "Epoch [9217/20000], Loss: 878.720703125, Entropy 422.0908508300781, Learning Rate: 0.0003125\n",
      "Epoch [9218/20000], Loss: 881.7958984375, Entropy 419.719482421875, Learning Rate: 0.0003125\n",
      "Epoch [9219/20000], Loss: 901.207763671875, Entropy 424.0641174316406, Learning Rate: 0.0003125\n",
      "Epoch [9220/20000], Loss: 836.7047119140625, Entropy 454.11474609375, Learning Rate: 0.0003125\n",
      "Epoch [9221/20000], Loss: 888.4677734375, Entropy 437.1400146484375, Learning Rate: 0.0003125\n",
      "Epoch [9222/20000], Loss: 870.7061767578125, Entropy 430.2738952636719, Learning Rate: 0.0003125\n",
      "Epoch [9223/20000], Loss: 852.1087646484375, Entropy 438.8052978515625, Learning Rate: 0.0003125\n",
      "Epoch [9224/20000], Loss: 849.8818359375, Entropy 428.59423828125, Learning Rate: 0.0003125\n",
      "Epoch [9225/20000], Loss: 892.3916015625, Entropy 426.8076477050781, Learning Rate: 0.0003125\n",
      "Epoch [9226/20000], Loss: 870.975341796875, Entropy 418.2198791503906, Learning Rate: 0.0003125\n",
      "Epoch [9227/20000], Loss: 937.01220703125, Entropy 420.3830871582031, Learning Rate: 0.0003125\n",
      "Epoch [9228/20000], Loss: 843.182861328125, Entropy 424.0452880859375, Learning Rate: 0.0003125\n",
      "Epoch [9229/20000], Loss: 837.121826171875, Entropy 436.1956787109375, Learning Rate: 0.0003125\n",
      "Epoch [9230/20000], Loss: 883.8402099609375, Entropy 411.7828369140625, Learning Rate: 0.0003125\n",
      "Epoch [9231/20000], Loss: 890.978759765625, Entropy 438.83447265625, Learning Rate: 0.0003125\n",
      "Epoch [9232/20000], Loss: 878.006591796875, Entropy 425.2774658203125, Learning Rate: 0.0003125\n",
      "Epoch [9233/20000], Loss: 898.4075927734375, Entropy 417.9445495605469, Learning Rate: 0.0003125\n",
      "Epoch [9234/20000], Loss: 893.3193969726562, Entropy 429.25396728515625, Learning Rate: 0.0003125\n",
      "Epoch [9235/20000], Loss: 922.8077392578125, Entropy 419.1717224121094, Learning Rate: 0.0003125\n",
      "Epoch [9236/20000], Loss: 829.002197265625, Entropy 434.5413818359375, Learning Rate: 0.0003125\n",
      "Epoch [9237/20000], Loss: 864.1448364257812, Entropy 416.85662841796875, Learning Rate: 0.0003125\n",
      "Epoch [9238/20000], Loss: 915.3046875, Entropy 411.326904296875, Learning Rate: 0.0003125\n",
      "Epoch [9239/20000], Loss: 857.256103515625, Entropy 418.9732360839844, Learning Rate: 0.0003125\n",
      "Epoch [9240/20000], Loss: 886.8272705078125, Entropy 407.5212097167969, Learning Rate: 0.0003125\n",
      "Epoch [9241/20000], Loss: 843.12060546875, Entropy 449.3455810546875, Learning Rate: 0.0003125\n",
      "Epoch [9242/20000], Loss: 845.6553955078125, Entropy 421.718505859375, Learning Rate: 0.0003125\n",
      "Epoch [9243/20000], Loss: 849.0335693359375, Entropy 433.2599182128906, Learning Rate: 0.0003125\n",
      "Epoch [9244/20000], Loss: 867.2979736328125, Entropy 439.3836669921875, Learning Rate: 0.0003125\n",
      "Epoch [9245/20000], Loss: 864.1881103515625, Entropy 430.9714660644531, Learning Rate: 0.0003125\n",
      "Epoch [9246/20000], Loss: 858.3345947265625, Entropy 421.9802551269531, Learning Rate: 0.0003125\n",
      "Epoch [9247/20000], Loss: 901.66650390625, Entropy 435.06787109375, Learning Rate: 0.0003125\n",
      "Epoch [9248/20000], Loss: 907.4876098632812, Entropy 430.86273193359375, Learning Rate: 0.0003125\n",
      "Epoch [9249/20000], Loss: 871.0009765625, Entropy 410.60498046875, Learning Rate: 0.0003125\n",
      "Epoch [9250/20000], Loss: 905.48681640625, Entropy 402.6081237792969, Learning Rate: 0.0003125\n",
      "Epoch [9251/20000], Loss: 881.4993286132812, Entropy 430.45013427734375, Learning Rate: 0.0003125\n",
      "Epoch [9252/20000], Loss: 835.786865234375, Entropy 429.0308532714844, Learning Rate: 0.0003125\n",
      "Epoch [9253/20000], Loss: 876.41162109375, Entropy 436.9585876464844, Learning Rate: 0.0003125\n",
      "Epoch [9254/20000], Loss: 902.5269775390625, Entropy 418.3151550292969, Learning Rate: 0.0003125\n",
      "Epoch [9255/20000], Loss: 890.8649291992188, Entropy 418.40130615234375, Learning Rate: 0.0003125\n",
      "Epoch [9256/20000], Loss: 860.3407592773438, Entropy 424.82257080078125, Learning Rate: 0.0003125\n",
      "Epoch [9257/20000], Loss: 843.2315063476562, Entropy 439.16461181640625, Learning Rate: 0.0003125\n",
      "Epoch [9258/20000], Loss: 885.543212890625, Entropy 429.4104309082031, Learning Rate: 0.0003125\n",
      "Epoch [9259/20000], Loss: 864.6837768554688, Entropy 417.09234619140625, Learning Rate: 0.0003125\n",
      "Epoch [9260/20000], Loss: 843.8233642578125, Entropy 436.3642578125, Learning Rate: 0.0003125\n",
      "Epoch [9261/20000], Loss: 842.3842163085938, Entropy 435.83514404296875, Learning Rate: 0.0003125\n",
      "Epoch [9262/20000], Loss: 894.7220458984375, Entropy 427.3752746582031, Learning Rate: 0.0003125\n",
      "Epoch [9263/20000], Loss: 848.3547973632812, Entropy 438.81341552734375, Learning Rate: 0.0003125\n",
      "Epoch [9264/20000], Loss: 846.69580078125, Entropy 416.7790832519531, Learning Rate: 0.0003125\n",
      "Epoch [9265/20000], Loss: 858.6243286132812, Entropy 417.21771240234375, Learning Rate: 0.0003125\n",
      "Epoch [9266/20000], Loss: 926.45458984375, Entropy 438.9149169921875, Learning Rate: 0.0003125\n",
      "Epoch [9267/20000], Loss: 877.9036254882812, Entropy 411.76092529296875, Learning Rate: 0.0003125\n",
      "Epoch [9268/20000], Loss: 877.6864013671875, Entropy 423.953857421875, Learning Rate: 0.0003125\n",
      "Epoch [9269/20000], Loss: 902.006103515625, Entropy 418.3786926269531, Learning Rate: 0.0003125\n",
      "Epoch [9270/20000], Loss: 890.728515625, Entropy 429.3171081542969, Learning Rate: 0.0003125\n",
      "Epoch [9271/20000], Loss: 884.1138916015625, Entropy 431.3073425292969, Learning Rate: 0.0003125\n",
      "Epoch [9272/20000], Loss: 880.920654296875, Entropy 404.580078125, Learning Rate: 0.0003125\n",
      "Epoch [9273/20000], Loss: 902.11474609375, Entropy 425.7990417480469, Learning Rate: 0.0003125\n",
      "Epoch [9274/20000], Loss: 831.747802734375, Entropy 441.5051574707031, Learning Rate: 0.0003125\n",
      "Epoch [9275/20000], Loss: 910.0379638671875, Entropy 426.8816833496094, Learning Rate: 0.0003125\n",
      "Epoch [9276/20000], Loss: 913.5628662109375, Entropy 415.1005859375, Learning Rate: 0.0003125\n",
      "Epoch [9277/20000], Loss: 946.652099609375, Entropy 426.26171875, Learning Rate: 0.0003125\n",
      "Epoch [9278/20000], Loss: 872.5191650390625, Entropy 430.35302734375, Learning Rate: 0.0003125\n",
      "Epoch [9279/20000], Loss: 817.87646484375, Entropy 422.2303161621094, Learning Rate: 0.0003125\n",
      "Epoch [9280/20000], Loss: 917.4912109375, Entropy 441.5487365722656, Learning Rate: 0.0003125\n",
      "Epoch [9281/20000], Loss: 883.0842895507812, Entropy 419.17095947265625, Learning Rate: 0.0003125\n",
      "Epoch [9282/20000], Loss: 864.594970703125, Entropy 435.7588195800781, Learning Rate: 0.0003125\n",
      "Epoch [9283/20000], Loss: 885.0597534179688, Entropy 412.55694580078125, Learning Rate: 0.0003125\n",
      "Epoch [9284/20000], Loss: 967.822265625, Entropy 409.2133483886719, Learning Rate: 0.0003125\n",
      "Epoch [9285/20000], Loss: 828.669677734375, Entropy 422.0467834472656, Learning Rate: 0.0003125\n",
      "Epoch [9286/20000], Loss: 821.37646484375, Entropy 437.9851379394531, Learning Rate: 0.0003125\n",
      "Epoch [9287/20000], Loss: 824.5672607421875, Entropy 414.0262451171875, Learning Rate: 0.0003125\n",
      "Epoch [9288/20000], Loss: 904.2117919921875, Entropy 431.8039855957031, Learning Rate: 0.0003125\n",
      "Epoch [9289/20000], Loss: 884.486572265625, Entropy 428.1958923339844, Learning Rate: 0.0003125\n",
      "Epoch [9290/20000], Loss: 920.7589111328125, Entropy 410.5944519042969, Learning Rate: 0.0003125\n",
      "Epoch [9291/20000], Loss: 901.6396484375, Entropy 432.8665466308594, Learning Rate: 0.0003125\n",
      "Epoch [9292/20000], Loss: 883.8619995117188, Entropy 424.43072509765625, Learning Rate: 0.0003125\n",
      "Epoch [9293/20000], Loss: 889.3046264648438, Entropy 420.51556396484375, Learning Rate: 0.0003125\n",
      "Epoch [9294/20000], Loss: 875.91796875, Entropy 427.8713073730469, Learning Rate: 0.0003125\n",
      "Epoch [9295/20000], Loss: 919.5042724609375, Entropy 420.3467102050781, Learning Rate: 0.0003125\n",
      "Epoch [9296/20000], Loss: 868.4409790039062, Entropy 427.91326904296875, Learning Rate: 0.0003125\n",
      "Epoch [9297/20000], Loss: 865.6913452148438, Entropy 439.25701904296875, Learning Rate: 0.0003125\n",
      "Epoch [9298/20000], Loss: 862.0960693359375, Entropy 435.2186279296875, Learning Rate: 0.0003125\n",
      "Epoch [9299/20000], Loss: 906.912353515625, Entropy 421.7562255859375, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9300/20000], Loss: 920.6854248046875, Entropy 429.5572204589844, Learning Rate: 0.0003125\n",
      "Epoch [9301/20000], Loss: 884.838134765625, Entropy 426.8723449707031, Learning Rate: 0.0003125\n",
      "Epoch [9302/20000], Loss: 874.4888916015625, Entropy 428.50341796875, Learning Rate: 0.0003125\n",
      "Epoch [9303/20000], Loss: 851.7261962890625, Entropy 431.9274597167969, Learning Rate: 0.0003125\n",
      "Epoch [9304/20000], Loss: 897.4664916992188, Entropy 437.85504150390625, Learning Rate: 0.0003125\n",
      "Epoch [9305/20000], Loss: 865.8355102539062, Entropy 431.11468505859375, Learning Rate: 0.0003125\n",
      "Epoch [9306/20000], Loss: 873.5734252929688, Entropy 411.39691162109375, Learning Rate: 0.0003125\n",
      "Epoch [9307/20000], Loss: 924.0931396484375, Entropy 424.0096740722656, Learning Rate: 0.0003125\n",
      "Epoch [9308/20000], Loss: 880.0413818359375, Entropy 434.9736633300781, Learning Rate: 0.0003125\n",
      "Epoch [9309/20000], Loss: 831.933837890625, Entropy 432.3185729980469, Learning Rate: 0.0003125\n",
      "Epoch [9310/20000], Loss: 878.1612548828125, Entropy 403.3070373535156, Learning Rate: 0.0003125\n",
      "Epoch [9311/20000], Loss: 862.1826171875, Entropy 421.6790771484375, Learning Rate: 0.0003125\n",
      "Epoch [9312/20000], Loss: 806.7122802734375, Entropy 445.9846496582031, Learning Rate: 0.0003125\n",
      "Epoch [9313/20000], Loss: 883.338623046875, Entropy 442.6275634765625, Learning Rate: 0.0003125\n",
      "Epoch [9314/20000], Loss: 822.68017578125, Entropy 436.99560546875, Learning Rate: 0.0003125\n",
      "Epoch [9315/20000], Loss: 879.53662109375, Entropy 431.3475341796875, Learning Rate: 0.0003125\n",
      "Epoch [9316/20000], Loss: 835.3050537109375, Entropy 426.2780456542969, Learning Rate: 0.0003125\n",
      "Epoch [9317/20000], Loss: 864.0980224609375, Entropy 439.5272216796875, Learning Rate: 0.0003125\n",
      "Epoch [9318/20000], Loss: 873.8321533203125, Entropy 427.7385559082031, Learning Rate: 0.0003125\n",
      "Epoch [9319/20000], Loss: 881.2889404296875, Entropy 422.6546630859375, Learning Rate: 0.0003125\n",
      "Epoch [9320/20000], Loss: 863.0474243164062, Entropy 435.86553955078125, Learning Rate: 0.0003125\n",
      "Epoch [9321/20000], Loss: 790.8577880859375, Entropy 447.2590637207031, Learning Rate: 0.0003125\n",
      "Epoch [9322/20000], Loss: 893.4117431640625, Entropy 440.0168762207031, Learning Rate: 0.0003125\n",
      "Epoch [9323/20000], Loss: 913.7197265625, Entropy 427.363037109375, Learning Rate: 0.0003125\n",
      "Epoch [9324/20000], Loss: 902.8519287109375, Entropy 425.3870544433594, Learning Rate: 0.0003125\n",
      "Epoch [9325/20000], Loss: 882.0301513671875, Entropy 417.4210205078125, Learning Rate: 0.0003125\n",
      "Epoch [9326/20000], Loss: 912.452392578125, Entropy 418.8172302246094, Learning Rate: 0.0003125\n",
      "Epoch [9327/20000], Loss: 851.4668579101562, Entropy 420.86041259765625, Learning Rate: 0.0003125\n",
      "Epoch [9328/20000], Loss: 875.1887817382812, Entropy 434.60076904296875, Learning Rate: 0.0003125\n",
      "Epoch [9329/20000], Loss: 896.4141845703125, Entropy 438.0395202636719, Learning Rate: 0.0003125\n",
      "Epoch [9330/20000], Loss: 913.6507568359375, Entropy 409.0729675292969, Learning Rate: 0.0003125\n",
      "Epoch [9331/20000], Loss: 858.5034790039062, Entropy 419.98638916015625, Learning Rate: 0.0003125\n",
      "Epoch [9332/20000], Loss: 869.1966552734375, Entropy 426.3654479980469, Learning Rate: 0.0003125\n",
      "Epoch [9333/20000], Loss: 864.1591796875, Entropy 435.8639831542969, Learning Rate: 0.0003125\n",
      "Epoch [9334/20000], Loss: 843.6203002929688, Entropy 436.29925537109375, Learning Rate: 0.0003125\n",
      "Epoch [9335/20000], Loss: 839.0306396484375, Entropy 440.1484375, Learning Rate: 0.0003125\n",
      "Epoch [9336/20000], Loss: 851.5230102539062, Entropy 425.80950927734375, Learning Rate: 0.0003125\n",
      "Epoch [9337/20000], Loss: 891.1309204101562, Entropy 423.09588623046875, Learning Rate: 0.0003125\n",
      "Epoch [9338/20000], Loss: 856.4510498046875, Entropy 424.6116027832031, Learning Rate: 0.0003125\n",
      "Epoch [9339/20000], Loss: 860.1617431640625, Entropy 425.9871520996094, Learning Rate: 0.0003125\n",
      "Epoch [9340/20000], Loss: 805.958984375, Entropy 436.375, Learning Rate: 0.0003125\n",
      "Epoch [9341/20000], Loss: 883.87646484375, Entropy 419.4026184082031, Learning Rate: 0.0003125\n",
      "Epoch [9342/20000], Loss: 847.4168090820312, Entropy 427.25811767578125, Learning Rate: 0.0003125\n",
      "Epoch [9343/20000], Loss: 851.2474365234375, Entropy 409.2746276855469, Learning Rate: 0.0003125\n",
      "Epoch [9344/20000], Loss: 857.251220703125, Entropy 442.6597595214844, Learning Rate: 0.0003125\n",
      "Epoch [9345/20000], Loss: 901.953369140625, Entropy 439.4388122558594, Learning Rate: 0.0003125\n",
      "Epoch [9346/20000], Loss: 849.95068359375, Entropy 421.7442626953125, Learning Rate: 0.0003125\n",
      "Epoch [9347/20000], Loss: 865.85791015625, Entropy 431.015380859375, Learning Rate: 0.0003125\n",
      "Epoch [9348/20000], Loss: 854.1234741210938, Entropy 428.11346435546875, Learning Rate: 0.0003125\n",
      "Epoch [9349/20000], Loss: 883.5166015625, Entropy 428.89599609375, Learning Rate: 0.0003125\n",
      "Epoch [9350/20000], Loss: 842.660400390625, Entropy 441.6325378417969, Learning Rate: 0.0003125\n",
      "Epoch [9351/20000], Loss: 856.8565673828125, Entropy 431.6463928222656, Learning Rate: 0.0003125\n",
      "Epoch [9352/20000], Loss: 911.0703125, Entropy 419.0953674316406, Learning Rate: 0.0003125\n",
      "Epoch [9353/20000], Loss: 888.8923950195312, Entropy 423.14019775390625, Learning Rate: 0.0003125\n",
      "Epoch [9354/20000], Loss: 814.546875, Entropy 423.6068115234375, Learning Rate: 0.0003125\n",
      "Epoch [9355/20000], Loss: 926.301513671875, Entropy 421.2633972167969, Learning Rate: 0.0003125\n",
      "Epoch [9356/20000], Loss: 882.1793212890625, Entropy 426.0456848144531, Learning Rate: 0.0003125\n",
      "Epoch [9357/20000], Loss: 899.242919921875, Entropy 430.5602111816406, Learning Rate: 0.0003125\n",
      "Epoch [9358/20000], Loss: 880.6712036132812, Entropy 437.73626708984375, Learning Rate: 0.0003125\n",
      "Epoch [9359/20000], Loss: 873.4503173828125, Entropy 443.1051025390625, Learning Rate: 0.0003125\n",
      "Epoch [9360/20000], Loss: 878.5706787109375, Entropy 428.127685546875, Learning Rate: 0.0003125\n",
      "Epoch [9361/20000], Loss: 908.929931640625, Entropy 420.5047912597656, Learning Rate: 0.0003125\n",
      "Epoch [9362/20000], Loss: 880.143798828125, Entropy 452.1844482421875, Learning Rate: 0.0003125\n",
      "Epoch [9363/20000], Loss: 878.8885498046875, Entropy 417.2387390136719, Learning Rate: 0.0003125\n",
      "Epoch [9364/20000], Loss: 883.7640380859375, Entropy 433.3092956542969, Learning Rate: 0.0003125\n",
      "Epoch [9365/20000], Loss: 842.33984375, Entropy 437.0373229980469, Learning Rate: 0.0003125\n",
      "Epoch [9366/20000], Loss: 834.7367553710938, Entropy 428.61358642578125, Learning Rate: 0.0003125\n",
      "Epoch [9367/20000], Loss: 859.2906494140625, Entropy 418.9348449707031, Learning Rate: 0.0003125\n",
      "Epoch [9368/20000], Loss: 871.089111328125, Entropy 439.1848449707031, Learning Rate: 0.0003125\n",
      "Epoch [9369/20000], Loss: 904.375, Entropy 431.2052917480469, Learning Rate: 0.0003125\n",
      "Epoch [9370/20000], Loss: 887.4541015625, Entropy 417.6984558105469, Learning Rate: 0.0003125\n",
      "Epoch [9371/20000], Loss: 859.272216796875, Entropy 436.1851501464844, Learning Rate: 0.0003125\n",
      "Epoch [9372/20000], Loss: 865.5777587890625, Entropy 430.1769104003906, Learning Rate: 0.0003125\n",
      "Epoch [9373/20000], Loss: 816.7777709960938, Entropy 426.65179443359375, Learning Rate: 0.0003125\n",
      "Epoch [9374/20000], Loss: 910.4783935546875, Entropy 415.765625, Learning Rate: 0.0003125\n",
      "Epoch [9375/20000], Loss: 840.5984497070312, Entropy 434.96868896484375, Learning Rate: 0.0003125\n",
      "Epoch [9376/20000], Loss: 873.7415161132812, Entropy 423.06182861328125, Learning Rate: 0.0003125\n",
      "Epoch [9377/20000], Loss: 871.6090087890625, Entropy 427.9166259765625, Learning Rate: 0.0003125\n",
      "Epoch [9378/20000], Loss: 900.5767211914062, Entropy 425.59857177734375, Learning Rate: 0.0003125\n",
      "Epoch [9379/20000], Loss: 883.7762451171875, Entropy 427.5484924316406, Learning Rate: 0.0003125\n",
      "Epoch [9380/20000], Loss: 863.7764892578125, Entropy 428.0362548828125, Learning Rate: 0.0003125\n",
      "Epoch [9381/20000], Loss: 902.91796875, Entropy 440.6317138671875, Learning Rate: 0.0003125\n",
      "Epoch [9382/20000], Loss: 875.099853515625, Entropy 431.7395324707031, Learning Rate: 0.0003125\n",
      "Epoch [9383/20000], Loss: 845.6354370117188, Entropy 415.77935791015625, Learning Rate: 0.0003125\n",
      "Epoch [9384/20000], Loss: 876.7575073242188, Entropy 429.55975341796875, Learning Rate: 0.0003125\n",
      "Epoch [9385/20000], Loss: 813.395263671875, Entropy 429.579833984375, Learning Rate: 0.0003125\n",
      "Epoch [9386/20000], Loss: 869.5694580078125, Entropy 424.8670959472656, Learning Rate: 0.0003125\n",
      "Epoch [9387/20000], Loss: 826.7244873046875, Entropy 441.3070068359375, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9388/20000], Loss: 815.351318359375, Entropy 434.1022644042969, Learning Rate: 0.0003125\n",
      "Epoch [9389/20000], Loss: 895.394287109375, Entropy 424.4162292480469, Learning Rate: 0.0003125\n",
      "Epoch [9390/20000], Loss: 878.7259521484375, Entropy 411.4704284667969, Learning Rate: 0.0003125\n",
      "Epoch [9391/20000], Loss: 840.7669677734375, Entropy 429.4593200683594, Learning Rate: 0.0003125\n",
      "Epoch [9392/20000], Loss: 870.7554931640625, Entropy 435.6874084472656, Learning Rate: 0.0003125\n",
      "Epoch [9393/20000], Loss: 854.2940673828125, Entropy 424.2578125, Learning Rate: 0.0003125\n",
      "Epoch [9394/20000], Loss: 851.027099609375, Entropy 419.812255859375, Learning Rate: 0.0003125\n",
      "Epoch [9395/20000], Loss: 866.384521484375, Entropy 435.8895263671875, Learning Rate: 0.0003125\n",
      "Epoch [9396/20000], Loss: 855.46630859375, Entropy 418.1164855957031, Learning Rate: 0.0003125\n",
      "Epoch [9397/20000], Loss: 884.5557250976562, Entropy 419.83990478515625, Learning Rate: 0.0003125\n",
      "Epoch [9398/20000], Loss: 921.710693359375, Entropy 419.9939880371094, Learning Rate: 0.0003125\n",
      "Epoch [9399/20000], Loss: 863.8216552734375, Entropy 435.1303405761719, Learning Rate: 0.0003125\n",
      "Epoch [9400/20000], Loss: 864.9863891601562, Entropy 442.49407958984375, Learning Rate: 0.0003125\n",
      "Epoch [9401/20000], Loss: 846.19970703125, Entropy 443.831787109375, Learning Rate: 0.0003125\n",
      "Epoch [9402/20000], Loss: 921.2864990234375, Entropy 418.8211669921875, Learning Rate: 0.0003125\n",
      "Epoch [9403/20000], Loss: 846.4029541015625, Entropy 431.3656921386719, Learning Rate: 0.0003125\n",
      "Epoch [9404/20000], Loss: 825.2900390625, Entropy 429.2408752441406, Learning Rate: 0.0003125\n",
      "Epoch [9405/20000], Loss: 910.7252807617188, Entropy 427.04815673828125, Learning Rate: 0.0003125\n",
      "Epoch [9406/20000], Loss: 882.1094970703125, Entropy 430.5724182128906, Learning Rate: 0.0003125\n",
      "Epoch [9407/20000], Loss: 830.1842041015625, Entropy 431.5657653808594, Learning Rate: 0.0003125\n",
      "Epoch [9408/20000], Loss: 845.7488403320312, Entropy 428.71685791015625, Learning Rate: 0.0003125\n",
      "Epoch [9409/20000], Loss: 867.049072265625, Entropy 421.3927917480469, Learning Rate: 0.0003125\n",
      "Epoch [9410/20000], Loss: 830.7783203125, Entropy 441.2765808105469, Learning Rate: 0.0003125\n",
      "Epoch [9411/20000], Loss: 871.11669921875, Entropy 420.1065673828125, Learning Rate: 0.0003125\n",
      "Epoch [9412/20000], Loss: 888.95361328125, Entropy 424.7296447753906, Learning Rate: 0.0003125\n",
      "Epoch [9413/20000], Loss: 820.7384033203125, Entropy 441.8176574707031, Learning Rate: 0.0003125\n",
      "Epoch [9414/20000], Loss: 900.5404052734375, Entropy 433.6816711425781, Learning Rate: 0.0003125\n",
      "Epoch [9415/20000], Loss: 854.8447875976562, Entropy 409.29302978515625, Learning Rate: 0.0003125\n",
      "Epoch [9416/20000], Loss: 872.1510009765625, Entropy 423.1610107421875, Learning Rate: 0.0003125\n",
      "Epoch [9417/20000], Loss: 871.1202392578125, Entropy 439.0135803222656, Learning Rate: 0.0003125\n",
      "Epoch [9418/20000], Loss: 818.7615966796875, Entropy 440.59130859375, Learning Rate: 0.0003125\n",
      "Epoch [9419/20000], Loss: 881.185791015625, Entropy 416.8424377441406, Learning Rate: 0.0003125\n",
      "Epoch [9420/20000], Loss: 884.3015747070312, Entropy 419.49664306640625, Learning Rate: 0.0003125\n",
      "Epoch [9421/20000], Loss: 911.3732299804688, Entropy 411.13507080078125, Learning Rate: 0.0003125\n",
      "Epoch [9422/20000], Loss: 814.0802612304688, Entropy 441.13763427734375, Learning Rate: 0.0003125\n",
      "Epoch [9423/20000], Loss: 842.9600830078125, Entropy 430.8896789550781, Learning Rate: 0.0003125\n",
      "Epoch [9424/20000], Loss: 838.985107421875, Entropy 434.2880859375, Learning Rate: 0.0003125\n",
      "Epoch [9425/20000], Loss: 879.0591430664062, Entropy 437.59381103515625, Learning Rate: 0.0003125\n",
      "Epoch [9426/20000], Loss: 821.4530029296875, Entropy 432.0964050292969, Learning Rate: 0.0003125\n",
      "Epoch [9427/20000], Loss: 898.21923828125, Entropy 425.4978942871094, Learning Rate: 0.0003125\n",
      "Epoch [9428/20000], Loss: 894.17138671875, Entropy 428.8059387207031, Learning Rate: 0.0003125\n",
      "Epoch [9429/20000], Loss: 858.0355224609375, Entropy 439.1513671875, Learning Rate: 0.0003125\n",
      "Epoch [9430/20000], Loss: 835.0892333984375, Entropy 454.7121887207031, Learning Rate: 0.0003125\n",
      "Epoch [9431/20000], Loss: 892.3685913085938, Entropy 437.54632568359375, Learning Rate: 0.0003125\n",
      "Epoch [9432/20000], Loss: 913.7374267578125, Entropy 424.642333984375, Learning Rate: 0.0003125\n",
      "Epoch [9433/20000], Loss: 828.17724609375, Entropy 441.1748352050781, Learning Rate: 0.0003125\n",
      "Epoch [9434/20000], Loss: 898.7423095703125, Entropy 437.1811218261719, Learning Rate: 0.0003125\n",
      "Epoch [9435/20000], Loss: 905.3193359375, Entropy 440.5218505859375, Learning Rate: 0.0003125\n",
      "Epoch [9436/20000], Loss: 832.439453125, Entropy 427.3069152832031, Learning Rate: 0.0003125\n",
      "Epoch [9437/20000], Loss: 877.6321411132812, Entropy 430.94647216796875, Learning Rate: 0.0003125\n",
      "Epoch [9438/20000], Loss: 888.7974853515625, Entropy 430.46826171875, Learning Rate: 0.0003125\n",
      "Epoch [9439/20000], Loss: 825.4683837890625, Entropy 446.0440979003906, Learning Rate: 0.0003125\n",
      "Epoch [9440/20000], Loss: 834.2860107421875, Entropy 440.1649475097656, Learning Rate: 0.0003125\n",
      "Epoch [9441/20000], Loss: 863.9005126953125, Entropy 408.7323303222656, Learning Rate: 0.0003125\n",
      "Epoch [9442/20000], Loss: 912.23779296875, Entropy 423.6383972167969, Learning Rate: 0.0003125\n",
      "Epoch [9443/20000], Loss: 853.7420043945312, Entropy 440.62945556640625, Learning Rate: 0.0003125\n",
      "Epoch [9444/20000], Loss: 894.621826171875, Entropy 418.1360778808594, Learning Rate: 0.0003125\n",
      "Epoch [9445/20000], Loss: 862.0886840820312, Entropy 434.41070556640625, Learning Rate: 0.0003125\n",
      "Epoch [9446/20000], Loss: 855.1732788085938, Entropy 426.16705322265625, Learning Rate: 0.0003125\n",
      "Epoch [9447/20000], Loss: 878.849365234375, Entropy 422.9803771972656, Learning Rate: 0.0003125\n",
      "Epoch [9448/20000], Loss: 883.6324462890625, Entropy 429.873779296875, Learning Rate: 0.0003125\n",
      "Epoch [9449/20000], Loss: 883.464599609375, Entropy 425.0319519042969, Learning Rate: 0.0003125\n",
      "Epoch [9450/20000], Loss: 893.14404296875, Entropy 437.7440490722656, Learning Rate: 0.0003125\n",
      "Epoch [9451/20000], Loss: 890.6956176757812, Entropy 437.96795654296875, Learning Rate: 0.0003125\n",
      "Epoch [9452/20000], Loss: 868.00634765625, Entropy 442.3228759765625, Learning Rate: 0.0003125\n",
      "Epoch [9453/20000], Loss: 893.16943359375, Entropy 419.1109924316406, Learning Rate: 0.0003125\n",
      "Epoch [9454/20000], Loss: 845.3826904296875, Entropy 437.9308776855469, Learning Rate: 0.0003125\n",
      "Epoch [9455/20000], Loss: 841.4898681640625, Entropy 421.6484680175781, Learning Rate: 0.0003125\n",
      "Epoch [9456/20000], Loss: 887.32568359375, Entropy 413.7630615234375, Learning Rate: 0.0003125\n",
      "Epoch [9457/20000], Loss: 816.995849609375, Entropy 420.514404296875, Learning Rate: 0.0003125\n",
      "Epoch [9458/20000], Loss: 886.996337890625, Entropy 434.2220153808594, Learning Rate: 0.0003125\n",
      "Epoch [9459/20000], Loss: 885.755126953125, Entropy 428.0984191894531, Learning Rate: 0.0003125\n",
      "Epoch [9460/20000], Loss: 880.015380859375, Entropy 438.3717956542969, Learning Rate: 0.0003125\n",
      "Epoch [9461/20000], Loss: 833.73583984375, Entropy 437.0587463378906, Learning Rate: 0.0003125\n",
      "Epoch [9462/20000], Loss: 816.8848876953125, Entropy 451.62646484375, Learning Rate: 0.0003125\n",
      "Epoch [9463/20000], Loss: 860.1845092773438, Entropy 445.50897216796875, Learning Rate: 0.0003125\n",
      "Epoch [9464/20000], Loss: 842.0574951171875, Entropy 430.9798278808594, Learning Rate: 0.0003125\n",
      "Epoch [9465/20000], Loss: 856.6914672851562, Entropy 428.07366943359375, Learning Rate: 0.0003125\n",
      "Epoch [9466/20000], Loss: 896.671142578125, Entropy 425.0783996582031, Learning Rate: 0.0003125\n",
      "Epoch [9467/20000], Loss: 828.289794921875, Entropy 430.8922424316406, Learning Rate: 0.0003125\n",
      "Epoch [9468/20000], Loss: 870.33154296875, Entropy 428.7800598144531, Learning Rate: 0.0003125\n",
      "Epoch [9469/20000], Loss: 887.557373046875, Entropy 451.4835510253906, Learning Rate: 0.0003125\n",
      "Epoch [9470/20000], Loss: 882.6279296875, Entropy 408.8957214355469, Learning Rate: 0.0003125\n",
      "Epoch [9471/20000], Loss: 934.8580932617188, Entropy 435.72674560546875, Learning Rate: 0.0003125\n",
      "Epoch [9472/20000], Loss: 836.5338134765625, Entropy 448.6265869140625, Learning Rate: 0.0003125\n",
      "Epoch [9473/20000], Loss: 875.7060546875, Entropy 438.7882995605469, Learning Rate: 0.0003125\n",
      "Epoch [9474/20000], Loss: 862.1114501953125, Entropy 439.99169921875, Learning Rate: 0.0003125\n",
      "Epoch [9475/20000], Loss: 855.445556640625, Entropy 419.7014465332031, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9476/20000], Loss: 885.796630859375, Entropy 437.8596496582031, Learning Rate: 0.0003125\n",
      "Epoch [9477/20000], Loss: 909.057861328125, Entropy 444.3829650878906, Learning Rate: 0.0003125\n",
      "Epoch [9478/20000], Loss: 887.260009765625, Entropy 425.5110778808594, Learning Rate: 0.0003125\n",
      "Epoch [9479/20000], Loss: 875.9573364257812, Entropy 429.79437255859375, Learning Rate: 0.0003125\n",
      "Epoch [9480/20000], Loss: 858.4683837890625, Entropy 445.0518798828125, Learning Rate: 0.0003125\n",
      "Epoch [9481/20000], Loss: 928.8187255859375, Entropy 436.6282653808594, Learning Rate: 0.0003125\n",
      "Epoch [9482/20000], Loss: 846.9849853515625, Entropy 424.1443786621094, Learning Rate: 0.0003125\n",
      "Epoch [9483/20000], Loss: 851.704345703125, Entropy 422.8806457519531, Learning Rate: 0.0003125\n",
      "Epoch [9484/20000], Loss: 850.7734375, Entropy 452.9181213378906, Learning Rate: 0.0003125\n",
      "Epoch [9485/20000], Loss: 848.060546875, Entropy 427.7267761230469, Learning Rate: 0.0003125\n",
      "Epoch [9486/20000], Loss: 921.2327880859375, Entropy 417.6360168457031, Learning Rate: 0.0003125\n",
      "Epoch [9487/20000], Loss: 900.2584228515625, Entropy 436.7635803222656, Learning Rate: 0.0003125\n",
      "Epoch [9488/20000], Loss: 837.9388427734375, Entropy 431.36328125, Learning Rate: 0.0003125\n",
      "Epoch [9489/20000], Loss: 877.3768310546875, Entropy 418.2336730957031, Learning Rate: 0.0003125\n",
      "Epoch [9490/20000], Loss: 850.6396484375, Entropy 446.5662841796875, Learning Rate: 0.0003125\n",
      "Epoch [9491/20000], Loss: 838.7783813476562, Entropy 426.55389404296875, Learning Rate: 0.0003125\n",
      "Epoch [9492/20000], Loss: 917.14013671875, Entropy 416.8995361328125, Learning Rate: 0.0003125\n",
      "Epoch [9493/20000], Loss: 832.601806640625, Entropy 428.0771789550781, Learning Rate: 0.0003125\n",
      "Epoch [9494/20000], Loss: 872.898193359375, Entropy 426.7752990722656, Learning Rate: 0.0003125\n",
      "Epoch [9495/20000], Loss: 876.1942138671875, Entropy 423.6195983886719, Learning Rate: 0.0003125\n",
      "Epoch [9496/20000], Loss: 826.2031860351562, Entropy 436.02008056640625, Learning Rate: 0.0003125\n",
      "Epoch [9497/20000], Loss: 854.9296264648438, Entropy 431.80218505859375, Learning Rate: 0.0003125\n",
      "Epoch [9498/20000], Loss: 871.3319091796875, Entropy 406.1310119628906, Learning Rate: 0.0003125\n",
      "Epoch [9499/20000], Loss: 874.2159423828125, Entropy 435.673828125, Learning Rate: 0.0003125\n",
      "Epoch [9500/20000], Loss: 878.169189453125, Entropy 427.4066162109375, Learning Rate: 0.0003125\n",
      "Epoch [9501/20000], Loss: 828.65087890625, Entropy 431.7117919921875, Learning Rate: 0.0003125\n",
      "Epoch [9502/20000], Loss: 891.2210693359375, Entropy 425.6463623046875, Learning Rate: 0.0003125\n",
      "Epoch [9503/20000], Loss: 937.1925048828125, Entropy 442.8293762207031, Learning Rate: 0.0003125\n",
      "Epoch [9504/20000], Loss: 896.6551513671875, Entropy 431.8082580566406, Learning Rate: 0.0003125\n",
      "Epoch [9505/20000], Loss: 847.3670654296875, Entropy 433.2189636230469, Learning Rate: 0.0003125\n",
      "Epoch [9506/20000], Loss: 858.6550903320312, Entropy 440.11248779296875, Learning Rate: 0.0003125\n",
      "Epoch [9507/20000], Loss: 815.7059326171875, Entropy 439.853271484375, Learning Rate: 0.0003125\n",
      "Epoch [9508/20000], Loss: 809.7265625, Entropy 429.7809143066406, Learning Rate: 0.0003125\n",
      "Epoch [9509/20000], Loss: 846.121337890625, Entropy 436.794677734375, Learning Rate: 0.0003125\n",
      "Epoch [9510/20000], Loss: 943.7332763671875, Entropy 419.0870361328125, Learning Rate: 0.0003125\n",
      "Epoch [9511/20000], Loss: 858.2500610351562, Entropy 413.95465087890625, Learning Rate: 0.0003125\n",
      "Epoch [9512/20000], Loss: 912.6798095703125, Entropy 428.3603820800781, Learning Rate: 0.0003125\n",
      "Epoch [9513/20000], Loss: 909.3773193359375, Entropy 434.454833984375, Learning Rate: 0.0003125\n",
      "Epoch [9514/20000], Loss: 905.815673828125, Entropy 427.416748046875, Learning Rate: 0.0003125\n",
      "Epoch [9515/20000], Loss: 822.24267578125, Entropy 422.8146667480469, Learning Rate: 0.0003125\n",
      "Epoch [9516/20000], Loss: 843.6580810546875, Entropy 427.2288818359375, Learning Rate: 0.0003125\n",
      "Epoch [9517/20000], Loss: 868.143798828125, Entropy 417.8980407714844, Learning Rate: 0.0003125\n",
      "Epoch [9518/20000], Loss: 889.6640625, Entropy 427.9164733886719, Learning Rate: 0.0003125\n",
      "Epoch [9519/20000], Loss: 855.2991943359375, Entropy 432.87744140625, Learning Rate: 0.0003125\n",
      "Epoch [9520/20000], Loss: 895.09765625, Entropy 421.0357666015625, Learning Rate: 0.0003125\n",
      "Epoch [9521/20000], Loss: 814.3529663085938, Entropy 454.63726806640625, Learning Rate: 0.0003125\n",
      "Epoch [9522/20000], Loss: 836.72509765625, Entropy 429.6102294921875, Learning Rate: 0.0003125\n",
      "Epoch [9523/20000], Loss: 873.9019775390625, Entropy 449.7828369140625, Learning Rate: 0.0003125\n",
      "Epoch [9524/20000], Loss: 871.8186645507812, Entropy 433.58782958984375, Learning Rate: 0.0003125\n",
      "Epoch [9525/20000], Loss: 881.4636840820312, Entropy 429.26910400390625, Learning Rate: 0.0003125\n",
      "Epoch [9526/20000], Loss: 868.0731201171875, Entropy 425.1165771484375, Learning Rate: 0.0003125\n",
      "Epoch [9527/20000], Loss: 841.7360229492188, Entropy 429.51373291015625, Learning Rate: 0.0003125\n",
      "Epoch [9528/20000], Loss: 942.8919677734375, Entropy 424.8986511230469, Learning Rate: 0.0003125\n",
      "Epoch [9529/20000], Loss: 876.1627197265625, Entropy 430.6501159667969, Learning Rate: 0.0003125\n",
      "Epoch [9530/20000], Loss: 847.5819091796875, Entropy 436.87060546875, Learning Rate: 0.0003125\n",
      "Epoch [9531/20000], Loss: 895.744873046875, Entropy 436.620849609375, Learning Rate: 0.0003125\n",
      "Epoch [9532/20000], Loss: 865.1968383789062, Entropy 437.93988037109375, Learning Rate: 0.0003125\n",
      "Epoch [9533/20000], Loss: 905.398681640625, Entropy 442.3229064941406, Learning Rate: 0.0003125\n",
      "Epoch [9534/20000], Loss: 864.2163696289062, Entropy 436.49652099609375, Learning Rate: 0.0003125\n",
      "Epoch [9535/20000], Loss: 902.2156982421875, Entropy 432.6263427734375, Learning Rate: 0.0003125\n",
      "Epoch [9536/20000], Loss: 835.2549438476562, Entropy 455.27960205078125, Learning Rate: 0.0003125\n",
      "Epoch [9537/20000], Loss: 851.82568359375, Entropy 435.5921630859375, Learning Rate: 0.0003125\n",
      "Epoch [9538/20000], Loss: 897.259765625, Entropy 429.7236328125, Learning Rate: 0.0003125\n",
      "Epoch [9539/20000], Loss: 832.1463623046875, Entropy 433.2020568847656, Learning Rate: 0.0003125\n",
      "Epoch [9540/20000], Loss: 840.7481079101562, Entropy 442.17181396484375, Learning Rate: 0.0003125\n",
      "Epoch [9541/20000], Loss: 830.5797119140625, Entropy 444.5936279296875, Learning Rate: 0.0003125\n",
      "Epoch [9542/20000], Loss: 871.16064453125, Entropy 428.5765380859375, Learning Rate: 0.0003125\n",
      "Epoch [9543/20000], Loss: 927.5723876953125, Entropy 433.9014892578125, Learning Rate: 0.0003125\n",
      "Epoch [9544/20000], Loss: 846.6511840820312, Entropy 437.95709228515625, Learning Rate: 0.0003125\n",
      "Epoch [9545/20000], Loss: 860.0995483398438, Entropy 429.72991943359375, Learning Rate: 0.0003125\n",
      "Epoch [9546/20000], Loss: 891.8941650390625, Entropy 438.1420593261719, Learning Rate: 0.0003125\n",
      "Epoch [9547/20000], Loss: 919.661865234375, Entropy 418.8844299316406, Learning Rate: 0.0003125\n",
      "Epoch [9548/20000], Loss: 869.22998046875, Entropy 437.3193664550781, Learning Rate: 0.0003125\n",
      "Epoch [9549/20000], Loss: 934.9317626953125, Entropy 429.296630859375, Learning Rate: 0.0003125\n",
      "Epoch [9550/20000], Loss: 880.2544555664062, Entropy 438.07611083984375, Learning Rate: 0.0003125\n",
      "Epoch [9551/20000], Loss: 891.03515625, Entropy 414.419677734375, Learning Rate: 0.0003125\n",
      "Epoch [9552/20000], Loss: 868.95849609375, Entropy 435.3459167480469, Learning Rate: 0.0003125\n",
      "Epoch [9553/20000], Loss: 877.0401611328125, Entropy 412.0885925292969, Learning Rate: 0.0003125\n",
      "Epoch [9554/20000], Loss: 828.5833129882812, Entropy 444.81341552734375, Learning Rate: 0.0003125\n",
      "Epoch [9555/20000], Loss: 876.2249755859375, Entropy 440.7756042480469, Learning Rate: 0.0003125\n",
      "Epoch [9556/20000], Loss: 870.0350341796875, Entropy 419.4956970214844, Learning Rate: 0.0003125\n",
      "Epoch [9557/20000], Loss: 892.7224731445312, Entropy 419.08306884765625, Learning Rate: 0.0003125\n",
      "Epoch [9558/20000], Loss: 875.9296875, Entropy 435.65283203125, Learning Rate: 0.0003125\n",
      "Epoch [9559/20000], Loss: 877.268798828125, Entropy 433.6520080566406, Learning Rate: 0.0003125\n",
      "Epoch [9560/20000], Loss: 899.8543090820312, Entropy 430.76788330078125, Learning Rate: 0.0003125\n",
      "Epoch [9561/20000], Loss: 877.4029541015625, Entropy 424.4524841308594, Learning Rate: 0.0003125\n",
      "Epoch [9562/20000], Loss: 960.7787475585938, Entropy 425.48492431640625, Learning Rate: 0.0003125\n",
      "Epoch [9563/20000], Loss: 852.5723876953125, Entropy 443.5467834472656, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9564/20000], Loss: 849.22119140625, Entropy 426.6727294921875, Learning Rate: 0.0003125\n",
      "Epoch [9565/20000], Loss: 891.5205078125, Entropy 427.1571350097656, Learning Rate: 0.0003125\n",
      "Epoch [9566/20000], Loss: 863.88818359375, Entropy 421.1109924316406, Learning Rate: 0.0003125\n",
      "Epoch [9567/20000], Loss: 880.8690185546875, Entropy 435.0925598144531, Learning Rate: 0.0003125\n",
      "Epoch [9568/20000], Loss: 837.1455688476562, Entropy 421.51458740234375, Learning Rate: 0.0003125\n",
      "Epoch [9569/20000], Loss: 815.5072631835938, Entropy 436.58221435546875, Learning Rate: 0.0003125\n",
      "Epoch [9570/20000], Loss: 851.6846923828125, Entropy 434.493896484375, Learning Rate: 0.0003125\n",
      "Epoch [9571/20000], Loss: 870.9375, Entropy 435.6454162597656, Learning Rate: 0.0003125\n",
      "Epoch [9572/20000], Loss: 881.4412841796875, Entropy 443.7068786621094, Learning Rate: 0.0003125\n",
      "Epoch [9573/20000], Loss: 808.9991455078125, Entropy 439.6961669921875, Learning Rate: 0.0003125\n",
      "Epoch [9574/20000], Loss: 947.501708984375, Entropy 415.9354248046875, Learning Rate: 0.0003125\n",
      "Epoch [9575/20000], Loss: 894.126953125, Entropy 434.70849609375, Learning Rate: 0.0003125\n",
      "Epoch [9576/20000], Loss: 862.392333984375, Entropy 433.4679870605469, Learning Rate: 0.0003125\n",
      "Epoch [9577/20000], Loss: 825.0152587890625, Entropy 450.5596923828125, Learning Rate: 0.0003125\n",
      "Epoch [9578/20000], Loss: 874.670654296875, Entropy 432.2344970703125, Learning Rate: 0.0003125\n",
      "Epoch [9579/20000], Loss: 913.7880859375, Entropy 438.2909240722656, Learning Rate: 0.0003125\n",
      "Epoch [9580/20000], Loss: 857.021728515625, Entropy 428.0831298828125, Learning Rate: 0.0003125\n",
      "Epoch [9581/20000], Loss: 860.7720947265625, Entropy 427.0179443359375, Learning Rate: 0.0003125\n",
      "Epoch [9582/20000], Loss: 867.486083984375, Entropy 431.6656799316406, Learning Rate: 0.0003125\n",
      "Epoch [9583/20000], Loss: 901.49658203125, Entropy 434.4305114746094, Learning Rate: 0.0003125\n",
      "Epoch [9584/20000], Loss: 884.058349609375, Entropy 435.8814697265625, Learning Rate: 0.0003125\n",
      "Epoch [9585/20000], Loss: 880.5826416015625, Entropy 411.4045104980469, Learning Rate: 0.0003125\n",
      "Epoch [9586/20000], Loss: 834.7318115234375, Entropy 452.941162109375, Learning Rate: 0.0003125\n",
      "Epoch [9587/20000], Loss: 886.3773193359375, Entropy 418.04736328125, Learning Rate: 0.0003125\n",
      "Epoch [9588/20000], Loss: 879.0181884765625, Entropy 430.0420227050781, Learning Rate: 0.0003125\n",
      "Epoch [9589/20000], Loss: 849.704833984375, Entropy 422.3720703125, Learning Rate: 0.0003125\n",
      "Epoch [9590/20000], Loss: 846.9459228515625, Entropy 428.8759765625, Learning Rate: 0.0003125\n",
      "Epoch [9591/20000], Loss: 842.06201171875, Entropy 449.2287292480469, Learning Rate: 0.0003125\n",
      "Epoch [9592/20000], Loss: 880.4856567382812, Entropy 425.08111572265625, Learning Rate: 0.0003125\n",
      "Epoch [9593/20000], Loss: 846.7736206054688, Entropy 435.98138427734375, Learning Rate: 0.0003125\n",
      "Epoch [9594/20000], Loss: 884.01513671875, Entropy 433.0150146484375, Learning Rate: 0.0003125\n",
      "Epoch [9595/20000], Loss: 830.5491333007812, Entropy 434.22015380859375, Learning Rate: 0.0003125\n",
      "Epoch [9596/20000], Loss: 847.2250366210938, Entropy 427.58123779296875, Learning Rate: 0.0003125\n",
      "Epoch [9597/20000], Loss: 803.7825317382812, Entropy 445.03826904296875, Learning Rate: 0.0003125\n",
      "Epoch [9598/20000], Loss: 878.7669677734375, Entropy 428.1881408691406, Learning Rate: 0.0003125\n",
      "Epoch [9599/20000], Loss: 826.9868774414062, Entropy 447.09686279296875, Learning Rate: 0.0003125\n",
      "Epoch [9600/20000], Loss: 853.378662109375, Entropy 432.2330017089844, Learning Rate: 0.0003125\n",
      "Epoch [9601/20000], Loss: 874.1481323242188, Entropy 430.87359619140625, Learning Rate: 0.0003125\n",
      "Epoch [9602/20000], Loss: 852.588623046875, Entropy 427.4236145019531, Learning Rate: 0.0003125\n",
      "Epoch [9603/20000], Loss: 864.8758544921875, Entropy 427.3928527832031, Learning Rate: 0.0003125\n",
      "Epoch [9604/20000], Loss: 874.7107543945312, Entropy 427.92437744140625, Learning Rate: 0.0003125\n",
      "Epoch [9605/20000], Loss: 860.0194702148438, Entropy 431.95257568359375, Learning Rate: 0.0003125\n",
      "Epoch [9606/20000], Loss: 851.9650268554688, Entropy 441.05889892578125, Learning Rate: 0.0003125\n",
      "Epoch [9607/20000], Loss: 856.651611328125, Entropy 434.4686279296875, Learning Rate: 0.0003125\n",
      "Epoch [9608/20000], Loss: 835.8317260742188, Entropy 443.52276611328125, Learning Rate: 0.0003125\n",
      "Epoch [9609/20000], Loss: 872.212158203125, Entropy 435.2566833496094, Learning Rate: 0.0003125\n",
      "Epoch [9610/20000], Loss: 899.5429077148438, Entropy 447.92083740234375, Learning Rate: 0.0003125\n",
      "Epoch [9611/20000], Loss: 832.82421875, Entropy 436.2028503417969, Learning Rate: 0.0003125\n",
      "Epoch [9612/20000], Loss: 853.3829345703125, Entropy 444.8274841308594, Learning Rate: 0.0003125\n",
      "Epoch [9613/20000], Loss: 869.6910400390625, Entropy 429.2179260253906, Learning Rate: 0.0003125\n",
      "Epoch [9614/20000], Loss: 875.84912109375, Entropy 438.6041259765625, Learning Rate: 0.0003125\n",
      "Epoch [9615/20000], Loss: 879.514404296875, Entropy 424.87255859375, Learning Rate: 0.0003125\n",
      "Epoch [9616/20000], Loss: 846.3709716796875, Entropy 432.8094482421875, Learning Rate: 0.0003125\n",
      "Epoch [9617/20000], Loss: 902.8226318359375, Entropy 433.4973449707031, Learning Rate: 0.0003125\n",
      "Epoch [9618/20000], Loss: 876.6680908203125, Entropy 427.2478942871094, Learning Rate: 0.0003125\n",
      "Epoch [9619/20000], Loss: 837.9381713867188, Entropy 452.76385498046875, Learning Rate: 0.0003125\n",
      "Epoch [9620/20000], Loss: 851.6021728515625, Entropy 422.5987854003906, Learning Rate: 0.0003125\n",
      "Epoch [9621/20000], Loss: 898.8565063476562, Entropy 419.29913330078125, Learning Rate: 0.0003125\n",
      "Epoch [9622/20000], Loss: 882.34619140625, Entropy 447.0271301269531, Learning Rate: 0.0003125\n",
      "Epoch [9623/20000], Loss: 857.142333984375, Entropy 412.6792907714844, Learning Rate: 0.0003125\n",
      "Epoch [9624/20000], Loss: 906.0154418945312, Entropy 445.57232666015625, Learning Rate: 0.0003125\n",
      "Epoch [9625/20000], Loss: 959.6173095703125, Entropy 433.4124755859375, Learning Rate: 0.0003125\n",
      "Epoch [9626/20000], Loss: 881.078857421875, Entropy 440.7923278808594, Learning Rate: 0.0003125\n",
      "Epoch [9627/20000], Loss: 903.100341796875, Entropy 442.62744140625, Learning Rate: 0.0003125\n",
      "Epoch [9628/20000], Loss: 843.123779296875, Entropy 430.1105651855469, Learning Rate: 0.0003125\n",
      "Epoch [9629/20000], Loss: 845.1554565429688, Entropy 438.99249267578125, Learning Rate: 0.0003125\n",
      "Epoch [9630/20000], Loss: 834.190673828125, Entropy 435.8821105957031, Learning Rate: 0.0003125\n",
      "Epoch [9631/20000], Loss: 847.4279174804688, Entropy 446.38140869140625, Learning Rate: 0.0003125\n",
      "Epoch [9632/20000], Loss: 801.8568115234375, Entropy 439.5755310058594, Learning Rate: 0.0003125\n",
      "Epoch [9633/20000], Loss: 831.384521484375, Entropy 447.0934143066406, Learning Rate: 0.0003125\n",
      "Epoch [9634/20000], Loss: 859.6998291015625, Entropy 434.5311584472656, Learning Rate: 0.0003125\n",
      "Epoch [9635/20000], Loss: 848.577392578125, Entropy 433.0872497558594, Learning Rate: 0.0003125\n",
      "Epoch [9636/20000], Loss: 886.3367919921875, Entropy 435.319091796875, Learning Rate: 0.0003125\n",
      "Epoch [9637/20000], Loss: 839.01220703125, Entropy 448.6117858886719, Learning Rate: 0.0003125\n",
      "Epoch [9638/20000], Loss: 841.2115478515625, Entropy 441.5087890625, Learning Rate: 0.0003125\n",
      "Epoch [9639/20000], Loss: 914.700439453125, Entropy 426.3611145019531, Learning Rate: 0.0003125\n",
      "Epoch [9640/20000], Loss: 885.9136962890625, Entropy 423.1915588378906, Learning Rate: 0.0003125\n",
      "Epoch [9641/20000], Loss: 845.6719970703125, Entropy 431.2303771972656, Learning Rate: 0.0003125\n",
      "Epoch [9642/20000], Loss: 842.2210083007812, Entropy 440.44439697265625, Learning Rate: 0.0003125\n",
      "Epoch [9643/20000], Loss: 927.319091796875, Entropy 430.3842468261719, Learning Rate: 0.0003125\n",
      "Epoch [9644/20000], Loss: 845.8221435546875, Entropy 421.6671142578125, Learning Rate: 0.0003125\n",
      "Epoch [9645/20000], Loss: 891.9058837890625, Entropy 419.0993347167969, Learning Rate: 0.0003125\n",
      "Epoch [9646/20000], Loss: 935.3766479492188, Entropy 425.60076904296875, Learning Rate: 0.0003125\n",
      "Epoch [9647/20000], Loss: 839.4879760742188, Entropy 418.17669677734375, Learning Rate: 0.0003125\n",
      "Epoch [9648/20000], Loss: 889.7181396484375, Entropy 431.8796691894531, Learning Rate: 0.0003125\n",
      "Epoch [9649/20000], Loss: 822.8369140625, Entropy 435.9239807128906, Learning Rate: 0.0003125\n",
      "Epoch [9650/20000], Loss: 895.64306640625, Entropy 439.4602355957031, Learning Rate: 0.0003125\n",
      "Epoch [9651/20000], Loss: 883.8463134765625, Entropy 414.7111511230469, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9652/20000], Loss: 904.8280029296875, Entropy 428.4306945800781, Learning Rate: 0.0003125\n",
      "Epoch [9653/20000], Loss: 879.727783203125, Entropy 447.1462707519531, Learning Rate: 0.0003125\n",
      "Epoch [9654/20000], Loss: 792.579345703125, Entropy 439.8924560546875, Learning Rate: 0.0003125\n",
      "Epoch [9655/20000], Loss: 843.2634887695312, Entropy 415.56475830078125, Learning Rate: 0.0003125\n",
      "Epoch [9656/20000], Loss: 852.5001220703125, Entropy 454.5721130371094, Learning Rate: 0.0003125\n",
      "Epoch [9657/20000], Loss: 831.91796875, Entropy 442.2560119628906, Learning Rate: 0.0003125\n",
      "Epoch [9658/20000], Loss: 906.0093383789062, Entropy 419.38873291015625, Learning Rate: 0.0003125\n",
      "Epoch [9659/20000], Loss: 816.0469970703125, Entropy 435.9825744628906, Learning Rate: 0.0003125\n",
      "Epoch [9660/20000], Loss: 889.3198852539062, Entropy 429.94158935546875, Learning Rate: 0.0003125\n",
      "Epoch [9661/20000], Loss: 861.1224365234375, Entropy 423.1917724609375, Learning Rate: 0.0003125\n",
      "Epoch [9662/20000], Loss: 864.541748046875, Entropy 430.5129699707031, Learning Rate: 0.0003125\n",
      "Epoch [9663/20000], Loss: 903.0076904296875, Entropy 423.8929138183594, Learning Rate: 0.0003125\n",
      "Epoch [9664/20000], Loss: 852.09765625, Entropy 450.4820251464844, Learning Rate: 0.0003125\n",
      "Epoch [9665/20000], Loss: 848.25634765625, Entropy 439.2304992675781, Learning Rate: 0.0003125\n",
      "Epoch [9666/20000], Loss: 851.654052734375, Entropy 439.3535461425781, Learning Rate: 0.0003125\n",
      "Epoch [9667/20000], Loss: 874.9797973632812, Entropy 426.32830810546875, Learning Rate: 0.0003125\n",
      "Epoch [9668/20000], Loss: 844.638427734375, Entropy 438.8751525878906, Learning Rate: 0.0003125\n",
      "Epoch [9669/20000], Loss: 870.5499267578125, Entropy 446.6439208984375, Learning Rate: 0.0003125\n",
      "Epoch [9670/20000], Loss: 860.3048095703125, Entropy 443.1271057128906, Learning Rate: 0.0003125\n",
      "Epoch [9671/20000], Loss: 884.7764282226562, Entropy 434.27825927734375, Learning Rate: 0.0003125\n",
      "Epoch [9672/20000], Loss: 872.4080810546875, Entropy 434.1853942871094, Learning Rate: 0.0003125\n",
      "Epoch [9673/20000], Loss: 897.554931640625, Entropy 416.1252136230469, Learning Rate: 0.0003125\n",
      "Epoch [9674/20000], Loss: 882.2027587890625, Entropy 431.7320251464844, Learning Rate: 0.0003125\n",
      "Epoch [9675/20000], Loss: 861.6851806640625, Entropy 433.5452880859375, Learning Rate: 0.0003125\n",
      "Epoch [9676/20000], Loss: 830.55322265625, Entropy 443.9444580078125, Learning Rate: 0.0003125\n",
      "Epoch [9677/20000], Loss: 837.296875, Entropy 430.0950927734375, Learning Rate: 0.0003125\n",
      "Epoch [9678/20000], Loss: 912.9237060546875, Entropy 443.5262756347656, Learning Rate: 0.0003125\n",
      "Epoch [9679/20000], Loss: 846.5872802734375, Entropy 453.2606201171875, Learning Rate: 0.0003125\n",
      "Epoch [9680/20000], Loss: 851.386962890625, Entropy 440.138671875, Learning Rate: 0.0003125\n",
      "Epoch [9681/20000], Loss: 859.5576782226562, Entropy 437.79437255859375, Learning Rate: 0.0003125\n",
      "Epoch [9682/20000], Loss: 879.8368530273438, Entropy 434.81878662109375, Learning Rate: 0.0003125\n",
      "Epoch [9683/20000], Loss: 905.6622924804688, Entropy 439.82427978515625, Learning Rate: 0.0003125\n",
      "Epoch [9684/20000], Loss: 846.282958984375, Entropy 433.8749084472656, Learning Rate: 0.0003125\n",
      "Epoch [9685/20000], Loss: 884.0260620117188, Entropy 428.71917724609375, Learning Rate: 0.0003125\n",
      "Epoch [9686/20000], Loss: 868.7349853515625, Entropy 441.1466369628906, Learning Rate: 0.0003125\n",
      "Epoch [9687/20000], Loss: 831.4205322265625, Entropy 439.6988830566406, Learning Rate: 0.0003125\n",
      "Epoch [9688/20000], Loss: 858.963623046875, Entropy 422.7386474609375, Learning Rate: 0.0003125\n",
      "Epoch [9689/20000], Loss: 880.4674682617188, Entropy 428.97662353515625, Learning Rate: 0.0003125\n",
      "Epoch [9690/20000], Loss: 820.331787109375, Entropy 417.2643737792969, Learning Rate: 0.0003125\n",
      "Epoch [9691/20000], Loss: 932.9954223632812, Entropy 428.46905517578125, Learning Rate: 0.0003125\n",
      "Epoch [9692/20000], Loss: 882.3687744140625, Entropy 432.0396728515625, Learning Rate: 0.0003125\n",
      "Epoch [9693/20000], Loss: 832.8764038085938, Entropy 432.84088134765625, Learning Rate: 0.0003125\n",
      "Epoch [9694/20000], Loss: 896.469970703125, Entropy 440.4016418457031, Learning Rate: 0.0003125\n",
      "Epoch [9695/20000], Loss: 848.8892822265625, Entropy 436.2563171386719, Learning Rate: 0.0003125\n",
      "Epoch [9696/20000], Loss: 853.1968994140625, Entropy 431.0491943359375, Learning Rate: 0.0003125\n",
      "Epoch [9697/20000], Loss: 867.1546630859375, Entropy 445.3348693847656, Learning Rate: 0.0003125\n",
      "Epoch [9698/20000], Loss: 865.7161254882812, Entropy 442.49456787109375, Learning Rate: 0.0003125\n",
      "Epoch [9699/20000], Loss: 828.274169921875, Entropy 461.4361572265625, Learning Rate: 0.0003125\n",
      "Epoch [9700/20000], Loss: 857.96875, Entropy 418.5841064453125, Learning Rate: 0.0003125\n",
      "Epoch [9701/20000], Loss: 901.5491943359375, Entropy 423.4028015136719, Learning Rate: 0.0003125\n",
      "Epoch [9702/20000], Loss: 893.37841796875, Entropy 447.1385192871094, Learning Rate: 0.0003125\n",
      "Epoch [9703/20000], Loss: 862.042724609375, Entropy 439.9921875, Learning Rate: 0.0003125\n",
      "Epoch [9704/20000], Loss: 859.8327026367188, Entropy 444.49517822265625, Learning Rate: 0.0003125\n",
      "Epoch [9705/20000], Loss: 860.967041015625, Entropy 421.7518005371094, Learning Rate: 0.0003125\n",
      "Epoch [9706/20000], Loss: 894.161376953125, Entropy 439.6952209472656, Learning Rate: 0.0003125\n",
      "Epoch [9707/20000], Loss: 838.0086669921875, Entropy 444.2890625, Learning Rate: 0.0003125\n",
      "Epoch [9708/20000], Loss: 853.4326171875, Entropy 431.3615417480469, Learning Rate: 0.0003125\n",
      "Epoch [9709/20000], Loss: 874.977783203125, Entropy 454.3924255371094, Learning Rate: 0.0003125\n",
      "Epoch [9710/20000], Loss: 822.10791015625, Entropy 426.5703430175781, Learning Rate: 0.0003125\n",
      "Epoch [9711/20000], Loss: 883.6204833984375, Entropy 429.056396484375, Learning Rate: 0.0003125\n",
      "Epoch [9712/20000], Loss: 815.9239501953125, Entropy 432.6993408203125, Learning Rate: 0.0003125\n",
      "Epoch [9713/20000], Loss: 872.7489013671875, Entropy 427.7000427246094, Learning Rate: 0.0003125\n",
      "Epoch [9714/20000], Loss: 863.119873046875, Entropy 439.24365234375, Learning Rate: 0.0003125\n",
      "Epoch [9715/20000], Loss: 881.558349609375, Entropy 438.5426330566406, Learning Rate: 0.0003125\n",
      "Epoch [9716/20000], Loss: 909.885986328125, Entropy 435.5438232421875, Learning Rate: 0.0003125\n",
      "Epoch [9717/20000], Loss: 839.4024658203125, Entropy 426.7761535644531, Learning Rate: 0.0003125\n",
      "Epoch [9718/20000], Loss: 890.13623046875, Entropy 439.4006042480469, Learning Rate: 0.0003125\n",
      "Epoch [9719/20000], Loss: 836.9983520507812, Entropy 455.60565185546875, Learning Rate: 0.0003125\n",
      "Epoch [9720/20000], Loss: 882.0096435546875, Entropy 425.884521484375, Learning Rate: 0.0003125\n",
      "Epoch [9721/20000], Loss: 902.1395263671875, Entropy 433.3001403808594, Learning Rate: 0.0003125\n",
      "Epoch [9722/20000], Loss: 836.80859375, Entropy 422.56201171875, Learning Rate: 0.0003125\n",
      "Epoch [9723/20000], Loss: 836.37109375, Entropy 444.870849609375, Learning Rate: 0.0003125\n",
      "Epoch [9724/20000], Loss: 899.3304443359375, Entropy 420.1366271972656, Learning Rate: 0.0003125\n",
      "Epoch [9725/20000], Loss: 862.1013793945312, Entropy 431.13409423828125, Learning Rate: 0.0003125\n",
      "Epoch [9726/20000], Loss: 861.7561645507812, Entropy 419.80718994140625, Learning Rate: 0.0003125\n",
      "Epoch [9727/20000], Loss: 899.7268676757812, Entropy 438.36749267578125, Learning Rate: 0.0003125\n",
      "Epoch [9728/20000], Loss: 876.0284423828125, Entropy 458.4446716308594, Learning Rate: 0.0003125\n",
      "Epoch [9729/20000], Loss: 862.0399169921875, Entropy 428.336669921875, Learning Rate: 0.0003125\n",
      "Epoch [9730/20000], Loss: 913.8165283203125, Entropy 435.00537109375, Learning Rate: 0.0003125\n",
      "Epoch [9731/20000], Loss: 893.57958984375, Entropy 437.8785095214844, Learning Rate: 0.0003125\n",
      "Epoch [9732/20000], Loss: 947.032470703125, Entropy 424.9722595214844, Learning Rate: 0.0003125\n",
      "Epoch [9733/20000], Loss: 900.8505859375, Entropy 442.9615478515625, Learning Rate: 0.0003125\n",
      "Epoch [9734/20000], Loss: 852.577392578125, Entropy 439.6031494140625, Learning Rate: 0.0003125\n",
      "Epoch [9735/20000], Loss: 841.8709716796875, Entropy 453.9161376953125, Learning Rate: 0.0003125\n",
      "Epoch [9736/20000], Loss: 869.2549438476562, Entropy 439.74310302734375, Learning Rate: 0.0003125\n",
      "Epoch [9737/20000], Loss: 852.3807983398438, Entropy 447.75885009765625, Learning Rate: 0.0003125\n",
      "Epoch [9738/20000], Loss: 822.3560791015625, Entropy 445.5373840332031, Learning Rate: 0.0003125\n",
      "Epoch [9739/20000], Loss: 896.1217041015625, Entropy 445.8564453125, Learning Rate: 0.0003125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9740/20000], Loss: 855.9324951171875, Entropy 433.1651611328125, Learning Rate: 0.0003125\n",
      "Epoch [9741/20000], Loss: 849.811767578125, Entropy 447.4547119140625, Learning Rate: 0.0003125\n",
      "Epoch [9742/20000], Loss: 840.202880859375, Entropy 444.5036926269531, Learning Rate: 0.0003125\n",
      "Epoch [9743/20000], Loss: 881.260009765625, Entropy 431.8333435058594, Learning Rate: 0.0003125\n",
      "Epoch [9744/20000], Loss: 890.24560546875, Entropy 426.1346435546875, Learning Rate: 0.0003125\n",
      "Epoch [9745/20000], Loss: 831.8790283203125, Entropy 447.3736877441406, Learning Rate: 0.0003125\n",
      "Epoch [9746/20000], Loss: 832.6914672851562, Entropy 440.42999267578125, Learning Rate: 0.0003125\n",
      "Epoch [9747/20000], Loss: 902.5416259765625, Entropy 420.8477783203125, Learning Rate: 0.0003125\n",
      "Epoch [9748/20000], Loss: 855.37158203125, Entropy 436.4895324707031, Learning Rate: 0.0003125\n",
      "Epoch [9749/20000], Loss: 889.6661376953125, Entropy 437.4566650390625, Learning Rate: 0.0003125\n",
      "Epoch [9750/20000], Loss: 862.95263671875, Entropy 437.3815612792969, Learning Rate: 0.0003125\n",
      "Epoch [9751/20000], Loss: 923.415771484375, Entropy 439.8304138183594, Learning Rate: 0.0003125\n",
      "Epoch [9752/20000], Loss: 844.23876953125, Entropy 463.8135986328125, Learning Rate: 0.0003125\n",
      "Epoch [9753/20000], Loss: 880.0606079101562, Entropy 430.11480712890625, Learning Rate: 0.0003125\n",
      "Epoch [9754/20000], Loss: 865.7046508789062, Entropy 451.58233642578125, Learning Rate: 0.0003125\n",
      "Epoch [9755/20000], Loss: 934.2960205078125, Entropy 423.4039611816406, Learning Rate: 0.0003125\n",
      "Epoch [9756/20000], Loss: 814.552001953125, Entropy 443.3885498046875, Learning Rate: 0.0003125\n",
      "Epoch [9757/20000], Loss: 835.2276000976562, Entropy 446.53826904296875, Learning Rate: 0.0003125\n",
      "Epoch [9758/20000], Loss: 914.6788330078125, Entropy 434.1122131347656, Learning Rate: 0.0003125\n",
      "Epoch [9759/20000], Loss: 914.92822265625, Entropy 431.064208984375, Learning Rate: 0.0003125\n",
      "Epoch [9760/20000], Loss: 900.27001953125, Entropy 443.7925720214844, Learning Rate: 0.0003125\n",
      "Epoch [9761/20000], Loss: 816.0958251953125, Entropy 447.9797668457031, Learning Rate: 0.0003125\n",
      "Epoch [9762/20000], Loss: 859.6471557617188, Entropy 441.04388427734375, Learning Rate: 0.0003125\n",
      "Epoch [9763/20000], Loss: 875.9912109375, Entropy 428.1613464355469, Learning Rate: 0.0003125\n",
      "Epoch [9764/20000], Loss: 860.7232055664062, Entropy 450.18304443359375, Learning Rate: 0.0003125\n",
      "Epoch [9765/20000], Loss: 844.9517822265625, Entropy 446.3353271484375, Learning Rate: 0.0003125\n",
      "Epoch [9766/20000], Loss: 883.8466796875, Entropy 419.5926513671875, Learning Rate: 0.0003125\n",
      "Epoch [9767/20000], Loss: 883.4151000976562, Entropy 439.74224853515625, Learning Rate: 0.0003125\n",
      "Epoch [9768/20000], Loss: 864.7745361328125, Entropy 433.9517822265625, Learning Rate: 0.0003125\n",
      "Epoch [9769/20000], Loss: 868.808837890625, Entropy 439.8731689453125, Learning Rate: 0.0003125\n",
      "Epoch [9770/20000], Loss: 876.9862670898438, Entropy 433.65765380859375, Learning Rate: 0.0003125\n",
      "Epoch [9771/20000], Loss: 859.855712890625, Entropy 423.8224182128906, Learning Rate: 0.0003125\n",
      "Epoch [9772/20000], Loss: 866.6766357421875, Entropy 430.0201416015625, Learning Rate: 0.0003125\n",
      "Epoch [9773/20000], Loss: 841.2965087890625, Entropy 442.1486511230469, Learning Rate: 0.0003125\n",
      "Epoch [9774/20000], Loss: 880.07666015625, Entropy 433.1392822265625, Learning Rate: 0.0003125\n",
      "Epoch [9775/20000], Loss: 889.2335205078125, Entropy 435.6676025390625, Learning Rate: 0.0003125\n",
      "Epoch [9776/20000], Loss: 832.4300537109375, Entropy 432.857421875, Learning Rate: 0.0003125\n",
      "Epoch [9777/20000], Loss: 917.625732421875, Entropy 422.3414001464844, Learning Rate: 0.0003125\n",
      "Epoch [9778/20000], Loss: 854.1407470703125, Entropy 437.3287658691406, Learning Rate: 0.0003125\n",
      "Epoch [9779/20000], Loss: 890.0999145507812, Entropy 434.96771240234375, Learning Rate: 0.0003125\n",
      "Epoch [9780/20000], Loss: 902.089111328125, Entropy 442.4441223144531, Learning Rate: 0.0003125\n",
      "Epoch [9781/20000], Loss: 830.4599609375, Entropy 422.9192199707031, Learning Rate: 0.0003125\n",
      "Epoch [9782/20000], Loss: 804.7977294921875, Entropy 443.1969299316406, Learning Rate: 0.0003125\n",
      "Epoch [9783/20000], Loss: 896.271240234375, Entropy 433.8435974121094, Learning Rate: 0.0003125\n",
      "Epoch [9784/20000], Loss: 859.6204223632812, Entropy 439.41021728515625, Learning Rate: 0.0003125\n",
      "Epoch [9785/20000], Loss: 886.898681640625, Entropy 439.1342468261719, Learning Rate: 0.0003125\n",
      "Epoch [9786/20000], Loss: 840.4375610351562, Entropy 436.13861083984375, Learning Rate: 0.0003125\n",
      "Epoch [9787/20000], Loss: 893.2333984375, Entropy 440.4454650878906, Learning Rate: 0.0003125\n",
      "Epoch [9788/20000], Loss: 847.3995361328125, Entropy 433.5494384765625, Learning Rate: 0.0003125\n",
      "Epoch [9789/20000], Loss: 870.3095703125, Entropy 430.9803771972656, Learning Rate: 0.0003125\n",
      "Epoch [9790/20000], Loss: 867.5970458984375, Entropy 454.1083068847656, Learning Rate: 0.0003125\n",
      "Epoch [9791/20000], Loss: 823.8272094726562, Entropy 444.75335693359375, Learning Rate: 0.0003125\n",
      "Epoch [9792/20000], Loss: 844.6708984375, Entropy 446.6667785644531, Learning Rate: 0.0003125\n",
      "Epoch [9793/20000], Loss: 867.231689453125, Entropy 442.7498474121094, Learning Rate: 0.0003125\n",
      "Epoch [9794/20000], Loss: 897.6888427734375, Entropy 451.4779968261719, Learning Rate: 0.0003125\n",
      "Epoch [9795/20000], Loss: 838.7329711914062, Entropy 429.99810791015625, Learning Rate: 0.0003125\n",
      "Epoch [9796/20000], Loss: 867.0823974609375, Entropy 427.6472473144531, Learning Rate: 0.0003125\n",
      "Epoch [9797/20000], Loss: 870.13037109375, Entropy 439.1822509765625, Learning Rate: 0.0003125\n",
      "Epoch [9798/20000], Loss: 860.4849853515625, Entropy 452.3927917480469, Learning Rate: 0.0003125\n",
      "Epoch [9799/20000], Loss: 837.4819946289062, Entropy 444.08319091796875, Learning Rate: 0.0003125\n",
      "Epoch [9800/20000], Loss: 914.4775390625, Entropy 419.9148254394531, Learning Rate: 0.0003125\n",
      "Epoch [9801/20000], Loss: 898.008544921875, Entropy 434.6272277832031, Learning Rate: 0.0003125\n",
      "Epoch [9802/20000], Loss: 864.774169921875, Entropy 434.5982666015625, Learning Rate: 0.0003125\n",
      "Epoch [9803/20000], Loss: 868.8145141601562, Entropy 427.98956298828125, Learning Rate: 0.0003125\n",
      "Epoch [9804/20000], Loss: 847.3189697265625, Entropy 435.2767333984375, Learning Rate: 0.0003125\n",
      "Epoch [9805/20000], Loss: 849.332763671875, Entropy 436.601318359375, Learning Rate: 0.0003125\n",
      "Epoch [9806/20000], Loss: 866.814453125, Entropy 432.8161926269531, Learning Rate: 0.0003125\n",
      "Epoch [9807/20000], Loss: 865.59033203125, Entropy 450.9407043457031, Learning Rate: 0.0003125\n",
      "Epoch [9808/20000], Loss: 902.0776977539062, Entropy 448.62542724609375, Learning Rate: 0.0003125\n",
      "Epoch [9809/20000], Loss: 851.6338500976562, Entropy 453.58319091796875, Learning Rate: 0.0003125\n",
      "Epoch [9810/20000], Loss: 852.169921875, Entropy 442.40087890625, Learning Rate: 0.0003125\n",
      "Epoch [9811/20000], Loss: 841.0855712890625, Entropy 457.8514099121094, Learning Rate: 0.0003125\n",
      "Epoch [9812/20000], Loss: 845.4758911132812, Entropy 431.98272705078125, Learning Rate: 0.0003125\n",
      "Epoch [9813/20000], Loss: 821.804931640625, Entropy 449.8531188964844, Learning Rate: 0.0003125\n",
      "Epoch [9814/20000], Loss: 947.05517578125, Entropy 433.11669921875, Learning Rate: 0.0003125\n",
      "Epoch [9815/20000], Loss: 912.9888305664062, Entropy 440.10955810546875, Learning Rate: 0.0003125\n",
      "Epoch [9816/20000], Loss: 865.7681884765625, Entropy 435.3881530761719, Learning Rate: 0.0003125\n",
      "Epoch [9817/20000], Loss: 872.5956420898438, Entropy 422.36358642578125, Learning Rate: 0.0003125\n",
      "Epoch [9818/20000], Loss: 856.127197265625, Entropy 432.1811218261719, Learning Rate: 0.0003125\n",
      "Epoch [9819/20000], Loss: 853.1135864257812, Entropy 430.04022216796875, Learning Rate: 0.0003125\n",
      "Epoch [9820/20000], Loss: 878.6149291992188, Entropy 439.99530029296875, Learning Rate: 0.0003125\n",
      "Epoch [9821/20000], Loss: 814.3421020507812, Entropy 432.98248291015625, Learning Rate: 0.0003125\n",
      "Epoch [9822/20000], Loss: 840.4490966796875, Entropy 449.1129455566406, Learning Rate: 0.0003125\n",
      "Epoch [9823/20000], Loss: 963.9332275390625, Entropy 425.4959716796875, Learning Rate: 0.00015625\n",
      "Epoch [9824/20000], Loss: 845.2880249023438, Entropy 448.34918212890625, Learning Rate: 0.00015625\n",
      "Epoch [9825/20000], Loss: 891.7207641601562, Entropy 445.62847900390625, Learning Rate: 0.00015625\n",
      "Epoch [9826/20000], Loss: 882.986572265625, Entropy 426.0558166503906, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9827/20000], Loss: 832.712158203125, Entropy 435.1099853515625, Learning Rate: 0.00015625\n",
      "Epoch [9828/20000], Loss: 846.0999755859375, Entropy 450.9231872558594, Learning Rate: 0.00015625\n",
      "Epoch [9829/20000], Loss: 880.2757568359375, Entropy 429.9736633300781, Learning Rate: 0.00015625\n",
      "Epoch [9830/20000], Loss: 856.2135620117188, Entropy 444.55499267578125, Learning Rate: 0.00015625\n",
      "Epoch [9831/20000], Loss: 857.0945434570312, Entropy 430.26763916015625, Learning Rate: 0.00015625\n",
      "Epoch [9832/20000], Loss: 888.61083984375, Entropy 430.7330322265625, Learning Rate: 0.00015625\n",
      "Epoch [9833/20000], Loss: 925.5520629882812, Entropy 443.66754150390625, Learning Rate: 0.00015625\n",
      "Epoch [9834/20000], Loss: 872.146240234375, Entropy 443.0643005371094, Learning Rate: 0.00015625\n",
      "Epoch [9835/20000], Loss: 852.2205810546875, Entropy 437.9227294921875, Learning Rate: 0.00015625\n",
      "Epoch [9836/20000], Loss: 838.3568115234375, Entropy 437.7654113769531, Learning Rate: 0.00015625\n",
      "Epoch [9837/20000], Loss: 899.4910888671875, Entropy 425.4318542480469, Learning Rate: 0.00015625\n",
      "Epoch [9838/20000], Loss: 849.6136474609375, Entropy 437.5540466308594, Learning Rate: 0.00015625\n",
      "Epoch [9839/20000], Loss: 878.1572265625, Entropy 444.5574035644531, Learning Rate: 0.00015625\n",
      "Epoch [9840/20000], Loss: 883.3200073242188, Entropy 445.01092529296875, Learning Rate: 0.00015625\n",
      "Epoch [9841/20000], Loss: 885.8250732421875, Entropy 434.2314453125, Learning Rate: 0.00015625\n",
      "Epoch [9842/20000], Loss: 832.0945434570312, Entropy 449.03900146484375, Learning Rate: 0.00015625\n",
      "Epoch [9843/20000], Loss: 880.30419921875, Entropy 419.0783386230469, Learning Rate: 0.00015625\n",
      "Epoch [9844/20000], Loss: 911.643798828125, Entropy 437.3383483886719, Learning Rate: 0.00015625\n",
      "Epoch [9845/20000], Loss: 820.049560546875, Entropy 440.6587829589844, Learning Rate: 0.00015625\n",
      "Epoch [9846/20000], Loss: 850.406005859375, Entropy 448.1224060058594, Learning Rate: 0.00015625\n",
      "Epoch [9847/20000], Loss: 860.01513671875, Entropy 443.2990417480469, Learning Rate: 0.00015625\n",
      "Epoch [9848/20000], Loss: 877.0399169921875, Entropy 432.6649169921875, Learning Rate: 0.00015625\n",
      "Epoch [9849/20000], Loss: 890.481201171875, Entropy 436.4320068359375, Learning Rate: 0.00015625\n",
      "Epoch [9850/20000], Loss: 854.3811645507812, Entropy 445.87823486328125, Learning Rate: 0.00015625\n",
      "Epoch [9851/20000], Loss: 931.2254638671875, Entropy 440.0685729980469, Learning Rate: 0.00015625\n",
      "Epoch [9852/20000], Loss: 868.535888671875, Entropy 442.2940673828125, Learning Rate: 0.00015625\n",
      "Epoch [9853/20000], Loss: 912.5118408203125, Entropy 434.8096008300781, Learning Rate: 0.00015625\n",
      "Epoch [9854/20000], Loss: 897.543212890625, Entropy 431.1981201171875, Learning Rate: 0.00015625\n",
      "Epoch [9855/20000], Loss: 859.3861083984375, Entropy 446.2898254394531, Learning Rate: 0.00015625\n",
      "Epoch [9856/20000], Loss: 855.6942138671875, Entropy 434.0918273925781, Learning Rate: 0.00015625\n",
      "Epoch [9857/20000], Loss: 928.7835693359375, Entropy 434.1497802734375, Learning Rate: 0.00015625\n",
      "Epoch [9858/20000], Loss: 811.7819213867188, Entropy 437.05303955078125, Learning Rate: 0.00015625\n",
      "Epoch [9859/20000], Loss: 885.313720703125, Entropy 447.0422668457031, Learning Rate: 0.00015625\n",
      "Epoch [9860/20000], Loss: 847.1898193359375, Entropy 438.8699645996094, Learning Rate: 0.00015625\n",
      "Epoch [9861/20000], Loss: 872.31591796875, Entropy 428.830810546875, Learning Rate: 0.00015625\n",
      "Epoch [9862/20000], Loss: 840.151611328125, Entropy 442.7484130859375, Learning Rate: 0.00015625\n",
      "Epoch [9863/20000], Loss: 898.949462890625, Entropy 436.0348815917969, Learning Rate: 0.00015625\n",
      "Epoch [9864/20000], Loss: 846.8065185546875, Entropy 454.63232421875, Learning Rate: 0.00015625\n",
      "Epoch [9865/20000], Loss: 839.0490112304688, Entropy 439.68646240234375, Learning Rate: 0.00015625\n",
      "Epoch [9866/20000], Loss: 858.5318603515625, Entropy 446.8179931640625, Learning Rate: 0.00015625\n",
      "Epoch [9867/20000], Loss: 875.9178466796875, Entropy 450.2328796386719, Learning Rate: 0.00015625\n",
      "Epoch [9868/20000], Loss: 807.24853515625, Entropy 449.2146911621094, Learning Rate: 0.00015625\n",
      "Epoch [9869/20000], Loss: 854.777099609375, Entropy 442.8041687011719, Learning Rate: 0.00015625\n",
      "Epoch [9870/20000], Loss: 802.5362548828125, Entropy 450.67431640625, Learning Rate: 0.00015625\n",
      "Epoch [9871/20000], Loss: 876.9109497070312, Entropy 447.75921630859375, Learning Rate: 0.00015625\n",
      "Epoch [9872/20000], Loss: 876.29296875, Entropy 440.197021484375, Learning Rate: 0.00015625\n",
      "Epoch [9873/20000], Loss: 908.0443115234375, Entropy 444.1291198730469, Learning Rate: 0.00015625\n",
      "Epoch [9874/20000], Loss: 841.4246826171875, Entropy 427.9924011230469, Learning Rate: 0.00015625\n",
      "Epoch [9875/20000], Loss: 855.8005981445312, Entropy 441.60589599609375, Learning Rate: 0.00015625\n",
      "Epoch [9876/20000], Loss: 883.6326904296875, Entropy 427.0448303222656, Learning Rate: 0.00015625\n",
      "Epoch [9877/20000], Loss: 855.5916748046875, Entropy 421.6439208984375, Learning Rate: 0.00015625\n",
      "Epoch [9878/20000], Loss: 874.3955078125, Entropy 447.2541198730469, Learning Rate: 0.00015625\n",
      "Epoch [9879/20000], Loss: 834.396484375, Entropy 442.1579284667969, Learning Rate: 0.00015625\n",
      "Epoch [9880/20000], Loss: 881.4525756835938, Entropy 435.94085693359375, Learning Rate: 0.00015625\n",
      "Epoch [9881/20000], Loss: 883.370361328125, Entropy 430.82177734375, Learning Rate: 0.00015625\n",
      "Epoch [9882/20000], Loss: 836.0675048828125, Entropy 439.8658752441406, Learning Rate: 0.00015625\n",
      "Epoch [9883/20000], Loss: 856.1605224609375, Entropy 431.0235900878906, Learning Rate: 0.00015625\n",
      "Epoch [9884/20000], Loss: 839.8306884765625, Entropy 441.6272277832031, Learning Rate: 0.00015625\n",
      "Epoch [9885/20000], Loss: 943.8844604492188, Entropy 436.94476318359375, Learning Rate: 0.00015625\n",
      "Epoch [9886/20000], Loss: 873.4913330078125, Entropy 439.2840270996094, Learning Rate: 0.00015625\n",
      "Epoch [9887/20000], Loss: 881.052490234375, Entropy 450.2578125, Learning Rate: 0.00015625\n",
      "Epoch [9888/20000], Loss: 881.46142578125, Entropy 447.5314636230469, Learning Rate: 0.00015625\n",
      "Epoch [9889/20000], Loss: 849.38818359375, Entropy 455.80029296875, Learning Rate: 0.00015625\n",
      "Epoch [9890/20000], Loss: 884.596923828125, Entropy 415.4153747558594, Learning Rate: 0.00015625\n",
      "Epoch [9891/20000], Loss: 838.2781982421875, Entropy 421.36669921875, Learning Rate: 0.00015625\n",
      "Epoch [9892/20000], Loss: 876.7857666015625, Entropy 428.1475830078125, Learning Rate: 0.00015625\n",
      "Epoch [9893/20000], Loss: 897.54248046875, Entropy 433.2058410644531, Learning Rate: 0.00015625\n",
      "Epoch [9894/20000], Loss: 849.4591064453125, Entropy 440.8533935546875, Learning Rate: 0.00015625\n",
      "Epoch [9895/20000], Loss: 851.14697265625, Entropy 450.9559326171875, Learning Rate: 0.00015625\n",
      "Epoch [9896/20000], Loss: 825.7153930664062, Entropy 446.25054931640625, Learning Rate: 0.00015625\n",
      "Epoch [9897/20000], Loss: 820.269287109375, Entropy 447.10498046875, Learning Rate: 0.00015625\n",
      "Epoch [9898/20000], Loss: 881.4183349609375, Entropy 430.7972412109375, Learning Rate: 0.00015625\n",
      "Epoch [9899/20000], Loss: 903.9174194335938, Entropy 438.66656494140625, Learning Rate: 0.00015625\n",
      "Epoch [9900/20000], Loss: 902.3451538085938, Entropy 455.06292724609375, Learning Rate: 0.00015625\n",
      "Epoch [9901/20000], Loss: 950.6253662109375, Entropy 442.7021484375, Learning Rate: 0.00015625\n",
      "Epoch [9902/20000], Loss: 909.9469604492188, Entropy 426.90521240234375, Learning Rate: 0.00015625\n",
      "Epoch [9903/20000], Loss: 869.7801513671875, Entropy 448.3160095214844, Learning Rate: 0.00015625\n",
      "Epoch [9904/20000], Loss: 811.52587890625, Entropy 436.17333984375, Learning Rate: 0.00015625\n",
      "Epoch [9905/20000], Loss: 843.8662719726562, Entropy 433.07525634765625, Learning Rate: 0.00015625\n",
      "Epoch [9906/20000], Loss: 866.9282836914062, Entropy 448.34515380859375, Learning Rate: 0.00015625\n",
      "Epoch [9907/20000], Loss: 873.3147583007812, Entropy 457.00335693359375, Learning Rate: 0.00015625\n",
      "Epoch [9908/20000], Loss: 826.5853271484375, Entropy 437.79248046875, Learning Rate: 0.00015625\n",
      "Epoch [9909/20000], Loss: 856.7039184570312, Entropy 440.31695556640625, Learning Rate: 0.00015625\n",
      "Epoch [9910/20000], Loss: 860.6357421875, Entropy 435.7193603515625, Learning Rate: 0.00015625\n",
      "Epoch [9911/20000], Loss: 895.59765625, Entropy 451.863037109375, Learning Rate: 0.00015625\n",
      "Epoch [9912/20000], Loss: 846.603759765625, Entropy 439.2811279296875, Learning Rate: 0.00015625\n",
      "Epoch [9913/20000], Loss: 883.59716796875, Entropy 442.5584411621094, Learning Rate: 0.00015625\n",
      "Epoch [9914/20000], Loss: 840.195068359375, Entropy 424.6474914550781, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9915/20000], Loss: 869.3253173828125, Entropy 450.3446350097656, Learning Rate: 0.00015625\n",
      "Epoch [9916/20000], Loss: 909.5106201171875, Entropy 424.7331848144531, Learning Rate: 0.00015625\n",
      "Epoch [9917/20000], Loss: 864.19482421875, Entropy 444.0914611816406, Learning Rate: 0.00015625\n",
      "Epoch [9918/20000], Loss: 847.207275390625, Entropy 457.2986145019531, Learning Rate: 0.00015625\n",
      "Epoch [9919/20000], Loss: 919.5723876953125, Entropy 449.8690490722656, Learning Rate: 0.00015625\n",
      "Epoch [9920/20000], Loss: 835.78125, Entropy 440.6676330566406, Learning Rate: 0.00015625\n",
      "Epoch [9921/20000], Loss: 863.80322265625, Entropy 441.1729736328125, Learning Rate: 0.00015625\n",
      "Epoch [9922/20000], Loss: 834.6766357421875, Entropy 441.9921875, Learning Rate: 0.00015625\n",
      "Epoch [9923/20000], Loss: 853.3436279296875, Entropy 445.42138671875, Learning Rate: 0.00015625\n",
      "Epoch [9924/20000], Loss: 838.10009765625, Entropy 431.3658752441406, Learning Rate: 0.00015625\n",
      "Epoch [9925/20000], Loss: 912.498046875, Entropy 434.478515625, Learning Rate: 0.00015625\n",
      "Epoch [9926/20000], Loss: 897.4075317382812, Entropy 451.14019775390625, Learning Rate: 0.00015625\n",
      "Epoch [9927/20000], Loss: 875.5577392578125, Entropy 435.9306640625, Learning Rate: 0.00015625\n",
      "Epoch [9928/20000], Loss: 867.064453125, Entropy 433.0015563964844, Learning Rate: 0.00015625\n",
      "Epoch [9929/20000], Loss: 852.951171875, Entropy 437.6727294921875, Learning Rate: 0.00015625\n",
      "Epoch [9930/20000], Loss: 861.5772705078125, Entropy 441.7875671386719, Learning Rate: 0.00015625\n",
      "Epoch [9931/20000], Loss: 838.9947509765625, Entropy 449.6380615234375, Learning Rate: 0.00015625\n",
      "Epoch [9932/20000], Loss: 856.4807739257812, Entropy 443.25933837890625, Learning Rate: 0.00015625\n",
      "Epoch [9933/20000], Loss: 819.501708984375, Entropy 439.0685729980469, Learning Rate: 0.00015625\n",
      "Epoch [9934/20000], Loss: 860.89013671875, Entropy 442.9293518066406, Learning Rate: 0.00015625\n",
      "Epoch [9935/20000], Loss: 817.43359375, Entropy 436.5789489746094, Learning Rate: 0.00015625\n",
      "Epoch [9936/20000], Loss: 917.4432373046875, Entropy 424.0205383300781, Learning Rate: 0.00015625\n",
      "Epoch [9937/20000], Loss: 899.2060546875, Entropy 449.8190612792969, Learning Rate: 0.00015625\n",
      "Epoch [9938/20000], Loss: 862.91064453125, Entropy 435.5249938964844, Learning Rate: 0.00015625\n",
      "Epoch [9939/20000], Loss: 877.8050537109375, Entropy 430.5535888671875, Learning Rate: 0.00015625\n",
      "Epoch [9940/20000], Loss: 863.8701782226562, Entropy 453.74310302734375, Learning Rate: 0.00015625\n",
      "Epoch [9941/20000], Loss: 830.8291015625, Entropy 450.5162353515625, Learning Rate: 0.00015625\n",
      "Epoch [9942/20000], Loss: 839.080810546875, Entropy 445.8082580566406, Learning Rate: 0.00015625\n",
      "Epoch [9943/20000], Loss: 860.7188720703125, Entropy 441.9942626953125, Learning Rate: 0.00015625\n",
      "Epoch [9944/20000], Loss: 853.6241455078125, Entropy 419.5210876464844, Learning Rate: 0.00015625\n",
      "Epoch [9945/20000], Loss: 877.4098510742188, Entropy 438.54010009765625, Learning Rate: 0.00015625\n",
      "Epoch [9946/20000], Loss: 833.8560180664062, Entropy 432.66424560546875, Learning Rate: 0.00015625\n",
      "Epoch [9947/20000], Loss: 856.1834106445312, Entropy 451.24517822265625, Learning Rate: 0.00015625\n",
      "Epoch [9948/20000], Loss: 830.6807861328125, Entropy 454.2139892578125, Learning Rate: 0.00015625\n",
      "Epoch [9949/20000], Loss: 840.8953857421875, Entropy 442.2106018066406, Learning Rate: 0.00015625\n",
      "Epoch [9950/20000], Loss: 847.60009765625, Entropy 433.0801086425781, Learning Rate: 0.00015625\n",
      "Epoch [9951/20000], Loss: 824.767578125, Entropy 452.2526550292969, Learning Rate: 0.00015625\n",
      "Epoch [9952/20000], Loss: 868.615234375, Entropy 425.0377197265625, Learning Rate: 0.00015625\n",
      "Epoch [9953/20000], Loss: 844.6724853515625, Entropy 445.7964782714844, Learning Rate: 0.00015625\n",
      "Epoch [9954/20000], Loss: 901.6962280273438, Entropy 434.67498779296875, Learning Rate: 0.00015625\n",
      "Epoch [9955/20000], Loss: 859.0589599609375, Entropy 440.9331970214844, Learning Rate: 0.00015625\n",
      "Epoch [9956/20000], Loss: 816.0771484375, Entropy 438.6061706542969, Learning Rate: 0.00015625\n",
      "Epoch [9957/20000], Loss: 917.65966796875, Entropy 435.7550048828125, Learning Rate: 0.00015625\n",
      "Epoch [9958/20000], Loss: 863.3839111328125, Entropy 450.6369934082031, Learning Rate: 0.00015625\n",
      "Epoch [9959/20000], Loss: 834.963134765625, Entropy 445.2380065917969, Learning Rate: 0.00015625\n",
      "Epoch [9960/20000], Loss: 853.7996826171875, Entropy 438.4319152832031, Learning Rate: 0.00015625\n",
      "Epoch [9961/20000], Loss: 905.856689453125, Entropy 429.34326171875, Learning Rate: 0.00015625\n",
      "Epoch [9962/20000], Loss: 846.7536010742188, Entropy 443.52178955078125, Learning Rate: 0.00015625\n",
      "Epoch [9963/20000], Loss: 875.52490234375, Entropy 439.6620178222656, Learning Rate: 0.00015625\n",
      "Epoch [9964/20000], Loss: 912.9415893554688, Entropy 425.23663330078125, Learning Rate: 0.00015625\n",
      "Epoch [9965/20000], Loss: 875.8739013671875, Entropy 444.4699401855469, Learning Rate: 0.00015625\n",
      "Epoch [9966/20000], Loss: 899.5188598632812, Entropy 451.64678955078125, Learning Rate: 0.00015625\n",
      "Epoch [9967/20000], Loss: 832.7015380859375, Entropy 443.4948425292969, Learning Rate: 0.00015625\n",
      "Epoch [9968/20000], Loss: 840.5609741210938, Entropy 452.91119384765625, Learning Rate: 0.00015625\n",
      "Epoch [9969/20000], Loss: 848.5385131835938, Entropy 442.71795654296875, Learning Rate: 0.00015625\n",
      "Epoch [9970/20000], Loss: 816.0184326171875, Entropy 444.70458984375, Learning Rate: 0.00015625\n",
      "Epoch [9971/20000], Loss: 883.841064453125, Entropy 448.5306396484375, Learning Rate: 0.00015625\n",
      "Epoch [9972/20000], Loss: 850.321044921875, Entropy 440.546630859375, Learning Rate: 0.00015625\n",
      "Epoch [9973/20000], Loss: 834.10693359375, Entropy 446.9830627441406, Learning Rate: 0.00015625\n",
      "Epoch [9974/20000], Loss: 857.051513671875, Entropy 421.1278076171875, Learning Rate: 0.00015625\n",
      "Epoch [9975/20000], Loss: 856.7111206054688, Entropy 425.59857177734375, Learning Rate: 0.00015625\n",
      "Epoch [9976/20000], Loss: 848.1792602539062, Entropy 433.46514892578125, Learning Rate: 0.00015625\n",
      "Epoch [9977/20000], Loss: 872.9176025390625, Entropy 437.3346862792969, Learning Rate: 0.00015625\n",
      "Epoch [9978/20000], Loss: 828.1630859375, Entropy 444.4548645019531, Learning Rate: 0.00015625\n",
      "Epoch [9979/20000], Loss: 852.5853881835938, Entropy 418.05487060546875, Learning Rate: 0.00015625\n",
      "Epoch [9980/20000], Loss: 848.952392578125, Entropy 435.3854675292969, Learning Rate: 0.00015625\n",
      "Epoch [9981/20000], Loss: 889.93212890625, Entropy 448.8492431640625, Learning Rate: 0.00015625\n",
      "Epoch [9982/20000], Loss: 850.3375854492188, Entropy 438.05999755859375, Learning Rate: 0.00015625\n",
      "Epoch [9983/20000], Loss: 897.6171875, Entropy 440.7038269042969, Learning Rate: 0.00015625\n",
      "Epoch [9984/20000], Loss: 827.2406005859375, Entropy 434.8281555175781, Learning Rate: 0.00015625\n",
      "Epoch [9985/20000], Loss: 812.98486328125, Entropy 452.1426086425781, Learning Rate: 0.00015625\n",
      "Epoch [9986/20000], Loss: 841.57080078125, Entropy 449.5035705566406, Learning Rate: 0.00015625\n",
      "Epoch [9987/20000], Loss: 897.823974609375, Entropy 454.8664855957031, Learning Rate: 0.00015625\n",
      "Epoch [9988/20000], Loss: 844.5888061523438, Entropy 425.31182861328125, Learning Rate: 0.00015625\n",
      "Epoch [9989/20000], Loss: 915.4915771484375, Entropy 436.4027099609375, Learning Rate: 0.00015625\n",
      "Epoch [9990/20000], Loss: 855.185302734375, Entropy 451.7671813964844, Learning Rate: 0.00015625\n",
      "Epoch [9991/20000], Loss: 974.7530517578125, Entropy 430.24072265625, Learning Rate: 0.00015625\n",
      "Epoch [9992/20000], Loss: 885.29150390625, Entropy 442.3699645996094, Learning Rate: 0.00015625\n",
      "Epoch [9993/20000], Loss: 818.108154296875, Entropy 433.4797058105469, Learning Rate: 0.00015625\n",
      "Epoch [9994/20000], Loss: 863.7559814453125, Entropy 445.133544921875, Learning Rate: 0.00015625\n",
      "Epoch [9995/20000], Loss: 860.803466796875, Entropy 442.1335754394531, Learning Rate: 0.00015625\n",
      "Epoch [9996/20000], Loss: 850.37841796875, Entropy 431.3241271972656, Learning Rate: 0.00015625\n",
      "Epoch [9997/20000], Loss: 884.7556762695312, Entropy 442.19366455078125, Learning Rate: 0.00015625\n",
      "Epoch [9998/20000], Loss: 882.6114501953125, Entropy 443.8272705078125, Learning Rate: 0.00015625\n",
      "Epoch [9999/20000], Loss: 836.2588500976562, Entropy 453.76458740234375, Learning Rate: 0.00015625\n",
      "Epoch [10000/20000], Loss: 854.9068603515625, Entropy 445.0958251953125, Learning Rate: 0.00015625\n",
      "Epoch [10001/20000], Loss: 877.0361328125, Entropy 434.8953857421875, Learning Rate: 0.00015625\n",
      "Epoch [10002/20000], Loss: 866.2608642578125, Entropy 438.8801574707031, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10003/20000], Loss: 845.2672119140625, Entropy 438.2017517089844, Learning Rate: 0.00015625\n",
      "Epoch [10004/20000], Loss: 953.8182373046875, Entropy 434.0920104980469, Learning Rate: 0.00015625\n",
      "Epoch [10005/20000], Loss: 853.036865234375, Entropy 458.4420166015625, Learning Rate: 0.00015625\n",
      "Epoch [10006/20000], Loss: 850.4649047851562, Entropy 438.64605712890625, Learning Rate: 0.00015625\n",
      "Epoch [10007/20000], Loss: 864.2398681640625, Entropy 448.6024169921875, Learning Rate: 0.00015625\n",
      "Epoch [10008/20000], Loss: 875.6990356445312, Entropy 425.19622802734375, Learning Rate: 0.00015625\n",
      "Epoch [10009/20000], Loss: 846.9371337890625, Entropy 450.3262023925781, Learning Rate: 0.00015625\n",
      "Epoch [10010/20000], Loss: 880.1282348632812, Entropy 440.77886962890625, Learning Rate: 0.00015625\n",
      "Epoch [10011/20000], Loss: 866.2725219726562, Entropy 439.62982177734375, Learning Rate: 0.00015625\n",
      "Epoch [10012/20000], Loss: 846.3998413085938, Entropy 433.96319580078125, Learning Rate: 0.00015625\n",
      "Epoch [10013/20000], Loss: 896.7802734375, Entropy 433.4697570800781, Learning Rate: 0.00015625\n",
      "Epoch [10014/20000], Loss: 864.4888916015625, Entropy 450.5757751464844, Learning Rate: 0.00015625\n",
      "Epoch [10015/20000], Loss: 862.40283203125, Entropy 441.6089782714844, Learning Rate: 0.00015625\n",
      "Epoch [10016/20000], Loss: 842.43798828125, Entropy 452.455078125, Learning Rate: 0.00015625\n",
      "Epoch [10017/20000], Loss: 870.845703125, Entropy 436.8693542480469, Learning Rate: 0.00015625\n",
      "Epoch [10018/20000], Loss: 853.6851196289062, Entropy 444.54547119140625, Learning Rate: 0.00015625\n",
      "Epoch [10019/20000], Loss: 893.7036743164062, Entropy 450.23297119140625, Learning Rate: 0.00015625\n",
      "Epoch [10020/20000], Loss: 890.6482543945312, Entropy 425.60198974609375, Learning Rate: 0.00015625\n",
      "Epoch [10021/20000], Loss: 906.4217529296875, Entropy 442.7347412109375, Learning Rate: 0.00015625\n",
      "Epoch [10022/20000], Loss: 876.218994140625, Entropy 444.03515625, Learning Rate: 0.00015625\n",
      "Epoch [10023/20000], Loss: 843.4127807617188, Entropy 423.03753662109375, Learning Rate: 0.00015625\n",
      "Epoch [10024/20000], Loss: 867.9713745117188, Entropy 435.22454833984375, Learning Rate: 0.00015625\n",
      "Epoch [10025/20000], Loss: 867.3238525390625, Entropy 442.20849609375, Learning Rate: 0.00015625\n",
      "Epoch [10026/20000], Loss: 885.3819580078125, Entropy 453.3876953125, Learning Rate: 0.00015625\n",
      "Epoch [10027/20000], Loss: 880.2608032226562, Entropy 427.87030029296875, Learning Rate: 0.00015625\n",
      "Epoch [10028/20000], Loss: 863.7940673828125, Entropy 452.1162414550781, Learning Rate: 0.00015625\n",
      "Epoch [10029/20000], Loss: 847.17724609375, Entropy 451.0185852050781, Learning Rate: 0.00015625\n",
      "Epoch [10030/20000], Loss: 902.390380859375, Entropy 451.96875, Learning Rate: 0.00015625\n",
      "Epoch [10031/20000], Loss: 873.5020751953125, Entropy 450.365234375, Learning Rate: 0.00015625\n",
      "Epoch [10032/20000], Loss: 811.96826171875, Entropy 461.3670349121094, Learning Rate: 0.00015625\n",
      "Epoch [10033/20000], Loss: 907.71435546875, Entropy 438.46484375, Learning Rate: 0.00015625\n",
      "Epoch [10034/20000], Loss: 897.71435546875, Entropy 439.5526428222656, Learning Rate: 0.00015625\n",
      "Epoch [10035/20000], Loss: 901.00390625, Entropy 439.8304138183594, Learning Rate: 0.00015625\n",
      "Epoch [10036/20000], Loss: 890.4711303710938, Entropy 441.75201416015625, Learning Rate: 0.00015625\n",
      "Epoch [10037/20000], Loss: 830.7833251953125, Entropy 445.7268371582031, Learning Rate: 0.00015625\n",
      "Epoch [10038/20000], Loss: 876.8300170898438, Entropy 448.75091552734375, Learning Rate: 0.00015625\n",
      "Epoch [10039/20000], Loss: 900.5708618164062, Entropy 444.74945068359375, Learning Rate: 0.00015625\n",
      "Epoch [10040/20000], Loss: 871.3563232421875, Entropy 435.0068664550781, Learning Rate: 0.00015625\n",
      "Epoch [10041/20000], Loss: 845.8937377929688, Entropy 423.35894775390625, Learning Rate: 0.00015625\n",
      "Epoch [10042/20000], Loss: 834.8941650390625, Entropy 441.9128723144531, Learning Rate: 0.00015625\n",
      "Epoch [10043/20000], Loss: 851.8421630859375, Entropy 438.1614074707031, Learning Rate: 0.00015625\n",
      "Epoch [10044/20000], Loss: 861.7141723632812, Entropy 420.69647216796875, Learning Rate: 0.00015625\n",
      "Epoch [10045/20000], Loss: 924.6390991210938, Entropy 436.37139892578125, Learning Rate: 0.00015625\n",
      "Epoch [10046/20000], Loss: 869.6395263671875, Entropy 428.1742858886719, Learning Rate: 0.00015625\n",
      "Epoch [10047/20000], Loss: 862.2060546875, Entropy 432.6358642578125, Learning Rate: 0.00015625\n",
      "Epoch [10048/20000], Loss: 854.2774658203125, Entropy 449.8651428222656, Learning Rate: 0.00015625\n",
      "Epoch [10049/20000], Loss: 866.6783447265625, Entropy 443.021484375, Learning Rate: 0.00015625\n",
      "Epoch [10050/20000], Loss: 869.911376953125, Entropy 447.0736083984375, Learning Rate: 0.00015625\n",
      "Epoch [10051/20000], Loss: 871.5814819335938, Entropy 455.01007080078125, Learning Rate: 0.00015625\n",
      "Epoch [10052/20000], Loss: 864.349609375, Entropy 454.4244384765625, Learning Rate: 0.00015625\n",
      "Epoch [10053/20000], Loss: 848.2381591796875, Entropy 436.0959777832031, Learning Rate: 0.00015625\n",
      "Epoch [10054/20000], Loss: 831.676025390625, Entropy 435.9361572265625, Learning Rate: 0.00015625\n",
      "Epoch [10055/20000], Loss: 879.3997802734375, Entropy 438.0624084472656, Learning Rate: 0.00015625\n",
      "Epoch [10056/20000], Loss: 864.8140258789062, Entropy 460.89410400390625, Learning Rate: 0.00015625\n",
      "Epoch [10057/20000], Loss: 803.1293334960938, Entropy 443.41693115234375, Learning Rate: 0.00015625\n",
      "Epoch [10058/20000], Loss: 921.9248657226562, Entropy 445.06536865234375, Learning Rate: 0.00015625\n",
      "Epoch [10059/20000], Loss: 886.335205078125, Entropy 441.28271484375, Learning Rate: 0.00015625\n",
      "Epoch [10060/20000], Loss: 877.3829345703125, Entropy 439.6435852050781, Learning Rate: 0.00015625\n",
      "Epoch [10061/20000], Loss: 863.57470703125, Entropy 433.7100830078125, Learning Rate: 0.00015625\n",
      "Epoch [10062/20000], Loss: 837.4425048828125, Entropy 424.427490234375, Learning Rate: 0.00015625\n",
      "Epoch [10063/20000], Loss: 868.882080078125, Entropy 430.69091796875, Learning Rate: 0.00015625\n",
      "Epoch [10064/20000], Loss: 956.6300048828125, Entropy 428.8573303222656, Learning Rate: 0.00015625\n",
      "Epoch [10065/20000], Loss: 934.1973876953125, Entropy 426.841796875, Learning Rate: 0.00015625\n",
      "Epoch [10066/20000], Loss: 836.1082763671875, Entropy 432.2803039550781, Learning Rate: 0.00015625\n",
      "Epoch [10067/20000], Loss: 844.2886352539062, Entropy 439.99505615234375, Learning Rate: 0.00015625\n",
      "Epoch [10068/20000], Loss: 857.1229248046875, Entropy 455.1622619628906, Learning Rate: 0.00015625\n",
      "Epoch [10069/20000], Loss: 852.1375122070312, Entropy 442.76593017578125, Learning Rate: 0.00015625\n",
      "Epoch [10070/20000], Loss: 842.0806274414062, Entropy 459.60955810546875, Learning Rate: 0.00015625\n",
      "Epoch [10071/20000], Loss: 862.4331665039062, Entropy 441.00201416015625, Learning Rate: 0.00015625\n",
      "Epoch [10072/20000], Loss: 857.2520751953125, Entropy 430.2730712890625, Learning Rate: 0.00015625\n",
      "Epoch [10073/20000], Loss: 795.2139892578125, Entropy 455.911865234375, Learning Rate: 0.00015625\n",
      "Epoch [10074/20000], Loss: 836.5865478515625, Entropy 435.3233947753906, Learning Rate: 0.00015625\n",
      "Epoch [10075/20000], Loss: 882.4471435546875, Entropy 421.1072692871094, Learning Rate: 0.00015625\n",
      "Epoch [10076/20000], Loss: 858.3570556640625, Entropy 436.4884033203125, Learning Rate: 0.00015625\n",
      "Epoch [10077/20000], Loss: 862.1231689453125, Entropy 427.9333801269531, Learning Rate: 0.00015625\n",
      "Epoch [10078/20000], Loss: 889.1297607421875, Entropy 430.9407043457031, Learning Rate: 0.00015625\n",
      "Epoch [10079/20000], Loss: 833.5789794921875, Entropy 439.1003723144531, Learning Rate: 0.00015625\n",
      "Epoch [10080/20000], Loss: 858.8104248046875, Entropy 442.6795654296875, Learning Rate: 0.00015625\n",
      "Epoch [10081/20000], Loss: 853.3040161132812, Entropy 433.21136474609375, Learning Rate: 0.00015625\n",
      "Epoch [10082/20000], Loss: 938.930908203125, Entropy 421.2691345214844, Learning Rate: 0.00015625\n",
      "Epoch [10083/20000], Loss: 880.8607177734375, Entropy 437.4517517089844, Learning Rate: 0.00015625\n",
      "Epoch [10084/20000], Loss: 854.6307373046875, Entropy 443.1239013671875, Learning Rate: 0.00015625\n",
      "Epoch [10085/20000], Loss: 845.4379272460938, Entropy 438.59674072265625, Learning Rate: 0.00015625\n",
      "Epoch [10086/20000], Loss: 844.0137939453125, Entropy 456.1952209472656, Learning Rate: 0.00015625\n",
      "Epoch [10087/20000], Loss: 850.9658203125, Entropy 439.6117858886719, Learning Rate: 0.00015625\n",
      "Epoch [10088/20000], Loss: 828.1417846679688, Entropy 433.75506591796875, Learning Rate: 0.00015625\n",
      "Epoch [10089/20000], Loss: 867.2149658203125, Entropy 432.4325256347656, Learning Rate: 0.00015625\n",
      "Epoch [10090/20000], Loss: 829.972900390625, Entropy 461.3433532714844, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10091/20000], Loss: 856.8820190429688, Entropy 451.34393310546875, Learning Rate: 0.00015625\n",
      "Epoch [10092/20000], Loss: 912.7491455078125, Entropy 428.243896484375, Learning Rate: 0.00015625\n",
      "Epoch [10093/20000], Loss: 844.1329345703125, Entropy 443.86328125, Learning Rate: 0.00015625\n",
      "Epoch [10094/20000], Loss: 861.8577880859375, Entropy 441.7085876464844, Learning Rate: 0.00015625\n",
      "Epoch [10095/20000], Loss: 818.496337890625, Entropy 452.8409423828125, Learning Rate: 0.00015625\n",
      "Epoch [10096/20000], Loss: 838.886474609375, Entropy 446.36181640625, Learning Rate: 0.00015625\n",
      "Epoch [10097/20000], Loss: 892.5400390625, Entropy 447.1608581542969, Learning Rate: 0.00015625\n",
      "Epoch [10098/20000], Loss: 831.7056884765625, Entropy 437.5046081542969, Learning Rate: 0.00015625\n",
      "Epoch [10099/20000], Loss: 830.7913818359375, Entropy 451.56884765625, Learning Rate: 0.00015625\n",
      "Epoch [10100/20000], Loss: 869.822021484375, Entropy 435.2769470214844, Learning Rate: 0.00015625\n",
      "Epoch [10101/20000], Loss: 835.7476196289062, Entropy 445.09881591796875, Learning Rate: 0.00015625\n",
      "Epoch [10102/20000], Loss: 865.72216796875, Entropy 440.1634521484375, Learning Rate: 0.00015625\n",
      "Epoch [10103/20000], Loss: 833.942138671875, Entropy 442.6769714355469, Learning Rate: 0.00015625\n",
      "Epoch [10104/20000], Loss: 900.879150390625, Entropy 444.3719787597656, Learning Rate: 0.00015625\n",
      "Epoch [10105/20000], Loss: 858.68505859375, Entropy 447.6248474121094, Learning Rate: 0.00015625\n",
      "Epoch [10106/20000], Loss: 842.108642578125, Entropy 451.3167724609375, Learning Rate: 0.00015625\n",
      "Epoch [10107/20000], Loss: 824.6224365234375, Entropy 454.09912109375, Learning Rate: 0.00015625\n",
      "Epoch [10108/20000], Loss: 888.9088134765625, Entropy 434.6514892578125, Learning Rate: 0.00015625\n",
      "Epoch [10109/20000], Loss: 824.858642578125, Entropy 415.4850769042969, Learning Rate: 0.00015625\n",
      "Epoch [10110/20000], Loss: 806.90771484375, Entropy 453.5480041503906, Learning Rate: 0.00015625\n",
      "Epoch [10111/20000], Loss: 837.128662109375, Entropy 436.8711242675781, Learning Rate: 0.00015625\n",
      "Epoch [10112/20000], Loss: 902.204833984375, Entropy 431.9610595703125, Learning Rate: 0.00015625\n",
      "Epoch [10113/20000], Loss: 844.6385498046875, Entropy 446.1705322265625, Learning Rate: 0.00015625\n",
      "Epoch [10114/20000], Loss: 905.8560791015625, Entropy 459.6463623046875, Learning Rate: 0.00015625\n",
      "Epoch [10115/20000], Loss: 838.229248046875, Entropy 462.3667907714844, Learning Rate: 0.00015625\n",
      "Epoch [10116/20000], Loss: 842.6175537109375, Entropy 435.9869384765625, Learning Rate: 0.00015625\n",
      "Epoch [10117/20000], Loss: 907.082275390625, Entropy 437.14111328125, Learning Rate: 0.00015625\n",
      "Epoch [10118/20000], Loss: 856.1881103515625, Entropy 435.4438171386719, Learning Rate: 0.00015625\n",
      "Epoch [10119/20000], Loss: 873.6823120117188, Entropy 436.36260986328125, Learning Rate: 0.00015625\n",
      "Epoch [10120/20000], Loss: 895.760498046875, Entropy 445.4068908691406, Learning Rate: 0.00015625\n",
      "Epoch [10121/20000], Loss: 865.384033203125, Entropy 431.6189880371094, Learning Rate: 0.00015625\n",
      "Epoch [10122/20000], Loss: 863.8280029296875, Entropy 440.2822570800781, Learning Rate: 0.00015625\n",
      "Epoch [10123/20000], Loss: 851.5294189453125, Entropy 423.1949768066406, Learning Rate: 0.00015625\n",
      "Epoch [10124/20000], Loss: 841.0650634765625, Entropy 462.3323974609375, Learning Rate: 0.00015625\n",
      "Epoch [10125/20000], Loss: 849.730224609375, Entropy 444.7544860839844, Learning Rate: 0.00015625\n",
      "Epoch [10126/20000], Loss: 873.5657348632812, Entropy 438.05865478515625, Learning Rate: 0.00015625\n",
      "Epoch [10127/20000], Loss: 890.6521606445312, Entropy 441.03729248046875, Learning Rate: 0.00015625\n",
      "Epoch [10128/20000], Loss: 827.0926513671875, Entropy 443.9598388671875, Learning Rate: 0.00015625\n",
      "Epoch [10129/20000], Loss: 877.4251708984375, Entropy 455.3767395019531, Learning Rate: 0.00015625\n",
      "Epoch [10130/20000], Loss: 888.01806640625, Entropy 433.8503112792969, Learning Rate: 0.00015625\n",
      "Epoch [10131/20000], Loss: 792.2974853515625, Entropy 442.0874938964844, Learning Rate: 0.00015625\n",
      "Epoch [10132/20000], Loss: 815.9791870117188, Entropy 439.06561279296875, Learning Rate: 0.00015625\n",
      "Epoch [10133/20000], Loss: 918.315673828125, Entropy 441.1162109375, Learning Rate: 0.00015625\n",
      "Epoch [10134/20000], Loss: 833.5916748046875, Entropy 435.8639831542969, Learning Rate: 0.00015625\n",
      "Epoch [10135/20000], Loss: 895.7296752929688, Entropy 434.14300537109375, Learning Rate: 0.00015625\n",
      "Epoch [10136/20000], Loss: 887.22265625, Entropy 429.815185546875, Learning Rate: 0.00015625\n",
      "Epoch [10137/20000], Loss: 851.4761962890625, Entropy 449.2801208496094, Learning Rate: 0.00015625\n",
      "Epoch [10138/20000], Loss: 838.6550903320312, Entropy 434.98101806640625, Learning Rate: 0.00015625\n",
      "Epoch [10139/20000], Loss: 933.20263671875, Entropy 425.1295166015625, Learning Rate: 0.00015625\n",
      "Epoch [10140/20000], Loss: 885.8502197265625, Entropy 426.0925598144531, Learning Rate: 0.00015625\n",
      "Epoch [10141/20000], Loss: 855.9161376953125, Entropy 438.1390075683594, Learning Rate: 0.00015625\n",
      "Epoch [10142/20000], Loss: 868.6112060546875, Entropy 430.5378112792969, Learning Rate: 0.00015625\n",
      "Epoch [10143/20000], Loss: 790.173828125, Entropy 447.5813903808594, Learning Rate: 0.00015625\n",
      "Epoch [10144/20000], Loss: 859.109130859375, Entropy 456.6443176269531, Learning Rate: 0.00015625\n",
      "Epoch [10145/20000], Loss: 872.9443969726562, Entropy 447.55120849609375, Learning Rate: 0.00015625\n",
      "Epoch [10146/20000], Loss: 877.50830078125, Entropy 416.5650634765625, Learning Rate: 0.00015625\n",
      "Epoch [10147/20000], Loss: 920.2413330078125, Entropy 441.0261535644531, Learning Rate: 0.00015625\n",
      "Epoch [10148/20000], Loss: 866.4674072265625, Entropy 439.3449401855469, Learning Rate: 0.00015625\n",
      "Epoch [10149/20000], Loss: 849.928955078125, Entropy 447.1252136230469, Learning Rate: 0.00015625\n",
      "Epoch [10150/20000], Loss: 824.268310546875, Entropy 455.8835144042969, Learning Rate: 0.00015625\n",
      "Epoch [10151/20000], Loss: 831.6683349609375, Entropy 448.8721008300781, Learning Rate: 0.00015625\n",
      "Epoch [10152/20000], Loss: 851.2618408203125, Entropy 453.1609802246094, Learning Rate: 0.00015625\n",
      "Epoch [10153/20000], Loss: 890.5745849609375, Entropy 436.8595275878906, Learning Rate: 0.00015625\n",
      "Epoch [10154/20000], Loss: 913.10009765625, Entropy 428.4656982421875, Learning Rate: 0.00015625\n",
      "Epoch [10155/20000], Loss: 884.4198608398438, Entropy 444.10382080078125, Learning Rate: 0.00015625\n",
      "Epoch [10156/20000], Loss: 870.38134765625, Entropy 437.6475830078125, Learning Rate: 0.00015625\n",
      "Epoch [10157/20000], Loss: 867.2254028320312, Entropy 438.42828369140625, Learning Rate: 0.00015625\n",
      "Epoch [10158/20000], Loss: 895.0152587890625, Entropy 436.1214599609375, Learning Rate: 0.00015625\n",
      "Epoch [10159/20000], Loss: 889.7802734375, Entropy 433.8846130371094, Learning Rate: 0.00015625\n",
      "Epoch [10160/20000], Loss: 893.3106689453125, Entropy 464.4686584472656, Learning Rate: 0.00015625\n",
      "Epoch [10161/20000], Loss: 874.765869140625, Entropy 427.4772644042969, Learning Rate: 0.00015625\n",
      "Epoch [10162/20000], Loss: 877.2860717773438, Entropy 438.03546142578125, Learning Rate: 0.00015625\n",
      "Epoch [10163/20000], Loss: 883.419677734375, Entropy 436.8631286621094, Learning Rate: 0.00015625\n",
      "Epoch [10164/20000], Loss: 833.2855224609375, Entropy 452.6402282714844, Learning Rate: 0.00015625\n",
      "Epoch [10165/20000], Loss: 865.8001708984375, Entropy 446.2313232421875, Learning Rate: 0.00015625\n",
      "Epoch [10166/20000], Loss: 846.4232788085938, Entropy 440.33148193359375, Learning Rate: 0.00015625\n",
      "Epoch [10167/20000], Loss: 856.2093505859375, Entropy 427.6950378417969, Learning Rate: 0.00015625\n",
      "Epoch [10168/20000], Loss: 871.5910034179688, Entropy 438.83355712890625, Learning Rate: 0.00015625\n",
      "Epoch [10169/20000], Loss: 843.127197265625, Entropy 444.0627136230469, Learning Rate: 0.00015625\n",
      "Epoch [10170/20000], Loss: 868.2171630859375, Entropy 429.3689880371094, Learning Rate: 0.00015625\n",
      "Epoch [10171/20000], Loss: 880.6995849609375, Entropy 443.6578674316406, Learning Rate: 0.00015625\n",
      "Epoch [10172/20000], Loss: 849.28466796875, Entropy 436.0246887207031, Learning Rate: 0.00015625\n",
      "Epoch [10173/20000], Loss: 885.8967895507812, Entropy 433.48333740234375, Learning Rate: 0.00015625\n",
      "Epoch [10174/20000], Loss: 831.64892578125, Entropy 462.1586608886719, Learning Rate: 0.00015625\n",
      "Epoch [10175/20000], Loss: 846.5284423828125, Entropy 441.9103698730469, Learning Rate: 0.00015625\n",
      "Epoch [10176/20000], Loss: 911.1970825195312, Entropy 439.44879150390625, Learning Rate: 0.00015625\n",
      "Epoch [10177/20000], Loss: 871.6824951171875, Entropy 445.1770935058594, Learning Rate: 0.00015625\n",
      "Epoch [10178/20000], Loss: 871.3621826171875, Entropy 436.375, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10179/20000], Loss: 852.39013671875, Entropy 436.4149475097656, Learning Rate: 0.00015625\n",
      "Epoch [10180/20000], Loss: 839.975830078125, Entropy 424.6007385253906, Learning Rate: 0.00015625\n",
      "Epoch [10181/20000], Loss: 872.8978271484375, Entropy 453.0130310058594, Learning Rate: 0.00015625\n",
      "Epoch [10182/20000], Loss: 855.0390014648438, Entropy 424.68475341796875, Learning Rate: 0.00015625\n",
      "Epoch [10183/20000], Loss: 816.582763671875, Entropy 451.2235107421875, Learning Rate: 0.00015625\n",
      "Epoch [10184/20000], Loss: 857.2779541015625, Entropy 443.8514709472656, Learning Rate: 0.00015625\n",
      "Epoch [10185/20000], Loss: 888.6290283203125, Entropy 440.2557067871094, Learning Rate: 0.00015625\n",
      "Epoch [10186/20000], Loss: 837.4990844726562, Entropy 441.05462646484375, Learning Rate: 0.00015625\n",
      "Epoch [10187/20000], Loss: 816.9183349609375, Entropy 435.1549072265625, Learning Rate: 0.00015625\n",
      "Epoch [10188/20000], Loss: 890.8177490234375, Entropy 440.0428161621094, Learning Rate: 0.00015625\n",
      "Epoch [10189/20000], Loss: 835.500732421875, Entropy 432.9901123046875, Learning Rate: 0.00015625\n",
      "Epoch [10190/20000], Loss: 848.6285400390625, Entropy 444.800537109375, Learning Rate: 0.00015625\n",
      "Epoch [10191/20000], Loss: 855.2518920898438, Entropy 446.87786865234375, Learning Rate: 0.00015625\n",
      "Epoch [10192/20000], Loss: 883.6195678710938, Entropy 443.62139892578125, Learning Rate: 0.00015625\n",
      "Epoch [10193/20000], Loss: 922.4105224609375, Entropy 449.7370910644531, Learning Rate: 0.00015625\n",
      "Epoch [10194/20000], Loss: 840.622314453125, Entropy 436.023193359375, Learning Rate: 0.00015625\n",
      "Epoch [10195/20000], Loss: 886.626708984375, Entropy 450.251953125, Learning Rate: 0.00015625\n",
      "Epoch [10196/20000], Loss: 802.49755859375, Entropy 453.164794921875, Learning Rate: 0.00015625\n",
      "Epoch [10197/20000], Loss: 865.6312255859375, Entropy 439.9867858886719, Learning Rate: 0.00015625\n",
      "Epoch [10198/20000], Loss: 825.86865234375, Entropy 452.5699157714844, Learning Rate: 0.00015625\n",
      "Epoch [10199/20000], Loss: 878.504150390625, Entropy 445.2711486816406, Learning Rate: 0.00015625\n",
      "Epoch [10200/20000], Loss: 885.9039306640625, Entropy 448.3768005371094, Learning Rate: 0.00015625\n",
      "Epoch [10201/20000], Loss: 871.9309692382812, Entropy 462.33245849609375, Learning Rate: 0.00015625\n",
      "Epoch [10202/20000], Loss: 904.8626708984375, Entropy 439.9598388671875, Learning Rate: 0.00015625\n",
      "Epoch [10203/20000], Loss: 900.8303833007812, Entropy 439.30804443359375, Learning Rate: 0.00015625\n",
      "Epoch [10204/20000], Loss: 804.1497802734375, Entropy 450.0539855957031, Learning Rate: 0.00015625\n",
      "Epoch [10205/20000], Loss: 890.811767578125, Entropy 429.550537109375, Learning Rate: 0.00015625\n",
      "Epoch [10206/20000], Loss: 843.9154052734375, Entropy 442.5223693847656, Learning Rate: 0.00015625\n",
      "Epoch [10207/20000], Loss: 867.5145874023438, Entropy 436.72662353515625, Learning Rate: 0.00015625\n",
      "Epoch [10208/20000], Loss: 812.9205322265625, Entropy 435.2530822753906, Learning Rate: 0.00015625\n",
      "Epoch [10209/20000], Loss: 822.6832275390625, Entropy 463.7381591796875, Learning Rate: 0.00015625\n",
      "Epoch [10210/20000], Loss: 875.306884765625, Entropy 443.1124267578125, Learning Rate: 0.00015625\n",
      "Epoch [10211/20000], Loss: 812.0582275390625, Entropy 447.2237548828125, Learning Rate: 0.00015625\n",
      "Epoch [10212/20000], Loss: 856.756103515625, Entropy 434.7884521484375, Learning Rate: 0.00015625\n",
      "Epoch [10213/20000], Loss: 900.273681640625, Entropy 439.10888671875, Learning Rate: 0.00015625\n",
      "Epoch [10214/20000], Loss: 849.004638671875, Entropy 427.6550598144531, Learning Rate: 0.00015625\n",
      "Epoch [10215/20000], Loss: 854.1910400390625, Entropy 434.4537048339844, Learning Rate: 0.00015625\n",
      "Epoch [10216/20000], Loss: 849.843017578125, Entropy 445.98291015625, Learning Rate: 0.00015625\n",
      "Epoch [10217/20000], Loss: 859.587646484375, Entropy 433.2624816894531, Learning Rate: 0.00015625\n",
      "Epoch [10218/20000], Loss: 888.9767456054688, Entropy 431.80194091796875, Learning Rate: 0.00015625\n",
      "Epoch [10219/20000], Loss: 876.5634765625, Entropy 432.3538513183594, Learning Rate: 0.00015625\n",
      "Epoch [10220/20000], Loss: 836.239501953125, Entropy 441.9744567871094, Learning Rate: 0.00015625\n",
      "Epoch [10221/20000], Loss: 863.6141967773438, Entropy 450.67047119140625, Learning Rate: 0.00015625\n",
      "Epoch [10222/20000], Loss: 858.210205078125, Entropy 449.9486999511719, Learning Rate: 0.00015625\n",
      "Epoch [10223/20000], Loss: 863.0494384765625, Entropy 437.552734375, Learning Rate: 0.00015625\n",
      "Epoch [10224/20000], Loss: 803.501708984375, Entropy 453.119873046875, Learning Rate: 0.00015625\n",
      "Epoch [10225/20000], Loss: 867.4857177734375, Entropy 416.4342956542969, Learning Rate: 0.00015625\n",
      "Epoch [10226/20000], Loss: 866.142822265625, Entropy 436.81298828125, Learning Rate: 0.00015625\n",
      "Epoch [10227/20000], Loss: 839.940673828125, Entropy 448.5378723144531, Learning Rate: 0.00015625\n",
      "Epoch [10228/20000], Loss: 856.4083251953125, Entropy 439.08349609375, Learning Rate: 0.00015625\n",
      "Epoch [10229/20000], Loss: 896.5416259765625, Entropy 439.7795715332031, Learning Rate: 0.00015625\n",
      "Epoch [10230/20000], Loss: 849.9031982421875, Entropy 444.4700927734375, Learning Rate: 0.00015625\n",
      "Epoch [10231/20000], Loss: 896.1160888671875, Entropy 442.4067077636719, Learning Rate: 0.00015625\n",
      "Epoch [10232/20000], Loss: 928.1355590820312, Entropy 433.81268310546875, Learning Rate: 0.00015625\n",
      "Epoch [10233/20000], Loss: 873.8628540039062, Entropy 427.36651611328125, Learning Rate: 0.00015625\n",
      "Epoch [10234/20000], Loss: 861.3312377929688, Entropy 445.11627197265625, Learning Rate: 0.00015625\n",
      "Epoch [10235/20000], Loss: 826.2380981445312, Entropy 447.92523193359375, Learning Rate: 0.00015625\n",
      "Epoch [10236/20000], Loss: 899.2996826171875, Entropy 423.5403747558594, Learning Rate: 0.00015625\n",
      "Epoch [10237/20000], Loss: 848.51904296875, Entropy 450.8055419921875, Learning Rate: 0.00015625\n",
      "Epoch [10238/20000], Loss: 896.421875, Entropy 448.6396789550781, Learning Rate: 0.00015625\n",
      "Epoch [10239/20000], Loss: 854.6231689453125, Entropy 427.1921081542969, Learning Rate: 0.00015625\n",
      "Epoch [10240/20000], Loss: 894.01806640625, Entropy 434.9944763183594, Learning Rate: 0.00015625\n",
      "Epoch [10241/20000], Loss: 811.1940307617188, Entropy 440.53521728515625, Learning Rate: 0.00015625\n",
      "Epoch [10242/20000], Loss: 824.589599609375, Entropy 439.2633056640625, Learning Rate: 0.00015625\n",
      "Epoch [10243/20000], Loss: 868.1724853515625, Entropy 447.3956604003906, Learning Rate: 0.00015625\n",
      "Epoch [10244/20000], Loss: 851.6519775390625, Entropy 448.275390625, Learning Rate: 0.00015625\n",
      "Epoch [10245/20000], Loss: 902.728759765625, Entropy 427.5481262207031, Learning Rate: 0.00015625\n",
      "Epoch [10246/20000], Loss: 861.3375854492188, Entropy 438.60931396484375, Learning Rate: 0.00015625\n",
      "Epoch [10247/20000], Loss: 814.960205078125, Entropy 457.8288269042969, Learning Rate: 0.00015625\n",
      "Epoch [10248/20000], Loss: 854.62451171875, Entropy 447.5481262207031, Learning Rate: 0.00015625\n",
      "Epoch [10249/20000], Loss: 862.9734497070312, Entropy 427.43450927734375, Learning Rate: 0.00015625\n",
      "Epoch [10250/20000], Loss: 875.232421875, Entropy 435.958740234375, Learning Rate: 0.00015625\n",
      "Epoch [10251/20000], Loss: 855.4764404296875, Entropy 461.7978820800781, Learning Rate: 0.00015625\n",
      "Epoch [10252/20000], Loss: 905.5859375, Entropy 440.0052490234375, Learning Rate: 0.00015625\n",
      "Epoch [10253/20000], Loss: 866.2365112304688, Entropy 443.81536865234375, Learning Rate: 0.00015625\n",
      "Epoch [10254/20000], Loss: 892.1131591796875, Entropy 427.3479919433594, Learning Rate: 0.00015625\n",
      "Epoch [10255/20000], Loss: 849.5403442382812, Entropy 444.44366455078125, Learning Rate: 0.00015625\n",
      "Epoch [10256/20000], Loss: 879.5097045898438, Entropy 441.91632080078125, Learning Rate: 0.00015625\n",
      "Epoch [10257/20000], Loss: 841.3577270507812, Entropy 446.08734130859375, Learning Rate: 0.00015625\n",
      "Epoch [10258/20000], Loss: 881.740478515625, Entropy 452.2508544921875, Learning Rate: 0.00015625\n",
      "Epoch [10259/20000], Loss: 834.2863159179688, Entropy 460.40679931640625, Learning Rate: 0.00015625\n",
      "Epoch [10260/20000], Loss: 886.5491333007812, Entropy 432.17120361328125, Learning Rate: 0.00015625\n",
      "Epoch [10261/20000], Loss: 867.0802001953125, Entropy 432.6530456542969, Learning Rate: 0.00015625\n",
      "Epoch [10262/20000], Loss: 866.1768798828125, Entropy 452.0162048339844, Learning Rate: 0.00015625\n",
      "Epoch [10263/20000], Loss: 894.0614013671875, Entropy 446.8707275390625, Learning Rate: 0.00015625\n",
      "Epoch [10264/20000], Loss: 856.470703125, Entropy 436.5570983886719, Learning Rate: 0.00015625\n",
      "Epoch [10265/20000], Loss: 831.3343505859375, Entropy 450.1037902832031, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10266/20000], Loss: 805.5250854492188, Entropy 452.43194580078125, Learning Rate: 0.00015625\n",
      "Epoch [10267/20000], Loss: 839.3318481445312, Entropy 448.28863525390625, Learning Rate: 0.00015625\n",
      "Epoch [10268/20000], Loss: 820.1328125, Entropy 447.7767028808594, Learning Rate: 0.00015625\n",
      "Epoch [10269/20000], Loss: 865.134521484375, Entropy 452.6947021484375, Learning Rate: 0.00015625\n",
      "Epoch [10270/20000], Loss: 954.7386474609375, Entropy 435.5900573730469, Learning Rate: 0.00015625\n",
      "Epoch [10271/20000], Loss: 855.3815307617188, Entropy 436.15045166015625, Learning Rate: 0.00015625\n",
      "Epoch [10272/20000], Loss: 830.0950927734375, Entropy 431.5401611328125, Learning Rate: 0.00015625\n",
      "Epoch [10273/20000], Loss: 900.03955078125, Entropy 420.0696105957031, Learning Rate: 0.00015625\n",
      "Epoch [10274/20000], Loss: 923.2927856445312, Entropy 450.29229736328125, Learning Rate: 0.00015625\n",
      "Epoch [10275/20000], Loss: 848.2801513671875, Entropy 450.2098693847656, Learning Rate: 0.00015625\n",
      "Epoch [10276/20000], Loss: 862.3056640625, Entropy 457.0894470214844, Learning Rate: 0.00015625\n",
      "Epoch [10277/20000], Loss: 857.950439453125, Entropy 444.6798400878906, Learning Rate: 0.00015625\n",
      "Epoch [10278/20000], Loss: 886.8536987304688, Entropy 449.85296630859375, Learning Rate: 0.00015625\n",
      "Epoch [10279/20000], Loss: 870.1181640625, Entropy 438.21484375, Learning Rate: 0.00015625\n",
      "Epoch [10280/20000], Loss: 851.616943359375, Entropy 449.2389831542969, Learning Rate: 0.00015625\n",
      "Epoch [10281/20000], Loss: 863.4944458007812, Entropy 448.54901123046875, Learning Rate: 0.00015625\n",
      "Epoch [10282/20000], Loss: 839.453857421875, Entropy 437.8270263671875, Learning Rate: 0.00015625\n",
      "Epoch [10283/20000], Loss: 852.0081787109375, Entropy 444.6141052246094, Learning Rate: 0.00015625\n",
      "Epoch [10284/20000], Loss: 884.0287475585938, Entropy 452.68572998046875, Learning Rate: 0.00015625\n",
      "Epoch [10285/20000], Loss: 845.7659912109375, Entropy 448.4128112792969, Learning Rate: 0.00015625\n",
      "Epoch [10286/20000], Loss: 902.36474609375, Entropy 435.1374206542969, Learning Rate: 0.00015625\n",
      "Epoch [10287/20000], Loss: 849.712646484375, Entropy 435.3688049316406, Learning Rate: 0.00015625\n",
      "Epoch [10288/20000], Loss: 871.0430297851562, Entropy 435.56927490234375, Learning Rate: 0.00015625\n",
      "Epoch [10289/20000], Loss: 796.1099243164062, Entropy 455.06976318359375, Learning Rate: 0.00015625\n",
      "Epoch [10290/20000], Loss: 864.063720703125, Entropy 452.1646423339844, Learning Rate: 0.00015625\n",
      "Epoch [10291/20000], Loss: 882.8214111328125, Entropy 443.1375732421875, Learning Rate: 0.00015625\n",
      "Epoch [10292/20000], Loss: 840.639404296875, Entropy 446.6981201171875, Learning Rate: 0.00015625\n",
      "Epoch [10293/20000], Loss: 885.8099365234375, Entropy 445.3224182128906, Learning Rate: 0.00015625\n",
      "Epoch [10294/20000], Loss: 850.545166015625, Entropy 437.2669677734375, Learning Rate: 0.00015625\n",
      "Epoch [10295/20000], Loss: 897.4406127929688, Entropy 434.82904052734375, Learning Rate: 0.00015625\n",
      "Epoch [10296/20000], Loss: 880.53369140625, Entropy 432.9216613769531, Learning Rate: 0.00015625\n",
      "Epoch [10297/20000], Loss: 845.9136352539062, Entropy 446.73565673828125, Learning Rate: 0.00015625\n",
      "Epoch [10298/20000], Loss: 884.996337890625, Entropy 428.1231994628906, Learning Rate: 0.00015625\n",
      "Epoch [10299/20000], Loss: 889.1990966796875, Entropy 426.5118408203125, Learning Rate: 0.00015625\n",
      "Epoch [10300/20000], Loss: 889.32177734375, Entropy 440.3960876464844, Learning Rate: 0.00015625\n",
      "Epoch [10301/20000], Loss: 827.8057250976562, Entropy 438.68695068359375, Learning Rate: 0.00015625\n",
      "Epoch [10302/20000], Loss: 863.4321899414062, Entropy 425.10882568359375, Learning Rate: 0.00015625\n",
      "Epoch [10303/20000], Loss: 878.637939453125, Entropy 451.0629577636719, Learning Rate: 0.00015625\n",
      "Epoch [10304/20000], Loss: 844.675537109375, Entropy 447.3552551269531, Learning Rate: 0.00015625\n",
      "Epoch [10305/20000], Loss: 931.69580078125, Entropy 441.2349853515625, Learning Rate: 0.00015625\n",
      "Epoch [10306/20000], Loss: 874.23486328125, Entropy 435.50048828125, Learning Rate: 0.00015625\n",
      "Epoch [10307/20000], Loss: 887.9949340820312, Entropy 446.70013427734375, Learning Rate: 0.00015625\n",
      "Epoch [10308/20000], Loss: 862.1068115234375, Entropy 438.986083984375, Learning Rate: 0.00015625\n",
      "Epoch [10309/20000], Loss: 841.19482421875, Entropy 450.5078125, Learning Rate: 0.00015625\n",
      "Epoch [10310/20000], Loss: 846.6539306640625, Entropy 440.0467834472656, Learning Rate: 0.00015625\n",
      "Epoch [10311/20000], Loss: 865.2811889648438, Entropy 434.96331787109375, Learning Rate: 0.00015625\n",
      "Epoch [10312/20000], Loss: 824.0730590820312, Entropy 441.02960205078125, Learning Rate: 0.00015625\n",
      "Epoch [10313/20000], Loss: 895.2705078125, Entropy 440.2134704589844, Learning Rate: 0.00015625\n",
      "Epoch [10314/20000], Loss: 844.1705322265625, Entropy 446.8853759765625, Learning Rate: 0.00015625\n",
      "Epoch [10315/20000], Loss: 851.1803588867188, Entropy 451.28521728515625, Learning Rate: 0.00015625\n",
      "Epoch [10316/20000], Loss: 838.3836059570312, Entropy 439.83831787109375, Learning Rate: 0.00015625\n",
      "Epoch [10317/20000], Loss: 872.9091796875, Entropy 445.6115417480469, Learning Rate: 0.00015625\n",
      "Epoch [10318/20000], Loss: 866.90576171875, Entropy 435.654296875, Learning Rate: 0.00015625\n",
      "Epoch [10319/20000], Loss: 918.4276123046875, Entropy 448.4837646484375, Learning Rate: 0.00015625\n",
      "Epoch [10320/20000], Loss: 859.8843383789062, Entropy 454.27593994140625, Learning Rate: 0.00015625\n",
      "Epoch [10321/20000], Loss: 844.3114013671875, Entropy 464.3216247558594, Learning Rate: 0.00015625\n",
      "Epoch [10322/20000], Loss: 912.9998168945312, Entropy 435.97515869140625, Learning Rate: 0.00015625\n",
      "Epoch [10323/20000], Loss: 830.610107421875, Entropy 433.861083984375, Learning Rate: 0.00015625\n",
      "Epoch [10324/20000], Loss: 852.9552001953125, Entropy 444.3526611328125, Learning Rate: 0.00015625\n",
      "Epoch [10325/20000], Loss: 805.726318359375, Entropy 440.3695983886719, Learning Rate: 0.00015625\n",
      "Epoch [10326/20000], Loss: 851.7093505859375, Entropy 444.8157958984375, Learning Rate: 0.00015625\n",
      "Epoch [10327/20000], Loss: 879.48291015625, Entropy 437.970458984375, Learning Rate: 0.00015625\n",
      "Epoch [10328/20000], Loss: 850.5170288085938, Entropy 449.69000244140625, Learning Rate: 0.00015625\n",
      "Epoch [10329/20000], Loss: 838.2864990234375, Entropy 443.7486572265625, Learning Rate: 0.00015625\n",
      "Epoch [10330/20000], Loss: 831.3548583984375, Entropy 464.4842834472656, Learning Rate: 0.00015625\n",
      "Epoch [10331/20000], Loss: 860.8695068359375, Entropy 443.8514404296875, Learning Rate: 0.00015625\n",
      "Epoch [10332/20000], Loss: 825.44677734375, Entropy 431.0244445800781, Learning Rate: 0.00015625\n",
      "Epoch [10333/20000], Loss: 847.1860961914062, Entropy 444.73626708984375, Learning Rate: 0.00015625\n",
      "Epoch [10334/20000], Loss: 879.4783325195312, Entropy 436.65325927734375, Learning Rate: 0.00015625\n",
      "Epoch [10335/20000], Loss: 857.8446044921875, Entropy 446.9527587890625, Learning Rate: 0.00015625\n",
      "Epoch [10336/20000], Loss: 885.6277465820312, Entropy 451.11444091796875, Learning Rate: 0.00015625\n",
      "Epoch [10337/20000], Loss: 879.506591796875, Entropy 426.4404602050781, Learning Rate: 0.00015625\n",
      "Epoch [10338/20000], Loss: 854.7128295898438, Entropy 456.60491943359375, Learning Rate: 0.00015625\n",
      "Epoch [10339/20000], Loss: 857.3345947265625, Entropy 438.2838134765625, Learning Rate: 0.00015625\n",
      "Epoch [10340/20000], Loss: 863.479736328125, Entropy 439.8182373046875, Learning Rate: 0.00015625\n",
      "Epoch [10341/20000], Loss: 842.858642578125, Entropy 446.2335510253906, Learning Rate: 0.00015625\n",
      "Epoch [10342/20000], Loss: 859.6285400390625, Entropy 456.9405822753906, Learning Rate: 0.00015625\n",
      "Epoch [10343/20000], Loss: 817.2840576171875, Entropy 446.3040771484375, Learning Rate: 0.00015625\n",
      "Epoch [10344/20000], Loss: 826.6192016601562, Entropy 449.74798583984375, Learning Rate: 0.00015625\n",
      "Epoch [10345/20000], Loss: 833.673828125, Entropy 444.6952209472656, Learning Rate: 0.00015625\n",
      "Epoch [10346/20000], Loss: 885.767333984375, Entropy 453.6873474121094, Learning Rate: 0.00015625\n",
      "Epoch [10347/20000], Loss: 825.7528076171875, Entropy 457.7701416015625, Learning Rate: 0.00015625\n",
      "Epoch [10348/20000], Loss: 899.5242919921875, Entropy 446.2557067871094, Learning Rate: 0.00015625\n",
      "Epoch [10349/20000], Loss: 869.8453369140625, Entropy 434.6474914550781, Learning Rate: 0.00015625\n",
      "Epoch [10350/20000], Loss: 894.884765625, Entropy 434.6558837890625, Learning Rate: 0.00015625\n",
      "Epoch [10351/20000], Loss: 855.1807250976562, Entropy 449.60845947265625, Learning Rate: 0.00015625\n",
      "Epoch [10352/20000], Loss: 823.490966796875, Entropy 449.3996887207031, Learning Rate: 0.00015625\n",
      "Epoch [10353/20000], Loss: 868.510986328125, Entropy 431.3323669433594, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10354/20000], Loss: 823.60498046875, Entropy 437.1134338378906, Learning Rate: 0.00015625\n",
      "Epoch [10355/20000], Loss: 851.698974609375, Entropy 445.3507080078125, Learning Rate: 0.00015625\n",
      "Epoch [10356/20000], Loss: 823.5438232421875, Entropy 441.423828125, Learning Rate: 0.00015625\n",
      "Epoch [10357/20000], Loss: 867.892822265625, Entropy 434.0682678222656, Learning Rate: 0.00015625\n",
      "Epoch [10358/20000], Loss: 830.1295166015625, Entropy 460.9349365234375, Learning Rate: 0.00015625\n",
      "Epoch [10359/20000], Loss: 823.2435302734375, Entropy 454.9811096191406, Learning Rate: 0.00015625\n",
      "Epoch [10360/20000], Loss: 884.0247192382812, Entropy 443.28509521484375, Learning Rate: 0.00015625\n",
      "Epoch [10361/20000], Loss: 857.1434326171875, Entropy 447.1668701171875, Learning Rate: 0.00015625\n",
      "Epoch [10362/20000], Loss: 865.6058959960938, Entropy 427.83233642578125, Learning Rate: 0.00015625\n",
      "Epoch [10363/20000], Loss: 810.2418823242188, Entropy 453.01568603515625, Learning Rate: 0.00015625\n",
      "Epoch [10364/20000], Loss: 921.11962890625, Entropy 435.9943542480469, Learning Rate: 0.00015625\n",
      "Epoch [10365/20000], Loss: 860.3592529296875, Entropy 426.0943908691406, Learning Rate: 0.00015625\n",
      "Epoch [10366/20000], Loss: 874.2203369140625, Entropy 427.4453430175781, Learning Rate: 0.00015625\n",
      "Epoch [10367/20000], Loss: 854.364501953125, Entropy 457.4248352050781, Learning Rate: 0.00015625\n",
      "Epoch [10368/20000], Loss: 838.7579956054688, Entropy 438.22210693359375, Learning Rate: 0.00015625\n",
      "Epoch [10369/20000], Loss: 809.4990234375, Entropy 460.2669372558594, Learning Rate: 0.00015625\n",
      "Epoch [10370/20000], Loss: 844.3649291992188, Entropy 446.79302978515625, Learning Rate: 0.00015625\n",
      "Epoch [10371/20000], Loss: 865.3150634765625, Entropy 434.4735107421875, Learning Rate: 0.00015625\n",
      "Epoch [10372/20000], Loss: 787.248779296875, Entropy 456.5260009765625, Learning Rate: 0.00015625\n",
      "Epoch [10373/20000], Loss: 902.27587890625, Entropy 438.55810546875, Learning Rate: 0.00015625\n",
      "Epoch [10374/20000], Loss: 826.294677734375, Entropy 455.3220520019531, Learning Rate: 0.00015625\n",
      "Epoch [10375/20000], Loss: 898.294677734375, Entropy 451.9929504394531, Learning Rate: 0.00015625\n",
      "Epoch [10376/20000], Loss: 869.1324462890625, Entropy 449.2906188964844, Learning Rate: 0.00015625\n",
      "Epoch [10377/20000], Loss: 869.8997802734375, Entropy 432.1374816894531, Learning Rate: 0.00015625\n",
      "Epoch [10378/20000], Loss: 864.0574340820312, Entropy 452.15289306640625, Learning Rate: 0.00015625\n",
      "Epoch [10379/20000], Loss: 887.2978515625, Entropy 441.8009948730469, Learning Rate: 0.00015625\n",
      "Epoch [10380/20000], Loss: 845.0863647460938, Entropy 441.53021240234375, Learning Rate: 0.00015625\n",
      "Epoch [10381/20000], Loss: 882.93505859375, Entropy 434.2564392089844, Learning Rate: 0.00015625\n",
      "Epoch [10382/20000], Loss: 862.7545166015625, Entropy 452.7063903808594, Learning Rate: 0.00015625\n",
      "Epoch [10383/20000], Loss: 946.6632080078125, Entropy 426.2251892089844, Learning Rate: 0.00015625\n",
      "Epoch [10384/20000], Loss: 829.2298583984375, Entropy 461.4757385253906, Learning Rate: 0.00015625\n",
      "Epoch [10385/20000], Loss: 895.1039428710938, Entropy 437.70989990234375, Learning Rate: 0.00015625\n",
      "Epoch [10386/20000], Loss: 861.3665161132812, Entropy 436.54583740234375, Learning Rate: 0.00015625\n",
      "Epoch [10387/20000], Loss: 874.969970703125, Entropy 435.8271789550781, Learning Rate: 0.00015625\n",
      "Epoch [10388/20000], Loss: 917.934814453125, Entropy 433.5445251464844, Learning Rate: 0.00015625\n",
      "Epoch [10389/20000], Loss: 912.2691040039062, Entropy 440.79193115234375, Learning Rate: 0.00015625\n",
      "Epoch [10390/20000], Loss: 823.3074951171875, Entropy 446.6081848144531, Learning Rate: 0.00015625\n",
      "Epoch [10391/20000], Loss: 869.379150390625, Entropy 445.2969055175781, Learning Rate: 0.00015625\n",
      "Epoch [10392/20000], Loss: 867.9996337890625, Entropy 462.4765625, Learning Rate: 0.00015625\n",
      "Epoch [10393/20000], Loss: 872.4558715820312, Entropy 449.94195556640625, Learning Rate: 0.00015625\n",
      "Epoch [10394/20000], Loss: 857.3580322265625, Entropy 445.2745361328125, Learning Rate: 0.00015625\n",
      "Epoch [10395/20000], Loss: 861.9619140625, Entropy 444.10693359375, Learning Rate: 0.00015625\n",
      "Epoch [10396/20000], Loss: 869.9307250976562, Entropy 437.93609619140625, Learning Rate: 0.00015625\n",
      "Epoch [10397/20000], Loss: 857.733642578125, Entropy 446.1155700683594, Learning Rate: 0.00015625\n",
      "Epoch [10398/20000], Loss: 831.8692626953125, Entropy 437.5584716796875, Learning Rate: 0.00015625\n",
      "Epoch [10399/20000], Loss: 873.5106201171875, Entropy 436.3371887207031, Learning Rate: 0.00015625\n",
      "Epoch [10400/20000], Loss: 869.53369140625, Entropy 429.8331604003906, Learning Rate: 0.00015625\n",
      "Epoch [10401/20000], Loss: 908.632080078125, Entropy 437.1363525390625, Learning Rate: 0.00015625\n",
      "Epoch [10402/20000], Loss: 886.4169921875, Entropy 441.8385925292969, Learning Rate: 0.00015625\n",
      "Epoch [10403/20000], Loss: 876.4847412109375, Entropy 444.4742736816406, Learning Rate: 0.00015625\n",
      "Epoch [10404/20000], Loss: 885.535888671875, Entropy 432.0740661621094, Learning Rate: 0.00015625\n",
      "Epoch [10405/20000], Loss: 942.2378540039062, Entropy 427.36834716796875, Learning Rate: 0.00015625\n",
      "Epoch [10406/20000], Loss: 887.6258544921875, Entropy 445.6271667480469, Learning Rate: 0.00015625\n",
      "Epoch [10407/20000], Loss: 838.9520263671875, Entropy 428.9284973144531, Learning Rate: 0.00015625\n",
      "Epoch [10408/20000], Loss: 843.046875, Entropy 445.7035827636719, Learning Rate: 0.00015625\n",
      "Epoch [10409/20000], Loss: 897.414306640625, Entropy 448.2074890136719, Learning Rate: 0.00015625\n",
      "Epoch [10410/20000], Loss: 889.8587646484375, Entropy 431.7088928222656, Learning Rate: 0.00015625\n",
      "Epoch [10411/20000], Loss: 840.133056640625, Entropy 456.4674072265625, Learning Rate: 0.00015625\n",
      "Epoch [10412/20000], Loss: 913.9064331054688, Entropy 439.97161865234375, Learning Rate: 0.00015625\n",
      "Epoch [10413/20000], Loss: 851.1300659179688, Entropy 448.52056884765625, Learning Rate: 0.00015625\n",
      "Epoch [10414/20000], Loss: 834.212158203125, Entropy 455.4520263671875, Learning Rate: 0.00015625\n",
      "Epoch [10415/20000], Loss: 801.2379150390625, Entropy 455.8495178222656, Learning Rate: 0.00015625\n",
      "Epoch [10416/20000], Loss: 830.041015625, Entropy 441.0879211425781, Learning Rate: 0.00015625\n",
      "Epoch [10417/20000], Loss: 829.6608276367188, Entropy 444.32574462890625, Learning Rate: 0.00015625\n",
      "Epoch [10418/20000], Loss: 864.22119140625, Entropy 440.5436096191406, Learning Rate: 0.00015625\n",
      "Epoch [10419/20000], Loss: 871.7164306640625, Entropy 442.5950622558594, Learning Rate: 0.00015625\n",
      "Epoch [10420/20000], Loss: 875.683349609375, Entropy 461.4331970214844, Learning Rate: 0.00015625\n",
      "Epoch [10421/20000], Loss: 870.730712890625, Entropy 458.0332336425781, Learning Rate: 0.00015625\n",
      "Epoch [10422/20000], Loss: 843.6920776367188, Entropy 440.13189697265625, Learning Rate: 0.00015625\n",
      "Epoch [10423/20000], Loss: 911.4295654296875, Entropy 433.43701171875, Learning Rate: 0.00015625\n",
      "Epoch [10424/20000], Loss: 831.1583251953125, Entropy 456.1007080078125, Learning Rate: 0.00015625\n",
      "Epoch [10425/20000], Loss: 842.5700073242188, Entropy 458.91314697265625, Learning Rate: 0.00015625\n",
      "Epoch [10426/20000], Loss: 847.5513916015625, Entropy 440.6896057128906, Learning Rate: 0.00015625\n",
      "Epoch [10427/20000], Loss: 841.63671875, Entropy 439.9637756347656, Learning Rate: 0.00015625\n",
      "Epoch [10428/20000], Loss: 854.64794921875, Entropy 441.3980712890625, Learning Rate: 0.00015625\n",
      "Epoch [10429/20000], Loss: 884.3749389648438, Entropy 461.70758056640625, Learning Rate: 0.00015625\n",
      "Epoch [10430/20000], Loss: 888.667724609375, Entropy 436.6681823730469, Learning Rate: 0.00015625\n",
      "Epoch [10431/20000], Loss: 847.3751220703125, Entropy 447.0753479003906, Learning Rate: 0.00015625\n",
      "Epoch [10432/20000], Loss: 876.57373046875, Entropy 444.8052978515625, Learning Rate: 0.00015625\n",
      "Epoch [10433/20000], Loss: 942.02294921875, Entropy 443.5227355957031, Learning Rate: 0.00015625\n",
      "Epoch [10434/20000], Loss: 846.8601684570312, Entropy 433.93096923828125, Learning Rate: 0.00015625\n",
      "Epoch [10435/20000], Loss: 902.9967041015625, Entropy 434.2210388183594, Learning Rate: 0.00015625\n",
      "Epoch [10436/20000], Loss: 817.7799072265625, Entropy 464.9117126464844, Learning Rate: 0.00015625\n",
      "Epoch [10437/20000], Loss: 870.2889404296875, Entropy 442.3720703125, Learning Rate: 0.00015625\n",
      "Epoch [10438/20000], Loss: 928.7721557617188, Entropy 433.65191650390625, Learning Rate: 0.00015625\n",
      "Epoch [10439/20000], Loss: 885.2544555664062, Entropy 437.40679931640625, Learning Rate: 0.00015625\n",
      "Epoch [10440/20000], Loss: 854.5238647460938, Entropy 457.14984130859375, Learning Rate: 0.00015625\n",
      "Epoch [10441/20000], Loss: 918.8556518554688, Entropy 453.36614990234375, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10442/20000], Loss: 878.21240234375, Entropy 440.0387268066406, Learning Rate: 0.00015625\n",
      "Epoch [10443/20000], Loss: 833.691650390625, Entropy 437.4335021972656, Learning Rate: 0.00015625\n",
      "Epoch [10444/20000], Loss: 862.644775390625, Entropy 434.6902160644531, Learning Rate: 0.00015625\n",
      "Epoch [10445/20000], Loss: 847.7679443359375, Entropy 453.6413879394531, Learning Rate: 0.00015625\n",
      "Epoch [10446/20000], Loss: 811.375732421875, Entropy 435.7003479003906, Learning Rate: 0.00015625\n",
      "Epoch [10447/20000], Loss: 848.2972412109375, Entropy 460.5643615722656, Learning Rate: 0.00015625\n",
      "Epoch [10448/20000], Loss: 923.0967407226562, Entropy 446.52764892578125, Learning Rate: 0.00015625\n",
      "Epoch [10449/20000], Loss: 839.4315185546875, Entropy 454.4837341308594, Learning Rate: 0.00015625\n",
      "Epoch [10450/20000], Loss: 845.4033203125, Entropy 454.9224853515625, Learning Rate: 0.00015625\n",
      "Epoch [10451/20000], Loss: 876.68505859375, Entropy 436.8210754394531, Learning Rate: 0.00015625\n",
      "Epoch [10452/20000], Loss: 887.151123046875, Entropy 448.2082824707031, Learning Rate: 0.00015625\n",
      "Epoch [10453/20000], Loss: 856.5418701171875, Entropy 441.2669372558594, Learning Rate: 0.00015625\n",
      "Epoch [10454/20000], Loss: 866.3473510742188, Entropy 450.46319580078125, Learning Rate: 0.00015625\n",
      "Epoch [10455/20000], Loss: 819.138916015625, Entropy 433.4797668457031, Learning Rate: 0.00015625\n",
      "Epoch [10456/20000], Loss: 825.8116455078125, Entropy 433.7231140136719, Learning Rate: 0.00015625\n",
      "Epoch [10457/20000], Loss: 843.9213256835938, Entropy 435.31402587890625, Learning Rate: 0.00015625\n",
      "Epoch [10458/20000], Loss: 857.6343994140625, Entropy 442.65966796875, Learning Rate: 0.00015625\n",
      "Epoch [10459/20000], Loss: 864.918212890625, Entropy 445.4742736816406, Learning Rate: 0.00015625\n",
      "Epoch [10460/20000], Loss: 906.4546508789062, Entropy 440.44171142578125, Learning Rate: 0.00015625\n",
      "Epoch [10461/20000], Loss: 827.338623046875, Entropy 451.6529846191406, Learning Rate: 0.00015625\n",
      "Epoch [10462/20000], Loss: 831.6635131835938, Entropy 450.68084716796875, Learning Rate: 0.00015625\n",
      "Epoch [10463/20000], Loss: 841.9810791015625, Entropy 435.3372497558594, Learning Rate: 0.00015625\n",
      "Epoch [10464/20000], Loss: 824.83642578125, Entropy 435.6109924316406, Learning Rate: 0.00015625\n",
      "Epoch [10465/20000], Loss: 837.1904907226562, Entropy 457.28436279296875, Learning Rate: 0.00015625\n",
      "Epoch [10466/20000], Loss: 914.5377807617188, Entropy 461.16217041015625, Learning Rate: 0.00015625\n",
      "Epoch [10467/20000], Loss: 862.0765991210938, Entropy 443.88897705078125, Learning Rate: 0.00015625\n",
      "Epoch [10468/20000], Loss: 893.6939086914062, Entropy 461.88116455078125, Learning Rate: 0.00015625\n",
      "Epoch [10469/20000], Loss: 831.1490478515625, Entropy 440.7413635253906, Learning Rate: 0.00015625\n",
      "Epoch [10470/20000], Loss: 835.390869140625, Entropy 446.251953125, Learning Rate: 0.00015625\n",
      "Epoch [10471/20000], Loss: 868.33447265625, Entropy 449.26513671875, Learning Rate: 0.00015625\n",
      "Epoch [10472/20000], Loss: 884.1876220703125, Entropy 445.0115661621094, Learning Rate: 0.00015625\n",
      "Epoch [10473/20000], Loss: 863.6507568359375, Entropy 452.7132568359375, Learning Rate: 0.00015625\n",
      "Epoch [10474/20000], Loss: 911.9617919921875, Entropy 430.3263244628906, Learning Rate: 0.00015625\n",
      "Epoch [10475/20000], Loss: 860.669189453125, Entropy 447.7773742675781, Learning Rate: 0.00015625\n",
      "Epoch [10476/20000], Loss: 818.85498046875, Entropy 470.6829833984375, Learning Rate: 0.00015625\n",
      "Epoch [10477/20000], Loss: 836.71728515625, Entropy 445.1443176269531, Learning Rate: 0.00015625\n",
      "Epoch [10478/20000], Loss: 852.1375122070312, Entropy 469.59954833984375, Learning Rate: 0.00015625\n",
      "Epoch [10479/20000], Loss: 880.9736328125, Entropy 446.9900817871094, Learning Rate: 0.00015625\n",
      "Epoch [10480/20000], Loss: 874.6337890625, Entropy 448.6568908691406, Learning Rate: 0.00015625\n",
      "Epoch [10481/20000], Loss: 874.062255859375, Entropy 438.9236755371094, Learning Rate: 0.00015625\n",
      "Epoch [10482/20000], Loss: 860.53271484375, Entropy 446.1265563964844, Learning Rate: 0.00015625\n",
      "Epoch [10483/20000], Loss: 862.93359375, Entropy 447.18017578125, Learning Rate: 0.00015625\n",
      "Epoch [10484/20000], Loss: 882.9664306640625, Entropy 434.5163269042969, Learning Rate: 0.00015625\n",
      "Epoch [10485/20000], Loss: 836.5032348632812, Entropy 463.79730224609375, Learning Rate: 0.00015625\n",
      "Epoch [10486/20000], Loss: 882.3880615234375, Entropy 440.5783386230469, Learning Rate: 0.00015625\n",
      "Epoch [10487/20000], Loss: 909.0651245117188, Entropy 428.11419677734375, Learning Rate: 0.00015625\n",
      "Epoch [10488/20000], Loss: 861.6585693359375, Entropy 443.7133483886719, Learning Rate: 0.00015625\n",
      "Epoch [10489/20000], Loss: 863.3794555664062, Entropy 439.91314697265625, Learning Rate: 0.00015625\n",
      "Epoch [10490/20000], Loss: 803.3934326171875, Entropy 448.6055908203125, Learning Rate: 0.00015625\n",
      "Epoch [10491/20000], Loss: 852.3157958984375, Entropy 445.3100280761719, Learning Rate: 0.00015625\n",
      "Epoch [10492/20000], Loss: 866.1246948242188, Entropy 443.32818603515625, Learning Rate: 0.00015625\n",
      "Epoch [10493/20000], Loss: 850.88818359375, Entropy 457.7463073730469, Learning Rate: 0.00015625\n",
      "Epoch [10494/20000], Loss: 920.3973388671875, Entropy 439.32421875, Learning Rate: 0.00015625\n",
      "Epoch [10495/20000], Loss: 878.1373291015625, Entropy 439.5660705566406, Learning Rate: 0.00015625\n",
      "Epoch [10496/20000], Loss: 815.3990478515625, Entropy 448.4820556640625, Learning Rate: 0.00015625\n",
      "Epoch [10497/20000], Loss: 867.5155639648438, Entropy 438.67999267578125, Learning Rate: 0.00015625\n",
      "Epoch [10498/20000], Loss: 859.1072998046875, Entropy 434.0663146972656, Learning Rate: 0.00015625\n",
      "Epoch [10499/20000], Loss: 820.7035522460938, Entropy 448.36456298828125, Learning Rate: 0.00015625\n",
      "Epoch [10500/20000], Loss: 865.640869140625, Entropy 438.1649475097656, Learning Rate: 0.00015625\n",
      "Epoch [10501/20000], Loss: 850.1060180664062, Entropy 430.14349365234375, Learning Rate: 0.00015625\n",
      "Epoch [10502/20000], Loss: 902.2203369140625, Entropy 446.1618347167969, Learning Rate: 0.00015625\n",
      "Epoch [10503/20000], Loss: 881.7411499023438, Entropy 453.54913330078125, Learning Rate: 0.00015625\n",
      "Epoch [10504/20000], Loss: 855.6041259765625, Entropy 450.7164611816406, Learning Rate: 0.00015625\n",
      "Epoch [10505/20000], Loss: 850.934814453125, Entropy 438.2088317871094, Learning Rate: 0.00015625\n",
      "Epoch [10506/20000], Loss: 848.8158569335938, Entropy 455.84600830078125, Learning Rate: 0.00015625\n",
      "Epoch [10507/20000], Loss: 852.49267578125, Entropy 445.1033020019531, Learning Rate: 0.00015625\n",
      "Epoch [10508/20000], Loss: 826.990966796875, Entropy 446.2886657714844, Learning Rate: 0.00015625\n",
      "Epoch [10509/20000], Loss: 860.542724609375, Entropy 442.2593688964844, Learning Rate: 0.00015625\n",
      "Epoch [10510/20000], Loss: 862.4450073242188, Entropy 453.04803466796875, Learning Rate: 0.00015625\n",
      "Epoch [10511/20000], Loss: 857.31787109375, Entropy 451.9870910644531, Learning Rate: 0.00015625\n",
      "Epoch [10512/20000], Loss: 881.4542236328125, Entropy 428.0897521972656, Learning Rate: 0.00015625\n",
      "Epoch [10513/20000], Loss: 841.56298828125, Entropy 443.0452880859375, Learning Rate: 0.00015625\n",
      "Epoch [10514/20000], Loss: 887.39794921875, Entropy 448.36767578125, Learning Rate: 0.00015625\n",
      "Epoch [10515/20000], Loss: 884.0975341796875, Entropy 430.5643615722656, Learning Rate: 0.00015625\n",
      "Epoch [10516/20000], Loss: 853.8350830078125, Entropy 453.73583984375, Learning Rate: 0.00015625\n",
      "Epoch [10517/20000], Loss: 879.2242431640625, Entropy 447.7024230957031, Learning Rate: 0.00015625\n",
      "Epoch [10518/20000], Loss: 856.220703125, Entropy 442.185546875, Learning Rate: 0.00015625\n",
      "Epoch [10519/20000], Loss: 832.245849609375, Entropy 460.7911376953125, Learning Rate: 0.00015625\n",
      "Epoch [10520/20000], Loss: 867.033935546875, Entropy 437.5937805175781, Learning Rate: 0.00015625\n",
      "Epoch [10521/20000], Loss: 863.2825317382812, Entropy 430.69683837890625, Learning Rate: 0.00015625\n",
      "Epoch [10522/20000], Loss: 844.3248291015625, Entropy 454.3858642578125, Learning Rate: 0.00015625\n",
      "Epoch [10523/20000], Loss: 863.3162841796875, Entropy 433.2275695800781, Learning Rate: 0.00015625\n",
      "Epoch [10524/20000], Loss: 841.185546875, Entropy 451.6818542480469, Learning Rate: 0.00015625\n",
      "Epoch [10525/20000], Loss: 801.3804321289062, Entropy 465.25823974609375, Learning Rate: 0.00015625\n",
      "Epoch [10526/20000], Loss: 865.087646484375, Entropy 437.2401123046875, Learning Rate: 0.00015625\n",
      "Epoch [10527/20000], Loss: 837.0709838867188, Entropy 445.67889404296875, Learning Rate: 0.00015625\n",
      "Epoch [10528/20000], Loss: 844.974365234375, Entropy 439.3659362792969, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10529/20000], Loss: 868.147216796875, Entropy 447.1698303222656, Learning Rate: 0.00015625\n",
      "Epoch [10530/20000], Loss: 863.1094970703125, Entropy 442.9808349609375, Learning Rate: 0.00015625\n",
      "Epoch [10531/20000], Loss: 830.0037841796875, Entropy 443.8946838378906, Learning Rate: 0.00015625\n",
      "Epoch [10532/20000], Loss: 833.1365966796875, Entropy 447.724365234375, Learning Rate: 0.00015625\n",
      "Epoch [10533/20000], Loss: 852.0821533203125, Entropy 442.3050231933594, Learning Rate: 0.00015625\n",
      "Epoch [10534/20000], Loss: 882.5297241210938, Entropy 431.25677490234375, Learning Rate: 0.00015625\n",
      "Epoch [10535/20000], Loss: 869.6478881835938, Entropy 450.55889892578125, Learning Rate: 0.00015625\n",
      "Epoch [10536/20000], Loss: 874.9318237304688, Entropy 431.88140869140625, Learning Rate: 0.00015625\n",
      "Epoch [10537/20000], Loss: 845.4571533203125, Entropy 458.0193176269531, Learning Rate: 0.00015625\n",
      "Epoch [10538/20000], Loss: 898.684814453125, Entropy 437.7069091796875, Learning Rate: 0.00015625\n",
      "Epoch [10539/20000], Loss: 872.6248779296875, Entropy 456.8395080566406, Learning Rate: 0.00015625\n",
      "Epoch [10540/20000], Loss: 839.163818359375, Entropy 466.7778015136719, Learning Rate: 0.00015625\n",
      "Epoch [10541/20000], Loss: 912.665771484375, Entropy 444.8554992675781, Learning Rate: 0.00015625\n",
      "Epoch [10542/20000], Loss: 834.4906616210938, Entropy 460.69256591796875, Learning Rate: 0.00015625\n",
      "Epoch [10543/20000], Loss: 824.5137939453125, Entropy 451.554443359375, Learning Rate: 0.00015625\n",
      "Epoch [10544/20000], Loss: 854.04150390625, Entropy 461.2278137207031, Learning Rate: 0.00015625\n",
      "Epoch [10545/20000], Loss: 903.1483154296875, Entropy 449.1379699707031, Learning Rate: 0.00015625\n",
      "Epoch [10546/20000], Loss: 862.239501953125, Entropy 440.1385803222656, Learning Rate: 0.00015625\n",
      "Epoch [10547/20000], Loss: 844.1566772460938, Entropy 456.30279541015625, Learning Rate: 0.00015625\n",
      "Epoch [10548/20000], Loss: 862.260498046875, Entropy 433.4236755371094, Learning Rate: 0.00015625\n",
      "Epoch [10549/20000], Loss: 889.941162109375, Entropy 428.7300720214844, Learning Rate: 0.00015625\n",
      "Epoch [10550/20000], Loss: 879.1224365234375, Entropy 431.4723815917969, Learning Rate: 0.00015625\n",
      "Epoch [10551/20000], Loss: 823.3919677734375, Entropy 451.5499267578125, Learning Rate: 0.00015625\n",
      "Epoch [10552/20000], Loss: 847.029296875, Entropy 448.7341003417969, Learning Rate: 0.00015625\n",
      "Epoch [10553/20000], Loss: 817.9674072265625, Entropy 460.8580322265625, Learning Rate: 0.00015625\n",
      "Epoch [10554/20000], Loss: 863.869873046875, Entropy 438.55224609375, Learning Rate: 0.00015625\n",
      "Epoch [10555/20000], Loss: 874.8067016601562, Entropy 448.72576904296875, Learning Rate: 0.00015625\n",
      "Epoch [10556/20000], Loss: 915.089599609375, Entropy 461.8006896972656, Learning Rate: 0.00015625\n",
      "Epoch [10557/20000], Loss: 852.8291015625, Entropy 457.9014892578125, Learning Rate: 0.00015625\n",
      "Epoch [10558/20000], Loss: 821.7215576171875, Entropy 454.9609680175781, Learning Rate: 0.00015625\n",
      "Epoch [10559/20000], Loss: 846.2779541015625, Entropy 457.1495361328125, Learning Rate: 0.00015625\n",
      "Epoch [10560/20000], Loss: 899.32275390625, Entropy 421.8448791503906, Learning Rate: 0.00015625\n",
      "Epoch [10561/20000], Loss: 822.0610961914062, Entropy 443.16839599609375, Learning Rate: 0.00015625\n",
      "Epoch [10562/20000], Loss: 794.1585083007812, Entropy 449.26300048828125, Learning Rate: 0.00015625\n",
      "Epoch [10563/20000], Loss: 853.6984252929688, Entropy 441.52044677734375, Learning Rate: 0.00015625\n",
      "Epoch [10564/20000], Loss: 862.3939208984375, Entropy 459.14306640625, Learning Rate: 0.00015625\n",
      "Epoch [10565/20000], Loss: 862.1060791015625, Entropy 442.4159240722656, Learning Rate: 0.00015625\n",
      "Epoch [10566/20000], Loss: 843.89453125, Entropy 435.4379577636719, Learning Rate: 0.00015625\n",
      "Epoch [10567/20000], Loss: 858.352294921875, Entropy 442.5001220703125, Learning Rate: 0.00015625\n",
      "Epoch [10568/20000], Loss: 833.4149780273438, Entropy 443.83221435546875, Learning Rate: 0.00015625\n",
      "Epoch [10569/20000], Loss: 870.4088134765625, Entropy 455.4353942871094, Learning Rate: 0.00015625\n",
      "Epoch [10570/20000], Loss: 876.6917724609375, Entropy 442.7947998046875, Learning Rate: 0.00015625\n",
      "Epoch [10571/20000], Loss: 871.484619140625, Entropy 438.369873046875, Learning Rate: 0.00015625\n",
      "Epoch [10572/20000], Loss: 913.5731201171875, Entropy 442.5015563964844, Learning Rate: 0.00015625\n",
      "Epoch [10573/20000], Loss: 854.6163940429688, Entropy 438.21441650390625, Learning Rate: 0.00015625\n",
      "Epoch [10574/20000], Loss: 848.4911499023438, Entropy 447.72161865234375, Learning Rate: 0.00015625\n",
      "Epoch [10575/20000], Loss: 834.9657592773438, Entropy 447.46539306640625, Learning Rate: 0.00015625\n",
      "Epoch [10576/20000], Loss: 824.291748046875, Entropy 447.798583984375, Learning Rate: 0.00015625\n",
      "Epoch [10577/20000], Loss: 835.4859619140625, Entropy 435.90576171875, Learning Rate: 0.00015625\n",
      "Epoch [10578/20000], Loss: 811.6778564453125, Entropy 452.0633850097656, Learning Rate: 0.00015625\n",
      "Epoch [10579/20000], Loss: 838.7674560546875, Entropy 442.0838928222656, Learning Rate: 0.00015625\n",
      "Epoch [10580/20000], Loss: 851.9854736328125, Entropy 430.3289794921875, Learning Rate: 0.00015625\n",
      "Epoch [10581/20000], Loss: 867.02392578125, Entropy 430.1056213378906, Learning Rate: 0.00015625\n",
      "Epoch [10582/20000], Loss: 888.7406616210938, Entropy 423.54022216796875, Learning Rate: 0.00015625\n",
      "Epoch [10583/20000], Loss: 928.2750244140625, Entropy 439.0177307128906, Learning Rate: 0.00015625\n",
      "Epoch [10584/20000], Loss: 830.208984375, Entropy 451.2844543457031, Learning Rate: 0.00015625\n",
      "Epoch [10585/20000], Loss: 845.5120239257812, Entropy 435.15435791015625, Learning Rate: 0.00015625\n",
      "Epoch [10586/20000], Loss: 869.8674926757812, Entropy 432.94525146484375, Learning Rate: 0.00015625\n",
      "Epoch [10587/20000], Loss: 882.3263549804688, Entropy 465.90643310546875, Learning Rate: 0.00015625\n",
      "Epoch [10588/20000], Loss: 848.209716796875, Entropy 459.0157470703125, Learning Rate: 0.00015625\n",
      "Epoch [10589/20000], Loss: 876.87890625, Entropy 447.5892028808594, Learning Rate: 0.00015625\n",
      "Epoch [10590/20000], Loss: 872.8211669921875, Entropy 439.2938232421875, Learning Rate: 0.00015625\n",
      "Epoch [10591/20000], Loss: 871.855224609375, Entropy 454.0788269042969, Learning Rate: 0.00015625\n",
      "Epoch [10592/20000], Loss: 851.7994384765625, Entropy 432.6140441894531, Learning Rate: 0.00015625\n",
      "Epoch [10593/20000], Loss: 890.866455078125, Entropy 445.8502502441406, Learning Rate: 0.00015625\n",
      "Epoch [10594/20000], Loss: 843.2701416015625, Entropy 444.992919921875, Learning Rate: 0.00015625\n",
      "Epoch [10595/20000], Loss: 852.5999145507812, Entropy 456.80108642578125, Learning Rate: 0.00015625\n",
      "Epoch [10596/20000], Loss: 881.229248046875, Entropy 444.210693359375, Learning Rate: 0.00015625\n",
      "Epoch [10597/20000], Loss: 848.845458984375, Entropy 449.0808410644531, Learning Rate: 0.00015625\n",
      "Epoch [10598/20000], Loss: 905.7353515625, Entropy 448.8189697265625, Learning Rate: 0.00015625\n",
      "Epoch [10599/20000], Loss: 811.9151611328125, Entropy 445.0055236816406, Learning Rate: 0.00015625\n",
      "Epoch [10600/20000], Loss: 884.8095703125, Entropy 449.6522521972656, Learning Rate: 0.00015625\n",
      "Epoch [10601/20000], Loss: 877.1842041015625, Entropy 426.2406921386719, Learning Rate: 0.00015625\n",
      "Epoch [10602/20000], Loss: 860.2384033203125, Entropy 448.6492004394531, Learning Rate: 0.00015625\n",
      "Epoch [10603/20000], Loss: 801.4912109375, Entropy 453.6498718261719, Learning Rate: 0.00015625\n",
      "Epoch [10604/20000], Loss: 867.8856201171875, Entropy 450.431396484375, Learning Rate: 0.00015625\n",
      "Epoch [10605/20000], Loss: 874.15625, Entropy 431.3464660644531, Learning Rate: 0.00015625\n",
      "Epoch [10606/20000], Loss: 840.9552001953125, Entropy 452.30078125, Learning Rate: 0.00015625\n",
      "Epoch [10607/20000], Loss: 899.977783203125, Entropy 448.5628662109375, Learning Rate: 0.00015625\n",
      "Epoch [10608/20000], Loss: 892.98828125, Entropy 433.978271484375, Learning Rate: 0.00015625\n",
      "Epoch [10609/20000], Loss: 843.202880859375, Entropy 459.5684814453125, Learning Rate: 0.00015625\n",
      "Epoch [10610/20000], Loss: 854.8778076171875, Entropy 452.8307800292969, Learning Rate: 0.00015625\n",
      "Epoch [10611/20000], Loss: 801.1066284179688, Entropy 456.50994873046875, Learning Rate: 0.00015625\n",
      "Epoch [10612/20000], Loss: 859.9736938476562, Entropy 454.02972412109375, Learning Rate: 0.00015625\n",
      "Epoch [10613/20000], Loss: 835.308837890625, Entropy 462.0752258300781, Learning Rate: 0.00015625\n",
      "Epoch [10614/20000], Loss: 876.369384765625, Entropy 461.5599060058594, Learning Rate: 0.00015625\n",
      "Epoch [10615/20000], Loss: 843.1505737304688, Entropy 468.61663818359375, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10616/20000], Loss: 892.8328857421875, Entropy 449.7174072265625, Learning Rate: 0.00015625\n",
      "Epoch [10617/20000], Loss: 858.7113037109375, Entropy 466.7789611816406, Learning Rate: 0.00015625\n",
      "Epoch [10618/20000], Loss: 865.833251953125, Entropy 447.2354736328125, Learning Rate: 0.00015625\n",
      "Epoch [10619/20000], Loss: 824.94580078125, Entropy 464.6833801269531, Learning Rate: 0.00015625\n",
      "Epoch [10620/20000], Loss: 880.3868408203125, Entropy 455.2640380859375, Learning Rate: 0.00015625\n",
      "Epoch [10621/20000], Loss: 865.8562622070312, Entropy 446.59027099609375, Learning Rate: 0.00015625\n",
      "Epoch [10622/20000], Loss: 814.7405395507812, Entropy 448.71990966796875, Learning Rate: 0.00015625\n",
      "Epoch [10623/20000], Loss: 823.1903076171875, Entropy 465.308349609375, Learning Rate: 0.00015625\n",
      "Epoch [10624/20000], Loss: 893.5632934570312, Entropy 454.17413330078125, Learning Rate: 0.00015625\n",
      "Epoch [10625/20000], Loss: 848.9998168945312, Entropy 460.65716552734375, Learning Rate: 0.00015625\n",
      "Epoch [10626/20000], Loss: 901.135498046875, Entropy 468.4919738769531, Learning Rate: 0.00015625\n",
      "Epoch [10627/20000], Loss: 871.5703125, Entropy 448.5036315917969, Learning Rate: 0.00015625\n",
      "Epoch [10628/20000], Loss: 848.4013671875, Entropy 454.6099853515625, Learning Rate: 0.00015625\n",
      "Epoch [10629/20000], Loss: 833.2642822265625, Entropy 445.9417724609375, Learning Rate: 0.00015625\n",
      "Epoch [10630/20000], Loss: 899.2621459960938, Entropy 447.04107666015625, Learning Rate: 0.00015625\n",
      "Epoch [10631/20000], Loss: 850.7559814453125, Entropy 439.9293518066406, Learning Rate: 0.00015625\n",
      "Epoch [10632/20000], Loss: 874.5318603515625, Entropy 452.6512451171875, Learning Rate: 0.00015625\n",
      "Epoch [10633/20000], Loss: 896.74267578125, Entropy 433.8443603515625, Learning Rate: 0.00015625\n",
      "Epoch [10634/20000], Loss: 980.3990478515625, Entropy 445.0849914550781, Learning Rate: 0.00015625\n",
      "Epoch [10635/20000], Loss: 895.91650390625, Entropy 447.5333557128906, Learning Rate: 0.00015625\n",
      "Epoch [10636/20000], Loss: 909.5128173828125, Entropy 474.1603088378906, Learning Rate: 0.00015625\n",
      "Epoch [10637/20000], Loss: 866.4219970703125, Entropy 450.3922424316406, Learning Rate: 0.00015625\n",
      "Epoch [10638/20000], Loss: 829.2637939453125, Entropy 446.9714050292969, Learning Rate: 0.00015625\n",
      "Epoch [10639/20000], Loss: 854.444091796875, Entropy 443.3499755859375, Learning Rate: 0.00015625\n",
      "Epoch [10640/20000], Loss: 893.673583984375, Entropy 447.2012023925781, Learning Rate: 0.00015625\n",
      "Epoch [10641/20000], Loss: 789.9588623046875, Entropy 464.7666931152344, Learning Rate: 0.00015625\n",
      "Epoch [10642/20000], Loss: 813.753173828125, Entropy 445.6239013671875, Learning Rate: 0.00015625\n",
      "Epoch [10643/20000], Loss: 888.511962890625, Entropy 460.9732971191406, Learning Rate: 0.00015625\n",
      "Epoch [10644/20000], Loss: 851.1689453125, Entropy 447.45947265625, Learning Rate: 0.00015625\n",
      "Epoch [10645/20000], Loss: 816.330322265625, Entropy 444.1877136230469, Learning Rate: 0.00015625\n",
      "Epoch [10646/20000], Loss: 876.1309814453125, Entropy 455.7239685058594, Learning Rate: 0.00015625\n",
      "Epoch [10647/20000], Loss: 933.6600341796875, Entropy 438.3546447753906, Learning Rate: 0.00015625\n",
      "Epoch [10648/20000], Loss: 848.3433837890625, Entropy 433.9067077636719, Learning Rate: 0.00015625\n",
      "Epoch [10649/20000], Loss: 875.73291015625, Entropy 436.7828674316406, Learning Rate: 0.00015625\n",
      "Epoch [10650/20000], Loss: 830.2811889648438, Entropy 456.77789306640625, Learning Rate: 0.00015625\n",
      "Epoch [10651/20000], Loss: 891.6461791992188, Entropy 436.80023193359375, Learning Rate: 0.00015625\n",
      "Epoch [10652/20000], Loss: 859.9698486328125, Entropy 458.3638916015625, Learning Rate: 0.00015625\n",
      "Epoch [10653/20000], Loss: 840.8924560546875, Entropy 462.6387939453125, Learning Rate: 0.00015625\n",
      "Epoch [10654/20000], Loss: 867.0623779296875, Entropy 448.8951416015625, Learning Rate: 0.00015625\n",
      "Epoch [10655/20000], Loss: 842.9771728515625, Entropy 429.2532043457031, Learning Rate: 0.00015625\n",
      "Epoch [10656/20000], Loss: 897.6364135742188, Entropy 452.59381103515625, Learning Rate: 0.00015625\n",
      "Epoch [10657/20000], Loss: 860.0735473632812, Entropy 456.51263427734375, Learning Rate: 0.00015625\n",
      "Epoch [10658/20000], Loss: 850.7194213867188, Entropy 442.13568115234375, Learning Rate: 0.00015625\n",
      "Epoch [10659/20000], Loss: 853.4617919921875, Entropy 458.1575622558594, Learning Rate: 0.00015625\n",
      "Epoch [10660/20000], Loss: 881.8536376953125, Entropy 446.8853759765625, Learning Rate: 0.00015625\n",
      "Epoch [10661/20000], Loss: 883.789306640625, Entropy 446.5807189941406, Learning Rate: 0.00015625\n",
      "Epoch [10662/20000], Loss: 875.5174560546875, Entropy 458.1752624511719, Learning Rate: 0.00015625\n",
      "Epoch [10663/20000], Loss: 868.9400024414062, Entropy 445.46343994140625, Learning Rate: 0.00015625\n",
      "Epoch [10664/20000], Loss: 863.78515625, Entropy 462.87451171875, Learning Rate: 0.00015625\n",
      "Epoch [10665/20000], Loss: 861.3199462890625, Entropy 442.8011474609375, Learning Rate: 0.00015625\n",
      "Epoch [10666/20000], Loss: 813.7564697265625, Entropy 451.38232421875, Learning Rate: 0.00015625\n",
      "Epoch [10667/20000], Loss: 809.7503051757812, Entropy 451.44635009765625, Learning Rate: 0.00015625\n",
      "Epoch [10668/20000], Loss: 893.3264770507812, Entropy 443.67828369140625, Learning Rate: 0.00015625\n",
      "Epoch [10669/20000], Loss: 854.339111328125, Entropy 438.9580383300781, Learning Rate: 0.00015625\n",
      "Epoch [10670/20000], Loss: 825.411865234375, Entropy 450.83740234375, Learning Rate: 0.00015625\n",
      "Epoch [10671/20000], Loss: 852.7091674804688, Entropy 459.71209716796875, Learning Rate: 0.00015625\n",
      "Epoch [10672/20000], Loss: 826.5035400390625, Entropy 457.9412536621094, Learning Rate: 0.00015625\n",
      "Epoch [10673/20000], Loss: 837.994384765625, Entropy 453.0138854980469, Learning Rate: 0.00015625\n",
      "Epoch [10674/20000], Loss: 839.6735229492188, Entropy 451.38507080078125, Learning Rate: 0.00015625\n",
      "Epoch [10675/20000], Loss: 866.72412109375, Entropy 437.1673583984375, Learning Rate: 0.00015625\n",
      "Epoch [10676/20000], Loss: 886.178955078125, Entropy 451.4811706542969, Learning Rate: 0.00015625\n",
      "Epoch [10677/20000], Loss: 876.8418579101562, Entropy 453.45513916015625, Learning Rate: 0.00015625\n",
      "Epoch [10678/20000], Loss: 867.490966796875, Entropy 444.2719421386719, Learning Rate: 0.00015625\n",
      "Epoch [10679/20000], Loss: 822.338134765625, Entropy 444.6463317871094, Learning Rate: 0.00015625\n",
      "Epoch [10680/20000], Loss: 879.0533447265625, Entropy 455.4434814453125, Learning Rate: 0.00015625\n",
      "Epoch [10681/20000], Loss: 839.0823974609375, Entropy 460.6268005371094, Learning Rate: 0.00015625\n",
      "Epoch [10682/20000], Loss: 851.0230712890625, Entropy 457.6076354980469, Learning Rate: 0.00015625\n",
      "Epoch [10683/20000], Loss: 831.5878295898438, Entropy 457.16827392578125, Learning Rate: 0.00015625\n",
      "Epoch [10684/20000], Loss: 886.5185546875, Entropy 439.080810546875, Learning Rate: 0.00015625\n",
      "Epoch [10685/20000], Loss: 890.83984375, Entropy 446.9841613769531, Learning Rate: 0.00015625\n",
      "Epoch [10686/20000], Loss: 880.5948486328125, Entropy 450.5128173828125, Learning Rate: 0.00015625\n",
      "Epoch [10687/20000], Loss: 841.9860229492188, Entropy 443.34613037109375, Learning Rate: 0.00015625\n",
      "Epoch [10688/20000], Loss: 842.39599609375, Entropy 452.5604248046875, Learning Rate: 0.00015625\n",
      "Epoch [10689/20000], Loss: 865.36572265625, Entropy 448.493896484375, Learning Rate: 0.00015625\n",
      "Epoch [10690/20000], Loss: 881.1158447265625, Entropy 451.5047607421875, Learning Rate: 0.00015625\n",
      "Epoch [10691/20000], Loss: 849.7437744140625, Entropy 451.3434753417969, Learning Rate: 0.00015625\n",
      "Epoch [10692/20000], Loss: 846.1846313476562, Entropy 461.74029541015625, Learning Rate: 0.00015625\n",
      "Epoch [10693/20000], Loss: 917.2073974609375, Entropy 440.1944885253906, Learning Rate: 0.00015625\n",
      "Epoch [10694/20000], Loss: 825.3880004882812, Entropy 441.88592529296875, Learning Rate: 0.00015625\n",
      "Epoch [10695/20000], Loss: 882.54541015625, Entropy 435.8889465332031, Learning Rate: 0.00015625\n",
      "Epoch [10696/20000], Loss: 856.2244873046875, Entropy 450.8113098144531, Learning Rate: 0.00015625\n",
      "Epoch [10697/20000], Loss: 844.56591796875, Entropy 460.1533508300781, Learning Rate: 0.00015625\n",
      "Epoch [10698/20000], Loss: 847.8887939453125, Entropy 454.3740539550781, Learning Rate: 0.00015625\n",
      "Epoch [10699/20000], Loss: 857.7857666015625, Entropy 446.3232727050781, Learning Rate: 0.00015625\n",
      "Epoch [10700/20000], Loss: 852.8487548828125, Entropy 459.7899475097656, Learning Rate: 0.00015625\n",
      "Epoch [10701/20000], Loss: 830.1631469726562, Entropy 458.02203369140625, Learning Rate: 0.00015625\n",
      "Epoch [10702/20000], Loss: 887.4891357421875, Entropy 446.7356872558594, Learning Rate: 0.00015625\n",
      "Epoch [10703/20000], Loss: 825.7633056640625, Entropy 464.050537109375, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10704/20000], Loss: 895.4042358398438, Entropy 444.09100341796875, Learning Rate: 0.00015625\n",
      "Epoch [10705/20000], Loss: 895.6134033203125, Entropy 439.2917175292969, Learning Rate: 0.00015625\n",
      "Epoch [10706/20000], Loss: 828.1093139648438, Entropy 450.49029541015625, Learning Rate: 0.00015625\n",
      "Epoch [10707/20000], Loss: 865.0260620117188, Entropy 446.37127685546875, Learning Rate: 0.00015625\n",
      "Epoch [10708/20000], Loss: 845.8854370117188, Entropy 452.05035400390625, Learning Rate: 0.00015625\n",
      "Epoch [10709/20000], Loss: 914.9196166992188, Entropy 435.09906005859375, Learning Rate: 0.00015625\n",
      "Epoch [10710/20000], Loss: 890.65185546875, Entropy 455.6086730957031, Learning Rate: 0.00015625\n",
      "Epoch [10711/20000], Loss: 831.9232177734375, Entropy 440.0893249511719, Learning Rate: 0.00015625\n",
      "Epoch [10712/20000], Loss: 853.723388671875, Entropy 442.57421875, Learning Rate: 0.00015625\n",
      "Epoch [10713/20000], Loss: 842.1873779296875, Entropy 447.8963623046875, Learning Rate: 0.00015625\n",
      "Epoch [10714/20000], Loss: 885.2330322265625, Entropy 455.4413146972656, Learning Rate: 0.00015625\n",
      "Epoch [10715/20000], Loss: 866.985107421875, Entropy 443.9412841796875, Learning Rate: 0.00015625\n",
      "Epoch [10716/20000], Loss: 901.8121337890625, Entropy 453.2859802246094, Learning Rate: 0.00015625\n",
      "Epoch [10717/20000], Loss: 890.1658935546875, Entropy 441.234375, Learning Rate: 0.00015625\n",
      "Epoch [10718/20000], Loss: 866.2823486328125, Entropy 438.9684143066406, Learning Rate: 0.00015625\n",
      "Epoch [10719/20000], Loss: 888.86474609375, Entropy 454.4143371582031, Learning Rate: 0.00015625\n",
      "Epoch [10720/20000], Loss: 815.1632080078125, Entropy 446.3033447265625, Learning Rate: 0.00015625\n",
      "Epoch [10721/20000], Loss: 883.048095703125, Entropy 430.7052001953125, Learning Rate: 0.00015625\n",
      "Epoch [10722/20000], Loss: 860.880615234375, Entropy 451.81591796875, Learning Rate: 0.00015625\n",
      "Epoch [10723/20000], Loss: 855.7326049804688, Entropy 438.50921630859375, Learning Rate: 0.00015625\n",
      "Epoch [10724/20000], Loss: 851.2689819335938, Entropy 449.72686767578125, Learning Rate: 0.00015625\n",
      "Epoch [10725/20000], Loss: 861.5985107421875, Entropy 441.4852294921875, Learning Rate: 0.00015625\n",
      "Epoch [10726/20000], Loss: 825.2495727539062, Entropy 458.84857177734375, Learning Rate: 0.00015625\n",
      "Epoch [10727/20000], Loss: 894.000244140625, Entropy 457.56787109375, Learning Rate: 0.00015625\n",
      "Epoch [10728/20000], Loss: 907.6953125, Entropy 449.5509338378906, Learning Rate: 0.00015625\n",
      "Epoch [10729/20000], Loss: 893.195556640625, Entropy 442.1590576171875, Learning Rate: 0.00015625\n",
      "Epoch [10730/20000], Loss: 877.597900390625, Entropy 439.8536376953125, Learning Rate: 0.00015625\n",
      "Epoch [10731/20000], Loss: 873.7412719726562, Entropy 451.73870849609375, Learning Rate: 0.00015625\n",
      "Epoch [10732/20000], Loss: 905.3872680664062, Entropy 445.69390869140625, Learning Rate: 0.00015625\n",
      "Epoch [10733/20000], Loss: 851.99072265625, Entropy 444.4335021972656, Learning Rate: 0.00015625\n",
      "Epoch [10734/20000], Loss: 838.705810546875, Entropy 450.1418151855469, Learning Rate: 0.00015625\n",
      "Epoch [10735/20000], Loss: 842.3796997070312, Entropy 445.84600830078125, Learning Rate: 0.00015625\n",
      "Epoch [10736/20000], Loss: 881.2003173828125, Entropy 444.6885986328125, Learning Rate: 0.00015625\n",
      "Epoch [10737/20000], Loss: 854.470947265625, Entropy 454.9211730957031, Learning Rate: 0.00015625\n",
      "Epoch [10738/20000], Loss: 853.2928466796875, Entropy 447.1151123046875, Learning Rate: 0.00015625\n",
      "Epoch [10739/20000], Loss: 885.2073974609375, Entropy 449.4772644042969, Learning Rate: 0.00015625\n",
      "Epoch [10740/20000], Loss: 861.8470458984375, Entropy 460.1921081542969, Learning Rate: 0.00015625\n",
      "Epoch [10741/20000], Loss: 858.89697265625, Entropy 461.1942138671875, Learning Rate: 0.00015625\n",
      "Epoch [10742/20000], Loss: 876.22607421875, Entropy 457.1910705566406, Learning Rate: 0.00015625\n",
      "Epoch [10743/20000], Loss: 884.8499145507812, Entropy 461.52398681640625, Learning Rate: 0.00015625\n",
      "Epoch [10744/20000], Loss: 886.7333984375, Entropy 441.3669128417969, Learning Rate: 0.00015625\n",
      "Epoch [10745/20000], Loss: 833.8812255859375, Entropy 445.4929504394531, Learning Rate: 0.00015625\n",
      "Epoch [10746/20000], Loss: 865.2548828125, Entropy 443.7364807128906, Learning Rate: 0.00015625\n",
      "Epoch [10747/20000], Loss: 871.989501953125, Entropy 449.0848388671875, Learning Rate: 0.00015625\n",
      "Epoch [10748/20000], Loss: 900.0908203125, Entropy 420.6238098144531, Learning Rate: 0.00015625\n",
      "Epoch [10749/20000], Loss: 877.3983154296875, Entropy 452.2417907714844, Learning Rate: 0.00015625\n",
      "Epoch [10750/20000], Loss: 850.6435546875, Entropy 451.616943359375, Learning Rate: 0.00015625\n",
      "Epoch [10751/20000], Loss: 939.451904296875, Entropy 449.6700439453125, Learning Rate: 0.00015625\n",
      "Epoch [10752/20000], Loss: 890.2498168945312, Entropy 435.76104736328125, Learning Rate: 0.00015625\n",
      "Epoch [10753/20000], Loss: 898.0296020507812, Entropy 451.38922119140625, Learning Rate: 0.00015625\n",
      "Epoch [10754/20000], Loss: 849.7455444335938, Entropy 441.63018798828125, Learning Rate: 0.00015625\n",
      "Epoch [10755/20000], Loss: 852.6993408203125, Entropy 452.7863464355469, Learning Rate: 0.00015625\n",
      "Epoch [10756/20000], Loss: 897.728271484375, Entropy 456.6604309082031, Learning Rate: 0.00015625\n",
      "Epoch [10757/20000], Loss: 931.2562255859375, Entropy 444.3454895019531, Learning Rate: 0.00015625\n",
      "Epoch [10758/20000], Loss: 883.9952392578125, Entropy 442.8058166503906, Learning Rate: 0.00015625\n",
      "Epoch [10759/20000], Loss: 917.615234375, Entropy 433.3319396972656, Learning Rate: 0.00015625\n",
      "Epoch [10760/20000], Loss: 878.8934936523438, Entropy 455.95465087890625, Learning Rate: 0.00015625\n",
      "Epoch [10761/20000], Loss: 874.9262084960938, Entropy 436.02886962890625, Learning Rate: 0.00015625\n",
      "Epoch [10762/20000], Loss: 840.6593627929688, Entropy 443.68292236328125, Learning Rate: 0.00015625\n",
      "Epoch [10763/20000], Loss: 864.7924194335938, Entropy 444.10089111328125, Learning Rate: 0.00015625\n",
      "Epoch [10764/20000], Loss: 878.7078857421875, Entropy 439.5601501464844, Learning Rate: 0.00015625\n",
      "Epoch [10765/20000], Loss: 854.170166015625, Entropy 462.4813232421875, Learning Rate: 0.00015625\n",
      "Epoch [10766/20000], Loss: 837.2532958984375, Entropy 439.9588623046875, Learning Rate: 0.00015625\n",
      "Epoch [10767/20000], Loss: 874.7034912109375, Entropy 442.1817626953125, Learning Rate: 0.00015625\n",
      "Epoch [10768/20000], Loss: 862.7244873046875, Entropy 453.3247985839844, Learning Rate: 0.00015625\n",
      "Epoch [10769/20000], Loss: 875.192138671875, Entropy 451.5472412109375, Learning Rate: 0.00015625\n",
      "Epoch [10770/20000], Loss: 864.6658325195312, Entropy 432.79095458984375, Learning Rate: 0.00015625\n",
      "Epoch [10771/20000], Loss: 858.396484375, Entropy 437.1700744628906, Learning Rate: 0.00015625\n",
      "Epoch [10772/20000], Loss: 908.271728515625, Entropy 439.3243408203125, Learning Rate: 0.00015625\n",
      "Epoch [10773/20000], Loss: 875.58203125, Entropy 429.5950927734375, Learning Rate: 0.00015625\n",
      "Epoch [10774/20000], Loss: 895.3862915039062, Entropy 437.69842529296875, Learning Rate: 0.00015625\n",
      "Epoch [10775/20000], Loss: 867.0692138671875, Entropy 451.4010925292969, Learning Rate: 0.00015625\n",
      "Epoch [10776/20000], Loss: 882.849853515625, Entropy 443.8244323730469, Learning Rate: 0.00015625\n",
      "Epoch [10777/20000], Loss: 862.125244140625, Entropy 440.6314697265625, Learning Rate: 0.00015625\n",
      "Epoch [10778/20000], Loss: 903.0155029296875, Entropy 447.0202941894531, Learning Rate: 0.00015625\n",
      "Epoch [10779/20000], Loss: 937.3114013671875, Entropy 440.11767578125, Learning Rate: 0.00015625\n",
      "Epoch [10780/20000], Loss: 930.8472290039062, Entropy 452.72589111328125, Learning Rate: 0.00015625\n",
      "Epoch [10781/20000], Loss: 842.7933349609375, Entropy 452.0191345214844, Learning Rate: 0.00015625\n",
      "Epoch [10782/20000], Loss: 858.7447509765625, Entropy 459.52001953125, Learning Rate: 0.00015625\n",
      "Epoch [10783/20000], Loss: 832.739501953125, Entropy 440.6328125, Learning Rate: 0.00015625\n",
      "Epoch [10784/20000], Loss: 845.9993896484375, Entropy 443.1002197265625, Learning Rate: 0.00015625\n",
      "Epoch [10785/20000], Loss: 881.2251586914062, Entropy 459.08013916015625, Learning Rate: 0.00015625\n",
      "Epoch [10786/20000], Loss: 861.7476806640625, Entropy 443.8780517578125, Learning Rate: 0.00015625\n",
      "Epoch [10787/20000], Loss: 881.0565795898438, Entropy 440.65521240234375, Learning Rate: 0.00015625\n",
      "Epoch [10788/20000], Loss: 831.2049560546875, Entropy 453.4717102050781, Learning Rate: 0.00015625\n",
      "Epoch [10789/20000], Loss: 831.8902587890625, Entropy 456.3184814453125, Learning Rate: 0.00015625\n",
      "Epoch [10790/20000], Loss: 894.17919921875, Entropy 427.1346740722656, Learning Rate: 0.00015625\n",
      "Epoch [10791/20000], Loss: 824.957763671875, Entropy 465.1950378417969, Learning Rate: 0.00015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10792/20000], Loss: 884.6585693359375, Entropy 439.8266296386719, Learning Rate: 0.00015625\n",
      "Epoch [10793/20000], Loss: 876.844970703125, Entropy 462.5352783203125, Learning Rate: 0.00015625\n",
      "Epoch [10794/20000], Loss: 897.5086669921875, Entropy 444.4703674316406, Learning Rate: 0.00015625\n",
      "Epoch [10795/20000], Loss: 832.240966796875, Entropy 443.49609375, Learning Rate: 0.00015625\n",
      "Epoch [10796/20000], Loss: 868.7642822265625, Entropy 438.3567810058594, Learning Rate: 0.00015625\n",
      "Epoch [10797/20000], Loss: 856.3652954101562, Entropy 438.17095947265625, Learning Rate: 0.00015625\n",
      "Epoch [10798/20000], Loss: 838.5714111328125, Entropy 458.7750244140625, Learning Rate: 0.00015625\n",
      "Epoch [10799/20000], Loss: 893.9689331054688, Entropy 449.02850341796875, Learning Rate: 0.00015625\n",
      "Epoch [10800/20000], Loss: 858.88525390625, Entropy 438.3839111328125, Learning Rate: 0.00015625\n",
      "Epoch [10801/20000], Loss: 865.3046875, Entropy 452.8382568359375, Learning Rate: 0.00015625\n",
      "Epoch [10802/20000], Loss: 868.07958984375, Entropy 448.8251953125, Learning Rate: 0.00015625\n",
      "Epoch [10803/20000], Loss: 871.1797485351562, Entropy 433.31976318359375, Learning Rate: 0.00015625\n",
      "Epoch [10804/20000], Loss: 886.4326171875, Entropy 450.1301574707031, Learning Rate: 0.00015625\n",
      "Epoch [10805/20000], Loss: 851.4038696289062, Entropy 453.81280517578125, Learning Rate: 0.00015625\n",
      "Epoch [10806/20000], Loss: 832.7848510742188, Entropy 447.01861572265625, Learning Rate: 0.00015625\n",
      "Epoch [10807/20000], Loss: 854.7578125, Entropy 440.0582275390625, Learning Rate: 0.00015625\n",
      "Epoch [10808/20000], Loss: 855.66162109375, Entropy 459.37646484375, Learning Rate: 0.00015625\n",
      "Epoch [10809/20000], Loss: 848.3663330078125, Entropy 447.6951904296875, Learning Rate: 0.00015625\n",
      "Epoch [10810/20000], Loss: 842.9454345703125, Entropy 459.0121765136719, Learning Rate: 0.00015625\n",
      "Epoch [10811/20000], Loss: 898.4593505859375, Entropy 441.2228088378906, Learning Rate: 0.00015625\n",
      "Epoch [10812/20000], Loss: 847.2620239257812, Entropy 436.21771240234375, Learning Rate: 0.00015625\n",
      "Epoch [10813/20000], Loss: 899.195068359375, Entropy 445.8598327636719, Learning Rate: 0.00015625\n",
      "Epoch [10814/20000], Loss: 915.9954833984375, Entropy 441.3721923828125, Learning Rate: 0.00015625\n",
      "Epoch [10815/20000], Loss: 831.8426513671875, Entropy 443.2839660644531, Learning Rate: 0.00015625\n",
      "Epoch [10816/20000], Loss: 891.89404296875, Entropy 456.552490234375, Learning Rate: 0.00015625\n",
      "Epoch [10817/20000], Loss: 841.2928466796875, Entropy 453.8054504394531, Learning Rate: 0.00015625\n",
      "Epoch [10818/20000], Loss: 813.5647583007812, Entropy 454.75531005859375, Learning Rate: 0.00015625\n",
      "Epoch [10819/20000], Loss: 860.5474853515625, Entropy 451.2776184082031, Learning Rate: 0.00015625\n",
      "Epoch [10820/20000], Loss: 840.6378173828125, Entropy 451.8493957519531, Learning Rate: 0.00015625\n",
      "Epoch [10821/20000], Loss: 898.74072265625, Entropy 447.88330078125, Learning Rate: 0.00015625\n",
      "Epoch [10822/20000], Loss: 824.98681640625, Entropy 460.4347229003906, Learning Rate: 0.00015625\n",
      "Epoch [10823/20000], Loss: 856.5447387695312, Entropy 454.94329833984375, Learning Rate: 0.00015625\n",
      "Epoch [10824/20000], Loss: 867.3010864257812, Entropy 439.58258056640625, Learning Rate: 0.00015625\n",
      "Epoch [10825/20000], Loss: 875.9389038085938, Entropy 447.14642333984375, Learning Rate: 0.00015625\n",
      "Epoch [10826/20000], Loss: 859.4766845703125, Entropy 443.0319519042969, Learning Rate: 0.00015625\n",
      "Epoch [10827/20000], Loss: 886.343505859375, Entropy 450.472412109375, Learning Rate: 0.00015625\n",
      "Epoch [10828/20000], Loss: 881.1837158203125, Entropy 442.2342834472656, Learning Rate: 0.00015625\n",
      "Epoch [10829/20000], Loss: 888.5865478515625, Entropy 460.9981689453125, Learning Rate: 0.00015625\n",
      "Epoch [10830/20000], Loss: 854.66796875, Entropy 442.9215087890625, Learning Rate: 0.00015625\n",
      "Epoch [10831/20000], Loss: 842.0514526367188, Entropy 445.91888427734375, Learning Rate: 0.00015625\n",
      "Epoch [10832/20000], Loss: 832.9306030273438, Entropy 427.06939697265625, Learning Rate: 0.00015625\n",
      "Epoch [10833/20000], Loss: 876.4238891601562, Entropy 440.96185302734375, Learning Rate: 0.00015625\n",
      "Epoch [10834/20000], Loss: 857.9772338867188, Entropy 453.95428466796875, Learning Rate: 0.00015625\n",
      "Epoch [10835/20000], Loss: 851.3233642578125, Entropy 454.2122497558594, Learning Rate: 0.00015625\n",
      "Epoch [10836/20000], Loss: 836.6654663085938, Entropy 458.18304443359375, Learning Rate: 0.00015625\n",
      "Epoch [10837/20000], Loss: 789.7880859375, Entropy 449.0266418457031, Learning Rate: 0.00015625\n",
      "Epoch [10838/20000], Loss: 855.487548828125, Entropy 469.9036560058594, Learning Rate: 0.00015625\n",
      "Epoch [10839/20000], Loss: 920.1810913085938, Entropy 440.27093505859375, Learning Rate: 0.00015625\n",
      "Epoch [10840/20000], Loss: 865.5315551757812, Entropy 447.81341552734375, Learning Rate: 0.00015625\n",
      "Epoch [10841/20000], Loss: 822.38134765625, Entropy 463.4792175292969, Learning Rate: 0.00015625\n",
      "Epoch [10842/20000], Loss: 832.6143798828125, Entropy 450.3297119140625, Learning Rate: 0.00015625\n",
      "Epoch [10843/20000], Loss: 815.201171875, Entropy 468.1120300292969, Learning Rate: 0.00015625\n",
      "Epoch [10844/20000], Loss: 914.5641479492188, Entropy 448.80706787109375, Learning Rate: 0.00015625\n",
      "Epoch [10845/20000], Loss: 907.81103515625, Entropy 437.2286682128906, Learning Rate: 0.00015625\n",
      "Epoch [10846/20000], Loss: 829.897705078125, Entropy 464.2002258300781, Learning Rate: 0.00015625\n",
      "Epoch [10847/20000], Loss: 860.760009765625, Entropy 438.9787292480469, Learning Rate: 0.00015625\n",
      "Epoch [10848/20000], Loss: 879.74267578125, Entropy 450.8104553222656, Learning Rate: 0.00015625\n",
      "Epoch [10849/20000], Loss: 834.10302734375, Entropy 468.1744689941406, Learning Rate: 0.00015625\n",
      "Epoch [10850/20000], Loss: 847.4647216796875, Entropy 449.6047668457031, Learning Rate: 0.00015625\n",
      "Epoch [10851/20000], Loss: 882.5654296875, Entropy 448.722412109375, Learning Rate: 0.00015625\n",
      "Epoch [10852/20000], Loss: 845.5548095703125, Entropy 447.6885986328125, Learning Rate: 0.00015625\n",
      "Epoch [10853/20000], Loss: 893.9619140625, Entropy 447.6728820800781, Learning Rate: 0.00015625\n",
      "Epoch [10854/20000], Loss: 844.1476440429688, Entropy 448.59991455078125, Learning Rate: 0.00015625\n",
      "Epoch [10855/20000], Loss: 851.5336303710938, Entropy 434.61370849609375, Learning Rate: 0.00015625\n",
      "Epoch [10856/20000], Loss: 866.0506591796875, Entropy 455.0258483886719, Learning Rate: 0.00015625\n",
      "Epoch [10857/20000], Loss: 858.1514282226562, Entropy 435.25250244140625, Learning Rate: 0.00015625\n",
      "Epoch [10858/20000], Loss: 856.139892578125, Entropy 430.9250183105469, Learning Rate: 0.00015625\n",
      "Epoch [10859/20000], Loss: 848.0405883789062, Entropy 454.27642822265625, Learning Rate: 0.00015625\n",
      "Epoch [10860/20000], Loss: 865.56494140625, Entropy 465.6181945800781, Learning Rate: 0.00015625\n",
      "Epoch [10861/20000], Loss: 886.849609375, Entropy 444.1081237792969, Learning Rate: 0.00015625\n",
      "Epoch [10862/20000], Loss: 858.720703125, Entropy 453.1790466308594, Learning Rate: 0.00015625\n",
      "Epoch [10863/20000], Loss: 861.6397705078125, Entropy 447.08447265625, Learning Rate: 0.00015625\n",
      "Epoch [10864/20000], Loss: 832.50830078125, Entropy 455.4427185058594, Learning Rate: 0.00015625\n",
      "Epoch [10865/20000], Loss: 842.7882080078125, Entropy 459.5528869628906, Learning Rate: 0.00015625\n",
      "Epoch [10866/20000], Loss: 880.22509765625, Entropy 426.3658142089844, Learning Rate: 0.00015625\n",
      "Epoch [10867/20000], Loss: 855.94921875, Entropy 457.8153076171875, Learning Rate: 0.00015625\n",
      "Epoch [10868/20000], Loss: 863.06884765625, Entropy 447.5260314941406, Learning Rate: 0.00015625\n",
      "Epoch [10869/20000], Loss: 872.5582275390625, Entropy 453.4910888671875, Learning Rate: 0.00015625\n",
      "Epoch [10870/20000], Loss: 828.16455078125, Entropy 468.7506408691406, Learning Rate: 0.00015625\n",
      "Epoch [10871/20000], Loss: 821.17041015625, Entropy 457.6621398925781, Learning Rate: 0.00015625\n",
      "Epoch [10872/20000], Loss: 842.8304443359375, Entropy 472.0706787109375, Learning Rate: 0.00015625\n",
      "Epoch [10873/20000], Loss: 818.28125, Entropy 450.1424865722656, Learning Rate: 0.00015625\n",
      "Epoch [10874/20000], Loss: 865.2164306640625, Entropy 449.6686096191406, Learning Rate: 7.8125e-05\n",
      "Epoch [10875/20000], Loss: 832.853515625, Entropy 445.3938903808594, Learning Rate: 7.8125e-05\n",
      "Epoch [10876/20000], Loss: 837.51611328125, Entropy 465.6942138671875, Learning Rate: 7.8125e-05\n",
      "Epoch [10877/20000], Loss: 848.2149658203125, Entropy 432.5061340332031, Learning Rate: 7.8125e-05\n",
      "Epoch [10878/20000], Loss: 875.25146484375, Entropy 437.4389343261719, Learning Rate: 7.8125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10879/20000], Loss: 867.4642944335938, Entropy 447.57818603515625, Learning Rate: 7.8125e-05\n",
      "Epoch [10880/20000], Loss: 864.42822265625, Entropy 436.8455810546875, Learning Rate: 7.8125e-05\n",
      "Epoch [10881/20000], Loss: 923.4874877929688, Entropy 459.47369384765625, Learning Rate: 7.8125e-05\n",
      "Epoch [10882/20000], Loss: 861.539794921875, Entropy 442.1188049316406, Learning Rate: 7.8125e-05\n",
      "Epoch [10883/20000], Loss: 840.47119140625, Entropy 454.02294921875, Learning Rate: 7.8125e-05\n",
      "Epoch [10884/20000], Loss: 859.4448852539062, Entropy 441.14178466796875, Learning Rate: 7.8125e-05\n",
      "Epoch [10885/20000], Loss: 836.0743408203125, Entropy 457.4387512207031, Learning Rate: 7.8125e-05\n",
      "Epoch [10886/20000], Loss: 904.675048828125, Entropy 442.234130859375, Learning Rate: 7.8125e-05\n",
      "Epoch [10887/20000], Loss: 832.5899658203125, Entropy 439.4527282714844, Learning Rate: 7.8125e-05\n",
      "Epoch [10888/20000], Loss: 866.3486328125, Entropy 446.6163330078125, Learning Rate: 7.8125e-05\n",
      "Epoch [10889/20000], Loss: 808.660400390625, Entropy 455.1060485839844, Learning Rate: 7.8125e-05\n",
      "Epoch [10890/20000], Loss: 810.63330078125, Entropy 442.7117919921875, Learning Rate: 7.8125e-05\n",
      "Epoch [10891/20000], Loss: 793.95703125, Entropy 449.0373229980469, Learning Rate: 7.8125e-05\n",
      "Epoch [10892/20000], Loss: 867.3472900390625, Entropy 441.0979309082031, Learning Rate: 7.8125e-05\n",
      "Epoch [10893/20000], Loss: 816.890380859375, Entropy 448.9831848144531, Learning Rate: 7.8125e-05\n",
      "Epoch [10894/20000], Loss: 824.7723388671875, Entropy 460.6829528808594, Learning Rate: 7.8125e-05\n",
      "Epoch [10895/20000], Loss: 867.11083984375, Entropy 439.8292236328125, Learning Rate: 7.8125e-05\n",
      "Epoch [10896/20000], Loss: 913.120849609375, Entropy 448.4580383300781, Learning Rate: 7.8125e-05\n",
      "Epoch [10897/20000], Loss: 893.6381225585938, Entropy 442.58929443359375, Learning Rate: 7.8125e-05\n",
      "Epoch [10898/20000], Loss: 902.6717529296875, Entropy 445.3641662597656, Learning Rate: 7.8125e-05\n",
      "Epoch [10899/20000], Loss: 902.3399658203125, Entropy 437.5502624511719, Learning Rate: 7.8125e-05\n",
      "Epoch [10900/20000], Loss: 855.6922607421875, Entropy 457.3653869628906, Learning Rate: 7.8125e-05\n",
      "Epoch [10901/20000], Loss: 794.4437866210938, Entropy 445.79779052734375, Learning Rate: 7.8125e-05\n",
      "Epoch [10902/20000], Loss: 875.9951782226562, Entropy 444.93121337890625, Learning Rate: 7.8125e-05\n",
      "Epoch [10903/20000], Loss: 876.921142578125, Entropy 436.4909973144531, Learning Rate: 7.8125e-05\n",
      "Epoch [10904/20000], Loss: 859.38330078125, Entropy 431.4229736328125, Learning Rate: 7.8125e-05\n",
      "Epoch [10905/20000], Loss: 864.0098876953125, Entropy 453.7722473144531, Learning Rate: 7.8125e-05\n",
      "Epoch [10906/20000], Loss: 876.4481201171875, Entropy 449.9806213378906, Learning Rate: 7.8125e-05\n",
      "Epoch [10907/20000], Loss: 882.8944091796875, Entropy 456.9459228515625, Learning Rate: 7.8125e-05\n",
      "Epoch [10908/20000], Loss: 861.7291259765625, Entropy 456.4195556640625, Learning Rate: 7.8125e-05\n",
      "Epoch [10909/20000], Loss: 890.275390625, Entropy 449.3097229003906, Learning Rate: 7.8125e-05\n",
      "Epoch [10910/20000], Loss: 855.5375366210938, Entropy 459.43072509765625, Learning Rate: 7.8125e-05\n",
      "Epoch [10911/20000], Loss: 857.4930419921875, Entropy 451.7639465332031, Learning Rate: 7.8125e-05\n",
      "Epoch [10912/20000], Loss: 870.5068359375, Entropy 464.0902099609375, Learning Rate: 7.8125e-05\n",
      "Epoch [10913/20000], Loss: 878.08251953125, Entropy 448.0867919921875, Learning Rate: 7.8125e-05\n",
      "Epoch [10914/20000], Loss: 868.4458618164062, Entropy 440.69537353515625, Learning Rate: 7.8125e-05\n",
      "Epoch [10915/20000], Loss: 806.07421875, Entropy 452.72314453125, Learning Rate: 7.8125e-05\n",
      "Epoch [10916/20000], Loss: 858.7499389648438, Entropy 442.16156005859375, Learning Rate: 7.8125e-05\n",
      "Epoch [10917/20000], Loss: 851.6158447265625, Entropy 451.3626403808594, Learning Rate: 7.8125e-05\n",
      "Epoch [10918/20000], Loss: 855.8671875, Entropy 458.7016296386719, Learning Rate: 7.8125e-05\n",
      "Epoch [10919/20000], Loss: 849.8165283203125, Entropy 453.5639343261719, Learning Rate: 7.8125e-05\n",
      "Epoch [10920/20000], Loss: 847.0089111328125, Entropy 450.8617858886719, Learning Rate: 7.8125e-05\n",
      "Epoch [10921/20000], Loss: 830.1025390625, Entropy 449.7967834472656, Learning Rate: 7.8125e-05\n",
      "Epoch [10922/20000], Loss: 837.2073974609375, Entropy 457.7640380859375, Learning Rate: 7.8125e-05\n",
      "Epoch [10923/20000], Loss: 837.2334594726562, Entropy 437.51080322265625, Learning Rate: 7.8125e-05\n",
      "Epoch [10924/20000], Loss: 907.9326171875, Entropy 447.3876953125, Learning Rate: 7.8125e-05\n",
      "Epoch [10925/20000], Loss: 880.2576904296875, Entropy 431.5018005371094, Learning Rate: 7.8125e-05\n",
      "Epoch [10926/20000], Loss: 931.100830078125, Entropy 456.5328674316406, Learning Rate: 7.8125e-05\n",
      "Epoch [10927/20000], Loss: 876.39892578125, Entropy 435.224609375, Learning Rate: 7.8125e-05\n",
      "Epoch [10928/20000], Loss: 887.349609375, Entropy 441.6799011230469, Learning Rate: 7.8125e-05\n",
      "Epoch [10929/20000], Loss: 866.3658447265625, Entropy 461.9684143066406, Learning Rate: 7.8125e-05\n",
      "Epoch [10930/20000], Loss: 888.1505126953125, Entropy 431.6175231933594, Learning Rate: 7.8125e-05\n",
      "Epoch [10931/20000], Loss: 862.949951171875, Entropy 457.6724853515625, Learning Rate: 7.8125e-05\n",
      "Epoch [10932/20000], Loss: 856.9892578125, Entropy 465.9478759765625, Learning Rate: 7.8125e-05\n",
      "Epoch [10933/20000], Loss: 866.7794799804688, Entropy 463.69293212890625, Learning Rate: 7.8125e-05\n",
      "Epoch [10934/20000], Loss: 864.2906494140625, Entropy 454.1936950683594, Learning Rate: 7.8125e-05\n",
      "Epoch [10935/20000], Loss: 862.5016479492188, Entropy 451.91607666015625, Learning Rate: 7.8125e-05\n",
      "Epoch [10936/20000], Loss: 805.6646118164062, Entropy 454.79779052734375, Learning Rate: 7.8125e-05\n",
      "Epoch [10937/20000], Loss: 868.4002685546875, Entropy 452.0451354980469, Learning Rate: 7.8125e-05\n",
      "Epoch [10938/20000], Loss: 867.5872802734375, Entropy 444.2754211425781, Learning Rate: 7.8125e-05\n",
      "Epoch [10939/20000], Loss: 878.5818481445312, Entropy 471.28326416015625, Learning Rate: 7.8125e-05\n",
      "Epoch [10940/20000], Loss: 871.802734375, Entropy 447.930419921875, Learning Rate: 7.8125e-05\n",
      "Epoch [10941/20000], Loss: 836.49267578125, Entropy 446.6738586425781, Learning Rate: 7.8125e-05\n",
      "Epoch [10942/20000], Loss: 859.25146484375, Entropy 448.8811340332031, Learning Rate: 7.8125e-05\n",
      "Epoch [10943/20000], Loss: 860.4415283203125, Entropy 460.5784912109375, Learning Rate: 7.8125e-05\n",
      "Epoch [10944/20000], Loss: 875.7190551757812, Entropy 446.41326904296875, Learning Rate: 7.8125e-05\n",
      "Epoch [10945/20000], Loss: 816.9364013671875, Entropy 458.9025573730469, Learning Rate: 7.8125e-05\n",
      "Epoch [10946/20000], Loss: 813.7279052734375, Entropy 450.8659362792969, Learning Rate: 7.8125e-05\n",
      "Epoch [10947/20000], Loss: 919.6102294921875, Entropy 444.8143005371094, Learning Rate: 7.8125e-05\n",
      "Epoch [10948/20000], Loss: 862.0564575195312, Entropy 454.05169677734375, Learning Rate: 7.8125e-05\n",
      "Epoch [10949/20000], Loss: 840.8837890625, Entropy 461.1784973144531, Learning Rate: 7.8125e-05\n",
      "Epoch [10950/20000], Loss: 886.733642578125, Entropy 442.6549377441406, Learning Rate: 7.8125e-05\n",
      "Epoch [10951/20000], Loss: 877.08251953125, Entropy 445.1049499511719, Learning Rate: 7.8125e-05\n",
      "Epoch [10952/20000], Loss: 906.1875, Entropy 445.7020263671875, Learning Rate: 7.8125e-05\n",
      "Epoch [10953/20000], Loss: 793.7081298828125, Entropy 446.0696716308594, Learning Rate: 7.8125e-05\n",
      "Epoch [10954/20000], Loss: 844.67431640625, Entropy 450.2608947753906, Learning Rate: 7.8125e-05\n",
      "Epoch [10955/20000], Loss: 878.8031005859375, Entropy 435.3614807128906, Learning Rate: 7.8125e-05\n",
      "Epoch [10956/20000], Loss: 823.93896484375, Entropy 448.4110107421875, Learning Rate: 7.8125e-05\n",
      "Epoch [10957/20000], Loss: 797.3358154296875, Entropy 451.5611572265625, Learning Rate: 7.8125e-05\n",
      "Epoch [10958/20000], Loss: 866.4993896484375, Entropy 452.7330322265625, Learning Rate: 7.8125e-05\n",
      "Epoch [10959/20000], Loss: 855.2022705078125, Entropy 469.1343688964844, Learning Rate: 7.8125e-05\n",
      "Epoch [10960/20000], Loss: 890.864013671875, Entropy 455.0909423828125, Learning Rate: 7.8125e-05\n",
      "Epoch [10961/20000], Loss: 864.460205078125, Entropy 454.14794921875, Learning Rate: 7.8125e-05\n",
      "Epoch [10962/20000], Loss: 814.1475830078125, Entropy 457.8313903808594, Learning Rate: 7.8125e-05\n",
      "Epoch [10963/20000], Loss: 863.3526611328125, Entropy 439.609375, Learning Rate: 7.8125e-05\n",
      "Epoch [10964/20000], Loss: 869.9681396484375, Entropy 445.0845642089844, Learning Rate: 7.8125e-05\n",
      "Epoch [10965/20000], Loss: 869.729736328125, Entropy 436.4717102050781, Learning Rate: 7.8125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10966/20000], Loss: 851.8838500976562, Entropy 453.65289306640625, Learning Rate: 7.8125e-05\n",
      "Epoch [10967/20000], Loss: 875.0594482421875, Entropy 440.2205505371094, Learning Rate: 7.8125e-05\n",
      "Epoch [10968/20000], Loss: 926.0745849609375, Entropy 450.9845275878906, Learning Rate: 7.8125e-05\n",
      "Epoch [10969/20000], Loss: 844.4212036132812, Entropy 453.74163818359375, Learning Rate: 7.8125e-05\n",
      "Epoch [10970/20000], Loss: 844.926025390625, Entropy 428.0164794921875, Learning Rate: 7.8125e-05\n",
      "Epoch [10971/20000], Loss: 888.181640625, Entropy 456.8531799316406, Learning Rate: 7.8125e-05\n",
      "Epoch [10972/20000], Loss: 844.8189697265625, Entropy 455.8249206542969, Learning Rate: 7.8125e-05\n",
      "Epoch [10973/20000], Loss: 860.3115234375, Entropy 441.4266662597656, Learning Rate: 7.8125e-05\n",
      "Epoch [10974/20000], Loss: 902.18798828125, Entropy 446.1558532714844, Learning Rate: 7.8125e-05\n",
      "Epoch [10975/20000], Loss: 879.2718505859375, Entropy 462.7797546386719, Learning Rate: 7.8125e-05\n",
      "Epoch [10976/20000], Loss: 855.7740478515625, Entropy 456.2127990722656, Learning Rate: 7.8125e-05\n",
      "Epoch [10977/20000], Loss: 878.4205322265625, Entropy 442.3893737792969, Learning Rate: 7.8125e-05\n",
      "Epoch [10978/20000], Loss: 836.438232421875, Entropy 457.45654296875, Learning Rate: 7.8125e-05\n",
      "Epoch [10979/20000], Loss: 863.1017456054688, Entropy 450.01995849609375, Learning Rate: 7.8125e-05\n",
      "Epoch [10980/20000], Loss: 834.01953125, Entropy 442.2037353515625, Learning Rate: 7.8125e-05\n",
      "Epoch [10981/20000], Loss: 871.9822998046875, Entropy 455.4842224121094, Learning Rate: 7.8125e-05\n",
      "Epoch [10982/20000], Loss: 799.106689453125, Entropy 469.3595275878906, Learning Rate: 7.8125e-05\n",
      "Epoch [10983/20000], Loss: 823.3318481445312, Entropy 452.37982177734375, Learning Rate: 7.8125e-05\n",
      "Epoch [10984/20000], Loss: 850.9830322265625, Entropy 444.4577941894531, Learning Rate: 7.8125e-05\n",
      "Epoch [10985/20000], Loss: 870.9212036132812, Entropy 434.20428466796875, Learning Rate: 7.8125e-05\n",
      "Epoch [10986/20000], Loss: 825.75341796875, Entropy 449.6564636230469, Learning Rate: 7.8125e-05\n",
      "Epoch [10987/20000], Loss: 874.528564453125, Entropy 444.5435791015625, Learning Rate: 7.8125e-05\n",
      "Epoch [10988/20000], Loss: 849.4537963867188, Entropy 438.59271240234375, Learning Rate: 7.8125e-05\n",
      "Epoch [10989/20000], Loss: 861.354248046875, Entropy 450.925537109375, Learning Rate: 7.8125e-05\n",
      "Epoch [10990/20000], Loss: 857.8748779296875, Entropy 449.2449035644531, Learning Rate: 7.8125e-05\n",
      "Epoch [10991/20000], Loss: 841.4762573242188, Entropy 464.76483154296875, Learning Rate: 7.8125e-05\n",
      "Epoch [10992/20000], Loss: 873.4505615234375, Entropy 441.8508605957031, Learning Rate: 7.8125e-05\n",
      "Epoch [10993/20000], Loss: 836.41015625, Entropy 432.6927185058594, Learning Rate: 7.8125e-05\n",
      "Epoch [10994/20000], Loss: 932.416748046875, Entropy 429.4213562011719, Learning Rate: 7.8125e-05\n",
      "Epoch [10995/20000], Loss: 867.7318115234375, Entropy 448.8362121582031, Learning Rate: 7.8125e-05\n",
      "Epoch [10996/20000], Loss: 910.603515625, Entropy 455.6639404296875, Learning Rate: 7.8125e-05\n",
      "Epoch [10997/20000], Loss: 919.5916137695312, Entropy 451.36614990234375, Learning Rate: 7.8125e-05\n",
      "Epoch [10998/20000], Loss: 886.093017578125, Entropy 450.787841796875, Learning Rate: 7.8125e-05\n",
      "Epoch [10999/20000], Loss: 863.72509765625, Entropy 458.3567199707031, Learning Rate: 7.8125e-05\n",
      "Epoch [11000/20000], Loss: 910.0316162109375, Entropy 435.2977600097656, Learning Rate: 7.8125e-05\n",
      "Epoch [11001/20000], Loss: 874.951416015625, Entropy 459.0782775878906, Learning Rate: 7.8125e-05\n",
      "Epoch [11002/20000], Loss: 839.0316162109375, Entropy 442.1801452636719, Learning Rate: 7.8125e-05\n",
      "Epoch [11003/20000], Loss: 869.140869140625, Entropy 447.1684265136719, Learning Rate: 7.8125e-05\n",
      "Epoch [11004/20000], Loss: 861.828857421875, Entropy 451.26171875, Learning Rate: 7.8125e-05\n",
      "Epoch [11005/20000], Loss: 847.1799926757812, Entropy 454.53253173828125, Learning Rate: 7.8125e-05\n",
      "Epoch [11006/20000], Loss: 870.501220703125, Entropy 465.2002258300781, Learning Rate: 7.8125e-05\n",
      "Epoch [11007/20000], Loss: 843.5625610351562, Entropy 456.60272216796875, Learning Rate: 7.8125e-05\n",
      "Epoch [11008/20000], Loss: 918.7747802734375, Entropy 440.1044006347656, Learning Rate: 7.8125e-05\n",
      "Epoch [11009/20000], Loss: 869.795166015625, Entropy 456.8677978515625, Learning Rate: 7.8125e-05\n",
      "Epoch [11010/20000], Loss: 874.1080322265625, Entropy 454.7060546875, Learning Rate: 7.8125e-05\n",
      "Epoch [11011/20000], Loss: 887.5068359375, Entropy 460.9631042480469, Learning Rate: 7.8125e-05\n",
      "Epoch [11012/20000], Loss: 846.664306640625, Entropy 444.5435485839844, Learning Rate: 7.8125e-05\n",
      "Epoch [11013/20000], Loss: 861.2484130859375, Entropy 442.6391296386719, Learning Rate: 7.8125e-05\n",
      "Epoch [11014/20000], Loss: 827.9342041015625, Entropy 439.1492004394531, Learning Rate: 7.8125e-05\n",
      "Epoch [11015/20000], Loss: 884.29541015625, Entropy 431.7510070800781, Learning Rate: 7.8125e-05\n",
      "Epoch [11016/20000], Loss: 838.67431640625, Entropy 466.105224609375, Learning Rate: 7.8125e-05\n",
      "Epoch [11017/20000], Loss: 883.2430419921875, Entropy 440.1628723144531, Learning Rate: 7.8125e-05\n",
      "Epoch [11018/20000], Loss: 863.919677734375, Entropy 454.7618713378906, Learning Rate: 7.8125e-05\n",
      "Epoch [11019/20000], Loss: 864.025146484375, Entropy 442.9723205566406, Learning Rate: 7.8125e-05\n",
      "Epoch [11020/20000], Loss: 863.894287109375, Entropy 453.6828918457031, Learning Rate: 7.8125e-05\n",
      "Epoch [11021/20000], Loss: 881.298095703125, Entropy 453.7592468261719, Learning Rate: 7.8125e-05\n",
      "Epoch [11022/20000], Loss: 866.139404296875, Entropy 444.5138854980469, Learning Rate: 7.8125e-05\n",
      "Epoch [11023/20000], Loss: 851.1893310546875, Entropy 452.75146484375, Learning Rate: 7.8125e-05\n",
      "Epoch [11024/20000], Loss: 885.9571533203125, Entropy 455.0367431640625, Learning Rate: 7.8125e-05\n",
      "Epoch [11025/20000], Loss: 904.8460083007812, Entropy 453.23944091796875, Learning Rate: 7.8125e-05\n",
      "Epoch [11026/20000], Loss: 875.513427734375, Entropy 449.2624816894531, Learning Rate: 7.8125e-05\n",
      "Epoch [11027/20000], Loss: 869.0339965820312, Entropy 469.51177978515625, Learning Rate: 7.8125e-05\n",
      "Epoch [11028/20000], Loss: 879.28515625, Entropy 458.8224792480469, Learning Rate: 7.8125e-05\n",
      "Epoch [11029/20000], Loss: 813.2611083984375, Entropy 454.08935546875, Learning Rate: 7.8125e-05\n",
      "Epoch [11030/20000], Loss: 868.3856201171875, Entropy 450.2610778808594, Learning Rate: 7.8125e-05\n",
      "Epoch [11031/20000], Loss: 857.5372314453125, Entropy 450.6344299316406, Learning Rate: 7.8125e-05\n",
      "Epoch [11032/20000], Loss: 859.87451171875, Entropy 437.0667724609375, Learning Rate: 7.8125e-05\n",
      "Epoch [11033/20000], Loss: 851.8179931640625, Entropy 442.2825622558594, Learning Rate: 7.8125e-05\n",
      "Epoch [11034/20000], Loss: 894.6923828125, Entropy 466.7575378417969, Learning Rate: 7.8125e-05\n",
      "Epoch [11035/20000], Loss: 922.5122680664062, Entropy 437.34014892578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11036/20000], Loss: 807.895263671875, Entropy 466.2807312011719, Learning Rate: 7.8125e-05\n",
      "Epoch [11037/20000], Loss: 834.6651000976562, Entropy 449.99639892578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11038/20000], Loss: 907.0802001953125, Entropy 445.8200988769531, Learning Rate: 7.8125e-05\n",
      "Epoch [11039/20000], Loss: 908.5416259765625, Entropy 445.3489990234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11040/20000], Loss: 852.4178466796875, Entropy 451.6903076171875, Learning Rate: 7.8125e-05\n",
      "Epoch [11041/20000], Loss: 821.6135864257812, Entropy 461.17413330078125, Learning Rate: 7.8125e-05\n",
      "Epoch [11042/20000], Loss: 837.7805786132812, Entropy 449.73541259765625, Learning Rate: 7.8125e-05\n",
      "Epoch [11043/20000], Loss: 873.8673095703125, Entropy 438.9862365722656, Learning Rate: 7.8125e-05\n",
      "Epoch [11044/20000], Loss: 892.6773071289062, Entropy 454.57745361328125, Learning Rate: 7.8125e-05\n",
      "Epoch [11045/20000], Loss: 818.8179931640625, Entropy 437.1090393066406, Learning Rate: 7.8125e-05\n",
      "Epoch [11046/20000], Loss: 872.015380859375, Entropy 436.5677490234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11047/20000], Loss: 858.6285400390625, Entropy 458.2082824707031, Learning Rate: 7.8125e-05\n",
      "Epoch [11048/20000], Loss: 899.141357421875, Entropy 449.3870849609375, Learning Rate: 7.8125e-05\n",
      "Epoch [11049/20000], Loss: 868.7758178710938, Entropy 449.48944091796875, Learning Rate: 7.8125e-05\n",
      "Epoch [11050/20000], Loss: 905.878173828125, Entropy 455.7630615234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11051/20000], Loss: 872.9573974609375, Entropy 445.0647888183594, Learning Rate: 7.8125e-05\n",
      "Epoch [11052/20000], Loss: 902.791259765625, Entropy 443.8367919921875, Learning Rate: 7.8125e-05\n",
      "Epoch [11053/20000], Loss: 891.9974365234375, Entropy 451.9029541015625, Learning Rate: 7.8125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11054/20000], Loss: 906.3519287109375, Entropy 449.0452880859375, Learning Rate: 7.8125e-05\n",
      "Epoch [11055/20000], Loss: 836.79638671875, Entropy 445.8040771484375, Learning Rate: 7.8125e-05\n",
      "Epoch [11056/20000], Loss: 926.7489013671875, Entropy 448.4588317871094, Learning Rate: 7.8125e-05\n",
      "Epoch [11057/20000], Loss: 882.49169921875, Entropy 444.9052734375, Learning Rate: 7.8125e-05\n",
      "Epoch [11058/20000], Loss: 933.6394653320312, Entropy 448.15875244140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11059/20000], Loss: 860.7648315429688, Entropy 446.64093017578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11060/20000], Loss: 870.1478271484375, Entropy 446.9064025878906, Learning Rate: 7.8125e-05\n",
      "Epoch [11061/20000], Loss: 823.7120361328125, Entropy 459.3114318847656, Learning Rate: 7.8125e-05\n",
      "Epoch [11062/20000], Loss: 823.0978393554688, Entropy 460.90570068359375, Learning Rate: 7.8125e-05\n",
      "Epoch [11063/20000], Loss: 866.4043579101562, Entropy 446.17889404296875, Learning Rate: 7.8125e-05\n",
      "Epoch [11064/20000], Loss: 856.340576171875, Entropy 465.5334167480469, Learning Rate: 7.8125e-05\n",
      "Epoch [11065/20000], Loss: 897.8517456054688, Entropy 444.59698486328125, Learning Rate: 7.8125e-05\n",
      "Epoch [11066/20000], Loss: 880.0179443359375, Entropy 449.4388122558594, Learning Rate: 7.8125e-05\n",
      "Epoch [11067/20000], Loss: 845.86669921875, Entropy 451.2708740234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11068/20000], Loss: 909.3045654296875, Entropy 440.9986572265625, Learning Rate: 7.8125e-05\n",
      "Epoch [11069/20000], Loss: 862.4522705078125, Entropy 462.3670959472656, Learning Rate: 7.8125e-05\n",
      "Epoch [11070/20000], Loss: 842.4278564453125, Entropy 449.7543640136719, Learning Rate: 7.8125e-05\n",
      "Epoch [11071/20000], Loss: 894.9932861328125, Entropy 459.4485778808594, Learning Rate: 7.8125e-05\n",
      "Epoch [11072/20000], Loss: 857.9739990234375, Entropy 455.1890869140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11073/20000], Loss: 849.4625244140625, Entropy 432.5509338378906, Learning Rate: 7.8125e-05\n",
      "Epoch [11074/20000], Loss: 912.4898071289062, Entropy 444.62457275390625, Learning Rate: 7.8125e-05\n",
      "Epoch [11075/20000], Loss: 821.6366577148438, Entropy 446.58099365234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11076/20000], Loss: 846.1533813476562, Entropy 456.60784912109375, Learning Rate: 7.8125e-05\n",
      "Epoch [11077/20000], Loss: 801.5077514648438, Entropy 468.74810791015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11078/20000], Loss: 860.2421875, Entropy 447.1849060058594, Learning Rate: 7.8125e-05\n",
      "Epoch [11079/20000], Loss: 821.1622314453125, Entropy 458.2986755371094, Learning Rate: 7.8125e-05\n",
      "Epoch [11080/20000], Loss: 823.2973022460938, Entropy 461.90045166015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11081/20000], Loss: 855.941650390625, Entropy 454.4330749511719, Learning Rate: 7.8125e-05\n",
      "Epoch [11082/20000], Loss: 882.07373046875, Entropy 442.903076171875, Learning Rate: 7.8125e-05\n",
      "Epoch [11083/20000], Loss: 886.4567260742188, Entropy 455.69451904296875, Learning Rate: 7.8125e-05\n",
      "Epoch [11084/20000], Loss: 876.2205810546875, Entropy 445.2231140136719, Learning Rate: 7.8125e-05\n",
      "Epoch [11085/20000], Loss: 813.7515258789062, Entropy 450.82476806640625, Learning Rate: 7.8125e-05\n",
      "Epoch [11086/20000], Loss: 854.716064453125, Entropy 448.0928039550781, Learning Rate: 7.8125e-05\n",
      "Epoch [11087/20000], Loss: 823.574462890625, Entropy 452.14697265625, Learning Rate: 7.8125e-05\n",
      "Epoch [11088/20000], Loss: 944.4005126953125, Entropy 429.8913269042969, Learning Rate: 7.8125e-05\n",
      "Epoch [11089/20000], Loss: 863.81103515625, Entropy 448.2883605957031, Learning Rate: 7.8125e-05\n",
      "Epoch [11090/20000], Loss: 840.5062255859375, Entropy 449.0066223144531, Learning Rate: 7.8125e-05\n",
      "Epoch [11091/20000], Loss: 834.0570068359375, Entropy 451.7790832519531, Learning Rate: 7.8125e-05\n",
      "Epoch [11092/20000], Loss: 882.9921875, Entropy 432.413818359375, Learning Rate: 7.8125e-05\n",
      "Epoch [11093/20000], Loss: 856.833251953125, Entropy 448.4633483886719, Learning Rate: 7.8125e-05\n",
      "Epoch [11094/20000], Loss: 896.5037841796875, Entropy 471.0459289550781, Learning Rate: 7.8125e-05\n",
      "Epoch [11095/20000], Loss: 913.085693359375, Entropy 438.8042907714844, Learning Rate: 7.8125e-05\n",
      "Epoch [11096/20000], Loss: 874.0828857421875, Entropy 443.7557678222656, Learning Rate: 7.8125e-05\n",
      "Epoch [11097/20000], Loss: 873.2227783203125, Entropy 447.6324157714844, Learning Rate: 7.8125e-05\n",
      "Epoch [11098/20000], Loss: 824.9725341796875, Entropy 468.5413818359375, Learning Rate: 7.8125e-05\n",
      "Epoch [11099/20000], Loss: 904.0298461914062, Entropy 440.10015869140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11100/20000], Loss: 810.647216796875, Entropy 450.1100769042969, Learning Rate: 7.8125e-05\n",
      "Epoch [11101/20000], Loss: 813.6268310546875, Entropy 452.5854187011719, Learning Rate: 7.8125e-05\n",
      "Epoch [11102/20000], Loss: 853.2120361328125, Entropy 451.44140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11103/20000], Loss: 890.936767578125, Entropy 438.3076477050781, Learning Rate: 7.8125e-05\n",
      "Epoch [11104/20000], Loss: 858.389892578125, Entropy 454.2027587890625, Learning Rate: 7.8125e-05\n",
      "Epoch [11105/20000], Loss: 877.8079833984375, Entropy 448.8395690917969, Learning Rate: 7.8125e-05\n",
      "Epoch [11106/20000], Loss: 824.2039794921875, Entropy 447.1699523925781, Learning Rate: 7.8125e-05\n",
      "Epoch [11107/20000], Loss: 865.5245361328125, Entropy 448.555419921875, Learning Rate: 7.8125e-05\n",
      "Epoch [11108/20000], Loss: 868.3994140625, Entropy 452.5927734375, Learning Rate: 7.8125e-05\n",
      "Epoch [11109/20000], Loss: 841.634033203125, Entropy 461.4415588378906, Learning Rate: 7.8125e-05\n",
      "Epoch [11110/20000], Loss: 829.859619140625, Entropy 461.5368957519531, Learning Rate: 7.8125e-05\n",
      "Epoch [11111/20000], Loss: 868.4763793945312, Entropy 456.04327392578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11112/20000], Loss: 876.849365234375, Entropy 438.34716796875, Learning Rate: 7.8125e-05\n",
      "Epoch [11113/20000], Loss: 884.153076171875, Entropy 456.052490234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11114/20000], Loss: 892.593505859375, Entropy 436.7553405761719, Learning Rate: 7.8125e-05\n",
      "Epoch [11115/20000], Loss: 863.9407958984375, Entropy 438.4814453125, Learning Rate: 7.8125e-05\n",
      "Epoch [11116/20000], Loss: 875.2083740234375, Entropy 434.5333557128906, Learning Rate: 7.8125e-05\n",
      "Epoch [11117/20000], Loss: 858.071533203125, Entropy 456.3689880371094, Learning Rate: 7.8125e-05\n",
      "Epoch [11118/20000], Loss: 860.9105224609375, Entropy 446.5085754394531, Learning Rate: 7.8125e-05\n",
      "Epoch [11119/20000], Loss: 878.8992919921875, Entropy 446.1716613769531, Learning Rate: 7.8125e-05\n",
      "Epoch [11120/20000], Loss: 907.306884765625, Entropy 430.38525390625, Learning Rate: 7.8125e-05\n",
      "Epoch [11121/20000], Loss: 808.659912109375, Entropy 465.1769104003906, Learning Rate: 7.8125e-05\n",
      "Epoch [11122/20000], Loss: 854.470947265625, Entropy 451.9219970703125, Learning Rate: 7.8125e-05\n",
      "Epoch [11123/20000], Loss: 841.5511474609375, Entropy 449.1692199707031, Learning Rate: 7.8125e-05\n",
      "Epoch [11124/20000], Loss: 874.3673706054688, Entropy 456.44781494140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11125/20000], Loss: 814.407470703125, Entropy 457.3394470214844, Learning Rate: 7.8125e-05\n",
      "Epoch [11126/20000], Loss: 834.0279541015625, Entropy 445.2442626953125, Learning Rate: 7.8125e-05\n",
      "Epoch [11127/20000], Loss: 866.1661376953125, Entropy 460.7887268066406, Learning Rate: 7.8125e-05\n",
      "Epoch [11128/20000], Loss: 853.9871826171875, Entropy 452.8416442871094, Learning Rate: 7.8125e-05\n",
      "Epoch [11129/20000], Loss: 861.045654296875, Entropy 463.4392395019531, Learning Rate: 7.8125e-05\n",
      "Epoch [11130/20000], Loss: 849.4072875976562, Entropy 439.46343994140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11131/20000], Loss: 848.79296875, Entropy 440.2809753417969, Learning Rate: 7.8125e-05\n",
      "Epoch [11132/20000], Loss: 834.1702880859375, Entropy 470.2538146972656, Learning Rate: 7.8125e-05\n",
      "Epoch [11133/20000], Loss: 860.95361328125, Entropy 463.5688171386719, Learning Rate: 7.8125e-05\n",
      "Epoch [11134/20000], Loss: 917.761962890625, Entropy 442.4117736816406, Learning Rate: 7.8125e-05\n",
      "Epoch [11135/20000], Loss: 851.9872436523438, Entropy 447.41351318359375, Learning Rate: 7.8125e-05\n",
      "Epoch [11136/20000], Loss: 854.0693359375, Entropy 452.07861328125, Learning Rate: 7.8125e-05\n",
      "Epoch [11137/20000], Loss: 857.946533203125, Entropy 441.7590637207031, Learning Rate: 7.8125e-05\n",
      "Epoch [11138/20000], Loss: 881.9130859375, Entropy 448.6360168457031, Learning Rate: 7.8125e-05\n",
      "Epoch [11139/20000], Loss: 836.350341796875, Entropy 456.0064697265625, Learning Rate: 7.8125e-05\n",
      "Epoch [11140/20000], Loss: 834.623779296875, Entropy 453.8650207519531, Learning Rate: 7.8125e-05\n",
      "Epoch [11141/20000], Loss: 828.6953735351562, Entropy 447.11334228515625, Learning Rate: 7.8125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11142/20000], Loss: 890.495849609375, Entropy 464.5163269042969, Learning Rate: 7.8125e-05\n",
      "Epoch [11143/20000], Loss: 842.3592529296875, Entropy 467.1751708984375, Learning Rate: 7.8125e-05\n",
      "Epoch [11144/20000], Loss: 901.6262817382812, Entropy 456.52276611328125, Learning Rate: 7.8125e-05\n",
      "Epoch [11145/20000], Loss: 887.399658203125, Entropy 464.2303771972656, Learning Rate: 7.8125e-05\n",
      "Epoch [11146/20000], Loss: 839.7597045898438, Entropy 469.03460693359375, Learning Rate: 7.8125e-05\n",
      "Epoch [11147/20000], Loss: 879.99169921875, Entropy 439.2228088378906, Learning Rate: 7.8125e-05\n",
      "Epoch [11148/20000], Loss: 882.2427978515625, Entropy 465.2657165527344, Learning Rate: 7.8125e-05\n",
      "Epoch [11149/20000], Loss: 826.033935546875, Entropy 459.1765441894531, Learning Rate: 7.8125e-05\n",
      "Epoch [11150/20000], Loss: 903.3726806640625, Entropy 459.10888671875, Learning Rate: 7.8125e-05\n",
      "Epoch [11151/20000], Loss: 879.2659301757812, Entropy 449.52935791015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11152/20000], Loss: 862.3828735351562, Entropy 453.65631103515625, Learning Rate: 7.8125e-05\n",
      "Epoch [11153/20000], Loss: 853.9334716796875, Entropy 445.3931884765625, Learning Rate: 7.8125e-05\n",
      "Epoch [11154/20000], Loss: 872.6519775390625, Entropy 444.8274841308594, Learning Rate: 7.8125e-05\n",
      "Epoch [11155/20000], Loss: 876.5110473632812, Entropy 456.46905517578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11156/20000], Loss: 845.4622802734375, Entropy 447.8083801269531, Learning Rate: 7.8125e-05\n",
      "Epoch [11157/20000], Loss: 820.958740234375, Entropy 448.6206970214844, Learning Rate: 7.8125e-05\n",
      "Epoch [11158/20000], Loss: 881.4644775390625, Entropy 456.3946838378906, Learning Rate: 7.8125e-05\n",
      "Epoch [11159/20000], Loss: 849.509033203125, Entropy 438.0802307128906, Learning Rate: 7.8125e-05\n",
      "Epoch [11160/20000], Loss: 799.489990234375, Entropy 453.2184143066406, Learning Rate: 7.8125e-05\n",
      "Epoch [11161/20000], Loss: 856.0924682617188, Entropy 444.12689208984375, Learning Rate: 7.8125e-05\n",
      "Epoch [11162/20000], Loss: 839.9318237304688, Entropy 450.75445556640625, Learning Rate: 7.8125e-05\n",
      "Epoch [11163/20000], Loss: 839.6710205078125, Entropy 439.4150695800781, Learning Rate: 7.8125e-05\n",
      "Epoch [11164/20000], Loss: 856.1607666015625, Entropy 441.1896667480469, Learning Rate: 7.8125e-05\n",
      "Epoch [11165/20000], Loss: 880.5106201171875, Entropy 451.5545959472656, Learning Rate: 7.8125e-05\n",
      "Epoch [11166/20000], Loss: 896.290771484375, Entropy 452.6048278808594, Learning Rate: 7.8125e-05\n",
      "Epoch [11167/20000], Loss: 916.058837890625, Entropy 437.5279541015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11168/20000], Loss: 842.43310546875, Entropy 464.8609619140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11169/20000], Loss: 862.4993896484375, Entropy 434.5362548828125, Learning Rate: 7.8125e-05\n",
      "Epoch [11170/20000], Loss: 848.1079711914062, Entropy 445.40960693359375, Learning Rate: 7.8125e-05\n",
      "Epoch [11171/20000], Loss: 818.052734375, Entropy 441.3992919921875, Learning Rate: 7.8125e-05\n",
      "Epoch [11172/20000], Loss: 844.78173828125, Entropy 441.4847106933594, Learning Rate: 7.8125e-05\n",
      "Epoch [11173/20000], Loss: 886.8311157226562, Entropy 448.98358154296875, Learning Rate: 7.8125e-05\n",
      "Epoch [11174/20000], Loss: 888.958251953125, Entropy 434.9482727050781, Learning Rate: 7.8125e-05\n",
      "Epoch [11175/20000], Loss: 816.5921020507812, Entropy 459.70306396484375, Learning Rate: 7.8125e-05\n",
      "Epoch [11176/20000], Loss: 879.927001953125, Entropy 468.36279296875, Learning Rate: 7.8125e-05\n",
      "Epoch [11177/20000], Loss: 852.1219482421875, Entropy 449.565185546875, Learning Rate: 7.8125e-05\n",
      "Epoch [11178/20000], Loss: 840.0787353515625, Entropy 452.9923400878906, Learning Rate: 7.8125e-05\n",
      "Epoch [11179/20000], Loss: 897.465087890625, Entropy 435.6064758300781, Learning Rate: 7.8125e-05\n",
      "Epoch [11180/20000], Loss: 817.740234375, Entropy 453.2737121582031, Learning Rate: 7.8125e-05\n",
      "Epoch [11181/20000], Loss: 850.2322998046875, Entropy 462.01318359375, Learning Rate: 7.8125e-05\n",
      "Epoch [11182/20000], Loss: 848.05322265625, Entropy 443.5744934082031, Learning Rate: 7.8125e-05\n",
      "Epoch [11183/20000], Loss: 867.472900390625, Entropy 461.9483642578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11184/20000], Loss: 825.4654541015625, Entropy 446.2823486328125, Learning Rate: 7.8125e-05\n",
      "Epoch [11185/20000], Loss: 871.37548828125, Entropy 443.4059753417969, Learning Rate: 7.8125e-05\n",
      "Epoch [11186/20000], Loss: 858.711669921875, Entropy 452.4848327636719, Learning Rate: 7.8125e-05\n",
      "Epoch [11187/20000], Loss: 829.3480834960938, Entropy 449.69097900390625, Learning Rate: 7.8125e-05\n",
      "Epoch [11188/20000], Loss: 883.34423828125, Entropy 452.7467956542969, Learning Rate: 7.8125e-05\n",
      "Epoch [11189/20000], Loss: 917.5776977539062, Entropy 440.40325927734375, Learning Rate: 7.8125e-05\n",
      "Epoch [11190/20000], Loss: 897.326416015625, Entropy 443.2196044921875, Learning Rate: 7.8125e-05\n",
      "Epoch [11191/20000], Loss: 858.1900634765625, Entropy 444.8678894042969, Learning Rate: 7.8125e-05\n",
      "Epoch [11192/20000], Loss: 858.9429931640625, Entropy 454.8684997558594, Learning Rate: 7.8125e-05\n",
      "Epoch [11193/20000], Loss: 867.605712890625, Entropy 442.828125, Learning Rate: 7.8125e-05\n",
      "Epoch [11194/20000], Loss: 867.9070434570312, Entropy 457.37701416015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11195/20000], Loss: 854.4805908203125, Entropy 444.6399841308594, Learning Rate: 7.8125e-05\n",
      "Epoch [11196/20000], Loss: 879.5374755859375, Entropy 438.0130310058594, Learning Rate: 7.8125e-05\n",
      "Epoch [11197/20000], Loss: 861.9827270507812, Entropy 457.54632568359375, Learning Rate: 7.8125e-05\n",
      "Epoch [11198/20000], Loss: 848.6774291992188, Entropy 454.58258056640625, Learning Rate: 7.8125e-05\n",
      "Epoch [11199/20000], Loss: 868.2503051757812, Entropy 451.98638916015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11200/20000], Loss: 829.5970458984375, Entropy 451.3404541015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11201/20000], Loss: 830.6771240234375, Entropy 450.968505859375, Learning Rate: 7.8125e-05\n",
      "Epoch [11202/20000], Loss: 833.5614013671875, Entropy 447.8626708984375, Learning Rate: 7.8125e-05\n",
      "Epoch [11203/20000], Loss: 808.8931884765625, Entropy 453.0459289550781, Learning Rate: 7.8125e-05\n",
      "Epoch [11204/20000], Loss: 878.3043212890625, Entropy 437.931640625, Learning Rate: 7.8125e-05\n",
      "Epoch [11205/20000], Loss: 825.8411254882812, Entropy 464.19036865234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11206/20000], Loss: 832.1380615234375, Entropy 446.7644958496094, Learning Rate: 7.8125e-05\n",
      "Epoch [11207/20000], Loss: 869.55224609375, Entropy 435.1405029296875, Learning Rate: 7.8125e-05\n",
      "Epoch [11208/20000], Loss: 857.2819213867188, Entropy 448.85833740234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11209/20000], Loss: 822.55126953125, Entropy 461.6119689941406, Learning Rate: 7.8125e-05\n",
      "Epoch [11210/20000], Loss: 892.606201171875, Entropy 454.8018798828125, Learning Rate: 7.8125e-05\n",
      "Epoch [11211/20000], Loss: 842.9105224609375, Entropy 454.3045654296875, Learning Rate: 7.8125e-05\n",
      "Epoch [11212/20000], Loss: 818.00537109375, Entropy 467.1770935058594, Learning Rate: 7.8125e-05\n",
      "Epoch [11213/20000], Loss: 819.441650390625, Entropy 445.9761657714844, Learning Rate: 7.8125e-05\n",
      "Epoch [11214/20000], Loss: 829.3054809570312, Entropy 456.41339111328125, Learning Rate: 7.8125e-05\n",
      "Epoch [11215/20000], Loss: 852.996337890625, Entropy 428.4566650390625, Learning Rate: 7.8125e-05\n",
      "Epoch [11216/20000], Loss: 865.4564819335938, Entropy 439.10406494140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11217/20000], Loss: 825.4532470703125, Entropy 464.18896484375, Learning Rate: 7.8125e-05\n",
      "Epoch [11218/20000], Loss: 853.06689453125, Entropy 448.4975280761719, Learning Rate: 7.8125e-05\n",
      "Epoch [11219/20000], Loss: 866.5529174804688, Entropy 462.63482666015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11220/20000], Loss: 841.47265625, Entropy 446.8077392578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11221/20000], Loss: 795.37353515625, Entropy 455.2889709472656, Learning Rate: 7.8125e-05\n",
      "Epoch [11222/20000], Loss: 875.4620971679688, Entropy 451.58026123046875, Learning Rate: 7.8125e-05\n",
      "Epoch [11223/20000], Loss: 837.843505859375, Entropy 469.25537109375, Learning Rate: 7.8125e-05\n",
      "Epoch [11224/20000], Loss: 810.605224609375, Entropy 440.2106018066406, Learning Rate: 7.8125e-05\n",
      "Epoch [11225/20000], Loss: 842.057861328125, Entropy 450.130859375, Learning Rate: 7.8125e-05\n",
      "Epoch [11226/20000], Loss: 875.8909912109375, Entropy 458.0051574707031, Learning Rate: 7.8125e-05\n",
      "Epoch [11227/20000], Loss: 828.947998046875, Entropy 463.9526062011719, Learning Rate: 7.8125e-05\n",
      "Epoch [11228/20000], Loss: 815.0633544921875, Entropy 464.6315612792969, Learning Rate: 7.8125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11229/20000], Loss: 870.7030029296875, Entropy 438.2663879394531, Learning Rate: 7.8125e-05\n",
      "Epoch [11230/20000], Loss: 828.6641845703125, Entropy 456.5360107421875, Learning Rate: 7.8125e-05\n",
      "Epoch [11231/20000], Loss: 908.4655151367188, Entropy 441.19036865234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11232/20000], Loss: 832.8949584960938, Entropy 457.91217041015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11233/20000], Loss: 846.2776489257812, Entropy 450.15167236328125, Learning Rate: 7.8125e-05\n",
      "Epoch [11234/20000], Loss: 841.3504028320312, Entropy 466.85467529296875, Learning Rate: 7.8125e-05\n",
      "Epoch [11235/20000], Loss: 907.2005615234375, Entropy 435.6265563964844, Learning Rate: 7.8125e-05\n",
      "Epoch [11236/20000], Loss: 811.044921875, Entropy 450.7546691894531, Learning Rate: 7.8125e-05\n",
      "Epoch [11237/20000], Loss: 911.7459716796875, Entropy 450.8717041015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11238/20000], Loss: 830.5344848632812, Entropy 457.90350341796875, Learning Rate: 7.8125e-05\n",
      "Epoch [11239/20000], Loss: 820.3693237304688, Entropy 451.81365966796875, Learning Rate: 7.8125e-05\n",
      "Epoch [11240/20000], Loss: 896.3421630859375, Entropy 441.5615539550781, Learning Rate: 7.8125e-05\n",
      "Epoch [11241/20000], Loss: 858.8936157226562, Entropy 450.76007080078125, Learning Rate: 7.8125e-05\n",
      "Epoch [11242/20000], Loss: 849.1104736328125, Entropy 439.6170654296875, Learning Rate: 7.8125e-05\n",
      "Epoch [11243/20000], Loss: 834.218017578125, Entropy 462.3800964355469, Learning Rate: 7.8125e-05\n",
      "Epoch [11244/20000], Loss: 875.86474609375, Entropy 442.5990905761719, Learning Rate: 7.8125e-05\n",
      "Epoch [11245/20000], Loss: 843.567138671875, Entropy 454.0885009765625, Learning Rate: 7.8125e-05\n",
      "Epoch [11246/20000], Loss: 847.2263793945312, Entropy 446.12127685546875, Learning Rate: 7.8125e-05\n",
      "Epoch [11247/20000], Loss: 878.7694091796875, Entropy 467.35498046875, Learning Rate: 7.8125e-05\n",
      "Epoch [11248/20000], Loss: 795.258544921875, Entropy 466.3904724121094, Learning Rate: 7.8125e-05\n",
      "Epoch [11249/20000], Loss: 850.9013061523438, Entropy 454.65863037109375, Learning Rate: 7.8125e-05\n",
      "Epoch [11250/20000], Loss: 822.969482421875, Entropy 483.8086242675781, Learning Rate: 7.8125e-05\n",
      "Epoch [11251/20000], Loss: 896.9009399414062, Entropy 437.66607666015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11252/20000], Loss: 844.2432861328125, Entropy 454.2653503417969, Learning Rate: 7.8125e-05\n",
      "Epoch [11253/20000], Loss: 913.4698486328125, Entropy 447.6278076171875, Learning Rate: 7.8125e-05\n",
      "Epoch [11254/20000], Loss: 823.8809204101562, Entropy 460.91229248046875, Learning Rate: 7.8125e-05\n",
      "Epoch [11255/20000], Loss: 921.2725219726562, Entropy 443.92864990234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11256/20000], Loss: 885.06494140625, Entropy 446.0328369140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11257/20000], Loss: 857.2072143554688, Entropy 445.04290771484375, Learning Rate: 7.8125e-05\n",
      "Epoch [11258/20000], Loss: 873.68701171875, Entropy 455.9412841796875, Learning Rate: 7.8125e-05\n",
      "Epoch [11259/20000], Loss: 862.9998779296875, Entropy 469.650146484375, Learning Rate: 7.8125e-05\n",
      "Epoch [11260/20000], Loss: 827.349853515625, Entropy 454.93359375, Learning Rate: 7.8125e-05\n",
      "Epoch [11261/20000], Loss: 878.5077514648438, Entropy 466.03936767578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11262/20000], Loss: 898.9969482421875, Entropy 446.948974609375, Learning Rate: 7.8125e-05\n",
      "Epoch [11263/20000], Loss: 854.2438354492188, Entropy 465.75787353515625, Learning Rate: 7.8125e-05\n",
      "Epoch [11264/20000], Loss: 838.6553955078125, Entropy 449.5037536621094, Learning Rate: 7.8125e-05\n",
      "Epoch [11265/20000], Loss: 861.6094360351562, Entropy 438.63812255859375, Learning Rate: 7.8125e-05\n",
      "Epoch [11266/20000], Loss: 829.401123046875, Entropy 445.6158752441406, Learning Rate: 7.8125e-05\n",
      "Epoch [11267/20000], Loss: 900.2490234375, Entropy 436.3206787109375, Learning Rate: 7.8125e-05\n",
      "Epoch [11268/20000], Loss: 892.400146484375, Entropy 444.4952392578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11269/20000], Loss: 881.4160766601562, Entropy 453.67218017578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11270/20000], Loss: 918.7426147460938, Entropy 458.77764892578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11271/20000], Loss: 886.5216064453125, Entropy 442.9449768066406, Learning Rate: 7.8125e-05\n",
      "Epoch [11272/20000], Loss: 811.7919921875, Entropy 454.2499084472656, Learning Rate: 7.8125e-05\n",
      "Epoch [11273/20000], Loss: 852.5001220703125, Entropy 464.6988830566406, Learning Rate: 7.8125e-05\n",
      "Epoch [11274/20000], Loss: 815.9986572265625, Entropy 460.3453674316406, Learning Rate: 7.8125e-05\n",
      "Epoch [11275/20000], Loss: 903.8436279296875, Entropy 446.2747497558594, Learning Rate: 7.8125e-05\n",
      "Epoch [11276/20000], Loss: 903.1634521484375, Entropy 428.9739990234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11277/20000], Loss: 843.7999267578125, Entropy 470.4757385253906, Learning Rate: 7.8125e-05\n",
      "Epoch [11278/20000], Loss: 838.556640625, Entropy 446.2742919921875, Learning Rate: 7.8125e-05\n",
      "Epoch [11279/20000], Loss: 811.820068359375, Entropy 467.2171325683594, Learning Rate: 7.8125e-05\n",
      "Epoch [11280/20000], Loss: 852.9849853515625, Entropy 451.4432678222656, Learning Rate: 7.8125e-05\n",
      "Epoch [11281/20000], Loss: 849.6376953125, Entropy 454.3908996582031, Learning Rate: 7.8125e-05\n",
      "Epoch [11282/20000], Loss: 846.182861328125, Entropy 453.5209655761719, Learning Rate: 7.8125e-05\n",
      "Epoch [11283/20000], Loss: 814.349853515625, Entropy 445.0754699707031, Learning Rate: 7.8125e-05\n",
      "Epoch [11284/20000], Loss: 850.080322265625, Entropy 448.6836242675781, Learning Rate: 7.8125e-05\n",
      "Epoch [11285/20000], Loss: 869.452880859375, Entropy 450.2108459472656, Learning Rate: 7.8125e-05\n",
      "Epoch [11286/20000], Loss: 815.2001953125, Entropy 463.7545166015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11287/20000], Loss: 809.86865234375, Entropy 465.68115234375, Learning Rate: 7.8125e-05\n",
      "Epoch [11288/20000], Loss: 887.5587158203125, Entropy 465.6587829589844, Learning Rate: 7.8125e-05\n",
      "Epoch [11289/20000], Loss: 814.855224609375, Entropy 464.3824157714844, Learning Rate: 7.8125e-05\n",
      "Epoch [11290/20000], Loss: 854.76025390625, Entropy 468.0197448730469, Learning Rate: 7.8125e-05\n",
      "Epoch [11291/20000], Loss: 900.6561279296875, Entropy 459.9107360839844, Learning Rate: 7.8125e-05\n",
      "Epoch [11292/20000], Loss: 818.345947265625, Entropy 459.7744445800781, Learning Rate: 7.8125e-05\n",
      "Epoch [11293/20000], Loss: 821.715576171875, Entropy 465.1184387207031, Learning Rate: 7.8125e-05\n",
      "Epoch [11294/20000], Loss: 863.65185546875, Entropy 443.3783874511719, Learning Rate: 7.8125e-05\n",
      "Epoch [11295/20000], Loss: 826.1468505859375, Entropy 459.3052978515625, Learning Rate: 7.8125e-05\n",
      "Epoch [11296/20000], Loss: 846.7889404296875, Entropy 445.9267578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11297/20000], Loss: 885.6563720703125, Entropy 457.618896484375, Learning Rate: 7.8125e-05\n",
      "Epoch [11298/20000], Loss: 813.1749267578125, Entropy 466.2429504394531, Learning Rate: 7.8125e-05\n",
      "Epoch [11299/20000], Loss: 895.34619140625, Entropy 438.5305480957031, Learning Rate: 7.8125e-05\n",
      "Epoch [11300/20000], Loss: 830.9025268554688, Entropy 445.92828369140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11301/20000], Loss: 837.3058471679688, Entropy 450.99713134765625, Learning Rate: 7.8125e-05\n",
      "Epoch [11302/20000], Loss: 836.9242553710938, Entropy 450.23358154296875, Learning Rate: 7.8125e-05\n",
      "Epoch [11303/20000], Loss: 790.148193359375, Entropy 449.0235900878906, Learning Rate: 7.8125e-05\n",
      "Epoch [11304/20000], Loss: 848.517578125, Entropy 454.5414733886719, Learning Rate: 7.8125e-05\n",
      "Epoch [11305/20000], Loss: 890.4373168945312, Entropy 456.56158447265625, Learning Rate: 7.8125e-05\n",
      "Epoch [11306/20000], Loss: 838.1903076171875, Entropy 454.7940979003906, Learning Rate: 7.8125e-05\n",
      "Epoch [11307/20000], Loss: 868.4613037109375, Entropy 447.1983642578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11308/20000], Loss: 837.0459594726562, Entropy 453.15032958984375, Learning Rate: 7.8125e-05\n",
      "Epoch [11309/20000], Loss: 825.4501342773438, Entropy 473.19573974609375, Learning Rate: 7.8125e-05\n",
      "Epoch [11310/20000], Loss: 826.544921875, Entropy 455.5484924316406, Learning Rate: 7.8125e-05\n",
      "Epoch [11311/20000], Loss: 870.2471923828125, Entropy 455.5171813964844, Learning Rate: 7.8125e-05\n",
      "Epoch [11312/20000], Loss: 858.1007080078125, Entropy 437.9414367675781, Learning Rate: 7.8125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11313/20000], Loss: 856.19287109375, Entropy 463.1949768066406, Learning Rate: 7.8125e-05\n",
      "Epoch [11314/20000], Loss: 889.4166259765625, Entropy 438.8546142578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11315/20000], Loss: 890.1640014648438, Entropy 441.55426025390625, Learning Rate: 7.8125e-05\n",
      "Epoch [11316/20000], Loss: 874.4024047851562, Entropy 470.49725341796875, Learning Rate: 7.8125e-05\n",
      "Epoch [11317/20000], Loss: 895.5301513671875, Entropy 459.15576171875, Learning Rate: 7.8125e-05\n",
      "Epoch [11318/20000], Loss: 818.504150390625, Entropy 444.7989807128906, Learning Rate: 7.8125e-05\n",
      "Epoch [11319/20000], Loss: 855.1102905273438, Entropy 452.74127197265625, Learning Rate: 7.8125e-05\n",
      "Epoch [11320/20000], Loss: 851.3616943359375, Entropy 446.9456481933594, Learning Rate: 7.8125e-05\n",
      "Epoch [11321/20000], Loss: 851.5569458007812, Entropy 457.71917724609375, Learning Rate: 7.8125e-05\n",
      "Epoch [11322/20000], Loss: 843.536865234375, Entropy 468.4566345214844, Learning Rate: 7.8125e-05\n",
      "Epoch [11323/20000], Loss: 832.604248046875, Entropy 453.0227355957031, Learning Rate: 7.8125e-05\n",
      "Epoch [11324/20000], Loss: 882.129150390625, Entropy 461.1788330078125, Learning Rate: 7.8125e-05\n",
      "Epoch [11325/20000], Loss: 887.666748046875, Entropy 443.1094970703125, Learning Rate: 7.8125e-05\n",
      "Epoch [11326/20000], Loss: 902.0386962890625, Entropy 450.5938720703125, Learning Rate: 7.8125e-05\n",
      "Epoch [11327/20000], Loss: 862.4024658203125, Entropy 446.8370361328125, Learning Rate: 7.8125e-05\n",
      "Epoch [11328/20000], Loss: 839.469482421875, Entropy 454.72314453125, Learning Rate: 7.8125e-05\n",
      "Epoch [11329/20000], Loss: 831.4472045898438, Entropy 464.69000244140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11330/20000], Loss: 826.7900390625, Entropy 455.3722839355469, Learning Rate: 7.8125e-05\n",
      "Epoch [11331/20000], Loss: 838.02880859375, Entropy 466.4839782714844, Learning Rate: 7.8125e-05\n",
      "Epoch [11332/20000], Loss: 856.977783203125, Entropy 460.5796813964844, Learning Rate: 7.8125e-05\n",
      "Epoch [11333/20000], Loss: 845.92578125, Entropy 464.2932434082031, Learning Rate: 7.8125e-05\n",
      "Epoch [11334/20000], Loss: 858.8419189453125, Entropy 445.6432189941406, Learning Rate: 7.8125e-05\n",
      "Epoch [11335/20000], Loss: 892.1868896484375, Entropy 429.4208984375, Learning Rate: 7.8125e-05\n",
      "Epoch [11336/20000], Loss: 856.613525390625, Entropy 444.0473327636719, Learning Rate: 7.8125e-05\n",
      "Epoch [11337/20000], Loss: 887.0672607421875, Entropy 457.6502380371094, Learning Rate: 7.8125e-05\n",
      "Epoch [11338/20000], Loss: 862.1616821289062, Entropy 434.46636962890625, Learning Rate: 7.8125e-05\n",
      "Epoch [11339/20000], Loss: 865.0567626953125, Entropy 455.6754455566406, Learning Rate: 7.8125e-05\n",
      "Epoch [11340/20000], Loss: 840.8187255859375, Entropy 443.8772888183594, Learning Rate: 7.8125e-05\n",
      "Epoch [11341/20000], Loss: 819.7946166992188, Entropy 444.69647216796875, Learning Rate: 7.8125e-05\n",
      "Epoch [11342/20000], Loss: 894.9769897460938, Entropy 427.95245361328125, Learning Rate: 7.8125e-05\n",
      "Epoch [11343/20000], Loss: 890.9691772460938, Entropy 445.27593994140625, Learning Rate: 7.8125e-05\n",
      "Epoch [11344/20000], Loss: 862.5620727539062, Entropy 444.96917724609375, Learning Rate: 7.8125e-05\n",
      "Epoch [11345/20000], Loss: 870.013427734375, Entropy 442.6188049316406, Learning Rate: 7.8125e-05\n",
      "Epoch [11346/20000], Loss: 839.19189453125, Entropy 451.001953125, Learning Rate: 7.8125e-05\n",
      "Epoch [11347/20000], Loss: 846.7891845703125, Entropy 448.8885803222656, Learning Rate: 7.8125e-05\n",
      "Epoch [11348/20000], Loss: 814.2055053710938, Entropy 463.17388916015625, Learning Rate: 7.8125e-05\n",
      "Epoch [11349/20000], Loss: 827.2232666015625, Entropy 463.6872253417969, Learning Rate: 7.8125e-05\n",
      "Epoch [11350/20000], Loss: 828.9603271484375, Entropy 446.5610046386719, Learning Rate: 7.8125e-05\n",
      "Epoch [11351/20000], Loss: 824.1212158203125, Entropy 467.4226379394531, Learning Rate: 7.8125e-05\n",
      "Epoch [11352/20000], Loss: 930.5875244140625, Entropy 449.3780517578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11353/20000], Loss: 877.4868774414062, Entropy 442.24017333984375, Learning Rate: 7.8125e-05\n",
      "Epoch [11354/20000], Loss: 795.8475341796875, Entropy 464.9474182128906, Learning Rate: 7.8125e-05\n",
      "Epoch [11355/20000], Loss: 850.8612060546875, Entropy 457.6988220214844, Learning Rate: 7.8125e-05\n",
      "Epoch [11356/20000], Loss: 873.1657104492188, Entropy 438.78546142578125, Learning Rate: 7.8125e-05\n",
      "Epoch [11357/20000], Loss: 851.6819458007812, Entropy 450.50970458984375, Learning Rate: 7.8125e-05\n",
      "Epoch [11358/20000], Loss: 864.145263671875, Entropy 463.0997619628906, Learning Rate: 7.8125e-05\n",
      "Epoch [11359/20000], Loss: 884.11474609375, Entropy 443.7117004394531, Learning Rate: 7.8125e-05\n",
      "Epoch [11360/20000], Loss: 965.923095703125, Entropy 437.7131652832031, Learning Rate: 7.8125e-05\n",
      "Epoch [11361/20000], Loss: 845.0780029296875, Entropy 466.876953125, Learning Rate: 7.8125e-05\n",
      "Epoch [11362/20000], Loss: 862.22314453125, Entropy 446.2016296386719, Learning Rate: 7.8125e-05\n",
      "Epoch [11363/20000], Loss: 842.292724609375, Entropy 455.3085021972656, Learning Rate: 7.8125e-05\n",
      "Epoch [11364/20000], Loss: 817.9299926757812, Entropy 469.43682861328125, Learning Rate: 7.8125e-05\n",
      "Epoch [11365/20000], Loss: 867.92822265625, Entropy 439.9195251464844, Learning Rate: 7.8125e-05\n",
      "Epoch [11366/20000], Loss: 877.9130859375, Entropy 440.0018005371094, Learning Rate: 7.8125e-05\n",
      "Epoch [11367/20000], Loss: 836.6824951171875, Entropy 451.5423278808594, Learning Rate: 7.8125e-05\n",
      "Epoch [11368/20000], Loss: 826.1412963867188, Entropy 452.55120849609375, Learning Rate: 7.8125e-05\n",
      "Epoch [11369/20000], Loss: 862.907470703125, Entropy 459.2912902832031, Learning Rate: 7.8125e-05\n",
      "Epoch [11370/20000], Loss: 830.7236328125, Entropy 452.7611999511719, Learning Rate: 7.8125e-05\n",
      "Epoch [11371/20000], Loss: 855.9169921875, Entropy 455.416748046875, Learning Rate: 7.8125e-05\n",
      "Epoch [11372/20000], Loss: 893.7371826171875, Entropy 430.0764465332031, Learning Rate: 7.8125e-05\n",
      "Epoch [11373/20000], Loss: 863.7440185546875, Entropy 450.04833984375, Learning Rate: 7.8125e-05\n",
      "Epoch [11374/20000], Loss: 871.8380126953125, Entropy 451.6912841796875, Learning Rate: 7.8125e-05\n",
      "Epoch [11375/20000], Loss: 815.6991577148438, Entropy 453.77593994140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11376/20000], Loss: 888.6375732421875, Entropy 441.8055725097656, Learning Rate: 3.90625e-05\n",
      "Epoch [11377/20000], Loss: 891.369384765625, Entropy 443.0533142089844, Learning Rate: 3.90625e-05\n",
      "Epoch [11378/20000], Loss: 821.7459716796875, Entropy 463.9952697753906, Learning Rate: 3.90625e-05\n",
      "Epoch [11379/20000], Loss: 868.139404296875, Entropy 450.679443359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11380/20000], Loss: 822.4337768554688, Entropy 464.14801025390625, Learning Rate: 3.90625e-05\n",
      "Epoch [11381/20000], Loss: 841.5159912109375, Entropy 459.1327209472656, Learning Rate: 3.90625e-05\n",
      "Epoch [11382/20000], Loss: 850.16943359375, Entropy 430.0151062011719, Learning Rate: 3.90625e-05\n",
      "Epoch [11383/20000], Loss: 842.9739990234375, Entropy 443.7945556640625, Learning Rate: 3.90625e-05\n",
      "Epoch [11384/20000], Loss: 807.8449096679688, Entropy 457.41119384765625, Learning Rate: 3.90625e-05\n",
      "Epoch [11385/20000], Loss: 873.0454711914062, Entropy 451.59429931640625, Learning Rate: 3.90625e-05\n",
      "Epoch [11386/20000], Loss: 882.947998046875, Entropy 447.5968017578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11387/20000], Loss: 838.18212890625, Entropy 467.4383544921875, Learning Rate: 3.90625e-05\n",
      "Epoch [11388/20000], Loss: 832.1195068359375, Entropy 446.7556457519531, Learning Rate: 3.90625e-05\n",
      "Epoch [11389/20000], Loss: 860.326904296875, Entropy 457.0550537109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11390/20000], Loss: 901.7511596679688, Entropy 443.38507080078125, Learning Rate: 3.90625e-05\n",
      "Epoch [11391/20000], Loss: 923.403076171875, Entropy 458.4855041503906, Learning Rate: 3.90625e-05\n",
      "Epoch [11392/20000], Loss: 862.0587158203125, Entropy 450.7434387207031, Learning Rate: 3.90625e-05\n",
      "Epoch [11393/20000], Loss: 827.9161987304688, Entropy 455.16497802734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11394/20000], Loss: 831.7061767578125, Entropy 446.5957336425781, Learning Rate: 3.90625e-05\n",
      "Epoch [11395/20000], Loss: 875.9161987304688, Entropy 450.90093994140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11396/20000], Loss: 862.5120849609375, Entropy 458.562255859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11397/20000], Loss: 809.849609375, Entropy 457.16845703125, Learning Rate: 3.90625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11398/20000], Loss: 839.3612060546875, Entropy 442.4685363769531, Learning Rate: 3.90625e-05\n",
      "Epoch [11399/20000], Loss: 857.8199462890625, Entropy 463.42578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11400/20000], Loss: 887.21728515625, Entropy 451.1883850097656, Learning Rate: 3.90625e-05\n",
      "Epoch [11401/20000], Loss: 831.7139892578125, Entropy 464.5804138183594, Learning Rate: 3.90625e-05\n",
      "Epoch [11402/20000], Loss: 851.701171875, Entropy 472.6946105957031, Learning Rate: 3.90625e-05\n",
      "Epoch [11403/20000], Loss: 849.76123046875, Entropy 441.3263854980469, Learning Rate: 3.90625e-05\n",
      "Epoch [11404/20000], Loss: 824.0714111328125, Entropy 461.048583984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11405/20000], Loss: 863.7786865234375, Entropy 445.4770202636719, Learning Rate: 3.90625e-05\n",
      "Epoch [11406/20000], Loss: 842.33935546875, Entropy 457.9622497558594, Learning Rate: 3.90625e-05\n",
      "Epoch [11407/20000], Loss: 895.5523681640625, Entropy 454.9192199707031, Learning Rate: 3.90625e-05\n",
      "Epoch [11408/20000], Loss: 839.0701293945312, Entropy 457.24151611328125, Learning Rate: 3.90625e-05\n",
      "Epoch [11409/20000], Loss: 831.8463134765625, Entropy 448.8099060058594, Learning Rate: 3.90625e-05\n",
      "Epoch [11410/20000], Loss: 843.1798095703125, Entropy 450.3629150390625, Learning Rate: 3.90625e-05\n",
      "Epoch [11411/20000], Loss: 842.0380859375, Entropy 460.3390808105469, Learning Rate: 3.90625e-05\n",
      "Epoch [11412/20000], Loss: 825.4802856445312, Entropy 458.14556884765625, Learning Rate: 3.90625e-05\n",
      "Epoch [11413/20000], Loss: 880.1689453125, Entropy 447.7928771972656, Learning Rate: 3.90625e-05\n",
      "Epoch [11414/20000], Loss: 835.9970703125, Entropy 452.5983581542969, Learning Rate: 3.90625e-05\n",
      "Epoch [11415/20000], Loss: 856.4132080078125, Entropy 457.1105041503906, Learning Rate: 3.90625e-05\n",
      "Epoch [11416/20000], Loss: 841.9716796875, Entropy 457.3228454589844, Learning Rate: 3.90625e-05\n",
      "Epoch [11417/20000], Loss: 821.5794677734375, Entropy 452.0277099609375, Learning Rate: 3.90625e-05\n",
      "Epoch [11418/20000], Loss: 896.5462646484375, Entropy 447.6434326171875, Learning Rate: 3.90625e-05\n",
      "Epoch [11419/20000], Loss: 921.5987548828125, Entropy 444.0242614746094, Learning Rate: 3.90625e-05\n",
      "Epoch [11420/20000], Loss: 873.3878173828125, Entropy 446.4679260253906, Learning Rate: 3.90625e-05\n",
      "Epoch [11421/20000], Loss: 827.323974609375, Entropy 455.8782653808594, Learning Rate: 3.90625e-05\n",
      "Epoch [11422/20000], Loss: 863.0369262695312, Entropy 462.23150634765625, Learning Rate: 3.90625e-05\n",
      "Epoch [11423/20000], Loss: 900.4473876953125, Entropy 435.0978698730469, Learning Rate: 3.90625e-05\n",
      "Epoch [11424/20000], Loss: 833.61474609375, Entropy 464.2557373046875, Learning Rate: 3.90625e-05\n",
      "Epoch [11425/20000], Loss: 851.4754638671875, Entropy 469.3460693359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11426/20000], Loss: 872.0316162109375, Entropy 443.3193664550781, Learning Rate: 3.90625e-05\n",
      "Epoch [11427/20000], Loss: 852.5350341796875, Entropy 456.41162109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11428/20000], Loss: 855.3599853515625, Entropy 452.0960693359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11429/20000], Loss: 893.4779663085938, Entropy 458.19683837890625, Learning Rate: 3.90625e-05\n",
      "Epoch [11430/20000], Loss: 885.892822265625, Entropy 448.4527893066406, Learning Rate: 3.90625e-05\n",
      "Epoch [11431/20000], Loss: 815.0416870117188, Entropy 441.13873291015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11432/20000], Loss: 862.4869384765625, Entropy 464.1815185546875, Learning Rate: 3.90625e-05\n",
      "Epoch [11433/20000], Loss: 870.5191650390625, Entropy 457.1801452636719, Learning Rate: 3.90625e-05\n",
      "Epoch [11434/20000], Loss: 850.2825317382812, Entropy 440.37652587890625, Learning Rate: 3.90625e-05\n",
      "Epoch [11435/20000], Loss: 869.8589477539062, Entropy 444.59320068359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11436/20000], Loss: 870.297607421875, Entropy 445.3934020996094, Learning Rate: 3.90625e-05\n",
      "Epoch [11437/20000], Loss: 880.0565185546875, Entropy 455.7004699707031, Learning Rate: 3.90625e-05\n",
      "Epoch [11438/20000], Loss: 801.047607421875, Entropy 449.9912414550781, Learning Rate: 3.90625e-05\n",
      "Epoch [11439/20000], Loss: 798.3140869140625, Entropy 455.2703552246094, Learning Rate: 3.90625e-05\n",
      "Epoch [11440/20000], Loss: 903.8667602539062, Entropy 454.09613037109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11441/20000], Loss: 833.5578002929688, Entropy 461.46246337890625, Learning Rate: 3.90625e-05\n",
      "Epoch [11442/20000], Loss: 861.3629760742188, Entropy 443.90826416015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11443/20000], Loss: 854.6513671875, Entropy 451.7041015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11444/20000], Loss: 857.245361328125, Entropy 462.4675598144531, Learning Rate: 3.90625e-05\n",
      "Epoch [11445/20000], Loss: 813.7764282226562, Entropy 455.77728271484375, Learning Rate: 3.90625e-05\n",
      "Epoch [11446/20000], Loss: 838.38623046875, Entropy 471.4627990722656, Learning Rate: 3.90625e-05\n",
      "Epoch [11447/20000], Loss: 861.11962890625, Entropy 460.28271484375, Learning Rate: 3.90625e-05\n",
      "Epoch [11448/20000], Loss: 799.2451782226562, Entropy 459.66693115234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11449/20000], Loss: 842.195556640625, Entropy 443.0103759765625, Learning Rate: 3.90625e-05\n",
      "Epoch [11450/20000], Loss: 832.8670654296875, Entropy 447.2153015136719, Learning Rate: 3.90625e-05\n",
      "Epoch [11451/20000], Loss: 824.62451171875, Entropy 456.1458740234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11452/20000], Loss: 888.101806640625, Entropy 450.2821350097656, Learning Rate: 3.90625e-05\n",
      "Epoch [11453/20000], Loss: 800.7603759765625, Entropy 457.5079345703125, Learning Rate: 3.90625e-05\n",
      "Epoch [11454/20000], Loss: 852.4593505859375, Entropy 449.442138671875, Learning Rate: 3.90625e-05\n",
      "Epoch [11455/20000], Loss: 858.1011962890625, Entropy 447.1842041015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11456/20000], Loss: 837.390380859375, Entropy 443.1142883300781, Learning Rate: 3.90625e-05\n",
      "Epoch [11457/20000], Loss: 855.0773315429688, Entropy 463.97564697265625, Learning Rate: 3.90625e-05\n",
      "Epoch [11458/20000], Loss: 857.5972900390625, Entropy 453.4958190917969, Learning Rate: 3.90625e-05\n",
      "Epoch [11459/20000], Loss: 839.046630859375, Entropy 459.2856140136719, Learning Rate: 3.90625e-05\n",
      "Epoch [11460/20000], Loss: 910.3646240234375, Entropy 449.1019287109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11461/20000], Loss: 882.166748046875, Entropy 458.24951171875, Learning Rate: 3.90625e-05\n",
      "Epoch [11462/20000], Loss: 852.0045166015625, Entropy 451.5579528808594, Learning Rate: 3.90625e-05\n",
      "Epoch [11463/20000], Loss: 836.764404296875, Entropy 449.1375427246094, Learning Rate: 3.90625e-05\n",
      "Epoch [11464/20000], Loss: 832.1064453125, Entropy 454.4969787597656, Learning Rate: 3.90625e-05\n",
      "Epoch [11465/20000], Loss: 901.755615234375, Entropy 452.600341796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11466/20000], Loss: 861.2731323242188, Entropy 453.73529052734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11467/20000], Loss: 834.244873046875, Entropy 467.5233459472656, Learning Rate: 3.90625e-05\n",
      "Epoch [11468/20000], Loss: 800.730712890625, Entropy 447.5921325683594, Learning Rate: 3.90625e-05\n",
      "Epoch [11469/20000], Loss: 847.5406494140625, Entropy 451.509521484375, Learning Rate: 3.90625e-05\n",
      "Epoch [11470/20000], Loss: 877.221923828125, Entropy 441.85693359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11471/20000], Loss: 848.58447265625, Entropy 463.0138854980469, Learning Rate: 3.90625e-05\n",
      "Epoch [11472/20000], Loss: 831.4664306640625, Entropy 450.42041015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11473/20000], Loss: 899.816650390625, Entropy 440.9308166503906, Learning Rate: 3.90625e-05\n",
      "Epoch [11474/20000], Loss: 863.5076293945312, Entropy 443.92877197265625, Learning Rate: 3.90625e-05\n",
      "Epoch [11475/20000], Loss: 870.100830078125, Entropy 429.5369567871094, Learning Rate: 3.90625e-05\n",
      "Epoch [11476/20000], Loss: 790.322998046875, Entropy 442.744140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11477/20000], Loss: 812.083984375, Entropy 453.281005859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11478/20000], Loss: 883.2320556640625, Entropy 437.0131530761719, Learning Rate: 3.90625e-05\n",
      "Epoch [11479/20000], Loss: 808.4987182617188, Entropy 458.26458740234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11480/20000], Loss: 872.1710205078125, Entropy 464.1383361816406, Learning Rate: 3.90625e-05\n",
      "Epoch [11481/20000], Loss: 809.653564453125, Entropy 464.1717224121094, Learning Rate: 3.90625e-05\n",
      "Epoch [11482/20000], Loss: 854.6036376953125, Entropy 462.9610900878906, Learning Rate: 3.90625e-05\n",
      "Epoch [11483/20000], Loss: 862.9925537109375, Entropy 446.0975036621094, Learning Rate: 3.90625e-05\n",
      "Epoch [11484/20000], Loss: 840.394287109375, Entropy 451.9800720214844, Learning Rate: 3.90625e-05\n",
      "Epoch [11485/20000], Loss: 846.0025634765625, Entropy 439.0690612792969, Learning Rate: 3.90625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11486/20000], Loss: 837.474365234375, Entropy 461.1596374511719, Learning Rate: 3.90625e-05\n",
      "Epoch [11487/20000], Loss: 859.3125, Entropy 459.4268798828125, Learning Rate: 3.90625e-05\n",
      "Epoch [11488/20000], Loss: 840.0022583007812, Entropy 461.14862060546875, Learning Rate: 3.90625e-05\n",
      "Epoch [11489/20000], Loss: 870.8833618164062, Entropy 468.80096435546875, Learning Rate: 3.90625e-05\n",
      "Epoch [11490/20000], Loss: 901.4734497070312, Entropy 455.60833740234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11491/20000], Loss: 892.82666015625, Entropy 450.6204833984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11492/20000], Loss: 916.5072021484375, Entropy 444.1887512207031, Learning Rate: 3.90625e-05\n",
      "Epoch [11493/20000], Loss: 890.38232421875, Entropy 434.2499084472656, Learning Rate: 3.90625e-05\n",
      "Epoch [11494/20000], Loss: 875.1639404296875, Entropy 464.5513916015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11495/20000], Loss: 788.57275390625, Entropy 477.72998046875, Learning Rate: 3.90625e-05\n",
      "Epoch [11496/20000], Loss: 901.5028076171875, Entropy 448.0345153808594, Learning Rate: 3.90625e-05\n",
      "Epoch [11497/20000], Loss: 841.6095581054688, Entropy 453.81390380859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11498/20000], Loss: 831.7081298828125, Entropy 449.1876220703125, Learning Rate: 3.90625e-05\n",
      "Epoch [11499/20000], Loss: 846.9813232421875, Entropy 453.6193542480469, Learning Rate: 3.90625e-05\n",
      "Epoch [11500/20000], Loss: 891.6036376953125, Entropy 450.7635192871094, Learning Rate: 3.90625e-05\n",
      "Epoch [11501/20000], Loss: 820.43212890625, Entropy 466.0191345214844, Learning Rate: 3.90625e-05\n",
      "Epoch [11502/20000], Loss: 897.3612060546875, Entropy 451.2125549316406, Learning Rate: 3.90625e-05\n",
      "Epoch [11503/20000], Loss: 854.197021484375, Entropy 452.4926452636719, Learning Rate: 3.90625e-05\n",
      "Epoch [11504/20000], Loss: 884.2757568359375, Entropy 451.1932373046875, Learning Rate: 3.90625e-05\n",
      "Epoch [11505/20000], Loss: 835.2546997070312, Entropy 464.37432861328125, Learning Rate: 3.90625e-05\n",
      "Epoch [11506/20000], Loss: 871.1688232421875, Entropy 457.4145202636719, Learning Rate: 3.90625e-05\n",
      "Epoch [11507/20000], Loss: 872.0068359375, Entropy 451.2762756347656, Learning Rate: 3.90625e-05\n",
      "Epoch [11508/20000], Loss: 897.6756591796875, Entropy 461.3114013671875, Learning Rate: 3.90625e-05\n",
      "Epoch [11509/20000], Loss: 888.4600830078125, Entropy 464.7392883300781, Learning Rate: 3.90625e-05\n",
      "Epoch [11510/20000], Loss: 889.611328125, Entropy 440.9392395019531, Learning Rate: 3.90625e-05\n",
      "Epoch [11511/20000], Loss: 885.8185424804688, Entropy 452.69232177734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11512/20000], Loss: 764.5824584960938, Entropy 466.69732666015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11513/20000], Loss: 856.21435546875, Entropy 438.1048278808594, Learning Rate: 3.90625e-05\n",
      "Epoch [11514/20000], Loss: 827.9061889648438, Entropy 453.22100830078125, Learning Rate: 3.90625e-05\n",
      "Epoch [11515/20000], Loss: 850.799560546875, Entropy 455.8472595214844, Learning Rate: 3.90625e-05\n",
      "Epoch [11516/20000], Loss: 833.4471435546875, Entropy 470.2596740722656, Learning Rate: 3.90625e-05\n",
      "Epoch [11517/20000], Loss: 856.8321533203125, Entropy 464.71044921875, Learning Rate: 3.90625e-05\n",
      "Epoch [11518/20000], Loss: 858.081298828125, Entropy 460.759765625, Learning Rate: 3.90625e-05\n",
      "Epoch [11519/20000], Loss: 858.6691284179688, Entropy 469.26959228515625, Learning Rate: 3.90625e-05\n",
      "Epoch [11520/20000], Loss: 821.1009521484375, Entropy 457.5767822265625, Learning Rate: 3.90625e-05\n",
      "Epoch [11521/20000], Loss: 897.006103515625, Entropy 450.9216613769531, Learning Rate: 3.90625e-05\n",
      "Epoch [11522/20000], Loss: 824.951904296875, Entropy 453.9569396972656, Learning Rate: 3.90625e-05\n",
      "Epoch [11523/20000], Loss: 914.883544921875, Entropy 452.556396484375, Learning Rate: 3.90625e-05\n",
      "Epoch [11524/20000], Loss: 848.977783203125, Entropy 449.2379150390625, Learning Rate: 3.90625e-05\n",
      "Epoch [11525/20000], Loss: 825.9927978515625, Entropy 454.0649108886719, Learning Rate: 3.90625e-05\n",
      "Epoch [11526/20000], Loss: 833.8216552734375, Entropy 456.0826416015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11527/20000], Loss: 817.99951171875, Entropy 456.3567810058594, Learning Rate: 3.90625e-05\n",
      "Epoch [11528/20000], Loss: 840.0430908203125, Entropy 454.8014221191406, Learning Rate: 3.90625e-05\n",
      "Epoch [11529/20000], Loss: 843.764892578125, Entropy 461.4237060546875, Learning Rate: 3.90625e-05\n",
      "Epoch [11530/20000], Loss: 868.1082763671875, Entropy 447.1826477050781, Learning Rate: 3.90625e-05\n",
      "Epoch [11531/20000], Loss: 839.2260131835938, Entropy 448.35675048828125, Learning Rate: 3.90625e-05\n",
      "Epoch [11532/20000], Loss: 856.98388671875, Entropy 444.7161560058594, Learning Rate: 3.90625e-05\n",
      "Epoch [11533/20000], Loss: 843.539306640625, Entropy 449.7734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11534/20000], Loss: 888.1854248046875, Entropy 456.8775939941406, Learning Rate: 3.90625e-05\n",
      "Epoch [11535/20000], Loss: 860.0772705078125, Entropy 444.3434753417969, Learning Rate: 3.90625e-05\n",
      "Epoch [11536/20000], Loss: 853.5855712890625, Entropy 445.3625183105469, Learning Rate: 3.90625e-05\n",
      "Epoch [11537/20000], Loss: 881.9903564453125, Entropy 443.7033386230469, Learning Rate: 3.90625e-05\n",
      "Epoch [11538/20000], Loss: 883.1101684570312, Entropy 424.36224365234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11539/20000], Loss: 866.2401123046875, Entropy 457.4222412109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11540/20000], Loss: 845.6865234375, Entropy 438.2526550292969, Learning Rate: 3.90625e-05\n",
      "Epoch [11541/20000], Loss: 877.654541015625, Entropy 439.4711608886719, Learning Rate: 3.90625e-05\n",
      "Epoch [11542/20000], Loss: 858.893310546875, Entropy 479.0749206542969, Learning Rate: 3.90625e-05\n",
      "Epoch [11543/20000], Loss: 888.1943359375, Entropy 462.0484313964844, Learning Rate: 3.90625e-05\n",
      "Epoch [11544/20000], Loss: 869.694580078125, Entropy 442.0062255859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11545/20000], Loss: 843.8624267578125, Entropy 446.5533142089844, Learning Rate: 3.90625e-05\n",
      "Epoch [11546/20000], Loss: 880.04443359375, Entropy 424.8287353515625, Learning Rate: 3.90625e-05\n",
      "Epoch [11547/20000], Loss: 837.3615112304688, Entropy 450.53631591796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11548/20000], Loss: 850.595947265625, Entropy 449.1564636230469, Learning Rate: 3.90625e-05\n",
      "Epoch [11549/20000], Loss: 835.6024169921875, Entropy 448.4050598144531, Learning Rate: 3.90625e-05\n",
      "Epoch [11550/20000], Loss: 842.2509155273438, Entropy 446.95343017578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11551/20000], Loss: 874.7860107421875, Entropy 443.2876892089844, Learning Rate: 3.90625e-05\n",
      "Epoch [11552/20000], Loss: 863.2874145507812, Entropy 450.19866943359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11553/20000], Loss: 831.4105224609375, Entropy 456.2427978515625, Learning Rate: 3.90625e-05\n",
      "Epoch [11554/20000], Loss: 868.656005859375, Entropy 464.6364440917969, Learning Rate: 3.90625e-05\n",
      "Epoch [11555/20000], Loss: 819.11181640625, Entropy 452.3422546386719, Learning Rate: 3.90625e-05\n",
      "Epoch [11556/20000], Loss: 910.4483642578125, Entropy 450.9103698730469, Learning Rate: 3.90625e-05\n",
      "Epoch [11557/20000], Loss: 923.8157348632812, Entropy 451.88287353515625, Learning Rate: 3.90625e-05\n",
      "Epoch [11558/20000], Loss: 860.1500244140625, Entropy 454.4606018066406, Learning Rate: 3.90625e-05\n",
      "Epoch [11559/20000], Loss: 882.335693359375, Entropy 448.2461242675781, Learning Rate: 3.90625e-05\n",
      "Epoch [11560/20000], Loss: 837.804443359375, Entropy 459.1275329589844, Learning Rate: 3.90625e-05\n",
      "Epoch [11561/20000], Loss: 890.72119140625, Entropy 448.80810546875, Learning Rate: 3.90625e-05\n",
      "Epoch [11562/20000], Loss: 859.5352783203125, Entropy 451.2816162109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11563/20000], Loss: 892.856689453125, Entropy 466.2881164550781, Learning Rate: 3.90625e-05\n",
      "Epoch [11564/20000], Loss: 836.872314453125, Entropy 462.2091979980469, Learning Rate: 3.90625e-05\n",
      "Epoch [11565/20000], Loss: 880.7258911132812, Entropy 452.26116943359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11566/20000], Loss: 878.2125854492188, Entropy 458.12799072265625, Learning Rate: 3.90625e-05\n",
      "Epoch [11567/20000], Loss: 865.454833984375, Entropy 466.9510803222656, Learning Rate: 3.90625e-05\n",
      "Epoch [11568/20000], Loss: 843.1907958984375, Entropy 447.0050354003906, Learning Rate: 3.90625e-05\n",
      "Epoch [11569/20000], Loss: 880.763671875, Entropy 453.5950927734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11570/20000], Loss: 828.9951171875, Entropy 461.2652893066406, Learning Rate: 3.90625e-05\n",
      "Epoch [11571/20000], Loss: 891.1575317382812, Entropy 468.10858154296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11572/20000], Loss: 798.6882934570312, Entropy 471.06060791015625, Learning Rate: 3.90625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11573/20000], Loss: 848.2789916992188, Entropy 444.08062744140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11574/20000], Loss: 890.0020751953125, Entropy 457.7737731933594, Learning Rate: 3.90625e-05\n",
      "Epoch [11575/20000], Loss: 920.0679321289062, Entropy 444.13922119140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11576/20000], Loss: 871.9362182617188, Entropy 431.10174560546875, Learning Rate: 3.90625e-05\n",
      "Epoch [11577/20000], Loss: 862.8185424804688, Entropy 447.70428466796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11578/20000], Loss: 869.2754516601562, Entropy 450.16949462890625, Learning Rate: 3.90625e-05\n",
      "Epoch [11579/20000], Loss: 835.70458984375, Entropy 454.9028015136719, Learning Rate: 3.90625e-05\n",
      "Epoch [11580/20000], Loss: 798.715576171875, Entropy 450.2762756347656, Learning Rate: 3.90625e-05\n",
      "Epoch [11581/20000], Loss: 831.8695678710938, Entropy 453.05877685546875, Learning Rate: 3.90625e-05\n",
      "Epoch [11582/20000], Loss: 871.698974609375, Entropy 438.8199768066406, Learning Rate: 3.90625e-05\n",
      "Epoch [11583/20000], Loss: 842.6300048828125, Entropy 455.0080261230469, Learning Rate: 3.90625e-05\n",
      "Epoch [11584/20000], Loss: 866.5637817382812, Entropy 437.45526123046875, Learning Rate: 3.90625e-05\n",
      "Epoch [11585/20000], Loss: 854.2940673828125, Entropy 460.1607971191406, Learning Rate: 3.90625e-05\n",
      "Epoch [11586/20000], Loss: 838.0203857421875, Entropy 457.609375, Learning Rate: 3.90625e-05\n",
      "Epoch [11587/20000], Loss: 826.6502685546875, Entropy 471.4010009765625, Learning Rate: 3.90625e-05\n",
      "Epoch [11588/20000], Loss: 838.2257080078125, Entropy 450.9715270996094, Learning Rate: 3.90625e-05\n",
      "Epoch [11589/20000], Loss: 896.0394287109375, Entropy 444.7679138183594, Learning Rate: 3.90625e-05\n",
      "Epoch [11590/20000], Loss: 853.162353515625, Entropy 446.01220703125, Learning Rate: 3.90625e-05\n",
      "Epoch [11591/20000], Loss: 821.40234375, Entropy 463.0704650878906, Learning Rate: 3.90625e-05\n",
      "Epoch [11592/20000], Loss: 843.0855712890625, Entropy 470.2610168457031, Learning Rate: 3.90625e-05\n",
      "Epoch [11593/20000], Loss: 900.87451171875, Entropy 434.25, Learning Rate: 3.90625e-05\n",
      "Epoch [11594/20000], Loss: 860.5404052734375, Entropy 464.6529541015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11595/20000], Loss: 822.4068603515625, Entropy 458.2113952636719, Learning Rate: 3.90625e-05\n",
      "Epoch [11596/20000], Loss: 821.2554321289062, Entropy 455.32525634765625, Learning Rate: 3.90625e-05\n",
      "Epoch [11597/20000], Loss: 835.0643310546875, Entropy 463.6330871582031, Learning Rate: 3.90625e-05\n",
      "Epoch [11598/20000], Loss: 867.7789306640625, Entropy 444.6861267089844, Learning Rate: 3.90625e-05\n",
      "Epoch [11599/20000], Loss: 845.53125, Entropy 458.0893249511719, Learning Rate: 3.90625e-05\n",
      "Epoch [11600/20000], Loss: 823.8639526367188, Entropy 451.79718017578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11601/20000], Loss: 791.3546142578125, Entropy 463.8050537109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11602/20000], Loss: 826.2970581054688, Entropy 455.53375244140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11603/20000], Loss: 855.90087890625, Entropy 462.4021911621094, Learning Rate: 3.90625e-05\n",
      "Epoch [11604/20000], Loss: 822.9761962890625, Entropy 449.7210693359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11605/20000], Loss: 902.3768310546875, Entropy 458.7889099121094, Learning Rate: 3.90625e-05\n",
      "Epoch [11606/20000], Loss: 877.4296875, Entropy 439.8294372558594, Learning Rate: 3.90625e-05\n",
      "Epoch [11607/20000], Loss: 815.9705810546875, Entropy 449.2643127441406, Learning Rate: 3.90625e-05\n",
      "Epoch [11608/20000], Loss: 842.2693481445312, Entropy 454.31976318359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11609/20000], Loss: 904.75634765625, Entropy 444.2454528808594, Learning Rate: 3.90625e-05\n",
      "Epoch [11610/20000], Loss: 896.135498046875, Entropy 444.6932067871094, Learning Rate: 3.90625e-05\n",
      "Epoch [11611/20000], Loss: 886.2049560546875, Entropy 457.1669006347656, Learning Rate: 3.90625e-05\n",
      "Epoch [11612/20000], Loss: 872.7310791015625, Entropy 468.3160705566406, Learning Rate: 3.90625e-05\n",
      "Epoch [11613/20000], Loss: 841.4779052734375, Entropy 454.8450927734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11614/20000], Loss: 791.480712890625, Entropy 459.036865234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11615/20000], Loss: 864.82373046875, Entropy 451.0481262207031, Learning Rate: 3.90625e-05\n",
      "Epoch [11616/20000], Loss: 896.4965209960938, Entropy 441.01373291015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11617/20000], Loss: 864.0390625, Entropy 455.18701171875, Learning Rate: 3.90625e-05\n",
      "Epoch [11618/20000], Loss: 824.2728271484375, Entropy 469.6839294433594, Learning Rate: 3.90625e-05\n",
      "Epoch [11619/20000], Loss: 849.0404052734375, Entropy 444.8736877441406, Learning Rate: 3.90625e-05\n",
      "Epoch [11620/20000], Loss: 853.5211181640625, Entropy 457.035888671875, Learning Rate: 3.90625e-05\n",
      "Epoch [11621/20000], Loss: 840.8541259765625, Entropy 438.1109619140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11622/20000], Loss: 895.6222534179688, Entropy 461.27203369140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11623/20000], Loss: 880.640625, Entropy 447.7812805175781, Learning Rate: 3.90625e-05\n",
      "Epoch [11624/20000], Loss: 789.798095703125, Entropy 460.8827209472656, Learning Rate: 3.90625e-05\n",
      "Epoch [11625/20000], Loss: 840.40869140625, Entropy 453.304443359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11626/20000], Loss: 868.824951171875, Entropy 456.4983825683594, Learning Rate: 3.90625e-05\n",
      "Epoch [11627/20000], Loss: 821.5552368164062, Entropy 470.37396240234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11628/20000], Loss: 897.274658203125, Entropy 439.8905029296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11629/20000], Loss: 792.8758544921875, Entropy 471.29541015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11630/20000], Loss: 924.2996826171875, Entropy 431.5050048828125, Learning Rate: 3.90625e-05\n",
      "Epoch [11631/20000], Loss: 859.119384765625, Entropy 451.6781921386719, Learning Rate: 3.90625e-05\n",
      "Epoch [11632/20000], Loss: 835.2164306640625, Entropy 452.4664611816406, Learning Rate: 3.90625e-05\n",
      "Epoch [11633/20000], Loss: 899.001708984375, Entropy 460.3928527832031, Learning Rate: 3.90625e-05\n",
      "Epoch [11634/20000], Loss: 865.138671875, Entropy 447.7615966796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11635/20000], Loss: 867.7576293945312, Entropy 455.65814208984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11636/20000], Loss: 883.1458740234375, Entropy 447.5071716308594, Learning Rate: 3.90625e-05\n",
      "Epoch [11637/20000], Loss: 915.2078857421875, Entropy 446.2346496582031, Learning Rate: 3.90625e-05\n",
      "Epoch [11638/20000], Loss: 942.5177001953125, Entropy 455.1387023925781, Learning Rate: 3.90625e-05\n",
      "Epoch [11639/20000], Loss: 818.8560791015625, Entropy 455.9007568359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11640/20000], Loss: 850.944091796875, Entropy 450.9773864746094, Learning Rate: 3.90625e-05\n",
      "Epoch [11641/20000], Loss: 862.0257568359375, Entropy 455.5132751464844, Learning Rate: 3.90625e-05\n",
      "Epoch [11642/20000], Loss: 865.38623046875, Entropy 444.8576354980469, Learning Rate: 3.90625e-05\n",
      "Epoch [11643/20000], Loss: 872.363525390625, Entropy 438.6292724609375, Learning Rate: 3.90625e-05\n",
      "Epoch [11644/20000], Loss: 824.9239501953125, Entropy 458.3711242675781, Learning Rate: 3.90625e-05\n",
      "Epoch [11645/20000], Loss: 911.197021484375, Entropy 452.4319152832031, Learning Rate: 3.90625e-05\n",
      "Epoch [11646/20000], Loss: 874.2265014648438, Entropy 447.74908447265625, Learning Rate: 3.90625e-05\n",
      "Epoch [11647/20000], Loss: 871.9518432617188, Entropy 446.09967041015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11648/20000], Loss: 833.0789794921875, Entropy 455.5545959472656, Learning Rate: 3.90625e-05\n",
      "Epoch [11649/20000], Loss: 836.3326416015625, Entropy 450.3277282714844, Learning Rate: 3.90625e-05\n",
      "Epoch [11650/20000], Loss: 901.3801879882812, Entropy 453.23358154296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11651/20000], Loss: 842.2396240234375, Entropy 453.7710876464844, Learning Rate: 3.90625e-05\n",
      "Epoch [11652/20000], Loss: 874.4993286132812, Entropy 447.73529052734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11653/20000], Loss: 848.9761962890625, Entropy 459.3324890136719, Learning Rate: 3.90625e-05\n",
      "Epoch [11654/20000], Loss: 883.611572265625, Entropy 448.8550720214844, Learning Rate: 3.90625e-05\n",
      "Epoch [11655/20000], Loss: 856.170654296875, Entropy 461.30029296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11656/20000], Loss: 834.8673095703125, Entropy 456.8026123046875, Learning Rate: 3.90625e-05\n",
      "Epoch [11657/20000], Loss: 906.0341186523438, Entropy 451.23272705078125, Learning Rate: 3.90625e-05\n",
      "Epoch [11658/20000], Loss: 830.5050659179688, Entropy 455.31365966796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11659/20000], Loss: 812.09521484375, Entropy 463.07861328125, Learning Rate: 3.90625e-05\n",
      "Epoch [11660/20000], Loss: 866.772705078125, Entropy 467.132568359375, Learning Rate: 3.90625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11661/20000], Loss: 862.2493896484375, Entropy 458.8975830078125, Learning Rate: 3.90625e-05\n",
      "Epoch [11662/20000], Loss: 847.1611938476562, Entropy 441.89349365234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11663/20000], Loss: 845.9888916015625, Entropy 467.3138427734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11664/20000], Loss: 859.5255126953125, Entropy 443.9192199707031, Learning Rate: 3.90625e-05\n",
      "Epoch [11665/20000], Loss: 886.3258056640625, Entropy 449.7215881347656, Learning Rate: 3.90625e-05\n",
      "Epoch [11666/20000], Loss: 851.0703735351562, Entropy 456.41595458984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11667/20000], Loss: 878.58154296875, Entropy 460.8287658691406, Learning Rate: 3.90625e-05\n",
      "Epoch [11668/20000], Loss: 932.54296875, Entropy 452.5238952636719, Learning Rate: 3.90625e-05\n",
      "Epoch [11669/20000], Loss: 863.9910278320312, Entropy 455.02484130859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11670/20000], Loss: 852.957275390625, Entropy 449.9846496582031, Learning Rate: 3.90625e-05\n",
      "Epoch [11671/20000], Loss: 917.069091796875, Entropy 443.5883483886719, Learning Rate: 3.90625e-05\n",
      "Epoch [11672/20000], Loss: 891.439697265625, Entropy 434.3246765136719, Learning Rate: 3.90625e-05\n",
      "Epoch [11673/20000], Loss: 855.00537109375, Entropy 460.7423095703125, Learning Rate: 3.90625e-05\n",
      "Epoch [11674/20000], Loss: 886.9606323242188, Entropy 448.29351806640625, Learning Rate: 3.90625e-05\n",
      "Epoch [11675/20000], Loss: 884.0377197265625, Entropy 442.8006591796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11676/20000], Loss: 825.8028564453125, Entropy 462.4667053222656, Learning Rate: 3.90625e-05\n",
      "Epoch [11677/20000], Loss: 804.7335205078125, Entropy 460.3863220214844, Learning Rate: 3.90625e-05\n",
      "Epoch [11678/20000], Loss: 839.1593627929688, Entropy 451.64166259765625, Learning Rate: 3.90625e-05\n",
      "Epoch [11679/20000], Loss: 851.073486328125, Entropy 444.4339294433594, Learning Rate: 3.90625e-05\n",
      "Epoch [11680/20000], Loss: 830.2831420898438, Entropy 462.48260498046875, Learning Rate: 3.90625e-05\n",
      "Epoch [11681/20000], Loss: 873.0775146484375, Entropy 461.8359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11682/20000], Loss: 875.7752685546875, Entropy 460.0828857421875, Learning Rate: 3.90625e-05\n",
      "Epoch [11683/20000], Loss: 861.7290649414062, Entropy 446.91351318359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11684/20000], Loss: 884.925537109375, Entropy 461.2638244628906, Learning Rate: 3.90625e-05\n",
      "Epoch [11685/20000], Loss: 912.4823608398438, Entropy 450.40069580078125, Learning Rate: 3.90625e-05\n",
      "Epoch [11686/20000], Loss: 832.3201904296875, Entropy 451.1251220703125, Learning Rate: 3.90625e-05\n",
      "Epoch [11687/20000], Loss: 824.0542602539062, Entropy 469.61810302734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11688/20000], Loss: 881.6026611328125, Entropy 440.7698059082031, Learning Rate: 3.90625e-05\n",
      "Epoch [11689/20000], Loss: 887.2835693359375, Entropy 464.9339599609375, Learning Rate: 3.90625e-05\n",
      "Epoch [11690/20000], Loss: 851.8229370117188, Entropy 446.98858642578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11691/20000], Loss: 858.1303100585938, Entropy 445.57647705078125, Learning Rate: 3.90625e-05\n",
      "Epoch [11692/20000], Loss: 868.5032348632812, Entropy 452.16925048828125, Learning Rate: 3.90625e-05\n",
      "Epoch [11693/20000], Loss: 884.044921875, Entropy 448.2471008300781, Learning Rate: 3.90625e-05\n",
      "Epoch [11694/20000], Loss: 824.2489013671875, Entropy 461.443359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11695/20000], Loss: 871.7958984375, Entropy 450.0156555175781, Learning Rate: 3.90625e-05\n",
      "Epoch [11696/20000], Loss: 819.3405151367188, Entropy 455.29583740234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11697/20000], Loss: 813.1359252929688, Entropy 458.57318115234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11698/20000], Loss: 840.5925903320312, Entropy 458.39166259765625, Learning Rate: 3.90625e-05\n",
      "Epoch [11699/20000], Loss: 862.8182373046875, Entropy 442.33251953125, Learning Rate: 3.90625e-05\n",
      "Epoch [11700/20000], Loss: 857.997802734375, Entropy 442.7327880859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11701/20000], Loss: 856.3697509765625, Entropy 452.1510925292969, Learning Rate: 3.90625e-05\n",
      "Epoch [11702/20000], Loss: 838.041748046875, Entropy 463.6803894042969, Learning Rate: 3.90625e-05\n",
      "Epoch [11703/20000], Loss: 860.4478759765625, Entropy 447.1609802246094, Learning Rate: 3.90625e-05\n",
      "Epoch [11704/20000], Loss: 842.4278564453125, Entropy 454.4806213378906, Learning Rate: 3.90625e-05\n",
      "Epoch [11705/20000], Loss: 826.7847290039062, Entropy 460.63116455078125, Learning Rate: 3.90625e-05\n",
      "Epoch [11706/20000], Loss: 869.3037109375, Entropy 463.053466796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11707/20000], Loss: 803.9398193359375, Entropy 478.9435119628906, Learning Rate: 3.90625e-05\n",
      "Epoch [11708/20000], Loss: 845.39794921875, Entropy 453.4563903808594, Learning Rate: 3.90625e-05\n",
      "Epoch [11709/20000], Loss: 842.599853515625, Entropy 455.8287048339844, Learning Rate: 3.90625e-05\n",
      "Epoch [11710/20000], Loss: 810.713623046875, Entropy 459.2881164550781, Learning Rate: 3.90625e-05\n",
      "Epoch [11711/20000], Loss: 851.4262084960938, Entropy 456.97662353515625, Learning Rate: 3.90625e-05\n",
      "Epoch [11712/20000], Loss: 863.8807373046875, Entropy 454.5579528808594, Learning Rate: 3.90625e-05\n",
      "Epoch [11713/20000], Loss: 888.2301025390625, Entropy 466.33740234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11714/20000], Loss: 845.8770751953125, Entropy 462.96044921875, Learning Rate: 3.90625e-05\n",
      "Epoch [11715/20000], Loss: 906.0048217773438, Entropy 444.03765869140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11716/20000], Loss: 836.4375610351562, Entropy 462.44329833984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11717/20000], Loss: 827.634033203125, Entropy 458.8973083496094, Learning Rate: 3.90625e-05\n",
      "Epoch [11718/20000], Loss: 843.5819091796875, Entropy 459.0810852050781, Learning Rate: 3.90625e-05\n",
      "Epoch [11719/20000], Loss: 830.1558837890625, Entropy 445.6268005371094, Learning Rate: 3.90625e-05\n",
      "Epoch [11720/20000], Loss: 861.6527099609375, Entropy 448.4184265136719, Learning Rate: 3.90625e-05\n",
      "Epoch [11721/20000], Loss: 891.3782348632812, Entropy 445.09967041015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11722/20000], Loss: 804.2378540039062, Entropy 456.71148681640625, Learning Rate: 3.90625e-05\n",
      "Epoch [11723/20000], Loss: 858.5484008789062, Entropy 454.34564208984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11724/20000], Loss: 874.1956176757812, Entropy 458.09405517578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11725/20000], Loss: 853.59765625, Entropy 448.6921081542969, Learning Rate: 3.90625e-05\n",
      "Epoch [11726/20000], Loss: 852.705810546875, Entropy 426.3835144042969, Learning Rate: 3.90625e-05\n",
      "Epoch [11727/20000], Loss: 847.32861328125, Entropy 473.2034912109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11728/20000], Loss: 842.8934326171875, Entropy 443.8922424316406, Learning Rate: 3.90625e-05\n",
      "Epoch [11729/20000], Loss: 877.3211669921875, Entropy 448.4685363769531, Learning Rate: 3.90625e-05\n",
      "Epoch [11730/20000], Loss: 888.6229248046875, Entropy 458.9189453125, Learning Rate: 3.90625e-05\n",
      "Epoch [11731/20000], Loss: 841.6262817382812, Entropy 451.15545654296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11732/20000], Loss: 876.7672729492188, Entropy 439.71343994140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11733/20000], Loss: 835.2091064453125, Entropy 461.8274230957031, Learning Rate: 3.90625e-05\n",
      "Epoch [11734/20000], Loss: 887.4329833984375, Entropy 442.8265075683594, Learning Rate: 3.90625e-05\n",
      "Epoch [11735/20000], Loss: 871.2440795898438, Entropy 456.29437255859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11736/20000], Loss: 863.2962646484375, Entropy 448.9605712890625, Learning Rate: 3.90625e-05\n",
      "Epoch [11737/20000], Loss: 814.4742431640625, Entropy 471.2237243652344, Learning Rate: 3.90625e-05\n",
      "Epoch [11738/20000], Loss: 843.4146728515625, Entropy 451.3729553222656, Learning Rate: 3.90625e-05\n",
      "Epoch [11739/20000], Loss: 922.6372680664062, Entropy 437.91302490234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11740/20000], Loss: 887.368408203125, Entropy 456.2578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11741/20000], Loss: 815.86279296875, Entropy 455.9513244628906, Learning Rate: 3.90625e-05\n",
      "Epoch [11742/20000], Loss: 819.2481079101562, Entropy 461.95074462890625, Learning Rate: 3.90625e-05\n",
      "Epoch [11743/20000], Loss: 886.7587890625, Entropy 457.4346008300781, Learning Rate: 3.90625e-05\n",
      "Epoch [11744/20000], Loss: 854.67578125, Entropy 470.9826354980469, Learning Rate: 3.90625e-05\n",
      "Epoch [11745/20000], Loss: 835.8392333984375, Entropy 452.8988037109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11746/20000], Loss: 860.1019287109375, Entropy 439.4338684082031, Learning Rate: 3.90625e-05\n",
      "Epoch [11747/20000], Loss: 841.9453735351562, Entropy 446.64776611328125, Learning Rate: 3.90625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11748/20000], Loss: 839.9979248046875, Entropy 460.1301574707031, Learning Rate: 3.90625e-05\n",
      "Epoch [11749/20000], Loss: 854.3740844726562, Entropy 450.37701416015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11750/20000], Loss: 800.4734497070312, Entropy 457.83428955078125, Learning Rate: 3.90625e-05\n",
      "Epoch [11751/20000], Loss: 876.8907470703125, Entropy 466.9799499511719, Learning Rate: 3.90625e-05\n",
      "Epoch [11752/20000], Loss: 892.0089111328125, Entropy 447.8587341308594, Learning Rate: 3.90625e-05\n",
      "Epoch [11753/20000], Loss: 833.5069580078125, Entropy 464.1589660644531, Learning Rate: 3.90625e-05\n",
      "Epoch [11754/20000], Loss: 909.7825927734375, Entropy 456.7102355957031, Learning Rate: 3.90625e-05\n",
      "Epoch [11755/20000], Loss: 839.3804321289062, Entropy 455.50897216796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11756/20000], Loss: 871.8514404296875, Entropy 441.7938537597656, Learning Rate: 3.90625e-05\n",
      "Epoch [11757/20000], Loss: 832.6625366210938, Entropy 457.76873779296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11758/20000], Loss: 861.1065673828125, Entropy 450.392333984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11759/20000], Loss: 846.48681640625, Entropy 446.5223083496094, Learning Rate: 3.90625e-05\n",
      "Epoch [11760/20000], Loss: 848.7320556640625, Entropy 441.1363220214844, Learning Rate: 3.90625e-05\n",
      "Epoch [11761/20000], Loss: 880.6734008789062, Entropy 452.87066650390625, Learning Rate: 3.90625e-05\n",
      "Epoch [11762/20000], Loss: 879.771728515625, Entropy 455.1119384765625, Learning Rate: 3.90625e-05\n",
      "Epoch [11763/20000], Loss: 847.1624755859375, Entropy 453.4460754394531, Learning Rate: 3.90625e-05\n",
      "Epoch [11764/20000], Loss: 909.4509887695312, Entropy 446.20208740234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11765/20000], Loss: 852.4367065429688, Entropy 452.01019287109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11766/20000], Loss: 904.0183715820312, Entropy 434.34564208984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11767/20000], Loss: 866.7869873046875, Entropy 464.636474609375, Learning Rate: 3.90625e-05\n",
      "Epoch [11768/20000], Loss: 792.4437866210938, Entropy 463.63995361328125, Learning Rate: 3.90625e-05\n",
      "Epoch [11769/20000], Loss: 799.8388671875, Entropy 461.12255859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11770/20000], Loss: 909.757568359375, Entropy 462.3702392578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11771/20000], Loss: 835.7830810546875, Entropy 469.3045349121094, Learning Rate: 3.90625e-05\n",
      "Epoch [11772/20000], Loss: 838.268310546875, Entropy 455.9649963378906, Learning Rate: 3.90625e-05\n",
      "Epoch [11773/20000], Loss: 857.2489013671875, Entropy 465.697021484375, Learning Rate: 3.90625e-05\n",
      "Epoch [11774/20000], Loss: 880.7899169921875, Entropy 458.7347412109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11775/20000], Loss: 808.0436401367188, Entropy 458.67279052734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11776/20000], Loss: 821.2401123046875, Entropy 466.9227294921875, Learning Rate: 3.90625e-05\n",
      "Epoch [11777/20000], Loss: 826.0637817382812, Entropy 481.67779541015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11778/20000], Loss: 835.5799560546875, Entropy 472.4920959472656, Learning Rate: 3.90625e-05\n",
      "Epoch [11779/20000], Loss: 822.5320434570312, Entropy 486.36724853515625, Learning Rate: 3.90625e-05\n",
      "Epoch [11780/20000], Loss: 851.5543823242188, Entropy 450.38311767578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11781/20000], Loss: 829.4471435546875, Entropy 455.4696044921875, Learning Rate: 3.90625e-05\n",
      "Epoch [11782/20000], Loss: 858.8525390625, Entropy 447.4063720703125, Learning Rate: 3.90625e-05\n",
      "Epoch [11783/20000], Loss: 811.537841796875, Entropy 463.0122985839844, Learning Rate: 3.90625e-05\n",
      "Epoch [11784/20000], Loss: 847.588134765625, Entropy 455.4515075683594, Learning Rate: 3.90625e-05\n",
      "Epoch [11785/20000], Loss: 884.2427368164062, Entropy 459.32562255859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11786/20000], Loss: 835.2418212890625, Entropy 459.5198974609375, Learning Rate: 3.90625e-05\n",
      "Epoch [11787/20000], Loss: 836.3333740234375, Entropy 454.9143981933594, Learning Rate: 3.90625e-05\n",
      "Epoch [11788/20000], Loss: 831.3927001953125, Entropy 458.86279296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11789/20000], Loss: 801.84423828125, Entropy 459.5426330566406, Learning Rate: 3.90625e-05\n",
      "Epoch [11790/20000], Loss: 819.910400390625, Entropy 458.744140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11791/20000], Loss: 824.32861328125, Entropy 459.0637512207031, Learning Rate: 3.90625e-05\n",
      "Epoch [11792/20000], Loss: 885.3233642578125, Entropy 449.7750244140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11793/20000], Loss: 818.708984375, Entropy 460.4144287109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11794/20000], Loss: 821.254150390625, Entropy 471.7508850097656, Learning Rate: 3.90625e-05\n",
      "Epoch [11795/20000], Loss: 898.9624633789062, Entropy 438.00006103515625, Learning Rate: 3.90625e-05\n",
      "Epoch [11796/20000], Loss: 899.700439453125, Entropy 451.93115234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11797/20000], Loss: 792.8760986328125, Entropy 461.4248352050781, Learning Rate: 3.90625e-05\n",
      "Epoch [11798/20000], Loss: 852.2718505859375, Entropy 447.5289611816406, Learning Rate: 3.90625e-05\n",
      "Epoch [11799/20000], Loss: 867.0330810546875, Entropy 460.4009704589844, Learning Rate: 3.90625e-05\n",
      "Epoch [11800/20000], Loss: 830.5826416015625, Entropy 474.1103515625, Learning Rate: 3.90625e-05\n",
      "Epoch [11801/20000], Loss: 873.8184814453125, Entropy 459.5369873046875, Learning Rate: 3.90625e-05\n",
      "Epoch [11802/20000], Loss: 847.7515869140625, Entropy 466.0264587402344, Learning Rate: 3.90625e-05\n",
      "Epoch [11803/20000], Loss: 819.8548583984375, Entropy 440.4002380371094, Learning Rate: 3.90625e-05\n",
      "Epoch [11804/20000], Loss: 798.2763671875, Entropy 456.798583984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11805/20000], Loss: 852.9124755859375, Entropy 460.4909973144531, Learning Rate: 3.90625e-05\n",
      "Epoch [11806/20000], Loss: 874.1715698242188, Entropy 452.64398193359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11807/20000], Loss: 840.3364868164062, Entropy 456.85931396484375, Learning Rate: 3.90625e-05\n",
      "Epoch [11808/20000], Loss: 833.9185180664062, Entropy 442.74725341796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11809/20000], Loss: 936.8558349609375, Entropy 457.0495300292969, Learning Rate: 3.90625e-05\n",
      "Epoch [11810/20000], Loss: 864.3763427734375, Entropy 446.7935485839844, Learning Rate: 3.90625e-05\n",
      "Epoch [11811/20000], Loss: 842.8243408203125, Entropy 460.7233581542969, Learning Rate: 3.90625e-05\n",
      "Epoch [11812/20000], Loss: 878.7265625, Entropy 454.0007019042969, Learning Rate: 3.90625e-05\n",
      "Epoch [11813/20000], Loss: 914.2906494140625, Entropy 444.5745544433594, Learning Rate: 3.90625e-05\n",
      "Epoch [11814/20000], Loss: 833.220703125, Entropy 459.1619567871094, Learning Rate: 3.90625e-05\n",
      "Epoch [11815/20000], Loss: 815.9749755859375, Entropy 449.4309997558594, Learning Rate: 3.90625e-05\n",
      "Epoch [11816/20000], Loss: 900.0481567382812, Entropy 452.40655517578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11817/20000], Loss: 853.4349365234375, Entropy 460.5460205078125, Learning Rate: 3.90625e-05\n",
      "Epoch [11818/20000], Loss: 885.3860473632812, Entropy 449.68267822265625, Learning Rate: 3.90625e-05\n",
      "Epoch [11819/20000], Loss: 929.7166748046875, Entropy 451.3133239746094, Learning Rate: 3.90625e-05\n",
      "Epoch [11820/20000], Loss: 943.2919921875, Entropy 447.9033203125, Learning Rate: 3.90625e-05\n",
      "Epoch [11821/20000], Loss: 820.1250610351562, Entropy 464.44854736328125, Learning Rate: 3.90625e-05\n",
      "Epoch [11822/20000], Loss: 842.2781982421875, Entropy 451.96923828125, Learning Rate: 3.90625e-05\n",
      "Epoch [11823/20000], Loss: 866.3006591796875, Entropy 453.0142517089844, Learning Rate: 3.90625e-05\n",
      "Epoch [11824/20000], Loss: 915.027587890625, Entropy 451.9357604980469, Learning Rate: 3.90625e-05\n",
      "Epoch [11825/20000], Loss: 878.5556640625, Entropy 455.9029846191406, Learning Rate: 3.90625e-05\n",
      "Epoch [11826/20000], Loss: 864.7694091796875, Entropy 464.9871826171875, Learning Rate: 3.90625e-05\n",
      "Epoch [11827/20000], Loss: 867.1890258789062, Entropy 463.09466552734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11828/20000], Loss: 831.197265625, Entropy 467.2810974121094, Learning Rate: 3.90625e-05\n",
      "Epoch [11829/20000], Loss: 871.5528564453125, Entropy 453.3629150390625, Learning Rate: 3.90625e-05\n",
      "Epoch [11830/20000], Loss: 927.8927001953125, Entropy 446.59326171875, Learning Rate: 3.90625e-05\n",
      "Epoch [11831/20000], Loss: 873.3783569335938, Entropy 455.71929931640625, Learning Rate: 3.90625e-05\n",
      "Epoch [11832/20000], Loss: 863.1495361328125, Entropy 460.1519470214844, Learning Rate: 3.90625e-05\n",
      "Epoch [11833/20000], Loss: 841.62353515625, Entropy 460.2005310058594, Learning Rate: 3.90625e-05\n",
      "Epoch [11834/20000], Loss: 850.1647338867188, Entropy 450.03350830078125, Learning Rate: 3.90625e-05\n",
      "Epoch [11835/20000], Loss: 877.05859375, Entropy 463.5045166015625, Learning Rate: 3.90625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11836/20000], Loss: 833.4493408203125, Entropy 466.5748291015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11837/20000], Loss: 869.228271484375, Entropy 449.5857849121094, Learning Rate: 3.90625e-05\n",
      "Epoch [11838/20000], Loss: 860.3699951171875, Entropy 465.1740417480469, Learning Rate: 3.90625e-05\n",
      "Epoch [11839/20000], Loss: 823.4918212890625, Entropy 464.93896484375, Learning Rate: 3.90625e-05\n",
      "Epoch [11840/20000], Loss: 892.04150390625, Entropy 459.1131286621094, Learning Rate: 3.90625e-05\n",
      "Epoch [11841/20000], Loss: 836.19677734375, Entropy 450.3447570800781, Learning Rate: 3.90625e-05\n",
      "Epoch [11842/20000], Loss: 839.8190307617188, Entropy 463.82513427734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11843/20000], Loss: 856.0335083007812, Entropy 452.08734130859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11844/20000], Loss: 873.107177734375, Entropy 448.9017333984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11845/20000], Loss: 823.93408203125, Entropy 448.1568603515625, Learning Rate: 3.90625e-05\n",
      "Epoch [11846/20000], Loss: 879.011962890625, Entropy 458.5256042480469, Learning Rate: 3.90625e-05\n",
      "Epoch [11847/20000], Loss: 902.9832763671875, Entropy 454.454833984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11848/20000], Loss: 911.3890380859375, Entropy 445.2293701171875, Learning Rate: 3.90625e-05\n",
      "Epoch [11849/20000], Loss: 837.6861572265625, Entropy 459.354248046875, Learning Rate: 3.90625e-05\n",
      "Epoch [11850/20000], Loss: 887.9329833984375, Entropy 460.7172546386719, Learning Rate: 3.90625e-05\n",
      "Epoch [11851/20000], Loss: 873.72509765625, Entropy 458.07568359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11852/20000], Loss: 867.834228515625, Entropy 443.6915283203125, Learning Rate: 3.90625e-05\n",
      "Epoch [11853/20000], Loss: 859.5001220703125, Entropy 450.3146057128906, Learning Rate: 3.90625e-05\n",
      "Epoch [11854/20000], Loss: 860.092529296875, Entropy 446.0501403808594, Learning Rate: 3.90625e-05\n",
      "Epoch [11855/20000], Loss: 879.8389892578125, Entropy 450.6325378417969, Learning Rate: 3.90625e-05\n",
      "Epoch [11856/20000], Loss: 846.6624755859375, Entropy 460.8777770996094, Learning Rate: 3.90625e-05\n",
      "Epoch [11857/20000], Loss: 803.8870849609375, Entropy 461.6217956542969, Learning Rate: 3.90625e-05\n",
      "Epoch [11858/20000], Loss: 857.294921875, Entropy 453.989990234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11859/20000], Loss: 856.204833984375, Entropy 453.6182861328125, Learning Rate: 3.90625e-05\n",
      "Epoch [11860/20000], Loss: 895.6408081054688, Entropy 448.34039306640625, Learning Rate: 3.90625e-05\n",
      "Epoch [11861/20000], Loss: 869.2107543945312, Entropy 454.70111083984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11862/20000], Loss: 858.10791015625, Entropy 462.2069091796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11863/20000], Loss: 792.7080078125, Entropy 455.773193359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11864/20000], Loss: 854.459716796875, Entropy 451.4819030761719, Learning Rate: 3.90625e-05\n",
      "Epoch [11865/20000], Loss: 898.927490234375, Entropy 461.6341247558594, Learning Rate: 3.90625e-05\n",
      "Epoch [11866/20000], Loss: 880.73486328125, Entropy 459.8367919921875, Learning Rate: 3.90625e-05\n",
      "Epoch [11867/20000], Loss: 844.9115600585938, Entropy 470.18450927734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11868/20000], Loss: 810.114501953125, Entropy 452.4753723144531, Learning Rate: 3.90625e-05\n",
      "Epoch [11869/20000], Loss: 843.8450927734375, Entropy 453.31494140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11870/20000], Loss: 779.5128173828125, Entropy 465.1322021484375, Learning Rate: 3.90625e-05\n",
      "Epoch [11871/20000], Loss: 844.16015625, Entropy 465.78125, Learning Rate: 3.90625e-05\n",
      "Epoch [11872/20000], Loss: 843.5953369140625, Entropy 462.7757568359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11873/20000], Loss: 865.196044921875, Entropy 461.3588562011719, Learning Rate: 3.90625e-05\n",
      "Epoch [11874/20000], Loss: 852.9618530273438, Entropy 467.39312744140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11875/20000], Loss: 849.8709716796875, Entropy 464.3707275390625, Learning Rate: 3.90625e-05\n",
      "Epoch [11876/20000], Loss: 793.642333984375, Entropy 466.6271057128906, Learning Rate: 3.90625e-05\n",
      "Epoch [11877/20000], Loss: 872.9356689453125, Entropy 452.4906005859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11878/20000], Loss: 867.62451171875, Entropy 446.6819763183594, Learning Rate: 3.90625e-05\n",
      "Epoch [11879/20000], Loss: 867.2252197265625, Entropy 465.9617004394531, Learning Rate: 3.90625e-05\n",
      "Epoch [11880/20000], Loss: 877.3765869140625, Entropy 447.1893615722656, Learning Rate: 3.90625e-05\n",
      "Epoch [11881/20000], Loss: 857.103515625, Entropy 463.1413269042969, Learning Rate: 3.90625e-05\n",
      "Epoch [11882/20000], Loss: 861.7191772460938, Entropy 445.96685791015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11883/20000], Loss: 871.3564453125, Entropy 462.6329345703125, Learning Rate: 3.90625e-05\n",
      "Epoch [11884/20000], Loss: 881.408935546875, Entropy 447.0638122558594, Learning Rate: 3.90625e-05\n",
      "Epoch [11885/20000], Loss: 845.0684204101562, Entropy 452.67437744140625, Learning Rate: 3.90625e-05\n",
      "Epoch [11886/20000], Loss: 861.3878173828125, Entropy 443.4512023925781, Learning Rate: 3.90625e-05\n",
      "Epoch [11887/20000], Loss: 816.6655883789062, Entropy 468.98162841796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11888/20000], Loss: 832.6318359375, Entropy 459.3316345214844, Learning Rate: 3.90625e-05\n",
      "Epoch [11889/20000], Loss: 862.2312622070312, Entropy 442.96429443359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11890/20000], Loss: 837.6903076171875, Entropy 457.6567077636719, Learning Rate: 3.90625e-05\n",
      "Epoch [11891/20000], Loss: 879.8220825195312, Entropy 458.00408935546875, Learning Rate: 3.90625e-05\n",
      "Epoch [11892/20000], Loss: 826.0816650390625, Entropy 458.6321716308594, Learning Rate: 3.90625e-05\n",
      "Epoch [11893/20000], Loss: 844.8367309570312, Entropy 442.39483642578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11894/20000], Loss: 857.109619140625, Entropy 468.3030090332031, Learning Rate: 3.90625e-05\n",
      "Epoch [11895/20000], Loss: 892.629150390625, Entropy 440.5696716308594, Learning Rate: 3.90625e-05\n",
      "Epoch [11896/20000], Loss: 882.1993408203125, Entropy 445.3248596191406, Learning Rate: 3.90625e-05\n",
      "Epoch [11897/20000], Loss: 796.451171875, Entropy 468.2986145019531, Learning Rate: 3.90625e-05\n",
      "Epoch [11898/20000], Loss: 831.19921875, Entropy 453.7809143066406, Learning Rate: 3.90625e-05\n",
      "Epoch [11899/20000], Loss: 785.4075927734375, Entropy 461.2252502441406, Learning Rate: 3.90625e-05\n",
      "Epoch [11900/20000], Loss: 856.027587890625, Entropy 451.4319763183594, Learning Rate: 3.90625e-05\n",
      "Epoch [11901/20000], Loss: 828.0236206054688, Entropy 456.10504150390625, Learning Rate: 3.90625e-05\n",
      "Epoch [11902/20000], Loss: 837.842529296875, Entropy 458.9812316894531, Learning Rate: 3.90625e-05\n",
      "Epoch [11903/20000], Loss: 850.0313720703125, Entropy 465.5435791015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11904/20000], Loss: 892.209228515625, Entropy 453.5346374511719, Learning Rate: 3.90625e-05\n",
      "Epoch [11905/20000], Loss: 860.5249633789062, Entropy 461.25604248046875, Learning Rate: 3.90625e-05\n",
      "Epoch [11906/20000], Loss: 839.4680786132812, Entropy 458.22381591796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11907/20000], Loss: 832.0107421875, Entropy 463.2935485839844, Learning Rate: 3.90625e-05\n",
      "Epoch [11908/20000], Loss: 841.271240234375, Entropy 461.4112854003906, Learning Rate: 3.90625e-05\n",
      "Epoch [11909/20000], Loss: 869.7188720703125, Entropy 465.4979553222656, Learning Rate: 3.90625e-05\n",
      "Epoch [11910/20000], Loss: 878.0235595703125, Entropy 445.6907958984375, Learning Rate: 3.90625e-05\n",
      "Epoch [11911/20000], Loss: 829.516357421875, Entropy 447.5272521972656, Learning Rate: 3.90625e-05\n",
      "Epoch [11912/20000], Loss: 833.5079345703125, Entropy 460.3287658691406, Learning Rate: 3.90625e-05\n",
      "Epoch [11913/20000], Loss: 869.3118896484375, Entropy 459.419677734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11914/20000], Loss: 823.6873168945312, Entropy 449.36749267578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11915/20000], Loss: 851.683837890625, Entropy 452.3504638671875, Learning Rate: 3.90625e-05\n",
      "Epoch [11916/20000], Loss: 885.2705078125, Entropy 451.1050720214844, Learning Rate: 3.90625e-05\n",
      "Epoch [11917/20000], Loss: 920.6658935546875, Entropy 439.8084411621094, Learning Rate: 3.90625e-05\n",
      "Epoch [11918/20000], Loss: 831.4951782226562, Entropy 456.60443115234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11919/20000], Loss: 830.06298828125, Entropy 472.6677551269531, Learning Rate: 3.90625e-05\n",
      "Epoch [11920/20000], Loss: 848.71826171875, Entropy 454.044677734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11921/20000], Loss: 866.056884765625, Entropy 467.5828857421875, Learning Rate: 3.90625e-05\n",
      "Epoch [11922/20000], Loss: 862.1911010742188, Entropy 460.64666748046875, Learning Rate: 3.90625e-05\n",
      "Epoch [11923/20000], Loss: 861.4874267578125, Entropy 459.0679626464844, Learning Rate: 3.90625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11924/20000], Loss: 845.6904296875, Entropy 454.5533447265625, Learning Rate: 3.90625e-05\n",
      "Epoch [11925/20000], Loss: 818.0411376953125, Entropy 466.2420959472656, Learning Rate: 3.90625e-05\n",
      "Epoch [11926/20000], Loss: 808.5374755859375, Entropy 448.5643005371094, Learning Rate: 3.90625e-05\n",
      "Epoch [11927/20000], Loss: 846.8920288085938, Entropy 469.36077880859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11928/20000], Loss: 807.9940185546875, Entropy 445.2962341308594, Learning Rate: 3.90625e-05\n",
      "Epoch [11929/20000], Loss: 922.692626953125, Entropy 450.6661682128906, Learning Rate: 3.90625e-05\n",
      "Epoch [11930/20000], Loss: 849.8159790039062, Entropy 472.10272216796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11931/20000], Loss: 803.6849365234375, Entropy 469.6773986816406, Learning Rate: 3.90625e-05\n",
      "Epoch [11932/20000], Loss: 840.7923583984375, Entropy 459.5748291015625, Learning Rate: 3.90625e-05\n",
      "Epoch [11933/20000], Loss: 867.216552734375, Entropy 453.1399841308594, Learning Rate: 3.90625e-05\n",
      "Epoch [11934/20000], Loss: 831.6864013671875, Entropy 444.2729187011719, Learning Rate: 3.90625e-05\n",
      "Epoch [11935/20000], Loss: 838.8616943359375, Entropy 466.3004150390625, Learning Rate: 3.90625e-05\n",
      "Epoch [11936/20000], Loss: 872.8414306640625, Entropy 456.4221496582031, Learning Rate: 3.90625e-05\n",
      "Epoch [11937/20000], Loss: 880.3108520507812, Entropy 446.96624755859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11938/20000], Loss: 867.511962890625, Entropy 462.5964050292969, Learning Rate: 3.90625e-05\n",
      "Epoch [11939/20000], Loss: 846.5941772460938, Entropy 441.64849853515625, Learning Rate: 3.90625e-05\n",
      "Epoch [11940/20000], Loss: 931.2532958984375, Entropy 450.0481872558594, Learning Rate: 3.90625e-05\n",
      "Epoch [11941/20000], Loss: 855.57421875, Entropy 459.4613952636719, Learning Rate: 3.90625e-05\n",
      "Epoch [11942/20000], Loss: 816.9361572265625, Entropy 466.2995910644531, Learning Rate: 3.90625e-05\n",
      "Epoch [11943/20000], Loss: 892.4649658203125, Entropy 456.5221862792969, Learning Rate: 3.90625e-05\n",
      "Epoch [11944/20000], Loss: 806.1104736328125, Entropy 454.2916564941406, Learning Rate: 3.90625e-05\n",
      "Epoch [11945/20000], Loss: 795.265380859375, Entropy 461.181640625, Learning Rate: 3.90625e-05\n",
      "Epoch [11946/20000], Loss: 843.4849243164062, Entropy 458.05755615234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11947/20000], Loss: 909.41259765625, Entropy 453.6356201171875, Learning Rate: 3.90625e-05\n",
      "Epoch [11948/20000], Loss: 861.6670532226562, Entropy 463.14007568359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11949/20000], Loss: 847.114013671875, Entropy 436.9968566894531, Learning Rate: 3.90625e-05\n",
      "Epoch [11950/20000], Loss: 866.804931640625, Entropy 462.1634521484375, Learning Rate: 3.90625e-05\n",
      "Epoch [11951/20000], Loss: 868.6295166015625, Entropy 456.6358642578125, Learning Rate: 3.90625e-05\n",
      "Epoch [11952/20000], Loss: 893.46142578125, Entropy 451.2122497558594, Learning Rate: 3.90625e-05\n",
      "Epoch [11953/20000], Loss: 956.6600341796875, Entropy 458.7205810546875, Learning Rate: 3.90625e-05\n",
      "Epoch [11954/20000], Loss: 872.5255126953125, Entropy 458.2478942871094, Learning Rate: 3.90625e-05\n",
      "Epoch [11955/20000], Loss: 875.942626953125, Entropy 463.1639404296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11956/20000], Loss: 889.8414306640625, Entropy 446.7613830566406, Learning Rate: 3.90625e-05\n",
      "Epoch [11957/20000], Loss: 855.2625732421875, Entropy 456.1551513671875, Learning Rate: 3.90625e-05\n",
      "Epoch [11958/20000], Loss: 880.381591796875, Entropy 454.5452880859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11959/20000], Loss: 868.8695068359375, Entropy 439.1310729980469, Learning Rate: 3.90625e-05\n",
      "Epoch [11960/20000], Loss: 878.3651123046875, Entropy 460.7326354980469, Learning Rate: 3.90625e-05\n",
      "Epoch [11961/20000], Loss: 835.81640625, Entropy 469.0403747558594, Learning Rate: 3.90625e-05\n",
      "Epoch [11962/20000], Loss: 894.6605224609375, Entropy 441.7975769042969, Learning Rate: 3.90625e-05\n",
      "Epoch [11963/20000], Loss: 889.9959716796875, Entropy 455.8385009765625, Learning Rate: 3.90625e-05\n",
      "Epoch [11964/20000], Loss: 860.32275390625, Entropy 453.452880859375, Learning Rate: 3.90625e-05\n",
      "Epoch [11965/20000], Loss: 829.4754638671875, Entropy 458.25390625, Learning Rate: 3.90625e-05\n",
      "Epoch [11966/20000], Loss: 842.112060546875, Entropy 461.0927734375, Learning Rate: 3.90625e-05\n",
      "Epoch [11967/20000], Loss: 838.3072509765625, Entropy 468.9485778808594, Learning Rate: 3.90625e-05\n",
      "Epoch [11968/20000], Loss: 888.57861328125, Entropy 455.4119567871094, Learning Rate: 3.90625e-05\n",
      "Epoch [11969/20000], Loss: 875.9242553710938, Entropy 452.44842529296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11970/20000], Loss: 845.9674072265625, Entropy 449.7186279296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11971/20000], Loss: 863.9308471679688, Entropy 449.46490478515625, Learning Rate: 3.90625e-05\n",
      "Epoch [11972/20000], Loss: 819.7822265625, Entropy 461.4236145019531, Learning Rate: 3.90625e-05\n",
      "Epoch [11973/20000], Loss: 869.45263671875, Entropy 445.23388671875, Learning Rate: 3.90625e-05\n",
      "Epoch [11974/20000], Loss: 886.507080078125, Entropy 454.3085021972656, Learning Rate: 3.90625e-05\n",
      "Epoch [11975/20000], Loss: 850.7446899414062, Entropy 474.32147216796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11976/20000], Loss: 841.8133544921875, Entropy 449.9534912109375, Learning Rate: 3.90625e-05\n",
      "Epoch [11977/20000], Loss: 861.115234375, Entropy 455.6298828125, Learning Rate: 3.90625e-05\n",
      "Epoch [11978/20000], Loss: 865.3323974609375, Entropy 455.2737121582031, Learning Rate: 3.90625e-05\n",
      "Epoch [11979/20000], Loss: 851.3009033203125, Entropy 451.1969299316406, Learning Rate: 3.90625e-05\n",
      "Epoch [11980/20000], Loss: 873.641845703125, Entropy 443.3221130371094, Learning Rate: 3.90625e-05\n",
      "Epoch [11981/20000], Loss: 835.832763671875, Entropy 459.3515930175781, Learning Rate: 3.90625e-05\n",
      "Epoch [11982/20000], Loss: 891.5088500976562, Entropy 458.06951904296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11983/20000], Loss: 835.736572265625, Entropy 463.7926330566406, Learning Rate: 3.90625e-05\n",
      "Epoch [11984/20000], Loss: 840.935546875, Entropy 451.9309997558594, Learning Rate: 3.90625e-05\n",
      "Epoch [11985/20000], Loss: 849.188720703125, Entropy 446.803466796875, Learning Rate: 3.90625e-05\n",
      "Epoch [11986/20000], Loss: 871.8223876953125, Entropy 455.9499206542969, Learning Rate: 3.90625e-05\n",
      "Epoch [11987/20000], Loss: 861.0244140625, Entropy 451.5997619628906, Learning Rate: 3.90625e-05\n",
      "Epoch [11988/20000], Loss: 857.84375, Entropy 453.1468200683594, Learning Rate: 3.90625e-05\n",
      "Epoch [11989/20000], Loss: 832.4296875, Entropy 467.2242431640625, Learning Rate: 3.90625e-05\n",
      "Epoch [11990/20000], Loss: 881.200439453125, Entropy 465.9742736816406, Learning Rate: 3.90625e-05\n",
      "Epoch [11991/20000], Loss: 813.4202880859375, Entropy 467.8826904296875, Learning Rate: 3.90625e-05\n",
      "Epoch [11992/20000], Loss: 812.869384765625, Entropy 464.0130615234375, Learning Rate: 3.90625e-05\n",
      "Epoch [11993/20000], Loss: 871.837158203125, Entropy 450.8531188964844, Learning Rate: 3.90625e-05\n",
      "Epoch [11994/20000], Loss: 886.5223999023438, Entropy 450.77398681640625, Learning Rate: 3.90625e-05\n",
      "Epoch [11995/20000], Loss: 864.8944091796875, Entropy 461.4214782714844, Learning Rate: 3.90625e-05\n",
      "Epoch [11996/20000], Loss: 881.2145385742188, Entropy 457.17523193359375, Learning Rate: 3.90625e-05\n",
      "Epoch [11997/20000], Loss: 839.1085205078125, Entropy 457.1360778808594, Learning Rate: 3.90625e-05\n",
      "Epoch [11998/20000], Loss: 861.28369140625, Entropy 462.2846374511719, Learning Rate: 3.90625e-05\n",
      "Epoch [11999/20000], Loss: 831.326171875, Entropy 454.1117858886719, Learning Rate: 3.90625e-05\n",
      "Epoch [12000/20000], Loss: 890.0179443359375, Entropy 450.45361328125, Learning Rate: 3.90625e-05\n",
      "Epoch [12001/20000], Loss: 837.5496826171875, Entropy 460.9291687011719, Learning Rate: 3.90625e-05\n",
      "Epoch [12002/20000], Loss: 809.4815673828125, Entropy 459.5954284667969, Learning Rate: 3.90625e-05\n",
      "Epoch [12003/20000], Loss: 859.1777954101562, Entropy 469.39215087890625, Learning Rate: 3.90625e-05\n",
      "Epoch [12004/20000], Loss: 801.4454345703125, Entropy 457.15185546875, Learning Rate: 3.90625e-05\n",
      "Epoch [12005/20000], Loss: 914.999755859375, Entropy 451.4422607421875, Learning Rate: 3.90625e-05\n",
      "Epoch [12006/20000], Loss: 851.8714599609375, Entropy 454.1686706542969, Learning Rate: 3.90625e-05\n",
      "Epoch [12007/20000], Loss: 850.92041015625, Entropy 447.3521728515625, Learning Rate: 3.90625e-05\n",
      "Epoch [12008/20000], Loss: 873.9368896484375, Entropy 452.0655822753906, Learning Rate: 3.90625e-05\n",
      "Epoch [12009/20000], Loss: 838.5496826171875, Entropy 464.4236755371094, Learning Rate: 3.90625e-05\n",
      "Epoch [12010/20000], Loss: 836.83984375, Entropy 467.3564758300781, Learning Rate: 3.90625e-05\n",
      "Epoch [12011/20000], Loss: 883.73974609375, Entropy 457.4836120605469, Learning Rate: 3.90625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12012/20000], Loss: 872.647705078125, Entropy 447.4430236816406, Learning Rate: 3.90625e-05\n",
      "Epoch [12013/20000], Loss: 863.7322998046875, Entropy 455.4383544921875, Learning Rate: 3.90625e-05\n",
      "Epoch [12014/20000], Loss: 882.1485595703125, Entropy 448.2818298339844, Learning Rate: 1.953125e-05\n",
      "Epoch [12015/20000], Loss: 806.4453125, Entropy 460.4733581542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12016/20000], Loss: 826.7918701171875, Entropy 464.9778137207031, Learning Rate: 1.953125e-05\n",
      "Epoch [12017/20000], Loss: 872.7015380859375, Entropy 451.1868591308594, Learning Rate: 1.953125e-05\n",
      "Epoch [12018/20000], Loss: 815.1566162109375, Entropy 467.7664794921875, Learning Rate: 1.953125e-05\n",
      "Epoch [12019/20000], Loss: 875.1821899414062, Entropy 456.33038330078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12020/20000], Loss: 816.648681640625, Entropy 451.0123291015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12021/20000], Loss: 882.6007690429688, Entropy 455.25311279296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12022/20000], Loss: 928.6695556640625, Entropy 426.841796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12023/20000], Loss: 843.9803466796875, Entropy 451.41064453125, Learning Rate: 1.953125e-05\n",
      "Epoch [12024/20000], Loss: 942.8958740234375, Entropy 448.2684326171875, Learning Rate: 1.953125e-05\n",
      "Epoch [12025/20000], Loss: 885.2885131835938, Entropy 457.51129150390625, Learning Rate: 1.953125e-05\n",
      "Epoch [12026/20000], Loss: 878.5343017578125, Entropy 455.3708190917969, Learning Rate: 1.953125e-05\n",
      "Epoch [12027/20000], Loss: 863.9967041015625, Entropy 452.1565246582031, Learning Rate: 1.953125e-05\n",
      "Epoch [12028/20000], Loss: 860.0889892578125, Entropy 466.2209167480469, Learning Rate: 1.953125e-05\n",
      "Epoch [12029/20000], Loss: 875.7913818359375, Entropy 462.6608581542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12030/20000], Loss: 830.385986328125, Entropy 455.8123474121094, Learning Rate: 1.953125e-05\n",
      "Epoch [12031/20000], Loss: 872.411865234375, Entropy 440.6784973144531, Learning Rate: 1.953125e-05\n",
      "Epoch [12032/20000], Loss: 877.9832763671875, Entropy 439.8443298339844, Learning Rate: 1.953125e-05\n",
      "Epoch [12033/20000], Loss: 840.7901000976562, Entropy 455.66998291015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12034/20000], Loss: 842.1522216796875, Entropy 452.3267822265625, Learning Rate: 1.953125e-05\n",
      "Epoch [12035/20000], Loss: 866.2318115234375, Entropy 456.51953125, Learning Rate: 1.953125e-05\n",
      "Epoch [12036/20000], Loss: 843.9241943359375, Entropy 465.053466796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12037/20000], Loss: 830.8717041015625, Entropy 469.7867126464844, Learning Rate: 1.953125e-05\n",
      "Epoch [12038/20000], Loss: 856.8713989257812, Entropy 454.25433349609375, Learning Rate: 1.953125e-05\n",
      "Epoch [12039/20000], Loss: 862.3485107421875, Entropy 461.5649108886719, Learning Rate: 1.953125e-05\n",
      "Epoch [12040/20000], Loss: 914.1517333984375, Entropy 430.4222412109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12041/20000], Loss: 920.026611328125, Entropy 441.2554931640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12042/20000], Loss: 829.8220825195312, Entropy 447.16351318359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12043/20000], Loss: 825.1845703125, Entropy 447.6898193359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12044/20000], Loss: 831.4530639648438, Entropy 458.91070556640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12045/20000], Loss: 803.1937866210938, Entropy 447.72076416015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12046/20000], Loss: 875.8678588867188, Entropy 452.11273193359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12047/20000], Loss: 894.31103515625, Entropy 456.7750549316406, Learning Rate: 1.953125e-05\n",
      "Epoch [12048/20000], Loss: 900.9188232421875, Entropy 445.5202941894531, Learning Rate: 1.953125e-05\n",
      "Epoch [12049/20000], Loss: 834.63623046875, Entropy 472.1520080566406, Learning Rate: 1.953125e-05\n",
      "Epoch [12050/20000], Loss: 815.0947265625, Entropy 452.3122253417969, Learning Rate: 1.953125e-05\n",
      "Epoch [12051/20000], Loss: 863.7701416015625, Entropy 452.7529602050781, Learning Rate: 1.953125e-05\n",
      "Epoch [12052/20000], Loss: 873.489501953125, Entropy 464.2749938964844, Learning Rate: 1.953125e-05\n",
      "Epoch [12053/20000], Loss: 856.9259033203125, Entropy 463.1750793457031, Learning Rate: 1.953125e-05\n",
      "Epoch [12054/20000], Loss: 863.4714965820312, Entropy 466.35687255859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12055/20000], Loss: 869.6937255859375, Entropy 461.2325744628906, Learning Rate: 1.953125e-05\n",
      "Epoch [12056/20000], Loss: 860.5625, Entropy 449.5987548828125, Learning Rate: 1.953125e-05\n",
      "Epoch [12057/20000], Loss: 818.771484375, Entropy 461.9071044921875, Learning Rate: 1.953125e-05\n",
      "Epoch [12058/20000], Loss: 870.71630859375, Entropy 465.2313232421875, Learning Rate: 1.953125e-05\n",
      "Epoch [12059/20000], Loss: 817.2575073242188, Entropy 463.84686279296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12060/20000], Loss: 827.096923828125, Entropy 456.8468017578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12061/20000], Loss: 844.9990234375, Entropy 452.0031433105469, Learning Rate: 1.953125e-05\n",
      "Epoch [12062/20000], Loss: 868.58935546875, Entropy 451.6262512207031, Learning Rate: 1.953125e-05\n",
      "Epoch [12063/20000], Loss: 872.332275390625, Entropy 460.714599609375, Learning Rate: 1.953125e-05\n",
      "Epoch [12064/20000], Loss: 881.6937255859375, Entropy 449.1981201171875, Learning Rate: 1.953125e-05\n",
      "Epoch [12065/20000], Loss: 842.1991577148438, Entropy 445.93011474609375, Learning Rate: 1.953125e-05\n",
      "Epoch [12066/20000], Loss: 863.08544921875, Entropy 451.6373291015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12067/20000], Loss: 914.1416015625, Entropy 442.4991455078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12068/20000], Loss: 852.6133422851562, Entropy 458.50836181640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12069/20000], Loss: 850.033935546875, Entropy 453.6785888671875, Learning Rate: 1.953125e-05\n",
      "Epoch [12070/20000], Loss: 913.9525146484375, Entropy 446.2738037109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12071/20000], Loss: 843.60205078125, Entropy 456.15771484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12072/20000], Loss: 866.3358764648438, Entropy 434.22406005859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12073/20000], Loss: 898.038818359375, Entropy 463.2529602050781, Learning Rate: 1.953125e-05\n",
      "Epoch [12074/20000], Loss: 854.619873046875, Entropy 459.5904846191406, Learning Rate: 1.953125e-05\n",
      "Epoch [12075/20000], Loss: 891.3414306640625, Entropy 441.0338134765625, Learning Rate: 1.953125e-05\n",
      "Epoch [12076/20000], Loss: 850.7686767578125, Entropy 449.4518127441406, Learning Rate: 1.953125e-05\n",
      "Epoch [12077/20000], Loss: 872.633056640625, Entropy 462.3565673828125, Learning Rate: 1.953125e-05\n",
      "Epoch [12078/20000], Loss: 850.9584350585938, Entropy 453.80218505859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12079/20000], Loss: 828.66552734375, Entropy 454.4031066894531, Learning Rate: 1.953125e-05\n",
      "Epoch [12080/20000], Loss: 956.3831787109375, Entropy 446.3188171386719, Learning Rate: 1.953125e-05\n",
      "Epoch [12081/20000], Loss: 855.7855834960938, Entropy 462.07171630859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12082/20000], Loss: 856.1668701171875, Entropy 465.6109313964844, Learning Rate: 1.953125e-05\n",
      "Epoch [12083/20000], Loss: 837.4550170898438, Entropy 460.86053466796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12084/20000], Loss: 823.40185546875, Entropy 465.1678161621094, Learning Rate: 1.953125e-05\n",
      "Epoch [12085/20000], Loss: 796.45947265625, Entropy 469.2525329589844, Learning Rate: 1.953125e-05\n",
      "Epoch [12086/20000], Loss: 875.4754638671875, Entropy 462.5924987792969, Learning Rate: 1.953125e-05\n",
      "Epoch [12087/20000], Loss: 888.0491333007812, Entropy 440.55291748046875, Learning Rate: 1.953125e-05\n",
      "Epoch [12088/20000], Loss: 888.7928466796875, Entropy 440.7325439453125, Learning Rate: 1.953125e-05\n",
      "Epoch [12089/20000], Loss: 841.27294921875, Entropy 476.8102111816406, Learning Rate: 1.953125e-05\n",
      "Epoch [12090/20000], Loss: 871.474609375, Entropy 451.0069580078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12091/20000], Loss: 840.201904296875, Entropy 438.5538330078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12092/20000], Loss: 921.7638549804688, Entropy 442.91326904296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12093/20000], Loss: 902.2578125, Entropy 448.5998229980469, Learning Rate: 1.953125e-05\n",
      "Epoch [12094/20000], Loss: 862.175048828125, Entropy 454.7342224121094, Learning Rate: 1.953125e-05\n",
      "Epoch [12095/20000], Loss: 868.5952758789062, Entropy 434.28021240234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12096/20000], Loss: 892.0704345703125, Entropy 439.7197265625, Learning Rate: 1.953125e-05\n",
      "Epoch [12097/20000], Loss: 874.4068603515625, Entropy 441.8880615234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12098/20000], Loss: 849.9437255859375, Entropy 446.470458984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12099/20000], Loss: 861.3812255859375, Entropy 437.9168395996094, Learning Rate: 1.953125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12100/20000], Loss: 884.5679931640625, Entropy 442.0826721191406, Learning Rate: 1.953125e-05\n",
      "Epoch [12101/20000], Loss: 889.343017578125, Entropy 434.5495300292969, Learning Rate: 1.953125e-05\n",
      "Epoch [12102/20000], Loss: 795.1923828125, Entropy 457.927490234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12103/20000], Loss: 920.2124633789062, Entropy 454.06170654296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12104/20000], Loss: 860.2827758789062, Entropy 460.22906494140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12105/20000], Loss: 846.1724853515625, Entropy 458.4851379394531, Learning Rate: 1.953125e-05\n",
      "Epoch [12106/20000], Loss: 842.8614501953125, Entropy 443.6217041015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12107/20000], Loss: 800.8045043945312, Entropy 484.30010986328125, Learning Rate: 1.953125e-05\n",
      "Epoch [12108/20000], Loss: 844.1205444335938, Entropy 464.48468017578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12109/20000], Loss: 881.251708984375, Entropy 461.53173828125, Learning Rate: 1.953125e-05\n",
      "Epoch [12110/20000], Loss: 851.07861328125, Entropy 460.4856262207031, Learning Rate: 1.953125e-05\n",
      "Epoch [12111/20000], Loss: 997.9202880859375, Entropy 459.15478515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12112/20000], Loss: 829.9361572265625, Entropy 466.6804504394531, Learning Rate: 1.953125e-05\n",
      "Epoch [12113/20000], Loss: 857.6561279296875, Entropy 466.1586608886719, Learning Rate: 1.953125e-05\n",
      "Epoch [12114/20000], Loss: 897.9600830078125, Entropy 462.3391418457031, Learning Rate: 1.953125e-05\n",
      "Epoch [12115/20000], Loss: 857.7728271484375, Entropy 450.4498291015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12116/20000], Loss: 916.8377685546875, Entropy 463.83349609375, Learning Rate: 1.953125e-05\n",
      "Epoch [12117/20000], Loss: 871.46875, Entropy 442.0379333496094, Learning Rate: 1.953125e-05\n",
      "Epoch [12118/20000], Loss: 890.5333251953125, Entropy 446.1120300292969, Learning Rate: 1.953125e-05\n",
      "Epoch [12119/20000], Loss: 809.1328125, Entropy 466.0672302246094, Learning Rate: 1.953125e-05\n",
      "Epoch [12120/20000], Loss: 818.6170654296875, Entropy 455.3397521972656, Learning Rate: 1.953125e-05\n",
      "Epoch [12121/20000], Loss: 851.6495361328125, Entropy 460.5594177246094, Learning Rate: 1.953125e-05\n",
      "Epoch [12122/20000], Loss: 907.7412719726562, Entropy 457.84991455078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12123/20000], Loss: 818.4736328125, Entropy 467.36376953125, Learning Rate: 1.953125e-05\n",
      "Epoch [12124/20000], Loss: 835.4325561523438, Entropy 466.82476806640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12125/20000], Loss: 868.0206298828125, Entropy 453.933349609375, Learning Rate: 1.953125e-05\n",
      "Epoch [12126/20000], Loss: 824.499267578125, Entropy 456.5017395019531, Learning Rate: 1.953125e-05\n",
      "Epoch [12127/20000], Loss: 850.2515869140625, Entropy 452.191162109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12128/20000], Loss: 842.3270263671875, Entropy 465.7645263671875, Learning Rate: 1.953125e-05\n",
      "Epoch [12129/20000], Loss: 826.7752685546875, Entropy 459.2113952636719, Learning Rate: 1.953125e-05\n",
      "Epoch [12130/20000], Loss: 828.6463012695312, Entropy 461.26031494140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12131/20000], Loss: 804.9315185546875, Entropy 484.57958984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12132/20000], Loss: 853.0733642578125, Entropy 451.8006286621094, Learning Rate: 1.953125e-05\n",
      "Epoch [12133/20000], Loss: 820.1529541015625, Entropy 455.5091552734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12134/20000], Loss: 870.96630859375, Entropy 455.0416259765625, Learning Rate: 1.953125e-05\n",
      "Epoch [12135/20000], Loss: 841.8346557617188, Entropy 453.63494873046875, Learning Rate: 1.953125e-05\n",
      "Epoch [12136/20000], Loss: 868.4322509765625, Entropy 444.6961364746094, Learning Rate: 1.953125e-05\n",
      "Epoch [12137/20000], Loss: 878.4661865234375, Entropy 467.9921875, Learning Rate: 1.953125e-05\n",
      "Epoch [12138/20000], Loss: 883.9429321289062, Entropy 457.36187744140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12139/20000], Loss: 877.636962890625, Entropy 449.0270690917969, Learning Rate: 1.953125e-05\n",
      "Epoch [12140/20000], Loss: 861.4185791015625, Entropy 461.89111328125, Learning Rate: 1.953125e-05\n",
      "Epoch [12141/20000], Loss: 823.0210571289062, Entropy 453.18988037109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12142/20000], Loss: 835.82275390625, Entropy 474.7658996582031, Learning Rate: 1.953125e-05\n",
      "Epoch [12143/20000], Loss: 864.437744140625, Entropy 460.7572021484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12144/20000], Loss: 882.0313720703125, Entropy 465.20849609375, Learning Rate: 1.953125e-05\n",
      "Epoch [12145/20000], Loss: 824.3739013671875, Entropy 435.9004211425781, Learning Rate: 1.953125e-05\n",
      "Epoch [12146/20000], Loss: 802.5151977539062, Entropy 463.17486572265625, Learning Rate: 1.953125e-05\n",
      "Epoch [12147/20000], Loss: 892.6983032226562, Entropy 449.21820068359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12148/20000], Loss: 861.41357421875, Entropy 463.0657653808594, Learning Rate: 1.953125e-05\n",
      "Epoch [12149/20000], Loss: 848.0877685546875, Entropy 461.7058410644531, Learning Rate: 1.953125e-05\n",
      "Epoch [12150/20000], Loss: 865.281494140625, Entropy 468.8766174316406, Learning Rate: 1.953125e-05\n",
      "Epoch [12151/20000], Loss: 880.5993041992188, Entropy 455.46612548828125, Learning Rate: 1.953125e-05\n",
      "Epoch [12152/20000], Loss: 863.341796875, Entropy 458.4022521972656, Learning Rate: 1.953125e-05\n",
      "Epoch [12153/20000], Loss: 891.5858154296875, Entropy 457.4957580566406, Learning Rate: 1.953125e-05\n",
      "Epoch [12154/20000], Loss: 868.9428100585938, Entropy 452.80889892578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12155/20000], Loss: 867.376220703125, Entropy 461.1960144042969, Learning Rate: 1.953125e-05\n",
      "Epoch [12156/20000], Loss: 923.286376953125, Entropy 465.2792053222656, Learning Rate: 1.953125e-05\n",
      "Epoch [12157/20000], Loss: 838.679931640625, Entropy 457.6963195800781, Learning Rate: 1.953125e-05\n",
      "Epoch [12158/20000], Loss: 901.5235595703125, Entropy 426.1770324707031, Learning Rate: 1.953125e-05\n",
      "Epoch [12159/20000], Loss: 855.7897338867188, Entropy 459.68084716796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12160/20000], Loss: 857.511474609375, Entropy 455.3072204589844, Learning Rate: 1.953125e-05\n",
      "Epoch [12161/20000], Loss: 872.9481811523438, Entropy 466.09393310546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12162/20000], Loss: 811.6256103515625, Entropy 462.3601379394531, Learning Rate: 1.953125e-05\n",
      "Epoch [12163/20000], Loss: 823.0283203125, Entropy 449.9529724121094, Learning Rate: 1.953125e-05\n",
      "Epoch [12164/20000], Loss: 915.173583984375, Entropy 460.861328125, Learning Rate: 1.953125e-05\n",
      "Epoch [12165/20000], Loss: 853.455078125, Entropy 445.8083801269531, Learning Rate: 1.953125e-05\n",
      "Epoch [12166/20000], Loss: 880.51416015625, Entropy 473.8111572265625, Learning Rate: 1.953125e-05\n",
      "Epoch [12167/20000], Loss: 864.490478515625, Entropy 434.407958984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12168/20000], Loss: 865.8946533203125, Entropy 465.3275146484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12169/20000], Loss: 866.8320922851562, Entropy 443.46722412109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12170/20000], Loss: 882.947509765625, Entropy 456.26025390625, Learning Rate: 1.953125e-05\n",
      "Epoch [12171/20000], Loss: 840.455322265625, Entropy 463.3814392089844, Learning Rate: 1.953125e-05\n",
      "Epoch [12172/20000], Loss: 812.372802734375, Entropy 460.9725036621094, Learning Rate: 1.953125e-05\n",
      "Epoch [12173/20000], Loss: 924.3587646484375, Entropy 459.4324645996094, Learning Rate: 1.953125e-05\n",
      "Epoch [12174/20000], Loss: 862.75390625, Entropy 462.1898193359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12175/20000], Loss: 867.905517578125, Entropy 454.7239074707031, Learning Rate: 1.953125e-05\n",
      "Epoch [12176/20000], Loss: 878.6152954101562, Entropy 450.89202880859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12177/20000], Loss: 837.792236328125, Entropy 474.302734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12178/20000], Loss: 836.1728515625, Entropy 463.6004333496094, Learning Rate: 1.953125e-05\n",
      "Epoch [12179/20000], Loss: 845.4078979492188, Entropy 450.38482666015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12180/20000], Loss: 911.287109375, Entropy 458.2587890625, Learning Rate: 1.953125e-05\n",
      "Epoch [12181/20000], Loss: 938.7705688476562, Entropy 439.97320556640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12182/20000], Loss: 861.1749267578125, Entropy 452.8163146972656, Learning Rate: 1.953125e-05\n",
      "Epoch [12183/20000], Loss: 830.3707885742188, Entropy 479.74859619140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12184/20000], Loss: 828.9879150390625, Entropy 454.6516418457031, Learning Rate: 1.953125e-05\n",
      "Epoch [12185/20000], Loss: 890.6679077148438, Entropy 458.25677490234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12186/20000], Loss: 821.40625, Entropy 455.5330505371094, Learning Rate: 1.953125e-05\n",
      "Epoch [12187/20000], Loss: 828.8483276367188, Entropy 456.89263916015625, Learning Rate: 1.953125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12188/20000], Loss: 859.8282470703125, Entropy 451.203125, Learning Rate: 1.953125e-05\n",
      "Epoch [12189/20000], Loss: 873.74951171875, Entropy 463.3701171875, Learning Rate: 1.953125e-05\n",
      "Epoch [12190/20000], Loss: 823.2426147460938, Entropy 451.51763916015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12191/20000], Loss: 884.7122802734375, Entropy 457.5590515136719, Learning Rate: 1.953125e-05\n",
      "Epoch [12192/20000], Loss: 868.6043701171875, Entropy 445.6937255859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12193/20000], Loss: 872.866455078125, Entropy 460.5203857421875, Learning Rate: 1.953125e-05\n",
      "Epoch [12194/20000], Loss: 883.3429565429688, Entropy 460.37945556640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12195/20000], Loss: 836.048583984375, Entropy 463.2160339355469, Learning Rate: 1.953125e-05\n",
      "Epoch [12196/20000], Loss: 835.1285400390625, Entropy 458.1134338378906, Learning Rate: 1.953125e-05\n",
      "Epoch [12197/20000], Loss: 836.3724365234375, Entropy 452.2837829589844, Learning Rate: 1.953125e-05\n",
      "Epoch [12198/20000], Loss: 889.2626953125, Entropy 446.9577331542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12199/20000], Loss: 831.23828125, Entropy 463.472412109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12200/20000], Loss: 826.2127685546875, Entropy 460.4098205566406, Learning Rate: 1.953125e-05\n",
      "Epoch [12201/20000], Loss: 851.8056640625, Entropy 460.6474914550781, Learning Rate: 1.953125e-05\n",
      "Epoch [12202/20000], Loss: 865.0968017578125, Entropy 454.4541320800781, Learning Rate: 1.953125e-05\n",
      "Epoch [12203/20000], Loss: 821.523681640625, Entropy 470.4646301269531, Learning Rate: 1.953125e-05\n",
      "Epoch [12204/20000], Loss: 863.491943359375, Entropy 455.50634765625, Learning Rate: 1.953125e-05\n",
      "Epoch [12205/20000], Loss: 801.576416015625, Entropy 467.2032165527344, Learning Rate: 1.953125e-05\n",
      "Epoch [12206/20000], Loss: 869.180908203125, Entropy 468.6090087890625, Learning Rate: 1.953125e-05\n",
      "Epoch [12207/20000], Loss: 789.9158325195312, Entropy 458.08502197265625, Learning Rate: 1.953125e-05\n",
      "Epoch [12208/20000], Loss: 813.3714599609375, Entropy 455.20751953125, Learning Rate: 1.953125e-05\n",
      "Epoch [12209/20000], Loss: 863.5799560546875, Entropy 446.8701477050781, Learning Rate: 1.953125e-05\n",
      "Epoch [12210/20000], Loss: 842.060546875, Entropy 466.2582702636719, Learning Rate: 1.953125e-05\n",
      "Epoch [12211/20000], Loss: 828.9420166015625, Entropy 458.8224792480469, Learning Rate: 1.953125e-05\n",
      "Epoch [12212/20000], Loss: 883.957763671875, Entropy 461.0344543457031, Learning Rate: 1.953125e-05\n",
      "Epoch [12213/20000], Loss: 817.91748046875, Entropy 468.9889831542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12214/20000], Loss: 830.38720703125, Entropy 449.5558776855469, Learning Rate: 1.953125e-05\n",
      "Epoch [12215/20000], Loss: 848.642333984375, Entropy 457.1833190917969, Learning Rate: 1.953125e-05\n",
      "Epoch [12216/20000], Loss: 766.923583984375, Entropy 469.5838928222656, Learning Rate: 1.953125e-05\n",
      "Epoch [12217/20000], Loss: 816.8150024414062, Entropy 466.88922119140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12218/20000], Loss: 861.424560546875, Entropy 449.5203857421875, Learning Rate: 1.953125e-05\n",
      "Epoch [12219/20000], Loss: 873.0076293945312, Entropy 446.42108154296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12220/20000], Loss: 757.203369140625, Entropy 466.607421875, Learning Rate: 1.953125e-05\n",
      "Epoch [12221/20000], Loss: 937.6033325195312, Entropy 438.82574462890625, Learning Rate: 1.953125e-05\n",
      "Epoch [12222/20000], Loss: 837.4385986328125, Entropy 449.0947570800781, Learning Rate: 1.953125e-05\n",
      "Epoch [12223/20000], Loss: 830.0419921875, Entropy 461.7095947265625, Learning Rate: 1.953125e-05\n",
      "Epoch [12224/20000], Loss: 856.928466796875, Entropy 437.7864074707031, Learning Rate: 1.953125e-05\n",
      "Epoch [12225/20000], Loss: 923.2808227539062, Entropy 466.69757080078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12226/20000], Loss: 824.0277709960938, Entropy 466.24273681640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12227/20000], Loss: 877.63134765625, Entropy 466.7057800292969, Learning Rate: 1.953125e-05\n",
      "Epoch [12228/20000], Loss: 845.3111572265625, Entropy 445.4072570800781, Learning Rate: 1.953125e-05\n",
      "Epoch [12229/20000], Loss: 855.5656127929688, Entropy 456.64581298828125, Learning Rate: 1.953125e-05\n",
      "Epoch [12230/20000], Loss: 894.2493896484375, Entropy 468.1564636230469, Learning Rate: 1.953125e-05\n",
      "Epoch [12231/20000], Loss: 841.6141357421875, Entropy 456.1210021972656, Learning Rate: 1.953125e-05\n",
      "Epoch [12232/20000], Loss: 847.8821411132812, Entropy 459.37921142578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12233/20000], Loss: 878.5220947265625, Entropy 442.5330505371094, Learning Rate: 1.953125e-05\n",
      "Epoch [12234/20000], Loss: 837.3367919921875, Entropy 451.8653869628906, Learning Rate: 1.953125e-05\n",
      "Epoch [12235/20000], Loss: 832.7603759765625, Entropy 461.7800598144531, Learning Rate: 1.953125e-05\n",
      "Epoch [12236/20000], Loss: 799.6285400390625, Entropy 479.8852233886719, Learning Rate: 1.953125e-05\n",
      "Epoch [12237/20000], Loss: 832.4789428710938, Entropy 479.25628662109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12238/20000], Loss: 993.903076171875, Entropy 439.2117919921875, Learning Rate: 1.953125e-05\n",
      "Epoch [12239/20000], Loss: 895.70947265625, Entropy 443.5584716796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12240/20000], Loss: 916.869873046875, Entropy 452.4267578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12241/20000], Loss: 867.2879638671875, Entropy 457.7710876464844, Learning Rate: 1.953125e-05\n",
      "Epoch [12242/20000], Loss: 875.357177734375, Entropy 440.24658203125, Learning Rate: 1.953125e-05\n",
      "Epoch [12243/20000], Loss: 822.5486450195312, Entropy 466.05999755859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12244/20000], Loss: 861.5775146484375, Entropy 455.2501525878906, Learning Rate: 1.953125e-05\n",
      "Epoch [12245/20000], Loss: 884.8997802734375, Entropy 448.83935546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12246/20000], Loss: 871.4056396484375, Entropy 471.5653076171875, Learning Rate: 1.953125e-05\n",
      "Epoch [12247/20000], Loss: 894.9422607421875, Entropy 455.8699951171875, Learning Rate: 1.953125e-05\n",
      "Epoch [12248/20000], Loss: 855.7935791015625, Entropy 448.8723449707031, Learning Rate: 1.953125e-05\n",
      "Epoch [12249/20000], Loss: 799.8736572265625, Entropy 450.6741027832031, Learning Rate: 1.953125e-05\n",
      "Epoch [12250/20000], Loss: 904.5740966796875, Entropy 442.9530334472656, Learning Rate: 1.953125e-05\n",
      "Epoch [12251/20000], Loss: 852.4592895507812, Entropy 447.38397216796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12252/20000], Loss: 853.0791015625, Entropy 457.1257629394531, Learning Rate: 1.953125e-05\n",
      "Epoch [12253/20000], Loss: 842.7650146484375, Entropy 454.5247497558594, Learning Rate: 1.953125e-05\n",
      "Epoch [12254/20000], Loss: 860.3690185546875, Entropy 444.0736999511719, Learning Rate: 1.953125e-05\n",
      "Epoch [12255/20000], Loss: 841.99853515625, Entropy 462.0924072265625, Learning Rate: 1.953125e-05\n",
      "Epoch [12256/20000], Loss: 847.763671875, Entropy 457.0519104003906, Learning Rate: 1.953125e-05\n",
      "Epoch [12257/20000], Loss: 908.8804931640625, Entropy 443.4693908691406, Learning Rate: 1.953125e-05\n",
      "Epoch [12258/20000], Loss: 833.3711547851562, Entropy 478.57037353515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12259/20000], Loss: 870.295166015625, Entropy 468.3433837890625, Learning Rate: 1.953125e-05\n",
      "Epoch [12260/20000], Loss: 875.867431640625, Entropy 459.1310729980469, Learning Rate: 1.953125e-05\n",
      "Epoch [12261/20000], Loss: 866.2077026367188, Entropy 454.92279052734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12262/20000], Loss: 847.3416137695312, Entropy 447.86541748046875, Learning Rate: 1.953125e-05\n",
      "Epoch [12263/20000], Loss: 882.3675537109375, Entropy 466.6422424316406, Learning Rate: 1.953125e-05\n",
      "Epoch [12264/20000], Loss: 851.9627685546875, Entropy 456.44873046875, Learning Rate: 1.953125e-05\n",
      "Epoch [12265/20000], Loss: 883.1461181640625, Entropy 460.1638488769531, Learning Rate: 1.953125e-05\n",
      "Epoch [12266/20000], Loss: 883.77978515625, Entropy 452.2626953125, Learning Rate: 1.953125e-05\n",
      "Epoch [12267/20000], Loss: 851.7064208984375, Entropy 458.1485900878906, Learning Rate: 1.953125e-05\n",
      "Epoch [12268/20000], Loss: 902.7483520507812, Entropy 448.94476318359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12269/20000], Loss: 857.0701904296875, Entropy 458.2986145019531, Learning Rate: 1.953125e-05\n",
      "Epoch [12270/20000], Loss: 846.9593505859375, Entropy 467.5914001464844, Learning Rate: 1.953125e-05\n",
      "Epoch [12271/20000], Loss: 840.7572021484375, Entropy 444.3934631347656, Learning Rate: 1.953125e-05\n",
      "Epoch [12272/20000], Loss: 894.9729614257812, Entropy 458.59222412109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12273/20000], Loss: 886.033203125, Entropy 464.813232421875, Learning Rate: 1.953125e-05\n",
      "Epoch [12274/20000], Loss: 857.1611328125, Entropy 460.0772705078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12275/20000], Loss: 834.5167846679688, Entropy 452.52728271484375, Learning Rate: 1.953125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12276/20000], Loss: 905.3212890625, Entropy 444.0552978515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12277/20000], Loss: 836.97021484375, Entropy 457.5012512207031, Learning Rate: 1.953125e-05\n",
      "Epoch [12278/20000], Loss: 844.1009521484375, Entropy 472.0256042480469, Learning Rate: 1.953125e-05\n",
      "Epoch [12279/20000], Loss: 834.9378662109375, Entropy 446.3408203125, Learning Rate: 1.953125e-05\n",
      "Epoch [12280/20000], Loss: 832.468994140625, Entropy 444.9522705078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12281/20000], Loss: 848.4481201171875, Entropy 465.2979431152344, Learning Rate: 1.953125e-05\n",
      "Epoch [12282/20000], Loss: 928.481689453125, Entropy 460.9291687011719, Learning Rate: 1.953125e-05\n",
      "Epoch [12283/20000], Loss: 824.9794921875, Entropy 463.9309387207031, Learning Rate: 1.953125e-05\n",
      "Epoch [12284/20000], Loss: 883.5170288085938, Entropy 450.08148193359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12285/20000], Loss: 847.95166015625, Entropy 452.0360412597656, Learning Rate: 1.953125e-05\n",
      "Epoch [12286/20000], Loss: 841.965576171875, Entropy 449.7838134765625, Learning Rate: 1.953125e-05\n",
      "Epoch [12287/20000], Loss: 863.3135986328125, Entropy 449.7307434082031, Learning Rate: 1.953125e-05\n",
      "Epoch [12288/20000], Loss: 832.780029296875, Entropy 463.5237121582031, Learning Rate: 1.953125e-05\n",
      "Epoch [12289/20000], Loss: 843.697265625, Entropy 460.6224060058594, Learning Rate: 1.953125e-05\n",
      "Epoch [12290/20000], Loss: 866.91357421875, Entropy 457.220458984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12291/20000], Loss: 961.638427734375, Entropy 442.0830383300781, Learning Rate: 1.953125e-05\n",
      "Epoch [12292/20000], Loss: 922.928955078125, Entropy 460.7307434082031, Learning Rate: 1.953125e-05\n",
      "Epoch [12293/20000], Loss: 873.299560546875, Entropy 467.663818359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12294/20000], Loss: 866.5516357421875, Entropy 465.753662109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12295/20000], Loss: 822.1852416992188, Entropy 456.79388427734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12296/20000], Loss: 840.2662963867188, Entropy 451.16607666015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12297/20000], Loss: 851.9124145507812, Entropy 464.28802490234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12298/20000], Loss: 830.3673095703125, Entropy 455.5958557128906, Learning Rate: 1.953125e-05\n",
      "Epoch [12299/20000], Loss: 837.409912109375, Entropy 456.4107666015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12300/20000], Loss: 857.1765747070312, Entropy 446.34271240234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12301/20000], Loss: 853.8842163085938, Entropy 450.23492431640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12302/20000], Loss: 888.61376953125, Entropy 462.3869323730469, Learning Rate: 1.953125e-05\n",
      "Epoch [12303/20000], Loss: 887.6968994140625, Entropy 448.5437316894531, Learning Rate: 1.953125e-05\n",
      "Epoch [12304/20000], Loss: 860.7282104492188, Entropy 444.81829833984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12305/20000], Loss: 874.795166015625, Entropy 457.3667907714844, Learning Rate: 1.953125e-05\n",
      "Epoch [12306/20000], Loss: 921.2224731445312, Entropy 440.09527587890625, Learning Rate: 1.953125e-05\n",
      "Epoch [12307/20000], Loss: 873.7598876953125, Entropy 460.5962829589844, Learning Rate: 1.953125e-05\n",
      "Epoch [12308/20000], Loss: 873.2437744140625, Entropy 472.7959899902344, Learning Rate: 1.953125e-05\n",
      "Epoch [12309/20000], Loss: 865.774658203125, Entropy 462.1340637207031, Learning Rate: 1.953125e-05\n",
      "Epoch [12310/20000], Loss: 874.4404296875, Entropy 457.7388610839844, Learning Rate: 1.953125e-05\n",
      "Epoch [12311/20000], Loss: 845.4721069335938, Entropy 470.11907958984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12312/20000], Loss: 786.8848876953125, Entropy 454.3561706542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12313/20000], Loss: 841.264404296875, Entropy 453.3476867675781, Learning Rate: 1.953125e-05\n",
      "Epoch [12314/20000], Loss: 839.7706298828125, Entropy 451.146484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12315/20000], Loss: 865.7303466796875, Entropy 448.4482727050781, Learning Rate: 1.953125e-05\n",
      "Epoch [12316/20000], Loss: 845.9532470703125, Entropy 457.9202880859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12317/20000], Loss: 819.7923583984375, Entropy 451.209716796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12318/20000], Loss: 828.927734375, Entropy 446.4200439453125, Learning Rate: 1.953125e-05\n",
      "Epoch [12319/20000], Loss: 881.333984375, Entropy 442.9114685058594, Learning Rate: 1.953125e-05\n",
      "Epoch [12320/20000], Loss: 827.1940307617188, Entropy 465.69171142578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12321/20000], Loss: 821.8599853515625, Entropy 456.4689025878906, Learning Rate: 1.953125e-05\n",
      "Epoch [12322/20000], Loss: 866.2314453125, Entropy 448.0897521972656, Learning Rate: 1.953125e-05\n",
      "Epoch [12323/20000], Loss: 847.3895263671875, Entropy 466.84716796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12324/20000], Loss: 846.5228271484375, Entropy 447.7513427734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12325/20000], Loss: 834.5537109375, Entropy 439.2747497558594, Learning Rate: 1.953125e-05\n",
      "Epoch [12326/20000], Loss: 863.7789306640625, Entropy 458.771484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12327/20000], Loss: 851.2857666015625, Entropy 455.228271484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12328/20000], Loss: 908.6627197265625, Entropy 462.8692626953125, Learning Rate: 1.953125e-05\n",
      "Epoch [12329/20000], Loss: 896.0228271484375, Entropy 457.4537658691406, Learning Rate: 1.953125e-05\n",
      "Epoch [12330/20000], Loss: 868.5416870117188, Entropy 449.95904541015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12331/20000], Loss: 884.3023681640625, Entropy 462.0771789550781, Learning Rate: 1.953125e-05\n",
      "Epoch [12332/20000], Loss: 801.3079833984375, Entropy 455.603271484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12333/20000], Loss: 855.1048583984375, Entropy 451.6481018066406, Learning Rate: 1.953125e-05\n",
      "Epoch [12334/20000], Loss: 895.08447265625, Entropy 447.3595886230469, Learning Rate: 1.953125e-05\n",
      "Epoch [12335/20000], Loss: 882.905029296875, Entropy 454.5301513671875, Learning Rate: 1.953125e-05\n",
      "Epoch [12336/20000], Loss: 822.9484252929688, Entropy 456.80499267578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12337/20000], Loss: 910.5167236328125, Entropy 448.1775817871094, Learning Rate: 1.953125e-05\n",
      "Epoch [12338/20000], Loss: 893.422607421875, Entropy 443.4574890136719, Learning Rate: 1.953125e-05\n",
      "Epoch [12339/20000], Loss: 883.0831909179688, Entropy 456.65179443359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12340/20000], Loss: 857.2779541015625, Entropy 461.7465515136719, Learning Rate: 1.953125e-05\n",
      "Epoch [12341/20000], Loss: 865.8701171875, Entropy 441.232421875, Learning Rate: 1.953125e-05\n",
      "Epoch [12342/20000], Loss: 840.349609375, Entropy 462.3063049316406, Learning Rate: 1.953125e-05\n",
      "Epoch [12343/20000], Loss: 898.226806640625, Entropy 436.556640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12344/20000], Loss: 829.5172729492188, Entropy 454.96441650390625, Learning Rate: 1.953125e-05\n",
      "Epoch [12345/20000], Loss: 898.2451782226562, Entropy 460.48773193359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12346/20000], Loss: 834.798095703125, Entropy 456.0735168457031, Learning Rate: 1.953125e-05\n",
      "Epoch [12347/20000], Loss: 864.2890625, Entropy 461.8248291015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12348/20000], Loss: 814.4390869140625, Entropy 463.4964599609375, Learning Rate: 1.953125e-05\n",
      "Epoch [12349/20000], Loss: 882.1798706054688, Entropy 449.97381591796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12350/20000], Loss: 885.7783203125, Entropy 453.1804504394531, Learning Rate: 1.953125e-05\n",
      "Epoch [12351/20000], Loss: 843.642822265625, Entropy 445.9287414550781, Learning Rate: 1.953125e-05\n",
      "Epoch [12352/20000], Loss: 860.6360473632812, Entropy 446.01568603515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12353/20000], Loss: 845.6705932617188, Entropy 482.73248291015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12354/20000], Loss: 811.13134765625, Entropy 466.7549133300781, Learning Rate: 1.953125e-05\n",
      "Epoch [12355/20000], Loss: 857.3729248046875, Entropy 465.3487243652344, Learning Rate: 1.953125e-05\n",
      "Epoch [12356/20000], Loss: 832.921142578125, Entropy 446.9217529296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12357/20000], Loss: 834.9397583007812, Entropy 462.54888916015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12358/20000], Loss: 915.3081665039062, Entropy 443.98150634765625, Learning Rate: 1.953125e-05\n",
      "Epoch [12359/20000], Loss: 821.7838745117188, Entropy 456.60162353515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12360/20000], Loss: 879.493408203125, Entropy 454.3157043457031, Learning Rate: 1.953125e-05\n",
      "Epoch [12361/20000], Loss: 871.44384765625, Entropy 459.5032653808594, Learning Rate: 1.953125e-05\n",
      "Epoch [12362/20000], Loss: 840.549072265625, Entropy 451.0568542480469, Learning Rate: 1.953125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12363/20000], Loss: 855.998046875, Entropy 441.2270202636719, Learning Rate: 1.953125e-05\n",
      "Epoch [12364/20000], Loss: 864.1611328125, Entropy 460.9335021972656, Learning Rate: 1.953125e-05\n",
      "Epoch [12365/20000], Loss: 832.002197265625, Entropy 464.7792053222656, Learning Rate: 1.953125e-05\n",
      "Epoch [12366/20000], Loss: 837.0965576171875, Entropy 451.1275634765625, Learning Rate: 1.953125e-05\n",
      "Epoch [12367/20000], Loss: 828.8735961914062, Entropy 450.83929443359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12368/20000], Loss: 822.4642333984375, Entropy 454.5689392089844, Learning Rate: 1.953125e-05\n",
      "Epoch [12369/20000], Loss: 821.4851684570312, Entropy 454.44781494140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12370/20000], Loss: 861.334716796875, Entropy 469.3138122558594, Learning Rate: 1.953125e-05\n",
      "Epoch [12371/20000], Loss: 830.8861083984375, Entropy 451.9952392578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12372/20000], Loss: 828.4658203125, Entropy 457.7740478515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12373/20000], Loss: 858.8726196289062, Entropy 462.38702392578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12374/20000], Loss: 880.763671875, Entropy 452.8088073730469, Learning Rate: 1.953125e-05\n",
      "Epoch [12375/20000], Loss: 879.08642578125, Entropy 459.8350830078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12376/20000], Loss: 834.7274169921875, Entropy 465.4207763671875, Learning Rate: 1.953125e-05\n",
      "Epoch [12377/20000], Loss: 860.3196411132812, Entropy 461.29095458984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12378/20000], Loss: 880.31591796875, Entropy 456.0632019042969, Learning Rate: 1.953125e-05\n",
      "Epoch [12379/20000], Loss: 829.1038818359375, Entropy 473.8279724121094, Learning Rate: 1.953125e-05\n",
      "Epoch [12380/20000], Loss: 802.2844848632812, Entropy 464.00140380859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12381/20000], Loss: 884.9747314453125, Entropy 466.8400573730469, Learning Rate: 1.953125e-05\n",
      "Epoch [12382/20000], Loss: 846.9219360351562, Entropy 456.68255615234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12383/20000], Loss: 847.0352783203125, Entropy 463.7722473144531, Learning Rate: 1.953125e-05\n",
      "Epoch [12384/20000], Loss: 867.2025756835938, Entropy 451.80413818359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12385/20000], Loss: 817.3955078125, Entropy 455.9020080566406, Learning Rate: 1.953125e-05\n",
      "Epoch [12386/20000], Loss: 904.0103149414062, Entropy 433.53045654296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12387/20000], Loss: 835.0247802734375, Entropy 461.45068359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12388/20000], Loss: 855.904052734375, Entropy 457.5811767578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12389/20000], Loss: 914.2874755859375, Entropy 449.4733581542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12390/20000], Loss: 866.9209594726562, Entropy 447.23052978515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12391/20000], Loss: 831.6014404296875, Entropy 456.3332214355469, Learning Rate: 1.953125e-05\n",
      "Epoch [12392/20000], Loss: 861.7179565429688, Entropy 450.52862548828125, Learning Rate: 1.953125e-05\n",
      "Epoch [12393/20000], Loss: 846.5125732421875, Entropy 449.9664001464844, Learning Rate: 1.953125e-05\n",
      "Epoch [12394/20000], Loss: 835.1712646484375, Entropy 452.0565185546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12395/20000], Loss: 841.659423828125, Entropy 456.28857421875, Learning Rate: 1.953125e-05\n",
      "Epoch [12396/20000], Loss: 835.857421875, Entropy 462.9904479980469, Learning Rate: 1.953125e-05\n",
      "Epoch [12397/20000], Loss: 836.6453857421875, Entropy 457.6794128417969, Learning Rate: 1.953125e-05\n",
      "Epoch [12398/20000], Loss: 815.5606079101562, Entropy 462.04522705078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12399/20000], Loss: 822.468505859375, Entropy 466.8962097167969, Learning Rate: 1.953125e-05\n",
      "Epoch [12400/20000], Loss: 856.78759765625, Entropy 439.0062255859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12401/20000], Loss: 856.1139526367188, Entropy 471.39263916015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12402/20000], Loss: 829.2990112304688, Entropy 459.52459716796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12403/20000], Loss: 856.4376831054688, Entropy 439.31756591796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12404/20000], Loss: 840.9630126953125, Entropy 451.1010437011719, Learning Rate: 1.953125e-05\n",
      "Epoch [12405/20000], Loss: 877.267822265625, Entropy 456.3996276855469, Learning Rate: 1.953125e-05\n",
      "Epoch [12406/20000], Loss: 808.103515625, Entropy 458.90771484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12407/20000], Loss: 837.05517578125, Entropy 461.3924865722656, Learning Rate: 1.953125e-05\n",
      "Epoch [12408/20000], Loss: 853.4117431640625, Entropy 450.3713073730469, Learning Rate: 1.953125e-05\n",
      "Epoch [12409/20000], Loss: 863.0821533203125, Entropy 468.5908203125, Learning Rate: 1.953125e-05\n",
      "Epoch [12410/20000], Loss: 878.0404663085938, Entropy 450.94549560546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12411/20000], Loss: 817.5796508789062, Entropy 478.94390869140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12412/20000], Loss: 790.543212890625, Entropy 458.4757995605469, Learning Rate: 1.953125e-05\n",
      "Epoch [12413/20000], Loss: 847.0328369140625, Entropy 446.2437438964844, Learning Rate: 1.953125e-05\n",
      "Epoch [12414/20000], Loss: 834.9462890625, Entropy 466.4186706542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12415/20000], Loss: 839.282470703125, Entropy 463.514404296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12416/20000], Loss: 864.6231689453125, Entropy 456.181640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12417/20000], Loss: 833.6676025390625, Entropy 452.5551452636719, Learning Rate: 1.953125e-05\n",
      "Epoch [12418/20000], Loss: 886.776123046875, Entropy 464.2757263183594, Learning Rate: 1.953125e-05\n",
      "Epoch [12419/20000], Loss: 851.0829467773438, Entropy 456.79095458984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12420/20000], Loss: 851.5767211914062, Entropy 442.23382568359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12421/20000], Loss: 809.59423828125, Entropy 462.6745300292969, Learning Rate: 1.953125e-05\n",
      "Epoch [12422/20000], Loss: 851.05224609375, Entropy 460.8998718261719, Learning Rate: 1.953125e-05\n",
      "Epoch [12423/20000], Loss: 859.1415405273438, Entropy 460.82171630859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12424/20000], Loss: 888.57421875, Entropy 455.6770935058594, Learning Rate: 1.953125e-05\n",
      "Epoch [12425/20000], Loss: 880.2807006835938, Entropy 444.62493896484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12426/20000], Loss: 832.6698608398438, Entropy 454.50225830078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12427/20000], Loss: 844.9876708984375, Entropy 459.7673034667969, Learning Rate: 1.953125e-05\n",
      "Epoch [12428/20000], Loss: 881.7239990234375, Entropy 464.2764587402344, Learning Rate: 1.953125e-05\n",
      "Epoch [12429/20000], Loss: 828.095703125, Entropy 452.969482421875, Learning Rate: 1.953125e-05\n",
      "Epoch [12430/20000], Loss: 835.5189208984375, Entropy 448.4084167480469, Learning Rate: 1.953125e-05\n",
      "Epoch [12431/20000], Loss: 832.0443115234375, Entropy 464.5926513671875, Learning Rate: 1.953125e-05\n",
      "Epoch [12432/20000], Loss: 875.2401733398438, Entropy 453.02728271484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12433/20000], Loss: 891.215576171875, Entropy 449.1265869140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12434/20000], Loss: 891.271240234375, Entropy 447.9602966308594, Learning Rate: 1.953125e-05\n",
      "Epoch [12435/20000], Loss: 862.4329223632812, Entropy 459.35638427734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12436/20000], Loss: 865.1180419921875, Entropy 464.4831848144531, Learning Rate: 1.953125e-05\n",
      "Epoch [12437/20000], Loss: 860.9697265625, Entropy 454.8712463378906, Learning Rate: 1.953125e-05\n",
      "Epoch [12438/20000], Loss: 826.8936767578125, Entropy 459.1824951171875, Learning Rate: 1.953125e-05\n",
      "Epoch [12439/20000], Loss: 874.9252319335938, Entropy 459.39569091796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12440/20000], Loss: 842.46923828125, Entropy 475.32177734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12441/20000], Loss: 837.4750366210938, Entropy 452.24444580078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12442/20000], Loss: 844.8626708984375, Entropy 446.6117858886719, Learning Rate: 1.953125e-05\n",
      "Epoch [12443/20000], Loss: 811.9127197265625, Entropy 452.5054931640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12444/20000], Loss: 872.8938598632812, Entropy 445.89874267578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12445/20000], Loss: 845.5968627929688, Entropy 469.28570556640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12446/20000], Loss: 867.9314575195312, Entropy 444.70318603515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12447/20000], Loss: 872.4615478515625, Entropy 469.9709777832031, Learning Rate: 1.953125e-05\n",
      "Epoch [12448/20000], Loss: 902.541748046875, Entropy 456.6441955566406, Learning Rate: 1.953125e-05\n",
      "Epoch [12449/20000], Loss: 816.799560546875, Entropy 459.9397888183594, Learning Rate: 1.953125e-05\n",
      "Epoch [12450/20000], Loss: 850.6842041015625, Entropy 456.4072570800781, Learning Rate: 1.953125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12451/20000], Loss: 908.3807373046875, Entropy 453.2409973144531, Learning Rate: 1.953125e-05\n",
      "Epoch [12452/20000], Loss: 872.158935546875, Entropy 447.560302734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12453/20000], Loss: 827.128662109375, Entropy 450.3436279296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12454/20000], Loss: 819.0336303710938, Entropy 461.92767333984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12455/20000], Loss: 875.07958984375, Entropy 456.1279602050781, Learning Rate: 1.953125e-05\n",
      "Epoch [12456/20000], Loss: 835.3094482421875, Entropy 452.4011535644531, Learning Rate: 1.953125e-05\n",
      "Epoch [12457/20000], Loss: 835.0032958984375, Entropy 460.1424255371094, Learning Rate: 1.953125e-05\n",
      "Epoch [12458/20000], Loss: 848.5106201171875, Entropy 460.9410095214844, Learning Rate: 1.953125e-05\n",
      "Epoch [12459/20000], Loss: 885.4473876953125, Entropy 460.57666015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12460/20000], Loss: 824.9600830078125, Entropy 468.0132751464844, Learning Rate: 1.953125e-05\n",
      "Epoch [12461/20000], Loss: 799.5989379882812, Entropy 460.76202392578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12462/20000], Loss: 845.8828125, Entropy 459.7271728515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12463/20000], Loss: 846.4151000976562, Entropy 452.43695068359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12464/20000], Loss: 854.0670166015625, Entropy 461.1650695800781, Learning Rate: 1.953125e-05\n",
      "Epoch [12465/20000], Loss: 837.8836059570312, Entropy 468.05865478515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12466/20000], Loss: 807.1512451171875, Entropy 458.837890625, Learning Rate: 1.953125e-05\n",
      "Epoch [12467/20000], Loss: 868.40087890625, Entropy 471.7588195800781, Learning Rate: 1.953125e-05\n",
      "Epoch [12468/20000], Loss: 826.260009765625, Entropy 469.6950988769531, Learning Rate: 1.953125e-05\n",
      "Epoch [12469/20000], Loss: 800.0146484375, Entropy 460.6530456542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12470/20000], Loss: 817.3341064453125, Entropy 459.7060852050781, Learning Rate: 1.953125e-05\n",
      "Epoch [12471/20000], Loss: 832.6119384765625, Entropy 460.80859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12472/20000], Loss: 835.9044189453125, Entropy 452.744873046875, Learning Rate: 1.953125e-05\n",
      "Epoch [12473/20000], Loss: 810.919921875, Entropy 459.1482849121094, Learning Rate: 1.953125e-05\n",
      "Epoch [12474/20000], Loss: 860.3516235351562, Entropy 461.32415771484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12475/20000], Loss: 855.5528564453125, Entropy 458.1141052246094, Learning Rate: 1.953125e-05\n",
      "Epoch [12476/20000], Loss: 877.957763671875, Entropy 454.9259033203125, Learning Rate: 1.953125e-05\n",
      "Epoch [12477/20000], Loss: 830.959716796875, Entropy 463.1526794433594, Learning Rate: 1.953125e-05\n",
      "Epoch [12478/20000], Loss: 860.1142578125, Entropy 454.2730407714844, Learning Rate: 1.953125e-05\n",
      "Epoch [12479/20000], Loss: 800.1947021484375, Entropy 456.04296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12480/20000], Loss: 858.275390625, Entropy 463.9991455078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12481/20000], Loss: 854.3448486328125, Entropy 454.1096496582031, Learning Rate: 1.953125e-05\n",
      "Epoch [12482/20000], Loss: 864.328125, Entropy 460.7507019042969, Learning Rate: 1.953125e-05\n",
      "Epoch [12483/20000], Loss: 854.7225341796875, Entropy 466.2924499511719, Learning Rate: 1.953125e-05\n",
      "Epoch [12484/20000], Loss: 872.9696044921875, Entropy 461.4083557128906, Learning Rate: 1.953125e-05\n",
      "Epoch [12485/20000], Loss: 848.86865234375, Entropy 479.6059265136719, Learning Rate: 1.953125e-05\n",
      "Epoch [12486/20000], Loss: 805.1864013671875, Entropy 450.7632751464844, Learning Rate: 1.953125e-05\n",
      "Epoch [12487/20000], Loss: 843.1790161132812, Entropy 461.21990966796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12488/20000], Loss: 850.591064453125, Entropy 463.2021789550781, Learning Rate: 1.953125e-05\n",
      "Epoch [12489/20000], Loss: 877.290283203125, Entropy 461.19091796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12490/20000], Loss: 861.9429931640625, Entropy 460.4366455078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12491/20000], Loss: 863.1636962890625, Entropy 466.2095947265625, Learning Rate: 1.953125e-05\n",
      "Epoch [12492/20000], Loss: 862.048583984375, Entropy 460.7292175292969, Learning Rate: 1.953125e-05\n",
      "Epoch [12493/20000], Loss: 854.7520751953125, Entropy 452.263671875, Learning Rate: 1.953125e-05\n",
      "Epoch [12494/20000], Loss: 875.47607421875, Entropy 444.5490417480469, Learning Rate: 1.953125e-05\n",
      "Epoch [12495/20000], Loss: 844.5533447265625, Entropy 450.8463134765625, Learning Rate: 1.953125e-05\n",
      "Epoch [12496/20000], Loss: 891.4224853515625, Entropy 458.76953125, Learning Rate: 1.953125e-05\n",
      "Epoch [12497/20000], Loss: 908.0941162109375, Entropy 454.5892028808594, Learning Rate: 1.953125e-05\n",
      "Epoch [12498/20000], Loss: 829.249267578125, Entropy 444.9483947753906, Learning Rate: 1.953125e-05\n",
      "Epoch [12499/20000], Loss: 848.2658081054688, Entropy 446.57647705078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12500/20000], Loss: 916.315673828125, Entropy 450.0417175292969, Learning Rate: 1.953125e-05\n",
      "Epoch [12501/20000], Loss: 861.4234008789062, Entropy 452.28607177734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12502/20000], Loss: 839.0439453125, Entropy 465.0294494628906, Learning Rate: 1.953125e-05\n",
      "Epoch [12503/20000], Loss: 820.9351196289062, Entropy 466.69158935546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12504/20000], Loss: 912.2808837890625, Entropy 446.6316223144531, Learning Rate: 1.953125e-05\n",
      "Epoch [12505/20000], Loss: 798.887451171875, Entropy 472.7642822265625, Learning Rate: 1.953125e-05\n",
      "Epoch [12506/20000], Loss: 854.7264404296875, Entropy 464.212646484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12507/20000], Loss: 830.8901977539062, Entropy 458.66265869140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12508/20000], Loss: 857.093994140625, Entropy 470.4791259765625, Learning Rate: 1.953125e-05\n",
      "Epoch [12509/20000], Loss: 876.659912109375, Entropy 438.6534423828125, Learning Rate: 1.953125e-05\n",
      "Epoch [12510/20000], Loss: 826.71923828125, Entropy 457.9889831542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12511/20000], Loss: 881.3831176757812, Entropy 450.98931884765625, Learning Rate: 1.953125e-05\n",
      "Epoch [12512/20000], Loss: 852.3525390625, Entropy 463.9417724609375, Learning Rate: 1.953125e-05\n",
      "Epoch [12513/20000], Loss: 837.7462158203125, Entropy 461.0600280761719, Learning Rate: 1.953125e-05\n",
      "Epoch [12514/20000], Loss: 856.181396484375, Entropy 461.7139892578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12515/20000], Loss: 908.8152465820312, Entropy 439.90374755859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12516/20000], Loss: 886.19140625, Entropy 447.0323486328125, Learning Rate: 1.953125e-05\n",
      "Epoch [12517/20000], Loss: 862.666015625, Entropy 467.7273864746094, Learning Rate: 1.953125e-05\n",
      "Epoch [12518/20000], Loss: 837.4056396484375, Entropy 457.88671875, Learning Rate: 1.953125e-05\n",
      "Epoch [12519/20000], Loss: 864.9132080078125, Entropy 433.164794921875, Learning Rate: 1.953125e-05\n",
      "Epoch [12520/20000], Loss: 911.9564819335938, Entropy 445.09893798828125, Learning Rate: 1.953125e-05\n",
      "Epoch [12521/20000], Loss: 852.822021484375, Entropy 451.5016174316406, Learning Rate: 1.953125e-05\n",
      "Epoch [12522/20000], Loss: 843.5712280273438, Entropy 455.64703369140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12523/20000], Loss: 874.8038330078125, Entropy 441.7033386230469, Learning Rate: 1.953125e-05\n",
      "Epoch [12524/20000], Loss: 908.1358642578125, Entropy 468.1177978515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12525/20000], Loss: 871.4229736328125, Entropy 473.5640869140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12526/20000], Loss: 894.1392822265625, Entropy 450.8609313964844, Learning Rate: 1.953125e-05\n",
      "Epoch [12527/20000], Loss: 844.898681640625, Entropy 441.1405029296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12528/20000], Loss: 858.1048583984375, Entropy 482.8194885253906, Learning Rate: 1.953125e-05\n",
      "Epoch [12529/20000], Loss: 895.240234375, Entropy 453.2413635253906, Learning Rate: 1.953125e-05\n",
      "Epoch [12530/20000], Loss: 898.0707397460938, Entropy 455.71575927734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12531/20000], Loss: 878.2666015625, Entropy 443.533203125, Learning Rate: 1.953125e-05\n",
      "Epoch [12532/20000], Loss: 839.5235595703125, Entropy 461.552490234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12533/20000], Loss: 847.7099609375, Entropy 463.1774597167969, Learning Rate: 1.953125e-05\n",
      "Epoch [12534/20000], Loss: 864.3458251953125, Entropy 446.0257568359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12535/20000], Loss: 909.4288330078125, Entropy 458.0699157714844, Learning Rate: 1.953125e-05\n",
      "Epoch [12536/20000], Loss: 839.7566528320312, Entropy 473.26812744140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12537/20000], Loss: 810.202880859375, Entropy 472.2844543457031, Learning Rate: 1.953125e-05\n",
      "Epoch [12538/20000], Loss: 876.3853759765625, Entropy 452.0272521972656, Learning Rate: 1.953125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12539/20000], Loss: 821.2236328125, Entropy 464.0967102050781, Learning Rate: 1.953125e-05\n",
      "Epoch [12540/20000], Loss: 867.2123413085938, Entropy 458.91595458984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12541/20000], Loss: 857.0673828125, Entropy 442.4929504394531, Learning Rate: 1.953125e-05\n",
      "Epoch [12542/20000], Loss: 845.415283203125, Entropy 453.74755859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12543/20000], Loss: 845.6119384765625, Entropy 466.1842956542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12544/20000], Loss: 845.9036865234375, Entropy 458.57373046875, Learning Rate: 1.953125e-05\n",
      "Epoch [12545/20000], Loss: 798.875244140625, Entropy 477.2432556152344, Learning Rate: 1.953125e-05\n",
      "Epoch [12546/20000], Loss: 830.4984741210938, Entropy 463.02349853515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12547/20000], Loss: 817.4373168945312, Entropy 452.84075927734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12548/20000], Loss: 833.84423828125, Entropy 461.3717041015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12549/20000], Loss: 899.0716552734375, Entropy 449.1227111816406, Learning Rate: 1.953125e-05\n",
      "Epoch [12550/20000], Loss: 901.77880859375, Entropy 445.393310546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12551/20000], Loss: 910.66455078125, Entropy 436.3301086425781, Learning Rate: 1.953125e-05\n",
      "Epoch [12552/20000], Loss: 853.056396484375, Entropy 462.0748291015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12553/20000], Loss: 886.4549560546875, Entropy 465.4943542480469, Learning Rate: 1.953125e-05\n",
      "Epoch [12554/20000], Loss: 833.5269775390625, Entropy 468.0238037109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12555/20000], Loss: 883.703857421875, Entropy 476.7638854980469, Learning Rate: 1.953125e-05\n",
      "Epoch [12556/20000], Loss: 864.159423828125, Entropy 450.1773986816406, Learning Rate: 1.953125e-05\n",
      "Epoch [12557/20000], Loss: 872.5316162109375, Entropy 458.648193359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12558/20000], Loss: 862.4664306640625, Entropy 446.1156005859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12559/20000], Loss: 889.211181640625, Entropy 440.0084228515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12560/20000], Loss: 838.4248046875, Entropy 459.274169921875, Learning Rate: 1.953125e-05\n",
      "Epoch [12561/20000], Loss: 848.49462890625, Entropy 460.8897705078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12562/20000], Loss: 869.2760009765625, Entropy 457.4044494628906, Learning Rate: 1.953125e-05\n",
      "Epoch [12563/20000], Loss: 845.9542236328125, Entropy 452.8074951171875, Learning Rate: 1.953125e-05\n",
      "Epoch [12564/20000], Loss: 826.1649169921875, Entropy 464.7725524902344, Learning Rate: 1.953125e-05\n",
      "Epoch [12565/20000], Loss: 886.8829345703125, Entropy 452.1786193847656, Learning Rate: 1.953125e-05\n",
      "Epoch [12566/20000], Loss: 819.3394775390625, Entropy 451.7427673339844, Learning Rate: 1.953125e-05\n",
      "Epoch [12567/20000], Loss: 846.5809326171875, Entropy 459.9774169921875, Learning Rate: 1.953125e-05\n",
      "Epoch [12568/20000], Loss: 853.775146484375, Entropy 467.3628845214844, Learning Rate: 1.953125e-05\n",
      "Epoch [12569/20000], Loss: 868.5147705078125, Entropy 457.0699157714844, Learning Rate: 1.953125e-05\n",
      "Epoch [12570/20000], Loss: 903.8455810546875, Entropy 450.7464599609375, Learning Rate: 1.953125e-05\n",
      "Epoch [12571/20000], Loss: 888.0444946289062, Entropy 446.13702392578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12572/20000], Loss: 920.0836181640625, Entropy 461.2375793457031, Learning Rate: 1.953125e-05\n",
      "Epoch [12573/20000], Loss: 944.1112060546875, Entropy 448.8547058105469, Learning Rate: 1.953125e-05\n",
      "Epoch [12574/20000], Loss: 852.6730346679688, Entropy 457.02764892578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12575/20000], Loss: 903.2412109375, Entropy 451.2275390625, Learning Rate: 1.953125e-05\n",
      "Epoch [12576/20000], Loss: 838.914794921875, Entropy 467.0378723144531, Learning Rate: 1.953125e-05\n",
      "Epoch [12577/20000], Loss: 903.2293701171875, Entropy 454.9290466308594, Learning Rate: 1.953125e-05\n",
      "Epoch [12578/20000], Loss: 848.572265625, Entropy 468.6165466308594, Learning Rate: 1.953125e-05\n",
      "Epoch [12579/20000], Loss: 854.661865234375, Entropy 444.0108642578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12580/20000], Loss: 829.1224365234375, Entropy 453.9066162109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12581/20000], Loss: 883.815673828125, Entropy 444.7292785644531, Learning Rate: 1.953125e-05\n",
      "Epoch [12582/20000], Loss: 854.186279296875, Entropy 454.4136657714844, Learning Rate: 1.953125e-05\n",
      "Epoch [12583/20000], Loss: 895.1841430664062, Entropy 459.06695556640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12584/20000], Loss: 862.3068237304688, Entropy 457.47906494140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12585/20000], Loss: 899.0977783203125, Entropy 450.0934143066406, Learning Rate: 1.953125e-05\n",
      "Epoch [12586/20000], Loss: 888.2433471679688, Entropy 455.96734619140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12587/20000], Loss: 835.963623046875, Entropy 448.5963439941406, Learning Rate: 1.953125e-05\n",
      "Epoch [12588/20000], Loss: 826.0479125976562, Entropy 465.11944580078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12589/20000], Loss: 817.7950439453125, Entropy 455.9241638183594, Learning Rate: 1.953125e-05\n",
      "Epoch [12590/20000], Loss: 823.0023803710938, Entropy 452.95367431640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12591/20000], Loss: 815.302001953125, Entropy 472.3604431152344, Learning Rate: 1.953125e-05\n",
      "Epoch [12592/20000], Loss: 847.8642578125, Entropy 474.6235046386719, Learning Rate: 1.953125e-05\n",
      "Epoch [12593/20000], Loss: 861.3523559570312, Entropy 461.22161865234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12594/20000], Loss: 901.5679931640625, Entropy 472.0027770996094, Learning Rate: 1.953125e-05\n",
      "Epoch [12595/20000], Loss: 813.89697265625, Entropy 456.6127624511719, Learning Rate: 1.953125e-05\n",
      "Epoch [12596/20000], Loss: 848.089111328125, Entropy 470.2680358886719, Learning Rate: 1.953125e-05\n",
      "Epoch [12597/20000], Loss: 879.9095458984375, Entropy 450.7987060546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12598/20000], Loss: 878.8541259765625, Entropy 460.685546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12599/20000], Loss: 916.3209838867188, Entropy 448.89654541015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12600/20000], Loss: 852.4107666015625, Entropy 450.1872863769531, Learning Rate: 1.953125e-05\n",
      "Epoch [12601/20000], Loss: 812.8916015625, Entropy 478.4007263183594, Learning Rate: 1.953125e-05\n",
      "Epoch [12602/20000], Loss: 867.3125, Entropy 461.7501525878906, Learning Rate: 1.953125e-05\n",
      "Epoch [12603/20000], Loss: 847.556640625, Entropy 432.8531494140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12604/20000], Loss: 902.716064453125, Entropy 464.0682067871094, Learning Rate: 1.953125e-05\n",
      "Epoch [12605/20000], Loss: 855.1859741210938, Entropy 445.16314697265625, Learning Rate: 1.953125e-05\n",
      "Epoch [12606/20000], Loss: 810.6671142578125, Entropy 459.0284423828125, Learning Rate: 1.953125e-05\n",
      "Epoch [12607/20000], Loss: 855.4574584960938, Entropy 457.16632080078125, Learning Rate: 1.953125e-05\n",
      "Epoch [12608/20000], Loss: 866.761962890625, Entropy 462.943359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12609/20000], Loss: 879.8568115234375, Entropy 455.8361511230469, Learning Rate: 1.953125e-05\n",
      "Epoch [12610/20000], Loss: 839.202880859375, Entropy 445.4634704589844, Learning Rate: 1.953125e-05\n",
      "Epoch [12611/20000], Loss: 914.0692138671875, Entropy 450.4451599121094, Learning Rate: 1.953125e-05\n",
      "Epoch [12612/20000], Loss: 875.73779296875, Entropy 453.7096862792969, Learning Rate: 1.953125e-05\n",
      "Epoch [12613/20000], Loss: 810.8203125, Entropy 489.6510925292969, Learning Rate: 1.953125e-05\n",
      "Epoch [12614/20000], Loss: 903.9814453125, Entropy 464.818359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12615/20000], Loss: 879.7216796875, Entropy 462.4013671875, Learning Rate: 1.953125e-05\n",
      "Epoch [12616/20000], Loss: 829.632080078125, Entropy 466.2225646972656, Learning Rate: 1.953125e-05\n",
      "Epoch [12617/20000], Loss: 832.2266845703125, Entropy 465.54638671875, Learning Rate: 1.953125e-05\n",
      "Epoch [12618/20000], Loss: 851.892333984375, Entropy 453.6614990234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12619/20000], Loss: 839.064208984375, Entropy 442.3293762207031, Learning Rate: 1.953125e-05\n",
      "Epoch [12620/20000], Loss: 815.952880859375, Entropy 440.5437316894531, Learning Rate: 1.953125e-05\n",
      "Epoch [12621/20000], Loss: 828.8541259765625, Entropy 457.11083984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12622/20000], Loss: 856.2743530273438, Entropy 451.12908935546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12623/20000], Loss: 826.7017822265625, Entropy 453.0734558105469, Learning Rate: 1.953125e-05\n",
      "Epoch [12624/20000], Loss: 887.7115478515625, Entropy 447.724365234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12625/20000], Loss: 855.7283935546875, Entropy 452.0773620605469, Learning Rate: 1.953125e-05\n",
      "Epoch [12626/20000], Loss: 860.6246337890625, Entropy 478.0607604980469, Learning Rate: 1.953125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12627/20000], Loss: 859.081787109375, Entropy 458.8731689453125, Learning Rate: 1.953125e-05\n",
      "Epoch [12628/20000], Loss: 852.4330444335938, Entropy 463.18756103515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12629/20000], Loss: 881.4282836914062, Entropy 454.56304931640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12630/20000], Loss: 861.9605102539062, Entropy 465.02069091796875, Learning Rate: 1.953125e-05\n",
      "Epoch [12631/20000], Loss: 829.6390380859375, Entropy 466.0199890136719, Learning Rate: 1.953125e-05\n",
      "Epoch [12632/20000], Loss: 854.1580810546875, Entropy 466.8849792480469, Learning Rate: 1.953125e-05\n",
      "Epoch [12633/20000], Loss: 836.9249267578125, Entropy 453.9254455566406, Learning Rate: 1.953125e-05\n",
      "Epoch [12634/20000], Loss: 887.8461303710938, Entropy 441.86138916015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12635/20000], Loss: 879.254638671875, Entropy 445.9146728515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12636/20000], Loss: 900.0119018554688, Entropy 439.96295166015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12637/20000], Loss: 785.5301513671875, Entropy 462.9039611816406, Learning Rate: 1.953125e-05\n",
      "Epoch [12638/20000], Loss: 833.16064453125, Entropy 458.4552307128906, Learning Rate: 1.953125e-05\n",
      "Epoch [12639/20000], Loss: 846.7947387695312, Entropy 455.71820068359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12640/20000], Loss: 818.043701171875, Entropy 460.21435546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12641/20000], Loss: 909.849853515625, Entropy 449.0748291015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12642/20000], Loss: 866.8554077148438, Entropy 440.26971435546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12643/20000], Loss: 881.8933715820312, Entropy 458.10406494140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12644/20000], Loss: 879.70556640625, Entropy 458.4500732421875, Learning Rate: 1.953125e-05\n",
      "Epoch [12645/20000], Loss: 891.899658203125, Entropy 452.8182067871094, Learning Rate: 1.953125e-05\n",
      "Epoch [12646/20000], Loss: 847.5220947265625, Entropy 450.7250671386719, Learning Rate: 1.953125e-05\n",
      "Epoch [12647/20000], Loss: 836.409912109375, Entropy 462.103271484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12648/20000], Loss: 872.7711791992188, Entropy 467.10076904296875, Learning Rate: 1.953125e-05\n",
      "Epoch [12649/20000], Loss: 860.4276123046875, Entropy 463.105224609375, Learning Rate: 1.953125e-05\n",
      "Epoch [12650/20000], Loss: 884.038330078125, Entropy 450.1983642578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12651/20000], Loss: 855.492919921875, Entropy 462.1817932128906, Learning Rate: 1.953125e-05\n",
      "Epoch [12652/20000], Loss: 801.5067749023438, Entropy 449.53057861328125, Learning Rate: 1.953125e-05\n",
      "Epoch [12653/20000], Loss: 831.6959228515625, Entropy 468.5959167480469, Learning Rate: 1.953125e-05\n",
      "Epoch [12654/20000], Loss: 808.50390625, Entropy 472.0085754394531, Learning Rate: 1.953125e-05\n",
      "Epoch [12655/20000], Loss: 868.4150390625, Entropy 461.8955383300781, Learning Rate: 1.953125e-05\n",
      "Epoch [12656/20000], Loss: 890.1146240234375, Entropy 462.1263122558594, Learning Rate: 1.953125e-05\n",
      "Epoch [12657/20000], Loss: 849.7637939453125, Entropy 463.7452392578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12658/20000], Loss: 858.33984375, Entropy 446.5279235839844, Learning Rate: 1.953125e-05\n",
      "Epoch [12659/20000], Loss: 842.2838745117188, Entropy 466.12127685546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12660/20000], Loss: 847.489013671875, Entropy 460.2938232421875, Learning Rate: 1.953125e-05\n",
      "Epoch [12661/20000], Loss: 839.8941650390625, Entropy 442.11083984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12662/20000], Loss: 840.03515625, Entropy 456.0934143066406, Learning Rate: 1.953125e-05\n",
      "Epoch [12663/20000], Loss: 868.4282836914062, Entropy 467.44281005859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12664/20000], Loss: 912.9146728515625, Entropy 462.7155456542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12665/20000], Loss: 877.2359619140625, Entropy 456.5031433105469, Learning Rate: 1.953125e-05\n",
      "Epoch [12666/20000], Loss: 843.0250244140625, Entropy 452.5046081542969, Learning Rate: 1.953125e-05\n",
      "Epoch [12667/20000], Loss: 822.058837890625, Entropy 476.513427734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12668/20000], Loss: 861.5443725585938, Entropy 453.05169677734375, Learning Rate: 1.953125e-05\n",
      "Epoch [12669/20000], Loss: 830.2037963867188, Entropy 461.96685791015625, Learning Rate: 1.953125e-05\n",
      "Epoch [12670/20000], Loss: 899.3485107421875, Entropy 452.90625, Learning Rate: 1.953125e-05\n",
      "Epoch [12671/20000], Loss: 859.7588500976562, Entropy 454.11773681640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12672/20000], Loss: 844.396240234375, Entropy 469.974609375, Learning Rate: 1.953125e-05\n",
      "Epoch [12673/20000], Loss: 901.164306640625, Entropy 459.1800537109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12674/20000], Loss: 854.616943359375, Entropy 455.0766296386719, Learning Rate: 1.953125e-05\n",
      "Epoch [12675/20000], Loss: 868.9627685546875, Entropy 442.1779479980469, Learning Rate: 1.953125e-05\n",
      "Epoch [12676/20000], Loss: 849.7186279296875, Entropy 456.4764099121094, Learning Rate: 1.953125e-05\n",
      "Epoch [12677/20000], Loss: 883.791748046875, Entropy 458.144287109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12678/20000], Loss: 824.091796875, Entropy 461.2198486328125, Learning Rate: 1.953125e-05\n",
      "Epoch [12679/20000], Loss: 845.9046020507812, Entropy 463.81658935546875, Learning Rate: 1.953125e-05\n",
      "Epoch [12680/20000], Loss: 864.20751953125, Entropy 447.2519836425781, Learning Rate: 1.953125e-05\n",
      "Epoch [12681/20000], Loss: 838.4962768554688, Entropy 463.97210693359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12682/20000], Loss: 808.47705078125, Entropy 440.3448486328125, Learning Rate: 1.953125e-05\n",
      "Epoch [12683/20000], Loss: 833.6235961914062, Entropy 450.63226318359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12684/20000], Loss: 907.6622314453125, Entropy 462.5559997558594, Learning Rate: 1.953125e-05\n",
      "Epoch [12685/20000], Loss: 933.1076049804688, Entropy 449.93609619140625, Learning Rate: 1.953125e-05\n",
      "Epoch [12686/20000], Loss: 813.9429931640625, Entropy 473.9068298339844, Learning Rate: 1.953125e-05\n",
      "Epoch [12687/20000], Loss: 791.5317993164062, Entropy 474.26507568359375, Learning Rate: 1.953125e-05\n",
      "Epoch [12688/20000], Loss: 864.7304077148438, Entropy 454.21124267578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12689/20000], Loss: 789.87353515625, Entropy 467.2537536621094, Learning Rate: 1.953125e-05\n",
      "Epoch [12690/20000], Loss: 866.2697143554688, Entropy 459.66802978515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12691/20000], Loss: 877.822021484375, Entropy 465.8733825683594, Learning Rate: 1.953125e-05\n",
      "Epoch [12692/20000], Loss: 843.5023193359375, Entropy 446.6759338378906, Learning Rate: 1.953125e-05\n",
      "Epoch [12693/20000], Loss: 865.0013427734375, Entropy 453.24365234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12694/20000], Loss: 890.2227783203125, Entropy 456.9011535644531, Learning Rate: 1.953125e-05\n",
      "Epoch [12695/20000], Loss: 836.8134765625, Entropy 467.6221008300781, Learning Rate: 1.953125e-05\n",
      "Epoch [12696/20000], Loss: 852.0283203125, Entropy 453.6221923828125, Learning Rate: 1.953125e-05\n",
      "Epoch [12697/20000], Loss: 860.955322265625, Entropy 468.1299743652344, Learning Rate: 1.953125e-05\n",
      "Epoch [12698/20000], Loss: 835.76025390625, Entropy 457.4874267578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12699/20000], Loss: 820.827880859375, Entropy 472.0459289550781, Learning Rate: 1.953125e-05\n",
      "Epoch [12700/20000], Loss: 869.0806274414062, Entropy 465.11358642578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12701/20000], Loss: 827.9219970703125, Entropy 456.8796691894531, Learning Rate: 1.953125e-05\n",
      "Epoch [12702/20000], Loss: 823.015380859375, Entropy 467.5099792480469, Learning Rate: 1.953125e-05\n",
      "Epoch [12703/20000], Loss: 856.4066162109375, Entropy 447.8890380859375, Learning Rate: 1.953125e-05\n",
      "Epoch [12704/20000], Loss: 821.34375, Entropy 452.4041442871094, Learning Rate: 1.953125e-05\n",
      "Epoch [12705/20000], Loss: 888.7706298828125, Entropy 437.9302978515625, Learning Rate: 1.953125e-05\n",
      "Epoch [12706/20000], Loss: 818.7981567382812, Entropy 455.84405517578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12707/20000], Loss: 858.6093139648438, Entropy 458.79193115234375, Learning Rate: 1.953125e-05\n",
      "Epoch [12708/20000], Loss: 861.0411987304688, Entropy 464.29742431640625, Learning Rate: 1.953125e-05\n",
      "Epoch [12709/20000], Loss: 853.12939453125, Entropy 447.3845520019531, Learning Rate: 1.953125e-05\n",
      "Epoch [12710/20000], Loss: 855.8250732421875, Entropy 453.7147521972656, Learning Rate: 1.953125e-05\n",
      "Epoch [12711/20000], Loss: 835.5294799804688, Entropy 474.30621337890625, Learning Rate: 1.953125e-05\n",
      "Epoch [12712/20000], Loss: 872.7906494140625, Entropy 455.4555358886719, Learning Rate: 1.953125e-05\n",
      "Epoch [12713/20000], Loss: 836.3795166015625, Entropy 458.6121826171875, Learning Rate: 1.953125e-05\n",
      "Epoch [12714/20000], Loss: 867.4893798828125, Entropy 460.2496643066406, Learning Rate: 1.953125e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12715/20000], Loss: 836.1719360351562, Entropy 451.12689208984375, Learning Rate: 1.953125e-05\n",
      "Epoch [12716/20000], Loss: 860.7516479492188, Entropy 462.75421142578125, Learning Rate: 1.953125e-05\n",
      "Epoch [12717/20000], Loss: 856.097412109375, Entropy 460.4542236328125, Learning Rate: 1.953125e-05\n",
      "Epoch [12718/20000], Loss: 833.7957763671875, Entropy 459.3719177246094, Learning Rate: 1.953125e-05\n",
      "Epoch [12719/20000], Loss: 853.3064575195312, Entropy 436.80316162109375, Learning Rate: 1.953125e-05\n",
      "Epoch [12720/20000], Loss: 831.1475219726562, Entropy 450.72650146484375, Learning Rate: 1.953125e-05\n",
      "Epoch [12721/20000], Loss: 843.580078125, Entropy 467.7212829589844, Learning Rate: 1.953125e-05\n",
      "Epoch [12722/20000], Loss: 900.0843505859375, Entropy 446.1529235839844, Learning Rate: 9.765625e-06\n",
      "Epoch [12723/20000], Loss: 868.9547119140625, Entropy 445.5304870605469, Learning Rate: 9.765625e-06\n",
      "Epoch [12724/20000], Loss: 823.7202758789062, Entropy 465.52825927734375, Learning Rate: 9.765625e-06\n",
      "Epoch [12725/20000], Loss: 850.8230590820312, Entropy 468.51654052734375, Learning Rate: 9.765625e-06\n",
      "Epoch [12726/20000], Loss: 874.8955688476562, Entropy 457.27850341796875, Learning Rate: 9.765625e-06\n",
      "Epoch [12727/20000], Loss: 886.4664306640625, Entropy 453.8424377441406, Learning Rate: 9.765625e-06\n",
      "Epoch [12728/20000], Loss: 831.022705078125, Entropy 456.5824890136719, Learning Rate: 9.765625e-06\n",
      "Epoch [12729/20000], Loss: 855.6612548828125, Entropy 460.3332824707031, Learning Rate: 9.765625e-06\n",
      "Epoch [12730/20000], Loss: 843.1192626953125, Entropy 458.4039001464844, Learning Rate: 9.765625e-06\n",
      "Epoch [12731/20000], Loss: 839.7333984375, Entropy 456.1850280761719, Learning Rate: 9.765625e-06\n",
      "Epoch [12732/20000], Loss: 828.3580322265625, Entropy 461.7878112792969, Learning Rate: 9.765625e-06\n",
      "Epoch [12733/20000], Loss: 869.1715698242188, Entropy 450.18560791015625, Learning Rate: 9.765625e-06\n",
      "Epoch [12734/20000], Loss: 869.948486328125, Entropy 475.9083557128906, Learning Rate: 9.765625e-06\n",
      "Epoch [12735/20000], Loss: 808.65234375, Entropy 462.619140625, Learning Rate: 9.765625e-06\n",
      "Epoch [12736/20000], Loss: 865.6223754882812, Entropy 444.38177490234375, Learning Rate: 9.765625e-06\n",
      "Epoch [12737/20000], Loss: 879.0963134765625, Entropy 455.316162109375, Learning Rate: 9.765625e-06\n",
      "Epoch [12738/20000], Loss: 857.9725341796875, Entropy 459.7467956542969, Learning Rate: 9.765625e-06\n",
      "Epoch [12739/20000], Loss: 876.0081176757812, Entropy 459.98687744140625, Learning Rate: 9.765625e-06\n",
      "Epoch [12740/20000], Loss: 857.3828125, Entropy 458.5417175292969, Learning Rate: 9.765625e-06\n",
      "Epoch [12741/20000], Loss: 817.213134765625, Entropy 474.6575012207031, Learning Rate: 9.765625e-06\n",
      "Epoch [12742/20000], Loss: 874.971923828125, Entropy 443.3271789550781, Learning Rate: 9.765625e-06\n",
      "Epoch [12743/20000], Loss: 810.4447021484375, Entropy 445.5715026855469, Learning Rate: 9.765625e-06\n",
      "Epoch [12744/20000], Loss: 859.2198486328125, Entropy 458.3760986328125, Learning Rate: 9.765625e-06\n",
      "Epoch [12745/20000], Loss: 828.67041015625, Entropy 456.14013671875, Learning Rate: 9.765625e-06\n",
      "Epoch [12746/20000], Loss: 846.1214599609375, Entropy 460.6559143066406, Learning Rate: 9.765625e-06\n",
      "Epoch [12747/20000], Loss: 865.411865234375, Entropy 451.5509338378906, Learning Rate: 9.765625e-06\n",
      "Epoch [12748/20000], Loss: 819.375732421875, Entropy 480.0856018066406, Learning Rate: 9.765625e-06\n",
      "Epoch [12749/20000], Loss: 829.942626953125, Entropy 464.6874084472656, Learning Rate: 9.765625e-06\n",
      "Epoch [12750/20000], Loss: 825.214111328125, Entropy 458.5467529296875, Learning Rate: 9.765625e-06\n",
      "Epoch [12751/20000], Loss: 810.5189208984375, Entropy 461.7591247558594, Learning Rate: 9.765625e-06\n",
      "Epoch [12752/20000], Loss: 833.325927734375, Entropy 462.9303894042969, Learning Rate: 9.765625e-06\n",
      "Epoch [12753/20000], Loss: 872.2676391601562, Entropy 444.74920654296875, Learning Rate: 9.765625e-06\n",
      "Epoch [12754/20000], Loss: 846.5703125, Entropy 457.6310729980469, Learning Rate: 9.765625e-06\n",
      "Epoch [12755/20000], Loss: 865.1644287109375, Entropy 454.0353088378906, Learning Rate: 9.765625e-06\n",
      "Epoch [12756/20000], Loss: 832.2099609375, Entropy 460.5727233886719, Learning Rate: 9.765625e-06\n",
      "Epoch [12757/20000], Loss: 838.734130859375, Entropy 464.0592041015625, Learning Rate: 9.765625e-06\n",
      "Epoch [12758/20000], Loss: 868.4196166992188, Entropy 450.77264404296875, Learning Rate: 9.765625e-06\n",
      "Epoch [12759/20000], Loss: 850.173828125, Entropy 454.1506652832031, Learning Rate: 9.765625e-06\n",
      "Epoch [12760/20000], Loss: 830.6314086914062, Entropy 465.29046630859375, Learning Rate: 9.765625e-06\n",
      "Epoch [12761/20000], Loss: 871.2591552734375, Entropy 432.0350036621094, Learning Rate: 9.765625e-06\n",
      "Epoch [12762/20000], Loss: 880.7808227539062, Entropy 441.11138916015625, Learning Rate: 9.765625e-06\n",
      "Epoch [12763/20000], Loss: 899.70166015625, Entropy 454.5307312011719, Learning Rate: 9.765625e-06\n",
      "Epoch [12764/20000], Loss: 870.5025024414062, Entropy 457.31304931640625, Learning Rate: 9.765625e-06\n",
      "Epoch [12765/20000], Loss: 833.78466796875, Entropy 452.4170227050781, Learning Rate: 9.765625e-06\n",
      "Epoch [12766/20000], Loss: 895.6754150390625, Entropy 468.540771484375, Learning Rate: 9.765625e-06\n",
      "Epoch [12767/20000], Loss: 864.1474609375, Entropy 455.2896728515625, Learning Rate: 9.765625e-06\n",
      "Epoch [12768/20000], Loss: 853.3191528320312, Entropy 453.50726318359375, Learning Rate: 9.765625e-06\n",
      "Epoch [12769/20000], Loss: 857.0765380859375, Entropy 463.7678527832031, Learning Rate: 9.765625e-06\n",
      "Epoch [12770/20000], Loss: 873.6964111328125, Entropy 450.843505859375, Learning Rate: 9.765625e-06\n",
      "Epoch [12771/20000], Loss: 823.6583862304688, Entropy 467.47186279296875, Learning Rate: 9.765625e-06\n",
      "Epoch [12772/20000], Loss: 849.518798828125, Entropy 452.6824035644531, Learning Rate: 9.765625e-06\n",
      "Epoch [12773/20000], Loss: 860.5443115234375, Entropy 463.3412780761719, Learning Rate: 9.765625e-06\n",
      "Epoch [12774/20000], Loss: 827.1437377929688, Entropy 457.16656494140625, Learning Rate: 9.765625e-06\n",
      "Epoch [12775/20000], Loss: 816.2354736328125, Entropy 464.2856140136719, Learning Rate: 9.765625e-06\n",
      "Epoch [12776/20000], Loss: 842.269287109375, Entropy 457.2795104980469, Learning Rate: 9.765625e-06\n",
      "Epoch [12777/20000], Loss: 824.1922607421875, Entropy 458.6924133300781, Learning Rate: 9.765625e-06\n",
      "Epoch [12778/20000], Loss: 848.6561279296875, Entropy 464.93505859375, Learning Rate: 9.765625e-06\n",
      "Epoch [12779/20000], Loss: 828.0909423828125, Entropy 456.7210693359375, Learning Rate: 9.765625e-06\n",
      "Epoch [12780/20000], Loss: 874.9879150390625, Entropy 438.2504577636719, Learning Rate: 9.765625e-06\n",
      "Epoch [12781/20000], Loss: 865.8983154296875, Entropy 461.445068359375, Learning Rate: 9.765625e-06\n",
      "Epoch [12782/20000], Loss: 841.5902099609375, Entropy 465.9035339355469, Learning Rate: 9.765625e-06\n",
      "Epoch [12783/20000], Loss: 985.7017822265625, Entropy 452.3009948730469, Learning Rate: 9.765625e-06\n",
      "Epoch [12784/20000], Loss: 902.67626953125, Entropy 457.8327331542969, Learning Rate: 9.765625e-06\n",
      "Epoch [12785/20000], Loss: 850.5101318359375, Entropy 452.5301818847656, Learning Rate: 9.765625e-06\n",
      "Epoch [12786/20000], Loss: 868.95556640625, Entropy 464.3972473144531, Learning Rate: 9.765625e-06\n",
      "Epoch [12787/20000], Loss: 825.0250244140625, Entropy 468.1883850097656, Learning Rate: 9.765625e-06\n",
      "Epoch [12788/20000], Loss: 833.6282958984375, Entropy 470.2785949707031, Learning Rate: 9.765625e-06\n",
      "Epoch [12789/20000], Loss: 822.5038452148438, Entropy 460.62054443359375, Learning Rate: 9.765625e-06\n",
      "Epoch [12790/20000], Loss: 886.369140625, Entropy 458.6051330566406, Learning Rate: 9.765625e-06\n",
      "Epoch [12791/20000], Loss: 814.7440795898438, Entropy 461.19171142578125, Learning Rate: 9.765625e-06\n",
      "Epoch [12792/20000], Loss: 836.31591796875, Entropy 454.4393005371094, Learning Rate: 9.765625e-06\n",
      "Epoch [12793/20000], Loss: 877.095947265625, Entropy 472.9621887207031, Learning Rate: 9.765625e-06\n",
      "Epoch [12794/20000], Loss: 858.1525268554688, Entropy 444.88726806640625, Learning Rate: 9.765625e-06\n",
      "Epoch [12795/20000], Loss: 861.7794189453125, Entropy 453.2007751464844, Learning Rate: 9.765625e-06\n",
      "Epoch [12796/20000], Loss: 910.6326904296875, Entropy 464.9070129394531, Learning Rate: 9.765625e-06\n",
      "Epoch [12797/20000], Loss: 840.2353515625, Entropy 463.3585510253906, Learning Rate: 9.765625e-06\n",
      "Epoch [12798/20000], Loss: 877.447509765625, Entropy 447.27685546875, Learning Rate: 9.765625e-06\n",
      "Epoch [12799/20000], Loss: 876.7244262695312, Entropy 459.96600341796875, Learning Rate: 9.765625e-06\n",
      "Epoch [12800/20000], Loss: 831.0648803710938, Entropy 487.04449462890625, Learning Rate: 9.765625e-06\n",
      "Epoch [12801/20000], Loss: 794.8916015625, Entropy 470.3631896972656, Learning Rate: 9.765625e-06\n",
      "Epoch [12802/20000], Loss: 848.989501953125, Entropy 450.86328125, Learning Rate: 9.765625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12803/20000], Loss: 820.8138427734375, Entropy 470.406005859375, Learning Rate: 9.765625e-06\n",
      "Epoch [12804/20000], Loss: 844.3814697265625, Entropy 445.802490234375, Learning Rate: 9.765625e-06\n",
      "Epoch [12805/20000], Loss: 817.5118408203125, Entropy 461.9613952636719, Learning Rate: 9.765625e-06\n",
      "Epoch [12806/20000], Loss: 847.5953369140625, Entropy 445.4219970703125, Learning Rate: 9.765625e-06\n",
      "Epoch [12807/20000], Loss: 863.1820678710938, Entropy 462.52777099609375, Learning Rate: 9.765625e-06\n",
      "Epoch [12808/20000], Loss: 870.14794921875, Entropy 450.5933532714844, Learning Rate: 9.765625e-06\n",
      "Epoch [12809/20000], Loss: 839.893310546875, Entropy 450.742919921875, Learning Rate: 9.765625e-06\n",
      "Epoch [12810/20000], Loss: 907.5145874023438, Entropy 468.47869873046875, Learning Rate: 9.765625e-06\n",
      "Epoch [12811/20000], Loss: 853.056640625, Entropy 454.2200927734375, Learning Rate: 9.765625e-06\n",
      "Epoch [12812/20000], Loss: 853.5440673828125, Entropy 453.3365478515625, Learning Rate: 9.765625e-06\n",
      "Epoch [12813/20000], Loss: 856.521728515625, Entropy 460.478515625, Learning Rate: 9.765625e-06\n",
      "Epoch [12814/20000], Loss: 854.7110595703125, Entropy 461.0654296875, Learning Rate: 9.765625e-06\n",
      "Epoch [12815/20000], Loss: 849.286865234375, Entropy 455.7890625, Learning Rate: 9.765625e-06\n",
      "Epoch [12816/20000], Loss: 873.3047485351562, Entropy 468.44647216796875, Learning Rate: 9.765625e-06\n",
      "Epoch [12817/20000], Loss: 840.2823486328125, Entropy 456.6546630859375, Learning Rate: 9.765625e-06\n",
      "Epoch [12818/20000], Loss: 933.3333740234375, Entropy 463.4500732421875, Learning Rate: 9.765625e-06\n",
      "Epoch [12819/20000], Loss: 844.6456298828125, Entropy 454.0086669921875, Learning Rate: 9.765625e-06\n",
      "Epoch [12820/20000], Loss: 839.427490234375, Entropy 462.1517028808594, Learning Rate: 9.765625e-06\n",
      "Epoch [12821/20000], Loss: 874.2601318359375, Entropy 458.7833251953125, Learning Rate: 9.765625e-06\n",
      "Epoch [12822/20000], Loss: 912.25537109375, Entropy 456.3402099609375, Learning Rate: 9.765625e-06\n",
      "Epoch [12823/20000], Loss: 827.9912109375, Entropy 456.8475036621094, Learning Rate: 9.765625e-06\n",
      "Epoch [12824/20000], Loss: 804.905029296875, Entropy 452.5116271972656, Learning Rate: 9.765625e-06\n",
      "Epoch [12825/20000], Loss: 896.077880859375, Entropy 455.1409912109375, Learning Rate: 9.765625e-06\n",
      "Epoch [12826/20000], Loss: 889.7906494140625, Entropy 472.6524353027344, Learning Rate: 9.765625e-06\n",
      "Epoch [12827/20000], Loss: 861.1181030273438, Entropy 448.21331787109375, Learning Rate: 9.765625e-06\n",
      "Epoch [12828/20000], Loss: 837.3115234375, Entropy 455.8009948730469, Learning Rate: 9.765625e-06\n",
      "Epoch [12829/20000], Loss: 855.9541625976562, Entropy 445.23883056640625, Learning Rate: 9.765625e-06\n",
      "Epoch [12830/20000], Loss: 875.827392578125, Entropy 453.1061706542969, Learning Rate: 9.765625e-06\n",
      "Epoch [12831/20000], Loss: 886.5185546875, Entropy 447.7116394042969, Learning Rate: 9.765625e-06\n",
      "Epoch [12832/20000], Loss: 897.8135986328125, Entropy 458.3331298828125, Learning Rate: 9.765625e-06\n",
      "Epoch [12833/20000], Loss: 862.9403076171875, Entropy 447.6842956542969, Learning Rate: 9.765625e-06\n",
      "Epoch [12834/20000], Loss: 820.7012939453125, Entropy 457.2555847167969, Learning Rate: 9.765625e-06\n",
      "Epoch [12835/20000], Loss: 883.7783203125, Entropy 450.0978698730469, Learning Rate: 9.765625e-06\n",
      "Epoch [12836/20000], Loss: 801.4281005859375, Entropy 470.7861633300781, Learning Rate: 9.765625e-06\n",
      "Epoch [12837/20000], Loss: 823.2032470703125, Entropy 445.1647644042969, Learning Rate: 9.765625e-06\n",
      "Epoch [12838/20000], Loss: 885.2176513671875, Entropy 473.9071044921875, Learning Rate: 9.765625e-06\n",
      "Epoch [12839/20000], Loss: 808.6737060546875, Entropy 487.1804504394531, Learning Rate: 9.765625e-06\n",
      "Epoch [12840/20000], Loss: 838.7113037109375, Entropy 453.20458984375, Learning Rate: 9.765625e-06\n",
      "Epoch [12841/20000], Loss: 856.5577392578125, Entropy 464.8609313964844, Learning Rate: 9.765625e-06\n",
      "Epoch [12842/20000], Loss: 863.6456298828125, Entropy 441.27294921875, Learning Rate: 9.765625e-06\n",
      "Epoch [12843/20000], Loss: 850.514404296875, Entropy 466.9744567871094, Learning Rate: 9.765625e-06\n",
      "Epoch [12844/20000], Loss: 853.9501953125, Entropy 482.9139709472656, Learning Rate: 9.765625e-06\n",
      "Epoch [12845/20000], Loss: 847.60400390625, Entropy 478.01123046875, Learning Rate: 9.765625e-06\n",
      "Epoch [12846/20000], Loss: 864.681884765625, Entropy 446.6983642578125, Learning Rate: 9.765625e-06\n",
      "Epoch [12847/20000], Loss: 851.827880859375, Entropy 467.0166931152344, Learning Rate: 9.765625e-06\n",
      "Epoch [12848/20000], Loss: 882.77978515625, Entropy 458.667724609375, Learning Rate: 9.765625e-06\n",
      "Epoch [12849/20000], Loss: 855.961669921875, Entropy 448.841552734375, Learning Rate: 9.765625e-06\n",
      "Epoch [12850/20000], Loss: 863.2366943359375, Entropy 459.4710388183594, Learning Rate: 9.765625e-06\n",
      "Epoch [12851/20000], Loss: 818.2034301757812, Entropy 455.34820556640625, Learning Rate: 9.765625e-06\n",
      "Epoch [12852/20000], Loss: 794.104248046875, Entropy 468.7026062011719, Learning Rate: 9.765625e-06\n",
      "Epoch [12853/20000], Loss: 837.6088256835938, Entropy 450.29022216796875, Learning Rate: 9.765625e-06\n",
      "Epoch [12854/20000], Loss: 840.72265625, Entropy 459.3648681640625, Learning Rate: 9.765625e-06\n",
      "Epoch [12855/20000], Loss: 839.04052734375, Entropy 447.9518737792969, Learning Rate: 9.765625e-06\n",
      "Epoch [12856/20000], Loss: 852.7803344726562, Entropy 452.52630615234375, Learning Rate: 9.765625e-06\n",
      "Epoch [12857/20000], Loss: 850.1099853515625, Entropy 446.9596862792969, Learning Rate: 9.765625e-06\n",
      "Epoch [12858/20000], Loss: 909.6607666015625, Entropy 464.5848693847656, Learning Rate: 9.765625e-06\n",
      "Epoch [12859/20000], Loss: 802.213623046875, Entropy 459.2314758300781, Learning Rate: 9.765625e-06\n",
      "Epoch [12860/20000], Loss: 814.8787841796875, Entropy 461.3352966308594, Learning Rate: 9.765625e-06\n",
      "Epoch [12861/20000], Loss: 821.7679443359375, Entropy 471.6697082519531, Learning Rate: 9.765625e-06\n",
      "Epoch [12862/20000], Loss: 856.4581298828125, Entropy 463.1957702636719, Learning Rate: 9.765625e-06\n",
      "Epoch [12863/20000], Loss: 862.80517578125, Entropy 458.8126220703125, Learning Rate: 9.765625e-06\n",
      "Epoch [12864/20000], Loss: 878.817138671875, Entropy 453.4284973144531, Learning Rate: 9.765625e-06\n",
      "Epoch [12865/20000], Loss: 886.826416015625, Entropy 451.7619323730469, Learning Rate: 9.765625e-06\n",
      "Epoch [12866/20000], Loss: 888.2972412109375, Entropy 458.0188903808594, Learning Rate: 9.765625e-06\n",
      "Epoch [12867/20000], Loss: 854.4490966796875, Entropy 457.2919006347656, Learning Rate: 9.765625e-06\n",
      "Epoch [12868/20000], Loss: 832.2887573242188, Entropy 464.89166259765625, Learning Rate: 9.765625e-06\n",
      "Epoch [12869/20000], Loss: 835.6123046875, Entropy 456.2009582519531, Learning Rate: 9.765625e-06\n",
      "Epoch [12870/20000], Loss: 875.3209228515625, Entropy 449.5670471191406, Learning Rate: 9.765625e-06\n",
      "Epoch [12871/20000], Loss: 854.274169921875, Entropy 457.2136535644531, Learning Rate: 9.765625e-06\n",
      "Epoch [12872/20000], Loss: 838.4932861328125, Entropy 461.8940124511719, Learning Rate: 9.765625e-06\n",
      "Epoch [12873/20000], Loss: 853.1021728515625, Entropy 449.7599182128906, Learning Rate: 9.765625e-06\n",
      "Epoch [12874/20000], Loss: 870.1384887695312, Entropy 458.51898193359375, Learning Rate: 9.765625e-06\n",
      "Epoch [12875/20000], Loss: 847.6090087890625, Entropy 451.9781188964844, Learning Rate: 9.765625e-06\n",
      "Epoch [12876/20000], Loss: 889.2847290039062, Entropy 445.39508056640625, Learning Rate: 9.765625e-06\n",
      "Epoch [12877/20000], Loss: 912.121337890625, Entropy 452.7286682128906, Learning Rate: 9.765625e-06\n",
      "Epoch [12878/20000], Loss: 862.0282592773438, Entropy 459.24237060546875, Learning Rate: 9.765625e-06\n",
      "Epoch [12879/20000], Loss: 818.67919921875, Entropy 457.20654296875, Learning Rate: 9.765625e-06\n",
      "Epoch [12880/20000], Loss: 866.59130859375, Entropy 460.9501953125, Learning Rate: 9.765625e-06\n",
      "Epoch [12881/20000], Loss: 853.2940673828125, Entropy 473.7337646484375, Learning Rate: 9.765625e-06\n",
      "Epoch [12882/20000], Loss: 849.6006469726562, Entropy 466.98590087890625, Learning Rate: 9.765625e-06\n",
      "Epoch [12883/20000], Loss: 900.89306640625, Entropy 443.61669921875, Learning Rate: 9.765625e-06\n",
      "Epoch [12884/20000], Loss: 871.06591796875, Entropy 449.2522888183594, Learning Rate: 9.765625e-06\n",
      "Epoch [12885/20000], Loss: 902.57568359375, Entropy 439.5940246582031, Learning Rate: 9.765625e-06\n",
      "Epoch [12886/20000], Loss: 845.8035888671875, Entropy 468.424560546875, Learning Rate: 9.765625e-06\n",
      "Epoch [12887/20000], Loss: 873.3348388671875, Entropy 460.10791015625, Learning Rate: 9.765625e-06\n",
      "Epoch [12888/20000], Loss: 896.4053955078125, Entropy 462.6799621582031, Learning Rate: 9.765625e-06\n",
      "Epoch [12889/20000], Loss: 846.030029296875, Entropy 463.5947265625, Learning Rate: 9.765625e-06\n",
      "Epoch [12890/20000], Loss: 916.2845458984375, Entropy 456.8338928222656, Learning Rate: 9.765625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12891/20000], Loss: 871.5429077148438, Entropy 453.75421142578125, Learning Rate: 9.765625e-06\n",
      "Epoch [12892/20000], Loss: 905.0641479492188, Entropy 450.45648193359375, Learning Rate: 9.765625e-06\n",
      "Epoch [12893/20000], Loss: 863.2160034179688, Entropy 449.11871337890625, Learning Rate: 9.765625e-06\n",
      "Epoch [12894/20000], Loss: 905.883056640625, Entropy 428.3021240234375, Learning Rate: 9.765625e-06\n",
      "Epoch [12895/20000], Loss: 871.03955078125, Entropy 460.7166442871094, Learning Rate: 9.765625e-06\n",
      "Epoch [12896/20000], Loss: 873.8631591796875, Entropy 462.64599609375, Learning Rate: 9.765625e-06\n",
      "Epoch [12897/20000], Loss: 875.4684448242188, Entropy 448.13421630859375, Learning Rate: 9.765625e-06\n",
      "Epoch [12898/20000], Loss: 860.86572265625, Entropy 469.2274475097656, Learning Rate: 9.765625e-06\n",
      "Epoch [12899/20000], Loss: 860.6934814453125, Entropy 459.2317810058594, Learning Rate: 9.765625e-06\n",
      "Epoch [12900/20000], Loss: 871.6790771484375, Entropy 457.0965576171875, Learning Rate: 9.765625e-06\n",
      "Epoch [12901/20000], Loss: 882.39306640625, Entropy 450.1632995605469, Learning Rate: 9.765625e-06\n",
      "Epoch [12902/20000], Loss: 864.7169189453125, Entropy 456.9632568359375, Learning Rate: 9.765625e-06\n",
      "Epoch [12903/20000], Loss: 893.1595458984375, Entropy 458.5671081542969, Learning Rate: 9.765625e-06\n",
      "Epoch [12904/20000], Loss: 863.37890625, Entropy 463.9884033203125, Learning Rate: 9.765625e-06\n",
      "Epoch [12905/20000], Loss: 774.0003662109375, Entropy 452.2738952636719, Learning Rate: 9.765625e-06\n",
      "Epoch [12906/20000], Loss: 881.2041015625, Entropy 455.8076477050781, Learning Rate: 9.765625e-06\n",
      "Epoch [12907/20000], Loss: 842.3175048828125, Entropy 455.5967102050781, Learning Rate: 9.765625e-06\n",
      "Epoch [12908/20000], Loss: 824.203369140625, Entropy 461.8778076171875, Learning Rate: 9.765625e-06\n",
      "Epoch [12909/20000], Loss: 861.5068359375, Entropy 450.7628173828125, Learning Rate: 9.765625e-06\n",
      "Epoch [12910/20000], Loss: 845.7664184570312, Entropy 463.12603759765625, Learning Rate: 9.765625e-06\n",
      "Epoch [12911/20000], Loss: 831.79443359375, Entropy 458.9346923828125, Learning Rate: 9.765625e-06\n",
      "Epoch [12912/20000], Loss: 843.7076416015625, Entropy 458.1636657714844, Learning Rate: 9.765625e-06\n",
      "Epoch [12913/20000], Loss: 856.7725219726562, Entropy 459.03900146484375, Learning Rate: 9.765625e-06\n",
      "Epoch [12914/20000], Loss: 835.233642578125, Entropy 469.2557678222656, Learning Rate: 9.765625e-06\n",
      "Epoch [12915/20000], Loss: 866.6231689453125, Entropy 442.86474609375, Learning Rate: 9.765625e-06\n",
      "Epoch [12916/20000], Loss: 897.1776123046875, Entropy 452.3265075683594, Learning Rate: 9.765625e-06\n",
      "Epoch [12917/20000], Loss: 849.43408203125, Entropy 466.4006042480469, Learning Rate: 9.765625e-06\n",
      "Epoch [12918/20000], Loss: 867.728271484375, Entropy 454.7563171386719, Learning Rate: 9.765625e-06\n",
      "Epoch [12919/20000], Loss: 856.9627685546875, Entropy 454.2000732421875, Learning Rate: 9.765625e-06\n",
      "Epoch [12920/20000], Loss: 936.0804443359375, Entropy 453.6251220703125, Learning Rate: 9.765625e-06\n",
      "Epoch [12921/20000], Loss: 845.522705078125, Entropy 469.5213317871094, Learning Rate: 9.765625e-06\n",
      "Epoch [12922/20000], Loss: 849.97314453125, Entropy 461.3580322265625, Learning Rate: 9.765625e-06\n",
      "Epoch [12923/20000], Loss: 861.5260009765625, Entropy 457.6443786621094, Learning Rate: 9.765625e-06\n",
      "Epoch [12924/20000], Loss: 828.8578491210938, Entropy 475.14007568359375, Learning Rate: 9.765625e-06\n",
      "Epoch [12925/20000], Loss: 883.1053466796875, Entropy 457.4033203125, Learning Rate: 9.765625e-06\n",
      "Epoch [12926/20000], Loss: 855.23583984375, Entropy 442.6837158203125, Learning Rate: 9.765625e-06\n",
      "Epoch [12927/20000], Loss: 817.3712158203125, Entropy 471.9990539550781, Learning Rate: 9.765625e-06\n",
      "Epoch [12928/20000], Loss: 876.6953125, Entropy 444.3984375, Learning Rate: 9.765625e-06\n",
      "Epoch [12929/20000], Loss: 880.03564453125, Entropy 445.88232421875, Learning Rate: 9.765625e-06\n",
      "Epoch [12930/20000], Loss: 822.1129150390625, Entropy 467.3814392089844, Learning Rate: 9.765625e-06\n",
      "Epoch [12931/20000], Loss: 819.077392578125, Entropy 464.5867919921875, Learning Rate: 9.765625e-06\n",
      "Epoch [12932/20000], Loss: 822.2958984375, Entropy 460.4992370605469, Learning Rate: 9.765625e-06\n",
      "Epoch [12933/20000], Loss: 838.683837890625, Entropy 446.1271667480469, Learning Rate: 9.765625e-06\n",
      "Epoch [12934/20000], Loss: 867.6799926757812, Entropy 457.04290771484375, Learning Rate: 9.765625e-06\n",
      "Epoch [12935/20000], Loss: 870.86279296875, Entropy 460.9313049316406, Learning Rate: 9.765625e-06\n",
      "Epoch [12936/20000], Loss: 838.6654663085938, Entropy 460.48553466796875, Learning Rate: 9.765625e-06\n",
      "Epoch [12937/20000], Loss: 837.36328125, Entropy 459.69873046875, Learning Rate: 9.765625e-06\n",
      "Epoch [12938/20000], Loss: 925.3291015625, Entropy 447.7335205078125, Learning Rate: 9.765625e-06\n",
      "Epoch [12939/20000], Loss: 854.67333984375, Entropy 465.0001525878906, Learning Rate: 9.765625e-06\n",
      "Epoch [12940/20000], Loss: 828.277587890625, Entropy 450.4478759765625, Learning Rate: 9.765625e-06\n",
      "Epoch [12941/20000], Loss: 863.0662841796875, Entropy 449.6942443847656, Learning Rate: 9.765625e-06\n",
      "Epoch [12942/20000], Loss: 847.3944091796875, Entropy 470.17333984375, Learning Rate: 9.765625e-06\n",
      "Epoch [12943/20000], Loss: 868.5408935546875, Entropy 454.6288146972656, Learning Rate: 9.765625e-06\n",
      "Epoch [12944/20000], Loss: 834.434814453125, Entropy 457.9716796875, Learning Rate: 9.765625e-06\n",
      "Epoch [12945/20000], Loss: 822.6148681640625, Entropy 451.2601013183594, Learning Rate: 9.765625e-06\n",
      "Epoch [12946/20000], Loss: 869.1395263671875, Entropy 452.564208984375, Learning Rate: 9.765625e-06\n",
      "Epoch [12947/20000], Loss: 825.2899169921875, Entropy 452.2593994140625, Learning Rate: 9.765625e-06\n",
      "Epoch [12948/20000], Loss: 796.555419921875, Entropy 462.2085876464844, Learning Rate: 9.765625e-06\n",
      "Epoch [12949/20000], Loss: 869.8325805664062, Entropy 455.82537841796875, Learning Rate: 9.765625e-06\n",
      "Epoch [12950/20000], Loss: 843.21630859375, Entropy 461.7155456542969, Learning Rate: 9.765625e-06\n",
      "Epoch [12951/20000], Loss: 849.1436767578125, Entropy 453.9781494140625, Learning Rate: 9.765625e-06\n",
      "Epoch [12952/20000], Loss: 880.0857543945312, Entropy 457.61614990234375, Learning Rate: 9.765625e-06\n",
      "Epoch [12953/20000], Loss: 844.6309204101562, Entropy 455.12115478515625, Learning Rate: 9.765625e-06\n",
      "Epoch [12954/20000], Loss: 897.3494873046875, Entropy 463.1717529296875, Learning Rate: 9.765625e-06\n",
      "Epoch [12955/20000], Loss: 871.015380859375, Entropy 454.18115234375, Learning Rate: 9.765625e-06\n",
      "Epoch [12956/20000], Loss: 837.31787109375, Entropy 458.1833190917969, Learning Rate: 9.765625e-06\n",
      "Epoch [12957/20000], Loss: 891.5172729492188, Entropy 452.98382568359375, Learning Rate: 9.765625e-06\n",
      "Epoch [12958/20000], Loss: 827.2345581054688, Entropy 460.95611572265625, Learning Rate: 9.765625e-06\n",
      "Epoch [12959/20000], Loss: 857.1123657226562, Entropy 440.18951416015625, Learning Rate: 9.765625e-06\n",
      "Epoch [12960/20000], Loss: 867.5608520507812, Entropy 450.54534912109375, Learning Rate: 9.765625e-06\n",
      "Epoch [12961/20000], Loss: 850.377685546875, Entropy 468.5844421386719, Learning Rate: 9.765625e-06\n",
      "Epoch [12962/20000], Loss: 857.5599365234375, Entropy 452.890625, Learning Rate: 9.765625e-06\n",
      "Epoch [12963/20000], Loss: 875.869873046875, Entropy 462.3959655761719, Learning Rate: 9.765625e-06\n",
      "Epoch [12964/20000], Loss: 863.6309814453125, Entropy 451.9742736816406, Learning Rate: 9.765625e-06\n",
      "Epoch [12965/20000], Loss: 838.802001953125, Entropy 460.5242004394531, Learning Rate: 9.765625e-06\n",
      "Epoch [12966/20000], Loss: 835.0010986328125, Entropy 466.7632751464844, Learning Rate: 9.765625e-06\n",
      "Epoch [12967/20000], Loss: 843.9647216796875, Entropy 465.1911315917969, Learning Rate: 9.765625e-06\n",
      "Epoch [12968/20000], Loss: 843.4984130859375, Entropy 467.52685546875, Learning Rate: 9.765625e-06\n",
      "Epoch [12969/20000], Loss: 803.6759643554688, Entropy 457.74261474609375, Learning Rate: 9.765625e-06\n",
      "Epoch [12970/20000], Loss: 845.2703857421875, Entropy 464.0374755859375, Learning Rate: 9.765625e-06\n",
      "Epoch [12971/20000], Loss: 849.568115234375, Entropy 455.384521484375, Learning Rate: 9.765625e-06\n",
      "Epoch [12972/20000], Loss: 843.0765380859375, Entropy 465.0218505859375, Learning Rate: 9.765625e-06\n",
      "Epoch [12973/20000], Loss: 923.3631591796875, Entropy 434.9181213378906, Learning Rate: 9.765625e-06\n",
      "Epoch [12974/20000], Loss: 835.7384033203125, Entropy 472.84130859375, Learning Rate: 9.765625e-06\n",
      "Epoch [12975/20000], Loss: 834.2781982421875, Entropy 466.7039489746094, Learning Rate: 9.765625e-06\n",
      "Epoch [12976/20000], Loss: 844.7738037109375, Entropy 466.2077941894531, Learning Rate: 9.765625e-06\n",
      "Epoch [12977/20000], Loss: 894.8795166015625, Entropy 465.2645263671875, Learning Rate: 9.765625e-06\n",
      "Epoch [12978/20000], Loss: 854.7435302734375, Entropy 455.347412109375, Learning Rate: 9.765625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12979/20000], Loss: 832.784423828125, Entropy 466.4412536621094, Learning Rate: 9.765625e-06\n",
      "Epoch [12980/20000], Loss: 883.9151611328125, Entropy 451.69384765625, Learning Rate: 9.765625e-06\n",
      "Epoch [12981/20000], Loss: 875.465576171875, Entropy 460.7991943359375, Learning Rate: 9.765625e-06\n",
      "Epoch [12982/20000], Loss: 856.923583984375, Entropy 466.8131408691406, Learning Rate: 9.765625e-06\n",
      "Epoch [12983/20000], Loss: 828.534912109375, Entropy 455.3391418457031, Learning Rate: 9.765625e-06\n",
      "Epoch [12984/20000], Loss: 832.046875, Entropy 469.9442443847656, Learning Rate: 9.765625e-06\n",
      "Epoch [12985/20000], Loss: 861.1007080078125, Entropy 447.6468505859375, Learning Rate: 9.765625e-06\n",
      "Epoch [12986/20000], Loss: 847.8267822265625, Entropy 465.1243591308594, Learning Rate: 9.765625e-06\n",
      "Epoch [12987/20000], Loss: 812.81591796875, Entropy 467.3526306152344, Learning Rate: 9.765625e-06\n",
      "Epoch [12988/20000], Loss: 854.819091796875, Entropy 459.3008117675781, Learning Rate: 9.765625e-06\n",
      "Epoch [12989/20000], Loss: 891.6405029296875, Entropy 455.1568908691406, Learning Rate: 9.765625e-06\n",
      "Epoch [12990/20000], Loss: 880.0377807617188, Entropy 457.78936767578125, Learning Rate: 9.765625e-06\n",
      "Epoch [12991/20000], Loss: 875.025634765625, Entropy 475.4839782714844, Learning Rate: 9.765625e-06\n",
      "Epoch [12992/20000], Loss: 856.2352294921875, Entropy 457.7608642578125, Learning Rate: 9.765625e-06\n",
      "Epoch [12993/20000], Loss: 905.301513671875, Entropy 458.1892395019531, Learning Rate: 9.765625e-06\n",
      "Epoch [12994/20000], Loss: 857.79736328125, Entropy 453.0972595214844, Learning Rate: 9.765625e-06\n",
      "Epoch [12995/20000], Loss: 854.9146728515625, Entropy 466.6343994140625, Learning Rate: 9.765625e-06\n",
      "Epoch [12996/20000], Loss: 822.972412109375, Entropy 454.0487365722656, Learning Rate: 9.765625e-06\n",
      "Epoch [12997/20000], Loss: 846.774658203125, Entropy 450.1363220214844, Learning Rate: 9.765625e-06\n",
      "Epoch [12998/20000], Loss: 799.0685424804688, Entropy 456.97076416015625, Learning Rate: 9.765625e-06\n",
      "Epoch [12999/20000], Loss: 839.9195556640625, Entropy 460.9774169921875, Learning Rate: 9.765625e-06\n",
      "Epoch [13000/20000], Loss: 928.9846801757812, Entropy 467.62591552734375, Learning Rate: 9.765625e-06\n",
      "Epoch [13001/20000], Loss: 804.928466796875, Entropy 474.97607421875, Learning Rate: 9.765625e-06\n",
      "Epoch [13002/20000], Loss: 868.986572265625, Entropy 453.5306091308594, Learning Rate: 9.765625e-06\n",
      "Epoch [13003/20000], Loss: 891.5637817382812, Entropy 454.31060791015625, Learning Rate: 9.765625e-06\n",
      "Epoch [13004/20000], Loss: 808.7061767578125, Entropy 455.8548278808594, Learning Rate: 9.765625e-06\n",
      "Epoch [13005/20000], Loss: 855.7149658203125, Entropy 484.3619689941406, Learning Rate: 9.765625e-06\n",
      "Epoch [13006/20000], Loss: 874.021728515625, Entropy 474.7080993652344, Learning Rate: 9.765625e-06\n",
      "Epoch [13007/20000], Loss: 835.0003051757812, Entropy 461.65631103515625, Learning Rate: 9.765625e-06\n",
      "Epoch [13008/20000], Loss: 810.810791015625, Entropy 460.197021484375, Learning Rate: 9.765625e-06\n",
      "Epoch [13009/20000], Loss: 869.1475830078125, Entropy 450.97265625, Learning Rate: 9.765625e-06\n",
      "Epoch [13010/20000], Loss: 830.06103515625, Entropy 467.8564758300781, Learning Rate: 9.765625e-06\n",
      "Epoch [13011/20000], Loss: 815.5977783203125, Entropy 466.7507629394531, Learning Rate: 9.765625e-06\n",
      "Epoch [13012/20000], Loss: 821.37255859375, Entropy 462.302490234375, Learning Rate: 9.765625e-06\n",
      "Epoch [13013/20000], Loss: 900.958984375, Entropy 459.333740234375, Learning Rate: 9.765625e-06\n",
      "Epoch [13014/20000], Loss: 865.902587890625, Entropy 462.81591796875, Learning Rate: 9.765625e-06\n",
      "Epoch [13015/20000], Loss: 820.0189208984375, Entropy 455.4947204589844, Learning Rate: 9.765625e-06\n",
      "Epoch [13016/20000], Loss: 868.3944091796875, Entropy 441.93359375, Learning Rate: 9.765625e-06\n",
      "Epoch [13017/20000], Loss: 881.6268920898438, Entropy 447.98297119140625, Learning Rate: 9.765625e-06\n",
      "Epoch [13018/20000], Loss: 831.3847045898438, Entropy 443.86663818359375, Learning Rate: 9.765625e-06\n",
      "Epoch [13019/20000], Loss: 925.4129638671875, Entropy 441.3020324707031, Learning Rate: 9.765625e-06\n",
      "Epoch [13020/20000], Loss: 877.491943359375, Entropy 452.3102111816406, Learning Rate: 9.765625e-06\n",
      "Epoch [13021/20000], Loss: 897.1400146484375, Entropy 457.4051208496094, Learning Rate: 9.765625e-06\n",
      "Epoch [13022/20000], Loss: 929.600830078125, Entropy 434.8797302246094, Learning Rate: 9.765625e-06\n",
      "Epoch [13023/20000], Loss: 875.2833251953125, Entropy 473.9153137207031, Learning Rate: 9.765625e-06\n",
      "Epoch [13024/20000], Loss: 869.2074584960938, Entropy 449.02264404296875, Learning Rate: 9.765625e-06\n",
      "Epoch [13025/20000], Loss: 864.6205444335938, Entropy 443.08819580078125, Learning Rate: 9.765625e-06\n",
      "Epoch [13026/20000], Loss: 894.411376953125, Entropy 444.2710876464844, Learning Rate: 9.765625e-06\n",
      "Epoch [13027/20000], Loss: 850.1534423828125, Entropy 450.5778503417969, Learning Rate: 9.765625e-06\n",
      "Epoch [13028/20000], Loss: 842.732421875, Entropy 450.15234375, Learning Rate: 9.765625e-06\n",
      "Epoch [13029/20000], Loss: 852.987548828125, Entropy 433.8420715332031, Learning Rate: 9.765625e-06\n",
      "Epoch [13030/20000], Loss: 886.6904296875, Entropy 450.7603759765625, Learning Rate: 9.765625e-06\n",
      "Epoch [13031/20000], Loss: 869.753662109375, Entropy 470.3992004394531, Learning Rate: 9.765625e-06\n",
      "Epoch [13032/20000], Loss: 876.8861083984375, Entropy 442.8794860839844, Learning Rate: 9.765625e-06\n",
      "Epoch [13033/20000], Loss: 846.7979736328125, Entropy 463.65234375, Learning Rate: 9.765625e-06\n",
      "Epoch [13034/20000], Loss: 842.4686279296875, Entropy 445.1168212890625, Learning Rate: 9.765625e-06\n",
      "Epoch [13035/20000], Loss: 819.7060546875, Entropy 467.734375, Learning Rate: 9.765625e-06\n",
      "Epoch [13036/20000], Loss: 876.4005737304688, Entropy 468.59490966796875, Learning Rate: 9.765625e-06\n",
      "Epoch [13037/20000], Loss: 885.869140625, Entropy 439.8835754394531, Learning Rate: 9.765625e-06\n",
      "Epoch [13038/20000], Loss: 845.6760864257812, Entropy 460.84429931640625, Learning Rate: 9.765625e-06\n",
      "Epoch [13039/20000], Loss: 840.7542114257812, Entropy 448.48956298828125, Learning Rate: 9.765625e-06\n",
      "Epoch [13040/20000], Loss: 895.6674194335938, Entropy 460.27166748046875, Learning Rate: 9.765625e-06\n",
      "Epoch [13041/20000], Loss: 936.122802734375, Entropy 425.2083435058594, Learning Rate: 9.765625e-06\n",
      "Epoch [13042/20000], Loss: 814.9005737304688, Entropy 479.02435302734375, Learning Rate: 9.765625e-06\n",
      "Epoch [13043/20000], Loss: 825.4522705078125, Entropy 453.566162109375, Learning Rate: 9.765625e-06\n",
      "Epoch [13044/20000], Loss: 842.31640625, Entropy 455.0106201171875, Learning Rate: 9.765625e-06\n",
      "Epoch [13045/20000], Loss: 860.2813720703125, Entropy 461.47265625, Learning Rate: 9.765625e-06\n",
      "Epoch [13046/20000], Loss: 827.2658081054688, Entropy 467.17327880859375, Learning Rate: 9.765625e-06\n",
      "Epoch [13047/20000], Loss: 878.8582763671875, Entropy 459.9139709472656, Learning Rate: 9.765625e-06\n",
      "Epoch [13048/20000], Loss: 823.27587890625, Entropy 462.7355041503906, Learning Rate: 9.765625e-06\n",
      "Epoch [13049/20000], Loss: 806.4776611328125, Entropy 453.7184143066406, Learning Rate: 9.765625e-06\n",
      "Epoch [13050/20000], Loss: 825.8173828125, Entropy 464.5082702636719, Learning Rate: 9.765625e-06\n",
      "Epoch [13051/20000], Loss: 865.591552734375, Entropy 447.3489074707031, Learning Rate: 9.765625e-06\n",
      "Epoch [13052/20000], Loss: 848.91796875, Entropy 456.407958984375, Learning Rate: 9.765625e-06\n",
      "Epoch [13053/20000], Loss: 914.0938110351562, Entropy 438.96929931640625, Learning Rate: 9.765625e-06\n",
      "Epoch [13054/20000], Loss: 883.4449462890625, Entropy 445.0653076171875, Learning Rate: 9.765625e-06\n",
      "Epoch [13055/20000], Loss: 911.9718627929688, Entropy 465.18450927734375, Learning Rate: 9.765625e-06\n",
      "Epoch [13056/20000], Loss: 855.0430908203125, Entropy 468.5152282714844, Learning Rate: 9.765625e-06\n",
      "Epoch [13057/20000], Loss: 794.014892578125, Entropy 471.9080810546875, Learning Rate: 9.765625e-06\n",
      "Epoch [13058/20000], Loss: 878.149169921875, Entropy 454.4461364746094, Learning Rate: 9.765625e-06\n",
      "Epoch [13059/20000], Loss: 896.5550537109375, Entropy 455.9864501953125, Learning Rate: 9.765625e-06\n",
      "Epoch [13060/20000], Loss: 868.945068359375, Entropy 459.6271057128906, Learning Rate: 9.765625e-06\n",
      "Epoch [13061/20000], Loss: 868.5299682617188, Entropy 459.08123779296875, Learning Rate: 9.765625e-06\n",
      "Epoch [13062/20000], Loss: 826.9859619140625, Entropy 456.501953125, Learning Rate: 9.765625e-06\n",
      "Epoch [13063/20000], Loss: 929.892822265625, Entropy 457.912109375, Learning Rate: 9.765625e-06\n",
      "Epoch [13064/20000], Loss: 877.468017578125, Entropy 469.0199890136719, Learning Rate: 9.765625e-06\n",
      "Epoch [13065/20000], Loss: 872.8906860351562, Entropy 456.06304931640625, Learning Rate: 9.765625e-06\n",
      "Epoch [13066/20000], Loss: 880.639892578125, Entropy 448.9527282714844, Learning Rate: 9.765625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13067/20000], Loss: 855.608154296875, Entropy 450.0006408691406, Learning Rate: 9.765625e-06\n",
      "Epoch [13068/20000], Loss: 916.7298583984375, Entropy 456.2820129394531, Learning Rate: 9.765625e-06\n",
      "Epoch [13069/20000], Loss: 851.2425537109375, Entropy 461.3670959472656, Learning Rate: 9.765625e-06\n",
      "Epoch [13070/20000], Loss: 854.8734741210938, Entropy 456.53326416015625, Learning Rate: 9.765625e-06\n",
      "Epoch [13071/20000], Loss: 854.21142578125, Entropy 455.8931884765625, Learning Rate: 9.765625e-06\n",
      "Epoch [13072/20000], Loss: 859.9861450195312, Entropy 452.93353271484375, Learning Rate: 9.765625e-06\n",
      "Epoch [13073/20000], Loss: 837.353271484375, Entropy 476.2991943359375, Learning Rate: 9.765625e-06\n",
      "Epoch [13074/20000], Loss: 862.1727294921875, Entropy 445.48828125, Learning Rate: 9.765625e-06\n",
      "Epoch [13075/20000], Loss: 866.3966674804688, Entropy 458.93109130859375, Learning Rate: 9.765625e-06\n",
      "Epoch [13076/20000], Loss: 832.552001953125, Entropy 456.5381164550781, Learning Rate: 9.765625e-06\n",
      "Epoch [13077/20000], Loss: 821.7056884765625, Entropy 449.26611328125, Learning Rate: 9.765625e-06\n",
      "Epoch [13078/20000], Loss: 865.12255859375, Entropy 453.6002502441406, Learning Rate: 9.765625e-06\n",
      "Epoch [13079/20000], Loss: 850.8544921875, Entropy 452.6197509765625, Learning Rate: 9.765625e-06\n",
      "Epoch [13080/20000], Loss: 847.8345947265625, Entropy 449.8895263671875, Learning Rate: 9.765625e-06\n",
      "Epoch [13081/20000], Loss: 873.4492797851562, Entropy 468.21673583984375, Learning Rate: 9.765625e-06\n",
      "Epoch [13082/20000], Loss: 877.9295654296875, Entropy 448.2069396972656, Learning Rate: 9.765625e-06\n",
      "Epoch [13083/20000], Loss: 829.2679443359375, Entropy 464.7037048339844, Learning Rate: 9.765625e-06\n",
      "Epoch [13084/20000], Loss: 828.900634765625, Entropy 464.1495666503906, Learning Rate: 9.765625e-06\n",
      "Epoch [13085/20000], Loss: 891.8193359375, Entropy 449.4889221191406, Learning Rate: 9.765625e-06\n",
      "Epoch [13086/20000], Loss: 832.3736572265625, Entropy 453.4483947753906, Learning Rate: 9.765625e-06\n",
      "Epoch [13087/20000], Loss: 870.60595703125, Entropy 456.9722900390625, Learning Rate: 9.765625e-06\n",
      "Epoch [13088/20000], Loss: 838.4620361328125, Entropy 466.5565185546875, Learning Rate: 9.765625e-06\n",
      "Epoch [13089/20000], Loss: 852.1339111328125, Entropy 464.2970275878906, Learning Rate: 9.765625e-06\n",
      "Epoch [13090/20000], Loss: 833.874755859375, Entropy 461.9499816894531, Learning Rate: 9.765625e-06\n",
      "Epoch [13091/20000], Loss: 862.3277587890625, Entropy 450.8215637207031, Learning Rate: 9.765625e-06\n",
      "Epoch [13092/20000], Loss: 888.7791748046875, Entropy 439.8552551269531, Learning Rate: 9.765625e-06\n",
      "Epoch [13093/20000], Loss: 922.06689453125, Entropy 454.2726745605469, Learning Rate: 9.765625e-06\n",
      "Epoch [13094/20000], Loss: 880.243896484375, Entropy 448.3640441894531, Learning Rate: 9.765625e-06\n",
      "Epoch [13095/20000], Loss: 852.566650390625, Entropy 475.1076965332031, Learning Rate: 9.765625e-06\n",
      "Epoch [13096/20000], Loss: 878.0162353515625, Entropy 458.7237548828125, Learning Rate: 9.765625e-06\n",
      "Epoch [13097/20000], Loss: 893.1929321289062, Entropy 442.03277587890625, Learning Rate: 9.765625e-06\n",
      "Epoch [13098/20000], Loss: 853.0921630859375, Entropy 459.5027770996094, Learning Rate: 9.765625e-06\n",
      "Epoch [13099/20000], Loss: 860.5023803710938, Entropy 457.05413818359375, Learning Rate: 9.765625e-06\n",
      "Epoch [13100/20000], Loss: 911.0637817382812, Entropy 445.78656005859375, Learning Rate: 9.765625e-06\n",
      "Epoch [13101/20000], Loss: 883.36181640625, Entropy 448.6356201171875, Learning Rate: 9.765625e-06\n",
      "Epoch [13102/20000], Loss: 848.91357421875, Entropy 463.7242431640625, Learning Rate: 9.765625e-06\n",
      "Epoch [13103/20000], Loss: 846.531982421875, Entropy 457.7927551269531, Learning Rate: 9.765625e-06\n",
      "Epoch [13104/20000], Loss: 814.66943359375, Entropy 461.3955383300781, Learning Rate: 9.765625e-06\n",
      "Epoch [13105/20000], Loss: 885.93994140625, Entropy 456.1710205078125, Learning Rate: 9.765625e-06\n",
      "Epoch [13106/20000], Loss: 862.8445434570312, Entropy 449.79498291015625, Learning Rate: 9.765625e-06\n",
      "Epoch [13107/20000], Loss: 870.9234619140625, Entropy 463.16552734375, Learning Rate: 9.765625e-06\n",
      "Epoch [13108/20000], Loss: 871.6461181640625, Entropy 463.8332824707031, Learning Rate: 9.765625e-06\n",
      "Epoch [13109/20000], Loss: 858.4757080078125, Entropy 460.95703125, Learning Rate: 9.765625e-06\n",
      "Epoch [13110/20000], Loss: 849.7220458984375, Entropy 448.9739990234375, Learning Rate: 9.765625e-06\n",
      "Epoch [13111/20000], Loss: 853.6818237304688, Entropy 450.58880615234375, Learning Rate: 9.765625e-06\n",
      "Epoch [13112/20000], Loss: 852.0654296875, Entropy 442.9418640136719, Learning Rate: 9.765625e-06\n",
      "Epoch [13113/20000], Loss: 853.4803466796875, Entropy 460.3345947265625, Learning Rate: 9.765625e-06\n",
      "Epoch [13114/20000], Loss: 830.971923828125, Entropy 459.40966796875, Learning Rate: 9.765625e-06\n",
      "Epoch [13115/20000], Loss: 910.2840576171875, Entropy 455.8426818847656, Learning Rate: 9.765625e-06\n",
      "Epoch [13116/20000], Loss: 785.62939453125, Entropy 459.2986755371094, Learning Rate: 9.765625e-06\n",
      "Epoch [13117/20000], Loss: 815.5853271484375, Entropy 461.501953125, Learning Rate: 9.765625e-06\n",
      "Epoch [13118/20000], Loss: 886.1954345703125, Entropy 455.5135192871094, Learning Rate: 9.765625e-06\n",
      "Epoch [13119/20000], Loss: 866.3145141601562, Entropy 462.64056396484375, Learning Rate: 9.765625e-06\n",
      "Epoch [13120/20000], Loss: 886.9046020507812, Entropy 455.32550048828125, Learning Rate: 9.765625e-06\n",
      "Epoch [13121/20000], Loss: 874.0240478515625, Entropy 449.2367858886719, Learning Rate: 9.765625e-06\n",
      "Epoch [13122/20000], Loss: 826.7269287109375, Entropy 462.8302917480469, Learning Rate: 9.765625e-06\n",
      "Epoch [13123/20000], Loss: 895.9151611328125, Entropy 448.3907775878906, Learning Rate: 9.765625e-06\n",
      "Epoch [13124/20000], Loss: 837.407470703125, Entropy 464.6606140136719, Learning Rate: 9.765625e-06\n",
      "Epoch [13125/20000], Loss: 859.7052001953125, Entropy 456.5650634765625, Learning Rate: 9.765625e-06\n",
      "Epoch [13126/20000], Loss: 842.7633056640625, Entropy 451.8097229003906, Learning Rate: 9.765625e-06\n",
      "Epoch [13127/20000], Loss: 845.0233154296875, Entropy 478.81787109375, Learning Rate: 9.765625e-06\n",
      "Epoch [13128/20000], Loss: 871.0159912109375, Entropy 443.0392761230469, Learning Rate: 9.765625e-06\n",
      "Epoch [13129/20000], Loss: 831.843017578125, Entropy 448.4032287597656, Learning Rate: 9.765625e-06\n",
      "Epoch [13130/20000], Loss: 899.1007690429688, Entropy 449.36871337890625, Learning Rate: 9.765625e-06\n",
      "Epoch [13131/20000], Loss: 833.0296630859375, Entropy 452.4173278808594, Learning Rate: 9.765625e-06\n",
      "Epoch [13132/20000], Loss: 861.8364868164062, Entropy 459.10101318359375, Learning Rate: 9.765625e-06\n",
      "Epoch [13133/20000], Loss: 878.4716796875, Entropy 464.6257629394531, Learning Rate: 9.765625e-06\n",
      "Epoch [13134/20000], Loss: 891.18310546875, Entropy 455.5129699707031, Learning Rate: 9.765625e-06\n",
      "Epoch [13135/20000], Loss: 832.430419921875, Entropy 459.3813171386719, Learning Rate: 9.765625e-06\n",
      "Epoch [13136/20000], Loss: 865.5424194335938, Entropy 444.38238525390625, Learning Rate: 9.765625e-06\n",
      "Epoch [13137/20000], Loss: 896.9415893554688, Entropy 435.44390869140625, Learning Rate: 9.765625e-06\n",
      "Epoch [13138/20000], Loss: 867.6185302734375, Entropy 468.2282409667969, Learning Rate: 9.765625e-06\n",
      "Epoch [13139/20000], Loss: 839.28271484375, Entropy 471.6405029296875, Learning Rate: 9.765625e-06\n",
      "Epoch [13140/20000], Loss: 891.4281005859375, Entropy 446.4408874511719, Learning Rate: 9.765625e-06\n",
      "Epoch [13141/20000], Loss: 867.3135986328125, Entropy 445.7678527832031, Learning Rate: 9.765625e-06\n",
      "Epoch [13142/20000], Loss: 902.402099609375, Entropy 460.7479553222656, Learning Rate: 9.765625e-06\n",
      "Epoch [13143/20000], Loss: 873.0477294921875, Entropy 462.5143737792969, Learning Rate: 9.765625e-06\n",
      "Epoch [13144/20000], Loss: 861.6644287109375, Entropy 456.8609924316406, Learning Rate: 9.765625e-06\n",
      "Epoch [13145/20000], Loss: 869.2686767578125, Entropy 430.0400695800781, Learning Rate: 9.765625e-06\n",
      "Epoch [13146/20000], Loss: 911.6078491210938, Entropy 446.39898681640625, Learning Rate: 9.765625e-06\n",
      "Epoch [13147/20000], Loss: 789.663330078125, Entropy 479.0818786621094, Learning Rate: 9.765625e-06\n",
      "Epoch [13148/20000], Loss: 880.4765625, Entropy 458.8383483886719, Learning Rate: 9.765625e-06\n",
      "Epoch [13149/20000], Loss: 869.461669921875, Entropy 461.56201171875, Learning Rate: 9.765625e-06\n",
      "Epoch [13150/20000], Loss: 842.432373046875, Entropy 465.3675537109375, Learning Rate: 9.765625e-06\n",
      "Epoch [13151/20000], Loss: 844.4883422851562, Entropy 472.91143798828125, Learning Rate: 9.765625e-06\n",
      "Epoch [13152/20000], Loss: 865.064697265625, Entropy 443.419677734375, Learning Rate: 9.765625e-06\n",
      "Epoch [13153/20000], Loss: 902.313232421875, Entropy 456.5975646972656, Learning Rate: 9.765625e-06\n",
      "Epoch [13154/20000], Loss: 860.9329833984375, Entropy 466.4688415527344, Learning Rate: 9.765625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13155/20000], Loss: 901.3843994140625, Entropy 453.6871643066406, Learning Rate: 9.765625e-06\n",
      "Epoch [13156/20000], Loss: 826.2149658203125, Entropy 460.9067077636719, Learning Rate: 9.765625e-06\n",
      "Epoch [13157/20000], Loss: 858.152099609375, Entropy 460.1242980957031, Learning Rate: 9.765625e-06\n",
      "Epoch [13158/20000], Loss: 862.76904296875, Entropy 449.2649230957031, Learning Rate: 9.765625e-06\n",
      "Epoch [13159/20000], Loss: 915.003662109375, Entropy 441.85302734375, Learning Rate: 9.765625e-06\n",
      "Epoch [13160/20000], Loss: 911.4473876953125, Entropy 455.5914611816406, Learning Rate: 9.765625e-06\n",
      "Epoch [13161/20000], Loss: 861.7138671875, Entropy 453.8192443847656, Learning Rate: 9.765625e-06\n",
      "Epoch [13162/20000], Loss: 863.0225830078125, Entropy 448.3692932128906, Learning Rate: 9.765625e-06\n",
      "Epoch [13163/20000], Loss: 805.0323486328125, Entropy 466.4807434082031, Learning Rate: 9.765625e-06\n",
      "Epoch [13164/20000], Loss: 892.0078125, Entropy 463.5836486816406, Learning Rate: 9.765625e-06\n",
      "Epoch [13165/20000], Loss: 855.41015625, Entropy 454.8875732421875, Learning Rate: 9.765625e-06\n",
      "Epoch [13166/20000], Loss: 789.6661376953125, Entropy 459.9415588378906, Learning Rate: 9.765625e-06\n",
      "Epoch [13167/20000], Loss: 848.195556640625, Entropy 471.6103820800781, Learning Rate: 9.765625e-06\n",
      "Epoch [13168/20000], Loss: 850.8665771484375, Entropy 457.3329162597656, Learning Rate: 9.765625e-06\n",
      "Epoch [13169/20000], Loss: 853.727783203125, Entropy 465.9616394042969, Learning Rate: 9.765625e-06\n",
      "Epoch [13170/20000], Loss: 838.8212890625, Entropy 440.2127990722656, Learning Rate: 9.765625e-06\n",
      "Epoch [13171/20000], Loss: 860.9617919921875, Entropy 452.9093933105469, Learning Rate: 9.765625e-06\n",
      "Epoch [13172/20000], Loss: 826.0421142578125, Entropy 451.9559326171875, Learning Rate: 9.765625e-06\n",
      "Epoch [13173/20000], Loss: 847.5860595703125, Entropy 463.6148986816406, Learning Rate: 9.765625e-06\n",
      "Epoch [13174/20000], Loss: 915.5362548828125, Entropy 461.9002380371094, Learning Rate: 9.765625e-06\n",
      "Epoch [13175/20000], Loss: 866.0824584960938, Entropy 478.77386474609375, Learning Rate: 9.765625e-06\n",
      "Epoch [13176/20000], Loss: 858.5157470703125, Entropy 452.5697937011719, Learning Rate: 9.765625e-06\n",
      "Epoch [13177/20000], Loss: 871.6280517578125, Entropy 462.5534973144531, Learning Rate: 9.765625e-06\n",
      "Epoch [13178/20000], Loss: 843.77490234375, Entropy 470.5846862792969, Learning Rate: 9.765625e-06\n",
      "Epoch [13179/20000], Loss: 817.8807373046875, Entropy 463.4593505859375, Learning Rate: 9.765625e-06\n",
      "Epoch [13180/20000], Loss: 964.88623046875, Entropy 452.4537353515625, Learning Rate: 9.765625e-06\n",
      "Epoch [13181/20000], Loss: 882.6827392578125, Entropy 468.44189453125, Learning Rate: 9.765625e-06\n",
      "Epoch [13182/20000], Loss: 841.6881103515625, Entropy 443.0323791503906, Learning Rate: 9.765625e-06\n",
      "Epoch [13183/20000], Loss: 861.4542236328125, Entropy 443.4302978515625, Learning Rate: 9.765625e-06\n",
      "Epoch [13184/20000], Loss: 857.0731811523438, Entropy 462.02679443359375, Learning Rate: 9.765625e-06\n",
      "Epoch [13185/20000], Loss: 785.8864135742188, Entropy 451.53900146484375, Learning Rate: 9.765625e-06\n",
      "Epoch [13186/20000], Loss: 850.1317138671875, Entropy 451.002685546875, Learning Rate: 9.765625e-06\n",
      "Epoch [13187/20000], Loss: 944.6107177734375, Entropy 450.4809265136719, Learning Rate: 9.765625e-06\n",
      "Epoch [13188/20000], Loss: 845.51318359375, Entropy 464.362060546875, Learning Rate: 9.765625e-06\n",
      "Epoch [13189/20000], Loss: 909.0709228515625, Entropy 452.1960754394531, Learning Rate: 9.765625e-06\n",
      "Epoch [13190/20000], Loss: 843.2987060546875, Entropy 448.1900634765625, Learning Rate: 9.765625e-06\n",
      "Epoch [13191/20000], Loss: 883.0728149414062, Entropy 452.43743896484375, Learning Rate: 9.765625e-06\n",
      "Epoch [13192/20000], Loss: 837.3114013671875, Entropy 469.8834228515625, Learning Rate: 9.765625e-06\n",
      "Epoch [13193/20000], Loss: 841.9849243164062, Entropy 458.72320556640625, Learning Rate: 9.765625e-06\n",
      "Epoch [13194/20000], Loss: 875.5914306640625, Entropy 450.2184143066406, Learning Rate: 9.765625e-06\n",
      "Epoch [13195/20000], Loss: 931.92822265625, Entropy 456.108154296875, Learning Rate: 9.765625e-06\n",
      "Epoch [13196/20000], Loss: 862.8410034179688, Entropy 457.45355224609375, Learning Rate: 9.765625e-06\n",
      "Epoch [13197/20000], Loss: 833.4887084960938, Entropy 474.98858642578125, Learning Rate: 9.765625e-06\n",
      "Epoch [13198/20000], Loss: 826.7393798828125, Entropy 451.7897644042969, Learning Rate: 9.765625e-06\n",
      "Epoch [13199/20000], Loss: 836.3748779296875, Entropy 456.3995361328125, Learning Rate: 9.765625e-06\n",
      "Epoch [13200/20000], Loss: 899.744384765625, Entropy 446.9849853515625, Learning Rate: 9.765625e-06\n",
      "Epoch [13201/20000], Loss: 856.1611328125, Entropy 456.2819519042969, Learning Rate: 9.765625e-06\n",
      "Epoch [13202/20000], Loss: 841.114013671875, Entropy 458.0910339355469, Learning Rate: 9.765625e-06\n",
      "Epoch [13203/20000], Loss: 877.6380004882812, Entropy 462.27557373046875, Learning Rate: 9.765625e-06\n",
      "Epoch [13204/20000], Loss: 846.0403442382812, Entropy 472.28631591796875, Learning Rate: 9.765625e-06\n",
      "Epoch [13205/20000], Loss: 858.46240234375, Entropy 446.3249206542969, Learning Rate: 9.765625e-06\n",
      "Epoch [13206/20000], Loss: 877.552978515625, Entropy 465.9894714355469, Learning Rate: 9.765625e-06\n",
      "Epoch [13207/20000], Loss: 832.2084350585938, Entropy 470.66400146484375, Learning Rate: 9.765625e-06\n",
      "Epoch [13208/20000], Loss: 842.1228637695312, Entropy 439.96832275390625, Learning Rate: 9.765625e-06\n",
      "Epoch [13209/20000], Loss: 833.3348388671875, Entropy 452.2556457519531, Learning Rate: 9.765625e-06\n",
      "Epoch [13210/20000], Loss: 840.6583251953125, Entropy 453.935546875, Learning Rate: 9.765625e-06\n",
      "Epoch [13211/20000], Loss: 790.4378662109375, Entropy 463.8353271484375, Learning Rate: 9.765625e-06\n",
      "Epoch [13212/20000], Loss: 896.3419799804688, Entropy 463.26702880859375, Learning Rate: 9.765625e-06\n",
      "Epoch [13213/20000], Loss: 844.856689453125, Entropy 459.1187438964844, Learning Rate: 9.765625e-06\n",
      "Epoch [13214/20000], Loss: 904.2119140625, Entropy 451.3893127441406, Learning Rate: 9.765625e-06\n",
      "Epoch [13215/20000], Loss: 840.1935424804688, Entropy 458.19525146484375, Learning Rate: 9.765625e-06\n",
      "Epoch [13216/20000], Loss: 847.4033203125, Entropy 452.7179260253906, Learning Rate: 9.765625e-06\n",
      "Epoch [13217/20000], Loss: 820.037841796875, Entropy 470.71875, Learning Rate: 9.765625e-06\n",
      "Epoch [13218/20000], Loss: 867.3795776367188, Entropy 449.47308349609375, Learning Rate: 9.765625e-06\n",
      "Epoch [13219/20000], Loss: 873.2943115234375, Entropy 442.7013244628906, Learning Rate: 9.765625e-06\n",
      "Epoch [13220/20000], Loss: 868.043701171875, Entropy 459.1734924316406, Learning Rate: 9.765625e-06\n",
      "Epoch [13221/20000], Loss: 839.745849609375, Entropy 471.39111328125, Learning Rate: 9.765625e-06\n",
      "Epoch [13222/20000], Loss: 867.3919677734375, Entropy 456.2567138671875, Learning Rate: 9.765625e-06\n",
      "Epoch [13223/20000], Loss: 822.6781005859375, Entropy 463.0872497558594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13224/20000], Loss: 880.1115112304688, Entropy 458.09771728515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13225/20000], Loss: 823.083984375, Entropy 454.5040283203125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13226/20000], Loss: 868.481689453125, Entropy 465.4669494628906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13227/20000], Loss: 854.3887939453125, Entropy 446.1513977050781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13228/20000], Loss: 840.67822265625, Entropy 451.7442626953125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13229/20000], Loss: 879.8137817382812, Entropy 462.87835693359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13230/20000], Loss: 839.1912841796875, Entropy 471.0318603515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13231/20000], Loss: 844.3143310546875, Entropy 459.8648986816406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13232/20000], Loss: 851.1973876953125, Entropy 462.6890869140625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13233/20000], Loss: 855.4049682617188, Entropy 445.71636962890625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13234/20000], Loss: 886.43408203125, Entropy 453.956787109375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13235/20000], Loss: 847.4755859375, Entropy 458.0577392578125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13236/20000], Loss: 866.4140625, Entropy 461.7261962890625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13237/20000], Loss: 908.288330078125, Entropy 452.1259765625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13238/20000], Loss: 835.1051635742188, Entropy 456.62420654296875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13239/20000], Loss: 903.3695068359375, Entropy 447.4353942871094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13240/20000], Loss: 857.36572265625, Entropy 452.41748046875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13241/20000], Loss: 843.8267211914062, Entropy 464.27288818359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13242/20000], Loss: 893.057373046875, Entropy 461.6133117675781, Learning Rate: 4.8828125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13243/20000], Loss: 874.953857421875, Entropy 442.7391052246094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13244/20000], Loss: 898.9376831054688, Entropy 461.74945068359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13245/20000], Loss: 890.994140625, Entropy 444.8150329589844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13246/20000], Loss: 887.0965576171875, Entropy 449.1578674316406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13247/20000], Loss: 859.8319091796875, Entropy 448.7431945800781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13248/20000], Loss: 862.3777465820312, Entropy 448.03411865234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13249/20000], Loss: 866.4598999023438, Entropy 450.13287353515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13250/20000], Loss: 846.1639404296875, Entropy 441.4776611328125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13251/20000], Loss: 847.0081787109375, Entropy 461.9217224121094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13252/20000], Loss: 895.9212646484375, Entropy 469.5410461425781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13253/20000], Loss: 850.55126953125, Entropy 442.8909606933594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13254/20000], Loss: 872.97802734375, Entropy 455.7853698730469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13255/20000], Loss: 837.387451171875, Entropy 468.9142150878906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13256/20000], Loss: 876.5062255859375, Entropy 450.9324951171875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13257/20000], Loss: 858.0267333984375, Entropy 444.6291809082031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13258/20000], Loss: 817.63037109375, Entropy 473.2744140625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13259/20000], Loss: 862.375244140625, Entropy 444.69921875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13260/20000], Loss: 812.1239013671875, Entropy 470.3973388671875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13261/20000], Loss: 841.1939697265625, Entropy 475.0422668457031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13262/20000], Loss: 848.4954833984375, Entropy 451.50732421875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13263/20000], Loss: 884.47705078125, Entropy 469.185546875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13264/20000], Loss: 881.7320556640625, Entropy 452.9927978515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13265/20000], Loss: 860.8021240234375, Entropy 444.9320068359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13266/20000], Loss: 904.1005859375, Entropy 458.1594543457031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13267/20000], Loss: 868.6683349609375, Entropy 453.595703125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13268/20000], Loss: 813.7201538085938, Entropy 470.73065185546875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13269/20000], Loss: 871.46435546875, Entropy 464.4381103515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13270/20000], Loss: 884.0321044921875, Entropy 448.8888854980469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13271/20000], Loss: 835.7477416992188, Entropy 449.24896240234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13272/20000], Loss: 869.454833984375, Entropy 457.30419921875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13273/20000], Loss: 870.60009765625, Entropy 449.054443359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13274/20000], Loss: 872.052978515625, Entropy 446.796630859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13275/20000], Loss: 857.398193359375, Entropy 452.091796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13276/20000], Loss: 837.9456787109375, Entropy 448.6907653808594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13277/20000], Loss: 802.8447265625, Entropy 471.5471496582031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13278/20000], Loss: 805.6946411132812, Entropy 463.52349853515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13279/20000], Loss: 874.031005859375, Entropy 457.4463195800781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13280/20000], Loss: 836.48193359375, Entropy 460.3129577636719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13281/20000], Loss: 882.551025390625, Entropy 461.5255432128906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13282/20000], Loss: 867.4102783203125, Entropy 444.39599609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13283/20000], Loss: 852.025390625, Entropy 452.5364990234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13284/20000], Loss: 845.809814453125, Entropy 452.5970153808594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13285/20000], Loss: 870.1556396484375, Entropy 454.1468505859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13286/20000], Loss: 856.279052734375, Entropy 453.8713073730469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13287/20000], Loss: 852.1314697265625, Entropy 451.9573974609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13288/20000], Loss: 870.4798583984375, Entropy 463.6457824707031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13289/20000], Loss: 885.8221435546875, Entropy 453.2464599609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13290/20000], Loss: 862.6714477539062, Entropy 466.48284912109375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13291/20000], Loss: 819.6997680664062, Entropy 456.95538330078125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13292/20000], Loss: 838.7684326171875, Entropy 468.6435546875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13293/20000], Loss: 877.1251220703125, Entropy 463.8363037109375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13294/20000], Loss: 868.3377685546875, Entropy 449.2693786621094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13295/20000], Loss: 836.5443725585938, Entropy 459.24517822265625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13296/20000], Loss: 850.11767578125, Entropy 457.6705017089844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13297/20000], Loss: 823.935791015625, Entropy 467.5140380859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13298/20000], Loss: 866.14111328125, Entropy 456.0360107421875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13299/20000], Loss: 870.515625, Entropy 455.6941833496094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13300/20000], Loss: 865.205810546875, Entropy 471.8406677246094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13301/20000], Loss: 796.8935546875, Entropy 450.7105712890625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13302/20000], Loss: 870.1982421875, Entropy 449.8163146972656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13303/20000], Loss: 823.3801879882812, Entropy 453.29425048828125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13304/20000], Loss: 806.78515625, Entropy 473.6540222167969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13305/20000], Loss: 858.1143798828125, Entropy 459.2684631347656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13306/20000], Loss: 883.846435546875, Entropy 458.134765625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13307/20000], Loss: 847.0558471679688, Entropy 459.08709716796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13308/20000], Loss: 808.5865478515625, Entropy 460.0225830078125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13309/20000], Loss: 854.3525390625, Entropy 466.5409240722656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13310/20000], Loss: 898.390625, Entropy 433.3963623046875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13311/20000], Loss: 855.58740234375, Entropy 461.697998046875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13312/20000], Loss: 840.5489501953125, Entropy 464.0788879394531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13313/20000], Loss: 879.1000366210938, Entropy 467.89288330078125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13314/20000], Loss: 846.4750366210938, Entropy 463.89007568359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13315/20000], Loss: 838.66845703125, Entropy 455.9172058105469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13316/20000], Loss: 869.6798706054688, Entropy 452.66998291015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13317/20000], Loss: 909.8408203125, Entropy 456.962890625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13318/20000], Loss: 817.5819091796875, Entropy 455.564208984375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13319/20000], Loss: 875.4793701171875, Entropy 441.8321838378906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13320/20000], Loss: 840.8604736328125, Entropy 474.3538818359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13321/20000], Loss: 901.1588745117188, Entropy 444.78607177734375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13322/20000], Loss: 833.3770751953125, Entropy 457.7340087890625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13323/20000], Loss: 831.4442138671875, Entropy 463.6899108886719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13324/20000], Loss: 824.2115478515625, Entropy 450.8539733886719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13325/20000], Loss: 842.592529296875, Entropy 455.2506103515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13326/20000], Loss: 833.363525390625, Entropy 462.6777648925781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13327/20000], Loss: 834.5363159179688, Entropy 470.22161865234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13328/20000], Loss: 876.4017333984375, Entropy 456.8064880371094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13329/20000], Loss: 836.489013671875, Entropy 463.25537109375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13330/20000], Loss: 870.7735595703125, Entropy 441.2690124511719, Learning Rate: 4.8828125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13331/20000], Loss: 896.27294921875, Entropy 454.9343566894531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13332/20000], Loss: 821.5753173828125, Entropy 459.8961486816406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13333/20000], Loss: 878.4737548828125, Entropy 450.0412292480469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13334/20000], Loss: 900.7918701171875, Entropy 441.7009582519531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13335/20000], Loss: 902.6723022460938, Entropy 448.79913330078125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13336/20000], Loss: 904.4561767578125, Entropy 449.1028747558594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13337/20000], Loss: 815.6115112304688, Entropy 467.24273681640625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13338/20000], Loss: 870.6210327148438, Entropy 461.38787841796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13339/20000], Loss: 812.3259887695312, Entropy 471.74432373046875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13340/20000], Loss: 779.8626708984375, Entropy 463.1301574707031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13341/20000], Loss: 841.014892578125, Entropy 456.706787109375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13342/20000], Loss: 856.8333740234375, Entropy 467.6617431640625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13343/20000], Loss: 842.9348754882812, Entropy 447.54193115234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13344/20000], Loss: 898.0679931640625, Entropy 453.462890625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13345/20000], Loss: 872.3794555664062, Entropy 443.17132568359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13346/20000], Loss: 851.3140258789062, Entropy 429.02935791015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13347/20000], Loss: 887.2877197265625, Entropy 453.376953125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13348/20000], Loss: 844.9595336914062, Entropy 461.00701904296875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13349/20000], Loss: 857.0982666015625, Entropy 450.8537292480469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13350/20000], Loss: 854.7164306640625, Entropy 455.624755859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13351/20000], Loss: 842.79443359375, Entropy 462.8674621582031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13352/20000], Loss: 856.238037109375, Entropy 468.300537109375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13353/20000], Loss: 838.8175048828125, Entropy 457.8908996582031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13354/20000], Loss: 796.2179565429688, Entropy 455.69061279296875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13355/20000], Loss: 902.7781372070312, Entropy 449.44293212890625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13356/20000], Loss: 837.6322631835938, Entropy 462.07366943359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13357/20000], Loss: 830.1476440429688, Entropy 459.62750244140625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13358/20000], Loss: 866.6878662109375, Entropy 441.7527160644531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13359/20000], Loss: 841.4142456054688, Entropy 473.17303466796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13360/20000], Loss: 867.5838623046875, Entropy 445.5385437011719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13361/20000], Loss: 895.315673828125, Entropy 456.308349609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13362/20000], Loss: 825.818603515625, Entropy 463.3835754394531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13363/20000], Loss: 845.16552734375, Entropy 455.0247497558594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13364/20000], Loss: 825.91162109375, Entropy 466.1383361816406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13365/20000], Loss: 815.18310546875, Entropy 462.0513610839844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13366/20000], Loss: 881.406982421875, Entropy 463.6779479980469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13367/20000], Loss: 835.9876098632812, Entropy 476.01751708984375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13368/20000], Loss: 905.6055908203125, Entropy 465.4609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13369/20000], Loss: 815.7686157226562, Entropy 463.53411865234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13370/20000], Loss: 879.7835693359375, Entropy 461.6028137207031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13371/20000], Loss: 837.98876953125, Entropy 477.1044006347656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13372/20000], Loss: 848.2312622070312, Entropy 468.22650146484375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13373/20000], Loss: 864.385498046875, Entropy 451.5671081542969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13374/20000], Loss: 830.3768310546875, Entropy 463.4206848144531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13375/20000], Loss: 816.2597045898438, Entropy 477.32147216796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13376/20000], Loss: 966.580810546875, Entropy 448.6207580566406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13377/20000], Loss: 887.3162841796875, Entropy 467.8599853515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13378/20000], Loss: 803.9254150390625, Entropy 453.1920166015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13379/20000], Loss: 894.567626953125, Entropy 448.3214111328125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13380/20000], Loss: 822.6392822265625, Entropy 458.9834289550781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13381/20000], Loss: 861.1650390625, Entropy 449.870849609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13382/20000], Loss: 866.2854614257812, Entropy 458.35211181640625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13383/20000], Loss: 839.20556640625, Entropy 463.9652099609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13384/20000], Loss: 881.4906616210938, Entropy 457.52081298828125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13385/20000], Loss: 879.4208374023438, Entropy 468.69476318359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13386/20000], Loss: 900.930419921875, Entropy 464.2911682128906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13387/20000], Loss: 846.0015869140625, Entropy 458.1075439453125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13388/20000], Loss: 865.411376953125, Entropy 458.2830810546875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13389/20000], Loss: 868.113525390625, Entropy 460.1360778808594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13390/20000], Loss: 865.18701171875, Entropy 445.5870361328125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13391/20000], Loss: 870.5693969726562, Entropy 458.54107666015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13392/20000], Loss: 900.922119140625, Entropy 461.9217224121094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13393/20000], Loss: 835.556396484375, Entropy 456.2358093261719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13394/20000], Loss: 861.183349609375, Entropy 449.0682067871094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13395/20000], Loss: 895.9417724609375, Entropy 460.5181884765625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13396/20000], Loss: 820.9632568359375, Entropy 470.8553161621094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13397/20000], Loss: 877.6654663085938, Entropy 460.71478271484375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13398/20000], Loss: 916.437744140625, Entropy 445.1897277832031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13399/20000], Loss: 874.8134765625, Entropy 465.481201171875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13400/20000], Loss: 884.0224609375, Entropy 448.9931945800781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13401/20000], Loss: 875.3026123046875, Entropy 465.2933349609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13402/20000], Loss: 863.2210083007812, Entropy 452.15740966796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13403/20000], Loss: 886.5135498046875, Entropy 452.2445068359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13404/20000], Loss: 805.5288696289062, Entropy 480.65582275390625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13405/20000], Loss: 852.373046875, Entropy 468.79443359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13406/20000], Loss: 847.7728271484375, Entropy 447.9638977050781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13407/20000], Loss: 882.275390625, Entropy 443.9485778808594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13408/20000], Loss: 878.0473022460938, Entropy 443.32550048828125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13409/20000], Loss: 808.031982421875, Entropy 466.58349609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13410/20000], Loss: 808.365234375, Entropy 472.2917785644531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13411/20000], Loss: 846.2742919921875, Entropy 448.494140625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13412/20000], Loss: 874.6575927734375, Entropy 453.417724609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13413/20000], Loss: 841.4635620117188, Entropy 488.07330322265625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13414/20000], Loss: 839.444091796875, Entropy 468.6919860839844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13415/20000], Loss: 799.9514770507812, Entropy 454.32464599609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13416/20000], Loss: 856.12744140625, Entropy 456.9366760253906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13417/20000], Loss: 836.0355224609375, Entropy 446.026123046875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13418/20000], Loss: 877.162353515625, Entropy 443.5939636230469, Learning Rate: 4.8828125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13419/20000], Loss: 816.690673828125, Entropy 455.8180236816406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13420/20000], Loss: 883.3463134765625, Entropy 456.5260925292969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13421/20000], Loss: 887.205322265625, Entropy 456.0827331542969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13422/20000], Loss: 835.8841552734375, Entropy 466.5533447265625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13423/20000], Loss: 783.9886474609375, Entropy 457.4502258300781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13424/20000], Loss: 884.6657104492188, Entropy 456.75445556640625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13425/20000], Loss: 850.494140625, Entropy 451.7677917480469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13426/20000], Loss: 860.9406127929688, Entropy 464.78265380859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13427/20000], Loss: 838.2098999023438, Entropy 461.49627685546875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13428/20000], Loss: 810.0667114257812, Entropy 476.01190185546875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13429/20000], Loss: 843.0162353515625, Entropy 464.9969177246094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13430/20000], Loss: 889.4298095703125, Entropy 451.1709289550781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13431/20000], Loss: 840.5853271484375, Entropy 468.8680725097656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13432/20000], Loss: 849.5887451171875, Entropy 461.6714782714844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13433/20000], Loss: 842.4898681640625, Entropy 458.9952392578125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13434/20000], Loss: 854.9654541015625, Entropy 459.7417907714844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13435/20000], Loss: 821.9425048828125, Entropy 459.5295715332031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13436/20000], Loss: 892.8426513671875, Entropy 457.27294921875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13437/20000], Loss: 874.1737060546875, Entropy 455.6751708984375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13438/20000], Loss: 802.0009765625, Entropy 449.1531677246094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13439/20000], Loss: 816.787841796875, Entropy 456.9520568847656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13440/20000], Loss: 869.647705078125, Entropy 445.3624572753906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13441/20000], Loss: 892.2890014648438, Entropy 434.45733642578125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13442/20000], Loss: 845.2923583984375, Entropy 460.2331237792969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13443/20000], Loss: 864.8604736328125, Entropy 463.8179626464844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13444/20000], Loss: 850.84521484375, Entropy 450.1934814453125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13445/20000], Loss: 830.3876342773438, Entropy 466.30804443359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13446/20000], Loss: 878.245849609375, Entropy 462.5306091308594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13447/20000], Loss: 833.2634887695312, Entropy 453.41363525390625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13448/20000], Loss: 829.6069946289062, Entropy 444.37835693359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13449/20000], Loss: 867.6109619140625, Entropy 467.06005859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13450/20000], Loss: 837.0831298828125, Entropy 472.91796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13451/20000], Loss: 871.1708984375, Entropy 462.6265563964844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13452/20000], Loss: 893.9760131835938, Entropy 454.73956298828125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13453/20000], Loss: 857.2583618164062, Entropy 435.66522216796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13454/20000], Loss: 870.0958251953125, Entropy 457.1344299316406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13455/20000], Loss: 854.2857666015625, Entropy 461.0425720214844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13456/20000], Loss: 852.845703125, Entropy 473.4893493652344, Learning Rate: 4.8828125e-06\n",
      "Epoch [13457/20000], Loss: 844.68701171875, Entropy 475.09326171875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13458/20000], Loss: 810.1749267578125, Entropy 462.5306091308594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13459/20000], Loss: 824.254638671875, Entropy 469.3610534667969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13460/20000], Loss: 880.305419921875, Entropy 448.78515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13461/20000], Loss: 892.3583984375, Entropy 453.4478759765625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13462/20000], Loss: 863.6773681640625, Entropy 456.8214111328125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13463/20000], Loss: 890.924072265625, Entropy 438.30126953125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13464/20000], Loss: 853.6453247070312, Entropy 467.86383056640625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13465/20000], Loss: 862.1199951171875, Entropy 465.7590637207031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13466/20000], Loss: 846.1776733398438, Entropy 464.33343505859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13467/20000], Loss: 870.283203125, Entropy 449.7337341308594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13468/20000], Loss: 892.8616943359375, Entropy 461.39111328125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13469/20000], Loss: 832.9442749023438, Entropy 456.53631591796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13470/20000], Loss: 827.1956176757812, Entropy 458.90374755859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13471/20000], Loss: 867.8978271484375, Entropy 461.162109375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13472/20000], Loss: 812.1438598632812, Entropy 446.36383056640625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13473/20000], Loss: 825.3574829101562, Entropy 466.95855712890625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13474/20000], Loss: 826.3720703125, Entropy 463.9958801269531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13475/20000], Loss: 868.630126953125, Entropy 460.9420471191406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13476/20000], Loss: 880.771240234375, Entropy 463.2111511230469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13477/20000], Loss: 824.8658447265625, Entropy 449.1792907714844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13478/20000], Loss: 820.5380859375, Entropy 468.6348571777344, Learning Rate: 4.8828125e-06\n",
      "Epoch [13479/20000], Loss: 872.9515991210938, Entropy 469.55706787109375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13480/20000], Loss: 856.165771484375, Entropy 465.7601013183594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13481/20000], Loss: 822.3380126953125, Entropy 466.0270080566406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13482/20000], Loss: 851.81787109375, Entropy 459.2284240722656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13483/20000], Loss: 846.187255859375, Entropy 466.34375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13484/20000], Loss: 871.3870849609375, Entropy 454.3647155761719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13485/20000], Loss: 845.9798583984375, Entropy 452.3704833984375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13486/20000], Loss: 866.7091674804688, Entropy 462.43450927734375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13487/20000], Loss: 820.1376342773438, Entropy 468.81390380859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13488/20000], Loss: 884.7781982421875, Entropy 455.013671875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13489/20000], Loss: 853.621337890625, Entropy 460.0367126464844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13490/20000], Loss: 796.3967895507812, Entropy 463.89105224609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13491/20000], Loss: 826.25830078125, Entropy 458.0523986816406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13492/20000], Loss: 849.0845947265625, Entropy 455.4312438964844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13493/20000], Loss: 812.146240234375, Entropy 460.458740234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13494/20000], Loss: 863.4446411132812, Entropy 442.98724365234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13495/20000], Loss: 849.6788330078125, Entropy 461.573974609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13496/20000], Loss: 856.5723876953125, Entropy 460.9930419921875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13497/20000], Loss: 809.8594970703125, Entropy 445.8371276855469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13498/20000], Loss: 853.9979248046875, Entropy 461.8688659667969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13499/20000], Loss: 825.5736694335938, Entropy 448.94757080078125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13500/20000], Loss: 858.0524291992188, Entropy 458.79693603515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13501/20000], Loss: 844.1106567382812, Entropy 433.44256591796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13502/20000], Loss: 805.334228515625, Entropy 475.9661865234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13503/20000], Loss: 903.2252197265625, Entropy 455.4273376464844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13504/20000], Loss: 839.5914916992188, Entropy 461.05120849609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13505/20000], Loss: 828.0947265625, Entropy 462.408935546875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13506/20000], Loss: 881.176025390625, Entropy 452.0499267578125, Learning Rate: 4.8828125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13507/20000], Loss: 858.70068359375, Entropy 451.6473388671875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13508/20000], Loss: 868.587890625, Entropy 463.0964050292969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13509/20000], Loss: 896.5050659179688, Entropy 439.22515869140625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13510/20000], Loss: 845.6552124023438, Entropy 445.43939208984375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13511/20000], Loss: 839.0470581054688, Entropy 449.95257568359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13512/20000], Loss: 834.0950927734375, Entropy 474.7325134277344, Learning Rate: 4.8828125e-06\n",
      "Epoch [13513/20000], Loss: 859.2823486328125, Entropy 464.5492858886719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13514/20000], Loss: 832.2344970703125, Entropy 452.1700744628906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13515/20000], Loss: 871.3990478515625, Entropy 462.2474365234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13516/20000], Loss: 844.2503662109375, Entropy 471.3215637207031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13517/20000], Loss: 851.552001953125, Entropy 453.9158630371094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13518/20000], Loss: 831.048828125, Entropy 443.1628723144531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13519/20000], Loss: 835.860107421875, Entropy 465.41015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13520/20000], Loss: 820.3131103515625, Entropy 463.8099365234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13521/20000], Loss: 838.766845703125, Entropy 447.5821838378906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13522/20000], Loss: 860.9608154296875, Entropy 458.0101013183594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13523/20000], Loss: 851.355224609375, Entropy 468.2862243652344, Learning Rate: 4.8828125e-06\n",
      "Epoch [13524/20000], Loss: 847.37646484375, Entropy 472.4100646972656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13525/20000], Loss: 878.8682861328125, Entropy 472.4861145019531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13526/20000], Loss: 810.0687255859375, Entropy 465.3086242675781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13527/20000], Loss: 829.092529296875, Entropy 465.8128662109375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13528/20000], Loss: 881.63671875, Entropy 474.3576354980469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13529/20000], Loss: 852.0562133789062, Entropy 456.91412353515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13530/20000], Loss: 867.11181640625, Entropy 453.2640380859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13531/20000], Loss: 852.9266357421875, Entropy 452.8289794921875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13532/20000], Loss: 849.2630615234375, Entropy 460.9664001464844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13533/20000], Loss: 902.6749877929688, Entropy 458.34808349609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13534/20000], Loss: 859.2012939453125, Entropy 462.3719177246094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13535/20000], Loss: 869.2060546875, Entropy 450.5589294433594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13536/20000], Loss: 805.4359741210938, Entropy 465.40972900390625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13537/20000], Loss: 884.0498046875, Entropy 466.2414855957031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13538/20000], Loss: 894.2072143554688, Entropy 461.98187255859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13539/20000], Loss: 855.6961059570312, Entropy 455.56927490234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13540/20000], Loss: 847.2687377929688, Entropy 471.33123779296875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13541/20000], Loss: 893.3916015625, Entropy 442.9767761230469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13542/20000], Loss: 833.068115234375, Entropy 452.6761474609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13543/20000], Loss: 842.4554443359375, Entropy 473.9192199707031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13544/20000], Loss: 863.79931640625, Entropy 454.1885986328125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13545/20000], Loss: 874.2401123046875, Entropy 464.5479431152344, Learning Rate: 4.8828125e-06\n",
      "Epoch [13546/20000], Loss: 839.6522827148438, Entropy 471.05694580078125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13547/20000], Loss: 892.2019653320312, Entropy 450.93377685546875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13548/20000], Loss: 808.230712890625, Entropy 484.194091796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13549/20000], Loss: 853.60302734375, Entropy 454.8049621582031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13550/20000], Loss: 850.8961181640625, Entropy 465.033203125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13551/20000], Loss: 897.2362060546875, Entropy 442.7986755371094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13552/20000], Loss: 912.34765625, Entropy 462.5972595214844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13553/20000], Loss: 826.5260620117188, Entropy 458.08013916015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13554/20000], Loss: 824.2186279296875, Entropy 467.6348571777344, Learning Rate: 4.8828125e-06\n",
      "Epoch [13555/20000], Loss: 866.1643676757812, Entropy 462.64508056640625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13556/20000], Loss: 808.7307739257812, Entropy 459.37628173828125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13557/20000], Loss: 822.8909912109375, Entropy 463.2049865722656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13558/20000], Loss: 894.611083984375, Entropy 451.0649108886719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13559/20000], Loss: 857.2471923828125, Entropy 459.7087707519531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13560/20000], Loss: 864.9405517578125, Entropy 458.0920715332031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13561/20000], Loss: 868.2883911132812, Entropy 458.12396240234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13562/20000], Loss: 877.3582763671875, Entropy 468.3457336425781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13563/20000], Loss: 810.8784790039062, Entropy 455.34039306640625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13564/20000], Loss: 814.3265380859375, Entropy 456.6857604980469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13565/20000], Loss: 840.3658447265625, Entropy 462.4125671386719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13566/20000], Loss: 840.526123046875, Entropy 449.81591796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13567/20000], Loss: 879.29443359375, Entropy 459.1527099609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13568/20000], Loss: 859.1170654296875, Entropy 460.8705749511719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13569/20000], Loss: 869.9163818359375, Entropy 463.2410888671875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13570/20000], Loss: 867.4500732421875, Entropy 460.8542785644531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13571/20000], Loss: 867.1568603515625, Entropy 444.48876953125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13572/20000], Loss: 874.405517578125, Entropy 442.0548400878906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13573/20000], Loss: 845.4752197265625, Entropy 457.3216247558594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13574/20000], Loss: 833.1694946289062, Entropy 459.98468017578125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13575/20000], Loss: 864.6488037109375, Entropy 463.7614440917969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13576/20000], Loss: 816.927001953125, Entropy 456.5696105957031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13577/20000], Loss: 925.5699462890625, Entropy 461.1041564941406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13578/20000], Loss: 843.4674072265625, Entropy 444.2130126953125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13579/20000], Loss: 849.3209228515625, Entropy 462.7747497558594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13580/20000], Loss: 865.02880859375, Entropy 472.3965148925781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13581/20000], Loss: 837.8068237304688, Entropy 453.57708740234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13582/20000], Loss: 840.9493408203125, Entropy 448.474365234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13583/20000], Loss: 827.8518676757812, Entropy 451.36358642578125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13584/20000], Loss: 873.5748291015625, Entropy 454.9552307128906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13585/20000], Loss: 843.3888549804688, Entropy 452.38763427734375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13586/20000], Loss: 856.933837890625, Entropy 454.8582763671875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13587/20000], Loss: 892.7322998046875, Entropy 469.2572937011719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13588/20000], Loss: 812.223876953125, Entropy 460.9247131347656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13589/20000], Loss: 816.28466796875, Entropy 472.6789855957031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13590/20000], Loss: 852.3929443359375, Entropy 453.7806091308594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13591/20000], Loss: 808.2022094726562, Entropy 467.94818115234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13592/20000], Loss: 811.874267578125, Entropy 462.3310852050781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13593/20000], Loss: 836.5353393554688, Entropy 474.24053955078125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13594/20000], Loss: 834.8750610351562, Entropy 461.87396240234375, Learning Rate: 4.8828125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13595/20000], Loss: 881.797119140625, Entropy 448.1163635253906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13596/20000], Loss: 863.3076782226562, Entropy 464.68646240234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13597/20000], Loss: 925.9041748046875, Entropy 444.6609802246094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13598/20000], Loss: 816.7044067382812, Entropy 469.90155029296875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13599/20000], Loss: 860.2901611328125, Entropy 463.8107604980469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13600/20000], Loss: 822.6451416015625, Entropy 465.2705993652344, Learning Rate: 4.8828125e-06\n",
      "Epoch [13601/20000], Loss: 868.563720703125, Entropy 454.0090637207031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13602/20000], Loss: 856.489990234375, Entropy 472.8656921386719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13603/20000], Loss: 865.7703247070312, Entropy 459.89739990234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13604/20000], Loss: 831.7943115234375, Entropy 456.5083923339844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13605/20000], Loss: 840.202392578125, Entropy 456.8966369628906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13606/20000], Loss: 889.4322509765625, Entropy 467.8016662597656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13607/20000], Loss: 872.4822387695312, Entropy 446.47222900390625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13608/20000], Loss: 883.4817504882812, Entropy 446.00006103515625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13609/20000], Loss: 873.4093017578125, Entropy 459.4830322265625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13610/20000], Loss: 805.0419921875, Entropy 469.4091796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13611/20000], Loss: 867.6995849609375, Entropy 458.5281982421875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13612/20000], Loss: 835.5457763671875, Entropy 436.3700256347656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13613/20000], Loss: 792.8531494140625, Entropy 455.0921630859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13614/20000], Loss: 869.1212158203125, Entropy 446.4206848144531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13615/20000], Loss: 888.424560546875, Entropy 453.52490234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13616/20000], Loss: 855.0843505859375, Entropy 442.1340026855469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13617/20000], Loss: 854.365966796875, Entropy 464.8428649902344, Learning Rate: 4.8828125e-06\n",
      "Epoch [13618/20000], Loss: 806.5958251953125, Entropy 465.3863220214844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13619/20000], Loss: 793.1939697265625, Entropy 472.6443786621094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13620/20000], Loss: 853.5811767578125, Entropy 453.0128479003906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13621/20000], Loss: 821.0065307617188, Entropy 458.50421142578125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13622/20000], Loss: 845.1898193359375, Entropy 451.7016296386719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13623/20000], Loss: 847.696044921875, Entropy 438.0885925292969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13624/20000], Loss: 838.8894653320312, Entropy 453.54803466796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13625/20000], Loss: 887.5453491210938, Entropy 442.94586181640625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13626/20000], Loss: 800.2654418945312, Entropy 445.75250244140625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13627/20000], Loss: 838.8392333984375, Entropy 456.1827697753906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13628/20000], Loss: 877.6803588867188, Entropy 473.86676025390625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13629/20000], Loss: 874.9298095703125, Entropy 461.8082275390625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13630/20000], Loss: 864.3408203125, Entropy 460.28857421875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13631/20000], Loss: 834.1201171875, Entropy 467.450927734375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13632/20000], Loss: 859.2332763671875, Entropy 455.2300720214844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13633/20000], Loss: 871.4869995117188, Entropy 457.69573974609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13634/20000], Loss: 844.7505493164062, Entropy 455.78265380859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13635/20000], Loss: 873.2342529296875, Entropy 457.2322692871094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13636/20000], Loss: 893.515625, Entropy 471.4606018066406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13637/20000], Loss: 820.713134765625, Entropy 455.0357666015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13638/20000], Loss: 867.1995849609375, Entropy 469.7718505859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13639/20000], Loss: 823.1062622070312, Entropy 451.44976806640625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13640/20000], Loss: 852.8549194335938, Entropy 471.30804443359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13641/20000], Loss: 864.7681884765625, Entropy 436.0377197265625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13642/20000], Loss: 887.5433349609375, Entropy 460.5403137207031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13643/20000], Loss: 842.6279296875, Entropy 462.7699890136719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13644/20000], Loss: 877.5635986328125, Entropy 462.3867492675781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13645/20000], Loss: 887.4158325195312, Entropy 450.73919677734375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13646/20000], Loss: 891.0084228515625, Entropy 458.7794494628906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13647/20000], Loss: 865.3445434570312, Entropy 463.10357666015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13648/20000], Loss: 807.1465454101562, Entropy 445.23248291015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13649/20000], Loss: 849.7859497070312, Entropy 452.16522216796875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13650/20000], Loss: 851.7982177734375, Entropy 462.7967224121094, Learning Rate: 4.8828125e-06\n",
      "Epoch [13651/20000], Loss: 833.5047607421875, Entropy 467.1844787597656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13652/20000], Loss: 866.0557250976562, Entropy 468.20904541015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13653/20000], Loss: 821.7889404296875, Entropy 457.5174865722656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13654/20000], Loss: 861.7988891601562, Entropy 464.88299560546875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13655/20000], Loss: 895.92431640625, Entropy 450.1668701171875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13656/20000], Loss: 804.534423828125, Entropy 463.0348815917969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13657/20000], Loss: 861.3358764648438, Entropy 446.62432861328125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13658/20000], Loss: 873.8117065429688, Entropy 466.97076416015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13659/20000], Loss: 850.7611083984375, Entropy 452.2810363769531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13660/20000], Loss: 893.1759033203125, Entropy 466.4163818359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13661/20000], Loss: 834.950927734375, Entropy 447.3799133300781, Learning Rate: 4.8828125e-06\n",
      "Epoch [13662/20000], Loss: 814.3843994140625, Entropy 468.6236572265625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13663/20000], Loss: 813.6265258789062, Entropy 456.06842041015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13664/20000], Loss: 886.9754638671875, Entropy 451.9410095214844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13665/20000], Loss: 861.28076171875, Entropy 463.7239685058594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13666/20000], Loss: 849.2684326171875, Entropy 455.9132080078125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13667/20000], Loss: 859.49462890625, Entropy 449.7855224609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13668/20000], Loss: 872.51416015625, Entropy 473.5343322753906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13669/20000], Loss: 821.2379150390625, Entropy 464.1589050292969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13670/20000], Loss: 885.0391845703125, Entropy 451.3844909667969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13671/20000], Loss: 840.5352172851562, Entropy 457.28082275390625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13672/20000], Loss: 825.1383056640625, Entropy 466.3249816894531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13673/20000], Loss: 862.86328125, Entropy 461.2522277832031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13674/20000], Loss: 813.726806640625, Entropy 470.3487243652344, Learning Rate: 4.8828125e-06\n",
      "Epoch [13675/20000], Loss: 882.9462890625, Entropy 449.9549560546875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13676/20000], Loss: 880.1153564453125, Entropy 443.5946044921875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13677/20000], Loss: 874.586181640625, Entropy 457.7192077636719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13678/20000], Loss: 867.4417724609375, Entropy 454.5953063964844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13679/20000], Loss: 883.1568603515625, Entropy 455.18408203125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13680/20000], Loss: 889.72119140625, Entropy 471.5744323730469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13681/20000], Loss: 843.9368286132812, Entropy 451.72064208984375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13682/20000], Loss: 867.2777099609375, Entropy 468.6500549316406, Learning Rate: 4.8828125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13683/20000], Loss: 817.4931030273438, Entropy 452.69879150390625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13684/20000], Loss: 861.34375, Entropy 466.3034973144531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13685/20000], Loss: 865.8746948242188, Entropy 452.55181884765625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13686/20000], Loss: 917.40869140625, Entropy 454.65576171875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13687/20000], Loss: 819.259033203125, Entropy 450.9240417480469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13688/20000], Loss: 839.560302734375, Entropy 455.095458984375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13689/20000], Loss: 876.8101806640625, Entropy 448.7804260253906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13690/20000], Loss: 857.6690673828125, Entropy 465.5872497558594, Learning Rate: 4.8828125e-06\n",
      "Epoch [13691/20000], Loss: 839.172119140625, Entropy 451.7821350097656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13692/20000], Loss: 823.196044921875, Entropy 463.18505859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13693/20000], Loss: 814.9519653320312, Entropy 472.62164306640625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13694/20000], Loss: 839.9321899414062, Entropy 458.81842041015625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13695/20000], Loss: 863.1420288085938, Entropy 475.20172119140625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13696/20000], Loss: 816.3260498046875, Entropy 461.7477111816406, Learning Rate: 4.8828125e-06\n",
      "Epoch [13697/20000], Loss: 837.2398071289062, Entropy 461.16717529296875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13698/20000], Loss: 862.8321533203125, Entropy 463.4486999511719, Learning Rate: 4.8828125e-06\n",
      "Epoch [13699/20000], Loss: 825.96240234375, Entropy 468.1023254394531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13700/20000], Loss: 800.0911865234375, Entropy 477.4886474609375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13701/20000], Loss: 895.8140258789062, Entropy 443.87359619140625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13702/20000], Loss: 862.2935791015625, Entropy 453.0694885253906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13703/20000], Loss: 831.7833251953125, Entropy 451.8081970214844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13704/20000], Loss: 841.2765502929688, Entropy 437.64373779296875, Learning Rate: 4.8828125e-06\n",
      "Epoch [13705/20000], Loss: 865.7000732421875, Entropy 452.5054626464844, Learning Rate: 4.8828125e-06\n",
      "Epoch [13706/20000], Loss: 851.4940795898438, Entropy 456.36480712890625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13707/20000], Loss: 823.3845825195312, Entropy 462.88018798828125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13708/20000], Loss: 840.2093505859375, Entropy 470.0573425292969, Learning Rate: 4.8828125e-06\n",
      "Epoch [13709/20000], Loss: 838.7520141601562, Entropy 450.80230712890625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13710/20000], Loss: 827.34033203125, Entropy 471.8342590332031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13711/20000], Loss: 848.0189208984375, Entropy 456.5837707519531, Learning Rate: 4.8828125e-06\n",
      "Epoch [13712/20000], Loss: 833.8997192382812, Entropy 453.74163818359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13713/20000], Loss: 906.8056640625, Entropy 440.8070068359375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13714/20000], Loss: 828.0035400390625, Entropy 467.8155822753906, Learning Rate: 4.8828125e-06\n",
      "Epoch [13715/20000], Loss: 876.1250610351562, Entropy 459.94036865234375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13716/20000], Loss: 876.2754516601562, Entropy 452.68267822265625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13717/20000], Loss: 854.313232421875, Entropy 455.5439453125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13718/20000], Loss: 867.2103271484375, Entropy 461.9254150390625, Learning Rate: 4.8828125e-06\n",
      "Epoch [13719/20000], Loss: 884.59521484375, Entropy 452.7850646972656, Learning Rate: 4.8828125e-06\n",
      "Epoch [13720/20000], Loss: 909.9864501953125, Entropy 457.5325012207031, Learning Rate: 4.8828125e-06\n",
      "Epoch [13721/20000], Loss: 836.2929077148438, Entropy 460.62042236328125, Learning Rate: 4.8828125e-06\n",
      "Epoch [13722/20000], Loss: 859.74658203125, Entropy 448.4162292480469, Learning Rate: 4.8828125e-06\n",
      "Epoch [13723/20000], Loss: 891.1723022460938, Entropy 459.77484130859375, Learning Rate: 4.8828125e-06\n",
      "Epoch [13724/20000], Loss: 825.5284423828125, Entropy 477.1448059082031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13725/20000], Loss: 857.0517578125, Entropy 459.7401428222656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13726/20000], Loss: 864.5364990234375, Entropy 455.5622253417969, Learning Rate: 2.44140625e-06\n",
      "Epoch [13727/20000], Loss: 944.9052734375, Entropy 457.7419738769531, Learning Rate: 2.44140625e-06\n",
      "Epoch [13728/20000], Loss: 852.6920166015625, Entropy 443.6274108886719, Learning Rate: 2.44140625e-06\n",
      "Epoch [13729/20000], Loss: 860.619873046875, Entropy 440.8185119628906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13730/20000], Loss: 857.2449951171875, Entropy 454.8583984375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13731/20000], Loss: 790.1864013671875, Entropy 452.7366943359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13732/20000], Loss: 831.5816040039062, Entropy 461.32537841796875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13733/20000], Loss: 839.6075439453125, Entropy 443.8356018066406, Learning Rate: 2.44140625e-06\n",
      "Epoch [13734/20000], Loss: 894.5859375, Entropy 458.6035461425781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13735/20000], Loss: 852.2926025390625, Entropy 450.6037292480469, Learning Rate: 2.44140625e-06\n",
      "Epoch [13736/20000], Loss: 859.0711059570312, Entropy 445.57550048828125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13737/20000], Loss: 863.1190795898438, Entropy 454.16802978515625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13738/20000], Loss: 861.7227783203125, Entropy 441.9695129394531, Learning Rate: 2.44140625e-06\n",
      "Epoch [13739/20000], Loss: 885.1474609375, Entropy 445.4231262207031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13740/20000], Loss: 868.775634765625, Entropy 454.2063903808594, Learning Rate: 2.44140625e-06\n",
      "Epoch [13741/20000], Loss: 846.2437133789062, Entropy 448.25323486328125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13742/20000], Loss: 881.60595703125, Entropy 451.5859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13743/20000], Loss: 856.181396484375, Entropy 455.5632019042969, Learning Rate: 2.44140625e-06\n",
      "Epoch [13744/20000], Loss: 863.681396484375, Entropy 467.4793395996094, Learning Rate: 2.44140625e-06\n",
      "Epoch [13745/20000], Loss: 858.3123779296875, Entropy 459.8270263671875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13746/20000], Loss: 848.031005859375, Entropy 454.1768798828125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13747/20000], Loss: 834.4814453125, Entropy 468.9245300292969, Learning Rate: 2.44140625e-06\n",
      "Epoch [13748/20000], Loss: 839.4273071289062, Entropy 459.78546142578125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13749/20000], Loss: 822.427978515625, Entropy 465.7781982421875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13750/20000], Loss: 836.12841796875, Entropy 465.5297546386719, Learning Rate: 2.44140625e-06\n",
      "Epoch [13751/20000], Loss: 897.0650634765625, Entropy 457.9789733886719, Learning Rate: 2.44140625e-06\n",
      "Epoch [13752/20000], Loss: 889.14111328125, Entropy 459.8682861328125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13753/20000], Loss: 851.2899169921875, Entropy 464.3788146972656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13754/20000], Loss: 912.536376953125, Entropy 455.3193664550781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13755/20000], Loss: 856.6926879882812, Entropy 461.61138916015625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13756/20000], Loss: 864.6618041992188, Entropy 450.60565185546875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13757/20000], Loss: 837.010009765625, Entropy 444.2307434082031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13758/20000], Loss: 812.32861328125, Entropy 464.4464416503906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13759/20000], Loss: 841.289794921875, Entropy 454.86474609375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13760/20000], Loss: 847.0642700195312, Entropy 468.54693603515625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13761/20000], Loss: 855.731201171875, Entropy 448.1853942871094, Learning Rate: 2.44140625e-06\n",
      "Epoch [13762/20000], Loss: 960.671875, Entropy 462.9586486816406, Learning Rate: 2.44140625e-06\n",
      "Epoch [13763/20000], Loss: 865.93359375, Entropy 476.3910827636719, Learning Rate: 2.44140625e-06\n",
      "Epoch [13764/20000], Loss: 835.1744384765625, Entropy 461.4075622558594, Learning Rate: 2.44140625e-06\n",
      "Epoch [13765/20000], Loss: 837.154296875, Entropy 434.5071716308594, Learning Rate: 2.44140625e-06\n",
      "Epoch [13766/20000], Loss: 886.0828247070312, Entropy 462.11090087890625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13767/20000], Loss: 877.9072875976562, Entropy 479.47967529296875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13768/20000], Loss: 867.7388916015625, Entropy 460.68603515625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13769/20000], Loss: 868.0133666992188, Entropy 465.03656005859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13770/20000], Loss: 864.3067016601562, Entropy 474.51654052734375, Learning Rate: 2.44140625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13771/20000], Loss: 848.4969482421875, Entropy 483.6338806152344, Learning Rate: 2.44140625e-06\n",
      "Epoch [13772/20000], Loss: 837.447021484375, Entropy 463.53759765625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13773/20000], Loss: 861.1961669921875, Entropy 444.7770080566406, Learning Rate: 2.44140625e-06\n",
      "Epoch [13774/20000], Loss: 855.140625, Entropy 452.7054138183594, Learning Rate: 2.44140625e-06\n",
      "Epoch [13775/20000], Loss: 875.3540649414062, Entropy 447.34295654296875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13776/20000], Loss: 843.7427978515625, Entropy 458.01318359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13777/20000], Loss: 831.46337890625, Entropy 464.6310119628906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13778/20000], Loss: 824.77392578125, Entropy 451.645263671875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13779/20000], Loss: 803.9539184570312, Entropy 448.51226806640625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13780/20000], Loss: 896.5008544921875, Entropy 461.3358459472656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13781/20000], Loss: 806.3837890625, Entropy 472.570068359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13782/20000], Loss: 883.6680908203125, Entropy 456.9093933105469, Learning Rate: 2.44140625e-06\n",
      "Epoch [13783/20000], Loss: 843.2420043945312, Entropy 454.89263916015625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13784/20000], Loss: 892.3001708984375, Entropy 457.4064025878906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13785/20000], Loss: 858.5708618164062, Entropy 452.21502685546875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13786/20000], Loss: 835.2178344726562, Entropy 463.91107177734375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13787/20000], Loss: 840.4046630859375, Entropy 449.8877258300781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13788/20000], Loss: 904.4150390625, Entropy 443.4931945800781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13789/20000], Loss: 829.544189453125, Entropy 462.12255859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13790/20000], Loss: 846.7821655273438, Entropy 448.22088623046875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13791/20000], Loss: 857.201904296875, Entropy 454.2195739746094, Learning Rate: 2.44140625e-06\n",
      "Epoch [13792/20000], Loss: 873.3360595703125, Entropy 458.7828063964844, Learning Rate: 2.44140625e-06\n",
      "Epoch [13793/20000], Loss: 833.70556640625, Entropy 450.2551574707031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13794/20000], Loss: 824.349365234375, Entropy 454.9580383300781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13795/20000], Loss: 859.9599609375, Entropy 448.3981018066406, Learning Rate: 2.44140625e-06\n",
      "Epoch [13796/20000], Loss: 842.1885986328125, Entropy 451.9632568359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13797/20000], Loss: 839.8426513671875, Entropy 472.0685729980469, Learning Rate: 2.44140625e-06\n",
      "Epoch [13798/20000], Loss: 832.24560546875, Entropy 470.39208984375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13799/20000], Loss: 887.82421875, Entropy 452.3757629394531, Learning Rate: 2.44140625e-06\n",
      "Epoch [13800/20000], Loss: 878.687255859375, Entropy 462.0989990234375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13801/20000], Loss: 826.3840942382812, Entropy 467.79156494140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13802/20000], Loss: 850.4019775390625, Entropy 435.4129638671875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13803/20000], Loss: 818.1650390625, Entropy 444.63818359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13804/20000], Loss: 817.5084228515625, Entropy 459.5478820800781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13805/20000], Loss: 892.1513671875, Entropy 470.2813415527344, Learning Rate: 2.44140625e-06\n",
      "Epoch [13806/20000], Loss: 824.7037353515625, Entropy 453.6624755859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13807/20000], Loss: 831.6085205078125, Entropy 467.4954528808594, Learning Rate: 2.44140625e-06\n",
      "Epoch [13808/20000], Loss: 874.4916381835938, Entropy 470.16156005859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13809/20000], Loss: 845.12109375, Entropy 461.8114013671875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13810/20000], Loss: 846.2893676757812, Entropy 458.26983642578125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13811/20000], Loss: 890.0501708984375, Entropy 457.4666442871094, Learning Rate: 2.44140625e-06\n",
      "Epoch [13812/20000], Loss: 856.1781616210938, Entropy 454.00091552734375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13813/20000], Loss: 840.809814453125, Entropy 463.9849853515625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13814/20000], Loss: 850.8187255859375, Entropy 456.8031005859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13815/20000], Loss: 830.73486328125, Entropy 465.0181884765625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13816/20000], Loss: 862.605712890625, Entropy 459.32958984375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13817/20000], Loss: 871.266357421875, Entropy 469.8528137207031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13818/20000], Loss: 843.423095703125, Entropy 470.7772521972656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13819/20000], Loss: 854.7239990234375, Entropy 455.4581604003906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13820/20000], Loss: 873.793212890625, Entropy 466.9162902832031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13821/20000], Loss: 891.0281372070312, Entropy 440.25872802734375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13822/20000], Loss: 877.1531982421875, Entropy 453.3341369628906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13823/20000], Loss: 839.0318603515625, Entropy 454.9053955078125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13824/20000], Loss: 886.65185546875, Entropy 456.0289306640625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13825/20000], Loss: 852.971923828125, Entropy 460.9083557128906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13826/20000], Loss: 850.224365234375, Entropy 458.4346008300781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13827/20000], Loss: 883.7916259765625, Entropy 456.3240661621094, Learning Rate: 2.44140625e-06\n",
      "Epoch [13828/20000], Loss: 889.0598754882812, Entropy 460.26397705078125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13829/20000], Loss: 832.7994384765625, Entropy 469.1040954589844, Learning Rate: 2.44140625e-06\n",
      "Epoch [13830/20000], Loss: 885.8160400390625, Entropy 449.5959167480469, Learning Rate: 2.44140625e-06\n",
      "Epoch [13831/20000], Loss: 855.6085205078125, Entropy 444.8316345214844, Learning Rate: 2.44140625e-06\n",
      "Epoch [13832/20000], Loss: 869.0159912109375, Entropy 458.0599365234375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13833/20000], Loss: 866.220947265625, Entropy 447.32666015625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13834/20000], Loss: 895.9568481445312, Entropy 445.65716552734375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13835/20000], Loss: 852.98681640625, Entropy 460.6473083496094, Learning Rate: 2.44140625e-06\n",
      "Epoch [13836/20000], Loss: 870.9248046875, Entropy 447.5760803222656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13837/20000], Loss: 808.96875, Entropy 443.98046875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13838/20000], Loss: 848.76611328125, Entropy 456.2287292480469, Learning Rate: 2.44140625e-06\n",
      "Epoch [13839/20000], Loss: 850.9329833984375, Entropy 444.7613830566406, Learning Rate: 2.44140625e-06\n",
      "Epoch [13840/20000], Loss: 826.1178588867188, Entropy 455.93194580078125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13841/20000], Loss: 839.9703369140625, Entropy 459.5281066894531, Learning Rate: 2.44140625e-06\n",
      "Epoch [13842/20000], Loss: 886.437744140625, Entropy 433.4178771972656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13843/20000], Loss: 918.7129516601562, Entropy 442.17926025390625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13844/20000], Loss: 872.7965087890625, Entropy 447.7903137207031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13845/20000], Loss: 930.0206298828125, Entropy 442.7719421386719, Learning Rate: 2.44140625e-06\n",
      "Epoch [13846/20000], Loss: 893.9759521484375, Entropy 464.8892822265625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13847/20000], Loss: 833.85693359375, Entropy 479.4664001464844, Learning Rate: 2.44140625e-06\n",
      "Epoch [13848/20000], Loss: 851.8392333984375, Entropy 451.1064453125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13849/20000], Loss: 875.7830810546875, Entropy 460.4495849609375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13850/20000], Loss: 876.9950561523438, Entropy 459.24896240234375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13851/20000], Loss: 815.442138671875, Entropy 456.6552734375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13852/20000], Loss: 836.2900390625, Entropy 470.81787109375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13853/20000], Loss: 841.6951904296875, Entropy 459.9401550292969, Learning Rate: 2.44140625e-06\n",
      "Epoch [13854/20000], Loss: 834.9376220703125, Entropy 459.1622314453125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13855/20000], Loss: 845.1806640625, Entropy 446.1246643066406, Learning Rate: 2.44140625e-06\n",
      "Epoch [13856/20000], Loss: 859.4402465820312, Entropy 457.75335693359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13857/20000], Loss: 865.7052612304688, Entropy 460.12237548828125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13858/20000], Loss: 888.4602661132812, Entropy 446.18194580078125, Learning Rate: 2.44140625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13859/20000], Loss: 827.9058227539062, Entropy 448.34393310546875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13860/20000], Loss: 829.36376953125, Entropy 458.6781005859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13861/20000], Loss: 853.7835083007812, Entropy 449.98553466796875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13862/20000], Loss: 828.1910400390625, Entropy 465.6881103515625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13863/20000], Loss: 864.43408203125, Entropy 468.4510803222656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13864/20000], Loss: 885.6900634765625, Entropy 446.9617004394531, Learning Rate: 2.44140625e-06\n",
      "Epoch [13865/20000], Loss: 831.224853515625, Entropy 466.8542785644531, Learning Rate: 2.44140625e-06\n",
      "Epoch [13866/20000], Loss: 882.5906982421875, Entropy 467.0857238769531, Learning Rate: 2.44140625e-06\n",
      "Epoch [13867/20000], Loss: 918.691650390625, Entropy 452.3983459472656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13868/20000], Loss: 830.5416259765625, Entropy 460.2102966308594, Learning Rate: 2.44140625e-06\n",
      "Epoch [13869/20000], Loss: 877.501220703125, Entropy 433.9906005859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13870/20000], Loss: 887.0067138671875, Entropy 457.5174255371094, Learning Rate: 2.44140625e-06\n",
      "Epoch [13871/20000], Loss: 826.6098022460938, Entropy 468.60272216796875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13872/20000], Loss: 824.684814453125, Entropy 454.0624084472656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13873/20000], Loss: 850.31982421875, Entropy 455.8409423828125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13874/20000], Loss: 890.1989135742188, Entropy 449.50201416015625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13875/20000], Loss: 850.3731689453125, Entropy 460.6134338378906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13876/20000], Loss: 832.6661376953125, Entropy 460.1383056640625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13877/20000], Loss: 869.8447265625, Entropy 454.0400695800781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13878/20000], Loss: 841.37548828125, Entropy 456.4847412109375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13879/20000], Loss: 877.1068725585938, Entropy 438.68878173828125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13880/20000], Loss: 963.2723388671875, Entropy 463.2292175292969, Learning Rate: 2.44140625e-06\n",
      "Epoch [13881/20000], Loss: 842.596435546875, Entropy 460.22021484375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13882/20000], Loss: 855.3101806640625, Entropy 479.2512512207031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13883/20000], Loss: 900.7626342773438, Entropy 444.91546630859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13884/20000], Loss: 881.9276733398438, Entropy 452.82928466796875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13885/20000], Loss: 854.8943481445312, Entropy 455.30865478515625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13886/20000], Loss: 825.63916015625, Entropy 470.2119445800781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13887/20000], Loss: 890.566162109375, Entropy 437.2145690917969, Learning Rate: 2.44140625e-06\n",
      "Epoch [13888/20000], Loss: 877.4108276367188, Entropy 450.14410400390625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13889/20000], Loss: 847.6033325195312, Entropy 455.89971923828125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13890/20000], Loss: 810.345458984375, Entropy 454.5604248046875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13891/20000], Loss: 875.30322265625, Entropy 461.404296875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13892/20000], Loss: 841.0460205078125, Entropy 457.9902648925781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13893/20000], Loss: 873.007568359375, Entropy 453.9111328125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13894/20000], Loss: 871.1066284179688, Entropy 439.59918212890625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13895/20000], Loss: 820.518310546875, Entropy 463.4137878417969, Learning Rate: 2.44140625e-06\n",
      "Epoch [13896/20000], Loss: 825.1121826171875, Entropy 464.1951599121094, Learning Rate: 2.44140625e-06\n",
      "Epoch [13897/20000], Loss: 888.9937744140625, Entropy 457.8499450683594, Learning Rate: 2.44140625e-06\n",
      "Epoch [13898/20000], Loss: 860.4042358398438, Entropy 461.39935302734375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13899/20000], Loss: 857.9881591796875, Entropy 454.9999084472656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13900/20000], Loss: 864.618408203125, Entropy 449.365966796875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13901/20000], Loss: 914.219970703125, Entropy 439.921875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13902/20000], Loss: 847.0223388671875, Entropy 454.4234313964844, Learning Rate: 2.44140625e-06\n",
      "Epoch [13903/20000], Loss: 861.675537109375, Entropy 447.7638854980469, Learning Rate: 2.44140625e-06\n",
      "Epoch [13904/20000], Loss: 862.520751953125, Entropy 461.6474914550781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13905/20000], Loss: 874.9647216796875, Entropy 459.281982421875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13906/20000], Loss: 870.1353759765625, Entropy 438.1404113769531, Learning Rate: 2.44140625e-06\n",
      "Epoch [13907/20000], Loss: 841.4869384765625, Entropy 463.5359191894531, Learning Rate: 2.44140625e-06\n",
      "Epoch [13908/20000], Loss: 813.034423828125, Entropy 475.0860900878906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13909/20000], Loss: 852.2709350585938, Entropy 450.69097900390625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13910/20000], Loss: 852.2562255859375, Entropy 462.80126953125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13911/20000], Loss: 862.5902099609375, Entropy 452.6593933105469, Learning Rate: 2.44140625e-06\n",
      "Epoch [13912/20000], Loss: 856.9114990234375, Entropy 454.1902770996094, Learning Rate: 2.44140625e-06\n",
      "Epoch [13913/20000], Loss: 816.7373046875, Entropy 467.0917053222656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13914/20000], Loss: 829.0841064453125, Entropy 453.406005859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13915/20000], Loss: 855.2459716796875, Entropy 456.3970947265625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13916/20000], Loss: 866.2884521484375, Entropy 467.6724548339844, Learning Rate: 2.44140625e-06\n",
      "Epoch [13917/20000], Loss: 844.7088012695312, Entropy 466.51715087890625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13918/20000], Loss: 877.2900390625, Entropy 467.9845886230469, Learning Rate: 2.44140625e-06\n",
      "Epoch [13919/20000], Loss: 886.0886840820312, Entropy 456.01483154296875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13920/20000], Loss: 841.357421875, Entropy 458.7269287109375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13921/20000], Loss: 816.1454467773438, Entropy 455.04095458984375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13922/20000], Loss: 787.3336181640625, Entropy 468.5567932128906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13923/20000], Loss: 815.5775146484375, Entropy 462.3121032714844, Learning Rate: 2.44140625e-06\n",
      "Epoch [13924/20000], Loss: 845.4767456054688, Entropy 447.00616455078125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13925/20000], Loss: 814.198974609375, Entropy 460.5068359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13926/20000], Loss: 840.0919799804688, Entropy 452.04412841796875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13927/20000], Loss: 881.9451904296875, Entropy 450.5283508300781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13928/20000], Loss: 847.2767333984375, Entropy 457.4670104980469, Learning Rate: 2.44140625e-06\n",
      "Epoch [13929/20000], Loss: 851.5557861328125, Entropy 471.0057373046875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13930/20000], Loss: 819.94140625, Entropy 462.474365234375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13931/20000], Loss: 801.7564697265625, Entropy 437.3309326171875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13932/20000], Loss: 848.8167724609375, Entropy 454.2098388671875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13933/20000], Loss: 950.1976928710938, Entropy 479.15643310546875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13934/20000], Loss: 825.108642578125, Entropy 465.3065490722656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13935/20000], Loss: 834.1550903320312, Entropy 448.47076416015625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13936/20000], Loss: 845.1456298828125, Entropy 469.3240051269531, Learning Rate: 2.44140625e-06\n",
      "Epoch [13937/20000], Loss: 875.3935546875, Entropy 451.6833190917969, Learning Rate: 2.44140625e-06\n",
      "Epoch [13938/20000], Loss: 829.25830078125, Entropy 479.1977233886719, Learning Rate: 2.44140625e-06\n",
      "Epoch [13939/20000], Loss: 900.576416015625, Entropy 456.8520202636719, Learning Rate: 2.44140625e-06\n",
      "Epoch [13940/20000], Loss: 840.5950927734375, Entropy 455.3453063964844, Learning Rate: 2.44140625e-06\n",
      "Epoch [13941/20000], Loss: 824.0966796875, Entropy 471.070068359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13942/20000], Loss: 854.83740234375, Entropy 464.2598571777344, Learning Rate: 2.44140625e-06\n",
      "Epoch [13943/20000], Loss: 792.3865356445312, Entropy 455.56927490234375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13944/20000], Loss: 886.9984130859375, Entropy 474.3771057128906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13945/20000], Loss: 845.92138671875, Entropy 455.0501708984375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13946/20000], Loss: 815.0885009765625, Entropy 468.3269348144531, Learning Rate: 2.44140625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13947/20000], Loss: 825.7335205078125, Entropy 471.1955871582031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13948/20000], Loss: 873.0130615234375, Entropy 450.8045959472656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13949/20000], Loss: 787.7483520507812, Entropy 462.66937255859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13950/20000], Loss: 884.661865234375, Entropy 462.1340637207031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13951/20000], Loss: 882.26416015625, Entropy 455.9178771972656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13952/20000], Loss: 864.0817260742188, Entropy 458.41827392578125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13953/20000], Loss: 816.7526245117188, Entropy 469.09234619140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13954/20000], Loss: 848.2289428710938, Entropy 451.52435302734375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13955/20000], Loss: 871.31103515625, Entropy 455.5763244628906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13956/20000], Loss: 853.3079833984375, Entropy 449.8394775390625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13957/20000], Loss: 845.9102783203125, Entropy 459.55908203125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13958/20000], Loss: 848.2388916015625, Entropy 459.2952575683594, Learning Rate: 2.44140625e-06\n",
      "Epoch [13959/20000], Loss: 855.6088256835938, Entropy 456.14862060546875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13960/20000], Loss: 867.0322265625, Entropy 459.1588439941406, Learning Rate: 2.44140625e-06\n",
      "Epoch [13961/20000], Loss: 860.9340209960938, Entropy 462.26971435546875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13962/20000], Loss: 869.6209716796875, Entropy 442.4934387207031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13963/20000], Loss: 884.538330078125, Entropy 465.3542175292969, Learning Rate: 2.44140625e-06\n",
      "Epoch [13964/20000], Loss: 876.8511352539062, Entropy 467.45855712890625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13965/20000], Loss: 921.5476684570312, Entropy 443.50787353515625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13966/20000], Loss: 871.64990234375, Entropy 459.2231140136719, Learning Rate: 2.44140625e-06\n",
      "Epoch [13967/20000], Loss: 803.7811279296875, Entropy 467.2284240722656, Learning Rate: 2.44140625e-06\n",
      "Epoch [13968/20000], Loss: 865.1297607421875, Entropy 464.968505859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13969/20000], Loss: 875.8112182617188, Entropy 449.63470458984375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13970/20000], Loss: 839.4248657226562, Entropy 450.46734619140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13971/20000], Loss: 905.5520629882812, Entropy 449.84881591796875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13972/20000], Loss: 862.1885986328125, Entropy 452.9772033691406, Learning Rate: 2.44140625e-06\n",
      "Epoch [13973/20000], Loss: 881.5374145507812, Entropy 454.55328369140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13974/20000], Loss: 864.34375, Entropy 472.820068359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13975/20000], Loss: 825.18310546875, Entropy 472.7839050292969, Learning Rate: 2.44140625e-06\n",
      "Epoch [13976/20000], Loss: 817.4970703125, Entropy 464.9434509277344, Learning Rate: 2.44140625e-06\n",
      "Epoch [13977/20000], Loss: 865.473388671875, Entropy 444.53955078125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13978/20000], Loss: 848.87060546875, Entropy 452.7685852050781, Learning Rate: 2.44140625e-06\n",
      "Epoch [13979/20000], Loss: 860.6983642578125, Entropy 453.6046447753906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13980/20000], Loss: 921.1654052734375, Entropy 455.66845703125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13981/20000], Loss: 879.454345703125, Entropy 471.8162841796875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13982/20000], Loss: 818.2313842773438, Entropy 476.38287353515625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13983/20000], Loss: 847.51904296875, Entropy 452.5509948730469, Learning Rate: 2.44140625e-06\n",
      "Epoch [13984/20000], Loss: 842.9625244140625, Entropy 453.6297607421875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13985/20000], Loss: 864.1309204101562, Entropy 457.02752685546875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13986/20000], Loss: 855.8755493164062, Entropy 448.28924560546875, Learning Rate: 2.44140625e-06\n",
      "Epoch [13987/20000], Loss: 851.029052734375, Entropy 458.0811767578125, Learning Rate: 2.44140625e-06\n",
      "Epoch [13988/20000], Loss: 848.6007690429688, Entropy 453.33441162109375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13989/20000], Loss: 810.279296875, Entropy 466.7452697753906, Learning Rate: 2.44140625e-06\n",
      "Epoch [13990/20000], Loss: 835.01318359375, Entropy 453.8177185058594, Learning Rate: 2.44140625e-06\n",
      "Epoch [13991/20000], Loss: 873.3616943359375, Entropy 469.5205993652344, Learning Rate: 2.44140625e-06\n",
      "Epoch [13992/20000], Loss: 865.5779418945312, Entropy 456.68609619140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [13993/20000], Loss: 859.844970703125, Entropy 452.1020202636719, Learning Rate: 2.44140625e-06\n",
      "Epoch [13994/20000], Loss: 860.896728515625, Entropy 456.9416809082031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13995/20000], Loss: 903.0667114257812, Entropy 454.70697021484375, Learning Rate: 2.44140625e-06\n",
      "Epoch [13996/20000], Loss: 833.10400390625, Entropy 461.9393005371094, Learning Rate: 2.44140625e-06\n",
      "Epoch [13997/20000], Loss: 856.9903564453125, Entropy 460.1012878417969, Learning Rate: 2.44140625e-06\n",
      "Epoch [13998/20000], Loss: 876.97412109375, Entropy 470.9358215332031, Learning Rate: 2.44140625e-06\n",
      "Epoch [13999/20000], Loss: 872.6923828125, Entropy 459.5436096191406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14000/20000], Loss: 830.9949951171875, Entropy 438.9485778808594, Learning Rate: 2.44140625e-06\n",
      "Epoch [14001/20000], Loss: 859.666748046875, Entropy 457.6385803222656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14002/20000], Loss: 834.0289306640625, Entropy 474.9352722167969, Learning Rate: 2.44140625e-06\n",
      "Epoch [14003/20000], Loss: 884.4984130859375, Entropy 461.2699890136719, Learning Rate: 2.44140625e-06\n",
      "Epoch [14004/20000], Loss: 791.5394287109375, Entropy 456.46533203125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14005/20000], Loss: 832.3096923828125, Entropy 455.7557067871094, Learning Rate: 2.44140625e-06\n",
      "Epoch [14006/20000], Loss: 849.0128784179688, Entropy 463.28643798828125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14007/20000], Loss: 888.053955078125, Entropy 452.1804504394531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14008/20000], Loss: 884.498046875, Entropy 468.7203063964844, Learning Rate: 2.44140625e-06\n",
      "Epoch [14009/20000], Loss: 821.4876708984375, Entropy 449.89306640625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14010/20000], Loss: 828.5466918945312, Entropy 454.42401123046875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14011/20000], Loss: 811.501708984375, Entropy 465.2566833496094, Learning Rate: 2.44140625e-06\n",
      "Epoch [14012/20000], Loss: 830.6097412109375, Entropy 458.9133605957031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14013/20000], Loss: 882.8905029296875, Entropy 449.1990051269531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14014/20000], Loss: 871.3941040039062, Entropy 450.45416259765625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14015/20000], Loss: 826.267822265625, Entropy 482.6412658691406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14016/20000], Loss: 877.5513916015625, Entropy 451.3869934082031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14017/20000], Loss: 823.9422607421875, Entropy 462.2839660644531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14018/20000], Loss: 861.1048583984375, Entropy 452.0970153808594, Learning Rate: 2.44140625e-06\n",
      "Epoch [14019/20000], Loss: 860.4138793945312, Entropy 467.86090087890625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14020/20000], Loss: 795.7371826171875, Entropy 466.1801452636719, Learning Rate: 2.44140625e-06\n",
      "Epoch [14021/20000], Loss: 897.7215576171875, Entropy 443.1580505371094, Learning Rate: 2.44140625e-06\n",
      "Epoch [14022/20000], Loss: 824.444580078125, Entropy 470.0803527832031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14023/20000], Loss: 855.20458984375, Entropy 455.6701354980469, Learning Rate: 2.44140625e-06\n",
      "Epoch [14024/20000], Loss: 864.0328979492188, Entropy 447.03643798828125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14025/20000], Loss: 855.2869262695312, Entropy 454.16632080078125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14026/20000], Loss: 843.4186401367188, Entropy 452.83233642578125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14027/20000], Loss: 896.2185668945312, Entropy 446.65374755859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14028/20000], Loss: 901.473388671875, Entropy 447.6336975097656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14029/20000], Loss: 857.54638671875, Entropy 460.7702941894531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14030/20000], Loss: 874.6744384765625, Entropy 442.6246337890625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14031/20000], Loss: 860.885498046875, Entropy 451.4644470214844, Learning Rate: 2.44140625e-06\n",
      "Epoch [14032/20000], Loss: 834.093994140625, Entropy 464.6109924316406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14033/20000], Loss: 815.54541015625, Entropy 461.9509582519531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14034/20000], Loss: 875.428955078125, Entropy 449.5980224609375, Learning Rate: 2.44140625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14035/20000], Loss: 878.097412109375, Entropy 452.6341247558594, Learning Rate: 2.44140625e-06\n",
      "Epoch [14036/20000], Loss: 859.79150390625, Entropy 450.8052978515625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14037/20000], Loss: 835.7933349609375, Entropy 467.2554626464844, Learning Rate: 2.44140625e-06\n",
      "Epoch [14038/20000], Loss: 846.9024658203125, Entropy 447.9747619628906, Learning Rate: 2.44140625e-06\n",
      "Epoch [14039/20000], Loss: 854.56591796875, Entropy 440.2696838378906, Learning Rate: 2.44140625e-06\n",
      "Epoch [14040/20000], Loss: 843.4067993164062, Entropy 446.56353759765625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14041/20000], Loss: 848.7156982421875, Entropy 455.259765625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14042/20000], Loss: 853.8312377929688, Entropy 469.66754150390625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14043/20000], Loss: 823.7265625, Entropy 474.3991394042969, Learning Rate: 2.44140625e-06\n",
      "Epoch [14044/20000], Loss: 866.1686401367188, Entropy 458.30010986328125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14045/20000], Loss: 848.25146484375, Entropy 474.6259765625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14046/20000], Loss: 896.5884399414062, Entropy 450.48468017578125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14047/20000], Loss: 859.9157104492188, Entropy 466.40472412109375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14048/20000], Loss: 840.6422119140625, Entropy 466.203125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14049/20000], Loss: 823.0367431640625, Entropy 467.5271911621094, Learning Rate: 2.44140625e-06\n",
      "Epoch [14050/20000], Loss: 852.8233032226562, Entropy 472.59515380859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14051/20000], Loss: 802.4083251953125, Entropy 467.3209533691406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14052/20000], Loss: 837.906982421875, Entropy 451.4422607421875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14053/20000], Loss: 826.3700561523438, Entropy 462.01007080078125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14054/20000], Loss: 808.196533203125, Entropy 462.6921691894531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14055/20000], Loss: 804.2058715820312, Entropy 463.41156005859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14056/20000], Loss: 884.9762573242188, Entropy 467.01788330078125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14057/20000], Loss: 853.703369140625, Entropy 453.7591247558594, Learning Rate: 2.44140625e-06\n",
      "Epoch [14058/20000], Loss: 832.2235107421875, Entropy 441.5362548828125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14059/20000], Loss: 867.7030029296875, Entropy 476.3369140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14060/20000], Loss: 849.28759765625, Entropy 450.7873229980469, Learning Rate: 2.44140625e-06\n",
      "Epoch [14061/20000], Loss: 826.3248291015625, Entropy 468.2572937011719, Learning Rate: 2.44140625e-06\n",
      "Epoch [14062/20000], Loss: 797.1519775390625, Entropy 461.1750793457031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14063/20000], Loss: 915.5557861328125, Entropy 455.140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14064/20000], Loss: 874.5511474609375, Entropy 448.8433532714844, Learning Rate: 2.44140625e-06\n",
      "Epoch [14065/20000], Loss: 897.9999389648438, Entropy 452.47515869140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14066/20000], Loss: 874.9301147460938, Entropy 457.25421142578125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14067/20000], Loss: 882.1522216796875, Entropy 457.4065246582031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14068/20000], Loss: 832.2054443359375, Entropy 462.4920959472656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14069/20000], Loss: 826.4319458007812, Entropy 468.88421630859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14070/20000], Loss: 870.6693115234375, Entropy 467.2437438964844, Learning Rate: 2.44140625e-06\n",
      "Epoch [14071/20000], Loss: 854.2838134765625, Entropy 469.8074035644531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14072/20000], Loss: 838.1546630859375, Entropy 459.5524597167969, Learning Rate: 2.44140625e-06\n",
      "Epoch [14073/20000], Loss: 907.5758666992188, Entropy 462.70904541015625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14074/20000], Loss: 924.4164428710938, Entropy 451.15130615234375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14075/20000], Loss: 897.802001953125, Entropy 454.8985900878906, Learning Rate: 2.44140625e-06\n",
      "Epoch [14076/20000], Loss: 884.90234375, Entropy 457.914794921875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14077/20000], Loss: 827.0684204101562, Entropy 448.98968505859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14078/20000], Loss: 836.990234375, Entropy 455.4201354980469, Learning Rate: 2.44140625e-06\n",
      "Epoch [14079/20000], Loss: 897.1154174804688, Entropy 469.03045654296875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14080/20000], Loss: 860.700439453125, Entropy 452.2059020996094, Learning Rate: 2.44140625e-06\n",
      "Epoch [14081/20000], Loss: 832.7891845703125, Entropy 454.0387268066406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14082/20000], Loss: 867.2102661132812, Entropy 460.24420166015625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14083/20000], Loss: 842.7960205078125, Entropy 441.9227600097656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14084/20000], Loss: 904.767822265625, Entropy 445.1551818847656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14085/20000], Loss: 845.0179443359375, Entropy 471.3824768066406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14086/20000], Loss: 838.8740234375, Entropy 459.5300598144531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14087/20000], Loss: 865.2447509765625, Entropy 456.0961608886719, Learning Rate: 2.44140625e-06\n",
      "Epoch [14088/20000], Loss: 820.2789306640625, Entropy 470.1946105957031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14089/20000], Loss: 846.1083984375, Entropy 462.7135009765625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14090/20000], Loss: 851.3384399414062, Entropy 461.33123779296875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14091/20000], Loss: 857.17041015625, Entropy 463.3330078125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14092/20000], Loss: 800.994140625, Entropy 476.7438049316406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14093/20000], Loss: 849.767578125, Entropy 445.0459289550781, Learning Rate: 2.44140625e-06\n",
      "Epoch [14094/20000], Loss: 844.5706787109375, Entropy 465.4940490722656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14095/20000], Loss: 835.96484375, Entropy 452.39306640625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14096/20000], Loss: 852.0054931640625, Entropy 456.1170349121094, Learning Rate: 2.44140625e-06\n",
      "Epoch [14097/20000], Loss: 814.7843017578125, Entropy 458.12109375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14098/20000], Loss: 862.471435546875, Entropy 453.2523193359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14099/20000], Loss: 895.0501708984375, Entropy 459.841552734375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14100/20000], Loss: 856.289306640625, Entropy 465.9967346191406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14101/20000], Loss: 850.2225952148438, Entropy 469.98187255859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14102/20000], Loss: 831.2464599609375, Entropy 458.905029296875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14103/20000], Loss: 868.1318359375, Entropy 461.0452880859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14104/20000], Loss: 864.6754150390625, Entropy 471.409912109375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14105/20000], Loss: 894.2284545898438, Entropy 448.57672119140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14106/20000], Loss: 834.8804931640625, Entropy 467.1242980957031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14107/20000], Loss: 878.5513916015625, Entropy 449.2016296386719, Learning Rate: 2.44140625e-06\n",
      "Epoch [14108/20000], Loss: 879.934326171875, Entropy 446.7668762207031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14109/20000], Loss: 806.8634033203125, Entropy 459.0389709472656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14110/20000], Loss: 859.6094970703125, Entropy 454.8707275390625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14111/20000], Loss: 857.00830078125, Entropy 452.2182312011719, Learning Rate: 2.44140625e-06\n",
      "Epoch [14112/20000], Loss: 883.6583251953125, Entropy 467.4375915527344, Learning Rate: 2.44140625e-06\n",
      "Epoch [14113/20000], Loss: 824.5843505859375, Entropy 462.6081848144531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14114/20000], Loss: 897.37939453125, Entropy 451.6063537597656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14115/20000], Loss: 863.2686767578125, Entropy 466.4762268066406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14116/20000], Loss: 847.9141845703125, Entropy 466.69140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14117/20000], Loss: 781.0858764648438, Entropy 474.05999755859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14118/20000], Loss: 839.0946044921875, Entropy 457.8355712890625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14119/20000], Loss: 854.412841796875, Entropy 441.1783447265625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14120/20000], Loss: 865.2371826171875, Entropy 458.025390625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14121/20000], Loss: 848.26708984375, Entropy 475.1246337890625, Learning Rate: 2.44140625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14122/20000], Loss: 864.9029541015625, Entropy 462.8539733886719, Learning Rate: 2.44140625e-06\n",
      "Epoch [14123/20000], Loss: 851.6907958984375, Entropy 454.6051330566406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14124/20000], Loss: 860.54248046875, Entropy 458.8450012207031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14125/20000], Loss: 882.0597534179688, Entropy 465.63531494140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14126/20000], Loss: 870.3515625, Entropy 467.2644348144531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14127/20000], Loss: 820.7786865234375, Entropy 483.0473327636719, Learning Rate: 2.44140625e-06\n",
      "Epoch [14128/20000], Loss: 868.8911743164062, Entropy 454.66363525390625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14129/20000], Loss: 881.0677490234375, Entropy 435.2676086425781, Learning Rate: 2.44140625e-06\n",
      "Epoch [14130/20000], Loss: 905.496337890625, Entropy 458.6254577636719, Learning Rate: 2.44140625e-06\n",
      "Epoch [14131/20000], Loss: 850.9819946289062, Entropy 455.14385986328125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14132/20000], Loss: 845.0658569335938, Entropy 468.93084716796875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14133/20000], Loss: 866.9527587890625, Entropy 461.4000549316406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14134/20000], Loss: 866.843017578125, Entropy 473.6228942871094, Learning Rate: 2.44140625e-06\n",
      "Epoch [14135/20000], Loss: 874.422607421875, Entropy 451.4556884765625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14136/20000], Loss: 866.2496337890625, Entropy 474.0902099609375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14137/20000], Loss: 878.73828125, Entropy 453.263427734375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14138/20000], Loss: 892.7962036132812, Entropy 469.13433837890625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14139/20000], Loss: 878.7818603515625, Entropy 448.3879699707031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14140/20000], Loss: 847.1486206054688, Entropy 461.90435791015625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14141/20000], Loss: 828.8251342773438, Entropy 461.34234619140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14142/20000], Loss: 836.806640625, Entropy 456.9364318847656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14143/20000], Loss: 918.8732299804688, Entropy 449.25616455078125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14144/20000], Loss: 835.0963134765625, Entropy 463.5121765136719, Learning Rate: 2.44140625e-06\n",
      "Epoch [14145/20000], Loss: 856.1456298828125, Entropy 456.9784240722656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14146/20000], Loss: 862.3253173828125, Entropy 480.0618591308594, Learning Rate: 2.44140625e-06\n",
      "Epoch [14147/20000], Loss: 933.001220703125, Entropy 441.8883361816406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14148/20000], Loss: 820.3585205078125, Entropy 463.8580322265625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14149/20000], Loss: 876.8695678710938, Entropy 449.02545166015625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14150/20000], Loss: 888.8118896484375, Entropy 450.3450012207031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14151/20000], Loss: 837.634033203125, Entropy 460.5650634765625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14152/20000], Loss: 862.268310546875, Entropy 445.7774963378906, Learning Rate: 2.44140625e-06\n",
      "Epoch [14153/20000], Loss: 848.1226806640625, Entropy 442.2166748046875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14154/20000], Loss: 871.8948974609375, Entropy 452.0232238769531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14155/20000], Loss: 870.1302490234375, Entropy 461.6878662109375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14156/20000], Loss: 853.339111328125, Entropy 469.413818359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14157/20000], Loss: 867.9417724609375, Entropy 441.4825744628906, Learning Rate: 2.44140625e-06\n",
      "Epoch [14158/20000], Loss: 878.2698974609375, Entropy 455.9228820800781, Learning Rate: 2.44140625e-06\n",
      "Epoch [14159/20000], Loss: 953.3762817382812, Entropy 454.28460693359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14160/20000], Loss: 840.74951171875, Entropy 446.7154541015625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14161/20000], Loss: 887.4671630859375, Entropy 447.0177917480469, Learning Rate: 2.44140625e-06\n",
      "Epoch [14162/20000], Loss: 951.903076171875, Entropy 462.2723388671875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14163/20000], Loss: 876.1527709960938, Entropy 456.55999755859375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14164/20000], Loss: 841.818115234375, Entropy 469.1402282714844, Learning Rate: 2.44140625e-06\n",
      "Epoch [14165/20000], Loss: 844.9312133789062, Entropy 485.99127197265625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14166/20000], Loss: 833.538330078125, Entropy 460.0339660644531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14167/20000], Loss: 841.9745483398438, Entropy 465.81927490234375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14168/20000], Loss: 926.7000732421875, Entropy 452.0696716308594, Learning Rate: 2.44140625e-06\n",
      "Epoch [14169/20000], Loss: 905.5968017578125, Entropy 455.2737121582031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14170/20000], Loss: 889.69775390625, Entropy 454.6227722167969, Learning Rate: 2.44140625e-06\n",
      "Epoch [14171/20000], Loss: 833.690673828125, Entropy 464.5816955566406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14172/20000], Loss: 788.4663696289062, Entropy 458.21331787109375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14173/20000], Loss: 835.7421875, Entropy 460.5198059082031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14174/20000], Loss: 828.5810546875, Entropy 455.1375427246094, Learning Rate: 2.44140625e-06\n",
      "Epoch [14175/20000], Loss: 859.9124145507812, Entropy 459.87750244140625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14176/20000], Loss: 862.120361328125, Entropy 461.6636962890625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14177/20000], Loss: 844.8917236328125, Entropy 448.8283386230469, Learning Rate: 2.44140625e-06\n",
      "Epoch [14178/20000], Loss: 873.6856689453125, Entropy 452.1907043457031, Learning Rate: 2.44140625e-06\n",
      "Epoch [14179/20000], Loss: 858.6650390625, Entropy 454.9958190917969, Learning Rate: 2.44140625e-06\n",
      "Epoch [14180/20000], Loss: 834.4762573242188, Entropy 466.48358154296875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14181/20000], Loss: 881.2042236328125, Entropy 450.3489685058594, Learning Rate: 2.44140625e-06\n",
      "Epoch [14182/20000], Loss: 871.3157958984375, Entropy 455.89453125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14183/20000], Loss: 852.098388671875, Entropy 469.7253112792969, Learning Rate: 2.44140625e-06\n",
      "Epoch [14184/20000], Loss: 821.1798706054688, Entropy 464.84490966796875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14185/20000], Loss: 856.9055786132812, Entropy 462.47845458984375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14186/20000], Loss: 818.27001953125, Entropy 461.8624267578125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14187/20000], Loss: 838.4154663085938, Entropy 461.85931396484375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14188/20000], Loss: 859.63818359375, Entropy 461.8133850097656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14189/20000], Loss: 862.839599609375, Entropy 454.8359375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14190/20000], Loss: 834.00732421875, Entropy 471.4969787597656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14191/20000], Loss: 925.680419921875, Entropy 457.1712341308594, Learning Rate: 2.44140625e-06\n",
      "Epoch [14192/20000], Loss: 863.9801025390625, Entropy 464.9581604003906, Learning Rate: 2.44140625e-06\n",
      "Epoch [14193/20000], Loss: 812.68701171875, Entropy 463.2208557128906, Learning Rate: 2.44140625e-06\n",
      "Epoch [14194/20000], Loss: 823.087158203125, Entropy 463.1561584472656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14195/20000], Loss: 871.0120849609375, Entropy 456.9852294921875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14196/20000], Loss: 872.958251953125, Entropy 453.2048034667969, Learning Rate: 2.44140625e-06\n",
      "Epoch [14197/20000], Loss: 874.76025390625, Entropy 447.0962829589844, Learning Rate: 2.44140625e-06\n",
      "Epoch [14198/20000], Loss: 880.786376953125, Entropy 461.5417175292969, Learning Rate: 2.44140625e-06\n",
      "Epoch [14199/20000], Loss: 846.1383056640625, Entropy 468.6598205566406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14200/20000], Loss: 821.828369140625, Entropy 453.2264099121094, Learning Rate: 2.44140625e-06\n",
      "Epoch [14201/20000], Loss: 805.7687377929688, Entropy 464.89837646484375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14202/20000], Loss: 841.8013305664062, Entropy 467.75054931640625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14203/20000], Loss: 842.9241943359375, Entropy 469.7002868652344, Learning Rate: 2.44140625e-06\n",
      "Epoch [14204/20000], Loss: 871.6452026367188, Entropy 457.53729248046875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14205/20000], Loss: 809.85546875, Entropy 460.5123291015625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14206/20000], Loss: 839.28564453125, Entropy 454.7447509765625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14207/20000], Loss: 906.47314453125, Entropy 453.3210754394531, Learning Rate: 2.44140625e-06\n",
      "Epoch [14208/20000], Loss: 842.3499755859375, Entropy 465.3846740722656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14209/20000], Loss: 885.996826171875, Entropy 467.3519287109375, Learning Rate: 2.44140625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14210/20000], Loss: 919.4176025390625, Entropy 458.8738708496094, Learning Rate: 2.44140625e-06\n",
      "Epoch [14211/20000], Loss: 887.8645629882812, Entropy 456.59881591796875, Learning Rate: 2.44140625e-06\n",
      "Epoch [14212/20000], Loss: 815.64404296875, Entropy 445.7961120605469, Learning Rate: 2.44140625e-06\n",
      "Epoch [14213/20000], Loss: 841.279052734375, Entropy 447.8915100097656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14214/20000], Loss: 832.6854858398438, Entropy 479.61383056640625, Learning Rate: 2.44140625e-06\n",
      "Epoch [14215/20000], Loss: 876.3798828125, Entropy 447.5543518066406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14216/20000], Loss: 883.4503173828125, Entropy 469.9549865722656, Learning Rate: 2.44140625e-06\n",
      "Epoch [14217/20000], Loss: 864.9306640625, Entropy 454.0543518066406, Learning Rate: 2.44140625e-06\n",
      "Epoch [14218/20000], Loss: 827.3218994140625, Entropy 454.6710205078125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14219/20000], Loss: 856.9075927734375, Entropy 454.2864685058594, Learning Rate: 2.44140625e-06\n",
      "Epoch [14220/20000], Loss: 811.1605224609375, Entropy 450.0531921386719, Learning Rate: 2.44140625e-06\n",
      "Epoch [14221/20000], Loss: 846.91455078125, Entropy 458.5848083496094, Learning Rate: 2.44140625e-06\n",
      "Epoch [14222/20000], Loss: 894.7691650390625, Entropy 456.1299133300781, Learning Rate: 2.44140625e-06\n",
      "Epoch [14223/20000], Loss: 825.8381958007812, Entropy 456.98333740234375, Learning Rate: 2.44140625e-06\n",
      "Epoch [14224/20000], Loss: 827.9755249023438, Entropy 472.76983642578125, Learning Rate: 2.44140625e-06\n",
      "Epoch [14225/20000], Loss: 873.7097778320312, Entropy 446.68389892578125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14226/20000], Loss: 823.5093994140625, Entropy 458.6612854003906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14227/20000], Loss: 885.330078125, Entropy 456.5736083984375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14228/20000], Loss: 826.80712890625, Entropy 453.6163330078125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14229/20000], Loss: 840.15673828125, Entropy 463.7491455078125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14230/20000], Loss: 821.8070068359375, Entropy 471.5339660644531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14231/20000], Loss: 879.163330078125, Entropy 471.947265625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14232/20000], Loss: 826.3719482421875, Entropy 455.7574768066406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14233/20000], Loss: 839.935302734375, Entropy 453.7518615722656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14234/20000], Loss: 831.0335693359375, Entropy 465.2146911621094, Learning Rate: 1.220703125e-06\n",
      "Epoch [14235/20000], Loss: 882.9302978515625, Entropy 470.8209228515625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14236/20000], Loss: 848.8404541015625, Entropy 462.7742004394531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14237/20000], Loss: 846.5809326171875, Entropy 464.0527648925781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14238/20000], Loss: 863.378173828125, Entropy 463.3419494628906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14239/20000], Loss: 844.6688232421875, Entropy 464.8938903808594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14240/20000], Loss: 815.404052734375, Entropy 446.3659973144531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14241/20000], Loss: 832.0746459960938, Entropy 451.91632080078125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14242/20000], Loss: 839.0648193359375, Entropy 454.5827941894531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14243/20000], Loss: 923.6368408203125, Entropy 452.1815490722656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14244/20000], Loss: 837.7353515625, Entropy 455.6214599609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14245/20000], Loss: 799.3160400390625, Entropy 450.1392822265625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14246/20000], Loss: 880.234619140625, Entropy 447.2365417480469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14247/20000], Loss: 859.0742797851562, Entropy 464.43475341796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14248/20000], Loss: 937.562744140625, Entropy 453.6235046386719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14249/20000], Loss: 859.3399047851562, Entropy 452.58038330078125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14250/20000], Loss: 846.3817138671875, Entropy 443.8311767578125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14251/20000], Loss: 857.526611328125, Entropy 456.7301025390625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14252/20000], Loss: 898.7528686523438, Entropy 448.62738037109375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14253/20000], Loss: 871.21240234375, Entropy 463.7325744628906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14254/20000], Loss: 824.3932495117188, Entropy 460.16217041015625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14255/20000], Loss: 872.8099975585938, Entropy 457.60479736328125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14256/20000], Loss: 928.7158203125, Entropy 454.0339050292969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14257/20000], Loss: 867.8090209960938, Entropy 474.55584716796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14258/20000], Loss: 824.8155517578125, Entropy 468.7716369628906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14259/20000], Loss: 894.7032470703125, Entropy 453.4508972167969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14260/20000], Loss: 809.236083984375, Entropy 461.2962341308594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14261/20000], Loss: 861.494384765625, Entropy 468.6653137207031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14262/20000], Loss: 853.7705688476562, Entropy 455.38726806640625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14263/20000], Loss: 825.2431640625, Entropy 458.7054443359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14264/20000], Loss: 848.6174926757812, Entropy 467.85150146484375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14265/20000], Loss: 829.27001953125, Entropy 454.74609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14266/20000], Loss: 869.2973022460938, Entropy 456.58831787109375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14267/20000], Loss: 887.659423828125, Entropy 455.4463195800781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14268/20000], Loss: 827.2258911132812, Entropy 466.53607177734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14269/20000], Loss: 848.42919921875, Entropy 468.46142578125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14270/20000], Loss: 827.5999145507812, Entropy 466.07281494140625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14271/20000], Loss: 854.65673828125, Entropy 471.778076171875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14272/20000], Loss: 839.9544677734375, Entropy 462.6731872558594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14273/20000], Loss: 865.1241455078125, Entropy 446.4008483886719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14274/20000], Loss: 848.9254150390625, Entropy 460.07958984375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14275/20000], Loss: 875.8716430664062, Entropy 467.35833740234375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14276/20000], Loss: 835.7510986328125, Entropy 460.640625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14277/20000], Loss: 864.12841796875, Entropy 465.0212707519531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14278/20000], Loss: 821.929443359375, Entropy 468.4297180175781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14279/20000], Loss: 853.9876708984375, Entropy 454.4567565917969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14280/20000], Loss: 841.949951171875, Entropy 461.7676086425781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14281/20000], Loss: 841.0338134765625, Entropy 470.4730224609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14282/20000], Loss: 833.8988647460938, Entropy 450.35931396484375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14283/20000], Loss: 866.2450561523438, Entropy 437.25616455078125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14284/20000], Loss: 857.25634765625, Entropy 467.1002502441406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14285/20000], Loss: 846.964111328125, Entropy 445.3606262207031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14286/20000], Loss: 849.63720703125, Entropy 448.0972595214844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14287/20000], Loss: 847.328369140625, Entropy 449.3345642089844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14288/20000], Loss: 844.1396484375, Entropy 451.7819519042969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14289/20000], Loss: 822.4588623046875, Entropy 458.6158142089844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14290/20000], Loss: 831.1768798828125, Entropy 484.5749816894531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14291/20000], Loss: 850.003662109375, Entropy 464.7658386230469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14292/20000], Loss: 830.6159057617188, Entropy 474.00006103515625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14293/20000], Loss: 900.0173950195312, Entropy 459.35491943359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14294/20000], Loss: 872.5269775390625, Entropy 444.41015625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14295/20000], Loss: 814.0015258789062, Entropy 464.97515869140625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14296/20000], Loss: 847.6666259765625, Entropy 460.5543518066406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14297/20000], Loss: 819.9829711914062, Entropy 468.78546142578125, Learning Rate: 1.220703125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14298/20000], Loss: 835.88916015625, Entropy 443.4783020019531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14299/20000], Loss: 836.2046508789062, Entropy 470.30853271484375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14300/20000], Loss: 835.3543701171875, Entropy 473.3326416015625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14301/20000], Loss: 826.5435791015625, Entropy 461.8661193847656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14302/20000], Loss: 823.0087890625, Entropy 469.6347961425781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14303/20000], Loss: 873.616943359375, Entropy 457.1600646972656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14304/20000], Loss: 834.33154296875, Entropy 465.8327941894531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14305/20000], Loss: 882.7090454101562, Entropy 453.82965087890625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14306/20000], Loss: 843.8193359375, Entropy 461.6207275390625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14307/20000], Loss: 841.9270629882812, Entropy 474.65179443359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14308/20000], Loss: 804.163330078125, Entropy 455.2961730957031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14309/20000], Loss: 842.3209228515625, Entropy 462.35009765625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14310/20000], Loss: 859.2578125, Entropy 454.7970886230469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14311/20000], Loss: 856.2415771484375, Entropy 457.1936950683594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14312/20000], Loss: 848.6029052734375, Entropy 450.2253112792969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14313/20000], Loss: 812.5479736328125, Entropy 468.2408447265625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14314/20000], Loss: 826.0430908203125, Entropy 461.30615234375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14315/20000], Loss: 836.2388916015625, Entropy 454.3752746582031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14316/20000], Loss: 852.4931030273438, Entropy 466.00323486328125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14317/20000], Loss: 852.4281005859375, Entropy 451.7327880859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14318/20000], Loss: 888.5515747070312, Entropy 471.99664306640625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14319/20000], Loss: 806.4876708984375, Entropy 476.8045959472656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14320/20000], Loss: 862.58642578125, Entropy 454.5064697265625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14321/20000], Loss: 828.3370361328125, Entropy 473.2734680175781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14322/20000], Loss: 803.8138427734375, Entropy 450.9576110839844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14323/20000], Loss: 840.22265625, Entropy 465.423583984375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14324/20000], Loss: 828.6607666015625, Entropy 460.2122497558594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14325/20000], Loss: 821.2548217773438, Entropy 472.02862548828125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14326/20000], Loss: 905.97314453125, Entropy 453.2667236328125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14327/20000], Loss: 850.8992919921875, Entropy 466.7703857421875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14328/20000], Loss: 888.2735595703125, Entropy 448.9252624511719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14329/20000], Loss: 837.8367919921875, Entropy 464.3968200683594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14330/20000], Loss: 886.0396118164062, Entropy 459.75628662109375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14331/20000], Loss: 864.9346923828125, Entropy 462.9468688964844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14332/20000], Loss: 818.9686889648438, Entropy 458.59710693359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14333/20000], Loss: 838.6707763671875, Entropy 466.6748352050781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14334/20000], Loss: 827.3883056640625, Entropy 443.4836120605469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14335/20000], Loss: 851.0864868164062, Entropy 448.53228759765625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14336/20000], Loss: 856.986083984375, Entropy 448.8436584472656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14337/20000], Loss: 795.14404296875, Entropy 464.1368713378906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14338/20000], Loss: 844.0286865234375, Entropy 469.700927734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14339/20000], Loss: 856.396240234375, Entropy 456.270263671875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14340/20000], Loss: 842.7437744140625, Entropy 470.4773864746094, Learning Rate: 1.220703125e-06\n",
      "Epoch [14341/20000], Loss: 833.1932373046875, Entropy 470.9031982421875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14342/20000], Loss: 872.8197631835938, Entropy 457.93365478515625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14343/20000], Loss: 879.453369140625, Entropy 459.9503173828125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14344/20000], Loss: 905.392578125, Entropy 450.4636535644531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14345/20000], Loss: 852.3897705078125, Entropy 472.5590515136719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14346/20000], Loss: 858.2852783203125, Entropy 442.3775634765625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14347/20000], Loss: 895.068359375, Entropy 454.7150573730469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14348/20000], Loss: 863.23193359375, Entropy 456.7707824707031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14349/20000], Loss: 863.992431640625, Entropy 448.1795349121094, Learning Rate: 1.220703125e-06\n",
      "Epoch [14350/20000], Loss: 791.4043579101562, Entropy 464.85760498046875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14351/20000], Loss: 852.2728271484375, Entropy 463.452880859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14352/20000], Loss: 849.72509765625, Entropy 465.4989013671875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14353/20000], Loss: 859.4779052734375, Entropy 454.8400573730469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14354/20000], Loss: 880.31298828125, Entropy 450.6819763183594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14355/20000], Loss: 861.1054077148438, Entropy 460.06732177734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14356/20000], Loss: 905.0897216796875, Entropy 442.5745849609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14357/20000], Loss: 891.9075927734375, Entropy 460.6966247558594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14358/20000], Loss: 875.3464965820312, Entropy 452.92303466796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14359/20000], Loss: 874.8806762695312, Entropy 466.78802490234375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14360/20000], Loss: 899.4267578125, Entropy 455.8563537597656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14361/20000], Loss: 867.2791748046875, Entropy 455.979248046875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14362/20000], Loss: 891.932861328125, Entropy 456.3483581542969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14363/20000], Loss: 845.169921875, Entropy 443.0962829589844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14364/20000], Loss: 859.9027709960938, Entropy 449.74432373046875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14365/20000], Loss: 854.1423950195312, Entropy 472.26507568359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14366/20000], Loss: 901.085205078125, Entropy 461.5103454589844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14367/20000], Loss: 866.0010986328125, Entropy 450.2029113769531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14368/20000], Loss: 859.3189697265625, Entropy 459.4410400390625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14369/20000], Loss: 842.3209228515625, Entropy 467.3972473144531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14370/20000], Loss: 801.790283203125, Entropy 465.7925720214844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14371/20000], Loss: 857.4156494140625, Entropy 465.6075134277344, Learning Rate: 1.220703125e-06\n",
      "Epoch [14372/20000], Loss: 879.2264404296875, Entropy 458.3901062011719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14373/20000], Loss: 832.60693359375, Entropy 467.7978515625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14374/20000], Loss: 867.3450317382812, Entropy 468.68109130859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14375/20000], Loss: 825.25341796875, Entropy 460.5436706542969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14376/20000], Loss: 840.4562377929688, Entropy 459.20037841796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14377/20000], Loss: 828.9249267578125, Entropy 469.3775634765625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14378/20000], Loss: 861.798095703125, Entropy 460.1255798339844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14379/20000], Loss: 836.142333984375, Entropy 441.1437072753906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14380/20000], Loss: 828.20556640625, Entropy 453.068359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14381/20000], Loss: 900.544677734375, Entropy 454.3264465332031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14382/20000], Loss: 970.385498046875, Entropy 455.53759765625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14383/20000], Loss: 812.5350341796875, Entropy 455.5594482421875, Learning Rate: 1.220703125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14384/20000], Loss: 859.9093017578125, Entropy 463.1456298828125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14385/20000], Loss: 793.7593994140625, Entropy 459.4242858886719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14386/20000], Loss: 899.230224609375, Entropy 462.8944396972656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14387/20000], Loss: 809.804443359375, Entropy 458.85546875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14388/20000], Loss: 859.936279296875, Entropy 453.1827697753906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14389/20000], Loss: 843.2116088867188, Entropy 461.71636962890625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14390/20000], Loss: 865.2498779296875, Entropy 456.3782043457031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14391/20000], Loss: 843.5776977539062, Entropy 441.23150634765625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14392/20000], Loss: 844.466552734375, Entropy 461.9594421386719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14393/20000], Loss: 870.552978515625, Entropy 454.5838623046875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14394/20000], Loss: 880.9920654296875, Entropy 456.827880859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14395/20000], Loss: 825.2779541015625, Entropy 447.2317810058594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14396/20000], Loss: 889.14208984375, Entropy 454.2423400878906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14397/20000], Loss: 842.593017578125, Entropy 469.9530334472656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14398/20000], Loss: 858.7548217773438, Entropy 462.88604736328125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14399/20000], Loss: 875.7847900390625, Entropy 446.5445251464844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14400/20000], Loss: 848.1068115234375, Entropy 470.5780334472656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14401/20000], Loss: 907.4666748046875, Entropy 453.4318542480469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14402/20000], Loss: 836.5615234375, Entropy 458.0299987792969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14403/20000], Loss: 833.1378173828125, Entropy 468.5364685058594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14404/20000], Loss: 854.7269287109375, Entropy 454.6238098144531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14405/20000], Loss: 849.6986083984375, Entropy 457.8822937011719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14406/20000], Loss: 874.2092895507812, Entropy 461.19281005859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14407/20000], Loss: 870.4754638671875, Entropy 454.65234375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14408/20000], Loss: 845.0615234375, Entropy 457.4756164550781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14409/20000], Loss: 833.0667724609375, Entropy 486.8187255859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14410/20000], Loss: 806.239013671875, Entropy 463.70068359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14411/20000], Loss: 849.3451538085938, Entropy 455.91497802734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14412/20000], Loss: 874.2921142578125, Entropy 465.1058044433594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14413/20000], Loss: 836.3319091796875, Entropy 444.5045166015625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14414/20000], Loss: 843.3084716796875, Entropy 459.6131286621094, Learning Rate: 1.220703125e-06\n",
      "Epoch [14415/20000], Loss: 862.1062622070312, Entropy 441.98248291015625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14416/20000], Loss: 796.521240234375, Entropy 475.0984191894531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14417/20000], Loss: 844.5038452148438, Entropy 474.31488037109375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14418/20000], Loss: 825.9725341796875, Entropy 465.3031005859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14419/20000], Loss: 841.5657958984375, Entropy 466.7336730957031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14420/20000], Loss: 839.3842163085938, Entropy 455.89959716796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14421/20000], Loss: 848.6817626953125, Entropy 460.4351501464844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14422/20000], Loss: 832.74462890625, Entropy 460.509765625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14423/20000], Loss: 838.1873168945312, Entropy 467.98529052734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14424/20000], Loss: 793.6632080078125, Entropy 469.0570983886719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14425/20000], Loss: 826.6103515625, Entropy 448.6618957519531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14426/20000], Loss: 835.115478515625, Entropy 459.1232604980469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14427/20000], Loss: 847.1088256835938, Entropy 461.62811279296875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14428/20000], Loss: 802.1133422851562, Entropy 443.50714111328125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14429/20000], Loss: 896.175537109375, Entropy 459.5702209472656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14430/20000], Loss: 877.72998046875, Entropy 460.333740234375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14431/20000], Loss: 874.6597900390625, Entropy 457.9173278808594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14432/20000], Loss: 807.766357421875, Entropy 457.0083923339844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14433/20000], Loss: 854.094482421875, Entropy 473.99951171875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14434/20000], Loss: 876.5632934570312, Entropy 450.19854736328125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14435/20000], Loss: 901.485595703125, Entropy 455.7281799316406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14436/20000], Loss: 815.0996704101562, Entropy 453.74859619140625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14437/20000], Loss: 827.0164184570312, Entropy 443.91912841796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14438/20000], Loss: 839.341796875, Entropy 437.1731872558594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14439/20000], Loss: 814.899658203125, Entropy 464.5693664550781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14440/20000], Loss: 844.5106811523438, Entropy 459.43963623046875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14441/20000], Loss: 910.6209716796875, Entropy 454.1835021972656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14442/20000], Loss: 892.5160522460938, Entropy 466.52081298828125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14443/20000], Loss: 869.4339599609375, Entropy 469.6038818359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14444/20000], Loss: 839.760498046875, Entropy 467.3718566894531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14445/20000], Loss: 832.21240234375, Entropy 464.8596496582031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14446/20000], Loss: 886.8062133789062, Entropy 464.07537841796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14447/20000], Loss: 892.20556640625, Entropy 456.376953125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14448/20000], Loss: 819.16455078125, Entropy 459.1709289550781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14449/20000], Loss: 836.8512573242188, Entropy 460.08294677734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14450/20000], Loss: 862.9380493164062, Entropy 471.37225341796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14451/20000], Loss: 877.46923828125, Entropy 457.282958984375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14452/20000], Loss: 839.61279296875, Entropy 463.5361633300781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14453/20000], Loss: 840.3590087890625, Entropy 459.294677734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14454/20000], Loss: 872.3839111328125, Entropy 466.4670104980469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14455/20000], Loss: 805.475341796875, Entropy 467.8801574707031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14456/20000], Loss: 816.0073852539062, Entropy 482.96307373046875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14457/20000], Loss: 835.8074340820312, Entropy 455.79156494140625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14458/20000], Loss: 888.4859619140625, Entropy 447.2833557128906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14459/20000], Loss: 860.27490234375, Entropy 462.9607849121094, Learning Rate: 1.220703125e-06\n",
      "Epoch [14460/20000], Loss: 857.8506469726562, Entropy 470.94366455078125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14461/20000], Loss: 884.3809814453125, Entropy 445.6852111816406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14462/20000], Loss: 853.6766967773438, Entropy 458.23284912109375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14463/20000], Loss: 858.2031860351562, Entropy 461.96051025390625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14464/20000], Loss: 830.6371459960938, Entropy 446.04437255859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14465/20000], Loss: 889.574951171875, Entropy 458.5090637207031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14466/20000], Loss: 836.4598388671875, Entropy 471.1814880371094, Learning Rate: 1.220703125e-06\n",
      "Epoch [14467/20000], Loss: 876.14501953125, Entropy 455.7773742675781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14468/20000], Loss: 884.657958984375, Entropy 455.5716552734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14469/20000], Loss: 837.819091796875, Entropy 448.6976013183594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14470/20000], Loss: 846.8477783203125, Entropy 456.9664306640625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14471/20000], Loss: 818.8963623046875, Entropy 459.6946105957031, Learning Rate: 1.220703125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14472/20000], Loss: 887.488037109375, Entropy 449.4228820800781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14473/20000], Loss: 843.1087646484375, Entropy 449.1109313964844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14474/20000], Loss: 860.0806884765625, Entropy 459.4393005371094, Learning Rate: 1.220703125e-06\n",
      "Epoch [14475/20000], Loss: 855.0629272460938, Entropy 459.06781005859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14476/20000], Loss: 809.8087768554688, Entropy 461.40045166015625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14477/20000], Loss: 816.586181640625, Entropy 459.0462646484375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14478/20000], Loss: 871.1616821289062, Entropy 447.46771240234375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14479/20000], Loss: 856.8661499023438, Entropy 459.03533935546875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14480/20000], Loss: 831.5214233398438, Entropy 463.28460693359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14481/20000], Loss: 870.9305419921875, Entropy 459.0191345214844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14482/20000], Loss: 907.6143798828125, Entropy 455.2965393066406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14483/20000], Loss: 846.764404296875, Entropy 453.3944396972656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14484/20000], Loss: 857.821533203125, Entropy 452.7552185058594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14485/20000], Loss: 893.8534545898438, Entropy 458.21624755859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14486/20000], Loss: 849.572265625, Entropy 455.857421875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14487/20000], Loss: 852.4532470703125, Entropy 456.4453430175781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14488/20000], Loss: 870.5244140625, Entropy 480.3126525878906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14489/20000], Loss: 834.604736328125, Entropy 467.0339050292969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14490/20000], Loss: 894.8505859375, Entropy 461.0849609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14491/20000], Loss: 862.6627197265625, Entropy 466.19775390625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14492/20000], Loss: 800.4085693359375, Entropy 470.0704650878906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14493/20000], Loss: 872.271728515625, Entropy 467.578857421875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14494/20000], Loss: 870.9862060546875, Entropy 468.6863098144531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14495/20000], Loss: 888.787109375, Entropy 459.1549072265625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14496/20000], Loss: 842.7815551757812, Entropy 470.00714111328125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14497/20000], Loss: 844.9954833984375, Entropy 470.806396484375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14498/20000], Loss: 839.9381103515625, Entropy 461.8238525390625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14499/20000], Loss: 830.3831787109375, Entropy 459.5267028808594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14500/20000], Loss: 849.5792236328125, Entropy 451.036865234375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14501/20000], Loss: 878.1026000976562, Entropy 479.60797119140625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14502/20000], Loss: 876.603515625, Entropy 461.3323974609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14503/20000], Loss: 853.1842041015625, Entropy 461.3297424316406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14504/20000], Loss: 881.9647216796875, Entropy 448.7451171875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14505/20000], Loss: 863.0225830078125, Entropy 452.0469055175781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14506/20000], Loss: 885.883056640625, Entropy 449.1867980957031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14507/20000], Loss: 865.539306640625, Entropy 457.8709411621094, Learning Rate: 1.220703125e-06\n",
      "Epoch [14508/20000], Loss: 904.296875, Entropy 462.5926818847656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14509/20000], Loss: 865.75537109375, Entropy 458.5247497558594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14510/20000], Loss: 839.4856567382812, Entropy 446.71014404296875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14511/20000], Loss: 870.323974609375, Entropy 456.84423828125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14512/20000], Loss: 855.2469482421875, Entropy 472.88037109375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14513/20000], Loss: 944.658447265625, Entropy 458.3072204589844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14514/20000], Loss: 847.44091796875, Entropy 461.8449401855469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14515/20000], Loss: 831.442138671875, Entropy 475.482177734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14516/20000], Loss: 854.7836303710938, Entropy 446.03460693359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14517/20000], Loss: 858.9990844726562, Entropy 455.87359619140625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14518/20000], Loss: 866.6653442382812, Entropy 467.64154052734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14519/20000], Loss: 835.3212890625, Entropy 464.216796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14520/20000], Loss: 825.8665161132812, Entropy 460.94769287109375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14521/20000], Loss: 837.742431640625, Entropy 467.0757751464844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14522/20000], Loss: 783.0747680664062, Entropy 463.27728271484375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14523/20000], Loss: 853.6734008789062, Entropy 452.78863525390625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14524/20000], Loss: 892.7235717773438, Entropy 454.18951416015625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14525/20000], Loss: 859.8096313476562, Entropy 454.86358642578125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14526/20000], Loss: 852.632080078125, Entropy 466.853515625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14527/20000], Loss: 831.5638427734375, Entropy 469.2176513671875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14528/20000], Loss: 870.4093627929688, Entropy 443.66937255859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14529/20000], Loss: 898.9234619140625, Entropy 448.5882568359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14530/20000], Loss: 833.6585083007812, Entropy 453.75396728515625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14531/20000], Loss: 809.6031494140625, Entropy 460.8782043457031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14532/20000], Loss: 851.9208984375, Entropy 459.0390930175781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14533/20000], Loss: 932.272705078125, Entropy 456.27099609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14534/20000], Loss: 816.5115966796875, Entropy 460.3543701171875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14535/20000], Loss: 877.8896484375, Entropy 450.4793701171875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14536/20000], Loss: 847.024658203125, Entropy 453.2942199707031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14537/20000], Loss: 821.1848754882812, Entropy 468.51483154296875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14538/20000], Loss: 852.1162109375, Entropy 458.6038513183594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14539/20000], Loss: 893.7022705078125, Entropy 467.1561584472656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14540/20000], Loss: 878.782958984375, Entropy 450.8572998046875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14541/20000], Loss: 830.6337890625, Entropy 455.4302673339844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14542/20000], Loss: 853.674072265625, Entropy 456.8511962890625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14543/20000], Loss: 860.87646484375, Entropy 467.3232727050781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14544/20000], Loss: 907.497802734375, Entropy 450.2945251464844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14545/20000], Loss: 891.098388671875, Entropy 465.2662353515625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14546/20000], Loss: 871.4086303710938, Entropy 459.96539306640625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14547/20000], Loss: 819.5274047851562, Entropy 444.90655517578125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14548/20000], Loss: 837.236572265625, Entropy 449.9615478515625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14549/20000], Loss: 860.5958251953125, Entropy 460.722412109375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14550/20000], Loss: 877.0469360351562, Entropy 458.76470947265625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14551/20000], Loss: 882.78369140625, Entropy 446.3908996582031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14552/20000], Loss: 768.9703369140625, Entropy 480.6529541015625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14553/20000], Loss: 867.411865234375, Entropy 447.3377990722656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14554/20000], Loss: 801.5868530273438, Entropy 464.51678466796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14555/20000], Loss: 878.2650756835938, Entropy 461.90643310546875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14556/20000], Loss: 874.0191650390625, Entropy 451.7707824707031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14557/20000], Loss: 822.8397827148438, Entropy 480.78802490234375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14558/20000], Loss: 867.8533935546875, Entropy 464.7713317871094, Learning Rate: 1.220703125e-06\n",
      "Epoch [14559/20000], Loss: 830.6873779296875, Entropy 448.3348388671875, Learning Rate: 1.220703125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14560/20000], Loss: 849.7122802734375, Entropy 468.6933898925781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14561/20000], Loss: 823.8204345703125, Entropy 452.6484375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14562/20000], Loss: 815.35986328125, Entropy 462.85693359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14563/20000], Loss: 808.058837890625, Entropy 468.8780822753906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14564/20000], Loss: 839.67333984375, Entropy 448.0492858886719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14565/20000], Loss: 859.06396484375, Entropy 456.1126708984375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14566/20000], Loss: 828.9838256835938, Entropy 446.40570068359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14567/20000], Loss: 838.9884033203125, Entropy 456.2814025878906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14568/20000], Loss: 845.8400268554688, Entropy 463.64764404296875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14569/20000], Loss: 852.22509765625, Entropy 461.9416809082031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14570/20000], Loss: 871.6831665039062, Entropy 472.12518310546875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14571/20000], Loss: 885.206298828125, Entropy 463.2643127441406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14572/20000], Loss: 896.1514892578125, Entropy 447.5978698730469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14573/20000], Loss: 885.598876953125, Entropy 455.8867492675781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14574/20000], Loss: 836.1028442382812, Entropy 464.71575927734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14575/20000], Loss: 822.762939453125, Entropy 469.5500793457031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14576/20000], Loss: 842.5850830078125, Entropy 448.2890625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14577/20000], Loss: 832.619384765625, Entropy 466.8728332519531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14578/20000], Loss: 843.3361206054688, Entropy 471.59417724609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14579/20000], Loss: 843.7445068359375, Entropy 448.4306640625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14580/20000], Loss: 860.0762939453125, Entropy 442.6536560058594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14581/20000], Loss: 834.0562744140625, Entropy 456.1506652832031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14582/20000], Loss: 877.2932739257812, Entropy 467.24078369140625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14583/20000], Loss: 858.3779296875, Entropy 464.703857421875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14584/20000], Loss: 813.010009765625, Entropy 462.9737548828125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14585/20000], Loss: 875.6859130859375, Entropy 454.2200012207031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14586/20000], Loss: 870.3878173828125, Entropy 454.8814697265625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14587/20000], Loss: 803.932373046875, Entropy 466.3130798339844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14588/20000], Loss: 802.4124145507812, Entropy 464.63482666015625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14589/20000], Loss: 841.5335083007812, Entropy 468.75018310546875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14590/20000], Loss: 882.3568115234375, Entropy 451.8203125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14591/20000], Loss: 894.9378662109375, Entropy 465.0833435058594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14592/20000], Loss: 846.131103515625, Entropy 465.2385559082031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14593/20000], Loss: 808.4789428710938, Entropy 457.74957275390625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14594/20000], Loss: 881.736572265625, Entropy 458.7196044921875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14595/20000], Loss: 820.2445678710938, Entropy 458.33648681640625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14596/20000], Loss: 800.2592163085938, Entropy 461.79632568359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14597/20000], Loss: 870.0375366210938, Entropy 447.43084716796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14598/20000], Loss: 827.0302734375, Entropy 452.5881652832031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14599/20000], Loss: 827.344970703125, Entropy 455.1109924316406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14600/20000], Loss: 820.523681640625, Entropy 453.6989440917969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14601/20000], Loss: 848.4508056640625, Entropy 463.89013671875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14602/20000], Loss: 846.7421875, Entropy 448.4539794921875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14603/20000], Loss: 849.8237915039062, Entropy 479.92474365234375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14604/20000], Loss: 885.4609375, Entropy 454.0719299316406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14605/20000], Loss: 840.6676025390625, Entropy 452.3443603515625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14606/20000], Loss: 858.7872314453125, Entropy 440.5042724609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14607/20000], Loss: 893.9951171875, Entropy 466.79296875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14608/20000], Loss: 888.29443359375, Entropy 461.6895751953125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14609/20000], Loss: 873.1536865234375, Entropy 441.2934265136719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14610/20000], Loss: 854.9814453125, Entropy 472.6876525878906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14611/20000], Loss: 899.3798217773438, Entropy 437.89703369140625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14612/20000], Loss: 900.1243896484375, Entropy 456.8773193359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14613/20000], Loss: 849.2196044921875, Entropy 453.5018615722656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14614/20000], Loss: 851.6275634765625, Entropy 454.0680236816406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14615/20000], Loss: 864.7171630859375, Entropy 462.7806396484375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14616/20000], Loss: 833.5343017578125, Entropy 474.6788024902344, Learning Rate: 1.220703125e-06\n",
      "Epoch [14617/20000], Loss: 876.906005859375, Entropy 446.4477233886719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14618/20000], Loss: 828.1861572265625, Entropy 460.9783935546875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14619/20000], Loss: 860.0416259765625, Entropy 456.8146057128906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14620/20000], Loss: 879.44775390625, Entropy 466.2776184082031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14621/20000], Loss: 839.535400390625, Entropy 445.4971923828125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14622/20000], Loss: 889.271484375, Entropy 456.6009216308594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14623/20000], Loss: 844.4979248046875, Entropy 453.6496276855469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14624/20000], Loss: 860.923828125, Entropy 451.4583435058594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14625/20000], Loss: 826.87158203125, Entropy 462.5792236328125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14626/20000], Loss: 869.6805419921875, Entropy 440.7537841796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14627/20000], Loss: 849.7238159179688, Entropy 467.95318603515625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14628/20000], Loss: 843.2286376953125, Entropy 473.5169677734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14629/20000], Loss: 856.3970947265625, Entropy 465.1550598144531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14630/20000], Loss: 857.1842041015625, Entropy 457.6585388183594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14631/20000], Loss: 851.3023681640625, Entropy 462.6203918457031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14632/20000], Loss: 816.36474609375, Entropy 457.4674987792969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14633/20000], Loss: 875.1331787109375, Entropy 451.7911682128906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14634/20000], Loss: 859.1951904296875, Entropy 459.6291809082031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14635/20000], Loss: 856.7857666015625, Entropy 471.845458984375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14636/20000], Loss: 830.735595703125, Entropy 456.5397644042969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14637/20000], Loss: 852.4217529296875, Entropy 461.6566162109375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14638/20000], Loss: 826.863037109375, Entropy 471.26171875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14639/20000], Loss: 872.726806640625, Entropy 449.904052734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14640/20000], Loss: 800.3912353515625, Entropy 473.7655029296875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14641/20000], Loss: 781.7052001953125, Entropy 467.03125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14642/20000], Loss: 808.5983276367188, Entropy 465.72808837890625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14643/20000], Loss: 842.0594482421875, Entropy 468.5570068359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14644/20000], Loss: 881.3680419921875, Entropy 460.10693359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14645/20000], Loss: 829.2384033203125, Entropy 457.5693664550781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14646/20000], Loss: 873.7577514648438, Entropy 458.42218017578125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14647/20000], Loss: 811.4888305664062, Entropy 463.33294677734375, Learning Rate: 1.220703125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14648/20000], Loss: 891.478515625, Entropy 440.0124206542969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14649/20000], Loss: 832.6080322265625, Entropy 463.498046875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14650/20000], Loss: 880.4822998046875, Entropy 445.4075012207031, Learning Rate: 1.220703125e-06\n",
      "Epoch [14651/20000], Loss: 836.4244995117188, Entropy 457.30950927734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14652/20000], Loss: 860.2848510742188, Entropy 445.43658447265625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14653/20000], Loss: 832.75048828125, Entropy 466.67724609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14654/20000], Loss: 857.01220703125, Entropy 453.4816589355469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14655/20000], Loss: 890.8084716796875, Entropy 451.0693664550781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14656/20000], Loss: 839.2757568359375, Entropy 462.3548278808594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14657/20000], Loss: 866.12548828125, Entropy 467.3498229980469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14658/20000], Loss: 873.088134765625, Entropy 468.0693664550781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14659/20000], Loss: 824.6854248046875, Entropy 448.1246032714844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14660/20000], Loss: 888.8486328125, Entropy 463.6977233886719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14661/20000], Loss: 842.5230102539062, Entropy 458.47808837890625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14662/20000], Loss: 861.3123779296875, Entropy 449.679443359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14663/20000], Loss: 807.21533203125, Entropy 465.2076110839844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14664/20000], Loss: 821.7341918945312, Entropy 463.46124267578125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14665/20000], Loss: 824.264404296875, Entropy 471.5475158691406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14666/20000], Loss: 905.9825439453125, Entropy 455.4830322265625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14667/20000], Loss: 808.0562744140625, Entropy 445.0870361328125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14668/20000], Loss: 860.8746948242188, Entropy 450.23468017578125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14669/20000], Loss: 814.0130615234375, Entropy 473.1170959472656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14670/20000], Loss: 821.6168212890625, Entropy 461.6914367675781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14671/20000], Loss: 938.97509765625, Entropy 461.5017395019531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14672/20000], Loss: 878.9918212890625, Entropy 467.6446228027344, Learning Rate: 1.220703125e-06\n",
      "Epoch [14673/20000], Loss: 823.4703369140625, Entropy 457.6199645996094, Learning Rate: 1.220703125e-06\n",
      "Epoch [14674/20000], Loss: 863.753662109375, Entropy 465.9631042480469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14675/20000], Loss: 868.3530883789062, Entropy 457.89593505859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14676/20000], Loss: 867.9766235351562, Entropy 461.95892333984375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14677/20000], Loss: 833.17041015625, Entropy 456.6564636230469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14678/20000], Loss: 854.4215087890625, Entropy 460.9281005859375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14679/20000], Loss: 817.092041015625, Entropy 460.9053955078125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14680/20000], Loss: 835.2122802734375, Entropy 457.0401916503906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14681/20000], Loss: 813.1902465820312, Entropy 461.62689208984375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14682/20000], Loss: 877.6439208984375, Entropy 454.7931213378906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14683/20000], Loss: 858.09423828125, Entropy 459.7731018066406, Learning Rate: 1.220703125e-06\n",
      "Epoch [14684/20000], Loss: 873.5603637695312, Entropy 434.67132568359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14685/20000], Loss: 836.7359619140625, Entropy 470.67724609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14686/20000], Loss: 851.6878662109375, Entropy 453.550537109375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14687/20000], Loss: 868.4339599609375, Entropy 460.4856872558594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14688/20000], Loss: 851.4862060546875, Entropy 450.2392883300781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14689/20000], Loss: 844.98291015625, Entropy 474.8748779296875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14690/20000], Loss: 809.4954833984375, Entropy 470.9285888671875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14691/20000], Loss: 862.474609375, Entropy 476.7117004394531, Learning Rate: 1.220703125e-06\n",
      "Epoch [14692/20000], Loss: 823.7738647460938, Entropy 460.87164306640625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14693/20000], Loss: 871.6891479492188, Entropy 457.93389892578125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14694/20000], Loss: 837.6826782226562, Entropy 465.91131591796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14695/20000], Loss: 887.2926025390625, Entropy 440.8207702636719, Learning Rate: 1.220703125e-06\n",
      "Epoch [14696/20000], Loss: 845.06396484375, Entropy 462.9795227050781, Learning Rate: 1.220703125e-06\n",
      "Epoch [14697/20000], Loss: 842.115234375, Entropy 445.3033142089844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14698/20000], Loss: 890.5283813476562, Entropy 450.55694580078125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14699/20000], Loss: 854.95068359375, Entropy 475.56787109375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14700/20000], Loss: 866.1637573242188, Entropy 462.30694580078125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14701/20000], Loss: 867.829833984375, Entropy 465.9338073730469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14702/20000], Loss: 878.7216796875, Entropy 453.90234375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14703/20000], Loss: 858.307861328125, Entropy 468.0625915527344, Learning Rate: 1.220703125e-06\n",
      "Epoch [14704/20000], Loss: 841.5528564453125, Entropy 466.5498046875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14705/20000], Loss: 894.8564453125, Entropy 445.7780456542969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14706/20000], Loss: 848.3316650390625, Entropy 473.966796875, Learning Rate: 1.220703125e-06\n",
      "Epoch [14707/20000], Loss: 877.4075927734375, Entropy 472.7003479003906, Learning Rate: 1.220703125e-06\n",
      "Epoch [14708/20000], Loss: 845.3211059570312, Entropy 466.87359619140625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14709/20000], Loss: 811.825927734375, Entropy 457.767333984375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14710/20000], Loss: 818.2875366210938, Entropy 458.38922119140625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14711/20000], Loss: 862.5284423828125, Entropy 449.1095886230469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14712/20000], Loss: 866.6940307617188, Entropy 454.68255615234375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14713/20000], Loss: 848.5769653320312, Entropy 464.09405517578125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14714/20000], Loss: 840.7471923828125, Entropy 474.3282165527344, Learning Rate: 1.220703125e-06\n",
      "Epoch [14715/20000], Loss: 888.2998046875, Entropy 444.9645690917969, Learning Rate: 1.220703125e-06\n",
      "Epoch [14716/20000], Loss: 832.9902954101562, Entropy 469.56353759765625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14717/20000], Loss: 813.8245849609375, Entropy 465.7775573730469, Learning Rate: 1.220703125e-06\n",
      "Epoch [14718/20000], Loss: 863.9396362304688, Entropy 459.91497802734375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14719/20000], Loss: 871.43701171875, Entropy 461.8782653808594, Learning Rate: 1.220703125e-06\n",
      "Epoch [14720/20000], Loss: 862.13916015625, Entropy 450.1327209472656, Learning Rate: 1.220703125e-06\n",
      "Epoch [14721/20000], Loss: 861.7491455078125, Entropy 466.8951416015625, Learning Rate: 1.220703125e-06\n",
      "Epoch [14722/20000], Loss: 860.443115234375, Entropy 465.693359375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14723/20000], Loss: 852.3428955078125, Entropy 452.7393798828125, Learning Rate: 1.220703125e-06\n",
      "Epoch [14724/20000], Loss: 779.6314086914062, Entropy 478.89886474609375, Learning Rate: 1.220703125e-06\n",
      "Epoch [14725/20000], Loss: 846.8392333984375, Entropy 466.9644470214844, Learning Rate: 1.220703125e-06\n",
      "Epoch [14726/20000], Loss: 869.9114379882812, Entropy 434.88873291015625, Learning Rate: 6.103515625e-07\n",
      "12220\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    optimizer = GeNVariationalInference(logtarget,\n",
    "\t\t                                    0, 100, 1000, 50, 100,\n",
    "\t\t                                    20000, .01, .000001, 500, .5,\n",
    "\t\t                                    device, True, temp_dir, save_best=True)\n",
    "    best_epoch, scores=optimizer.run(GeN)\n",
    "print(best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nLPP: 1.2007985 (0.38947624)\n",
      "Squared Error: 0.36871108 (1.45352)\n"
     ]
    }
   ],
   "source": [
    "nLPP_train, nLPP_validation, nLPP_test, RSE_train, RSE_validation, RSE_test=setup.evaluate_metrics(GeN(1000).detach(),device)\n",
    "print('nLPP: '+str(nLPP_test[0].float().cpu().numpy())+' ('+str(nLPP_test[1].float().cpu().numpy())+')')\n",
    "print('Squared Error: '+str(RSE_test[0].float().cpu().numpy())+' ('+str(RSE_test[1].float().cpu().numpy())+')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setup.plot:\n",
    "    fig=makePlot(setup,GeN,noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=[]\n",
    "\n",
    "for i in range(colors.shape[0]):\n",
    "    for j in range(colors.shape[1]):\n",
    "        xy_noise=torch.Tensor([noise[0][i,j],noise[1][i,j]]).unsqueeze(0).to(device)\n",
    "        Z.append(GeN.components[0].hnet(xy_noise).detach().squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14400, 751])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z=torch.stack(Z)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C=[]\n",
    "for i in range(colors.shape[0]):\n",
    "    for j in range(colors.shape[1]):\n",
    "        C.append(colors[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_stack=torch.stack(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0971568822860718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14400, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "myTSNE=TSNE(n_components=3,init='pca',perplexity=10)\n",
    "X_embedded =myTSNE.fit_transform(Z.cpu())\n",
    "print(myTSNE.kl_divergence_)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"720\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(X_embedded[:,0],X_embedded[:,1],X_embedded[:,2], c=colors_stack, linewidth=0.2)\n",
    "plt.title('3D t-SNE of '+str(param_count)+'-dimensional transformed noise')\n",
    "plt.show()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
