{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://github.com/ColCarroll/minimc.git\n",
    "\"\"\"\n",
    "\n",
    "def leapfrog(q, p, dVdq, potential, path_len, step_size):\n",
    "    \"\"\"Leapfrog integrator for Hamiltonian Monte Carlo.\n",
    "    Parameters\n",
    "    ----------\n",
    "    q : np.floatX\n",
    "        Initial position\n",
    "    p : np.floatX\n",
    "        Initial momentum\n",
    "    dVdq : np.floatX\n",
    "        Gradient of the potential at the initial coordinates\n",
    "    potential : callable\n",
    "        Value and gradient of the potential\n",
    "    path_len : float\n",
    "        How long to integrate for\n",
    "    step_size : float\n",
    "        How long each integration step should be\n",
    "    Returns\n",
    "    -------\n",
    "    q, p : np.floatX, np.floatX\n",
    "        New position and momentum\n",
    "    \"\"\"\n",
    "    q, p = np.copy(q), np.copy(p)\n",
    "\n",
    "    p -= step_size * dVdq / 2  # half step\n",
    "    for _ in np.arange(path_len):#np.arange(np.round(path_len / step_size) - 1):\n",
    "        q += step_size * p  # whole step\n",
    "        V, dVdq = potential(q)\n",
    "        p -= step_size * dVdq  # whole step\n",
    "    q += step_size * p  # whole step\n",
    "    V, dVdq = potential(q)\n",
    "    p -= step_size * dVdq / 2  # half step\n",
    "\n",
    "    # momentum flip at end\n",
    "    return q, -p, V, dVdq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from:\n",
    "https://github.com/ColCarroll/minimc.git\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def hamiltonian_monte_carlo(\n",
    "    numiter,\n",
    "    burnin,\n",
    "    thinning,\n",
    "    potential,\n",
    "    initial_position,\n",
    "    initial_potential=None,\n",
    "    initial_potential_grad=None,\n",
    "    path_len=1,\n",
    "    initial_step_size=0.1,\n",
    "    integrator=leapfrog,\n",
    "    max_energy_change=1000.0,\n",
    "):\n",
    "    \"\"\"Run Hamiltonian Monte Carlo sampling.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples to return\n",
    "    negative_log_prob : callable\n",
    "        The negative log probability to sample from\n",
    "    initial_position : np.array\n",
    "        A place to start sampling from.\n",
    "    tune: int\n",
    "        Number of iterations to run tuning\n",
    "    path_len : float\n",
    "        How long each integration path is. Smaller is faster and more correlated.\n",
    "    initial_step_size : float\n",
    "        How long each integration step is. This will be tuned automatically.\n",
    "    max_energy_change : float\n",
    "        The largest tolerable integration error. Transitions with energy changes\n",
    "        larger than this will be declared divergences.\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        Array of length `n_samples`.\n",
    "    \"\"\"\n",
    "    acceptance_count=0\n",
    "    initial_position = np.array(initial_position)\n",
    "    if initial_potential is None or initial_potential_grad is None:\n",
    "        initial_potential, initial_potential_grad = potential(initial_position)\n",
    "\n",
    "    q_last=initial_position\n",
    "    # collect all our samples in a list\n",
    "    samples = []\n",
    "    accept_rates= []\n",
    "    \n",
    "    # Keep a single object for momentum resamplingtqdm\n",
    "    momentum = st.norm(0, 1)\n",
    "\n",
    "    step_size = initial_step_size\n",
    "    \n",
    "    with trange(numiter) as tr:\n",
    "        for t in tr:\n",
    "            p0=momentum.rvs(size=initial_position.shape[:1])\n",
    "            # Integrate over our path to get a new position and momentum\n",
    "            q_new, p_new, final_V, final_dVdq = integrator(\n",
    "                q_last,\n",
    "                p0,\n",
    "                initial_potential_grad,\n",
    "                potential,\n",
    "                path_len=path_len,#2* np.random.rand()* path_len,  # We jitter the path length a bit\n",
    "                step_size=step_size,\n",
    "            )\n",
    "\n",
    "            start_log_p = np.sum(momentum.logpdf(p0)) - initial_potential\n",
    "            new_log_p = np.sum(momentum.logpdf(p_new)) - final_V\n",
    "            energy_change = new_log_p - start_log_p\n",
    "\n",
    "            # Check Metropolis acceptance criterion\n",
    "            p_accept = min(1, np.exp(energy_change))\n",
    "            if np.random.rand() < p_accept:\n",
    "                acceptance_count+=1\n",
    "                initial_potential = final_V\n",
    "                initial_potential_grad = final_dVdq\n",
    "            else:\n",
    "                q_new=q_last\n",
    "\n",
    "            if (t - burnin) % thinning == 0:\n",
    "                        samples.append(q_new)\n",
    "\n",
    "            acceptance_rate=acceptance_count/(t+1)\n",
    "            if t % 50 ==0:\n",
    "                accept_rates.append(acceptance_rate)\n",
    "                if acceptance_rate < 0.2:\n",
    "                    step_size*=0.9\n",
    "                if acceptance_rate > 0.8:\n",
    "                    step_size*=1.1\n",
    "\n",
    "            tr.set_description('HMC')        \n",
    "            tr.set_postfix(accept_rate=acceptance_rate, step=step_size)\n",
    "\n",
    "            q_last=q_new\n",
    "    return samples, accept_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device ='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def logmvnpdf(theta, mu, v=1.):\n",
    "    \"\"\"\n",
    "    Evaluation of log proba with density N(0,v*I_n)\n",
    "\n",
    "    Parameters:\n",
    "    x (Tensor): Data tensor of size NxD\n",
    "\n",
    "    Returns:\n",
    "    logproba (Tensor): size N, vector of log probabilities\n",
    "    \"\"\"\n",
    "    dim = theta.shape[1]\n",
    "    S = v*torch.ones(dim).type_as(theta).to(device)\n",
    "    n_x = theta.shape[0]\n",
    "\n",
    "    H = S.view(dim, 1, 1).inverse().view(1, 1, dim)\n",
    "    d = ((theta-mu.view(1, dim))**2).view(n_x, dim)\n",
    "    const = 0.5*S.log().sum()+0.5*dim*torch.tensor(2*math.pi).log()\n",
    "    return -0.5*(H*d).sum(2).squeeze()-const\n",
    "\n",
    "def logpdf(x):\n",
    "    mu0=torch.tensor(-4.)\n",
    "    mu1=torch.tensor(2.)\n",
    "    lp=torch.stack([logmvnpdf(x,mu0,1.),logmvnpdf(x,mu1,.5)], dim=0).logsumexp(0)-torch.tensor(2.).log()\n",
    "    return lp\n",
    "\n",
    "\n",
    "def potential(x):\n",
    "    theta=torch.Tensor(x).requires_grad_(True).float()\n",
    "    #print(x)\n",
    "    lp=logpdf(theta.unsqueeze(0))\n",
    "    lp.backward()\n",
    "    return -lp.detach().numpy(), -theta.grad.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c4cf74e5a242d6a3c21f37a712383c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "samples, accept_rates = hamiltonian_monte_carlo(20000,1000,20, potential,\n",
    "                                  initial_position=np.zeros(1), \n",
    "                                  initial_step_size=0.5,\n",
    "#                                  tune=,\n",
    "                                 #path_len=100,\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f96ae1fc1d0>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns                                 \n",
    "%matplotlib inline\n",
    "X=torch.arange(-9,9,.05).unsqueeze(1)\n",
    "\n",
    "Y=logpdf(X).exp()\n",
    "C=(Y*.05).sum()\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X,Y/C, label='target')\n",
    "sns.distplot(samples[0:-1:3], label='HMC')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from Experiments.foong import Setup\n",
    "layerwidth=50\n",
    "nblayers=1\n",
    "setup=Setup(device,layerwidth=layerwidth,nblayers=nblayers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target density #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_count=setup.param_count\n",
    "logposterior=setup.logposterior\n",
    "\n",
    "size_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20000], Loss: 50869.16796875, Learning Rate: 0.01\n",
      "Epoch [2/20000], Loss: 32397.59765625, Learning Rate: 0.01\n",
      "Epoch [3/20000], Loss: 18841.34765625, Learning Rate: 0.01\n",
      "Epoch [4/20000], Loss: 9972.7470703125, Learning Rate: 0.01\n",
      "Epoch [5/20000], Loss: 5227.96533203125, Learning Rate: 0.01\n",
      "Epoch [6/20000], Loss: 3692.835693359375, Learning Rate: 0.01\n",
      "Epoch [7/20000], Loss: 4268.6337890625, Learning Rate: 0.01\n",
      "Epoch [8/20000], Loss: 5905.0927734375, Learning Rate: 0.01\n",
      "Epoch [9/20000], Loss: 7757.56396484375, Learning Rate: 0.01\n",
      "Epoch [10/20000], Loss: 9239.658203125, Learning Rate: 0.01\n",
      "Epoch [11/20000], Loss: 10022.3134765625, Learning Rate: 0.01\n",
      "Epoch [12/20000], Loss: 10007.947265625, Learning Rate: 0.01\n",
      "Epoch [13/20000], Loss: 9281.61328125, Learning Rate: 0.01\n",
      "Epoch [14/20000], Loss: 8045.95556640625, Learning Rate: 0.01\n",
      "Epoch [15/20000], Loss: 6556.52294921875, Learning Rate: 0.01\n",
      "Epoch [16/20000], Loss: 5069.76318359375, Learning Rate: 0.01\n",
      "Epoch [17/20000], Loss: 3805.353271484375, Learning Rate: 0.01\n",
      "Epoch [18/20000], Loss: 2919.074462890625, Learning Rate: 0.01\n",
      "Epoch [19/20000], Loss: 2483.693603515625, Learning Rate: 0.01\n",
      "Epoch [20/20000], Loss: 2479.917724609375, Learning Rate: 0.01\n",
      "Epoch [21/20000], Loss: 2802.81103515625, Learning Rate: 0.01\n",
      "Epoch [22/20000], Loss: 3288.6484375, Learning Rate: 0.01\n",
      "Epoch [23/20000], Loss: 3759.58349609375, Learning Rate: 0.01\n",
      "Epoch [24/20000], Loss: 4072.04833984375, Learning Rate: 0.01\n",
      "Epoch [25/20000], Loss: 4149.755859375, Learning Rate: 0.01\n",
      "Epoch [26/20000], Loss: 3990.634765625, Learning Rate: 0.01\n",
      "Epoch [27/20000], Loss: 3650.94921875, Learning Rate: 0.01\n",
      "Epoch [28/20000], Loss: 3217.919189453125, Learning Rate: 0.01\n",
      "Epoch [29/20000], Loss: 2781.7978515625, Learning Rate: 0.01\n",
      "Epoch [30/20000], Loss: 2414.668212890625, Learning Rate: 0.01\n",
      "Epoch [31/20000], Loss: 2159.079833984375, Learning Rate: 0.01\n",
      "Epoch [32/20000], Loss: 2026.3438720703125, Learning Rate: 0.01\n",
      "Epoch [33/20000], Loss: 2001.9647216796875, Learning Rate: 0.01\n",
      "Epoch [34/20000], Loss: 2054.512451171875, Learning Rate: 0.01\n",
      "Epoch [35/20000], Loss: 2144.81591796875, Learning Rate: 0.01\n",
      "Epoch [36/20000], Loss: 2233.84033203125, Learning Rate: 0.01\n",
      "Epoch [37/20000], Loss: 2288.961669921875, Learning Rate: 0.01\n",
      "Epoch [38/20000], Loss: 2288.849365234375, Learning Rate: 0.01\n",
      "Epoch [39/20000], Loss: 2226.71923828125, Learning Rate: 0.01\n",
      "Epoch [40/20000], Loss: 2111.13232421875, Learning Rate: 0.01\n",
      "Epoch [41/20000], Loss: 1963.4183349609375, Learning Rate: 0.01\n",
      "Epoch [42/20000], Loss: 1811.702880859375, Learning Rate: 0.01\n",
      "Epoch [43/20000], Loss: 1682.892333984375, Learning Rate: 0.01\n",
      "Epoch [44/20000], Loss: 1595.113037109375, Learning Rate: 0.01\n",
      "Epoch [45/20000], Loss: 1553.182373046875, Learning Rate: 0.01\n",
      "Epoch [46/20000], Loss: 1548.644775390625, Learning Rate: 0.01\n",
      "Epoch [47/20000], Loss: 1564.12060546875, Learning Rate: 0.01\n",
      "Epoch [48/20000], Loss: 1580.0057373046875, Learning Rate: 0.01\n",
      "Epoch [49/20000], Loss: 1580.8206787109375, Learning Rate: 0.01\n",
      "Epoch [50/20000], Loss: 1558.9742431640625, Learning Rate: 0.01\n",
      "Epoch [51/20000], Loss: 1515.1859130859375, Learning Rate: 0.01\n",
      "Epoch [52/20000], Loss: 1456.226806640625, Learning Rate: 0.01\n",
      "Epoch [53/20000], Loss: 1391.51611328125, Learning Rate: 0.01\n",
      "Epoch [54/20000], Loss: 1330.010009765625, Learning Rate: 0.01\n",
      "Epoch [55/20000], Loss: 1278.2041015625, Learning Rate: 0.01\n",
      "Epoch [56/20000], Loss: 1239.291015625, Learning Rate: 0.01\n",
      "Epoch [57/20000], Loss: 1213.143798828125, Learning Rate: 0.01\n",
      "Epoch [58/20000], Loss: 1196.807373046875, Learning Rate: 0.01\n",
      "Epoch [59/20000], Loss: 1185.389404296875, Learning Rate: 0.01\n",
      "Epoch [60/20000], Loss: 1173.3516845703125, Learning Rate: 0.01\n",
      "Epoch [61/20000], Loss: 1156.0462646484375, Learning Rate: 0.01\n",
      "Epoch [62/20000], Loss: 1131.0875244140625, Learning Rate: 0.01\n",
      "Epoch [63/20000], Loss: 1099.0029296875, Learning Rate: 0.01\n",
      "Epoch [64/20000], Loss: 1062.843017578125, Learning Rate: 0.01\n",
      "Epoch [65/20000], Loss: 1026.857177734375, Learning Rate: 0.01\n",
      "Epoch [66/20000], Loss: 994.8233642578125, Learning Rate: 0.01\n",
      "Epoch [67/20000], Loss: 968.74462890625, Learning Rate: 0.01\n",
      "Epoch [68/20000], Loss: 948.4274291992188, Learning Rate: 0.01\n",
      "Epoch [69/20000], Loss: 931.976806640625, Learning Rate: 0.01\n",
      "Epoch [70/20000], Loss: 916.830322265625, Learning Rate: 0.01\n",
      "Epoch [71/20000], Loss: 900.7860717773438, Learning Rate: 0.01\n",
      "Epoch [72/20000], Loss: 882.621337890625, Learning Rate: 0.01\n",
      "Epoch [73/20000], Loss: 862.2024536132812, Learning Rate: 0.01\n",
      "Epoch [74/20000], Loss: 840.225830078125, Learning Rate: 0.01\n",
      "Epoch [75/20000], Loss: 817.8259887695312, Learning Rate: 0.01\n",
      "Epoch [76/20000], Loss: 796.1971435546875, Learning Rate: 0.01\n",
      "Epoch [77/20000], Loss: 776.28857421875, Learning Rate: 0.01\n",
      "Epoch [78/20000], Loss: 758.5784301757812, Learning Rate: 0.01\n",
      "Epoch [79/20000], Loss: 742.958251953125, Learning Rate: 0.01\n",
      "Epoch [80/20000], Loss: 728.78955078125, Learning Rate: 0.01\n",
      "Epoch [81/20000], Loss: 715.1480712890625, Learning Rate: 0.01\n",
      "Epoch [82/20000], Loss: 701.1880493164062, Learning Rate: 0.01\n",
      "Epoch [83/20000], Loss: 686.46337890625, Learning Rate: 0.01\n",
      "Epoch [84/20000], Loss: 671.0662841796875, Learning Rate: 0.01\n",
      "Epoch [85/20000], Loss: 655.510009765625, Learning Rate: 0.01\n",
      "Epoch [86/20000], Loss: 640.4458618164062, Learning Rate: 0.01\n",
      "Epoch [87/20000], Loss: 626.3657836914062, Learning Rate: 0.01\n",
      "Epoch [88/20000], Loss: 613.4346923828125, Learning Rate: 0.01\n",
      "Epoch [89/20000], Loss: 601.5015869140625, Learning Rate: 0.01\n",
      "Epoch [90/20000], Loss: 590.23193359375, Learning Rate: 0.01\n",
      "Epoch [91/20000], Loss: 579.2740478515625, Learning Rate: 0.01\n",
      "Epoch [92/20000], Loss: 568.3778076171875, Learning Rate: 0.01\n",
      "Epoch [93/20000], Loss: 557.44140625, Learning Rate: 0.01\n",
      "Epoch [94/20000], Loss: 546.5040893554688, Learning Rate: 0.01\n",
      "Epoch [95/20000], Loss: 535.7085571289062, Learning Rate: 0.01\n",
      "Epoch [96/20000], Loss: 525.244140625, Learning Rate: 0.01\n",
      "Epoch [97/20000], Loss: 515.2772216796875, Learning Rate: 0.01\n",
      "Epoch [98/20000], Loss: 505.8897705078125, Learning Rate: 0.01\n",
      "Epoch [99/20000], Loss: 497.04559326171875, Learning Rate: 0.01\n",
      "Epoch [100/20000], Loss: 488.609619140625, Learning Rate: 0.01\n",
      "Epoch [101/20000], Loss: 480.4104309082031, Learning Rate: 0.01\n",
      "Epoch [102/20000], Loss: 472.3191833496094, Learning Rate: 0.01\n",
      "Epoch [103/20000], Loss: 464.2975158691406, Learning Rate: 0.01\n",
      "Epoch [104/20000], Loss: 456.39593505859375, Learning Rate: 0.01\n",
      "Epoch [105/20000], Loss: 448.7093811035156, Learning Rate: 0.01\n",
      "Epoch [106/20000], Loss: 441.32073974609375, Learning Rate: 0.01\n",
      "Epoch [107/20000], Loss: 434.26507568359375, Learning Rate: 0.01\n",
      "Epoch [108/20000], Loss: 427.52423095703125, Learning Rate: 0.01\n",
      "Epoch [109/20000], Loss: 421.04510498046875, Learning Rate: 0.01\n",
      "Epoch [110/20000], Loss: 414.7657470703125, Learning Rate: 0.01\n",
      "Epoch [111/20000], Loss: 408.6356201171875, Learning Rate: 0.01\n",
      "Epoch [112/20000], Loss: 402.6263122558594, Learning Rate: 0.01\n",
      "Epoch [113/20000], Loss: 396.7366943359375, Learning Rate: 0.01\n",
      "Epoch [114/20000], Loss: 390.9866943359375, Learning Rate: 0.01\n",
      "Epoch [115/20000], Loss: 385.4061584472656, Learning Rate: 0.01\n",
      "Epoch [116/20000], Loss: 380.01922607421875, Learning Rate: 0.01\n",
      "Epoch [117/20000], Loss: 374.82940673828125, Learning Rate: 0.01\n",
      "Epoch [118/20000], Loss: 369.81719970703125, Learning Rate: 0.01\n",
      "Epoch [119/20000], Loss: 364.9486389160156, Learning Rate: 0.01\n",
      "Epoch [120/20000], Loss: 360.1921691894531, Learning Rate: 0.01\n",
      "Epoch [121/20000], Loss: 355.5298156738281, Learning Rate: 0.01\n",
      "Epoch [122/20000], Loss: 350.9609069824219, Learning Rate: 0.01\n",
      "Epoch [123/20000], Loss: 346.4977722167969, Learning Rate: 0.01\n",
      "Epoch [124/20000], Loss: 342.15093994140625, Learning Rate: 0.01\n",
      "Epoch [125/20000], Loss: 337.9262390136719, Learning Rate: 0.01\n",
      "Epoch [126/20000], Loss: 333.8196105957031, Learning Rate: 0.01\n",
      "Epoch [127/20000], Loss: 329.8202819824219, Learning Rate: 0.01\n",
      "Epoch [128/20000], Loss: 325.91571044921875, Learning Rate: 0.01\n",
      "Epoch [129/20000], Loss: 322.09320068359375, Learning Rate: 0.01\n",
      "Epoch [130/20000], Loss: 318.34515380859375, Learning Rate: 0.01\n",
      "Epoch [131/20000], Loss: 314.6685791015625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [132/20000], Loss: 311.0654602050781, Learning Rate: 0.01\n",
      "Epoch [133/20000], Loss: 307.54010009765625, Learning Rate: 0.01\n",
      "Epoch [134/20000], Loss: 304.09466552734375, Learning Rate: 0.01\n",
      "Epoch [135/20000], Loss: 300.7266540527344, Learning Rate: 0.01\n",
      "Epoch [136/20000], Loss: 297.43060302734375, Learning Rate: 0.01\n",
      "Epoch [137/20000], Loss: 294.1982116699219, Learning Rate: 0.01\n",
      "Epoch [138/20000], Loss: 291.0238037109375, Learning Rate: 0.01\n",
      "Epoch [139/20000], Loss: 287.9034423828125, Learning Rate: 0.01\n",
      "Epoch [140/20000], Loss: 284.8382568359375, Learning Rate: 0.01\n",
      "Epoch [141/20000], Loss: 281.8280944824219, Learning Rate: 0.01\n",
      "Epoch [142/20000], Loss: 278.8743591308594, Learning Rate: 0.01\n",
      "Epoch [143/20000], Loss: 275.9767761230469, Learning Rate: 0.01\n",
      "Epoch [144/20000], Loss: 273.13177490234375, Learning Rate: 0.01\n",
      "Epoch [145/20000], Loss: 270.3376770019531, Learning Rate: 0.01\n",
      "Epoch [146/20000], Loss: 267.59027099609375, Learning Rate: 0.01\n",
      "Epoch [147/20000], Loss: 264.88812255859375, Learning Rate: 0.01\n",
      "Epoch [148/20000], Loss: 262.22900390625, Learning Rate: 0.01\n",
      "Epoch [149/20000], Loss: 259.6126708984375, Learning Rate: 0.01\n",
      "Epoch [150/20000], Loss: 257.040283203125, Learning Rate: 0.01\n",
      "Epoch [151/20000], Loss: 254.51101684570312, Learning Rate: 0.01\n",
      "Epoch [152/20000], Loss: 252.02459716796875, Learning Rate: 0.01\n",
      "Epoch [153/20000], Loss: 249.57888793945312, Learning Rate: 0.01\n",
      "Epoch [154/20000], Loss: 247.17276000976562, Learning Rate: 0.01\n",
      "Epoch [155/20000], Loss: 244.80419921875, Learning Rate: 0.01\n",
      "Epoch [156/20000], Loss: 242.4724578857422, Learning Rate: 0.01\n",
      "Epoch [157/20000], Loss: 240.1770782470703, Learning Rate: 0.01\n",
      "Epoch [158/20000], Loss: 237.91812133789062, Learning Rate: 0.01\n",
      "Epoch [159/20000], Loss: 235.69554138183594, Learning Rate: 0.01\n",
      "Epoch [160/20000], Loss: 233.5086212158203, Learning Rate: 0.01\n",
      "Epoch [161/20000], Loss: 231.35655212402344, Learning Rate: 0.01\n",
      "Epoch [162/20000], Loss: 229.2388153076172, Learning Rate: 0.01\n",
      "Epoch [163/20000], Loss: 227.15432739257812, Learning Rate: 0.01\n",
      "Epoch [164/20000], Loss: 225.1023406982422, Learning Rate: 0.01\n",
      "Epoch [165/20000], Loss: 223.0825958251953, Learning Rate: 0.01\n",
      "Epoch [166/20000], Loss: 221.0945281982422, Learning Rate: 0.01\n",
      "Epoch [167/20000], Loss: 219.13858032226562, Learning Rate: 0.01\n",
      "Epoch [168/20000], Loss: 217.21392822265625, Learning Rate: 0.01\n",
      "Epoch [169/20000], Loss: 215.32017517089844, Learning Rate: 0.01\n",
      "Epoch [170/20000], Loss: 213.45700073242188, Learning Rate: 0.01\n",
      "Epoch [171/20000], Loss: 211.62350463867188, Learning Rate: 0.01\n",
      "Epoch [172/20000], Loss: 209.8192596435547, Learning Rate: 0.01\n",
      "Epoch [173/20000], Loss: 208.04428100585938, Learning Rate: 0.01\n",
      "Epoch [174/20000], Loss: 206.298095703125, Learning Rate: 0.01\n",
      "Epoch [175/20000], Loss: 204.5803985595703, Learning Rate: 0.01\n",
      "Epoch [176/20000], Loss: 202.8907012939453, Learning Rate: 0.01\n",
      "Epoch [177/20000], Loss: 201.22894287109375, Learning Rate: 0.01\n",
      "Epoch [178/20000], Loss: 199.5947265625, Learning Rate: 0.01\n",
      "Epoch [179/20000], Loss: 197.98731994628906, Learning Rate: 0.01\n",
      "Epoch [180/20000], Loss: 196.40652465820312, Learning Rate: 0.01\n",
      "Epoch [181/20000], Loss: 194.85214233398438, Learning Rate: 0.01\n",
      "Epoch [182/20000], Loss: 193.32376098632812, Learning Rate: 0.01\n",
      "Epoch [183/20000], Loss: 191.82098388671875, Learning Rate: 0.01\n",
      "Epoch [184/20000], Loss: 190.3438720703125, Learning Rate: 0.01\n",
      "Epoch [185/20000], Loss: 188.8917236328125, Learning Rate: 0.01\n",
      "Epoch [186/20000], Loss: 187.46420288085938, Learning Rate: 0.01\n",
      "Epoch [187/20000], Loss: 186.06103515625, Learning Rate: 0.01\n",
      "Epoch [188/20000], Loss: 184.6820068359375, Learning Rate: 0.01\n",
      "Epoch [189/20000], Loss: 183.32659912109375, Learning Rate: 0.01\n",
      "Epoch [190/20000], Loss: 181.9948272705078, Learning Rate: 0.01\n",
      "Epoch [191/20000], Loss: 180.68589782714844, Learning Rate: 0.01\n",
      "Epoch [192/20000], Loss: 179.4000244140625, Learning Rate: 0.01\n",
      "Epoch [193/20000], Loss: 178.13668823242188, Learning Rate: 0.01\n",
      "Epoch [194/20000], Loss: 176.8951873779297, Learning Rate: 0.01\n",
      "Epoch [195/20000], Loss: 175.67579650878906, Learning Rate: 0.01\n",
      "Epoch [196/20000], Loss: 174.4779052734375, Learning Rate: 0.01\n",
      "Epoch [197/20000], Loss: 173.30117797851562, Learning Rate: 0.01\n",
      "Epoch [198/20000], Loss: 172.14556884765625, Learning Rate: 0.01\n",
      "Epoch [199/20000], Loss: 171.0104217529297, Learning Rate: 0.01\n",
      "Epoch [200/20000], Loss: 169.895751953125, Learning Rate: 0.01\n",
      "Epoch [201/20000], Loss: 168.80096435546875, Learning Rate: 0.01\n",
      "Epoch [202/20000], Loss: 167.72592163085938, Learning Rate: 0.01\n",
      "Epoch [203/20000], Loss: 166.6702880859375, Learning Rate: 0.01\n",
      "Epoch [204/20000], Loss: 165.63372802734375, Learning Rate: 0.01\n",
      "Epoch [205/20000], Loss: 164.6159210205078, Learning Rate: 0.01\n",
      "Epoch [206/20000], Loss: 163.61691284179688, Learning Rate: 0.01\n",
      "Epoch [207/20000], Loss: 162.63583374023438, Learning Rate: 0.01\n",
      "Epoch [208/20000], Loss: 161.6729736328125, Learning Rate: 0.01\n",
      "Epoch [209/20000], Loss: 160.72747802734375, Learning Rate: 0.01\n",
      "Epoch [210/20000], Loss: 159.7996368408203, Learning Rate: 0.01\n",
      "Epoch [211/20000], Loss: 158.88864135742188, Learning Rate: 0.01\n",
      "Epoch [212/20000], Loss: 157.99459838867188, Learning Rate: 0.01\n",
      "Epoch [213/20000], Loss: 157.1171112060547, Learning Rate: 0.01\n",
      "Epoch [214/20000], Loss: 156.25558471679688, Learning Rate: 0.01\n",
      "Epoch [215/20000], Loss: 155.41049194335938, Learning Rate: 0.01\n",
      "Epoch [216/20000], Loss: 154.58082580566406, Learning Rate: 0.01\n",
      "Epoch [217/20000], Loss: 153.766845703125, Learning Rate: 0.01\n",
      "Epoch [218/20000], Loss: 152.96778869628906, Learning Rate: 0.01\n",
      "Epoch [219/20000], Loss: 152.18359375, Learning Rate: 0.01\n",
      "Epoch [220/20000], Loss: 151.41409301757812, Learning Rate: 0.01\n",
      "Epoch [221/20000], Loss: 150.6591796875, Learning Rate: 0.01\n",
      "Epoch [222/20000], Loss: 149.91810607910156, Learning Rate: 0.01\n",
      "Epoch [223/20000], Loss: 149.1910858154297, Learning Rate: 0.01\n",
      "Epoch [224/20000], Loss: 148.47763061523438, Learning Rate: 0.01\n",
      "Epoch [225/20000], Loss: 147.77752685546875, Learning Rate: 0.01\n",
      "Epoch [226/20000], Loss: 147.09066772460938, Learning Rate: 0.01\n",
      "Epoch [227/20000], Loss: 146.41650390625, Learning Rate: 0.01\n",
      "Epoch [228/20000], Loss: 145.7550811767578, Learning Rate: 0.01\n",
      "Epoch [229/20000], Loss: 145.10629272460938, Learning Rate: 0.01\n",
      "Epoch [230/20000], Loss: 144.4695281982422, Learning Rate: 0.01\n",
      "Epoch [231/20000], Loss: 143.84478759765625, Learning Rate: 0.01\n",
      "Epoch [232/20000], Loss: 143.23153686523438, Learning Rate: 0.01\n",
      "Epoch [233/20000], Loss: 142.63006591796875, Learning Rate: 0.01\n",
      "Epoch [234/20000], Loss: 142.03994750976562, Learning Rate: 0.01\n",
      "Epoch [235/20000], Loss: 141.46055603027344, Learning Rate: 0.01\n",
      "Epoch [236/20000], Loss: 140.8922119140625, Learning Rate: 0.01\n",
      "Epoch [237/20000], Loss: 140.33447265625, Learning Rate: 0.01\n",
      "Epoch [238/20000], Loss: 139.78726196289062, Learning Rate: 0.01\n",
      "Epoch [239/20000], Loss: 139.25006103515625, Learning Rate: 0.01\n",
      "Epoch [240/20000], Loss: 138.722900390625, Learning Rate: 0.01\n",
      "Epoch [241/20000], Loss: 138.2054901123047, Learning Rate: 0.01\n",
      "Epoch [242/20000], Loss: 137.69805908203125, Learning Rate: 0.01\n",
      "Epoch [243/20000], Loss: 137.19973754882812, Learning Rate: 0.01\n",
      "Epoch [244/20000], Loss: 136.71078491210938, Learning Rate: 0.01\n",
      "Epoch [245/20000], Loss: 136.2307586669922, Learning Rate: 0.01\n",
      "Epoch [246/20000], Loss: 135.75967407226562, Learning Rate: 0.01\n",
      "Epoch [247/20000], Loss: 135.29739379882812, Learning Rate: 0.01\n",
      "Epoch [248/20000], Loss: 134.84326171875, Learning Rate: 0.01\n",
      "Epoch [249/20000], Loss: 134.3976593017578, Learning Rate: 0.01\n",
      "Epoch [250/20000], Loss: 133.9600830078125, Learning Rate: 0.01\n",
      "Epoch [251/20000], Loss: 133.5306396484375, Learning Rate: 0.01\n",
      "Epoch [252/20000], Loss: 133.10874938964844, Learning Rate: 0.01\n",
      "Epoch [253/20000], Loss: 132.69464111328125, Learning Rate: 0.01\n",
      "Epoch [254/20000], Loss: 132.28794860839844, Learning Rate: 0.01\n",
      "Epoch [255/20000], Loss: 131.888427734375, Learning Rate: 0.01\n",
      "Epoch [256/20000], Loss: 131.49630737304688, Learning Rate: 0.01\n",
      "Epoch [257/20000], Loss: 131.1107635498047, Learning Rate: 0.01\n",
      "Epoch [258/20000], Loss: 130.73220825195312, Learning Rate: 0.01\n",
      "Epoch [259/20000], Loss: 130.36041259765625, Learning Rate: 0.01\n",
      "Epoch [260/20000], Loss: 129.99517822265625, Learning Rate: 0.01\n",
      "Epoch [261/20000], Loss: 129.63626098632812, Learning Rate: 0.01\n",
      "Epoch [262/20000], Loss: 129.2836151123047, Learning Rate: 0.01\n",
      "Epoch [263/20000], Loss: 128.93710327148438, Learning Rate: 0.01\n",
      "Epoch [264/20000], Loss: 128.59628295898438, Learning Rate: 0.01\n",
      "Epoch [265/20000], Loss: 128.261474609375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [266/20000], Loss: 127.93229675292969, Learning Rate: 0.01\n",
      "Epoch [267/20000], Loss: 127.60874938964844, Learning Rate: 0.01\n",
      "Epoch [268/20000], Loss: 127.29045867919922, Learning Rate: 0.01\n",
      "Epoch [269/20000], Loss: 126.9774398803711, Learning Rate: 0.01\n",
      "Epoch [270/20000], Loss: 126.66979217529297, Learning Rate: 0.01\n",
      "Epoch [271/20000], Loss: 126.36711883544922, Learning Rate: 0.01\n",
      "Epoch [272/20000], Loss: 126.0693359375, Learning Rate: 0.01\n",
      "Epoch [273/20000], Loss: 125.77643585205078, Learning Rate: 0.01\n",
      "Epoch [274/20000], Loss: 125.48799896240234, Learning Rate: 0.01\n",
      "Epoch [275/20000], Loss: 125.20416259765625, Learning Rate: 0.01\n",
      "Epoch [276/20000], Loss: 124.9250259399414, Learning Rate: 0.01\n",
      "Epoch [277/20000], Loss: 124.65011596679688, Learning Rate: 0.01\n",
      "Epoch [278/20000], Loss: 124.37948608398438, Learning Rate: 0.01\n",
      "Epoch [279/20000], Loss: 124.11273193359375, Learning Rate: 0.01\n",
      "Epoch [280/20000], Loss: 123.85037994384766, Learning Rate: 0.01\n",
      "Epoch [281/20000], Loss: 123.59181213378906, Learning Rate: 0.01\n",
      "Epoch [282/20000], Loss: 123.33717346191406, Learning Rate: 0.01\n",
      "Epoch [283/20000], Loss: 123.08619689941406, Learning Rate: 0.01\n",
      "Epoch [284/20000], Loss: 122.83868408203125, Learning Rate: 0.01\n",
      "Epoch [285/20000], Loss: 122.59510040283203, Learning Rate: 0.01\n",
      "Epoch [286/20000], Loss: 122.35476684570312, Learning Rate: 0.01\n",
      "Epoch [287/20000], Loss: 122.1176528930664, Learning Rate: 0.01\n",
      "Epoch [288/20000], Loss: 121.8841552734375, Learning Rate: 0.01\n",
      "Epoch [289/20000], Loss: 121.65342712402344, Learning Rate: 0.01\n",
      "Epoch [290/20000], Loss: 121.42633819580078, Learning Rate: 0.01\n",
      "Epoch [291/20000], Loss: 121.20181274414062, Learning Rate: 0.01\n",
      "Epoch [292/20000], Loss: 120.98052215576172, Learning Rate: 0.01\n",
      "Epoch [293/20000], Loss: 120.7619857788086, Learning Rate: 0.01\n",
      "Epoch [294/20000], Loss: 120.5461654663086, Learning Rate: 0.01\n",
      "Epoch [295/20000], Loss: 120.33316040039062, Learning Rate: 0.01\n",
      "Epoch [296/20000], Loss: 120.12284851074219, Learning Rate: 0.01\n",
      "Epoch [297/20000], Loss: 119.91493225097656, Learning Rate: 0.01\n",
      "Epoch [298/20000], Loss: 119.70951843261719, Learning Rate: 0.01\n",
      "Epoch [299/20000], Loss: 119.50657653808594, Learning Rate: 0.01\n",
      "Epoch [300/20000], Loss: 119.30598449707031, Learning Rate: 0.01\n",
      "Epoch [301/20000], Loss: 119.10798645019531, Learning Rate: 0.01\n",
      "Epoch [302/20000], Loss: 118.91188049316406, Learning Rate: 0.01\n",
      "Epoch [303/20000], Loss: 118.71792602539062, Learning Rate: 0.01\n",
      "Epoch [304/20000], Loss: 118.5262222290039, Learning Rate: 0.01\n",
      "Epoch [305/20000], Loss: 118.33634948730469, Learning Rate: 0.01\n",
      "Epoch [306/20000], Loss: 118.14845275878906, Learning Rate: 0.01\n",
      "Epoch [307/20000], Loss: 117.96257019042969, Learning Rate: 0.01\n",
      "Epoch [308/20000], Loss: 117.7786865234375, Learning Rate: 0.01\n",
      "Epoch [309/20000], Loss: 117.59638977050781, Learning Rate: 0.01\n",
      "Epoch [310/20000], Loss: 117.41587829589844, Learning Rate: 0.01\n",
      "Epoch [311/20000], Loss: 117.23711395263672, Learning Rate: 0.01\n",
      "Epoch [312/20000], Loss: 117.05992126464844, Learning Rate: 0.01\n",
      "Epoch [313/20000], Loss: 116.88458251953125, Learning Rate: 0.01\n",
      "Epoch [314/20000], Loss: 116.7105712890625, Learning Rate: 0.01\n",
      "Epoch [315/20000], Loss: 116.53804016113281, Learning Rate: 0.01\n",
      "Epoch [316/20000], Loss: 116.36675262451172, Learning Rate: 0.01\n",
      "Epoch [317/20000], Loss: 116.19725036621094, Learning Rate: 0.01\n",
      "Epoch [318/20000], Loss: 116.02897644042969, Learning Rate: 0.01\n",
      "Epoch [319/20000], Loss: 115.86175537109375, Learning Rate: 0.01\n",
      "Epoch [320/20000], Loss: 115.69596862792969, Learning Rate: 0.01\n",
      "Epoch [321/20000], Loss: 115.53137969970703, Learning Rate: 0.01\n",
      "Epoch [322/20000], Loss: 115.36811065673828, Learning Rate: 0.01\n",
      "Epoch [323/20000], Loss: 115.20584869384766, Learning Rate: 0.01\n",
      "Epoch [324/20000], Loss: 115.04470825195312, Learning Rate: 0.01\n",
      "Epoch [325/20000], Loss: 114.88458251953125, Learning Rate: 0.01\n",
      "Epoch [326/20000], Loss: 114.7253646850586, Learning Rate: 0.01\n",
      "Epoch [327/20000], Loss: 114.56732177734375, Learning Rate: 0.01\n",
      "Epoch [328/20000], Loss: 114.4101791381836, Learning Rate: 0.01\n",
      "Epoch [329/20000], Loss: 114.25387573242188, Learning Rate: 0.01\n",
      "Epoch [330/20000], Loss: 114.09855651855469, Learning Rate: 0.01\n",
      "Epoch [331/20000], Loss: 113.9439926147461, Learning Rate: 0.01\n",
      "Epoch [332/20000], Loss: 113.7901840209961, Learning Rate: 0.01\n",
      "Epoch [333/20000], Loss: 113.63714599609375, Learning Rate: 0.01\n",
      "Epoch [334/20000], Loss: 113.48489379882812, Learning Rate: 0.01\n",
      "Epoch [335/20000], Loss: 113.33332061767578, Learning Rate: 0.01\n",
      "Epoch [336/20000], Loss: 113.18231964111328, Learning Rate: 0.01\n",
      "Epoch [337/20000], Loss: 113.03225708007812, Learning Rate: 0.01\n",
      "Epoch [338/20000], Loss: 112.88264465332031, Learning Rate: 0.01\n",
      "Epoch [339/20000], Loss: 112.7336654663086, Learning Rate: 0.01\n",
      "Epoch [340/20000], Loss: 112.58515167236328, Learning Rate: 0.01\n",
      "Epoch [341/20000], Loss: 112.43702697753906, Learning Rate: 0.01\n",
      "Epoch [342/20000], Loss: 112.28974151611328, Learning Rate: 0.01\n",
      "Epoch [343/20000], Loss: 112.14273834228516, Learning Rate: 0.01\n",
      "Epoch [344/20000], Loss: 111.99618530273438, Learning Rate: 0.01\n",
      "Epoch [345/20000], Loss: 111.85014343261719, Learning Rate: 0.01\n",
      "Epoch [346/20000], Loss: 111.70452117919922, Learning Rate: 0.01\n",
      "Epoch [347/20000], Loss: 111.55916595458984, Learning Rate: 0.01\n",
      "Epoch [348/20000], Loss: 111.41412353515625, Learning Rate: 0.01\n",
      "Epoch [349/20000], Loss: 111.26954650878906, Learning Rate: 0.01\n",
      "Epoch [350/20000], Loss: 111.12527465820312, Learning Rate: 0.01\n",
      "Epoch [351/20000], Loss: 110.98130798339844, Learning Rate: 0.01\n",
      "Epoch [352/20000], Loss: 110.83759307861328, Learning Rate: 0.01\n",
      "Epoch [353/20000], Loss: 110.69391632080078, Learning Rate: 0.01\n",
      "Epoch [354/20000], Loss: 110.55068969726562, Learning Rate: 0.01\n",
      "Epoch [355/20000], Loss: 110.40777587890625, Learning Rate: 0.01\n",
      "Epoch [356/20000], Loss: 110.26478576660156, Learning Rate: 0.01\n",
      "Epoch [357/20000], Loss: 110.12201690673828, Learning Rate: 0.01\n",
      "Epoch [358/20000], Loss: 109.97956848144531, Learning Rate: 0.01\n",
      "Epoch [359/20000], Loss: 109.83711242675781, Learning Rate: 0.01\n",
      "Epoch [360/20000], Loss: 109.6948471069336, Learning Rate: 0.01\n",
      "Epoch [361/20000], Loss: 109.5525894165039, Learning Rate: 0.01\n",
      "Epoch [362/20000], Loss: 109.41039276123047, Learning Rate: 0.01\n",
      "Epoch [363/20000], Loss: 109.26831817626953, Learning Rate: 0.01\n",
      "Epoch [364/20000], Loss: 109.12632751464844, Learning Rate: 0.01\n",
      "Epoch [365/20000], Loss: 108.98424530029297, Learning Rate: 0.01\n",
      "Epoch [366/20000], Loss: 108.84236907958984, Learning Rate: 0.01\n",
      "Epoch [367/20000], Loss: 108.70030212402344, Learning Rate: 0.01\n",
      "Epoch [368/20000], Loss: 108.55831146240234, Learning Rate: 0.01\n",
      "Epoch [369/20000], Loss: 108.41619873046875, Learning Rate: 0.01\n",
      "Epoch [370/20000], Loss: 108.27421569824219, Learning Rate: 0.01\n",
      "Epoch [371/20000], Loss: 108.13201904296875, Learning Rate: 0.01\n",
      "Epoch [372/20000], Loss: 107.9898910522461, Learning Rate: 0.01\n",
      "Epoch [373/20000], Loss: 107.84754943847656, Learning Rate: 0.01\n",
      "Epoch [374/20000], Loss: 107.7051010131836, Learning Rate: 0.01\n",
      "Epoch [375/20000], Loss: 107.56241607666016, Learning Rate: 0.01\n",
      "Epoch [376/20000], Loss: 107.41999053955078, Learning Rate: 0.01\n",
      "Epoch [377/20000], Loss: 107.27721405029297, Learning Rate: 0.01\n",
      "Epoch [378/20000], Loss: 107.13428497314453, Learning Rate: 0.01\n",
      "Epoch [379/20000], Loss: 106.99108123779297, Learning Rate: 0.01\n",
      "Epoch [380/20000], Loss: 106.84776306152344, Learning Rate: 0.01\n",
      "Epoch [381/20000], Loss: 106.70443725585938, Learning Rate: 0.01\n",
      "Epoch [382/20000], Loss: 106.560546875, Learning Rate: 0.01\n",
      "Epoch [383/20000], Loss: 106.41671752929688, Learning Rate: 0.01\n",
      "Epoch [384/20000], Loss: 106.27256774902344, Learning Rate: 0.01\n",
      "Epoch [385/20000], Loss: 106.1282958984375, Learning Rate: 0.01\n",
      "Epoch [386/20000], Loss: 105.98390197753906, Learning Rate: 0.01\n",
      "Epoch [387/20000], Loss: 105.83891296386719, Learning Rate: 0.01\n",
      "Epoch [388/20000], Loss: 105.69403839111328, Learning Rate: 0.01\n",
      "Epoch [389/20000], Loss: 105.54863739013672, Learning Rate: 0.01\n",
      "Epoch [390/20000], Loss: 105.40294647216797, Learning Rate: 0.01\n",
      "Epoch [391/20000], Loss: 105.25719451904297, Learning Rate: 0.01\n",
      "Epoch [392/20000], Loss: 105.11088562011719, Learning Rate: 0.01\n",
      "Epoch [393/20000], Loss: 104.96422576904297, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [394/20000], Loss: 104.8176040649414, Learning Rate: 0.01\n",
      "Epoch [395/20000], Loss: 104.6704330444336, Learning Rate: 0.01\n",
      "Epoch [396/20000], Loss: 104.52310180664062, Learning Rate: 0.01\n",
      "Epoch [397/20000], Loss: 104.37529754638672, Learning Rate: 0.01\n",
      "Epoch [398/20000], Loss: 104.22722625732422, Learning Rate: 0.01\n",
      "Epoch [399/20000], Loss: 104.07866668701172, Learning Rate: 0.01\n",
      "Epoch [400/20000], Loss: 103.92991638183594, Learning Rate: 0.01\n",
      "Epoch [401/20000], Loss: 103.78070831298828, Learning Rate: 0.01\n",
      "Epoch [402/20000], Loss: 103.6312255859375, Learning Rate: 0.01\n",
      "Epoch [403/20000], Loss: 103.48129272460938, Learning Rate: 0.01\n",
      "Epoch [404/20000], Loss: 103.33092498779297, Learning Rate: 0.01\n",
      "Epoch [405/20000], Loss: 103.18030548095703, Learning Rate: 0.01\n",
      "Epoch [406/20000], Loss: 103.02931213378906, Learning Rate: 0.01\n",
      "Epoch [407/20000], Loss: 102.87794494628906, Learning Rate: 0.01\n",
      "Epoch [408/20000], Loss: 102.7259292602539, Learning Rate: 0.01\n",
      "Epoch [409/20000], Loss: 102.57357025146484, Learning Rate: 0.01\n",
      "Epoch [410/20000], Loss: 102.42121887207031, Learning Rate: 0.01\n",
      "Epoch [411/20000], Loss: 102.26807403564453, Learning Rate: 0.01\n",
      "Epoch [412/20000], Loss: 102.11474609375, Learning Rate: 0.01\n",
      "Epoch [413/20000], Loss: 101.96047973632812, Learning Rate: 0.01\n",
      "Epoch [414/20000], Loss: 101.80648040771484, Learning Rate: 0.01\n",
      "Epoch [415/20000], Loss: 101.65186309814453, Learning Rate: 0.01\n",
      "Epoch [416/20000], Loss: 101.49676513671875, Learning Rate: 0.01\n",
      "Epoch [417/20000], Loss: 101.34107971191406, Learning Rate: 0.01\n",
      "Epoch [418/20000], Loss: 101.18521118164062, Learning Rate: 0.01\n",
      "Epoch [419/20000], Loss: 101.02890014648438, Learning Rate: 0.01\n",
      "Epoch [420/20000], Loss: 100.87193298339844, Learning Rate: 0.01\n",
      "Epoch [421/20000], Loss: 100.71476745605469, Learning Rate: 0.01\n",
      "Epoch [422/20000], Loss: 100.55728912353516, Learning Rate: 0.01\n",
      "Epoch [423/20000], Loss: 100.39923095703125, Learning Rate: 0.01\n",
      "Epoch [424/20000], Loss: 100.24081420898438, Learning Rate: 0.01\n",
      "Epoch [425/20000], Loss: 100.0819091796875, Learning Rate: 0.01\n",
      "Epoch [426/20000], Loss: 99.92281341552734, Learning Rate: 0.01\n",
      "Epoch [427/20000], Loss: 99.76301574707031, Learning Rate: 0.01\n",
      "Epoch [428/20000], Loss: 99.6030502319336, Learning Rate: 0.01\n",
      "Epoch [429/20000], Loss: 99.44249725341797, Learning Rate: 0.01\n",
      "Epoch [430/20000], Loss: 99.28156280517578, Learning Rate: 0.01\n",
      "Epoch [431/20000], Loss: 99.1204833984375, Learning Rate: 0.01\n",
      "Epoch [432/20000], Loss: 98.95893859863281, Learning Rate: 0.01\n",
      "Epoch [433/20000], Loss: 98.79674530029297, Learning Rate: 0.01\n",
      "Epoch [434/20000], Loss: 98.63445281982422, Learning Rate: 0.01\n",
      "Epoch [435/20000], Loss: 98.47171783447266, Learning Rate: 0.01\n",
      "Epoch [436/20000], Loss: 98.30839538574219, Learning Rate: 0.01\n",
      "Epoch [437/20000], Loss: 98.14515686035156, Learning Rate: 0.01\n",
      "Epoch [438/20000], Loss: 97.98138427734375, Learning Rate: 0.01\n",
      "Epoch [439/20000], Loss: 97.81720733642578, Learning Rate: 0.01\n",
      "Epoch [440/20000], Loss: 97.65283203125, Learning Rate: 0.01\n",
      "Epoch [441/20000], Loss: 97.48808288574219, Learning Rate: 0.01\n",
      "Epoch [442/20000], Loss: 97.32308959960938, Learning Rate: 0.01\n",
      "Epoch [443/20000], Loss: 97.15791320800781, Learning Rate: 0.01\n",
      "Epoch [444/20000], Loss: 96.99230194091797, Learning Rate: 0.01\n",
      "Epoch [445/20000], Loss: 96.82635498046875, Learning Rate: 0.01\n",
      "Epoch [446/20000], Loss: 96.66022491455078, Learning Rate: 0.01\n",
      "Epoch [447/20000], Loss: 96.49397277832031, Learning Rate: 0.01\n",
      "Epoch [448/20000], Loss: 96.32756042480469, Learning Rate: 0.01\n",
      "Epoch [449/20000], Loss: 96.16071319580078, Learning Rate: 0.01\n",
      "Epoch [450/20000], Loss: 95.99383544921875, Learning Rate: 0.01\n",
      "Epoch [451/20000], Loss: 95.82677459716797, Learning Rate: 0.01\n",
      "Epoch [452/20000], Loss: 95.65951538085938, Learning Rate: 0.01\n",
      "Epoch [453/20000], Loss: 95.49212646484375, Learning Rate: 0.01\n",
      "Epoch [454/20000], Loss: 95.32454681396484, Learning Rate: 0.01\n",
      "Epoch [455/20000], Loss: 95.15702056884766, Learning Rate: 0.01\n",
      "Epoch [456/20000], Loss: 94.9893798828125, Learning Rate: 0.01\n",
      "Epoch [457/20000], Loss: 94.82171630859375, Learning Rate: 0.01\n",
      "Epoch [458/20000], Loss: 94.65402221679688, Learning Rate: 0.01\n",
      "Epoch [459/20000], Loss: 94.48628997802734, Learning Rate: 0.01\n",
      "Epoch [460/20000], Loss: 94.31847381591797, Learning Rate: 0.01\n",
      "Epoch [461/20000], Loss: 94.15087127685547, Learning Rate: 0.01\n",
      "Epoch [462/20000], Loss: 93.98332214355469, Learning Rate: 0.01\n",
      "Epoch [463/20000], Loss: 93.81568908691406, Learning Rate: 0.01\n",
      "Epoch [464/20000], Loss: 93.64827728271484, Learning Rate: 0.01\n",
      "Epoch [465/20000], Loss: 93.48087310791016, Learning Rate: 0.01\n",
      "Epoch [466/20000], Loss: 93.31358337402344, Learning Rate: 0.01\n",
      "Epoch [467/20000], Loss: 93.14662170410156, Learning Rate: 0.01\n",
      "Epoch [468/20000], Loss: 92.9797592163086, Learning Rate: 0.01\n",
      "Epoch [469/20000], Loss: 92.8131332397461, Learning Rate: 0.01\n",
      "Epoch [470/20000], Loss: 92.64691162109375, Learning Rate: 0.01\n",
      "Epoch [471/20000], Loss: 92.48089599609375, Learning Rate: 0.01\n",
      "Epoch [472/20000], Loss: 92.31523895263672, Learning Rate: 0.01\n",
      "Epoch [473/20000], Loss: 92.14977264404297, Learning Rate: 0.01\n",
      "Epoch [474/20000], Loss: 91.98469543457031, Learning Rate: 0.01\n",
      "Epoch [475/20000], Loss: 91.81999206542969, Learning Rate: 0.01\n",
      "Epoch [476/20000], Loss: 91.6558609008789, Learning Rate: 0.01\n",
      "Epoch [477/20000], Loss: 91.4921875, Learning Rate: 0.01\n",
      "Epoch [478/20000], Loss: 91.3287582397461, Learning Rate: 0.01\n",
      "Epoch [479/20000], Loss: 91.16584014892578, Learning Rate: 0.01\n",
      "Epoch [480/20000], Loss: 91.00350189208984, Learning Rate: 0.01\n",
      "Epoch [481/20000], Loss: 90.84178924560547, Learning Rate: 0.01\n",
      "Epoch [482/20000], Loss: 90.68052673339844, Learning Rate: 0.01\n",
      "Epoch [483/20000], Loss: 90.51994323730469, Learning Rate: 0.01\n",
      "Epoch [484/20000], Loss: 90.36012268066406, Learning Rate: 0.01\n",
      "Epoch [485/20000], Loss: 90.20077514648438, Learning Rate: 0.01\n",
      "Epoch [486/20000], Loss: 90.04203796386719, Learning Rate: 0.01\n",
      "Epoch [487/20000], Loss: 89.8841552734375, Learning Rate: 0.01\n",
      "Epoch [488/20000], Loss: 89.72696685791016, Learning Rate: 0.01\n",
      "Epoch [489/20000], Loss: 89.57042694091797, Learning Rate: 0.01\n",
      "Epoch [490/20000], Loss: 89.41497039794922, Learning Rate: 0.01\n",
      "Epoch [491/20000], Loss: 89.2600326538086, Learning Rate: 0.01\n",
      "Epoch [492/20000], Loss: 89.10597229003906, Learning Rate: 0.01\n",
      "Epoch [493/20000], Loss: 88.95286560058594, Learning Rate: 0.01\n",
      "Epoch [494/20000], Loss: 88.80036163330078, Learning Rate: 0.01\n",
      "Epoch [495/20000], Loss: 88.64903259277344, Learning Rate: 0.01\n",
      "Epoch [496/20000], Loss: 88.49832916259766, Learning Rate: 0.01\n",
      "Epoch [497/20000], Loss: 88.34877014160156, Learning Rate: 0.01\n",
      "Epoch [498/20000], Loss: 88.20020294189453, Learning Rate: 0.01\n",
      "Epoch [499/20000], Loss: 88.05237579345703, Learning Rate: 0.01\n",
      "Epoch [500/20000], Loss: 87.9055404663086, Learning Rate: 0.01\n",
      "Epoch [501/20000], Loss: 87.75990295410156, Learning Rate: 0.01\n",
      "Epoch [502/20000], Loss: 87.61517333984375, Learning Rate: 0.01\n",
      "Epoch [503/20000], Loss: 87.47132110595703, Learning Rate: 0.01\n",
      "Epoch [504/20000], Loss: 87.32889556884766, Learning Rate: 0.01\n",
      "Epoch [505/20000], Loss: 87.18706512451172, Learning Rate: 0.01\n",
      "Epoch [506/20000], Loss: 87.04652404785156, Learning Rate: 0.01\n",
      "Epoch [507/20000], Loss: 86.90702056884766, Learning Rate: 0.01\n",
      "Epoch [508/20000], Loss: 86.76873016357422, Learning Rate: 0.01\n",
      "Epoch [509/20000], Loss: 86.63133239746094, Learning Rate: 0.01\n",
      "Epoch [510/20000], Loss: 86.49513244628906, Learning Rate: 0.01\n",
      "Epoch [511/20000], Loss: 86.36011505126953, Learning Rate: 0.01\n",
      "Epoch [512/20000], Loss: 86.22612762451172, Learning Rate: 0.01\n",
      "Epoch [513/20000], Loss: 86.09337615966797, Learning Rate: 0.01\n",
      "Epoch [514/20000], Loss: 85.96186828613281, Learning Rate: 0.01\n",
      "Epoch [515/20000], Loss: 85.83116149902344, Learning Rate: 0.01\n",
      "Epoch [516/20000], Loss: 85.7018814086914, Learning Rate: 0.01\n",
      "Epoch [517/20000], Loss: 85.5736312866211, Learning Rate: 0.01\n",
      "Epoch [518/20000], Loss: 85.44645690917969, Learning Rate: 0.01\n",
      "Epoch [519/20000], Loss: 85.32069396972656, Learning Rate: 0.01\n",
      "Epoch [520/20000], Loss: 85.19593048095703, Learning Rate: 0.01\n",
      "Epoch [521/20000], Loss: 85.07223510742188, Learning Rate: 0.01\n",
      "Epoch [522/20000], Loss: 84.94982147216797, Learning Rate: 0.01\n",
      "Epoch [523/20000], Loss: 84.82836151123047, Learning Rate: 0.01\n",
      "Epoch [524/20000], Loss: 84.70832824707031, Learning Rate: 0.01\n",
      "Epoch [525/20000], Loss: 84.58927917480469, Learning Rate: 0.01\n",
      "Epoch [526/20000], Loss: 84.47135925292969, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [527/20000], Loss: 84.3548583984375, Learning Rate: 0.01\n",
      "Epoch [528/20000], Loss: 84.23926544189453, Learning Rate: 0.01\n",
      "Epoch [529/20000], Loss: 84.1247329711914, Learning Rate: 0.01\n",
      "Epoch [530/20000], Loss: 84.01161193847656, Learning Rate: 0.01\n",
      "Epoch [531/20000], Loss: 83.8995361328125, Learning Rate: 0.01\n",
      "Epoch [532/20000], Loss: 83.78842163085938, Learning Rate: 0.01\n",
      "Epoch [533/20000], Loss: 83.67855834960938, Learning Rate: 0.01\n",
      "Epoch [534/20000], Loss: 83.56978607177734, Learning Rate: 0.01\n",
      "Epoch [535/20000], Loss: 83.46211242675781, Learning Rate: 0.01\n",
      "Epoch [536/20000], Loss: 83.3555908203125, Learning Rate: 0.01\n",
      "Epoch [537/20000], Loss: 83.25020599365234, Learning Rate: 0.01\n",
      "Epoch [538/20000], Loss: 83.14559936523438, Learning Rate: 0.01\n",
      "Epoch [539/20000], Loss: 83.04228210449219, Learning Rate: 0.01\n",
      "Epoch [540/20000], Loss: 82.9399642944336, Learning Rate: 0.01\n",
      "Epoch [541/20000], Loss: 82.8388671875, Learning Rate: 0.01\n",
      "Epoch [542/20000], Loss: 82.73876190185547, Learning Rate: 0.01\n",
      "Epoch [543/20000], Loss: 82.63955688476562, Learning Rate: 0.01\n",
      "Epoch [544/20000], Loss: 82.54141235351562, Learning Rate: 0.01\n",
      "Epoch [545/20000], Loss: 82.44442749023438, Learning Rate: 0.01\n",
      "Epoch [546/20000], Loss: 82.34817504882812, Learning Rate: 0.01\n",
      "Epoch [547/20000], Loss: 82.2530517578125, Learning Rate: 0.01\n",
      "Epoch [548/20000], Loss: 82.15904235839844, Learning Rate: 0.01\n",
      "Epoch [549/20000], Loss: 82.06588745117188, Learning Rate: 0.01\n",
      "Epoch [550/20000], Loss: 81.9737548828125, Learning Rate: 0.01\n",
      "Epoch [551/20000], Loss: 81.88258361816406, Learning Rate: 0.01\n",
      "Epoch [552/20000], Loss: 81.79229736328125, Learning Rate: 0.01\n",
      "Epoch [553/20000], Loss: 81.70297241210938, Learning Rate: 0.01\n",
      "Epoch [554/20000], Loss: 81.61466979980469, Learning Rate: 0.01\n",
      "Epoch [555/20000], Loss: 81.527099609375, Learning Rate: 0.01\n",
      "Epoch [556/20000], Loss: 81.4404296875, Learning Rate: 0.01\n",
      "Epoch [557/20000], Loss: 81.35481262207031, Learning Rate: 0.01\n",
      "Epoch [558/20000], Loss: 81.26988220214844, Learning Rate: 0.01\n",
      "Epoch [559/20000], Loss: 81.18603515625, Learning Rate: 0.01\n",
      "Epoch [560/20000], Loss: 81.10295104980469, Learning Rate: 0.01\n",
      "Epoch [561/20000], Loss: 81.02061462402344, Learning Rate: 0.01\n",
      "Epoch [562/20000], Loss: 80.93919372558594, Learning Rate: 0.01\n",
      "Epoch [563/20000], Loss: 80.85847473144531, Learning Rate: 0.01\n",
      "Epoch [564/20000], Loss: 80.77873229980469, Learning Rate: 0.01\n",
      "Epoch [565/20000], Loss: 80.69978332519531, Learning Rate: 0.01\n",
      "Epoch [566/20000], Loss: 80.62144470214844, Learning Rate: 0.01\n",
      "Epoch [567/20000], Loss: 80.54405212402344, Learning Rate: 0.01\n",
      "Epoch [568/20000], Loss: 80.46743774414062, Learning Rate: 0.01\n",
      "Epoch [569/20000], Loss: 80.39158630371094, Learning Rate: 0.01\n",
      "Epoch [570/20000], Loss: 80.31634521484375, Learning Rate: 0.01\n",
      "Epoch [571/20000], Loss: 80.24191284179688, Learning Rate: 0.01\n",
      "Epoch [572/20000], Loss: 80.16816711425781, Learning Rate: 0.01\n",
      "Epoch [573/20000], Loss: 80.09513854980469, Learning Rate: 0.01\n",
      "Epoch [574/20000], Loss: 80.02285766601562, Learning Rate: 0.01\n",
      "Epoch [575/20000], Loss: 79.9512939453125, Learning Rate: 0.01\n",
      "Epoch [576/20000], Loss: 79.88031005859375, Learning Rate: 0.01\n",
      "Epoch [577/20000], Loss: 79.81002807617188, Learning Rate: 0.01\n",
      "Epoch [578/20000], Loss: 79.74044799804688, Learning Rate: 0.01\n",
      "Epoch [579/20000], Loss: 79.67164611816406, Learning Rate: 0.01\n",
      "Epoch [580/20000], Loss: 79.60316467285156, Learning Rate: 0.01\n",
      "Epoch [581/20000], Loss: 79.53559875488281, Learning Rate: 0.01\n",
      "Epoch [582/20000], Loss: 79.46853637695312, Learning Rate: 0.01\n",
      "Epoch [583/20000], Loss: 79.402099609375, Learning Rate: 0.01\n",
      "Epoch [584/20000], Loss: 79.33625793457031, Learning Rate: 0.01\n",
      "Epoch [585/20000], Loss: 79.27114868164062, Learning Rate: 0.01\n",
      "Epoch [586/20000], Loss: 79.20651245117188, Learning Rate: 0.01\n",
      "Epoch [587/20000], Loss: 79.14251708984375, Learning Rate: 0.01\n",
      "Epoch [588/20000], Loss: 79.07902526855469, Learning Rate: 0.01\n",
      "Epoch [589/20000], Loss: 79.01606750488281, Learning Rate: 0.01\n",
      "Epoch [590/20000], Loss: 78.95375061035156, Learning Rate: 0.01\n",
      "Epoch [591/20000], Loss: 78.89202880859375, Learning Rate: 0.01\n",
      "Epoch [592/20000], Loss: 78.83062744140625, Learning Rate: 0.01\n",
      "Epoch [593/20000], Loss: 78.7698974609375, Learning Rate: 0.01\n",
      "Epoch [594/20000], Loss: 78.7098388671875, Learning Rate: 0.01\n",
      "Epoch [595/20000], Loss: 78.65011596679688, Learning Rate: 0.01\n",
      "Epoch [596/20000], Loss: 78.59088134765625, Learning Rate: 0.01\n",
      "Epoch [597/20000], Loss: 78.53216552734375, Learning Rate: 0.01\n",
      "Epoch [598/20000], Loss: 78.47390747070312, Learning Rate: 0.01\n",
      "Epoch [599/20000], Loss: 78.41622924804688, Learning Rate: 0.01\n",
      "Epoch [600/20000], Loss: 78.35903930664062, Learning Rate: 0.01\n",
      "Epoch [601/20000], Loss: 78.30218505859375, Learning Rate: 0.01\n",
      "Epoch [602/20000], Loss: 78.24601745605469, Learning Rate: 0.01\n",
      "Epoch [603/20000], Loss: 78.19007873535156, Learning Rate: 0.01\n",
      "Epoch [604/20000], Loss: 78.13468933105469, Learning Rate: 0.01\n",
      "Epoch [605/20000], Loss: 78.07966613769531, Learning Rate: 0.01\n",
      "Epoch [606/20000], Loss: 78.02519226074219, Learning Rate: 0.01\n",
      "Epoch [607/20000], Loss: 77.97111511230469, Learning Rate: 0.01\n",
      "Epoch [608/20000], Loss: 77.91748046875, Learning Rate: 0.01\n",
      "Epoch [609/20000], Loss: 77.8642578125, Learning Rate: 0.01\n",
      "Epoch [610/20000], Loss: 77.81134033203125, Learning Rate: 0.01\n",
      "Epoch [611/20000], Loss: 77.75877380371094, Learning Rate: 0.01\n",
      "Epoch [612/20000], Loss: 77.70689392089844, Learning Rate: 0.01\n",
      "Epoch [613/20000], Loss: 77.65519714355469, Learning Rate: 0.01\n",
      "Epoch [614/20000], Loss: 77.60401916503906, Learning Rate: 0.01\n",
      "Epoch [615/20000], Loss: 77.55308532714844, Learning Rate: 0.01\n",
      "Epoch [616/20000], Loss: 77.50259399414062, Learning Rate: 0.01\n",
      "Epoch [617/20000], Loss: 77.45254516601562, Learning Rate: 0.01\n",
      "Epoch [618/20000], Loss: 77.40264892578125, Learning Rate: 0.01\n",
      "Epoch [619/20000], Loss: 77.35330200195312, Learning Rate: 0.01\n",
      "Epoch [620/20000], Loss: 77.30427551269531, Learning Rate: 0.01\n",
      "Epoch [621/20000], Loss: 77.25541687011719, Learning Rate: 0.01\n",
      "Epoch [622/20000], Loss: 77.20704650878906, Learning Rate: 0.01\n",
      "Epoch [623/20000], Loss: 77.15901184082031, Learning Rate: 0.01\n",
      "Epoch [624/20000], Loss: 77.11138916015625, Learning Rate: 0.01\n",
      "Epoch [625/20000], Loss: 77.06413269042969, Learning Rate: 0.01\n",
      "Epoch [626/20000], Loss: 77.01707458496094, Learning Rate: 0.01\n",
      "Epoch [627/20000], Loss: 76.97018432617188, Learning Rate: 0.01\n",
      "Epoch [628/20000], Loss: 76.92396545410156, Learning Rate: 0.01\n",
      "Epoch [629/20000], Loss: 76.87777709960938, Learning Rate: 0.01\n",
      "Epoch [630/20000], Loss: 76.83200073242188, Learning Rate: 0.01\n",
      "Epoch [631/20000], Loss: 76.7864990234375, Learning Rate: 0.01\n",
      "Epoch [632/20000], Loss: 76.74128723144531, Learning Rate: 0.01\n",
      "Epoch [633/20000], Loss: 76.69619750976562, Learning Rate: 0.01\n",
      "Epoch [634/20000], Loss: 76.651611328125, Learning Rate: 0.01\n",
      "Epoch [635/20000], Loss: 76.60739135742188, Learning Rate: 0.01\n",
      "Epoch [636/20000], Loss: 76.56326293945312, Learning Rate: 0.01\n",
      "Epoch [637/20000], Loss: 76.51951599121094, Learning Rate: 0.01\n",
      "Epoch [638/20000], Loss: 76.47589111328125, Learning Rate: 0.01\n",
      "Epoch [639/20000], Loss: 76.43263244628906, Learning Rate: 0.01\n",
      "Epoch [640/20000], Loss: 76.3896484375, Learning Rate: 0.01\n",
      "Epoch [641/20000], Loss: 76.34718322753906, Learning Rate: 0.01\n",
      "Epoch [642/20000], Loss: 76.30451965332031, Learning Rate: 0.01\n",
      "Epoch [643/20000], Loss: 76.26228332519531, Learning Rate: 0.01\n",
      "Epoch [644/20000], Loss: 76.22041320800781, Learning Rate: 0.01\n",
      "Epoch [645/20000], Loss: 76.1785888671875, Learning Rate: 0.01\n",
      "Epoch [646/20000], Loss: 76.13711547851562, Learning Rate: 0.01\n",
      "Epoch [647/20000], Loss: 76.09577941894531, Learning Rate: 0.01\n",
      "Epoch [648/20000], Loss: 76.0548095703125, Learning Rate: 0.01\n",
      "Epoch [649/20000], Loss: 76.01406860351562, Learning Rate: 0.01\n",
      "Epoch [650/20000], Loss: 75.9735107421875, Learning Rate: 0.01\n",
      "Epoch [651/20000], Loss: 75.93313598632812, Learning Rate: 0.01\n",
      "Epoch [652/20000], Loss: 75.89305114746094, Learning Rate: 0.01\n",
      "Epoch [653/20000], Loss: 75.85316467285156, Learning Rate: 0.01\n",
      "Epoch [654/20000], Loss: 75.81350708007812, Learning Rate: 0.01\n",
      "Epoch [655/20000], Loss: 75.77400207519531, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [656/20000], Loss: 75.7347412109375, Learning Rate: 0.01\n",
      "Epoch [657/20000], Loss: 75.69558715820312, Learning Rate: 0.01\n",
      "Epoch [658/20000], Loss: 75.65689086914062, Learning Rate: 0.01\n",
      "Epoch [659/20000], Loss: 75.61820983886719, Learning Rate: 0.01\n",
      "Epoch [660/20000], Loss: 75.57975769042969, Learning Rate: 0.01\n",
      "Epoch [661/20000], Loss: 75.5416259765625, Learning Rate: 0.01\n",
      "Epoch [662/20000], Loss: 75.50349426269531, Learning Rate: 0.01\n",
      "Epoch [663/20000], Loss: 75.46554565429688, Learning Rate: 0.01\n",
      "Epoch [664/20000], Loss: 75.42778015136719, Learning Rate: 0.01\n",
      "Epoch [665/20000], Loss: 75.39035034179688, Learning Rate: 0.01\n",
      "Epoch [666/20000], Loss: 75.35292053222656, Learning Rate: 0.01\n",
      "Epoch [667/20000], Loss: 75.31596374511719, Learning Rate: 0.01\n",
      "Epoch [668/20000], Loss: 75.2789306640625, Learning Rate: 0.01\n",
      "Epoch [669/20000], Loss: 75.24205017089844, Learning Rate: 0.01\n",
      "Epoch [670/20000], Loss: 75.20545959472656, Learning Rate: 0.01\n",
      "Epoch [671/20000], Loss: 75.16900634765625, Learning Rate: 0.01\n",
      "Epoch [672/20000], Loss: 75.13267517089844, Learning Rate: 0.01\n",
      "Epoch [673/20000], Loss: 75.09657287597656, Learning Rate: 0.01\n",
      "Epoch [674/20000], Loss: 75.06059265136719, Learning Rate: 0.01\n",
      "Epoch [675/20000], Loss: 75.02488708496094, Learning Rate: 0.01\n",
      "Epoch [676/20000], Loss: 74.98918151855469, Learning Rate: 0.01\n",
      "Epoch [677/20000], Loss: 74.95358276367188, Learning Rate: 0.01\n",
      "Epoch [678/20000], Loss: 74.91822814941406, Learning Rate: 0.01\n",
      "Epoch [679/20000], Loss: 74.88308715820312, Learning Rate: 0.01\n",
      "Epoch [680/20000], Loss: 74.84808349609375, Learning Rate: 0.01\n",
      "Epoch [681/20000], Loss: 74.81320190429688, Learning Rate: 0.01\n",
      "Epoch [682/20000], Loss: 74.77845764160156, Learning Rate: 0.01\n",
      "Epoch [683/20000], Loss: 74.74388122558594, Learning Rate: 0.01\n",
      "Epoch [684/20000], Loss: 74.70930480957031, Learning Rate: 0.01\n",
      "Epoch [685/20000], Loss: 74.67514038085938, Learning Rate: 0.01\n",
      "Epoch [686/20000], Loss: 74.64085388183594, Learning Rate: 0.01\n",
      "Epoch [687/20000], Loss: 74.606689453125, Learning Rate: 0.01\n",
      "Epoch [688/20000], Loss: 74.57272338867188, Learning Rate: 0.01\n",
      "Epoch [689/20000], Loss: 74.53892517089844, Learning Rate: 0.01\n",
      "Epoch [690/20000], Loss: 74.50521850585938, Learning Rate: 0.01\n",
      "Epoch [691/20000], Loss: 74.47186279296875, Learning Rate: 0.01\n",
      "Epoch [692/20000], Loss: 74.43827819824219, Learning Rate: 0.01\n",
      "Epoch [693/20000], Loss: 74.40509033203125, Learning Rate: 0.01\n",
      "Epoch [694/20000], Loss: 74.37191772460938, Learning Rate: 0.01\n",
      "Epoch [695/20000], Loss: 74.33882141113281, Learning Rate: 0.01\n",
      "Epoch [696/20000], Loss: 74.30595397949219, Learning Rate: 0.01\n",
      "Epoch [697/20000], Loss: 74.27316284179688, Learning Rate: 0.01\n",
      "Epoch [698/20000], Loss: 74.24044799804688, Learning Rate: 0.01\n",
      "Epoch [699/20000], Loss: 74.20779418945312, Learning Rate: 0.01\n",
      "Epoch [700/20000], Loss: 74.17539978027344, Learning Rate: 0.01\n",
      "Epoch [701/20000], Loss: 74.14299011230469, Learning Rate: 0.01\n",
      "Epoch [702/20000], Loss: 74.11062622070312, Learning Rate: 0.01\n",
      "Epoch [703/20000], Loss: 74.07864379882812, Learning Rate: 0.01\n",
      "Epoch [704/20000], Loss: 74.04658508300781, Learning Rate: 0.01\n",
      "Epoch [705/20000], Loss: 74.01461791992188, Learning Rate: 0.01\n",
      "Epoch [706/20000], Loss: 73.98284912109375, Learning Rate: 0.01\n",
      "Epoch [707/20000], Loss: 73.95103454589844, Learning Rate: 0.01\n",
      "Epoch [708/20000], Loss: 73.91957092285156, Learning Rate: 0.01\n",
      "Epoch [709/20000], Loss: 73.88786315917969, Learning Rate: 0.01\n",
      "Epoch [710/20000], Loss: 73.85653686523438, Learning Rate: 0.01\n",
      "Epoch [711/20000], Loss: 73.82510375976562, Learning Rate: 0.01\n",
      "Epoch [712/20000], Loss: 73.7939453125, Learning Rate: 0.01\n",
      "Epoch [713/20000], Loss: 73.76277160644531, Learning Rate: 0.01\n",
      "Epoch [714/20000], Loss: 73.73162841796875, Learning Rate: 0.01\n",
      "Epoch [715/20000], Loss: 73.70071411132812, Learning Rate: 0.01\n",
      "Epoch [716/20000], Loss: 73.66993713378906, Learning Rate: 0.01\n",
      "Epoch [717/20000], Loss: 73.63902282714844, Learning Rate: 0.01\n",
      "Epoch [718/20000], Loss: 73.6083984375, Learning Rate: 0.01\n",
      "Epoch [719/20000], Loss: 73.57783508300781, Learning Rate: 0.01\n",
      "Epoch [720/20000], Loss: 73.54739379882812, Learning Rate: 0.01\n",
      "Epoch [721/20000], Loss: 73.51678466796875, Learning Rate: 0.01\n",
      "Epoch [722/20000], Loss: 73.48651123046875, Learning Rate: 0.01\n",
      "Epoch [723/20000], Loss: 73.45628356933594, Learning Rate: 0.01\n",
      "Epoch [724/20000], Loss: 73.42599487304688, Learning Rate: 0.01\n",
      "Epoch [725/20000], Loss: 73.39588928222656, Learning Rate: 0.01\n",
      "Epoch [726/20000], Loss: 73.36599731445312, Learning Rate: 0.01\n",
      "Epoch [727/20000], Loss: 73.33607482910156, Learning Rate: 0.01\n",
      "Epoch [728/20000], Loss: 73.30615234375, Learning Rate: 0.01\n",
      "Epoch [729/20000], Loss: 73.27647399902344, Learning Rate: 0.01\n",
      "Epoch [730/20000], Loss: 73.24659729003906, Learning Rate: 0.01\n",
      "Epoch [731/20000], Loss: 73.21702575683594, Learning Rate: 0.01\n",
      "Epoch [732/20000], Loss: 73.18751525878906, Learning Rate: 0.01\n",
      "Epoch [733/20000], Loss: 73.15794372558594, Learning Rate: 0.01\n",
      "Epoch [734/20000], Loss: 73.12849426269531, Learning Rate: 0.01\n",
      "Epoch [735/20000], Loss: 73.09906005859375, Learning Rate: 0.01\n",
      "Epoch [736/20000], Loss: 73.06976318359375, Learning Rate: 0.01\n",
      "Epoch [737/20000], Loss: 73.04063415527344, Learning Rate: 0.01\n",
      "Epoch [738/20000], Loss: 73.01145935058594, Learning Rate: 0.01\n",
      "Epoch [739/20000], Loss: 72.98239135742188, Learning Rate: 0.01\n",
      "Epoch [740/20000], Loss: 72.95333862304688, Learning Rate: 0.01\n",
      "Epoch [741/20000], Loss: 72.92440795898438, Learning Rate: 0.01\n",
      "Epoch [742/20000], Loss: 72.89540100097656, Learning Rate: 0.01\n",
      "Epoch [743/20000], Loss: 72.86665344238281, Learning Rate: 0.01\n",
      "Epoch [744/20000], Loss: 72.83782958984375, Learning Rate: 0.01\n",
      "Epoch [745/20000], Loss: 72.8092041015625, Learning Rate: 0.01\n",
      "Epoch [746/20000], Loss: 72.78054809570312, Learning Rate: 0.01\n",
      "Epoch [747/20000], Loss: 72.75192260742188, Learning Rate: 0.01\n",
      "Epoch [748/20000], Loss: 72.72337341308594, Learning Rate: 0.01\n",
      "Epoch [749/20000], Loss: 72.69496154785156, Learning Rate: 0.01\n",
      "Epoch [750/20000], Loss: 72.66654968261719, Learning Rate: 0.01\n",
      "Epoch [751/20000], Loss: 72.63818359375, Learning Rate: 0.01\n",
      "Epoch [752/20000], Loss: 72.60995483398438, Learning Rate: 0.01\n",
      "Epoch [753/20000], Loss: 72.58174133300781, Learning Rate: 0.01\n",
      "Epoch [754/20000], Loss: 72.55352783203125, Learning Rate: 0.01\n",
      "Epoch [755/20000], Loss: 72.52529907226562, Learning Rate: 0.01\n",
      "Epoch [756/20000], Loss: 72.497314453125, Learning Rate: 0.01\n",
      "Epoch [757/20000], Loss: 72.46917724609375, Learning Rate: 0.01\n",
      "Epoch [758/20000], Loss: 72.44123840332031, Learning Rate: 0.01\n",
      "Epoch [759/20000], Loss: 72.41336059570312, Learning Rate: 0.01\n",
      "Epoch [760/20000], Loss: 72.38552856445312, Learning Rate: 0.01\n",
      "Epoch [761/20000], Loss: 72.35774230957031, Learning Rate: 0.01\n",
      "Epoch [762/20000], Loss: 72.32989501953125, Learning Rate: 0.01\n",
      "Epoch [763/20000], Loss: 72.30210876464844, Learning Rate: 0.01\n",
      "Epoch [764/20000], Loss: 72.27458190917969, Learning Rate: 0.01\n",
      "Epoch [765/20000], Loss: 72.24684143066406, Learning Rate: 0.01\n",
      "Epoch [766/20000], Loss: 72.21923828125, Learning Rate: 0.01\n",
      "Epoch [767/20000], Loss: 72.19181823730469, Learning Rate: 0.01\n",
      "Epoch [768/20000], Loss: 72.16424560546875, Learning Rate: 0.01\n",
      "Epoch [769/20000], Loss: 72.13688659667969, Learning Rate: 0.01\n",
      "Epoch [770/20000], Loss: 72.10946655273438, Learning Rate: 0.01\n",
      "Epoch [771/20000], Loss: 72.08209228515625, Learning Rate: 0.01\n",
      "Epoch [772/20000], Loss: 72.05477905273438, Learning Rate: 0.01\n",
      "Epoch [773/20000], Loss: 72.02755737304688, Learning Rate: 0.01\n",
      "Epoch [774/20000], Loss: 72.00027465820312, Learning Rate: 0.01\n",
      "Epoch [775/20000], Loss: 71.97328186035156, Learning Rate: 0.01\n",
      "Epoch [776/20000], Loss: 71.94590759277344, Learning Rate: 0.01\n",
      "Epoch [777/20000], Loss: 71.91891479492188, Learning Rate: 0.01\n",
      "Epoch [778/20000], Loss: 71.89183044433594, Learning Rate: 0.01\n",
      "Epoch [779/20000], Loss: 71.86474609375, Learning Rate: 0.01\n",
      "Epoch [780/20000], Loss: 71.83784484863281, Learning Rate: 0.01\n",
      "Epoch [781/20000], Loss: 71.81076049804688, Learning Rate: 0.01\n",
      "Epoch [782/20000], Loss: 71.78388977050781, Learning Rate: 0.01\n",
      "Epoch [783/20000], Loss: 71.75711059570312, Learning Rate: 0.01\n",
      "Epoch [784/20000], Loss: 71.7303466796875, Learning Rate: 0.01\n",
      "Epoch [785/20000], Loss: 71.70347595214844, Learning Rate: 0.01\n",
      "Epoch [786/20000], Loss: 71.6767578125, Learning Rate: 0.01\n",
      "Epoch [787/20000], Loss: 71.64999389648438, Learning Rate: 0.01\n",
      "Epoch [788/20000], Loss: 71.62338256835938, Learning Rate: 0.01\n",
      "Epoch [789/20000], Loss: 71.59669494628906, Learning Rate: 0.01\n",
      "Epoch [790/20000], Loss: 71.570068359375, Learning Rate: 0.01\n",
      "Epoch [791/20000], Loss: 71.5435791015625, Learning Rate: 0.01\n",
      "Epoch [792/20000], Loss: 71.51690673828125, Learning Rate: 0.01\n",
      "Epoch [793/20000], Loss: 71.49058532714844, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [794/20000], Loss: 71.46409606933594, Learning Rate: 0.01\n",
      "Epoch [795/20000], Loss: 71.43748474121094, Learning Rate: 0.01\n",
      "Epoch [796/20000], Loss: 71.41110229492188, Learning Rate: 0.01\n",
      "Epoch [797/20000], Loss: 71.38475036621094, Learning Rate: 0.01\n",
      "Epoch [798/20000], Loss: 71.35845947265625, Learning Rate: 0.01\n",
      "Epoch [799/20000], Loss: 71.33219909667969, Learning Rate: 0.01\n",
      "Epoch [800/20000], Loss: 71.30584716796875, Learning Rate: 0.01\n",
      "Epoch [801/20000], Loss: 71.27963256835938, Learning Rate: 0.01\n",
      "Epoch [802/20000], Loss: 71.25335693359375, Learning Rate: 0.01\n",
      "Epoch [803/20000], Loss: 71.2271728515625, Learning Rate: 0.01\n",
      "Epoch [804/20000], Loss: 71.2010498046875, Learning Rate: 0.01\n",
      "Epoch [805/20000], Loss: 71.17486572265625, Learning Rate: 0.01\n",
      "Epoch [806/20000], Loss: 71.14871215820312, Learning Rate: 0.01\n",
      "Epoch [807/20000], Loss: 71.12261962890625, Learning Rate: 0.01\n",
      "Epoch [808/20000], Loss: 71.09663391113281, Learning Rate: 0.01\n",
      "Epoch [809/20000], Loss: 71.07069396972656, Learning Rate: 0.01\n",
      "Epoch [810/20000], Loss: 71.04464721679688, Learning Rate: 0.01\n",
      "Epoch [811/20000], Loss: 71.01866149902344, Learning Rate: 0.01\n",
      "Epoch [812/20000], Loss: 70.99259948730469, Learning Rate: 0.01\n",
      "Epoch [813/20000], Loss: 70.96693420410156, Learning Rate: 0.01\n",
      "Epoch [814/20000], Loss: 70.94084167480469, Learning Rate: 0.01\n",
      "Epoch [815/20000], Loss: 70.91505432128906, Learning Rate: 0.01\n",
      "Epoch [816/20000], Loss: 70.88920593261719, Learning Rate: 0.01\n",
      "Epoch [817/20000], Loss: 70.86347961425781, Learning Rate: 0.01\n",
      "Epoch [818/20000], Loss: 70.83773803710938, Learning Rate: 0.01\n",
      "Epoch [819/20000], Loss: 70.81192016601562, Learning Rate: 0.01\n",
      "Epoch [820/20000], Loss: 70.78608703613281, Learning Rate: 0.01\n",
      "Epoch [821/20000], Loss: 70.76057434082031, Learning Rate: 0.01\n",
      "Epoch [822/20000], Loss: 70.73480224609375, Learning Rate: 0.01\n",
      "Epoch [823/20000], Loss: 70.70919799804688, Learning Rate: 0.01\n",
      "Epoch [824/20000], Loss: 70.68342590332031, Learning Rate: 0.01\n",
      "Epoch [825/20000], Loss: 70.65794372558594, Learning Rate: 0.01\n",
      "Epoch [826/20000], Loss: 70.63232421875, Learning Rate: 0.01\n",
      "Epoch [827/20000], Loss: 70.60673522949219, Learning Rate: 0.01\n",
      "Epoch [828/20000], Loss: 70.58128356933594, Learning Rate: 0.01\n",
      "Epoch [829/20000], Loss: 70.55561828613281, Learning Rate: 0.01\n",
      "Epoch [830/20000], Loss: 70.53016662597656, Learning Rate: 0.01\n",
      "Epoch [831/20000], Loss: 70.504638671875, Learning Rate: 0.01\n",
      "Epoch [832/20000], Loss: 70.47929382324219, Learning Rate: 0.01\n",
      "Epoch [833/20000], Loss: 70.45381164550781, Learning Rate: 0.01\n",
      "Epoch [834/20000], Loss: 70.42835998535156, Learning Rate: 0.01\n",
      "Epoch [835/20000], Loss: 70.40296936035156, Learning Rate: 0.01\n",
      "Epoch [836/20000], Loss: 70.37771606445312, Learning Rate: 0.01\n",
      "Epoch [837/20000], Loss: 70.35223388671875, Learning Rate: 0.01\n",
      "Epoch [838/20000], Loss: 70.32699584960938, Learning Rate: 0.01\n",
      "Epoch [839/20000], Loss: 70.30162048339844, Learning Rate: 0.01\n",
      "Epoch [840/20000], Loss: 70.27647399902344, Learning Rate: 0.01\n",
      "Epoch [841/20000], Loss: 70.25114440917969, Learning Rate: 0.01\n",
      "Epoch [842/20000], Loss: 70.22563171386719, Learning Rate: 0.01\n",
      "Epoch [843/20000], Loss: 70.20072937011719, Learning Rate: 0.01\n",
      "Epoch [844/20000], Loss: 70.17550659179688, Learning Rate: 0.01\n",
      "Epoch [845/20000], Loss: 70.15022277832031, Learning Rate: 0.01\n",
      "Epoch [846/20000], Loss: 70.1251220703125, Learning Rate: 0.01\n",
      "Epoch [847/20000], Loss: 70.09991455078125, Learning Rate: 0.01\n",
      "Epoch [848/20000], Loss: 70.07498168945312, Learning Rate: 0.01\n",
      "Epoch [849/20000], Loss: 70.04965209960938, Learning Rate: 0.01\n",
      "Epoch [850/20000], Loss: 70.02459716796875, Learning Rate: 0.01\n",
      "Epoch [851/20000], Loss: 69.99946594238281, Learning Rate: 0.01\n",
      "Epoch [852/20000], Loss: 69.97450256347656, Learning Rate: 0.01\n",
      "Epoch [853/20000], Loss: 69.94943237304688, Learning Rate: 0.01\n",
      "Epoch [854/20000], Loss: 69.92437744140625, Learning Rate: 0.01\n",
      "Epoch [855/20000], Loss: 69.89920043945312, Learning Rate: 0.01\n",
      "Epoch [856/20000], Loss: 69.87446594238281, Learning Rate: 0.01\n",
      "Epoch [857/20000], Loss: 69.84938049316406, Learning Rate: 0.01\n",
      "Epoch [858/20000], Loss: 69.82453918457031, Learning Rate: 0.01\n",
      "Epoch [859/20000], Loss: 69.79946899414062, Learning Rate: 0.01\n",
      "Epoch [860/20000], Loss: 69.77458190917969, Learning Rate: 0.01\n",
      "Epoch [861/20000], Loss: 69.74967956542969, Learning Rate: 0.01\n",
      "Epoch [862/20000], Loss: 69.72482299804688, Learning Rate: 0.01\n",
      "Epoch [863/20000], Loss: 69.69984436035156, Learning Rate: 0.01\n",
      "Epoch [864/20000], Loss: 69.67507934570312, Learning Rate: 0.01\n",
      "Epoch [865/20000], Loss: 69.65022277832031, Learning Rate: 0.01\n",
      "Epoch [866/20000], Loss: 69.62539672851562, Learning Rate: 0.01\n",
      "Epoch [867/20000], Loss: 69.60052490234375, Learning Rate: 0.01\n",
      "Epoch [868/20000], Loss: 69.57583618164062, Learning Rate: 0.01\n",
      "Epoch [869/20000], Loss: 69.55099487304688, Learning Rate: 0.01\n",
      "Epoch [870/20000], Loss: 69.52632141113281, Learning Rate: 0.01\n",
      "Epoch [871/20000], Loss: 69.50149536132812, Learning Rate: 0.01\n",
      "Epoch [872/20000], Loss: 69.47669982910156, Learning Rate: 0.01\n",
      "Epoch [873/20000], Loss: 69.45213317871094, Learning Rate: 0.01\n",
      "Epoch [874/20000], Loss: 69.42727661132812, Learning Rate: 0.01\n",
      "Epoch [875/20000], Loss: 69.40255737304688, Learning Rate: 0.01\n",
      "Epoch [876/20000], Loss: 69.37791442871094, Learning Rate: 0.01\n",
      "Epoch [877/20000], Loss: 69.35321044921875, Learning Rate: 0.01\n",
      "Epoch [878/20000], Loss: 69.32864379882812, Learning Rate: 0.01\n",
      "Epoch [879/20000], Loss: 69.30393981933594, Learning Rate: 0.01\n",
      "Epoch [880/20000], Loss: 69.27934265136719, Learning Rate: 0.01\n",
      "Epoch [881/20000], Loss: 69.2547607421875, Learning Rate: 0.01\n",
      "Epoch [882/20000], Loss: 69.23007202148438, Learning Rate: 0.01\n",
      "Epoch [883/20000], Loss: 69.20555114746094, Learning Rate: 0.01\n",
      "Epoch [884/20000], Loss: 69.18110656738281, Learning Rate: 0.01\n",
      "Epoch [885/20000], Loss: 69.15641784667969, Learning Rate: 0.01\n",
      "Epoch [886/20000], Loss: 69.13206481933594, Learning Rate: 0.01\n",
      "Epoch [887/20000], Loss: 69.10734558105469, Learning Rate: 0.01\n",
      "Epoch [888/20000], Loss: 69.08291625976562, Learning Rate: 0.01\n",
      "Epoch [889/20000], Loss: 69.05845642089844, Learning Rate: 0.01\n",
      "Epoch [890/20000], Loss: 69.03384399414062, Learning Rate: 0.01\n",
      "Epoch [891/20000], Loss: 69.00930786132812, Learning Rate: 0.01\n",
      "Epoch [892/20000], Loss: 68.98493957519531, Learning Rate: 0.01\n",
      "Epoch [893/20000], Loss: 68.96046447753906, Learning Rate: 0.01\n",
      "Epoch [894/20000], Loss: 68.9359130859375, Learning Rate: 0.01\n",
      "Epoch [895/20000], Loss: 68.91159057617188, Learning Rate: 0.01\n",
      "Epoch [896/20000], Loss: 68.8870849609375, Learning Rate: 0.01\n",
      "Epoch [897/20000], Loss: 68.86271667480469, Learning Rate: 0.01\n",
      "Epoch [898/20000], Loss: 68.83843994140625, Learning Rate: 0.01\n",
      "Epoch [899/20000], Loss: 68.81404113769531, Learning Rate: 0.01\n",
      "Epoch [900/20000], Loss: 68.78973388671875, Learning Rate: 0.01\n",
      "Epoch [901/20000], Loss: 68.76545715332031, Learning Rate: 0.01\n",
      "Epoch [902/20000], Loss: 68.74101257324219, Learning Rate: 0.01\n",
      "Epoch [903/20000], Loss: 68.716796875, Learning Rate: 0.01\n",
      "Epoch [904/20000], Loss: 68.69235229492188, Learning Rate: 0.01\n",
      "Epoch [905/20000], Loss: 68.66795349121094, Learning Rate: 0.01\n",
      "Epoch [906/20000], Loss: 68.64375305175781, Learning Rate: 0.01\n",
      "Epoch [907/20000], Loss: 68.61947631835938, Learning Rate: 0.01\n",
      "Epoch [908/20000], Loss: 68.59513854980469, Learning Rate: 0.01\n",
      "Epoch [909/20000], Loss: 68.5709228515625, Learning Rate: 0.01\n",
      "Epoch [910/20000], Loss: 68.54666137695312, Learning Rate: 0.01\n",
      "Epoch [911/20000], Loss: 68.52247619628906, Learning Rate: 0.01\n",
      "Epoch [912/20000], Loss: 68.49806213378906, Learning Rate: 0.01\n",
      "Epoch [913/20000], Loss: 68.47401428222656, Learning Rate: 0.01\n",
      "Epoch [914/20000], Loss: 68.44972229003906, Learning Rate: 0.01\n",
      "Epoch [915/20000], Loss: 68.42562866210938, Learning Rate: 0.01\n",
      "Epoch [916/20000], Loss: 68.40141296386719, Learning Rate: 0.01\n",
      "Epoch [917/20000], Loss: 68.37736511230469, Learning Rate: 0.01\n",
      "Epoch [918/20000], Loss: 68.3531494140625, Learning Rate: 0.01\n",
      "Epoch [919/20000], Loss: 68.32914733886719, Learning Rate: 0.01\n",
      "Epoch [920/20000], Loss: 68.30484008789062, Learning Rate: 0.01\n",
      "Epoch [921/20000], Loss: 68.28060913085938, Learning Rate: 0.01\n",
      "Epoch [922/20000], Loss: 68.25650024414062, Learning Rate: 0.01\n",
      "Epoch [923/20000], Loss: 68.2325439453125, Learning Rate: 0.01\n",
      "Epoch [924/20000], Loss: 68.20846557617188, Learning Rate: 0.01\n",
      "Epoch [925/20000], Loss: 68.18438720703125, Learning Rate: 0.01\n",
      "Epoch [926/20000], Loss: 68.16029357910156, Learning Rate: 0.01\n",
      "Epoch [927/20000], Loss: 68.1361083984375, Learning Rate: 0.01\n",
      "Epoch [928/20000], Loss: 68.11210632324219, Learning Rate: 0.01\n",
      "Epoch [929/20000], Loss: 68.08811950683594, Learning Rate: 0.01\n",
      "Epoch [930/20000], Loss: 68.06411743164062, Learning Rate: 0.01\n",
      "Epoch [931/20000], Loss: 68.04014587402344, Learning Rate: 0.01\n",
      "Epoch [932/20000], Loss: 68.01602172851562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [933/20000], Loss: 67.99211120605469, Learning Rate: 0.01\n",
      "Epoch [934/20000], Loss: 67.96821594238281, Learning Rate: 0.01\n",
      "Epoch [935/20000], Loss: 67.944091796875, Learning Rate: 0.01\n",
      "Epoch [936/20000], Loss: 67.92008972167969, Learning Rate: 0.01\n",
      "Epoch [937/20000], Loss: 67.89613342285156, Learning Rate: 0.01\n",
      "Epoch [938/20000], Loss: 67.87225341796875, Learning Rate: 0.01\n",
      "Epoch [939/20000], Loss: 67.84823608398438, Learning Rate: 0.01\n",
      "Epoch [940/20000], Loss: 67.82431030273438, Learning Rate: 0.01\n",
      "Epoch [941/20000], Loss: 67.80052185058594, Learning Rate: 0.01\n",
      "Epoch [942/20000], Loss: 67.77647399902344, Learning Rate: 0.01\n",
      "Epoch [943/20000], Loss: 67.75265502929688, Learning Rate: 0.01\n",
      "Epoch [944/20000], Loss: 67.72872924804688, Learning Rate: 0.01\n",
      "Epoch [945/20000], Loss: 67.70475769042969, Learning Rate: 0.01\n",
      "Epoch [946/20000], Loss: 67.68096923828125, Learning Rate: 0.01\n",
      "Epoch [947/20000], Loss: 67.65711975097656, Learning Rate: 0.01\n",
      "Epoch [948/20000], Loss: 67.63337707519531, Learning Rate: 0.01\n",
      "Epoch [949/20000], Loss: 67.60946655273438, Learning Rate: 0.01\n",
      "Epoch [950/20000], Loss: 67.58558654785156, Learning Rate: 0.01\n",
      "Epoch [951/20000], Loss: 67.56163024902344, Learning Rate: 0.01\n",
      "Epoch [952/20000], Loss: 67.53802490234375, Learning Rate: 0.01\n",
      "Epoch [953/20000], Loss: 67.51419067382812, Learning Rate: 0.01\n",
      "Epoch [954/20000], Loss: 67.49046325683594, Learning Rate: 0.01\n",
      "Epoch [955/20000], Loss: 67.46665954589844, Learning Rate: 0.01\n",
      "Epoch [956/20000], Loss: 67.44284057617188, Learning Rate: 0.01\n",
      "Epoch [957/20000], Loss: 67.41908264160156, Learning Rate: 0.01\n",
      "Epoch [958/20000], Loss: 67.39529418945312, Learning Rate: 0.01\n",
      "Epoch [959/20000], Loss: 67.37152099609375, Learning Rate: 0.01\n",
      "Epoch [960/20000], Loss: 67.34785461425781, Learning Rate: 0.01\n",
      "Epoch [961/20000], Loss: 67.32402038574219, Learning Rate: 0.01\n",
      "Epoch [962/20000], Loss: 67.30032348632812, Learning Rate: 0.01\n",
      "Epoch [963/20000], Loss: 67.276611328125, Learning Rate: 0.01\n",
      "Epoch [964/20000], Loss: 67.25297546386719, Learning Rate: 0.01\n",
      "Epoch [965/20000], Loss: 67.22930908203125, Learning Rate: 0.01\n",
      "Epoch [966/20000], Loss: 67.20556640625, Learning Rate: 0.01\n",
      "Epoch [967/20000], Loss: 67.18183898925781, Learning Rate: 0.01\n",
      "Epoch [968/20000], Loss: 67.15818786621094, Learning Rate: 0.01\n",
      "Epoch [969/20000], Loss: 67.13462829589844, Learning Rate: 0.01\n",
      "Epoch [970/20000], Loss: 67.11106872558594, Learning Rate: 0.01\n",
      "Epoch [971/20000], Loss: 67.08731079101562, Learning Rate: 0.01\n",
      "Epoch [972/20000], Loss: 67.06365966796875, Learning Rate: 0.01\n",
      "Epoch [973/20000], Loss: 67.04000854492188, Learning Rate: 0.01\n",
      "Epoch [974/20000], Loss: 67.01649475097656, Learning Rate: 0.01\n",
      "Epoch [975/20000], Loss: 66.99281311035156, Learning Rate: 0.01\n",
      "Epoch [976/20000], Loss: 66.96923828125, Learning Rate: 0.01\n",
      "Epoch [977/20000], Loss: 66.94577026367188, Learning Rate: 0.01\n",
      "Epoch [978/20000], Loss: 66.92204284667969, Learning Rate: 0.01\n",
      "Epoch [979/20000], Loss: 66.89857482910156, Learning Rate: 0.01\n",
      "Epoch [980/20000], Loss: 66.87506103515625, Learning Rate: 0.01\n",
      "Epoch [981/20000], Loss: 66.85142517089844, Learning Rate: 0.01\n",
      "Epoch [982/20000], Loss: 66.82792663574219, Learning Rate: 0.01\n",
      "Epoch [983/20000], Loss: 66.80436706542969, Learning Rate: 0.01\n",
      "Epoch [984/20000], Loss: 66.78094482421875, Learning Rate: 0.01\n",
      "Epoch [985/20000], Loss: 66.75733947753906, Learning Rate: 0.01\n",
      "Epoch [986/20000], Loss: 66.73387145996094, Learning Rate: 0.01\n",
      "Epoch [987/20000], Loss: 66.71044921875, Learning Rate: 0.01\n",
      "Epoch [988/20000], Loss: 66.68687438964844, Learning Rate: 0.01\n",
      "Epoch [989/20000], Loss: 66.66346740722656, Learning Rate: 0.01\n",
      "Epoch [990/20000], Loss: 66.64007568359375, Learning Rate: 0.01\n",
      "Epoch [991/20000], Loss: 66.61659240722656, Learning Rate: 0.01\n",
      "Epoch [992/20000], Loss: 66.59309387207031, Learning Rate: 0.01\n",
      "Epoch [993/20000], Loss: 66.56953430175781, Learning Rate: 0.01\n",
      "Epoch [994/20000], Loss: 66.54621887207031, Learning Rate: 0.01\n",
      "Epoch [995/20000], Loss: 66.52279663085938, Learning Rate: 0.01\n",
      "Epoch [996/20000], Loss: 66.49932861328125, Learning Rate: 0.01\n",
      "Epoch [997/20000], Loss: 66.47593688964844, Learning Rate: 0.01\n",
      "Epoch [998/20000], Loss: 66.45256042480469, Learning Rate: 0.01\n",
      "Epoch [999/20000], Loss: 66.42921447753906, Learning Rate: 0.01\n",
      "Epoch [1000/20000], Loss: 66.40582275390625, Learning Rate: 0.01\n",
      "Epoch [1001/20000], Loss: 66.38243103027344, Learning Rate: 0.01\n",
      "Epoch [1002/20000], Loss: 66.35903930664062, Learning Rate: 0.01\n",
      "Epoch [1003/20000], Loss: 66.33578491210938, Learning Rate: 0.01\n",
      "Epoch [1004/20000], Loss: 66.31234741210938, Learning Rate: 0.01\n",
      "Epoch [1005/20000], Loss: 66.28904724121094, Learning Rate: 0.01\n",
      "Epoch [1006/20000], Loss: 66.26580810546875, Learning Rate: 0.01\n",
      "Epoch [1007/20000], Loss: 66.24249267578125, Learning Rate: 0.01\n",
      "Epoch [1008/20000], Loss: 66.21920776367188, Learning Rate: 0.01\n",
      "Epoch [1009/20000], Loss: 66.19587707519531, Learning Rate: 0.01\n",
      "Epoch [1010/20000], Loss: 66.17250061035156, Learning Rate: 0.01\n",
      "Epoch [1011/20000], Loss: 66.14926147460938, Learning Rate: 0.01\n",
      "Epoch [1012/20000], Loss: 66.12614440917969, Learning Rate: 0.01\n",
      "Epoch [1013/20000], Loss: 66.10284423828125, Learning Rate: 0.01\n",
      "Epoch [1014/20000], Loss: 66.07952880859375, Learning Rate: 0.01\n",
      "Epoch [1015/20000], Loss: 66.05633544921875, Learning Rate: 0.01\n",
      "Epoch [1016/20000], Loss: 66.03305053710938, Learning Rate: 0.01\n",
      "Epoch [1017/20000], Loss: 66.00971984863281, Learning Rate: 0.01\n",
      "Epoch [1018/20000], Loss: 65.98658752441406, Learning Rate: 0.01\n",
      "Epoch [1019/20000], Loss: 65.96342468261719, Learning Rate: 0.01\n",
      "Epoch [1020/20000], Loss: 65.94023132324219, Learning Rate: 0.01\n",
      "Epoch [1021/20000], Loss: 65.9171142578125, Learning Rate: 0.01\n",
      "Epoch [1022/20000], Loss: 65.89381408691406, Learning Rate: 0.01\n",
      "Epoch [1023/20000], Loss: 65.87060546875, Learning Rate: 0.01\n",
      "Epoch [1024/20000], Loss: 65.84747314453125, Learning Rate: 0.01\n",
      "Epoch [1025/20000], Loss: 65.82432556152344, Learning Rate: 0.01\n",
      "Epoch [1026/20000], Loss: 65.80120849609375, Learning Rate: 0.01\n",
      "Epoch [1027/20000], Loss: 65.77793884277344, Learning Rate: 0.01\n",
      "Epoch [1028/20000], Loss: 65.75489807128906, Learning Rate: 0.01\n",
      "Epoch [1029/20000], Loss: 65.73179626464844, Learning Rate: 0.01\n",
      "Epoch [1030/20000], Loss: 65.70867919921875, Learning Rate: 0.01\n",
      "Epoch [1031/20000], Loss: 65.68569946289062, Learning Rate: 0.01\n",
      "Epoch [1032/20000], Loss: 65.66255187988281, Learning Rate: 0.01\n",
      "Epoch [1033/20000], Loss: 65.63935852050781, Learning Rate: 0.01\n",
      "Epoch [1034/20000], Loss: 65.61636352539062, Learning Rate: 0.01\n",
      "Epoch [1035/20000], Loss: 65.59329223632812, Learning Rate: 0.01\n",
      "Epoch [1036/20000], Loss: 65.5701904296875, Learning Rate: 0.01\n",
      "Epoch [1037/20000], Loss: 65.54708862304688, Learning Rate: 0.01\n",
      "Epoch [1038/20000], Loss: 65.5240478515625, Learning Rate: 0.01\n",
      "Epoch [1039/20000], Loss: 65.50106811523438, Learning Rate: 0.01\n",
      "Epoch [1040/20000], Loss: 65.47808837890625, Learning Rate: 0.01\n",
      "Epoch [1041/20000], Loss: 65.45510864257812, Learning Rate: 0.01\n",
      "Epoch [1042/20000], Loss: 65.43218994140625, Learning Rate: 0.01\n",
      "Epoch [1043/20000], Loss: 65.40902709960938, Learning Rate: 0.01\n",
      "Epoch [1044/20000], Loss: 65.3861083984375, Learning Rate: 0.01\n",
      "Epoch [1045/20000], Loss: 65.36308288574219, Learning Rate: 0.01\n",
      "Epoch [1046/20000], Loss: 65.34014892578125, Learning Rate: 0.01\n",
      "Epoch [1047/20000], Loss: 65.31718444824219, Learning Rate: 0.01\n",
      "Epoch [1048/20000], Loss: 65.29421997070312, Learning Rate: 0.01\n",
      "Epoch [1049/20000], Loss: 65.27134704589844, Learning Rate: 0.01\n",
      "Epoch [1050/20000], Loss: 65.24832153320312, Learning Rate: 0.01\n",
      "Epoch [1051/20000], Loss: 65.22543334960938, Learning Rate: 0.01\n",
      "Epoch [1052/20000], Loss: 65.20248413085938, Learning Rate: 0.01\n",
      "Epoch [1053/20000], Loss: 65.17965698242188, Learning Rate: 0.01\n",
      "Epoch [1054/20000], Loss: 65.15660095214844, Learning Rate: 0.01\n",
      "Epoch [1055/20000], Loss: 65.13369750976562, Learning Rate: 0.01\n",
      "Epoch [1056/20000], Loss: 65.11102294921875, Learning Rate: 0.01\n",
      "Epoch [1057/20000], Loss: 65.08810424804688, Learning Rate: 0.01\n",
      "Epoch [1058/20000], Loss: 65.06507873535156, Learning Rate: 0.01\n",
      "Epoch [1059/20000], Loss: 65.04231262207031, Learning Rate: 0.01\n",
      "Epoch [1060/20000], Loss: 65.0194091796875, Learning Rate: 0.01\n",
      "Epoch [1061/20000], Loss: 64.99658203125, Learning Rate: 0.01\n",
      "Epoch [1062/20000], Loss: 64.9737548828125, Learning Rate: 0.01\n",
      "Epoch [1063/20000], Loss: 64.95103454589844, Learning Rate: 0.01\n",
      "Epoch [1064/20000], Loss: 64.92820739746094, Learning Rate: 0.01\n",
      "Epoch [1065/20000], Loss: 64.90545654296875, Learning Rate: 0.01\n",
      "Epoch [1066/20000], Loss: 64.88261413574219, Learning Rate: 0.01\n",
      "Epoch [1067/20000], Loss: 64.85975646972656, Learning Rate: 0.01\n",
      "Epoch [1068/20000], Loss: 64.83709716796875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1069/20000], Loss: 64.814208984375, Learning Rate: 0.01\n",
      "Epoch [1070/20000], Loss: 64.79148864746094, Learning Rate: 0.01\n",
      "Epoch [1071/20000], Loss: 64.76873779296875, Learning Rate: 0.01\n",
      "Epoch [1072/20000], Loss: 64.74609375, Learning Rate: 0.01\n",
      "Epoch [1073/20000], Loss: 64.7232666015625, Learning Rate: 0.01\n",
      "Epoch [1074/20000], Loss: 64.70054626464844, Learning Rate: 0.01\n",
      "Epoch [1075/20000], Loss: 64.67776489257812, Learning Rate: 0.01\n",
      "Epoch [1076/20000], Loss: 64.65524291992188, Learning Rate: 0.01\n",
      "Epoch [1077/20000], Loss: 64.63236999511719, Learning Rate: 0.01\n",
      "Epoch [1078/20000], Loss: 64.60968017578125, Learning Rate: 0.01\n",
      "Epoch [1079/20000], Loss: 64.58706665039062, Learning Rate: 0.01\n",
      "Epoch [1080/20000], Loss: 64.56448364257812, Learning Rate: 0.01\n",
      "Epoch [1081/20000], Loss: 64.54173278808594, Learning Rate: 0.01\n",
      "Epoch [1082/20000], Loss: 64.51910400390625, Learning Rate: 0.01\n",
      "Epoch [1083/20000], Loss: 64.49647521972656, Learning Rate: 0.01\n",
      "Epoch [1084/20000], Loss: 64.47392272949219, Learning Rate: 0.01\n",
      "Epoch [1085/20000], Loss: 64.45111083984375, Learning Rate: 0.01\n",
      "Epoch [1086/20000], Loss: 64.42854309082031, Learning Rate: 0.01\n",
      "Epoch [1087/20000], Loss: 64.40597534179688, Learning Rate: 0.01\n",
      "Epoch [1088/20000], Loss: 64.38339233398438, Learning Rate: 0.01\n",
      "Epoch [1089/20000], Loss: 64.36087036132812, Learning Rate: 0.01\n",
      "Epoch [1090/20000], Loss: 64.33828735351562, Learning Rate: 0.01\n",
      "Epoch [1091/20000], Loss: 64.31556701660156, Learning Rate: 0.01\n",
      "Epoch [1092/20000], Loss: 64.29318237304688, Learning Rate: 0.01\n",
      "Epoch [1093/20000], Loss: 64.27047729492188, Learning Rate: 0.01\n",
      "Epoch [1094/20000], Loss: 64.24810791015625, Learning Rate: 0.01\n",
      "Epoch [1095/20000], Loss: 64.22541809082031, Learning Rate: 0.01\n",
      "Epoch [1096/20000], Loss: 64.20303344726562, Learning Rate: 0.01\n",
      "Epoch [1097/20000], Loss: 64.1805419921875, Learning Rate: 0.01\n",
      "Epoch [1098/20000], Loss: 64.15800476074219, Learning Rate: 0.01\n",
      "Epoch [1099/20000], Loss: 64.13545227050781, Learning Rate: 0.01\n",
      "Epoch [1100/20000], Loss: 64.11299133300781, Learning Rate: 0.01\n",
      "Epoch [1101/20000], Loss: 64.09051513671875, Learning Rate: 0.01\n",
      "Epoch [1102/20000], Loss: 64.06826782226562, Learning Rate: 0.01\n",
      "Epoch [1103/20000], Loss: 64.04556274414062, Learning Rate: 0.01\n",
      "Epoch [1104/20000], Loss: 64.02313232421875, Learning Rate: 0.01\n",
      "Epoch [1105/20000], Loss: 64.00071716308594, Learning Rate: 0.01\n",
      "Epoch [1106/20000], Loss: 63.978363037109375, Learning Rate: 0.01\n",
      "Epoch [1107/20000], Loss: 63.95591735839844, Learning Rate: 0.01\n",
      "Epoch [1108/20000], Loss: 63.933380126953125, Learning Rate: 0.01\n",
      "Epoch [1109/20000], Loss: 63.9110107421875, Learning Rate: 0.01\n",
      "Epoch [1110/20000], Loss: 63.888641357421875, Learning Rate: 0.01\n",
      "Epoch [1111/20000], Loss: 63.86634826660156, Learning Rate: 0.01\n",
      "Epoch [1112/20000], Loss: 63.8438720703125, Learning Rate: 0.01\n",
      "Epoch [1113/20000], Loss: 63.82147216796875, Learning Rate: 0.01\n",
      "Epoch [1114/20000], Loss: 63.7991943359375, Learning Rate: 0.01\n",
      "Epoch [1115/20000], Loss: 63.77687072753906, Learning Rate: 0.01\n",
      "Epoch [1116/20000], Loss: 63.75450134277344, Learning Rate: 0.01\n",
      "Epoch [1117/20000], Loss: 63.73219299316406, Learning Rate: 0.01\n",
      "Epoch [1118/20000], Loss: 63.70982360839844, Learning Rate: 0.01\n",
      "Epoch [1119/20000], Loss: 63.68754577636719, Learning Rate: 0.01\n",
      "Epoch [1120/20000], Loss: 63.665313720703125, Learning Rate: 0.01\n",
      "Epoch [1121/20000], Loss: 63.64302062988281, Learning Rate: 0.01\n",
      "Epoch [1122/20000], Loss: 63.620635986328125, Learning Rate: 0.01\n",
      "Epoch [1123/20000], Loss: 63.59846496582031, Learning Rate: 0.01\n",
      "Epoch [1124/20000], Loss: 63.57612609863281, Learning Rate: 0.01\n",
      "Epoch [1125/20000], Loss: 63.55381774902344, Learning Rate: 0.01\n",
      "Epoch [1126/20000], Loss: 63.53167724609375, Learning Rate: 0.01\n",
      "Epoch [1127/20000], Loss: 63.50935363769531, Learning Rate: 0.01\n",
      "Epoch [1128/20000], Loss: 63.487274169921875, Learning Rate: 0.01\n",
      "Epoch [1129/20000], Loss: 63.46490478515625, Learning Rate: 0.01\n",
      "Epoch [1130/20000], Loss: 63.44279479980469, Learning Rate: 0.01\n",
      "Epoch [1131/20000], Loss: 63.42051696777344, Learning Rate: 0.01\n",
      "Epoch [1132/20000], Loss: 63.39837646484375, Learning Rate: 0.01\n",
      "Epoch [1133/20000], Loss: 63.37623596191406, Learning Rate: 0.01\n",
      "Epoch [1134/20000], Loss: 63.35401916503906, Learning Rate: 0.01\n",
      "Epoch [1135/20000], Loss: 63.33192443847656, Learning Rate: 0.01\n",
      "Epoch [1136/20000], Loss: 63.30973815917969, Learning Rate: 0.01\n",
      "Epoch [1137/20000], Loss: 63.28752136230469, Learning Rate: 0.01\n",
      "Epoch [1138/20000], Loss: 63.26536560058594, Learning Rate: 0.01\n",
      "Epoch [1139/20000], Loss: 63.24345397949219, Learning Rate: 0.01\n",
      "Epoch [1140/20000], Loss: 63.2213134765625, Learning Rate: 0.01\n",
      "Epoch [1141/20000], Loss: 63.19921875, Learning Rate: 0.01\n",
      "Epoch [1142/20000], Loss: 63.176971435546875, Learning Rate: 0.01\n",
      "Epoch [1143/20000], Loss: 63.154937744140625, Learning Rate: 0.01\n",
      "Epoch [1144/20000], Loss: 63.132965087890625, Learning Rate: 0.01\n",
      "Epoch [1145/20000], Loss: 63.11079406738281, Learning Rate: 0.01\n",
      "Epoch [1146/20000], Loss: 63.08885192871094, Learning Rate: 0.01\n",
      "Epoch [1147/20000], Loss: 63.066802978515625, Learning Rate: 0.01\n",
      "Epoch [1148/20000], Loss: 63.044647216796875, Learning Rate: 0.01\n",
      "Epoch [1149/20000], Loss: 63.022552490234375, Learning Rate: 0.01\n",
      "Epoch [1150/20000], Loss: 63.00065612792969, Learning Rate: 0.01\n",
      "Epoch [1151/20000], Loss: 62.9786376953125, Learning Rate: 0.01\n",
      "Epoch [1152/20000], Loss: 62.95664978027344, Learning Rate: 0.01\n",
      "Epoch [1153/20000], Loss: 62.93470764160156, Learning Rate: 0.01\n",
      "Epoch [1154/20000], Loss: 62.91265869140625, Learning Rate: 0.01\n",
      "Epoch [1155/20000], Loss: 62.890777587890625, Learning Rate: 0.01\n",
      "Epoch [1156/20000], Loss: 62.86866760253906, Learning Rate: 0.01\n",
      "Epoch [1157/20000], Loss: 62.84690856933594, Learning Rate: 0.01\n",
      "Epoch [1158/20000], Loss: 62.82487487792969, Learning Rate: 0.01\n",
      "Epoch [1159/20000], Loss: 62.80291748046875, Learning Rate: 0.01\n",
      "Epoch [1160/20000], Loss: 62.78108215332031, Learning Rate: 0.01\n",
      "Epoch [1161/20000], Loss: 62.75910949707031, Learning Rate: 0.01\n",
      "Epoch [1162/20000], Loss: 62.73724365234375, Learning Rate: 0.01\n",
      "Epoch [1163/20000], Loss: 62.71528625488281, Learning Rate: 0.01\n",
      "Epoch [1164/20000], Loss: 62.693450927734375, Learning Rate: 0.01\n",
      "Epoch [1165/20000], Loss: 62.67161560058594, Learning Rate: 0.01\n",
      "Epoch [1166/20000], Loss: 62.64967346191406, Learning Rate: 0.01\n",
      "Epoch [1167/20000], Loss: 62.627716064453125, Learning Rate: 0.01\n",
      "Epoch [1168/20000], Loss: 62.60601806640625, Learning Rate: 0.01\n",
      "Epoch [1169/20000], Loss: 62.584320068359375, Learning Rate: 0.01\n",
      "Epoch [1170/20000], Loss: 62.56236267089844, Learning Rate: 0.01\n",
      "Epoch [1171/20000], Loss: 62.54054260253906, Learning Rate: 0.01\n",
      "Epoch [1172/20000], Loss: 62.51872253417969, Learning Rate: 0.01\n",
      "Epoch [1173/20000], Loss: 62.49696350097656, Learning Rate: 0.01\n",
      "Epoch [1174/20000], Loss: 62.47517395019531, Learning Rate: 0.01\n",
      "Epoch [1175/20000], Loss: 62.45332336425781, Learning Rate: 0.01\n",
      "Epoch [1176/20000], Loss: 62.431640625, Learning Rate: 0.01\n",
      "Epoch [1177/20000], Loss: 62.409912109375, Learning Rate: 0.01\n",
      "Epoch [1178/20000], Loss: 62.388092041015625, Learning Rate: 0.01\n",
      "Epoch [1179/20000], Loss: 62.36637878417969, Learning Rate: 0.01\n",
      "Epoch [1180/20000], Loss: 62.34471130371094, Learning Rate: 0.01\n",
      "Epoch [1181/20000], Loss: 62.32292175292969, Learning Rate: 0.01\n",
      "Epoch [1182/20000], Loss: 62.30125427246094, Learning Rate: 0.01\n",
      "Epoch [1183/20000], Loss: 62.27935791015625, Learning Rate: 0.01\n",
      "Epoch [1184/20000], Loss: 62.25787353515625, Learning Rate: 0.01\n",
      "Epoch [1185/20000], Loss: 62.23614501953125, Learning Rate: 0.01\n",
      "Epoch [1186/20000], Loss: 62.21455383300781, Learning Rate: 0.01\n",
      "Epoch [1187/20000], Loss: 62.192962646484375, Learning Rate: 0.01\n",
      "Epoch [1188/20000], Loss: 62.171173095703125, Learning Rate: 0.01\n",
      "Epoch [1189/20000], Loss: 62.149658203125, Learning Rate: 0.01\n",
      "Epoch [1190/20000], Loss: 62.127960205078125, Learning Rate: 0.01\n",
      "Epoch [1191/20000], Loss: 62.106353759765625, Learning Rate: 0.01\n",
      "Epoch [1192/20000], Loss: 62.084747314453125, Learning Rate: 0.01\n",
      "Epoch [1193/20000], Loss: 62.06321716308594, Learning Rate: 0.01\n",
      "Epoch [1194/20000], Loss: 62.04154968261719, Learning Rate: 0.01\n",
      "Epoch [1195/20000], Loss: 62.01997375488281, Learning Rate: 0.01\n",
      "Epoch [1196/20000], Loss: 61.998504638671875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1197/20000], Loss: 61.976959228515625, Learning Rate: 0.01\n",
      "Epoch [1198/20000], Loss: 61.955322265625, Learning Rate: 0.01\n",
      "Epoch [1199/20000], Loss: 61.9337158203125, Learning Rate: 0.01\n",
      "Epoch [1200/20000], Loss: 61.91218566894531, Learning Rate: 0.01\n",
      "Epoch [1201/20000], Loss: 61.89068603515625, Learning Rate: 0.01\n",
      "Epoch [1202/20000], Loss: 61.869232177734375, Learning Rate: 0.01\n",
      "Epoch [1203/20000], Loss: 61.847686767578125, Learning Rate: 0.01\n",
      "Epoch [1204/20000], Loss: 61.82627868652344, Learning Rate: 0.01\n",
      "Epoch [1205/20000], Loss: 61.80482482910156, Learning Rate: 0.01\n",
      "Epoch [1206/20000], Loss: 61.78337097167969, Learning Rate: 0.01\n",
      "Epoch [1207/20000], Loss: 61.76179504394531, Learning Rate: 0.01\n",
      "Epoch [1208/20000], Loss: 61.74037170410156, Learning Rate: 0.01\n",
      "Epoch [1209/20000], Loss: 61.718841552734375, Learning Rate: 0.01\n",
      "Epoch [1210/20000], Loss: 61.697509765625, Learning Rate: 0.01\n",
      "Epoch [1211/20000], Loss: 61.67604064941406, Learning Rate: 0.01\n",
      "Epoch [1212/20000], Loss: 61.6546630859375, Learning Rate: 0.01\n",
      "Epoch [1213/20000], Loss: 61.63323974609375, Learning Rate: 0.01\n",
      "Epoch [1214/20000], Loss: 61.61192321777344, Learning Rate: 0.01\n",
      "Epoch [1215/20000], Loss: 61.59059143066406, Learning Rate: 0.01\n",
      "Epoch [1216/20000], Loss: 61.56913757324219, Learning Rate: 0.01\n",
      "Epoch [1217/20000], Loss: 61.54779052734375, Learning Rate: 0.01\n",
      "Epoch [1218/20000], Loss: 61.52653503417969, Learning Rate: 0.01\n",
      "Epoch [1219/20000], Loss: 61.50517272949219, Learning Rate: 0.01\n",
      "Epoch [1220/20000], Loss: 61.483795166015625, Learning Rate: 0.01\n",
      "Epoch [1221/20000], Loss: 61.46244812011719, Learning Rate: 0.01\n",
      "Epoch [1222/20000], Loss: 61.441070556640625, Learning Rate: 0.01\n",
      "Epoch [1223/20000], Loss: 61.419830322265625, Learning Rate: 0.01\n",
      "Epoch [1224/20000], Loss: 61.39849853515625, Learning Rate: 0.01\n",
      "Epoch [1225/20000], Loss: 61.37721252441406, Learning Rate: 0.01\n",
      "Epoch [1226/20000], Loss: 61.35600280761719, Learning Rate: 0.01\n",
      "Epoch [1227/20000], Loss: 61.33476257324219, Learning Rate: 0.01\n",
      "Epoch [1228/20000], Loss: 61.31343078613281, Learning Rate: 0.01\n",
      "Epoch [1229/20000], Loss: 61.29237365722656, Learning Rate: 0.01\n",
      "Epoch [1230/20000], Loss: 61.27093505859375, Learning Rate: 0.01\n",
      "Epoch [1231/20000], Loss: 61.24980163574219, Learning Rate: 0.01\n",
      "Epoch [1232/20000], Loss: 61.22850036621094, Learning Rate: 0.01\n",
      "Epoch [1233/20000], Loss: 61.20756530761719, Learning Rate: 0.01\n",
      "Epoch [1234/20000], Loss: 61.186065673828125, Learning Rate: 0.01\n",
      "Epoch [1235/20000], Loss: 61.16510009765625, Learning Rate: 0.01\n",
      "Epoch [1236/20000], Loss: 61.14396667480469, Learning Rate: 0.01\n",
      "Epoch [1237/20000], Loss: 61.122650146484375, Learning Rate: 0.01\n",
      "Epoch [1238/20000], Loss: 61.10173034667969, Learning Rate: 0.01\n",
      "Epoch [1239/20000], Loss: 61.08033752441406, Learning Rate: 0.01\n",
      "Epoch [1240/20000], Loss: 61.05938720703125, Learning Rate: 0.01\n",
      "Epoch [1241/20000], Loss: 61.03810119628906, Learning Rate: 0.01\n",
      "Epoch [1242/20000], Loss: 61.017120361328125, Learning Rate: 0.01\n",
      "Epoch [1243/20000], Loss: 60.99603271484375, Learning Rate: 0.01\n",
      "Epoch [1244/20000], Loss: 60.97492980957031, Learning Rate: 0.01\n",
      "Epoch [1245/20000], Loss: 60.95379638671875, Learning Rate: 0.01\n",
      "Epoch [1246/20000], Loss: 60.93281555175781, Learning Rate: 0.01\n",
      "Epoch [1247/20000], Loss: 60.91175842285156, Learning Rate: 0.01\n",
      "Epoch [1248/20000], Loss: 60.8907470703125, Learning Rate: 0.01\n",
      "Epoch [1249/20000], Loss: 60.86973571777344, Learning Rate: 0.01\n",
      "Epoch [1250/20000], Loss: 60.84852600097656, Learning Rate: 0.01\n",
      "Epoch [1251/20000], Loss: 60.82762145996094, Learning Rate: 0.01\n",
      "Epoch [1252/20000], Loss: 60.806549072265625, Learning Rate: 0.01\n",
      "Epoch [1253/20000], Loss: 60.78559875488281, Learning Rate: 0.01\n",
      "Epoch [1254/20000], Loss: 60.76454162597656, Learning Rate: 0.01\n",
      "Epoch [1255/20000], Loss: 60.7435302734375, Learning Rate: 0.01\n",
      "Epoch [1256/20000], Loss: 60.72257995605469, Learning Rate: 0.01\n",
      "Epoch [1257/20000], Loss: 60.70172119140625, Learning Rate: 0.01\n",
      "Epoch [1258/20000], Loss: 60.6807861328125, Learning Rate: 0.01\n",
      "Epoch [1259/20000], Loss: 60.65977478027344, Learning Rate: 0.01\n",
      "Epoch [1260/20000], Loss: 60.63885498046875, Learning Rate: 0.01\n",
      "Epoch [1261/20000], Loss: 60.61793518066406, Learning Rate: 0.01\n",
      "Epoch [1262/20000], Loss: 60.59716796875, Learning Rate: 0.01\n",
      "Epoch [1263/20000], Loss: 60.57623291015625, Learning Rate: 0.01\n",
      "Epoch [1264/20000], Loss: 60.55534362792969, Learning Rate: 0.01\n",
      "Epoch [1265/20000], Loss: 60.534423828125, Learning Rate: 0.01\n",
      "Epoch [1266/20000], Loss: 60.513580322265625, Learning Rate: 0.01\n",
      "Epoch [1267/20000], Loss: 60.49267578125, Learning Rate: 0.01\n",
      "Epoch [1268/20000], Loss: 60.471771240234375, Learning Rate: 0.01\n",
      "Epoch [1269/20000], Loss: 60.45086669921875, Learning Rate: 0.01\n",
      "Epoch [1270/20000], Loss: 60.43017578125, Learning Rate: 0.01\n",
      "Epoch [1271/20000], Loss: 60.40928649902344, Learning Rate: 0.01\n",
      "Epoch [1272/20000], Loss: 60.38865661621094, Learning Rate: 0.01\n",
      "Epoch [1273/20000], Loss: 60.367767333984375, Learning Rate: 0.01\n",
      "Epoch [1274/20000], Loss: 60.34698486328125, Learning Rate: 0.01\n",
      "Epoch [1275/20000], Loss: 60.32618713378906, Learning Rate: 0.01\n",
      "Epoch [1276/20000], Loss: 60.305572509765625, Learning Rate: 0.01\n",
      "Epoch [1277/20000], Loss: 60.28477478027344, Learning Rate: 0.01\n",
      "Epoch [1278/20000], Loss: 60.26397705078125, Learning Rate: 0.01\n",
      "Epoch [1279/20000], Loss: 60.2431640625, Learning Rate: 0.01\n",
      "Epoch [1280/20000], Loss: 60.22239685058594, Learning Rate: 0.01\n",
      "Epoch [1281/20000], Loss: 60.201812744140625, Learning Rate: 0.01\n",
      "Epoch [1282/20000], Loss: 60.181121826171875, Learning Rate: 0.01\n",
      "Epoch [1283/20000], Loss: 60.16038513183594, Learning Rate: 0.01\n",
      "Epoch [1284/20000], Loss: 60.13972473144531, Learning Rate: 0.01\n",
      "Epoch [1285/20000], Loss: 60.11903381347656, Learning Rate: 0.01\n",
      "Epoch [1286/20000], Loss: 60.09844970703125, Learning Rate: 0.01\n",
      "Epoch [1287/20000], Loss: 60.07780456542969, Learning Rate: 0.01\n",
      "Epoch [1288/20000], Loss: 60.057098388671875, Learning Rate: 0.01\n",
      "Epoch [1289/20000], Loss: 60.03656005859375, Learning Rate: 0.01\n",
      "Epoch [1290/20000], Loss: 60.01585388183594, Learning Rate: 0.01\n",
      "Epoch [1291/20000], Loss: 59.9952392578125, Learning Rate: 0.01\n",
      "Epoch [1292/20000], Loss: 59.97462463378906, Learning Rate: 0.01\n",
      "Epoch [1293/20000], Loss: 59.954132080078125, Learning Rate: 0.01\n",
      "Epoch [1294/20000], Loss: 59.93359375, Learning Rate: 0.01\n",
      "Epoch [1295/20000], Loss: 59.912872314453125, Learning Rate: 0.01\n",
      "Epoch [1296/20000], Loss: 59.892242431640625, Learning Rate: 0.01\n",
      "Epoch [1297/20000], Loss: 59.87181091308594, Learning Rate: 0.01\n",
      "Epoch [1298/20000], Loss: 59.85133361816406, Learning Rate: 0.01\n",
      "Epoch [1299/20000], Loss: 59.830780029296875, Learning Rate: 0.01\n",
      "Epoch [1300/20000], Loss: 59.81022644042969, Learning Rate: 0.01\n",
      "Epoch [1301/20000], Loss: 59.789581298828125, Learning Rate: 0.01\n",
      "Epoch [1302/20000], Loss: 59.76918029785156, Learning Rate: 0.01\n",
      "Epoch [1303/20000], Loss: 59.748687744140625, Learning Rate: 0.01\n",
      "Epoch [1304/20000], Loss: 59.72831726074219, Learning Rate: 0.01\n",
      "Epoch [1305/20000], Loss: 59.70787048339844, Learning Rate: 0.01\n",
      "Epoch [1306/20000], Loss: 59.6873779296875, Learning Rate: 0.01\n",
      "Epoch [1307/20000], Loss: 59.666778564453125, Learning Rate: 0.01\n",
      "Epoch [1308/20000], Loss: 59.646453857421875, Learning Rate: 0.01\n",
      "Epoch [1309/20000], Loss: 59.62602233886719, Learning Rate: 0.01\n",
      "Epoch [1310/20000], Loss: 59.605621337890625, Learning Rate: 0.01\n",
      "Epoch [1311/20000], Loss: 59.585174560546875, Learning Rate: 0.01\n",
      "Epoch [1312/20000], Loss: 59.564849853515625, Learning Rate: 0.01\n",
      "Epoch [1313/20000], Loss: 59.54443359375, Learning Rate: 0.01\n",
      "Epoch [1314/20000], Loss: 59.52397155761719, Learning Rate: 0.01\n",
      "Epoch [1315/20000], Loss: 59.50370788574219, Learning Rate: 0.01\n",
      "Epoch [1316/20000], Loss: 59.48338317871094, Learning Rate: 0.01\n",
      "Epoch [1317/20000], Loss: 59.4630126953125, Learning Rate: 0.01\n",
      "Epoch [1318/20000], Loss: 59.442626953125, Learning Rate: 0.01\n",
      "Epoch [1319/20000], Loss: 59.42240905761719, Learning Rate: 0.01\n",
      "Epoch [1320/20000], Loss: 59.40193176269531, Learning Rate: 0.01\n",
      "Epoch [1321/20000], Loss: 59.381744384765625, Learning Rate: 0.01\n",
      "Epoch [1322/20000], Loss: 59.361358642578125, Learning Rate: 0.01\n",
      "Epoch [1323/20000], Loss: 59.34117126464844, Learning Rate: 0.01\n",
      "Epoch [1324/20000], Loss: 59.320892333984375, Learning Rate: 0.01\n",
      "Epoch [1325/20000], Loss: 59.30049133300781, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1326/20000], Loss: 59.28028869628906, Learning Rate: 0.01\n",
      "Epoch [1327/20000], Loss: 59.26014709472656, Learning Rate: 0.01\n",
      "Epoch [1328/20000], Loss: 59.23991394042969, Learning Rate: 0.01\n",
      "Epoch [1329/20000], Loss: 59.21955871582031, Learning Rate: 0.01\n",
      "Epoch [1330/20000], Loss: 59.19947814941406, Learning Rate: 0.01\n",
      "Epoch [1331/20000], Loss: 59.17919921875, Learning Rate: 0.01\n",
      "Epoch [1332/20000], Loss: 59.15904235839844, Learning Rate: 0.01\n",
      "Epoch [1333/20000], Loss: 59.13877868652344, Learning Rate: 0.01\n",
      "Epoch [1334/20000], Loss: 59.11869812011719, Learning Rate: 0.01\n",
      "Epoch [1335/20000], Loss: 59.09857177734375, Learning Rate: 0.01\n",
      "Epoch [1336/20000], Loss: 59.078338623046875, Learning Rate: 0.01\n",
      "Epoch [1337/20000], Loss: 59.058319091796875, Learning Rate: 0.01\n",
      "Epoch [1338/20000], Loss: 59.038177490234375, Learning Rate: 0.01\n",
      "Epoch [1339/20000], Loss: 59.01792907714844, Learning Rate: 0.01\n",
      "Epoch [1340/20000], Loss: 58.9979248046875, Learning Rate: 0.01\n",
      "Epoch [1341/20000], Loss: 58.97776794433594, Learning Rate: 0.01\n",
      "Epoch [1342/20000], Loss: 58.95777893066406, Learning Rate: 0.01\n",
      "Epoch [1343/20000], Loss: 58.93769836425781, Learning Rate: 0.01\n",
      "Epoch [1344/20000], Loss: 58.917633056640625, Learning Rate: 0.01\n",
      "Epoch [1345/20000], Loss: 58.89752197265625, Learning Rate: 0.01\n",
      "Epoch [1346/20000], Loss: 58.87744140625, Learning Rate: 0.01\n",
      "Epoch [1347/20000], Loss: 58.85755920410156, Learning Rate: 0.01\n",
      "Epoch [1348/20000], Loss: 58.837310791015625, Learning Rate: 0.01\n",
      "Epoch [1349/20000], Loss: 58.8173828125, Learning Rate: 0.01\n",
      "Epoch [1350/20000], Loss: 58.797271728515625, Learning Rate: 0.01\n",
      "Epoch [1351/20000], Loss: 58.7774658203125, Learning Rate: 0.01\n",
      "Epoch [1352/20000], Loss: 58.7574462890625, Learning Rate: 0.01\n",
      "Epoch [1353/20000], Loss: 58.73750305175781, Learning Rate: 0.01\n",
      "Epoch [1354/20000], Loss: 58.71745300292969, Learning Rate: 0.01\n",
      "Epoch [1355/20000], Loss: 58.69746398925781, Learning Rate: 0.01\n",
      "Epoch [1356/20000], Loss: 58.67756652832031, Learning Rate: 0.01\n",
      "Epoch [1357/20000], Loss: 58.65765380859375, Learning Rate: 0.01\n",
      "Epoch [1358/20000], Loss: 58.63771057128906, Learning Rate: 0.01\n",
      "Epoch [1359/20000], Loss: 58.617706298828125, Learning Rate: 0.01\n",
      "Epoch [1360/20000], Loss: 58.59783935546875, Learning Rate: 0.01\n",
      "Epoch [1361/20000], Loss: 58.577911376953125, Learning Rate: 0.01\n",
      "Epoch [1362/20000], Loss: 58.558135986328125, Learning Rate: 0.01\n",
      "Epoch [1363/20000], Loss: 58.53822326660156, Learning Rate: 0.01\n",
      "Epoch [1364/20000], Loss: 58.51837158203125, Learning Rate: 0.01\n",
      "Epoch [1365/20000], Loss: 58.498504638671875, Learning Rate: 0.01\n",
      "Epoch [1366/20000], Loss: 58.47865295410156, Learning Rate: 0.01\n",
      "Epoch [1367/20000], Loss: 58.45881652832031, Learning Rate: 0.01\n",
      "Epoch [1368/20000], Loss: 58.438995361328125, Learning Rate: 0.01\n",
      "Epoch [1369/20000], Loss: 58.419158935546875, Learning Rate: 0.01\n",
      "Epoch [1370/20000], Loss: 58.399322509765625, Learning Rate: 0.01\n",
      "Epoch [1371/20000], Loss: 58.379486083984375, Learning Rate: 0.01\n",
      "Epoch [1372/20000], Loss: 58.3597412109375, Learning Rate: 0.01\n",
      "Epoch [1373/20000], Loss: 58.339996337890625, Learning Rate: 0.01\n",
      "Epoch [1374/20000], Loss: 58.320220947265625, Learning Rate: 0.01\n",
      "Epoch [1375/20000], Loss: 58.30052185058594, Learning Rate: 0.01\n",
      "Epoch [1376/20000], Loss: 58.28070068359375, Learning Rate: 0.01\n",
      "Epoch [1377/20000], Loss: 58.26106262207031, Learning Rate: 0.01\n",
      "Epoch [1378/20000], Loss: 58.241241455078125, Learning Rate: 0.01\n",
      "Epoch [1379/20000], Loss: 58.22154235839844, Learning Rate: 0.01\n",
      "Epoch [1380/20000], Loss: 58.20185852050781, Learning Rate: 0.01\n",
      "Epoch [1381/20000], Loss: 58.18208312988281, Learning Rate: 0.01\n",
      "Epoch [1382/20000], Loss: 58.16241455078125, Learning Rate: 0.01\n",
      "Epoch [1383/20000], Loss: 58.142852783203125, Learning Rate: 0.01\n",
      "Epoch [1384/20000], Loss: 58.12303161621094, Learning Rate: 0.01\n",
      "Epoch [1385/20000], Loss: 58.103302001953125, Learning Rate: 0.01\n",
      "Epoch [1386/20000], Loss: 58.083831787109375, Learning Rate: 0.01\n",
      "Epoch [1387/20000], Loss: 58.064208984375, Learning Rate: 0.01\n",
      "Epoch [1388/20000], Loss: 58.044586181640625, Learning Rate: 0.01\n",
      "Epoch [1389/20000], Loss: 58.02491760253906, Learning Rate: 0.01\n",
      "Epoch [1390/20000], Loss: 58.005401611328125, Learning Rate: 0.01\n",
      "Epoch [1391/20000], Loss: 57.98564147949219, Learning Rate: 0.01\n",
      "Epoch [1392/20000], Loss: 57.966217041015625, Learning Rate: 0.01\n",
      "Epoch [1393/20000], Loss: 57.9466552734375, Learning Rate: 0.01\n",
      "Epoch [1394/20000], Loss: 57.92718505859375, Learning Rate: 0.01\n",
      "Epoch [1395/20000], Loss: 57.907440185546875, Learning Rate: 0.01\n",
      "Epoch [1396/20000], Loss: 57.88798522949219, Learning Rate: 0.01\n",
      "Epoch [1397/20000], Loss: 57.868438720703125, Learning Rate: 0.01\n",
      "Epoch [1398/20000], Loss: 57.84877014160156, Learning Rate: 0.01\n",
      "Epoch [1399/20000], Loss: 57.82942199707031, Learning Rate: 0.01\n",
      "Epoch [1400/20000], Loss: 57.809783935546875, Learning Rate: 0.01\n",
      "Epoch [1401/20000], Loss: 57.79032897949219, Learning Rate: 0.01\n",
      "Epoch [1402/20000], Loss: 57.77088928222656, Learning Rate: 0.01\n",
      "Epoch [1403/20000], Loss: 57.75154113769531, Learning Rate: 0.01\n",
      "Epoch [1404/20000], Loss: 57.73193359375, Learning Rate: 0.01\n",
      "Epoch [1405/20000], Loss: 57.71257019042969, Learning Rate: 0.01\n",
      "Epoch [1406/20000], Loss: 57.693084716796875, Learning Rate: 0.01\n",
      "Epoch [1407/20000], Loss: 57.67362976074219, Learning Rate: 0.01\n",
      "Epoch [1408/20000], Loss: 57.65409851074219, Learning Rate: 0.01\n",
      "Epoch [1409/20000], Loss: 57.634674072265625, Learning Rate: 0.01\n",
      "Epoch [1410/20000], Loss: 57.61528015136719, Learning Rate: 0.01\n",
      "Epoch [1411/20000], Loss: 57.59590148925781, Learning Rate: 0.01\n",
      "Epoch [1412/20000], Loss: 57.576507568359375, Learning Rate: 0.01\n",
      "Epoch [1413/20000], Loss: 57.55717468261719, Learning Rate: 0.01\n",
      "Epoch [1414/20000], Loss: 57.53778076171875, Learning Rate: 0.01\n",
      "Epoch [1415/20000], Loss: 57.51850891113281, Learning Rate: 0.01\n",
      "Epoch [1416/20000], Loss: 57.49919128417969, Learning Rate: 0.01\n",
      "Epoch [1417/20000], Loss: 57.47975158691406, Learning Rate: 0.01\n",
      "Epoch [1418/20000], Loss: 57.46046447753906, Learning Rate: 0.01\n",
      "Epoch [1419/20000], Loss: 57.44117736816406, Learning Rate: 0.01\n",
      "Epoch [1420/20000], Loss: 57.42179870605469, Learning Rate: 0.01\n",
      "Epoch [1421/20000], Loss: 57.402496337890625, Learning Rate: 0.01\n",
      "Epoch [1422/20000], Loss: 57.38328552246094, Learning Rate: 0.01\n",
      "Epoch [1423/20000], Loss: 57.36399841308594, Learning Rate: 0.01\n",
      "Epoch [1424/20000], Loss: 57.34461975097656, Learning Rate: 0.01\n",
      "Epoch [1425/20000], Loss: 57.3253173828125, Learning Rate: 0.01\n",
      "Epoch [1426/20000], Loss: 57.30616760253906, Learning Rate: 0.01\n",
      "Epoch [1427/20000], Loss: 57.28691101074219, Learning Rate: 0.01\n",
      "Epoch [1428/20000], Loss: 57.267822265625, Learning Rate: 0.01\n",
      "Epoch [1429/20000], Loss: 57.24853515625, Learning Rate: 0.01\n",
      "Epoch [1430/20000], Loss: 57.229339599609375, Learning Rate: 0.01\n",
      "Epoch [1431/20000], Loss: 57.21015930175781, Learning Rate: 0.01\n",
      "Epoch [1432/20000], Loss: 57.19102478027344, Learning Rate: 0.01\n",
      "Epoch [1433/20000], Loss: 57.171630859375, Learning Rate: 0.01\n",
      "Epoch [1434/20000], Loss: 57.152679443359375, Learning Rate: 0.01\n",
      "Epoch [1435/20000], Loss: 57.13343811035156, Learning Rate: 0.01\n",
      "Epoch [1436/20000], Loss: 57.114288330078125, Learning Rate: 0.01\n",
      "Epoch [1437/20000], Loss: 57.09519958496094, Learning Rate: 0.01\n",
      "Epoch [1438/20000], Loss: 57.07597351074219, Learning Rate: 0.01\n",
      "Epoch [1439/20000], Loss: 57.05682373046875, Learning Rate: 0.01\n",
      "Epoch [1440/20000], Loss: 57.03776550292969, Learning Rate: 0.01\n",
      "Epoch [1441/20000], Loss: 57.018768310546875, Learning Rate: 0.01\n",
      "Epoch [1442/20000], Loss: 56.99958801269531, Learning Rate: 0.01\n",
      "Epoch [1443/20000], Loss: 56.98052978515625, Learning Rate: 0.01\n",
      "Epoch [1444/20000], Loss: 56.96142578125, Learning Rate: 0.01\n",
      "Epoch [1445/20000], Loss: 56.94233703613281, Learning Rate: 0.01\n",
      "Epoch [1446/20000], Loss: 56.92327880859375, Learning Rate: 0.01\n",
      "Epoch [1447/20000], Loss: 56.904327392578125, Learning Rate: 0.01\n",
      "Epoch [1448/20000], Loss: 56.88520812988281, Learning Rate: 0.01\n",
      "Epoch [1449/20000], Loss: 56.86616516113281, Learning Rate: 0.01\n",
      "Epoch [1450/20000], Loss: 56.84718322753906, Learning Rate: 0.01\n",
      "Epoch [1451/20000], Loss: 56.82823181152344, Learning Rate: 0.01\n",
      "Epoch [1452/20000], Loss: 56.809295654296875, Learning Rate: 0.01\n",
      "Epoch [1453/20000], Loss: 56.790252685546875, Learning Rate: 0.01\n",
      "Epoch [1454/20000], Loss: 56.771240234375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1455/20000], Loss: 56.75227355957031, Learning Rate: 0.01\n",
      "Epoch [1456/20000], Loss: 56.73333740234375, Learning Rate: 0.01\n",
      "Epoch [1457/20000], Loss: 56.71437072753906, Learning Rate: 0.01\n",
      "Epoch [1458/20000], Loss: 56.695343017578125, Learning Rate: 0.01\n",
      "Epoch [1459/20000], Loss: 56.67643737792969, Learning Rate: 0.01\n",
      "Epoch [1460/20000], Loss: 56.65757751464844, Learning Rate: 0.01\n",
      "Epoch [1461/20000], Loss: 56.63874816894531, Learning Rate: 0.01\n",
      "Epoch [1462/20000], Loss: 56.61970520019531, Learning Rate: 0.01\n",
      "Epoch [1463/20000], Loss: 56.60087585449219, Learning Rate: 0.01\n",
      "Epoch [1464/20000], Loss: 56.58197021484375, Learning Rate: 0.01\n",
      "Epoch [1465/20000], Loss: 56.563018798828125, Learning Rate: 0.01\n",
      "Epoch [1466/20000], Loss: 56.544219970703125, Learning Rate: 0.01\n",
      "Epoch [1467/20000], Loss: 56.52543640136719, Learning Rate: 0.01\n",
      "Epoch [1468/20000], Loss: 56.50651550292969, Learning Rate: 0.01\n",
      "Epoch [1469/20000], Loss: 56.48773193359375, Learning Rate: 0.01\n",
      "Epoch [1470/20000], Loss: 56.46888732910156, Learning Rate: 0.01\n",
      "Epoch [1471/20000], Loss: 56.449981689453125, Learning Rate: 0.01\n",
      "Epoch [1472/20000], Loss: 56.43128967285156, Learning Rate: 0.01\n",
      "Epoch [1473/20000], Loss: 56.41246032714844, Learning Rate: 0.01\n",
      "Epoch [1474/20000], Loss: 56.39372253417969, Learning Rate: 0.01\n",
      "Epoch [1475/20000], Loss: 56.37495422363281, Learning Rate: 0.01\n",
      "Epoch [1476/20000], Loss: 56.356170654296875, Learning Rate: 0.01\n",
      "Epoch [1477/20000], Loss: 56.33729553222656, Learning Rate: 0.01\n",
      "Epoch [1478/20000], Loss: 56.31864929199219, Learning Rate: 0.01\n",
      "Epoch [1479/20000], Loss: 56.29991149902344, Learning Rate: 0.01\n",
      "Epoch [1480/20000], Loss: 56.28126525878906, Learning Rate: 0.01\n",
      "Epoch [1481/20000], Loss: 56.262298583984375, Learning Rate: 0.01\n",
      "Epoch [1482/20000], Loss: 56.24363708496094, Learning Rate: 0.01\n",
      "Epoch [1483/20000], Loss: 56.225006103515625, Learning Rate: 0.01\n",
      "Epoch [1484/20000], Loss: 56.206298828125, Learning Rate: 0.01\n",
      "Epoch [1485/20000], Loss: 56.18763732910156, Learning Rate: 0.01\n",
      "Epoch [1486/20000], Loss: 56.16886901855469, Learning Rate: 0.01\n",
      "Epoch [1487/20000], Loss: 56.15031433105469, Learning Rate: 0.01\n",
      "Epoch [1488/20000], Loss: 56.13165283203125, Learning Rate: 0.01\n",
      "Epoch [1489/20000], Loss: 56.11308288574219, Learning Rate: 0.01\n",
      "Epoch [1490/20000], Loss: 56.094329833984375, Learning Rate: 0.01\n",
      "Epoch [1491/20000], Loss: 56.075653076171875, Learning Rate: 0.01\n",
      "Epoch [1492/20000], Loss: 56.05718994140625, Learning Rate: 0.01\n",
      "Epoch [1493/20000], Loss: 56.03846740722656, Learning Rate: 0.01\n",
      "Epoch [1494/20000], Loss: 56.01985168457031, Learning Rate: 0.01\n",
      "Epoch [1495/20000], Loss: 56.00135803222656, Learning Rate: 0.01\n",
      "Epoch [1496/20000], Loss: 55.982666015625, Learning Rate: 0.01\n",
      "Epoch [1497/20000], Loss: 55.964141845703125, Learning Rate: 0.01\n",
      "Epoch [1498/20000], Loss: 55.94566345214844, Learning Rate: 0.01\n",
      "Epoch [1499/20000], Loss: 55.92716979980469, Learning Rate: 0.01\n",
      "Epoch [1500/20000], Loss: 55.90852355957031, Learning Rate: 0.01\n",
      "Epoch [1501/20000], Loss: 55.890045166015625, Learning Rate: 0.01\n",
      "Epoch [1502/20000], Loss: 55.87138366699219, Learning Rate: 0.01\n",
      "Epoch [1503/20000], Loss: 55.852935791015625, Learning Rate: 0.01\n",
      "Epoch [1504/20000], Loss: 55.83430480957031, Learning Rate: 0.01\n",
      "Epoch [1505/20000], Loss: 55.81587219238281, Learning Rate: 0.01\n",
      "Epoch [1506/20000], Loss: 55.79743957519531, Learning Rate: 0.01\n",
      "Epoch [1507/20000], Loss: 55.778961181640625, Learning Rate: 0.01\n",
      "Epoch [1508/20000], Loss: 55.76048278808594, Learning Rate: 0.01\n",
      "Epoch [1509/20000], Loss: 55.7420654296875, Learning Rate: 0.01\n",
      "Epoch [1510/20000], Loss: 55.723541259765625, Learning Rate: 0.01\n",
      "Epoch [1511/20000], Loss: 55.705078125, Learning Rate: 0.01\n",
      "Epoch [1512/20000], Loss: 55.68670654296875, Learning Rate: 0.01\n",
      "Epoch [1513/20000], Loss: 55.66825866699219, Learning Rate: 0.01\n",
      "Epoch [1514/20000], Loss: 55.64985656738281, Learning Rate: 0.01\n",
      "Epoch [1515/20000], Loss: 55.631439208984375, Learning Rate: 0.01\n",
      "Epoch [1516/20000], Loss: 55.612945556640625, Learning Rate: 0.01\n",
      "Epoch [1517/20000], Loss: 55.594635009765625, Learning Rate: 0.01\n",
      "Epoch [1518/20000], Loss: 55.57627868652344, Learning Rate: 0.01\n",
      "Epoch [1519/20000], Loss: 55.557861328125, Learning Rate: 0.01\n",
      "Epoch [1520/20000], Loss: 55.53944396972656, Learning Rate: 0.01\n",
      "Epoch [1521/20000], Loss: 55.52107238769531, Learning Rate: 0.01\n",
      "Epoch [1522/20000], Loss: 55.50276184082031, Learning Rate: 0.01\n",
      "Epoch [1523/20000], Loss: 55.48448181152344, Learning Rate: 0.01\n",
      "Epoch [1524/20000], Loss: 55.46612548828125, Learning Rate: 0.01\n",
      "Epoch [1525/20000], Loss: 55.44776916503906, Learning Rate: 0.01\n",
      "Epoch [1526/20000], Loss: 55.42942810058594, Learning Rate: 0.01\n",
      "Epoch [1527/20000], Loss: 55.411224365234375, Learning Rate: 0.01\n",
      "Epoch [1528/20000], Loss: 55.39286804199219, Learning Rate: 0.01\n",
      "Epoch [1529/20000], Loss: 55.374603271484375, Learning Rate: 0.01\n",
      "Epoch [1530/20000], Loss: 55.35627746582031, Learning Rate: 0.01\n",
      "Epoch [1531/20000], Loss: 55.33811950683594, Learning Rate: 0.01\n",
      "Epoch [1532/20000], Loss: 55.31977844238281, Learning Rate: 0.01\n",
      "Epoch [1533/20000], Loss: 55.3016357421875, Learning Rate: 0.01\n",
      "Epoch [1534/20000], Loss: 55.28343200683594, Learning Rate: 0.01\n",
      "Epoch [1535/20000], Loss: 55.265106201171875, Learning Rate: 0.01\n",
      "Epoch [1536/20000], Loss: 55.246826171875, Learning Rate: 0.01\n",
      "Epoch [1537/20000], Loss: 55.228546142578125, Learning Rate: 0.01\n",
      "Epoch [1538/20000], Loss: 55.21052551269531, Learning Rate: 0.01\n",
      "Epoch [1539/20000], Loss: 55.192352294921875, Learning Rate: 0.01\n",
      "Epoch [1540/20000], Loss: 55.17411804199219, Learning Rate: 0.01\n",
      "Epoch [1541/20000], Loss: 55.155853271484375, Learning Rate: 0.01\n",
      "Epoch [1542/20000], Loss: 55.13775634765625, Learning Rate: 0.01\n",
      "Epoch [1543/20000], Loss: 55.11962890625, Learning Rate: 0.01\n",
      "Epoch [1544/20000], Loss: 55.10139465332031, Learning Rate: 0.01\n",
      "Epoch [1545/20000], Loss: 55.08343505859375, Learning Rate: 0.01\n",
      "Epoch [1546/20000], Loss: 55.06513977050781, Learning Rate: 0.01\n",
      "Epoch [1547/20000], Loss: 55.04701232910156, Learning Rate: 0.01\n",
      "Epoch [1548/20000], Loss: 55.02891540527344, Learning Rate: 0.01\n",
      "Epoch [1549/20000], Loss: 55.01072692871094, Learning Rate: 0.01\n",
      "Epoch [1550/20000], Loss: 54.99267578125, Learning Rate: 0.01\n",
      "Epoch [1551/20000], Loss: 54.97462463378906, Learning Rate: 0.01\n",
      "Epoch [1552/20000], Loss: 54.95658874511719, Learning Rate: 0.01\n",
      "Epoch [1553/20000], Loss: 54.93833923339844, Learning Rate: 0.01\n",
      "Epoch [1554/20000], Loss: 54.92034912109375, Learning Rate: 0.01\n",
      "Epoch [1555/20000], Loss: 54.90234375, Learning Rate: 0.01\n",
      "Epoch [1556/20000], Loss: 54.88426208496094, Learning Rate: 0.01\n",
      "Epoch [1557/20000], Loss: 54.866241455078125, Learning Rate: 0.01\n",
      "Epoch [1558/20000], Loss: 54.848236083984375, Learning Rate: 0.01\n",
      "Epoch [1559/20000], Loss: 54.830169677734375, Learning Rate: 0.01\n",
      "Epoch [1560/20000], Loss: 54.81224060058594, Learning Rate: 0.01\n",
      "Epoch [1561/20000], Loss: 54.79414367675781, Learning Rate: 0.01\n",
      "Epoch [1562/20000], Loss: 54.776123046875, Learning Rate: 0.01\n",
      "Epoch [1563/20000], Loss: 54.758056640625, Learning Rate: 0.01\n",
      "Epoch [1564/20000], Loss: 54.74024963378906, Learning Rate: 0.01\n",
      "Epoch [1565/20000], Loss: 54.72222900390625, Learning Rate: 0.01\n",
      "Epoch [1566/20000], Loss: 54.7042236328125, Learning Rate: 0.01\n",
      "Epoch [1567/20000], Loss: 54.686370849609375, Learning Rate: 0.01\n",
      "Epoch [1568/20000], Loss: 54.66835021972656, Learning Rate: 0.01\n",
      "Epoch [1569/20000], Loss: 54.65046691894531, Learning Rate: 0.01\n",
      "Epoch [1570/20000], Loss: 54.63236999511719, Learning Rate: 0.01\n",
      "Epoch [1571/20000], Loss: 54.61466979980469, Learning Rate: 0.01\n",
      "Epoch [1572/20000], Loss: 54.596710205078125, Learning Rate: 0.01\n",
      "Epoch [1573/20000], Loss: 54.57879638671875, Learning Rate: 0.01\n",
      "Epoch [1574/20000], Loss: 54.560821533203125, Learning Rate: 0.01\n",
      "Epoch [1575/20000], Loss: 54.54298400878906, Learning Rate: 0.01\n",
      "Epoch [1576/20000], Loss: 54.525115966796875, Learning Rate: 0.01\n",
      "Epoch [1577/20000], Loss: 54.50715637207031, Learning Rate: 0.01\n",
      "Epoch [1578/20000], Loss: 54.489288330078125, Learning Rate: 0.01\n",
      "Epoch [1579/20000], Loss: 54.471435546875, Learning Rate: 0.01\n",
      "Epoch [1580/20000], Loss: 54.453582763671875, Learning Rate: 0.01\n",
      "Epoch [1581/20000], Loss: 54.43577575683594, Learning Rate: 0.01\n",
      "Epoch [1582/20000], Loss: 54.417999267578125, Learning Rate: 0.01\n",
      "Epoch [1583/20000], Loss: 54.40010070800781, Learning Rate: 0.01\n",
      "Epoch [1584/20000], Loss: 54.38227844238281, Learning Rate: 0.01\n",
      "Epoch [1585/20000], Loss: 54.36456298828125, Learning Rate: 0.01\n",
      "Epoch [1586/20000], Loss: 54.34686279296875, Learning Rate: 0.01\n",
      "Epoch [1587/20000], Loss: 54.328826904296875, Learning Rate: 0.01\n",
      "Epoch [1588/20000], Loss: 54.31114196777344, Learning Rate: 0.01\n",
      "Epoch [1589/20000], Loss: 54.293487548828125, Learning Rate: 0.01\n",
      "Epoch [1590/20000], Loss: 54.275543212890625, Learning Rate: 0.01\n",
      "Epoch [1591/20000], Loss: 54.25794982910156, Learning Rate: 0.01\n",
      "Epoch [1592/20000], Loss: 54.24009704589844, Learning Rate: 0.01\n",
      "Epoch [1593/20000], Loss: 54.22233581542969, Learning Rate: 0.01\n",
      "Epoch [1594/20000], Loss: 54.20463562011719, Learning Rate: 0.01\n",
      "Epoch [1595/20000], Loss: 54.18687438964844, Learning Rate: 0.01\n",
      "Epoch [1596/20000], Loss: 54.16925048828125, Learning Rate: 0.01\n",
      "Epoch [1597/20000], Loss: 54.151519775390625, Learning Rate: 0.01\n",
      "Epoch [1598/20000], Loss: 54.13380432128906, Learning Rate: 0.01\n",
      "Epoch [1599/20000], Loss: 54.11625671386719, Learning Rate: 0.01\n",
      "Epoch [1600/20000], Loss: 54.09846496582031, Learning Rate: 0.01\n",
      "Epoch [1601/20000], Loss: 54.08074951171875, Learning Rate: 0.01\n",
      "Epoch [1602/20000], Loss: 54.06303405761719, Learning Rate: 0.01\n",
      "Epoch [1603/20000], Loss: 54.04542541503906, Learning Rate: 0.01\n",
      "Epoch [1604/20000], Loss: 54.02790832519531, Learning Rate: 0.01\n",
      "Epoch [1605/20000], Loss: 54.01005554199219, Learning Rate: 0.01\n",
      "Epoch [1606/20000], Loss: 53.99249267578125, Learning Rate: 0.01\n",
      "Epoch [1607/20000], Loss: 53.97496032714844, Learning Rate: 0.01\n",
      "Epoch [1608/20000], Loss: 53.95729064941406, Learning Rate: 0.01\n",
      "Epoch [1609/20000], Loss: 53.9395751953125, Learning Rate: 0.01\n",
      "Epoch [1610/20000], Loss: 53.92207336425781, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1611/20000], Loss: 53.90435791015625, Learning Rate: 0.01\n",
      "Epoch [1612/20000], Loss: 53.88677978515625, Learning Rate: 0.01\n",
      "Epoch [1613/20000], Loss: 53.869293212890625, Learning Rate: 0.01\n",
      "Epoch [1614/20000], Loss: 53.85163879394531, Learning Rate: 0.01\n",
      "Epoch [1615/20000], Loss: 53.83418273925781, Learning Rate: 0.01\n",
      "Epoch [1616/20000], Loss: 53.81646728515625, Learning Rate: 0.01\n",
      "Epoch [1617/20000], Loss: 53.79911804199219, Learning Rate: 0.01\n",
      "Epoch [1618/20000], Loss: 53.78141784667969, Learning Rate: 0.01\n",
      "Epoch [1619/20000], Loss: 53.764068603515625, Learning Rate: 0.01\n",
      "Epoch [1620/20000], Loss: 53.74644470214844, Learning Rate: 0.01\n",
      "Epoch [1621/20000], Loss: 53.728912353515625, Learning Rate: 0.01\n",
      "Epoch [1622/20000], Loss: 53.71134948730469, Learning Rate: 0.01\n",
      "Epoch [1623/20000], Loss: 53.69389343261719, Learning Rate: 0.01\n",
      "Epoch [1624/20000], Loss: 53.67633056640625, Learning Rate: 0.01\n",
      "Epoch [1625/20000], Loss: 53.65901184082031, Learning Rate: 0.01\n",
      "Epoch [1626/20000], Loss: 53.641510009765625, Learning Rate: 0.01\n",
      "Epoch [1627/20000], Loss: 53.62397766113281, Learning Rate: 0.01\n",
      "Epoch [1628/20000], Loss: 53.606536865234375, Learning Rate: 0.01\n",
      "Epoch [1629/20000], Loss: 53.589080810546875, Learning Rate: 0.01\n",
      "Epoch [1630/20000], Loss: 53.571533203125, Learning Rate: 0.01\n",
      "Epoch [1631/20000], Loss: 53.554107666015625, Learning Rate: 0.01\n",
      "Epoch [1632/20000], Loss: 53.536712646484375, Learning Rate: 0.01\n",
      "Epoch [1633/20000], Loss: 53.51934814453125, Learning Rate: 0.01\n",
      "Epoch [1634/20000], Loss: 53.50189208984375, Learning Rate: 0.01\n",
      "Epoch [1635/20000], Loss: 53.48445129394531, Learning Rate: 0.01\n",
      "Epoch [1636/20000], Loss: 53.467041015625, Learning Rate: 0.01\n",
      "Epoch [1637/20000], Loss: 53.449615478515625, Learning Rate: 0.01\n",
      "Epoch [1638/20000], Loss: 53.432373046875, Learning Rate: 0.01\n",
      "Epoch [1639/20000], Loss: 53.41496276855469, Learning Rate: 0.01\n",
      "Epoch [1640/20000], Loss: 53.39759826660156, Learning Rate: 0.01\n",
      "Epoch [1641/20000], Loss: 53.38017272949219, Learning Rate: 0.01\n",
      "Epoch [1642/20000], Loss: 53.36283874511719, Learning Rate: 0.01\n",
      "Epoch [1643/20000], Loss: 53.34541320800781, Learning Rate: 0.01\n",
      "Epoch [1644/20000], Loss: 53.32794189453125, Learning Rate: 0.01\n",
      "Epoch [1645/20000], Loss: 53.31072998046875, Learning Rate: 0.01\n",
      "Epoch [1646/20000], Loss: 53.29347229003906, Learning Rate: 0.01\n",
      "Epoch [1647/20000], Loss: 53.27613830566406, Learning Rate: 0.01\n",
      "Epoch [1648/20000], Loss: 53.25889587402344, Learning Rate: 0.01\n",
      "Epoch [1649/20000], Loss: 53.241546630859375, Learning Rate: 0.01\n",
      "Epoch [1650/20000], Loss: 53.224151611328125, Learning Rate: 0.01\n",
      "Epoch [1651/20000], Loss: 53.2069091796875, Learning Rate: 0.01\n",
      "Epoch [1652/20000], Loss: 53.18951416015625, Learning Rate: 0.01\n",
      "Epoch [1653/20000], Loss: 53.17237854003906, Learning Rate: 0.01\n",
      "Epoch [1654/20000], Loss: 53.1551513671875, Learning Rate: 0.01\n",
      "Epoch [1655/20000], Loss: 53.13795471191406, Learning Rate: 0.01\n",
      "Epoch [1656/20000], Loss: 53.120513916015625, Learning Rate: 0.01\n",
      "Epoch [1657/20000], Loss: 53.10328674316406, Learning Rate: 0.01\n",
      "Epoch [1658/20000], Loss: 53.08619689941406, Learning Rate: 0.01\n",
      "Epoch [1659/20000], Loss: 53.06890869140625, Learning Rate: 0.01\n",
      "Epoch [1660/20000], Loss: 53.05158996582031, Learning Rate: 0.01\n",
      "Epoch [1661/20000], Loss: 53.03443908691406, Learning Rate: 0.01\n",
      "Epoch [1662/20000], Loss: 53.01725769042969, Learning Rate: 0.01\n",
      "Epoch [1663/20000], Loss: 53.00010681152344, Learning Rate: 0.01\n",
      "Epoch [1664/20000], Loss: 52.98283386230469, Learning Rate: 0.01\n",
      "Epoch [1665/20000], Loss: 52.96568298339844, Learning Rate: 0.01\n",
      "Epoch [1666/20000], Loss: 52.94842529296875, Learning Rate: 0.01\n",
      "Epoch [1667/20000], Loss: 52.93132019042969, Learning Rate: 0.01\n",
      "Epoch [1668/20000], Loss: 52.91412353515625, Learning Rate: 0.01\n",
      "Epoch [1669/20000], Loss: 52.89698791503906, Learning Rate: 0.01\n",
      "Epoch [1670/20000], Loss: 52.879791259765625, Learning Rate: 0.01\n",
      "Epoch [1671/20000], Loss: 52.86259460449219, Learning Rate: 0.01\n",
      "Epoch [1672/20000], Loss: 52.84552001953125, Learning Rate: 0.01\n",
      "Epoch [1673/20000], Loss: 52.82832336425781, Learning Rate: 0.01\n",
      "Epoch [1674/20000], Loss: 52.811309814453125, Learning Rate: 0.01\n",
      "Epoch [1675/20000], Loss: 52.79414367675781, Learning Rate: 0.01\n",
      "Epoch [1676/20000], Loss: 52.7769775390625, Learning Rate: 0.01\n",
      "Epoch [1677/20000], Loss: 52.75993347167969, Learning Rate: 0.01\n",
      "Epoch [1678/20000], Loss: 52.742767333984375, Learning Rate: 0.01\n",
      "Epoch [1679/20000], Loss: 52.72576904296875, Learning Rate: 0.01\n",
      "Epoch [1680/20000], Loss: 52.7086181640625, Learning Rate: 0.01\n",
      "Epoch [1681/20000], Loss: 52.69160461425781, Learning Rate: 0.01\n",
      "Epoch [1682/20000], Loss: 52.674560546875, Learning Rate: 0.01\n",
      "Epoch [1683/20000], Loss: 52.657470703125, Learning Rate: 0.01\n",
      "Epoch [1684/20000], Loss: 52.64039611816406, Learning Rate: 0.01\n",
      "Epoch [1685/20000], Loss: 52.62335205078125, Learning Rate: 0.01\n",
      "Epoch [1686/20000], Loss: 52.60627746582031, Learning Rate: 0.01\n",
      "Epoch [1687/20000], Loss: 52.589385986328125, Learning Rate: 0.01\n",
      "Epoch [1688/20000], Loss: 52.572265625, Learning Rate: 0.01\n",
      "Epoch [1689/20000], Loss: 52.55523681640625, Learning Rate: 0.01\n",
      "Epoch [1690/20000], Loss: 52.5382080078125, Learning Rate: 0.01\n",
      "Epoch [1691/20000], Loss: 52.52128601074219, Learning Rate: 0.01\n",
      "Epoch [1692/20000], Loss: 52.50421142578125, Learning Rate: 0.01\n",
      "Epoch [1693/20000], Loss: 52.4871826171875, Learning Rate: 0.01\n",
      "Epoch [1694/20000], Loss: 52.47016906738281, Learning Rate: 0.01\n",
      "Epoch [1695/20000], Loss: 52.45323181152344, Learning Rate: 0.01\n",
      "Epoch [1696/20000], Loss: 52.43620300292969, Learning Rate: 0.01\n",
      "Epoch [1697/20000], Loss: 52.41938781738281, Learning Rate: 0.01\n",
      "Epoch [1698/20000], Loss: 52.40251159667969, Learning Rate: 0.01\n",
      "Epoch [1699/20000], Loss: 52.385528564453125, Learning Rate: 0.01\n",
      "Epoch [1700/20000], Loss: 52.36851501464844, Learning Rate: 0.01\n",
      "Epoch [1701/20000], Loss: 52.35157775878906, Learning Rate: 0.01\n",
      "Epoch [1702/20000], Loss: 52.334686279296875, Learning Rate: 0.01\n",
      "Epoch [1703/20000], Loss: 52.31785583496094, Learning Rate: 0.01\n",
      "Epoch [1704/20000], Loss: 52.30091857910156, Learning Rate: 0.01\n",
      "Epoch [1705/20000], Loss: 52.28395080566406, Learning Rate: 0.01\n",
      "Epoch [1706/20000], Loss: 52.26695251464844, Learning Rate: 0.01\n",
      "Epoch [1707/20000], Loss: 52.25007629394531, Learning Rate: 0.01\n",
      "Epoch [1708/20000], Loss: 52.233154296875, Learning Rate: 0.01\n",
      "Epoch [1709/20000], Loss: 52.216522216796875, Learning Rate: 0.01\n",
      "Epoch [1710/20000], Loss: 52.199462890625, Learning Rate: 0.01\n",
      "Epoch [1711/20000], Loss: 52.182525634765625, Learning Rate: 0.01\n",
      "Epoch [1712/20000], Loss: 52.165740966796875, Learning Rate: 0.01\n",
      "Epoch [1713/20000], Loss: 52.14888000488281, Learning Rate: 0.01\n",
      "Epoch [1714/20000], Loss: 52.131988525390625, Learning Rate: 0.01\n",
      "Epoch [1715/20000], Loss: 52.11524963378906, Learning Rate: 0.01\n",
      "Epoch [1716/20000], Loss: 52.0982666015625, Learning Rate: 0.01\n",
      "Epoch [1717/20000], Loss: 52.08155822753906, Learning Rate: 0.01\n",
      "Epoch [1718/20000], Loss: 52.06477355957031, Learning Rate: 0.01\n",
      "Epoch [1719/20000], Loss: 52.04783630371094, Learning Rate: 0.01\n",
      "Epoch [1720/20000], Loss: 52.030975341796875, Learning Rate: 0.01\n",
      "Epoch [1721/20000], Loss: 52.01426696777344, Learning Rate: 0.01\n",
      "Epoch [1722/20000], Loss: 51.99749755859375, Learning Rate: 0.01\n",
      "Epoch [1723/20000], Loss: 51.9805908203125, Learning Rate: 0.01\n",
      "Epoch [1724/20000], Loss: 51.96392822265625, Learning Rate: 0.01\n",
      "Epoch [1725/20000], Loss: 51.94709777832031, Learning Rate: 0.01\n",
      "Epoch [1726/20000], Loss: 51.930145263671875, Learning Rate: 0.01\n",
      "Epoch [1727/20000], Loss: 51.913482666015625, Learning Rate: 0.01\n",
      "Epoch [1728/20000], Loss: 51.896759033203125, Learning Rate: 0.01\n",
      "Epoch [1729/20000], Loss: 51.88005065917969, Learning Rate: 0.01\n",
      "Epoch [1730/20000], Loss: 51.86322021484375, Learning Rate: 0.01\n",
      "Epoch [1731/20000], Loss: 51.84654235839844, Learning Rate: 0.01\n",
      "Epoch [1732/20000], Loss: 51.82994079589844, Learning Rate: 0.01\n",
      "Epoch [1733/20000], Loss: 51.81300354003906, Learning Rate: 0.01\n",
      "Epoch [1734/20000], Loss: 51.79634094238281, Learning Rate: 0.01\n",
      "Epoch [1735/20000], Loss: 51.77964782714844, Learning Rate: 0.01\n",
      "Epoch [1736/20000], Loss: 51.76292419433594, Learning Rate: 0.01\n",
      "Epoch [1737/20000], Loss: 51.7462158203125, Learning Rate: 0.01\n",
      "Epoch [1738/20000], Loss: 51.72955322265625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1739/20000], Loss: 51.71296691894531, Learning Rate: 0.01\n",
      "Epoch [1740/20000], Loss: 51.696136474609375, Learning Rate: 0.01\n",
      "Epoch [1741/20000], Loss: 51.67951965332031, Learning Rate: 0.01\n",
      "Epoch [1742/20000], Loss: 51.66288757324219, Learning Rate: 0.01\n",
      "Epoch [1743/20000], Loss: 51.64619445800781, Learning Rate: 0.01\n",
      "Epoch [1744/20000], Loss: 51.629638671875, Learning Rate: 0.01\n",
      "Epoch [1745/20000], Loss: 51.61289978027344, Learning Rate: 0.01\n",
      "Epoch [1746/20000], Loss: 51.59632873535156, Learning Rate: 0.01\n",
      "Epoch [1747/20000], Loss: 51.57957458496094, Learning Rate: 0.01\n",
      "Epoch [1748/20000], Loss: 51.562835693359375, Learning Rate: 0.01\n",
      "Epoch [1749/20000], Loss: 51.54634094238281, Learning Rate: 0.01\n",
      "Epoch [1750/20000], Loss: 51.529693603515625, Learning Rate: 0.01\n",
      "Epoch [1751/20000], Loss: 51.512939453125, Learning Rate: 0.01\n",
      "Epoch [1752/20000], Loss: 51.49668884277344, Learning Rate: 0.01\n",
      "Epoch [1753/20000], Loss: 51.47990417480469, Learning Rate: 0.01\n",
      "Epoch [1754/20000], Loss: 51.46330261230469, Learning Rate: 0.01\n",
      "Epoch [1755/20000], Loss: 51.44673156738281, Learning Rate: 0.01\n",
      "Epoch [1756/20000], Loss: 51.43017578125, Learning Rate: 0.01\n",
      "Epoch [1757/20000], Loss: 51.413482666015625, Learning Rate: 0.01\n",
      "Epoch [1758/20000], Loss: 51.39698791503906, Learning Rate: 0.01\n",
      "Epoch [1759/20000], Loss: 51.38037109375, Learning Rate: 0.01\n",
      "Epoch [1760/20000], Loss: 51.36381530761719, Learning Rate: 0.01\n",
      "Epoch [1761/20000], Loss: 51.347320556640625, Learning Rate: 0.01\n",
      "Epoch [1762/20000], Loss: 51.33073425292969, Learning Rate: 0.01\n",
      "Epoch [1763/20000], Loss: 51.314208984375, Learning Rate: 0.01\n",
      "Epoch [1764/20000], Loss: 51.29766845703125, Learning Rate: 0.01\n",
      "Epoch [1765/20000], Loss: 51.28106689453125, Learning Rate: 0.01\n",
      "Epoch [1766/20000], Loss: 51.26458740234375, Learning Rate: 0.01\n",
      "Epoch [1767/20000], Loss: 51.24812316894531, Learning Rate: 0.01\n",
      "Epoch [1768/20000], Loss: 51.2315673828125, Learning Rate: 0.01\n",
      "Epoch [1769/20000], Loss: 51.215087890625, Learning Rate: 0.01\n",
      "Epoch [1770/20000], Loss: 51.19865417480469, Learning Rate: 0.01\n",
      "Epoch [1771/20000], Loss: 51.182159423828125, Learning Rate: 0.01\n",
      "Epoch [1772/20000], Loss: 51.16566467285156, Learning Rate: 0.01\n",
      "Epoch [1773/20000], Loss: 51.14924621582031, Learning Rate: 0.01\n",
      "Epoch [1774/20000], Loss: 51.13262939453125, Learning Rate: 0.01\n",
      "Epoch [1775/20000], Loss: 51.11619567871094, Learning Rate: 0.01\n",
      "Epoch [1776/20000], Loss: 51.09977722167969, Learning Rate: 0.01\n",
      "Epoch [1777/20000], Loss: 51.08323669433594, Learning Rate: 0.01\n",
      "Epoch [1778/20000], Loss: 51.06684875488281, Learning Rate: 0.01\n",
      "Epoch [1779/20000], Loss: 51.05027770996094, Learning Rate: 0.01\n",
      "Epoch [1780/20000], Loss: 51.03385925292969, Learning Rate: 0.01\n",
      "Epoch [1781/20000], Loss: 51.01756286621094, Learning Rate: 0.01\n",
      "Epoch [1782/20000], Loss: 51.001068115234375, Learning Rate: 0.01\n",
      "Epoch [1783/20000], Loss: 50.984649658203125, Learning Rate: 0.01\n",
      "Epoch [1784/20000], Loss: 50.96820068359375, Learning Rate: 0.01\n",
      "Epoch [1785/20000], Loss: 50.95195007324219, Learning Rate: 0.01\n",
      "Epoch [1786/20000], Loss: 50.93537902832031, Learning Rate: 0.01\n",
      "Epoch [1787/20000], Loss: 50.91899108886719, Learning Rate: 0.01\n",
      "Epoch [1788/20000], Loss: 50.90252685546875, Learning Rate: 0.01\n",
      "Epoch [1789/20000], Loss: 50.88615417480469, Learning Rate: 0.01\n",
      "Epoch [1790/20000], Loss: 50.869903564453125, Learning Rate: 0.01\n",
      "Epoch [1791/20000], Loss: 50.853515625, Learning Rate: 0.01\n",
      "Epoch [1792/20000], Loss: 50.83717346191406, Learning Rate: 0.01\n",
      "Epoch [1793/20000], Loss: 50.82073974609375, Learning Rate: 0.01\n",
      "Epoch [1794/20000], Loss: 50.804351806640625, Learning Rate: 0.01\n",
      "Epoch [1795/20000], Loss: 50.78785705566406, Learning Rate: 0.01\n",
      "Epoch [1796/20000], Loss: 50.77165222167969, Learning Rate: 0.01\n",
      "Epoch [1797/20000], Loss: 50.755340576171875, Learning Rate: 0.01\n",
      "Epoch [1798/20000], Loss: 50.73899841308594, Learning Rate: 0.01\n",
      "Epoch [1799/20000], Loss: 50.722625732421875, Learning Rate: 0.01\n",
      "Epoch [1800/20000], Loss: 50.70635986328125, Learning Rate: 0.01\n",
      "Epoch [1801/20000], Loss: 50.69001770019531, Learning Rate: 0.01\n",
      "Epoch [1802/20000], Loss: 50.67375183105469, Learning Rate: 0.01\n",
      "Epoch [1803/20000], Loss: 50.65736389160156, Learning Rate: 0.01\n",
      "Epoch [1804/20000], Loss: 50.64111328125, Learning Rate: 0.01\n",
      "Epoch [1805/20000], Loss: 50.62489318847656, Learning Rate: 0.01\n",
      "Epoch [1806/20000], Loss: 50.608428955078125, Learning Rate: 0.01\n",
      "Epoch [1807/20000], Loss: 50.59210205078125, Learning Rate: 0.01\n",
      "Epoch [1808/20000], Loss: 50.575775146484375, Learning Rate: 0.01\n",
      "Epoch [1809/20000], Loss: 50.5595703125, Learning Rate: 0.01\n",
      "Epoch [1810/20000], Loss: 50.54328918457031, Learning Rate: 0.01\n",
      "Epoch [1811/20000], Loss: 50.52714538574219, Learning Rate: 0.01\n",
      "Epoch [1812/20000], Loss: 50.51080322265625, Learning Rate: 0.01\n",
      "Epoch [1813/20000], Loss: 50.49461364746094, Learning Rate: 0.01\n",
      "Epoch [1814/20000], Loss: 50.47833251953125, Learning Rate: 0.01\n",
      "Epoch [1815/20000], Loss: 50.46202087402344, Learning Rate: 0.01\n",
      "Epoch [1816/20000], Loss: 50.44584655761719, Learning Rate: 0.01\n",
      "Epoch [1817/20000], Loss: 50.429595947265625, Learning Rate: 0.01\n",
      "Epoch [1818/20000], Loss: 50.41337585449219, Learning Rate: 0.01\n",
      "Epoch [1819/20000], Loss: 50.39707946777344, Learning Rate: 0.01\n",
      "Epoch [1820/20000], Loss: 50.380889892578125, Learning Rate: 0.01\n",
      "Epoch [1821/20000], Loss: 50.36463928222656, Learning Rate: 0.01\n",
      "Epoch [1822/20000], Loss: 50.34846496582031, Learning Rate: 0.01\n",
      "Epoch [1823/20000], Loss: 50.33233642578125, Learning Rate: 0.01\n",
      "Epoch [1824/20000], Loss: 50.31602478027344, Learning Rate: 0.01\n",
      "Epoch [1825/20000], Loss: 50.29988098144531, Learning Rate: 0.01\n",
      "Epoch [1826/20000], Loss: 50.28367614746094, Learning Rate: 0.01\n",
      "Epoch [1827/20000], Loss: 50.267608642578125, Learning Rate: 0.01\n",
      "Epoch [1828/20000], Loss: 50.251495361328125, Learning Rate: 0.01\n",
      "Epoch [1829/20000], Loss: 50.23521423339844, Learning Rate: 0.01\n",
      "Epoch [1830/20000], Loss: 50.219024658203125, Learning Rate: 0.01\n",
      "Epoch [1831/20000], Loss: 50.20283508300781, Learning Rate: 0.01\n",
      "Epoch [1832/20000], Loss: 50.186737060546875, Learning Rate: 0.01\n",
      "Epoch [1833/20000], Loss: 50.17060852050781, Learning Rate: 0.01\n",
      "Epoch [1834/20000], Loss: 50.15443420410156, Learning Rate: 0.01\n",
      "Epoch [1835/20000], Loss: 50.1383056640625, Learning Rate: 0.01\n",
      "Epoch [1836/20000], Loss: 50.122283935546875, Learning Rate: 0.01\n",
      "Epoch [1837/20000], Loss: 50.10594177246094, Learning Rate: 0.01\n",
      "Epoch [1838/20000], Loss: 50.0899658203125, Learning Rate: 0.01\n",
      "Epoch [1839/20000], Loss: 50.07366943359375, Learning Rate: 0.01\n",
      "Epoch [1840/20000], Loss: 50.05760192871094, Learning Rate: 0.01\n",
      "Epoch [1841/20000], Loss: 50.04158020019531, Learning Rate: 0.01\n",
      "Epoch [1842/20000], Loss: 50.025421142578125, Learning Rate: 0.01\n",
      "Epoch [1843/20000], Loss: 50.00929260253906, Learning Rate: 0.01\n",
      "Epoch [1844/20000], Loss: 49.99322509765625, Learning Rate: 0.01\n",
      "Epoch [1845/20000], Loss: 49.97721862792969, Learning Rate: 0.01\n",
      "Epoch [1846/20000], Loss: 49.9610595703125, Learning Rate: 0.01\n",
      "Epoch [1847/20000], Loss: 49.9449462890625, Learning Rate: 0.01\n",
      "Epoch [1848/20000], Loss: 49.92893981933594, Learning Rate: 0.01\n",
      "Epoch [1849/20000], Loss: 49.91276550292969, Learning Rate: 0.01\n",
      "Epoch [1850/20000], Loss: 49.896759033203125, Learning Rate: 0.01\n",
      "Epoch [1851/20000], Loss: 49.880706787109375, Learning Rate: 0.01\n",
      "Epoch [1852/20000], Loss: 49.86468505859375, Learning Rate: 0.01\n",
      "Epoch [1853/20000], Loss: 49.848724365234375, Learning Rate: 0.01\n",
      "Epoch [1854/20000], Loss: 49.83256530761719, Learning Rate: 0.01\n",
      "Epoch [1855/20000], Loss: 49.81651306152344, Learning Rate: 0.01\n",
      "Epoch [1856/20000], Loss: 49.800506591796875, Learning Rate: 0.01\n",
      "Epoch [1857/20000], Loss: 49.784423828125, Learning Rate: 0.01\n",
      "Epoch [1858/20000], Loss: 49.76844787597656, Learning Rate: 0.01\n",
      "Epoch [1859/20000], Loss: 49.75254821777344, Learning Rate: 0.01\n",
      "Epoch [1860/20000], Loss: 49.736358642578125, Learning Rate: 0.01\n",
      "Epoch [1861/20000], Loss: 49.72035217285156, Learning Rate: 0.01\n",
      "Epoch [1862/20000], Loss: 49.70442199707031, Learning Rate: 0.01\n",
      "Epoch [1863/20000], Loss: 49.68849182128906, Learning Rate: 0.01\n",
      "Epoch [1864/20000], Loss: 49.67240905761719, Learning Rate: 0.01\n",
      "Epoch [1865/20000], Loss: 49.65643310546875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1866/20000], Loss: 49.640380859375, Learning Rate: 0.01\n",
      "Epoch [1867/20000], Loss: 49.62451171875, Learning Rate: 0.01\n",
      "Epoch [1868/20000], Loss: 49.60850524902344, Learning Rate: 0.01\n",
      "Epoch [1869/20000], Loss: 49.592437744140625, Learning Rate: 0.01\n",
      "Epoch [1870/20000], Loss: 49.57661437988281, Learning Rate: 0.01\n",
      "Epoch [1871/20000], Loss: 49.560638427734375, Learning Rate: 0.01\n",
      "Epoch [1872/20000], Loss: 49.54461669921875, Learning Rate: 0.01\n",
      "Epoch [1873/20000], Loss: 49.52870178222656, Learning Rate: 0.01\n",
      "Epoch [1874/20000], Loss: 49.51261901855469, Learning Rate: 0.01\n",
      "Epoch [1875/20000], Loss: 49.4967041015625, Learning Rate: 0.01\n",
      "Epoch [1876/20000], Loss: 49.48088073730469, Learning Rate: 0.01\n",
      "Epoch [1877/20000], Loss: 49.464935302734375, Learning Rate: 0.01\n",
      "Epoch [1878/20000], Loss: 49.448974609375, Learning Rate: 0.01\n",
      "Epoch [1879/20000], Loss: 49.43299865722656, Learning Rate: 0.01\n",
      "Epoch [1880/20000], Loss: 49.41725158691406, Learning Rate: 0.01\n",
      "Epoch [1881/20000], Loss: 49.4012451171875, Learning Rate: 0.01\n",
      "Epoch [1882/20000], Loss: 49.38543701171875, Learning Rate: 0.01\n",
      "Epoch [1883/20000], Loss: 49.369415283203125, Learning Rate: 0.01\n",
      "Epoch [1884/20000], Loss: 49.35340881347656, Learning Rate: 0.01\n",
      "Epoch [1885/20000], Loss: 49.337615966796875, Learning Rate: 0.01\n",
      "Epoch [1886/20000], Loss: 49.321685791015625, Learning Rate: 0.01\n",
      "Epoch [1887/20000], Loss: 49.30580139160156, Learning Rate: 0.01\n",
      "Epoch [1888/20000], Loss: 49.28981018066406, Learning Rate: 0.01\n",
      "Epoch [1889/20000], Loss: 49.27410888671875, Learning Rate: 0.01\n",
      "Epoch [1890/20000], Loss: 49.25816345214844, Learning Rate: 0.01\n",
      "Epoch [1891/20000], Loss: 49.24241638183594, Learning Rate: 0.01\n",
      "Epoch [1892/20000], Loss: 49.22636413574219, Learning Rate: 0.01\n",
      "Epoch [1893/20000], Loss: 49.21055603027344, Learning Rate: 0.01\n",
      "Epoch [1894/20000], Loss: 49.19474792480469, Learning Rate: 0.01\n",
      "Epoch [1895/20000], Loss: 49.17887878417969, Learning Rate: 0.01\n",
      "Epoch [1896/20000], Loss: 49.163055419921875, Learning Rate: 0.01\n",
      "Epoch [1897/20000], Loss: 49.14723205566406, Learning Rate: 0.01\n",
      "Epoch [1898/20000], Loss: 49.131439208984375, Learning Rate: 0.01\n",
      "Epoch [1899/20000], Loss: 49.11543273925781, Learning Rate: 0.01\n",
      "Epoch [1900/20000], Loss: 49.09967041015625, Learning Rate: 0.01\n",
      "Epoch [1901/20000], Loss: 49.08381652832031, Learning Rate: 0.01\n",
      "Epoch [1902/20000], Loss: 49.06816101074219, Learning Rate: 0.01\n",
      "Epoch [1903/20000], Loss: 49.05235290527344, Learning Rate: 0.01\n",
      "Epoch [1904/20000], Loss: 49.03648376464844, Learning Rate: 0.01\n",
      "Epoch [1905/20000], Loss: 49.02073669433594, Learning Rate: 0.01\n",
      "Epoch [1906/20000], Loss: 49.00486755371094, Learning Rate: 0.01\n",
      "Epoch [1907/20000], Loss: 48.98899841308594, Learning Rate: 0.01\n",
      "Epoch [1908/20000], Loss: 48.973297119140625, Learning Rate: 0.01\n",
      "Epoch [1909/20000], Loss: 48.95738220214844, Learning Rate: 0.01\n",
      "Epoch [1910/20000], Loss: 48.941558837890625, Learning Rate: 0.01\n",
      "Epoch [1911/20000], Loss: 48.92588806152344, Learning Rate: 0.01\n",
      "Epoch [1912/20000], Loss: 48.91015625, Learning Rate: 0.01\n",
      "Epoch [1913/20000], Loss: 48.89434814453125, Learning Rate: 0.01\n",
      "Epoch [1914/20000], Loss: 48.87849426269531, Learning Rate: 0.01\n",
      "Epoch [1915/20000], Loss: 48.86274719238281, Learning Rate: 0.01\n",
      "Epoch [1916/20000], Loss: 48.84718322753906, Learning Rate: 0.01\n",
      "Epoch [1917/20000], Loss: 48.83125305175781, Learning Rate: 0.01\n",
      "Epoch [1918/20000], Loss: 48.81565856933594, Learning Rate: 0.01\n",
      "Epoch [1919/20000], Loss: 48.79981994628906, Learning Rate: 0.01\n",
      "Epoch [1920/20000], Loss: 48.784088134765625, Learning Rate: 0.01\n",
      "Epoch [1921/20000], Loss: 48.76826477050781, Learning Rate: 0.01\n",
      "Epoch [1922/20000], Loss: 48.75263977050781, Learning Rate: 0.01\n",
      "Epoch [1923/20000], Loss: 48.736907958984375, Learning Rate: 0.01\n",
      "Epoch [1924/20000], Loss: 48.72119140625, Learning Rate: 0.01\n",
      "Epoch [1925/20000], Loss: 48.7054443359375, Learning Rate: 0.01\n",
      "Epoch [1926/20000], Loss: 48.689788818359375, Learning Rate: 0.01\n",
      "Epoch [1927/20000], Loss: 48.67405700683594, Learning Rate: 0.01\n",
      "Epoch [1928/20000], Loss: 48.65838623046875, Learning Rate: 0.01\n",
      "Epoch [1929/20000], Loss: 48.64268493652344, Learning Rate: 0.01\n",
      "Epoch [1930/20000], Loss: 48.62693786621094, Learning Rate: 0.01\n",
      "Epoch [1931/20000], Loss: 48.61103820800781, Learning Rate: 0.01\n",
      "Epoch [1932/20000], Loss: 48.59553527832031, Learning Rate: 0.01\n",
      "Epoch [1933/20000], Loss: 48.57984924316406, Learning Rate: 0.01\n",
      "Epoch [1934/20000], Loss: 48.56416320800781, Learning Rate: 0.01\n",
      "Epoch [1935/20000], Loss: 48.54852294921875, Learning Rate: 0.01\n",
      "Epoch [1936/20000], Loss: 48.53282165527344, Learning Rate: 0.01\n",
      "Epoch [1937/20000], Loss: 48.51707458496094, Learning Rate: 0.01\n",
      "Epoch [1938/20000], Loss: 48.50154113769531, Learning Rate: 0.01\n",
      "Epoch [1939/20000], Loss: 48.48579406738281, Learning Rate: 0.01\n",
      "Epoch [1940/20000], Loss: 48.47015380859375, Learning Rate: 0.01\n",
      "Epoch [1941/20000], Loss: 48.45452880859375, Learning Rate: 0.01\n",
      "Epoch [1942/20000], Loss: 48.43890380859375, Learning Rate: 0.01\n",
      "Epoch [1943/20000], Loss: 48.42329406738281, Learning Rate: 0.01\n",
      "Epoch [1944/20000], Loss: 48.407562255859375, Learning Rate: 0.01\n",
      "Epoch [1945/20000], Loss: 48.39202880859375, Learning Rate: 0.01\n",
      "Epoch [1946/20000], Loss: 48.376312255859375, Learning Rate: 0.01\n",
      "Epoch [1947/20000], Loss: 48.36077880859375, Learning Rate: 0.01\n",
      "Epoch [1948/20000], Loss: 48.34513854980469, Learning Rate: 0.01\n",
      "Epoch [1949/20000], Loss: 48.32936096191406, Learning Rate: 0.01\n",
      "Epoch [1950/20000], Loss: 48.31390380859375, Learning Rate: 0.01\n",
      "Epoch [1951/20000], Loss: 48.29823303222656, Learning Rate: 0.01\n",
      "Epoch [1952/20000], Loss: 48.28253173828125, Learning Rate: 0.01\n",
      "Epoch [1953/20000], Loss: 48.2669677734375, Learning Rate: 0.01\n",
      "Epoch [1954/20000], Loss: 48.251373291015625, Learning Rate: 0.01\n",
      "Epoch [1955/20000], Loss: 48.235870361328125, Learning Rate: 0.01\n",
      "Epoch [1956/20000], Loss: 48.22027587890625, Learning Rate: 0.01\n",
      "Epoch [1957/20000], Loss: 48.20452880859375, Learning Rate: 0.01\n",
      "Epoch [1958/20000], Loss: 48.18901062011719, Learning Rate: 0.01\n",
      "Epoch [1959/20000], Loss: 48.17350769042969, Learning Rate: 0.01\n",
      "Epoch [1960/20000], Loss: 48.15782165527344, Learning Rate: 0.01\n",
      "Epoch [1961/20000], Loss: 48.14231872558594, Learning Rate: 0.01\n",
      "Epoch [1962/20000], Loss: 48.12672424316406, Learning Rate: 0.01\n",
      "Epoch [1963/20000], Loss: 48.111083984375, Learning Rate: 0.01\n",
      "Epoch [1964/20000], Loss: 48.09568786621094, Learning Rate: 0.01\n",
      "Epoch [1965/20000], Loss: 48.080047607421875, Learning Rate: 0.01\n",
      "Epoch [1966/20000], Loss: 48.064544677734375, Learning Rate: 0.01\n",
      "Epoch [1967/20000], Loss: 48.04901123046875, Learning Rate: 0.01\n",
      "Epoch [1968/20000], Loss: 48.03349304199219, Learning Rate: 0.01\n",
      "Epoch [1969/20000], Loss: 48.01789855957031, Learning Rate: 0.01\n",
      "Epoch [1970/20000], Loss: 48.00233459472656, Learning Rate: 0.01\n",
      "Epoch [1971/20000], Loss: 47.98677062988281, Learning Rate: 0.01\n",
      "Epoch [1972/20000], Loss: 47.97125244140625, Learning Rate: 0.01\n",
      "Epoch [1973/20000], Loss: 47.95579528808594, Learning Rate: 0.01\n",
      "Epoch [1974/20000], Loss: 47.940216064453125, Learning Rate: 0.01\n",
      "Epoch [1975/20000], Loss: 47.924591064453125, Learning Rate: 0.01\n",
      "Epoch [1976/20000], Loss: 47.90922546386719, Learning Rate: 0.01\n",
      "Epoch [1977/20000], Loss: 47.89369201660156, Learning Rate: 0.01\n",
      "Epoch [1978/20000], Loss: 47.878143310546875, Learning Rate: 0.01\n",
      "Epoch [1979/20000], Loss: 47.86274719238281, Learning Rate: 0.01\n",
      "Epoch [1980/20000], Loss: 47.84716796875, Learning Rate: 0.01\n",
      "Epoch [1981/20000], Loss: 47.83164978027344, Learning Rate: 0.01\n",
      "Epoch [1982/20000], Loss: 47.81617736816406, Learning Rate: 0.01\n",
      "Epoch [1983/20000], Loss: 47.800689697265625, Learning Rate: 0.01\n",
      "Epoch [1984/20000], Loss: 47.78517150878906, Learning Rate: 0.01\n",
      "Epoch [1985/20000], Loss: 47.769683837890625, Learning Rate: 0.01\n",
      "Epoch [1986/20000], Loss: 47.75421142578125, Learning Rate: 0.01\n",
      "Epoch [1987/20000], Loss: 47.73872375488281, Learning Rate: 0.01\n",
      "Epoch [1988/20000], Loss: 47.72328186035156, Learning Rate: 0.01\n",
      "Epoch [1989/20000], Loss: 47.7078857421875, Learning Rate: 0.01\n",
      "Epoch [1990/20000], Loss: 47.69252014160156, Learning Rate: 0.01\n",
      "Epoch [1991/20000], Loss: 47.67695617675781, Learning Rate: 0.01\n",
      "Epoch [1992/20000], Loss: 47.66133117675781, Learning Rate: 0.01\n",
      "Epoch [1993/20000], Loss: 47.64607238769531, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1994/20000], Loss: 47.63067626953125, Learning Rate: 0.01\n",
      "Epoch [1995/20000], Loss: 47.615081787109375, Learning Rate: 0.01\n",
      "Epoch [1996/20000], Loss: 47.599761962890625, Learning Rate: 0.01\n",
      "Epoch [1997/20000], Loss: 47.58424377441406, Learning Rate: 0.01\n",
      "Epoch [1998/20000], Loss: 47.5687255859375, Learning Rate: 0.01\n",
      "Epoch [1999/20000], Loss: 47.55340576171875, Learning Rate: 0.01\n",
      "Epoch [2000/20000], Loss: 47.5379638671875, Learning Rate: 0.01\n",
      "Epoch [2001/20000], Loss: 47.52253723144531, Learning Rate: 0.01\n",
      "Epoch [2002/20000], Loss: 47.50715637207031, Learning Rate: 0.01\n",
      "Epoch [2003/20000], Loss: 47.49165344238281, Learning Rate: 0.01\n",
      "Epoch [2004/20000], Loss: 47.476318359375, Learning Rate: 0.01\n",
      "Epoch [2005/20000], Loss: 47.46086120605469, Learning Rate: 0.01\n",
      "Epoch [2006/20000], Loss: 47.445465087890625, Learning Rate: 0.01\n",
      "Epoch [2007/20000], Loss: 47.43003845214844, Learning Rate: 0.01\n",
      "Epoch [2008/20000], Loss: 47.41462707519531, Learning Rate: 0.01\n",
      "Epoch [2009/20000], Loss: 47.39933776855469, Learning Rate: 0.01\n",
      "Epoch [2010/20000], Loss: 47.38398742675781, Learning Rate: 0.01\n",
      "Epoch [2011/20000], Loss: 47.36842346191406, Learning Rate: 0.01\n",
      "Epoch [2012/20000], Loss: 47.35308837890625, Learning Rate: 0.01\n",
      "Epoch [2013/20000], Loss: 47.33775329589844, Learning Rate: 0.01\n",
      "Epoch [2014/20000], Loss: 47.32231140136719, Learning Rate: 0.01\n",
      "Epoch [2015/20000], Loss: 47.30696105957031, Learning Rate: 0.01\n",
      "Epoch [2016/20000], Loss: 47.29167175292969, Learning Rate: 0.01\n",
      "Epoch [2017/20000], Loss: 47.27618408203125, Learning Rate: 0.01\n",
      "Epoch [2018/20000], Loss: 47.260833740234375, Learning Rate: 0.01\n",
      "Epoch [2019/20000], Loss: 47.24546813964844, Learning Rate: 0.01\n",
      "Epoch [2020/20000], Loss: 47.23002624511719, Learning Rate: 0.01\n",
      "Epoch [2021/20000], Loss: 47.21485900878906, Learning Rate: 0.01\n",
      "Epoch [2022/20000], Loss: 47.19947814941406, Learning Rate: 0.01\n",
      "Epoch [2023/20000], Loss: 47.184112548828125, Learning Rate: 0.01\n",
      "Epoch [2024/20000], Loss: 47.16864013671875, Learning Rate: 0.01\n",
      "Epoch [2025/20000], Loss: 47.15348815917969, Learning Rate: 0.01\n",
      "Epoch [2026/20000], Loss: 47.13810729980469, Learning Rate: 0.01\n",
      "Epoch [2027/20000], Loss: 47.12275695800781, Learning Rate: 0.01\n",
      "Epoch [2028/20000], Loss: 47.10758972167969, Learning Rate: 0.01\n",
      "Epoch [2029/20000], Loss: 47.09210205078125, Learning Rate: 0.01\n",
      "Epoch [2030/20000], Loss: 47.076690673828125, Learning Rate: 0.01\n",
      "Epoch [2031/20000], Loss: 47.06132507324219, Learning Rate: 0.01\n",
      "Epoch [2032/20000], Loss: 47.04608154296875, Learning Rate: 0.01\n",
      "Epoch [2033/20000], Loss: 47.030792236328125, Learning Rate: 0.01\n",
      "Epoch [2034/20000], Loss: 47.015411376953125, Learning Rate: 0.01\n",
      "Epoch [2035/20000], Loss: 47.00019836425781, Learning Rate: 0.01\n",
      "Epoch [2036/20000], Loss: 46.98492431640625, Learning Rate: 0.01\n",
      "Epoch [2037/20000], Loss: 46.9696044921875, Learning Rate: 0.01\n",
      "Epoch [2038/20000], Loss: 46.95428466796875, Learning Rate: 0.01\n",
      "Epoch [2039/20000], Loss: 46.938934326171875, Learning Rate: 0.01\n",
      "Epoch [2040/20000], Loss: 46.92362976074219, Learning Rate: 0.01\n",
      "Epoch [2041/20000], Loss: 46.90837097167969, Learning Rate: 0.01\n",
      "Epoch [2042/20000], Loss: 46.893096923828125, Learning Rate: 0.01\n",
      "Epoch [2043/20000], Loss: 46.87794494628906, Learning Rate: 0.01\n",
      "Epoch [2044/20000], Loss: 46.86253356933594, Learning Rate: 0.01\n",
      "Epoch [2045/20000], Loss: 46.8472900390625, Learning Rate: 0.01\n",
      "Epoch [2046/20000], Loss: 46.83195495605469, Learning Rate: 0.01\n",
      "Epoch [2047/20000], Loss: 46.81671142578125, Learning Rate: 0.01\n",
      "Epoch [2048/20000], Loss: 46.80155944824219, Learning Rate: 0.01\n",
      "Epoch [2049/20000], Loss: 46.7862548828125, Learning Rate: 0.01\n",
      "Epoch [2050/20000], Loss: 46.77095031738281, Learning Rate: 0.01\n",
      "Epoch [2051/20000], Loss: 46.75572204589844, Learning Rate: 0.01\n",
      "Epoch [2052/20000], Loss: 46.74046325683594, Learning Rate: 0.01\n",
      "Epoch [2053/20000], Loss: 46.725189208984375, Learning Rate: 0.01\n",
      "Epoch [2054/20000], Loss: 46.70991516113281, Learning Rate: 0.01\n",
      "Epoch [2055/20000], Loss: 46.69471740722656, Learning Rate: 0.01\n",
      "Epoch [2056/20000], Loss: 46.6795654296875, Learning Rate: 0.01\n",
      "Epoch [2057/20000], Loss: 46.664306640625, Learning Rate: 0.01\n",
      "Epoch [2058/20000], Loss: 46.649078369140625, Learning Rate: 0.01\n",
      "Epoch [2059/20000], Loss: 46.63377380371094, Learning Rate: 0.01\n",
      "Epoch [2060/20000], Loss: 46.61866760253906, Learning Rate: 0.01\n",
      "Epoch [2061/20000], Loss: 46.60345458984375, Learning Rate: 0.01\n",
      "Epoch [2062/20000], Loss: 46.58811950683594, Learning Rate: 0.01\n",
      "Epoch [2063/20000], Loss: 46.572906494140625, Learning Rate: 0.01\n",
      "Epoch [2064/20000], Loss: 46.557586669921875, Learning Rate: 0.01\n",
      "Epoch [2065/20000], Loss: 46.54240417480469, Learning Rate: 0.01\n",
      "Epoch [2066/20000], Loss: 46.52714538574219, Learning Rate: 0.01\n",
      "Epoch [2067/20000], Loss: 46.5120849609375, Learning Rate: 0.01\n",
      "Epoch [2068/20000], Loss: 46.49699401855469, Learning Rate: 0.01\n",
      "Epoch [2069/20000], Loss: 46.48170471191406, Learning Rate: 0.01\n",
      "Epoch [2070/20000], Loss: 46.46649169921875, Learning Rate: 0.01\n",
      "Epoch [2071/20000], Loss: 46.451324462890625, Learning Rate: 0.01\n",
      "Epoch [2072/20000], Loss: 46.436004638671875, Learning Rate: 0.01\n",
      "Epoch [2073/20000], Loss: 46.42088317871094, Learning Rate: 0.01\n",
      "Epoch [2074/20000], Loss: 46.40574645996094, Learning Rate: 0.01\n",
      "Epoch [2075/20000], Loss: 46.390533447265625, Learning Rate: 0.01\n",
      "Epoch [2076/20000], Loss: 46.375335693359375, Learning Rate: 0.01\n",
      "Epoch [2077/20000], Loss: 46.3602294921875, Learning Rate: 0.01\n",
      "Epoch [2078/20000], Loss: 46.344970703125, Learning Rate: 0.01\n",
      "Epoch [2079/20000], Loss: 46.329803466796875, Learning Rate: 0.01\n",
      "Epoch [2080/20000], Loss: 46.31452941894531, Learning Rate: 0.01\n",
      "Epoch [2081/20000], Loss: 46.299560546875, Learning Rate: 0.01\n",
      "Epoch [2082/20000], Loss: 46.28437805175781, Learning Rate: 0.01\n",
      "Epoch [2083/20000], Loss: 46.26921081542969, Learning Rate: 0.01\n",
      "Epoch [2084/20000], Loss: 46.254180908203125, Learning Rate: 0.01\n",
      "Epoch [2085/20000], Loss: 46.238922119140625, Learning Rate: 0.01\n",
      "Epoch [2086/20000], Loss: 46.2237548828125, Learning Rate: 0.01\n",
      "Epoch [2087/20000], Loss: 46.20857238769531, Learning Rate: 0.01\n",
      "Epoch [2088/20000], Loss: 46.193450927734375, Learning Rate: 0.01\n",
      "Epoch [2089/20000], Loss: 46.17835998535156, Learning Rate: 0.01\n",
      "Epoch [2090/20000], Loss: 46.163177490234375, Learning Rate: 0.01\n",
      "Epoch [2091/20000], Loss: 46.148162841796875, Learning Rate: 0.01\n",
      "Epoch [2092/20000], Loss: 46.13288879394531, Learning Rate: 0.01\n",
      "Epoch [2093/20000], Loss: 46.11781311035156, Learning Rate: 0.01\n",
      "Epoch [2094/20000], Loss: 46.10276794433594, Learning Rate: 0.01\n",
      "Epoch [2095/20000], Loss: 46.08753967285156, Learning Rate: 0.01\n",
      "Epoch [2096/20000], Loss: 46.07258605957031, Learning Rate: 0.01\n",
      "Epoch [2097/20000], Loss: 46.05726623535156, Learning Rate: 0.01\n",
      "Epoch [2098/20000], Loss: 46.04229736328125, Learning Rate: 0.01\n",
      "Epoch [2099/20000], Loss: 46.02720642089844, Learning Rate: 0.01\n",
      "Epoch [2100/20000], Loss: 46.01191711425781, Learning Rate: 0.01\n",
      "Epoch [2101/20000], Loss: 45.996917724609375, Learning Rate: 0.01\n",
      "Epoch [2102/20000], Loss: 45.98187255859375, Learning Rate: 0.01\n",
      "Epoch [2103/20000], Loss: 45.966583251953125, Learning Rate: 0.01\n",
      "Epoch [2104/20000], Loss: 45.95164489746094, Learning Rate: 0.01\n",
      "Epoch [2105/20000], Loss: 45.9365234375, Learning Rate: 0.01\n",
      "Epoch [2106/20000], Loss: 45.92146301269531, Learning Rate: 0.01\n",
      "Epoch [2107/20000], Loss: 45.90641784667969, Learning Rate: 0.01\n",
      "Epoch [2108/20000], Loss: 45.8912353515625, Learning Rate: 0.01\n",
      "Epoch [2109/20000], Loss: 45.87626647949219, Learning Rate: 0.01\n",
      "Epoch [2110/20000], Loss: 45.86103820800781, Learning Rate: 0.01\n",
      "Epoch [2111/20000], Loss: 45.84600830078125, Learning Rate: 0.01\n",
      "Epoch [2112/20000], Loss: 45.83099365234375, Learning Rate: 0.01\n",
      "Epoch [2113/20000], Loss: 45.81584167480469, Learning Rate: 0.01\n",
      "Epoch [2114/20000], Loss: 45.80082702636719, Learning Rate: 0.01\n",
      "Epoch [2115/20000], Loss: 45.785797119140625, Learning Rate: 0.01\n",
      "Epoch [2116/20000], Loss: 45.77056884765625, Learning Rate: 0.01\n",
      "Epoch [2117/20000], Loss: 45.75553894042969, Learning Rate: 0.01\n",
      "Epoch [2118/20000], Loss: 45.7406005859375, Learning Rate: 0.01\n",
      "Epoch [2119/20000], Loss: 45.72540283203125, Learning Rate: 0.01\n",
      "Epoch [2120/20000], Loss: 45.71040344238281, Learning Rate: 0.01\n",
      "Epoch [2121/20000], Loss: 45.69537353515625, Learning Rate: 0.01\n",
      "Epoch [2122/20000], Loss: 45.680389404296875, Learning Rate: 0.01\n",
      "Epoch [2123/20000], Loss: 45.66526794433594, Learning Rate: 0.01\n",
      "Epoch [2124/20000], Loss: 45.65025329589844, Learning Rate: 0.01\n",
      "Epoch [2125/20000], Loss: 45.635223388671875, Learning Rate: 0.01\n",
      "Epoch [2126/20000], Loss: 45.62019348144531, Learning Rate: 0.01\n",
      "Epoch [2127/20000], Loss: 45.605133056640625, Learning Rate: 0.01\n",
      "Epoch [2128/20000], Loss: 45.59004211425781, Learning Rate: 0.01\n",
      "Epoch [2129/20000], Loss: 45.5750732421875, Learning Rate: 0.01\n",
      "Epoch [2130/20000], Loss: 45.56016540527344, Learning Rate: 0.01\n",
      "Epoch [2131/20000], Loss: 45.545013427734375, Learning Rate: 0.01\n",
      "Epoch [2132/20000], Loss: 45.53001403808594, Learning Rate: 0.01\n",
      "Epoch [2133/20000], Loss: 45.5150146484375, Learning Rate: 0.01\n",
      "Epoch [2134/20000], Loss: 45.5, Learning Rate: 0.01\n",
      "Epoch [2135/20000], Loss: 45.48503112792969, Learning Rate: 0.01\n",
      "Epoch [2136/20000], Loss: 45.470123291015625, Learning Rate: 0.01\n",
      "Epoch [2137/20000], Loss: 45.4549560546875, Learning Rate: 0.01\n",
      "Epoch [2138/20000], Loss: 45.44000244140625, Learning Rate: 0.01\n",
      "Epoch [2139/20000], Loss: 45.424957275390625, Learning Rate: 0.01\n",
      "Epoch [2140/20000], Loss: 45.40992736816406, Learning Rate: 0.01\n",
      "Epoch [2141/20000], Loss: 45.394989013671875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2142/20000], Loss: 45.380035400390625, Learning Rate: 0.01\n",
      "Epoch [2143/20000], Loss: 45.36509704589844, Learning Rate: 0.01\n",
      "Epoch [2144/20000], Loss: 45.35014343261719, Learning Rate: 0.01\n",
      "Epoch [2145/20000], Loss: 45.33506774902344, Learning Rate: 0.01\n",
      "Epoch [2146/20000], Loss: 45.32012939453125, Learning Rate: 0.01\n",
      "Epoch [2147/20000], Loss: 45.3050537109375, Learning Rate: 0.01\n",
      "Epoch [2148/20000], Loss: 45.29020690917969, Learning Rate: 0.01\n",
      "Epoch [2149/20000], Loss: 45.275054931640625, Learning Rate: 0.01\n",
      "Epoch [2150/20000], Loss: 45.260223388671875, Learning Rate: 0.01\n",
      "Epoch [2151/20000], Loss: 45.2452392578125, Learning Rate: 0.01\n",
      "Epoch [2152/20000], Loss: 45.230316162109375, Learning Rate: 0.01\n",
      "Epoch [2153/20000], Loss: 45.215423583984375, Learning Rate: 0.01\n",
      "Epoch [2154/20000], Loss: 45.20030212402344, Learning Rate: 0.01\n",
      "Epoch [2155/20000], Loss: 45.18544006347656, Learning Rate: 0.01\n",
      "Epoch [2156/20000], Loss: 45.17039489746094, Learning Rate: 0.01\n",
      "Epoch [2157/20000], Loss: 45.15538024902344, Learning Rate: 0.01\n",
      "Epoch [2158/20000], Loss: 45.14064025878906, Learning Rate: 0.01\n",
      "Epoch [2159/20000], Loss: 45.1256103515625, Learning Rate: 0.01\n",
      "Epoch [2160/20000], Loss: 45.110595703125, Learning Rate: 0.01\n",
      "Epoch [2161/20000], Loss: 45.095733642578125, Learning Rate: 0.01\n",
      "Epoch [2162/20000], Loss: 45.080657958984375, Learning Rate: 0.01\n",
      "Epoch [2163/20000], Loss: 45.065704345703125, Learning Rate: 0.01\n",
      "Epoch [2164/20000], Loss: 45.050811767578125, Learning Rate: 0.01\n",
      "Epoch [2165/20000], Loss: 45.03593444824219, Learning Rate: 0.01\n",
      "Epoch [2166/20000], Loss: 45.02093505859375, Learning Rate: 0.01\n",
      "Epoch [2167/20000], Loss: 45.00616455078125, Learning Rate: 0.01\n",
      "Epoch [2168/20000], Loss: 44.99113464355469, Learning Rate: 0.01\n",
      "Epoch [2169/20000], Loss: 44.976104736328125, Learning Rate: 0.01\n",
      "Epoch [2170/20000], Loss: 44.961395263671875, Learning Rate: 0.01\n",
      "Epoch [2171/20000], Loss: 44.94639587402344, Learning Rate: 0.01\n",
      "Epoch [2172/20000], Loss: 44.93157958984375, Learning Rate: 0.01\n",
      "Epoch [2173/20000], Loss: 44.91644287109375, Learning Rate: 0.01\n",
      "Epoch [2174/20000], Loss: 44.90159606933594, Learning Rate: 0.01\n",
      "Epoch [2175/20000], Loss: 44.88670349121094, Learning Rate: 0.01\n",
      "Epoch [2176/20000], Loss: 44.87187194824219, Learning Rate: 0.01\n",
      "Epoch [2177/20000], Loss: 44.85694885253906, Learning Rate: 0.01\n",
      "Epoch [2178/20000], Loss: 44.84196472167969, Learning Rate: 0.01\n",
      "Epoch [2179/20000], Loss: 44.8271484375, Learning Rate: 0.01\n",
      "Epoch [2180/20000], Loss: 44.81217956542969, Learning Rate: 0.01\n",
      "Epoch [2181/20000], Loss: 44.79731750488281, Learning Rate: 0.01\n",
      "Epoch [2182/20000], Loss: 44.782501220703125, Learning Rate: 0.01\n",
      "Epoch [2183/20000], Loss: 44.76756286621094, Learning Rate: 0.01\n",
      "Epoch [2184/20000], Loss: 44.7525634765625, Learning Rate: 0.01\n",
      "Epoch [2185/20000], Loss: 44.73773193359375, Learning Rate: 0.01\n",
      "Epoch [2186/20000], Loss: 44.7227783203125, Learning Rate: 0.01\n",
      "Epoch [2187/20000], Loss: 44.7080078125, Learning Rate: 0.01\n",
      "Epoch [2188/20000], Loss: 44.69313049316406, Learning Rate: 0.01\n",
      "Epoch [2189/20000], Loss: 44.67803955078125, Learning Rate: 0.01\n",
      "Epoch [2190/20000], Loss: 44.663330078125, Learning Rate: 0.01\n",
      "Epoch [2191/20000], Loss: 44.64851379394531, Learning Rate: 0.01\n",
      "Epoch [2192/20000], Loss: 44.63365173339844, Learning Rate: 0.01\n",
      "Epoch [2193/20000], Loss: 44.61885070800781, Learning Rate: 0.01\n",
      "Epoch [2194/20000], Loss: 44.6038818359375, Learning Rate: 0.01\n",
      "Epoch [2195/20000], Loss: 44.588958740234375, Learning Rate: 0.01\n",
      "Epoch [2196/20000], Loss: 44.574005126953125, Learning Rate: 0.01\n",
      "Epoch [2197/20000], Loss: 44.559356689453125, Learning Rate: 0.01\n",
      "Epoch [2198/20000], Loss: 44.544525146484375, Learning Rate: 0.01\n",
      "Epoch [2199/20000], Loss: 44.52961730957031, Learning Rate: 0.01\n",
      "Epoch [2200/20000], Loss: 44.51484680175781, Learning Rate: 0.01\n",
      "Epoch [2201/20000], Loss: 44.50006103515625, Learning Rate: 0.01\n",
      "Epoch [2202/20000], Loss: 44.48516845703125, Learning Rate: 0.01\n",
      "Epoch [2203/20000], Loss: 44.47021484375, Learning Rate: 0.01\n",
      "Epoch [2204/20000], Loss: 44.45530700683594, Learning Rate: 0.01\n",
      "Epoch [2205/20000], Loss: 44.440521240234375, Learning Rate: 0.01\n",
      "Epoch [2206/20000], Loss: 44.42573547363281, Learning Rate: 0.01\n",
      "Epoch [2207/20000], Loss: 44.41096496582031, Learning Rate: 0.01\n",
      "Epoch [2208/20000], Loss: 44.39599609375, Learning Rate: 0.01\n",
      "Epoch [2209/20000], Loss: 44.38124084472656, Learning Rate: 0.01\n",
      "Epoch [2210/20000], Loss: 44.366424560546875, Learning Rate: 0.01\n",
      "Epoch [2211/20000], Loss: 44.35162353515625, Learning Rate: 0.01\n",
      "Epoch [2212/20000], Loss: 44.33662414550781, Learning Rate: 0.01\n",
      "Epoch [2213/20000], Loss: 44.321868896484375, Learning Rate: 0.01\n",
      "Epoch [2214/20000], Loss: 44.30711364746094, Learning Rate: 0.01\n",
      "Epoch [2215/20000], Loss: 44.29246520996094, Learning Rate: 0.01\n",
      "Epoch [2216/20000], Loss: 44.27754211425781, Learning Rate: 0.01\n",
      "Epoch [2217/20000], Loss: 44.26274108886719, Learning Rate: 0.01\n",
      "Epoch [2218/20000], Loss: 44.24787902832031, Learning Rate: 0.01\n",
      "Epoch [2219/20000], Loss: 44.23301696777344, Learning Rate: 0.01\n",
      "Epoch [2220/20000], Loss: 44.21827697753906, Learning Rate: 0.01\n",
      "Epoch [2221/20000], Loss: 44.203460693359375, Learning Rate: 0.01\n",
      "Epoch [2222/20000], Loss: 44.18864440917969, Learning Rate: 0.01\n",
      "Epoch [2223/20000], Loss: 44.17387390136719, Learning Rate: 0.01\n",
      "Epoch [2224/20000], Loss: 44.158905029296875, Learning Rate: 0.01\n",
      "Epoch [2225/20000], Loss: 44.144256591796875, Learning Rate: 0.01\n",
      "Epoch [2226/20000], Loss: 44.1295166015625, Learning Rate: 0.01\n",
      "Epoch [2227/20000], Loss: 44.11485290527344, Learning Rate: 0.01\n",
      "Epoch [2228/20000], Loss: 44.099822998046875, Learning Rate: 0.01\n",
      "Epoch [2229/20000], Loss: 44.08506774902344, Learning Rate: 0.01\n",
      "Epoch [2230/20000], Loss: 44.07025146484375, Learning Rate: 0.01\n",
      "Epoch [2231/20000], Loss: 44.05548095703125, Learning Rate: 0.01\n",
      "Epoch [2232/20000], Loss: 44.04078674316406, Learning Rate: 0.01\n",
      "Epoch [2233/20000], Loss: 44.02593994140625, Learning Rate: 0.01\n",
      "Epoch [2234/20000], Loss: 44.0111083984375, Learning Rate: 0.01\n",
      "Epoch [2235/20000], Loss: 43.99639892578125, Learning Rate: 0.01\n",
      "Epoch [2236/20000], Loss: 43.981658935546875, Learning Rate: 0.01\n",
      "Epoch [2237/20000], Loss: 43.96687316894531, Learning Rate: 0.01\n",
      "Epoch [2238/20000], Loss: 43.95213317871094, Learning Rate: 0.01\n",
      "Epoch [2239/20000], Loss: 43.93739318847656, Learning Rate: 0.01\n",
      "Epoch [2240/20000], Loss: 43.922576904296875, Learning Rate: 0.01\n",
      "Epoch [2241/20000], Loss: 43.90789794921875, Learning Rate: 0.01\n",
      "Epoch [2242/20000], Loss: 43.89314270019531, Learning Rate: 0.01\n",
      "Epoch [2243/20000], Loss: 43.878387451171875, Learning Rate: 0.01\n",
      "Epoch [2244/20000], Loss: 43.863555908203125, Learning Rate: 0.01\n",
      "Epoch [2245/20000], Loss: 43.84886169433594, Learning Rate: 0.01\n",
      "Epoch [2246/20000], Loss: 43.83404541015625, Learning Rate: 0.01\n",
      "Epoch [2247/20000], Loss: 43.81932067871094, Learning Rate: 0.01\n",
      "Epoch [2248/20000], Loss: 43.804656982421875, Learning Rate: 0.01\n",
      "Epoch [2249/20000], Loss: 43.789825439453125, Learning Rate: 0.01\n",
      "Epoch [2250/20000], Loss: 43.775054931640625, Learning Rate: 0.01\n",
      "Epoch [2251/20000], Loss: 43.76036071777344, Learning Rate: 0.01\n",
      "Epoch [2252/20000], Loss: 43.74566650390625, Learning Rate: 0.01\n",
      "Epoch [2253/20000], Loss: 43.73089599609375, Learning Rate: 0.01\n",
      "Epoch [2254/20000], Loss: 43.71607971191406, Learning Rate: 0.01\n",
      "Epoch [2255/20000], Loss: 43.7015380859375, Learning Rate: 0.01\n",
      "Epoch [2256/20000], Loss: 43.686676025390625, Learning Rate: 0.01\n",
      "Epoch [2257/20000], Loss: 43.67193603515625, Learning Rate: 0.01\n",
      "Epoch [2258/20000], Loss: 43.65728759765625, Learning Rate: 0.01\n",
      "Epoch [2259/20000], Loss: 43.64251708984375, Learning Rate: 0.01\n",
      "Epoch [2260/20000], Loss: 43.6278076171875, Learning Rate: 0.01\n",
      "Epoch [2261/20000], Loss: 43.61309814453125, Learning Rate: 0.01\n",
      "Epoch [2262/20000], Loss: 43.598388671875, Learning Rate: 0.01\n",
      "Epoch [2263/20000], Loss: 43.58369445800781, Learning Rate: 0.01\n",
      "Epoch [2264/20000], Loss: 43.568939208984375, Learning Rate: 0.01\n",
      "Epoch [2265/20000], Loss: 43.55426025390625, Learning Rate: 0.01\n",
      "Epoch [2266/20000], Loss: 43.539520263671875, Learning Rate: 0.01\n",
      "Epoch [2267/20000], Loss: 43.52490234375, Learning Rate: 0.01\n",
      "Epoch [2268/20000], Loss: 43.51014709472656, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2269/20000], Loss: 43.4954833984375, Learning Rate: 0.01\n",
      "Epoch [2270/20000], Loss: 43.48069763183594, Learning Rate: 0.01\n",
      "Epoch [2271/20000], Loss: 43.46595764160156, Learning Rate: 0.01\n",
      "Epoch [2272/20000], Loss: 43.451385498046875, Learning Rate: 0.01\n",
      "Epoch [2273/20000], Loss: 43.436676025390625, Learning Rate: 0.01\n",
      "Epoch [2274/20000], Loss: 43.42204284667969, Learning Rate: 0.01\n",
      "Epoch [2275/20000], Loss: 43.40718078613281, Learning Rate: 0.01\n",
      "Epoch [2276/20000], Loss: 43.39259338378906, Learning Rate: 0.01\n",
      "Epoch [2277/20000], Loss: 43.37786865234375, Learning Rate: 0.01\n",
      "Epoch [2278/20000], Loss: 43.363311767578125, Learning Rate: 0.01\n",
      "Epoch [2279/20000], Loss: 43.3485107421875, Learning Rate: 0.01\n",
      "Epoch [2280/20000], Loss: 43.33390808105469, Learning Rate: 0.01\n",
      "Epoch [2281/20000], Loss: 43.31922912597656, Learning Rate: 0.01\n",
      "Epoch [2282/20000], Loss: 43.30448913574219, Learning Rate: 0.01\n",
      "Epoch [2283/20000], Loss: 43.28985595703125, Learning Rate: 0.01\n",
      "Epoch [2284/20000], Loss: 43.27513122558594, Learning Rate: 0.01\n",
      "Epoch [2285/20000], Loss: 43.26048278808594, Learning Rate: 0.01\n",
      "Epoch [2286/20000], Loss: 43.2457275390625, Learning Rate: 0.01\n",
      "Epoch [2287/20000], Loss: 43.2310791015625, Learning Rate: 0.01\n",
      "Epoch [2288/20000], Loss: 43.21649169921875, Learning Rate: 0.01\n",
      "Epoch [2289/20000], Loss: 43.20184326171875, Learning Rate: 0.01\n",
      "Epoch [2290/20000], Loss: 43.18717956542969, Learning Rate: 0.01\n",
      "Epoch [2291/20000], Loss: 43.17250061035156, Learning Rate: 0.01\n",
      "Epoch [2292/20000], Loss: 43.157806396484375, Learning Rate: 0.01\n",
      "Epoch [2293/20000], Loss: 43.14323425292969, Learning Rate: 0.01\n",
      "Epoch [2294/20000], Loss: 43.12861633300781, Learning Rate: 0.01\n",
      "Epoch [2295/20000], Loss: 43.11383056640625, Learning Rate: 0.01\n",
      "Epoch [2296/20000], Loss: 43.09925842285156, Learning Rate: 0.01\n",
      "Epoch [2297/20000], Loss: 43.08445739746094, Learning Rate: 0.01\n",
      "Epoch [2298/20000], Loss: 43.06983947753906, Learning Rate: 0.01\n",
      "Epoch [2299/20000], Loss: 43.0552978515625, Learning Rate: 0.01\n",
      "Epoch [2300/20000], Loss: 43.040679931640625, Learning Rate: 0.01\n",
      "Epoch [2301/20000], Loss: 43.0260009765625, Learning Rate: 0.01\n",
      "Epoch [2302/20000], Loss: 43.011505126953125, Learning Rate: 0.01\n",
      "Epoch [2303/20000], Loss: 42.996795654296875, Learning Rate: 0.01\n",
      "Epoch [2304/20000], Loss: 42.982086181640625, Learning Rate: 0.01\n",
      "Epoch [2305/20000], Loss: 42.96746826171875, Learning Rate: 0.01\n",
      "Epoch [2306/20000], Loss: 42.95292663574219, Learning Rate: 0.01\n",
      "Epoch [2307/20000], Loss: 42.938262939453125, Learning Rate: 0.01\n",
      "Epoch [2308/20000], Loss: 42.92366027832031, Learning Rate: 0.01\n",
      "Epoch [2309/20000], Loss: 42.90899658203125, Learning Rate: 0.01\n",
      "Epoch [2310/20000], Loss: 42.89436340332031, Learning Rate: 0.01\n",
      "Epoch [2311/20000], Loss: 42.87962341308594, Learning Rate: 0.01\n",
      "Epoch [2312/20000], Loss: 42.86517333984375, Learning Rate: 0.01\n",
      "Epoch [2313/20000], Loss: 42.85044860839844, Learning Rate: 0.01\n",
      "Epoch [2314/20000], Loss: 42.835845947265625, Learning Rate: 0.01\n",
      "Epoch [2315/20000], Loss: 42.82127380371094, Learning Rate: 0.01\n",
      "Epoch [2316/20000], Loss: 42.80665588378906, Learning Rate: 0.01\n",
      "Epoch [2317/20000], Loss: 42.79206848144531, Learning Rate: 0.01\n",
      "Epoch [2318/20000], Loss: 42.77754211425781, Learning Rate: 0.01\n",
      "Epoch [2319/20000], Loss: 42.762939453125, Learning Rate: 0.01\n",
      "Epoch [2320/20000], Loss: 42.74822998046875, Learning Rate: 0.01\n",
      "Epoch [2321/20000], Loss: 42.73365783691406, Learning Rate: 0.01\n",
      "Epoch [2322/20000], Loss: 42.71907043457031, Learning Rate: 0.01\n",
      "Epoch [2323/20000], Loss: 42.70442199707031, Learning Rate: 0.01\n",
      "Epoch [2324/20000], Loss: 42.689788818359375, Learning Rate: 0.01\n",
      "Epoch [2325/20000], Loss: 42.67521667480469, Learning Rate: 0.01\n",
      "Epoch [2326/20000], Loss: 42.660552978515625, Learning Rate: 0.01\n",
      "Epoch [2327/20000], Loss: 42.64598083496094, Learning Rate: 0.01\n",
      "Epoch [2328/20000], Loss: 42.6314697265625, Learning Rate: 0.01\n",
      "Epoch [2329/20000], Loss: 42.61683654785156, Learning Rate: 0.01\n",
      "Epoch [2330/20000], Loss: 42.602386474609375, Learning Rate: 0.01\n",
      "Epoch [2331/20000], Loss: 42.58772277832031, Learning Rate: 0.01\n",
      "Epoch [2332/20000], Loss: 42.573089599609375, Learning Rate: 0.01\n",
      "Epoch [2333/20000], Loss: 42.55857849121094, Learning Rate: 0.01\n",
      "Epoch [2334/20000], Loss: 42.54405212402344, Learning Rate: 0.01\n",
      "Epoch [2335/20000], Loss: 42.52937316894531, Learning Rate: 0.01\n",
      "Epoch [2336/20000], Loss: 42.51487731933594, Learning Rate: 0.01\n",
      "Epoch [2337/20000], Loss: 42.500213623046875, Learning Rate: 0.01\n",
      "Epoch [2338/20000], Loss: 42.485595703125, Learning Rate: 0.01\n",
      "Epoch [2339/20000], Loss: 42.47111511230469, Learning Rate: 0.01\n",
      "Epoch [2340/20000], Loss: 42.45660400390625, Learning Rate: 0.01\n",
      "Epoch [2341/20000], Loss: 42.441986083984375, Learning Rate: 0.01\n",
      "Epoch [2342/20000], Loss: 42.42741394042969, Learning Rate: 0.01\n",
      "Epoch [2343/20000], Loss: 42.412872314453125, Learning Rate: 0.01\n",
      "Epoch [2344/20000], Loss: 42.39836120605469, Learning Rate: 0.01\n",
      "Epoch [2345/20000], Loss: 42.38374328613281, Learning Rate: 0.01\n",
      "Epoch [2346/20000], Loss: 42.3690185546875, Learning Rate: 0.01\n",
      "Epoch [2347/20000], Loss: 42.3546142578125, Learning Rate: 0.01\n",
      "Epoch [2348/20000], Loss: 42.34007263183594, Learning Rate: 0.01\n",
      "Epoch [2349/20000], Loss: 42.32548522949219, Learning Rate: 0.01\n",
      "Epoch [2350/20000], Loss: 42.31097412109375, Learning Rate: 0.01\n",
      "Epoch [2351/20000], Loss: 42.29644775390625, Learning Rate: 0.01\n",
      "Epoch [2352/20000], Loss: 42.28175354003906, Learning Rate: 0.01\n",
      "Epoch [2353/20000], Loss: 42.26731872558594, Learning Rate: 0.01\n",
      "Epoch [2354/20000], Loss: 42.25274658203125, Learning Rate: 0.01\n",
      "Epoch [2355/20000], Loss: 42.238128662109375, Learning Rate: 0.01\n",
      "Epoch [2356/20000], Loss: 42.22373962402344, Learning Rate: 0.01\n",
      "Epoch [2357/20000], Loss: 42.20921325683594, Learning Rate: 0.01\n",
      "Epoch [2358/20000], Loss: 42.19459533691406, Learning Rate: 0.01\n",
      "Epoch [2359/20000], Loss: 42.18006896972656, Learning Rate: 0.01\n",
      "Epoch [2360/20000], Loss: 42.16554260253906, Learning Rate: 0.01\n",
      "Epoch [2361/20000], Loss: 42.150970458984375, Learning Rate: 0.01\n",
      "Epoch [2362/20000], Loss: 42.13652038574219, Learning Rate: 0.01\n",
      "Epoch [2363/20000], Loss: 42.122039794921875, Learning Rate: 0.01\n",
      "Epoch [2364/20000], Loss: 42.10746765136719, Learning Rate: 0.01\n",
      "Epoch [2365/20000], Loss: 42.09288024902344, Learning Rate: 0.01\n",
      "Epoch [2366/20000], Loss: 42.07841491699219, Learning Rate: 0.01\n",
      "Epoch [2367/20000], Loss: 42.06382751464844, Learning Rate: 0.01\n",
      "Epoch [2368/20000], Loss: 42.04925537109375, Learning Rate: 0.01\n",
      "Epoch [2369/20000], Loss: 42.034759521484375, Learning Rate: 0.01\n",
      "Epoch [2370/20000], Loss: 42.02027893066406, Learning Rate: 0.01\n",
      "Epoch [2371/20000], Loss: 42.005889892578125, Learning Rate: 0.01\n",
      "Epoch [2372/20000], Loss: 41.99128723144531, Learning Rate: 0.01\n",
      "Epoch [2373/20000], Loss: 41.97682189941406, Learning Rate: 0.01\n",
      "Epoch [2374/20000], Loss: 41.9622802734375, Learning Rate: 0.01\n",
      "Epoch [2375/20000], Loss: 41.94770812988281, Learning Rate: 0.01\n",
      "Epoch [2376/20000], Loss: 41.9332275390625, Learning Rate: 0.01\n",
      "Epoch [2377/20000], Loss: 41.91876220703125, Learning Rate: 0.01\n",
      "Epoch [2378/20000], Loss: 41.904266357421875, Learning Rate: 0.01\n",
      "Epoch [2379/20000], Loss: 41.88966369628906, Learning Rate: 0.01\n",
      "Epoch [2380/20000], Loss: 41.875213623046875, Learning Rate: 0.01\n",
      "Epoch [2381/20000], Loss: 41.86064147949219, Learning Rate: 0.01\n",
      "Epoch [2382/20000], Loss: 41.846282958984375, Learning Rate: 0.01\n",
      "Epoch [2383/20000], Loss: 41.831817626953125, Learning Rate: 0.01\n",
      "Epoch [2384/20000], Loss: 41.81718444824219, Learning Rate: 0.01\n",
      "Epoch [2385/20000], Loss: 41.802734375, Learning Rate: 0.01\n",
      "Epoch [2386/20000], Loss: 41.78825378417969, Learning Rate: 0.01\n",
      "Epoch [2387/20000], Loss: 41.773651123046875, Learning Rate: 0.01\n",
      "Epoch [2388/20000], Loss: 41.759124755859375, Learning Rate: 0.01\n",
      "Epoch [2389/20000], Loss: 41.744842529296875, Learning Rate: 0.01\n",
      "Epoch [2390/20000], Loss: 41.7303466796875, Learning Rate: 0.01\n",
      "Epoch [2391/20000], Loss: 41.71577453613281, Learning Rate: 0.01\n",
      "Epoch [2392/20000], Loss: 41.70127868652344, Learning Rate: 0.01\n",
      "Epoch [2393/20000], Loss: 41.68690490722656, Learning Rate: 0.01\n",
      "Epoch [2394/20000], Loss: 41.67236328125, Learning Rate: 0.01\n",
      "Epoch [2395/20000], Loss: 41.65789794921875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2396/20000], Loss: 41.6434326171875, Learning Rate: 0.01\n",
      "Epoch [2397/20000], Loss: 41.62892150878906, Learning Rate: 0.01\n",
      "Epoch [2398/20000], Loss: 41.61451721191406, Learning Rate: 0.01\n",
      "Epoch [2399/20000], Loss: 41.60008239746094, Learning Rate: 0.01\n",
      "Epoch [2400/20000], Loss: 41.585540771484375, Learning Rate: 0.01\n",
      "Epoch [2401/20000], Loss: 41.57106018066406, Learning Rate: 0.01\n",
      "Epoch [2402/20000], Loss: 41.55653381347656, Learning Rate: 0.01\n",
      "Epoch [2403/20000], Loss: 41.54212951660156, Learning Rate: 0.01\n",
      "Epoch [2404/20000], Loss: 41.527740478515625, Learning Rate: 0.01\n",
      "Epoch [2405/20000], Loss: 41.51318359375, Learning Rate: 0.01\n",
      "Epoch [2406/20000], Loss: 41.49873352050781, Learning Rate: 0.01\n",
      "Epoch [2407/20000], Loss: 41.484283447265625, Learning Rate: 0.01\n",
      "Epoch [2408/20000], Loss: 41.469879150390625, Learning Rate: 0.01\n",
      "Epoch [2409/20000], Loss: 41.455291748046875, Learning Rate: 0.01\n",
      "Epoch [2410/20000], Loss: 41.44099426269531, Learning Rate: 0.01\n",
      "Epoch [2411/20000], Loss: 41.426361083984375, Learning Rate: 0.01\n",
      "Epoch [2412/20000], Loss: 41.41200256347656, Learning Rate: 0.01\n",
      "Epoch [2413/20000], Loss: 41.39756774902344, Learning Rate: 0.01\n",
      "Epoch [2414/20000], Loss: 41.383148193359375, Learning Rate: 0.01\n",
      "Epoch [2415/20000], Loss: 41.36854553222656, Learning Rate: 0.01\n",
      "Epoch [2416/20000], Loss: 41.354248046875, Learning Rate: 0.01\n",
      "Epoch [2417/20000], Loss: 41.33982849121094, Learning Rate: 0.01\n",
      "Epoch [2418/20000], Loss: 41.32539367675781, Learning Rate: 0.01\n",
      "Epoch [2419/20000], Loss: 41.31092834472656, Learning Rate: 0.01\n",
      "Epoch [2420/20000], Loss: 41.29652404785156, Learning Rate: 0.01\n",
      "Epoch [2421/20000], Loss: 41.28208923339844, Learning Rate: 0.01\n",
      "Epoch [2422/20000], Loss: 41.26768493652344, Learning Rate: 0.01\n",
      "Epoch [2423/20000], Loss: 41.25315856933594, Learning Rate: 0.01\n",
      "Epoch [2424/20000], Loss: 41.238739013671875, Learning Rate: 0.01\n",
      "Epoch [2425/20000], Loss: 41.224334716796875, Learning Rate: 0.01\n",
      "Epoch [2426/20000], Loss: 41.21000671386719, Learning Rate: 0.01\n",
      "Epoch [2427/20000], Loss: 41.195587158203125, Learning Rate: 0.01\n",
      "Epoch [2428/20000], Loss: 41.18110656738281, Learning Rate: 0.01\n",
      "Epoch [2429/20000], Loss: 41.16668701171875, Learning Rate: 0.01\n",
      "Epoch [2430/20000], Loss: 41.1522216796875, Learning Rate: 0.01\n",
      "Epoch [2431/20000], Loss: 41.13771057128906, Learning Rate: 0.01\n",
      "Epoch [2432/20000], Loss: 41.12336730957031, Learning Rate: 0.01\n",
      "Epoch [2433/20000], Loss: 41.10896301269531, Learning Rate: 0.01\n",
      "Epoch [2434/20000], Loss: 41.094482421875, Learning Rate: 0.01\n",
      "Epoch [2435/20000], Loss: 41.080047607421875, Learning Rate: 0.01\n",
      "Epoch [2436/20000], Loss: 41.06578063964844, Learning Rate: 0.01\n",
      "Epoch [2437/20000], Loss: 41.05126953125, Learning Rate: 0.01\n",
      "Epoch [2438/20000], Loss: 41.03688049316406, Learning Rate: 0.01\n",
      "Epoch [2439/20000], Loss: 41.02247619628906, Learning Rate: 0.01\n",
      "Epoch [2440/20000], Loss: 41.00811767578125, Learning Rate: 0.01\n",
      "Epoch [2441/20000], Loss: 40.99369812011719, Learning Rate: 0.01\n",
      "Epoch [2442/20000], Loss: 40.97920227050781, Learning Rate: 0.01\n",
      "Epoch [2443/20000], Loss: 40.96492004394531, Learning Rate: 0.01\n",
      "Epoch [2444/20000], Loss: 40.95045471191406, Learning Rate: 0.01\n",
      "Epoch [2445/20000], Loss: 40.93597412109375, Learning Rate: 0.01\n",
      "Epoch [2446/20000], Loss: 40.92152404785156, Learning Rate: 0.01\n",
      "Epoch [2447/20000], Loss: 40.90733337402344, Learning Rate: 0.01\n",
      "Epoch [2448/20000], Loss: 40.89292907714844, Learning Rate: 0.01\n",
      "Epoch [2449/20000], Loss: 40.87858581542969, Learning Rate: 0.01\n",
      "Epoch [2450/20000], Loss: 40.86412048339844, Learning Rate: 0.01\n",
      "Epoch [2451/20000], Loss: 40.849700927734375, Learning Rate: 0.01\n",
      "Epoch [2452/20000], Loss: 40.83546447753906, Learning Rate: 0.01\n",
      "Epoch [2453/20000], Loss: 40.82098388671875, Learning Rate: 0.01\n",
      "Epoch [2454/20000], Loss: 40.80656433105469, Learning Rate: 0.01\n",
      "Epoch [2455/20000], Loss: 40.792236328125, Learning Rate: 0.01\n",
      "Epoch [2456/20000], Loss: 40.77784729003906, Learning Rate: 0.01\n",
      "Epoch [2457/20000], Loss: 40.763397216796875, Learning Rate: 0.01\n",
      "Epoch [2458/20000], Loss: 40.74900817871094, Learning Rate: 0.01\n",
      "Epoch [2459/20000], Loss: 40.734619140625, Learning Rate: 0.01\n",
      "Epoch [2460/20000], Loss: 40.72027587890625, Learning Rate: 0.01\n",
      "Epoch [2461/20000], Loss: 40.7059326171875, Learning Rate: 0.01\n",
      "Epoch [2462/20000], Loss: 40.69151306152344, Learning Rate: 0.01\n",
      "Epoch [2463/20000], Loss: 40.67713928222656, Learning Rate: 0.01\n",
      "Epoch [2464/20000], Loss: 40.662841796875, Learning Rate: 0.01\n",
      "Epoch [2465/20000], Loss: 40.64839172363281, Learning Rate: 0.01\n",
      "Epoch [2466/20000], Loss: 40.634063720703125, Learning Rate: 0.01\n",
      "Epoch [2467/20000], Loss: 40.61964416503906, Learning Rate: 0.01\n",
      "Epoch [2468/20000], Loss: 40.6053466796875, Learning Rate: 0.01\n",
      "Epoch [2469/20000], Loss: 40.590972900390625, Learning Rate: 0.01\n",
      "Epoch [2470/20000], Loss: 40.576629638671875, Learning Rate: 0.01\n",
      "Epoch [2471/20000], Loss: 40.56221008300781, Learning Rate: 0.01\n",
      "Epoch [2472/20000], Loss: 40.54783630371094, Learning Rate: 0.01\n",
      "Epoch [2473/20000], Loss: 40.53355407714844, Learning Rate: 0.01\n",
      "Epoch [2474/20000], Loss: 40.51921081542969, Learning Rate: 0.01\n",
      "Epoch [2475/20000], Loss: 40.5048828125, Learning Rate: 0.01\n",
      "Epoch [2476/20000], Loss: 40.49043273925781, Learning Rate: 0.01\n",
      "Epoch [2477/20000], Loss: 40.47611999511719, Learning Rate: 0.01\n",
      "Epoch [2478/20000], Loss: 40.46177673339844, Learning Rate: 0.01\n",
      "Epoch [2479/20000], Loss: 40.4473876953125, Learning Rate: 0.01\n",
      "Epoch [2480/20000], Loss: 40.432952880859375, Learning Rate: 0.01\n",
      "Epoch [2481/20000], Loss: 40.41853332519531, Learning Rate: 0.01\n",
      "Epoch [2482/20000], Loss: 40.40437316894531, Learning Rate: 0.01\n",
      "Epoch [2483/20000], Loss: 40.38999938964844, Learning Rate: 0.01\n",
      "Epoch [2484/20000], Loss: 40.37559509277344, Learning Rate: 0.01\n",
      "Epoch [2485/20000], Loss: 40.36134338378906, Learning Rate: 0.01\n",
      "Epoch [2486/20000], Loss: 40.34698486328125, Learning Rate: 0.01\n",
      "Epoch [2487/20000], Loss: 40.33262634277344, Learning Rate: 0.01\n",
      "Epoch [2488/20000], Loss: 40.318328857421875, Learning Rate: 0.01\n",
      "Epoch [2489/20000], Loss: 40.30401611328125, Learning Rate: 0.01\n",
      "Epoch [2490/20000], Loss: 40.289642333984375, Learning Rate: 0.01\n",
      "Epoch [2491/20000], Loss: 40.275299072265625, Learning Rate: 0.01\n",
      "Epoch [2492/20000], Loss: 40.26092529296875, Learning Rate: 0.01\n",
      "Epoch [2493/20000], Loss: 40.24668884277344, Learning Rate: 0.01\n",
      "Epoch [2494/20000], Loss: 40.2322998046875, Learning Rate: 0.01\n",
      "Epoch [2495/20000], Loss: 40.21791076660156, Learning Rate: 0.01\n",
      "Epoch [2496/20000], Loss: 40.20375061035156, Learning Rate: 0.01\n",
      "Epoch [2497/20000], Loss: 40.18934631347656, Learning Rate: 0.01\n",
      "Epoch [2498/20000], Loss: 40.175018310546875, Learning Rate: 0.01\n",
      "Epoch [2499/20000], Loss: 40.16065979003906, Learning Rate: 0.01\n",
      "Epoch [2500/20000], Loss: 40.14642333984375, Learning Rate: 0.01\n",
      "Epoch [2501/20000], Loss: 40.13209533691406, Learning Rate: 0.01\n",
      "Epoch [2502/20000], Loss: 40.1177978515625, Learning Rate: 0.01\n",
      "Epoch [2503/20000], Loss: 40.103485107421875, Learning Rate: 0.01\n",
      "Epoch [2504/20000], Loss: 40.08905029296875, Learning Rate: 0.01\n",
      "Epoch [2505/20000], Loss: 40.074859619140625, Learning Rate: 0.01\n",
      "Epoch [2506/20000], Loss: 40.060455322265625, Learning Rate: 0.01\n",
      "Epoch [2507/20000], Loss: 40.04618835449219, Learning Rate: 0.01\n",
      "Epoch [2508/20000], Loss: 40.03190612792969, Learning Rate: 0.01\n",
      "Epoch [2509/20000], Loss: 40.01750183105469, Learning Rate: 0.01\n",
      "Epoch [2510/20000], Loss: 40.003204345703125, Learning Rate: 0.01\n",
      "Epoch [2511/20000], Loss: 39.988861083984375, Learning Rate: 0.01\n",
      "Epoch [2512/20000], Loss: 39.974761962890625, Learning Rate: 0.01\n",
      "Epoch [2513/20000], Loss: 39.96038818359375, Learning Rate: 0.01\n",
      "Epoch [2514/20000], Loss: 39.94599914550781, Learning Rate: 0.01\n",
      "Epoch [2515/20000], Loss: 39.931854248046875, Learning Rate: 0.01\n",
      "Epoch [2516/20000], Loss: 39.917510986328125, Learning Rate: 0.01\n",
      "Epoch [2517/20000], Loss: 39.9031982421875, Learning Rate: 0.01\n",
      "Epoch [2518/20000], Loss: 39.888916015625, Learning Rate: 0.01\n",
      "Epoch [2519/20000], Loss: 39.87452697753906, Learning Rate: 0.01\n",
      "Epoch [2520/20000], Loss: 39.860321044921875, Learning Rate: 0.01\n",
      "Epoch [2521/20000], Loss: 39.84596252441406, Learning Rate: 0.01\n",
      "Epoch [2522/20000], Loss: 39.83161926269531, Learning Rate: 0.01\n",
      "Epoch [2523/20000], Loss: 39.817413330078125, Learning Rate: 0.01\n",
      "Epoch [2524/20000], Loss: 39.80311584472656, Learning Rate: 0.01\n",
      "Epoch [2525/20000], Loss: 39.78886413574219, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2526/20000], Loss: 39.77452087402344, Learning Rate: 0.01\n",
      "Epoch [2527/20000], Loss: 39.76029968261719, Learning Rate: 0.01\n",
      "Epoch [2528/20000], Loss: 39.74603271484375, Learning Rate: 0.01\n",
      "Epoch [2529/20000], Loss: 39.731719970703125, Learning Rate: 0.01\n",
      "Epoch [2530/20000], Loss: 39.71742248535156, Learning Rate: 0.01\n",
      "Epoch [2531/20000], Loss: 39.70307922363281, Learning Rate: 0.01\n",
      "Epoch [2532/20000], Loss: 39.68890380859375, Learning Rate: 0.01\n",
      "Epoch [2533/20000], Loss: 39.674560546875, Learning Rate: 0.01\n",
      "Epoch [2534/20000], Loss: 39.66047668457031, Learning Rate: 0.01\n",
      "Epoch [2535/20000], Loss: 39.64595031738281, Learning Rate: 0.01\n",
      "Epoch [2536/20000], Loss: 39.63177490234375, Learning Rate: 0.01\n",
      "Epoch [2537/20000], Loss: 39.61753845214844, Learning Rate: 0.01\n",
      "Epoch [2538/20000], Loss: 39.60333251953125, Learning Rate: 0.01\n",
      "Epoch [2539/20000], Loss: 39.58905029296875, Learning Rate: 0.01\n",
      "Epoch [2540/20000], Loss: 39.574676513671875, Learning Rate: 0.01\n",
      "Epoch [2541/20000], Loss: 39.56036376953125, Learning Rate: 0.01\n",
      "Epoch [2542/20000], Loss: 39.54621887207031, Learning Rate: 0.01\n",
      "Epoch [2543/20000], Loss: 39.53193664550781, Learning Rate: 0.01\n",
      "Epoch [2544/20000], Loss: 39.51763916015625, Learning Rate: 0.01\n",
      "Epoch [2545/20000], Loss: 39.50341796875, Learning Rate: 0.01\n",
      "Epoch [2546/20000], Loss: 39.48915100097656, Learning Rate: 0.01\n",
      "Epoch [2547/20000], Loss: 39.474884033203125, Learning Rate: 0.01\n",
      "Epoch [2548/20000], Loss: 39.46063232421875, Learning Rate: 0.01\n",
      "Epoch [2549/20000], Loss: 39.44633483886719, Learning Rate: 0.01\n",
      "Epoch [2550/20000], Loss: 39.43211364746094, Learning Rate: 0.01\n",
      "Epoch [2551/20000], Loss: 39.417999267578125, Learning Rate: 0.01\n",
      "Epoch [2552/20000], Loss: 39.40361022949219, Learning Rate: 0.01\n",
      "Epoch [2553/20000], Loss: 39.38935852050781, Learning Rate: 0.01\n",
      "Epoch [2554/20000], Loss: 39.37518310546875, Learning Rate: 0.01\n",
      "Epoch [2555/20000], Loss: 39.36094665527344, Learning Rate: 0.01\n",
      "Epoch [2556/20000], Loss: 39.3466796875, Learning Rate: 0.01\n",
      "Epoch [2557/20000], Loss: 39.33235168457031, Learning Rate: 0.01\n",
      "Epoch [2558/20000], Loss: 39.3182373046875, Learning Rate: 0.01\n",
      "Epoch [2559/20000], Loss: 39.303985595703125, Learning Rate: 0.01\n",
      "Epoch [2560/20000], Loss: 39.289703369140625, Learning Rate: 0.01\n",
      "Epoch [2561/20000], Loss: 39.275482177734375, Learning Rate: 0.01\n",
      "Epoch [2562/20000], Loss: 39.261260986328125, Learning Rate: 0.01\n",
      "Epoch [2563/20000], Loss: 39.24696350097656, Learning Rate: 0.01\n",
      "Epoch [2564/20000], Loss: 39.23272705078125, Learning Rate: 0.01\n",
      "Epoch [2565/20000], Loss: 39.218505859375, Learning Rate: 0.01\n",
      "Epoch [2566/20000], Loss: 39.20440673828125, Learning Rate: 0.01\n",
      "Epoch [2567/20000], Loss: 39.190093994140625, Learning Rate: 0.01\n",
      "Epoch [2568/20000], Loss: 39.175872802734375, Learning Rate: 0.01\n",
      "Epoch [2569/20000], Loss: 39.161590576171875, Learning Rate: 0.01\n",
      "Epoch [2570/20000], Loss: 39.147369384765625, Learning Rate: 0.01\n",
      "Epoch [2571/20000], Loss: 39.13325500488281, Learning Rate: 0.01\n",
      "Epoch [2572/20000], Loss: 39.118896484375, Learning Rate: 0.01\n",
      "Epoch [2573/20000], Loss: 39.10478210449219, Learning Rate: 0.01\n",
      "Epoch [2574/20000], Loss: 39.090576171875, Learning Rate: 0.01\n",
      "Epoch [2575/20000], Loss: 39.07635498046875, Learning Rate: 0.01\n",
      "Epoch [2576/20000], Loss: 39.06199645996094, Learning Rate: 0.01\n",
      "Epoch [2577/20000], Loss: 39.04774475097656, Learning Rate: 0.01\n",
      "Epoch [2578/20000], Loss: 39.033660888671875, Learning Rate: 0.01\n",
      "Epoch [2579/20000], Loss: 39.01947021484375, Learning Rate: 0.01\n",
      "Epoch [2580/20000], Loss: 39.005218505859375, Learning Rate: 0.01\n",
      "Epoch [2581/20000], Loss: 38.991119384765625, Learning Rate: 0.01\n",
      "Epoch [2582/20000], Loss: 38.976806640625, Learning Rate: 0.01\n",
      "Epoch [2583/20000], Loss: 38.96260070800781, Learning Rate: 0.01\n",
      "Epoch [2584/20000], Loss: 38.94828796386719, Learning Rate: 0.01\n",
      "Epoch [2585/20000], Loss: 38.93408203125, Learning Rate: 0.01\n",
      "Epoch [2586/20000], Loss: 38.920013427734375, Learning Rate: 0.01\n",
      "Epoch [2587/20000], Loss: 38.90582275390625, Learning Rate: 0.01\n",
      "Epoch [2588/20000], Loss: 38.89158630371094, Learning Rate: 0.01\n",
      "Epoch [2589/20000], Loss: 38.87736511230469, Learning Rate: 0.01\n",
      "Epoch [2590/20000], Loss: 38.86314392089844, Learning Rate: 0.01\n",
      "Epoch [2591/20000], Loss: 38.848968505859375, Learning Rate: 0.01\n",
      "Epoch [2592/20000], Loss: 38.834716796875, Learning Rate: 0.01\n",
      "Epoch [2593/20000], Loss: 38.820526123046875, Learning Rate: 0.01\n",
      "Epoch [2594/20000], Loss: 38.80632019042969, Learning Rate: 0.01\n",
      "Epoch [2595/20000], Loss: 38.79222106933594, Learning Rate: 0.01\n",
      "Epoch [2596/20000], Loss: 38.777984619140625, Learning Rate: 0.01\n",
      "Epoch [2597/20000], Loss: 38.76387023925781, Learning Rate: 0.01\n",
      "Epoch [2598/20000], Loss: 38.74958801269531, Learning Rate: 0.01\n",
      "Epoch [2599/20000], Loss: 38.73554992675781, Learning Rate: 0.01\n",
      "Epoch [2600/20000], Loss: 38.721282958984375, Learning Rate: 0.01\n",
      "Epoch [2601/20000], Loss: 38.70710754394531, Learning Rate: 0.01\n",
      "Epoch [2602/20000], Loss: 38.69287109375, Learning Rate: 0.01\n",
      "Epoch [2603/20000], Loss: 38.67869567871094, Learning Rate: 0.01\n",
      "Epoch [2604/20000], Loss: 38.664459228515625, Learning Rate: 0.01\n",
      "Epoch [2605/20000], Loss: 38.65034484863281, Learning Rate: 0.01\n",
      "Epoch [2606/20000], Loss: 38.63609313964844, Learning Rate: 0.01\n",
      "Epoch [2607/20000], Loss: 38.622039794921875, Learning Rate: 0.01\n",
      "Epoch [2608/20000], Loss: 38.607696533203125, Learning Rate: 0.01\n",
      "Epoch [2609/20000], Loss: 38.59356689453125, Learning Rate: 0.01\n",
      "Epoch [2610/20000], Loss: 38.57945251464844, Learning Rate: 0.01\n",
      "Epoch [2611/20000], Loss: 38.56524658203125, Learning Rate: 0.01\n",
      "Epoch [2612/20000], Loss: 38.55108642578125, Learning Rate: 0.01\n",
      "Epoch [2613/20000], Loss: 38.536956787109375, Learning Rate: 0.01\n",
      "Epoch [2614/20000], Loss: 38.522735595703125, Learning Rate: 0.01\n",
      "Epoch [2615/20000], Loss: 38.508514404296875, Learning Rate: 0.01\n",
      "Epoch [2616/20000], Loss: 38.4945068359375, Learning Rate: 0.01\n",
      "Epoch [2617/20000], Loss: 38.48017883300781, Learning Rate: 0.01\n",
      "Epoch [2618/20000], Loss: 38.46600341796875, Learning Rate: 0.01\n",
      "Epoch [2619/20000], Loss: 38.451934814453125, Learning Rate: 0.01\n",
      "Epoch [2620/20000], Loss: 38.43772888183594, Learning Rate: 0.01\n",
      "Epoch [2621/20000], Loss: 38.42376708984375, Learning Rate: 0.01\n",
      "Epoch [2622/20000], Loss: 38.409454345703125, Learning Rate: 0.01\n",
      "Epoch [2623/20000], Loss: 38.395294189453125, Learning Rate: 0.01\n",
      "Epoch [2624/20000], Loss: 38.38128662109375, Learning Rate: 0.01\n",
      "Epoch [2625/20000], Loss: 38.36695861816406, Learning Rate: 0.01\n",
      "Epoch [2626/20000], Loss: 38.35295104980469, Learning Rate: 0.01\n",
      "Epoch [2627/20000], Loss: 38.33866882324219, Learning Rate: 0.01\n",
      "Epoch [2628/20000], Loss: 38.32452392578125, Learning Rate: 0.01\n",
      "Epoch [2629/20000], Loss: 38.31048583984375, Learning Rate: 0.01\n",
      "Epoch [2630/20000], Loss: 38.29631042480469, Learning Rate: 0.01\n",
      "Epoch [2631/20000], Loss: 38.282135009765625, Learning Rate: 0.01\n",
      "Epoch [2632/20000], Loss: 38.26789855957031, Learning Rate: 0.01\n",
      "Epoch [2633/20000], Loss: 38.25373840332031, Learning Rate: 0.01\n",
      "Epoch [2634/20000], Loss: 38.23973083496094, Learning Rate: 0.01\n",
      "Epoch [2635/20000], Loss: 38.2255859375, Learning Rate: 0.01\n",
      "Epoch [2636/20000], Loss: 38.21134948730469, Learning Rate: 0.01\n",
      "Epoch [2637/20000], Loss: 38.197265625, Learning Rate: 0.01\n",
      "Epoch [2638/20000], Loss: 38.183074951171875, Learning Rate: 0.01\n",
      "Epoch [2639/20000], Loss: 38.16899108886719, Learning Rate: 0.01\n",
      "Epoch [2640/20000], Loss: 38.15492248535156, Learning Rate: 0.01\n",
      "Epoch [2641/20000], Loss: 38.140655517578125, Learning Rate: 0.01\n",
      "Epoch [2642/20000], Loss: 38.126617431640625, Learning Rate: 0.01\n",
      "Epoch [2643/20000], Loss: 38.11236572265625, Learning Rate: 0.01\n",
      "Epoch [2644/20000], Loss: 38.09814453125, Learning Rate: 0.01\n",
      "Epoch [2645/20000], Loss: 38.084228515625, Learning Rate: 0.01\n",
      "Epoch [2646/20000], Loss: 38.07000732421875, Learning Rate: 0.01\n",
      "Epoch [2647/20000], Loss: 38.05589294433594, Learning Rate: 0.01\n",
      "Epoch [2648/20000], Loss: 38.041717529296875, Learning Rate: 0.01\n",
      "Epoch [2649/20000], Loss: 38.02769470214844, Learning Rate: 0.01\n",
      "Epoch [2650/20000], Loss: 38.013519287109375, Learning Rate: 0.01\n",
      "Epoch [2651/20000], Loss: 37.99943542480469, Learning Rate: 0.01\n",
      "Epoch [2652/20000], Loss: 37.98536682128906, Learning Rate: 0.01\n",
      "Epoch [2653/20000], Loss: 37.971160888671875, Learning Rate: 0.01\n",
      "Epoch [2654/20000], Loss: 37.95701599121094, Learning Rate: 0.01\n",
      "Epoch [2655/20000], Loss: 37.94288635253906, Learning Rate: 0.01\n",
      "Epoch [2656/20000], Loss: 37.92890930175781, Learning Rate: 0.01\n",
      "Epoch [2657/20000], Loss: 37.914703369140625, Learning Rate: 0.01\n",
      "Epoch [2658/20000], Loss: 37.90068054199219, Learning Rate: 0.01\n",
      "Epoch [2659/20000], Loss: 37.886566162109375, Learning Rate: 0.01\n",
      "Epoch [2660/20000], Loss: 37.872344970703125, Learning Rate: 0.01\n",
      "Epoch [2661/20000], Loss: 37.8583984375, Learning Rate: 0.01\n",
      "Epoch [2662/20000], Loss: 37.84425354003906, Learning Rate: 0.01\n",
      "Epoch [2663/20000], Loss: 37.83012390136719, Learning Rate: 0.01\n",
      "Epoch [2664/20000], Loss: 37.81599426269531, Learning Rate: 0.01\n",
      "Epoch [2665/20000], Loss: 37.80186462402344, Learning Rate: 0.01\n",
      "Epoch [2666/20000], Loss: 37.78776550292969, Learning Rate: 0.01\n",
      "Epoch [2667/20000], Loss: 37.77360534667969, Learning Rate: 0.01\n",
      "Epoch [2668/20000], Loss: 37.759613037109375, Learning Rate: 0.01\n",
      "Epoch [2669/20000], Loss: 37.74543762207031, Learning Rate: 0.01\n",
      "Epoch [2670/20000], Loss: 37.73130798339844, Learning Rate: 0.01\n",
      "Epoch [2671/20000], Loss: 37.71728515625, Learning Rate: 0.01\n",
      "Epoch [2672/20000], Loss: 37.70323181152344, Learning Rate: 0.01\n",
      "Epoch [2673/20000], Loss: 37.68907165527344, Learning Rate: 0.01\n",
      "Epoch [2674/20000], Loss: 37.67498779296875, Learning Rate: 0.01\n",
      "Epoch [2675/20000], Loss: 37.66093444824219, Learning Rate: 0.01\n",
      "Epoch [2676/20000], Loss: 37.646820068359375, Learning Rate: 0.01\n",
      "Epoch [2677/20000], Loss: 37.632720947265625, Learning Rate: 0.01\n",
      "Epoch [2678/20000], Loss: 37.618621826171875, Learning Rate: 0.01\n",
      "Epoch [2679/20000], Loss: 37.60447692871094, Learning Rate: 0.01\n",
      "Epoch [2680/20000], Loss: 37.590484619140625, Learning Rate: 0.01\n",
      "Epoch [2681/20000], Loss: 37.576263427734375, Learning Rate: 0.01\n",
      "Epoch [2682/20000], Loss: 37.562255859375, Learning Rate: 0.01\n",
      "Epoch [2683/20000], Loss: 37.548126220703125, Learning Rate: 0.01\n",
      "Epoch [2684/20000], Loss: 37.534149169921875, Learning Rate: 0.01\n",
      "Epoch [2685/20000], Loss: 37.51995849609375, Learning Rate: 0.01\n",
      "Epoch [2686/20000], Loss: 37.506011962890625, Learning Rate: 0.01\n",
      "Epoch [2687/20000], Loss: 37.49189758300781, Learning Rate: 0.01\n",
      "Epoch [2688/20000], Loss: 37.477752685546875, Learning Rate: 0.01\n",
      "Epoch [2689/20000], Loss: 37.46376037597656, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2690/20000], Loss: 37.44972229003906, Learning Rate: 0.01\n",
      "Epoch [2691/20000], Loss: 37.43565368652344, Learning Rate: 0.01\n",
      "Epoch [2692/20000], Loss: 37.42146301269531, Learning Rate: 0.01\n",
      "Epoch [2693/20000], Loss: 37.407501220703125, Learning Rate: 0.01\n",
      "Epoch [2694/20000], Loss: 37.39335632324219, Learning Rate: 0.01\n",
      "Epoch [2695/20000], Loss: 37.379364013671875, Learning Rate: 0.01\n",
      "Epoch [2696/20000], Loss: 37.36518859863281, Learning Rate: 0.01\n",
      "Epoch [2697/20000], Loss: 37.35125732421875, Learning Rate: 0.01\n",
      "Epoch [2698/20000], Loss: 37.337188720703125, Learning Rate: 0.01\n",
      "Epoch [2699/20000], Loss: 37.32310485839844, Learning Rate: 0.01\n",
      "Epoch [2700/20000], Loss: 37.308929443359375, Learning Rate: 0.01\n",
      "Epoch [2701/20000], Loss: 37.29498291015625, Learning Rate: 0.01\n",
      "Epoch [2702/20000], Loss: 37.28086853027344, Learning Rate: 0.01\n",
      "Epoch [2703/20000], Loss: 37.266815185546875, Learning Rate: 0.01\n",
      "Epoch [2704/20000], Loss: 37.25288391113281, Learning Rate: 0.01\n",
      "Epoch [2705/20000], Loss: 37.23875427246094, Learning Rate: 0.01\n",
      "Epoch [2706/20000], Loss: 37.224761962890625, Learning Rate: 0.01\n",
      "Epoch [2707/20000], Loss: 37.21070861816406, Learning Rate: 0.01\n",
      "Epoch [2708/20000], Loss: 37.19651794433594, Learning Rate: 0.01\n",
      "Epoch [2709/20000], Loss: 37.18254089355469, Learning Rate: 0.01\n",
      "Epoch [2710/20000], Loss: 37.168426513671875, Learning Rate: 0.01\n",
      "Epoch [2711/20000], Loss: 37.15446472167969, Learning Rate: 0.01\n",
      "Epoch [2712/20000], Loss: 37.14039611816406, Learning Rate: 0.01\n",
      "Epoch [2713/20000], Loss: 37.1263427734375, Learning Rate: 0.01\n",
      "Epoch [2714/20000], Loss: 37.112396240234375, Learning Rate: 0.01\n",
      "Epoch [2715/20000], Loss: 37.098297119140625, Learning Rate: 0.01\n",
      "Epoch [2716/20000], Loss: 37.08424377441406, Learning Rate: 0.01\n",
      "Epoch [2717/20000], Loss: 37.070159912109375, Learning Rate: 0.01\n",
      "Epoch [2718/20000], Loss: 37.056182861328125, Learning Rate: 0.01\n",
      "Epoch [2719/20000], Loss: 37.04219055175781, Learning Rate: 0.01\n",
      "Epoch [2720/20000], Loss: 37.0281982421875, Learning Rate: 0.01\n",
      "Epoch [2721/20000], Loss: 37.01405334472656, Learning Rate: 0.01\n",
      "Epoch [2722/20000], Loss: 37.00004577636719, Learning Rate: 0.01\n",
      "Epoch [2723/20000], Loss: 36.98602294921875, Learning Rate: 0.01\n",
      "Epoch [2724/20000], Loss: 36.97198486328125, Learning Rate: 0.01\n",
      "Epoch [2725/20000], Loss: 36.957977294921875, Learning Rate: 0.01\n",
      "Epoch [2726/20000], Loss: 36.94401550292969, Learning Rate: 0.01\n",
      "Epoch [2727/20000], Loss: 36.929931640625, Learning Rate: 0.01\n",
      "Epoch [2728/20000], Loss: 36.9158935546875, Learning Rate: 0.01\n",
      "Epoch [2729/20000], Loss: 36.90180969238281, Learning Rate: 0.01\n",
      "Epoch [2730/20000], Loss: 36.887939453125, Learning Rate: 0.01\n",
      "Epoch [2731/20000], Loss: 36.87382507324219, Learning Rate: 0.01\n",
      "Epoch [2732/20000], Loss: 36.85972595214844, Learning Rate: 0.01\n",
      "Epoch [2733/20000], Loss: 36.845703125, Learning Rate: 0.01\n",
      "Epoch [2734/20000], Loss: 36.831787109375, Learning Rate: 0.01\n",
      "Epoch [2735/20000], Loss: 36.81779479980469, Learning Rate: 0.01\n",
      "Epoch [2736/20000], Loss: 36.803802490234375, Learning Rate: 0.01\n",
      "Epoch [2737/20000], Loss: 36.78968811035156, Learning Rate: 0.01\n",
      "Epoch [2738/20000], Loss: 36.775634765625, Learning Rate: 0.01\n",
      "Epoch [2739/20000], Loss: 36.76171875, Learning Rate: 0.01\n",
      "Epoch [2740/20000], Loss: 36.747711181640625, Learning Rate: 0.01\n",
      "Epoch [2741/20000], Loss: 36.73381042480469, Learning Rate: 0.01\n",
      "Epoch [2742/20000], Loss: 36.71968078613281, Learning Rate: 0.01\n",
      "Epoch [2743/20000], Loss: 36.70562744140625, Learning Rate: 0.01\n",
      "Epoch [2744/20000], Loss: 36.69172668457031, Learning Rate: 0.01\n",
      "Epoch [2745/20000], Loss: 36.67755126953125, Learning Rate: 0.01\n",
      "Epoch [2746/20000], Loss: 36.663665771484375, Learning Rate: 0.01\n",
      "Epoch [2747/20000], Loss: 36.649566650390625, Learning Rate: 0.01\n",
      "Epoch [2748/20000], Loss: 36.635650634765625, Learning Rate: 0.01\n",
      "Epoch [2749/20000], Loss: 36.62190246582031, Learning Rate: 0.01\n",
      "Epoch [2750/20000], Loss: 36.60765075683594, Learning Rate: 0.01\n",
      "Epoch [2751/20000], Loss: 36.59373474121094, Learning Rate: 0.01\n",
      "Epoch [2752/20000], Loss: 36.57965087890625, Learning Rate: 0.01\n",
      "Epoch [2753/20000], Loss: 36.56568908691406, Learning Rate: 0.01\n",
      "Epoch [2754/20000], Loss: 36.55181884765625, Learning Rate: 0.01\n",
      "Epoch [2755/20000], Loss: 36.53773498535156, Learning Rate: 0.01\n",
      "Epoch [2756/20000], Loss: 36.52375793457031, Learning Rate: 0.01\n",
      "Epoch [2757/20000], Loss: 36.50982666015625, Learning Rate: 0.01\n",
      "Epoch [2758/20000], Loss: 36.49580383300781, Learning Rate: 0.01\n",
      "Epoch [2759/20000], Loss: 36.48176574707031, Learning Rate: 0.01\n",
      "Epoch [2760/20000], Loss: 36.46781921386719, Learning Rate: 0.01\n",
      "Epoch [2761/20000], Loss: 36.453826904296875, Learning Rate: 0.01\n",
      "Epoch [2762/20000], Loss: 36.439849853515625, Learning Rate: 0.01\n",
      "Epoch [2763/20000], Loss: 36.42585754394531, Learning Rate: 0.01\n",
      "Epoch [2764/20000], Loss: 36.41200256347656, Learning Rate: 0.01\n",
      "Epoch [2765/20000], Loss: 36.397918701171875, Learning Rate: 0.01\n",
      "Epoch [2766/20000], Loss: 36.38398742675781, Learning Rate: 0.01\n",
      "Epoch [2767/20000], Loss: 36.370086669921875, Learning Rate: 0.01\n",
      "Epoch [2768/20000], Loss: 36.35595703125, Learning Rate: 0.01\n",
      "Epoch [2769/20000], Loss: 36.342010498046875, Learning Rate: 0.01\n",
      "Epoch [2770/20000], Loss: 36.32814025878906, Learning Rate: 0.01\n",
      "Epoch [2771/20000], Loss: 36.31404113769531, Learning Rate: 0.01\n",
      "Epoch [2772/20000], Loss: 36.30010986328125, Learning Rate: 0.01\n",
      "Epoch [2773/20000], Loss: 36.2861328125, Learning Rate: 0.01\n",
      "Epoch [2774/20000], Loss: 36.27217102050781, Learning Rate: 0.01\n",
      "Epoch [2775/20000], Loss: 36.25819396972656, Learning Rate: 0.01\n",
      "Epoch [2776/20000], Loss: 36.24424743652344, Learning Rate: 0.01\n",
      "Epoch [2777/20000], Loss: 36.23028564453125, Learning Rate: 0.01\n",
      "Epoch [2778/20000], Loss: 36.21632385253906, Learning Rate: 0.01\n",
      "Epoch [2779/20000], Loss: 36.202423095703125, Learning Rate: 0.01\n",
      "Epoch [2780/20000], Loss: 36.18849182128906, Learning Rate: 0.01\n",
      "Epoch [2781/20000], Loss: 36.17448425292969, Learning Rate: 0.01\n",
      "Epoch [2782/20000], Loss: 36.16053771972656, Learning Rate: 0.01\n",
      "Epoch [2783/20000], Loss: 36.14654541015625, Learning Rate: 0.01\n",
      "Epoch [2784/20000], Loss: 36.132598876953125, Learning Rate: 0.01\n",
      "Epoch [2785/20000], Loss: 36.11863708496094, Learning Rate: 0.01\n",
      "Epoch [2786/20000], Loss: 36.104705810546875, Learning Rate: 0.01\n",
      "Epoch [2787/20000], Loss: 36.09074401855469, Learning Rate: 0.01\n",
      "Epoch [2788/20000], Loss: 36.07679748535156, Learning Rate: 0.01\n",
      "Epoch [2789/20000], Loss: 36.06292724609375, Learning Rate: 0.01\n",
      "Epoch [2790/20000], Loss: 36.04887390136719, Learning Rate: 0.01\n",
      "Epoch [2791/20000], Loss: 36.0350341796875, Learning Rate: 0.01\n",
      "Epoch [2792/20000], Loss: 36.021087646484375, Learning Rate: 0.01\n",
      "Epoch [2793/20000], Loss: 36.00706481933594, Learning Rate: 0.01\n",
      "Epoch [2794/20000], Loss: 35.99311828613281, Learning Rate: 0.01\n",
      "Epoch [2795/20000], Loss: 35.979248046875, Learning Rate: 0.01\n",
      "Epoch [2796/20000], Loss: 35.96531677246094, Learning Rate: 0.01\n",
      "Epoch [2797/20000], Loss: 35.95130920410156, Learning Rate: 0.01\n",
      "Epoch [2798/20000], Loss: 35.9375, Learning Rate: 0.01\n",
      "Epoch [2799/20000], Loss: 35.92350769042969, Learning Rate: 0.01\n",
      "Epoch [2800/20000], Loss: 35.90953063964844, Learning Rate: 0.01\n",
      "Epoch [2801/20000], Loss: 35.89556884765625, Learning Rate: 0.01\n",
      "Epoch [2802/20000], Loss: 35.881683349609375, Learning Rate: 0.01\n",
      "Epoch [2803/20000], Loss: 35.86778259277344, Learning Rate: 0.01\n",
      "Epoch [2804/20000], Loss: 35.85377502441406, Learning Rate: 0.01\n",
      "Epoch [2805/20000], Loss: 35.83995056152344, Learning Rate: 0.01\n",
      "Epoch [2806/20000], Loss: 35.82594299316406, Learning Rate: 0.01\n",
      "Epoch [2807/20000], Loss: 35.8121337890625, Learning Rate: 0.01\n",
      "Epoch [2808/20000], Loss: 35.79811096191406, Learning Rate: 0.01\n",
      "Epoch [2809/20000], Loss: 35.784149169921875, Learning Rate: 0.01\n",
      "Epoch [2810/20000], Loss: 35.770355224609375, Learning Rate: 0.01\n",
      "Epoch [2811/20000], Loss: 35.75628662109375, Learning Rate: 0.01\n",
      "Epoch [2812/20000], Loss: 35.74238586425781, Learning Rate: 0.01\n",
      "Epoch [2813/20000], Loss: 35.728607177734375, Learning Rate: 0.01\n",
      "Epoch [2814/20000], Loss: 35.714569091796875, Learning Rate: 0.01\n",
      "Epoch [2815/20000], Loss: 35.70069885253906, Learning Rate: 0.01\n",
      "Epoch [2816/20000], Loss: 35.68682861328125, Learning Rate: 0.01\n",
      "Epoch [2817/20000], Loss: 35.672882080078125, Learning Rate: 0.01\n",
      "Epoch [2818/20000], Loss: 35.65892028808594, Learning Rate: 0.01\n",
      "Epoch [2819/20000], Loss: 35.6451416015625, Learning Rate: 0.01\n",
      "Epoch [2820/20000], Loss: 35.631134033203125, Learning Rate: 0.01\n",
      "Epoch [2821/20000], Loss: 35.61732482910156, Learning Rate: 0.01\n",
      "Epoch [2822/20000], Loss: 35.60334777832031, Learning Rate: 0.01\n",
      "Epoch [2823/20000], Loss: 35.58943176269531, Learning Rate: 0.01\n",
      "Epoch [2824/20000], Loss: 35.57575988769531, Learning Rate: 0.01\n",
      "Epoch [2825/20000], Loss: 35.56170654296875, Learning Rate: 0.01\n",
      "Epoch [2826/20000], Loss: 35.54783630371094, Learning Rate: 0.01\n",
      "Epoch [2827/20000], Loss: 35.53376770019531, Learning Rate: 0.01\n",
      "Epoch [2828/20000], Loss: 35.52001953125, Learning Rate: 0.01\n",
      "Epoch [2829/20000], Loss: 35.50611877441406, Learning Rate: 0.01\n",
      "Epoch [2830/20000], Loss: 35.492095947265625, Learning Rate: 0.01\n",
      "Epoch [2831/20000], Loss: 35.47825622558594, Learning Rate: 0.01\n",
      "Epoch [2832/20000], Loss: 35.464385986328125, Learning Rate: 0.01\n",
      "Epoch [2833/20000], Loss: 35.45062255859375, Learning Rate: 0.01\n",
      "Epoch [2834/20000], Loss: 35.436676025390625, Learning Rate: 0.01\n",
      "Epoch [2835/20000], Loss: 35.422760009765625, Learning Rate: 0.01\n",
      "Epoch [2836/20000], Loss: 35.40892028808594, Learning Rate: 0.01\n",
      "Epoch [2837/20000], Loss: 35.39512634277344, Learning Rate: 0.01\n",
      "Epoch [2838/20000], Loss: 35.38105773925781, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2839/20000], Loss: 35.367340087890625, Learning Rate: 0.01\n",
      "Epoch [2840/20000], Loss: 35.35334777832031, Learning Rate: 0.01\n",
      "Epoch [2841/20000], Loss: 35.339447021484375, Learning Rate: 0.01\n",
      "Epoch [2842/20000], Loss: 35.32566833496094, Learning Rate: 0.01\n",
      "Epoch [2843/20000], Loss: 35.31166076660156, Learning Rate: 0.01\n",
      "Epoch [2844/20000], Loss: 35.2978515625, Learning Rate: 0.01\n",
      "Epoch [2845/20000], Loss: 35.28401184082031, Learning Rate: 0.01\n",
      "Epoch [2846/20000], Loss: 35.27008056640625, Learning Rate: 0.01\n",
      "Epoch [2847/20000], Loss: 35.2562255859375, Learning Rate: 0.01\n",
      "Epoch [2848/20000], Loss: 35.242462158203125, Learning Rate: 0.01\n",
      "Epoch [2849/20000], Loss: 35.22850036621094, Learning Rate: 0.01\n",
      "Epoch [2850/20000], Loss: 35.21467590332031, Learning Rate: 0.01\n",
      "Epoch [2851/20000], Loss: 35.200775146484375, Learning Rate: 0.01\n",
      "Epoch [2852/20000], Loss: 35.18690490722656, Learning Rate: 0.01\n",
      "Epoch [2853/20000], Loss: 35.173095703125, Learning Rate: 0.01\n",
      "Epoch [2854/20000], Loss: 35.15922546386719, Learning Rate: 0.01\n",
      "Epoch [2855/20000], Loss: 35.145416259765625, Learning Rate: 0.01\n",
      "Epoch [2856/20000], Loss: 35.13153076171875, Learning Rate: 0.01\n",
      "Epoch [2857/20000], Loss: 35.11767578125, Learning Rate: 0.01\n",
      "Epoch [2858/20000], Loss: 35.10382080078125, Learning Rate: 0.01\n",
      "Epoch [2859/20000], Loss: 35.0899658203125, Learning Rate: 0.01\n",
      "Epoch [2860/20000], Loss: 35.07615661621094, Learning Rate: 0.01\n",
      "Epoch [2861/20000], Loss: 35.06227111816406, Learning Rate: 0.01\n",
      "Epoch [2862/20000], Loss: 35.04844665527344, Learning Rate: 0.01\n",
      "Epoch [2863/20000], Loss: 35.03437805175781, Learning Rate: 0.01\n",
      "Epoch [2864/20000], Loss: 35.02073669433594, Learning Rate: 0.01\n",
      "Epoch [2865/20000], Loss: 35.006927490234375, Learning Rate: 0.01\n",
      "Epoch [2866/20000], Loss: 34.99305725097656, Learning Rate: 0.01\n",
      "Epoch [2867/20000], Loss: 34.97920227050781, Learning Rate: 0.01\n",
      "Epoch [2868/20000], Loss: 34.96533203125, Learning Rate: 0.01\n",
      "Epoch [2869/20000], Loss: 34.951568603515625, Learning Rate: 0.01\n",
      "Epoch [2870/20000], Loss: 34.93769836425781, Learning Rate: 0.01\n",
      "Epoch [2871/20000], Loss: 34.923919677734375, Learning Rate: 0.01\n",
      "Epoch [2872/20000], Loss: 34.91014099121094, Learning Rate: 0.01\n",
      "Epoch [2873/20000], Loss: 34.896148681640625, Learning Rate: 0.01\n",
      "Epoch [2874/20000], Loss: 34.882293701171875, Learning Rate: 0.01\n",
      "Epoch [2875/20000], Loss: 34.86846923828125, Learning Rate: 0.01\n",
      "Epoch [2876/20000], Loss: 34.85466003417969, Learning Rate: 0.01\n",
      "Epoch [2877/20000], Loss: 34.84075927734375, Learning Rate: 0.01\n",
      "Epoch [2878/20000], Loss: 34.826995849609375, Learning Rate: 0.01\n",
      "Epoch [2879/20000], Loss: 34.813262939453125, Learning Rate: 0.01\n",
      "Epoch [2880/20000], Loss: 34.7994384765625, Learning Rate: 0.01\n",
      "Epoch [2881/20000], Loss: 34.785552978515625, Learning Rate: 0.01\n",
      "Epoch [2882/20000], Loss: 34.77177429199219, Learning Rate: 0.01\n",
      "Epoch [2883/20000], Loss: 34.75791931152344, Learning Rate: 0.01\n",
      "Epoch [2884/20000], Loss: 34.74409484863281, Learning Rate: 0.01\n",
      "Epoch [2885/20000], Loss: 34.73027038574219, Learning Rate: 0.01\n",
      "Epoch [2886/20000], Loss: 34.71649169921875, Learning Rate: 0.01\n",
      "Epoch [2887/20000], Loss: 34.70269775390625, Learning Rate: 0.01\n",
      "Epoch [2888/20000], Loss: 34.688873291015625, Learning Rate: 0.01\n",
      "Epoch [2889/20000], Loss: 34.675079345703125, Learning Rate: 0.01\n",
      "Epoch [2890/20000], Loss: 34.661102294921875, Learning Rate: 0.01\n",
      "Epoch [2891/20000], Loss: 34.6474609375, Learning Rate: 0.01\n",
      "Epoch [2892/20000], Loss: 34.63360595703125, Learning Rate: 0.01\n",
      "Epoch [2893/20000], Loss: 34.61981201171875, Learning Rate: 0.01\n",
      "Epoch [2894/20000], Loss: 34.605987548828125, Learning Rate: 0.01\n",
      "Epoch [2895/20000], Loss: 34.5921630859375, Learning Rate: 0.01\n",
      "Epoch [2896/20000], Loss: 34.57843017578125, Learning Rate: 0.01\n",
      "Epoch [2897/20000], Loss: 34.5645751953125, Learning Rate: 0.01\n",
      "Epoch [2898/20000], Loss: 34.55085754394531, Learning Rate: 0.01\n",
      "Epoch [2899/20000], Loss: 34.53706359863281, Learning Rate: 0.01\n",
      "Epoch [2900/20000], Loss: 34.523193359375, Learning Rate: 0.01\n",
      "Epoch [2901/20000], Loss: 34.5093994140625, Learning Rate: 0.01\n",
      "Epoch [2902/20000], Loss: 34.49562072753906, Learning Rate: 0.01\n",
      "Epoch [2903/20000], Loss: 34.48182678222656, Learning Rate: 0.01\n",
      "Epoch [2904/20000], Loss: 34.46800231933594, Learning Rate: 0.01\n",
      "Epoch [2905/20000], Loss: 34.45433044433594, Learning Rate: 0.01\n",
      "Epoch [2906/20000], Loss: 34.44049072265625, Learning Rate: 0.01\n",
      "Epoch [2907/20000], Loss: 34.4266357421875, Learning Rate: 0.01\n",
      "Epoch [2908/20000], Loss: 34.41291809082031, Learning Rate: 0.01\n",
      "Epoch [2909/20000], Loss: 34.39918518066406, Learning Rate: 0.01\n",
      "Epoch [2910/20000], Loss: 34.385284423828125, Learning Rate: 0.01\n",
      "Epoch [2911/20000], Loss: 34.37152099609375, Learning Rate: 0.01\n",
      "Epoch [2912/20000], Loss: 34.35783386230469, Learning Rate: 0.01\n",
      "Epoch [2913/20000], Loss: 34.34413146972656, Learning Rate: 0.01\n",
      "Epoch [2914/20000], Loss: 34.33027648925781, Learning Rate: 0.01\n",
      "Epoch [2915/20000], Loss: 34.31636047363281, Learning Rate: 0.01\n",
      "Epoch [2916/20000], Loss: 34.30267333984375, Learning Rate: 0.01\n",
      "Epoch [2917/20000], Loss: 34.28887939453125, Learning Rate: 0.01\n",
      "Epoch [2918/20000], Loss: 34.275115966796875, Learning Rate: 0.01\n",
      "Epoch [2919/20000], Loss: 34.26130676269531, Learning Rate: 0.01\n",
      "Epoch [2920/20000], Loss: 34.24755859375, Learning Rate: 0.01\n",
      "Epoch [2921/20000], Loss: 34.23384094238281, Learning Rate: 0.01\n",
      "Epoch [2922/20000], Loss: 34.220123291015625, Learning Rate: 0.01\n",
      "Epoch [2923/20000], Loss: 34.20637512207031, Learning Rate: 0.01\n",
      "Epoch [2924/20000], Loss: 34.192596435546875, Learning Rate: 0.01\n",
      "Epoch [2925/20000], Loss: 34.17884826660156, Learning Rate: 0.01\n",
      "Epoch [2926/20000], Loss: 34.16505432128906, Learning Rate: 0.01\n",
      "Epoch [2927/20000], Loss: 34.1512451171875, Learning Rate: 0.01\n",
      "Epoch [2928/20000], Loss: 34.1375732421875, Learning Rate: 0.01\n",
      "Epoch [2929/20000], Loss: 34.12370300292969, Learning Rate: 0.01\n",
      "Epoch [2930/20000], Loss: 34.11003112792969, Learning Rate: 0.01\n",
      "Epoch [2931/20000], Loss: 34.0963134765625, Learning Rate: 0.01\n",
      "Epoch [2932/20000], Loss: 34.082489013671875, Learning Rate: 0.01\n",
      "Epoch [2933/20000], Loss: 34.06880187988281, Learning Rate: 0.01\n",
      "Epoch [2934/20000], Loss: 34.05499267578125, Learning Rate: 0.01\n",
      "Epoch [2935/20000], Loss: 34.04133605957031, Learning Rate: 0.01\n",
      "Epoch [2936/20000], Loss: 34.02754211425781, Learning Rate: 0.01\n",
      "Epoch [2937/20000], Loss: 34.01380920410156, Learning Rate: 0.01\n",
      "Epoch [2938/20000], Loss: 34.00013732910156, Learning Rate: 0.01\n",
      "Epoch [2939/20000], Loss: 33.98626708984375, Learning Rate: 0.01\n",
      "Epoch [2940/20000], Loss: 33.97265625, Learning Rate: 0.01\n",
      "Epoch [2941/20000], Loss: 33.958892822265625, Learning Rate: 0.01\n",
      "Epoch [2942/20000], Loss: 33.94514465332031, Learning Rate: 0.01\n",
      "Epoch [2943/20000], Loss: 33.93128967285156, Learning Rate: 0.01\n",
      "Epoch [2944/20000], Loss: 33.91761779785156, Learning Rate: 0.01\n",
      "Epoch [2945/20000], Loss: 33.90385437011719, Learning Rate: 0.01\n",
      "Epoch [2946/20000], Loss: 33.89033508300781, Learning Rate: 0.01\n",
      "Epoch [2947/20000], Loss: 33.87646484375, Learning Rate: 0.01\n",
      "Epoch [2948/20000], Loss: 33.86277770996094, Learning Rate: 0.01\n",
      "Epoch [2949/20000], Loss: 33.84901428222656, Learning Rate: 0.01\n",
      "Epoch [2950/20000], Loss: 33.83526611328125, Learning Rate: 0.01\n",
      "Epoch [2951/20000], Loss: 33.82170104980469, Learning Rate: 0.01\n",
      "Epoch [2952/20000], Loss: 33.80790710449219, Learning Rate: 0.01\n",
      "Epoch [2953/20000], Loss: 33.79420471191406, Learning Rate: 0.01\n",
      "Epoch [2954/20000], Loss: 33.78034973144531, Learning Rate: 0.01\n",
      "Epoch [2955/20000], Loss: 33.76678466796875, Learning Rate: 0.01\n",
      "Epoch [2956/20000], Loss: 33.753021240234375, Learning Rate: 0.01\n",
      "Epoch [2957/20000], Loss: 33.73927307128906, Learning Rate: 0.01\n",
      "Epoch [2958/20000], Loss: 33.72566223144531, Learning Rate: 0.01\n",
      "Epoch [2959/20000], Loss: 33.71185302734375, Learning Rate: 0.01\n",
      "Epoch [2960/20000], Loss: 33.69818115234375, Learning Rate: 0.01\n",
      "Epoch [2961/20000], Loss: 33.684417724609375, Learning Rate: 0.01\n",
      "Epoch [2962/20000], Loss: 33.67071533203125, Learning Rate: 0.01\n",
      "Epoch [2963/20000], Loss: 33.657135009765625, Learning Rate: 0.01\n",
      "Epoch [2964/20000], Loss: 33.64335632324219, Learning Rate: 0.01\n",
      "Epoch [2965/20000], Loss: 33.629730224609375, Learning Rate: 0.01\n",
      "Epoch [2966/20000], Loss: 33.61602783203125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2967/20000], Loss: 33.60221862792969, Learning Rate: 0.01\n",
      "Epoch [2968/20000], Loss: 33.588531494140625, Learning Rate: 0.01\n",
      "Epoch [2969/20000], Loss: 33.57489013671875, Learning Rate: 0.01\n",
      "Epoch [2970/20000], Loss: 33.561065673828125, Learning Rate: 0.01\n",
      "Epoch [2971/20000], Loss: 33.54747009277344, Learning Rate: 0.01\n",
      "Epoch [2972/20000], Loss: 33.5338134765625, Learning Rate: 0.01\n",
      "Epoch [2973/20000], Loss: 33.520111083984375, Learning Rate: 0.01\n",
      "Epoch [2974/20000], Loss: 33.506317138671875, Learning Rate: 0.01\n",
      "Epoch [2975/20000], Loss: 33.4927978515625, Learning Rate: 0.01\n",
      "Epoch [2976/20000], Loss: 33.47900390625, Learning Rate: 0.01\n",
      "Epoch [2977/20000], Loss: 33.46543884277344, Learning Rate: 0.01\n",
      "Epoch [2978/20000], Loss: 33.45159912109375, Learning Rate: 0.01\n",
      "Epoch [2979/20000], Loss: 33.43792724609375, Learning Rate: 0.01\n",
      "Epoch [2980/20000], Loss: 33.42439270019531, Learning Rate: 0.01\n",
      "Epoch [2981/20000], Loss: 33.41070556640625, Learning Rate: 0.01\n",
      "Epoch [2982/20000], Loss: 33.396942138671875, Learning Rate: 0.01\n",
      "Epoch [2983/20000], Loss: 33.38328552246094, Learning Rate: 0.01\n",
      "Epoch [2984/20000], Loss: 33.36955261230469, Learning Rate: 0.01\n",
      "Epoch [2985/20000], Loss: 33.3560791015625, Learning Rate: 0.01\n",
      "Epoch [2986/20000], Loss: 33.342254638671875, Learning Rate: 0.01\n",
      "Epoch [2987/20000], Loss: 33.32868957519531, Learning Rate: 0.01\n",
      "Epoch [2988/20000], Loss: 33.31486511230469, Learning Rate: 0.01\n",
      "Epoch [2989/20000], Loss: 33.30128479003906, Learning Rate: 0.01\n",
      "Epoch [2990/20000], Loss: 33.28752136230469, Learning Rate: 0.01\n",
      "Epoch [2991/20000], Loss: 33.274078369140625, Learning Rate: 0.01\n",
      "Epoch [2992/20000], Loss: 33.26042175292969, Learning Rate: 0.01\n",
      "Epoch [2993/20000], Loss: 33.24665832519531, Learning Rate: 0.01\n",
      "Epoch [2994/20000], Loss: 33.23297119140625, Learning Rate: 0.01\n",
      "Epoch [2995/20000], Loss: 33.21931457519531, Learning Rate: 0.01\n",
      "Epoch [2996/20000], Loss: 33.20573425292969, Learning Rate: 0.01\n",
      "Epoch [2997/20000], Loss: 33.192138671875, Learning Rate: 0.01\n",
      "Epoch [2998/20000], Loss: 33.17835998535156, Learning Rate: 0.01\n",
      "Epoch [2999/20000], Loss: 33.1646728515625, Learning Rate: 0.01\n",
      "Epoch [3000/20000], Loss: 33.15110778808594, Learning Rate: 0.01\n",
      "Epoch [3001/20000], Loss: 33.13740539550781, Learning Rate: 0.01\n",
      "Epoch [3002/20000], Loss: 33.12376403808594, Learning Rate: 0.01\n",
      "Epoch [3003/20000], Loss: 33.11015319824219, Learning Rate: 0.01\n",
      "Epoch [3004/20000], Loss: 33.09654235839844, Learning Rate: 0.01\n",
      "Epoch [3005/20000], Loss: 33.08293151855469, Learning Rate: 0.01\n",
      "Epoch [3006/20000], Loss: 33.06929016113281, Learning Rate: 0.01\n",
      "Epoch [3007/20000], Loss: 33.055572509765625, Learning Rate: 0.01\n",
      "Epoch [3008/20000], Loss: 33.04197692871094, Learning Rate: 0.01\n",
      "Epoch [3009/20000], Loss: 33.02833557128906, Learning Rate: 0.01\n",
      "Epoch [3010/20000], Loss: 33.014678955078125, Learning Rate: 0.01\n",
      "Epoch [3011/20000], Loss: 33.00090026855469, Learning Rate: 0.01\n",
      "Epoch [3012/20000], Loss: 32.987457275390625, Learning Rate: 0.01\n",
      "Epoch [3013/20000], Loss: 32.97389221191406, Learning Rate: 0.01\n",
      "Epoch [3014/20000], Loss: 32.96026611328125, Learning Rate: 0.01\n",
      "Epoch [3015/20000], Loss: 32.94654846191406, Learning Rate: 0.01\n",
      "Epoch [3016/20000], Loss: 32.93292236328125, Learning Rate: 0.01\n",
      "Epoch [3017/20000], Loss: 32.91935729980469, Learning Rate: 0.01\n",
      "Epoch [3018/20000], Loss: 32.90571594238281, Learning Rate: 0.01\n",
      "Epoch [3019/20000], Loss: 32.89207458496094, Learning Rate: 0.01\n",
      "Epoch [3020/20000], Loss: 32.87843322753906, Learning Rate: 0.01\n",
      "Epoch [3021/20000], Loss: 32.864898681640625, Learning Rate: 0.01\n",
      "Epoch [3022/20000], Loss: 32.851226806640625, Learning Rate: 0.01\n",
      "Epoch [3023/20000], Loss: 32.83769226074219, Learning Rate: 0.01\n",
      "Epoch [3024/20000], Loss: 32.82402038574219, Learning Rate: 0.01\n",
      "Epoch [3025/20000], Loss: 32.81047058105469, Learning Rate: 0.01\n",
      "Epoch [3026/20000], Loss: 32.796783447265625, Learning Rate: 0.01\n",
      "Epoch [3027/20000], Loss: 32.78326416015625, Learning Rate: 0.01\n",
      "Epoch [3028/20000], Loss: 32.76979064941406, Learning Rate: 0.01\n",
      "Epoch [3029/20000], Loss: 32.75596618652344, Learning Rate: 0.01\n",
      "Epoch [3030/20000], Loss: 32.74238586425781, Learning Rate: 0.01\n",
      "Epoch [3031/20000], Loss: 32.728759765625, Learning Rate: 0.01\n",
      "Epoch [3032/20000], Loss: 32.71528625488281, Learning Rate: 0.01\n",
      "Epoch [3033/20000], Loss: 32.70166015625, Learning Rate: 0.01\n",
      "Epoch [3034/20000], Loss: 32.68804931640625, Learning Rate: 0.01\n",
      "Epoch [3035/20000], Loss: 32.67437744140625, Learning Rate: 0.01\n",
      "Epoch [3036/20000], Loss: 32.660919189453125, Learning Rate: 0.01\n",
      "Epoch [3037/20000], Loss: 32.64729309082031, Learning Rate: 0.01\n",
      "Epoch [3038/20000], Loss: 32.63365173339844, Learning Rate: 0.01\n",
      "Epoch [3039/20000], Loss: 32.62013244628906, Learning Rate: 0.01\n",
      "Epoch [3040/20000], Loss: 32.60650634765625, Learning Rate: 0.01\n",
      "Epoch [3041/20000], Loss: 32.592987060546875, Learning Rate: 0.01\n",
      "Epoch [3042/20000], Loss: 32.57936096191406, Learning Rate: 0.01\n",
      "Epoch [3043/20000], Loss: 32.56585693359375, Learning Rate: 0.01\n",
      "Epoch [3044/20000], Loss: 32.55218505859375, Learning Rate: 0.01\n",
      "Epoch [3045/20000], Loss: 32.53871154785156, Learning Rate: 0.01\n",
      "Epoch [3046/20000], Loss: 32.525054931640625, Learning Rate: 0.01\n",
      "Epoch [3047/20000], Loss: 32.51153564453125, Learning Rate: 0.01\n",
      "Epoch [3048/20000], Loss: 32.49781799316406, Learning Rate: 0.01\n",
      "Epoch [3049/20000], Loss: 32.48426818847656, Learning Rate: 0.01\n",
      "Epoch [3050/20000], Loss: 32.47074890136719, Learning Rate: 0.01\n",
      "Epoch [3051/20000], Loss: 32.45716857910156, Learning Rate: 0.01\n",
      "Epoch [3052/20000], Loss: 32.443511962890625, Learning Rate: 0.01\n",
      "Epoch [3053/20000], Loss: 32.430084228515625, Learning Rate: 0.01\n",
      "Epoch [3054/20000], Loss: 32.41648864746094, Learning Rate: 0.01\n",
      "Epoch [3055/20000], Loss: 32.402984619140625, Learning Rate: 0.01\n",
      "Epoch [3056/20000], Loss: 32.389404296875, Learning Rate: 0.01\n",
      "Epoch [3057/20000], Loss: 32.37591552734375, Learning Rate: 0.01\n",
      "Epoch [3058/20000], Loss: 32.36238098144531, Learning Rate: 0.01\n",
      "Epoch [3059/20000], Loss: 32.34877014160156, Learning Rate: 0.01\n",
      "Epoch [3060/20000], Loss: 32.335113525390625, Learning Rate: 0.01\n",
      "Epoch [3061/20000], Loss: 32.321624755859375, Learning Rate: 0.01\n",
      "Epoch [3062/20000], Loss: 32.30805969238281, Learning Rate: 0.01\n",
      "Epoch [3063/20000], Loss: 32.29441833496094, Learning Rate: 0.01\n",
      "Epoch [3064/20000], Loss: 32.28102111816406, Learning Rate: 0.01\n",
      "Epoch [3065/20000], Loss: 32.267425537109375, Learning Rate: 0.01\n",
      "Epoch [3066/20000], Loss: 32.25389099121094, Learning Rate: 0.01\n",
      "Epoch [3067/20000], Loss: 32.2403564453125, Learning Rate: 0.01\n",
      "Epoch [3068/20000], Loss: 32.22688293457031, Learning Rate: 0.01\n",
      "Epoch [3069/20000], Loss: 32.213287353515625, Learning Rate: 0.01\n",
      "Epoch [3070/20000], Loss: 32.19981384277344, Learning Rate: 0.01\n",
      "Epoch [3071/20000], Loss: 32.18614196777344, Learning Rate: 0.01\n",
      "Epoch [3072/20000], Loss: 32.17262268066406, Learning Rate: 0.01\n",
      "Epoch [3073/20000], Loss: 32.159088134765625, Learning Rate: 0.01\n",
      "Epoch [3074/20000], Loss: 32.14564514160156, Learning Rate: 0.01\n",
      "Epoch [3075/20000], Loss: 32.132049560546875, Learning Rate: 0.01\n",
      "Epoch [3076/20000], Loss: 32.11854553222656, Learning Rate: 0.01\n",
      "Epoch [3077/20000], Loss: 32.104949951171875, Learning Rate: 0.01\n",
      "Epoch [3078/20000], Loss: 32.09150695800781, Learning Rate: 0.01\n",
      "Epoch [3079/20000], Loss: 32.078033447265625, Learning Rate: 0.01\n",
      "Epoch [3080/20000], Loss: 32.06451416015625, Learning Rate: 0.01\n",
      "Epoch [3081/20000], Loss: 32.051025390625, Learning Rate: 0.01\n",
      "Epoch [3082/20000], Loss: 32.037445068359375, Learning Rate: 0.01\n",
      "Epoch [3083/20000], Loss: 32.023956298828125, Learning Rate: 0.01\n",
      "Epoch [3084/20000], Loss: 32.010406494140625, Learning Rate: 0.01\n",
      "Epoch [3085/20000], Loss: 31.996826171875, Learning Rate: 0.01\n",
      "Epoch [3086/20000], Loss: 31.983367919921875, Learning Rate: 0.01\n",
      "Epoch [3087/20000], Loss: 31.96990966796875, Learning Rate: 0.01\n",
      "Epoch [3088/20000], Loss: 31.956375122070312, Learning Rate: 0.01\n",
      "Epoch [3089/20000], Loss: 31.942840576171875, Learning Rate: 0.01\n",
      "Epoch [3090/20000], Loss: 31.929336547851562, Learning Rate: 0.01\n",
      "Epoch [3091/20000], Loss: 31.91583251953125, Learning Rate: 0.01\n",
      "Epoch [3092/20000], Loss: 31.902420043945312, Learning Rate: 0.01\n",
      "Epoch [3093/20000], Loss: 31.888809204101562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3094/20000], Loss: 31.875411987304688, Learning Rate: 0.01\n",
      "Epoch [3095/20000], Loss: 31.861907958984375, Learning Rate: 0.01\n",
      "Epoch [3096/20000], Loss: 31.848297119140625, Learning Rate: 0.01\n",
      "Epoch [3097/20000], Loss: 31.8349609375, Learning Rate: 0.01\n",
      "Epoch [3098/20000], Loss: 31.821395874023438, Learning Rate: 0.01\n",
      "Epoch [3099/20000], Loss: 31.80792236328125, Learning Rate: 0.01\n",
      "Epoch [3100/20000], Loss: 31.794448852539062, Learning Rate: 0.01\n",
      "Epoch [3101/20000], Loss: 31.780990600585938, Learning Rate: 0.01\n",
      "Epoch [3102/20000], Loss: 31.7674560546875, Learning Rate: 0.01\n",
      "Epoch [3103/20000], Loss: 31.753952026367188, Learning Rate: 0.01\n",
      "Epoch [3104/20000], Loss: 31.740585327148438, Learning Rate: 0.01\n",
      "Epoch [3105/20000], Loss: 31.72698974609375, Learning Rate: 0.01\n",
      "Epoch [3106/20000], Loss: 31.713531494140625, Learning Rate: 0.01\n",
      "Epoch [3107/20000], Loss: 31.700057983398438, Learning Rate: 0.01\n",
      "Epoch [3108/20000], Loss: 31.686538696289062, Learning Rate: 0.01\n",
      "Epoch [3109/20000], Loss: 31.673095703125, Learning Rate: 0.01\n",
      "Epoch [3110/20000], Loss: 31.659759521484375, Learning Rate: 0.01\n",
      "Epoch [3111/20000], Loss: 31.646148681640625, Learning Rate: 0.01\n",
      "Epoch [3112/20000], Loss: 31.632720947265625, Learning Rate: 0.01\n",
      "Epoch [3113/20000], Loss: 31.619186401367188, Learning Rate: 0.01\n",
      "Epoch [3114/20000], Loss: 31.60577392578125, Learning Rate: 0.01\n",
      "Epoch [3115/20000], Loss: 31.592315673828125, Learning Rate: 0.01\n",
      "Epoch [3116/20000], Loss: 31.578948974609375, Learning Rate: 0.01\n",
      "Epoch [3117/20000], Loss: 31.565353393554688, Learning Rate: 0.01\n",
      "Epoch [3118/20000], Loss: 31.551834106445312, Learning Rate: 0.01\n",
      "Epoch [3119/20000], Loss: 31.538482666015625, Learning Rate: 0.01\n",
      "Epoch [3120/20000], Loss: 31.525054931640625, Learning Rate: 0.01\n",
      "Epoch [3121/20000], Loss: 31.51165771484375, Learning Rate: 0.01\n",
      "Epoch [3122/20000], Loss: 31.498184204101562, Learning Rate: 0.01\n",
      "Epoch [3123/20000], Loss: 31.484649658203125, Learning Rate: 0.01\n",
      "Epoch [3124/20000], Loss: 31.471221923828125, Learning Rate: 0.01\n",
      "Epoch [3125/20000], Loss: 31.457870483398438, Learning Rate: 0.01\n",
      "Epoch [3126/20000], Loss: 31.444412231445312, Learning Rate: 0.01\n",
      "Epoch [3127/20000], Loss: 31.430923461914062, Learning Rate: 0.01\n",
      "Epoch [3128/20000], Loss: 31.417434692382812, Learning Rate: 0.01\n",
      "Epoch [3129/20000], Loss: 31.404098510742188, Learning Rate: 0.01\n",
      "Epoch [3130/20000], Loss: 31.390594482421875, Learning Rate: 0.01\n",
      "Epoch [3131/20000], Loss: 31.377197265625, Learning Rate: 0.01\n",
      "Epoch [3132/20000], Loss: 31.363723754882812, Learning Rate: 0.01\n",
      "Epoch [3133/20000], Loss: 31.350265502929688, Learning Rate: 0.01\n",
      "Epoch [3134/20000], Loss: 31.336959838867188, Learning Rate: 0.01\n",
      "Epoch [3135/20000], Loss: 31.323501586914062, Learning Rate: 0.01\n",
      "Epoch [3136/20000], Loss: 31.309967041015625, Learning Rate: 0.01\n",
      "Epoch [3137/20000], Loss: 31.296554565429688, Learning Rate: 0.01\n",
      "Epoch [3138/20000], Loss: 31.283172607421875, Learning Rate: 0.01\n",
      "Epoch [3139/20000], Loss: 31.269790649414062, Learning Rate: 0.01\n",
      "Epoch [3140/20000], Loss: 31.256317138671875, Learning Rate: 0.01\n",
      "Epoch [3141/20000], Loss: 31.242919921875, Learning Rate: 0.01\n",
      "Epoch [3142/20000], Loss: 31.229507446289062, Learning Rate: 0.01\n",
      "Epoch [3143/20000], Loss: 31.216049194335938, Learning Rate: 0.01\n",
      "Epoch [3144/20000], Loss: 31.202667236328125, Learning Rate: 0.01\n",
      "Epoch [3145/20000], Loss: 31.18927001953125, Learning Rate: 0.01\n",
      "Epoch [3146/20000], Loss: 31.175872802734375, Learning Rate: 0.01\n",
      "Epoch [3147/20000], Loss: 31.162384033203125, Learning Rate: 0.01\n",
      "Epoch [3148/20000], Loss: 31.149017333984375, Learning Rate: 0.01\n",
      "Epoch [3149/20000], Loss: 31.135650634765625, Learning Rate: 0.01\n",
      "Epoch [3150/20000], Loss: 31.122222900390625, Learning Rate: 0.01\n",
      "Epoch [3151/20000], Loss: 31.108840942382812, Learning Rate: 0.01\n",
      "Epoch [3152/20000], Loss: 31.095489501953125, Learning Rate: 0.01\n",
      "Epoch [3153/20000], Loss: 31.082046508789062, Learning Rate: 0.01\n",
      "Epoch [3154/20000], Loss: 31.068649291992188, Learning Rate: 0.01\n",
      "Epoch [3155/20000], Loss: 31.05517578125, Learning Rate: 0.01\n",
      "Epoch [3156/20000], Loss: 31.041824340820312, Learning Rate: 0.01\n",
      "Epoch [3157/20000], Loss: 31.0284423828125, Learning Rate: 0.01\n",
      "Epoch [3158/20000], Loss: 31.015106201171875, Learning Rate: 0.01\n",
      "Epoch [3159/20000], Loss: 31.001693725585938, Learning Rate: 0.01\n",
      "Epoch [3160/20000], Loss: 30.988311767578125, Learning Rate: 0.01\n",
      "Epoch [3161/20000], Loss: 30.97491455078125, Learning Rate: 0.01\n",
      "Epoch [3162/20000], Loss: 30.9615478515625, Learning Rate: 0.01\n",
      "Epoch [3163/20000], Loss: 30.948150634765625, Learning Rate: 0.01\n",
      "Epoch [3164/20000], Loss: 30.934738159179688, Learning Rate: 0.01\n",
      "Epoch [3165/20000], Loss: 30.921478271484375, Learning Rate: 0.01\n",
      "Epoch [3166/20000], Loss: 30.907974243164062, Learning Rate: 0.01\n",
      "Epoch [3167/20000], Loss: 30.89471435546875, Learning Rate: 0.01\n",
      "Epoch [3168/20000], Loss: 30.881317138671875, Learning Rate: 0.01\n",
      "Epoch [3169/20000], Loss: 30.867919921875, Learning Rate: 0.01\n",
      "Epoch [3170/20000], Loss: 30.854629516601562, Learning Rate: 0.01\n",
      "Epoch [3171/20000], Loss: 30.841217041015625, Learning Rate: 0.01\n",
      "Epoch [3172/20000], Loss: 30.827774047851562, Learning Rate: 0.01\n",
      "Epoch [3173/20000], Loss: 30.814437866210938, Learning Rate: 0.01\n",
      "Epoch [3174/20000], Loss: 30.80108642578125, Learning Rate: 0.01\n",
      "Epoch [3175/20000], Loss: 30.787734985351562, Learning Rate: 0.01\n",
      "Epoch [3176/20000], Loss: 30.774368286132812, Learning Rate: 0.01\n",
      "Epoch [3177/20000], Loss: 30.761032104492188, Learning Rate: 0.01\n",
      "Epoch [3178/20000], Loss: 30.7476806640625, Learning Rate: 0.01\n",
      "Epoch [3179/20000], Loss: 30.73431396484375, Learning Rate: 0.01\n",
      "Epoch [3180/20000], Loss: 30.72100830078125, Learning Rate: 0.01\n",
      "Epoch [3181/20000], Loss: 30.707794189453125, Learning Rate: 0.01\n",
      "Epoch [3182/20000], Loss: 30.694198608398438, Learning Rate: 0.01\n",
      "Epoch [3183/20000], Loss: 30.680892944335938, Learning Rate: 0.01\n",
      "Epoch [3184/20000], Loss: 30.667556762695312, Learning Rate: 0.01\n",
      "Epoch [3185/20000], Loss: 30.654296875, Learning Rate: 0.01\n",
      "Epoch [3186/20000], Loss: 30.640884399414062, Learning Rate: 0.01\n",
      "Epoch [3187/20000], Loss: 30.627578735351562, Learning Rate: 0.01\n",
      "Epoch [3188/20000], Loss: 30.614242553710938, Learning Rate: 0.01\n",
      "Epoch [3189/20000], Loss: 30.600845336914062, Learning Rate: 0.01\n",
      "Epoch [3190/20000], Loss: 30.587600708007812, Learning Rate: 0.01\n",
      "Epoch [3191/20000], Loss: 30.57415771484375, Learning Rate: 0.01\n",
      "Epoch [3192/20000], Loss: 30.56085205078125, Learning Rate: 0.01\n",
      "Epoch [3193/20000], Loss: 30.5474853515625, Learning Rate: 0.01\n",
      "Epoch [3194/20000], Loss: 30.534164428710938, Learning Rate: 0.01\n",
      "Epoch [3195/20000], Loss: 30.520889282226562, Learning Rate: 0.01\n",
      "Epoch [3196/20000], Loss: 30.507522583007812, Learning Rate: 0.01\n",
      "Epoch [3197/20000], Loss: 30.494216918945312, Learning Rate: 0.01\n",
      "Epoch [3198/20000], Loss: 30.480926513671875, Learning Rate: 0.01\n",
      "Epoch [3199/20000], Loss: 30.467483520507812, Learning Rate: 0.01\n",
      "Epoch [3200/20000], Loss: 30.454345703125, Learning Rate: 0.01\n",
      "Epoch [3201/20000], Loss: 30.440994262695312, Learning Rate: 0.01\n",
      "Epoch [3202/20000], Loss: 30.427688598632812, Learning Rate: 0.01\n",
      "Epoch [3203/20000], Loss: 30.414321899414062, Learning Rate: 0.01\n",
      "Epoch [3204/20000], Loss: 30.401046752929688, Learning Rate: 0.01\n",
      "Epoch [3205/20000], Loss: 30.387710571289062, Learning Rate: 0.01\n",
      "Epoch [3206/20000], Loss: 30.37445068359375, Learning Rate: 0.01\n",
      "Epoch [3207/20000], Loss: 30.361129760742188, Learning Rate: 0.01\n",
      "Epoch [3208/20000], Loss: 30.3477783203125, Learning Rate: 0.01\n",
      "Epoch [3209/20000], Loss: 30.33447265625, Learning Rate: 0.01\n",
      "Epoch [3210/20000], Loss: 30.32122802734375, Learning Rate: 0.01\n",
      "Epoch [3211/20000], Loss: 30.307952880859375, Learning Rate: 0.01\n",
      "Epoch [3212/20000], Loss: 30.294631958007812, Learning Rate: 0.01\n",
      "Epoch [3213/20000], Loss: 30.281280517578125, Learning Rate: 0.01\n",
      "Epoch [3214/20000], Loss: 30.268051147460938, Learning Rate: 0.01\n",
      "Epoch [3215/20000], Loss: 30.254745483398438, Learning Rate: 0.01\n",
      "Epoch [3216/20000], Loss: 30.241470336914062, Learning Rate: 0.01\n",
      "Epoch [3217/20000], Loss: 30.228195190429688, Learning Rate: 0.01\n",
      "Epoch [3218/20000], Loss: 30.214752197265625, Learning Rate: 0.01\n",
      "Epoch [3219/20000], Loss: 30.201583862304688, Learning Rate: 0.01\n",
      "Epoch [3220/20000], Loss: 30.18829345703125, Learning Rate: 0.01\n",
      "Epoch [3221/20000], Loss: 30.175033569335938, Learning Rate: 0.01\n",
      "Epoch [3222/20000], Loss: 30.161727905273438, Learning Rate: 0.01\n",
      "Epoch [3223/20000], Loss: 30.148468017578125, Learning Rate: 0.01\n",
      "Epoch [3224/20000], Loss: 30.13519287109375, Learning Rate: 0.01\n",
      "Epoch [3225/20000], Loss: 30.121917724609375, Learning Rate: 0.01\n",
      "Epoch [3226/20000], Loss: 30.108673095703125, Learning Rate: 0.01\n",
      "Epoch [3227/20000], Loss: 30.095413208007812, Learning Rate: 0.01\n",
      "Epoch [3228/20000], Loss: 30.08203125, Learning Rate: 0.01\n",
      "Epoch [3229/20000], Loss: 30.06884765625, Learning Rate: 0.01\n",
      "Epoch [3230/20000], Loss: 30.0555419921875, Learning Rate: 0.01\n",
      "Epoch [3231/20000], Loss: 30.042312622070312, Learning Rate: 0.01\n",
      "Epoch [3232/20000], Loss: 30.028976440429688, Learning Rate: 0.01\n",
      "Epoch [3233/20000], Loss: 30.015823364257812, Learning Rate: 0.01\n",
      "Epoch [3234/20000], Loss: 30.00250244140625, Learning Rate: 0.01\n",
      "Epoch [3235/20000], Loss: 29.989349365234375, Learning Rate: 0.01\n",
      "Epoch [3236/20000], Loss: 29.976028442382812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3237/20000], Loss: 29.962677001953125, Learning Rate: 0.01\n",
      "Epoch [3238/20000], Loss: 29.94952392578125, Learning Rate: 0.01\n",
      "Epoch [3239/20000], Loss: 29.936187744140625, Learning Rate: 0.01\n",
      "Epoch [3240/20000], Loss: 29.922988891601562, Learning Rate: 0.01\n",
      "Epoch [3241/20000], Loss: 29.909713745117188, Learning Rate: 0.01\n",
      "Epoch [3242/20000], Loss: 29.896514892578125, Learning Rate: 0.01\n",
      "Epoch [3243/20000], Loss: 29.883316040039062, Learning Rate: 0.01\n",
      "Epoch [3244/20000], Loss: 29.870071411132812, Learning Rate: 0.01\n",
      "Epoch [3245/20000], Loss: 29.8568115234375, Learning Rate: 0.01\n",
      "Epoch [3246/20000], Loss: 29.843658447265625, Learning Rate: 0.01\n",
      "Epoch [3247/20000], Loss: 29.830368041992188, Learning Rate: 0.01\n",
      "Epoch [3248/20000], Loss: 29.817123413085938, Learning Rate: 0.01\n",
      "Epoch [3249/20000], Loss: 29.8040771484375, Learning Rate: 0.01\n",
      "Epoch [3250/20000], Loss: 29.790863037109375, Learning Rate: 0.01\n",
      "Epoch [3251/20000], Loss: 29.777877807617188, Learning Rate: 0.01\n",
      "Epoch [3252/20000], Loss: 29.764892578125, Learning Rate: 0.01\n",
      "Epoch [3253/20000], Loss: 29.75189208984375, Learning Rate: 0.01\n",
      "Epoch [3254/20000], Loss: 29.739303588867188, Learning Rate: 0.01\n",
      "Epoch [3255/20000], Loss: 29.727066040039062, Learning Rate: 0.01\n",
      "Epoch [3256/20000], Loss: 29.7152099609375, Learning Rate: 0.01\n",
      "Epoch [3257/20000], Loss: 29.70465087890625, Learning Rate: 0.01\n",
      "Epoch [3258/20000], Loss: 29.69537353515625, Learning Rate: 0.01\n",
      "Epoch [3259/20000], Loss: 29.688705444335938, Learning Rate: 0.01\n",
      "Epoch [3260/20000], Loss: 29.686203002929688, Learning Rate: 0.01\n",
      "Epoch [3261/20000], Loss: 29.690826416015625, Learning Rate: 0.01\n",
      "Epoch [3262/20000], Loss: 29.707412719726562, Learning Rate: 0.01\n",
      "Epoch [3263/20000], Loss: 29.744354248046875, Learning Rate: 0.01\n",
      "Epoch [3264/20000], Loss: 29.81585693359375, Learning Rate: 0.01\n",
      "Epoch [3265/20000], Loss: 29.945785522460938, Learning Rate: 0.01\n",
      "Epoch [3266/20000], Loss: 30.173797607421875, Learning Rate: 0.01\n",
      "Epoch [3267/20000], Loss: 30.565292358398438, Learning Rate: 0.01\n",
      "Epoch [3268/20000], Loss: 31.2193603515625, Learning Rate: 0.01\n",
      "Epoch [3269/20000], Loss: 32.27366638183594, Learning Rate: 0.01\n",
      "Epoch [3270/20000], Loss: 33.87152099609375, Learning Rate: 0.01\n",
      "Epoch [3271/20000], Loss: 36.05938720703125, Learning Rate: 0.01\n",
      "Epoch [3272/20000], Loss: 38.55259704589844, Learning Rate: 0.01\n",
      "Epoch [3273/20000], Loss: 40.50215148925781, Learning Rate: 0.01\n",
      "Epoch [3274/20000], Loss: 40.62696838378906, Learning Rate: 0.01\n",
      "Epoch [3275/20000], Loss: 38.1500244140625, Learning Rate: 0.01\n",
      "Epoch [3276/20000], Loss: 33.97273254394531, Learning Rate: 0.01\n",
      "Epoch [3277/20000], Loss: 30.442291259765625, Learning Rate: 0.01\n",
      "Epoch [3278/20000], Loss: 29.4630126953125, Learning Rate: 0.01\n",
      "Epoch [3279/20000], Loss: 30.956298828125, Learning Rate: 0.01\n",
      "Epoch [3280/20000], Loss: 33.175537109375, Learning Rate: 0.01\n",
      "Epoch [3281/20000], Loss: 34.1585693359375, Learning Rate: 0.01\n",
      "Epoch [3282/20000], Loss: 33.12713623046875, Learning Rate: 0.01\n",
      "Epoch [3283/20000], Loss: 31.0018310546875, Learning Rate: 0.01\n",
      "Epoch [3284/20000], Loss: 29.502655029296875, Learning Rate: 0.01\n",
      "Epoch [3285/20000], Loss: 29.5784912109375, Learning Rate: 0.01\n",
      "Epoch [3286/20000], Loss: 30.716812133789062, Learning Rate: 0.01\n",
      "Epoch [3287/20000], Loss: 31.631881713867188, Learning Rate: 0.01\n",
      "Epoch [3288/20000], Loss: 31.462554931640625, Learning Rate: 0.01\n",
      "Epoch [3289/20000], Loss: 30.425796508789062, Learning Rate: 0.01\n",
      "Epoch [3290/20000], Loss: 29.467620849609375, Learning Rate: 0.01\n",
      "Epoch [3291/20000], Loss: 29.308975219726562, Learning Rate: 0.01\n",
      "Epoch [3292/20000], Loss: 29.84716796875, Learning Rate: 0.01\n",
      "Epoch [3293/20000], Loss: 30.4013671875, Learning Rate: 0.01\n",
      "Epoch [3294/20000], Loss: 30.410446166992188, Learning Rate: 0.01\n",
      "Epoch [3295/20000], Loss: 29.897994995117188, Learning Rate: 0.01\n",
      "Epoch [3296/20000], Loss: 29.347305297851562, Learning Rate: 0.01\n",
      "Epoch [3297/20000], Loss: 29.191329956054688, Learning Rate: 0.01\n",
      "Epoch [3298/20000], Loss: 29.438674926757812, Learning Rate: 0.01\n",
      "Epoch [3299/20000], Loss: 29.7486572265625, Learning Rate: 0.01\n",
      "Epoch [3300/20000], Loss: 29.794967651367188, Learning Rate: 0.01\n",
      "Epoch [3301/20000], Loss: 29.545059204101562, Learning Rate: 0.01\n",
      "Epoch [3302/20000], Loss: 29.233261108398438, Learning Rate: 0.01\n",
      "Epoch [3303/20000], Loss: 29.105209350585938, Learning Rate: 0.01\n",
      "Epoch [3304/20000], Loss: 29.2032470703125, Learning Rate: 0.01\n",
      "Epoch [3305/20000], Loss: 29.3697509765625, Learning Rate: 0.01\n",
      "Epoch [3306/20000], Loss: 29.421279907226562, Learning Rate: 0.01\n",
      "Epoch [3307/20000], Loss: 29.309036254882812, Learning Rate: 0.01\n",
      "Epoch [3308/20000], Loss: 29.133895874023438, Learning Rate: 0.01\n",
      "Epoch [3309/20000], Loss: 29.031539916992188, Learning Rate: 0.01\n",
      "Epoch [3310/20000], Loss: 29.051528930664062, Learning Rate: 0.01\n",
      "Epoch [3311/20000], Loss: 29.133056640625, Learning Rate: 0.01\n",
      "Epoch [3312/20000], Loss: 29.177093505859375, Learning Rate: 0.01\n",
      "Epoch [3313/20000], Loss: 29.1361083984375, Learning Rate: 0.01\n",
      "Epoch [3314/20000], Loss: 29.041717529296875, Learning Rate: 0.01\n",
      "Epoch [3315/20000], Loss: 28.963729858398438, Learning Rate: 0.01\n",
      "Epoch [3316/20000], Loss: 28.9456787109375, Learning Rate: 0.01\n",
      "Epoch [3317/20000], Loss: 28.975067138671875, Learning Rate: 0.01\n",
      "Epoch [3318/20000], Loss: 29.004562377929688, Learning Rate: 0.01\n",
      "Epoch [3319/20000], Loss: 28.99639892578125, Learning Rate: 0.01\n",
      "Epoch [3320/20000], Loss: 28.950546264648438, Learning Rate: 0.01\n",
      "Epoch [3321/20000], Loss: 28.89630126953125, Learning Rate: 0.01\n",
      "Epoch [3322/20000], Loss: 28.864410400390625, Learning Rate: 0.01\n",
      "Epoch [3323/20000], Loss: 28.862548828125, Learning Rate: 0.01\n",
      "Epoch [3324/20000], Loss: 28.874069213867188, Learning Rate: 0.01\n",
      "Epoch [3325/20000], Loss: 28.876144409179688, Learning Rate: 0.01\n",
      "Epoch [3326/20000], Loss: 28.857269287109375, Learning Rate: 0.01\n",
      "Epoch [3327/20000], Loss: 28.824462890625, Learning Rate: 0.01\n",
      "Epoch [3328/20000], Loss: 28.793655395507812, Learning Rate: 0.01\n",
      "Epoch [3329/20000], Loss: 28.776046752929688, Learning Rate: 0.01\n",
      "Epoch [3330/20000], Loss: 28.77154541015625, Learning Rate: 0.01\n",
      "Epoch [3331/20000], Loss: 28.770675659179688, Learning Rate: 0.01\n",
      "Epoch [3332/20000], Loss: 28.763137817382812, Learning Rate: 0.01\n",
      "Epoch [3333/20000], Loss: 28.7457275390625, Learning Rate: 0.01\n",
      "Epoch [3334/20000], Loss: 28.7227783203125, Learning Rate: 0.01\n",
      "Epoch [3335/20000], Loss: 28.70196533203125, Learning Rate: 0.01\n",
      "Epoch [3336/20000], Loss: 28.687591552734375, Learning Rate: 0.01\n",
      "Epoch [3337/20000], Loss: 28.67913818359375, Learning Rate: 0.01\n",
      "Epoch [3338/20000], Loss: 28.672042846679688, Learning Rate: 0.01\n",
      "Epoch [3339/20000], Loss: 28.661849975585938, Learning Rate: 0.01\n",
      "Epoch [3340/20000], Loss: 28.647186279296875, Learning Rate: 0.01\n",
      "Epoch [3341/20000], Loss: 28.629730224609375, Learning Rate: 0.01\n",
      "Epoch [3342/20000], Loss: 28.612808227539062, Learning Rate: 0.01\n",
      "Epoch [3343/20000], Loss: 28.598846435546875, Learning Rate: 0.01\n",
      "Epoch [3344/20000], Loss: 28.5877685546875, Learning Rate: 0.01\n",
      "Epoch [3345/20000], Loss: 28.577926635742188, Learning Rate: 0.01\n",
      "Epoch [3346/20000], Loss: 28.567047119140625, Learning Rate: 0.01\n",
      "Epoch [3347/20000], Loss: 28.5538330078125, Learning Rate: 0.01\n",
      "Epoch [3348/20000], Loss: 28.539276123046875, Learning Rate: 0.01\n",
      "Epoch [3349/20000], Loss: 28.524398803710938, Learning Rate: 0.01\n",
      "Epoch [3350/20000], Loss: 28.510711669921875, Learning Rate: 0.01\n",
      "Epoch [3351/20000], Loss: 28.498077392578125, Learning Rate: 0.01\n",
      "Epoch [3352/20000], Loss: 28.48651123046875, Learning Rate: 0.01\n",
      "Epoch [3353/20000], Loss: 28.475128173828125, Learning Rate: 0.01\n",
      "Epoch [3354/20000], Loss: 28.462799072265625, Learning Rate: 0.01\n",
      "Epoch [3355/20000], Loss: 28.449722290039062, Learning Rate: 0.01\n",
      "Epoch [3356/20000], Loss: 28.4361572265625, Learning Rate: 0.01\n",
      "Epoch [3357/20000], Loss: 28.422561645507812, Learning Rate: 0.01\n",
      "Epoch [3358/20000], Loss: 28.40948486328125, Learning Rate: 0.01\n",
      "Epoch [3359/20000], Loss: 28.397109985351562, Learning Rate: 0.01\n",
      "Epoch [3360/20000], Loss: 28.385025024414062, Learning Rate: 0.01\n",
      "Epoch [3361/20000], Loss: 28.372833251953125, Learning Rate: 0.01\n",
      "Epoch [3362/20000], Loss: 28.360397338867188, Learning Rate: 0.01\n",
      "Epoch [3363/20000], Loss: 28.347640991210938, Learning Rate: 0.01\n",
      "Epoch [3364/20000], Loss: 28.33453369140625, Learning Rate: 0.01\n",
      "Epoch [3365/20000], Loss: 28.321548461914062, Learning Rate: 0.01\n",
      "Epoch [3366/20000], Loss: 28.308761596679688, Learning Rate: 0.01\n",
      "Epoch [3367/20000], Loss: 28.296096801757812, Learning Rate: 0.01\n",
      "Epoch [3368/20000], Loss: 28.28369140625, Learning Rate: 0.01\n",
      "Epoch [3369/20000], Loss: 28.271331787109375, Learning Rate: 0.01\n",
      "Epoch [3370/20000], Loss: 28.258773803710938, Learning Rate: 0.01\n",
      "Epoch [3371/20000], Loss: 28.246170043945312, Learning Rate: 0.01\n",
      "Epoch [3372/20000], Loss: 28.23345947265625, Learning Rate: 0.01\n",
      "Epoch [3373/20000], Loss: 28.22064208984375, Learning Rate: 0.01\n",
      "Epoch [3374/20000], Loss: 28.20794677734375, Learning Rate: 0.01\n",
      "Epoch [3375/20000], Loss: 28.195343017578125, Learning Rate: 0.01\n",
      "Epoch [3376/20000], Loss: 28.182708740234375, Learning Rate: 0.01\n",
      "Epoch [3377/20000], Loss: 28.170196533203125, Learning Rate: 0.01\n",
      "Epoch [3378/20000], Loss: 28.157745361328125, Learning Rate: 0.01\n",
      "Epoch [3379/20000], Loss: 28.14520263671875, Learning Rate: 0.01\n",
      "Epoch [3380/20000], Loss: 28.132568359375, Learning Rate: 0.01\n",
      "Epoch [3381/20000], Loss: 28.119873046875, Learning Rate: 0.01\n",
      "Epoch [3382/20000], Loss: 28.107223510742188, Learning Rate: 0.01\n",
      "Epoch [3383/20000], Loss: 28.094512939453125, Learning Rate: 0.01\n",
      "Epoch [3384/20000], Loss: 28.081939697265625, Learning Rate: 0.01\n",
      "Epoch [3385/20000], Loss: 28.069366455078125, Learning Rate: 0.01\n",
      "Epoch [3386/20000], Loss: 28.0567626953125, Learning Rate: 0.01\n",
      "Epoch [3387/20000], Loss: 28.0443115234375, Learning Rate: 0.01\n",
      "Epoch [3388/20000], Loss: 28.0316162109375, Learning Rate: 0.01\n",
      "Epoch [3389/20000], Loss: 28.019119262695312, Learning Rate: 0.01\n",
      "Epoch [3390/20000], Loss: 28.00653076171875, Learning Rate: 0.01\n",
      "Epoch [3391/20000], Loss: 27.993911743164062, Learning Rate: 0.01\n",
      "Epoch [3392/20000], Loss: 27.98138427734375, Learning Rate: 0.01\n",
      "Epoch [3393/20000], Loss: 27.968841552734375, Learning Rate: 0.01\n",
      "Epoch [3394/20000], Loss: 27.956192016601562, Learning Rate: 0.01\n",
      "Epoch [3395/20000], Loss: 27.94354248046875, Learning Rate: 0.01\n",
      "Epoch [3396/20000], Loss: 27.930999755859375, Learning Rate: 0.01\n",
      "Epoch [3397/20000], Loss: 27.918441772460938, Learning Rate: 0.01\n",
      "Epoch [3398/20000], Loss: 27.905853271484375, Learning Rate: 0.01\n",
      "Epoch [3399/20000], Loss: 27.893325805664062, Learning Rate: 0.01\n",
      "Epoch [3400/20000], Loss: 27.880645751953125, Learning Rate: 0.01\n",
      "Epoch [3401/20000], Loss: 27.868194580078125, Learning Rate: 0.01\n",
      "Epoch [3402/20000], Loss: 27.855697631835938, Learning Rate: 0.01\n",
      "Epoch [3403/20000], Loss: 27.843002319335938, Learning Rate: 0.01\n",
      "Epoch [3404/20000], Loss: 27.830551147460938, Learning Rate: 0.01\n",
      "Epoch [3405/20000], Loss: 27.817947387695312, Learning Rate: 0.01\n",
      "Epoch [3406/20000], Loss: 27.805282592773438, Learning Rate: 0.01\n",
      "Epoch [3407/20000], Loss: 27.79278564453125, Learning Rate: 0.01\n",
      "Epoch [3408/20000], Loss: 27.780166625976562, Learning Rate: 0.01\n",
      "Epoch [3409/20000], Loss: 27.76763916015625, Learning Rate: 0.01\n",
      "Epoch [3410/20000], Loss: 27.755081176757812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3411/20000], Loss: 27.742507934570312, Learning Rate: 0.01\n",
      "Epoch [3412/20000], Loss: 27.729934692382812, Learning Rate: 0.01\n",
      "Epoch [3413/20000], Loss: 27.717483520507812, Learning Rate: 0.01\n",
      "Epoch [3414/20000], Loss: 27.704971313476562, Learning Rate: 0.01\n",
      "Epoch [3415/20000], Loss: 27.692306518554688, Learning Rate: 0.01\n",
      "Epoch [3416/20000], Loss: 27.679794311523438, Learning Rate: 0.01\n",
      "Epoch [3417/20000], Loss: 27.667190551757812, Learning Rate: 0.01\n",
      "Epoch [3418/20000], Loss: 27.65472412109375, Learning Rate: 0.01\n",
      "Epoch [3419/20000], Loss: 27.642120361328125, Learning Rate: 0.01\n",
      "Epoch [3420/20000], Loss: 27.629592895507812, Learning Rate: 0.01\n",
      "Epoch [3421/20000], Loss: 27.616928100585938, Learning Rate: 0.01\n",
      "Epoch [3422/20000], Loss: 27.604400634765625, Learning Rate: 0.01\n",
      "Epoch [3423/20000], Loss: 27.5919189453125, Learning Rate: 0.01\n",
      "Epoch [3424/20000], Loss: 27.579330444335938, Learning Rate: 0.01\n",
      "Epoch [3425/20000], Loss: 27.566741943359375, Learning Rate: 0.01\n",
      "Epoch [3426/20000], Loss: 27.554183959960938, Learning Rate: 0.01\n",
      "Epoch [3427/20000], Loss: 27.54168701171875, Learning Rate: 0.01\n",
      "Epoch [3428/20000], Loss: 27.52911376953125, Learning Rate: 0.01\n",
      "Epoch [3429/20000], Loss: 27.516616821289062, Learning Rate: 0.01\n",
      "Epoch [3430/20000], Loss: 27.50408935546875, Learning Rate: 0.01\n",
      "Epoch [3431/20000], Loss: 27.49151611328125, Learning Rate: 0.01\n",
      "Epoch [3432/20000], Loss: 27.479019165039062, Learning Rate: 0.01\n",
      "Epoch [3433/20000], Loss: 27.46636962890625, Learning Rate: 0.01\n",
      "Epoch [3434/20000], Loss: 27.453994750976562, Learning Rate: 0.01\n",
      "Epoch [3435/20000], Loss: 27.44140625, Learning Rate: 0.01\n",
      "Epoch [3436/20000], Loss: 27.428802490234375, Learning Rate: 0.01\n",
      "Epoch [3437/20000], Loss: 27.416290283203125, Learning Rate: 0.01\n",
      "Epoch [3438/20000], Loss: 27.403778076171875, Learning Rate: 0.01\n",
      "Epoch [3439/20000], Loss: 27.391326904296875, Learning Rate: 0.01\n",
      "Epoch [3440/20000], Loss: 27.378768920898438, Learning Rate: 0.01\n",
      "Epoch [3441/20000], Loss: 27.366165161132812, Learning Rate: 0.01\n",
      "Epoch [3442/20000], Loss: 27.353622436523438, Learning Rate: 0.01\n",
      "Epoch [3443/20000], Loss: 27.34112548828125, Learning Rate: 0.01\n",
      "Epoch [3444/20000], Loss: 27.328643798828125, Learning Rate: 0.01\n",
      "Epoch [3445/20000], Loss: 27.316055297851562, Learning Rate: 0.01\n",
      "Epoch [3446/20000], Loss: 27.303436279296875, Learning Rate: 0.01\n",
      "Epoch [3447/20000], Loss: 27.290969848632812, Learning Rate: 0.01\n",
      "Epoch [3448/20000], Loss: 27.278549194335938, Learning Rate: 0.01\n",
      "Epoch [3449/20000], Loss: 27.26593017578125, Learning Rate: 0.01\n",
      "Epoch [3450/20000], Loss: 27.25341796875, Learning Rate: 0.01\n",
      "Epoch [3451/20000], Loss: 27.24090576171875, Learning Rate: 0.01\n",
      "Epoch [3452/20000], Loss: 27.22833251953125, Learning Rate: 0.01\n",
      "Epoch [3453/20000], Loss: 27.2158203125, Learning Rate: 0.01\n",
      "Epoch [3454/20000], Loss: 27.203262329101562, Learning Rate: 0.01\n",
      "Epoch [3455/20000], Loss: 27.190872192382812, Learning Rate: 0.01\n",
      "Epoch [3456/20000], Loss: 27.178329467773438, Learning Rate: 0.01\n",
      "Epoch [3457/20000], Loss: 27.165847778320312, Learning Rate: 0.01\n",
      "Epoch [3458/20000], Loss: 27.153289794921875, Learning Rate: 0.01\n",
      "Epoch [3459/20000], Loss: 27.140899658203125, Learning Rate: 0.01\n",
      "Epoch [3460/20000], Loss: 27.128387451171875, Learning Rate: 0.01\n",
      "Epoch [3461/20000], Loss: 27.115829467773438, Learning Rate: 0.01\n",
      "Epoch [3462/20000], Loss: 27.103439331054688, Learning Rate: 0.01\n",
      "Epoch [3463/20000], Loss: 27.091110229492188, Learning Rate: 0.01\n",
      "Epoch [3464/20000], Loss: 27.078765869140625, Learning Rate: 0.01\n",
      "Epoch [3465/20000], Loss: 27.066360473632812, Learning Rate: 0.01\n",
      "Epoch [3466/20000], Loss: 27.054306030273438, Learning Rate: 0.01\n",
      "Epoch [3467/20000], Loss: 27.042327880859375, Learning Rate: 0.01\n",
      "Epoch [3468/20000], Loss: 27.0306396484375, Learning Rate: 0.01\n",
      "Epoch [3469/20000], Loss: 27.01947021484375, Learning Rate: 0.01\n",
      "Epoch [3470/20000], Loss: 27.009033203125, Learning Rate: 0.01\n",
      "Epoch [3471/20000], Loss: 26.999710083007812, Learning Rate: 0.01\n",
      "Epoch [3472/20000], Loss: 26.992111206054688, Learning Rate: 0.01\n",
      "Epoch [3473/20000], Loss: 26.987335205078125, Learning Rate: 0.01\n",
      "Epoch [3474/20000], Loss: 26.987457275390625, Learning Rate: 0.01\n",
      "Epoch [3475/20000], Loss: 26.994979858398438, Learning Rate: 0.01\n",
      "Epoch [3476/20000], Loss: 27.014663696289062, Learning Rate: 0.01\n",
      "Epoch [3477/20000], Loss: 27.0546875, Learning Rate: 0.01\n",
      "Epoch [3478/20000], Loss: 27.127883911132812, Learning Rate: 0.01\n",
      "Epoch [3479/20000], Loss: 27.255752563476562, Learning Rate: 0.01\n",
      "Epoch [3480/20000], Loss: 27.473648071289062, Learning Rate: 0.01\n",
      "Epoch [3481/20000], Loss: 27.838882446289062, Learning Rate: 0.01\n",
      "Epoch [3482/20000], Loss: 28.441848754882812, Learning Rate: 0.01\n",
      "Epoch [3483/20000], Loss: 29.41851806640625, Learning Rate: 0.01\n",
      "Epoch [3484/20000], Loss: 30.950241088867188, Learning Rate: 0.01\n",
      "Epoch [3485/20000], Loss: 33.23101806640625, Learning Rate: 0.01\n",
      "Epoch [3486/20000], Loss: 36.32366943359375, Learning Rate: 0.01\n",
      "Epoch [3487/20000], Loss: 39.879608154296875, Learning Rate: 0.01\n",
      "Epoch [3488/20000], Loss: 42.74848937988281, Learning Rate: 0.01\n",
      "Epoch [3489/20000], Loss: 43.12249755859375, Learning Rate: 0.01\n",
      "Epoch [3490/20000], Loss: 39.7022705078125, Learning Rate: 0.01\n",
      "Epoch [3491/20000], Loss: 33.6219482421875, Learning Rate: 0.01\n",
      "Epoch [3492/20000], Loss: 28.315109252929688, Learning Rate: 0.01\n",
      "Epoch [3493/20000], Loss: 26.763824462890625, Learning Rate: 0.01\n",
      "Epoch [3494/20000], Loss: 28.953445434570312, Learning Rate: 0.01\n",
      "Epoch [3495/20000], Loss: 32.24446105957031, Learning Rate: 0.01\n",
      "Epoch [3496/20000], Loss: 33.633819580078125, Learning Rate: 0.01\n",
      "Epoch [3497/20000], Loss: 31.972549438476562, Learning Rate: 0.01\n",
      "Epoch [3498/20000], Loss: 28.79229736328125, Learning Rate: 0.01\n",
      "Epoch [3499/20000], Loss: 26.766448974609375, Learning Rate: 0.01\n",
      "Epoch [3500/20000], Loss: 27.177978515625, Learning Rate: 0.01\n",
      "Epoch [3501/20000], Loss: 28.982162475585938, Learning Rate: 0.01\n",
      "Epoch [3502/20000], Loss: 30.12890625, Learning Rate: 0.01\n",
      "Epoch [3503/20000], Loss: 29.519882202148438, Learning Rate: 0.01\n",
      "Epoch [3504/20000], Loss: 27.837371826171875, Learning Rate: 0.01\n",
      "Epoch [3505/20000], Loss: 26.660186767578125, Learning Rate: 0.01\n",
      "Epoch [3506/20000], Loss: 26.833892822265625, Learning Rate: 0.01\n",
      "Epoch [3507/20000], Loss: 27.811614990234375, Learning Rate: 0.01\n",
      "Epoch [3508/20000], Loss: 28.419448852539062, Learning Rate: 0.01\n",
      "Epoch [3509/20000], Loss: 28.050491333007812, Learning Rate: 0.01\n",
      "Epoch [3510/20000], Loss: 27.12890625, Learning Rate: 0.01\n",
      "Epoch [3511/20000], Loss: 26.529891967773438, Learning Rate: 0.01\n",
      "Epoch [3512/20000], Loss: 26.670745849609375, Learning Rate: 0.01\n",
      "Epoch [3513/20000], Loss: 27.203964233398438, Learning Rate: 0.01\n",
      "Epoch [3514/20000], Loss: 27.488784790039062, Learning Rate: 0.01\n",
      "Epoch [3515/20000], Loss: 27.242691040039062, Learning Rate: 0.01\n",
      "Epoch [3516/20000], Loss: 26.735977172851562, Learning Rate: 0.01\n",
      "Epoch [3517/20000], Loss: 26.436737060546875, Learning Rate: 0.01\n",
      "Epoch [3518/20000], Loss: 26.534408569335938, Learning Rate: 0.01\n",
      "Epoch [3519/20000], Loss: 26.81781005859375, Learning Rate: 0.01\n",
      "Epoch [3520/20000], Loss: 26.949188232421875, Learning Rate: 0.01\n",
      "Epoch [3521/20000], Loss: 26.797439575195312, Learning Rate: 0.01\n",
      "Epoch [3522/20000], Loss: 26.519805908203125, Learning Rate: 0.01\n",
      "Epoch [3523/20000], Loss: 26.360321044921875, Learning Rate: 0.01\n",
      "Epoch [3524/20000], Loss: 26.410919189453125, Learning Rate: 0.01\n",
      "Epoch [3525/20000], Loss: 26.556854248046875, Learning Rate: 0.01\n",
      "Epoch [3526/20000], Loss: 26.6220703125, Learning Rate: 0.01\n",
      "Epoch [3527/20000], Loss: 26.538055419921875, Learning Rate: 0.01\n",
      "Epoch [3528/20000], Loss: 26.384689331054688, Learning Rate: 0.01\n",
      "Epoch [3529/20000], Loss: 26.288772583007812, Learning Rate: 0.01\n",
      "Epoch [3530/20000], Loss: 26.303314208984375, Learning Rate: 0.01\n",
      "Epoch [3531/20000], Loss: 26.374481201171875, Learning Rate: 0.01\n",
      "Epoch [3532/20000], Loss: 26.41058349609375, Learning Rate: 0.01\n",
      "Epoch [3533/20000], Loss: 26.369110107421875, Learning Rate: 0.01\n",
      "Epoch [3534/20000], Loss: 26.283554077148438, Learning Rate: 0.01\n",
      "Epoch [3535/20000], Loss: 26.219253540039062, Learning Rate: 0.01\n",
      "Epoch [3536/20000], Loss: 26.211532592773438, Learning Rate: 0.01\n",
      "Epoch [3537/20000], Loss: 26.240951538085938, Learning Rate: 0.01\n",
      "Epoch [3538/20000], Loss: 26.26141357421875, Learning Rate: 0.01\n",
      "Epoch [3539/20000], Loss: 26.243453979492188, Learning Rate: 0.01\n",
      "Epoch [3540/20000], Loss: 26.195953369140625, Learning Rate: 0.01\n",
      "Epoch [3541/20000], Loss: 26.1507568359375, Learning Rate: 0.01\n",
      "Epoch [3542/20000], Loss: 26.1318359375, Learning Rate: 0.01\n",
      "Epoch [3543/20000], Loss: 26.1373291015625, Learning Rate: 0.01\n",
      "Epoch [3544/20000], Loss: 26.146194458007812, Learning Rate: 0.01\n",
      "Epoch [3545/20000], Loss: 26.139068603515625, Learning Rate: 0.01\n",
      "Epoch [3546/20000], Loss: 26.113174438476562, Learning Rate: 0.01\n",
      "Epoch [3547/20000], Loss: 26.08184814453125, Learning Rate: 0.01\n",
      "Epoch [3548/20000], Loss: 26.05963134765625, Learning Rate: 0.01\n",
      "Epoch [3549/20000], Loss: 26.051589965820312, Learning Rate: 0.01\n",
      "Epoch [3550/20000], Loss: 26.050827026367188, Learning Rate: 0.01\n",
      "Epoch [3551/20000], Loss: 26.046340942382812, Learning Rate: 0.01\n",
      "Epoch [3552/20000], Loss: 26.0322265625, Learning Rate: 0.01\n",
      "Epoch [3553/20000], Loss: 26.011077880859375, Learning Rate: 0.01\n",
      "Epoch [3554/20000], Loss: 25.990615844726562, Learning Rate: 0.01\n",
      "Epoch [3555/20000], Loss: 25.97601318359375, Learning Rate: 0.01\n",
      "Epoch [3556/20000], Loss: 25.968002319335938, Learning Rate: 0.01\n",
      "Epoch [3557/20000], Loss: 25.961639404296875, Learning Rate: 0.01\n",
      "Epoch [3558/20000], Loss: 25.952041625976562, Learning Rate: 0.01\n",
      "Epoch [3559/20000], Loss: 25.938003540039062, Learning Rate: 0.01\n",
      "Epoch [3560/20000], Loss: 25.921218872070312, Learning Rate: 0.01\n",
      "Epoch [3561/20000], Loss: 25.905487060546875, Learning Rate: 0.01\n",
      "Epoch [3562/20000], Loss: 25.8929443359375, Learning Rate: 0.01\n",
      "Epoch [3563/20000], Loss: 25.883193969726562, Learning Rate: 0.01\n",
      "Epoch [3564/20000], Loss: 25.873794555664062, Learning Rate: 0.01\n",
      "Epoch [3565/20000], Loss: 25.86279296875, Learning Rate: 0.01\n",
      "Epoch [3566/20000], Loss: 25.849899291992188, Learning Rate: 0.01\n",
      "Epoch [3567/20000], Loss: 25.835601806640625, Learning Rate: 0.01\n",
      "Epoch [3568/20000], Loss: 25.821884155273438, Learning Rate: 0.01\n",
      "Epoch [3569/20000], Loss: 25.809494018554688, Learning Rate: 0.01\n",
      "Epoch [3570/20000], Loss: 25.798599243164062, Learning Rate: 0.01\n",
      "Epoch [3571/20000], Loss: 25.787933349609375, Learning Rate: 0.01\n",
      "Epoch [3572/20000], Loss: 25.7767333984375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3573/20000], Loss: 25.764389038085938, Learning Rate: 0.01\n",
      "Epoch [3574/20000], Loss: 25.7515869140625, Learning Rate: 0.01\n",
      "Epoch [3575/20000], Loss: 25.738616943359375, Learning Rate: 0.01\n",
      "Epoch [3576/20000], Loss: 25.726333618164062, Learning Rate: 0.01\n",
      "Epoch [3577/20000], Loss: 25.714706420898438, Learning Rate: 0.01\n",
      "Epoch [3578/20000], Loss: 25.703338623046875, Learning Rate: 0.01\n",
      "Epoch [3579/20000], Loss: 25.69183349609375, Learning Rate: 0.01\n",
      "Epoch [3580/20000], Loss: 25.680099487304688, Learning Rate: 0.01\n",
      "Epoch [3581/20000], Loss: 25.66778564453125, Learning Rate: 0.01\n",
      "Epoch [3582/20000], Loss: 25.655548095703125, Learning Rate: 0.01\n",
      "Epoch [3583/20000], Loss: 25.6431884765625, Learning Rate: 0.01\n",
      "Epoch [3584/20000], Loss: 25.631134033203125, Learning Rate: 0.01\n",
      "Epoch [3585/20000], Loss: 25.619537353515625, Learning Rate: 0.01\n",
      "Epoch [3586/20000], Loss: 25.60797119140625, Learning Rate: 0.01\n",
      "Epoch [3587/20000], Loss: 25.596038818359375, Learning Rate: 0.01\n",
      "Epoch [3588/20000], Loss: 25.584243774414062, Learning Rate: 0.01\n",
      "Epoch [3589/20000], Loss: 25.572097778320312, Learning Rate: 0.01\n",
      "Epoch [3590/20000], Loss: 25.560043334960938, Learning Rate: 0.01\n",
      "Epoch [3591/20000], Loss: 25.547958374023438, Learning Rate: 0.01\n",
      "Epoch [3592/20000], Loss: 25.536163330078125, Learning Rate: 0.01\n",
      "Epoch [3593/20000], Loss: 25.524307250976562, Learning Rate: 0.01\n",
      "Epoch [3594/20000], Loss: 25.512588500976562, Learning Rate: 0.01\n",
      "Epoch [3595/20000], Loss: 25.50067138671875, Learning Rate: 0.01\n",
      "Epoch [3596/20000], Loss: 25.488876342773438, Learning Rate: 0.01\n",
      "Epoch [3597/20000], Loss: 25.477005004882812, Learning Rate: 0.01\n",
      "Epoch [3598/20000], Loss: 25.464950561523438, Learning Rate: 0.01\n",
      "Epoch [3599/20000], Loss: 25.452880859375, Learning Rate: 0.01\n",
      "Epoch [3600/20000], Loss: 25.441070556640625, Learning Rate: 0.01\n",
      "Epoch [3601/20000], Loss: 25.429214477539062, Learning Rate: 0.01\n",
      "Epoch [3602/20000], Loss: 25.4173583984375, Learning Rate: 0.01\n",
      "Epoch [3603/20000], Loss: 25.405487060546875, Learning Rate: 0.01\n",
      "Epoch [3604/20000], Loss: 25.393646240234375, Learning Rate: 0.01\n",
      "Epoch [3605/20000], Loss: 25.38177490234375, Learning Rate: 0.01\n",
      "Epoch [3606/20000], Loss: 25.369827270507812, Learning Rate: 0.01\n",
      "Epoch [3607/20000], Loss: 25.35797119140625, Learning Rate: 0.01\n",
      "Epoch [3608/20000], Loss: 25.345993041992188, Learning Rate: 0.01\n",
      "Epoch [3609/20000], Loss: 25.33416748046875, Learning Rate: 0.01\n",
      "Epoch [3610/20000], Loss: 25.322311401367188, Learning Rate: 0.01\n",
      "Epoch [3611/20000], Loss: 25.31036376953125, Learning Rate: 0.01\n",
      "Epoch [3612/20000], Loss: 25.298614501953125, Learning Rate: 0.01\n",
      "Epoch [3613/20000], Loss: 25.286651611328125, Learning Rate: 0.01\n",
      "Epoch [3614/20000], Loss: 25.27484130859375, Learning Rate: 0.01\n",
      "Epoch [3615/20000], Loss: 25.262924194335938, Learning Rate: 0.01\n",
      "Epoch [3616/20000], Loss: 25.251068115234375, Learning Rate: 0.01\n",
      "Epoch [3617/20000], Loss: 25.23919677734375, Learning Rate: 0.01\n",
      "Epoch [3618/20000], Loss: 25.227218627929688, Learning Rate: 0.01\n",
      "Epoch [3619/20000], Loss: 25.215301513671875, Learning Rate: 0.01\n",
      "Epoch [3620/20000], Loss: 25.203521728515625, Learning Rate: 0.01\n",
      "Epoch [3621/20000], Loss: 25.191619873046875, Learning Rate: 0.01\n",
      "Epoch [3622/20000], Loss: 25.179779052734375, Learning Rate: 0.01\n",
      "Epoch [3623/20000], Loss: 25.167861938476562, Learning Rate: 0.01\n",
      "Epoch [3624/20000], Loss: 25.156051635742188, Learning Rate: 0.01\n",
      "Epoch [3625/20000], Loss: 25.144088745117188, Learning Rate: 0.01\n",
      "Epoch [3626/20000], Loss: 25.132308959960938, Learning Rate: 0.01\n",
      "Epoch [3627/20000], Loss: 25.120330810546875, Learning Rate: 0.01\n",
      "Epoch [3628/20000], Loss: 25.108474731445312, Learning Rate: 0.01\n",
      "Epoch [3629/20000], Loss: 25.096649169921875, Learning Rate: 0.01\n",
      "Epoch [3630/20000], Loss: 25.08465576171875, Learning Rate: 0.01\n",
      "Epoch [3631/20000], Loss: 25.072906494140625, Learning Rate: 0.01\n",
      "Epoch [3632/20000], Loss: 25.060989379882812, Learning Rate: 0.01\n",
      "Epoch [3633/20000], Loss: 25.049041748046875, Learning Rate: 0.01\n",
      "Epoch [3634/20000], Loss: 25.037277221679688, Learning Rate: 0.01\n",
      "Epoch [3635/20000], Loss: 25.025375366210938, Learning Rate: 0.01\n",
      "Epoch [3636/20000], Loss: 25.013504028320312, Learning Rate: 0.01\n",
      "Epoch [3637/20000], Loss: 25.001617431640625, Learning Rate: 0.01\n",
      "Epoch [3638/20000], Loss: 24.989776611328125, Learning Rate: 0.01\n",
      "Epoch [3639/20000], Loss: 24.977890014648438, Learning Rate: 0.01\n",
      "Epoch [3640/20000], Loss: 24.965896606445312, Learning Rate: 0.01\n",
      "Epoch [3641/20000], Loss: 24.9541015625, Learning Rate: 0.01\n",
      "Epoch [3642/20000], Loss: 24.942291259765625, Learning Rate: 0.01\n",
      "Epoch [3643/20000], Loss: 24.930435180664062, Learning Rate: 0.01\n",
      "Epoch [3644/20000], Loss: 24.918472290039062, Learning Rate: 0.01\n",
      "Epoch [3645/20000], Loss: 24.906631469726562, Learning Rate: 0.01\n",
      "Epoch [3646/20000], Loss: 24.894729614257812, Learning Rate: 0.01\n",
      "Epoch [3647/20000], Loss: 24.88287353515625, Learning Rate: 0.01\n",
      "Epoch [3648/20000], Loss: 24.871109008789062, Learning Rate: 0.01\n",
      "Epoch [3649/20000], Loss: 24.859176635742188, Learning Rate: 0.01\n",
      "Epoch [3650/20000], Loss: 24.847335815429688, Learning Rate: 0.01\n",
      "Epoch [3651/20000], Loss: 24.835464477539062, Learning Rate: 0.01\n",
      "Epoch [3652/20000], Loss: 24.823577880859375, Learning Rate: 0.01\n",
      "Epoch [3653/20000], Loss: 24.811737060546875, Learning Rate: 0.01\n",
      "Epoch [3654/20000], Loss: 24.799835205078125, Learning Rate: 0.01\n",
      "Epoch [3655/20000], Loss: 24.787933349609375, Learning Rate: 0.01\n",
      "Epoch [3656/20000], Loss: 24.776092529296875, Learning Rate: 0.01\n",
      "Epoch [3657/20000], Loss: 24.764251708984375, Learning Rate: 0.01\n",
      "Epoch [3658/20000], Loss: 24.752410888671875, Learning Rate: 0.01\n",
      "Epoch [3659/20000], Loss: 24.740509033203125, Learning Rate: 0.01\n",
      "Epoch [3660/20000], Loss: 24.728561401367188, Learning Rate: 0.01\n",
      "Epoch [3661/20000], Loss: 24.716690063476562, Learning Rate: 0.01\n",
      "Epoch [3662/20000], Loss: 24.704864501953125, Learning Rate: 0.01\n",
      "Epoch [3663/20000], Loss: 24.693069458007812, Learning Rate: 0.01\n",
      "Epoch [3664/20000], Loss: 24.681182861328125, Learning Rate: 0.01\n",
      "Epoch [3665/20000], Loss: 24.669189453125, Learning Rate: 0.01\n",
      "Epoch [3666/20000], Loss: 24.657379150390625, Learning Rate: 0.01\n",
      "Epoch [3667/20000], Loss: 24.645523071289062, Learning Rate: 0.01\n",
      "Epoch [3668/20000], Loss: 24.633529663085938, Learning Rate: 0.01\n",
      "Epoch [3669/20000], Loss: 24.621688842773438, Learning Rate: 0.01\n",
      "Epoch [3670/20000], Loss: 24.609817504882812, Learning Rate: 0.01\n",
      "Epoch [3671/20000], Loss: 24.5980224609375, Learning Rate: 0.01\n",
      "Epoch [3672/20000], Loss: 24.58612060546875, Learning Rate: 0.01\n",
      "Epoch [3673/20000], Loss: 24.574234008789062, Learning Rate: 0.01\n",
      "Epoch [3674/20000], Loss: 24.562408447265625, Learning Rate: 0.01\n",
      "Epoch [3675/20000], Loss: 24.550567626953125, Learning Rate: 0.01\n",
      "Epoch [3676/20000], Loss: 24.538681030273438, Learning Rate: 0.01\n",
      "Epoch [3677/20000], Loss: 24.526809692382812, Learning Rate: 0.01\n",
      "Epoch [3678/20000], Loss: 24.514907836914062, Learning Rate: 0.01\n",
      "Epoch [3679/20000], Loss: 24.50311279296875, Learning Rate: 0.01\n",
      "Epoch [3680/20000], Loss: 24.491180419921875, Learning Rate: 0.01\n",
      "Epoch [3681/20000], Loss: 24.47930908203125, Learning Rate: 0.01\n",
      "Epoch [3682/20000], Loss: 24.467544555664062, Learning Rate: 0.01\n",
      "Epoch [3683/20000], Loss: 24.455596923828125, Learning Rate: 0.01\n",
      "Epoch [3684/20000], Loss: 24.443817138671875, Learning Rate: 0.01\n",
      "Epoch [3685/20000], Loss: 24.431884765625, Learning Rate: 0.01\n",
      "Epoch [3686/20000], Loss: 24.41998291015625, Learning Rate: 0.01\n",
      "Epoch [3687/20000], Loss: 24.408126831054688, Learning Rate: 0.01\n",
      "Epoch [3688/20000], Loss: 24.396347045898438, Learning Rate: 0.01\n",
      "Epoch [3689/20000], Loss: 24.384429931640625, Learning Rate: 0.01\n",
      "Epoch [3690/20000], Loss: 24.372543334960938, Learning Rate: 0.01\n",
      "Epoch [3691/20000], Loss: 24.360702514648438, Learning Rate: 0.01\n",
      "Epoch [3692/20000], Loss: 24.348892211914062, Learning Rate: 0.01\n",
      "Epoch [3693/20000], Loss: 24.336959838867188, Learning Rate: 0.01\n",
      "Epoch [3694/20000], Loss: 24.325180053710938, Learning Rate: 0.01\n",
      "Epoch [3695/20000], Loss: 24.313232421875, Learning Rate: 0.01\n",
      "Epoch [3696/20000], Loss: 24.301437377929688, Learning Rate: 0.01\n",
      "Epoch [3697/20000], Loss: 24.289520263671875, Learning Rate: 0.01\n",
      "Epoch [3698/20000], Loss: 24.277664184570312, Learning Rate: 0.01\n",
      "Epoch [3699/20000], Loss: 24.265960693359375, Learning Rate: 0.01\n",
      "Epoch [3700/20000], Loss: 24.25390625, Learning Rate: 0.01\n",
      "Epoch [3701/20000], Loss: 24.242019653320312, Learning Rate: 0.01\n",
      "Epoch [3702/20000], Loss: 24.230361938476562, Learning Rate: 0.01\n",
      "Epoch [3703/20000], Loss: 24.218475341796875, Learning Rate: 0.01\n",
      "Epoch [3704/20000], Loss: 24.2066650390625, Learning Rate: 0.01\n",
      "Epoch [3705/20000], Loss: 24.194931030273438, Learning Rate: 0.01\n",
      "Epoch [3706/20000], Loss: 24.183074951171875, Learning Rate: 0.01\n",
      "Epoch [3707/20000], Loss: 24.171478271484375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3708/20000], Loss: 24.16009521484375, Learning Rate: 0.01\n",
      "Epoch [3709/20000], Loss: 24.148681640625, Learning Rate: 0.01\n",
      "Epoch [3710/20000], Loss: 24.1376953125, Learning Rate: 0.01\n",
      "Epoch [3711/20000], Loss: 24.127365112304688, Learning Rate: 0.01\n",
      "Epoch [3712/20000], Loss: 24.117996215820312, Learning Rate: 0.01\n",
      "Epoch [3713/20000], Loss: 24.110122680664062, Learning Rate: 0.01\n",
      "Epoch [3714/20000], Loss: 24.104888916015625, Learning Rate: 0.01\n",
      "Epoch [3715/20000], Loss: 24.104171752929688, Learning Rate: 0.01\n",
      "Epoch [3716/20000], Loss: 24.111160278320312, Learning Rate: 0.01\n",
      "Epoch [3717/20000], Loss: 24.13079833984375, Learning Rate: 0.01\n",
      "Epoch [3718/20000], Loss: 24.172592163085938, Learning Rate: 0.01\n",
      "Epoch [3719/20000], Loss: 24.252273559570312, Learning Rate: 0.01\n",
      "Epoch [3720/20000], Loss: 24.39727783203125, Learning Rate: 0.01\n",
      "Epoch [3721/20000], Loss: 24.654754638671875, Learning Rate: 0.01\n",
      "Epoch [3722/20000], Loss: 25.105377197265625, Learning Rate: 0.01\n",
      "Epoch [3723/20000], Loss: 25.88421630859375, Learning Rate: 0.01\n",
      "Epoch [3724/20000], Loss: 27.206451416015625, Learning Rate: 0.01\n",
      "Epoch [3725/20000], Loss: 29.391891479492188, Learning Rate: 0.01\n",
      "Epoch [3726/20000], Loss: 32.81889343261719, Learning Rate: 0.01\n",
      "Epoch [3727/20000], Loss: 37.73158264160156, Learning Rate: 0.01\n",
      "Epoch [3728/20000], Loss: 43.6265869140625, Learning Rate: 0.01\n",
      "Epoch [3729/20000], Loss: 48.50434875488281, Learning Rate: 0.01\n",
      "Epoch [3730/20000], Loss: 48.76806640625, Learning Rate: 0.01\n",
      "Epoch [3731/20000], Loss: 42.20689392089844, Learning Rate: 0.01\n",
      "Epoch [3732/20000], Loss: 31.754196166992188, Learning Rate: 0.01\n",
      "Epoch [3733/20000], Loss: 24.597213745117188, Learning Rate: 0.01\n",
      "Epoch [3734/20000], Loss: 25.0589599609375, Learning Rate: 0.01\n",
      "Epoch [3735/20000], Loss: 30.602127075195312, Learning Rate: 0.01\n",
      "Epoch [3736/20000], Loss: 34.85533142089844, Learning Rate: 0.01\n",
      "Epoch [3737/20000], Loss: 33.45367431640625, Learning Rate: 0.01\n",
      "Epoch [3738/20000], Loss: 27.990341186523438, Learning Rate: 0.01\n",
      "Epoch [3739/20000], Loss: 24.041305541992188, Learning Rate: 0.01\n",
      "Epoch [3740/20000], Loss: 24.873077392578125, Learning Rate: 0.01\n",
      "Epoch [3741/20000], Loss: 28.276611328125, Learning Rate: 0.01\n",
      "Epoch [3742/20000], Loss: 29.787734985351562, Learning Rate: 0.01\n",
      "Epoch [3743/20000], Loss: 27.737640380859375, Learning Rate: 0.01\n",
      "Epoch [3744/20000], Loss: 24.644073486328125, Learning Rate: 0.01\n",
      "Epoch [3745/20000], Loss: 23.811737060546875, Learning Rate: 0.01\n",
      "Epoch [3746/20000], Loss: 25.45855712890625, Learning Rate: 0.01\n",
      "Epoch [3747/20000], Loss: 27.021820068359375, Learning Rate: 0.01\n",
      "Epoch [3748/20000], Loss: 26.523406982421875, Learning Rate: 0.01\n",
      "Epoch [3749/20000], Loss: 24.690597534179688, Learning Rate: 0.01\n",
      "Epoch [3750/20000], Loss: 23.673110961914062, Learning Rate: 0.01\n",
      "Epoch [3751/20000], Loss: 24.30712890625, Learning Rate: 0.01\n",
      "Epoch [3752/20000], Loss: 25.395828247070312, Learning Rate: 0.01\n",
      "Epoch [3753/20000], Loss: 25.461654663085938, Learning Rate: 0.01\n",
      "Epoch [3754/20000], Loss: 24.486419677734375, Learning Rate: 0.01\n",
      "Epoch [3755/20000], Loss: 23.669891357421875, Learning Rate: 0.01\n",
      "Epoch [3756/20000], Loss: 23.814178466796875, Learning Rate: 0.01\n",
      "Epoch [3757/20000], Loss: 24.472183227539062, Learning Rate: 0.01\n",
      "Epoch [3758/20000], Loss: 24.70013427734375, Learning Rate: 0.01\n",
      "Epoch [3759/20000], Loss: 24.234085083007812, Learning Rate: 0.01\n",
      "Epoch [3760/20000], Loss: 23.663970947265625, Learning Rate: 0.01\n",
      "Epoch [3761/20000], Loss: 23.593826293945312, Learning Rate: 0.01\n",
      "Epoch [3762/20000], Loss: 23.94537353515625, Learning Rate: 0.01\n",
      "Epoch [3763/20000], Loss: 24.179458618164062, Learning Rate: 0.01\n",
      "Epoch [3764/20000], Loss: 23.998077392578125, Learning Rate: 0.01\n",
      "Epoch [3765/20000], Loss: 23.63604736328125, Learning Rate: 0.01\n",
      "Epoch [3766/20000], Loss: 23.491653442382812, Learning Rate: 0.01\n",
      "Epoch [3767/20000], Loss: 23.64404296875, Learning Rate: 0.01\n",
      "Epoch [3768/20000], Loss: 23.826080322265625, Learning Rate: 0.01\n",
      "Epoch [3769/20000], Loss: 23.79168701171875, Learning Rate: 0.01\n",
      "Epoch [3770/20000], Loss: 23.585952758789062, Learning Rate: 0.01\n",
      "Epoch [3771/20000], Loss: 23.4385986328125, Learning Rate: 0.01\n",
      "Epoch [3772/20000], Loss: 23.472091674804688, Learning Rate: 0.01\n",
      "Epoch [3773/20000], Loss: 23.587127685546875, Learning Rate: 0.01\n",
      "Epoch [3774/20000], Loss: 23.615249633789062, Learning Rate: 0.01\n",
      "Epoch [3775/20000], Loss: 23.51727294921875, Learning Rate: 0.01\n",
      "Epoch [3776/20000], Loss: 23.400497436523438, Learning Rate: 0.01\n",
      "Epoch [3777/20000], Loss: 23.373123168945312, Learning Rate: 0.01\n",
      "Epoch [3778/20000], Loss: 23.426284790039062, Learning Rate: 0.01\n",
      "Epoch [3779/20000], Loss: 23.467361450195312, Learning Rate: 0.01\n",
      "Epoch [3780/20000], Loss: 23.434967041015625, Learning Rate: 0.01\n",
      "Epoch [3781/20000], Loss: 23.359329223632812, Learning Rate: 0.01\n",
      "Epoch [3782/20000], Loss: 23.310943603515625, Learning Rate: 0.01\n",
      "Epoch [3783/20000], Loss: 23.318771362304688, Learning Rate: 0.01\n",
      "Epoch [3784/20000], Loss: 23.347183227539062, Learning Rate: 0.01\n",
      "Epoch [3785/20000], Loss: 23.346511840820312, Learning Rate: 0.01\n",
      "Epoch [3786/20000], Loss: 23.307525634765625, Learning Rate: 0.01\n",
      "Epoch [3787/20000], Loss: 23.262786865234375, Learning Rate: 0.01\n",
      "Epoch [3788/20000], Loss: 23.24493408203125, Learning Rate: 0.01\n",
      "Epoch [3789/20000], Loss: 23.252883911132812, Learning Rate: 0.01\n",
      "Epoch [3790/20000], Loss: 23.259719848632812, Learning Rate: 0.01\n",
      "Epoch [3791/20000], Loss: 23.2451171875, Learning Rate: 0.01\n",
      "Epoch [3792/20000], Loss: 23.214492797851562, Learning Rate: 0.01\n",
      "Epoch [3793/20000], Loss: 23.188827514648438, Learning Rate: 0.01\n",
      "Epoch [3794/20000], Loss: 23.179855346679688, Learning Rate: 0.01\n",
      "Epoch [3795/20000], Loss: 23.180892944335938, Learning Rate: 0.01\n",
      "Epoch [3796/20000], Loss: 23.176513671875, Learning Rate: 0.01\n",
      "Epoch [3797/20000], Loss: 23.159927368164062, Learning Rate: 0.01\n",
      "Epoch [3798/20000], Loss: 23.137649536132812, Learning Rate: 0.01\n",
      "Epoch [3799/20000], Loss: 23.120819091796875, Learning Rate: 0.01\n",
      "Epoch [3800/20000], Loss: 23.11279296875, Learning Rate: 0.01\n",
      "Epoch [3801/20000], Loss: 23.108291625976562, Learning Rate: 0.01\n",
      "Epoch [3802/20000], Loss: 23.099441528320312, Learning Rate: 0.01\n",
      "Epoch [3803/20000], Loss: 23.084335327148438, Learning Rate: 0.01\n",
      "Epoch [3804/20000], Loss: 23.067291259765625, Learning Rate: 0.01\n",
      "Epoch [3805/20000], Loss: 23.05377197265625, Learning Rate: 0.01\n",
      "Epoch [3806/20000], Loss: 23.04473876953125, Learning Rate: 0.01\n",
      "Epoch [3807/20000], Loss: 23.037109375, Learning Rate: 0.01\n",
      "Epoch [3808/20000], Loss: 23.026840209960938, Learning Rate: 0.01\n",
      "Epoch [3809/20000], Loss: 23.013259887695312, Learning Rate: 0.01\n",
      "Epoch [3810/20000], Loss: 22.999114990234375, Learning Rate: 0.01\n",
      "Epoch [3811/20000], Loss: 22.986770629882812, Learning Rate: 0.01\n",
      "Epoch [3812/20000], Loss: 22.9766845703125, Learning Rate: 0.01\n",
      "Epoch [3813/20000], Loss: 22.96746826171875, Learning Rate: 0.01\n",
      "Epoch [3814/20000], Loss: 22.956863403320312, Learning Rate: 0.01\n",
      "Epoch [3815/20000], Loss: 22.944503784179688, Learning Rate: 0.01\n",
      "Epoch [3816/20000], Loss: 22.931686401367188, Learning Rate: 0.01\n",
      "Epoch [3817/20000], Loss: 22.919677734375, Learning Rate: 0.01\n",
      "Epoch [3818/20000], Loss: 22.909072875976562, Learning Rate: 0.01\n",
      "Epoch [3819/20000], Loss: 22.89886474609375, Learning Rate: 0.01\n",
      "Epoch [3820/20000], Loss: 22.88818359375, Learning Rate: 0.01\n",
      "Epoch [3821/20000], Loss: 22.876571655273438, Learning Rate: 0.01\n",
      "Epoch [3822/20000], Loss: 22.864456176757812, Learning Rate: 0.01\n",
      "Epoch [3823/20000], Loss: 22.852706909179688, Learning Rate: 0.01\n",
      "Epoch [3824/20000], Loss: 22.841644287109375, Learning Rate: 0.01\n",
      "Epoch [3825/20000], Loss: 22.831130981445312, Learning Rate: 0.01\n",
      "Epoch [3826/20000], Loss: 22.820159912109375, Learning Rate: 0.01\n",
      "Epoch [3827/20000], Loss: 22.808990478515625, Learning Rate: 0.01\n",
      "Epoch [3828/20000], Loss: 22.797210693359375, Learning Rate: 0.01\n",
      "Epoch [3829/20000], Loss: 22.785781860351562, Learning Rate: 0.01\n",
      "Epoch [3830/20000], Loss: 22.774581909179688, Learning Rate: 0.01\n",
      "Epoch [3831/20000], Loss: 22.763565063476562, Learning Rate: 0.01\n",
      "Epoch [3832/20000], Loss: 22.75262451171875, Learning Rate: 0.01\n",
      "Epoch [3833/20000], Loss: 22.741546630859375, Learning Rate: 0.01\n",
      "Epoch [3834/20000], Loss: 22.730178833007812, Learning Rate: 0.01\n",
      "Epoch [3835/20000], Loss: 22.718658447265625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3836/20000], Loss: 22.70745849609375, Learning Rate: 0.01\n",
      "Epoch [3837/20000], Loss: 22.696273803710938, Learning Rate: 0.01\n",
      "Epoch [3838/20000], Loss: 22.685256958007812, Learning Rate: 0.01\n",
      "Epoch [3839/20000], Loss: 22.67425537109375, Learning Rate: 0.01\n",
      "Epoch [3840/20000], Loss: 22.6629638671875, Learning Rate: 0.01\n",
      "Epoch [3841/20000], Loss: 22.65167236328125, Learning Rate: 0.01\n",
      "Epoch [3842/20000], Loss: 22.640518188476562, Learning Rate: 0.01\n",
      "Epoch [3843/20000], Loss: 22.6292724609375, Learning Rate: 0.01\n",
      "Epoch [3844/20000], Loss: 22.6180419921875, Learning Rate: 0.01\n",
      "Epoch [3845/20000], Loss: 22.607025146484375, Learning Rate: 0.01\n",
      "Epoch [3846/20000], Loss: 22.595779418945312, Learning Rate: 0.01\n",
      "Epoch [3847/20000], Loss: 22.58465576171875, Learning Rate: 0.01\n",
      "Epoch [3848/20000], Loss: 22.573394775390625, Learning Rate: 0.01\n",
      "Epoch [3849/20000], Loss: 22.562149047851562, Learning Rate: 0.01\n",
      "Epoch [3850/20000], Loss: 22.55096435546875, Learning Rate: 0.01\n",
      "Epoch [3851/20000], Loss: 22.539810180664062, Learning Rate: 0.01\n",
      "Epoch [3852/20000], Loss: 22.528671264648438, Learning Rate: 0.01\n",
      "Epoch [3853/20000], Loss: 22.517562866210938, Learning Rate: 0.01\n",
      "Epoch [3854/20000], Loss: 22.506362915039062, Learning Rate: 0.01\n",
      "Epoch [3855/20000], Loss: 22.49517822265625, Learning Rate: 0.01\n",
      "Epoch [3856/20000], Loss: 22.483901977539062, Learning Rate: 0.01\n",
      "Epoch [3857/20000], Loss: 22.472625732421875, Learning Rate: 0.01\n",
      "Epoch [3858/20000], Loss: 22.461502075195312, Learning Rate: 0.01\n",
      "Epoch [3859/20000], Loss: 22.450408935546875, Learning Rate: 0.01\n",
      "Epoch [3860/20000], Loss: 22.439117431640625, Learning Rate: 0.01\n",
      "Epoch [3861/20000], Loss: 22.428054809570312, Learning Rate: 0.01\n",
      "Epoch [3862/20000], Loss: 22.41680908203125, Learning Rate: 0.01\n",
      "Epoch [3863/20000], Loss: 22.405609130859375, Learning Rate: 0.01\n",
      "Epoch [3864/20000], Loss: 22.394439697265625, Learning Rate: 0.01\n",
      "Epoch [3865/20000], Loss: 22.383209228515625, Learning Rate: 0.01\n",
      "Epoch [3866/20000], Loss: 22.372039794921875, Learning Rate: 0.01\n",
      "Epoch [3867/20000], Loss: 22.360916137695312, Learning Rate: 0.01\n",
      "Epoch [3868/20000], Loss: 22.349685668945312, Learning Rate: 0.01\n",
      "Epoch [3869/20000], Loss: 22.338455200195312, Learning Rate: 0.01\n",
      "Epoch [3870/20000], Loss: 22.327301025390625, Learning Rate: 0.01\n",
      "Epoch [3871/20000], Loss: 22.316177368164062, Learning Rate: 0.01\n",
      "Epoch [3872/20000], Loss: 22.304946899414062, Learning Rate: 0.01\n",
      "Epoch [3873/20000], Loss: 22.293731689453125, Learning Rate: 0.01\n",
      "Epoch [3874/20000], Loss: 22.282516479492188, Learning Rate: 0.01\n",
      "Epoch [3875/20000], Loss: 22.271453857421875, Learning Rate: 0.01\n",
      "Epoch [3876/20000], Loss: 22.260147094726562, Learning Rate: 0.01\n",
      "Epoch [3877/20000], Loss: 22.248992919921875, Learning Rate: 0.01\n",
      "Epoch [3878/20000], Loss: 22.237686157226562, Learning Rate: 0.01\n",
      "Epoch [3879/20000], Loss: 22.226638793945312, Learning Rate: 0.01\n",
      "Epoch [3880/20000], Loss: 22.215423583984375, Learning Rate: 0.01\n",
      "Epoch [3881/20000], Loss: 22.204269409179688, Learning Rate: 0.01\n",
      "Epoch [3882/20000], Loss: 22.192962646484375, Learning Rate: 0.01\n",
      "Epoch [3883/20000], Loss: 22.1817626953125, Learning Rate: 0.01\n",
      "Epoch [3884/20000], Loss: 22.170730590820312, Learning Rate: 0.01\n",
      "Epoch [3885/20000], Loss: 22.159469604492188, Learning Rate: 0.01\n",
      "Epoch [3886/20000], Loss: 22.148269653320312, Learning Rate: 0.01\n",
      "Epoch [3887/20000], Loss: 22.136993408203125, Learning Rate: 0.01\n",
      "Epoch [3888/20000], Loss: 22.125778198242188, Learning Rate: 0.01\n",
      "Epoch [3889/20000], Loss: 22.114593505859375, Learning Rate: 0.01\n",
      "Epoch [3890/20000], Loss: 22.103439331054688, Learning Rate: 0.01\n",
      "Epoch [3891/20000], Loss: 22.092254638671875, Learning Rate: 0.01\n",
      "Epoch [3892/20000], Loss: 22.081024169921875, Learning Rate: 0.01\n",
      "Epoch [3893/20000], Loss: 22.069976806640625, Learning Rate: 0.01\n",
      "Epoch [3894/20000], Loss: 22.058624267578125, Learning Rate: 0.01\n",
      "Epoch [3895/20000], Loss: 22.047409057617188, Learning Rate: 0.01\n",
      "Epoch [3896/20000], Loss: 22.036270141601562, Learning Rate: 0.01\n",
      "Epoch [3897/20000], Loss: 22.025100708007812, Learning Rate: 0.01\n",
      "Epoch [3898/20000], Loss: 22.013809204101562, Learning Rate: 0.01\n",
      "Epoch [3899/20000], Loss: 22.002670288085938, Learning Rate: 0.01\n",
      "Epoch [3900/20000], Loss: 21.991485595703125, Learning Rate: 0.01\n",
      "Epoch [3901/20000], Loss: 21.9803466796875, Learning Rate: 0.01\n",
      "Epoch [3902/20000], Loss: 21.969039916992188, Learning Rate: 0.01\n",
      "Epoch [3903/20000], Loss: 21.9578857421875, Learning Rate: 0.01\n",
      "Epoch [3904/20000], Loss: 21.946731567382812, Learning Rate: 0.01\n",
      "Epoch [3905/20000], Loss: 21.935546875, Learning Rate: 0.01\n",
      "Epoch [3906/20000], Loss: 21.924179077148438, Learning Rate: 0.01\n",
      "Epoch [3907/20000], Loss: 21.913009643554688, Learning Rate: 0.01\n",
      "Epoch [3908/20000], Loss: 21.901718139648438, Learning Rate: 0.01\n",
      "Epoch [3909/20000], Loss: 21.890609741210938, Learning Rate: 0.01\n",
      "Epoch [3910/20000], Loss: 21.87945556640625, Learning Rate: 0.01\n",
      "Epoch [3911/20000], Loss: 21.8681640625, Learning Rate: 0.01\n",
      "Epoch [3912/20000], Loss: 21.857009887695312, Learning Rate: 0.01\n",
      "Epoch [3913/20000], Loss: 21.845855712890625, Learning Rate: 0.01\n",
      "Epoch [3914/20000], Loss: 21.834548950195312, Learning Rate: 0.01\n",
      "Epoch [3915/20000], Loss: 21.823394775390625, Learning Rate: 0.01\n",
      "Epoch [3916/20000], Loss: 21.812149047851562, Learning Rate: 0.01\n",
      "Epoch [3917/20000], Loss: 21.800979614257812, Learning Rate: 0.01\n",
      "Epoch [3918/20000], Loss: 21.789764404296875, Learning Rate: 0.01\n",
      "Epoch [3919/20000], Loss: 21.778579711914062, Learning Rate: 0.01\n",
      "Epoch [3920/20000], Loss: 21.767333984375, Learning Rate: 0.01\n",
      "Epoch [3921/20000], Loss: 21.756072998046875, Learning Rate: 0.01\n",
      "Epoch [3922/20000], Loss: 21.744903564453125, Learning Rate: 0.01\n",
      "Epoch [3923/20000], Loss: 21.7337646484375, Learning Rate: 0.01\n",
      "Epoch [3924/20000], Loss: 21.72247314453125, Learning Rate: 0.01\n",
      "Epoch [3925/20000], Loss: 21.711273193359375, Learning Rate: 0.01\n",
      "Epoch [3926/20000], Loss: 21.700027465820312, Learning Rate: 0.01\n",
      "Epoch [3927/20000], Loss: 21.68890380859375, Learning Rate: 0.01\n",
      "Epoch [3928/20000], Loss: 21.67767333984375, Learning Rate: 0.01\n",
      "Epoch [3929/20000], Loss: 21.666427612304688, Learning Rate: 0.01\n",
      "Epoch [3930/20000], Loss: 21.655258178710938, Learning Rate: 0.01\n",
      "Epoch [3931/20000], Loss: 21.644088745117188, Learning Rate: 0.01\n",
      "Epoch [3932/20000], Loss: 21.632843017578125, Learning Rate: 0.01\n",
      "Epoch [3933/20000], Loss: 21.621566772460938, Learning Rate: 0.01\n",
      "Epoch [3934/20000], Loss: 21.6103515625, Learning Rate: 0.01\n",
      "Epoch [3935/20000], Loss: 21.599151611328125, Learning Rate: 0.01\n",
      "Epoch [3936/20000], Loss: 21.587905883789062, Learning Rate: 0.01\n",
      "Epoch [3937/20000], Loss: 21.57672119140625, Learning Rate: 0.01\n",
      "Epoch [3938/20000], Loss: 21.565475463867188, Learning Rate: 0.01\n",
      "Epoch [3939/20000], Loss: 21.554183959960938, Learning Rate: 0.01\n",
      "Epoch [3940/20000], Loss: 21.543045043945312, Learning Rate: 0.01\n",
      "Epoch [3941/20000], Loss: 21.531845092773438, Learning Rate: 0.01\n",
      "Epoch [3942/20000], Loss: 21.52056884765625, Learning Rate: 0.01\n",
      "Epoch [3943/20000], Loss: 21.509368896484375, Learning Rate: 0.01\n",
      "Epoch [3944/20000], Loss: 21.498123168945312, Learning Rate: 0.01\n",
      "Epoch [3945/20000], Loss: 21.486846923828125, Learning Rate: 0.01\n",
      "Epoch [3946/20000], Loss: 21.4757080078125, Learning Rate: 0.01\n",
      "Epoch [3947/20000], Loss: 21.4644775390625, Learning Rate: 0.01\n",
      "Epoch [3948/20000], Loss: 21.453216552734375, Learning Rate: 0.01\n",
      "Epoch [3949/20000], Loss: 21.441986083984375, Learning Rate: 0.01\n",
      "Epoch [3950/20000], Loss: 21.430816650390625, Learning Rate: 0.01\n",
      "Epoch [3951/20000], Loss: 21.419631958007812, Learning Rate: 0.01\n",
      "Epoch [3952/20000], Loss: 21.408340454101562, Learning Rate: 0.01\n",
      "Epoch [3953/20000], Loss: 21.397232055664062, Learning Rate: 0.01\n",
      "Epoch [3954/20000], Loss: 21.385848999023438, Learning Rate: 0.01\n",
      "Epoch [3955/20000], Loss: 21.374710083007812, Learning Rate: 0.01\n",
      "Epoch [3956/20000], Loss: 21.363510131835938, Learning Rate: 0.01\n",
      "Epoch [3957/20000], Loss: 21.3521728515625, Learning Rate: 0.01\n",
      "Epoch [3958/20000], Loss: 21.341018676757812, Learning Rate: 0.01\n",
      "Epoch [3959/20000], Loss: 21.329742431640625, Learning Rate: 0.01\n",
      "Epoch [3960/20000], Loss: 21.318496704101562, Learning Rate: 0.01\n",
      "Epoch [3961/20000], Loss: 21.307281494140625, Learning Rate: 0.01\n",
      "Epoch [3962/20000], Loss: 21.2960205078125, Learning Rate: 0.01\n",
      "Epoch [3963/20000], Loss: 21.284744262695312, Learning Rate: 0.01\n",
      "Epoch [3964/20000], Loss: 21.273605346679688, Learning Rate: 0.01\n",
      "Epoch [3965/20000], Loss: 21.262405395507812, Learning Rate: 0.01\n",
      "Epoch [3966/20000], Loss: 21.251129150390625, Learning Rate: 0.01\n",
      "Epoch [3967/20000], Loss: 21.2398681640625, Learning Rate: 0.01\n",
      "Epoch [3968/20000], Loss: 21.22869873046875, Learning Rate: 0.01\n",
      "Epoch [3969/20000], Loss: 21.217391967773438, Learning Rate: 0.01\n",
      "Epoch [3970/20000], Loss: 21.206253051757812, Learning Rate: 0.01\n",
      "Epoch [3971/20000], Loss: 21.194915771484375, Learning Rate: 0.01\n",
      "Epoch [3972/20000], Loss: 21.18365478515625, Learning Rate: 0.01\n",
      "Epoch [3973/20000], Loss: 21.17254638671875, Learning Rate: 0.01\n",
      "Epoch [3974/20000], Loss: 21.161300659179688, Learning Rate: 0.01\n",
      "Epoch [3975/20000], Loss: 21.150070190429688, Learning Rate: 0.01\n",
      "Epoch [3976/20000], Loss: 21.138870239257812, Learning Rate: 0.01\n",
      "Epoch [3977/20000], Loss: 21.127532958984375, Learning Rate: 0.01\n",
      "Epoch [3978/20000], Loss: 21.116424560546875, Learning Rate: 0.01\n",
      "Epoch [3979/20000], Loss: 21.10516357421875, Learning Rate: 0.01\n",
      "Epoch [3980/20000], Loss: 21.093917846679688, Learning Rate: 0.01\n",
      "Epoch [3981/20000], Loss: 21.082748413085938, Learning Rate: 0.01\n",
      "Epoch [3982/20000], Loss: 21.071517944335938, Learning Rate: 0.01\n",
      "Epoch [3983/20000], Loss: 21.060409545898438, Learning Rate: 0.01\n",
      "Epoch [3984/20000], Loss: 21.049148559570312, Learning Rate: 0.01\n",
      "Epoch [3985/20000], Loss: 21.038192749023438, Learning Rate: 0.01\n",
      "Epoch [3986/20000], Loss: 21.027130126953125, Learning Rate: 0.01\n",
      "Epoch [3987/20000], Loss: 21.016326904296875, Learning Rate: 0.01\n",
      "Epoch [3988/20000], Loss: 21.00555419921875, Learning Rate: 0.01\n",
      "Epoch [3989/20000], Loss: 20.99505615234375, Learning Rate: 0.01\n",
      "Epoch [3990/20000], Loss: 20.98504638671875, Learning Rate: 0.01\n",
      "Epoch [3991/20000], Loss: 20.975753784179688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3992/20000], Loss: 20.967361450195312, Learning Rate: 0.01\n",
      "Epoch [3993/20000], Loss: 20.960494995117188, Learning Rate: 0.01\n",
      "Epoch [3994/20000], Loss: 20.9561767578125, Learning Rate: 0.01\n",
      "Epoch [3995/20000], Loss: 20.955902099609375, Learning Rate: 0.01\n",
      "Epoch [3996/20000], Loss: 20.961868286132812, Learning Rate: 0.01\n",
      "Epoch [3997/20000], Loss: 20.97833251953125, Learning Rate: 0.01\n",
      "Epoch [3998/20000], Loss: 21.011199951171875, Learning Rate: 0.01\n",
      "Epoch [3999/20000], Loss: 21.07110595703125, Learning Rate: 0.01\n",
      "Epoch [4000/20000], Loss: 21.175048828125, Learning Rate: 0.01\n",
      "Epoch [4001/20000], Loss: 21.350936889648438, Learning Rate: 0.01\n",
      "Epoch [4002/20000], Loss: 21.64447021484375, Learning Rate: 0.01\n",
      "Epoch [4003/20000], Loss: 22.128738403320312, Learning Rate: 0.01\n",
      "Epoch [4004/20000], Loss: 22.919570922851562, Learning Rate: 0.01\n",
      "Epoch [4005/20000], Loss: 24.188323974609375, Learning Rate: 0.01\n",
      "Epoch [4006/20000], Loss: 26.172698974609375, Learning Rate: 0.01\n",
      "Epoch [4007/20000], Loss: 29.1256103515625, Learning Rate: 0.01\n",
      "Epoch [4008/20000], Loss: 33.184326171875, Learning Rate: 0.01\n",
      "Epoch [4009/20000], Loss: 37.961456298828125, Learning Rate: 0.01\n",
      "Epoch [4010/20000], Loss: 42.12489318847656, Learning Rate: 0.01\n",
      "Epoch [4011/20000], Loss: 43.204071044921875, Learning Rate: 0.01\n",
      "Epoch [4012/20000], Loss: 39.22508239746094, Learning Rate: 0.01\n",
      "Epoch [4013/20000], Loss: 31.112457275390625, Learning Rate: 0.01\n",
      "Epoch [4014/20000], Loss: 23.44769287109375, Learning Rate: 0.01\n",
      "Epoch [4015/20000], Loss: 20.708999633789062, Learning Rate: 0.01\n",
      "Epoch [4016/20000], Loss: 23.380020141601562, Learning Rate: 0.01\n",
      "Epoch [4017/20000], Loss: 27.974761962890625, Learning Rate: 0.01\n",
      "Epoch [4018/20000], Loss: 30.140029907226562, Learning Rate: 0.01\n",
      "Epoch [4019/20000], Loss: 28.027145385742188, Learning Rate: 0.01\n",
      "Epoch [4020/20000], Loss: 23.631500244140625, Learning Rate: 0.01\n",
      "Epoch [4021/20000], Loss: 20.799285888671875, Learning Rate: 0.01\n",
      "Epoch [4022/20000], Loss: 21.41259765625, Learning Rate: 0.01\n",
      "Epoch [4023/20000], Loss: 23.951278686523438, Learning Rate: 0.01\n",
      "Epoch [4024/20000], Loss: 25.458984375, Learning Rate: 0.01\n",
      "Epoch [4025/20000], Loss: 24.445465087890625, Learning Rate: 0.01\n",
      "Epoch [4026/20000], Loss: 22.065948486328125, Learning Rate: 0.01\n",
      "Epoch [4027/20000], Loss: 20.61639404296875, Learning Rate: 0.01\n",
      "Epoch [4028/20000], Loss: 21.114761352539062, Learning Rate: 0.01\n",
      "Epoch [4029/20000], Loss: 22.531784057617188, Learning Rate: 0.01\n",
      "Epoch [4030/20000], Loss: 23.16131591796875, Learning Rate: 0.01\n",
      "Epoch [4031/20000], Loss: 22.385467529296875, Learning Rate: 0.01\n",
      "Epoch [4032/20000], Loss: 21.081985473632812, Learning Rate: 0.01\n",
      "Epoch [4033/20000], Loss: 20.504928588867188, Learning Rate: 0.01\n",
      "Epoch [4034/20000], Loss: 20.969818115234375, Learning Rate: 0.01\n",
      "Epoch [4035/20000], Loss: 21.7213134765625, Learning Rate: 0.01\n",
      "Epoch [4036/20000], Loss: 21.87591552734375, Learning Rate: 0.01\n",
      "Epoch [4037/20000], Loss: 21.30706787109375, Learning Rate: 0.01\n",
      "Epoch [4038/20000], Loss: 20.631149291992188, Learning Rate: 0.01\n",
      "Epoch [4039/20000], Loss: 20.459823608398438, Learning Rate: 0.01\n",
      "Epoch [4040/20000], Loss: 20.800003051757812, Learning Rate: 0.01\n",
      "Epoch [4041/20000], Loss: 21.163925170898438, Learning Rate: 0.01\n",
      "Epoch [4042/20000], Loss: 21.1385498046875, Learning Rate: 0.01\n",
      "Epoch [4043/20000], Loss: 20.771957397460938, Learning Rate: 0.01\n",
      "Epoch [4044/20000], Loss: 20.437759399414062, Learning Rate: 0.01\n",
      "Epoch [4045/20000], Loss: 20.41015625, Learning Rate: 0.01\n",
      "Epoch [4046/20000], Loss: 20.618392944335938, Learning Rate: 0.01\n",
      "Epoch [4047/20000], Loss: 20.782379150390625, Learning Rate: 0.01\n",
      "Epoch [4048/20000], Loss: 20.720748901367188, Learning Rate: 0.01\n",
      "Epoch [4049/20000], Loss: 20.50384521484375, Learning Rate: 0.01\n",
      "Epoch [4050/20000], Loss: 20.33758544921875, Learning Rate: 0.01\n",
      "Epoch [4051/20000], Loss: 20.34210205078125, Learning Rate: 0.01\n",
      "Epoch [4052/20000], Loss: 20.455596923828125, Learning Rate: 0.01\n",
      "Epoch [4053/20000], Loss: 20.527877807617188, Learning Rate: 0.01\n",
      "Epoch [4054/20000], Loss: 20.47772216796875, Learning Rate: 0.01\n",
      "Epoch [4055/20000], Loss: 20.353851318359375, Learning Rate: 0.01\n",
      "Epoch [4056/20000], Loss: 20.26470947265625, Learning Rate: 0.01\n",
      "Epoch [4057/20000], Loss: 20.266586303710938, Learning Rate: 0.01\n",
      "Epoch [4058/20000], Loss: 20.3228759765625, Learning Rate: 0.01\n",
      "Epoch [4059/20000], Loss: 20.355560302734375, Learning Rate: 0.01\n",
      "Epoch [4060/20000], Loss: 20.323760986328125, Learning Rate: 0.01\n",
      "Epoch [4061/20000], Loss: 20.252944946289062, Learning Rate: 0.01\n",
      "Epoch [4062/20000], Loss: 20.199005126953125, Learning Rate: 0.01\n",
      "Epoch [4063/20000], Loss: 20.1923828125, Learning Rate: 0.01\n",
      "Epoch [4064/20000], Loss: 20.216415405273438, Learning Rate: 0.01\n",
      "Epoch [4065/20000], Loss: 20.23114013671875, Learning Rate: 0.01\n",
      "Epoch [4066/20000], Loss: 20.21307373046875, Learning Rate: 0.01\n",
      "Epoch [4067/20000], Loss: 20.171798706054688, Learning Rate: 0.01\n",
      "Epoch [4068/20000], Loss: 20.1353759765625, Learning Rate: 0.01\n",
      "Epoch [4069/20000], Loss: 20.122390747070312, Learning Rate: 0.01\n",
      "Epoch [4070/20000], Loss: 20.128280639648438, Learning Rate: 0.01\n",
      "Epoch [4071/20000], Loss: 20.133636474609375, Learning Rate: 0.01\n",
      "Epoch [4072/20000], Loss: 20.12310791015625, Learning Rate: 0.01\n",
      "Epoch [4073/20000], Loss: 20.09857177734375, Learning Rate: 0.01\n",
      "Epoch [4074/20000], Loss: 20.072280883789062, Learning Rate: 0.01\n",
      "Epoch [4075/20000], Loss: 20.056427001953125, Learning Rate: 0.01\n",
      "Epoch [4076/20000], Loss: 20.052078247070312, Learning Rate: 0.01\n",
      "Epoch [4077/20000], Loss: 20.050750732421875, Learning Rate: 0.01\n",
      "Epoch [4078/20000], Loss: 20.04376220703125, Learning Rate: 0.01\n",
      "Epoch [4079/20000], Loss: 20.028091430664062, Learning Rate: 0.01\n",
      "Epoch [4080/20000], Loss: 20.0089111328125, Learning Rate: 0.01\n",
      "Epoch [4081/20000], Loss: 19.992843627929688, Learning Rate: 0.01\n",
      "Epoch [4082/20000], Loss: 19.982925415039062, Learning Rate: 0.01\n",
      "Epoch [4083/20000], Loss: 19.976898193359375, Learning Rate: 0.01\n",
      "Epoch [4084/20000], Loss: 19.969955444335938, Learning Rate: 0.01\n",
      "Epoch [4085/20000], Loss: 19.959014892578125, Learning Rate: 0.01\n",
      "Epoch [4086/20000], Loss: 19.9447021484375, Learning Rate: 0.01\n",
      "Epoch [4087/20000], Loss: 19.930023193359375, Learning Rate: 0.01\n",
      "Epoch [4088/20000], Loss: 19.91778564453125, Learning Rate: 0.01\n",
      "Epoch [4089/20000], Loss: 19.908477783203125, Learning Rate: 0.01\n",
      "Epoch [4090/20000], Loss: 19.900177001953125, Learning Rate: 0.01\n",
      "Epoch [4091/20000], Loss: 19.890777587890625, Learning Rate: 0.01\n",
      "Epoch [4092/20000], Loss: 19.879486083984375, Learning Rate: 0.01\n",
      "Epoch [4093/20000], Loss: 19.8668212890625, Learning Rate: 0.01\n",
      "Epoch [4094/20000], Loss: 19.854354858398438, Learning Rate: 0.01\n",
      "Epoch [4095/20000], Loss: 19.843170166015625, Learning Rate: 0.01\n",
      "Epoch [4096/20000], Loss: 19.833236694335938, Learning Rate: 0.01\n",
      "Epoch [4097/20000], Loss: 19.823837280273438, Learning Rate: 0.01\n",
      "Epoch [4098/20000], Loss: 19.813629150390625, Learning Rate: 0.01\n",
      "Epoch [4099/20000], Loss: 19.802688598632812, Learning Rate: 0.01\n",
      "Epoch [4100/20000], Loss: 19.790985107421875, Learning Rate: 0.01\n",
      "Epoch [4101/20000], Loss: 19.779434204101562, Learning Rate: 0.01\n",
      "Epoch [4102/20000], Loss: 19.768417358398438, Learning Rate: 0.01\n",
      "Epoch [4103/20000], Loss: 19.758132934570312, Learning Rate: 0.01\n",
      "Epoch [4104/20000], Loss: 19.747970581054688, Learning Rate: 0.01\n",
      "Epoch [4105/20000], Loss: 19.737640380859375, Learning Rate: 0.01\n",
      "Epoch [4106/20000], Loss: 19.72686767578125, Learning Rate: 0.01\n",
      "Epoch [4107/20000], Loss: 19.715744018554688, Learning Rate: 0.01\n",
      "Epoch [4108/20000], Loss: 19.704696655273438, Learning Rate: 0.01\n",
      "Epoch [4109/20000], Loss: 19.693832397460938, Learning Rate: 0.01\n",
      "Epoch [4110/20000], Loss: 19.683151245117188, Learning Rate: 0.01\n",
      "Epoch [4111/20000], Loss: 19.672775268554688, Learning Rate: 0.01\n",
      "Epoch [4112/20000], Loss: 19.662246704101562, Learning Rate: 0.01\n",
      "Epoch [4113/20000], Loss: 19.651702880859375, Learning Rate: 0.01\n",
      "Epoch [4114/20000], Loss: 19.6407470703125, Learning Rate: 0.01\n",
      "Epoch [4115/20000], Loss: 19.629928588867188, Learning Rate: 0.01\n",
      "Epoch [4116/20000], Loss: 19.619110107421875, Learning Rate: 0.01\n",
      "Epoch [4117/20000], Loss: 19.6082763671875, Learning Rate: 0.01\n",
      "Epoch [4118/20000], Loss: 19.597854614257812, Learning Rate: 0.01\n",
      "Epoch [4119/20000], Loss: 19.587234497070312, Learning Rate: 0.01\n",
      "Epoch [4120/20000], Loss: 19.576507568359375, Learning Rate: 0.01\n",
      "Epoch [4121/20000], Loss: 19.565719604492188, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4122/20000], Loss: 19.555084228515625, Learning Rate: 0.01\n",
      "Epoch [4123/20000], Loss: 19.544326782226562, Learning Rate: 0.01\n",
      "Epoch [4124/20000], Loss: 19.53363037109375, Learning Rate: 0.01\n",
      "Epoch [4125/20000], Loss: 19.522872924804688, Learning Rate: 0.01\n",
      "Epoch [4126/20000], Loss: 19.512298583984375, Learning Rate: 0.01\n",
      "Epoch [4127/20000], Loss: 19.50152587890625, Learning Rate: 0.01\n",
      "Epoch [4128/20000], Loss: 19.490875244140625, Learning Rate: 0.01\n",
      "Epoch [4129/20000], Loss: 19.480224609375, Learning Rate: 0.01\n",
      "Epoch [4130/20000], Loss: 19.469436645507812, Learning Rate: 0.01\n",
      "Epoch [4131/20000], Loss: 19.458786010742188, Learning Rate: 0.01\n",
      "Epoch [4132/20000], Loss: 19.448028564453125, Learning Rate: 0.01\n",
      "Epoch [4133/20000], Loss: 19.4373779296875, Learning Rate: 0.01\n",
      "Epoch [4134/20000], Loss: 19.426666259765625, Learning Rate: 0.01\n",
      "Epoch [4135/20000], Loss: 19.416000366210938, Learning Rate: 0.01\n",
      "Epoch [4136/20000], Loss: 19.405242919921875, Learning Rate: 0.01\n",
      "Epoch [4137/20000], Loss: 19.394607543945312, Learning Rate: 0.01\n",
      "Epoch [4138/20000], Loss: 19.383941650390625, Learning Rate: 0.01\n",
      "Epoch [4139/20000], Loss: 19.373153686523438, Learning Rate: 0.01\n",
      "Epoch [4140/20000], Loss: 19.362518310546875, Learning Rate: 0.01\n",
      "Epoch [4141/20000], Loss: 19.351760864257812, Learning Rate: 0.01\n",
      "Epoch [4142/20000], Loss: 19.341018676757812, Learning Rate: 0.01\n",
      "Epoch [4143/20000], Loss: 19.330413818359375, Learning Rate: 0.01\n",
      "Epoch [4144/20000], Loss: 19.319686889648438, Learning Rate: 0.01\n",
      "Epoch [4145/20000], Loss: 19.30889892578125, Learning Rate: 0.01\n",
      "Epoch [4146/20000], Loss: 19.298309326171875, Learning Rate: 0.01\n",
      "Epoch [4147/20000], Loss: 19.287628173828125, Learning Rate: 0.01\n",
      "Epoch [4148/20000], Loss: 19.27685546875, Learning Rate: 0.01\n",
      "Epoch [4149/20000], Loss: 19.266204833984375, Learning Rate: 0.01\n",
      "Epoch [4150/20000], Loss: 19.2554931640625, Learning Rate: 0.01\n",
      "Epoch [4151/20000], Loss: 19.244781494140625, Learning Rate: 0.01\n",
      "Epoch [4152/20000], Loss: 19.23406982421875, Learning Rate: 0.01\n",
      "Epoch [4153/20000], Loss: 19.22332763671875, Learning Rate: 0.01\n",
      "Epoch [4154/20000], Loss: 19.212570190429688, Learning Rate: 0.01\n",
      "Epoch [4155/20000], Loss: 19.202041625976562, Learning Rate: 0.01\n",
      "Epoch [4156/20000], Loss: 19.191253662109375, Learning Rate: 0.01\n",
      "Epoch [4157/20000], Loss: 19.180465698242188, Learning Rate: 0.01\n",
      "Epoch [4158/20000], Loss: 19.169784545898438, Learning Rate: 0.01\n",
      "Epoch [4159/20000], Loss: 19.159027099609375, Learning Rate: 0.01\n",
      "Epoch [4160/20000], Loss: 19.148391723632812, Learning Rate: 0.01\n",
      "Epoch [4161/20000], Loss: 19.137741088867188, Learning Rate: 0.01\n",
      "Epoch [4162/20000], Loss: 19.127044677734375, Learning Rate: 0.01\n",
      "Epoch [4163/20000], Loss: 19.116180419921875, Learning Rate: 0.01\n",
      "Epoch [4164/20000], Loss: 19.105575561523438, Learning Rate: 0.01\n",
      "Epoch [4165/20000], Loss: 19.094955444335938, Learning Rate: 0.01\n",
      "Epoch [4166/20000], Loss: 19.084152221679688, Learning Rate: 0.01\n",
      "Epoch [4167/20000], Loss: 19.073394775390625, Learning Rate: 0.01\n",
      "Epoch [4168/20000], Loss: 19.062713623046875, Learning Rate: 0.01\n",
      "Epoch [4169/20000], Loss: 19.051956176757812, Learning Rate: 0.01\n",
      "Epoch [4170/20000], Loss: 19.041229248046875, Learning Rate: 0.01\n",
      "Epoch [4171/20000], Loss: 19.030517578125, Learning Rate: 0.01\n",
      "Epoch [4172/20000], Loss: 19.019699096679688, Learning Rate: 0.01\n",
      "Epoch [4173/20000], Loss: 19.009201049804688, Learning Rate: 0.01\n",
      "Epoch [4174/20000], Loss: 18.9984130859375, Learning Rate: 0.01\n",
      "Epoch [4175/20000], Loss: 18.987594604492188, Learning Rate: 0.01\n",
      "Epoch [4176/20000], Loss: 18.976776123046875, Learning Rate: 0.01\n",
      "Epoch [4177/20000], Loss: 18.9661865234375, Learning Rate: 0.01\n",
      "Epoch [4178/20000], Loss: 18.955490112304688, Learning Rate: 0.01\n",
      "Epoch [4179/20000], Loss: 18.94476318359375, Learning Rate: 0.01\n",
      "Epoch [4180/20000], Loss: 18.934051513671875, Learning Rate: 0.01\n",
      "Epoch [4181/20000], Loss: 18.92327880859375, Learning Rate: 0.01\n",
      "Epoch [4182/20000], Loss: 18.912612915039062, Learning Rate: 0.01\n",
      "Epoch [4183/20000], Loss: 18.901840209960938, Learning Rate: 0.01\n",
      "Epoch [4184/20000], Loss: 18.89111328125, Learning Rate: 0.01\n",
      "Epoch [4185/20000], Loss: 18.8804931640625, Learning Rate: 0.01\n",
      "Epoch [4186/20000], Loss: 18.869705200195312, Learning Rate: 0.01\n",
      "Epoch [4187/20000], Loss: 18.858932495117188, Learning Rate: 0.01\n",
      "Epoch [4188/20000], Loss: 18.848236083984375, Learning Rate: 0.01\n",
      "Epoch [4189/20000], Loss: 18.837448120117188, Learning Rate: 0.01\n",
      "Epoch [4190/20000], Loss: 18.826736450195312, Learning Rate: 0.01\n",
      "Epoch [4191/20000], Loss: 18.81610107421875, Learning Rate: 0.01\n",
      "Epoch [4192/20000], Loss: 18.805328369140625, Learning Rate: 0.01\n",
      "Epoch [4193/20000], Loss: 18.794631958007812, Learning Rate: 0.01\n",
      "Epoch [4194/20000], Loss: 18.783782958984375, Learning Rate: 0.01\n",
      "Epoch [4195/20000], Loss: 18.77313232421875, Learning Rate: 0.01\n",
      "Epoch [4196/20000], Loss: 18.762298583984375, Learning Rate: 0.01\n",
      "Epoch [4197/20000], Loss: 18.751556396484375, Learning Rate: 0.01\n",
      "Epoch [4198/20000], Loss: 18.740875244140625, Learning Rate: 0.01\n",
      "Epoch [4199/20000], Loss: 18.7301025390625, Learning Rate: 0.01\n",
      "Epoch [4200/20000], Loss: 18.719451904296875, Learning Rate: 0.01\n",
      "Epoch [4201/20000], Loss: 18.708694458007812, Learning Rate: 0.01\n",
      "Epoch [4202/20000], Loss: 18.697967529296875, Learning Rate: 0.01\n",
      "Epoch [4203/20000], Loss: 18.687164306640625, Learning Rate: 0.01\n",
      "Epoch [4204/20000], Loss: 18.676422119140625, Learning Rate: 0.01\n",
      "Epoch [4205/20000], Loss: 18.665756225585938, Learning Rate: 0.01\n",
      "Epoch [4206/20000], Loss: 18.654953002929688, Learning Rate: 0.01\n",
      "Epoch [4207/20000], Loss: 18.64422607421875, Learning Rate: 0.01\n",
      "Epoch [4208/20000], Loss: 18.633438110351562, Learning Rate: 0.01\n",
      "Epoch [4209/20000], Loss: 18.62261962890625, Learning Rate: 0.01\n",
      "Epoch [4210/20000], Loss: 18.611923217773438, Learning Rate: 0.01\n",
      "Epoch [4211/20000], Loss: 18.601318359375, Learning Rate: 0.01\n",
      "Epoch [4212/20000], Loss: 18.590484619140625, Learning Rate: 0.01\n",
      "Epoch [4213/20000], Loss: 18.579788208007812, Learning Rate: 0.01\n",
      "Epoch [4214/20000], Loss: 18.5689697265625, Learning Rate: 0.01\n",
      "Epoch [4215/20000], Loss: 18.558258056640625, Learning Rate: 0.01\n",
      "Epoch [4216/20000], Loss: 18.547500610351562, Learning Rate: 0.01\n",
      "Epoch [4217/20000], Loss: 18.5367431640625, Learning Rate: 0.01\n",
      "Epoch [4218/20000], Loss: 18.526077270507812, Learning Rate: 0.01\n",
      "Epoch [4219/20000], Loss: 18.515243530273438, Learning Rate: 0.01\n",
      "Epoch [4220/20000], Loss: 18.504547119140625, Learning Rate: 0.01\n",
      "Epoch [4221/20000], Loss: 18.493698120117188, Learning Rate: 0.01\n",
      "Epoch [4222/20000], Loss: 18.483001708984375, Learning Rate: 0.01\n",
      "Epoch [4223/20000], Loss: 18.472305297851562, Learning Rate: 0.01\n",
      "Epoch [4224/20000], Loss: 18.461532592773438, Learning Rate: 0.01\n",
      "Epoch [4225/20000], Loss: 18.450714111328125, Learning Rate: 0.01\n",
      "Epoch [4226/20000], Loss: 18.440048217773438, Learning Rate: 0.01\n",
      "Epoch [4227/20000], Loss: 18.429229736328125, Learning Rate: 0.01\n",
      "Epoch [4228/20000], Loss: 18.418487548828125, Learning Rate: 0.01\n",
      "Epoch [4229/20000], Loss: 18.407638549804688, Learning Rate: 0.01\n",
      "Epoch [4230/20000], Loss: 18.39691162109375, Learning Rate: 0.01\n",
      "Epoch [4231/20000], Loss: 18.386245727539062, Learning Rate: 0.01\n",
      "Epoch [4232/20000], Loss: 18.37554931640625, Learning Rate: 0.01\n",
      "Epoch [4233/20000], Loss: 18.364730834960938, Learning Rate: 0.01\n",
      "Epoch [4234/20000], Loss: 18.354141235351562, Learning Rate: 0.01\n",
      "Epoch [4235/20000], Loss: 18.343246459960938, Learning Rate: 0.01\n",
      "Epoch [4236/20000], Loss: 18.332550048828125, Learning Rate: 0.01\n",
      "Epoch [4237/20000], Loss: 18.321868896484375, Learning Rate: 0.01\n",
      "Epoch [4238/20000], Loss: 18.311325073242188, Learning Rate: 0.01\n",
      "Epoch [4239/20000], Loss: 18.300689697265625, Learning Rate: 0.01\n",
      "Epoch [4240/20000], Loss: 18.290191650390625, Learning Rate: 0.01\n",
      "Epoch [4241/20000], Loss: 18.279953002929688, Learning Rate: 0.01\n",
      "Epoch [4242/20000], Loss: 18.269866943359375, Learning Rate: 0.01\n",
      "Epoch [4243/20000], Loss: 18.260177612304688, Learning Rate: 0.01\n",
      "Epoch [4244/20000], Loss: 18.250991821289062, Learning Rate: 0.01\n",
      "Epoch [4245/20000], Loss: 18.2427978515625, Learning Rate: 0.01\n",
      "Epoch [4246/20000], Loss: 18.23602294921875, Learning Rate: 0.01\n",
      "Epoch [4247/20000], Loss: 18.23162841796875, Learning Rate: 0.01\n",
      "Epoch [4248/20000], Loss: 18.230972290039062, Learning Rate: 0.01\n",
      "Epoch [4249/20000], Loss: 18.236343383789062, Learning Rate: 0.01\n",
      "Epoch [4250/20000], Loss: 18.251754760742188, Learning Rate: 0.01\n",
      "Epoch [4251/20000], Loss: 18.283096313476562, Learning Rate: 0.01\n",
      "Epoch [4252/20000], Loss: 18.341354370117188, Learning Rate: 0.01\n",
      "Epoch [4253/20000], Loss: 18.44329833984375, Learning Rate: 0.01\n",
      "Epoch [4254/20000], Loss: 18.6180419921875, Learning Rate: 0.01\n",
      "Epoch [4255/20000], Loss: 18.913192749023438, Learning Rate: 0.01\n",
      "Epoch [4256/20000], Loss: 19.406173706054688, Learning Rate: 0.01\n",
      "Epoch [4257/20000], Loss: 20.222091674804688, Learning Rate: 0.01\n",
      "Epoch [4258/20000], Loss: 21.551315307617188, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4259/20000], Loss: 23.666702270507812, Learning Rate: 0.01\n",
      "Epoch [4260/20000], Loss: 26.880142211914062, Learning Rate: 0.01\n",
      "Epoch [4261/20000], Loss: 31.4107666015625, Learning Rate: 0.01\n",
      "Epoch [4262/20000], Loss: 36.925506591796875, Learning Rate: 0.01\n",
      "Epoch [4263/20000], Loss: 41.995819091796875, Learning Rate: 0.01\n",
      "Epoch [4264/20000], Loss: 43.71739196777344, Learning Rate: 0.01\n",
      "Epoch [4265/20000], Loss: 39.514068603515625, Learning Rate: 0.01\n",
      "Epoch [4266/20000], Loss: 30.17724609375, Learning Rate: 0.01\n",
      "Epoch [4267/20000], Loss: 21.160247802734375, Learning Rate: 0.01\n",
      "Epoch [4268/20000], Loss: 18.006149291992188, Learning Rate: 0.01\n",
      "Epoch [4269/20000], Loss: 21.311492919921875, Learning Rate: 0.01\n",
      "Epoch [4270/20000], Loss: 26.708938598632812, Learning Rate: 0.01\n",
      "Epoch [4271/20000], Loss: 28.884735107421875, Learning Rate: 0.01\n",
      "Epoch [4272/20000], Loss: 25.9058837890625, Learning Rate: 0.01\n",
      "Epoch [4273/20000], Loss: 20.68560791015625, Learning Rate: 0.01\n",
      "Epoch [4274/20000], Loss: 17.954299926757812, Learning Rate: 0.01\n",
      "Epoch [4275/20000], Loss: 19.387557983398438, Learning Rate: 0.01\n",
      "Epoch [4276/20000], Loss: 22.459716796875, Learning Rate: 0.01\n",
      "Epoch [4277/20000], Loss: 23.56683349609375, Learning Rate: 0.01\n",
      "Epoch [4278/20000], Loss: 21.63525390625, Learning Rate: 0.01\n",
      "Epoch [4279/20000], Loss: 18.837417602539062, Learning Rate: 0.01\n",
      "Epoch [4280/20000], Loss: 17.888671875, Learning Rate: 0.01\n",
      "Epoch [4281/20000], Loss: 19.162979125976562, Learning Rate: 0.01\n",
      "Epoch [4282/20000], Loss: 20.728057861328125, Learning Rate: 0.01\n",
      "Epoch [4283/20000], Loss: 20.727630615234375, Learning Rate: 0.01\n",
      "Epoch [4284/20000], Loss: 19.26824951171875, Learning Rate: 0.01\n",
      "Epoch [4285/20000], Loss: 17.967864990234375, Learning Rate: 0.01\n",
      "Epoch [4286/20000], Loss: 18.001007080078125, Learning Rate: 0.01\n",
      "Epoch [4287/20000], Loss: 18.94610595703125, Learning Rate: 0.01\n",
      "Epoch [4288/20000], Loss: 19.526046752929688, Learning Rate: 0.01\n",
      "Epoch [4289/20000], Loss: 19.09429931640625, Learning Rate: 0.01\n",
      "Epoch [4290/20000], Loss: 18.187469482421875, Learning Rate: 0.01\n",
      "Epoch [4291/20000], Loss: 17.74957275390625, Learning Rate: 0.01\n",
      "Epoch [4292/20000], Loss: 18.063735961914062, Learning Rate: 0.01\n",
      "Epoch [4293/20000], Loss: 18.584304809570312, Learning Rate: 0.01\n",
      "Epoch [4294/20000], Loss: 18.662490844726562, Learning Rate: 0.01\n",
      "Epoch [4295/20000], Loss: 18.236358642578125, Learning Rate: 0.01\n",
      "Epoch [4296/20000], Loss: 17.783447265625, Learning Rate: 0.01\n",
      "Epoch [4297/20000], Loss: 17.722671508789062, Learning Rate: 0.01\n",
      "Epoch [4298/20000], Loss: 17.992706298828125, Learning Rate: 0.01\n",
      "Epoch [4299/20000], Loss: 18.208877563476562, Learning Rate: 0.01\n",
      "Epoch [4300/20000], Loss: 18.11865234375, Learning Rate: 0.01\n",
      "Epoch [4301/20000], Loss: 17.83355712890625, Learning Rate: 0.01\n",
      "Epoch [4302/20000], Loss: 17.644454956054688, Learning Rate: 0.01\n",
      "Epoch [4303/20000], Loss: 17.691390991210938, Learning Rate: 0.01\n",
      "Epoch [4304/20000], Loss: 17.851593017578125, Learning Rate: 0.01\n",
      "Epoch [4305/20000], Loss: 17.91436767578125, Learning Rate: 0.01\n",
      "Epoch [4306/20000], Loss: 17.80889892578125, Learning Rate: 0.01\n",
      "Epoch [4307/20000], Loss: 17.64727783203125, Learning Rate: 0.01\n",
      "Epoch [4308/20000], Loss: 17.57672119140625, Learning Rate: 0.01\n",
      "Epoch [4309/20000], Loss: 17.627792358398438, Learning Rate: 0.01\n",
      "Epoch [4310/20000], Loss: 17.706436157226562, Learning Rate: 0.01\n",
      "Epoch [4311/20000], Loss: 17.711135864257812, Learning Rate: 0.01\n",
      "Epoch [4312/20000], Loss: 17.63287353515625, Learning Rate: 0.01\n",
      "Epoch [4313/20000], Loss: 17.545684814453125, Learning Rate: 0.01\n",
      "Epoch [4314/20000], Loss: 17.518264770507812, Learning Rate: 0.01\n",
      "Epoch [4315/20000], Loss: 17.549697875976562, Learning Rate: 0.01\n",
      "Epoch [4316/20000], Loss: 17.583480834960938, Learning Rate: 0.01\n",
      "Epoch [4317/20000], Loss: 17.572433471679688, Learning Rate: 0.01\n",
      "Epoch [4318/20000], Loss: 17.521530151367188, Learning Rate: 0.01\n",
      "Epoch [4319/20000], Loss: 17.472640991210938, Learning Rate: 0.01\n",
      "Epoch [4320/20000], Loss: 17.457595825195312, Learning Rate: 0.01\n",
      "Epoch [4321/20000], Loss: 17.471572875976562, Learning Rate: 0.01\n",
      "Epoch [4322/20000], Loss: 17.483642578125, Learning Rate: 0.01\n",
      "Epoch [4323/20000], Loss: 17.471298217773438, Learning Rate: 0.01\n",
      "Epoch [4324/20000], Loss: 17.438735961914062, Learning Rate: 0.01\n",
      "Epoch [4325/20000], Loss: 17.408172607421875, Learning Rate: 0.01\n",
      "Epoch [4326/20000], Loss: 17.395523071289062, Learning Rate: 0.01\n",
      "Epoch [4327/20000], Loss: 17.398056030273438, Learning Rate: 0.01\n",
      "Epoch [4328/20000], Loss: 17.399993896484375, Learning Rate: 0.01\n",
      "Epoch [4329/20000], Loss: 17.389389038085938, Learning Rate: 0.01\n",
      "Epoch [4330/20000], Loss: 17.367950439453125, Learning Rate: 0.01\n",
      "Epoch [4331/20000], Loss: 17.346359252929688, Learning Rate: 0.01\n",
      "Epoch [4332/20000], Loss: 17.333724975585938, Learning Rate: 0.01\n",
      "Epoch [4333/20000], Loss: 17.329452514648438, Learning Rate: 0.01\n",
      "Epoch [4334/20000], Loss: 17.326278686523438, Learning Rate: 0.01\n",
      "Epoch [4335/20000], Loss: 17.31719970703125, Learning Rate: 0.01\n",
      "Epoch [4336/20000], Loss: 17.3018798828125, Learning Rate: 0.01\n",
      "Epoch [4337/20000], Loss: 17.28515625, Learning Rate: 0.01\n",
      "Epoch [4338/20000], Loss: 17.272506713867188, Learning Rate: 0.01\n",
      "Epoch [4339/20000], Loss: 17.2645263671875, Learning Rate: 0.01\n",
      "Epoch [4340/20000], Loss: 17.258331298828125, Learning Rate: 0.01\n",
      "Epoch [4341/20000], Loss: 17.249908447265625, Learning Rate: 0.01\n",
      "Epoch [4342/20000], Loss: 17.237899780273438, Learning Rate: 0.01\n",
      "Epoch [4343/20000], Loss: 17.224258422851562, Learning Rate: 0.01\n",
      "Epoch [4344/20000], Loss: 17.211761474609375, Learning Rate: 0.01\n",
      "Epoch [4345/20000], Loss: 17.20196533203125, Learning Rate: 0.01\n",
      "Epoch [4346/20000], Loss: 17.193634033203125, Learning Rate: 0.01\n",
      "Epoch [4347/20000], Loss: 17.18505859375, Learning Rate: 0.01\n",
      "Epoch [4348/20000], Loss: 17.174835205078125, Learning Rate: 0.01\n",
      "Epoch [4349/20000], Loss: 17.163101196289062, Learning Rate: 0.01\n",
      "Epoch [4350/20000], Loss: 17.151382446289062, Learning Rate: 0.01\n",
      "Epoch [4351/20000], Loss: 17.140640258789062, Learning Rate: 0.01\n",
      "Epoch [4352/20000], Loss: 17.131103515625, Learning Rate: 0.01\n",
      "Epoch [4353/20000], Loss: 17.121963500976562, Learning Rate: 0.01\n",
      "Epoch [4354/20000], Loss: 17.1121826171875, Learning Rate: 0.01\n",
      "Epoch [4355/20000], Loss: 17.10174560546875, Learning Rate: 0.01\n",
      "Epoch [4356/20000], Loss: 17.0906982421875, Learning Rate: 0.01\n",
      "Epoch [4357/20000], Loss: 17.079925537109375, Learning Rate: 0.01\n",
      "Epoch [4358/20000], Loss: 17.069549560546875, Learning Rate: 0.01\n",
      "Epoch [4359/20000], Loss: 17.059890747070312, Learning Rate: 0.01\n",
      "Epoch [4360/20000], Loss: 17.050048828125, Learning Rate: 0.01\n",
      "Epoch [4361/20000], Loss: 17.04022216796875, Learning Rate: 0.01\n",
      "Epoch [4362/20000], Loss: 17.029708862304688, Learning Rate: 0.01\n",
      "Epoch [4363/20000], Loss: 17.019149780273438, Learning Rate: 0.01\n",
      "Epoch [4364/20000], Loss: 17.008697509765625, Learning Rate: 0.01\n",
      "Epoch [4365/20000], Loss: 16.998519897460938, Learning Rate: 0.01\n",
      "Epoch [4366/20000], Loss: 16.988525390625, Learning Rate: 0.01\n",
      "Epoch [4367/20000], Loss: 16.978591918945312, Learning Rate: 0.01\n",
      "Epoch [4368/20000], Loss: 16.968536376953125, Learning Rate: 0.01\n",
      "Epoch [4369/20000], Loss: 16.958282470703125, Learning Rate: 0.01\n",
      "Epoch [4370/20000], Loss: 16.947860717773438, Learning Rate: 0.01\n",
      "Epoch [4371/20000], Loss: 16.937591552734375, Learning Rate: 0.01\n",
      "Epoch [4372/20000], Loss: 16.927383422851562, Learning Rate: 0.01\n",
      "Epoch [4373/20000], Loss: 16.91729736328125, Learning Rate: 0.01\n",
      "Epoch [4374/20000], Loss: 16.9072265625, Learning Rate: 0.01\n",
      "Epoch [4375/20000], Loss: 16.8970947265625, Learning Rate: 0.01\n",
      "Epoch [4376/20000], Loss: 16.886962890625, Learning Rate: 0.01\n",
      "Epoch [4377/20000], Loss: 16.876617431640625, Learning Rate: 0.01\n",
      "Epoch [4378/20000], Loss: 16.866470336914062, Learning Rate: 0.01\n",
      "Epoch [4379/20000], Loss: 16.856246948242188, Learning Rate: 0.01\n",
      "Epoch [4380/20000], Loss: 16.846115112304688, Learning Rate: 0.01\n",
      "Epoch [4381/20000], Loss: 16.835952758789062, Learning Rate: 0.01\n",
      "Epoch [4382/20000], Loss: 16.82574462890625, Learning Rate: 0.01\n",
      "Epoch [4383/20000], Loss: 16.815750122070312, Learning Rate: 0.01\n",
      "Epoch [4384/20000], Loss: 16.805419921875, Learning Rate: 0.01\n",
      "Epoch [4385/20000], Loss: 16.7952880859375, Learning Rate: 0.01\n",
      "Epoch [4386/20000], Loss: 16.78515625, Learning Rate: 0.01\n",
      "Epoch [4387/20000], Loss: 16.774887084960938, Learning Rate: 0.01\n",
      "Epoch [4388/20000], Loss: 16.7646484375, Learning Rate: 0.01\n",
      "Epoch [4389/20000], Loss: 16.754531860351562, Learning Rate: 0.01\n",
      "Epoch [4390/20000], Loss: 16.744461059570312, Learning Rate: 0.01\n",
      "Epoch [4391/20000], Loss: 16.73431396484375, Learning Rate: 0.01\n",
      "Epoch [4392/20000], Loss: 16.724014282226562, Learning Rate: 0.01\n",
      "Epoch [4393/20000], Loss: 16.7138671875, Learning Rate: 0.01\n",
      "Epoch [4394/20000], Loss: 16.703781127929688, Learning Rate: 0.01\n",
      "Epoch [4395/20000], Loss: 16.693466186523438, Learning Rate: 0.01\n",
      "Epoch [4396/20000], Loss: 16.683380126953125, Learning Rate: 0.01\n",
      "Epoch [4397/20000], Loss: 16.67315673828125, Learning Rate: 0.01\n",
      "Epoch [4398/20000], Loss: 16.662933349609375, Learning Rate: 0.01\n",
      "Epoch [4399/20000], Loss: 16.652740478515625, Learning Rate: 0.01\n",
      "Epoch [4400/20000], Loss: 16.642608642578125, Learning Rate: 0.01\n",
      "Epoch [4401/20000], Loss: 16.632339477539062, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4402/20000], Loss: 16.6221923828125, Learning Rate: 0.01\n",
      "Epoch [4403/20000], Loss: 16.612091064453125, Learning Rate: 0.01\n",
      "Epoch [4404/20000], Loss: 16.6019287109375, Learning Rate: 0.01\n",
      "Epoch [4405/20000], Loss: 16.591690063476562, Learning Rate: 0.01\n",
      "Epoch [4406/20000], Loss: 16.58148193359375, Learning Rate: 0.01\n",
      "Epoch [4407/20000], Loss: 16.571319580078125, Learning Rate: 0.01\n",
      "Epoch [4408/20000], Loss: 16.561065673828125, Learning Rate: 0.01\n",
      "Epoch [4409/20000], Loss: 16.55096435546875, Learning Rate: 0.01\n",
      "Epoch [4410/20000], Loss: 16.540740966796875, Learning Rate: 0.01\n",
      "Epoch [4411/20000], Loss: 16.5306396484375, Learning Rate: 0.01\n",
      "Epoch [4412/20000], Loss: 16.5203857421875, Learning Rate: 0.01\n",
      "Epoch [4413/20000], Loss: 16.510238647460938, Learning Rate: 0.01\n",
      "Epoch [4414/20000], Loss: 16.499893188476562, Learning Rate: 0.01\n",
      "Epoch [4415/20000], Loss: 16.489883422851562, Learning Rate: 0.01\n",
      "Epoch [4416/20000], Loss: 16.479598999023438, Learning Rate: 0.01\n",
      "Epoch [4417/20000], Loss: 16.469390869140625, Learning Rate: 0.01\n",
      "Epoch [4418/20000], Loss: 16.459213256835938, Learning Rate: 0.01\n",
      "Epoch [4419/20000], Loss: 16.449050903320312, Learning Rate: 0.01\n",
      "Epoch [4420/20000], Loss: 16.438796997070312, Learning Rate: 0.01\n",
      "Epoch [4421/20000], Loss: 16.428634643554688, Learning Rate: 0.01\n",
      "Epoch [4422/20000], Loss: 16.418380737304688, Learning Rate: 0.01\n",
      "Epoch [4423/20000], Loss: 16.408187866210938, Learning Rate: 0.01\n",
      "Epoch [4424/20000], Loss: 16.397979736328125, Learning Rate: 0.01\n",
      "Epoch [4425/20000], Loss: 16.387832641601562, Learning Rate: 0.01\n",
      "Epoch [4426/20000], Loss: 16.377685546875, Learning Rate: 0.01\n",
      "Epoch [4427/20000], Loss: 16.367523193359375, Learning Rate: 0.01\n",
      "Epoch [4428/20000], Loss: 16.357284545898438, Learning Rate: 0.01\n",
      "Epoch [4429/20000], Loss: 16.34710693359375, Learning Rate: 0.01\n",
      "Epoch [4430/20000], Loss: 16.336837768554688, Learning Rate: 0.01\n",
      "Epoch [4431/20000], Loss: 16.326568603515625, Learning Rate: 0.01\n",
      "Epoch [4432/20000], Loss: 16.31646728515625, Learning Rate: 0.01\n",
      "Epoch [4433/20000], Loss: 16.306243896484375, Learning Rate: 0.01\n",
      "Epoch [4434/20000], Loss: 16.296096801757812, Learning Rate: 0.01\n",
      "Epoch [4435/20000], Loss: 16.285797119140625, Learning Rate: 0.01\n",
      "Epoch [4436/20000], Loss: 16.275680541992188, Learning Rate: 0.01\n",
      "Epoch [4437/20000], Loss: 16.265411376953125, Learning Rate: 0.01\n",
      "Epoch [4438/20000], Loss: 16.25518798828125, Learning Rate: 0.01\n",
      "Epoch [4439/20000], Loss: 16.244903564453125, Learning Rate: 0.01\n",
      "Epoch [4440/20000], Loss: 16.2347412109375, Learning Rate: 0.01\n",
      "Epoch [4441/20000], Loss: 16.224639892578125, Learning Rate: 0.01\n",
      "Epoch [4442/20000], Loss: 16.214431762695312, Learning Rate: 0.01\n",
      "Epoch [4443/20000], Loss: 16.204071044921875, Learning Rate: 0.01\n",
      "Epoch [4444/20000], Loss: 16.1939697265625, Learning Rate: 0.01\n",
      "Epoch [4445/20000], Loss: 16.183685302734375, Learning Rate: 0.01\n",
      "Epoch [4446/20000], Loss: 16.173568725585938, Learning Rate: 0.01\n",
      "Epoch [4447/20000], Loss: 16.163299560546875, Learning Rate: 0.01\n",
      "Epoch [4448/20000], Loss: 16.153045654296875, Learning Rate: 0.01\n",
      "Epoch [4449/20000], Loss: 16.1429443359375, Learning Rate: 0.01\n",
      "Epoch [4450/20000], Loss: 16.132644653320312, Learning Rate: 0.01\n",
      "Epoch [4451/20000], Loss: 16.122467041015625, Learning Rate: 0.01\n",
      "Epoch [4452/20000], Loss: 16.11224365234375, Learning Rate: 0.01\n",
      "Epoch [4453/20000], Loss: 16.10205078125, Learning Rate: 0.01\n",
      "Epoch [4454/20000], Loss: 16.091888427734375, Learning Rate: 0.01\n",
      "Epoch [4455/20000], Loss: 16.081634521484375, Learning Rate: 0.01\n",
      "Epoch [4456/20000], Loss: 16.071365356445312, Learning Rate: 0.01\n",
      "Epoch [4457/20000], Loss: 16.061111450195312, Learning Rate: 0.01\n",
      "Epoch [4458/20000], Loss: 16.051055908203125, Learning Rate: 0.01\n",
      "Epoch [4459/20000], Loss: 16.04083251953125, Learning Rate: 0.01\n",
      "Epoch [4460/20000], Loss: 16.030532836914062, Learning Rate: 0.01\n",
      "Epoch [4461/20000], Loss: 16.020263671875, Learning Rate: 0.01\n",
      "Epoch [4462/20000], Loss: 16.010055541992188, Learning Rate: 0.01\n",
      "Epoch [4463/20000], Loss: 15.9998779296875, Learning Rate: 0.01\n",
      "Epoch [4464/20000], Loss: 15.989639282226562, Learning Rate: 0.01\n",
      "Epoch [4465/20000], Loss: 15.979415893554688, Learning Rate: 0.01\n",
      "Epoch [4466/20000], Loss: 15.969192504882812, Learning Rate: 0.01\n",
      "Epoch [4467/20000], Loss: 15.958938598632812, Learning Rate: 0.01\n",
      "Epoch [4468/20000], Loss: 15.948822021484375, Learning Rate: 0.01\n",
      "Epoch [4469/20000], Loss: 15.938430786132812, Learning Rate: 0.01\n",
      "Epoch [4470/20000], Loss: 15.928268432617188, Learning Rate: 0.01\n",
      "Epoch [4471/20000], Loss: 15.918106079101562, Learning Rate: 0.01\n",
      "Epoch [4472/20000], Loss: 15.9078369140625, Learning Rate: 0.01\n",
      "Epoch [4473/20000], Loss: 15.897613525390625, Learning Rate: 0.01\n",
      "Epoch [4474/20000], Loss: 15.887481689453125, Learning Rate: 0.01\n",
      "Epoch [4475/20000], Loss: 15.877059936523438, Learning Rate: 0.01\n",
      "Epoch [4476/20000], Loss: 15.866928100585938, Learning Rate: 0.01\n",
      "Epoch [4477/20000], Loss: 15.856658935546875, Learning Rate: 0.01\n",
      "Epoch [4478/20000], Loss: 15.846435546875, Learning Rate: 0.01\n",
      "Epoch [4479/20000], Loss: 15.836196899414062, Learning Rate: 0.01\n",
      "Epoch [4480/20000], Loss: 15.825973510742188, Learning Rate: 0.01\n",
      "Epoch [4481/20000], Loss: 15.8157958984375, Learning Rate: 0.01\n",
      "Epoch [4482/20000], Loss: 15.805557250976562, Learning Rate: 0.01\n",
      "Epoch [4483/20000], Loss: 15.795242309570312, Learning Rate: 0.01\n",
      "Epoch [4484/20000], Loss: 15.785079956054688, Learning Rate: 0.01\n",
      "Epoch [4485/20000], Loss: 15.774856567382812, Learning Rate: 0.01\n",
      "Epoch [4486/20000], Loss: 15.76458740234375, Learning Rate: 0.01\n",
      "Epoch [4487/20000], Loss: 15.754364013671875, Learning Rate: 0.01\n",
      "Epoch [4488/20000], Loss: 15.744186401367188, Learning Rate: 0.01\n",
      "Epoch [4489/20000], Loss: 15.733917236328125, Learning Rate: 0.01\n",
      "Epoch [4490/20000], Loss: 15.723617553710938, Learning Rate: 0.01\n",
      "Epoch [4491/20000], Loss: 15.71343994140625, Learning Rate: 0.01\n",
      "Epoch [4492/20000], Loss: 15.703277587890625, Learning Rate: 0.01\n",
      "Epoch [4493/20000], Loss: 15.692886352539062, Learning Rate: 0.01\n",
      "Epoch [4494/20000], Loss: 15.6827392578125, Learning Rate: 0.01\n",
      "Epoch [4495/20000], Loss: 15.67242431640625, Learning Rate: 0.01\n",
      "Epoch [4496/20000], Loss: 15.6622314453125, Learning Rate: 0.01\n",
      "Epoch [4497/20000], Loss: 15.651947021484375, Learning Rate: 0.01\n",
      "Epoch [4498/20000], Loss: 15.64166259765625, Learning Rate: 0.01\n",
      "Epoch [4499/20000], Loss: 15.631500244140625, Learning Rate: 0.01\n",
      "Epoch [4500/20000], Loss: 15.6212158203125, Learning Rate: 0.01\n",
      "Epoch [4501/20000], Loss: 15.61102294921875, Learning Rate: 0.01\n",
      "Epoch [4502/20000], Loss: 15.600830078125, Learning Rate: 0.01\n",
      "Epoch [4503/20000], Loss: 15.590621948242188, Learning Rate: 0.01\n",
      "Epoch [4504/20000], Loss: 15.58038330078125, Learning Rate: 0.01\n",
      "Epoch [4505/20000], Loss: 15.5701904296875, Learning Rate: 0.01\n",
      "Epoch [4506/20000], Loss: 15.560150146484375, Learning Rate: 0.01\n",
      "Epoch [4507/20000], Loss: 15.5501708984375, Learning Rate: 0.01\n",
      "Epoch [4508/20000], Loss: 15.540115356445312, Learning Rate: 0.01\n",
      "Epoch [4509/20000], Loss: 15.5302734375, Learning Rate: 0.01\n",
      "Epoch [4510/20000], Loss: 15.520706176757812, Learning Rate: 0.01\n",
      "Epoch [4511/20000], Loss: 15.511505126953125, Learning Rate: 0.01\n",
      "Epoch [4512/20000], Loss: 15.502761840820312, Learning Rate: 0.01\n",
      "Epoch [4513/20000], Loss: 15.495101928710938, Learning Rate: 0.01\n",
      "Epoch [4514/20000], Loss: 15.488693237304688, Learning Rate: 0.01\n",
      "Epoch [4515/20000], Loss: 15.484695434570312, Learning Rate: 0.01\n",
      "Epoch [4516/20000], Loss: 15.484283447265625, Learning Rate: 0.01\n",
      "Epoch [4517/20000], Loss: 15.489913940429688, Learning Rate: 0.01\n",
      "Epoch [4518/20000], Loss: 15.50518798828125, Learning Rate: 0.01\n",
      "Epoch [4519/20000], Loss: 15.536636352539062, Learning Rate: 0.01\n",
      "Epoch [4520/20000], Loss: 15.594451904296875, Learning Rate: 0.01\n",
      "Epoch [4521/20000], Loss: 15.69580078125, Learning Rate: 0.01\n",
      "Epoch [4522/20000], Loss: 15.86962890625, Learning Rate: 0.01\n",
      "Epoch [4523/20000], Loss: 16.163360595703125, Learning Rate: 0.01\n",
      "Epoch [4524/20000], Loss: 16.655517578125, Learning Rate: 0.01\n",
      "Epoch [4525/20000], Loss: 17.471237182617188, Learning Rate: 0.01\n",
      "Epoch [4526/20000], Loss: 18.807281494140625, Learning Rate: 0.01\n",
      "Epoch [4527/20000], Loss: 20.940231323242188, Learning Rate: 0.01\n",
      "Epoch [4528/20000], Loss: 24.215484619140625, Learning Rate: 0.01\n",
      "Epoch [4529/20000], Loss: 28.871826171875, Learning Rate: 0.01\n",
      "Epoch [4530/20000], Loss: 34.688995361328125, Learning Rate: 0.01\n",
      "Epoch [4531/20000], Loss: 40.20899963378906, Learning Rate: 0.01\n",
      "Epoch [4532/20000], Loss: 42.55314636230469, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4533/20000], Loss: 38.63604736328125, Learning Rate: 0.01\n",
      "Epoch [4534/20000], Loss: 29.007400512695312, Learning Rate: 0.01\n",
      "Epoch [4535/20000], Loss: 19.134796142578125, Learning Rate: 0.01\n",
      "Epoch [4536/20000], Loss: 15.252685546875, Learning Rate: 0.01\n",
      "Epoch [4537/20000], Loss: 18.44921875, Learning Rate: 0.01\n",
      "Epoch [4538/20000], Loss: 24.243927001953125, Learning Rate: 0.01\n",
      "Epoch [4539/20000], Loss: 26.840316772460938, Learning Rate: 0.01\n",
      "Epoch [4540/20000], Loss: 23.848907470703125, Learning Rate: 0.01\n",
      "Epoch [4541/20000], Loss: 18.2706298828125, Learning Rate: 0.01\n",
      "Epoch [4542/20000], Loss: 15.236480712890625, Learning Rate: 0.01\n",
      "Epoch [4543/20000], Loss: 16.705230712890625, Learning Rate: 0.01\n",
      "Epoch [4544/20000], Loss: 20.005889892578125, Learning Rate: 0.01\n",
      "Epoch [4545/20000], Loss: 21.1995849609375, Learning Rate: 0.01\n",
      "Epoch [4546/20000], Loss: 19.11871337890625, Learning Rate: 0.01\n",
      "Epoch [4547/20000], Loss: 16.1263427734375, Learning Rate: 0.01\n",
      "Epoch [4548/20000], Loss: 15.172378540039062, Learning Rate: 0.01\n",
      "Epoch [4549/20000], Loss: 16.596267700195312, Learning Rate: 0.01\n",
      "Epoch [4550/20000], Loss: 18.242874145507812, Learning Rate: 0.01\n",
      "Epoch [4551/20000], Loss: 18.144851684570312, Learning Rate: 0.01\n",
      "Epoch [4552/20000], Loss: 16.526290893554688, Learning Rate: 0.01\n",
      "Epoch [4553/20000], Loss: 15.200759887695312, Learning Rate: 0.01\n",
      "Epoch [4554/20000], Loss: 15.35516357421875, Learning Rate: 0.01\n",
      "Epoch [4555/20000], Loss: 16.401107788085938, Learning Rate: 0.01\n",
      "Epoch [4556/20000], Loss: 16.93377685546875, Learning Rate: 0.01\n",
      "Epoch [4557/20000], Loss: 16.363449096679688, Learning Rate: 0.01\n",
      "Epoch [4558/20000], Loss: 15.393707275390625, Learning Rate: 0.01\n",
      "Epoch [4559/20000], Loss: 15.031143188476562, Learning Rate: 0.01\n",
      "Epoch [4560/20000], Loss: 15.45550537109375, Learning Rate: 0.01\n",
      "Epoch [4561/20000], Loss: 15.987106323242188, Learning Rate: 0.01\n",
      "Epoch [4562/20000], Loss: 15.96893310546875, Learning Rate: 0.01\n",
      "Epoch [4563/20000], Loss: 15.455764770507812, Learning Rate: 0.01\n",
      "Epoch [4564/20000], Loss: 15.01971435546875, Learning Rate: 0.01\n",
      "Epoch [4565/20000], Loss: 15.048171997070312, Learning Rate: 0.01\n",
      "Epoch [4566/20000], Loss: 15.370895385742188, Learning Rate: 0.01\n",
      "Epoch [4567/20000], Loss: 15.5467529296875, Learning Rate: 0.01\n",
      "Epoch [4568/20000], Loss: 15.376388549804688, Learning Rate: 0.01\n",
      "Epoch [4569/20000], Loss: 15.0616455078125, Learning Rate: 0.01\n",
      "Epoch [4570/20000], Loss: 14.918899536132812, Learning Rate: 0.01\n",
      "Epoch [4571/20000], Loss: 15.0283203125, Learning Rate: 0.01\n",
      "Epoch [4572/20000], Loss: 15.197280883789062, Learning Rate: 0.01\n",
      "Epoch [4573/20000], Loss: 15.211837768554688, Learning Rate: 0.01\n",
      "Epoch [4574/20000], Loss: 15.0587158203125, Learning Rate: 0.01\n",
      "Epoch [4575/20000], Loss: 14.899612426757812, Learning Rate: 0.01\n",
      "Epoch [4576/20000], Loss: 14.872543334960938, Learning Rate: 0.01\n",
      "Epoch [4577/20000], Loss: 14.9569091796875, Learning Rate: 0.01\n",
      "Epoch [4578/20000], Loss: 15.025741577148438, Learning Rate: 0.01\n",
      "Epoch [4579/20000], Loss: 14.993148803710938, Learning Rate: 0.01\n",
      "Epoch [4580/20000], Loss: 14.891921997070312, Learning Rate: 0.01\n",
      "Epoch [4581/20000], Loss: 14.817367553710938, Learning Rate: 0.01\n",
      "Epoch [4582/20000], Loss: 14.820846557617188, Learning Rate: 0.01\n",
      "Epoch [4583/20000], Loss: 14.86773681640625, Learning Rate: 0.01\n",
      "Epoch [4584/20000], Loss: 14.88885498046875, Learning Rate: 0.01\n",
      "Epoch [4585/20000], Loss: 14.853744506835938, Learning Rate: 0.01\n",
      "Epoch [4586/20000], Loss: 14.7928466796875, Learning Rate: 0.01\n",
      "Epoch [4587/20000], Loss: 14.755905151367188, Learning Rate: 0.01\n",
      "Epoch [4588/20000], Loss: 14.760513305664062, Learning Rate: 0.01\n",
      "Epoch [4589/20000], Loss: 14.78173828125, Learning Rate: 0.01\n",
      "Epoch [4590/20000], Loss: 14.784042358398438, Learning Rate: 0.01\n",
      "Epoch [4591/20000], Loss: 14.7569580078125, Learning Rate: 0.01\n",
      "Epoch [4592/20000], Loss: 14.720062255859375, Learning Rate: 0.01\n",
      "Epoch [4593/20000], Loss: 14.697967529296875, Learning Rate: 0.01\n",
      "Epoch [4594/20000], Loss: 14.697616577148438, Learning Rate: 0.01\n",
      "Epoch [4595/20000], Loss: 14.704391479492188, Learning Rate: 0.01\n",
      "Epoch [4596/20000], Loss: 14.700149536132812, Learning Rate: 0.01\n",
      "Epoch [4597/20000], Loss: 14.680892944335938, Learning Rate: 0.01\n",
      "Epoch [4598/20000], Loss: 14.656646728515625, Learning Rate: 0.01\n",
      "Epoch [4599/20000], Loss: 14.64044189453125, Learning Rate: 0.01\n",
      "Epoch [4600/20000], Loss: 14.635543823242188, Learning Rate: 0.01\n",
      "Epoch [4601/20000], Loss: 14.634628295898438, Learning Rate: 0.01\n",
      "Epoch [4602/20000], Loss: 14.628509521484375, Learning Rate: 0.01\n",
      "Epoch [4603/20000], Loss: 14.614181518554688, Learning Rate: 0.01\n",
      "Epoch [4604/20000], Loss: 14.596771240234375, Learning Rate: 0.01\n",
      "Epoch [4605/20000], Loss: 14.582901000976562, Learning Rate: 0.01\n",
      "Epoch [4606/20000], Loss: 14.574996948242188, Learning Rate: 0.01\n",
      "Epoch [4607/20000], Loss: 14.569961547851562, Learning Rate: 0.01\n",
      "Epoch [4608/20000], Loss: 14.56298828125, Learning Rate: 0.01\n",
      "Epoch [4609/20000], Loss: 14.551773071289062, Learning Rate: 0.01\n",
      "Epoch [4610/20000], Loss: 14.538040161132812, Learning Rate: 0.01\n",
      "Epoch [4611/20000], Loss: 14.52557373046875, Learning Rate: 0.01\n",
      "Epoch [4612/20000], Loss: 14.5159912109375, Learning Rate: 0.01\n",
      "Epoch [4613/20000], Loss: 14.5086669921875, Learning Rate: 0.01\n",
      "Epoch [4614/20000], Loss: 14.500930786132812, Learning Rate: 0.01\n",
      "Epoch [4615/20000], Loss: 14.491195678710938, Learning Rate: 0.01\n",
      "Epoch [4616/20000], Loss: 14.479721069335938, Learning Rate: 0.01\n",
      "Epoch [4617/20000], Loss: 14.468124389648438, Learning Rate: 0.01\n",
      "Epoch [4618/20000], Loss: 14.458023071289062, Learning Rate: 0.01\n",
      "Epoch [4619/20000], Loss: 14.449172973632812, Learning Rate: 0.01\n",
      "Epoch [4620/20000], Loss: 14.4407958984375, Learning Rate: 0.01\n",
      "Epoch [4621/20000], Loss: 14.431716918945312, Learning Rate: 0.01\n",
      "Epoch [4622/20000], Loss: 14.421478271484375, Learning Rate: 0.01\n",
      "Epoch [4623/20000], Loss: 14.410812377929688, Learning Rate: 0.01\n",
      "Epoch [4624/20000], Loss: 14.400436401367188, Learning Rate: 0.01\n",
      "Epoch [4625/20000], Loss: 14.390899658203125, Learning Rate: 0.01\n",
      "Epoch [4626/20000], Loss: 14.381851196289062, Learning Rate: 0.01\n",
      "Epoch [4627/20000], Loss: 14.3726806640625, Learning Rate: 0.01\n",
      "Epoch [4628/20000], Loss: 14.363189697265625, Learning Rate: 0.01\n",
      "Epoch [4629/20000], Loss: 14.353134155273438, Learning Rate: 0.01\n",
      "Epoch [4630/20000], Loss: 14.343063354492188, Learning Rate: 0.01\n",
      "Epoch [4631/20000], Loss: 14.333175659179688, Learning Rate: 0.01\n",
      "Epoch [4632/20000], Loss: 14.323593139648438, Learning Rate: 0.01\n",
      "Epoch [4633/20000], Loss: 14.314315795898438, Learning Rate: 0.01\n",
      "Epoch [4634/20000], Loss: 14.304901123046875, Learning Rate: 0.01\n",
      "Epoch [4635/20000], Loss: 14.29522705078125, Learning Rate: 0.01\n",
      "Epoch [4636/20000], Loss: 14.285446166992188, Learning Rate: 0.01\n",
      "Epoch [4637/20000], Loss: 14.275665283203125, Learning Rate: 0.01\n",
      "Epoch [4638/20000], Loss: 14.265884399414062, Learning Rate: 0.01\n",
      "Epoch [4639/20000], Loss: 14.256210327148438, Learning Rate: 0.01\n",
      "Epoch [4640/20000], Loss: 14.246780395507812, Learning Rate: 0.01\n",
      "Epoch [4641/20000], Loss: 14.237213134765625, Learning Rate: 0.01\n",
      "Epoch [4642/20000], Loss: 14.227706909179688, Learning Rate: 0.01\n",
      "Epoch [4643/20000], Loss: 14.218002319335938, Learning Rate: 0.01\n",
      "Epoch [4644/20000], Loss: 14.208160400390625, Learning Rate: 0.01\n",
      "Epoch [4645/20000], Loss: 14.198501586914062, Learning Rate: 0.01\n",
      "Epoch [4646/20000], Loss: 14.188995361328125, Learning Rate: 0.01\n",
      "Epoch [4647/20000], Loss: 14.179351806640625, Learning Rate: 0.01\n",
      "Epoch [4648/20000], Loss: 14.169845581054688, Learning Rate: 0.01\n",
      "Epoch [4649/20000], Loss: 14.160202026367188, Learning Rate: 0.01\n",
      "Epoch [4650/20000], Loss: 14.150543212890625, Learning Rate: 0.01\n",
      "Epoch [4651/20000], Loss: 14.140869140625, Learning Rate: 0.01\n",
      "Epoch [4652/20000], Loss: 14.131195068359375, Learning Rate: 0.01\n",
      "Epoch [4653/20000], Loss: 14.121597290039062, Learning Rate: 0.01\n",
      "Epoch [4654/20000], Loss: 14.112014770507812, Learning Rate: 0.01\n",
      "Epoch [4655/20000], Loss: 14.102432250976562, Learning Rate: 0.01\n",
      "Epoch [4656/20000], Loss: 14.092788696289062, Learning Rate: 0.01\n",
      "Epoch [4657/20000], Loss: 14.083236694335938, Learning Rate: 0.01\n",
      "Epoch [4658/20000], Loss: 14.073455810546875, Learning Rate: 0.01\n",
      "Epoch [4659/20000], Loss: 14.063812255859375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4660/20000], Loss: 14.054275512695312, Learning Rate: 0.01\n",
      "Epoch [4661/20000], Loss: 14.0445556640625, Learning Rate: 0.01\n",
      "Epoch [4662/20000], Loss: 14.034988403320312, Learning Rate: 0.01\n",
      "Epoch [4663/20000], Loss: 14.025360107421875, Learning Rate: 0.01\n",
      "Epoch [4664/20000], Loss: 14.015670776367188, Learning Rate: 0.01\n",
      "Epoch [4665/20000], Loss: 14.00616455078125, Learning Rate: 0.01\n",
      "Epoch [4666/20000], Loss: 13.996414184570312, Learning Rate: 0.01\n",
      "Epoch [4667/20000], Loss: 13.986862182617188, Learning Rate: 0.01\n",
      "Epoch [4668/20000], Loss: 13.977188110351562, Learning Rate: 0.01\n",
      "Epoch [4669/20000], Loss: 13.967575073242188, Learning Rate: 0.01\n",
      "Epoch [4670/20000], Loss: 13.957916259765625, Learning Rate: 0.01\n",
      "Epoch [4671/20000], Loss: 13.948348999023438, Learning Rate: 0.01\n",
      "Epoch [4672/20000], Loss: 13.938705444335938, Learning Rate: 0.01\n",
      "Epoch [4673/20000], Loss: 13.929000854492188, Learning Rate: 0.01\n",
      "Epoch [4674/20000], Loss: 13.91943359375, Learning Rate: 0.01\n",
      "Epoch [4675/20000], Loss: 13.909698486328125, Learning Rate: 0.01\n",
      "Epoch [4676/20000], Loss: 13.90020751953125, Learning Rate: 0.01\n",
      "Epoch [4677/20000], Loss: 13.890594482421875, Learning Rate: 0.01\n",
      "Epoch [4678/20000], Loss: 13.880874633789062, Learning Rate: 0.01\n",
      "Epoch [4679/20000], Loss: 13.871307373046875, Learning Rate: 0.01\n",
      "Epoch [4680/20000], Loss: 13.861648559570312, Learning Rate: 0.01\n",
      "Epoch [4681/20000], Loss: 13.851959228515625, Learning Rate: 0.01\n",
      "Epoch [4682/20000], Loss: 13.842300415039062, Learning Rate: 0.01\n",
      "Epoch [4683/20000], Loss: 13.832672119140625, Learning Rate: 0.01\n",
      "Epoch [4684/20000], Loss: 13.823043823242188, Learning Rate: 0.01\n",
      "Epoch [4685/20000], Loss: 13.8134765625, Learning Rate: 0.01\n",
      "Epoch [4686/20000], Loss: 13.803817749023438, Learning Rate: 0.01\n",
      "Epoch [4687/20000], Loss: 13.794113159179688, Learning Rate: 0.01\n",
      "Epoch [4688/20000], Loss: 13.78448486328125, Learning Rate: 0.01\n",
      "Epoch [4689/20000], Loss: 13.774826049804688, Learning Rate: 0.01\n",
      "Epoch [4690/20000], Loss: 13.765228271484375, Learning Rate: 0.01\n",
      "Epoch [4691/20000], Loss: 13.755523681640625, Learning Rate: 0.01\n",
      "Epoch [4692/20000], Loss: 13.745986938476562, Learning Rate: 0.01\n",
      "Epoch [4693/20000], Loss: 13.73626708984375, Learning Rate: 0.01\n",
      "Epoch [4694/20000], Loss: 13.726669311523438, Learning Rate: 0.01\n",
      "Epoch [4695/20000], Loss: 13.717010498046875, Learning Rate: 0.01\n",
      "Epoch [4696/20000], Loss: 13.707427978515625, Learning Rate: 0.01\n",
      "Epoch [4697/20000], Loss: 13.697708129882812, Learning Rate: 0.01\n",
      "Epoch [4698/20000], Loss: 13.68817138671875, Learning Rate: 0.01\n",
      "Epoch [4699/20000], Loss: 13.678512573242188, Learning Rate: 0.01\n",
      "Epoch [4700/20000], Loss: 13.668853759765625, Learning Rate: 0.01\n",
      "Epoch [4701/20000], Loss: 13.659164428710938, Learning Rate: 0.01\n",
      "Epoch [4702/20000], Loss: 13.649566650390625, Learning Rate: 0.01\n",
      "Epoch [4703/20000], Loss: 13.639846801757812, Learning Rate: 0.01\n",
      "Epoch [4704/20000], Loss: 13.630218505859375, Learning Rate: 0.01\n",
      "Epoch [4705/20000], Loss: 13.620651245117188, Learning Rate: 0.01\n",
      "Epoch [4706/20000], Loss: 13.610992431640625, Learning Rate: 0.01\n",
      "Epoch [4707/20000], Loss: 13.60137939453125, Learning Rate: 0.01\n",
      "Epoch [4708/20000], Loss: 13.59161376953125, Learning Rate: 0.01\n",
      "Epoch [4709/20000], Loss: 13.581924438476562, Learning Rate: 0.01\n",
      "Epoch [4710/20000], Loss: 13.572402954101562, Learning Rate: 0.01\n",
      "Epoch [4711/20000], Loss: 13.562728881835938, Learning Rate: 0.01\n",
      "Epoch [4712/20000], Loss: 13.55303955078125, Learning Rate: 0.01\n",
      "Epoch [4713/20000], Loss: 13.543365478515625, Learning Rate: 0.01\n",
      "Epoch [4714/20000], Loss: 13.53369140625, Learning Rate: 0.01\n",
      "Epoch [4715/20000], Loss: 13.524093627929688, Learning Rate: 0.01\n",
      "Epoch [4716/20000], Loss: 13.514495849609375, Learning Rate: 0.01\n",
      "Epoch [4717/20000], Loss: 13.504852294921875, Learning Rate: 0.01\n",
      "Epoch [4718/20000], Loss: 13.495132446289062, Learning Rate: 0.01\n",
      "Epoch [4719/20000], Loss: 13.48541259765625, Learning Rate: 0.01\n",
      "Epoch [4720/20000], Loss: 13.47589111328125, Learning Rate: 0.01\n",
      "Epoch [4721/20000], Loss: 13.466201782226562, Learning Rate: 0.01\n",
      "Epoch [4722/20000], Loss: 13.456558227539062, Learning Rate: 0.01\n",
      "Epoch [4723/20000], Loss: 13.446884155273438, Learning Rate: 0.01\n",
      "Epoch [4724/20000], Loss: 13.437301635742188, Learning Rate: 0.01\n",
      "Epoch [4725/20000], Loss: 13.427566528320312, Learning Rate: 0.01\n",
      "Epoch [4726/20000], Loss: 13.417938232421875, Learning Rate: 0.01\n",
      "Epoch [4727/20000], Loss: 13.4083251953125, Learning Rate: 0.01\n",
      "Epoch [4728/20000], Loss: 13.398574829101562, Learning Rate: 0.01\n",
      "Epoch [4729/20000], Loss: 13.3890380859375, Learning Rate: 0.01\n",
      "Epoch [4730/20000], Loss: 13.379287719726562, Learning Rate: 0.01\n",
      "Epoch [4731/20000], Loss: 13.369674682617188, Learning Rate: 0.01\n",
      "Epoch [4732/20000], Loss: 13.360000610351562, Learning Rate: 0.01\n",
      "Epoch [4733/20000], Loss: 13.350357055664062, Learning Rate: 0.01\n",
      "Epoch [4734/20000], Loss: 13.340682983398438, Learning Rate: 0.01\n",
      "Epoch [4735/20000], Loss: 13.330963134765625, Learning Rate: 0.01\n",
      "Epoch [4736/20000], Loss: 13.321334838867188, Learning Rate: 0.01\n",
      "Epoch [4737/20000], Loss: 13.311630249023438, Learning Rate: 0.01\n",
      "Epoch [4738/20000], Loss: 13.302093505859375, Learning Rate: 0.01\n",
      "Epoch [4739/20000], Loss: 13.292465209960938, Learning Rate: 0.01\n",
      "Epoch [4740/20000], Loss: 13.282760620117188, Learning Rate: 0.01\n",
      "Epoch [4741/20000], Loss: 13.273147583007812, Learning Rate: 0.01\n",
      "Epoch [4742/20000], Loss: 13.263473510742188, Learning Rate: 0.01\n",
      "Epoch [4743/20000], Loss: 13.2537841796875, Learning Rate: 0.01\n",
      "Epoch [4744/20000], Loss: 13.244171142578125, Learning Rate: 0.01\n",
      "Epoch [4745/20000], Loss: 13.234375, Learning Rate: 0.01\n",
      "Epoch [4746/20000], Loss: 13.224853515625, Learning Rate: 0.01\n",
      "Epoch [4747/20000], Loss: 13.215179443359375, Learning Rate: 0.01\n",
      "Epoch [4748/20000], Loss: 13.205459594726562, Learning Rate: 0.01\n",
      "Epoch [4749/20000], Loss: 13.195892333984375, Learning Rate: 0.01\n",
      "Epoch [4750/20000], Loss: 13.18621826171875, Learning Rate: 0.01\n",
      "Epoch [4751/20000], Loss: 13.176559448242188, Learning Rate: 0.01\n",
      "Epoch [4752/20000], Loss: 13.166885375976562, Learning Rate: 0.01\n",
      "Epoch [4753/20000], Loss: 13.157180786132812, Learning Rate: 0.01\n",
      "Epoch [4754/20000], Loss: 13.147598266601562, Learning Rate: 0.01\n",
      "Epoch [4755/20000], Loss: 13.137802124023438, Learning Rate: 0.01\n",
      "Epoch [4756/20000], Loss: 13.128143310546875, Learning Rate: 0.01\n",
      "Epoch [4757/20000], Loss: 13.118515014648438, Learning Rate: 0.01\n",
      "Epoch [4758/20000], Loss: 13.10882568359375, Learning Rate: 0.01\n",
      "Epoch [4759/20000], Loss: 13.099227905273438, Learning Rate: 0.01\n",
      "Epoch [4760/20000], Loss: 13.089508056640625, Learning Rate: 0.01\n",
      "Epoch [4761/20000], Loss: 13.079879760742188, Learning Rate: 0.01\n",
      "Epoch [4762/20000], Loss: 13.0701904296875, Learning Rate: 0.01\n",
      "Epoch [4763/20000], Loss: 13.060577392578125, Learning Rate: 0.01\n",
      "Epoch [4764/20000], Loss: 13.0509033203125, Learning Rate: 0.01\n",
      "Epoch [4765/20000], Loss: 13.041305541992188, Learning Rate: 0.01\n",
      "Epoch [4766/20000], Loss: 13.031509399414062, Learning Rate: 0.01\n",
      "Epoch [4767/20000], Loss: 13.0218505859375, Learning Rate: 0.01\n",
      "Epoch [4768/20000], Loss: 13.012283325195312, Learning Rate: 0.01\n",
      "Epoch [4769/20000], Loss: 13.002578735351562, Learning Rate: 0.01\n",
      "Epoch [4770/20000], Loss: 12.992889404296875, Learning Rate: 0.01\n",
      "Epoch [4771/20000], Loss: 12.983200073242188, Learning Rate: 0.01\n",
      "Epoch [4772/20000], Loss: 12.9736328125, Learning Rate: 0.01\n",
      "Epoch [4773/20000], Loss: 12.96392822265625, Learning Rate: 0.01\n",
      "Epoch [4774/20000], Loss: 12.954269409179688, Learning Rate: 0.01\n",
      "Epoch [4775/20000], Loss: 12.944564819335938, Learning Rate: 0.01\n",
      "Epoch [4776/20000], Loss: 12.934967041015625, Learning Rate: 0.01\n",
      "Epoch [4777/20000], Loss: 12.925323486328125, Learning Rate: 0.01\n",
      "Epoch [4778/20000], Loss: 12.915664672851562, Learning Rate: 0.01\n",
      "Epoch [4779/20000], Loss: 12.905990600585938, Learning Rate: 0.01\n",
      "Epoch [4780/20000], Loss: 12.896530151367188, Learning Rate: 0.01\n",
      "Epoch [4781/20000], Loss: 12.886947631835938, Learning Rate: 0.01\n",
      "Epoch [4782/20000], Loss: 12.87738037109375, Learning Rate: 0.01\n",
      "Epoch [4783/20000], Loss: 12.868087768554688, Learning Rate: 0.01\n",
      "Epoch [4784/20000], Loss: 12.858673095703125, Learning Rate: 0.01\n",
      "Epoch [4785/20000], Loss: 12.849700927734375, Learning Rate: 0.01\n",
      "Epoch [4786/20000], Loss: 12.841033935546875, Learning Rate: 0.01\n",
      "Epoch [4787/20000], Loss: 12.833023071289062, Learning Rate: 0.01\n",
      "Epoch [4788/20000], Loss: 12.82568359375, Learning Rate: 0.01\n",
      "Epoch [4789/20000], Loss: 12.819808959960938, Learning Rate: 0.01\n",
      "Epoch [4790/20000], Loss: 12.816314697265625, Learning Rate: 0.01\n",
      "Epoch [4791/20000], Loss: 12.81640625, Learning Rate: 0.01\n",
      "Epoch [4792/20000], Loss: 12.8226318359375, Learning Rate: 0.01\n",
      "Epoch [4793/20000], Loss: 12.838546752929688, Learning Rate: 0.01\n",
      "Epoch [4794/20000], Loss: 12.870758056640625, Learning Rate: 0.01\n",
      "Epoch [4795/20000], Loss: 12.9296875, Learning Rate: 0.01\n",
      "Epoch [4796/20000], Loss: 13.032882690429688, Learning Rate: 0.01\n",
      "Epoch [4797/20000], Loss: 13.209930419921875, Learning Rate: 0.01\n",
      "Epoch [4798/20000], Loss: 13.509735107421875, Learning Rate: 0.01\n",
      "Epoch [4799/20000], Loss: 14.013214111328125, Learning Rate: 0.01\n",
      "Epoch [4800/20000], Loss: 14.850357055664062, Learning Rate: 0.01\n",
      "Epoch [4801/20000], Loss: 16.225830078125, Learning Rate: 0.01\n",
      "Epoch [4802/20000], Loss: 18.430221557617188, Learning Rate: 0.01\n",
      "Epoch [4803/20000], Loss: 21.829193115234375, Learning Rate: 0.01\n",
      "Epoch [4804/20000], Loss: 26.683731079101562, Learning Rate: 0.01\n",
      "Epoch [4805/20000], Loss: 32.7784423828125, Learning Rate: 0.01\n",
      "Epoch [4806/20000], Loss: 38.59318542480469, Learning Rate: 0.01\n",
      "Epoch [4807/20000], Loss: 41.08428955078125, Learning Rate: 0.01\n",
      "Epoch [4808/20000], Loss: 36.966064453125, Learning Rate: 0.01\n",
      "Epoch [4809/20000], Loss: 26.825424194335938, Learning Rate: 0.01\n",
      "Epoch [4810/20000], Loss: 16.5054931640625, Learning Rate: 0.01\n",
      "Epoch [4811/20000], Loss: 12.600540161132812, Learning Rate: 0.01\n",
      "Epoch [4812/20000], Loss: 16.133544921875, Learning Rate: 0.01\n",
      "Epoch [4813/20000], Loss: 22.217514038085938, Learning Rate: 0.01\n",
      "Epoch [4814/20000], Loss: 24.7127685546875, Learning Rate: 0.01\n",
      "Epoch [4815/20000], Loss: 21.30047607421875, Learning Rate: 0.01\n",
      "Epoch [4816/20000], Loss: 15.432968139648438, Learning Rate: 0.01\n",
      "Epoch [4817/20000], Loss: 12.549362182617188, Learning Rate: 0.01\n",
      "Epoch [4818/20000], Loss: 14.398605346679688, Learning Rate: 0.01\n",
      "Epoch [4819/20000], Loss: 17.849639892578125, Learning Rate: 0.01\n",
      "Epoch [4820/20000], Loss: 18.779251098632812, Learning Rate: 0.01\n",
      "Epoch [4821/20000], Loss: 16.318328857421875, Learning Rate: 0.01\n",
      "Epoch [4822/20000], Loss: 13.263259887695312, Learning Rate: 0.01\n",
      "Epoch [4823/20000], Loss: 12.607772827148438, Learning Rate: 0.01\n",
      "Epoch [4824/20000], Loss: 14.312545776367188, Learning Rate: 0.01\n",
      "Epoch [4825/20000], Loss: 15.892486572265625, Learning Rate: 0.01\n",
      "Epoch [4826/20000], Loss: 15.47198486328125, Learning Rate: 0.01\n",
      "Epoch [4827/20000], Loss: 13.657562255859375, Learning Rate: 0.01\n",
      "Epoch [4828/20000], Loss: 12.472091674804688, Learning Rate: 0.01\n",
      "Epoch [4829/20000], Loss: 12.905075073242188, Learning Rate: 0.01\n",
      "Epoch [4830/20000], Loss: 14.031829833984375, Learning Rate: 0.01\n",
      "Epoch [4831/20000], Loss: 14.370269775390625, Learning Rate: 0.01\n",
      "Epoch [4832/20000], Loss: 13.574813842773438, Learning Rate: 0.01\n",
      "Epoch [4833/20000], Loss: 12.60479736328125, Learning Rate: 0.01\n",
      "Epoch [4834/20000], Loss: 12.439544677734375, Learning Rate: 0.01\n",
      "Epoch [4835/20000], Loss: 13.010955810546875, Learning Rate: 0.01\n",
      "Epoch [4836/20000], Loss: 13.477569580078125, Learning Rate: 0.01\n",
      "Epoch [4837/20000], Loss: 13.277618408203125, Learning Rate: 0.01\n",
      "Epoch [4838/20000], Loss: 12.681045532226562, Learning Rate: 0.01\n",
      "Epoch [4839/20000], Loss: 12.339508056640625, Learning Rate: 0.01\n",
      "Epoch [4840/20000], Loss: 12.511947631835938, Learning Rate: 0.01\n",
      "Epoch [4841/20000], Loss: 12.862091064453125, Learning Rate: 0.01\n",
      "Epoch [4842/20000], Loss: 12.932907104492188, Learning Rate: 0.01\n",
      "Epoch [4843/20000], Loss: 12.656219482421875, Learning Rate: 0.01\n",
      "Epoch [4844/20000], Loss: 12.349456787109375, Learning Rate: 0.01\n",
      "Epoch [4845/20000], Loss: 12.304534912109375, Learning Rate: 0.01\n",
      "Epoch [4846/20000], Loss: 12.483139038085938, Learning Rate: 0.01\n",
      "Epoch [4847/20000], Loss: 12.62237548828125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4848/20000], Loss: 12.553375244140625, Learning Rate: 0.01\n",
      "Epoch [4849/20000], Loss: 12.359146118164062, Learning Rate: 0.01\n",
      "Epoch [4850/20000], Loss: 12.238998413085938, Learning Rate: 0.01\n",
      "Epoch [4851/20000], Loss: 12.27838134765625, Learning Rate: 0.01\n",
      "Epoch [4852/20000], Loss: 12.383407592773438, Learning Rate: 0.01\n",
      "Epoch [4853/20000], Loss: 12.412384033203125, Learning Rate: 0.01\n",
      "Epoch [4854/20000], Loss: 12.3297119140625, Learning Rate: 0.01\n",
      "Epoch [4855/20000], Loss: 12.221755981445312, Learning Rate: 0.01\n",
      "Epoch [4856/20000], Loss: 12.18450927734375, Learning Rate: 0.01\n",
      "Epoch [4857/20000], Loss: 12.224884033203125, Learning Rate: 0.01\n",
      "Epoch [4858/20000], Loss: 12.271469116210938, Learning Rate: 0.01\n",
      "Epoch [4859/20000], Loss: 12.260986328125, Learning Rate: 0.01\n",
      "Epoch [4860/20000], Loss: 12.200149536132812, Learning Rate: 0.01\n",
      "Epoch [4861/20000], Loss: 12.144332885742188, Learning Rate: 0.01\n",
      "Epoch [4862/20000], Loss: 12.134124755859375, Learning Rate: 0.01\n",
      "Epoch [4863/20000], Loss: 12.157608032226562, Learning Rate: 0.01\n",
      "Epoch [4864/20000], Loss: 12.173141479492188, Learning Rate: 0.01\n",
      "Epoch [4865/20000], Loss: 12.155136108398438, Learning Rate: 0.01\n",
      "Epoch [4866/20000], Loss: 12.1153564453125, Learning Rate: 0.01\n",
      "Epoch [4867/20000], Loss: 12.084884643554688, Learning Rate: 0.01\n",
      "Epoch [4868/20000], Loss: 12.080078125, Learning Rate: 0.01\n",
      "Epoch [4869/20000], Loss: 12.089645385742188, Learning Rate: 0.01\n",
      "Epoch [4870/20000], Loss: 12.091415405273438, Learning Rate: 0.01\n",
      "Epoch [4871/20000], Loss: 12.07501220703125, Learning Rate: 0.01\n",
      "Epoch [4872/20000], Loss: 12.04913330078125, Learning Rate: 0.01\n",
      "Epoch [4873/20000], Loss: 12.029876708984375, Learning Rate: 0.01\n",
      "Epoch [4874/20000], Loss: 12.024169921875, Learning Rate: 0.01\n",
      "Epoch [4875/20000], Loss: 12.025177001953125, Learning Rate: 0.01\n",
      "Epoch [4876/20000], Loss: 12.021591186523438, Learning Rate: 0.01\n",
      "Epoch [4877/20000], Loss: 12.008255004882812, Learning Rate: 0.01\n",
      "Epoch [4878/20000], Loss: 11.9903564453125, Learning Rate: 0.01\n",
      "Epoch [4879/20000], Loss: 11.975784301757812, Learning Rate: 0.01\n",
      "Epoch [4880/20000], Loss: 11.968338012695312, Learning Rate: 0.01\n",
      "Epoch [4881/20000], Loss: 11.964630126953125, Learning Rate: 0.01\n",
      "Epoch [4882/20000], Loss: 11.958740234375, Learning Rate: 0.01\n",
      "Epoch [4883/20000], Loss: 11.947830200195312, Learning Rate: 0.01\n",
      "Epoch [4884/20000], Loss: 11.934249877929688, Learning Rate: 0.01\n",
      "Epoch [4885/20000], Loss: 11.921829223632812, Learning Rate: 0.01\n",
      "Epoch [4886/20000], Loss: 11.9132080078125, Learning Rate: 0.01\n",
      "Epoch [4887/20000], Loss: 11.9068603515625, Learning Rate: 0.01\n",
      "Epoch [4888/20000], Loss: 11.899856567382812, Learning Rate: 0.01\n",
      "Epoch [4889/20000], Loss: 11.890365600585938, Learning Rate: 0.01\n",
      "Epoch [4890/20000], Loss: 11.879074096679688, Learning Rate: 0.01\n",
      "Epoch [4891/20000], Loss: 11.868057250976562, Learning Rate: 0.01\n",
      "Epoch [4892/20000], Loss: 11.858566284179688, Learning Rate: 0.01\n",
      "Epoch [4893/20000], Loss: 11.850814819335938, Learning Rate: 0.01\n",
      "Epoch [4894/20000], Loss: 11.843154907226562, Learning Rate: 0.01\n",
      "Epoch [4895/20000], Loss: 11.8343505859375, Learning Rate: 0.01\n",
      "Epoch [4896/20000], Loss: 11.824432373046875, Learning Rate: 0.01\n",
      "Epoch [4897/20000], Loss: 11.814163208007812, Learning Rate: 0.01\n",
      "Epoch [4898/20000], Loss: 11.804534912109375, Learning Rate: 0.01\n",
      "Epoch [4899/20000], Loss: 11.795822143554688, Learning Rate: 0.01\n",
      "Epoch [4900/20000], Loss: 11.78759765625, Learning Rate: 0.01\n",
      "Epoch [4901/20000], Loss: 11.779098510742188, Learning Rate: 0.01\n",
      "Epoch [4902/20000], Loss: 11.769805908203125, Learning Rate: 0.01\n",
      "Epoch [4903/20000], Loss: 11.760223388671875, Learning Rate: 0.01\n",
      "Epoch [4904/20000], Loss: 11.750732421875, Learning Rate: 0.01\n",
      "Epoch [4905/20000], Loss: 11.741653442382812, Learning Rate: 0.01\n",
      "Epoch [4906/20000], Loss: 11.73291015625, Learning Rate: 0.01\n",
      "Epoch [4907/20000], Loss: 11.724212646484375, Learning Rate: 0.01\n",
      "Epoch [4908/20000], Loss: 11.715316772460938, Learning Rate: 0.01\n",
      "Epoch [4909/20000], Loss: 11.706207275390625, Learning Rate: 0.01\n",
      "Epoch [4910/20000], Loss: 11.696868896484375, Learning Rate: 0.01\n",
      "Epoch [4911/20000], Loss: 11.687667846679688, Learning Rate: 0.01\n",
      "Epoch [4912/20000], Loss: 11.67864990234375, Learning Rate: 0.01\n",
      "Epoch [4913/20000], Loss: 11.669815063476562, Learning Rate: 0.01\n",
      "Epoch [4914/20000], Loss: 11.660964965820312, Learning Rate: 0.01\n",
      "Epoch [4915/20000], Loss: 11.652114868164062, Learning Rate: 0.01\n",
      "Epoch [4916/20000], Loss: 11.642807006835938, Learning Rate: 0.01\n",
      "Epoch [4917/20000], Loss: 11.633697509765625, Learning Rate: 0.01\n",
      "Epoch [4918/20000], Loss: 11.624557495117188, Learning Rate: 0.01\n",
      "Epoch [4919/20000], Loss: 11.615707397460938, Learning Rate: 0.01\n",
      "Epoch [4920/20000], Loss: 11.606719970703125, Learning Rate: 0.01\n",
      "Epoch [4921/20000], Loss: 11.597793579101562, Learning Rate: 0.01\n",
      "Epoch [4922/20000], Loss: 11.588821411132812, Learning Rate: 0.01\n",
      "Epoch [4923/20000], Loss: 11.57977294921875, Learning Rate: 0.01\n",
      "Epoch [4924/20000], Loss: 11.570755004882812, Learning Rate: 0.01\n",
      "Epoch [4925/20000], Loss: 11.561676025390625, Learning Rate: 0.01\n",
      "Epoch [4926/20000], Loss: 11.552627563476562, Learning Rate: 0.01\n",
      "Epoch [4927/20000], Loss: 11.543685913085938, Learning Rate: 0.01\n",
      "Epoch [4928/20000], Loss: 11.534713745117188, Learning Rate: 0.01\n",
      "Epoch [4929/20000], Loss: 11.525711059570312, Learning Rate: 0.01\n",
      "Epoch [4930/20000], Loss: 11.516647338867188, Learning Rate: 0.01\n",
      "Epoch [4931/20000], Loss: 11.507644653320312, Learning Rate: 0.01\n",
      "Epoch [4932/20000], Loss: 11.49871826171875, Learning Rate: 0.01\n",
      "Epoch [4933/20000], Loss: 11.489578247070312, Learning Rate: 0.01\n",
      "Epoch [4934/20000], Loss: 11.480621337890625, Learning Rate: 0.01\n",
      "Epoch [4935/20000], Loss: 11.471649169921875, Learning Rate: 0.01\n",
      "Epoch [4936/20000], Loss: 11.462677001953125, Learning Rate: 0.01\n",
      "Epoch [4937/20000], Loss: 11.453689575195312, Learning Rate: 0.01\n",
      "Epoch [4938/20000], Loss: 11.444686889648438, Learning Rate: 0.01\n",
      "Epoch [4939/20000], Loss: 11.435745239257812, Learning Rate: 0.01\n",
      "Epoch [4940/20000], Loss: 11.4266357421875, Learning Rate: 0.01\n",
      "Epoch [4941/20000], Loss: 11.417617797851562, Learning Rate: 0.01\n",
      "Epoch [4942/20000], Loss: 11.4085693359375, Learning Rate: 0.01\n",
      "Epoch [4943/20000], Loss: 11.399642944335938, Learning Rate: 0.01\n",
      "Epoch [4944/20000], Loss: 11.390670776367188, Learning Rate: 0.01\n",
      "Epoch [4945/20000], Loss: 11.381683349609375, Learning Rate: 0.01\n",
      "Epoch [4946/20000], Loss: 11.372695922851562, Learning Rate: 0.01\n",
      "Epoch [4947/20000], Loss: 11.363571166992188, Learning Rate: 0.01\n",
      "Epoch [4948/20000], Loss: 11.35467529296875, Learning Rate: 0.01\n",
      "Epoch [4949/20000], Loss: 11.345657348632812, Learning Rate: 0.01\n",
      "Epoch [4950/20000], Loss: 11.336654663085938, Learning Rate: 0.01\n",
      "Epoch [4951/20000], Loss: 11.327606201171875, Learning Rate: 0.01\n",
      "Epoch [4952/20000], Loss: 11.31866455078125, Learning Rate: 0.01\n",
      "Epoch [4953/20000], Loss: 11.30963134765625, Learning Rate: 0.01\n",
      "Epoch [4954/20000], Loss: 11.3006591796875, Learning Rate: 0.01\n",
      "Epoch [4955/20000], Loss: 11.291656494140625, Learning Rate: 0.01\n",
      "Epoch [4956/20000], Loss: 11.282669067382812, Learning Rate: 0.01\n",
      "Epoch [4957/20000], Loss: 11.273696899414062, Learning Rate: 0.01\n",
      "Epoch [4958/20000], Loss: 11.264617919921875, Learning Rate: 0.01\n",
      "Epoch [4959/20000], Loss: 11.25555419921875, Learning Rate: 0.01\n",
      "Epoch [4960/20000], Loss: 11.24664306640625, Learning Rate: 0.01\n",
      "Epoch [4961/20000], Loss: 11.237594604492188, Learning Rate: 0.01\n",
      "Epoch [4962/20000], Loss: 11.228744506835938, Learning Rate: 0.01\n",
      "Epoch [4963/20000], Loss: 11.219635009765625, Learning Rate: 0.01\n",
      "Epoch [4964/20000], Loss: 11.210678100585938, Learning Rate: 0.01\n",
      "Epoch [4965/20000], Loss: 11.201705932617188, Learning Rate: 0.01\n",
      "Epoch [4966/20000], Loss: 11.192657470703125, Learning Rate: 0.01\n",
      "Epoch [4967/20000], Loss: 11.183609008789062, Learning Rate: 0.01\n",
      "Epoch [4968/20000], Loss: 11.174728393554688, Learning Rate: 0.01\n",
      "Epoch [4969/20000], Loss: 11.165679931640625, Learning Rate: 0.01\n",
      "Epoch [4970/20000], Loss: 11.156661987304688, Learning Rate: 0.01\n",
      "Epoch [4971/20000], Loss: 11.1475830078125, Learning Rate: 0.01\n",
      "Epoch [4972/20000], Loss: 11.13861083984375, Learning Rate: 0.01\n",
      "Epoch [4973/20000], Loss: 11.129547119140625, Learning Rate: 0.01\n",
      "Epoch [4974/20000], Loss: 11.120651245117188, Learning Rate: 0.01\n",
      "Epoch [4975/20000], Loss: 11.111785888671875, Learning Rate: 0.01\n",
      "Epoch [4976/20000], Loss: 11.102691650390625, Learning Rate: 0.01\n",
      "Epoch [4977/20000], Loss: 11.093719482421875, Learning Rate: 0.01\n",
      "Epoch [4978/20000], Loss: 11.084747314453125, Learning Rate: 0.01\n",
      "Epoch [4979/20000], Loss: 11.075759887695312, Learning Rate: 0.01\n",
      "Epoch [4980/20000], Loss: 11.066650390625, Learning Rate: 0.01\n",
      "Epoch [4981/20000], Loss: 11.05767822265625, Learning Rate: 0.01\n",
      "Epoch [4982/20000], Loss: 11.048629760742188, Learning Rate: 0.01\n",
      "Epoch [4983/20000], Loss: 11.039688110351562, Learning Rate: 0.01\n",
      "Epoch [4984/20000], Loss: 11.030685424804688, Learning Rate: 0.01\n",
      "Epoch [4985/20000], Loss: 11.021697998046875, Learning Rate: 0.01\n",
      "Epoch [4986/20000], Loss: 11.012664794921875, Learning Rate: 0.01\n",
      "Epoch [4987/20000], Loss: 11.003677368164062, Learning Rate: 0.01\n",
      "Epoch [4988/20000], Loss: 10.9947509765625, Learning Rate: 0.01\n",
      "Epoch [4989/20000], Loss: 10.985702514648438, Learning Rate: 0.01\n",
      "Epoch [4990/20000], Loss: 10.976669311523438, Learning Rate: 0.01\n",
      "Epoch [4991/20000], Loss: 10.967742919921875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4992/20000], Loss: 10.958755493164062, Learning Rate: 0.01\n",
      "Epoch [4993/20000], Loss: 10.949676513671875, Learning Rate: 0.01\n",
      "Epoch [4994/20000], Loss: 10.940719604492188, Learning Rate: 0.01\n",
      "Epoch [4995/20000], Loss: 10.931747436523438, Learning Rate: 0.01\n",
      "Epoch [4996/20000], Loss: 10.922760009765625, Learning Rate: 0.01\n",
      "Epoch [4997/20000], Loss: 10.913665771484375, Learning Rate: 0.01\n",
      "Epoch [4998/20000], Loss: 10.904769897460938, Learning Rate: 0.01\n",
      "Epoch [4999/20000], Loss: 10.895706176757812, Learning Rate: 0.01\n",
      "Epoch [5000/20000], Loss: 10.886734008789062, Learning Rate: 0.01\n",
      "Epoch [5001/20000], Loss: 10.87774658203125, Learning Rate: 0.01\n",
      "Epoch [5002/20000], Loss: 10.868743896484375, Learning Rate: 0.01\n",
      "Epoch [5003/20000], Loss: 10.859771728515625, Learning Rate: 0.01\n",
      "Epoch [5004/20000], Loss: 10.850799560546875, Learning Rate: 0.01\n",
      "Epoch [5005/20000], Loss: 10.841751098632812, Learning Rate: 0.01\n",
      "Epoch [5006/20000], Loss: 10.83282470703125, Learning Rate: 0.01\n",
      "Epoch [5007/20000], Loss: 10.823837280273438, Learning Rate: 0.01\n",
      "Epoch [5008/20000], Loss: 10.814788818359375, Learning Rate: 0.01\n",
      "Epoch [5009/20000], Loss: 10.805892944335938, Learning Rate: 0.01\n",
      "Epoch [5010/20000], Loss: 10.796859741210938, Learning Rate: 0.01\n",
      "Epoch [5011/20000], Loss: 10.787918090820312, Learning Rate: 0.01\n",
      "Epoch [5012/20000], Loss: 10.778839111328125, Learning Rate: 0.01\n",
      "Epoch [5013/20000], Loss: 10.769851684570312, Learning Rate: 0.01\n",
      "Epoch [5014/20000], Loss: 10.760787963867188, Learning Rate: 0.01\n",
      "Epoch [5015/20000], Loss: 10.751846313476562, Learning Rate: 0.01\n",
      "Epoch [5016/20000], Loss: 10.742919921875, Learning Rate: 0.01\n",
      "Epoch [5017/20000], Loss: 10.733917236328125, Learning Rate: 0.01\n",
      "Epoch [5018/20000], Loss: 10.725006103515625, Learning Rate: 0.01\n",
      "Epoch [5019/20000], Loss: 10.715896606445312, Learning Rate: 0.01\n",
      "Epoch [5020/20000], Loss: 10.706893920898438, Learning Rate: 0.01\n",
      "Epoch [5021/20000], Loss: 10.69793701171875, Learning Rate: 0.01\n",
      "Epoch [5022/20000], Loss: 10.688919067382812, Learning Rate: 0.01\n",
      "Epoch [5023/20000], Loss: 10.680038452148438, Learning Rate: 0.01\n",
      "Epoch [5024/20000], Loss: 10.670928955078125, Learning Rate: 0.01\n",
      "Epoch [5025/20000], Loss: 10.661972045898438, Learning Rate: 0.01\n",
      "Epoch [5026/20000], Loss: 10.653045654296875, Learning Rate: 0.01\n",
      "Epoch [5027/20000], Loss: 10.64404296875, Learning Rate: 0.01\n",
      "Epoch [5028/20000], Loss: 10.63507080078125, Learning Rate: 0.01\n",
      "Epoch [5029/20000], Loss: 10.626083374023438, Learning Rate: 0.01\n",
      "Epoch [5030/20000], Loss: 10.617019653320312, Learning Rate: 0.01\n",
      "Epoch [5031/20000], Loss: 10.608016967773438, Learning Rate: 0.01\n",
      "Epoch [5032/20000], Loss: 10.599105834960938, Learning Rate: 0.01\n",
      "Epoch [5033/20000], Loss: 10.590118408203125, Learning Rate: 0.01\n",
      "Epoch [5034/20000], Loss: 10.58111572265625, Learning Rate: 0.01\n",
      "Epoch [5035/20000], Loss: 10.572158813476562, Learning Rate: 0.01\n",
      "Epoch [5036/20000], Loss: 10.563156127929688, Learning Rate: 0.01\n",
      "Epoch [5037/20000], Loss: 10.554168701171875, Learning Rate: 0.01\n",
      "Epoch [5038/20000], Loss: 10.545120239257812, Learning Rate: 0.01\n",
      "Epoch [5039/20000], Loss: 10.536148071289062, Learning Rate: 0.01\n",
      "Epoch [5040/20000], Loss: 10.527175903320312, Learning Rate: 0.01\n",
      "Epoch [5041/20000], Loss: 10.518218994140625, Learning Rate: 0.01\n",
      "Epoch [5042/20000], Loss: 10.50927734375, Learning Rate: 0.01\n",
      "Epoch [5043/20000], Loss: 10.500228881835938, Learning Rate: 0.01\n",
      "Epoch [5044/20000], Loss: 10.491302490234375, Learning Rate: 0.01\n",
      "Epoch [5045/20000], Loss: 10.48223876953125, Learning Rate: 0.01\n",
      "Epoch [5046/20000], Loss: 10.473281860351562, Learning Rate: 0.01\n",
      "Epoch [5047/20000], Loss: 10.464340209960938, Learning Rate: 0.01\n",
      "Epoch [5048/20000], Loss: 10.455337524414062, Learning Rate: 0.01\n",
      "Epoch [5049/20000], Loss: 10.446304321289062, Learning Rate: 0.01\n",
      "Epoch [5050/20000], Loss: 10.437332153320312, Learning Rate: 0.01\n",
      "Epoch [5051/20000], Loss: 10.428375244140625, Learning Rate: 0.01\n",
      "Epoch [5052/20000], Loss: 10.4193115234375, Learning Rate: 0.01\n",
      "Epoch [5053/20000], Loss: 10.410430908203125, Learning Rate: 0.01\n",
      "Epoch [5054/20000], Loss: 10.401519775390625, Learning Rate: 0.01\n",
      "Epoch [5055/20000], Loss: 10.392501831054688, Learning Rate: 0.01\n",
      "Epoch [5056/20000], Loss: 10.383560180664062, Learning Rate: 0.01\n",
      "Epoch [5057/20000], Loss: 10.374526977539062, Learning Rate: 0.01\n",
      "Epoch [5058/20000], Loss: 10.365570068359375, Learning Rate: 0.01\n",
      "Epoch [5059/20000], Loss: 10.3565673828125, Learning Rate: 0.01\n",
      "Epoch [5060/20000], Loss: 10.347671508789062, Learning Rate: 0.01\n",
      "Epoch [5061/20000], Loss: 10.338516235351562, Learning Rate: 0.01\n",
      "Epoch [5062/20000], Loss: 10.32965087890625, Learning Rate: 0.01\n",
      "Epoch [5063/20000], Loss: 10.320693969726562, Learning Rate: 0.01\n",
      "Epoch [5064/20000], Loss: 10.311721801757812, Learning Rate: 0.01\n",
      "Epoch [5065/20000], Loss: 10.302749633789062, Learning Rate: 0.01\n",
      "Epoch [5066/20000], Loss: 10.293777465820312, Learning Rate: 0.01\n",
      "Epoch [5067/20000], Loss: 10.2847900390625, Learning Rate: 0.01\n",
      "Epoch [5068/20000], Loss: 10.27587890625, Learning Rate: 0.01\n",
      "Epoch [5069/20000], Loss: 10.26690673828125, Learning Rate: 0.01\n",
      "Epoch [5070/20000], Loss: 10.258056640625, Learning Rate: 0.01\n",
      "Epoch [5071/20000], Loss: 10.249130249023438, Learning Rate: 0.01\n",
      "Epoch [5072/20000], Loss: 10.24029541015625, Learning Rate: 0.01\n",
      "Epoch [5073/20000], Loss: 10.231460571289062, Learning Rate: 0.01\n",
      "Epoch [5074/20000], Loss: 10.222930908203125, Learning Rate: 0.01\n",
      "Epoch [5075/20000], Loss: 10.214569091796875, Learning Rate: 0.01\n",
      "Epoch [5076/20000], Loss: 10.206527709960938, Learning Rate: 0.01\n",
      "Epoch [5077/20000], Loss: 10.19915771484375, Learning Rate: 0.01\n",
      "Epoch [5078/20000], Loss: 10.192626953125, Learning Rate: 0.01\n",
      "Epoch [5079/20000], Loss: 10.18798828125, Learning Rate: 0.01\n",
      "Epoch [5080/20000], Loss: 10.186141967773438, Learning Rate: 0.01\n",
      "Epoch [5081/20000], Loss: 10.188919067382812, Learning Rate: 0.01\n",
      "Epoch [5082/20000], Loss: 10.200042724609375, Learning Rate: 0.01\n",
      "Epoch [5083/20000], Loss: 10.22479248046875, Learning Rate: 0.01\n",
      "Epoch [5084/20000], Loss: 10.27337646484375, Learning Rate: 0.01\n",
      "Epoch [5085/20000], Loss: 10.362823486328125, Learning Rate: 0.01\n",
      "Epoch [5086/20000], Loss: 10.523086547851562, Learning Rate: 0.01\n",
      "Epoch [5087/20000], Loss: 10.805526733398438, Learning Rate: 0.01\n",
      "Epoch [5088/20000], Loss: 11.298873901367188, Learning Rate: 0.01\n",
      "Epoch [5089/20000], Loss: 12.153213500976562, Learning Rate: 0.01\n",
      "Epoch [5090/20000], Loss: 13.616180419921875, Learning Rate: 0.01\n",
      "Epoch [5091/20000], Loss: 16.064712524414062, Learning Rate: 0.01\n",
      "Epoch [5092/20000], Loss: 20.014389038085938, Learning Rate: 0.01\n",
      "Epoch [5093/20000], Loss: 25.923492431640625, Learning Rate: 0.01\n",
      "Epoch [5094/20000], Loss: 33.67828369140625, Learning Rate: 0.01\n",
      "Epoch [5095/20000], Loss: 41.3438720703125, Learning Rate: 0.01\n",
      "Epoch [5096/20000], Loss: 44.60279846191406, Learning Rate: 0.01\n",
      "Epoch [5097/20000], Loss: 38.731964111328125, Learning Rate: 0.01\n",
      "Epoch [5098/20000], Loss: 25.10003662109375, Learning Rate: 0.01\n",
      "Epoch [5099/20000], Loss: 12.821151733398438, Learning Rate: 0.01\n",
      "Epoch [5100/20000], Loss: 10.498855590820312, Learning Rate: 0.01\n",
      "Epoch [5101/20000], Loss: 17.210250854492188, Learning Rate: 0.01\n",
      "Epoch [5102/20000], Loss: 24.305755615234375, Learning Rate: 0.01\n",
      "Epoch [5103/20000], Loss: 24.046188354492188, Learning Rate: 0.01\n",
      "Epoch [5104/20000], Loss: 16.860397338867188, Learning Rate: 0.01\n",
      "Epoch [5105/20000], Loss: 10.61004638671875, Learning Rate: 0.01\n",
      "Epoch [5106/20000], Loss: 11.116958618164062, Learning Rate: 0.01\n",
      "Epoch [5107/20000], Loss: 15.894088745117188, Learning Rate: 0.01\n",
      "Epoch [5108/20000], Loss: 18.243789672851562, Learning Rate: 0.01\n",
      "Epoch [5109/20000], Loss: 15.353805541992188, Learning Rate: 0.01\n",
      "Epoch [5110/20000], Loss: 11.008102416992188, Learning Rate: 0.01\n",
      "Epoch [5111/20000], Loss: 10.132232666015625, Learning Rate: 0.01\n",
      "Epoch [5112/20000], Loss: 12.6983642578125, Learning Rate: 0.01\n",
      "Epoch [5113/20000], Loss: 14.625823974609375, Learning Rate: 0.01\n",
      "Epoch [5114/20000], Loss: 13.36676025390625, Learning Rate: 0.01\n",
      "Epoch [5115/20000], Loss: 10.732330322265625, Learning Rate: 0.01\n",
      "Epoch [5116/20000], Loss: 9.9267578125, Learning Rate: 0.01\n",
      "Epoch [5117/20000], Loss: 11.342926025390625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5118/20000], Loss: 12.591720581054688, Learning Rate: 0.01\n",
      "Epoch [5119/20000], Loss: 11.953506469726562, Learning Rate: 0.01\n",
      "Epoch [5120/20000], Loss: 10.384292602539062, Learning Rate: 0.01\n",
      "Epoch [5121/20000], Loss: 9.840011596679688, Learning Rate: 0.01\n",
      "Epoch [5122/20000], Loss: 10.6512451171875, Learning Rate: 0.01\n",
      "Epoch [5123/20000], Loss: 11.41534423828125, Learning Rate: 0.01\n",
      "Epoch [5124/20000], Loss: 11.063552856445312, Learning Rate: 0.01\n",
      "Epoch [5125/20000], Loss: 10.127532958984375, Learning Rate: 0.01\n",
      "Epoch [5126/20000], Loss: 9.777267456054688, Learning Rate: 0.01\n",
      "Epoch [5127/20000], Loss: 10.243255615234375, Learning Rate: 0.01\n",
      "Epoch [5128/20000], Loss: 10.7080078125, Learning Rate: 0.01\n",
      "Epoch [5129/20000], Loss: 10.51739501953125, Learning Rate: 0.01\n",
      "Epoch [5130/20000], Loss: 9.958465576171875, Learning Rate: 0.01\n",
      "Epoch [5131/20000], Loss: 9.7252197265625, Learning Rate: 0.01\n",
      "Epoch [5132/20000], Loss: 9.98345947265625, Learning Rate: 0.01\n",
      "Epoch [5133/20000], Loss: 10.2685546875, Learning Rate: 0.01\n",
      "Epoch [5134/20000], Loss: 10.175613403320312, Learning Rate: 0.01\n",
      "Epoch [5135/20000], Loss: 9.843887329101562, Learning Rate: 0.01\n",
      "Epoch [5136/20000], Loss: 9.680557250976562, Learning Rate: 0.01\n",
      "Epoch [5137/20000], Loss: 9.812835693359375, Learning Rate: 0.01\n",
      "Epoch [5138/20000], Loss: 9.988204956054688, Learning Rate: 0.01\n",
      "Epoch [5139/20000], Loss: 9.952926635742188, Learning Rate: 0.01\n",
      "Epoch [5140/20000], Loss: 9.758712768554688, Learning Rate: 0.01\n",
      "Epoch [5141/20000], Loss: 9.64013671875, Learning Rate: 0.01\n",
      "Epoch [5142/20000], Loss: 9.697723388671875, Learning Rate: 0.01\n",
      "Epoch [5143/20000], Loss: 9.803985595703125, Learning Rate: 0.01\n",
      "Epoch [5144/20000], Loss: 9.799545288085938, Learning Rate: 0.01\n",
      "Epoch [5145/20000], Loss: 9.688644409179688, Learning Rate: 0.01\n",
      "Epoch [5146/20000], Loss: 9.601730346679688, Learning Rate: 0.01\n",
      "Epoch [5147/20000], Loss: 9.616912841796875, Learning Rate: 0.01\n",
      "Epoch [5148/20000], Loss: 9.67828369140625, Learning Rate: 0.01\n",
      "Epoch [5149/20000], Loss: 9.687515258789062, Learning Rate: 0.01\n",
      "Epoch [5150/20000], Loss: 9.626968383789062, Learning Rate: 0.01\n",
      "Epoch [5151/20000], Loss: 9.56402587890625, Learning Rate: 0.01\n",
      "Epoch [5152/20000], Loss: 9.556427001953125, Learning Rate: 0.01\n",
      "Epoch [5153/20000], Loss: 9.587890625, Learning Rate: 0.01\n",
      "Epoch [5154/20000], Loss: 9.600784301757812, Learning Rate: 0.01\n",
      "Epoch [5155/20000], Loss: 9.57012939453125, Learning Rate: 0.01\n",
      "Epoch [5156/20000], Loss: 9.525970458984375, Learning Rate: 0.01\n",
      "Epoch [5157/20000], Loss: 9.508132934570312, Learning Rate: 0.01\n",
      "Epoch [5158/20000], Loss: 9.519195556640625, Learning Rate: 0.01\n",
      "Epoch [5159/20000], Loss: 9.529449462890625, Learning Rate: 0.01\n",
      "Epoch [5160/20000], Loss: 9.515869140625, Learning Rate: 0.01\n",
      "Epoch [5161/20000], Loss: 9.486709594726562, Learning Rate: 0.01\n",
      "Epoch [5162/20000], Loss: 9.466156005859375, Learning Rate: 0.01\n",
      "Epoch [5163/20000], Loss: 9.464263916015625, Learning Rate: 0.01\n",
      "Epoch [5164/20000], Loss: 9.46893310546875, Learning Rate: 0.01\n",
      "Epoch [5165/20000], Loss: 9.463455200195312, Learning Rate: 0.01\n",
      "Epoch [5166/20000], Loss: 9.445419311523438, Learning Rate: 0.01\n",
      "Epoch [5167/20000], Loss: 9.426849365234375, Learning Rate: 0.01\n",
      "Epoch [5168/20000], Loss: 9.417800903320312, Learning Rate: 0.01\n",
      "Epoch [5169/20000], Loss: 9.416427612304688, Learning Rate: 0.01\n",
      "Epoch [5170/20000], Loss: 9.413284301757812, Learning Rate: 0.01\n",
      "Epoch [5171/20000], Loss: 9.402328491210938, Learning Rate: 0.01\n",
      "Epoch [5172/20000], Loss: 9.387374877929688, Learning Rate: 0.01\n",
      "Epoch [5173/20000], Loss: 9.375762939453125, Learning Rate: 0.01\n",
      "Epoch [5174/20000], Loss: 9.369735717773438, Learning Rate: 0.01\n",
      "Epoch [5175/20000], Loss: 9.365615844726562, Learning Rate: 0.01\n",
      "Epoch [5176/20000], Loss: 9.35821533203125, Learning Rate: 0.01\n",
      "Epoch [5177/20000], Loss: 9.347015380859375, Learning Rate: 0.01\n",
      "Epoch [5178/20000], Loss: 9.3353271484375, Learning Rate: 0.01\n",
      "Epoch [5179/20000], Loss: 9.326644897460938, Learning Rate: 0.01\n",
      "Epoch [5180/20000], Loss: 9.320480346679688, Learning Rate: 0.01\n",
      "Epoch [5181/20000], Loss: 9.314071655273438, Learning Rate: 0.01\n",
      "Epoch [5182/20000], Loss: 9.305435180664062, Learning Rate: 0.01\n",
      "Epoch [5183/20000], Loss: 9.29522705078125, Learning Rate: 0.01\n",
      "Epoch [5184/20000], Loss: 9.285552978515625, Learning Rate: 0.01\n",
      "Epoch [5185/20000], Loss: 9.277557373046875, Learning Rate: 0.01\n",
      "Epoch [5186/20000], Loss: 9.270614624023438, Learning Rate: 0.01\n",
      "Epoch [5187/20000], Loss: 9.263153076171875, Learning Rate: 0.01\n",
      "Epoch [5188/20000], Loss: 9.254425048828125, Learning Rate: 0.01\n",
      "Epoch [5189/20000], Loss: 9.245025634765625, Learning Rate: 0.01\n",
      "Epoch [5190/20000], Loss: 9.236236572265625, Learning Rate: 0.01\n",
      "Epoch [5191/20000], Loss: 9.228363037109375, Learning Rate: 0.01\n",
      "Epoch [5192/20000], Loss: 9.220901489257812, Learning Rate: 0.01\n",
      "Epoch [5193/20000], Loss: 9.212905883789062, Learning Rate: 0.01\n",
      "Epoch [5194/20000], Loss: 9.20428466796875, Learning Rate: 0.01\n",
      "Epoch [5195/20000], Loss: 9.195480346679688, Learning Rate: 0.01\n",
      "Epoch [5196/20000], Loss: 9.18701171875, Learning Rate: 0.01\n",
      "Epoch [5197/20000], Loss: 9.179031372070312, Learning Rate: 0.01\n",
      "Epoch [5198/20000], Loss: 9.171279907226562, Learning Rate: 0.01\n",
      "Epoch [5199/20000], Loss: 9.1632080078125, Learning Rate: 0.01\n",
      "Epoch [5200/20000], Loss: 9.154754638671875, Learning Rate: 0.01\n",
      "Epoch [5201/20000], Loss: 9.146148681640625, Learning Rate: 0.01\n",
      "Epoch [5202/20000], Loss: 9.13787841796875, Learning Rate: 0.01\n",
      "Epoch [5203/20000], Loss: 9.129806518554688, Learning Rate: 0.01\n",
      "Epoch [5204/20000], Loss: 9.121841430664062, Learning Rate: 0.01\n",
      "Epoch [5205/20000], Loss: 9.113723754882812, Learning Rate: 0.01\n",
      "Epoch [5206/20000], Loss: 9.105331420898438, Learning Rate: 0.01\n",
      "Epoch [5207/20000], Loss: 9.097091674804688, Learning Rate: 0.01\n",
      "Epoch [5208/20000], Loss: 9.088699340820312, Learning Rate: 0.01\n",
      "Epoch [5209/20000], Loss: 9.08062744140625, Learning Rate: 0.01\n",
      "Epoch [5210/20000], Loss: 9.072601318359375, Learning Rate: 0.01\n",
      "Epoch [5211/20000], Loss: 9.064453125, Learning Rate: 0.01\n",
      "Epoch [5212/20000], Loss: 9.056167602539062, Learning Rate: 0.01\n",
      "Epoch [5213/20000], Loss: 9.04791259765625, Learning Rate: 0.01\n",
      "Epoch [5214/20000], Loss: 9.039627075195312, Learning Rate: 0.01\n",
      "Epoch [5215/20000], Loss: 9.031463623046875, Learning Rate: 0.01\n",
      "Epoch [5216/20000], Loss: 9.023330688476562, Learning Rate: 0.01\n",
      "Epoch [5217/20000], Loss: 9.015182495117188, Learning Rate: 0.01\n",
      "Epoch [5218/20000], Loss: 9.007049560546875, Learning Rate: 0.01\n",
      "Epoch [5219/20000], Loss: 8.998748779296875, Learning Rate: 0.01\n",
      "Epoch [5220/20000], Loss: 8.990554809570312, Learning Rate: 0.01\n",
      "Epoch [5221/20000], Loss: 8.982376098632812, Learning Rate: 0.01\n",
      "Epoch [5222/20000], Loss: 8.974136352539062, Learning Rate: 0.01\n",
      "Epoch [5223/20000], Loss: 8.966079711914062, Learning Rate: 0.01\n",
      "Epoch [5224/20000], Loss: 8.957916259765625, Learning Rate: 0.01\n",
      "Epoch [5225/20000], Loss: 8.949691772460938, Learning Rate: 0.01\n",
      "Epoch [5226/20000], Loss: 8.941482543945312, Learning Rate: 0.01\n",
      "Epoch [5227/20000], Loss: 8.93316650390625, Learning Rate: 0.01\n",
      "Epoch [5228/20000], Loss: 8.925033569335938, Learning Rate: 0.01\n",
      "Epoch [5229/20000], Loss: 8.916885375976562, Learning Rate: 0.01\n",
      "Epoch [5230/20000], Loss: 8.908660888671875, Learning Rate: 0.01\n",
      "Epoch [5231/20000], Loss: 8.9005126953125, Learning Rate: 0.01\n",
      "Epoch [5232/20000], Loss: 8.892349243164062, Learning Rate: 0.01\n",
      "Epoch [5233/20000], Loss: 8.884140014648438, Learning Rate: 0.01\n",
      "Epoch [5234/20000], Loss: 8.87603759765625, Learning Rate: 0.01\n",
      "Epoch [5235/20000], Loss: 8.867874145507812, Learning Rate: 0.01\n",
      "Epoch [5236/20000], Loss: 8.859664916992188, Learning Rate: 0.01\n",
      "Epoch [5237/20000], Loss: 8.851455688476562, Learning Rate: 0.01\n",
      "Epoch [5238/20000], Loss: 8.843368530273438, Learning Rate: 0.01\n",
      "Epoch [5239/20000], Loss: 8.835220336914062, Learning Rate: 0.01\n",
      "Epoch [5240/20000], Loss: 8.8270263671875, Learning Rate: 0.01\n",
      "Epoch [5241/20000], Loss: 8.818756103515625, Learning Rate: 0.01\n",
      "Epoch [5242/20000], Loss: 8.8106689453125, Learning Rate: 0.01\n",
      "Epoch [5243/20000], Loss: 8.802505493164062, Learning Rate: 0.01\n",
      "Epoch [5244/20000], Loss: 8.794265747070312, Learning Rate: 0.01\n",
      "Epoch [5245/20000], Loss: 8.786056518554688, Learning Rate: 0.01\n",
      "Epoch [5246/20000], Loss: 8.77789306640625, Learning Rate: 0.01\n",
      "Epoch [5247/20000], Loss: 8.769821166992188, Learning Rate: 0.01\n",
      "Epoch [5248/20000], Loss: 8.761520385742188, Learning Rate: 0.01\n",
      "Epoch [5249/20000], Loss: 8.75341796875, Learning Rate: 0.01\n",
      "Epoch [5250/20000], Loss: 8.745285034179688, Learning Rate: 0.01\n",
      "Epoch [5251/20000], Loss: 8.737106323242188, Learning Rate: 0.01\n",
      "Epoch [5252/20000], Loss: 8.728973388671875, Learning Rate: 0.01\n",
      "Epoch [5253/20000], Loss: 8.720794677734375, Learning Rate: 0.01\n",
      "Epoch [5254/20000], Loss: 8.712570190429688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5255/20000], Loss: 8.704391479492188, Learning Rate: 0.01\n",
      "Epoch [5256/20000], Loss: 8.696197509765625, Learning Rate: 0.01\n",
      "Epoch [5257/20000], Loss: 8.688125610351562, Learning Rate: 0.01\n",
      "Epoch [5258/20000], Loss: 8.679931640625, Learning Rate: 0.01\n",
      "Epoch [5259/20000], Loss: 8.67181396484375, Learning Rate: 0.01\n",
      "Epoch [5260/20000], Loss: 8.66363525390625, Learning Rate: 0.01\n",
      "Epoch [5261/20000], Loss: 8.6553955078125, Learning Rate: 0.01\n",
      "Epoch [5262/20000], Loss: 8.647354125976562, Learning Rate: 0.01\n",
      "Epoch [5263/20000], Loss: 8.63909912109375, Learning Rate: 0.01\n",
      "Epoch [5264/20000], Loss: 8.631072998046875, Learning Rate: 0.01\n",
      "Epoch [5265/20000], Loss: 8.622817993164062, Learning Rate: 0.01\n",
      "Epoch [5266/20000], Loss: 8.614532470703125, Learning Rate: 0.01\n",
      "Epoch [5267/20000], Loss: 8.606430053710938, Learning Rate: 0.01\n",
      "Epoch [5268/20000], Loss: 8.59820556640625, Learning Rate: 0.01\n",
      "Epoch [5269/20000], Loss: 8.59014892578125, Learning Rate: 0.01\n",
      "Epoch [5270/20000], Loss: 8.582046508789062, Learning Rate: 0.01\n",
      "Epoch [5271/20000], Loss: 8.5738525390625, Learning Rate: 0.01\n",
      "Epoch [5272/20000], Loss: 8.565673828125, Learning Rate: 0.01\n",
      "Epoch [5273/20000], Loss: 8.557540893554688, Learning Rate: 0.01\n",
      "Epoch [5274/20000], Loss: 8.5494384765625, Learning Rate: 0.01\n",
      "Epoch [5275/20000], Loss: 8.541213989257812, Learning Rate: 0.01\n",
      "Epoch [5276/20000], Loss: 8.5330810546875, Learning Rate: 0.01\n",
      "Epoch [5277/20000], Loss: 8.524948120117188, Learning Rate: 0.01\n",
      "Epoch [5278/20000], Loss: 8.516830444335938, Learning Rate: 0.01\n",
      "Epoch [5279/20000], Loss: 8.50860595703125, Learning Rate: 0.01\n",
      "Epoch [5280/20000], Loss: 8.500518798828125, Learning Rate: 0.01\n",
      "Epoch [5281/20000], Loss: 8.49237060546875, Learning Rate: 0.01\n",
      "Epoch [5282/20000], Loss: 8.484115600585938, Learning Rate: 0.01\n",
      "Epoch [5283/20000], Loss: 8.47607421875, Learning Rate: 0.01\n",
      "Epoch [5284/20000], Loss: 8.467864990234375, Learning Rate: 0.01\n",
      "Epoch [5285/20000], Loss: 8.459747314453125, Learning Rate: 0.01\n",
      "Epoch [5286/20000], Loss: 8.451568603515625, Learning Rate: 0.01\n",
      "Epoch [5287/20000], Loss: 8.443435668945312, Learning Rate: 0.01\n",
      "Epoch [5288/20000], Loss: 8.435287475585938, Learning Rate: 0.01\n",
      "Epoch [5289/20000], Loss: 8.427139282226562, Learning Rate: 0.01\n",
      "Epoch [5290/20000], Loss: 8.41900634765625, Learning Rate: 0.01\n",
      "Epoch [5291/20000], Loss: 8.41082763671875, Learning Rate: 0.01\n",
      "Epoch [5292/20000], Loss: 8.402618408203125, Learning Rate: 0.01\n",
      "Epoch [5293/20000], Loss: 8.394515991210938, Learning Rate: 0.01\n",
      "Epoch [5294/20000], Loss: 8.38641357421875, Learning Rate: 0.01\n",
      "Epoch [5295/20000], Loss: 8.378265380859375, Learning Rate: 0.01\n",
      "Epoch [5296/20000], Loss: 8.370147705078125, Learning Rate: 0.01\n",
      "Epoch [5297/20000], Loss: 8.362075805664062, Learning Rate: 0.01\n",
      "Epoch [5298/20000], Loss: 8.353866577148438, Learning Rate: 0.01\n",
      "Epoch [5299/20000], Loss: 8.345672607421875, Learning Rate: 0.01\n",
      "Epoch [5300/20000], Loss: 8.337646484375, Learning Rate: 0.01\n",
      "Epoch [5301/20000], Loss: 8.3294677734375, Learning Rate: 0.01\n",
      "Epoch [5302/20000], Loss: 8.321319580078125, Learning Rate: 0.01\n",
      "Epoch [5303/20000], Loss: 8.313156127929688, Learning Rate: 0.01\n",
      "Epoch [5304/20000], Loss: 8.305130004882812, Learning Rate: 0.01\n",
      "Epoch [5305/20000], Loss: 8.296905517578125, Learning Rate: 0.01\n",
      "Epoch [5306/20000], Loss: 8.28875732421875, Learning Rate: 0.01\n",
      "Epoch [5307/20000], Loss: 8.280624389648438, Learning Rate: 0.01\n",
      "Epoch [5308/20000], Loss: 8.272552490234375, Learning Rate: 0.01\n",
      "Epoch [5309/20000], Loss: 8.264373779296875, Learning Rate: 0.01\n",
      "Epoch [5310/20000], Loss: 8.256271362304688, Learning Rate: 0.01\n",
      "Epoch [5311/20000], Loss: 8.248138427734375, Learning Rate: 0.01\n",
      "Epoch [5312/20000], Loss: 8.240005493164062, Learning Rate: 0.01\n",
      "Epoch [5313/20000], Loss: 8.231857299804688, Learning Rate: 0.01\n",
      "Epoch [5314/20000], Loss: 8.223724365234375, Learning Rate: 0.01\n",
      "Epoch [5315/20000], Loss: 8.215545654296875, Learning Rate: 0.01\n",
      "Epoch [5316/20000], Loss: 8.207550048828125, Learning Rate: 0.01\n",
      "Epoch [5317/20000], Loss: 8.199447631835938, Learning Rate: 0.01\n",
      "Epoch [5318/20000], Loss: 8.19134521484375, Learning Rate: 0.01\n",
      "Epoch [5319/20000], Loss: 8.183074951171875, Learning Rate: 0.01\n",
      "Epoch [5320/20000], Loss: 8.175140380859375, Learning Rate: 0.01\n",
      "Epoch [5321/20000], Loss: 8.167037963867188, Learning Rate: 0.01\n",
      "Epoch [5322/20000], Loss: 8.159011840820312, Learning Rate: 0.01\n",
      "Epoch [5323/20000], Loss: 8.151123046875, Learning Rate: 0.01\n",
      "Epoch [5324/20000], Loss: 8.143325805664062, Learning Rate: 0.01\n",
      "Epoch [5325/20000], Loss: 8.135696411132812, Learning Rate: 0.01\n",
      "Epoch [5326/20000], Loss: 8.128433227539062, Learning Rate: 0.01\n",
      "Epoch [5327/20000], Loss: 8.121536254882812, Learning Rate: 0.01\n",
      "Epoch [5328/20000], Loss: 8.115386962890625, Learning Rate: 0.01\n",
      "Epoch [5329/20000], Loss: 8.11065673828125, Learning Rate: 0.01\n",
      "Epoch [5330/20000], Loss: 8.107894897460938, Learning Rate: 0.01\n",
      "Epoch [5331/20000], Loss: 8.10894775390625, Learning Rate: 0.01\n",
      "Epoch [5332/20000], Loss: 8.115615844726562, Learning Rate: 0.01\n",
      "Epoch [5333/20000], Loss: 8.13226318359375, Learning Rate: 0.01\n",
      "Epoch [5334/20000], Loss: 8.165771484375, Learning Rate: 0.01\n",
      "Epoch [5335/20000], Loss: 8.227401733398438, Learning Rate: 0.01\n",
      "Epoch [5336/20000], Loss: 8.336837768554688, Learning Rate: 0.01\n",
      "Epoch [5337/20000], Loss: 8.526580810546875, Learning Rate: 0.01\n",
      "Epoch [5338/20000], Loss: 8.851531982421875, Learning Rate: 0.01\n",
      "Epoch [5339/20000], Loss: 9.3983154296875, Learning Rate: 0.01\n",
      "Epoch [5340/20000], Loss: 10.299911499023438, Learning Rate: 0.01\n",
      "Epoch [5341/20000], Loss: 11.732955932617188, Learning Rate: 0.01\n",
      "Epoch [5342/20000], Loss: 13.8780517578125, Learning Rate: 0.01\n",
      "Epoch [5343/20000], Loss: 16.75732421875, Learning Rate: 0.01\n",
      "Epoch [5344/20000], Loss: 19.9404296875, Learning Rate: 0.01\n",
      "Epoch [5345/20000], Loss: 22.202926635742188, Learning Rate: 0.01\n",
      "Epoch [5346/20000], Loss: 21.855545043945312, Learning Rate: 0.01\n",
      "Epoch [5347/20000], Loss: 18.094985961914062, Learning Rate: 0.01\n",
      "Epoch [5348/20000], Loss: 12.581558227539062, Learning Rate: 0.01\n",
      "Epoch [5349/20000], Loss: 8.621978759765625, Learning Rate: 0.01\n",
      "Epoch [5350/20000], Loss: 8.318832397460938, Learning Rate: 0.01\n",
      "Epoch [5351/20000], Loss: 10.847030639648438, Learning Rate: 0.01\n",
      "Epoch [5352/20000], Loss: 13.467010498046875, Learning Rate: 0.01\n",
      "Epoch [5353/20000], Loss: 13.791793823242188, Learning Rate: 0.01\n",
      "Epoch [5354/20000], Loss: 11.596786499023438, Learning Rate: 0.01\n",
      "Epoch [5355/20000], Loss: 8.893157958984375, Learning Rate: 0.01\n",
      "Epoch [5356/20000], Loss: 7.9015960693359375, Learning Rate: 0.01\n",
      "Epoch [5357/20000], Loss: 8.9693603515625, Learning Rate: 0.01\n",
      "Epoch [5358/20000], Loss: 10.558486938476562, Learning Rate: 0.01\n",
      "Epoch [5359/20000], Loss: 10.94915771484375, Learning Rate: 0.01\n",
      "Epoch [5360/20000], Loss: 9.823562622070312, Learning Rate: 0.01\n",
      "Epoch [5361/20000], Loss: 8.355056762695312, Learning Rate: 0.01\n",
      "Epoch [5362/20000], Loss: 7.8605804443359375, Learning Rate: 0.01\n",
      "Epoch [5363/20000], Loss: 8.500137329101562, Learning Rate: 0.01\n",
      "Epoch [5364/20000], Loss: 9.334487915039062, Learning Rate: 0.01\n",
      "Epoch [5365/20000], Loss: 9.423477172851562, Learning Rate: 0.01\n",
      "Epoch [5366/20000], Loss: 8.722763061523438, Learning Rate: 0.01\n",
      "Epoch [5367/20000], Loss: 7.97235107421875, Learning Rate: 0.01\n",
      "Epoch [5368/20000], Loss: 7.8337554931640625, Learning Rate: 0.01\n",
      "Epoch [5369/20000], Loss: 8.2564697265625, Learning Rate: 0.01\n",
      "Epoch [5370/20000], Loss: 8.662002563476562, Learning Rate: 0.01\n",
      "Epoch [5371/20000], Loss: 8.602630615234375, Learning Rate: 0.01\n",
      "Epoch [5372/20000], Loss: 8.169952392578125, Learning Rate: 0.01\n",
      "Epoch [5373/20000], Loss: 7.8050537109375, Learning Rate: 0.01\n",
      "Epoch [5374/20000], Loss: 7.8040924072265625, Learning Rate: 0.01\n",
      "Epoch [5375/20000], Loss: 8.057388305664062, Learning Rate: 0.01\n",
      "Epoch [5376/20000], Loss: 8.234664916992188, Learning Rate: 0.01\n",
      "Epoch [5377/20000], Loss: 8.144882202148438, Learning Rate: 0.01\n",
      "Epoch [5378/20000], Loss: 7.8927001953125, Learning Rate: 0.01\n",
      "Epoch [5379/20000], Loss: 7.7212371826171875, Learning Rate: 0.01\n",
      "Epoch [5380/20000], Loss: 7.752166748046875, Learning Rate: 0.01\n",
      "Epoch [5381/20000], Loss: 7.8932037353515625, Learning Rate: 0.01\n",
      "Epoch [5382/20000], Loss: 7.9671478271484375, Learning Rate: 0.01\n",
      "Epoch [5383/20000], Loss: 7.8959503173828125, Learning Rate: 0.01\n",
      "Epoch [5384/20000], Loss: 7.75457763671875, Learning Rate: 0.01\n",
      "Epoch [5385/20000], Loss: 7.6704864501953125, Learning Rate: 0.01\n",
      "Epoch [5386/20000], Loss: 7.69403076171875, Learning Rate: 0.01\n",
      "Epoch [5387/20000], Loss: 7.7666778564453125, Learning Rate: 0.01\n",
      "Epoch [5388/20000], Loss: 7.7965545654296875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5389/20000], Loss: 7.7499542236328125, Learning Rate: 0.01\n",
      "Epoch [5390/20000], Loss: 7.6702880859375, Learning Rate: 0.01\n",
      "Epoch [5391/20000], Loss: 7.623687744140625, Learning Rate: 0.01\n",
      "Epoch [5392/20000], Loss: 7.63372802734375, Learning Rate: 0.01\n",
      "Epoch [5393/20000], Loss: 7.66888427734375, Learning Rate: 0.01\n",
      "Epoch [5394/20000], Loss: 7.681854248046875, Learning Rate: 0.01\n",
      "Epoch [5395/20000], Loss: 7.654510498046875, Learning Rate: 0.01\n",
      "Epoch [5396/20000], Loss: 7.6088714599609375, Learning Rate: 0.01\n",
      "Epoch [5397/20000], Loss: 7.579010009765625, Learning Rate: 0.01\n",
      "Epoch [5398/20000], Loss: 7.5789337158203125, Learning Rate: 0.01\n",
      "Epoch [5399/20000], Loss: 7.5939178466796875, Learning Rate: 0.01\n",
      "Epoch [5400/20000], Loss: 7.59954833984375, Learning Rate: 0.01\n",
      "Epoch [5401/20000], Loss: 7.5841522216796875, Learning Rate: 0.01\n",
      "Epoch [5402/20000], Loss: 7.556884765625, Learning Rate: 0.01\n",
      "Epoch [5403/20000], Loss: 7.5353240966796875, Learning Rate: 0.01\n",
      "Epoch [5404/20000], Loss: 7.5289764404296875, Learning Rate: 0.01\n",
      "Epoch [5405/20000], Loss: 7.5326995849609375, Learning Rate: 0.01\n",
      "Epoch [5406/20000], Loss: 7.534088134765625, Learning Rate: 0.01\n",
      "Epoch [5407/20000], Loss: 7.52520751953125, Learning Rate: 0.01\n",
      "Epoch [5408/20000], Loss: 7.5081329345703125, Learning Rate: 0.01\n",
      "Epoch [5409/20000], Loss: 7.4917144775390625, Learning Rate: 0.01\n",
      "Epoch [5410/20000], Loss: 7.482330322265625, Learning Rate: 0.01\n",
      "Epoch [5411/20000], Loss: 7.4795989990234375, Learning Rate: 0.01\n",
      "Epoch [5412/20000], Loss: 7.477783203125, Learning Rate: 0.01\n",
      "Epoch [5413/20000], Loss: 7.4716949462890625, Learning Rate: 0.01\n",
      "Epoch [5414/20000], Loss: 7.4605560302734375, Learning Rate: 0.01\n",
      "Epoch [5415/20000], Loss: 7.4478759765625, Learning Rate: 0.01\n",
      "Epoch [5416/20000], Loss: 7.437744140625, Learning Rate: 0.01\n",
      "Epoch [5417/20000], Loss: 7.43133544921875, Learning Rate: 0.01\n",
      "Epoch [5418/20000], Loss: 7.4269866943359375, Learning Rate: 0.01\n",
      "Epoch [5419/20000], Loss: 7.421600341796875, Learning Rate: 0.01\n",
      "Epoch [5420/20000], Loss: 7.4135894775390625, Learning Rate: 0.01\n",
      "Epoch [5421/20000], Loss: 7.4037017822265625, Learning Rate: 0.01\n",
      "Epoch [5422/20000], Loss: 7.39404296875, Learning Rate: 0.01\n",
      "Epoch [5423/20000], Loss: 7.386077880859375, Learning Rate: 0.01\n",
      "Epoch [5424/20000], Loss: 7.3798065185546875, Learning Rate: 0.01\n",
      "Epoch [5425/20000], Loss: 7.3740386962890625, Learning Rate: 0.01\n",
      "Epoch [5426/20000], Loss: 7.3673553466796875, Learning Rate: 0.01\n",
      "Epoch [5427/20000], Loss: 7.359527587890625, Learning Rate: 0.01\n",
      "Epoch [5428/20000], Loss: 7.3510284423828125, Learning Rate: 0.01\n",
      "Epoch [5429/20000], Loss: 7.3430023193359375, Learning Rate: 0.01\n",
      "Epoch [5430/20000], Loss: 7.3360748291015625, Learning Rate: 0.01\n",
      "Epoch [5431/20000], Loss: 7.3299713134765625, Learning Rate: 0.01\n",
      "Epoch [5432/20000], Loss: 7.3241729736328125, Learning Rate: 0.01\n",
      "Epoch [5433/20000], Loss: 7.3183135986328125, Learning Rate: 0.01\n",
      "Epoch [5434/20000], Loss: 7.312286376953125, Learning Rate: 0.01\n",
      "Epoch [5435/20000], Loss: 7.30657958984375, Learning Rate: 0.01\n",
      "Epoch [5436/20000], Loss: 7.302154541015625, Learning Rate: 0.01\n",
      "Epoch [5437/20000], Loss: 7.2993316650390625, Learning Rate: 0.01\n",
      "Epoch [5438/20000], Loss: 7.2989044189453125, Learning Rate: 0.01\n",
      "Epoch [5439/20000], Loss: 7.3013153076171875, Learning Rate: 0.01\n",
      "Epoch [5440/20000], Loss: 7.307373046875, Learning Rate: 0.01\n",
      "Epoch [5441/20000], Loss: 7.3192901611328125, Learning Rate: 0.01\n",
      "Epoch [5442/20000], Loss: 7.3395843505859375, Learning Rate: 0.01\n",
      "Epoch [5443/20000], Loss: 7.37286376953125, Learning Rate: 0.01\n",
      "Epoch [5444/20000], Loss: 7.4253692626953125, Learning Rate: 0.01\n",
      "Epoch [5445/20000], Loss: 7.506134033203125, Learning Rate: 0.01\n",
      "Epoch [5446/20000], Loss: 7.6293182373046875, Learning Rate: 0.01\n",
      "Epoch [5447/20000], Loss: 7.815185546875, Learning Rate: 0.01\n",
      "Epoch [5448/20000], Loss: 8.09405517578125, Learning Rate: 0.01\n",
      "Epoch [5449/20000], Loss: 8.511093139648438, Learning Rate: 0.01\n",
      "Epoch [5450/20000], Loss: 9.128097534179688, Learning Rate: 0.01\n",
      "Epoch [5451/20000], Loss: 10.03082275390625, Learning Rate: 0.01\n",
      "Epoch [5452/20000], Loss: 11.318939208984375, Learning Rate: 0.01\n",
      "Epoch [5453/20000], Loss: 13.097503662109375, Learning Rate: 0.01\n",
      "Epoch [5454/20000], Loss: 15.403717041015625, Learning Rate: 0.01\n",
      "Epoch [5455/20000], Loss: 18.130264282226562, Learning Rate: 0.01\n",
      "Epoch [5456/20000], Loss: 20.821823120117188, Learning Rate: 0.01\n",
      "Epoch [5457/20000], Loss: 22.654296875, Learning Rate: 0.01\n",
      "Epoch [5458/20000], Loss: 22.5157470703125, Learning Rate: 0.01\n",
      "Epoch [5459/20000], Loss: 19.802825927734375, Learning Rate: 0.01\n",
      "Epoch [5460/20000], Loss: 15.083419799804688, Learning Rate: 0.01\n",
      "Epoch [5461/20000], Loss: 10.281021118164062, Learning Rate: 0.01\n",
      "Epoch [5462/20000], Loss: 7.4480133056640625, Learning Rate: 0.01\n",
      "Epoch [5463/20000], Loss: 7.42730712890625, Learning Rate: 0.01\n",
      "Epoch [5464/20000], Loss: 9.4444580078125, Learning Rate: 0.01\n",
      "Epoch [5465/20000], Loss: 11.791107177734375, Learning Rate: 0.01\n",
      "Epoch [5466/20000], Loss: 12.87408447265625, Learning Rate: 0.01\n",
      "Epoch [5467/20000], Loss: 12.041900634765625, Learning Rate: 0.01\n",
      "Epoch [5468/20000], Loss: 9.916366577148438, Learning Rate: 0.01\n",
      "Epoch [5469/20000], Loss: 7.8730316162109375, Learning Rate: 0.01\n",
      "Epoch [5470/20000], Loss: 7.050872802734375, Learning Rate: 0.01\n",
      "Epoch [5471/20000], Loss: 7.6077117919921875, Learning Rate: 0.01\n",
      "Epoch [5472/20000], Loss: 8.792587280273438, Learning Rate: 0.01\n",
      "Epoch [5473/20000], Loss: 9.60137939453125, Learning Rate: 0.01\n",
      "Epoch [5474/20000], Loss: 9.461685180664062, Learning Rate: 0.01\n",
      "Epoch [5475/20000], Loss: 8.541091918945312, Learning Rate: 0.01\n",
      "Epoch [5476/20000], Loss: 7.5108795166015625, Learning Rate: 0.01\n",
      "Epoch [5477/20000], Loss: 7.0084686279296875, Learning Rate: 0.01\n",
      "Epoch [5478/20000], Loss: 7.1997833251953125, Learning Rate: 0.01\n",
      "Epoch [5479/20000], Loss: 7.7573089599609375, Learning Rate: 0.01\n",
      "Epoch [5480/20000], Loss: 8.17742919921875, Learning Rate: 0.01\n",
      "Epoch [5481/20000], Loss: 8.148956298828125, Learning Rate: 0.01\n",
      "Epoch [5482/20000], Loss: 7.7266082763671875, Learning Rate: 0.01\n",
      "Epoch [5483/20000], Loss: 7.2242279052734375, Learning Rate: 0.01\n",
      "Epoch [5484/20000], Loss: 6.954864501953125, Learning Rate: 0.01\n",
      "Epoch [5485/20000], Loss: 7.0173187255859375, Learning Rate: 0.01\n",
      "Epoch [5486/20000], Loss: 7.27215576171875, Learning Rate: 0.01\n",
      "Epoch [5487/20000], Loss: 7.4827423095703125, Learning Rate: 0.01\n",
      "Epoch [5488/20000], Loss: 7.49029541015625, Learning Rate: 0.01\n",
      "Epoch [5489/20000], Loss: 7.30316162109375, Learning Rate: 0.01\n",
      "Epoch [5490/20000], Loss: 7.057952880859375, Learning Rate: 0.01\n",
      "Epoch [5491/20000], Loss: 6.9044342041015625, Learning Rate: 0.01\n",
      "Epoch [5492/20000], Loss: 6.9049072265625, Learning Rate: 0.01\n",
      "Epoch [5493/20000], Loss: 7.010894775390625, Learning Rate: 0.01\n",
      "Epoch [5494/20000], Loss: 7.1174774169921875, Learning Rate: 0.01\n",
      "Epoch [5495/20000], Loss: 7.141265869140625, Learning Rate: 0.01\n",
      "Epoch [5496/20000], Loss: 7.0689544677734375, Learning Rate: 0.01\n",
      "Epoch [5497/20000], Loss: 6.9512939453125, Learning Rate: 0.01\n",
      "Epoch [5498/20000], Loss: 6.85821533203125, Learning Rate: 0.01\n",
      "Epoch [5499/20000], Loss: 6.83111572265625, Learning Rate: 0.01\n",
      "Epoch [5500/20000], Loss: 6.863250732421875, Learning Rate: 0.01\n",
      "Epoch [5501/20000], Loss: 6.9134674072265625, Learning Rate: 0.01\n",
      "Epoch [5502/20000], Loss: 6.938507080078125, Learning Rate: 0.01\n",
      "Epoch [5503/20000], Loss: 6.9195098876953125, Learning Rate: 0.01\n",
      "Epoch [5504/20000], Loss: 6.86785888671875, Learning Rate: 0.01\n",
      "Epoch [5505/20000], Loss: 6.8127593994140625, Learning Rate: 0.01\n",
      "Epoch [5506/20000], Loss: 6.779388427734375, Learning Rate: 0.01\n",
      "Epoch [5507/20000], Loss: 6.7762603759765625, Learning Rate: 0.01\n",
      "Epoch [5508/20000], Loss: 6.792083740234375, Learning Rate: 0.01\n",
      "Epoch [5509/20000], Loss: 6.80816650390625, Learning Rate: 0.01\n",
      "Epoch [5510/20000], Loss: 6.8090057373046875, Learning Rate: 0.01\n",
      "Epoch [5511/20000], Loss: 6.79144287109375, Learning Rate: 0.01\n",
      "Epoch [5512/20000], Loss: 6.762939453125, Learning Rate: 0.01\n",
      "Epoch [5513/20000], Loss: 6.7359771728515625, Learning Rate: 0.01\n",
      "Epoch [5514/20000], Loss: 6.7197723388671875, Learning Rate: 0.01\n",
      "Epoch [5515/20000], Loss: 6.7158660888671875, Learning Rate: 0.01\n",
      "Epoch [5516/20000], Loss: 6.7190399169921875, Learning Rate: 0.01\n",
      "Epoch [5517/20000], Loss: 6.7216033935546875, Learning Rate: 0.01\n",
      "Epoch [5518/20000], Loss: 6.7177581787109375, Learning Rate: 0.01\n",
      "Epoch [5519/20000], Loss: 6.7061614990234375, Learning Rate: 0.01\n",
      "Epoch [5520/20000], Loss: 6.69000244140625, Learning Rate: 0.01\n",
      "Epoch [5521/20000], Loss: 6.674102783203125, Learning Rate: 0.01\n",
      "Epoch [5522/20000], Loss: 6.662353515625, Learning Rate: 0.01\n",
      "Epoch [5523/20000], Loss: 6.6556549072265625, Learning Rate: 0.01\n",
      "Epoch [5524/20000], Loss: 6.6524200439453125, Learning Rate: 0.01\n",
      "Epoch [5525/20000], Loss: 6.64971923828125, Learning Rate: 0.01\n",
      "Epoch [5526/20000], Loss: 6.64495849609375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5527/20000], Loss: 6.6371917724609375, Learning Rate: 0.01\n",
      "Epoch [5528/20000], Loss: 6.6269989013671875, Learning Rate: 0.01\n",
      "Epoch [5529/20000], Loss: 6.6159515380859375, Learning Rate: 0.01\n",
      "Epoch [5530/20000], Loss: 6.6058807373046875, Learning Rate: 0.01\n",
      "Epoch [5531/20000], Loss: 6.59771728515625, Learning Rate: 0.01\n",
      "Epoch [5532/20000], Loss: 6.5913238525390625, Learning Rate: 0.01\n",
      "Epoch [5533/20000], Loss: 6.58599853515625, Learning Rate: 0.01\n",
      "Epoch [5534/20000], Loss: 6.58056640625, Learning Rate: 0.01\n",
      "Epoch [5535/20000], Loss: 6.5741424560546875, Learning Rate: 0.01\n",
      "Epoch [5536/20000], Loss: 6.56658935546875, Learning Rate: 0.01\n",
      "Epoch [5537/20000], Loss: 6.5582427978515625, Learning Rate: 0.01\n",
      "Epoch [5538/20000], Loss: 6.5495147705078125, Learning Rate: 0.01\n",
      "Epoch [5539/20000], Loss: 6.541290283203125, Learning Rate: 0.01\n",
      "Epoch [5540/20000], Loss: 6.5336456298828125, Learning Rate: 0.01\n",
      "Epoch [5541/20000], Loss: 6.5267791748046875, Learning Rate: 0.01\n",
      "Epoch [5542/20000], Loss: 6.52032470703125, Learning Rate: 0.01\n",
      "Epoch [5543/20000], Loss: 6.5138397216796875, Learning Rate: 0.01\n",
      "Epoch [5544/20000], Loss: 6.5070953369140625, Learning Rate: 0.01\n",
      "Epoch [5545/20000], Loss: 6.5, Learning Rate: 0.01\n",
      "Epoch [5546/20000], Loss: 6.4925537109375, Learning Rate: 0.01\n",
      "Epoch [5547/20000], Loss: 6.4849090576171875, Learning Rate: 0.01\n",
      "Epoch [5548/20000], Loss: 6.4772796630859375, Learning Rate: 0.01\n",
      "Epoch [5549/20000], Loss: 6.4698028564453125, Learning Rate: 0.01\n",
      "Epoch [5550/20000], Loss: 6.4625701904296875, Learning Rate: 0.01\n",
      "Epoch [5551/20000], Loss: 6.4555816650390625, Learning Rate: 0.01\n",
      "Epoch [5552/20000], Loss: 6.4486541748046875, Learning Rate: 0.01\n",
      "Epoch [5553/20000], Loss: 6.4418487548828125, Learning Rate: 0.01\n",
      "Epoch [5554/20000], Loss: 6.434906005859375, Learning Rate: 0.01\n",
      "Epoch [5555/20000], Loss: 6.427734375, Learning Rate: 0.01\n",
      "Epoch [5556/20000], Loss: 6.4205322265625, Learning Rate: 0.01\n",
      "Epoch [5557/20000], Loss: 6.4132232666015625, Learning Rate: 0.01\n",
      "Epoch [5558/20000], Loss: 6.405975341796875, Learning Rate: 0.01\n",
      "Epoch [5559/20000], Loss: 6.398712158203125, Learning Rate: 0.01\n",
      "Epoch [5560/20000], Loss: 6.3915557861328125, Learning Rate: 0.01\n",
      "Epoch [5561/20000], Loss: 6.3843841552734375, Learning Rate: 0.01\n",
      "Epoch [5562/20000], Loss: 6.3773345947265625, Learning Rate: 0.01\n",
      "Epoch [5563/20000], Loss: 6.370330810546875, Learning Rate: 0.01\n",
      "Epoch [5564/20000], Loss: 6.3632354736328125, Learning Rate: 0.01\n",
      "Epoch [5565/20000], Loss: 6.3563079833984375, Learning Rate: 0.01\n",
      "Epoch [5566/20000], Loss: 6.34912109375, Learning Rate: 0.01\n",
      "Epoch [5567/20000], Loss: 6.3419647216796875, Learning Rate: 0.01\n",
      "Epoch [5568/20000], Loss: 6.3348541259765625, Learning Rate: 0.01\n",
      "Epoch [5569/20000], Loss: 6.3276214599609375, Learning Rate: 0.01\n",
      "Epoch [5570/20000], Loss: 6.320465087890625, Learning Rate: 0.01\n",
      "Epoch [5571/20000], Loss: 6.3133544921875, Learning Rate: 0.01\n",
      "Epoch [5572/20000], Loss: 6.306243896484375, Learning Rate: 0.01\n",
      "Epoch [5573/20000], Loss: 6.29913330078125, Learning Rate: 0.01\n",
      "Epoch [5574/20000], Loss: 6.292022705078125, Learning Rate: 0.01\n",
      "Epoch [5575/20000], Loss: 6.284881591796875, Learning Rate: 0.01\n",
      "Epoch [5576/20000], Loss: 6.2777557373046875, Learning Rate: 0.01\n",
      "Epoch [5577/20000], Loss: 6.27069091796875, Learning Rate: 0.01\n",
      "Epoch [5578/20000], Loss: 6.2635650634765625, Learning Rate: 0.01\n",
      "Epoch [5579/20000], Loss: 6.25653076171875, Learning Rate: 0.01\n",
      "Epoch [5580/20000], Loss: 6.2493896484375, Learning Rate: 0.01\n",
      "Epoch [5581/20000], Loss: 6.2422637939453125, Learning Rate: 0.01\n",
      "Epoch [5582/20000], Loss: 6.2351837158203125, Learning Rate: 0.01\n",
      "Epoch [5583/20000], Loss: 6.228057861328125, Learning Rate: 0.01\n",
      "Epoch [5584/20000], Loss: 6.2208251953125, Learning Rate: 0.01\n",
      "Epoch [5585/20000], Loss: 6.2136993408203125, Learning Rate: 0.01\n",
      "Epoch [5586/20000], Loss: 6.20672607421875, Learning Rate: 0.01\n",
      "Epoch [5587/20000], Loss: 6.199493408203125, Learning Rate: 0.01\n",
      "Epoch [5588/20000], Loss: 6.1924285888671875, Learning Rate: 0.01\n",
      "Epoch [5589/20000], Loss: 6.185302734375, Learning Rate: 0.01\n",
      "Epoch [5590/20000], Loss: 6.17822265625, Learning Rate: 0.01\n",
      "Epoch [5591/20000], Loss: 6.1711578369140625, Learning Rate: 0.01\n",
      "Epoch [5592/20000], Loss: 6.1638946533203125, Learning Rate: 0.01\n",
      "Epoch [5593/20000], Loss: 6.1568756103515625, Learning Rate: 0.01\n",
      "Epoch [5594/20000], Loss: 6.1498260498046875, Learning Rate: 0.01\n",
      "Epoch [5595/20000], Loss: 6.142608642578125, Learning Rate: 0.01\n",
      "Epoch [5596/20000], Loss: 6.13555908203125, Learning Rate: 0.01\n",
      "Epoch [5597/20000], Loss: 6.12835693359375, Learning Rate: 0.01\n",
      "Epoch [5598/20000], Loss: 6.1213226318359375, Learning Rate: 0.01\n",
      "Epoch [5599/20000], Loss: 6.1140594482421875, Learning Rate: 0.01\n",
      "Epoch [5600/20000], Loss: 6.1071014404296875, Learning Rate: 0.01\n",
      "Epoch [5601/20000], Loss: 6.0999298095703125, Learning Rate: 0.01\n",
      "Epoch [5602/20000], Loss: 6.092803955078125, Learning Rate: 0.01\n",
      "Epoch [5603/20000], Loss: 6.0857086181640625, Learning Rate: 0.01\n",
      "Epoch [5604/20000], Loss: 6.0784759521484375, Learning Rate: 0.01\n",
      "Epoch [5605/20000], Loss: 6.07147216796875, Learning Rate: 0.01\n",
      "Epoch [5606/20000], Loss: 6.064300537109375, Learning Rate: 0.01\n",
      "Epoch [5607/20000], Loss: 6.0572357177734375, Learning Rate: 0.01\n",
      "Epoch [5608/20000], Loss: 6.0500946044921875, Learning Rate: 0.01\n",
      "Epoch [5609/20000], Loss: 6.0428924560546875, Learning Rate: 0.01\n",
      "Epoch [5610/20000], Loss: 6.035797119140625, Learning Rate: 0.01\n",
      "Epoch [5611/20000], Loss: 6.0287628173828125, Learning Rate: 0.01\n",
      "Epoch [5612/20000], Loss: 6.0216217041015625, Learning Rate: 0.01\n",
      "Epoch [5613/20000], Loss: 6.0144195556640625, Learning Rate: 0.01\n",
      "Epoch [5614/20000], Loss: 6.007293701171875, Learning Rate: 0.01\n",
      "Epoch [5615/20000], Loss: 6.0002593994140625, Learning Rate: 0.01\n",
      "Epoch [5616/20000], Loss: 5.9931488037109375, Learning Rate: 0.01\n",
      "Epoch [5617/20000], Loss: 5.9859161376953125, Learning Rate: 0.01\n",
      "Epoch [5618/20000], Loss: 5.9787750244140625, Learning Rate: 0.01\n",
      "Epoch [5619/20000], Loss: 5.9715728759765625, Learning Rate: 0.01\n",
      "Epoch [5620/20000], Loss: 5.964569091796875, Learning Rate: 0.01\n",
      "Epoch [5621/20000], Loss: 5.9574432373046875, Learning Rate: 0.01\n",
      "Epoch [5622/20000], Loss: 5.9503173828125, Learning Rate: 0.01\n",
      "Epoch [5623/20000], Loss: 5.943206787109375, Learning Rate: 0.01\n",
      "Epoch [5624/20000], Loss: 5.93609619140625, Learning Rate: 0.01\n",
      "Epoch [5625/20000], Loss: 5.928955078125, Learning Rate: 0.01\n",
      "Epoch [5626/20000], Loss: 5.9217987060546875, Learning Rate: 0.01\n",
      "Epoch [5627/20000], Loss: 5.914642333984375, Learning Rate: 0.01\n",
      "Epoch [5628/20000], Loss: 5.9075164794921875, Learning Rate: 0.01\n",
      "Epoch [5629/20000], Loss: 5.9004058837890625, Learning Rate: 0.01\n",
      "Epoch [5630/20000], Loss: 5.893341064453125, Learning Rate: 0.01\n",
      "Epoch [5631/20000], Loss: 5.88616943359375, Learning Rate: 0.01\n",
      "Epoch [5632/20000], Loss: 5.8790283203125, Learning Rate: 0.01\n",
      "Epoch [5633/20000], Loss: 5.871978759765625, Learning Rate: 0.01\n",
      "Epoch [5634/20000], Loss: 5.86480712890625, Learning Rate: 0.01\n",
      "Epoch [5635/20000], Loss: 5.8577117919921875, Learning Rate: 0.01\n",
      "Epoch [5636/20000], Loss: 5.8505706787109375, Learning Rate: 0.01\n",
      "Epoch [5637/20000], Loss: 5.843536376953125, Learning Rate: 0.01\n",
      "Epoch [5638/20000], Loss: 5.83648681640625, Learning Rate: 0.01\n",
      "Epoch [5639/20000], Loss: 5.8294219970703125, Learning Rate: 0.01\n",
      "Epoch [5640/20000], Loss: 5.8226165771484375, Learning Rate: 0.01\n",
      "Epoch [5641/20000], Loss: 5.8156585693359375, Learning Rate: 0.01\n",
      "Epoch [5642/20000], Loss: 5.8088531494140625, Learning Rate: 0.01\n",
      "Epoch [5643/20000], Loss: 5.80230712890625, Learning Rate: 0.01\n",
      "Epoch [5644/20000], Loss: 5.7960662841796875, Learning Rate: 0.01\n",
      "Epoch [5645/20000], Loss: 5.7901153564453125, Learning Rate: 0.01\n",
      "Epoch [5646/20000], Loss: 5.7848052978515625, Learning Rate: 0.01\n",
      "Epoch [5647/20000], Loss: 5.780487060546875, Learning Rate: 0.01\n",
      "Epoch [5648/20000], Loss: 5.77764892578125, Learning Rate: 0.01\n",
      "Epoch [5649/20000], Loss: 5.7770843505859375, Learning Rate: 0.01\n",
      "Epoch [5650/20000], Loss: 5.7802734375, Learning Rate: 0.01\n",
      "Epoch [5651/20000], Loss: 5.789093017578125, Learning Rate: 0.01\n",
      "Epoch [5652/20000], Loss: 5.8070526123046875, Learning Rate: 0.01\n",
      "Epoch [5653/20000], Loss: 5.839447021484375, Learning Rate: 0.01\n",
      "Epoch [5654/20000], Loss: 5.8948974609375, Learning Rate: 0.01\n",
      "Epoch [5655/20000], Loss: 5.9873809814453125, Learning Rate: 0.01\n",
      "Epoch [5656/20000], Loss: 6.13934326171875, Learning Rate: 0.01\n",
      "Epoch [5657/20000], Loss: 6.3869476318359375, Learning Rate: 0.01\n",
      "Epoch [5658/20000], Loss: 6.7878265380859375, Learning Rate: 0.01\n",
      "Epoch [5659/20000], Loss: 7.43182373046875, Learning Rate: 0.01\n",
      "Epoch [5660/20000], Loss: 8.457199096679688, Learning Rate: 0.01\n",
      "Epoch [5661/20000], Loss: 10.057952880859375, Learning Rate: 0.01\n",
      "Epoch [5662/20000], Loss: 12.486846923828125, Learning Rate: 0.01\n",
      "Epoch [5663/20000], Loss: 15.966842651367188, Learning Rate: 0.01\n",
      "Epoch [5664/20000], Loss: 20.526947021484375, Learning Rate: 0.01\n",
      "Epoch [5665/20000], Loss: 25.52685546875, Learning Rate: 0.01\n",
      "Epoch [5666/20000], Loss: 29.342880249023438, Learning Rate: 0.01\n",
      "Epoch [5667/20000], Loss: 29.402847290039062, Learning Rate: 0.01\n",
      "Epoch [5668/20000], Loss: 24.216278076171875, Learning Rate: 0.01\n",
      "Epoch [5669/20000], Loss: 15.494461059570312, Learning Rate: 0.01\n",
      "Epoch [5670/20000], Loss: 8.14990234375, Learning Rate: 0.01\n",
      "Epoch [5671/20000], Loss: 6.1974334716796875, Learning Rate: 0.01\n",
      "Epoch [5672/20000], Loss: 9.463470458984375, Learning Rate: 0.01\n",
      "Epoch [5673/20000], Loss: 14.150314331054688, Learning Rate: 0.01\n",
      "Epoch [5674/20000], Loss: 15.976058959960938, Learning Rate: 0.01\n",
      "Epoch [5675/20000], Loss: 13.392562866210938, Learning Rate: 0.01\n",
      "Epoch [5676/20000], Loss: 8.701492309570312, Learning Rate: 0.01\n",
      "Epoch [5677/20000], Loss: 5.8421630859375, Learning Rate: 0.01\n",
      "Epoch [5678/20000], Loss: 6.573699951171875, Learning Rate: 0.01\n",
      "Epoch [5679/20000], Loss: 9.202590942382812, Learning Rate: 0.01\n",
      "Epoch [5680/20000], Loss: 10.686843872070312, Learning Rate: 0.01\n",
      "Epoch [5681/20000], Loss: 9.561172485351562, Learning Rate: 0.01\n",
      "Epoch [5682/20000], Loss: 7.0653076171875, Learning Rate: 0.01\n",
      "Epoch [5683/20000], Loss: 5.562835693359375, Learning Rate: 0.01\n",
      "Epoch [5684/20000], Loss: 6.089874267578125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5685/20000], Loss: 7.5843353271484375, Learning Rate: 0.01\n",
      "Epoch [5686/20000], Loss: 8.27984619140625, Learning Rate: 0.01\n",
      "Epoch [5687/20000], Loss: 7.512542724609375, Learning Rate: 0.01\n",
      "Epoch [5688/20000], Loss: 6.160400390625, Learning Rate: 0.01\n",
      "Epoch [5689/20000], Loss: 5.52587890625, Learning Rate: 0.01\n",
      "Epoch [5690/20000], Loss: 5.9765472412109375, Learning Rate: 0.01\n",
      "Epoch [5691/20000], Loss: 6.762420654296875, Learning Rate: 0.01\n",
      "Epoch [5692/20000], Loss: 6.95648193359375, Learning Rate: 0.01\n",
      "Epoch [5693/20000], Loss: 6.3859405517578125, Learning Rate: 0.01\n",
      "Epoch [5694/20000], Loss: 5.6646270751953125, Learning Rate: 0.01\n",
      "Epoch [5695/20000], Loss: 5.4464111328125, Learning Rate: 0.01\n",
      "Epoch [5696/20000], Loss: 5.778411865234375, Learning Rate: 0.01\n",
      "Epoch [5697/20000], Loss: 6.17169189453125, Learning Rate: 0.01\n",
      "Epoch [5698/20000], Loss: 6.178314208984375, Learning Rate: 0.01\n",
      "Epoch [5699/20000], Loss: 5.8159942626953125, Learning Rate: 0.01\n",
      "Epoch [5700/20000], Loss: 5.461090087890625, Learning Rate: 0.01\n",
      "Epoch [5701/20000], Loss: 5.4156951904296875, Learning Rate: 0.01\n",
      "Epoch [5702/20000], Loss: 5.6298370361328125, Learning Rate: 0.01\n",
      "Epoch [5703/20000], Loss: 5.8177642822265625, Learning Rate: 0.01\n",
      "Epoch [5704/20000], Loss: 5.7761688232421875, Learning Rate: 0.01\n",
      "Epoch [5705/20000], Loss: 5.561065673828125, Learning Rate: 0.01\n",
      "Epoch [5706/20000], Loss: 5.3840179443359375, Learning Rate: 0.01\n",
      "Epoch [5707/20000], Loss: 5.3812255859375, Learning Rate: 0.01\n",
      "Epoch [5708/20000], Loss: 5.4997406005859375, Learning Rate: 0.01\n",
      "Epoch [5709/20000], Loss: 5.584320068359375, Learning Rate: 0.01\n",
      "Epoch [5710/20000], Loss: 5.54254150390625, Learning Rate: 0.01\n",
      "Epoch [5711/20000], Loss: 5.41851806640625, Learning Rate: 0.01\n",
      "Epoch [5712/20000], Loss: 5.324859619140625, Learning Rate: 0.01\n",
      "Epoch [5713/20000], Loss: 5.325439453125, Learning Rate: 0.01\n",
      "Epoch [5714/20000], Loss: 5.3862762451171875, Learning Rate: 0.01\n",
      "Epoch [5715/20000], Loss: 5.4257659912109375, Learning Rate: 0.01\n",
      "Epoch [5716/20000], Loss: 5.3985595703125, Learning Rate: 0.01\n",
      "Epoch [5717/20000], Loss: 5.329071044921875, Learning Rate: 0.01\n",
      "Epoch [5718/20000], Loss: 5.27581787109375, Learning Rate: 0.01\n",
      "Epoch [5719/20000], Loss: 5.2719879150390625, Learning Rate: 0.01\n",
      "Epoch [5720/20000], Loss: 5.3009185791015625, Learning Rate: 0.01\n",
      "Epoch [5721/20000], Loss: 5.32061767578125, Learning Rate: 0.01\n",
      "Epoch [5722/20000], Loss: 5.30584716796875, Learning Rate: 0.01\n",
      "Epoch [5723/20000], Loss: 5.266937255859375, Learning Rate: 0.01\n",
      "Epoch [5724/20000], Loss: 5.233673095703125, Learning Rate: 0.01\n",
      "Epoch [5725/20000], Loss: 5.225341796875, Learning Rate: 0.01\n",
      "Epoch [5726/20000], Loss: 5.2362060546875, Learning Rate: 0.01\n",
      "Epoch [5727/20000], Loss: 5.24560546875, Learning Rate: 0.01\n",
      "Epoch [5728/20000], Loss: 5.23809814453125, Learning Rate: 0.01\n",
      "Epoch [5729/20000], Loss: 5.216033935546875, Learning Rate: 0.01\n",
      "Epoch [5730/20000], Loss: 5.19366455078125, Learning Rate: 0.01\n",
      "Epoch [5731/20000], Loss: 5.1827545166015625, Learning Rate: 0.01\n",
      "Epoch [5732/20000], Loss: 5.1834716796875, Learning Rate: 0.01\n",
      "Epoch [5733/20000], Loss: 5.1864776611328125, Learning Rate: 0.01\n",
      "Epoch [5734/20000], Loss: 5.182403564453125, Learning Rate: 0.01\n",
      "Epoch [5735/20000], Loss: 5.1695098876953125, Learning Rate: 0.01\n",
      "Epoch [5736/20000], Loss: 5.1539306640625, Learning Rate: 0.01\n",
      "Epoch [5737/20000], Loss: 5.142730712890625, Learning Rate: 0.01\n",
      "Epoch [5738/20000], Loss: 5.138092041015625, Learning Rate: 0.01\n",
      "Epoch [5739/20000], Loss: 5.13665771484375, Learning Rate: 0.01\n",
      "Epoch [5740/20000], Loss: 5.1333465576171875, Learning Rate: 0.01\n",
      "Epoch [5741/20000], Loss: 5.1255950927734375, Learning Rate: 0.01\n",
      "Epoch [5742/20000], Loss: 5.1148529052734375, Learning Rate: 0.01\n",
      "Epoch [5743/20000], Loss: 5.104949951171875, Learning Rate: 0.01\n",
      "Epoch [5744/20000], Loss: 5.098388671875, Learning Rate: 0.01\n",
      "Epoch [5745/20000], Loss: 5.0947418212890625, Learning Rate: 0.01\n",
      "Epoch [5746/20000], Loss: 5.0919036865234375, Learning Rate: 0.01\n",
      "Epoch [5747/20000], Loss: 5.0878753662109375, Learning Rate: 0.01\n",
      "Epoch [5748/20000], Loss: 5.08245849609375, Learning Rate: 0.01\n",
      "Epoch [5749/20000], Loss: 5.0774078369140625, Learning Rate: 0.01\n",
      "Epoch [5750/20000], Loss: 5.0748443603515625, Learning Rate: 0.01\n",
      "Epoch [5751/20000], Loss: 5.0762176513671875, Learning Rate: 0.01\n",
      "Epoch [5752/20000], Loss: 5.08209228515625, Learning Rate: 0.01\n",
      "Epoch [5753/20000], Loss: 5.0927886962890625, Learning Rate: 0.01\n",
      "Epoch [5754/20000], Loss: 5.1098785400390625, Learning Rate: 0.01\n",
      "Epoch [5755/20000], Loss: 5.1370086669921875, Learning Rate: 0.01\n",
      "Epoch [5756/20000], Loss: 5.179962158203125, Learning Rate: 0.01\n",
      "Epoch [5757/20000], Loss: 5.247344970703125, Learning Rate: 0.01\n",
      "Epoch [5758/20000], Loss: 5.351043701171875, Learning Rate: 0.01\n",
      "Epoch [5759/20000], Loss: 5.5073699951171875, Learning Rate: 0.01\n",
      "Epoch [5760/20000], Loss: 5.740509033203125, Learning Rate: 0.01\n",
      "Epoch [5761/20000], Loss: 6.084075927734375, Learning Rate: 0.01\n",
      "Epoch [5762/20000], Loss: 6.58441162109375, Learning Rate: 0.01\n",
      "Epoch [5763/20000], Loss: 7.297088623046875, Learning Rate: 0.01\n",
      "Epoch [5764/20000], Loss: 8.277969360351562, Learning Rate: 0.01\n",
      "Epoch [5765/20000], Loss: 9.553085327148438, Learning Rate: 0.01\n",
      "Epoch [5766/20000], Loss: 11.067398071289062, Learning Rate: 0.01\n",
      "Epoch [5767/20000], Loss: 12.600006103515625, Learning Rate: 0.01\n",
      "Epoch [5768/20000], Loss: 13.71728515625, Learning Rate: 0.01\n",
      "Epoch [5769/20000], Loss: 13.8388671875, Learning Rate: 0.01\n",
      "Epoch [5770/20000], Loss: 12.565109252929688, Learning Rate: 0.01\n",
      "Epoch [5771/20000], Loss: 10.083709716796875, Learning Rate: 0.01\n",
      "Epoch [5772/20000], Loss: 7.3054351806640625, Learning Rate: 0.01\n",
      "Epoch [5773/20000], Loss: 5.3714141845703125, Learning Rate: 0.01\n",
      "Epoch [5774/20000], Loss: 4.9267578125, Learning Rate: 0.01\n",
      "Epoch [5775/20000], Loss: 5.7758941650390625, Learning Rate: 0.01\n",
      "Epoch [5776/20000], Loss: 7.1143798828125, Learning Rate: 0.01\n",
      "Epoch [5777/20000], Loss: 8.033584594726562, Learning Rate: 0.01\n",
      "Epoch [5778/20000], Loss: 7.9852142333984375, Learning Rate: 0.01\n",
      "Epoch [5779/20000], Loss: 7.0414886474609375, Learning Rate: 0.01\n",
      "Epoch [5780/20000], Loss: 5.802398681640625, Learning Rate: 0.01\n",
      "Epoch [5781/20000], Loss: 4.9746246337890625, Learning Rate: 0.01\n",
      "Epoch [5782/20000], Loss: 4.9045257568359375, Learning Rate: 0.01\n",
      "Epoch [5783/20000], Loss: 5.4195404052734375, Learning Rate: 0.01\n",
      "Epoch [5784/20000], Loss: 6.031494140625, Learning Rate: 0.01\n",
      "Epoch [5785/20000], Loss: 6.287994384765625, Learning Rate: 0.01\n",
      "Epoch [5786/20000], Loss: 6.0394134521484375, Learning Rate: 0.01\n",
      "Epoch [5787/20000], Loss: 5.48150634765625, Learning Rate: 0.01\n",
      "Epoch [5788/20000], Loss: 4.9768829345703125, Learning Rate: 0.01\n",
      "Epoch [5789/20000], Loss: 4.7931365966796875, Learning Rate: 0.01\n",
      "Epoch [5790/20000], Loss: 4.947998046875, Learning Rate: 0.01\n",
      "Epoch [5791/20000], Loss: 5.245849609375, Learning Rate: 0.01\n",
      "Epoch [5792/20000], Loss: 5.44244384765625, Learning Rate: 0.01\n",
      "Epoch [5793/20000], Loss: 5.405029296875, Learning Rate: 0.01\n",
      "Epoch [5794/20000], Loss: 5.1748046875, Learning Rate: 0.01\n",
      "Epoch [5795/20000], Loss: 4.9083251953125, Learning Rate: 0.01\n",
      "Epoch [5796/20000], Loss: 4.7572784423828125, Learning Rate: 0.01\n",
      "Epoch [5797/20000], Loss: 4.773956298828125, Learning Rate: 0.01\n",
      "Epoch [5798/20000], Loss: 4.8977813720703125, Learning Rate: 0.01\n",
      "Epoch [5799/20000], Loss: 5.0156707763671875, Learning Rate: 0.01\n",
      "Epoch [5800/20000], Loss: 5.0410614013671875, Learning Rate: 0.01\n",
      "Epoch [5801/20000], Loss: 4.9620361328125, Learning Rate: 0.01\n",
      "Epoch [5802/20000], Loss: 4.8329010009765625, Learning Rate: 0.01\n",
      "Epoch [5803/20000], Loss: 4.7290802001953125, Learning Rate: 0.01\n",
      "Epoch [5804/20000], Loss: 4.696807861328125, Learning Rate: 0.01\n",
      "Epoch [5805/20000], Loss: 4.73095703125, Learning Rate: 0.01\n",
      "Epoch [5806/20000], Loss: 4.7888031005859375, Learning Rate: 0.01\n",
      "Epoch [5807/20000], Loss: 4.8229217529296875, Learning Rate: 0.01\n",
      "Epoch [5808/20000], Loss: 4.809356689453125, Learning Rate: 0.01\n",
      "Epoch [5809/20000], Loss: 4.7571563720703125, Learning Rate: 0.01\n",
      "Epoch [5810/20000], Loss: 4.6958770751953125, Learning Rate: 0.01\n",
      "Epoch [5811/20000], Loss: 4.655303955078125, Learning Rate: 0.01\n",
      "Epoch [5812/20000], Loss: 4.6475982666015625, Learning Rate: 0.01\n",
      "Epoch [5813/20000], Loss: 4.6643218994140625, Learning Rate: 0.01\n",
      "Epoch [5814/20000], Loss: 4.6855316162109375, Learning Rate: 0.01\n",
      "Epoch [5815/20000], Loss: 4.6928863525390625, Learning Rate: 0.01\n",
      "Epoch [5816/20000], Loss: 4.6795654296875, Learning Rate: 0.01\n",
      "Epoch [5817/20000], Loss: 4.6513824462890625, Learning Rate: 0.01\n",
      "Epoch [5818/20000], Loss: 4.6214752197265625, Learning Rate: 0.01\n",
      "Epoch [5819/20000], Loss: 4.6009674072265625, Learning Rate: 0.01\n",
      "Epoch [5820/20000], Loss: 4.5943450927734375, Learning Rate: 0.01\n",
      "Epoch [5821/20000], Loss: 4.5979156494140625, Learning Rate: 0.01\n",
      "Epoch [5822/20000], Loss: 4.6035308837890625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5823/20000], Loss: 4.6035308837890625, Learning Rate: 0.01\n",
      "Epoch [5824/20000], Loss: 4.59515380859375, Learning Rate: 0.01\n",
      "Epoch [5825/20000], Loss: 4.580230712890625, Learning Rate: 0.01\n",
      "Epoch [5826/20000], Loss: 4.5633697509765625, Learning Rate: 0.01\n",
      "Epoch [5827/20000], Loss: 4.549713134765625, Learning Rate: 0.01\n",
      "Epoch [5828/20000], Loss: 4.5415191650390625, Learning Rate: 0.01\n",
      "Epoch [5829/20000], Loss: 4.5381011962890625, Learning Rate: 0.01\n",
      "Epoch [5830/20000], Loss: 4.53668212890625, Learning Rate: 0.01\n",
      "Epoch [5831/20000], Loss: 4.534210205078125, Learning Rate: 0.01\n",
      "Epoch [5832/20000], Loss: 4.52862548828125, Learning Rate: 0.01\n",
      "Epoch [5833/20000], Loss: 4.519927978515625, Learning Rate: 0.01\n",
      "Epoch [5834/20000], Loss: 4.5094757080078125, Learning Rate: 0.01\n",
      "Epoch [5835/20000], Loss: 4.49908447265625, Learning Rate: 0.01\n",
      "Epoch [5836/20000], Loss: 4.4903411865234375, Learning Rate: 0.01\n",
      "Epoch [5837/20000], Loss: 4.483795166015625, Learning Rate: 0.01\n",
      "Epoch [5838/20000], Loss: 4.4788360595703125, Learning Rate: 0.01\n",
      "Epoch [5839/20000], Loss: 4.474395751953125, Learning Rate: 0.01\n",
      "Epoch [5840/20000], Loss: 4.4693603515625, Learning Rate: 0.01\n",
      "Epoch [5841/20000], Loss: 4.463287353515625, Learning Rate: 0.01\n",
      "Epoch [5842/20000], Loss: 4.4559478759765625, Learning Rate: 0.01\n",
      "Epoch [5843/20000], Loss: 4.44793701171875, Learning Rate: 0.01\n",
      "Epoch [5844/20000], Loss: 4.4398040771484375, Learning Rate: 0.01\n",
      "Epoch [5845/20000], Loss: 4.432403564453125, Learning Rate: 0.01\n",
      "Epoch [5846/20000], Loss: 4.425689697265625, Learning Rate: 0.01\n",
      "Epoch [5847/20000], Loss: 4.4196624755859375, Learning Rate: 0.01\n",
      "Epoch [5848/20000], Loss: 4.4139556884765625, Learning Rate: 0.01\n",
      "Epoch [5849/20000], Loss: 4.408203125, Learning Rate: 0.01\n",
      "Epoch [5850/20000], Loss: 4.402130126953125, Learning Rate: 0.01\n",
      "Epoch [5851/20000], Loss: 4.395599365234375, Learning Rate: 0.01\n",
      "Epoch [5852/20000], Loss: 4.388641357421875, Learning Rate: 0.01\n",
      "Epoch [5853/20000], Loss: 4.38165283203125, Learning Rate: 0.01\n",
      "Epoch [5854/20000], Loss: 4.3748321533203125, Learning Rate: 0.01\n",
      "Epoch [5855/20000], Loss: 4.36798095703125, Learning Rate: 0.01\n",
      "Epoch [5856/20000], Loss: 4.361328125, Learning Rate: 0.01\n",
      "Epoch [5857/20000], Loss: 4.355072021484375, Learning Rate: 0.01\n",
      "Epoch [5858/20000], Loss: 4.348846435546875, Learning Rate: 0.01\n",
      "Epoch [5859/20000], Loss: 4.3426971435546875, Learning Rate: 0.01\n",
      "Epoch [5860/20000], Loss: 4.3363800048828125, Learning Rate: 0.01\n",
      "Epoch [5861/20000], Loss: 4.3299560546875, Learning Rate: 0.01\n",
      "Epoch [5862/20000], Loss: 4.3234405517578125, Learning Rate: 0.01\n",
      "Epoch [5863/20000], Loss: 4.3167877197265625, Learning Rate: 0.01\n",
      "Epoch [5864/20000], Loss: 4.31005859375, Learning Rate: 0.01\n",
      "Epoch [5865/20000], Loss: 4.30352783203125, Learning Rate: 0.01\n",
      "Epoch [5866/20000], Loss: 4.296966552734375, Learning Rate: 0.01\n",
      "Epoch [5867/20000], Loss: 4.2905426025390625, Learning Rate: 0.01\n",
      "Epoch [5868/20000], Loss: 4.2840118408203125, Learning Rate: 0.01\n",
      "Epoch [5869/20000], Loss: 4.2776947021484375, Learning Rate: 0.01\n",
      "Epoch [5870/20000], Loss: 4.2713165283203125, Learning Rate: 0.01\n",
      "Epoch [5871/20000], Loss: 4.2649078369140625, Learning Rate: 0.01\n",
      "Epoch [5872/20000], Loss: 4.25848388671875, Learning Rate: 0.01\n",
      "Epoch [5873/20000], Loss: 4.251983642578125, Learning Rate: 0.01\n",
      "Epoch [5874/20000], Loss: 4.2454986572265625, Learning Rate: 0.01\n",
      "Epoch [5875/20000], Loss: 4.238922119140625, Learning Rate: 0.01\n",
      "Epoch [5876/20000], Loss: 4.2324066162109375, Learning Rate: 0.01\n",
      "Epoch [5877/20000], Loss: 4.2259521484375, Learning Rate: 0.01\n",
      "Epoch [5878/20000], Loss: 4.2194976806640625, Learning Rate: 0.01\n",
      "Epoch [5879/20000], Loss: 4.2130584716796875, Learning Rate: 0.01\n",
      "Epoch [5880/20000], Loss: 4.2066650390625, Learning Rate: 0.01\n",
      "Epoch [5881/20000], Loss: 4.2000579833984375, Learning Rate: 0.01\n",
      "Epoch [5882/20000], Loss: 4.19366455078125, Learning Rate: 0.01\n",
      "Epoch [5883/20000], Loss: 4.1872406005859375, Learning Rate: 0.01\n",
      "Epoch [5884/20000], Loss: 4.180816650390625, Learning Rate: 0.01\n",
      "Epoch [5885/20000], Loss: 4.1742401123046875, Learning Rate: 0.01\n",
      "Epoch [5886/20000], Loss: 4.1678619384765625, Learning Rate: 0.01\n",
      "Epoch [5887/20000], Loss: 4.161407470703125, Learning Rate: 0.01\n",
      "Epoch [5888/20000], Loss: 4.1548614501953125, Learning Rate: 0.01\n",
      "Epoch [5889/20000], Loss: 4.1484832763671875, Learning Rate: 0.01\n",
      "Epoch [5890/20000], Loss: 4.141937255859375, Learning Rate: 0.01\n",
      "Epoch [5891/20000], Loss: 4.1353912353515625, Learning Rate: 0.01\n",
      "Epoch [5892/20000], Loss: 4.1290130615234375, Learning Rate: 0.01\n",
      "Epoch [5893/20000], Loss: 4.1224517822265625, Learning Rate: 0.01\n",
      "Epoch [5894/20000], Loss: 4.116058349609375, Learning Rate: 0.01\n",
      "Epoch [5895/20000], Loss: 4.109588623046875, Learning Rate: 0.01\n",
      "Epoch [5896/20000], Loss: 4.10308837890625, Learning Rate: 0.01\n",
      "Epoch [5897/20000], Loss: 4.09661865234375, Learning Rate: 0.01\n",
      "Epoch [5898/20000], Loss: 4.0900726318359375, Learning Rate: 0.01\n",
      "Epoch [5899/20000], Loss: 4.0836181640625, Learning Rate: 0.01\n",
      "Epoch [5900/20000], Loss: 4.0771942138671875, Learning Rate: 0.01\n",
      "Epoch [5901/20000], Loss: 4.0706787109375, Learning Rate: 0.01\n",
      "Epoch [5902/20000], Loss: 4.0642242431640625, Learning Rate: 0.01\n",
      "Epoch [5903/20000], Loss: 4.057769775390625, Learning Rate: 0.01\n",
      "Epoch [5904/20000], Loss: 4.0512847900390625, Learning Rate: 0.01\n",
      "Epoch [5905/20000], Loss: 4.0448455810546875, Learning Rate: 0.01\n",
      "Epoch [5906/20000], Loss: 4.0383148193359375, Learning Rate: 0.01\n",
      "Epoch [5907/20000], Loss: 4.0318450927734375, Learning Rate: 0.01\n",
      "Epoch [5908/20000], Loss: 4.025360107421875, Learning Rate: 0.01\n",
      "Epoch [5909/20000], Loss: 4.0188446044921875, Learning Rate: 0.01\n",
      "Epoch [5910/20000], Loss: 4.0124359130859375, Learning Rate: 0.01\n",
      "Epoch [5911/20000], Loss: 4.00592041015625, Learning Rate: 0.01\n",
      "Epoch [5912/20000], Loss: 3.9994659423828125, Learning Rate: 0.01\n",
      "Epoch [5913/20000], Loss: 3.992919921875, Learning Rate: 0.01\n",
      "Epoch [5914/20000], Loss: 3.9864959716796875, Learning Rate: 0.01\n",
      "Epoch [5915/20000], Loss: 3.97998046875, Learning Rate: 0.01\n",
      "Epoch [5916/20000], Loss: 3.9734649658203125, Learning Rate: 0.01\n",
      "Epoch [5917/20000], Loss: 3.9669647216796875, Learning Rate: 0.01\n",
      "Epoch [5918/20000], Loss: 3.960357666015625, Learning Rate: 0.01\n",
      "Epoch [5919/20000], Loss: 3.954010009765625, Learning Rate: 0.01\n",
      "Epoch [5920/20000], Loss: 3.947509765625, Learning Rate: 0.01\n",
      "Epoch [5921/20000], Loss: 3.9410400390625, Learning Rate: 0.01\n",
      "Epoch [5922/20000], Loss: 3.9344482421875, Learning Rate: 0.01\n",
      "Epoch [5923/20000], Loss: 3.927947998046875, Learning Rate: 0.01\n",
      "Epoch [5924/20000], Loss: 3.9215240478515625, Learning Rate: 0.01\n",
      "Epoch [5925/20000], Loss: 3.9150390625, Learning Rate: 0.01\n",
      "Epoch [5926/20000], Loss: 3.9085540771484375, Learning Rate: 0.01\n",
      "Epoch [5927/20000], Loss: 3.9019927978515625, Learning Rate: 0.01\n",
      "Epoch [5928/20000], Loss: 3.895477294921875, Learning Rate: 0.01\n",
      "Epoch [5929/20000], Loss: 3.8890838623046875, Learning Rate: 0.01\n",
      "Epoch [5930/20000], Loss: 3.8825531005859375, Learning Rate: 0.01\n",
      "Epoch [5931/20000], Loss: 3.8761138916015625, Learning Rate: 0.01\n",
      "Epoch [5932/20000], Loss: 3.86956787109375, Learning Rate: 0.01\n",
      "Epoch [5933/20000], Loss: 3.8631134033203125, Learning Rate: 0.01\n",
      "Epoch [5934/20000], Loss: 3.8565826416015625, Learning Rate: 0.01\n",
      "Epoch [5935/20000], Loss: 3.8500823974609375, Learning Rate: 0.01\n",
      "Epoch [5936/20000], Loss: 3.8436126708984375, Learning Rate: 0.01\n",
      "Epoch [5937/20000], Loss: 3.8371429443359375, Learning Rate: 0.01\n",
      "Epoch [5938/20000], Loss: 3.8307342529296875, Learning Rate: 0.01\n",
      "Epoch [5939/20000], Loss: 3.8242950439453125, Learning Rate: 0.01\n",
      "Epoch [5940/20000], Loss: 3.8179168701171875, Learning Rate: 0.01\n",
      "Epoch [5941/20000], Loss: 3.81158447265625, Learning Rate: 0.01\n",
      "Epoch [5942/20000], Loss: 3.8052520751953125, Learning Rate: 0.01\n",
      "Epoch [5943/20000], Loss: 3.79913330078125, Learning Rate: 0.01\n",
      "Epoch [5944/20000], Loss: 3.792999267578125, Learning Rate: 0.01\n",
      "Epoch [5945/20000], Loss: 3.7872467041015625, Learning Rate: 0.01\n",
      "Epoch [5946/20000], Loss: 3.78179931640625, Learning Rate: 0.01\n",
      "Epoch [5947/20000], Loss: 3.7767791748046875, Learning Rate: 0.01\n",
      "Epoch [5948/20000], Loss: 3.772491455078125, Learning Rate: 0.01\n",
      "Epoch [5949/20000], Loss: 3.7691497802734375, Learning Rate: 0.01\n",
      "Epoch [5950/20000], Loss: 3.767425537109375, Learning Rate: 0.01\n",
      "Epoch [5951/20000], Loss: 3.76806640625, Learning Rate: 0.01\n",
      "Epoch [5952/20000], Loss: 3.772369384765625, Learning Rate: 0.01\n",
      "Epoch [5953/20000], Loss: 3.7821807861328125, Learning Rate: 0.01\n",
      "Epoch [5954/20000], Loss: 3.8005523681640625, Learning Rate: 0.01\n",
      "Epoch [5955/20000], Loss: 3.83184814453125, Learning Rate: 0.01\n",
      "Epoch [5956/20000], Loss: 3.8836517333984375, Learning Rate: 0.01\n",
      "Epoch [5957/20000], Loss: 3.9669189453125, Learning Rate: 0.01\n",
      "Epoch [5958/20000], Loss: 4.099334716796875, Learning Rate: 0.01\n",
      "Epoch [5959/20000], Loss: 4.3077392578125, Learning Rate: 0.01\n",
      "Epoch [5960/20000], Loss: 4.6328277587890625, Learning Rate: 0.01\n",
      "Epoch [5961/20000], Loss: 5.134735107421875, Learning Rate: 0.01\n",
      "Epoch [5962/20000], Loss: 5.8975372314453125, Learning Rate: 0.01\n",
      "Epoch [5963/20000], Loss: 7.0262908935546875, Learning Rate: 0.01\n",
      "Epoch [5964/20000], Loss: 8.626480102539062, Learning Rate: 0.01\n",
      "Epoch [5965/20000], Loss: 10.7353515625, Learning Rate: 0.01\n",
      "Epoch [5966/20000], Loss: 13.190155029296875, Learning Rate: 0.01\n",
      "Epoch [5967/20000], Loss: 15.443099975585938, Learning Rate: 0.01\n",
      "Epoch [5968/20000], Loss: 16.51763916015625, Learning Rate: 0.01\n",
      "Epoch [5969/20000], Loss: 15.4022216796875, Learning Rate: 0.01\n",
      "Epoch [5970/20000], Loss: 11.978607177734375, Learning Rate: 0.01\n",
      "Epoch [5971/20000], Loss: 7.6440887451171875, Learning Rate: 0.01\n",
      "Epoch [5972/20000], Loss: 4.598846435546875, Learning Rate: 0.01\n",
      "Epoch [5973/20000], Loss: 4.1911163330078125, Learning Rate: 0.01\n",
      "Epoch [5974/20000], Loss: 6.0050201416015625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5975/20000], Loss: 8.361862182617188, Learning Rate: 0.01\n",
      "Epoch [5976/20000], Loss: 9.510818481445312, Learning Rate: 0.01\n",
      "Epoch [5977/20000], Loss: 8.741775512695312, Learning Rate: 0.01\n",
      "Epoch [5978/20000], Loss: 6.813568115234375, Learning Rate: 0.01\n",
      "Epoch [5979/20000], Loss: 5.3148040771484375, Learning Rate: 0.01\n",
      "Epoch [5980/20000], Loss: 5.315704345703125, Learning Rate: 0.01\n",
      "Epoch [5981/20000], Loss: 6.577789306640625, Learning Rate: 0.01\n",
      "Epoch [5982/20000], Loss: 7.931427001953125, Learning Rate: 0.01\n",
      "Epoch [5983/20000], Loss: 8.305160522460938, Learning Rate: 0.01\n",
      "Epoch [5984/20000], Loss: 7.542755126953125, Learning Rate: 0.01\n",
      "Epoch [5985/20000], Loss: 6.409881591796875, Learning Rate: 0.01\n",
      "Epoch [5986/20000], Loss: 5.8208465576171875, Learning Rate: 0.01\n",
      "Epoch [5987/20000], Loss: 6.0185546875, Learning Rate: 0.01\n",
      "Epoch [5988/20000], Loss: 6.47235107421875, Learning Rate: 0.01\n",
      "Epoch [5989/20000], Loss: 6.4581298828125, Learning Rate: 0.01\n",
      "Epoch [5990/20000], Loss: 5.734375, Learning Rate: 0.01\n",
      "Epoch [5991/20000], Loss: 4.7066650390625, Learning Rate: 0.01\n",
      "Epoch [5992/20000], Loss: 3.995452880859375, Learning Rate: 0.01\n",
      "Epoch [5993/20000], Loss: 3.8704376220703125, Learning Rate: 0.01\n",
      "Epoch [5994/20000], Loss: 4.0919647216796875, Learning Rate: 0.01\n",
      "Epoch [5995/20000], Loss: 4.2266693115234375, Learning Rate: 0.01\n",
      "Epoch [5996/20000], Loss: 4.0642242431640625, Learning Rate: 0.01\n",
      "Epoch [5997/20000], Loss: 3.7515106201171875, Learning Rate: 0.01\n",
      "Epoch [5998/20000], Loss: 3.5840606689453125, Learning Rate: 0.01\n",
      "Epoch [5999/20000], Loss: 3.7020721435546875, Learning Rate: 0.01\n",
      "Epoch [6000/20000], Loss: 3.98199462890625, Learning Rate: 0.01\n",
      "Epoch [6001/20000], Loss: 4.18212890625, Learning Rate: 0.01\n",
      "Epoch [6002/20000], Loss: 4.164886474609375, Learning Rate: 0.01\n",
      "Epoch [6003/20000], Loss: 3.9903564453125, Learning Rate: 0.01\n",
      "Epoch [6004/20000], Loss: 3.824859619140625, Learning Rate: 0.01\n",
      "Epoch [6005/20000], Loss: 3.7778167724609375, Learning Rate: 0.01\n",
      "Epoch [6006/20000], Loss: 3.821136474609375, Learning Rate: 0.01\n",
      "Epoch [6007/20000], Loss: 3.844573974609375, Learning Rate: 0.01\n",
      "Epoch [6008/20000], Loss: 3.7708587646484375, Learning Rate: 0.01\n",
      "Epoch [6009/20000], Loss: 3.61993408203125, Learning Rate: 0.01\n",
      "Epoch [6010/20000], Loss: 3.477569580078125, Learning Rate: 0.01\n",
      "Epoch [6011/20000], Loss: 3.41253662109375, Learning Rate: 0.01\n",
      "Epoch [6012/20000], Loss: 3.4235992431640625, Learning Rate: 0.01\n",
      "Epoch [6013/20000], Loss: 3.4546356201171875, Learning Rate: 0.01\n",
      "Epoch [6014/20000], Loss: 3.45257568359375, Learning Rate: 0.01\n",
      "Epoch [6015/20000], Loss: 3.409332275390625, Learning Rate: 0.01\n",
      "Epoch [6016/20000], Loss: 3.35845947265625, Learning Rate: 0.01\n",
      "Epoch [6017/20000], Loss: 3.3378143310546875, Learning Rate: 0.01\n",
      "Epoch [6018/20000], Loss: 3.3562164306640625, Learning Rate: 0.01\n",
      "Epoch [6019/20000], Loss: 3.390960693359375, Learning Rate: 0.01\n",
      "Epoch [6020/20000], Loss: 3.4113311767578125, Learning Rate: 0.01\n",
      "Epoch [6021/20000], Loss: 3.4042510986328125, Learning Rate: 0.01\n",
      "Epoch [6022/20000], Loss: 3.38067626953125, Learning Rate: 0.01\n",
      "Epoch [6023/20000], Loss: 3.3617095947265625, Learning Rate: 0.01\n",
      "Epoch [6024/20000], Loss: 3.3602447509765625, Learning Rate: 0.01\n",
      "Epoch [6025/20000], Loss: 3.371551513671875, Learning Rate: 0.01\n",
      "Epoch [6026/20000], Loss: 3.38153076171875, Learning Rate: 0.01\n",
      "Epoch [6027/20000], Loss: 3.3790740966796875, Learning Rate: 0.01\n",
      "Epoch [6028/20000], Loss: 3.3645477294921875, Learning Rate: 0.01\n",
      "Epoch [6029/20000], Loss: 3.3472137451171875, Learning Rate: 0.01\n",
      "Epoch [6030/20000], Loss: 3.3367919921875, Learning Rate: 0.01\n",
      "Epoch [6031/20000], Loss: 3.3359375, Learning Rate: 0.01\n",
      "Epoch [6032/20000], Loss: 3.3399200439453125, Learning Rate: 0.01\n",
      "Epoch [6033/20000], Loss: 3.3417205810546875, Learning Rate: 0.01\n",
      "Epoch [6034/20000], Loss: 3.33837890625, Learning Rate: 0.01\n",
      "Epoch [6035/20000], Loss: 3.331756591796875, Learning Rate: 0.01\n",
      "Epoch [6036/20000], Loss: 3.32708740234375, Learning Rate: 0.01\n",
      "Epoch [6037/20000], Loss: 3.328460693359375, Learning Rate: 0.01\n",
      "Epoch [6038/20000], Loss: 3.33642578125, Learning Rate: 0.01\n",
      "Epoch [6039/20000], Loss: 3.3484039306640625, Learning Rate: 0.01\n",
      "Epoch [6040/20000], Loss: 3.362152099609375, Learning Rate: 0.01\n",
      "Epoch [6041/20000], Loss: 3.3772125244140625, Learning Rate: 0.01\n",
      "Epoch [6042/20000], Loss: 3.3960113525390625, Learning Rate: 0.01\n",
      "Epoch [6043/20000], Loss: 3.4223175048828125, Learning Rate: 0.01\n",
      "Epoch [6044/20000], Loss: 3.4592742919921875, Learning Rate: 0.01\n",
      "Epoch [6045/20000], Loss: 3.5089569091796875, Learning Rate: 0.01\n",
      "Epoch [6046/20000], Loss: 3.572601318359375, Learning Rate: 0.01\n",
      "Epoch [6047/20000], Loss: 3.652069091796875, Learning Rate: 0.01\n",
      "Epoch [6048/20000], Loss: 3.7512664794921875, Learning Rate: 0.01\n",
      "Epoch [6049/20000], Loss: 3.875244140625, Learning Rate: 0.01\n",
      "Epoch [6050/20000], Loss: 4.0313568115234375, Learning Rate: 0.01\n",
      "Epoch [6051/20000], Loss: 4.2260894775390625, Learning Rate: 0.01\n",
      "Epoch [6052/20000], Loss: 4.46649169921875, Learning Rate: 0.01\n",
      "Epoch [6053/20000], Loss: 4.7566986083984375, Learning Rate: 0.01\n",
      "Epoch [6054/20000], Loss: 5.100128173828125, Learning Rate: 0.01\n",
      "Epoch [6055/20000], Loss: 5.4931488037109375, Learning Rate: 0.01\n",
      "Epoch [6056/20000], Loss: 5.9281005859375, Learning Rate: 0.01\n",
      "Epoch [6057/20000], Loss: 6.3795623779296875, Learning Rate: 0.01\n",
      "Epoch [6058/20000], Loss: 6.812347412109375, Learning Rate: 0.01\n",
      "Epoch [6059/20000], Loss: 7.1653289794921875, Learning Rate: 0.01\n",
      "Epoch [6060/20000], Loss: 7.3745269775390625, Learning Rate: 0.01\n",
      "Epoch [6061/20000], Loss: 7.364776611328125, Learning Rate: 0.01\n",
      "Epoch [6062/20000], Loss: 7.0950164794921875, Learning Rate: 0.01\n",
      "Epoch [6063/20000], Loss: 6.557159423828125, Learning Rate: 0.01\n",
      "Epoch [6064/20000], Loss: 5.81329345703125, Learning Rate: 0.01\n",
      "Epoch [6065/20000], Loss: 4.97137451171875, Learning Rate: 0.01\n",
      "Epoch [6066/20000], Loss: 4.171722412109375, Learning Rate: 0.01\n",
      "Epoch [6067/20000], Loss: 3.5366058349609375, Learning Rate: 0.01\n",
      "Epoch [6068/20000], Loss: 3.1428985595703125, Learning Rate: 0.01\n",
      "Epoch [6069/20000], Loss: 3.00433349609375, Learning Rate: 0.01\n",
      "Epoch [6070/20000], Loss: 3.079833984375, Learning Rate: 0.01\n",
      "Epoch [6071/20000], Loss: 3.29364013671875, Learning Rate: 0.01\n",
      "Epoch [6072/20000], Loss: 3.5584716796875, Learning Rate: 0.01\n",
      "Epoch [6073/20000], Loss: 3.796142578125, Learning Rate: 0.01\n",
      "Epoch [6074/20000], Loss: 3.9485321044921875, Learning Rate: 0.01\n",
      "Epoch [6075/20000], Loss: 3.987335205078125, Learning Rate: 0.01\n",
      "Epoch [6076/20000], Loss: 3.91162109375, Learning Rate: 0.01\n",
      "Epoch [6077/20000], Loss: 3.746368408203125, Learning Rate: 0.01\n",
      "Epoch [6078/20000], Loss: 3.531036376953125, Learning Rate: 0.01\n",
      "Epoch [6079/20000], Loss: 3.3105010986328125, Learning Rate: 0.01\n",
      "Epoch [6080/20000], Loss: 3.1230621337890625, Learning Rate: 0.01\n",
      "Epoch [6081/20000], Loss: 2.9930572509765625, Learning Rate: 0.01\n",
      "Epoch [6082/20000], Loss: 2.92877197265625, Learning Rate: 0.01\n",
      "Epoch [6083/20000], Loss: 2.923828125, Learning Rate: 0.01\n",
      "Epoch [6084/20000], Loss: 2.9618072509765625, Learning Rate: 0.01\n",
      "Epoch [6085/20000], Loss: 3.0221099853515625, Learning Rate: 0.01\n",
      "Epoch [6086/20000], Loss: 3.08477783203125, Learning Rate: 0.01\n",
      "Epoch [6087/20000], Loss: 3.1336822509765625, Learning Rate: 0.01\n",
      "Epoch [6088/20000], Loss: 3.1593475341796875, Learning Rate: 0.01\n",
      "Epoch [6089/20000], Loss: 3.1582183837890625, Learning Rate: 0.01\n",
      "Epoch [6090/20000], Loss: 3.1325836181640625, Learning Rate: 0.01\n",
      "Epoch [6091/20000], Loss: 3.088531494140625, Learning Rate: 0.01\n",
      "Epoch [6092/20000], Loss: 3.03387451171875, Learning Rate: 0.01\n",
      "Epoch [6093/20000], Loss: 2.977020263671875, Learning Rate: 0.01\n",
      "Epoch [6094/20000], Loss: 2.9251251220703125, Learning Rate: 0.01\n",
      "Epoch [6095/20000], Loss: 2.8825836181640625, Learning Rate: 0.01\n",
      "Epoch [6096/20000], Loss: 2.85205078125, Learning Rate: 0.01\n",
      "Epoch [6097/20000], Loss: 2.8335723876953125, Learning Rate: 0.01\n",
      "Epoch [6098/20000], Loss: 2.82562255859375, Learning Rate: 0.01\n",
      "Epoch [6099/20000], Loss: 2.82550048828125, Learning Rate: 0.01\n",
      "Epoch [6100/20000], Loss: 2.8304443359375, Learning Rate: 0.01\n",
      "Epoch [6101/20000], Loss: 2.837432861328125, Learning Rate: 0.01\n",
      "Epoch [6102/20000], Loss: 2.8442230224609375, Learning Rate: 0.01\n",
      "Epoch [6103/20000], Loss: 2.84906005859375, Learning Rate: 0.01\n",
      "Epoch [6104/20000], Loss: 2.8507232666015625, Learning Rate: 0.01\n",
      "Epoch [6105/20000], Loss: 2.8490142822265625, Learning Rate: 0.01\n",
      "Epoch [6106/20000], Loss: 2.84375, Learning Rate: 0.01\n",
      "Epoch [6107/20000], Loss: 2.835662841796875, Learning Rate: 0.01\n",
      "Epoch [6108/20000], Loss: 2.8251800537109375, Learning Rate: 0.01\n",
      "Epoch [6109/20000], Loss: 2.81317138671875, Learning Rate: 0.01\n",
      "Epoch [6110/20000], Loss: 2.8001251220703125, Learning Rate: 0.01\n",
      "Epoch [6111/20000], Loss: 2.78680419921875, Learning Rate: 0.01\n",
      "Epoch [6112/20000], Loss: 2.7736663818359375, Learning Rate: 0.01\n",
      "Epoch [6113/20000], Loss: 2.7610626220703125, Learning Rate: 0.01\n",
      "Epoch [6114/20000], Loss: 2.7490692138671875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6115/20000], Loss: 2.7379913330078125, Learning Rate: 0.01\n",
      "Epoch [6116/20000], Loss: 2.7277984619140625, Learning Rate: 0.01\n",
      "Epoch [6117/20000], Loss: 2.71826171875, Learning Rate: 0.01\n",
      "Epoch [6118/20000], Loss: 2.709625244140625, Learning Rate: 0.01\n",
      "Epoch [6119/20000], Loss: 2.7015838623046875, Learning Rate: 0.01\n",
      "Epoch [6120/20000], Loss: 2.694122314453125, Learning Rate: 0.01\n",
      "Epoch [6121/20000], Loss: 2.68701171875, Learning Rate: 0.01\n",
      "Epoch [6122/20000], Loss: 2.680389404296875, Learning Rate: 0.01\n",
      "Epoch [6123/20000], Loss: 2.6738739013671875, Learning Rate: 0.01\n",
      "Epoch [6124/20000], Loss: 2.6676025390625, Learning Rate: 0.01\n",
      "Epoch [6125/20000], Loss: 2.661468505859375, Learning Rate: 0.01\n",
      "Epoch [6126/20000], Loss: 2.655487060546875, Learning Rate: 0.01\n",
      "Epoch [6127/20000], Loss: 2.649627685546875, Learning Rate: 0.01\n",
      "Epoch [6128/20000], Loss: 2.64373779296875, Learning Rate: 0.01\n",
      "Epoch [6129/20000], Loss: 2.6379852294921875, Learning Rate: 0.01\n",
      "Epoch [6130/20000], Loss: 2.63214111328125, Learning Rate: 0.01\n",
      "Epoch [6131/20000], Loss: 2.6264495849609375, Learning Rate: 0.01\n",
      "Epoch [6132/20000], Loss: 2.6207733154296875, Learning Rate: 0.01\n",
      "Epoch [6133/20000], Loss: 2.6152801513671875, Learning Rate: 0.01\n",
      "Epoch [6134/20000], Loss: 2.6097564697265625, Learning Rate: 0.01\n",
      "Epoch [6135/20000], Loss: 2.6043853759765625, Learning Rate: 0.01\n",
      "Epoch [6136/20000], Loss: 2.599212646484375, Learning Rate: 0.01\n",
      "Epoch [6137/20000], Loss: 2.59429931640625, Learning Rate: 0.01\n",
      "Epoch [6138/20000], Loss: 2.589569091796875, Learning Rate: 0.01\n",
      "Epoch [6139/20000], Loss: 2.585296630859375, Learning Rate: 0.01\n",
      "Epoch [6140/20000], Loss: 2.5816192626953125, Learning Rate: 0.01\n",
      "Epoch [6141/20000], Loss: 2.578521728515625, Learning Rate: 0.01\n",
      "Epoch [6142/20000], Loss: 2.57647705078125, Learning Rate: 0.01\n",
      "Epoch [6143/20000], Loss: 2.5758514404296875, Learning Rate: 0.01\n",
      "Epoch [6144/20000], Loss: 2.5770721435546875, Learning Rate: 0.01\n",
      "Epoch [6145/20000], Loss: 2.580718994140625, Learning Rate: 0.01\n",
      "Epoch [6146/20000], Loss: 2.587982177734375, Learning Rate: 0.01\n",
      "Epoch [6147/20000], Loss: 2.6000823974609375, Learning Rate: 0.01\n",
      "Epoch [6148/20000], Loss: 2.6192474365234375, Learning Rate: 0.01\n",
      "Epoch [6149/20000], Loss: 2.6480865478515625, Learning Rate: 0.01\n",
      "Epoch [6150/20000], Loss: 2.690704345703125, Learning Rate: 0.01\n",
      "Epoch [6151/20000], Loss: 2.752960205078125, Learning Rate: 0.01\n",
      "Epoch [6152/20000], Loss: 2.8430633544921875, Learning Rate: 0.01\n",
      "Epoch [6153/20000], Loss: 2.9728546142578125, Learning Rate: 0.01\n",
      "Epoch [6154/20000], Loss: 3.1590423583984375, Learning Rate: 0.01\n",
      "Epoch [6155/20000], Loss: 3.4246063232421875, Learning Rate: 0.01\n",
      "Epoch [6156/20000], Loss: 3.8017425537109375, Learning Rate: 0.01\n",
      "Epoch [6157/20000], Loss: 4.3307647705078125, Learning Rate: 0.01\n",
      "Epoch [6158/20000], Loss: 5.0637664794921875, Learning Rate: 0.01\n",
      "Epoch [6159/20000], Loss: 6.0526885986328125, Learning Rate: 0.01\n",
      "Epoch [6160/20000], Loss: 7.3431854248046875, Learning Rate: 0.01\n",
      "Epoch [6161/20000], Loss: 8.925827026367188, Learning Rate: 0.01\n",
      "Epoch [6162/20000], Loss: 10.706085205078125, Learning Rate: 0.01\n",
      "Epoch [6163/20000], Loss: 12.400390625, Learning Rate: 0.01\n",
      "Epoch [6164/20000], Loss: 13.56341552734375, Learning Rate: 0.01\n",
      "Epoch [6165/20000], Loss: 13.608917236328125, Learning Rate: 0.01\n",
      "Epoch [6166/20000], Loss: 12.185546875, Learning Rate: 0.01\n",
      "Epoch [6167/20000], Loss: 9.435806274414062, Learning Rate: 0.01\n",
      "Epoch [6168/20000], Loss: 6.2058563232421875, Learning Rate: 0.01\n",
      "Epoch [6169/20000], Loss: 3.61712646484375, Learning Rate: 0.01\n",
      "Epoch [6170/20000], Loss: 2.4739990234375, Learning Rate: 0.01\n",
      "Epoch [6171/20000], Loss: 2.8432769775390625, Learning Rate: 0.01\n",
      "Epoch [6172/20000], Loss: 4.1314849853515625, Learning Rate: 0.01\n",
      "Epoch [6173/20000], Loss: 5.4661407470703125, Learning Rate: 0.01\n",
      "Epoch [6174/20000], Loss: 6.1080780029296875, Learning Rate: 0.01\n",
      "Epoch [6175/20000], Loss: 5.7639312744140625, Learning Rate: 0.01\n",
      "Epoch [6176/20000], Loss: 4.6639556884765625, Learning Rate: 0.01\n",
      "Epoch [6177/20000], Loss: 3.4068756103515625, Learning Rate: 0.01\n",
      "Epoch [6178/20000], Loss: 2.5837249755859375, Learning Rate: 0.01\n",
      "Epoch [6179/20000], Loss: 2.460906982421875, Learning Rate: 0.01\n",
      "Epoch [6180/20000], Loss: 2.89239501953125, Learning Rate: 0.01\n",
      "Epoch [6181/20000], Loss: 3.478851318359375, Learning Rate: 0.01\n",
      "Epoch [6182/20000], Loss: 3.82476806640625, Learning Rate: 0.01\n",
      "Epoch [6183/20000], Loss: 3.740325927734375, Learning Rate: 0.01\n",
      "Epoch [6184/20000], Loss: 3.306640625, Learning Rate: 0.01\n",
      "Epoch [6185/20000], Loss: 2.78118896484375, Learning Rate: 0.01\n",
      "Epoch [6186/20000], Loss: 2.4283294677734375, Learning Rate: 0.01\n",
      "Epoch [6187/20000], Loss: 2.36956787109375, Learning Rate: 0.01\n",
      "Epoch [6188/20000], Loss: 2.546173095703125, Learning Rate: 0.01\n",
      "Epoch [6189/20000], Loss: 2.787139892578125, Learning Rate: 0.01\n",
      "Epoch [6190/20000], Loss: 2.922760009765625, Learning Rate: 0.01\n",
      "Epoch [6191/20000], Loss: 2.8751068115234375, Learning Rate: 0.01\n",
      "Epoch [6192/20000], Loss: 2.6809234619140625, Learning Rate: 0.01\n",
      "Epoch [6193/20000], Loss: 2.4510650634765625, Learning Rate: 0.01\n",
      "Epoch [6194/20000], Loss: 2.296173095703125, Learning Rate: 0.01\n",
      "Epoch [6195/20000], Loss: 2.267730712890625, Learning Rate: 0.01\n",
      "Epoch [6196/20000], Loss: 2.3421783447265625, Learning Rate: 0.01\n",
      "Epoch [6197/20000], Loss: 2.4480133056640625, Learning Rate: 0.01\n",
      "Epoch [6198/20000], Loss: 2.5118255615234375, Learning Rate: 0.01\n",
      "Epoch [6199/20000], Loss: 2.4961395263671875, Learning Rate: 0.01\n",
      "Epoch [6200/20000], Loss: 2.411956787109375, Learning Rate: 0.01\n",
      "Epoch [6201/20000], Loss: 2.3031158447265625, Learning Rate: 0.01\n",
      "Epoch [6202/20000], Loss: 2.21844482421875, Learning Rate: 0.01\n",
      "Epoch [6203/20000], Loss: 2.1866302490234375, Learning Rate: 0.01\n",
      "Epoch [6204/20000], Loss: 2.20538330078125, Learning Rate: 0.01\n",
      "Epoch [6205/20000], Loss: 2.249542236328125, Learning Rate: 0.01\n",
      "Epoch [6206/20000], Loss: 2.2866973876953125, Learning Rate: 0.01\n",
      "Epoch [6207/20000], Loss: 2.2946014404296875, Learning Rate: 0.01\n",
      "Epoch [6208/20000], Loss: 2.26922607421875, Learning Rate: 0.01\n",
      "Epoch [6209/20000], Loss: 2.2224273681640625, Learning Rate: 0.01\n",
      "Epoch [6210/20000], Loss: 2.1742401123046875, Learning Rate: 0.01\n",
      "Epoch [6211/20000], Loss: 2.14111328125, Learning Rate: 0.01\n",
      "Epoch [6212/20000], Loss: 2.1299591064453125, Learning Rate: 0.01\n",
      "Epoch [6213/20000], Loss: 2.136962890625, Learning Rate: 0.01\n",
      "Epoch [6214/20000], Loss: 2.151153564453125, Learning Rate: 0.01\n",
      "Epoch [6215/20000], Loss: 2.16119384765625, Learning Rate: 0.01\n",
      "Epoch [6216/20000], Loss: 2.15972900390625, Learning Rate: 0.01\n",
      "Epoch [6217/20000], Loss: 2.146270751953125, Learning Rate: 0.01\n",
      "Epoch [6218/20000], Loss: 2.1253204345703125, Learning Rate: 0.01\n",
      "Epoch [6219/20000], Loss: 2.1036529541015625, Learning Rate: 0.01\n",
      "Epoch [6220/20000], Loss: 2.0869293212890625, Learning Rate: 0.01\n",
      "Epoch [6221/20000], Loss: 2.0775604248046875, Learning Rate: 0.01\n",
      "Epoch [6222/20000], Loss: 2.07501220703125, Learning Rate: 0.01\n",
      "Epoch [6223/20000], Loss: 2.0755462646484375, Learning Rate: 0.01\n",
      "Epoch [6224/20000], Loss: 2.075531005859375, Learning Rate: 0.01\n",
      "Epoch [6225/20000], Loss: 2.0723419189453125, Learning Rate: 0.01\n",
      "Epoch [6226/20000], Loss: 2.0649871826171875, Learning Rate: 0.01\n",
      "Epoch [6227/20000], Loss: 2.0543365478515625, Learning Rate: 0.01\n",
      "Epoch [6228/20000], Loss: 2.0427093505859375, Learning Rate: 0.01\n",
      "Epoch [6229/20000], Loss: 2.0318756103515625, Learning Rate: 0.01\n",
      "Epoch [6230/20000], Loss: 2.02325439453125, Learning Rate: 0.01\n",
      "Epoch [6231/20000], Loss: 2.0171661376953125, Learning Rate: 0.01\n",
      "Epoch [6232/20000], Loss: 2.013092041015625, Learning Rate: 0.01\n",
      "Epoch [6233/20000], Loss: 2.0095977783203125, Learning Rate: 0.01\n",
      "Epoch [6234/20000], Loss: 2.0057830810546875, Learning Rate: 0.01\n",
      "Epoch [6235/20000], Loss: 2.000701904296875, Learning Rate: 0.01\n",
      "Epoch [6236/20000], Loss: 1.9942626953125, Learning Rate: 0.01\n",
      "Epoch [6237/20000], Loss: 1.9865875244140625, Learning Rate: 0.01\n",
      "Epoch [6238/20000], Loss: 1.978424072265625, Learning Rate: 0.01\n",
      "Epoch [6239/20000], Loss: 1.9704437255859375, Learning Rate: 0.01\n",
      "Epoch [6240/20000], Loss: 1.9630889892578125, Learning Rate: 0.01\n",
      "Epoch [6241/20000], Loss: 1.95654296875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6242/20000], Loss: 1.950836181640625, Learning Rate: 0.01\n",
      "Epoch [6243/20000], Loss: 1.945465087890625, Learning Rate: 0.01\n",
      "Epoch [6244/20000], Loss: 1.9402923583984375, Learning Rate: 0.01\n",
      "Epoch [6245/20000], Loss: 1.9350433349609375, Learning Rate: 0.01\n",
      "Epoch [6246/20000], Loss: 1.92950439453125, Learning Rate: 0.01\n",
      "Epoch [6247/20000], Loss: 1.9233856201171875, Learning Rate: 0.01\n",
      "Epoch [6248/20000], Loss: 1.917144775390625, Learning Rate: 0.01\n",
      "Epoch [6249/20000], Loss: 1.91064453125, Learning Rate: 0.01\n",
      "Epoch [6250/20000], Loss: 1.90399169921875, Learning Rate: 0.01\n",
      "Epoch [6251/20000], Loss: 1.89764404296875, Learning Rate: 0.01\n",
      "Epoch [6252/20000], Loss: 1.8912353515625, Learning Rate: 0.01\n",
      "Epoch [6253/20000], Loss: 1.88525390625, Learning Rate: 0.01\n",
      "Epoch [6254/20000], Loss: 1.8792266845703125, Learning Rate: 0.01\n",
      "Epoch [6255/20000], Loss: 1.8734130859375, Learning Rate: 0.01\n",
      "Epoch [6256/20000], Loss: 1.8676605224609375, Learning Rate: 0.01\n",
      "Epoch [6257/20000], Loss: 1.8619537353515625, Learning Rate: 0.01\n",
      "Epoch [6258/20000], Loss: 1.856109619140625, Learning Rate: 0.01\n",
      "Epoch [6259/20000], Loss: 1.8501129150390625, Learning Rate: 0.01\n",
      "Epoch [6260/20000], Loss: 1.8441162109375, Learning Rate: 0.01\n",
      "Epoch [6261/20000], Loss: 1.837982177734375, Learning Rate: 0.01\n",
      "Epoch [6262/20000], Loss: 1.831878662109375, Learning Rate: 0.01\n",
      "Epoch [6263/20000], Loss: 1.825714111328125, Learning Rate: 0.01\n",
      "Epoch [6264/20000], Loss: 1.819671630859375, Learning Rate: 0.01\n",
      "Epoch [6265/20000], Loss: 1.813507080078125, Learning Rate: 0.01\n",
      "Epoch [6266/20000], Loss: 1.8074493408203125, Learning Rate: 0.01\n",
      "Epoch [6267/20000], Loss: 1.8014678955078125, Learning Rate: 0.01\n",
      "Epoch [6268/20000], Loss: 1.795440673828125, Learning Rate: 0.01\n",
      "Epoch [6269/20000], Loss: 1.789581298828125, Learning Rate: 0.01\n",
      "Epoch [6270/20000], Loss: 1.7836151123046875, Learning Rate: 0.01\n",
      "Epoch [6271/20000], Loss: 1.777801513671875, Learning Rate: 0.01\n",
      "Epoch [6272/20000], Loss: 1.7718963623046875, Learning Rate: 0.01\n",
      "Epoch [6273/20000], Loss: 1.765960693359375, Learning Rate: 0.01\n",
      "Epoch [6274/20000], Loss: 1.7601318359375, Learning Rate: 0.01\n",
      "Epoch [6275/20000], Loss: 1.7542724609375, Learning Rate: 0.01\n",
      "Epoch [6276/20000], Loss: 1.7484130859375, Learning Rate: 0.01\n",
      "Epoch [6277/20000], Loss: 1.7425689697265625, Learning Rate: 0.01\n",
      "Epoch [6278/20000], Loss: 1.7368621826171875, Learning Rate: 0.01\n",
      "Epoch [6279/20000], Loss: 1.731170654296875, Learning Rate: 0.01\n",
      "Epoch [6280/20000], Loss: 1.72564697265625, Learning Rate: 0.01\n",
      "Epoch [6281/20000], Loss: 1.7202911376953125, Learning Rate: 0.01\n",
      "Epoch [6282/20000], Loss: 1.7149200439453125, Learning Rate: 0.01\n",
      "Epoch [6283/20000], Loss: 1.710174560546875, Learning Rate: 0.01\n",
      "Epoch [6284/20000], Loss: 1.7056427001953125, Learning Rate: 0.01\n",
      "Epoch [6285/20000], Loss: 1.701812744140625, Learning Rate: 0.01\n",
      "Epoch [6286/20000], Loss: 1.6987762451171875, Learning Rate: 0.01\n",
      "Epoch [6287/20000], Loss: 1.6968841552734375, Learning Rate: 0.01\n",
      "Epoch [6288/20000], Loss: 1.6967010498046875, Learning Rate: 0.01\n",
      "Epoch [6289/20000], Loss: 1.698822021484375, Learning Rate: 0.01\n",
      "Epoch [6290/20000], Loss: 1.704437255859375, Learning Rate: 0.01\n",
      "Epoch [6291/20000], Loss: 1.7150115966796875, Learning Rate: 0.01\n",
      "Epoch [6292/20000], Loss: 1.732818603515625, Learning Rate: 0.01\n",
      "Epoch [6293/20000], Loss: 1.7612152099609375, Learning Rate: 0.01\n",
      "Epoch [6294/20000], Loss: 1.8050079345703125, Learning Rate: 0.01\n",
      "Epoch [6295/20000], Loss: 1.87152099609375, Learning Rate: 0.01\n",
      "Epoch [6296/20000], Loss: 1.9712677001953125, Learning Rate: 0.01\n",
      "Epoch [6297/20000], Loss: 2.119964599609375, Learning Rate: 0.01\n",
      "Epoch [6298/20000], Loss: 2.3397216796875, Learning Rate: 0.01\n",
      "Epoch [6299/20000], Loss: 2.66156005859375, Learning Rate: 0.01\n",
      "Epoch [6300/20000], Loss: 3.127349853515625, Learning Rate: 0.01\n",
      "Epoch [6301/20000], Loss: 3.788604736328125, Learning Rate: 0.01\n",
      "Epoch [6302/20000], Loss: 4.699859619140625, Learning Rate: 0.01\n",
      "Epoch [6303/20000], Loss: 5.8964691162109375, Learning Rate: 0.01\n",
      "Epoch [6304/20000], Loss: 7.3543701171875, Learning Rate: 0.01\n",
      "Epoch [6305/20000], Loss: 8.915679931640625, Learning Rate: 0.01\n",
      "Epoch [6306/20000], Loss: 10.230758666992188, Learning Rate: 0.01\n",
      "Epoch [6307/20000], Loss: 10.76422119140625, Learning Rate: 0.01\n",
      "Epoch [6308/20000], Loss: 10.027755737304688, Learning Rate: 0.01\n",
      "Epoch [6309/20000], Loss: 7.95361328125, Learning Rate: 0.01\n",
      "Epoch [6310/20000], Loss: 5.186798095703125, Learning Rate: 0.01\n",
      "Epoch [6311/20000], Loss: 2.824981689453125, Learning Rate: 0.01\n",
      "Epoch [6312/20000], Loss: 1.757293701171875, Learning Rate: 0.01\n",
      "Epoch [6313/20000], Loss: 2.1285400390625, Learning Rate: 0.01\n",
      "Epoch [6314/20000], Loss: 3.35430908203125, Learning Rate: 0.01\n",
      "Epoch [6315/20000], Loss: 4.5322113037109375, Learning Rate: 0.01\n",
      "Epoch [6316/20000], Loss: 4.9264984130859375, Learning Rate: 0.01\n",
      "Epoch [6317/20000], Loss: 4.322662353515625, Learning Rate: 0.01\n",
      "Epoch [6318/20000], Loss: 3.1095428466796875, Learning Rate: 0.01\n",
      "Epoch [6319/20000], Loss: 1.9982757568359375, Learning Rate: 0.01\n",
      "Epoch [6320/20000], Loss: 1.5410614013671875, Learning Rate: 0.01\n",
      "Epoch [6321/20000], Loss: 1.8062744140625, Learning Rate: 0.01\n",
      "Epoch [6322/20000], Loss: 2.4226837158203125, Learning Rate: 0.01\n",
      "Epoch [6323/20000], Loss: 2.88385009765625, Learning Rate: 0.01\n",
      "Epoch [6324/20000], Loss: 2.8746795654296875, Learning Rate: 0.01\n",
      "Epoch [6325/20000], Loss: 2.428131103515625, Learning Rate: 0.01\n",
      "Epoch [6326/20000], Loss: 1.85076904296875, Learning Rate: 0.01\n",
      "Epoch [6327/20000], Loss: 1.483154296875, Learning Rate: 0.01\n",
      "Epoch [6328/20000], Loss: 1.47613525390625, Learning Rate: 0.01\n",
      "Epoch [6329/20000], Loss: 1.73016357421875, Learning Rate: 0.01\n",
      "Epoch [6330/20000], Loss: 2.006256103515625, Learning Rate: 0.01\n",
      "Epoch [6331/20000], Loss: 2.1004180908203125, Learning Rate: 0.01\n",
      "Epoch [6332/20000], Loss: 1.962127685546875, Learning Rate: 0.01\n",
      "Epoch [6333/20000], Loss: 1.697662353515625, Learning Rate: 0.01\n",
      "Epoch [6334/20000], Loss: 1.4741973876953125, Learning Rate: 0.01\n",
      "Epoch [6335/20000], Loss: 1.40350341796875, Learning Rate: 0.01\n",
      "Epoch [6336/20000], Loss: 1.482635498046875, Learning Rate: 0.01\n",
      "Epoch [6337/20000], Loss: 1.6187896728515625, Learning Rate: 0.01\n",
      "Epoch [6338/20000], Loss: 1.70343017578125, Learning Rate: 0.01\n",
      "Epoch [6339/20000], Loss: 1.6817169189453125, Learning Rate: 0.01\n",
      "Epoch [6340/20000], Loss: 1.5747528076171875, Learning Rate: 0.01\n",
      "Epoch [6341/20000], Loss: 1.4518890380859375, Learning Rate: 0.01\n",
      "Epoch [6342/20000], Loss: 1.379638671875, Learning Rate: 0.01\n",
      "Epoch [6343/20000], Loss: 1.38226318359375, Learning Rate: 0.01\n",
      "Epoch [6344/20000], Loss: 1.4354705810546875, Learning Rate: 0.01\n",
      "Epoch [6345/20000], Loss: 1.4907073974609375, Learning Rate: 0.01\n",
      "Epoch [6346/20000], Loss: 1.5088043212890625, Learning Rate: 0.01\n",
      "Epoch [6347/20000], Loss: 1.4803619384765625, Learning Rate: 0.01\n",
      "Epoch [6348/20000], Loss: 1.42596435546875, Learning Rate: 0.01\n",
      "Epoch [6349/20000], Loss: 1.37774658203125, Learning Rate: 0.01\n",
      "Epoch [6350/20000], Loss: 1.3591156005859375, Learning Rate: 0.01\n",
      "Epoch [6351/20000], Loss: 1.3734283447265625, Learning Rate: 0.01\n",
      "Epoch [6352/20000], Loss: 1.4063720703125, Learning Rate: 0.01\n",
      "Epoch [6353/20000], Loss: 1.4376983642578125, Learning Rate: 0.01\n",
      "Epoch [6354/20000], Loss: 1.45428466796875, Learning Rate: 0.01\n",
      "Epoch [6355/20000], Loss: 1.4563140869140625, Learning Rate: 0.01\n",
      "Epoch [6356/20000], Loss: 1.455322265625, Learning Rate: 0.01\n",
      "Epoch [6357/20000], Loss: 1.46612548828125, Learning Rate: 0.01\n",
      "Epoch [6358/20000], Loss: 1.5004119873046875, Learning Rate: 0.01\n",
      "Epoch [6359/20000], Loss: 1.5616607666015625, Learning Rate: 0.01\n",
      "Epoch [6360/20000], Loss: 1.647918701171875, Learning Rate: 0.01\n",
      "Epoch [6361/20000], Loss: 1.7565765380859375, Learning Rate: 0.01\n",
      "Epoch [6362/20000], Loss: 1.8889007568359375, Learning Rate: 0.01\n",
      "Epoch [6363/20000], Loss: 2.0533447265625, Learning Rate: 0.01\n",
      "Epoch [6364/20000], Loss: 2.2653961181640625, Learning Rate: 0.01\n",
      "Epoch [6365/20000], Loss: 2.543121337890625, Learning Rate: 0.01\n",
      "Epoch [6366/20000], Loss: 2.9056549072265625, Learning Rate: 0.01\n",
      "Epoch [6367/20000], Loss: 3.3660125732421875, Learning Rate: 0.01\n",
      "Epoch [6368/20000], Loss: 3.9314727783203125, Learning Rate: 0.01\n",
      "Epoch [6369/20000], Loss: 4.591339111328125, Learning Rate: 0.01\n",
      "Epoch [6370/20000], Loss: 5.3185272216796875, Learning Rate: 0.01\n",
      "Epoch [6371/20000], Loss: 6.0451202392578125, Learning Rate: 0.01\n",
      "Epoch [6372/20000], Loss: 6.6748504638671875, Learning Rate: 0.01\n",
      "Epoch [6373/20000], Loss: 7.060943603515625, Learning Rate: 0.01\n",
      "Epoch [6374/20000], Loss: 7.066680908203125, Learning Rate: 0.01\n",
      "Epoch [6375/20000], Loss: 6.582275390625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6376/20000], Loss: 5.6268768310546875, Learning Rate: 0.01\n",
      "Epoch [6377/20000], Loss: 4.34808349609375, Learning Rate: 0.01\n",
      "Epoch [6378/20000], Loss: 3.021392822265625, Learning Rate: 0.01\n",
      "Epoch [6379/20000], Loss: 1.9341583251953125, Learning Rate: 0.01\n",
      "Epoch [6380/20000], Loss: 1.29034423828125, Learning Rate: 0.01\n",
      "Epoch [6381/20000], Loss: 1.1411895751953125, Learning Rate: 0.01\n",
      "Epoch [6382/20000], Loss: 1.3926544189453125, Learning Rate: 0.01\n",
      "Epoch [6383/20000], Loss: 1.8604736328125, Learning Rate: 0.01\n",
      "Epoch [6384/20000], Loss: 2.337890625, Learning Rate: 0.01\n",
      "Epoch [6385/20000], Loss: 2.6560821533203125, Learning Rate: 0.01\n",
      "Epoch [6386/20000], Loss: 2.718994140625, Learning Rate: 0.01\n",
      "Epoch [6387/20000], Loss: 2.5237884521484375, Learning Rate: 0.01\n",
      "Epoch [6388/20000], Loss: 2.1454925537109375, Learning Rate: 0.01\n",
      "Epoch [6389/20000], Loss: 1.708282470703125, Learning Rate: 0.01\n",
      "Epoch [6390/20000], Loss: 1.336090087890625, Learning Rate: 0.01\n",
      "Epoch [6391/20000], Loss: 1.1126556396484375, Learning Rate: 0.01\n",
      "Epoch [6392/20000], Loss: 1.061279296875, Learning Rate: 0.01\n",
      "Epoch [6393/20000], Loss: 1.148712158203125, Learning Rate: 0.01\n",
      "Epoch [6394/20000], Loss: 1.30804443359375, Learning Rate: 0.01\n",
      "Epoch [6395/20000], Loss: 1.465118408203125, Learning Rate: 0.01\n",
      "Epoch [6396/20000], Loss: 1.56280517578125, Learning Rate: 0.01\n",
      "Epoch [6397/20000], Loss: 1.5726470947265625, Learning Rate: 0.01\n",
      "Epoch [6398/20000], Loss: 1.498992919921875, Learning Rate: 0.01\n",
      "Epoch [6399/20000], Loss: 1.3697662353515625, Learning Rate: 0.01\n",
      "Epoch [6400/20000], Loss: 1.2246856689453125, Learning Rate: 0.01\n",
      "Epoch [6401/20000], Loss: 1.100677490234375, Learning Rate: 0.01\n",
      "Epoch [6402/20000], Loss: 1.0217132568359375, Learning Rate: 0.01\n",
      "Epoch [6403/20000], Loss: 0.9948883056640625, Learning Rate: 0.01\n",
      "Epoch [6404/20000], Loss: 1.011444091796875, Learning Rate: 0.01\n",
      "Epoch [6405/20000], Loss: 1.05340576171875, Learning Rate: 0.01\n",
      "Epoch [6406/20000], Loss: 1.0999755859375, Learning Rate: 0.01\n",
      "Epoch [6407/20000], Loss: 1.133941650390625, Learning Rate: 0.01\n",
      "Epoch [6408/20000], Loss: 1.14501953125, Learning Rate: 0.01\n",
      "Epoch [6409/20000], Loss: 1.1314544677734375, Learning Rate: 0.01\n",
      "Epoch [6410/20000], Loss: 1.0976715087890625, Learning Rate: 0.01\n",
      "Epoch [6411/20000], Loss: 1.0526580810546875, Learning Rate: 0.01\n",
      "Epoch [6412/20000], Loss: 1.00634765625, Learning Rate: 0.01\n",
      "Epoch [6413/20000], Loss: 0.96697998046875, Learning Rate: 0.01\n",
      "Epoch [6414/20000], Loss: 0.9395904541015625, Learning Rate: 0.01\n",
      "Epoch [6415/20000], Loss: 0.9256134033203125, Learning Rate: 0.01\n",
      "Epoch [6416/20000], Loss: 0.923309326171875, Learning Rate: 0.01\n",
      "Epoch [6417/20000], Loss: 0.9285888671875, Learning Rate: 0.01\n",
      "Epoch [6418/20000], Loss: 0.9369659423828125, Learning Rate: 0.01\n",
      "Epoch [6419/20000], Loss: 0.9443511962890625, Learning Rate: 0.01\n",
      "Epoch [6420/20000], Loss: 0.9475250244140625, Learning Rate: 0.01\n",
      "Epoch [6421/20000], Loss: 0.94512939453125, Learning Rate: 0.01\n",
      "Epoch [6422/20000], Loss: 0.9372711181640625, Learning Rate: 0.01\n",
      "Epoch [6423/20000], Loss: 0.9247894287109375, Learning Rate: 0.01\n",
      "Epoch [6424/20000], Loss: 0.909576416015625, Learning Rate: 0.01\n",
      "Epoch [6425/20000], Loss: 0.8935394287109375, Learning Rate: 0.01\n",
      "Epoch [6426/20000], Loss: 0.8781280517578125, Learning Rate: 0.01\n",
      "Epoch [6427/20000], Loss: 0.864715576171875, Learning Rate: 0.01\n",
      "Epoch [6428/20000], Loss: 0.8539276123046875, Learning Rate: 0.01\n",
      "Epoch [6429/20000], Loss: 0.84564208984375, Learning Rate: 0.01\n",
      "Epoch [6430/20000], Loss: 0.8397674560546875, Learning Rate: 0.01\n",
      "Epoch [6431/20000], Loss: 0.8354949951171875, Learning Rate: 0.01\n",
      "Epoch [6432/20000], Loss: 0.8322906494140625, Learning Rate: 0.01\n",
      "Epoch [6433/20000], Loss: 0.829345703125, Learning Rate: 0.01\n",
      "Epoch [6434/20000], Loss: 0.8263702392578125, Learning Rate: 0.01\n",
      "Epoch [6435/20000], Loss: 0.822784423828125, Learning Rate: 0.01\n",
      "Epoch [6436/20000], Loss: 0.8184356689453125, Learning Rate: 0.01\n",
      "Epoch [6437/20000], Loss: 0.8131866455078125, Learning Rate: 0.01\n",
      "Epoch [6438/20000], Loss: 0.80712890625, Learning Rate: 0.01\n",
      "Epoch [6439/20000], Loss: 0.8007049560546875, Learning Rate: 0.01\n",
      "Epoch [6440/20000], Loss: 0.79364013671875, Learning Rate: 0.01\n",
      "Epoch [6441/20000], Loss: 0.786285400390625, Learning Rate: 0.01\n",
      "Epoch [6442/20000], Loss: 0.77886962890625, Learning Rate: 0.01\n",
      "Epoch [6443/20000], Loss: 0.771484375, Learning Rate: 0.01\n",
      "Epoch [6444/20000], Loss: 0.764312744140625, Learning Rate: 0.01\n",
      "Epoch [6445/20000], Loss: 0.7572021484375, Learning Rate: 0.01\n",
      "Epoch [6446/20000], Loss: 0.7503814697265625, Learning Rate: 0.01\n",
      "Epoch [6447/20000], Loss: 0.7438507080078125, Learning Rate: 0.01\n",
      "Epoch [6448/20000], Loss: 0.737518310546875, Learning Rate: 0.01\n",
      "Epoch [6449/20000], Loss: 0.7312469482421875, Learning Rate: 0.01\n",
      "Epoch [6450/20000], Loss: 0.72528076171875, Learning Rate: 0.01\n",
      "Epoch [6451/20000], Loss: 0.7193756103515625, Learning Rate: 0.01\n",
      "Epoch [6452/20000], Loss: 0.7137298583984375, Learning Rate: 0.01\n",
      "Epoch [6453/20000], Loss: 0.7080841064453125, Learning Rate: 0.01\n",
      "Epoch [6454/20000], Loss: 0.702362060546875, Learning Rate: 0.01\n",
      "Epoch [6455/20000], Loss: 0.696807861328125, Learning Rate: 0.01\n",
      "Epoch [6456/20000], Loss: 0.691192626953125, Learning Rate: 0.01\n",
      "Epoch [6457/20000], Loss: 0.68572998046875, Learning Rate: 0.01\n",
      "Epoch [6458/20000], Loss: 0.68017578125, Learning Rate: 0.01\n",
      "Epoch [6459/20000], Loss: 0.6745452880859375, Learning Rate: 0.01\n",
      "Epoch [6460/20000], Loss: 0.6690673828125, Learning Rate: 0.01\n",
      "Epoch [6461/20000], Loss: 0.6634979248046875, Learning Rate: 0.01\n",
      "Epoch [6462/20000], Loss: 0.65802001953125, Learning Rate: 0.01\n",
      "Epoch [6463/20000], Loss: 0.6526336669921875, Learning Rate: 0.01\n",
      "Epoch [6464/20000], Loss: 0.64715576171875, Learning Rate: 0.01\n",
      "Epoch [6465/20000], Loss: 0.641815185546875, Learning Rate: 0.01\n",
      "Epoch [6466/20000], Loss: 0.6365509033203125, Learning Rate: 0.01\n",
      "Epoch [6467/20000], Loss: 0.631317138671875, Learning Rate: 0.01\n",
      "Epoch [6468/20000], Loss: 0.62640380859375, Learning Rate: 0.01\n",
      "Epoch [6469/20000], Loss: 0.6214752197265625, Learning Rate: 0.01\n",
      "Epoch [6470/20000], Loss: 0.6167144775390625, Learning Rate: 0.01\n",
      "Epoch [6471/20000], Loss: 0.6124114990234375, Learning Rate: 0.01\n",
      "Epoch [6472/20000], Loss: 0.6083221435546875, Learning Rate: 0.01\n",
      "Epoch [6473/20000], Loss: 0.604888916015625, Learning Rate: 0.01\n",
      "Epoch [6474/20000], Loss: 0.6020660400390625, Learning Rate: 0.01\n",
      "Epoch [6475/20000], Loss: 0.6000823974609375, Learning Rate: 0.01\n",
      "Epoch [6476/20000], Loss: 0.5994110107421875, Learning Rate: 0.01\n",
      "Epoch [6477/20000], Loss: 0.600433349609375, Learning Rate: 0.01\n",
      "Epoch [6478/20000], Loss: 0.603485107421875, Learning Rate: 0.01\n",
      "Epoch [6479/20000], Loss: 0.6097869873046875, Learning Rate: 0.01\n",
      "Epoch [6480/20000], Loss: 0.6202392578125, Learning Rate: 0.01\n",
      "Epoch [6481/20000], Loss: 0.6366424560546875, Learning Rate: 0.01\n",
      "Epoch [6482/20000], Loss: 0.6611175537109375, Learning Rate: 0.01\n",
      "Epoch [6483/20000], Loss: 0.697113037109375, Learning Rate: 0.01\n",
      "Epoch [6484/20000], Loss: 0.7492828369140625, Learning Rate: 0.01\n",
      "Epoch [6485/20000], Loss: 0.8238525390625, Learning Rate: 0.01\n",
      "Epoch [6486/20000], Loss: 0.9303436279296875, Learning Rate: 0.01\n",
      "Epoch [6487/20000], Loss: 1.081207275390625, Learning Rate: 0.01\n",
      "Epoch [6488/20000], Loss: 1.2945556640625, Learning Rate: 0.01\n",
      "Epoch [6489/20000], Loss: 1.594024658203125, Learning Rate: 0.01\n",
      "Epoch [6490/20000], Loss: 2.0115966796875, Learning Rate: 0.01\n",
      "Epoch [6491/20000], Loss: 2.5852813720703125, Learning Rate: 0.01\n",
      "Epoch [6492/20000], Loss: 3.3598480224609375, Learning Rate: 0.01\n",
      "Epoch [6493/20000], Loss: 4.3713226318359375, Learning Rate: 0.01\n",
      "Epoch [6494/20000], Loss: 5.6369781494140625, Learning Rate: 0.01\n",
      "Epoch [6495/20000], Loss: 7.1014556884765625, Learning Rate: 0.01\n",
      "Epoch [6496/20000], Loss: 8.615142822265625, Learning Rate: 0.01\n",
      "Epoch [6497/20000], Loss: 9.8543701171875, Learning Rate: 0.01\n",
      "Epoch [6498/20000], Loss: 10.402328491210938, Learning Rate: 0.01\n",
      "Epoch [6499/20000], Loss: 9.82904052734375, Learning Rate: 0.01\n",
      "Epoch [6500/20000], Loss: 8.040145874023438, Learning Rate: 0.01\n",
      "Epoch [6501/20000], Loss: 5.4144439697265625, Learning Rate: 0.01\n",
      "Epoch [6502/20000], Loss: 2.7942657470703125, Learning Rate: 0.01\n",
      "Epoch [6503/20000], Loss: 1.0291900634765625, Learning Rate: 0.01\n",
      "Epoch [6504/20000], Loss: 0.5480804443359375, Learning Rate: 0.01\n",
      "Epoch [6505/20000], Loss: 1.1834869384765625, Learning Rate: 0.01\n",
      "Epoch [6506/20000], Loss: 2.3425750732421875, Learning Rate: 0.01\n",
      "Epoch [6507/20000], Loss: 3.3281097412109375, Learning Rate: 0.01\n",
      "Epoch [6508/20000], Loss: 3.6312713623046875, Learning Rate: 0.01\n",
      "Epoch [6509/20000], Loss: 3.13165283203125, Learning Rate: 0.01\n",
      "Epoch [6510/20000], Loss: 2.1065826416015625, Learning Rate: 0.01\n",
      "Epoch [6511/20000], Loss: 1.0671234130859375, Learning Rate: 0.01\n",
      "Epoch [6512/20000], Loss: 0.4589691162109375, Learning Rate: 0.01\n",
      "Epoch [6513/20000], Loss: 0.4438629150390625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6514/20000], Loss: 0.8655548095703125, Learning Rate: 0.01\n",
      "Epoch [6515/20000], Loss: 1.3875579833984375, Learning Rate: 0.01\n",
      "Epoch [6516/20000], Loss: 1.6912689208984375, Learning Rate: 0.01\n",
      "Epoch [6517/20000], Loss: 1.62457275390625, Learning Rate: 0.01\n",
      "Epoch [6518/20000], Loss: 1.2498321533203125, Learning Rate: 0.01\n",
      "Epoch [6519/20000], Loss: 0.7753448486328125, Learning Rate: 0.01\n",
      "Epoch [6520/20000], Loss: 0.426116943359375, Learning Rate: 0.01\n",
      "Epoch [6521/20000], Loss: 0.324005126953125, Learning Rate: 0.01\n",
      "Epoch [6522/20000], Loss: 0.4458465576171875, Learning Rate: 0.01\n",
      "Epoch [6523/20000], Loss: 0.6636199951171875, Learning Rate: 0.01\n",
      "Epoch [6524/20000], Loss: 0.8294677734375, Learning Rate: 0.01\n",
      "Epoch [6525/20000], Loss: 0.851715087890625, Learning Rate: 0.01\n",
      "Epoch [6526/20000], Loss: 0.7284088134765625, Learning Rate: 0.01\n",
      "Epoch [6527/20000], Loss: 0.53155517578125, Learning Rate: 0.01\n",
      "Epoch [6528/20000], Loss: 0.356536865234375, Learning Rate: 0.01\n",
      "Epoch [6529/20000], Loss: 0.2711639404296875, Learning Rate: 0.01\n",
      "Epoch [6530/20000], Loss: 0.287445068359375, Learning Rate: 0.01\n",
      "Epoch [6531/20000], Loss: 0.367523193359375, Learning Rate: 0.01\n",
      "Epoch [6532/20000], Loss: 0.4515380859375, Learning Rate: 0.01\n",
      "Epoch [6533/20000], Loss: 0.489959716796875, Learning Rate: 0.01\n",
      "Epoch [6534/20000], Loss: 0.4652862548828125, Learning Rate: 0.01\n",
      "Epoch [6535/20000], Loss: 0.392822265625, Learning Rate: 0.01\n",
      "Epoch [6536/20000], Loss: 0.3074798583984375, Learning Rate: 0.01\n",
      "Epoch [6537/20000], Loss: 0.2432708740234375, Learning Rate: 0.01\n",
      "Epoch [6538/20000], Loss: 0.2185211181640625, Learning Rate: 0.01\n",
      "Epoch [6539/20000], Loss: 0.2303314208984375, Learning Rate: 0.01\n",
      "Epoch [6540/20000], Loss: 0.2607574462890625, Learning Rate: 0.01\n",
      "Epoch [6541/20000], Loss: 0.287200927734375, Learning Rate: 0.01\n",
      "Epoch [6542/20000], Loss: 0.293975830078125, Learning Rate: 0.01\n",
      "Epoch [6543/20000], Loss: 0.277130126953125, Learning Rate: 0.01\n",
      "Epoch [6544/20000], Loss: 0.24371337890625, Learning Rate: 0.01\n",
      "Epoch [6545/20000], Loss: 0.2066802978515625, Learning Rate: 0.01\n",
      "Epoch [6546/20000], Loss: 0.177764892578125, Learning Rate: 0.01\n",
      "Epoch [6547/20000], Loss: 0.163604736328125, Learning Rate: 0.01\n",
      "Epoch [6548/20000], Loss: 0.1632537841796875, Learning Rate: 0.01\n",
      "Epoch [6549/20000], Loss: 0.1707916259765625, Learning Rate: 0.01\n",
      "Epoch [6550/20000], Loss: 0.178314208984375, Learning Rate: 0.01\n",
      "Epoch [6551/20000], Loss: 0.179901123046875, Learning Rate: 0.01\n",
      "Epoch [6552/20000], Loss: 0.1729278564453125, Learning Rate: 0.01\n",
      "Epoch [6553/20000], Loss: 0.1588134765625, Learning Rate: 0.01\n",
      "Epoch [6554/20000], Loss: 0.141571044921875, Learning Rate: 0.01\n",
      "Epoch [6555/20000], Loss: 0.1253662109375, Learning Rate: 0.01\n",
      "Epoch [6556/20000], Loss: 0.11322021484375, Learning Rate: 0.01\n",
      "Epoch [6557/20000], Loss: 0.1064453125, Learning Rate: 0.01\n",
      "Epoch [6558/20000], Loss: 0.103759765625, Learning Rate: 0.01\n",
      "Epoch [6559/20000], Loss: 0.1030731201171875, Learning Rate: 0.01\n",
      "Epoch [6560/20000], Loss: 0.1019439697265625, Learning Rate: 0.01\n",
      "Epoch [6561/20000], Loss: 0.0986480712890625, Learning Rate: 0.01\n",
      "Epoch [6562/20000], Loss: 0.09246826171875, Learning Rate: 0.01\n",
      "Epoch [6563/20000], Loss: 0.0840606689453125, Learning Rate: 0.01\n",
      "Epoch [6564/20000], Loss: 0.07440185546875, Learning Rate: 0.01\n",
      "Epoch [6565/20000], Loss: 0.0648651123046875, Learning Rate: 0.01\n",
      "Epoch [6566/20000], Loss: 0.0566558837890625, Learning Rate: 0.01\n",
      "Epoch [6567/20000], Loss: 0.049835205078125, Learning Rate: 0.01\n",
      "Epoch [6568/20000], Loss: 0.044647216796875, Learning Rate: 0.01\n",
      "Epoch [6569/20000], Loss: 0.0403900146484375, Learning Rate: 0.01\n",
      "Epoch [6570/20000], Loss: 0.03643798828125, Learning Rate: 0.01\n",
      "Epoch [6571/20000], Loss: 0.032196044921875, Learning Rate: 0.01\n",
      "Epoch [6572/20000], Loss: 0.0272064208984375, Learning Rate: 0.01\n",
      "Epoch [6573/20000], Loss: 0.0214080810546875, Learning Rate: 0.01\n",
      "Epoch [6574/20000], Loss: 0.0151214599609375, Learning Rate: 0.01\n",
      "Epoch [6575/20000], Loss: 0.0084381103515625, Learning Rate: 0.01\n",
      "Epoch [6576/20000], Loss: 0.001953125, Learning Rate: 0.01\n",
      "Epoch [6577/20000], Loss: -0.0041046142578125, Learning Rate: 0.01\n",
      "Epoch [6578/20000], Loss: -0.0096435546875, Learning Rate: 0.01\n",
      "Epoch [6579/20000], Loss: -0.014312744140625, Learning Rate: 0.01\n",
      "Epoch [6580/20000], Loss: -0.0182952880859375, Learning Rate: 0.01\n",
      "Epoch [6581/20000], Loss: -0.0213470458984375, Learning Rate: 0.01\n",
      "Epoch [6582/20000], Loss: -0.0234832763671875, Learning Rate: 0.01\n",
      "Epoch [6583/20000], Loss: -0.02435302734375, Learning Rate: 0.01\n",
      "Epoch [6584/20000], Loss: -0.0234222412109375, Learning Rate: 0.01\n",
      "Epoch [6585/20000], Loss: -0.01959228515625, Learning Rate: 0.01\n",
      "Epoch [6586/20000], Loss: -0.01165771484375, Learning Rate: 0.01\n",
      "Epoch [6587/20000], Loss: 0.0027618408203125, Learning Rate: 0.01\n",
      "Epoch [6588/20000], Loss: 0.02703857421875, Learning Rate: 0.01\n",
      "Epoch [6589/20000], Loss: 0.0659637451171875, Learning Rate: 0.01\n",
      "Epoch [6590/20000], Loss: 0.127197265625, Learning Rate: 0.01\n",
      "Epoch [6591/20000], Loss: 0.22210693359375, Learning Rate: 0.01\n",
      "Epoch [6592/20000], Loss: 0.367340087890625, Learning Rate: 0.01\n",
      "Epoch [6593/20000], Loss: 0.588287353515625, Learning Rate: 0.01\n",
      "Epoch [6594/20000], Loss: 0.9208831787109375, Learning Rate: 0.01\n",
      "Epoch [6595/20000], Loss: 1.41558837890625, Learning Rate: 0.01\n",
      "Epoch [6596/20000], Loss: 2.13739013671875, Learning Rate: 0.01\n",
      "Epoch [6597/20000], Loss: 3.15948486328125, Learning Rate: 0.01\n",
      "Epoch [6598/20000], Loss: 4.537353515625, Learning Rate: 0.01\n",
      "Epoch [6599/20000], Loss: 6.25335693359375, Learning Rate: 0.01\n",
      "Epoch [6600/20000], Loss: 8.113876342773438, Learning Rate: 0.01\n",
      "Epoch [6601/20000], Loss: 9.658599853515625, Learning Rate: 0.01\n",
      "Epoch [6602/20000], Loss: 10.173583984375, Learning Rate: 0.01\n",
      "Epoch [6603/20000], Loss: 9.038909912109375, Learning Rate: 0.01\n",
      "Epoch [6604/20000], Loss: 6.2932281494140625, Learning Rate: 0.01\n",
      "Epoch [6605/20000], Loss: 2.962799072265625, Learning Rate: 0.01\n",
      "Epoch [6606/20000], Loss: 0.5294189453125, Learning Rate: 0.01\n",
      "Epoch [6607/20000], Loss: -0.079193115234375, Learning Rate: 0.01\n",
      "Epoch [6608/20000], Loss: 0.94085693359375, Learning Rate: 0.01\n",
      "Epoch [6609/20000], Loss: 2.5451812744140625, Learning Rate: 0.01\n",
      "Epoch [6610/20000], Loss: 3.5479583740234375, Learning Rate: 0.01\n",
      "Epoch [6611/20000], Loss: 3.2923736572265625, Learning Rate: 0.01\n",
      "Epoch [6612/20000], Loss: 1.9988250732421875, Learning Rate: 0.01\n",
      "Epoch [6613/20000], Loss: 0.5550537109375, Learning Rate: 0.01\n",
      "Epoch [6614/20000], Loss: -0.16180419921875, Learning Rate: 0.01\n",
      "Epoch [6615/20000], Loss: 0.0969390869140625, Learning Rate: 0.01\n",
      "Epoch [6616/20000], Loss: 0.8850860595703125, Learning Rate: 0.01\n",
      "Epoch [6617/20000], Loss: 1.488037109375, Learning Rate: 0.01\n",
      "Epoch [6618/20000], Loss: 1.4476165771484375, Learning Rate: 0.01\n",
      "Epoch [6619/20000], Loss: 0.8320159912109375, Learning Rate: 0.01\n",
      "Epoch [6620/20000], Loss: 0.109039306640625, Learning Rate: 0.01\n",
      "Epoch [6621/20000], Loss: -0.251068115234375, Learning Rate: 0.01\n",
      "Epoch [6622/20000], Loss: -0.1159515380859375, Learning Rate: 0.01\n",
      "Epoch [6623/20000], Loss: 0.2777862548828125, Learning Rate: 0.01\n",
      "Epoch [6624/20000], Loss: 0.56182861328125, Learning Rate: 0.01\n",
      "Epoch [6625/20000], Loss: 0.5181884765625, Learning Rate: 0.01\n",
      "Epoch [6626/20000], Loss: 0.204376220703125, Learning Rate: 0.01\n",
      "Epoch [6627/20000], Loss: -0.1370697021484375, Learning Rate: 0.01\n",
      "Epoch [6628/20000], Loss: -0.2871856689453125, Learning Rate: 0.01\n",
      "Epoch [6629/20000], Loss: -0.2025909423828125, Learning Rate: 0.01\n",
      "Epoch [6630/20000], Loss: -0.012969970703125, Learning Rate: 0.01\n",
      "Epoch [6631/20000], Loss: 0.10504150390625, Learning Rate: 0.01\n",
      "Epoch [6632/20000], Loss: 0.0605316162109375, Learning Rate: 0.01\n",
      "Epoch [6633/20000], Loss: -0.1048583984375, Learning Rate: 0.01\n",
      "Epoch [6634/20000], Loss: -0.270599365234375, Learning Rate: 0.01\n",
      "Epoch [6635/20000], Loss: -0.3377685546875, Learning Rate: 0.01\n",
      "Epoch [6636/20000], Loss: -0.2931365966796875, Learning Rate: 0.01\n",
      "Epoch [6637/20000], Loss: -0.2021331787109375, Learning Rate: 0.01\n",
      "Epoch [6638/20000], Loss: -0.148193359375, Learning Rate: 0.01\n",
      "Epoch [6639/20000], Loss: -0.1720428466796875, Learning Rate: 0.01\n",
      "Epoch [6640/20000], Loss: -0.2531890869140625, Learning Rate: 0.01\n",
      "Epoch [6641/20000], Loss: -0.3355865478515625, Learning Rate: 0.01\n",
      "Epoch [6642/20000], Loss: -0.37286376953125, Learning Rate: 0.01\n",
      "Epoch [6643/20000], Loss: -0.357452392578125, Learning Rate: 0.01\n",
      "Epoch [6644/20000], Loss: -0.3180694580078125, Learning Rate: 0.01\n",
      "Epoch [6645/20000], Loss: -0.2930908203125, Learning Rate: 0.01\n",
      "Epoch [6646/20000], Loss: -0.3037872314453125, Learning Rate: 0.01\n",
      "Epoch [6647/20000], Loss: -0.3433837890625, Learning Rate: 0.01\n",
      "Epoch [6648/20000], Loss: -0.387481689453125, Learning Rate: 0.01\n",
      "Epoch [6649/20000], Loss: -0.4131317138671875, Learning Rate: 0.01\n",
      "Epoch [6650/20000], Loss: -0.413330078125, Learning Rate: 0.01\n",
      "Epoch [6651/20000], Loss: -0.398681640625, Learning Rate: 0.01\n",
      "Epoch [6652/20000], Loss: -0.3863983154296875, Learning Rate: 0.01\n",
      "Epoch [6653/20000], Loss: -0.3888702392578125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6654/20000], Loss: -0.4066314697265625, Learning Rate: 0.01\n",
      "Epoch [6655/20000], Loss: -0.430450439453125, Learning Rate: 0.01\n",
      "Epoch [6656/20000], Loss: -0.448974609375, Learning Rate: 0.01\n",
      "Epoch [6657/20000], Loss: -0.4564056396484375, Learning Rate: 0.01\n",
      "Epoch [6658/20000], Loss: -0.4546051025390625, Learning Rate: 0.01\n",
      "Epoch [6659/20000], Loss: -0.4503936767578125, Learning Rate: 0.01\n",
      "Epoch [6660/20000], Loss: -0.4509124755859375, Learning Rate: 0.01\n",
      "Epoch [6661/20000], Loss: -0.4587860107421875, Learning Rate: 0.01\n",
      "Epoch [6662/20000], Loss: -0.4718170166015625, Learning Rate: 0.01\n",
      "Epoch [6663/20000], Loss: -0.485198974609375, Learning Rate: 0.01\n",
      "Epoch [6664/20000], Loss: -0.4948272705078125, Learning Rate: 0.01\n",
      "Epoch [6665/20000], Loss: -0.4996490478515625, Learning Rate: 0.01\n",
      "Epoch [6666/20000], Loss: -0.501129150390625, Learning Rate: 0.01\n",
      "Epoch [6667/20000], Loss: -0.502716064453125, Learning Rate: 0.01\n",
      "Epoch [6668/20000], Loss: -0.5068511962890625, Learning Rate: 0.01\n",
      "Epoch [6669/20000], Loss: -0.5140838623046875, Learning Rate: 0.01\n",
      "Epoch [6670/20000], Loss: -0.523101806640625, Learning Rate: 0.01\n",
      "Epoch [6671/20000], Loss: -0.5319366455078125, Learning Rate: 0.01\n",
      "Epoch [6672/20000], Loss: -0.5390625, Learning Rate: 0.01\n",
      "Epoch [6673/20000], Loss: -0.5439605712890625, Learning Rate: 0.01\n",
      "Epoch [6674/20000], Loss: -0.5477294921875, Learning Rate: 0.01\n",
      "Epoch [6675/20000], Loss: -0.5515289306640625, Learning Rate: 0.01\n",
      "Epoch [6676/20000], Loss: -0.5563812255859375, Learning Rate: 0.01\n",
      "Epoch [6677/20000], Loss: -0.5624847412109375, Learning Rate: 0.01\n",
      "Epoch [6678/20000], Loss: -0.5693511962890625, Learning Rate: 0.01\n",
      "Epoch [6679/20000], Loss: -0.576385498046875, Learning Rate: 0.01\n",
      "Epoch [6680/20000], Loss: -0.5825653076171875, Learning Rate: 0.01\n",
      "Epoch [6681/20000], Loss: -0.588043212890625, Learning Rate: 0.01\n",
      "Epoch [6682/20000], Loss: -0.592864990234375, Learning Rate: 0.01\n",
      "Epoch [6683/20000], Loss: -0.5975341796875, Learning Rate: 0.01\n",
      "Epoch [6684/20000], Loss: -0.602630615234375, Learning Rate: 0.01\n",
      "Epoch [6685/20000], Loss: -0.60809326171875, Learning Rate: 0.01\n",
      "Epoch [6686/20000], Loss: -0.6140289306640625, Learning Rate: 0.01\n",
      "Epoch [6687/20000], Loss: -0.6200103759765625, Learning Rate: 0.01\n",
      "Epoch [6688/20000], Loss: -0.625946044921875, Learning Rate: 0.01\n",
      "Epoch [6689/20000], Loss: -0.6315765380859375, Learning Rate: 0.01\n",
      "Epoch [6690/20000], Loss: -0.636810302734375, Learning Rate: 0.01\n",
      "Epoch [6691/20000], Loss: -0.6418609619140625, Learning Rate: 0.01\n",
      "Epoch [6692/20000], Loss: -0.646820068359375, Learning Rate: 0.01\n",
      "Epoch [6693/20000], Loss: -0.65191650390625, Learning Rate: 0.01\n",
      "Epoch [6694/20000], Loss: -0.6572113037109375, Learning Rate: 0.01\n",
      "Epoch [6695/20000], Loss: -0.6623687744140625, Learning Rate: 0.01\n",
      "Epoch [6696/20000], Loss: -0.6675567626953125, Learning Rate: 0.01\n",
      "Epoch [6697/20000], Loss: -0.67242431640625, Learning Rate: 0.01\n",
      "Epoch [6698/20000], Loss: -0.6768341064453125, Learning Rate: 0.01\n",
      "Epoch [6699/20000], Loss: -0.680755615234375, Learning Rate: 0.01\n",
      "Epoch [6700/20000], Loss: -0.6840362548828125, Learning Rate: 0.01\n",
      "Epoch [6701/20000], Loss: -0.6863555908203125, Learning Rate: 0.01\n",
      "Epoch [6702/20000], Loss: -0.68756103515625, Learning Rate: 0.01\n",
      "Epoch [6703/20000], Loss: -0.687164306640625, Learning Rate: 0.01\n",
      "Epoch [6704/20000], Loss: -0.684173583984375, Learning Rate: 0.01\n",
      "Epoch [6705/20000], Loss: -0.6774444580078125, Learning Rate: 0.01\n",
      "Epoch [6706/20000], Loss: -0.665252685546875, Learning Rate: 0.01\n",
      "Epoch [6707/20000], Loss: -0.64508056640625, Learning Rate: 0.01\n",
      "Epoch [6708/20000], Loss: -0.6129150390625, Learning Rate: 0.01\n",
      "Epoch [6709/20000], Loss: -0.5631256103515625, Learning Rate: 0.01\n",
      "Epoch [6710/20000], Loss: -0.4871063232421875, Learning Rate: 0.01\n",
      "Epoch [6711/20000], Loss: -0.372100830078125, Learning Rate: 0.01\n",
      "Epoch [6712/20000], Loss: -0.19903564453125, Learning Rate: 0.01\n",
      "Epoch [6713/20000], Loss: 0.0600128173828125, Learning Rate: 0.01\n",
      "Epoch [6714/20000], Loss: 0.4457855224609375, Learning Rate: 0.01\n",
      "Epoch [6715/20000], Loss: 1.01409912109375, Learning Rate: 0.01\n",
      "Epoch [6716/20000], Loss: 1.84130859375, Learning Rate: 0.01\n",
      "Epoch [6717/20000], Loss: 3.013824462890625, Learning Rate: 0.01\n",
      "Epoch [6718/20000], Loss: 4.6186065673828125, Learning Rate: 0.01\n",
      "Epoch [6719/20000], Loss: 6.674041748046875, Learning Rate: 0.01\n",
      "Epoch [6720/20000], Loss: 9.061737060546875, Learning Rate: 0.01\n",
      "Epoch [6721/20000], Loss: 11.349212646484375, Learning Rate: 0.01\n",
      "Epoch [6722/20000], Loss: 12.795989990234375, Learning Rate: 0.01\n",
      "Epoch [6723/20000], Loss: 12.457916259765625, Learning Rate: 0.01\n",
      "Epoch [6724/20000], Loss: 9.9036865234375, Learning Rate: 0.01\n",
      "Epoch [6725/20000], Loss: 5.743865966796875, Learning Rate: 0.01\n",
      "Epoch [6726/20000], Loss: 1.6854705810546875, Learning Rate: 0.01\n",
      "Epoch [6727/20000], Loss: -0.572723388671875, Learning Rate: 0.01\n",
      "Epoch [6728/20000], Loss: -0.4332427978515625, Learning Rate: 0.01\n",
      "Epoch [6729/20000], Loss: 1.34759521484375, Learning Rate: 0.01\n",
      "Epoch [6730/20000], Loss: 3.290771484375, Learning Rate: 0.01\n",
      "Epoch [6731/20000], Loss: 4.0878448486328125, Learning Rate: 0.01\n",
      "Epoch [6732/20000], Loss: 3.276153564453125, Learning Rate: 0.01\n",
      "Epoch [6733/20000], Loss: 1.4594573974609375, Learning Rate: 0.01\n",
      "Epoch [6734/20000], Loss: -0.1916351318359375, Learning Rate: 0.01\n",
      "Epoch [6735/20000], Loss: -0.7823486328125, Learning Rate: 0.01\n",
      "Epoch [6736/20000], Loss: -0.2627410888671875, Learning Rate: 0.01\n",
      "Epoch [6737/20000], Loss: 0.6956634521484375, Learning Rate: 0.01\n",
      "Epoch [6738/20000], Loss: 1.2755584716796875, Learning Rate: 0.01\n",
      "Epoch [6739/20000], Loss: 1.067779541015625, Learning Rate: 0.01\n",
      "Epoch [6740/20000], Loss: 0.2794036865234375, Learning Rate: 0.01\n",
      "Epoch [6741/20000], Loss: -0.515411376953125, Learning Rate: 0.01\n",
      "Epoch [6742/20000], Loss: -0.8355865478515625, Learning Rate: 0.01\n",
      "Epoch [6743/20000], Loss: -0.61578369140625, Learning Rate: 0.01\n",
      "Epoch [6744/20000], Loss: -0.1715240478515625, Learning Rate: 0.01\n",
      "Epoch [6745/20000], Loss: 0.0931854248046875, Learning Rate: 0.01\n",
      "Epoch [6746/20000], Loss: -0.02056884765625, Learning Rate: 0.01\n",
      "Epoch [6747/20000], Loss: -0.4059600830078125, Learning Rate: 0.01\n",
      "Epoch [6748/20000], Loss: -0.78216552734375, Learning Rate: 0.01\n",
      "Epoch [6749/20000], Loss: -0.9226531982421875, Learning Rate: 0.01\n",
      "Epoch [6750/20000], Loss: -0.80499267578125, Learning Rate: 0.01\n",
      "Epoch [6751/20000], Loss: -0.587310791015625, Learning Rate: 0.01\n",
      "Epoch [6752/20000], Loss: -0.46514892578125, Learning Rate: 0.01\n",
      "Epoch [6753/20000], Loss: -0.5305328369140625, Learning Rate: 0.01\n",
      "Epoch [6754/20000], Loss: -0.7281341552734375, Learning Rate: 0.01\n",
      "Epoch [6755/20000], Loss: -0.9204254150390625, Learning Rate: 0.01\n",
      "Epoch [6756/20000], Loss: -0.9966888427734375, Learning Rate: 0.01\n",
      "Epoch [6757/20000], Loss: -0.9430694580078125, Learning Rate: 0.01\n",
      "Epoch [6758/20000], Loss: -0.833953857421875, Learning Rate: 0.01\n",
      "Epoch [6759/20000], Loss: -0.765380859375, Learning Rate: 0.01\n",
      "Epoch [6760/20000], Loss: -0.7881927490234375, Learning Rate: 0.01\n",
      "Epoch [6761/20000], Loss: -0.8831939697265625, Learning Rate: 0.01\n",
      "Epoch [6762/20000], Loss: -0.9871978759765625, Learning Rate: 0.01\n",
      "Epoch [6763/20000], Loss: -1.0422821044921875, Learning Rate: 0.01\n",
      "Epoch [6764/20000], Loss: -1.0323486328125, Learning Rate: 0.01\n",
      "Epoch [6765/20000], Loss: -0.984771728515625, Learning Rate: 0.01\n",
      "Epoch [6766/20000], Loss: -0.944580078125, Learning Rate: 0.01\n",
      "Epoch [6767/20000], Loss: -0.9431610107421875, Learning Rate: 0.01\n",
      "Epoch [6768/20000], Loss: -0.9807891845703125, Learning Rate: 0.01\n",
      "Epoch [6769/20000], Loss: -1.0337677001953125, Learning Rate: 0.01\n",
      "Epoch [6770/20000], Loss: -1.072967529296875, Learning Rate: 0.01\n",
      "Epoch [6771/20000], Loss: -1.0837860107421875, Learning Rate: 0.01\n",
      "Epoch [6772/20000], Loss: -1.071319580078125, Learning Rate: 0.01\n",
      "Epoch [6773/20000], Loss: -1.053131103515625, Learning Rate: 0.01\n",
      "Epoch [6774/20000], Loss: -1.0467529296875, Learning Rate: 0.01\n",
      "Epoch [6775/20000], Loss: -1.05841064453125, Learning Rate: 0.01\n",
      "Epoch [6776/20000], Loss: -1.082244873046875, Learning Rate: 0.01\n",
      "Epoch [6777/20000], Loss: -1.1065673828125, Learning Rate: 0.01\n",
      "Epoch [6778/20000], Loss: -1.1214599609375, Learning Rate: 0.01\n",
      "Epoch [6779/20000], Loss: -1.124237060546875, Learning Rate: 0.01\n",
      "Epoch [6780/20000], Loss: -1.1201019287109375, Learning Rate: 0.01\n",
      "Epoch [6781/20000], Loss: -1.1168670654296875, Learning Rate: 0.01\n",
      "Epoch [6782/20000], Loss: -1.120086669921875, Learning Rate: 0.01\n",
      "Epoch [6783/20000], Loss: -1.1306610107421875, Learning Rate: 0.01\n",
      "Epoch [6784/20000], Loss: -1.1449127197265625, Learning Rate: 0.01\n",
      "Epoch [6785/20000], Loss: -1.157806396484375, Learning Rate: 0.01\n",
      "Epoch [6786/20000], Loss: -1.165985107421875, Learning Rate: 0.01\n",
      "Epoch [6787/20000], Loss: -1.1691741943359375, Learning Rate: 0.01\n",
      "Epoch [6788/20000], Loss: -1.169891357421875, Learning Rate: 0.01\n",
      "Epoch [6789/20000], Loss: -1.171600341796875, Learning Rate: 0.01\n",
      "Epoch [6790/20000], Loss: -1.1761322021484375, Learning Rate: 0.01\n",
      "Epoch [6791/20000], Loss: -1.18402099609375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6792/20000], Loss: -1.1934661865234375, Learning Rate: 0.01\n",
      "Epoch [6793/20000], Loss: -1.202392578125, Learning Rate: 0.01\n",
      "Epoch [6794/20000], Loss: -1.2093048095703125, Learning Rate: 0.01\n",
      "Epoch [6795/20000], Loss: -1.2142333984375, Learning Rate: 0.01\n",
      "Epoch [6796/20000], Loss: -1.2175445556640625, Learning Rate: 0.01\n",
      "Epoch [6797/20000], Loss: -1.220977783203125, Learning Rate: 0.01\n",
      "Epoch [6798/20000], Loss: -1.2254638671875, Learning Rate: 0.01\n",
      "Epoch [6799/20000], Loss: -1.231292724609375, Learning Rate: 0.01\n",
      "Epoch [6800/20000], Loss: -1.238189697265625, Learning Rate: 0.01\n",
      "Epoch [6801/20000], Loss: -1.2452850341796875, Learning Rate: 0.01\n",
      "Epoch [6802/20000], Loss: -1.251739501953125, Learning Rate: 0.01\n",
      "Epoch [6803/20000], Loss: -1.2573394775390625, Learning Rate: 0.01\n",
      "Epoch [6804/20000], Loss: -1.26226806640625, Learning Rate: 0.01\n",
      "Epoch [6805/20000], Loss: -1.266845703125, Learning Rate: 0.01\n",
      "Epoch [6806/20000], Loss: -1.2714691162109375, Learning Rate: 0.01\n",
      "Epoch [6807/20000], Loss: -1.2767333984375, Learning Rate: 0.01\n",
      "Epoch [6808/20000], Loss: -1.2824554443359375, Learning Rate: 0.01\n",
      "Epoch [6809/20000], Loss: -1.288330078125, Learning Rate: 0.01\n",
      "Epoch [6810/20000], Loss: -1.294281005859375, Learning Rate: 0.01\n",
      "Epoch [6811/20000], Loss: -1.3000335693359375, Learning Rate: 0.01\n",
      "Epoch [6812/20000], Loss: -1.305419921875, Learning Rate: 0.01\n",
      "Epoch [6813/20000], Loss: -1.3106689453125, Learning Rate: 0.01\n",
      "Epoch [6814/20000], Loss: -1.315704345703125, Learning Rate: 0.01\n",
      "Epoch [6815/20000], Loss: -1.320770263671875, Learning Rate: 0.01\n",
      "Epoch [6816/20000], Loss: -1.3261260986328125, Learning Rate: 0.01\n",
      "Epoch [6817/20000], Loss: -1.331451416015625, Learning Rate: 0.01\n",
      "Epoch [6818/20000], Loss: -1.3370361328125, Learning Rate: 0.01\n",
      "Epoch [6819/20000], Loss: -1.342681884765625, Learning Rate: 0.01\n",
      "Epoch [6820/20000], Loss: -1.3482666015625, Learning Rate: 0.01\n",
      "Epoch [6821/20000], Loss: -1.35382080078125, Learning Rate: 0.01\n",
      "Epoch [6822/20000], Loss: -1.35906982421875, Learning Rate: 0.01\n",
      "Epoch [6823/20000], Loss: -1.364288330078125, Learning Rate: 0.01\n",
      "Epoch [6824/20000], Loss: -1.3695831298828125, Learning Rate: 0.01\n",
      "Epoch [6825/20000], Loss: -1.3748626708984375, Learning Rate: 0.01\n",
      "Epoch [6826/20000], Loss: -1.3801116943359375, Learning Rate: 0.01\n",
      "Epoch [6827/20000], Loss: -1.3856658935546875, Learning Rate: 0.01\n",
      "Epoch [6828/20000], Loss: -1.3909912109375, Learning Rate: 0.01\n",
      "Epoch [6829/20000], Loss: -1.3965606689453125, Learning Rate: 0.01\n",
      "Epoch [6830/20000], Loss: -1.401947021484375, Learning Rate: 0.01\n",
      "Epoch [6831/20000], Loss: -1.4073028564453125, Learning Rate: 0.01\n",
      "Epoch [6832/20000], Loss: -1.4127197265625, Learning Rate: 0.01\n",
      "Epoch [6833/20000], Loss: -1.418121337890625, Learning Rate: 0.01\n",
      "Epoch [6834/20000], Loss: -1.4234619140625, Learning Rate: 0.01\n",
      "Epoch [6835/20000], Loss: -1.42877197265625, Learning Rate: 0.01\n",
      "Epoch [6836/20000], Loss: -1.4341583251953125, Learning Rate: 0.01\n",
      "Epoch [6837/20000], Loss: -1.43951416015625, Learning Rate: 0.01\n",
      "Epoch [6838/20000], Loss: -1.4448699951171875, Learning Rate: 0.01\n",
      "Epoch [6839/20000], Loss: -1.450286865234375, Learning Rate: 0.01\n",
      "Epoch [6840/20000], Loss: -1.4556884765625, Learning Rate: 0.01\n",
      "Epoch [6841/20000], Loss: -1.4610595703125, Learning Rate: 0.01\n",
      "Epoch [6842/20000], Loss: -1.466522216796875, Learning Rate: 0.01\n",
      "Epoch [6843/20000], Loss: -1.4718475341796875, Learning Rate: 0.01\n",
      "Epoch [6844/20000], Loss: -1.4771575927734375, Learning Rate: 0.01\n",
      "Epoch [6845/20000], Loss: -1.4826812744140625, Learning Rate: 0.01\n",
      "Epoch [6846/20000], Loss: -1.4880218505859375, Learning Rate: 0.01\n",
      "Epoch [6847/20000], Loss: -1.493408203125, Learning Rate: 0.01\n",
      "Epoch [6848/20000], Loss: -1.498779296875, Learning Rate: 0.01\n",
      "Epoch [6849/20000], Loss: -1.5041351318359375, Learning Rate: 0.01\n",
      "Epoch [6850/20000], Loss: -1.509552001953125, Learning Rate: 0.01\n",
      "Epoch [6851/20000], Loss: -1.514923095703125, Learning Rate: 0.01\n",
      "Epoch [6852/20000], Loss: -1.52032470703125, Learning Rate: 0.01\n",
      "Epoch [6853/20000], Loss: -1.5256805419921875, Learning Rate: 0.01\n",
      "Epoch [6854/20000], Loss: -1.5311431884765625, Learning Rate: 0.01\n",
      "Epoch [6855/20000], Loss: -1.53656005859375, Learning Rate: 0.01\n",
      "Epoch [6856/20000], Loss: -1.54193115234375, Learning Rate: 0.01\n",
      "Epoch [6857/20000], Loss: -1.54730224609375, Learning Rate: 0.01\n",
      "Epoch [6858/20000], Loss: -1.5527496337890625, Learning Rate: 0.01\n",
      "Epoch [6859/20000], Loss: -1.5580291748046875, Learning Rate: 0.01\n",
      "Epoch [6860/20000], Loss: -1.563507080078125, Learning Rate: 0.01\n",
      "Epoch [6861/20000], Loss: -1.56884765625, Learning Rate: 0.01\n",
      "Epoch [6862/20000], Loss: -1.5743408203125, Learning Rate: 0.01\n",
      "Epoch [6863/20000], Loss: -1.5796051025390625, Learning Rate: 0.01\n",
      "Epoch [6864/20000], Loss: -1.5850830078125, Learning Rate: 0.01\n",
      "Epoch [6865/20000], Loss: -1.590484619140625, Learning Rate: 0.01\n",
      "Epoch [6866/20000], Loss: -1.59588623046875, Learning Rate: 0.01\n",
      "Epoch [6867/20000], Loss: -1.601318359375, Learning Rate: 0.01\n",
      "Epoch [6868/20000], Loss: -1.6067352294921875, Learning Rate: 0.01\n",
      "Epoch [6869/20000], Loss: -1.612213134765625, Learning Rate: 0.01\n",
      "Epoch [6870/20000], Loss: -1.6175384521484375, Learning Rate: 0.01\n",
      "Epoch [6871/20000], Loss: -1.62298583984375, Learning Rate: 0.01\n",
      "Epoch [6872/20000], Loss: -1.628387451171875, Learning Rate: 0.01\n",
      "Epoch [6873/20000], Loss: -1.6338348388671875, Learning Rate: 0.01\n",
      "Epoch [6874/20000], Loss: -1.6392364501953125, Learning Rate: 0.01\n",
      "Epoch [6875/20000], Loss: -1.6446380615234375, Learning Rate: 0.01\n",
      "Epoch [6876/20000], Loss: -1.6500244140625, Learning Rate: 0.01\n",
      "Epoch [6877/20000], Loss: -1.655487060546875, Learning Rate: 0.01\n",
      "Epoch [6878/20000], Loss: -1.6609039306640625, Learning Rate: 0.01\n",
      "Epoch [6879/20000], Loss: -1.666351318359375, Learning Rate: 0.01\n",
      "Epoch [6880/20000], Loss: -1.6717376708984375, Learning Rate: 0.01\n",
      "Epoch [6881/20000], Loss: -1.6771697998046875, Learning Rate: 0.01\n",
      "Epoch [6882/20000], Loss: -1.6824951171875, Learning Rate: 0.01\n",
      "Epoch [6883/20000], Loss: -1.6878814697265625, Learning Rate: 0.01\n",
      "Epoch [6884/20000], Loss: -1.6934051513671875, Learning Rate: 0.01\n",
      "Epoch [6885/20000], Loss: -1.698822021484375, Learning Rate: 0.01\n",
      "Epoch [6886/20000], Loss: -1.704315185546875, Learning Rate: 0.01\n",
      "Epoch [6887/20000], Loss: -1.709686279296875, Learning Rate: 0.01\n",
      "Epoch [6888/20000], Loss: -1.715087890625, Learning Rate: 0.01\n",
      "Epoch [6889/20000], Loss: -1.7205047607421875, Learning Rate: 0.01\n",
      "Epoch [6890/20000], Loss: -1.725830078125, Learning Rate: 0.01\n",
      "Epoch [6891/20000], Loss: -1.731201171875, Learning Rate: 0.01\n",
      "Epoch [6892/20000], Loss: -1.7366790771484375, Learning Rate: 0.01\n",
      "Epoch [6893/20000], Loss: -1.741943359375, Learning Rate: 0.01\n",
      "Epoch [6894/20000], Loss: -1.7473297119140625, Learning Rate: 0.01\n",
      "Epoch [6895/20000], Loss: -1.752410888671875, Learning Rate: 0.01\n",
      "Epoch [6896/20000], Loss: -1.7574310302734375, Learning Rate: 0.01\n",
      "Epoch [6897/20000], Loss: -1.7622528076171875, Learning Rate: 0.01\n",
      "Epoch [6898/20000], Loss: -1.7666168212890625, Learning Rate: 0.01\n",
      "Epoch [6899/20000], Loss: -1.77056884765625, Learning Rate: 0.01\n",
      "Epoch [6900/20000], Loss: -1.77337646484375, Learning Rate: 0.01\n",
      "Epoch [6901/20000], Loss: -1.7745361328125, Learning Rate: 0.01\n",
      "Epoch [6902/20000], Loss: -1.7732391357421875, Learning Rate: 0.01\n",
      "Epoch [6903/20000], Loss: -1.767669677734375, Learning Rate: 0.01\n",
      "Epoch [6904/20000], Loss: -1.754974365234375, Learning Rate: 0.01\n",
      "Epoch [6905/20000], Loss: -1.730804443359375, Learning Rate: 0.01\n",
      "Epoch [6906/20000], Loss: -1.687286376953125, Learning Rate: 0.01\n",
      "Epoch [6907/20000], Loss: -1.6116943359375, Learning Rate: 0.01\n",
      "Epoch [6908/20000], Loss: -1.482696533203125, Learning Rate: 0.01\n",
      "Epoch [6909/20000], Loss: -1.2649078369140625, Learning Rate: 0.01\n",
      "Epoch [6910/20000], Loss: -0.9008941650390625, Learning Rate: 0.01\n",
      "Epoch [6911/20000], Loss: -0.2982330322265625, Learning Rate: 0.01\n",
      "Epoch [6912/20000], Loss: 0.6829833984375, Learning Rate: 0.01\n",
      "Epoch [6913/20000], Loss: 2.2364349365234375, Learning Rate: 0.01\n",
      "Epoch [6914/20000], Loss: 4.576507568359375, Learning Rate: 0.01\n",
      "Epoch [6915/20000], Loss: 7.80853271484375, Learning Rate: 0.01\n",
      "Epoch [6916/20000], Loss: 11.606521606445312, Learning Rate: 0.01\n",
      "Epoch [6917/20000], Loss: 14.797576904296875, Learning Rate: 0.01\n",
      "Epoch [6918/20000], Loss: 15.349884033203125, Learning Rate: 0.01\n",
      "Epoch [6919/20000], Loss: 11.792724609375, Learning Rate: 0.01\n",
      "Epoch [6920/20000], Loss: 5.3848419189453125, Learning Rate: 0.01\n",
      "Epoch [6921/20000], Loss: 0.1214447021484375, Learning Rate: 0.01\n",
      "Epoch [6922/20000], Loss: -0.7970123291015625, Learning Rate: 0.01\n",
      "Epoch [6923/20000], Loss: 2.1170806884765625, Learning Rate: 0.01\n",
      "Epoch [6924/20000], Loss: 5.4153900146484375, Learning Rate: 0.01\n",
      "Epoch [6925/20000], Loss: 5.7723388671875, Learning Rate: 0.01\n",
      "Epoch [6926/20000], Loss: 2.7877044677734375, Learning Rate: 0.01\n",
      "Epoch [6927/20000], Loss: -0.7037200927734375, Learning Rate: 0.01\n",
      "Epoch [6928/20000], Loss: -1.6854400634765625, Learning Rate: 0.01\n",
      "Epoch [6929/20000], Loss: -0.0498046875, Learning Rate: 0.01\n",
      "Epoch [6930/20000], Loss: 1.7525482177734375, Learning Rate: 0.01\n",
      "Epoch [6931/20000], Loss: 1.5738067626953125, Learning Rate: 0.01\n",
      "Epoch [6932/20000], Loss: -0.3037261962890625, Learning Rate: 0.01\n",
      "Epoch [6933/20000], Loss: -1.805572509765625, Learning Rate: 0.01\n",
      "Epoch [6934/20000], Loss: -1.573333740234375, Learning Rate: 0.01\n",
      "Epoch [6935/20000], Loss: -0.3188629150390625, Learning Rate: 0.01\n",
      "Epoch [6936/20000], Loss: 0.3005523681640625, Learning Rate: 0.01\n",
      "Epoch [6937/20000], Loss: -0.386444091796875, Learning Rate: 0.01\n",
      "Epoch [6938/20000], Loss: -1.5066070556640625, Learning Rate: 0.01\n",
      "Epoch [6939/20000], Loss: -1.8667449951171875, Learning Rate: 0.01\n",
      "Epoch [6940/20000], Loss: -1.3292236328125, Learning Rate: 0.01\n",
      "Epoch [6941/20000], Loss: -0.766998291015625, Learning Rate: 0.01\n",
      "Epoch [6942/20000], Loss: -0.9125213623046875, Learning Rate: 0.01\n",
      "Epoch [6943/20000], Loss: -1.5727691650390625, Learning Rate: 0.01\n",
      "Epoch [6944/20000], Loss: -2.0070953369140625, Learning Rate: 0.01\n",
      "Epoch [6945/20000], Loss: -1.85235595703125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6946/20000], Loss: -1.4557647705078125, Learning Rate: 0.01\n",
      "Epoch [6947/20000], Loss: -1.3558807373046875, Learning Rate: 0.01\n",
      "Epoch [6948/20000], Loss: -1.65191650390625, Learning Rate: 0.01\n",
      "Epoch [6949/20000], Loss: -1.978790283203125, Learning Rate: 0.01\n",
      "Epoch [6950/20000], Loss: -2.001251220703125, Learning Rate: 0.01\n",
      "Epoch [6951/20000], Loss: -1.782440185546875, Learning Rate: 0.01\n",
      "Epoch [6952/20000], Loss: -1.6379547119140625, Learning Rate: 0.01\n",
      "Epoch [6953/20000], Loss: -1.7410125732421875, Learning Rate: 0.01\n",
      "Epoch [6954/20000], Loss: -1.9592742919921875, Learning Rate: 0.01\n",
      "Epoch [6955/20000], Loss: -2.058624267578125, Learning Rate: 0.01\n",
      "Epoch [6956/20000], Loss: -1.9797515869140625, Learning Rate: 0.01\n",
      "Epoch [6957/20000], Loss: -1.8685455322265625, Learning Rate: 0.01\n",
      "Epoch [6958/20000], Loss: -1.87689208984375, Learning Rate: 0.01\n",
      "Epoch [6959/20000], Loss: -1.996063232421875, Learning Rate: 0.01\n",
      "Epoch [6960/20000], Loss: -2.0966949462890625, Learning Rate: 0.01\n",
      "Epoch [6961/20000], Loss: -2.0925750732421875, Learning Rate: 0.01\n",
      "Epoch [6962/20000], Loss: -2.0250396728515625, Learning Rate: 0.01\n",
      "Epoch [6963/20000], Loss: -1.9939117431640625, Learning Rate: 0.01\n",
      "Epoch [6964/20000], Loss: -2.0391693115234375, Learning Rate: 0.01\n",
      "Epoch [6965/20000], Loss: -2.11016845703125, Learning Rate: 0.01\n",
      "Epoch [6966/20000], Loss: -2.137542724609375, Learning Rate: 0.01\n",
      "Epoch [6967/20000], Loss: -2.110595703125, Learning Rate: 0.01\n",
      "Epoch [6968/20000], Loss: -2.07763671875, Learning Rate: 0.01\n",
      "Epoch [6969/20000], Loss: -2.0833282470703125, Learning Rate: 0.01\n",
      "Epoch [6970/20000], Loss: -2.122894287109375, Learning Rate: 0.01\n",
      "Epoch [6971/20000], Loss: -2.156707763671875, Learning Rate: 0.01\n",
      "Epoch [6972/20000], Loss: -2.1585235595703125, Learning Rate: 0.01\n",
      "Epoch [6973/20000], Loss: -2.140411376953125, Learning Rate: 0.01\n",
      "Epoch [6974/20000], Loss: -2.1323699951171875, Learning Rate: 0.01\n",
      "Epoch [6975/20000], Loss: -2.1481781005859375, Learning Rate: 0.01\n",
      "Epoch [6976/20000], Loss: -2.17437744140625, Learning Rate: 0.01\n",
      "Epoch [6977/20000], Loss: -2.189239501953125, Learning Rate: 0.01\n",
      "Epoch [6978/20000], Loss: -2.1867218017578125, Learning Rate: 0.01\n",
      "Epoch [6979/20000], Loss: -2.1794586181640625, Learning Rate: 0.01\n",
      "Epoch [6980/20000], Loss: -2.1822052001953125, Learning Rate: 0.01\n",
      "Epoch [6981/20000], Loss: -2.19677734375, Learning Rate: 0.01\n",
      "Epoch [6982/20000], Loss: -2.2124786376953125, Learning Rate: 0.01\n",
      "Epoch [6983/20000], Loss: -2.2195587158203125, Learning Rate: 0.01\n",
      "Epoch [6984/20000], Loss: -2.2184600830078125, Learning Rate: 0.01\n",
      "Epoch [6985/20000], Loss: -2.2174530029296875, Learning Rate: 0.01\n",
      "Epoch [6986/20000], Loss: -2.2232208251953125, Learning Rate: 0.01\n",
      "Epoch [6987/20000], Loss: -2.233978271484375, Learning Rate: 0.01\n",
      "Epoch [6988/20000], Loss: -2.2434844970703125, Learning Rate: 0.01\n",
      "Epoch [6989/20000], Loss: -2.2476806640625, Learning Rate: 0.01\n",
      "Epoch [6990/20000], Loss: -2.2481689453125, Learning Rate: 0.01\n",
      "Epoch [6991/20000], Loss: -2.2496337890625, Learning Rate: 0.01\n",
      "Epoch [6992/20000], Loss: -2.2545318603515625, Learning Rate: 0.01\n",
      "Epoch [6993/20000], Loss: -2.26141357421875, Learning Rate: 0.01\n",
      "Epoch [6994/20000], Loss: -2.2664642333984375, Learning Rate: 0.01\n",
      "Epoch [6995/20000], Loss: -2.2682342529296875, Learning Rate: 0.01\n",
      "Epoch [6996/20000], Loss: -2.267364501953125, Learning Rate: 0.01\n",
      "Epoch [6997/20000], Loss: -2.26678466796875, Learning Rate: 0.01\n",
      "Epoch [6998/20000], Loss: -2.2666473388671875, Learning Rate: 0.01\n",
      "Epoch [6999/20000], Loss: -2.2657318115234375, Learning Rate: 0.01\n",
      "Epoch [7000/20000], Loss: -2.2616729736328125, Learning Rate: 0.01\n",
      "Epoch [7001/20000], Loss: -2.25274658203125, Learning Rate: 0.01\n",
      "Epoch [7002/20000], Loss: -2.23883056640625, Learning Rate: 0.01\n",
      "Epoch [7003/20000], Loss: -2.2196197509765625, Learning Rate: 0.01\n",
      "Epoch [7004/20000], Loss: -2.1935272216796875, Learning Rate: 0.01\n",
      "Epoch [7005/20000], Loss: -2.157012939453125, Learning Rate: 0.01\n",
      "Epoch [7006/20000], Loss: -2.105377197265625, Learning Rate: 0.01\n",
      "Epoch [7007/20000], Loss: -2.0325927734375, Learning Rate: 0.01\n",
      "Epoch [7008/20000], Loss: -1.9320068359375, Learning Rate: 0.01\n",
      "Epoch [7009/20000], Loss: -1.7940826416015625, Learning Rate: 0.01\n",
      "Epoch [7010/20000], Loss: -1.6061553955078125, Learning Rate: 0.01\n",
      "Epoch [7011/20000], Loss: -1.3509521484375, Learning Rate: 0.01\n",
      "Epoch [7012/20000], Loss: -1.007171630859375, Learning Rate: 0.01\n",
      "Epoch [7013/20000], Loss: -0.54974365234375, Learning Rate: 0.01\n",
      "Epoch [7014/20000], Loss: 0.0439605712890625, Learning Rate: 0.01\n",
      "Epoch [7015/20000], Loss: 0.792633056640625, Learning Rate: 0.01\n",
      "Epoch [7016/20000], Loss: 1.6892242431640625, Learning Rate: 0.01\n",
      "Epoch [7017/20000], Loss: 2.6930389404296875, Learning Rate: 0.01\n",
      "Epoch [7018/20000], Loss: 3.6858062744140625, Learning Rate: 0.01\n",
      "Epoch [7019/20000], Loss: 4.4822235107421875, Learning Rate: 0.01\n",
      "Epoch [7020/20000], Loss: 4.815765380859375, Learning Rate: 0.01\n",
      "Epoch [7021/20000], Loss: 4.463287353515625, Learning Rate: 0.01\n",
      "Epoch [7022/20000], Loss: 3.333160400390625, Learning Rate: 0.01\n",
      "Epoch [7023/20000], Loss: 1.6280517578125, Learning Rate: 0.01\n",
      "Epoch [7024/20000], Loss: -0.2132720947265625, Learning Rate: 0.01\n",
      "Epoch [7025/20000], Loss: -1.6677398681640625, Learning Rate: 0.01\n",
      "Epoch [7026/20000], Loss: -2.3867340087890625, Learning Rate: 0.01\n",
      "Epoch [7027/20000], Loss: -2.3276824951171875, Learning Rate: 0.01\n",
      "Epoch [7028/20000], Loss: -1.7270660400390625, Learning Rate: 0.01\n",
      "Epoch [7029/20000], Loss: -0.963897705078125, Learning Rate: 0.01\n",
      "Epoch [7030/20000], Loss: -0.401824951171875, Learning Rate: 0.01\n",
      "Epoch [7031/20000], Loss: -0.2723388671875, Learning Rate: 0.01\n",
      "Epoch [7032/20000], Loss: -0.6028900146484375, Learning Rate: 0.01\n",
      "Epoch [7033/20000], Loss: -1.234954833984375, Learning Rate: 0.01\n",
      "Epoch [7034/20000], Loss: -1.901947021484375, Learning Rate: 0.01\n",
      "Epoch [7035/20000], Loss: -2.36248779296875, Learning Rate: 0.01\n",
      "Epoch [7036/20000], Loss: -2.49951171875, Learning Rate: 0.01\n",
      "Epoch [7037/20000], Loss: -2.3488616943359375, Learning Rate: 0.01\n",
      "Epoch [7038/20000], Loss: -2.05462646484375, Learning Rate: 0.01\n",
      "Epoch [7039/20000], Loss: -1.788787841796875, Learning Rate: 0.01\n",
      "Epoch [7040/20000], Loss: -1.67724609375, Learning Rate: 0.01\n",
      "Epoch [7041/20000], Loss: -1.7577056884765625, Learning Rate: 0.01\n",
      "Epoch [7042/20000], Loss: -1.981597900390625, Learning Rate: 0.01\n",
      "Epoch [7043/20000], Loss: -2.247802734375, Learning Rate: 0.01\n",
      "Epoch [7044/20000], Loss: -2.4553375244140625, Learning Rate: 0.01\n",
      "Epoch [7045/20000], Loss: -2.545196533203125, Learning Rate: 0.01\n",
      "Epoch [7046/20000], Loss: -2.51678466796875, Learning Rate: 0.01\n",
      "Epoch [7047/20000], Loss: -2.4160003662109375, Learning Rate: 0.01\n",
      "Epoch [7048/20000], Loss: -2.307464599609375, Learning Rate: 0.01\n",
      "Epoch [7049/20000], Loss: -2.2454071044921875, Learning Rate: 0.01\n",
      "Epoch [7050/20000], Loss: -2.2543487548828125, Learning Rate: 0.01\n",
      "Epoch [7051/20000], Loss: -2.325836181640625, Learning Rate: 0.01\n",
      "Epoch [7052/20000], Loss: -2.42767333984375, Learning Rate: 0.01\n",
      "Epoch [7053/20000], Loss: -2.5225372314453125, Learning Rate: 0.01\n",
      "Epoch [7054/20000], Loss: -2.5819244384765625, Learning Rate: 0.01\n",
      "Epoch [7055/20000], Loss: -2.5962371826171875, Learning Rate: 0.01\n",
      "Epoch [7056/20000], Loss: -2.5745697021484375, Learning Rate: 0.01\n",
      "Epoch [7057/20000], Loss: -2.536834716796875, Learning Rate: 0.01\n",
      "Epoch [7058/20000], Loss: -2.5047607421875, Learning Rate: 0.01\n",
      "Epoch [7059/20000], Loss: -2.4929656982421875, Learning Rate: 0.01\n",
      "Epoch [7060/20000], Loss: -2.506011962890625, Learning Rate: 0.01\n",
      "Epoch [7061/20000], Loss: -2.538360595703125, Learning Rate: 0.01\n",
      "Epoch [7062/20000], Loss: -2.57861328125, Learning Rate: 0.01\n",
      "Epoch [7063/20000], Loss: -2.614898681640625, Learning Rate: 0.01\n",
      "Epoch [7064/20000], Loss: -2.6388702392578125, Learning Rate: 0.01\n",
      "Epoch [7065/20000], Loss: -2.6478729248046875, Learning Rate: 0.01\n",
      "Epoch [7066/20000], Loss: -2.644775390625, Learning Rate: 0.01\n",
      "Epoch [7067/20000], Loss: -2.6355743408203125, Learning Rate: 0.01\n",
      "Epoch [7068/20000], Loss: -2.627044677734375, Learning Rate: 0.01\n",
      "Epoch [7069/20000], Loss: -2.623931884765625, Learning Rate: 0.01\n",
      "Epoch [7070/20000], Loss: -2.6286468505859375, Learning Rate: 0.01\n",
      "Epoch [7071/20000], Loss: -2.640411376953125, Learning Rate: 0.01\n",
      "Epoch [7072/20000], Loss: -2.656341552734375, Learning Rate: 0.01\n",
      "Epoch [7073/20000], Loss: -2.6731109619140625, Learning Rate: 0.01\n",
      "Epoch [7074/20000], Loss: -2.68731689453125, Learning Rate: 0.01\n",
      "Epoch [7075/20000], Loss: -2.697509765625, Learning Rate: 0.01\n",
      "Epoch [7076/20000], Loss: -2.703216552734375, Learning Rate: 0.01\n",
      "Epoch [7077/20000], Loss: -2.705657958984375, Learning Rate: 0.01\n",
      "Epoch [7078/20000], Loss: -2.70623779296875, Learning Rate: 0.01\n",
      "Epoch [7079/20000], Loss: -2.707061767578125, Learning Rate: 0.01\n",
      "Epoch [7080/20000], Loss: -2.70916748046875, Learning Rate: 0.01\n",
      "Epoch [7081/20000], Loss: -2.713287353515625, Learning Rate: 0.01\n",
      "Epoch [7082/20000], Loss: -2.7197113037109375, Learning Rate: 0.01\n",
      "Epoch [7083/20000], Loss: -2.7273712158203125, Learning Rate: 0.01\n",
      "Epoch [7084/20000], Loss: -2.7359161376953125, Learning Rate: 0.01\n",
      "Epoch [7085/20000], Loss: -2.7442474365234375, Learning Rate: 0.01\n",
      "Epoch [7086/20000], Loss: -2.7519073486328125, Learning Rate: 0.01\n",
      "Epoch [7087/20000], Loss: -2.7583770751953125, Learning Rate: 0.01\n",
      "Epoch [7088/20000], Loss: -2.7638092041015625, Learning Rate: 0.01\n",
      "Epoch [7089/20000], Loss: -2.768310546875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7090/20000], Loss: -2.772186279296875, Learning Rate: 0.01\n",
      "Epoch [7091/20000], Loss: -2.776031494140625, Learning Rate: 0.01\n",
      "Epoch [7092/20000], Loss: -2.7801055908203125, Learning Rate: 0.01\n",
      "Epoch [7093/20000], Loss: -2.7845001220703125, Learning Rate: 0.01\n",
      "Epoch [7094/20000], Loss: -2.7893524169921875, Learning Rate: 0.01\n",
      "Epoch [7095/20000], Loss: -2.7947235107421875, Learning Rate: 0.01\n",
      "Epoch [7096/20000], Loss: -2.80047607421875, Learning Rate: 0.01\n",
      "Epoch [7097/20000], Loss: -2.80633544921875, Learning Rate: 0.01\n",
      "Epoch [7098/20000], Loss: -2.8122100830078125, Learning Rate: 0.01\n",
      "Epoch [7099/20000], Loss: -2.8180084228515625, Learning Rate: 0.01\n",
      "Epoch [7100/20000], Loss: -2.8237152099609375, Learning Rate: 0.01\n",
      "Epoch [7101/20000], Loss: -2.8290557861328125, Learning Rate: 0.01\n",
      "Epoch [7102/20000], Loss: -2.8341522216796875, Learning Rate: 0.01\n",
      "Epoch [7103/20000], Loss: -2.8392486572265625, Learning Rate: 0.01\n",
      "Epoch [7104/20000], Loss: -2.8440704345703125, Learning Rate: 0.01\n",
      "Epoch [7105/20000], Loss: -2.848968505859375, Learning Rate: 0.01\n",
      "Epoch [7106/20000], Loss: -2.8537445068359375, Learning Rate: 0.01\n",
      "Epoch [7107/20000], Loss: -2.858642578125, Learning Rate: 0.01\n",
      "Epoch [7108/20000], Loss: -2.863525390625, Learning Rate: 0.01\n",
      "Epoch [7109/20000], Loss: -2.8683624267578125, Learning Rate: 0.01\n",
      "Epoch [7110/20000], Loss: -2.8734283447265625, Learning Rate: 0.01\n",
      "Epoch [7111/20000], Loss: -2.878570556640625, Learning Rate: 0.01\n",
      "Epoch [7112/20000], Loss: -2.883544921875, Learning Rate: 0.01\n",
      "Epoch [7113/20000], Loss: -2.8888092041015625, Learning Rate: 0.01\n",
      "Epoch [7114/20000], Loss: -2.8940887451171875, Learning Rate: 0.01\n",
      "Epoch [7115/20000], Loss: -2.8992919921875, Learning Rate: 0.01\n",
      "Epoch [7116/20000], Loss: -2.904388427734375, Learning Rate: 0.01\n",
      "Epoch [7117/20000], Loss: -2.9095306396484375, Learning Rate: 0.01\n",
      "Epoch [7118/20000], Loss: -2.9146881103515625, Learning Rate: 0.01\n",
      "Epoch [7119/20000], Loss: -2.9198760986328125, Learning Rate: 0.01\n",
      "Epoch [7120/20000], Loss: -2.9250030517578125, Learning Rate: 0.01\n",
      "Epoch [7121/20000], Loss: -2.930084228515625, Learning Rate: 0.01\n",
      "Epoch [7122/20000], Loss: -2.9352264404296875, Learning Rate: 0.01\n",
      "Epoch [7123/20000], Loss: -2.9402618408203125, Learning Rate: 0.01\n",
      "Epoch [7124/20000], Loss: -2.945343017578125, Learning Rate: 0.01\n",
      "Epoch [7125/20000], Loss: -2.950439453125, Learning Rate: 0.01\n",
      "Epoch [7126/20000], Loss: -2.9554901123046875, Learning Rate: 0.01\n",
      "Epoch [7127/20000], Loss: -2.9604644775390625, Learning Rate: 0.01\n",
      "Epoch [7128/20000], Loss: -2.965545654296875, Learning Rate: 0.01\n",
      "Epoch [7129/20000], Loss: -2.970611572265625, Learning Rate: 0.01\n",
      "Epoch [7130/20000], Loss: -2.97564697265625, Learning Rate: 0.01\n",
      "Epoch [7131/20000], Loss: -2.9806976318359375, Learning Rate: 0.01\n",
      "Epoch [7132/20000], Loss: -2.9857177734375, Learning Rate: 0.01\n",
      "Epoch [7133/20000], Loss: -2.9908599853515625, Learning Rate: 0.01\n",
      "Epoch [7134/20000], Loss: -2.9958038330078125, Learning Rate: 0.01\n",
      "Epoch [7135/20000], Loss: -3.0008544921875, Learning Rate: 0.01\n",
      "Epoch [7136/20000], Loss: -3.00592041015625, Learning Rate: 0.01\n",
      "Epoch [7137/20000], Loss: -3.0110321044921875, Learning Rate: 0.01\n",
      "Epoch [7138/20000], Loss: -3.01605224609375, Learning Rate: 0.01\n",
      "Epoch [7139/20000], Loss: -3.021148681640625, Learning Rate: 0.01\n",
      "Epoch [7140/20000], Loss: -3.026214599609375, Learning Rate: 0.01\n",
      "Epoch [7141/20000], Loss: -3.031219482421875, Learning Rate: 0.01\n",
      "Epoch [7142/20000], Loss: -3.0361480712890625, Learning Rate: 0.01\n",
      "Epoch [7143/20000], Loss: -3.04119873046875, Learning Rate: 0.01\n",
      "Epoch [7144/20000], Loss: -3.0461883544921875, Learning Rate: 0.01\n",
      "Epoch [7145/20000], Loss: -3.051116943359375, Learning Rate: 0.01\n",
      "Epoch [7146/20000], Loss: -3.0561065673828125, Learning Rate: 0.01\n",
      "Epoch [7147/20000], Loss: -3.06097412109375, Learning Rate: 0.01\n",
      "Epoch [7148/20000], Loss: -3.0657958984375, Learning Rate: 0.01\n",
      "Epoch [7149/20000], Loss: -3.0704803466796875, Learning Rate: 0.01\n",
      "Epoch [7150/20000], Loss: -3.075164794921875, Learning Rate: 0.01\n",
      "Epoch [7151/20000], Loss: -3.0797271728515625, Learning Rate: 0.01\n",
      "Epoch [7152/20000], Loss: -3.0839385986328125, Learning Rate: 0.01\n",
      "Epoch [7153/20000], Loss: -3.0880584716796875, Learning Rate: 0.01\n",
      "Epoch [7154/20000], Loss: -3.0917510986328125, Learning Rate: 0.01\n",
      "Epoch [7155/20000], Loss: -3.094970703125, Learning Rate: 0.01\n",
      "Epoch [7156/20000], Loss: -3.0974884033203125, Learning Rate: 0.01\n",
      "Epoch [7157/20000], Loss: -3.0991058349609375, Learning Rate: 0.01\n",
      "Epoch [7158/20000], Loss: -3.09930419921875, Learning Rate: 0.01\n",
      "Epoch [7159/20000], Loss: -3.097747802734375, Learning Rate: 0.01\n",
      "Epoch [7160/20000], Loss: -3.093353271484375, Learning Rate: 0.01\n",
      "Epoch [7161/20000], Loss: -3.0853271484375, Learning Rate: 0.01\n",
      "Epoch [7162/20000], Loss: -3.0718231201171875, Learning Rate: 0.01\n",
      "Epoch [7163/20000], Loss: -3.0502777099609375, Learning Rate: 0.01\n",
      "Epoch [7164/20000], Loss: -3.017578125, Learning Rate: 0.01\n",
      "Epoch [7165/20000], Loss: -2.968292236328125, Learning Rate: 0.01\n",
      "Epoch [7166/20000], Loss: -2.895233154296875, Learning Rate: 0.01\n",
      "Epoch [7167/20000], Loss: -2.787353515625, Learning Rate: 0.01\n",
      "Epoch [7168/20000], Loss: -2.6290740966796875, Learning Rate: 0.01\n",
      "Epoch [7169/20000], Loss: -2.3981170654296875, Learning Rate: 0.01\n",
      "Epoch [7170/20000], Loss: -2.0629730224609375, Learning Rate: 0.01\n",
      "Epoch [7171/20000], Loss: -1.5818328857421875, Learning Rate: 0.01\n",
      "Epoch [7172/20000], Loss: -0.9002685546875, Learning Rate: 0.01\n",
      "Epoch [7173/20000], Loss: 0.040374755859375, Learning Rate: 0.01\n",
      "Epoch [7174/20000], Loss: 1.29449462890625, Learning Rate: 0.01\n",
      "Epoch [7175/20000], Loss: 2.8644256591796875, Learning Rate: 0.01\n",
      "Epoch [7176/20000], Loss: 4.6575775146484375, Learning Rate: 0.01\n",
      "Epoch [7177/20000], Loss: 6.3748931884765625, Learning Rate: 0.01\n",
      "Epoch [7178/20000], Loss: 7.5213165283203125, Learning Rate: 0.01\n",
      "Epoch [7179/20000], Loss: 7.455657958984375, Learning Rate: 0.01\n",
      "Epoch [7180/20000], Loss: 5.8202362060546875, Learning Rate: 0.01\n",
      "Epoch [7181/20000], Loss: 2.8765716552734375, Learning Rate: 0.01\n",
      "Epoch [7182/20000], Loss: -0.33990478515625, Learning Rate: 0.01\n",
      "Epoch [7183/20000], Loss: -2.59979248046875, Learning Rate: 0.01\n",
      "Epoch [7184/20000], Loss: -3.2013397216796875, Learning Rate: 0.01\n",
      "Epoch [7185/20000], Loss: -2.33367919921875, Learning Rate: 0.01\n",
      "Epoch [7186/20000], Loss: -0.8431854248046875, Learning Rate: 0.01\n",
      "Epoch [7187/20000], Loss: 0.2843170166015625, Learning Rate: 0.01\n",
      "Epoch [7188/20000], Loss: 0.401611328125, Learning Rate: 0.01\n",
      "Epoch [7189/20000], Loss: -0.5019073486328125, Learning Rate: 0.01\n",
      "Epoch [7190/20000], Loss: -1.86419677734375, Learning Rate: 0.01\n",
      "Epoch [7191/20000], Loss: -2.9253692626953125, Learning Rate: 0.01\n",
      "Epoch [7192/20000], Loss: -3.222259521484375, Learning Rate: 0.01\n",
      "Epoch [7193/20000], Loss: -2.814605712890625, Learning Rate: 0.01\n",
      "Epoch [7194/20000], Loss: -2.151702880859375, Learning Rate: 0.01\n",
      "Epoch [7195/20000], Loss: -1.736907958984375, Learning Rate: 0.01\n",
      "Epoch [7196/20000], Loss: -1.8265228271484375, Learning Rate: 0.01\n",
      "Epoch [7197/20000], Loss: -2.3260040283203125, Learning Rate: 0.01\n",
      "Epoch [7198/20000], Loss: -2.9033660888671875, Learning Rate: 0.01\n",
      "Epoch [7199/20000], Loss: -3.238372802734375, Learning Rate: 0.01\n",
      "Epoch [7200/20000], Loss: -3.2171478271484375, Learning Rate: 0.01\n",
      "Epoch [7201/20000], Loss: -2.962005615234375, Learning Rate: 0.01\n",
      "Epoch [7202/20000], Loss: -2.7093505859375, Learning Rate: 0.01\n",
      "Epoch [7203/20000], Loss: -2.643096923828125, Learning Rate: 0.01\n",
      "Epoch [7204/20000], Loss: -2.7955322265625, Learning Rate: 0.01\n",
      "Epoch [7205/20000], Loss: -3.0558013916015625, Learning Rate: 0.01\n",
      "Epoch [7206/20000], Loss: -3.266998291015625, Learning Rate: 0.01\n",
      "Epoch [7207/20000], Loss: -3.3307952880859375, Learning Rate: 0.01\n",
      "Epoch [7208/20000], Loss: -3.256744384765625, Learning Rate: 0.01\n",
      "Epoch [7209/20000], Loss: -3.1346893310546875, Learning Rate: 0.01\n",
      "Epoch [7210/20000], Loss: -3.064178466796875, Learning Rate: 0.01\n",
      "Epoch [7211/20000], Loss: -3.0933837890625, Learning Rate: 0.01\n",
      "Epoch [7212/20000], Loss: -3.1998138427734375, Learning Rate: 0.01\n",
      "Epoch [7213/20000], Loss: -3.3180694580078125, Learning Rate: 0.01\n",
      "Epoch [7214/20000], Loss: -3.3870086669921875, Learning Rate: 0.01\n",
      "Epoch [7215/20000], Loss: -3.3856048583984375, Learning Rate: 0.01\n",
      "Epoch [7216/20000], Loss: -3.33734130859375, Learning Rate: 0.01\n",
      "Epoch [7217/20000], Loss: -3.288055419921875, Learning Rate: 0.01\n",
      "Epoch [7218/20000], Loss: -3.275054931640625, Learning Rate: 0.01\n",
      "Epoch [7219/20000], Loss: -3.3072509765625, Learning Rate: 0.01\n",
      "Epoch [7220/20000], Loss: -3.365814208984375, Learning Rate: 0.01\n",
      "Epoch [7221/20000], Loss: -3.4198760986328125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7222/20000], Loss: -3.44647216796875, Learning Rate: 0.01\n",
      "Epoch [7223/20000], Loss: -3.44183349609375, Learning Rate: 0.01\n",
      "Epoch [7224/20000], Loss: -3.4193572998046875, Learning Rate: 0.01\n",
      "Epoch [7225/20000], Loss: -3.399871826171875, Learning Rate: 0.01\n",
      "Epoch [7226/20000], Loss: -3.3980865478515625, Learning Rate: 0.01\n",
      "Epoch [7227/20000], Loss: -3.41656494140625, Learning Rate: 0.01\n",
      "Epoch [7228/20000], Loss: -3.446319580078125, Learning Rate: 0.01\n",
      "Epoch [7229/20000], Loss: -3.4742279052734375, Learning Rate: 0.01\n",
      "Epoch [7230/20000], Loss: -3.490478515625, Learning Rate: 0.01\n",
      "Epoch [7231/20000], Loss: -3.4925994873046875, Learning Rate: 0.01\n",
      "Epoch [7232/20000], Loss: -3.486114501953125, Learning Rate: 0.01\n",
      "Epoch [7233/20000], Loss: -3.47906494140625, Learning Rate: 0.01\n",
      "Epoch [7234/20000], Loss: -3.4786224365234375, Learning Rate: 0.01\n",
      "Epoch [7235/20000], Loss: -3.48681640625, Learning Rate: 0.01\n",
      "Epoch [7236/20000], Loss: -3.5013275146484375, Learning Rate: 0.01\n",
      "Epoch [7237/20000], Loss: -3.516937255859375, Learning Rate: 0.01\n",
      "Epoch [7238/20000], Loss: -3.529083251953125, Learning Rate: 0.01\n",
      "Epoch [7239/20000], Loss: -3.53564453125, Learning Rate: 0.01\n",
      "Epoch [7240/20000], Loss: -3.5374908447265625, Learning Rate: 0.01\n",
      "Epoch [7241/20000], Loss: -3.537506103515625, Learning Rate: 0.01\n",
      "Epoch [7242/20000], Loss: -3.5386505126953125, Learning Rate: 0.01\n",
      "Epoch [7243/20000], Loss: -3.5427398681640625, Learning Rate: 0.01\n",
      "Epoch [7244/20000], Loss: -3.5499267578125, Learning Rate: 0.01\n",
      "Epoch [7245/20000], Loss: -3.5589141845703125, Learning Rate: 0.01\n",
      "Epoch [7246/20000], Loss: -3.567626953125, Learning Rate: 0.01\n",
      "Epoch [7247/20000], Loss: -3.5748291015625, Learning Rate: 0.01\n",
      "Epoch [7248/20000], Loss: -3.5801544189453125, Learning Rate: 0.01\n",
      "Epoch [7249/20000], Loss: -3.583953857421875, Learning Rate: 0.01\n",
      "Epoch [7250/20000], Loss: -3.587158203125, Learning Rate: 0.01\n",
      "Epoch [7251/20000], Loss: -3.5908660888671875, Learning Rate: 0.01\n",
      "Epoch [7252/20000], Loss: -3.5956878662109375, Learning Rate: 0.01\n",
      "Epoch [7253/20000], Loss: -3.6014404296875, Learning Rate: 0.01\n",
      "Epoch [7254/20000], Loss: -3.6078643798828125, Learning Rate: 0.01\n",
      "Epoch [7255/20000], Loss: -3.6142730712890625, Learning Rate: 0.01\n",
      "Epoch [7256/20000], Loss: -3.6201019287109375, Learning Rate: 0.01\n",
      "Epoch [7257/20000], Loss: -3.6253509521484375, Learning Rate: 0.01\n",
      "Epoch [7258/20000], Loss: -3.6300201416015625, Learning Rate: 0.01\n",
      "Epoch [7259/20000], Loss: -3.6343841552734375, Learning Rate: 0.01\n",
      "Epoch [7260/20000], Loss: -3.638824462890625, Learning Rate: 0.01\n",
      "Epoch [7261/20000], Loss: -3.643646240234375, Learning Rate: 0.01\n",
      "Epoch [7262/20000], Loss: -3.6487884521484375, Learning Rate: 0.01\n",
      "Epoch [7263/20000], Loss: -3.6541900634765625, Learning Rate: 0.01\n",
      "Epoch [7264/20000], Loss: -3.6598358154296875, Learning Rate: 0.01\n",
      "Epoch [7265/20000], Loss: -3.6653594970703125, Learning Rate: 0.01\n",
      "Epoch [7266/20000], Loss: -3.6706085205078125, Learning Rate: 0.01\n",
      "Epoch [7267/20000], Loss: -3.675567626953125, Learning Rate: 0.01\n",
      "Epoch [7268/20000], Loss: -3.680419921875, Learning Rate: 0.01\n",
      "Epoch [7269/20000], Loss: -3.6852264404296875, Learning Rate: 0.01\n",
      "Epoch [7270/20000], Loss: -3.68994140625, Learning Rate: 0.01\n",
      "Epoch [7271/20000], Loss: -3.69488525390625, Learning Rate: 0.01\n",
      "Epoch [7272/20000], Loss: -3.6998748779296875, Learning Rate: 0.01\n",
      "Epoch [7273/20000], Loss: -3.70501708984375, Learning Rate: 0.01\n",
      "Epoch [7274/20000], Loss: -3.7103118896484375, Learning Rate: 0.01\n",
      "Epoch [7275/20000], Loss: -3.71551513671875, Learning Rate: 0.01\n",
      "Epoch [7276/20000], Loss: -3.720611572265625, Learning Rate: 0.01\n",
      "Epoch [7277/20000], Loss: -3.7257537841796875, Learning Rate: 0.01\n",
      "Epoch [7278/20000], Loss: -3.730743408203125, Learning Rate: 0.01\n",
      "Epoch [7279/20000], Loss: -3.7356414794921875, Learning Rate: 0.01\n",
      "Epoch [7280/20000], Loss: -3.7405853271484375, Learning Rate: 0.01\n",
      "Epoch [7281/20000], Loss: -3.74554443359375, Learning Rate: 0.01\n",
      "Epoch [7282/20000], Loss: -3.7505340576171875, Learning Rate: 0.01\n",
      "Epoch [7283/20000], Loss: -3.755523681640625, Learning Rate: 0.01\n",
      "Epoch [7284/20000], Loss: -3.7606048583984375, Learning Rate: 0.01\n",
      "Epoch [7285/20000], Loss: -3.7656402587890625, Learning Rate: 0.01\n",
      "Epoch [7286/20000], Loss: -3.7707366943359375, Learning Rate: 0.01\n",
      "Epoch [7287/20000], Loss: -3.7758026123046875, Learning Rate: 0.01\n",
      "Epoch [7288/20000], Loss: -3.7808380126953125, Learning Rate: 0.01\n",
      "Epoch [7289/20000], Loss: -3.785888671875, Learning Rate: 0.01\n",
      "Epoch [7290/20000], Loss: -3.790863037109375, Learning Rate: 0.01\n",
      "Epoch [7291/20000], Loss: -3.7958984375, Learning Rate: 0.01\n",
      "Epoch [7292/20000], Loss: -3.800872802734375, Learning Rate: 0.01\n",
      "Epoch [7293/20000], Loss: -3.8059234619140625, Learning Rate: 0.01\n",
      "Epoch [7294/20000], Loss: -3.8109130859375, Learning Rate: 0.01\n",
      "Epoch [7295/20000], Loss: -3.8159027099609375, Learning Rate: 0.01\n",
      "Epoch [7296/20000], Loss: -3.8210601806640625, Learning Rate: 0.01\n",
      "Epoch [7297/20000], Loss: -3.82598876953125, Learning Rate: 0.01\n",
      "Epoch [7298/20000], Loss: -3.8310699462890625, Learning Rate: 0.01\n",
      "Epoch [7299/20000], Loss: -3.8361663818359375, Learning Rate: 0.01\n",
      "Epoch [7300/20000], Loss: -3.8411865234375, Learning Rate: 0.01\n",
      "Epoch [7301/20000], Loss: -3.846160888671875, Learning Rate: 0.01\n",
      "Epoch [7302/20000], Loss: -3.851287841796875, Learning Rate: 0.01\n",
      "Epoch [7303/20000], Loss: -3.85626220703125, Learning Rate: 0.01\n",
      "Epoch [7304/20000], Loss: -3.8613128662109375, Learning Rate: 0.01\n",
      "Epoch [7305/20000], Loss: -3.8663330078125, Learning Rate: 0.01\n",
      "Epoch [7306/20000], Loss: -3.87139892578125, Learning Rate: 0.01\n",
      "Epoch [7307/20000], Loss: -3.8764190673828125, Learning Rate: 0.01\n",
      "Epoch [7308/20000], Loss: -3.8815155029296875, Learning Rate: 0.01\n",
      "Epoch [7309/20000], Loss: -3.8864593505859375, Learning Rate: 0.01\n",
      "Epoch [7310/20000], Loss: -3.8915863037109375, Learning Rate: 0.01\n",
      "Epoch [7311/20000], Loss: -3.8966217041015625, Learning Rate: 0.01\n",
      "Epoch [7312/20000], Loss: -3.901641845703125, Learning Rate: 0.01\n",
      "Epoch [7313/20000], Loss: -3.9066314697265625, Learning Rate: 0.01\n",
      "Epoch [7314/20000], Loss: -3.9117279052734375, Learning Rate: 0.01\n",
      "Epoch [7315/20000], Loss: -3.916717529296875, Learning Rate: 0.01\n",
      "Epoch [7316/20000], Loss: -3.92181396484375, Learning Rate: 0.01\n",
      "Epoch [7317/20000], Loss: -3.9268646240234375, Learning Rate: 0.01\n",
      "Epoch [7318/20000], Loss: -3.9319305419921875, Learning Rate: 0.01\n",
      "Epoch [7319/20000], Loss: -3.9370269775390625, Learning Rate: 0.01\n",
      "Epoch [7320/20000], Loss: -3.9420166015625, Learning Rate: 0.01\n",
      "Epoch [7321/20000], Loss: -3.94708251953125, Learning Rate: 0.01\n",
      "Epoch [7322/20000], Loss: -3.9520721435546875, Learning Rate: 0.01\n",
      "Epoch [7323/20000], Loss: -3.9572601318359375, Learning Rate: 0.01\n",
      "Epoch [7324/20000], Loss: -3.962249755859375, Learning Rate: 0.01\n",
      "Epoch [7325/20000], Loss: -3.9672698974609375, Learning Rate: 0.01\n",
      "Epoch [7326/20000], Loss: -3.9722900390625, Learning Rate: 0.01\n",
      "Epoch [7327/20000], Loss: -3.9773406982421875, Learning Rate: 0.01\n",
      "Epoch [7328/20000], Loss: -3.982452392578125, Learning Rate: 0.01\n",
      "Epoch [7329/20000], Loss: -3.9875030517578125, Learning Rate: 0.01\n",
      "Epoch [7330/20000], Loss: -3.992523193359375, Learning Rate: 0.01\n",
      "Epoch [7331/20000], Loss: -3.9976348876953125, Learning Rate: 0.01\n",
      "Epoch [7332/20000], Loss: -4.0025787353515625, Learning Rate: 0.01\n",
      "Epoch [7333/20000], Loss: -4.0075836181640625, Learning Rate: 0.01\n",
      "Epoch [7334/20000], Loss: -4.0126800537109375, Learning Rate: 0.01\n",
      "Epoch [7335/20000], Loss: -4.0176849365234375, Learning Rate: 0.01\n",
      "Epoch [7336/20000], Loss: -4.0226287841796875, Learning Rate: 0.01\n",
      "Epoch [7337/20000], Loss: -4.027618408203125, Learning Rate: 0.01\n",
      "Epoch [7338/20000], Loss: -4.032470703125, Learning Rate: 0.01\n",
      "Epoch [7339/20000], Loss: -4.0373382568359375, Learning Rate: 0.01\n",
      "Epoch [7340/20000], Loss: -4.0420074462890625, Learning Rate: 0.01\n",
      "Epoch [7341/20000], Loss: -4.046478271484375, Learning Rate: 0.01\n",
      "Epoch [7342/20000], Loss: -4.0508270263671875, Learning Rate: 0.01\n",
      "Epoch [7343/20000], Loss: -4.0548553466796875, Learning Rate: 0.01\n",
      "Epoch [7344/20000], Loss: -4.0582427978515625, Learning Rate: 0.01\n",
      "Epoch [7345/20000], Loss: -4.060791015625, Learning Rate: 0.01\n",
      "Epoch [7346/20000], Loss: -4.0621337890625, Learning Rate: 0.01\n",
      "Epoch [7347/20000], Loss: -4.0616912841796875, Learning Rate: 0.01\n",
      "Epoch [7348/20000], Loss: -4.0583953857421875, Learning Rate: 0.01\n",
      "Epoch [7349/20000], Loss: -4.0508270263671875, Learning Rate: 0.01\n",
      "Epoch [7350/20000], Loss: -4.0365447998046875, Learning Rate: 0.01\n",
      "Epoch [7351/20000], Loss: -4.011932373046875, Learning Rate: 0.01\n",
      "Epoch [7352/20000], Loss: -3.9716033935546875, Learning Rate: 0.01\n",
      "Epoch [7353/20000], Loss: -3.906402587890625, Learning Rate: 0.01\n",
      "Epoch [7354/20000], Loss: -3.8030242919921875, Learning Rate: 0.01\n",
      "Epoch [7355/20000], Loss: -3.6405792236328125, Learning Rate: 0.01\n",
      "Epoch [7356/20000], Loss: -3.3870086669921875, Learning Rate: 0.01\n",
      "Epoch [7357/20000], Loss: -2.995391845703125, Learning Rate: 0.01\n",
      "Epoch [7358/20000], Loss: -2.3987884521484375, Learning Rate: 0.01\n",
      "Epoch [7359/20000], Loss: -1.5108489990234375, Learning Rate: 0.01\n",
      "Epoch [7360/20000], Loss: -0.2373199462890625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7361/20000], Loss: 1.478851318359375, Learning Rate: 0.01\n",
      "Epoch [7362/20000], Loss: 3.561981201171875, Learning Rate: 0.01\n",
      "Epoch [7363/20000], Loss: 5.64892578125, Learning Rate: 0.01\n",
      "Epoch [7364/20000], Loss: 6.99658203125, Learning Rate: 0.01\n",
      "Epoch [7365/20000], Loss: 6.6768341064453125, Learning Rate: 0.01\n",
      "Epoch [7366/20000], Loss: 4.280731201171875, Learning Rate: 0.01\n",
      "Epoch [7367/20000], Loss: 0.625701904296875, Learning Rate: 0.01\n",
      "Epoch [7368/20000], Loss: -2.4647369384765625, Learning Rate: 0.01\n",
      "Epoch [7369/20000], Loss: -3.4946746826171875, Learning Rate: 0.01\n",
      "Epoch [7370/20000], Loss: -2.3935089111328125, Learning Rate: 0.01\n",
      "Epoch [7371/20000], Loss: -0.4160308837890625, Learning Rate: 0.01\n",
      "Epoch [7372/20000], Loss: 0.8110504150390625, Learning Rate: 0.01\n",
      "Epoch [7373/20000], Loss: 0.3417205810546875, Learning Rate: 0.01\n",
      "Epoch [7374/20000], Loss: -1.4518585205078125, Learning Rate: 0.01\n",
      "Epoch [7375/20000], Loss: -3.2264862060546875, Learning Rate: 0.01\n",
      "Epoch [7376/20000], Loss: -3.81939697265625, Learning Rate: 0.01\n",
      "Epoch [7377/20000], Loss: -3.1775360107421875, Learning Rate: 0.01\n",
      "Epoch [7378/20000], Loss: -2.2167816162109375, Learning Rate: 0.01\n",
      "Epoch [7379/20000], Loss: -1.9326934814453125, Learning Rate: 0.01\n",
      "Epoch [7380/20000], Loss: -2.59478759765625, Learning Rate: 0.01\n",
      "Epoch [7381/20000], Loss: -3.63006591796875, Learning Rate: 0.01\n",
      "Epoch [7382/20000], Loss: -4.227508544921875, Learning Rate: 0.01\n",
      "Epoch [7383/20000], Loss: -4.0679473876953125, Learning Rate: 0.01\n",
      "Epoch [7384/20000], Loss: -3.5077972412109375, Learning Rate: 0.01\n",
      "Epoch [7385/20000], Loss: -3.1564788818359375, Learning Rate: 0.01\n",
      "Epoch [7386/20000], Loss: -3.3238372802734375, Learning Rate: 0.01\n",
      "Epoch [7387/20000], Loss: -3.8153076171875, Learning Rate: 0.01\n",
      "Epoch [7388/20000], Loss: -4.1878509521484375, Learning Rate: 0.01\n",
      "Epoch [7389/20000], Loss: -4.1781768798828125, Learning Rate: 0.01\n",
      "Epoch [7390/20000], Loss: -3.8962249755859375, Learning Rate: 0.01\n",
      "Epoch [7391/20000], Loss: -3.6617584228515625, Learning Rate: 0.01\n",
      "Epoch [7392/20000], Loss: -3.6945343017578125, Learning Rate: 0.01\n",
      "Epoch [7393/20000], Loss: -3.9458160400390625, Learning Rate: 0.01\n",
      "Epoch [7394/20000], Loss: -4.1899871826171875, Learning Rate: 0.01\n",
      "Epoch [7395/20000], Loss: -4.24993896484375, Learning Rate: 0.01\n",
      "Epoch [7396/20000], Loss: -4.1385040283203125, Learning Rate: 0.01\n",
      "Epoch [7397/20000], Loss: -4.010528564453125, Learning Rate: 0.01\n",
      "Epoch [7398/20000], Loss: -4.0040283203125, Learning Rate: 0.01\n",
      "Epoch [7399/20000], Loss: -4.126617431640625, Learning Rate: 0.01\n",
      "Epoch [7400/20000], Loss: -4.27423095703125, Learning Rate: 0.01\n",
      "Epoch [7401/20000], Loss: -4.33941650390625, Learning Rate: 0.01\n",
      "Epoch [7402/20000], Loss: -4.302734375, Learning Rate: 0.01\n",
      "Epoch [7403/20000], Loss: -4.2312469482421875, Learning Rate: 0.01\n",
      "Epoch [7404/20000], Loss: -4.20635986328125, Learning Rate: 0.01\n",
      "Epoch [7405/20000], Loss: -4.2535400390625, Learning Rate: 0.01\n",
      "Epoch [7406/20000], Loss: -4.332427978515625, Learning Rate: 0.01\n",
      "Epoch [7407/20000], Loss: -4.3832550048828125, Learning Rate: 0.01\n",
      "Epoch [7408/20000], Loss: -4.3797149658203125, Learning Rate: 0.01\n",
      "Epoch [7409/20000], Loss: -4.34368896484375, Learning Rate: 0.01\n",
      "Epoch [7410/20000], Loss: -4.3188629150390625, Learning Rate: 0.01\n",
      "Epoch [7411/20000], Loss: -4.3303680419921875, Learning Rate: 0.01\n",
      "Epoch [7412/20000], Loss: -4.3687896728515625, Learning Rate: 0.01\n",
      "Epoch [7413/20000], Loss: -4.4048919677734375, Learning Rate: 0.01\n",
      "Epoch [7414/20000], Loss: -4.4163055419921875, Learning Rate: 0.01\n",
      "Epoch [7415/20000], Loss: -4.4041595458984375, Learning Rate: 0.01\n",
      "Epoch [7416/20000], Loss: -4.3879547119140625, Learning Rate: 0.01\n",
      "Epoch [7417/20000], Loss: -4.38604736328125, Learning Rate: 0.01\n",
      "Epoch [7418/20000], Loss: -4.4019775390625, Learning Rate: 0.01\n",
      "Epoch [7419/20000], Loss: -4.42431640625, Learning Rate: 0.01\n",
      "Epoch [7420/20000], Loss: -4.4389190673828125, Learning Rate: 0.01\n",
      "Epoch [7421/20000], Loss: -4.4398651123046875, Learning Rate: 0.01\n",
      "Epoch [7422/20000], Loss: -4.4326171875, Learning Rate: 0.01\n",
      "Epoch [7423/20000], Loss: -4.4273223876953125, Learning Rate: 0.01\n",
      "Epoch [7424/20000], Loss: -4.43035888671875, Learning Rate: 0.01\n",
      "Epoch [7425/20000], Loss: -4.4402618408203125, Learning Rate: 0.01\n",
      "Epoch [7426/20000], Loss: -4.449737548828125, Learning Rate: 0.01\n",
      "Epoch [7427/20000], Loss: -4.4528045654296875, Learning Rate: 0.01\n",
      "Epoch [7428/20000], Loss: -4.4484100341796875, Learning Rate: 0.01\n",
      "Epoch [7429/20000], Loss: -4.4400482177734375, Learning Rate: 0.01\n",
      "Epoch [7430/20000], Loss: -4.4317779541015625, Learning Rate: 0.01\n",
      "Epoch [7431/20000], Loss: -4.4250946044921875, Learning Rate: 0.01\n",
      "Epoch [7432/20000], Loss: -4.4173736572265625, Learning Rate: 0.01\n",
      "Epoch [7433/20000], Loss: -4.4040374755859375, Learning Rate: 0.01\n",
      "Epoch [7434/20000], Loss: -4.3809814453125, Learning Rate: 0.01\n",
      "Epoch [7435/20000], Loss: -4.346038818359375, Learning Rate: 0.01\n",
      "Epoch [7436/20000], Loss: -4.298309326171875, Learning Rate: 0.01\n",
      "Epoch [7437/20000], Loss: -4.2357635498046875, Learning Rate: 0.01\n",
      "Epoch [7438/20000], Loss: -4.1538848876953125, Learning Rate: 0.01\n",
      "Epoch [7439/20000], Loss: -4.04449462890625, Learning Rate: 0.01\n",
      "Epoch [7440/20000], Loss: -3.8972015380859375, Learning Rate: 0.01\n",
      "Epoch [7441/20000], Loss: -3.69891357421875, Learning Rate: 0.01\n",
      "Epoch [7442/20000], Loss: -3.4362945556640625, Learning Rate: 0.01\n",
      "Epoch [7443/20000], Loss: -3.0941009521484375, Learning Rate: 0.01\n",
      "Epoch [7444/20000], Loss: -2.65875244140625, Learning Rate: 0.01\n",
      "Epoch [7445/20000], Loss: -2.11865234375, Learning Rate: 0.01\n",
      "Epoch [7446/20000], Loss: -1.4768524169921875, Learning Rate: 0.01\n",
      "Epoch [7447/20000], Loss: -0.7547149658203125, Learning Rate: 0.01\n",
      "Epoch [7448/20000], Loss: -0.0194244384765625, Learning Rate: 0.01\n",
      "Epoch [7449/20000], Loss: 0.62139892578125, Learning Rate: 0.01\n",
      "Epoch [7450/20000], Loss: 1.0037384033203125, Learning Rate: 0.01\n",
      "Epoch [7451/20000], Loss: 0.9687957763671875, Learning Rate: 0.01\n",
      "Epoch [7452/20000], Loss: 0.4035186767578125, Learning Rate: 0.01\n",
      "Epoch [7453/20000], Loss: -0.64837646484375, Learning Rate: 0.01\n",
      "Epoch [7454/20000], Loss: -1.983306884765625, Learning Rate: 0.01\n",
      "Epoch [7455/20000], Loss: -3.2714996337890625, Learning Rate: 0.01\n",
      "Epoch [7456/20000], Loss: -4.2039947509765625, Learning Rate: 0.01\n",
      "Epoch [7457/20000], Loss: -4.6103515625, Learning Rate: 0.01\n",
      "Epoch [7458/20000], Loss: -4.5112152099609375, Learning Rate: 0.01\n",
      "Epoch [7459/20000], Loss: -4.0789794921875, Learning Rate: 0.01\n",
      "Epoch [7460/20000], Loss: -3.55328369140625, Learning Rate: 0.01\n",
      "Epoch [7461/20000], Loss: -3.157470703125, Learning Rate: 0.01\n",
      "Epoch [7462/20000], Loss: -3.032867431640625, Learning Rate: 0.01\n",
      "Epoch [7463/20000], Loss: -3.21002197265625, Learning Rate: 0.01\n",
      "Epoch [7464/20000], Loss: -3.6067657470703125, Learning Rate: 0.01\n",
      "Epoch [7465/20000], Loss: -4.0744781494140625, Learning Rate: 0.01\n",
      "Epoch [7466/20000], Loss: -4.458587646484375, Learning Rate: 0.01\n",
      "Epoch [7467/20000], Loss: -4.659637451171875, Learning Rate: 0.01\n",
      "Epoch [7468/20000], Loss: -4.660400390625, Learning Rate: 0.01\n",
      "Epoch [7469/20000], Loss: -4.5172882080078125, Learning Rate: 0.01\n",
      "Epoch [7470/20000], Loss: -4.324432373046875, Learning Rate: 0.01\n",
      "Epoch [7471/20000], Loss: -4.1739044189453125, Learning Rate: 0.01\n",
      "Epoch [7472/20000], Loss: -4.124053955078125, Learning Rate: 0.01\n",
      "Epoch [7473/20000], Loss: -4.18609619140625, Learning Rate: 0.01\n",
      "Epoch [7474/20000], Loss: -4.3292694091796875, Learning Rate: 0.01\n",
      "Epoch [7475/20000], Loss: -4.4987030029296875, Learning Rate: 0.01\n",
      "Epoch [7476/20000], Loss: -4.6400604248046875, Learning Rate: 0.01\n",
      "Epoch [7477/20000], Loss: -4.7187347412109375, Learning Rate: 0.01\n",
      "Epoch [7478/20000], Loss: -4.727813720703125, Learning Rate: 0.01\n",
      "Epoch [7479/20000], Loss: -4.6848907470703125, Learning Rate: 0.01\n",
      "Epoch [7480/20000], Loss: -4.62109375, Learning Rate: 0.01\n",
      "Epoch [7481/20000], Loss: -4.567474365234375, Learning Rate: 0.01\n",
      "Epoch [7482/20000], Loss: -4.545074462890625, Learning Rate: 0.01\n",
      "Epoch [7483/20000], Loss: -4.5604400634765625, Learning Rate: 0.01\n",
      "Epoch [7484/20000], Loss: -4.60595703125, Learning Rate: 0.01\n",
      "Epoch [7485/20000], Loss: -4.66607666015625, Learning Rate: 0.01\n",
      "Epoch [7486/20000], Loss: -4.7230682373046875, Learning Rate: 0.01\n",
      "Epoch [7487/20000], Loss: -4.763641357421875, Learning Rate: 0.01\n",
      "Epoch [7488/20000], Loss: -4.781951904296875, Learning Rate: 0.01\n",
      "Epoch [7489/20000], Loss: -4.780059814453125, Learning Rate: 0.01\n",
      "Epoch [7490/20000], Loss: -4.76556396484375, Learning Rate: 0.01\n",
      "Epoch [7491/20000], Loss: -4.747955322265625, Learning Rate: 0.01\n",
      "Epoch [7492/20000], Loss: -4.7356719970703125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7493/20000], Loss: -4.7335968017578125, Learning Rate: 0.01\n",
      "Epoch [7494/20000], Loss: -4.742950439453125, Learning Rate: 0.01\n",
      "Epoch [7495/20000], Loss: -4.7609100341796875, Learning Rate: 0.01\n",
      "Epoch [7496/20000], Loss: -4.78314208984375, Learning Rate: 0.01\n",
      "Epoch [7497/20000], Loss: -4.80487060546875, Learning Rate: 0.01\n",
      "Epoch [7498/20000], Loss: -4.8221893310546875, Learning Rate: 0.01\n",
      "Epoch [7499/20000], Loss: -4.8335418701171875, Learning Rate: 0.01\n",
      "Epoch [7500/20000], Loss: -4.838836669921875, Learning Rate: 0.01\n",
      "Epoch [7501/20000], Loss: -4.8395233154296875, Learning Rate: 0.01\n",
      "Epoch [7502/20000], Loss: -4.838043212890625, Learning Rate: 0.01\n",
      "Epoch [7503/20000], Loss: -4.8367462158203125, Learning Rate: 0.01\n",
      "Epoch [7504/20000], Loss: -4.837005615234375, Learning Rate: 0.01\n",
      "Epoch [7505/20000], Loss: -4.8401947021484375, Learning Rate: 0.01\n",
      "Epoch [7506/20000], Loss: -4.84613037109375, Learning Rate: 0.01\n",
      "Epoch [7507/20000], Loss: -4.8543853759765625, Learning Rate: 0.01\n",
      "Epoch [7508/20000], Loss: -4.8637542724609375, Learning Rate: 0.01\n",
      "Epoch [7509/20000], Loss: -4.873504638671875, Learning Rate: 0.01\n",
      "Epoch [7510/20000], Loss: -4.8824310302734375, Learning Rate: 0.01\n",
      "Epoch [7511/20000], Loss: -4.890106201171875, Learning Rate: 0.01\n",
      "Epoch [7512/20000], Loss: -4.8962860107421875, Learning Rate: 0.01\n",
      "Epoch [7513/20000], Loss: -4.901092529296875, Learning Rate: 0.01\n",
      "Epoch [7514/20000], Loss: -4.9050445556640625, Learning Rate: 0.01\n",
      "Epoch [7515/20000], Loss: -4.908355712890625, Learning Rate: 0.01\n",
      "Epoch [7516/20000], Loss: -4.91156005859375, Learning Rate: 0.01\n",
      "Epoch [7517/20000], Loss: -4.9149932861328125, Learning Rate: 0.01\n",
      "Epoch [7518/20000], Loss: -4.9189910888671875, Learning Rate: 0.01\n",
      "Epoch [7519/20000], Loss: -4.9234771728515625, Learning Rate: 0.01\n",
      "Epoch [7520/20000], Loss: -4.928619384765625, Learning Rate: 0.01\n",
      "Epoch [7521/20000], Loss: -4.9340057373046875, Learning Rate: 0.01\n",
      "Epoch [7522/20000], Loss: -4.939727783203125, Learning Rate: 0.01\n",
      "Epoch [7523/20000], Loss: -4.94561767578125, Learning Rate: 0.01\n",
      "Epoch [7524/20000], Loss: -4.9514617919921875, Learning Rate: 0.01\n",
      "Epoch [7525/20000], Loss: -4.9571380615234375, Learning Rate: 0.01\n",
      "Epoch [7526/20000], Loss: -4.9626312255859375, Learning Rate: 0.01\n",
      "Epoch [7527/20000], Loss: -4.967742919921875, Learning Rate: 0.01\n",
      "Epoch [7528/20000], Loss: -4.9727020263671875, Learning Rate: 0.01\n",
      "Epoch [7529/20000], Loss: -4.977630615234375, Learning Rate: 0.01\n",
      "Epoch [7530/20000], Loss: -4.982177734375, Learning Rate: 0.01\n",
      "Epoch [7531/20000], Loss: -4.9868011474609375, Learning Rate: 0.01\n",
      "Epoch [7532/20000], Loss: -4.991302490234375, Learning Rate: 0.01\n",
      "Epoch [7533/20000], Loss: -4.99578857421875, Learning Rate: 0.01\n",
      "Epoch [7534/20000], Loss: -5.000396728515625, Learning Rate: 0.01\n",
      "Epoch [7535/20000], Loss: -5.005096435546875, Learning Rate: 0.01\n",
      "Epoch [7536/20000], Loss: -5.009735107421875, Learning Rate: 0.01\n",
      "Epoch [7537/20000], Loss: -5.014434814453125, Learning Rate: 0.01\n",
      "Epoch [7538/20000], Loss: -5.0191497802734375, Learning Rate: 0.01\n",
      "Epoch [7539/20000], Loss: -5.02398681640625, Learning Rate: 0.01\n",
      "Epoch [7540/20000], Loss: -5.02880859375, Learning Rate: 0.01\n",
      "Epoch [7541/20000], Loss: -5.0338134765625, Learning Rate: 0.01\n",
      "Epoch [7542/20000], Loss: -5.0386505126953125, Learning Rate: 0.01\n",
      "Epoch [7543/20000], Loss: -5.04351806640625, Learning Rate: 0.01\n",
      "Epoch [7544/20000], Loss: -5.0484771728515625, Learning Rate: 0.01\n",
      "Epoch [7545/20000], Loss: -5.053375244140625, Learning Rate: 0.01\n",
      "Epoch [7546/20000], Loss: -5.0581512451171875, Learning Rate: 0.01\n",
      "Epoch [7547/20000], Loss: -5.0630645751953125, Learning Rate: 0.01\n",
      "Epoch [7548/20000], Loss: -5.0679473876953125, Learning Rate: 0.01\n",
      "Epoch [7549/20000], Loss: -5.0728607177734375, Learning Rate: 0.01\n",
      "Epoch [7550/20000], Loss: -5.0777587890625, Learning Rate: 0.01\n",
      "Epoch [7551/20000], Loss: -5.08245849609375, Learning Rate: 0.01\n",
      "Epoch [7552/20000], Loss: -5.087371826171875, Learning Rate: 0.01\n",
      "Epoch [7553/20000], Loss: -5.092254638671875, Learning Rate: 0.01\n",
      "Epoch [7554/20000], Loss: -5.0969696044921875, Learning Rate: 0.01\n",
      "Epoch [7555/20000], Loss: -5.10186767578125, Learning Rate: 0.01\n",
      "Epoch [7556/20000], Loss: -5.106658935546875, Learning Rate: 0.01\n",
      "Epoch [7557/20000], Loss: -5.1114959716796875, Learning Rate: 0.01\n",
      "Epoch [7558/20000], Loss: -5.11627197265625, Learning Rate: 0.01\n",
      "Epoch [7559/20000], Loss: -5.1210174560546875, Learning Rate: 0.01\n",
      "Epoch [7560/20000], Loss: -5.1258697509765625, Learning Rate: 0.01\n",
      "Epoch [7561/20000], Loss: -5.130645751953125, Learning Rate: 0.01\n",
      "Epoch [7562/20000], Loss: -5.135467529296875, Learning Rate: 0.01\n",
      "Epoch [7563/20000], Loss: -5.1402435302734375, Learning Rate: 0.01\n",
      "Epoch [7564/20000], Loss: -5.1449432373046875, Learning Rate: 0.01\n",
      "Epoch [7565/20000], Loss: -5.1497039794921875, Learning Rate: 0.01\n",
      "Epoch [7566/20000], Loss: -5.1544952392578125, Learning Rate: 0.01\n",
      "Epoch [7567/20000], Loss: -5.1592254638671875, Learning Rate: 0.01\n",
      "Epoch [7568/20000], Loss: -5.1639556884765625, Learning Rate: 0.01\n",
      "Epoch [7569/20000], Loss: -5.16864013671875, Learning Rate: 0.01\n",
      "Epoch [7570/20000], Loss: -5.1733551025390625, Learning Rate: 0.01\n",
      "Epoch [7571/20000], Loss: -5.177978515625, Learning Rate: 0.01\n",
      "Epoch [7572/20000], Loss: -5.1824188232421875, Learning Rate: 0.01\n",
      "Epoch [7573/20000], Loss: -5.18695068359375, Learning Rate: 0.01\n",
      "Epoch [7574/20000], Loss: -5.1912994384765625, Learning Rate: 0.01\n",
      "Epoch [7575/20000], Loss: -5.1954803466796875, Learning Rate: 0.01\n",
      "Epoch [7576/20000], Loss: -5.199371337890625, Learning Rate: 0.01\n",
      "Epoch [7577/20000], Loss: -5.203033447265625, Learning Rate: 0.01\n",
      "Epoch [7578/20000], Loss: -5.2062530517578125, Learning Rate: 0.01\n",
      "Epoch [7579/20000], Loss: -5.2089385986328125, Learning Rate: 0.01\n",
      "Epoch [7580/20000], Loss: -5.210784912109375, Learning Rate: 0.01\n",
      "Epoch [7581/20000], Loss: -5.21136474609375, Learning Rate: 0.01\n",
      "Epoch [7582/20000], Loss: -5.2102813720703125, Learning Rate: 0.01\n",
      "Epoch [7583/20000], Loss: -5.2068328857421875, Learning Rate: 0.01\n",
      "Epoch [7584/20000], Loss: -5.1998138427734375, Learning Rate: 0.01\n",
      "Epoch [7585/20000], Loss: -5.18798828125, Learning Rate: 0.01\n",
      "Epoch [7586/20000], Loss: -5.16888427734375, Learning Rate: 0.01\n",
      "Epoch [7587/20000], Loss: -5.13922119140625, Learning Rate: 0.01\n",
      "Epoch [7588/20000], Loss: -5.09423828125, Learning Rate: 0.01\n",
      "Epoch [7589/20000], Loss: -5.0268402099609375, Learning Rate: 0.01\n",
      "Epoch [7590/20000], Loss: -4.92669677734375, Learning Rate: 0.01\n",
      "Epoch [7591/20000], Loss: -4.77886962890625, Learning Rate: 0.01\n",
      "Epoch [7592/20000], Loss: -4.5614166259765625, Learning Rate: 0.01\n",
      "Epoch [7593/20000], Loss: -4.2442169189453125, Learning Rate: 0.01\n",
      "Epoch [7594/20000], Loss: -3.785491943359375, Learning Rate: 0.01\n",
      "Epoch [7595/20000], Loss: -3.1328277587890625, Learning Rate: 0.01\n",
      "Epoch [7596/20000], Loss: -2.2253875732421875, Learning Rate: 0.01\n",
      "Epoch [7597/20000], Loss: -1.014190673828125, Learning Rate: 0.01\n",
      "Epoch [7598/20000], Loss: 0.511199951171875, Learning Rate: 0.01\n",
      "Epoch [7599/20000], Loss: 2.2415924072265625, Learning Rate: 0.01\n",
      "Epoch [7600/20000], Loss: 3.89593505859375, Learning Rate: 0.01\n",
      "Epoch [7601/20000], Loss: 4.9441070556640625, Learning Rate: 0.01\n",
      "Epoch [7602/20000], Loss: 4.7924652099609375, Learning Rate: 0.01\n",
      "Epoch [7603/20000], Loss: 3.0789337158203125, Learning Rate: 0.01\n",
      "Epoch [7604/20000], Loss: 0.1666107177734375, Learning Rate: 0.01\n",
      "Epoch [7605/20000], Loss: -2.8929443359375, Learning Rate: 0.01\n",
      "Epoch [7606/20000], Loss: -4.8904266357421875, Learning Rate: 0.01\n",
      "Epoch [7607/20000], Loss: -5.23455810546875, Learning Rate: 0.01\n",
      "Epoch [7608/20000], Loss: -4.2276153564453125, Learning Rate: 0.01\n",
      "Epoch [7609/20000], Loss: -2.7730712890625, Learning Rate: 0.01\n",
      "Epoch [7610/20000], Loss: -1.8371429443359375, Learning Rate: 0.01\n",
      "Epoch [7611/20000], Loss: -1.960693359375, Learning Rate: 0.01\n",
      "Epoch [7612/20000], Loss: -3.023284912109375, Learning Rate: 0.01\n",
      "Epoch [7613/20000], Loss: -4.355621337890625, Learning Rate: 0.01\n",
      "Epoch [7614/20000], Loss: -5.2157135009765625, Learning Rate: 0.01\n",
      "Epoch [7615/20000], Loss: -5.26068115234375, Learning Rate: 0.01\n",
      "Epoch [7616/20000], Loss: -4.697113037109375, Learning Rate: 0.01\n",
      "Epoch [7617/20000], Loss: -4.0532989501953125, Learning Rate: 0.01\n",
      "Epoch [7618/20000], Loss: -3.80389404296875, Learning Rate: 0.01\n",
      "Epoch [7619/20000], Loss: -4.0949249267578125, Learning Rate: 0.01\n",
      "Epoch [7620/20000], Loss: -4.7034454345703125, Learning Rate: 0.01\n",
      "Epoch [7621/20000], Loss: -5.238525390625, Learning Rate: 0.01\n",
      "Epoch [7622/20000], Loss: -5.419586181640625, Learning Rate: 0.01\n",
      "Epoch [7623/20000], Loss: -5.23748779296875, Learning Rate: 0.01\n",
      "Epoch [7624/20000], Loss: -4.90966796875, Learning Rate: 0.01\n",
      "Epoch [7625/20000], Loss: -4.701080322265625, Learning Rate: 0.01\n",
      "Epoch [7626/20000], Loss: -4.7506561279296875, Learning Rate: 0.01\n",
      "Epoch [7627/20000], Loss: -5.007720947265625, Learning Rate: 0.01\n",
      "Epoch [7628/20000], Loss: -5.2983551025390625, Learning Rate: 0.01\n",
      "Epoch [7629/20000], Loss: -5.45758056640625, Learning Rate: 0.01\n",
      "Epoch [7630/20000], Loss: -5.4322662353515625, Learning Rate: 0.01\n",
      "Epoch [7631/20000], Loss: -5.2923431396484375, Learning Rate: 0.01\n",
      "Epoch [7632/20000], Loss: -5.16302490234375, Learning Rate: 0.01\n",
      "Epoch [7633/20000], Loss: -5.137603759765625, Learning Rate: 0.01\n",
      "Epoch [7634/20000], Loss: -5.2254638671875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7635/20000], Loss: -5.3641510009765625, Learning Rate: 0.01\n",
      "Epoch [7636/20000], Loss: -5.470947265625, Learning Rate: 0.01\n",
      "Epoch [7637/20000], Loss: -5.498260498046875, Learning Rate: 0.01\n",
      "Epoch [7638/20000], Loss: -5.455413818359375, Learning Rate: 0.01\n",
      "Epoch [7639/20000], Loss: -5.3914947509765625, Learning Rate: 0.01\n",
      "Epoch [7640/20000], Loss: -5.3581695556640625, Learning Rate: 0.01\n",
      "Epoch [7641/20000], Loss: -5.3777313232421875, Learning Rate: 0.01\n",
      "Epoch [7642/20000], Loss: -5.4363555908203125, Learning Rate: 0.01\n",
      "Epoch [7643/20000], Loss: -5.499298095703125, Learning Rate: 0.01\n",
      "Epoch [7644/20000], Loss: -5.53533935546875, Learning Rate: 0.01\n",
      "Epoch [7645/20000], Loss: -5.5347747802734375, Learning Rate: 0.01\n",
      "Epoch [7646/20000], Loss: -5.5104522705078125, Learning Rate: 0.01\n",
      "Epoch [7647/20000], Loss: -5.486419677734375, Learning Rate: 0.01\n",
      "Epoch [7648/20000], Loss: -5.481170654296875, Learning Rate: 0.01\n",
      "Epoch [7649/20000], Loss: -5.4992218017578125, Learning Rate: 0.01\n",
      "Epoch [7650/20000], Loss: -5.530609130859375, Learning Rate: 0.01\n",
      "Epoch [7651/20000], Loss: -5.5599212646484375, Learning Rate: 0.01\n",
      "Epoch [7652/20000], Loss: -5.575775146484375, Learning Rate: 0.01\n",
      "Epoch [7653/20000], Loss: -5.5761871337890625, Learning Rate: 0.01\n",
      "Epoch [7654/20000], Loss: -5.5677337646484375, Learning Rate: 0.01\n",
      "Epoch [7655/20000], Loss: -5.560577392578125, Learning Rate: 0.01\n",
      "Epoch [7656/20000], Loss: -5.5617523193359375, Learning Rate: 0.01\n",
      "Epoch [7657/20000], Loss: -5.5723724365234375, Learning Rate: 0.01\n",
      "Epoch [7658/20000], Loss: -5.58837890625, Learning Rate: 0.01\n",
      "Epoch [7659/20000], Loss: -5.6034698486328125, Learning Rate: 0.01\n",
      "Epoch [7660/20000], Loss: -5.61322021484375, Learning Rate: 0.01\n",
      "Epoch [7661/20000], Loss: -5.6165008544921875, Learning Rate: 0.01\n",
      "Epoch [7662/20000], Loss: -5.6158599853515625, Learning Rate: 0.01\n",
      "Epoch [7663/20000], Loss: -5.61529541015625, Learning Rate: 0.01\n",
      "Epoch [7664/20000], Loss: -5.6178131103515625, Learning Rate: 0.01\n",
      "Epoch [7665/20000], Loss: -5.6243133544921875, Learning Rate: 0.01\n",
      "Epoch [7666/20000], Loss: -5.63360595703125, Learning Rate: 0.01\n",
      "Epoch [7667/20000], Loss: -5.64312744140625, Learning Rate: 0.01\n",
      "Epoch [7668/20000], Loss: -5.6509246826171875, Learning Rate: 0.01\n",
      "Epoch [7669/20000], Loss: -5.6561737060546875, Learning Rate: 0.01\n",
      "Epoch [7670/20000], Loss: -5.6591796875, Learning Rate: 0.01\n",
      "Epoch [7671/20000], Loss: -5.6614532470703125, Learning Rate: 0.01\n",
      "Epoch [7672/20000], Loss: -5.6645050048828125, Learning Rate: 0.01\n",
      "Epoch [7673/20000], Loss: -5.6690521240234375, Learning Rate: 0.01\n",
      "Epoch [7674/20000], Loss: -5.6749420166015625, Learning Rate: 0.01\n",
      "Epoch [7675/20000], Loss: -5.6815338134765625, Learning Rate: 0.01\n",
      "Epoch [7676/20000], Loss: -5.688018798828125, Learning Rate: 0.01\n",
      "Epoch [7677/20000], Loss: -5.6937103271484375, Learning Rate: 0.01\n",
      "Epoch [7678/20000], Loss: -5.6983795166015625, Learning Rate: 0.01\n",
      "Epoch [7679/20000], Loss: -5.702301025390625, Learning Rate: 0.01\n",
      "Epoch [7680/20000], Loss: -5.7061920166015625, Learning Rate: 0.01\n",
      "Epoch [7681/20000], Loss: -5.710174560546875, Learning Rate: 0.01\n",
      "Epoch [7682/20000], Loss: -5.714996337890625, Learning Rate: 0.01\n",
      "Epoch [7683/20000], Loss: -5.72015380859375, Learning Rate: 0.01\n",
      "Epoch [7684/20000], Loss: -5.7255706787109375, Learning Rate: 0.01\n",
      "Epoch [7685/20000], Loss: -5.730987548828125, Learning Rate: 0.01\n",
      "Epoch [7686/20000], Loss: -5.7361907958984375, Learning Rate: 0.01\n",
      "Epoch [7687/20000], Loss: -5.7409820556640625, Learning Rate: 0.01\n",
      "Epoch [7688/20000], Loss: -5.7454681396484375, Learning Rate: 0.01\n",
      "Epoch [7689/20000], Loss: -5.7499542236328125, Learning Rate: 0.01\n",
      "Epoch [7690/20000], Loss: -5.7542724609375, Learning Rate: 0.01\n",
      "Epoch [7691/20000], Loss: -5.7588958740234375, Learning Rate: 0.01\n",
      "Epoch [7692/20000], Loss: -5.763763427734375, Learning Rate: 0.01\n",
      "Epoch [7693/20000], Loss: -5.7687225341796875, Learning Rate: 0.01\n",
      "Epoch [7694/20000], Loss: -5.773712158203125, Learning Rate: 0.01\n",
      "Epoch [7695/20000], Loss: -5.7785797119140625, Learning Rate: 0.01\n",
      "Epoch [7696/20000], Loss: -5.7835235595703125, Learning Rate: 0.01\n",
      "Epoch [7697/20000], Loss: -5.7882537841796875, Learning Rate: 0.01\n",
      "Epoch [7698/20000], Loss: -5.7928466796875, Learning Rate: 0.01\n",
      "Epoch [7699/20000], Loss: -5.7974090576171875, Learning Rate: 0.01\n",
      "Epoch [7700/20000], Loss: -5.8021392822265625, Learning Rate: 0.01\n",
      "Epoch [7701/20000], Loss: -5.8068084716796875, Learning Rate: 0.01\n",
      "Epoch [7702/20000], Loss: -5.8115386962890625, Learning Rate: 0.01\n",
      "Epoch [7703/20000], Loss: -5.816375732421875, Learning Rate: 0.01\n",
      "Epoch [7704/20000], Loss: -5.8212127685546875, Learning Rate: 0.01\n",
      "Epoch [7705/20000], Loss: -5.8260040283203125, Learning Rate: 0.01\n",
      "Epoch [7706/20000], Loss: -5.8306732177734375, Learning Rate: 0.01\n",
      "Epoch [7707/20000], Loss: -5.8355712890625, Learning Rate: 0.01\n",
      "Epoch [7708/20000], Loss: -5.84027099609375, Learning Rate: 0.01\n",
      "Epoch [7709/20000], Loss: -5.844970703125, Learning Rate: 0.01\n",
      "Epoch [7710/20000], Loss: -5.84967041015625, Learning Rate: 0.01\n",
      "Epoch [7711/20000], Loss: -5.8543548583984375, Learning Rate: 0.01\n",
      "Epoch [7712/20000], Loss: -5.85906982421875, Learning Rate: 0.01\n",
      "Epoch [7713/20000], Loss: -5.8638763427734375, Learning Rate: 0.01\n",
      "Epoch [7714/20000], Loss: -5.8685760498046875, Learning Rate: 0.01\n",
      "Epoch [7715/20000], Loss: -5.8733978271484375, Learning Rate: 0.01\n",
      "Epoch [7716/20000], Loss: -5.8781585693359375, Learning Rate: 0.01\n",
      "Epoch [7717/20000], Loss: -5.8828582763671875, Learning Rate: 0.01\n",
      "Epoch [7718/20000], Loss: -5.8876800537109375, Learning Rate: 0.01\n",
      "Epoch [7719/20000], Loss: -5.892425537109375, Learning Rate: 0.01\n",
      "Epoch [7720/20000], Loss: -5.8971710205078125, Learning Rate: 0.01\n",
      "Epoch [7721/20000], Loss: -5.9018707275390625, Learning Rate: 0.01\n",
      "Epoch [7722/20000], Loss: -5.906585693359375, Learning Rate: 0.01\n",
      "Epoch [7723/20000], Loss: -5.9113311767578125, Learning Rate: 0.01\n",
      "Epoch [7724/20000], Loss: -5.9161529541015625, Learning Rate: 0.01\n",
      "Epoch [7725/20000], Loss: -5.9208221435546875, Learning Rate: 0.01\n",
      "Epoch [7726/20000], Loss: -5.925567626953125, Learning Rate: 0.01\n",
      "Epoch [7727/20000], Loss: -5.930206298828125, Learning Rate: 0.01\n",
      "Epoch [7728/20000], Loss: -5.9350738525390625, Learning Rate: 0.01\n",
      "Epoch [7729/20000], Loss: -5.939788818359375, Learning Rate: 0.01\n",
      "Epoch [7730/20000], Loss: -5.9444427490234375, Learning Rate: 0.01\n",
      "Epoch [7731/20000], Loss: -5.9491424560546875, Learning Rate: 0.01\n",
      "Epoch [7732/20000], Loss: -5.95379638671875, Learning Rate: 0.01\n",
      "Epoch [7733/20000], Loss: -5.958343505859375, Learning Rate: 0.01\n",
      "Epoch [7734/20000], Loss: -5.962982177734375, Learning Rate: 0.01\n",
      "Epoch [7735/20000], Loss: -5.967437744140625, Learning Rate: 0.01\n",
      "Epoch [7736/20000], Loss: -5.9717864990234375, Learning Rate: 0.01\n",
      "Epoch [7737/20000], Loss: -5.9759521484375, Learning Rate: 0.01\n",
      "Epoch [7738/20000], Loss: -5.9798431396484375, Learning Rate: 0.01\n",
      "Epoch [7739/20000], Loss: -5.9831085205078125, Learning Rate: 0.01\n",
      "Epoch [7740/20000], Loss: -5.985809326171875, Learning Rate: 0.01\n",
      "Epoch [7741/20000], Loss: -5.9875335693359375, Learning Rate: 0.01\n",
      "Epoch [7742/20000], Loss: -5.987579345703125, Learning Rate: 0.01\n",
      "Epoch [7743/20000], Loss: -5.98516845703125, Learning Rate: 0.01\n",
      "Epoch [7744/20000], Loss: -5.9788360595703125, Learning Rate: 0.01\n",
      "Epoch [7745/20000], Loss: -5.9666748046875, Learning Rate: 0.01\n",
      "Epoch [7746/20000], Loss: -5.945220947265625, Learning Rate: 0.01\n",
      "Epoch [7747/20000], Loss: -5.9093170166015625, Learning Rate: 0.01\n",
      "Epoch [7748/20000], Loss: -5.8508453369140625, Learning Rate: 0.01\n",
      "Epoch [7749/20000], Loss: -5.7572174072265625, Learning Rate: 0.01\n",
      "Epoch [7750/20000], Loss: -5.60858154296875, Learning Rate: 0.01\n",
      "Epoch [7751/20000], Loss: -5.3746337890625, Learning Rate: 0.01\n",
      "Epoch [7752/20000], Loss: -5.01043701171875, Learning Rate: 0.01\n",
      "Epoch [7753/20000], Loss: -4.451507568359375, Learning Rate: 0.01\n",
      "Epoch [7754/20000], Loss: -3.6138916015625, Learning Rate: 0.01\n",
      "Epoch [7755/20000], Loss: -2.405609130859375, Learning Rate: 0.01\n",
      "Epoch [7756/20000], Loss: -0.77178955078125, Learning Rate: 0.01\n",
      "Epoch [7757/20000], Loss: 1.20855712890625, Learning Rate: 0.01\n",
      "Epoch [7758/20000], Loss: 3.1673583984375, Learning Rate: 0.01\n",
      "Epoch [7759/20000], Loss: 4.3570098876953125, Learning Rate: 0.01\n",
      "Epoch [7760/20000], Loss: 3.8715057373046875, Learning Rate: 0.01\n",
      "Epoch [7761/20000], Loss: 1.3711700439453125, Learning Rate: 0.01\n",
      "Epoch [7762/20000], Loss: -2.2305450439453125, Learning Rate: 0.01\n",
      "Epoch [7763/20000], Loss: -5.1085662841796875, Learning Rate: 0.01\n",
      "Epoch [7764/20000], Loss: -5.908050537109375, Learning Rate: 0.01\n",
      "Epoch [7765/20000], Loss: -4.7459564208984375, Learning Rate: 0.01\n",
      "Epoch [7766/20000], Loss: -2.9497833251953125, Learning Rate: 0.01\n",
      "Epoch [7767/20000], Loss: -2.0408782958984375, Learning Rate: 0.01\n",
      "Epoch [7768/20000], Loss: -2.7183837890625, Learning Rate: 0.01\n",
      "Epoch [7769/20000], Loss: -4.4205780029296875, Learning Rate: 0.01\n",
      "Epoch [7770/20000], Loss: -5.8365020751953125, Learning Rate: 0.01\n",
      "Epoch [7771/20000], Loss: -6.0518646240234375, Learning Rate: 0.01\n",
      "Epoch [7772/20000], Loss: -5.2378997802734375, Learning Rate: 0.01\n",
      "Epoch [7773/20000], Loss: -4.3316497802734375, Learning Rate: 0.01\n",
      "Epoch [7774/20000], Loss: -4.1721038818359375, Learning Rate: 0.01\n",
      "Epoch [7775/20000], Loss: -4.84869384765625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7776/20000], Loss: -5.740447998046875, Learning Rate: 0.01\n",
      "Epoch [7777/20000], Loss: -6.14398193359375, Learning Rate: 0.01\n",
      "Epoch [7778/20000], Loss: -5.888946533203125, Learning Rate: 0.01\n",
      "Epoch [7779/20000], Loss: -5.383880615234375, Learning Rate: 0.01\n",
      "Epoch [7780/20000], Loss: -5.164154052734375, Learning Rate: 0.01\n",
      "Epoch [7781/20000], Loss: -5.4198760986328125, Learning Rate: 0.01\n",
      "Epoch [7782/20000], Loss: -5.894744873046875, Learning Rate: 0.01\n",
      "Epoch [7783/20000], Loss: -6.1845855712890625, Learning Rate: 0.01\n",
      "Epoch [7784/20000], Loss: -6.11383056640625, Learning Rate: 0.01\n",
      "Epoch [7785/20000], Loss: -5.8446044921875, Learning Rate: 0.01\n",
      "Epoch [7786/20000], Loss: -5.6743621826171875, Learning Rate: 0.01\n",
      "Epoch [7787/20000], Loss: -5.758026123046875, Learning Rate: 0.01\n",
      "Epoch [7788/20000], Loss: -6.003387451171875, Learning Rate: 0.01\n",
      "Epoch [7789/20000], Loss: -6.1953887939453125, Learning Rate: 0.01\n",
      "Epoch [7790/20000], Loss: -6.204132080078125, Learning Rate: 0.01\n",
      "Epoch [7791/20000], Loss: -6.0796051025390625, Learning Rate: 0.01\n",
      "Epoch [7792/20000], Loss: -5.9733123779296875, Learning Rate: 0.01\n",
      "Epoch [7793/20000], Loss: -5.99102783203125, Learning Rate: 0.01\n",
      "Epoch [7794/20000], Loss: -6.111541748046875, Learning Rate: 0.01\n",
      "Epoch [7795/20000], Loss: -6.2293548583984375, Learning Rate: 0.01\n",
      "Epoch [7796/20000], Loss: -6.259765625, Learning Rate: 0.01\n",
      "Epoch [7797/20000], Loss: -6.20635986328125, Learning Rate: 0.01\n",
      "Epoch [7798/20000], Loss: -6.14105224609375, Learning Rate: 0.01\n",
      "Epoch [7799/20000], Loss: -6.1306915283203125, Learning Rate: 0.01\n",
      "Epoch [7800/20000], Loss: -6.1831207275390625, Learning Rate: 0.01\n",
      "Epoch [7801/20000], Loss: -6.252197265625, Learning Rate: 0.01\n",
      "Epoch [7802/20000], Loss: -6.2867279052734375, Learning Rate: 0.01\n",
      "Epoch [7803/20000], Loss: -6.2733154296875, Learning Rate: 0.01\n",
      "Epoch [7804/20000], Loss: -6.240081787109375, Learning Rate: 0.01\n",
      "Epoch [7805/20000], Loss: -6.22564697265625, Learning Rate: 0.01\n",
      "Epoch [7806/20000], Loss: -6.2453155517578125, Learning Rate: 0.01\n",
      "Epoch [7807/20000], Loss: -6.284088134765625, Learning Rate: 0.01\n",
      "Epoch [7808/20000], Loss: -6.3136444091796875, Learning Rate: 0.01\n",
      "Epoch [7809/20000], Loss: -6.318756103515625, Learning Rate: 0.01\n",
      "Epoch [7810/20000], Loss: -6.3059539794921875, Learning Rate: 0.01\n",
      "Epoch [7811/20000], Loss: -6.294464111328125, Learning Rate: 0.01\n",
      "Epoch [7812/20000], Loss: -6.298553466796875, Learning Rate: 0.01\n",
      "Epoch [7813/20000], Loss: -6.3170318603515625, Learning Rate: 0.01\n",
      "Epoch [7814/20000], Loss: -6.337921142578125, Learning Rate: 0.01\n",
      "Epoch [7815/20000], Loss: -6.3490753173828125, Learning Rate: 0.01\n",
      "Epoch [7816/20000], Loss: -6.348388671875, Learning Rate: 0.01\n",
      "Epoch [7817/20000], Loss: -6.3430938720703125, Learning Rate: 0.01\n",
      "Epoch [7818/20000], Loss: -6.34234619140625, Learning Rate: 0.01\n",
      "Epoch [7819/20000], Loss: -6.350006103515625, Learning Rate: 0.01\n",
      "Epoch [7820/20000], Loss: -6.36297607421875, Learning Rate: 0.01\n",
      "Epoch [7821/20000], Loss: -6.374298095703125, Learning Rate: 0.01\n",
      "Epoch [7822/20000], Loss: -6.37994384765625, Learning Rate: 0.01\n",
      "Epoch [7823/20000], Loss: -6.38067626953125, Learning Rate: 0.01\n",
      "Epoch [7824/20000], Loss: -6.3807220458984375, Learning Rate: 0.01\n",
      "Epoch [7825/20000], Loss: -6.3839569091796875, Learning Rate: 0.01\n",
      "Epoch [7826/20000], Loss: -6.39105224609375, Learning Rate: 0.01\n",
      "Epoch [7827/20000], Loss: -6.399932861328125, Learning Rate: 0.01\n",
      "Epoch [7828/20000], Loss: -6.407318115234375, Learning Rate: 0.01\n",
      "Epoch [7829/20000], Loss: -6.41180419921875, Learning Rate: 0.01\n",
      "Epoch [7830/20000], Loss: -6.414276123046875, Learning Rate: 0.01\n",
      "Epoch [7831/20000], Loss: -6.41668701171875, Learning Rate: 0.01\n",
      "Epoch [7832/20000], Loss: -6.42095947265625, Learning Rate: 0.01\n",
      "Epoch [7833/20000], Loss: -6.426971435546875, Learning Rate: 0.01\n",
      "Epoch [7834/20000], Loss: -6.43359375, Learning Rate: 0.01\n",
      "Epoch [7835/20000], Loss: -6.439453125, Learning Rate: 0.01\n",
      "Epoch [7836/20000], Loss: -6.443756103515625, Learning Rate: 0.01\n",
      "Epoch [7837/20000], Loss: -6.4472808837890625, Learning Rate: 0.01\n",
      "Epoch [7838/20000], Loss: -6.45086669921875, Learning Rate: 0.01\n",
      "Epoch [7839/20000], Loss: -6.4552001953125, Learning Rate: 0.01\n",
      "Epoch [7840/20000], Loss: -6.4604339599609375, Learning Rate: 0.01\n",
      "Epoch [7841/20000], Loss: -6.4658355712890625, Learning Rate: 0.01\n",
      "Epoch [7842/20000], Loss: -6.4710845947265625, Learning Rate: 0.01\n",
      "Epoch [7843/20000], Loss: -6.4755096435546875, Learning Rate: 0.01\n",
      "Epoch [7844/20000], Loss: -6.4794921875, Learning Rate: 0.01\n",
      "Epoch [7845/20000], Loss: -6.48345947265625, Learning Rate: 0.01\n",
      "Epoch [7846/20000], Loss: -6.487701416015625, Learning Rate: 0.01\n",
      "Epoch [7847/20000], Loss: -6.492340087890625, Learning Rate: 0.01\n",
      "Epoch [7848/20000], Loss: -6.497100830078125, Learning Rate: 0.01\n",
      "Epoch [7849/20000], Loss: -6.501800537109375, Learning Rate: 0.01\n",
      "Epoch [7850/20000], Loss: -6.506072998046875, Learning Rate: 0.01\n",
      "Epoch [7851/20000], Loss: -6.50994873046875, Learning Rate: 0.01\n",
      "Epoch [7852/20000], Loss: -6.5136566162109375, Learning Rate: 0.01\n",
      "Epoch [7853/20000], Loss: -6.5173797607421875, Learning Rate: 0.01\n",
      "Epoch [7854/20000], Loss: -6.5208892822265625, Learning Rate: 0.01\n",
      "Epoch [7855/20000], Loss: -6.524322509765625, Learning Rate: 0.01\n",
      "Epoch [7856/20000], Loss: -6.5273284912109375, Learning Rate: 0.01\n",
      "Epoch [7857/20000], Loss: -6.529541015625, Learning Rate: 0.01\n",
      "Epoch [7858/20000], Loss: -6.5308380126953125, Learning Rate: 0.01\n",
      "Epoch [7859/20000], Loss: -6.53082275390625, Learning Rate: 0.01\n",
      "Epoch [7860/20000], Loss: -6.5291748046875, Learning Rate: 0.01\n",
      "Epoch [7861/20000], Loss: -6.5255279541015625, Learning Rate: 0.01\n",
      "Epoch [7862/20000], Loss: -6.518707275390625, Learning Rate: 0.01\n",
      "Epoch [7863/20000], Loss: -6.507476806640625, Learning Rate: 0.01\n",
      "Epoch [7864/20000], Loss: -6.489898681640625, Learning Rate: 0.01\n",
      "Epoch [7865/20000], Loss: -6.46331787109375, Learning Rate: 0.01\n",
      "Epoch [7866/20000], Loss: -6.4239654541015625, Learning Rate: 0.01\n",
      "Epoch [7867/20000], Loss: -6.36627197265625, Learning Rate: 0.01\n",
      "Epoch [7868/20000], Loss: -6.2825164794921875, Learning Rate: 0.01\n",
      "Epoch [7869/20000], Loss: -6.161712646484375, Learning Rate: 0.01\n",
      "Epoch [7870/20000], Loss: -5.988250732421875, Learning Rate: 0.01\n",
      "Epoch [7871/20000], Loss: -5.74072265625, Learning Rate: 0.01\n",
      "Epoch [7872/20000], Loss: -5.3907928466796875, Learning Rate: 0.01\n",
      "Epoch [7873/20000], Loss: -4.9024505615234375, Learning Rate: 0.01\n",
      "Epoch [7874/20000], Loss: -4.236297607421875, Learning Rate: 0.01\n",
      "Epoch [7875/20000], Loss: -3.3546905517578125, Learning Rate: 0.01\n",
      "Epoch [7876/20000], Loss: -2.2486419677734375, Learning Rate: 0.01\n",
      "Epoch [7877/20000], Loss: -0.96295166015625, Learning Rate: 0.01\n",
      "Epoch [7878/20000], Loss: 0.3378448486328125, Learning Rate: 0.01\n",
      "Epoch [7879/20000], Loss: 1.358795166015625, Learning Rate: 0.01\n",
      "Epoch [7880/20000], Loss: 1.6762542724609375, Learning Rate: 0.01\n",
      "Epoch [7881/20000], Loss: 0.9514007568359375, Learning Rate: 0.01\n",
      "Epoch [7882/20000], Loss: -0.8474884033203125, Learning Rate: 0.01\n",
      "Epoch [7883/20000], Loss: -3.2246856689453125, Learning Rate: 0.01\n",
      "Epoch [7884/20000], Loss: -5.35882568359375, Learning Rate: 0.01\n",
      "Epoch [7885/20000], Loss: -6.5302734375, Learning Rate: 0.01\n",
      "Epoch [7886/20000], Loss: -6.5229034423828125, Learning Rate: 0.01\n",
      "Epoch [7887/20000], Loss: -5.66943359375, Learning Rate: 0.01\n",
      "Epoch [7888/20000], Loss: -4.6074676513671875, Learning Rate: 0.01\n",
      "Epoch [7889/20000], Loss: -3.95458984375, Learning Rate: 0.01\n",
      "Epoch [7890/20000], Loss: -4.038116455078125, Learning Rate: 0.01\n",
      "Epoch [7891/20000], Loss: -4.778717041015625, Learning Rate: 0.01\n",
      "Epoch [7892/20000], Loss: -5.75946044921875, Learning Rate: 0.01\n",
      "Epoch [7893/20000], Loss: -6.4927978515625, Learning Rate: 0.01\n",
      "Epoch [7894/20000], Loss: -6.6966400146484375, Learning Rate: 0.01\n",
      "Epoch [7895/20000], Loss: -6.4173431396484375, Learning Rate: 0.01\n",
      "Epoch [7896/20000], Loss: -5.94293212890625, Learning Rate: 0.01\n",
      "Epoch [7897/20000], Loss: -5.6049346923828125, Learning Rate: 0.01\n",
      "Epoch [7898/20000], Loss: -5.5987396240234375, Learning Rate: 0.01\n",
      "Epoch [7899/20000], Loss: -5.90264892578125, Learning Rate: 0.01\n",
      "Epoch [7900/20000], Loss: -6.3281097412109375, Learning Rate: 0.01\n",
      "Epoch [7901/20000], Loss: -6.64971923828125, Learning Rate: 0.01\n",
      "Epoch [7902/20000], Loss: -6.739227294921875, Learning Rate: 0.01\n",
      "Epoch [7903/20000], Loss: -6.6170196533203125, Learning Rate: 0.01\n",
      "Epoch [7904/20000], Loss: -6.4126434326171875, Learning Rate: 0.01\n",
      "Epoch [7905/20000], Loss: -6.2721710205078125, Learning Rate: 0.01\n",
      "Epoch [7906/20000], Loss: -6.277618408203125, Learning Rate: 0.01\n",
      "Epoch [7907/20000], Loss: -6.4153289794921875, Learning Rate: 0.01\n",
      "Epoch [7908/20000], Loss: -6.60137939453125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7909/20000], Loss: -6.7408447265625, Learning Rate: 0.01\n",
      "Epoch [7910/20000], Loss: -6.7809906005859375, Learning Rate: 0.01\n",
      "Epoch [7911/20000], Loss: -6.731536865234375, Learning Rate: 0.01\n",
      "Epoch [7912/20000], Loss: -6.64654541015625, Learning Rate: 0.01\n",
      "Epoch [7913/20000], Loss: -6.5873870849609375, Learning Rate: 0.01\n",
      "Epoch [7914/20000], Loss: -6.5889892578125, Learning Rate: 0.01\n",
      "Epoch [7915/20000], Loss: -6.647247314453125, Learning Rate: 0.01\n",
      "Epoch [7916/20000], Loss: -6.7290802001953125, Learning Rate: 0.01\n",
      "Epoch [7917/20000], Loss: -6.79498291015625, Learning Rate: 0.01\n",
      "Epoch [7918/20000], Loss: -6.8213043212890625, Learning Rate: 0.01\n",
      "Epoch [7919/20000], Loss: -6.80853271484375, Learning Rate: 0.01\n",
      "Epoch [7920/20000], Loss: -6.7761688232421875, Learning Rate: 0.01\n",
      "Epoch [7921/20000], Loss: -6.749786376953125, Learning Rate: 0.01\n",
      "Epoch [7922/20000], Loss: -6.746002197265625, Learning Rate: 0.01\n",
      "Epoch [7923/20000], Loss: -6.7672119140625, Learning Rate: 0.01\n",
      "Epoch [7924/20000], Loss: -6.8027496337890625, Learning Rate: 0.01\n",
      "Epoch [7925/20000], Loss: -6.836822509765625, Learning Rate: 0.01\n",
      "Epoch [7926/20000], Loss: -6.8573455810546875, Learning Rate: 0.01\n",
      "Epoch [7927/20000], Loss: -6.8609771728515625, Learning Rate: 0.01\n",
      "Epoch [7928/20000], Loss: -6.852783203125, Learning Rate: 0.01\n",
      "Epoch [7929/20000], Loss: -6.84222412109375, Learning Rate: 0.01\n",
      "Epoch [7930/20000], Loss: -6.8379364013671875, Learning Rate: 0.01\n",
      "Epoch [7931/20000], Loss: -6.8436279296875, Learning Rate: 0.01\n",
      "Epoch [7932/20000], Loss: -6.85809326171875, Learning Rate: 0.01\n",
      "Epoch [7933/20000], Loss: -6.8755340576171875, Learning Rate: 0.01\n",
      "Epoch [7934/20000], Loss: -6.890777587890625, Learning Rate: 0.01\n",
      "Epoch [7935/20000], Loss: -6.899932861328125, Learning Rate: 0.01\n",
      "Epoch [7936/20000], Loss: -6.902587890625, Learning Rate: 0.01\n",
      "Epoch [7937/20000], Loss: -6.9015960693359375, Learning Rate: 0.01\n",
      "Epoch [7938/20000], Loss: -6.9002685546875, Learning Rate: 0.01\n",
      "Epoch [7939/20000], Loss: -6.901641845703125, Learning Rate: 0.01\n",
      "Epoch [7940/20000], Loss: -6.906707763671875, Learning Rate: 0.01\n",
      "Epoch [7941/20000], Loss: -6.91510009765625, Learning Rate: 0.01\n",
      "Epoch [7942/20000], Loss: -6.924835205078125, Learning Rate: 0.01\n",
      "Epoch [7943/20000], Loss: -6.9336700439453125, Learning Rate: 0.01\n",
      "Epoch [7944/20000], Loss: -6.9403533935546875, Learning Rate: 0.01\n",
      "Epoch [7945/20000], Loss: -6.9445037841796875, Learning Rate: 0.01\n",
      "Epoch [7946/20000], Loss: -6.947113037109375, Learning Rate: 0.01\n",
      "Epoch [7947/20000], Loss: -6.9492034912109375, Learning Rate: 0.01\n",
      "Epoch [7948/20000], Loss: -6.9520416259765625, Learning Rate: 0.01\n",
      "Epoch [7949/20000], Loss: -6.956268310546875, Learning Rate: 0.01\n",
      "Epoch [7950/20000], Loss: -6.9617462158203125, Learning Rate: 0.01\n",
      "Epoch [7951/20000], Loss: -6.968048095703125, Learning Rate: 0.01\n",
      "Epoch [7952/20000], Loss: -6.974395751953125, Learning Rate: 0.01\n",
      "Epoch [7953/20000], Loss: -6.9802398681640625, Learning Rate: 0.01\n",
      "Epoch [7954/20000], Loss: -6.985107421875, Learning Rate: 0.01\n",
      "Epoch [7955/20000], Loss: -6.989349365234375, Learning Rate: 0.01\n",
      "Epoch [7956/20000], Loss: -6.9930419921875, Learning Rate: 0.01\n",
      "Epoch [7957/20000], Loss: -6.9966888427734375, Learning Rate: 0.01\n",
      "Epoch [7958/20000], Loss: -7.0006256103515625, Learning Rate: 0.01\n",
      "Epoch [7959/20000], Loss: -7.0048370361328125, Learning Rate: 0.01\n",
      "Epoch [7960/20000], Loss: -7.00982666015625, Learning Rate: 0.01\n",
      "Epoch [7961/20000], Loss: -7.014923095703125, Learning Rate: 0.01\n",
      "Epoch [7962/20000], Loss: -7.020050048828125, Learning Rate: 0.01\n",
      "Epoch [7963/20000], Loss: -7.0250244140625, Learning Rate: 0.01\n",
      "Epoch [7964/20000], Loss: -7.0297393798828125, Learning Rate: 0.01\n",
      "Epoch [7965/20000], Loss: -7.0341339111328125, Learning Rate: 0.01\n",
      "Epoch [7966/20000], Loss: -7.0384521484375, Learning Rate: 0.01\n",
      "Epoch [7967/20000], Loss: -7.0426483154296875, Learning Rate: 0.01\n",
      "Epoch [7968/20000], Loss: -7.0467987060546875, Learning Rate: 0.01\n",
      "Epoch [7969/20000], Loss: -7.0511474609375, Learning Rate: 0.01\n",
      "Epoch [7970/20000], Loss: -7.0557861328125, Learning Rate: 0.01\n",
      "Epoch [7971/20000], Loss: -7.060394287109375, Learning Rate: 0.01\n",
      "Epoch [7972/20000], Loss: -7.06494140625, Learning Rate: 0.01\n",
      "Epoch [7973/20000], Loss: -7.0697021484375, Learning Rate: 0.01\n",
      "Epoch [7974/20000], Loss: -7.0743560791015625, Learning Rate: 0.01\n",
      "Epoch [7975/20000], Loss: -7.0789794921875, Learning Rate: 0.01\n",
      "Epoch [7976/20000], Loss: -7.0833740234375, Learning Rate: 0.01\n",
      "Epoch [7977/20000], Loss: -7.087799072265625, Learning Rate: 0.01\n",
      "Epoch [7978/20000], Loss: -7.0922393798828125, Learning Rate: 0.01\n",
      "Epoch [7979/20000], Loss: -7.096649169921875, Learning Rate: 0.01\n",
      "Epoch [7980/20000], Loss: -7.101104736328125, Learning Rate: 0.01\n",
      "Epoch [7981/20000], Loss: -7.10546875, Learning Rate: 0.01\n",
      "Epoch [7982/20000], Loss: -7.109954833984375, Learning Rate: 0.01\n",
      "Epoch [7983/20000], Loss: -7.1145782470703125, Learning Rate: 0.01\n",
      "Epoch [7984/20000], Loss: -7.11907958984375, Learning Rate: 0.01\n",
      "Epoch [7985/20000], Loss: -7.12359619140625, Learning Rate: 0.01\n",
      "Epoch [7986/20000], Loss: -7.1280670166015625, Learning Rate: 0.01\n",
      "Epoch [7987/20000], Loss: -7.132598876953125, Learning Rate: 0.01\n",
      "Epoch [7988/20000], Loss: -7.137054443359375, Learning Rate: 0.01\n",
      "Epoch [7989/20000], Loss: -7.1415252685546875, Learning Rate: 0.01\n",
      "Epoch [7990/20000], Loss: -7.1461334228515625, Learning Rate: 0.01\n",
      "Epoch [7991/20000], Loss: -7.150543212890625, Learning Rate: 0.01\n",
      "Epoch [7992/20000], Loss: -7.1550445556640625, Learning Rate: 0.01\n",
      "Epoch [7993/20000], Loss: -7.1595458984375, Learning Rate: 0.01\n",
      "Epoch [7994/20000], Loss: -7.1639404296875, Learning Rate: 0.01\n",
      "Epoch [7995/20000], Loss: -7.1684417724609375, Learning Rate: 0.01\n",
      "Epoch [7996/20000], Loss: -7.1728973388671875, Learning Rate: 0.01\n",
      "Epoch [7997/20000], Loss: -7.17742919921875, Learning Rate: 0.01\n",
      "Epoch [7998/20000], Loss: -7.18194580078125, Learning Rate: 0.01\n",
      "Epoch [7999/20000], Loss: -7.186492919921875, Learning Rate: 0.01\n",
      "Epoch [8000/20000], Loss: -7.1909332275390625, Learning Rate: 0.01\n",
      "Epoch [8001/20000], Loss: -7.1955108642578125, Learning Rate: 0.01\n",
      "Epoch [8002/20000], Loss: -7.199981689453125, Learning Rate: 0.01\n",
      "Epoch [8003/20000], Loss: -7.2044830322265625, Learning Rate: 0.01\n",
      "Epoch [8004/20000], Loss: -7.2088775634765625, Learning Rate: 0.01\n",
      "Epoch [8005/20000], Loss: -7.2133941650390625, Learning Rate: 0.01\n",
      "Epoch [8006/20000], Loss: -7.2179412841796875, Learning Rate: 0.01\n",
      "Epoch [8007/20000], Loss: -7.2224273681640625, Learning Rate: 0.01\n",
      "Epoch [8008/20000], Loss: -7.2268829345703125, Learning Rate: 0.01\n",
      "Epoch [8009/20000], Loss: -7.231414794921875, Learning Rate: 0.01\n",
      "Epoch [8010/20000], Loss: -7.2359771728515625, Learning Rate: 0.01\n",
      "Epoch [8011/20000], Loss: -7.2404632568359375, Learning Rate: 0.01\n",
      "Epoch [8012/20000], Loss: -7.2449493408203125, Learning Rate: 0.01\n",
      "Epoch [8013/20000], Loss: -7.2493743896484375, Learning Rate: 0.01\n",
      "Epoch [8014/20000], Loss: -7.2539215087890625, Learning Rate: 0.01\n",
      "Epoch [8015/20000], Loss: -7.2584381103515625, Learning Rate: 0.01\n",
      "Epoch [8016/20000], Loss: -7.2629241943359375, Learning Rate: 0.01\n",
      "Epoch [8017/20000], Loss: -7.2674407958984375, Learning Rate: 0.01\n",
      "Epoch [8018/20000], Loss: -7.27197265625, Learning Rate: 0.01\n",
      "Epoch [8019/20000], Loss: -7.276458740234375, Learning Rate: 0.01\n",
      "Epoch [8020/20000], Loss: -7.281005859375, Learning Rate: 0.01\n",
      "Epoch [8021/20000], Loss: -7.285430908203125, Learning Rate: 0.01\n",
      "Epoch [8022/20000], Loss: -7.2899017333984375, Learning Rate: 0.01\n",
      "Epoch [8023/20000], Loss: -7.294586181640625, Learning Rate: 0.01\n",
      "Epoch [8024/20000], Loss: -7.299072265625, Learning Rate: 0.01\n",
      "Epoch [8025/20000], Loss: -7.303497314453125, Learning Rate: 0.01\n",
      "Epoch [8026/20000], Loss: -7.308013916015625, Learning Rate: 0.01\n",
      "Epoch [8027/20000], Loss: -7.3125152587890625, Learning Rate: 0.01\n",
      "Epoch [8028/20000], Loss: -7.317047119140625, Learning Rate: 0.01\n",
      "Epoch [8029/20000], Loss: -7.321563720703125, Learning Rate: 0.01\n",
      "Epoch [8030/20000], Loss: -7.326019287109375, Learning Rate: 0.01\n",
      "Epoch [8031/20000], Loss: -7.3305206298828125, Learning Rate: 0.01\n",
      "Epoch [8032/20000], Loss: -7.335205078125, Learning Rate: 0.01\n",
      "Epoch [8033/20000], Loss: -7.339569091796875, Learning Rate: 0.01\n",
      "Epoch [8034/20000], Loss: -7.3441314697265625, Learning Rate: 0.01\n",
      "Epoch [8035/20000], Loss: -7.348602294921875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8036/20000], Loss: -7.3531951904296875, Learning Rate: 0.01\n",
      "Epoch [8037/20000], Loss: -7.3577728271484375, Learning Rate: 0.01\n",
      "Epoch [8038/20000], Loss: -7.3621673583984375, Learning Rate: 0.01\n",
      "Epoch [8039/20000], Loss: -7.36676025390625, Learning Rate: 0.01\n",
      "Epoch [8040/20000], Loss: -7.3712921142578125, Learning Rate: 0.01\n",
      "Epoch [8041/20000], Loss: -7.375885009765625, Learning Rate: 0.01\n",
      "Epoch [8042/20000], Loss: -7.380340576171875, Learning Rate: 0.01\n",
      "Epoch [8043/20000], Loss: -7.3848419189453125, Learning Rate: 0.01\n",
      "Epoch [8044/20000], Loss: -7.3894195556640625, Learning Rate: 0.01\n",
      "Epoch [8045/20000], Loss: -7.39385986328125, Learning Rate: 0.01\n",
      "Epoch [8046/20000], Loss: -7.398468017578125, Learning Rate: 0.01\n",
      "Epoch [8047/20000], Loss: -7.4028778076171875, Learning Rate: 0.01\n",
      "Epoch [8048/20000], Loss: -7.407470703125, Learning Rate: 0.01\n",
      "Epoch [8049/20000], Loss: -7.4119415283203125, Learning Rate: 0.01\n",
      "Epoch [8050/20000], Loss: -7.4165191650390625, Learning Rate: 0.01\n",
      "Epoch [8051/20000], Loss: -7.4210662841796875, Learning Rate: 0.01\n",
      "Epoch [8052/20000], Loss: -7.425628662109375, Learning Rate: 0.01\n",
      "Epoch [8053/20000], Loss: -7.4301300048828125, Learning Rate: 0.01\n",
      "Epoch [8054/20000], Loss: -7.4347076416015625, Learning Rate: 0.01\n",
      "Epoch [8055/20000], Loss: -7.4391632080078125, Learning Rate: 0.01\n",
      "Epoch [8056/20000], Loss: -7.4437255859375, Learning Rate: 0.01\n",
      "Epoch [8057/20000], Loss: -7.4482879638671875, Learning Rate: 0.01\n",
      "Epoch [8058/20000], Loss: -7.4528350830078125, Learning Rate: 0.01\n",
      "Epoch [8059/20000], Loss: -7.45733642578125, Learning Rate: 0.01\n",
      "Epoch [8060/20000], Loss: -7.4618988037109375, Learning Rate: 0.01\n",
      "Epoch [8061/20000], Loss: -7.466339111328125, Learning Rate: 0.01\n",
      "Epoch [8062/20000], Loss: -7.47088623046875, Learning Rate: 0.01\n",
      "Epoch [8063/20000], Loss: -7.475433349609375, Learning Rate: 0.01\n",
      "Epoch [8064/20000], Loss: -7.4799957275390625, Learning Rate: 0.01\n",
      "Epoch [8065/20000], Loss: -7.4846038818359375, Learning Rate: 0.01\n",
      "Epoch [8066/20000], Loss: -7.4891357421875, Learning Rate: 0.01\n",
      "Epoch [8067/20000], Loss: -7.493682861328125, Learning Rate: 0.01\n",
      "Epoch [8068/20000], Loss: -7.4981231689453125, Learning Rate: 0.01\n",
      "Epoch [8069/20000], Loss: -7.502532958984375, Learning Rate: 0.01\n",
      "Epoch [8070/20000], Loss: -7.507049560546875, Learning Rate: 0.01\n",
      "Epoch [8071/20000], Loss: -7.5115509033203125, Learning Rate: 0.01\n",
      "Epoch [8072/20000], Loss: -7.515838623046875, Learning Rate: 0.01\n",
      "Epoch [8073/20000], Loss: -7.520172119140625, Learning Rate: 0.01\n",
      "Epoch [8074/20000], Loss: -7.524383544921875, Learning Rate: 0.01\n",
      "Epoch [8075/20000], Loss: -7.5282440185546875, Learning Rate: 0.01\n",
      "Epoch [8076/20000], Loss: -7.531951904296875, Learning Rate: 0.01\n",
      "Epoch [8077/20000], Loss: -7.535247802734375, Learning Rate: 0.01\n",
      "Epoch [8078/20000], Loss: -7.5378570556640625, Learning Rate: 0.01\n",
      "Epoch [8079/20000], Loss: -7.5394134521484375, Learning Rate: 0.01\n",
      "Epoch [8080/20000], Loss: -7.5392913818359375, Learning Rate: 0.01\n",
      "Epoch [8081/20000], Loss: -7.5365753173828125, Learning Rate: 0.01\n",
      "Epoch [8082/20000], Loss: -7.5299072265625, Learning Rate: 0.01\n",
      "Epoch [8083/20000], Loss: -7.516937255859375, Learning Rate: 0.01\n",
      "Epoch [8084/20000], Loss: -7.4938507080078125, Learning Rate: 0.01\n",
      "Epoch [8085/20000], Loss: -7.4549407958984375, Learning Rate: 0.01\n",
      "Epoch [8086/20000], Loss: -7.3907623291015625, Learning Rate: 0.01\n",
      "Epoch [8087/20000], Loss: -7.2863311767578125, Learning Rate: 0.01\n",
      "Epoch [8088/20000], Loss: -7.1179046630859375, Learning Rate: 0.01\n",
      "Epoch [8089/20000], Loss: -6.848114013671875, Learning Rate: 0.01\n",
      "Epoch [8090/20000], Loss: -6.4193878173828125, Learning Rate: 0.01\n",
      "Epoch [8091/20000], Loss: -5.74530029296875, Learning Rate: 0.01\n",
      "Epoch [8092/20000], Loss: -4.7063751220703125, Learning Rate: 0.01\n",
      "Epoch [8093/20000], Loss: -3.153076171875, Learning Rate: 0.01\n",
      "Epoch [8094/20000], Loss: -0.9570465087890625, Learning Rate: 0.01\n",
      "Epoch [8095/20000], Loss: 1.879913330078125, Learning Rate: 0.01\n",
      "Epoch [8096/20000], Loss: 4.9648895263671875, Learning Rate: 0.01\n",
      "Epoch [8097/20000], Loss: 7.31439208984375, Learning Rate: 0.01\n",
      "Epoch [8098/20000], Loss: 7.4168548583984375, Learning Rate: 0.01\n",
      "Epoch [8099/20000], Loss: 4.3298492431640625, Learning Rate: 0.01\n",
      "Epoch [8100/20000], Loss: -1.0273284912109375, Learning Rate: 0.01\n",
      "Epoch [8101/20000], Loss: -5.79742431640625, Learning Rate: 0.01\n",
      "Epoch [8102/20000], Loss: -7.4308624267578125, Learning Rate: 0.01\n",
      "Epoch [8103/20000], Loss: -5.7476043701171875, Learning Rate: 0.01\n",
      "Epoch [8104/20000], Loss: -2.83599853515625, Learning Rate: 0.01\n",
      "Epoch [8105/20000], Loss: -1.3168792724609375, Learning Rate: 0.01\n",
      "Epoch [8106/20000], Loss: -2.443634033203125, Learning Rate: 0.01\n",
      "Epoch [8107/20000], Loss: -5.2121429443359375, Learning Rate: 0.01\n",
      "Epoch [8108/20000], Loss: -7.3218231201171875, Learning Rate: 0.01\n",
      "Epoch [8109/20000], Loss: -7.3474578857421875, Learning Rate: 0.01\n",
      "Epoch [8110/20000], Loss: -5.8777923583984375, Learning Rate: 0.01\n",
      "Epoch [8111/20000], Loss: -4.6644287109375, Learning Rate: 0.01\n",
      "Epoch [8112/20000], Loss: -4.918975830078125, Learning Rate: 0.01\n",
      "Epoch [8113/20000], Loss: -6.3198089599609375, Learning Rate: 0.01\n",
      "Epoch [8114/20000], Loss: -7.5303192138671875, Learning Rate: 0.01\n",
      "Epoch [8115/20000], Loss: -7.6001739501953125, Learning Rate: 0.01\n",
      "Epoch [8116/20000], Loss: -6.7937774658203125, Learning Rate: 0.01\n",
      "Epoch [8117/20000], Loss: -6.1243743896484375, Learning Rate: 0.01\n",
      "Epoch [8118/20000], Loss: -6.2810821533203125, Learning Rate: 0.01\n",
      "Epoch [8119/20000], Loss: -7.0465545654296875, Learning Rate: 0.01\n",
      "Epoch [8120/20000], Loss: -7.65643310546875, Learning Rate: 0.01\n",
      "Epoch [8121/20000], Loss: -7.62860107421875, Learning Rate: 0.01\n",
      "Epoch [8122/20000], Loss: -7.171875, Learning Rate: 0.01\n",
      "Epoch [8123/20000], Loss: -6.8580474853515625, Learning Rate: 0.01\n",
      "Epoch [8124/20000], Loss: -7.0124969482421875, Learning Rate: 0.01\n",
      "Epoch [8125/20000], Loss: -7.450439453125, Learning Rate: 0.01\n",
      "Epoch [8126/20000], Loss: -7.7467193603515625, Learning Rate: 0.01\n",
      "Epoch [8127/20000], Loss: -7.68804931640625, Learning Rate: 0.01\n",
      "Epoch [8128/20000], Loss: -7.4359588623046875, Learning Rate: 0.01\n",
      "Epoch [8129/20000], Loss: -7.3018951416015625, Learning Rate: 0.01\n",
      "Epoch [8130/20000], Loss: -7.4222259521484375, Learning Rate: 0.01\n",
      "Epoch [8131/20000], Loss: -7.66339111328125, Learning Rate: 0.01\n",
      "Epoch [8132/20000], Loss: -7.799102783203125, Learning Rate: 0.01\n",
      "Epoch [8133/20000], Loss: -7.7438507080078125, Learning Rate: 0.01\n",
      "Epoch [8134/20000], Loss: -7.6064605712890625, Learning Rate: 0.01\n",
      "Epoch [8135/20000], Loss: -7.550323486328125, Learning Rate: 0.01\n",
      "Epoch [8136/20000], Loss: -7.6296234130859375, Learning Rate: 0.01\n",
      "Epoch [8137/20000], Loss: -7.7598876953125, Learning Rate: 0.01\n",
      "Epoch [8138/20000], Loss: -7.82415771484375, Learning Rate: 0.01\n",
      "Epoch [8139/20000], Loss: -7.788543701171875, Learning Rate: 0.01\n",
      "Epoch [8140/20000], Loss: -7.7174835205078125, Learning Rate: 0.01\n",
      "Epoch [8141/20000], Loss: -7.695037841796875, Learning Rate: 0.01\n",
      "Epoch [8142/20000], Loss: -7.74383544921875, Learning Rate: 0.01\n",
      "Epoch [8143/20000], Loss: -7.815765380859375, Learning Rate: 0.01\n",
      "Epoch [8144/20000], Loss: -7.8506011962890625, Learning Rate: 0.01\n",
      "Epoch [8145/20000], Loss: -7.8331146240234375, Learning Rate: 0.01\n",
      "Epoch [8146/20000], Loss: -7.7977447509765625, Learning Rate: 0.01\n",
      "Epoch [8147/20000], Loss: -7.7885284423828125, Learning Rate: 0.01\n",
      "Epoch [8148/20000], Loss: -7.8166046142578125, Learning Rate: 0.01\n",
      "Epoch [8149/20000], Loss: -7.8573150634765625, Learning Rate: 0.01\n",
      "Epoch [8150/20000], Loss: -7.8790130615234375, Learning Rate: 0.01\n",
      "Epoch [8151/20000], Loss: -7.872772216796875, Learning Rate: 0.01\n",
      "Epoch [8152/20000], Loss: -7.856109619140625, Learning Rate: 0.01\n",
      "Epoch [8153/20000], Loss: -7.8517303466796875, Learning Rate: 0.01\n",
      "Epoch [8154/20000], Loss: -7.867218017578125, Learning Rate: 0.01\n",
      "Epoch [8155/20000], Loss: -7.8908843994140625, Learning Rate: 0.01\n",
      "Epoch [8156/20000], Loss: -7.9059906005859375, Learning Rate: 0.01\n",
      "Epoch [8157/20000], Loss: -7.9063262939453125, Learning Rate: 0.01\n",
      "Epoch [8158/20000], Loss: -7.89959716796875, Learning Rate: 0.01\n",
      "Epoch [8159/20000], Loss: -7.8977813720703125, Learning Rate: 0.01\n",
      "Epoch [8160/20000], Loss: -7.906280517578125, Learning Rate: 0.01\n",
      "Epoch [8161/20000], Loss: -7.9206390380859375, Learning Rate: 0.01\n",
      "Epoch [8162/20000], Loss: -7.9319000244140625, Learning Rate: 0.01\n",
      "Epoch [8163/20000], Loss: -7.9358978271484375, Learning Rate: 0.01\n",
      "Epoch [8164/20000], Loss: -7.9347076416015625, Learning Rate: 0.01\n",
      "Epoch [8165/20000], Loss: -7.9346771240234375, Learning Rate: 0.01\n",
      "Epoch [8166/20000], Loss: -7.9398345947265625, Learning Rate: 0.01\n",
      "Epoch [8167/20000], Loss: -7.94891357421875, Learning Rate: 0.01\n",
      "Epoch [8168/20000], Loss: -7.9577484130859375, Learning Rate: 0.01\n",
      "Epoch [8169/20000], Loss: -7.963134765625, Learning Rate: 0.01\n",
      "Epoch [8170/20000], Loss: -7.965118408203125, Learning Rate: 0.01\n",
      "Epoch [8171/20000], Loss: -7.966705322265625, Learning Rate: 0.01\n",
      "Epoch [8172/20000], Loss: -7.9703521728515625, Learning Rate: 0.01\n",
      "Epoch [8173/20000], Loss: -7.9766082763671875, Learning Rate: 0.01\n",
      "Epoch [8174/20000], Loss: -7.9835662841796875, Learning Rate: 0.01\n",
      "Epoch [8175/20000], Loss: -7.9892120361328125, Learning Rate: 0.01\n",
      "Epoch [8176/20000], Loss: -7.9930419921875, Learning Rate: 0.01\n",
      "Epoch [8177/20000], Loss: -7.9957733154296875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8178/20000], Loss: -7.999237060546875, Learning Rate: 0.01\n",
      "Epoch [8179/20000], Loss: -8.003936767578125, Learning Rate: 0.01\n",
      "Epoch [8180/20000], Loss: -8.0093994140625, Learning Rate: 0.01\n",
      "Epoch [8181/20000], Loss: -8.014877319335938, Learning Rate: 0.01\n",
      "Epoch [8182/20000], Loss: -8.019287109375, Learning Rate: 0.01\n",
      "Epoch [8183/20000], Loss: -8.02288818359375, Learning Rate: 0.01\n",
      "Epoch [8184/20000], Loss: -8.026382446289062, Learning Rate: 0.01\n",
      "Epoch [8185/20000], Loss: -8.030166625976562, Learning Rate: 0.01\n",
      "Epoch [8186/20000], Loss: -8.034561157226562, Learning Rate: 0.01\n",
      "Epoch [8187/20000], Loss: -8.03900146484375, Learning Rate: 0.01\n",
      "Epoch [8188/20000], Loss: -8.043014526367188, Learning Rate: 0.01\n",
      "Epoch [8189/20000], Loss: -8.046295166015625, Learning Rate: 0.01\n",
      "Epoch [8190/20000], Loss: -8.048797607421875, Learning Rate: 0.01\n",
      "Epoch [8191/20000], Loss: -8.050735473632812, Learning Rate: 0.01\n",
      "Epoch [8192/20000], Loss: -8.052047729492188, Learning Rate: 0.01\n",
      "Epoch [8193/20000], Loss: -8.05242919921875, Learning Rate: 0.01\n",
      "Epoch [8194/20000], Loss: -8.051025390625, Learning Rate: 0.01\n",
      "Epoch [8195/20000], Loss: -8.046966552734375, Learning Rate: 0.01\n",
      "Epoch [8196/20000], Loss: -8.038711547851562, Learning Rate: 0.01\n",
      "Epoch [8197/20000], Loss: -8.024826049804688, Learning Rate: 0.01\n",
      "Epoch [8198/20000], Loss: -8.002914428710938, Learning Rate: 0.01\n",
      "Epoch [8199/20000], Loss: -7.9690093994140625, Learning Rate: 0.01\n",
      "Epoch [8200/20000], Loss: -7.9176025390625, Learning Rate: 0.01\n",
      "Epoch [8201/20000], Loss: -7.840301513671875, Learning Rate: 0.01\n",
      "Epoch [8202/20000], Loss: -7.7250213623046875, Learning Rate: 0.01\n",
      "Epoch [8203/20000], Loss: -7.5545654296875, Learning Rate: 0.01\n",
      "Epoch [8204/20000], Loss: -7.3057708740234375, Learning Rate: 0.01\n",
      "Epoch [8205/20000], Loss: -6.947509765625, Learning Rate: 0.01\n",
      "Epoch [8206/20000], Loss: -6.44354248046875, Learning Rate: 0.01\n",
      "Epoch [8207/20000], Loss: -5.75811767578125, Learning Rate: 0.01\n",
      "Epoch [8208/20000], Loss: -4.8768157958984375, Learning Rate: 0.01\n",
      "Epoch [8209/20000], Loss: -3.8375396728515625, Learning Rate: 0.01\n",
      "Epoch [8210/20000], Loss: -2.786865234375, Learning Rate: 0.01\n",
      "Epoch [8211/20000], Loss: -2.0062255859375, Learning Rate: 0.01\n",
      "Epoch [8212/20000], Loss: -1.8801116943359375, Learning Rate: 0.01\n",
      "Epoch [8213/20000], Loss: -2.6870880126953125, Learning Rate: 0.01\n",
      "Epoch [8214/20000], Loss: -4.34246826171875, Learning Rate: 0.01\n",
      "Epoch [8215/20000], Loss: -6.274993896484375, Learning Rate: 0.01\n",
      "Epoch [8216/20000], Loss: -7.7152557373046875, Learning Rate: 0.01\n",
      "Epoch [8217/20000], Loss: -8.170394897460938, Learning Rate: 0.01\n",
      "Epoch [8218/20000], Loss: -7.69805908203125, Learning Rate: 0.01\n",
      "Epoch [8219/20000], Loss: -6.797607421875, Learning Rate: 0.01\n",
      "Epoch [8220/20000], Loss: -6.089599609375, Learning Rate: 0.01\n",
      "Epoch [8221/20000], Loss: -5.9980926513671875, Learning Rate: 0.01\n",
      "Epoch [8222/20000], Loss: -6.54937744140625, Learning Rate: 0.01\n",
      "Epoch [8223/20000], Loss: -7.391326904296875, Learning Rate: 0.01\n",
      "Epoch [8224/20000], Loss: -8.038787841796875, Learning Rate: 0.01\n",
      "Epoch [8225/20000], Loss: -8.196334838867188, Learning Rate: 0.01\n",
      "Epoch [8226/20000], Loss: -7.91204833984375, Learning Rate: 0.01\n",
      "Epoch [8227/20000], Loss: -7.485595703125, Learning Rate: 0.01\n",
      "Epoch [8228/20000], Loss: -7.2427978515625, Learning Rate: 0.01\n",
      "Epoch [8229/20000], Loss: -7.3366851806640625, Learning Rate: 0.01\n",
      "Epoch [8230/20000], Loss: -7.682891845703125, Learning Rate: 0.01\n",
      "Epoch [8231/20000], Loss: -8.050262451171875, Learning Rate: 0.01\n",
      "Epoch [8232/20000], Loss: -8.23291015625, Learning Rate: 0.01\n",
      "Epoch [8233/20000], Loss: -8.176986694335938, Learning Rate: 0.01\n",
      "Epoch [8234/20000], Loss: -7.9869232177734375, Learning Rate: 0.01\n",
      "Epoch [8235/20000], Loss: -7.828399658203125, Learning Rate: 0.01\n",
      "Epoch [8236/20000], Loss: -7.8141021728515625, Learning Rate: 0.01\n",
      "Epoch [8237/20000], Loss: -7.9439239501953125, Learning Rate: 0.01\n",
      "Epoch [8238/20000], Loss: -8.125595092773438, Learning Rate: 0.01\n",
      "Epoch [8239/20000], Loss: -8.250289916992188, Learning Rate: 0.01\n",
      "Epoch [8240/20000], Loss: -8.26470947265625, Learning Rate: 0.01\n",
      "Epoch [8241/20000], Loss: -8.193634033203125, Learning Rate: 0.01\n",
      "Epoch [8242/20000], Loss: -8.109161376953125, Learning Rate: 0.01\n",
      "Epoch [8243/20000], Loss: -8.076553344726562, Learning Rate: 0.01\n",
      "Epoch [8244/20000], Loss: -8.115859985351562, Learning Rate: 0.01\n",
      "Epoch [8245/20000], Loss: -8.198287963867188, Learning Rate: 0.01\n",
      "Epoch [8246/20000], Loss: -8.27337646484375, Learning Rate: 0.01\n",
      "Epoch [8247/20000], Loss: -8.3046875, Learning Rate: 0.01\n",
      "Epoch [8248/20000], Loss: -8.288864135742188, Learning Rate: 0.01\n",
      "Epoch [8249/20000], Loss: -8.2513427734375, Learning Rate: 0.01\n",
      "Epoch [8250/20000], Loss: -8.225021362304688, Learning Rate: 0.01\n",
      "Epoch [8251/20000], Loss: -8.229324340820312, Learning Rate: 0.01\n",
      "Epoch [8252/20000], Loss: -8.261367797851562, Learning Rate: 0.01\n",
      "Epoch [8253/20000], Loss: -8.302093505859375, Learning Rate: 0.01\n",
      "Epoch [8254/20000], Loss: -8.33087158203125, Learning Rate: 0.01\n",
      "Epoch [8255/20000], Loss: -8.33782958984375, Learning Rate: 0.01\n",
      "Epoch [8256/20000], Loss: -8.327728271484375, Learning Rate: 0.01\n",
      "Epoch [8257/20000], Loss: -8.313613891601562, Learning Rate: 0.01\n",
      "Epoch [8258/20000], Loss: -8.308456420898438, Learning Rate: 0.01\n",
      "Epoch [8259/20000], Loss: -8.317123413085938, Learning Rate: 0.01\n",
      "Epoch [8260/20000], Loss: -8.335861206054688, Learning Rate: 0.01\n",
      "Epoch [8261/20000], Loss: -8.355621337890625, Learning Rate: 0.01\n",
      "Epoch [8262/20000], Loss: -8.368621826171875, Learning Rate: 0.01\n",
      "Epoch [8263/20000], Loss: -8.372283935546875, Learning Rate: 0.01\n",
      "Epoch [8264/20000], Loss: -8.369384765625, Learning Rate: 0.01\n",
      "Epoch [8265/20000], Loss: -8.36590576171875, Learning Rate: 0.01\n",
      "Epoch [8266/20000], Loss: -8.36676025390625, Learning Rate: 0.01\n",
      "Epoch [8267/20000], Loss: -8.373489379882812, Learning Rate: 0.01\n",
      "Epoch [8268/20000], Loss: -8.384201049804688, Learning Rate: 0.01\n",
      "Epoch [8269/20000], Loss: -8.395065307617188, Learning Rate: 0.01\n",
      "Epoch [8270/20000], Loss: -8.403030395507812, Learning Rate: 0.01\n",
      "Epoch [8271/20000], Loss: -8.40716552734375, Learning Rate: 0.01\n",
      "Epoch [8272/20000], Loss: -8.408462524414062, Learning Rate: 0.01\n",
      "Epoch [8273/20000], Loss: -8.409255981445312, Learning Rate: 0.01\n",
      "Epoch [8274/20000], Loss: -8.411773681640625, Learning Rate: 0.01\n",
      "Epoch [8275/20000], Loss: -8.416595458984375, Learning Rate: 0.01\n",
      "Epoch [8276/20000], Loss: -8.423431396484375, Learning Rate: 0.01\n",
      "Epoch [8277/20000], Loss: -8.430648803710938, Learning Rate: 0.01\n",
      "Epoch [8278/20000], Loss: -8.43682861328125, Learning Rate: 0.01\n",
      "Epoch [8279/20000], Loss: -8.441482543945312, Learning Rate: 0.01\n",
      "Epoch [8280/20000], Loss: -8.444808959960938, Learning Rate: 0.01\n",
      "Epoch [8281/20000], Loss: -8.44757080078125, Learning Rate: 0.01\n",
      "Epoch [8282/20000], Loss: -8.450759887695312, Learning Rate: 0.01\n",
      "Epoch [8283/20000], Loss: -8.454696655273438, Learning Rate: 0.01\n",
      "Epoch [8284/20000], Loss: -8.459686279296875, Learning Rate: 0.01\n",
      "Epoch [8285/20000], Loss: -8.465087890625, Learning Rate: 0.01\n",
      "Epoch [8286/20000], Loss: -8.470367431640625, Learning Rate: 0.01\n",
      "Epoch [8287/20000], Loss: -8.475189208984375, Learning Rate: 0.01\n",
      "Epoch [8288/20000], Loss: -8.479339599609375, Learning Rate: 0.01\n",
      "Epoch [8289/20000], Loss: -8.483078002929688, Learning Rate: 0.01\n",
      "Epoch [8290/20000], Loss: -8.48675537109375, Learning Rate: 0.01\n",
      "Epoch [8291/20000], Loss: -8.490585327148438, Learning Rate: 0.01\n",
      "Epoch [8292/20000], Loss: -8.494873046875, Learning Rate: 0.01\n",
      "Epoch [8293/20000], Loss: -8.499435424804688, Learning Rate: 0.01\n",
      "Epoch [8294/20000], Loss: -8.50408935546875, Learning Rate: 0.01\n",
      "Epoch [8295/20000], Loss: -8.508712768554688, Learning Rate: 0.01\n",
      "Epoch [8296/20000], Loss: -8.513153076171875, Learning Rate: 0.01\n",
      "Epoch [8297/20000], Loss: -8.517303466796875, Learning Rate: 0.01\n",
      "Epoch [8298/20000], Loss: -8.52142333984375, Learning Rate: 0.01\n",
      "Epoch [8299/20000], Loss: -8.525421142578125, Learning Rate: 0.01\n",
      "Epoch [8300/20000], Loss: -8.529449462890625, Learning Rate: 0.01\n",
      "Epoch [8301/20000], Loss: -8.533660888671875, Learning Rate: 0.01\n",
      "Epoch [8302/20000], Loss: -8.537948608398438, Learning Rate: 0.01\n",
      "Epoch [8303/20000], Loss: -8.542404174804688, Learning Rate: 0.01\n",
      "Epoch [8304/20000], Loss: -8.5467529296875, Learning Rate: 0.01\n",
      "Epoch [8305/20000], Loss: -8.551177978515625, Learning Rate: 0.01\n",
      "Epoch [8306/20000], Loss: -8.555374145507812, Learning Rate: 0.01\n",
      "Epoch [8307/20000], Loss: -8.559646606445312, Learning Rate: 0.01\n",
      "Epoch [8308/20000], Loss: -8.563720703125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8309/20000], Loss: -8.567901611328125, Learning Rate: 0.01\n",
      "Epoch [8310/20000], Loss: -8.572052001953125, Learning Rate: 0.01\n",
      "Epoch [8311/20000], Loss: -8.576324462890625, Learning Rate: 0.01\n",
      "Epoch [8312/20000], Loss: -8.580490112304688, Learning Rate: 0.01\n",
      "Epoch [8313/20000], Loss: -8.584854125976562, Learning Rate: 0.01\n",
      "Epoch [8314/20000], Loss: -8.589187622070312, Learning Rate: 0.01\n",
      "Epoch [8315/20000], Loss: -8.593414306640625, Learning Rate: 0.01\n",
      "Epoch [8316/20000], Loss: -8.597610473632812, Learning Rate: 0.01\n",
      "Epoch [8317/20000], Loss: -8.601852416992188, Learning Rate: 0.01\n",
      "Epoch [8318/20000], Loss: -8.606094360351562, Learning Rate: 0.01\n",
      "Epoch [8319/20000], Loss: -8.610244750976562, Learning Rate: 0.01\n",
      "Epoch [8320/20000], Loss: -8.61444091796875, Learning Rate: 0.01\n",
      "Epoch [8321/20000], Loss: -8.618682861328125, Learning Rate: 0.01\n",
      "Epoch [8322/20000], Loss: -8.62298583984375, Learning Rate: 0.01\n",
      "Epoch [8323/20000], Loss: -8.627212524414062, Learning Rate: 0.01\n",
      "Epoch [8324/20000], Loss: -8.631515502929688, Learning Rate: 0.01\n",
      "Epoch [8325/20000], Loss: -8.635787963867188, Learning Rate: 0.01\n",
      "Epoch [8326/20000], Loss: -8.63995361328125, Learning Rate: 0.01\n",
      "Epoch [8327/20000], Loss: -8.644180297851562, Learning Rate: 0.01\n",
      "Epoch [8328/20000], Loss: -8.648468017578125, Learning Rate: 0.01\n",
      "Epoch [8329/20000], Loss: -8.652633666992188, Learning Rate: 0.01\n",
      "Epoch [8330/20000], Loss: -8.65692138671875, Learning Rate: 0.01\n",
      "Epoch [8331/20000], Loss: -8.661163330078125, Learning Rate: 0.01\n",
      "Epoch [8332/20000], Loss: -8.665328979492188, Learning Rate: 0.01\n",
      "Epoch [8333/20000], Loss: -8.66961669921875, Learning Rate: 0.01\n",
      "Epoch [8334/20000], Loss: -8.673843383789062, Learning Rate: 0.01\n",
      "Epoch [8335/20000], Loss: -8.678131103515625, Learning Rate: 0.01\n",
      "Epoch [8336/20000], Loss: -8.682296752929688, Learning Rate: 0.01\n",
      "Epoch [8337/20000], Loss: -8.686599731445312, Learning Rate: 0.01\n",
      "Epoch [8338/20000], Loss: -8.690872192382812, Learning Rate: 0.01\n",
      "Epoch [8339/20000], Loss: -8.69512939453125, Learning Rate: 0.01\n",
      "Epoch [8340/20000], Loss: -8.699386596679688, Learning Rate: 0.01\n",
      "Epoch [8341/20000], Loss: -8.703628540039062, Learning Rate: 0.01\n",
      "Epoch [8342/20000], Loss: -8.707809448242188, Learning Rate: 0.01\n",
      "Epoch [8343/20000], Loss: -8.712051391601562, Learning Rate: 0.01\n",
      "Epoch [8344/20000], Loss: -8.71630859375, Learning Rate: 0.01\n",
      "Epoch [8345/20000], Loss: -8.720535278320312, Learning Rate: 0.01\n",
      "Epoch [8346/20000], Loss: -8.724777221679688, Learning Rate: 0.01\n",
      "Epoch [8347/20000], Loss: -8.729110717773438, Learning Rate: 0.01\n",
      "Epoch [8348/20000], Loss: -8.7332763671875, Learning Rate: 0.01\n",
      "Epoch [8349/20000], Loss: -8.737564086914062, Learning Rate: 0.01\n",
      "Epoch [8350/20000], Loss: -8.7418212890625, Learning Rate: 0.01\n",
      "Epoch [8351/20000], Loss: -8.746063232421875, Learning Rate: 0.01\n",
      "Epoch [8352/20000], Loss: -8.750289916992188, Learning Rate: 0.01\n",
      "Epoch [8353/20000], Loss: -8.754547119140625, Learning Rate: 0.01\n",
      "Epoch [8354/20000], Loss: -8.758865356445312, Learning Rate: 0.01\n",
      "Epoch [8355/20000], Loss: -8.763092041015625, Learning Rate: 0.01\n",
      "Epoch [8356/20000], Loss: -8.76727294921875, Learning Rate: 0.01\n",
      "Epoch [8357/20000], Loss: -8.7716064453125, Learning Rate: 0.01\n",
      "Epoch [8358/20000], Loss: -8.775863647460938, Learning Rate: 0.01\n",
      "Epoch [8359/20000], Loss: -8.7801513671875, Learning Rate: 0.01\n",
      "Epoch [8360/20000], Loss: -8.78436279296875, Learning Rate: 0.01\n",
      "Epoch [8361/20000], Loss: -8.788665771484375, Learning Rate: 0.01\n",
      "Epoch [8362/20000], Loss: -8.792922973632812, Learning Rate: 0.01\n",
      "Epoch [8363/20000], Loss: -8.79718017578125, Learning Rate: 0.01\n",
      "Epoch [8364/20000], Loss: -8.801422119140625, Learning Rate: 0.01\n",
      "Epoch [8365/20000], Loss: -8.805648803710938, Learning Rate: 0.01\n",
      "Epoch [8366/20000], Loss: -8.809967041015625, Learning Rate: 0.01\n",
      "Epoch [8367/20000], Loss: -8.81414794921875, Learning Rate: 0.01\n",
      "Epoch [8368/20000], Loss: -8.81842041015625, Learning Rate: 0.01\n",
      "Epoch [8369/20000], Loss: -8.822677612304688, Learning Rate: 0.01\n",
      "Epoch [8370/20000], Loss: -8.827056884765625, Learning Rate: 0.01\n",
      "Epoch [8371/20000], Loss: -8.831253051757812, Learning Rate: 0.01\n",
      "Epoch [8372/20000], Loss: -8.835479736328125, Learning Rate: 0.01\n",
      "Epoch [8373/20000], Loss: -8.839828491210938, Learning Rate: 0.01\n",
      "Epoch [8374/20000], Loss: -8.844100952148438, Learning Rate: 0.01\n",
      "Epoch [8375/20000], Loss: -8.848297119140625, Learning Rate: 0.01\n",
      "Epoch [8376/20000], Loss: -8.85260009765625, Learning Rate: 0.01\n",
      "Epoch [8377/20000], Loss: -8.856842041015625, Learning Rate: 0.01\n",
      "Epoch [8378/20000], Loss: -8.861114501953125, Learning Rate: 0.01\n",
      "Epoch [8379/20000], Loss: -8.865432739257812, Learning Rate: 0.01\n",
      "Epoch [8380/20000], Loss: -8.869552612304688, Learning Rate: 0.01\n",
      "Epoch [8381/20000], Loss: -8.873931884765625, Learning Rate: 0.01\n",
      "Epoch [8382/20000], Loss: -8.878250122070312, Learning Rate: 0.01\n",
      "Epoch [8383/20000], Loss: -8.88250732421875, Learning Rate: 0.01\n",
      "Epoch [8384/20000], Loss: -8.886734008789062, Learning Rate: 0.01\n",
      "Epoch [8385/20000], Loss: -8.89105224609375, Learning Rate: 0.01\n",
      "Epoch [8386/20000], Loss: -8.895294189453125, Learning Rate: 0.01\n",
      "Epoch [8387/20000], Loss: -8.899490356445312, Learning Rate: 0.01\n",
      "Epoch [8388/20000], Loss: -8.903854370117188, Learning Rate: 0.01\n",
      "Epoch [8389/20000], Loss: -8.908111572265625, Learning Rate: 0.01\n",
      "Epoch [8390/20000], Loss: -8.912338256835938, Learning Rate: 0.01\n",
      "Epoch [8391/20000], Loss: -8.916702270507812, Learning Rate: 0.01\n",
      "Epoch [8392/20000], Loss: -8.92095947265625, Learning Rate: 0.01\n",
      "Epoch [8393/20000], Loss: -8.92523193359375, Learning Rate: 0.01\n",
      "Epoch [8394/20000], Loss: -8.929595947265625, Learning Rate: 0.01\n",
      "Epoch [8395/20000], Loss: -8.933822631835938, Learning Rate: 0.01\n",
      "Epoch [8396/20000], Loss: -8.938125610351562, Learning Rate: 0.01\n",
      "Epoch [8397/20000], Loss: -8.94244384765625, Learning Rate: 0.01\n",
      "Epoch [8398/20000], Loss: -8.946624755859375, Learning Rate: 0.01\n",
      "Epoch [8399/20000], Loss: -8.951004028320312, Learning Rate: 0.01\n",
      "Epoch [8400/20000], Loss: -8.955230712890625, Learning Rate: 0.01\n",
      "Epoch [8401/20000], Loss: -8.959518432617188, Learning Rate: 0.01\n",
      "Epoch [8402/20000], Loss: -8.963790893554688, Learning Rate: 0.01\n",
      "Epoch [8403/20000], Loss: -8.968124389648438, Learning Rate: 0.01\n",
      "Epoch [8404/20000], Loss: -8.972274780273438, Learning Rate: 0.01\n",
      "Epoch [8405/20000], Loss: -8.976730346679688, Learning Rate: 0.01\n",
      "Epoch [8406/20000], Loss: -8.98089599609375, Learning Rate: 0.01\n",
      "Epoch [8407/20000], Loss: -8.985244750976562, Learning Rate: 0.01\n",
      "Epoch [8408/20000], Loss: -8.989532470703125, Learning Rate: 0.01\n",
      "Epoch [8409/20000], Loss: -8.99383544921875, Learning Rate: 0.01\n",
      "Epoch [8410/20000], Loss: -8.998062133789062, Learning Rate: 0.01\n",
      "Epoch [8411/20000], Loss: -9.002487182617188, Learning Rate: 0.01\n",
      "Epoch [8412/20000], Loss: -9.006698608398438, Learning Rate: 0.01\n",
      "Epoch [8413/20000], Loss: -9.010940551757812, Learning Rate: 0.01\n",
      "Epoch [8414/20000], Loss: -9.015274047851562, Learning Rate: 0.01\n",
      "Epoch [8415/20000], Loss: -9.01953125, Learning Rate: 0.01\n",
      "Epoch [8416/20000], Loss: -9.023880004882812, Learning Rate: 0.01\n",
      "Epoch [8417/20000], Loss: -9.028060913085938, Learning Rate: 0.01\n",
      "Epoch [8418/20000], Loss: -9.032424926757812, Learning Rate: 0.01\n",
      "Epoch [8419/20000], Loss: -9.0367431640625, Learning Rate: 0.01\n",
      "Epoch [8420/20000], Loss: -9.041015625, Learning Rate: 0.01\n",
      "Epoch [8421/20000], Loss: -9.045333862304688, Learning Rate: 0.01\n",
      "Epoch [8422/20000], Loss: -9.049591064453125, Learning Rate: 0.01\n",
      "Epoch [8423/20000], Loss: -9.053909301757812, Learning Rate: 0.01\n",
      "Epoch [8424/20000], Loss: -9.058212280273438, Learning Rate: 0.01\n",
      "Epoch [8425/20000], Loss: -9.062515258789062, Learning Rate: 0.01\n",
      "Epoch [8426/20000], Loss: -9.066848754882812, Learning Rate: 0.01\n",
      "Epoch [8427/20000], Loss: -9.071014404296875, Learning Rate: 0.01\n",
      "Epoch [8428/20000], Loss: -9.0753173828125, Learning Rate: 0.01\n",
      "Epoch [8429/20000], Loss: -9.07965087890625, Learning Rate: 0.01\n",
      "Epoch [8430/20000], Loss: -9.083953857421875, Learning Rate: 0.01\n",
      "Epoch [8431/20000], Loss: -9.08807373046875, Learning Rate: 0.01\n",
      "Epoch [8432/20000], Loss: -9.092391967773438, Learning Rate: 0.01\n",
      "Epoch [8433/20000], Loss: -9.096542358398438, Learning Rate: 0.01\n",
      "Epoch [8434/20000], Loss: -9.1007080078125, Learning Rate: 0.01\n",
      "Epoch [8435/20000], Loss: -9.104766845703125, Learning Rate: 0.01\n",
      "Epoch [8436/20000], Loss: -9.108840942382812, Learning Rate: 0.01\n",
      "Epoch [8437/20000], Loss: -9.112655639648438, Learning Rate: 0.01\n",
      "Epoch [8438/20000], Loss: -9.1163330078125, Learning Rate: 0.01\n",
      "Epoch [8439/20000], Loss: -9.119705200195312, Learning Rate: 0.01\n",
      "Epoch [8440/20000], Loss: -9.122711181640625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8441/20000], Loss: -9.12493896484375, Learning Rate: 0.01\n",
      "Epoch [8442/20000], Loss: -9.12640380859375, Learning Rate: 0.01\n",
      "Epoch [8443/20000], Loss: -9.126388549804688, Learning Rate: 0.01\n",
      "Epoch [8444/20000], Loss: -9.124114990234375, Learning Rate: 0.01\n",
      "Epoch [8445/20000], Loss: -9.11871337890625, Learning Rate: 0.01\n",
      "Epoch [8446/20000], Loss: -9.108352661132812, Learning Rate: 0.01\n",
      "Epoch [8447/20000], Loss: -9.090240478515625, Learning Rate: 0.01\n",
      "Epoch [8448/20000], Loss: -9.060882568359375, Learning Rate: 0.01\n",
      "Epoch [8449/20000], Loss: -9.01348876953125, Learning Rate: 0.01\n",
      "Epoch [8450/20000], Loss: -8.938720703125, Learning Rate: 0.01\n",
      "Epoch [8451/20000], Loss: -8.821670532226562, Learning Rate: 0.01\n",
      "Epoch [8452/20000], Loss: -8.639663696289062, Learning Rate: 0.01\n",
      "Epoch [8453/20000], Loss: -8.358749389648438, Learning Rate: 0.01\n",
      "Epoch [8454/20000], Loss: -7.9291229248046875, Learning Rate: 0.01\n",
      "Epoch [8455/20000], Loss: -7.28118896484375, Learning Rate: 0.01\n",
      "Epoch [8456/20000], Loss: -6.3270416259765625, Learning Rate: 0.01\n",
      "Epoch [8457/20000], Loss: -4.974212646484375, Learning Rate: 0.01\n",
      "Epoch [8458/20000], Loss: -3.1771697998046875, Learning Rate: 0.01\n",
      "Epoch [8459/20000], Loss: -1.032958984375, Learning Rate: 0.01\n",
      "Epoch [8460/20000], Loss: 1.05963134765625, Learning Rate: 0.01\n",
      "Epoch [8461/20000], Loss: 2.3283233642578125, Learning Rate: 0.01\n",
      "Epoch [8462/20000], Loss: 1.83966064453125, Learning Rate: 0.01\n",
      "Epoch [8463/20000], Loss: -0.7743988037109375, Learning Rate: 0.01\n",
      "Epoch [8464/20000], Loss: -4.639892578125, Learning Rate: 0.01\n",
      "Epoch [8465/20000], Loss: -7.8910064697265625, Learning Rate: 0.01\n",
      "Epoch [8466/20000], Loss: -9.012283325195312, Learning Rate: 0.01\n",
      "Epoch [8467/20000], Loss: -7.9279632568359375, Learning Rate: 0.01\n",
      "Epoch [8468/20000], Loss: -5.8971710205078125, Learning Rate: 0.01\n",
      "Epoch [8469/20000], Loss: -4.5601959228515625, Learning Rate: 0.01\n",
      "Epoch [8470/20000], Loss: -4.8958282470703125, Learning Rate: 0.01\n",
      "Epoch [8471/20000], Loss: -6.602447509765625, Learning Rate: 0.01\n",
      "Epoch [8472/20000], Loss: -8.393890380859375, Learning Rate: 0.01\n",
      "Epoch [8473/20000], Loss: -9.079238891601562, Learning Rate: 0.01\n",
      "Epoch [8474/20000], Loss: -8.49578857421875, Learning Rate: 0.01\n",
      "Epoch [8475/20000], Loss: -7.4676055908203125, Learning Rate: 0.01\n",
      "Epoch [8476/20000], Loss: -7.0065460205078125, Learning Rate: 0.01\n",
      "Epoch [8477/20000], Loss: -7.5012664794921875, Learning Rate: 0.01\n",
      "Epoch [8478/20000], Loss: -8.504165649414062, Learning Rate: 0.01\n",
      "Epoch [8479/20000], Loss: -9.214645385742188, Learning Rate: 0.01\n",
      "Epoch [8480/20000], Loss: -9.198867797851562, Learning Rate: 0.01\n",
      "Epoch [8481/20000], Loss: -8.682998657226562, Learning Rate: 0.01\n",
      "Epoch [8482/20000], Loss: -8.246917724609375, Learning Rate: 0.01\n",
      "Epoch [8483/20000], Loss: -8.289077758789062, Learning Rate: 0.01\n",
      "Epoch [8484/20000], Loss: -8.735031127929688, Learning Rate: 0.01\n",
      "Epoch [8485/20000], Loss: -9.18365478515625, Learning Rate: 0.01\n",
      "Epoch [8486/20000], Loss: -9.300094604492188, Learning Rate: 0.01\n",
      "Epoch [8487/20000], Loss: -9.08551025390625, Learning Rate: 0.01\n",
      "Epoch [8488/20000], Loss: -8.812286376953125, Learning Rate: 0.01\n",
      "Epoch [8489/20000], Loss: -8.748504638671875, Learning Rate: 0.01\n",
      "Epoch [8490/20000], Loss: -8.937347412109375, Learning Rate: 0.01\n",
      "Epoch [8491/20000], Loss: -9.203475952148438, Learning Rate: 0.01\n",
      "Epoch [8492/20000], Loss: -9.338790893554688, Learning Rate: 0.01\n",
      "Epoch [8493/20000], Loss: -9.281814575195312, Learning Rate: 0.01\n",
      "Epoch [8494/20000], Loss: -9.13958740234375, Learning Rate: 0.01\n",
      "Epoch [8495/20000], Loss: -9.06707763671875, Learning Rate: 0.01\n",
      "Epoch [8496/20000], Loss: -9.130355834960938, Learning Rate: 0.01\n",
      "Epoch [8497/20000], Loss: -9.269790649414062, Learning Rate: 0.01\n",
      "Epoch [8498/20000], Loss: -9.373199462890625, Learning Rate: 0.01\n",
      "Epoch [8499/20000], Loss: -9.377273559570312, Learning Rate: 0.01\n",
      "Epoch [8500/20000], Loss: -9.310455322265625, Learning Rate: 0.01\n",
      "Epoch [8501/20000], Loss: -9.251373291015625, Learning Rate: 0.01\n",
      "Epoch [8502/20000], Loss: -9.256301879882812, Learning Rate: 0.01\n",
      "Epoch [8503/20000], Loss: -9.31829833984375, Learning Rate: 0.01\n",
      "Epoch [8504/20000], Loss: -9.385345458984375, Learning Rate: 0.01\n",
      "Epoch [8505/20000], Loss: -9.410980224609375, Learning Rate: 0.01\n",
      "Epoch [8506/20000], Loss: -9.390090942382812, Learning Rate: 0.01\n",
      "Epoch [8507/20000], Loss: -9.35498046875, Learning Rate: 0.01\n",
      "Epoch [8508/20000], Loss: -9.34259033203125, Learning Rate: 0.01\n",
      "Epoch [8509/20000], Loss: -9.364608764648438, Learning Rate: 0.01\n",
      "Epoch [8510/20000], Loss: -9.40362548828125, Learning Rate: 0.01\n",
      "Epoch [8511/20000], Loss: -9.431915283203125, Learning Rate: 0.01\n",
      "Epoch [8512/20000], Loss: -9.435531616210938, Learning Rate: 0.01\n",
      "Epoch [8513/20000], Loss: -9.42181396484375, Learning Rate: 0.01\n",
      "Epoch [8514/20000], Loss: -9.410064697265625, Learning Rate: 0.01\n",
      "Epoch [8515/20000], Loss: -9.413803100585938, Learning Rate: 0.01\n",
      "Epoch [8516/20000], Loss: -9.432281494140625, Learning Rate: 0.01\n",
      "Epoch [8517/20000], Loss: -9.45355224609375, Learning Rate: 0.01\n",
      "Epoch [8518/20000], Loss: -9.465621948242188, Learning Rate: 0.01\n",
      "Epoch [8519/20000], Loss: -9.465789794921875, Learning Rate: 0.01\n",
      "Epoch [8520/20000], Loss: -9.46051025390625, Learning Rate: 0.01\n",
      "Epoch [8521/20000], Loss: -9.458587646484375, Learning Rate: 0.01\n",
      "Epoch [8522/20000], Loss: -9.465011596679688, Learning Rate: 0.01\n",
      "Epoch [8523/20000], Loss: -9.477340698242188, Learning Rate: 0.01\n",
      "Epoch [8524/20000], Loss: -9.489151000976562, Learning Rate: 0.01\n",
      "Epoch [8525/20000], Loss: -9.495697021484375, Learning Rate: 0.01\n",
      "Epoch [8526/20000], Loss: -9.496795654296875, Learning Rate: 0.01\n",
      "Epoch [8527/20000], Loss: -9.496185302734375, Learning Rate: 0.01\n",
      "Epoch [8528/20000], Loss: -9.497848510742188, Learning Rate: 0.01\n",
      "Epoch [8529/20000], Loss: -9.503738403320312, Learning Rate: 0.01\n",
      "Epoch [8530/20000], Loss: -9.511734008789062, Learning Rate: 0.01\n",
      "Epoch [8531/20000], Loss: -9.519256591796875, Learning Rate: 0.01\n",
      "Epoch [8532/20000], Loss: -9.524093627929688, Learning Rate: 0.01\n",
      "Epoch [8533/20000], Loss: -9.5263671875, Learning Rate: 0.01\n",
      "Epoch [8534/20000], Loss: -9.527908325195312, Learning Rate: 0.01\n",
      "Epoch [8535/20000], Loss: -9.530563354492188, Learning Rate: 0.01\n",
      "Epoch [8536/20000], Loss: -9.5350341796875, Learning Rate: 0.01\n",
      "Epoch [8537/20000], Loss: -9.5404052734375, Learning Rate: 0.01\n",
      "Epoch [8538/20000], Loss: -9.545501708984375, Learning Rate: 0.01\n",
      "Epoch [8539/20000], Loss: -9.549026489257812, Learning Rate: 0.01\n",
      "Epoch [8540/20000], Loss: -9.551177978515625, Learning Rate: 0.01\n",
      "Epoch [8541/20000], Loss: -9.552536010742188, Learning Rate: 0.01\n",
      "Epoch [8542/20000], Loss: -9.553924560546875, Learning Rate: 0.01\n",
      "Epoch [8543/20000], Loss: -9.555526733398438, Learning Rate: 0.01\n",
      "Epoch [8544/20000], Loss: -9.556961059570312, Learning Rate: 0.01\n",
      "Epoch [8545/20000], Loss: -9.557266235351562, Learning Rate: 0.01\n",
      "Epoch [8546/20000], Loss: -9.55560302734375, Learning Rate: 0.01\n",
      "Epoch [8547/20000], Loss: -9.551422119140625, Learning Rate: 0.01\n",
      "Epoch [8548/20000], Loss: -9.544174194335938, Learning Rate: 0.01\n",
      "Epoch [8549/20000], Loss: -9.533615112304688, Learning Rate: 0.01\n",
      "Epoch [8550/20000], Loss: -9.518402099609375, Learning Rate: 0.01\n",
      "Epoch [8551/20000], Loss: -9.496826171875, Learning Rate: 0.01\n",
      "Epoch [8552/20000], Loss: -9.466415405273438, Learning Rate: 0.01\n",
      "Epoch [8553/20000], Loss: -9.423492431640625, Learning Rate: 0.01\n",
      "Epoch [8554/20000], Loss: -9.363510131835938, Learning Rate: 0.01\n",
      "Epoch [8555/20000], Loss: -9.280609130859375, Learning Rate: 0.01\n",
      "Epoch [8556/20000], Loss: -9.167327880859375, Learning Rate: 0.01\n",
      "Epoch [8557/20000], Loss: -9.014022827148438, Learning Rate: 0.01\n",
      "Epoch [8558/20000], Loss: -8.809158325195312, Learning Rate: 0.01\n",
      "Epoch [8559/20000], Loss: -8.539688110351562, Learning Rate: 0.01\n",
      "Epoch [8560/20000], Loss: -8.193862915039062, Learning Rate: 0.01\n",
      "Epoch [8561/20000], Loss: -7.7655029296875, Learning Rate: 0.01\n",
      "Epoch [8562/20000], Loss: -7.262939453125, Learning Rate: 0.01\n",
      "Epoch [8563/20000], Loss: -6.71905517578125, Learning Rate: 0.01\n",
      "Epoch [8564/20000], Loss: -6.2042083740234375, Learning Rate: 0.01\n",
      "Epoch [8565/20000], Loss: -5.8288116455078125, Learning Rate: 0.01\n",
      "Epoch [8566/20000], Loss: -5.7260284423828125, Learning Rate: 0.01\n",
      "Epoch [8567/20000], Loss: -6.0013275146484375, Learning Rate: 0.01\n",
      "Epoch [8568/20000], Loss: -6.6674652099609375, Learning Rate: 0.01\n",
      "Epoch [8569/20000], Loss: -7.5982513427734375, Learning Rate: 0.01\n",
      "Epoch [8570/20000], Loss: -8.556915283203125, Learning Rate: 0.01\n",
      "Epoch [8571/20000], Loss: -9.2923583984375, Learning Rate: 0.01\n",
      "Epoch [8572/20000], Loss: -9.649307250976562, Learning Rate: 0.01\n",
      "Epoch [8573/20000], Loss: -9.620697021484375, Learning Rate: 0.01\n",
      "Epoch [8574/20000], Loss: -9.325607299804688, Learning Rate: 0.01\n",
      "Epoch [8575/20000], Loss: -8.94500732421875, Learning Rate: 0.01\n",
      "Epoch [8576/20000], Loss: -8.651870727539062, Learning Rate: 0.01\n",
      "Epoch [8577/20000], Loss: -8.559402465820312, Learning Rate: 0.01\n",
      "Epoch [8578/20000], Loss: -8.691726684570312, Learning Rate: 0.01\n",
      "Epoch [8579/20000], Loss: -8.986236572265625, Learning Rate: 0.01\n",
      "Epoch [8580/20000], Loss: -9.3270263671875, Learning Rate: 0.01\n",
      "Epoch [8581/20000], Loss: -9.596847534179688, Learning Rate: 0.01\n",
      "Epoch [8582/20000], Loss: -9.72381591796875, Learning Rate: 0.01\n",
      "Epoch [8583/20000], Loss: -9.702301025390625, Learning Rate: 0.01\n",
      "Epoch [8584/20000], Loss: -9.5821533203125, Learning Rate: 0.01\n",
      "Epoch [8585/20000], Loss: -9.439773559570312, Learning Rate: 0.01\n",
      "Epoch [8586/20000], Loss: -9.344940185546875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8587/20000], Loss: -9.335647583007812, Learning Rate: 0.01\n",
      "Epoch [8588/20000], Loss: -9.410171508789062, Learning Rate: 0.01\n",
      "Epoch [8589/20000], Loss: -9.534286499023438, Learning Rate: 0.01\n",
      "Epoch [8590/20000], Loss: -9.660125732421875, Learning Rate: 0.01\n",
      "Epoch [8591/20000], Loss: -9.747222900390625, Learning Rate: 0.01\n",
      "Epoch [8592/20000], Loss: -9.777084350585938, Learning Rate: 0.01\n",
      "Epoch [8593/20000], Loss: -9.755905151367188, Learning Rate: 0.01\n",
      "Epoch [8594/20000], Loss: -9.707305908203125, Learning Rate: 0.01\n",
      "Epoch [8595/20000], Loss: -9.659820556640625, Learning Rate: 0.01\n",
      "Epoch [8596/20000], Loss: -9.635696411132812, Learning Rate: 0.01\n",
      "Epoch [8597/20000], Loss: -9.643646240234375, Learning Rate: 0.01\n",
      "Epoch [8598/20000], Loss: -9.678955078125, Learning Rate: 0.01\n",
      "Epoch [8599/20000], Loss: -9.727691650390625, Learning Rate: 0.01\n",
      "Epoch [8600/20000], Loss: -9.773712158203125, Learning Rate: 0.01\n",
      "Epoch [8601/20000], Loss: -9.805145263671875, Learning Rate: 0.01\n",
      "Epoch [8602/20000], Loss: -9.817031860351562, Learning Rate: 0.01\n",
      "Epoch [8603/20000], Loss: -9.812332153320312, Learning Rate: 0.01\n",
      "Epoch [8604/20000], Loss: -9.79864501953125, Learning Rate: 0.01\n",
      "Epoch [8605/20000], Loss: -9.784805297851562, Learning Rate: 0.01\n",
      "Epoch [8606/20000], Loss: -9.777877807617188, Learning Rate: 0.01\n",
      "Epoch [8607/20000], Loss: -9.781173706054688, Learning Rate: 0.01\n",
      "Epoch [8608/20000], Loss: -9.793777465820312, Learning Rate: 0.01\n",
      "Epoch [8609/20000], Loss: -9.812088012695312, Learning Rate: 0.01\n",
      "Epoch [8610/20000], Loss: -9.831344604492188, Learning Rate: 0.01\n",
      "Epoch [8611/20000], Loss: -9.847137451171875, Learning Rate: 0.01\n",
      "Epoch [8612/20000], Loss: -9.857177734375, Learning Rate: 0.01\n",
      "Epoch [8613/20000], Loss: -9.861328125, Learning Rate: 0.01\n",
      "Epoch [8614/20000], Loss: -9.861068725585938, Learning Rate: 0.01\n",
      "Epoch [8615/20000], Loss: -9.858901977539062, Learning Rate: 0.01\n",
      "Epoch [8616/20000], Loss: -9.857452392578125, Learning Rate: 0.01\n",
      "Epoch [8617/20000], Loss: -9.858535766601562, Learning Rate: 0.01\n",
      "Epoch [8618/20000], Loss: -9.862777709960938, Learning Rate: 0.01\n",
      "Epoch [8619/20000], Loss: -9.86968994140625, Learning Rate: 0.01\n",
      "Epoch [8620/20000], Loss: -9.878326416015625, Learning Rate: 0.01\n",
      "Epoch [8621/20000], Loss: -9.887237548828125, Learning Rate: 0.01\n",
      "Epoch [8622/20000], Loss: -9.895248413085938, Learning Rate: 0.01\n",
      "Epoch [8623/20000], Loss: -9.901565551757812, Learning Rate: 0.01\n",
      "Epoch [8624/20000], Loss: -9.90625, Learning Rate: 0.01\n",
      "Epoch [8625/20000], Loss: -9.909439086914062, Learning Rate: 0.01\n",
      "Epoch [8626/20000], Loss: -9.911865234375, Learning Rate: 0.01\n",
      "Epoch [8627/20000], Loss: -9.914031982421875, Learning Rate: 0.01\n",
      "Epoch [8628/20000], Loss: -9.916702270507812, Learning Rate: 0.01\n",
      "Epoch [8629/20000], Loss: -9.920135498046875, Learning Rate: 0.01\n",
      "Epoch [8630/20000], Loss: -9.92431640625, Learning Rate: 0.01\n",
      "Epoch [8631/20000], Loss: -9.92913818359375, Learning Rate: 0.01\n",
      "Epoch [8632/20000], Loss: -9.934585571289062, Learning Rate: 0.01\n",
      "Epoch [8633/20000], Loss: -9.939956665039062, Learning Rate: 0.01\n",
      "Epoch [8634/20000], Loss: -9.945159912109375, Learning Rate: 0.01\n",
      "Epoch [8635/20000], Loss: -9.949996948242188, Learning Rate: 0.01\n",
      "Epoch [8636/20000], Loss: -9.9544677734375, Learning Rate: 0.01\n",
      "Epoch [8637/20000], Loss: -9.958572387695312, Learning Rate: 0.01\n",
      "Epoch [8638/20000], Loss: -9.962356567382812, Learning Rate: 0.01\n",
      "Epoch [8639/20000], Loss: -9.966033935546875, Learning Rate: 0.01\n",
      "Epoch [8640/20000], Loss: -9.969467163085938, Learning Rate: 0.01\n",
      "Epoch [8641/20000], Loss: -9.97308349609375, Learning Rate: 0.01\n",
      "Epoch [8642/20000], Loss: -9.97698974609375, Learning Rate: 0.01\n",
      "Epoch [8643/20000], Loss: -9.98095703125, Learning Rate: 0.01\n",
      "Epoch [8644/20000], Loss: -9.985061645507812, Learning Rate: 0.01\n",
      "Epoch [8645/20000], Loss: -9.989395141601562, Learning Rate: 0.01\n",
      "Epoch [8646/20000], Loss: -9.9937744140625, Learning Rate: 0.01\n",
      "Epoch [8647/20000], Loss: -9.998092651367188, Learning Rate: 0.01\n",
      "Epoch [8648/20000], Loss: -10.00238037109375, Learning Rate: 0.01\n",
      "Epoch [8649/20000], Loss: -10.00665283203125, Learning Rate: 0.01\n",
      "Epoch [8650/20000], Loss: -10.01080322265625, Learning Rate: 0.01\n",
      "Epoch [8651/20000], Loss: -10.014938354492188, Learning Rate: 0.01\n",
      "Epoch [8652/20000], Loss: -10.018966674804688, Learning Rate: 0.01\n",
      "Epoch [8653/20000], Loss: -10.02301025390625, Learning Rate: 0.01\n",
      "Epoch [8654/20000], Loss: -10.026931762695312, Learning Rate: 0.01\n",
      "Epoch [8655/20000], Loss: -10.030868530273438, Learning Rate: 0.01\n",
      "Epoch [8656/20000], Loss: -10.034805297851562, Learning Rate: 0.01\n",
      "Epoch [8657/20000], Loss: -10.038833618164062, Learning Rate: 0.01\n",
      "Epoch [8658/20000], Loss: -10.04278564453125, Learning Rate: 0.01\n",
      "Epoch [8659/20000], Loss: -10.046768188476562, Learning Rate: 0.01\n",
      "Epoch [8660/20000], Loss: -10.050796508789062, Learning Rate: 0.01\n",
      "Epoch [8661/20000], Loss: -10.054901123046875, Learning Rate: 0.01\n",
      "Epoch [8662/20000], Loss: -10.059051513671875, Learning Rate: 0.01\n",
      "Epoch [8663/20000], Loss: -10.063095092773438, Learning Rate: 0.01\n",
      "Epoch [8664/20000], Loss: -10.067230224609375, Learning Rate: 0.01\n",
      "Epoch [8665/20000], Loss: -10.071273803710938, Learning Rate: 0.01\n",
      "Epoch [8666/20000], Loss: -10.075363159179688, Learning Rate: 0.01\n",
      "Epoch [8667/20000], Loss: -10.079483032226562, Learning Rate: 0.01\n",
      "Epoch [8668/20000], Loss: -10.083572387695312, Learning Rate: 0.01\n",
      "Epoch [8669/20000], Loss: -10.0875244140625, Learning Rate: 0.01\n",
      "Epoch [8670/20000], Loss: -10.091629028320312, Learning Rate: 0.01\n",
      "Epoch [8671/20000], Loss: -10.095672607421875, Learning Rate: 0.01\n",
      "Epoch [8672/20000], Loss: -10.099700927734375, Learning Rate: 0.01\n",
      "Epoch [8673/20000], Loss: -10.103759765625, Learning Rate: 0.01\n",
      "Epoch [8674/20000], Loss: -10.10784912109375, Learning Rate: 0.01\n",
      "Epoch [8675/20000], Loss: -10.111846923828125, Learning Rate: 0.01\n",
      "Epoch [8676/20000], Loss: -10.115859985351562, Learning Rate: 0.01\n",
      "Epoch [8677/20000], Loss: -10.119979858398438, Learning Rate: 0.01\n",
      "Epoch [8678/20000], Loss: -10.123916625976562, Learning Rate: 0.01\n",
      "Epoch [8679/20000], Loss: -10.1280517578125, Learning Rate: 0.01\n",
      "Epoch [8680/20000], Loss: -10.132003784179688, Learning Rate: 0.01\n",
      "Epoch [8681/20000], Loss: -10.136077880859375, Learning Rate: 0.01\n",
      "Epoch [8682/20000], Loss: -10.140090942382812, Learning Rate: 0.01\n",
      "Epoch [8683/20000], Loss: -10.144195556640625, Learning Rate: 0.01\n",
      "Epoch [8684/20000], Loss: -10.148223876953125, Learning Rate: 0.01\n",
      "Epoch [8685/20000], Loss: -10.152236938476562, Learning Rate: 0.01\n",
      "Epoch [8686/20000], Loss: -10.156265258789062, Learning Rate: 0.01\n",
      "Epoch [8687/20000], Loss: -10.1602783203125, Learning Rate: 0.01\n",
      "Epoch [8688/20000], Loss: -10.164321899414062, Learning Rate: 0.01\n",
      "Epoch [8689/20000], Loss: -10.168426513671875, Learning Rate: 0.01\n",
      "Epoch [8690/20000], Loss: -10.17242431640625, Learning Rate: 0.01\n",
      "Epoch [8691/20000], Loss: -10.176437377929688, Learning Rate: 0.01\n",
      "Epoch [8692/20000], Loss: -10.180526733398438, Learning Rate: 0.01\n",
      "Epoch [8693/20000], Loss: -10.1845703125, Learning Rate: 0.01\n",
      "Epoch [8694/20000], Loss: -10.188507080078125, Learning Rate: 0.01\n",
      "Epoch [8695/20000], Loss: -10.192611694335938, Learning Rate: 0.01\n",
      "Epoch [8696/20000], Loss: -10.196578979492188, Learning Rate: 0.01\n",
      "Epoch [8697/20000], Loss: -10.200592041015625, Learning Rate: 0.01\n",
      "Epoch [8698/20000], Loss: -10.20452880859375, Learning Rate: 0.01\n",
      "Epoch [8699/20000], Loss: -10.20849609375, Learning Rate: 0.01\n",
      "Epoch [8700/20000], Loss: -10.212554931640625, Learning Rate: 0.01\n",
      "Epoch [8701/20000], Loss: -10.216400146484375, Learning Rate: 0.01\n",
      "Epoch [8702/20000], Loss: -10.220352172851562, Learning Rate: 0.01\n",
      "Epoch [8703/20000], Loss: -10.224166870117188, Learning Rate: 0.01\n",
      "Epoch [8704/20000], Loss: -10.228042602539062, Learning Rate: 0.01\n",
      "Epoch [8705/20000], Loss: -10.231643676757812, Learning Rate: 0.01\n",
      "Epoch [8706/20000], Loss: -10.23529052734375, Learning Rate: 0.01\n",
      "Epoch [8707/20000], Loss: -10.2386474609375, Learning Rate: 0.01\n",
      "Epoch [8708/20000], Loss: -10.241973876953125, Learning Rate: 0.01\n",
      "Epoch [8709/20000], Loss: -10.24493408203125, Learning Rate: 0.01\n",
      "Epoch [8710/20000], Loss: -10.247573852539062, Learning Rate: 0.01\n",
      "Epoch [8711/20000], Loss: -10.249649047851562, Learning Rate: 0.01\n",
      "Epoch [8712/20000], Loss: -10.251068115234375, Learning Rate: 0.01\n",
      "Epoch [8713/20000], Loss: -10.251617431640625, Learning Rate: 0.01\n",
      "Epoch [8714/20000], Loss: -10.250732421875, Learning Rate: 0.01\n",
      "Epoch [8715/20000], Loss: -10.248184204101562, Learning Rate: 0.01\n",
      "Epoch [8716/20000], Loss: -10.243057250976562, Learning Rate: 0.01\n",
      "Epoch [8717/20000], Loss: -10.234359741210938, Learning Rate: 0.01\n",
      "Epoch [8718/20000], Loss: -10.220733642578125, Learning Rate: 0.01\n",
      "Epoch [8719/20000], Loss: -10.200119018554688, Learning Rate: 0.01\n",
      "Epoch [8720/20000], Loss: -10.169342041015625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8721/20000], Loss: -10.124542236328125, Learning Rate: 0.01\n",
      "Epoch [8722/20000], Loss: -10.05975341796875, Learning Rate: 0.01\n",
      "Epoch [8723/20000], Loss: -9.96661376953125, Learning Rate: 0.01\n",
      "Epoch [8724/20000], Loss: -9.833724975585938, Learning Rate: 0.01\n",
      "Epoch [8725/20000], Loss: -9.645706176757812, Learning Rate: 0.01\n",
      "Epoch [8726/20000], Loss: -9.382400512695312, Learning Rate: 0.01\n",
      "Epoch [8727/20000], Loss: -9.019241333007812, Learning Rate: 0.01\n",
      "Epoch [8728/20000], Loss: -8.529464721679688, Learning Rate: 0.01\n",
      "Epoch [8729/20000], Loss: -7.8911590576171875, Learning Rate: 0.01\n",
      "Epoch [8730/20000], Loss: -7.10150146484375, Learning Rate: 0.01\n",
      "Epoch [8731/20000], Loss: -6.2005462646484375, Learning Rate: 0.01\n",
      "Epoch [8732/20000], Loss: -5.3006591796875, Learning Rate: 0.01\n",
      "Epoch [8733/20000], Loss: -4.605133056640625, Learning Rate: 0.01\n",
      "Epoch [8734/20000], Loss: -4.375457763671875, Learning Rate: 0.01\n",
      "Epoch [8735/20000], Loss: -4.8261566162109375, Learning Rate: 0.01\n",
      "Epoch [8736/20000], Loss: -5.9614105224609375, Learning Rate: 0.01\n",
      "Epoch [8737/20000], Loss: -7.4907379150390625, Learning Rate: 0.01\n",
      "Epoch [8738/20000], Loss: -8.921234130859375, Learning Rate: 0.01\n",
      "Epoch [8739/20000], Loss: -9.816879272460938, Learning Rate: 0.01\n",
      "Epoch [8740/20000], Loss: -10.017349243164062, Learning Rate: 0.01\n",
      "Epoch [8741/20000], Loss: -9.679397583007812, Learning Rate: 0.01\n",
      "Epoch [8742/20000], Loss: -9.1510009765625, Learning Rate: 0.01\n",
      "Epoch [8743/20000], Loss: -8.783126831054688, Learning Rate: 0.01\n",
      "Epoch [8744/20000], Loss: -8.7757568359375, Learning Rate: 0.01\n",
      "Epoch [8745/20000], Loss: -9.105636596679688, Learning Rate: 0.01\n",
      "Epoch [8746/20000], Loss: -9.572891235351562, Learning Rate: 0.01\n",
      "Epoch [8747/20000], Loss: -9.937301635742188, Learning Rate: 0.01\n",
      "Epoch [8748/20000], Loss: -10.062271118164062, Learning Rate: 0.01\n",
      "Epoch [8749/20000], Loss: -9.971282958984375, Learning Rate: 0.01\n",
      "Epoch [8750/20000], Loss: -9.804000854492188, Learning Rate: 0.01\n",
      "Epoch [8751/20000], Loss: -9.716842651367188, Learning Rate: 0.01\n",
      "Epoch [8752/20000], Loss: -9.78912353515625, Learning Rate: 0.01\n",
      "Epoch [8753/20000], Loss: -9.990264892578125, Learning Rate: 0.01\n",
      "Epoch [8754/20000], Loss: -10.210952758789062, Learning Rate: 0.01\n",
      "Epoch [8755/20000], Loss: -10.341064453125, Learning Rate: 0.01\n",
      "Epoch [8756/20000], Loss: -10.335205078125, Learning Rate: 0.01\n",
      "Epoch [8757/20000], Loss: -10.230300903320312, Learning Rate: 0.01\n",
      "Epoch [8758/20000], Loss: -10.113174438476562, Learning Rate: 0.01\n",
      "Epoch [8759/20000], Loss: -10.064590454101562, Learning Rate: 0.01\n",
      "Epoch [8760/20000], Loss: -10.116806030273438, Learning Rate: 0.01\n",
      "Epoch [8761/20000], Loss: -10.242294311523438, Learning Rate: 0.01\n",
      "Epoch [8762/20000], Loss: -10.377487182617188, Learning Rate: 0.01\n",
      "Epoch [8763/20000], Loss: -10.462570190429688, Learning Rate: 0.01\n",
      "Epoch [8764/20000], Loss: -10.473190307617188, Learning Rate: 0.01\n",
      "Epoch [8765/20000], Loss: -10.42645263671875, Learning Rate: 0.01\n",
      "Epoch [8766/20000], Loss: -10.364120483398438, Learning Rate: 0.01\n",
      "Epoch [8767/20000], Loss: -10.327056884765625, Learning Rate: 0.01\n",
      "Epoch [8768/20000], Loss: -10.334075927734375, Learning Rate: 0.01\n",
      "Epoch [8769/20000], Loss: -10.377838134765625, Learning Rate: 0.01\n",
      "Epoch [8770/20000], Loss: -10.433609008789062, Learning Rate: 0.01\n",
      "Epoch [8771/20000], Loss: -10.47637939453125, Learning Rate: 0.01\n",
      "Epoch [8772/20000], Loss: -10.493331909179688, Learning Rate: 0.01\n",
      "Epoch [8773/20000], Loss: -10.48779296875, Learning Rate: 0.01\n",
      "Epoch [8774/20000], Loss: -10.473464965820312, Learning Rate: 0.01\n",
      "Epoch [8775/20000], Loss: -10.465255737304688, Learning Rate: 0.01\n",
      "Epoch [8776/20000], Loss: -10.470703125, Learning Rate: 0.01\n",
      "Epoch [8777/20000], Loss: -10.488006591796875, Learning Rate: 0.01\n",
      "Epoch [8778/20000], Loss: -10.508987426757812, Learning Rate: 0.01\n",
      "Epoch [8779/20000], Loss: -10.524917602539062, Learning Rate: 0.01\n",
      "Epoch [8780/20000], Loss: -10.531219482421875, Learning Rate: 0.01\n",
      "Epoch [8781/20000], Loss: -10.529022216796875, Learning Rate: 0.01\n",
      "Epoch [8782/20000], Loss: -10.523941040039062, Learning Rate: 0.01\n",
      "Epoch [8783/20000], Loss: -10.5220947265625, Learning Rate: 0.01\n",
      "Epoch [8784/20000], Loss: -10.526931762695312, Learning Rate: 0.01\n",
      "Epoch [8785/20000], Loss: -10.538162231445312, Learning Rate: 0.01\n",
      "Epoch [8786/20000], Loss: -10.55242919921875, Learning Rate: 0.01\n",
      "Epoch [8787/20000], Loss: -10.565338134765625, Learning Rate: 0.01\n",
      "Epoch [8788/20000], Loss: -10.57366943359375, Learning Rate: 0.01\n",
      "Epoch [8789/20000], Loss: -10.577178955078125, Learning Rate: 0.01\n",
      "Epoch [8790/20000], Loss: -10.5771484375, Learning Rate: 0.01\n",
      "Epoch [8791/20000], Loss: -10.576339721679688, Learning Rate: 0.01\n",
      "Epoch [8792/20000], Loss: -10.577163696289062, Learning Rate: 0.01\n",
      "Epoch [8793/20000], Loss: -10.580902099609375, Learning Rate: 0.01\n",
      "Epoch [8794/20000], Loss: -10.587020874023438, Learning Rate: 0.01\n",
      "Epoch [8795/20000], Loss: -10.59429931640625, Learning Rate: 0.01\n",
      "Epoch [8796/20000], Loss: -10.6011962890625, Learning Rate: 0.01\n",
      "Epoch [8797/20000], Loss: -10.60687255859375, Learning Rate: 0.01\n",
      "Epoch [8798/20000], Loss: -10.610885620117188, Learning Rate: 0.01\n",
      "Epoch [8799/20000], Loss: -10.61395263671875, Learning Rate: 0.01\n",
      "Epoch [8800/20000], Loss: -10.616867065429688, Learning Rate: 0.01\n",
      "Epoch [8801/20000], Loss: -10.620376586914062, Learning Rate: 0.01\n",
      "Epoch [8802/20000], Loss: -10.62457275390625, Learning Rate: 0.01\n",
      "Epoch [8803/20000], Loss: -10.629791259765625, Learning Rate: 0.01\n",
      "Epoch [8804/20000], Loss: -10.635025024414062, Learning Rate: 0.01\n",
      "Epoch [8805/20000], Loss: -10.640060424804688, Learning Rate: 0.01\n",
      "Epoch [8806/20000], Loss: -10.644577026367188, Learning Rate: 0.01\n",
      "Epoch [8807/20000], Loss: -10.648406982421875, Learning Rate: 0.01\n",
      "Epoch [8808/20000], Loss: -10.651840209960938, Learning Rate: 0.01\n",
      "Epoch [8809/20000], Loss: -10.655059814453125, Learning Rate: 0.01\n",
      "Epoch [8810/20000], Loss: -10.658401489257812, Learning Rate: 0.01\n",
      "Epoch [8811/20000], Loss: -10.662124633789062, Learning Rate: 0.01\n",
      "Epoch [8812/20000], Loss: -10.666244506835938, Learning Rate: 0.01\n",
      "Epoch [8813/20000], Loss: -10.670608520507812, Learning Rate: 0.01\n",
      "Epoch [8814/20000], Loss: -10.675018310546875, Learning Rate: 0.01\n",
      "Epoch [8815/20000], Loss: -10.6793212890625, Learning Rate: 0.01\n",
      "Epoch [8816/20000], Loss: -10.6834716796875, Learning Rate: 0.01\n",
      "Epoch [8817/20000], Loss: -10.687408447265625, Learning Rate: 0.01\n",
      "Epoch [8818/20000], Loss: -10.691329956054688, Learning Rate: 0.01\n",
      "Epoch [8819/20000], Loss: -10.695098876953125, Learning Rate: 0.01\n",
      "Epoch [8820/20000], Loss: -10.698898315429688, Learning Rate: 0.01\n",
      "Epoch [8821/20000], Loss: -10.702789306640625, Learning Rate: 0.01\n",
      "Epoch [8822/20000], Loss: -10.706878662109375, Learning Rate: 0.01\n",
      "Epoch [8823/20000], Loss: -10.710968017578125, Learning Rate: 0.01\n",
      "Epoch [8824/20000], Loss: -10.715133666992188, Learning Rate: 0.01\n",
      "Epoch [8825/20000], Loss: -10.71923828125, Learning Rate: 0.01\n",
      "Epoch [8826/20000], Loss: -10.723342895507812, Learning Rate: 0.01\n",
      "Epoch [8827/20000], Loss: -10.727294921875, Learning Rate: 0.01\n",
      "Epoch [8828/20000], Loss: -10.73126220703125, Learning Rate: 0.01\n",
      "Epoch [8829/20000], Loss: -10.735000610351562, Learning Rate: 0.01\n",
      "Epoch [8830/20000], Loss: -10.738861083984375, Learning Rate: 0.01\n",
      "Epoch [8831/20000], Loss: -10.742782592773438, Learning Rate: 0.01\n",
      "Epoch [8832/20000], Loss: -10.746795654296875, Learning Rate: 0.01\n",
      "Epoch [8833/20000], Loss: -10.750701904296875, Learning Rate: 0.01\n",
      "Epoch [8834/20000], Loss: -10.754730224609375, Learning Rate: 0.01\n",
      "Epoch [8835/20000], Loss: -10.758712768554688, Learning Rate: 0.01\n",
      "Epoch [8836/20000], Loss: -10.76275634765625, Learning Rate: 0.01\n",
      "Epoch [8837/20000], Loss: -10.766647338867188, Learning Rate: 0.01\n",
      "Epoch [8838/20000], Loss: -10.770660400390625, Learning Rate: 0.01\n",
      "Epoch [8839/20000], Loss: -10.774505615234375, Learning Rate: 0.01\n",
      "Epoch [8840/20000], Loss: -10.778488159179688, Learning Rate: 0.01\n",
      "Epoch [8841/20000], Loss: -10.782424926757812, Learning Rate: 0.01\n",
      "Epoch [8842/20000], Loss: -10.786331176757812, Learning Rate: 0.01\n",
      "Epoch [8843/20000], Loss: -10.790206909179688, Learning Rate: 0.01\n",
      "Epoch [8844/20000], Loss: -10.794204711914062, Learning Rate: 0.01\n",
      "Epoch [8845/20000], Loss: -10.798095703125, Learning Rate: 0.01\n",
      "Epoch [8846/20000], Loss: -10.802154541015625, Learning Rate: 0.01\n",
      "Epoch [8847/20000], Loss: -10.8060302734375, Learning Rate: 0.01\n",
      "Epoch [8848/20000], Loss: -10.80999755859375, Learning Rate: 0.01\n",
      "Epoch [8849/20000], Loss: -10.81396484375, Learning Rate: 0.01\n",
      "Epoch [8850/20000], Loss: -10.817825317382812, Learning Rate: 0.01\n",
      "Epoch [8851/20000], Loss: -10.821762084960938, Learning Rate: 0.01\n",
      "Epoch [8852/20000], Loss: -10.825668334960938, Learning Rate: 0.01\n",
      "Epoch [8853/20000], Loss: -10.82965087890625, Learning Rate: 0.01\n",
      "Epoch [8854/20000], Loss: -10.83355712890625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8855/20000], Loss: -10.837493896484375, Learning Rate: 0.01\n",
      "Epoch [8856/20000], Loss: -10.841293334960938, Learning Rate: 0.01\n",
      "Epoch [8857/20000], Loss: -10.845184326171875, Learning Rate: 0.01\n",
      "Epoch [8858/20000], Loss: -10.849029541015625, Learning Rate: 0.01\n",
      "Epoch [8859/20000], Loss: -10.852630615234375, Learning Rate: 0.01\n",
      "Epoch [8860/20000], Loss: -10.856414794921875, Learning Rate: 0.01\n",
      "Epoch [8861/20000], Loss: -10.860076904296875, Learning Rate: 0.01\n",
      "Epoch [8862/20000], Loss: -10.86358642578125, Learning Rate: 0.01\n",
      "Epoch [8863/20000], Loss: -10.86700439453125, Learning Rate: 0.01\n",
      "Epoch [8864/20000], Loss: -10.869888305664062, Learning Rate: 0.01\n",
      "Epoch [8865/20000], Loss: -10.872665405273438, Learning Rate: 0.01\n",
      "Epoch [8866/20000], Loss: -10.874908447265625, Learning Rate: 0.01\n",
      "Epoch [8867/20000], Loss: -10.876449584960938, Learning Rate: 0.01\n",
      "Epoch [8868/20000], Loss: -10.876800537109375, Learning Rate: 0.01\n",
      "Epoch [8869/20000], Loss: -10.875762939453125, Learning Rate: 0.01\n",
      "Epoch [8870/20000], Loss: -10.87249755859375, Learning Rate: 0.01\n",
      "Epoch [8871/20000], Loss: -10.86602783203125, Learning Rate: 0.01\n",
      "Epoch [8872/20000], Loss: -10.854843139648438, Learning Rate: 0.01\n",
      "Epoch [8873/20000], Loss: -10.836776733398438, Learning Rate: 0.01\n",
      "Epoch [8874/20000], Loss: -10.808395385742188, Learning Rate: 0.01\n",
      "Epoch [8875/20000], Loss: -10.76507568359375, Learning Rate: 0.01\n",
      "Epoch [8876/20000], Loss: -10.699142456054688, Learning Rate: 0.01\n",
      "Epoch [8877/20000], Loss: -10.600326538085938, Learning Rate: 0.01\n",
      "Epoch [8878/20000], Loss: -10.452896118164062, Learning Rate: 0.01\n",
      "Epoch [8879/20000], Loss: -10.234329223632812, Learning Rate: 0.01\n",
      "Epoch [8880/20000], Loss: -9.913833618164062, Learning Rate: 0.01\n",
      "Epoch [8881/20000], Loss: -9.45050048828125, Learning Rate: 0.01\n",
      "Epoch [8882/20000], Loss: -8.795852661132812, Learning Rate: 0.01\n",
      "Epoch [8883/20000], Loss: -7.9035797119140625, Learning Rate: 0.01\n",
      "Epoch [8884/20000], Loss: -6.75445556640625, Learning Rate: 0.01\n",
      "Epoch [8885/20000], Loss: -5.407440185546875, Learning Rate: 0.01\n",
      "Epoch [8886/20000], Loss: -4.0639495849609375, Learning Rate: 0.01\n",
      "Epoch [8887/20000], Loss: -3.1190948486328125, Learning Rate: 0.01\n",
      "Epoch [8888/20000], Loss: -3.0770721435546875, Learning Rate: 0.01\n",
      "Epoch [8889/20000], Loss: -4.2794189453125, Learning Rate: 0.01\n",
      "Epoch [8890/20000], Loss: -6.5225067138671875, Learning Rate: 0.01\n",
      "Epoch [8891/20000], Loss: -8.977508544921875, Learning Rate: 0.01\n",
      "Epoch [8892/20000], Loss: -10.6304931640625, Learning Rate: 0.01\n",
      "Epoch [8893/20000], Loss: -10.940948486328125, Learning Rate: 0.01\n",
      "Epoch [8894/20000], Loss: -10.127349853515625, Learning Rate: 0.01\n",
      "Epoch [8895/20000], Loss: -8.93939208984375, Learning Rate: 0.01\n",
      "Epoch [8896/20000], Loss: -8.190902709960938, Learning Rate: 0.01\n",
      "Epoch [8897/20000], Loss: -8.334320068359375, Learning Rate: 0.01\n",
      "Epoch [8898/20000], Loss: -9.242416381835938, Learning Rate: 0.01\n",
      "Epoch [8899/20000], Loss: -10.327743530273438, Learning Rate: 0.01\n",
      "Epoch [8900/20000], Loss: -10.965438842773438, Learning Rate: 0.01\n",
      "Epoch [8901/20000], Loss: -10.909774780273438, Learning Rate: 0.01\n",
      "Epoch [8902/20000], Loss: -10.390579223632812, Learning Rate: 0.01\n",
      "Epoch [8903/20000], Loss: -9.881332397460938, Learning Rate: 0.01\n",
      "Epoch [8904/20000], Loss: -9.75946044921875, Learning Rate: 0.01\n",
      "Epoch [8905/20000], Loss: -10.083465576171875, Learning Rate: 0.01\n",
      "Epoch [8906/20000], Loss: -10.605484008789062, Learning Rate: 0.01\n",
      "Epoch [8907/20000], Loss: -10.983917236328125, Learning Rate: 0.01\n",
      "Epoch [8908/20000], Loss: -11.030929565429688, Learning Rate: 0.01\n",
      "Epoch [8909/20000], Loss: -10.810943603515625, Learning Rate: 0.01\n",
      "Epoch [8910/20000], Loss: -10.550384521484375, Learning Rate: 0.01\n",
      "Epoch [8911/20000], Loss: -10.4566650390625, Learning Rate: 0.01\n",
      "Epoch [8912/20000], Loss: -10.58624267578125, Learning Rate: 0.01\n",
      "Epoch [8913/20000], Loss: -10.83367919921875, Learning Rate: 0.01\n",
      "Epoch [8914/20000], Loss: -11.032379150390625, Learning Rate: 0.01\n",
      "Epoch [8915/20000], Loss: -11.077713012695312, Learning Rate: 0.01\n",
      "Epoch [8916/20000], Loss: -10.985794067382812, Learning Rate: 0.01\n",
      "Epoch [8917/20000], Loss: -10.859420776367188, Learning Rate: 0.01\n",
      "Epoch [8918/20000], Loss: -10.802871704101562, Learning Rate: 0.01\n",
      "Epoch [8919/20000], Loss: -10.853515625, Learning Rate: 0.01\n",
      "Epoch [8920/20000], Loss: -10.969696044921875, Learning Rate: 0.01\n",
      "Epoch [8921/20000], Loss: -11.073760986328125, Learning Rate: 0.01\n",
      "Epoch [8922/20000], Loss: -11.110000610351562, Learning Rate: 0.01\n",
      "Epoch [8923/20000], Loss: -11.076889038085938, Learning Rate: 0.01\n",
      "Epoch [8924/20000], Loss: -11.018081665039062, Learning Rate: 0.01\n",
      "Epoch [8925/20000], Loss: -10.984375, Learning Rate: 0.01\n",
      "Epoch [8926/20000], Loss: -11.000137329101562, Learning Rate: 0.01\n",
      "Epoch [8927/20000], Loss: -11.05291748046875, Learning Rate: 0.01\n",
      "Epoch [8928/20000], Loss: -11.108383178710938, Learning Rate: 0.01\n",
      "Epoch [8929/20000], Loss: -11.136825561523438, Learning Rate: 0.01\n",
      "Epoch [8930/20000], Loss: -11.130935668945312, Learning Rate: 0.01\n",
      "Epoch [8931/20000], Loss: -11.1065673828125, Learning Rate: 0.01\n",
      "Epoch [8932/20000], Loss: -11.087265014648438, Learning Rate: 0.01\n",
      "Epoch [8933/20000], Loss: -11.088851928710938, Learning Rate: 0.01\n",
      "Epoch [8934/20000], Loss: -11.110809326171875, Learning Rate: 0.01\n",
      "Epoch [8935/20000], Loss: -11.139968872070312, Learning Rate: 0.01\n",
      "Epoch [8936/20000], Loss: -11.161224365234375, Learning Rate: 0.01\n",
      "Epoch [8937/20000], Loss: -11.166900634765625, Learning Rate: 0.01\n",
      "Epoch [8938/20000], Loss: -11.160476684570312, Learning Rate: 0.01\n",
      "Epoch [8939/20000], Loss: -11.151412963867188, Learning Rate: 0.01\n",
      "Epoch [8940/20000], Loss: -11.149078369140625, Learning Rate: 0.01\n",
      "Epoch [8941/20000], Loss: -11.156845092773438, Learning Rate: 0.01\n",
      "Epoch [8942/20000], Loss: -11.17132568359375, Learning Rate: 0.01\n",
      "Epoch [8943/20000], Loss: -11.185806274414062, Learning Rate: 0.01\n",
      "Epoch [8944/20000], Loss: -11.19464111328125, Learning Rate: 0.01\n",
      "Epoch [8945/20000], Loss: -11.196975708007812, Learning Rate: 0.01\n",
      "Epoch [8946/20000], Loss: -11.195220947265625, Learning Rate: 0.01\n",
      "Epoch [8947/20000], Loss: -11.193878173828125, Learning Rate: 0.01\n",
      "Epoch [8948/20000], Loss: -11.196334838867188, Learning Rate: 0.01\n",
      "Epoch [8949/20000], Loss: -11.202926635742188, Learning Rate: 0.01\n",
      "Epoch [8950/20000], Loss: -11.211807250976562, Learning Rate: 0.01\n",
      "Epoch [8951/20000], Loss: -11.220046997070312, Learning Rate: 0.01\n",
      "Epoch [8952/20000], Loss: -11.225570678710938, Learning Rate: 0.01\n",
      "Epoch [8953/20000], Loss: -11.228286743164062, Learning Rate: 0.01\n",
      "Epoch [8954/20000], Loss: -11.229629516601562, Learning Rate: 0.01\n",
      "Epoch [8955/20000], Loss: -11.231231689453125, Learning Rate: 0.01\n",
      "Epoch [8956/20000], Loss: -11.234664916992188, Learning Rate: 0.01\n",
      "Epoch [8957/20000], Loss: -11.239776611328125, Learning Rate: 0.01\n",
      "Epoch [8958/20000], Loss: -11.24578857421875, Learning Rate: 0.01\n",
      "Epoch [8959/20000], Loss: -11.25146484375, Learning Rate: 0.01\n",
      "Epoch [8960/20000], Loss: -11.256195068359375, Learning Rate: 0.01\n",
      "Epoch [8961/20000], Loss: -11.259567260742188, Learning Rate: 0.01\n",
      "Epoch [8962/20000], Loss: -11.262374877929688, Learning Rate: 0.01\n",
      "Epoch [8963/20000], Loss: -11.265228271484375, Learning Rate: 0.01\n",
      "Epoch [8964/20000], Loss: -11.268814086914062, Learning Rate: 0.01\n",
      "Epoch [8965/20000], Loss: -11.272933959960938, Learning Rate: 0.01\n",
      "Epoch [8966/20000], Loss: -11.277633666992188, Learning Rate: 0.01\n",
      "Epoch [8967/20000], Loss: -11.282394409179688, Learning Rate: 0.01\n",
      "Epoch [8968/20000], Loss: -11.286773681640625, Learning Rate: 0.01\n",
      "Epoch [8969/20000], Loss: -11.290603637695312, Learning Rate: 0.01\n",
      "Epoch [8970/20000], Loss: -11.293991088867188, Learning Rate: 0.01\n",
      "Epoch [8971/20000], Loss: -11.297470092773438, Learning Rate: 0.01\n",
      "Epoch [8972/20000], Loss: -11.300949096679688, Learning Rate: 0.01\n",
      "Epoch [8973/20000], Loss: -11.3048095703125, Learning Rate: 0.01\n",
      "Epoch [8974/20000], Loss: -11.308868408203125, Learning Rate: 0.01\n",
      "Epoch [8975/20000], Loss: -11.313034057617188, Learning Rate: 0.01\n",
      "Epoch [8976/20000], Loss: -11.3172607421875, Learning Rate: 0.01\n",
      "Epoch [8977/20000], Loss: -11.321212768554688, Learning Rate: 0.01\n",
      "Epoch [8978/20000], Loss: -11.32501220703125, Learning Rate: 0.01\n",
      "Epoch [8979/20000], Loss: -11.32867431640625, Learning Rate: 0.01\n",
      "Epoch [8980/20000], Loss: -11.332351684570312, Learning Rate: 0.01\n",
      "Epoch [8981/20000], Loss: -11.33612060546875, Learning Rate: 0.01\n",
      "Epoch [8982/20000], Loss: -11.339935302734375, Learning Rate: 0.01\n",
      "Epoch [8983/20000], Loss: -11.343887329101562, Learning Rate: 0.01\n",
      "Epoch [8984/20000], Loss: -11.34783935546875, Learning Rate: 0.01\n",
      "Epoch [8985/20000], Loss: -11.351821899414062, Learning Rate: 0.01\n",
      "Epoch [8986/20000], Loss: -11.355865478515625, Learning Rate: 0.01\n",
      "Epoch [8987/20000], Loss: -11.359619140625, Learning Rate: 0.01\n",
      "Epoch [8988/20000], Loss: -11.363372802734375, Learning Rate: 0.01\n",
      "Epoch [8989/20000], Loss: -11.367080688476562, Learning Rate: 0.01\n",
      "Epoch [8990/20000], Loss: -11.370941162109375, Learning Rate: 0.01\n",
      "Epoch [8991/20000], Loss: -11.374786376953125, Learning Rate: 0.01\n",
      "Epoch [8992/20000], Loss: -11.378631591796875, Learning Rate: 0.01\n",
      "Epoch [8993/20000], Loss: -11.38250732421875, Learning Rate: 0.01\n",
      "Epoch [8994/20000], Loss: -11.386444091796875, Learning Rate: 0.01\n",
      "Epoch [8995/20000], Loss: -11.390228271484375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8996/20000], Loss: -11.394058227539062, Learning Rate: 0.01\n",
      "Epoch [8997/20000], Loss: -11.397979736328125, Learning Rate: 0.01\n",
      "Epoch [8998/20000], Loss: -11.401718139648438, Learning Rate: 0.01\n",
      "Epoch [8999/20000], Loss: -11.40557861328125, Learning Rate: 0.01\n",
      "Epoch [9000/20000], Loss: -11.409423828125, Learning Rate: 0.01\n",
      "Epoch [9001/20000], Loss: -11.413253784179688, Learning Rate: 0.01\n",
      "Epoch [9002/20000], Loss: -11.417083740234375, Learning Rate: 0.01\n",
      "Epoch [9003/20000], Loss: -11.420928955078125, Learning Rate: 0.01\n",
      "Epoch [9004/20000], Loss: -11.424835205078125, Learning Rate: 0.01\n",
      "Epoch [9005/20000], Loss: -11.428665161132812, Learning Rate: 0.01\n",
      "Epoch [9006/20000], Loss: -11.432586669921875, Learning Rate: 0.01\n",
      "Epoch [9007/20000], Loss: -11.436431884765625, Learning Rate: 0.01\n",
      "Epoch [9008/20000], Loss: -11.440200805664062, Learning Rate: 0.01\n",
      "Epoch [9009/20000], Loss: -11.444015502929688, Learning Rate: 0.01\n",
      "Epoch [9010/20000], Loss: -11.447860717773438, Learning Rate: 0.01\n",
      "Epoch [9011/20000], Loss: -11.45166015625, Learning Rate: 0.01\n",
      "Epoch [9012/20000], Loss: -11.455581665039062, Learning Rate: 0.01\n",
      "Epoch [9013/20000], Loss: -11.45941162109375, Learning Rate: 0.01\n",
      "Epoch [9014/20000], Loss: -11.4632568359375, Learning Rate: 0.01\n",
      "Epoch [9015/20000], Loss: -11.467117309570312, Learning Rate: 0.01\n",
      "Epoch [9016/20000], Loss: -11.47088623046875, Learning Rate: 0.01\n",
      "Epoch [9017/20000], Loss: -11.474655151367188, Learning Rate: 0.01\n",
      "Epoch [9018/20000], Loss: -11.478530883789062, Learning Rate: 0.01\n",
      "Epoch [9019/20000], Loss: -11.482254028320312, Learning Rate: 0.01\n",
      "Epoch [9020/20000], Loss: -11.4859619140625, Learning Rate: 0.01\n",
      "Epoch [9021/20000], Loss: -11.489608764648438, Learning Rate: 0.01\n",
      "Epoch [9022/20000], Loss: -11.49334716796875, Learning Rate: 0.01\n",
      "Epoch [9023/20000], Loss: -11.496932983398438, Learning Rate: 0.01\n",
      "Epoch [9024/20000], Loss: -11.500213623046875, Learning Rate: 0.01\n",
      "Epoch [9025/20000], Loss: -11.503509521484375, Learning Rate: 0.01\n",
      "Epoch [9026/20000], Loss: -11.50634765625, Learning Rate: 0.01\n",
      "Epoch [9027/20000], Loss: -11.50885009765625, Learning Rate: 0.01\n",
      "Epoch [9028/20000], Loss: -11.510696411132812, Learning Rate: 0.01\n",
      "Epoch [9029/20000], Loss: -11.511581420898438, Learning Rate: 0.01\n",
      "Epoch [9030/20000], Loss: -11.511093139648438, Learning Rate: 0.01\n",
      "Epoch [9031/20000], Loss: -11.50836181640625, Learning Rate: 0.01\n",
      "Epoch [9032/20000], Loss: -11.502517700195312, Learning Rate: 0.01\n",
      "Epoch [9033/20000], Loss: -11.491806030273438, Learning Rate: 0.01\n",
      "Epoch [9034/20000], Loss: -11.473709106445312, Learning Rate: 0.01\n",
      "Epoch [9035/20000], Loss: -11.444488525390625, Learning Rate: 0.01\n",
      "Epoch [9036/20000], Loss: -11.398300170898438, Learning Rate: 0.01\n",
      "Epoch [9037/20000], Loss: -11.326431274414062, Learning Rate: 0.01\n",
      "Epoch [9038/20000], Loss: -11.21600341796875, Learning Rate: 0.01\n",
      "Epoch [9039/20000], Loss: -11.047409057617188, Learning Rate: 0.01\n",
      "Epoch [9040/20000], Loss: -10.793472290039062, Learning Rate: 0.01\n",
      "Epoch [9041/20000], Loss: -10.416351318359375, Learning Rate: 0.01\n",
      "Epoch [9042/20000], Loss: -9.87109375, Learning Rate: 0.01\n",
      "Epoch [9043/20000], Loss: -9.112030029296875, Learning Rate: 0.01\n",
      "Epoch [9044/20000], Loss: -8.123138427734375, Learning Rate: 0.01\n",
      "Epoch [9045/20000], Loss: -6.9626617431640625, Learning Rate: 0.01\n",
      "Epoch [9046/20000], Loss: -5.84503173828125, Learning Rate: 0.01\n",
      "Epoch [9047/20000], Loss: -5.1580963134765625, Learning Rate: 0.01\n",
      "Epoch [9048/20000], Loss: -5.3744354248046875, Learning Rate: 0.01\n",
      "Epoch [9049/20000], Loss: -6.6903076171875, Learning Rate: 0.01\n",
      "Epoch [9050/20000], Loss: -8.722610473632812, Learning Rate: 0.01\n",
      "Epoch [9051/20000], Loss: -10.573028564453125, Learning Rate: 0.01\n",
      "Epoch [9052/20000], Loss: -11.438507080078125, Learning Rate: 0.01\n",
      "Epoch [9053/20000], Loss: -11.15802001953125, Learning Rate: 0.01\n",
      "Epoch [9054/20000], Loss: -10.239242553710938, Learning Rate: 0.01\n",
      "Epoch [9055/20000], Loss: -9.460220336914062, Learning Rate: 0.01\n",
      "Epoch [9056/20000], Loss: -9.3818359375, Learning Rate: 0.01\n",
      "Epoch [9057/20000], Loss: -10.036270141601562, Learning Rate: 0.01\n",
      "Epoch [9058/20000], Loss: -10.939117431640625, Learning Rate: 0.01\n",
      "Epoch [9059/20000], Loss: -11.490310668945312, Learning Rate: 0.01\n",
      "Epoch [9060/20000], Loss: -11.428329467773438, Learning Rate: 0.01\n",
      "Epoch [9061/20000], Loss: -10.969818115234375, Learning Rate: 0.01\n",
      "Epoch [9062/20000], Loss: -10.573379516601562, Learning Rate: 0.01\n",
      "Epoch [9063/20000], Loss: -10.572311401367188, Learning Rate: 0.01\n",
      "Epoch [9064/20000], Loss: -10.957534790039062, Learning Rate: 0.01\n",
      "Epoch [9065/20000], Loss: -11.427932739257812, Learning Rate: 0.01\n",
      "Epoch [9066/20000], Loss: -11.663116455078125, Learning Rate: 0.01\n",
      "Epoch [9067/20000], Loss: -11.567794799804688, Learning Rate: 0.01\n",
      "Epoch [9068/20000], Loss: -11.3045654296875, Learning Rate: 0.01\n",
      "Epoch [9069/20000], Loss: -11.12567138671875, Learning Rate: 0.01\n",
      "Epoch [9070/20000], Loss: -11.171417236328125, Learning Rate: 0.01\n",
      "Epoch [9071/20000], Loss: -11.388076782226562, Learning Rate: 0.01\n",
      "Epoch [9072/20000], Loss: -11.60211181640625, Learning Rate: 0.01\n",
      "Epoch [9073/20000], Loss: -11.674774169921875, Learning Rate: 0.01\n",
      "Epoch [9074/20000], Loss: -11.600189208984375, Learning Rate: 0.01\n",
      "Epoch [9075/20000], Loss: -11.483139038085938, Learning Rate: 0.01\n",
      "Epoch [9076/20000], Loss: -11.438217163085938, Learning Rate: 0.01\n",
      "Epoch [9077/20000], Loss: -11.501358032226562, Learning Rate: 0.01\n",
      "Epoch [9078/20000], Loss: -11.617935180664062, Learning Rate: 0.01\n",
      "Epoch [9079/20000], Loss: -11.701156616210938, Learning Rate: 0.01\n",
      "Epoch [9080/20000], Loss: -11.704315185546875, Learning Rate: 0.01\n",
      "Epoch [9081/20000], Loss: -11.649124145507812, Learning Rate: 0.01\n",
      "Epoch [9082/20000], Loss: -11.596176147460938, Learning Rate: 0.01\n",
      "Epoch [9083/20000], Loss: -11.593063354492188, Learning Rate: 0.01\n",
      "Epoch [9084/20000], Loss: -11.641403198242188, Learning Rate: 0.01\n",
      "Epoch [9085/20000], Loss: -11.704452514648438, Learning Rate: 0.01\n",
      "Epoch [9086/20000], Loss: -11.741287231445312, Learning Rate: 0.01\n",
      "Epoch [9087/20000], Loss: -11.73779296875, Learning Rate: 0.01\n",
      "Epoch [9088/20000], Loss: -11.710922241210938, Learning Rate: 0.01\n",
      "Epoch [9089/20000], Loss: -11.690872192382812, Learning Rate: 0.01\n",
      "Epoch [9090/20000], Loss: -11.69622802734375, Learning Rate: 0.01\n",
      "Epoch [9091/20000], Loss: -11.723052978515625, Learning Rate: 0.01\n",
      "Epoch [9092/20000], Loss: -11.752532958984375, Learning Rate: 0.01\n",
      "Epoch [9093/20000], Loss: -11.767318725585938, Learning Rate: 0.01\n",
      "Epoch [9094/20000], Loss: -11.763916015625, Learning Rate: 0.01\n",
      "Epoch [9095/20000], Loss: -11.75238037109375, Learning Rate: 0.01\n",
      "Epoch [9096/20000], Loss: -11.7467041015625, Learning Rate: 0.01\n",
      "Epoch [9097/20000], Loss: -11.7537841796875, Learning Rate: 0.01\n",
      "Epoch [9098/20000], Loss: -11.7705078125, Learning Rate: 0.01\n",
      "Epoch [9099/20000], Loss: -11.787261962890625, Learning Rate: 0.01\n",
      "Epoch [9100/20000], Loss: -11.79638671875, Learning Rate: 0.01\n",
      "Epoch [9101/20000], Loss: -11.796417236328125, Learning Rate: 0.01\n",
      "Epoch [9102/20000], Loss: -11.792617797851562, Learning Rate: 0.01\n",
      "Epoch [9103/20000], Loss: -11.791427612304688, Learning Rate: 0.01\n",
      "Epoch [9104/20000], Loss: -11.796035766601562, Learning Rate: 0.01\n",
      "Epoch [9105/20000], Loss: -11.805389404296875, Learning Rate: 0.01\n",
      "Epoch [9106/20000], Loss: -11.815017700195312, Learning Rate: 0.01\n",
      "Epoch [9107/20000], Loss: -11.821334838867188, Learning Rate: 0.01\n",
      "Epoch [9108/20000], Loss: -11.823577880859375, Learning Rate: 0.01\n",
      "Epoch [9109/20000], Loss: -11.823883056640625, Learning Rate: 0.01\n",
      "Epoch [9110/20000], Loss: -11.82513427734375, Learning Rate: 0.01\n",
      "Epoch [9111/20000], Loss: -11.8291015625, Learning Rate: 0.01\n",
      "Epoch [9112/20000], Loss: -11.835494995117188, Learning Rate: 0.01\n",
      "Epoch [9113/20000], Loss: -11.842269897460938, Learning Rate: 0.01\n",
      "Epoch [9114/20000], Loss: -11.847808837890625, Learning Rate: 0.01\n",
      "Epoch [9115/20000], Loss: -11.851348876953125, Learning Rate: 0.01\n",
      "Epoch [9116/20000], Loss: -11.853591918945312, Learning Rate: 0.01\n",
      "Epoch [9117/20000], Loss: -11.8558349609375, Learning Rate: 0.01\n",
      "Epoch [9118/20000], Loss: -11.859161376953125, Learning Rate: 0.01\n",
      "Epoch [9119/20000], Loss: -11.863677978515625, Learning Rate: 0.01\n",
      "Epoch [9120/20000], Loss: -11.86865234375, Learning Rate: 0.01\n",
      "Epoch [9121/20000], Loss: -11.873428344726562, Learning Rate: 0.01\n",
      "Epoch [9122/20000], Loss: -11.877288818359375, Learning Rate: 0.01\n",
      "Epoch [9123/20000], Loss: -11.880508422851562, Learning Rate: 0.01\n",
      "Epoch [9124/20000], Loss: -11.883392333984375, Learning Rate: 0.01\n",
      "Epoch [9125/20000], Loss: -11.886749267578125, Learning Rate: 0.01\n",
      "Epoch [9126/20000], Loss: -11.890518188476562, Learning Rate: 0.01\n",
      "Epoch [9127/20000], Loss: -11.894805908203125, Learning Rate: 0.01\n",
      "Epoch [9128/20000], Loss: -11.899078369140625, Learning Rate: 0.01\n",
      "Epoch [9129/20000], Loss: -11.903152465820312, Learning Rate: 0.01\n",
      "Epoch [9130/20000], Loss: -11.906875610351562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9131/20000], Loss: -11.910247802734375, Learning Rate: 0.01\n",
      "Epoch [9132/20000], Loss: -11.913619995117188, Learning Rate: 0.01\n",
      "Epoch [9133/20000], Loss: -11.917236328125, Learning Rate: 0.01\n",
      "Epoch [9134/20000], Loss: -11.921066284179688, Learning Rate: 0.01\n",
      "Epoch [9135/20000], Loss: -11.925064086914062, Learning Rate: 0.01\n",
      "Epoch [9136/20000], Loss: -11.928939819335938, Learning Rate: 0.01\n",
      "Epoch [9137/20000], Loss: -11.932861328125, Learning Rate: 0.01\n",
      "Epoch [9138/20000], Loss: -11.93646240234375, Learning Rate: 0.01\n",
      "Epoch [9139/20000], Loss: -11.940032958984375, Learning Rate: 0.01\n",
      "Epoch [9140/20000], Loss: -11.943634033203125, Learning Rate: 0.01\n",
      "Epoch [9141/20000], Loss: -11.947250366210938, Learning Rate: 0.01\n",
      "Epoch [9142/20000], Loss: -11.951019287109375, Learning Rate: 0.01\n",
      "Epoch [9143/20000], Loss: -11.954803466796875, Learning Rate: 0.01\n",
      "Epoch [9144/20000], Loss: -11.958648681640625, Learning Rate: 0.01\n",
      "Epoch [9145/20000], Loss: -11.962356567382812, Learning Rate: 0.01\n",
      "Epoch [9146/20000], Loss: -11.966018676757812, Learning Rate: 0.01\n",
      "Epoch [9147/20000], Loss: -11.969635009765625, Learning Rate: 0.01\n",
      "Epoch [9148/20000], Loss: -11.973281860351562, Learning Rate: 0.01\n",
      "Epoch [9149/20000], Loss: -11.9769287109375, Learning Rate: 0.01\n",
      "Epoch [9150/20000], Loss: -11.980743408203125, Learning Rate: 0.01\n",
      "Epoch [9151/20000], Loss: -11.984405517578125, Learning Rate: 0.01\n",
      "Epoch [9152/20000], Loss: -11.9881591796875, Learning Rate: 0.01\n",
      "Epoch [9153/20000], Loss: -11.991912841796875, Learning Rate: 0.01\n",
      "Epoch [9154/20000], Loss: -11.995620727539062, Learning Rate: 0.01\n",
      "Epoch [9155/20000], Loss: -11.999237060546875, Learning Rate: 0.01\n",
      "Epoch [9156/20000], Loss: -12.002883911132812, Learning Rate: 0.01\n",
      "Epoch [9157/20000], Loss: -12.006591796875, Learning Rate: 0.01\n",
      "Epoch [9158/20000], Loss: -12.010238647460938, Learning Rate: 0.01\n",
      "Epoch [9159/20000], Loss: -12.014022827148438, Learning Rate: 0.01\n",
      "Epoch [9160/20000], Loss: -12.017684936523438, Learning Rate: 0.01\n",
      "Epoch [9161/20000], Loss: -12.0213623046875, Learning Rate: 0.01\n",
      "Epoch [9162/20000], Loss: -12.025039672851562, Learning Rate: 0.01\n",
      "Epoch [9163/20000], Loss: -12.028701782226562, Learning Rate: 0.01\n",
      "Epoch [9164/20000], Loss: -12.032379150390625, Learning Rate: 0.01\n",
      "Epoch [9165/20000], Loss: -12.036026000976562, Learning Rate: 0.01\n",
      "Epoch [9166/20000], Loss: -12.039688110351562, Learning Rate: 0.01\n",
      "Epoch [9167/20000], Loss: -12.043319702148438, Learning Rate: 0.01\n",
      "Epoch [9168/20000], Loss: -12.046981811523438, Learning Rate: 0.01\n",
      "Epoch [9169/20000], Loss: -12.050567626953125, Learning Rate: 0.01\n",
      "Epoch [9170/20000], Loss: -12.054092407226562, Learning Rate: 0.01\n",
      "Epoch [9171/20000], Loss: -12.057586669921875, Learning Rate: 0.01\n",
      "Epoch [9172/20000], Loss: -12.06103515625, Learning Rate: 0.01\n",
      "Epoch [9173/20000], Loss: -12.064453125, Learning Rate: 0.01\n",
      "Epoch [9174/20000], Loss: -12.067672729492188, Learning Rate: 0.01\n",
      "Epoch [9175/20000], Loss: -12.070770263671875, Learning Rate: 0.01\n",
      "Epoch [9176/20000], Loss: -12.073623657226562, Learning Rate: 0.01\n",
      "Epoch [9177/20000], Loss: -12.076309204101562, Learning Rate: 0.01\n",
      "Epoch [9178/20000], Loss: -12.07843017578125, Learning Rate: 0.01\n",
      "Epoch [9179/20000], Loss: -12.080078125, Learning Rate: 0.01\n",
      "Epoch [9180/20000], Loss: -12.080886840820312, Learning Rate: 0.01\n",
      "Epoch [9181/20000], Loss: -12.08062744140625, Learning Rate: 0.01\n",
      "Epoch [9182/20000], Loss: -12.078781127929688, Learning Rate: 0.01\n",
      "Epoch [9183/20000], Loss: -12.0748291015625, Learning Rate: 0.01\n",
      "Epoch [9184/20000], Loss: -12.067581176757812, Learning Rate: 0.01\n",
      "Epoch [9185/20000], Loss: -12.05584716796875, Learning Rate: 0.01\n",
      "Epoch [9186/20000], Loss: -12.037567138671875, Learning Rate: 0.01\n",
      "Epoch [9187/20000], Loss: -12.009918212890625, Learning Rate: 0.01\n",
      "Epoch [9188/20000], Loss: -11.96893310546875, Learning Rate: 0.01\n",
      "Epoch [9189/20000], Loss: -11.908294677734375, Learning Rate: 0.01\n",
      "Epoch [9190/20000], Loss: -11.819900512695312, Learning Rate: 0.01\n",
      "Epoch [9191/20000], Loss: -11.691329956054688, Learning Rate: 0.01\n",
      "Epoch [9192/20000], Loss: -11.50616455078125, Learning Rate: 0.01\n",
      "Epoch [9193/20000], Loss: -11.242095947265625, Learning Rate: 0.01\n",
      "Epoch [9194/20000], Loss: -10.8709716796875, Learning Rate: 0.01\n",
      "Epoch [9195/20000], Loss: -10.360641479492188, Learning Rate: 0.01\n",
      "Epoch [9196/20000], Loss: -9.683212280273438, Learning Rate: 0.01\n",
      "Epoch [9197/20000], Loss: -8.830734252929688, Learning Rate: 0.01\n",
      "Epoch [9198/20000], Loss: -7.84564208984375, Learning Rate: 0.01\n",
      "Epoch [9199/20000], Loss: -6.8611602783203125, Learning Rate: 0.01\n",
      "Epoch [9200/20000], Loss: -6.1261444091796875, Learning Rate: 0.01\n",
      "Epoch [9201/20000], Loss: -5.9693145751953125, Learning Rate: 0.01\n",
      "Epoch [9202/20000], Loss: -6.6466522216796875, Learning Rate: 0.01\n",
      "Epoch [9203/20000], Loss: -8.12152099609375, Learning Rate: 0.01\n",
      "Epoch [9204/20000], Loss: -9.951187133789062, Learning Rate: 0.01\n",
      "Epoch [9205/20000], Loss: -11.468551635742188, Learning Rate: 0.01\n",
      "Epoch [9206/20000], Loss: -12.16326904296875, Learning Rate: 0.01\n",
      "Epoch [9207/20000], Loss: -11.961441040039062, Learning Rate: 0.01\n",
      "Epoch [9208/20000], Loss: -11.207199096679688, Learning Rate: 0.01\n",
      "Epoch [9209/20000], Loss: -10.4326171875, Learning Rate: 0.01\n",
      "Epoch [9210/20000], Loss: -10.088699340820312, Learning Rate: 0.01\n",
      "Epoch [9211/20000], Loss: -10.346466064453125, Learning Rate: 0.01\n",
      "Epoch [9212/20000], Loss: -11.035552978515625, Learning Rate: 0.01\n",
      "Epoch [9213/20000], Loss: -11.766952514648438, Learning Rate: 0.01\n",
      "Epoch [9214/20000], Loss: -12.180435180664062, Learning Rate: 0.01\n",
      "Epoch [9215/20000], Loss: -12.15130615234375, Learning Rate: 0.01\n",
      "Epoch [9216/20000], Loss: -11.821304321289062, Learning Rate: 0.01\n",
      "Epoch [9217/20000], Loss: -11.46917724609375, Learning Rate: 0.01\n",
      "Epoch [9218/20000], Loss: -11.332138061523438, Learning Rate: 0.01\n",
      "Epoch [9219/20000], Loss: -11.4803466796875, Learning Rate: 0.01\n",
      "Epoch [9220/20000], Loss: -11.805023193359375, Learning Rate: 0.01\n",
      "Epoch [9221/20000], Loss: -12.111221313476562, Learning Rate: 0.01\n",
      "Epoch [9222/20000], Loss: -12.247177124023438, Learning Rate: 0.01\n",
      "Epoch [9223/20000], Loss: -12.18756103515625, Learning Rate: 0.01\n",
      "Epoch [9224/20000], Loss: -12.023910522460938, Learning Rate: 0.01\n",
      "Epoch [9225/20000], Loss: -11.888702392578125, Learning Rate: 0.01\n",
      "Epoch [9226/20000], Loss: -11.871292114257812, Learning Rate: 0.01\n",
      "Epoch [9227/20000], Loss: -11.974472045898438, Learning Rate: 0.01\n",
      "Epoch [9228/20000], Loss: -12.1290283203125, Learning Rate: 0.01\n",
      "Epoch [9229/20000], Loss: -12.247467041015625, Learning Rate: 0.01\n",
      "Epoch [9230/20000], Loss: -12.279327392578125, Learning Rate: 0.01\n",
      "Epoch [9231/20000], Loss: -12.233016967773438, Learning Rate: 0.01\n",
      "Epoch [9232/20000], Loss: -12.1595458984375, Learning Rate: 0.01\n",
      "Epoch [9233/20000], Loss: -12.114898681640625, Learning Rate: 0.01\n",
      "Epoch [9234/20000], Loss: -12.127288818359375, Learning Rate: 0.01\n",
      "Epoch [9235/20000], Loss: -12.18621826171875, Learning Rate: 0.01\n",
      "Epoch [9236/20000], Loss: -12.256423950195312, Learning Rate: 0.01\n",
      "Epoch [9237/20000], Loss: -12.302413940429688, Learning Rate: 0.01\n",
      "Epoch [9238/20000], Loss: -12.30889892578125, Learning Rate: 0.01\n",
      "Epoch [9239/20000], Loss: -12.285049438476562, Learning Rate: 0.01\n",
      "Epoch [9240/20000], Loss: -12.255279541015625, Learning Rate: 0.01\n",
      "Epoch [9241/20000], Loss: -12.241836547851562, Learning Rate: 0.01\n",
      "Epoch [9242/20000], Loss: -12.253402709960938, Learning Rate: 0.01\n",
      "Epoch [9243/20000], Loss: -12.283340454101562, Learning Rate: 0.01\n",
      "Epoch [9244/20000], Loss: -12.315826416015625, Learning Rate: 0.01\n",
      "Epoch [9245/20000], Loss: -12.336624145507812, Learning Rate: 0.01\n",
      "Epoch [9246/20000], Loss: -12.340377807617188, Learning Rate: 0.01\n",
      "Epoch [9247/20000], Loss: -12.331756591796875, Learning Rate: 0.01\n",
      "Epoch [9248/20000], Loss: -12.32073974609375, Learning Rate: 0.01\n",
      "Epoch [9249/20000], Loss: -12.316650390625, Learning Rate: 0.01\n",
      "Epoch [9250/20000], Loss: -12.32330322265625, Learning Rate: 0.01\n",
      "Epoch [9251/20000], Loss: -12.337905883789062, Learning Rate: 0.01\n",
      "Epoch [9252/20000], Loss: -12.35418701171875, Learning Rate: 0.01\n",
      "Epoch [9253/20000], Loss: -12.36614990234375, Learning Rate: 0.01\n",
      "Epoch [9254/20000], Loss: -12.371109008789062, Learning Rate: 0.01\n",
      "Epoch [9255/20000], Loss: -12.37030029296875, Learning Rate: 0.01\n",
      "Epoch [9256/20000], Loss: -12.367538452148438, Learning Rate: 0.01\n",
      "Epoch [9257/20000], Loss: -12.366928100585938, Learning Rate: 0.01\n",
      "Epoch [9258/20000], Loss: -12.370452880859375, Learning Rate: 0.01\n",
      "Epoch [9259/20000], Loss: -12.377792358398438, Learning Rate: 0.01\n",
      "Epoch [9260/20000], Loss: -12.386703491210938, Learning Rate: 0.01\n",
      "Epoch [9261/20000], Loss: -12.394744873046875, Learning Rate: 0.01\n",
      "Epoch [9262/20000], Loss: -12.400222778320312, Learning Rate: 0.01\n",
      "Epoch [9263/20000], Loss: -12.4029541015625, Learning Rate: 0.01\n",
      "Epoch [9264/20000], Loss: -12.404083251953125, Learning Rate: 0.01\n",
      "Epoch [9265/20000], Loss: -12.405426025390625, Learning Rate: 0.01\n",
      "Epoch [9266/20000], Loss: -12.408035278320312, Learning Rate: 0.01\n",
      "Epoch [9267/20000], Loss: -12.412246704101562, Learning Rate: 0.01\n",
      "Epoch [9268/20000], Loss: -12.417755126953125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9269/20000], Loss: -12.423477172851562, Learning Rate: 0.01\n",
      "Epoch [9270/20000], Loss: -12.428604125976562, Learning Rate: 0.01\n",
      "Epoch [9271/20000], Loss: -12.43267822265625, Learning Rate: 0.01\n",
      "Epoch [9272/20000], Loss: -12.435745239257812, Learning Rate: 0.01\n",
      "Epoch [9273/20000], Loss: -12.438369750976562, Learning Rate: 0.01\n",
      "Epoch [9274/20000], Loss: -12.441131591796875, Learning Rate: 0.01\n",
      "Epoch [9275/20000], Loss: -12.44439697265625, Learning Rate: 0.01\n",
      "Epoch [9276/20000], Loss: -12.448348999023438, Learning Rate: 0.01\n",
      "Epoch [9277/20000], Loss: -12.452682495117188, Learning Rate: 0.01\n",
      "Epoch [9278/20000], Loss: -12.456985473632812, Learning Rate: 0.01\n",
      "Epoch [9279/20000], Loss: -12.461212158203125, Learning Rate: 0.01\n",
      "Epoch [9280/20000], Loss: -12.465057373046875, Learning Rate: 0.01\n",
      "Epoch [9281/20000], Loss: -12.468505859375, Learning Rate: 0.01\n",
      "Epoch [9282/20000], Loss: -12.47174072265625, Learning Rate: 0.01\n",
      "Epoch [9283/20000], Loss: -12.47509765625, Learning Rate: 0.01\n",
      "Epoch [9284/20000], Loss: -12.478515625, Learning Rate: 0.01\n",
      "Epoch [9285/20000], Loss: -12.4820556640625, Learning Rate: 0.01\n",
      "Epoch [9286/20000], Loss: -12.485992431640625, Learning Rate: 0.01\n",
      "Epoch [9287/20000], Loss: -12.48980712890625, Learning Rate: 0.01\n",
      "Epoch [9288/20000], Loss: -12.49371337890625, Learning Rate: 0.01\n",
      "Epoch [9289/20000], Loss: -12.497573852539062, Learning Rate: 0.01\n",
      "Epoch [9290/20000], Loss: -12.501068115234375, Learning Rate: 0.01\n",
      "Epoch [9291/20000], Loss: -12.504608154296875, Learning Rate: 0.01\n",
      "Epoch [9292/20000], Loss: -12.508132934570312, Learning Rate: 0.01\n",
      "Epoch [9293/20000], Loss: -12.511688232421875, Learning Rate: 0.01\n",
      "Epoch [9294/20000], Loss: -12.515106201171875, Learning Rate: 0.01\n",
      "Epoch [9295/20000], Loss: -12.51885986328125, Learning Rate: 0.01\n",
      "Epoch [9296/20000], Loss: -12.522445678710938, Learning Rate: 0.01\n",
      "Epoch [9297/20000], Loss: -12.526214599609375, Learning Rate: 0.01\n",
      "Epoch [9298/20000], Loss: -12.529922485351562, Learning Rate: 0.01\n",
      "Epoch [9299/20000], Loss: -12.533523559570312, Learning Rate: 0.01\n",
      "Epoch [9300/20000], Loss: -12.537155151367188, Learning Rate: 0.01\n",
      "Epoch [9301/20000], Loss: -12.540817260742188, Learning Rate: 0.01\n",
      "Epoch [9302/20000], Loss: -12.54437255859375, Learning Rate: 0.01\n",
      "Epoch [9303/20000], Loss: -12.547927856445312, Learning Rate: 0.01\n",
      "Epoch [9304/20000], Loss: -12.551498413085938, Learning Rate: 0.01\n",
      "Epoch [9305/20000], Loss: -12.555084228515625, Learning Rate: 0.01\n",
      "Epoch [9306/20000], Loss: -12.55877685546875, Learning Rate: 0.01\n",
      "Epoch [9307/20000], Loss: -12.562347412109375, Learning Rate: 0.01\n",
      "Epoch [9308/20000], Loss: -12.566055297851562, Learning Rate: 0.01\n",
      "Epoch [9309/20000], Loss: -12.5697021484375, Learning Rate: 0.01\n",
      "Epoch [9310/20000], Loss: -12.573272705078125, Learning Rate: 0.01\n",
      "Epoch [9311/20000], Loss: -12.576889038085938, Learning Rate: 0.01\n",
      "Epoch [9312/20000], Loss: -12.58050537109375, Learning Rate: 0.01\n",
      "Epoch [9313/20000], Loss: -12.584136962890625, Learning Rate: 0.01\n",
      "Epoch [9314/20000], Loss: -12.587753295898438, Learning Rate: 0.01\n",
      "Epoch [9315/20000], Loss: -12.5914306640625, Learning Rate: 0.01\n",
      "Epoch [9316/20000], Loss: -12.594879150390625, Learning Rate: 0.01\n",
      "Epoch [9317/20000], Loss: -12.598587036132812, Learning Rate: 0.01\n",
      "Epoch [9318/20000], Loss: -12.602203369140625, Learning Rate: 0.01\n",
      "Epoch [9319/20000], Loss: -12.605850219726562, Learning Rate: 0.01\n",
      "Epoch [9320/20000], Loss: -12.609420776367188, Learning Rate: 0.01\n",
      "Epoch [9321/20000], Loss: -12.613021850585938, Learning Rate: 0.01\n",
      "Epoch [9322/20000], Loss: -12.61669921875, Learning Rate: 0.01\n",
      "Epoch [9323/20000], Loss: -12.620285034179688, Learning Rate: 0.01\n",
      "Epoch [9324/20000], Loss: -12.623931884765625, Learning Rate: 0.01\n",
      "Epoch [9325/20000], Loss: -12.62750244140625, Learning Rate: 0.01\n",
      "Epoch [9326/20000], Loss: -12.631072998046875, Learning Rate: 0.01\n",
      "Epoch [9327/20000], Loss: -12.634735107421875, Learning Rate: 0.01\n",
      "Epoch [9328/20000], Loss: -12.638381958007812, Learning Rate: 0.01\n",
      "Epoch [9329/20000], Loss: -12.641937255859375, Learning Rate: 0.01\n",
      "Epoch [9330/20000], Loss: -12.645553588867188, Learning Rate: 0.01\n",
      "Epoch [9331/20000], Loss: -12.649246215820312, Learning Rate: 0.01\n",
      "Epoch [9332/20000], Loss: -12.652816772460938, Learning Rate: 0.01\n",
      "Epoch [9333/20000], Loss: -12.656448364257812, Learning Rate: 0.01\n",
      "Epoch [9334/20000], Loss: -12.660125732421875, Learning Rate: 0.01\n",
      "Epoch [9335/20000], Loss: -12.663726806640625, Learning Rate: 0.01\n",
      "Epoch [9336/20000], Loss: -12.667312622070312, Learning Rate: 0.01\n",
      "Epoch [9337/20000], Loss: -12.670974731445312, Learning Rate: 0.01\n",
      "Epoch [9338/20000], Loss: -12.674606323242188, Learning Rate: 0.01\n",
      "Epoch [9339/20000], Loss: -12.678237915039062, Learning Rate: 0.01\n",
      "Epoch [9340/20000], Loss: -12.681869506835938, Learning Rate: 0.01\n",
      "Epoch [9341/20000], Loss: -12.685440063476562, Learning Rate: 0.01\n",
      "Epoch [9342/20000], Loss: -12.689132690429688, Learning Rate: 0.01\n",
      "Epoch [9343/20000], Loss: -12.69268798828125, Learning Rate: 0.01\n",
      "Epoch [9344/20000], Loss: -12.696304321289062, Learning Rate: 0.01\n",
      "Epoch [9345/20000], Loss: -12.699905395507812, Learning Rate: 0.01\n",
      "Epoch [9346/20000], Loss: -12.703536987304688, Learning Rate: 0.01\n",
      "Epoch [9347/20000], Loss: -12.707122802734375, Learning Rate: 0.01\n",
      "Epoch [9348/20000], Loss: -12.710891723632812, Learning Rate: 0.01\n",
      "Epoch [9349/20000], Loss: -12.714462280273438, Learning Rate: 0.01\n",
      "Epoch [9350/20000], Loss: -12.718093872070312, Learning Rate: 0.01\n",
      "Epoch [9351/20000], Loss: -12.721725463867188, Learning Rate: 0.01\n",
      "Epoch [9352/20000], Loss: -12.725250244140625, Learning Rate: 0.01\n",
      "Epoch [9353/20000], Loss: -12.728866577148438, Learning Rate: 0.01\n",
      "Epoch [9354/20000], Loss: -12.732498168945312, Learning Rate: 0.01\n",
      "Epoch [9355/20000], Loss: -12.736175537109375, Learning Rate: 0.01\n",
      "Epoch [9356/20000], Loss: -12.73980712890625, Learning Rate: 0.01\n",
      "Epoch [9357/20000], Loss: -12.743438720703125, Learning Rate: 0.01\n",
      "Epoch [9358/20000], Loss: -12.747024536132812, Learning Rate: 0.01\n",
      "Epoch [9359/20000], Loss: -12.750625610351562, Learning Rate: 0.01\n",
      "Epoch [9360/20000], Loss: -12.754287719726562, Learning Rate: 0.01\n",
      "Epoch [9361/20000], Loss: -12.757888793945312, Learning Rate: 0.01\n",
      "Epoch [9362/20000], Loss: -12.761566162109375, Learning Rate: 0.01\n",
      "Epoch [9363/20000], Loss: -12.765121459960938, Learning Rate: 0.01\n",
      "Epoch [9364/20000], Loss: -12.768798828125, Learning Rate: 0.01\n",
      "Epoch [9365/20000], Loss: -12.7723388671875, Learning Rate: 0.01\n",
      "Epoch [9366/20000], Loss: -12.775970458984375, Learning Rate: 0.01\n",
      "Epoch [9367/20000], Loss: -12.779586791992188, Learning Rate: 0.01\n",
      "Epoch [9368/20000], Loss: -12.783309936523438, Learning Rate: 0.01\n",
      "Epoch [9369/20000], Loss: -12.786788940429688, Learning Rate: 0.01\n",
      "Epoch [9370/20000], Loss: -12.790359497070312, Learning Rate: 0.01\n",
      "Epoch [9371/20000], Loss: -12.793975830078125, Learning Rate: 0.01\n",
      "Epoch [9372/20000], Loss: -12.79754638671875, Learning Rate: 0.01\n",
      "Epoch [9373/20000], Loss: -12.8011474609375, Learning Rate: 0.01\n",
      "Epoch [9374/20000], Loss: -12.804641723632812, Learning Rate: 0.01\n",
      "Epoch [9375/20000], Loss: -12.808074951171875, Learning Rate: 0.01\n",
      "Epoch [9376/20000], Loss: -12.811386108398438, Learning Rate: 0.01\n",
      "Epoch [9377/20000], Loss: -12.814590454101562, Learning Rate: 0.01\n",
      "Epoch [9378/20000], Loss: -12.81768798828125, Learning Rate: 0.01\n",
      "Epoch [9379/20000], Loss: -12.820632934570312, Learning Rate: 0.01\n",
      "Epoch [9380/20000], Loss: -12.82305908203125, Learning Rate: 0.01\n",
      "Epoch [9381/20000], Loss: -12.82513427734375, Learning Rate: 0.01\n",
      "Epoch [9382/20000], Loss: -12.826385498046875, Learning Rate: 0.01\n",
      "Epoch [9383/20000], Loss: -12.826675415039062, Learning Rate: 0.01\n",
      "Epoch [9384/20000], Loss: -12.825424194335938, Learning Rate: 0.01\n",
      "Epoch [9385/20000], Loss: -12.821823120117188, Learning Rate: 0.01\n",
      "Epoch [9386/20000], Loss: -12.81475830078125, Learning Rate: 0.01\n",
      "Epoch [9387/20000], Loss: -12.802642822265625, Learning Rate: 0.01\n",
      "Epoch [9388/20000], Loss: -12.78289794921875, Learning Rate: 0.01\n",
      "Epoch [9389/20000], Loss: -12.751846313476562, Learning Rate: 0.01\n",
      "Epoch [9390/20000], Loss: -12.703521728515625, Learning Rate: 0.01\n",
      "Epoch [9391/20000], Loss: -12.629898071289062, Learning Rate: 0.01\n",
      "Epoch [9392/20000], Loss: -12.51861572265625, Learning Rate: 0.01\n",
      "Epoch [9393/20000], Loss: -12.351760864257812, Learning Rate: 0.01\n",
      "Epoch [9394/20000], Loss: -12.105575561523438, Learning Rate: 0.01\n",
      "Epoch [9395/20000], Loss: -11.748626708984375, Learning Rate: 0.01\n",
      "Epoch [9396/20000], Loss: -11.247482299804688, Learning Rate: 0.01\n",
      "Epoch [9397/20000], Loss: -10.573501586914062, Learning Rate: 0.01\n",
      "Epoch [9398/20000], Loss: -9.73681640625, Learning Rate: 0.01\n",
      "Epoch [9399/20000], Loss: -8.8155517578125, Learning Rate: 0.01\n",
      "Epoch [9400/20000], Loss: -8.026290893554688, Learning Rate: 0.01\n",
      "Epoch [9401/20000], Loss: -7.68609619140625, Learning Rate: 0.01\n",
      "Epoch [9402/20000], Loss: -8.127395629882812, Learning Rate: 0.01\n",
      "Epoch [9403/20000], Loss: -9.379898071289062, Learning Rate: 0.01\n",
      "Epoch [9404/20000], Loss: -11.040451049804688, Learning Rate: 0.01\n",
      "Epoch [9405/20000], Loss: -12.396881103515625, Learning Rate: 0.01\n",
      "Epoch [9406/20000], Loss: -12.915985107421875, Learning Rate: 0.01\n",
      "Epoch [9407/20000], Loss: -12.5677490234375, Learning Rate: 0.01\n",
      "Epoch [9408/20000], Loss: -11.783065795898438, Learning Rate: 0.01\n",
      "Epoch [9409/20000], Loss: -11.1494140625, Learning Rate: 0.01\n",
      "Epoch [9410/20000], Loss: -11.077728271484375, Learning Rate: 0.01\n",
      "Epoch [9411/20000], Loss: -11.594223022460938, Learning Rate: 0.01\n",
      "Epoch [9412/20000], Loss: -12.34356689453125, Learning Rate: 0.01\n",
      "Epoch [9413/20000], Loss: -12.863723754882812, Learning Rate: 0.01\n",
      "Epoch [9414/20000], Loss: -12.909317016601562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9415/20000], Loss: -12.586334228515625, Learning Rate: 0.01\n",
      "Epoch [9416/20000], Loss: -12.218093872070312, Learning Rate: 0.01\n",
      "Epoch [9417/20000], Loss: -12.095703125, Learning Rate: 0.01\n",
      "Epoch [9418/20000], Loss: -12.293869018554688, Learning Rate: 0.01\n",
      "Epoch [9419/20000], Loss: -12.652114868164062, Learning Rate: 0.01\n",
      "Epoch [9420/20000], Loss: -12.92413330078125, Learning Rate: 0.01\n",
      "Epoch [9421/20000], Loss: -12.96453857421875, Learning Rate: 0.01\n",
      "Epoch [9422/20000], Loss: -12.8135986328125, Learning Rate: 0.01\n",
      "Epoch [9423/20000], Loss: -12.633895874023438, Learning Rate: 0.01\n",
      "Epoch [9424/20000], Loss: -12.575210571289062, Learning Rate: 0.01\n",
      "Epoch [9425/20000], Loss: -12.674636840820312, Learning Rate: 0.01\n",
      "Epoch [9426/20000], Loss: -12.850173950195312, Learning Rate: 0.01\n",
      "Epoch [9427/20000], Loss: -12.980194091796875, Learning Rate: 0.01\n",
      "Epoch [9428/20000], Loss: -12.996612548828125, Learning Rate: 0.01\n",
      "Epoch [9429/20000], Loss: -12.922454833984375, Learning Rate: 0.01\n",
      "Epoch [9430/20000], Loss: -12.838165283203125, Learning Rate: 0.01\n",
      "Epoch [9431/20000], Loss: -12.814956665039062, Learning Rate: 0.01\n",
      "Epoch [9432/20000], Loss: -12.867416381835938, Learning Rate: 0.01\n",
      "Epoch [9433/20000], Loss: -12.954132080078125, Learning Rate: 0.01\n",
      "Epoch [9434/20000], Loss: -13.017333984375, Learning Rate: 0.01\n",
      "Epoch [9435/20000], Loss: -13.025680541992188, Learning Rate: 0.01\n",
      "Epoch [9436/20000], Loss: -12.991287231445312, Learning Rate: 0.01\n",
      "Epoch [9437/20000], Loss: -12.95263671875, Learning Rate: 0.01\n",
      "Epoch [9438/20000], Loss: -12.943161010742188, Learning Rate: 0.01\n",
      "Epoch [9439/20000], Loss: -12.9696044921875, Learning Rate: 0.01\n",
      "Epoch [9440/20000], Loss: -13.012832641601562, Learning Rate: 0.01\n",
      "Epoch [9441/20000], Loss: -13.04583740234375, Learning Rate: 0.01\n",
      "Epoch [9442/20000], Loss: -13.05316162109375, Learning Rate: 0.01\n",
      "Epoch [9443/20000], Loss: -13.039520263671875, Learning Rate: 0.01\n",
      "Epoch [9444/20000], Loss: -13.022445678710938, Learning Rate: 0.01\n",
      "Epoch [9445/20000], Loss: -13.017791748046875, Learning Rate: 0.01\n",
      "Epoch [9446/20000], Loss: -13.030410766601562, Learning Rate: 0.01\n",
      "Epoch [9447/20000], Loss: -13.052398681640625, Learning Rate: 0.01\n",
      "Epoch [9448/20000], Loss: -13.07122802734375, Learning Rate: 0.01\n",
      "Epoch [9449/20000], Loss: -13.07861328125, Learning Rate: 0.01\n",
      "Epoch [9450/20000], Loss: -13.075393676757812, Learning Rate: 0.01\n",
      "Epoch [9451/20000], Loss: -13.068679809570312, Learning Rate: 0.01\n",
      "Epoch [9452/20000], Loss: -13.0662841796875, Learning Rate: 0.01\n",
      "Epoch [9453/20000], Loss: -13.072021484375, Learning Rate: 0.01\n",
      "Epoch [9454/20000], Loss: -13.083038330078125, Learning Rate: 0.01\n",
      "Epoch [9455/20000], Loss: -13.094223022460938, Learning Rate: 0.01\n",
      "Epoch [9456/20000], Loss: -13.100875854492188, Learning Rate: 0.01\n",
      "Epoch [9457/20000], Loss: -13.1019287109375, Learning Rate: 0.01\n",
      "Epoch [9458/20000], Loss: -13.099868774414062, Learning Rate: 0.01\n",
      "Epoch [9459/20000], Loss: -13.098403930664062, Learning Rate: 0.01\n",
      "Epoch [9460/20000], Loss: -13.09991455078125, Learning Rate: 0.01\n",
      "Epoch [9461/20000], Loss: -13.103912353515625, Learning Rate: 0.01\n",
      "Epoch [9462/20000], Loss: -13.1085205078125, Learning Rate: 0.01\n",
      "Epoch [9463/20000], Loss: -13.110916137695312, Learning Rate: 0.01\n",
      "Epoch [9464/20000], Loss: -13.109603881835938, Learning Rate: 0.01\n",
      "Epoch [9465/20000], Loss: -13.104522705078125, Learning Rate: 0.01\n",
      "Epoch [9466/20000], Loss: -13.09649658203125, Learning Rate: 0.01\n",
      "Epoch [9467/20000], Loss: -13.086166381835938, Learning Rate: 0.01\n",
      "Epoch [9468/20000], Loss: -13.07275390625, Learning Rate: 0.01\n",
      "Epoch [9469/20000], Loss: -13.054122924804688, Learning Rate: 0.01\n",
      "Epoch [9470/20000], Loss: -13.026885986328125, Learning Rate: 0.01\n",
      "Epoch [9471/20000], Loss: -12.98663330078125, Learning Rate: 0.01\n",
      "Epoch [9472/20000], Loss: -12.928741455078125, Learning Rate: 0.01\n",
      "Epoch [9473/20000], Loss: -12.847152709960938, Learning Rate: 0.01\n",
      "Epoch [9474/20000], Loss: -12.734359741210938, Learning Rate: 0.01\n",
      "Epoch [9475/20000], Loss: -12.579940795898438, Learning Rate: 0.01\n",
      "Epoch [9476/20000], Loss: -12.370635986328125, Learning Rate: 0.01\n",
      "Epoch [9477/20000], Loss: -12.09112548828125, Learning Rate: 0.01\n",
      "Epoch [9478/20000], Loss: -11.725311279296875, Learning Rate: 0.01\n",
      "Epoch [9479/20000], Loss: -11.263137817382812, Learning Rate: 0.01\n",
      "Epoch [9480/20000], Loss: -10.708908081054688, Learning Rate: 0.01\n",
      "Epoch [9481/20000], Loss: -10.097030639648438, Learning Rate: 0.01\n",
      "Epoch [9482/20000], Loss: -9.50634765625, Learning Rate: 0.01\n",
      "Epoch [9483/20000], Loss: -9.069381713867188, Learning Rate: 0.01\n",
      "Epoch [9484/20000], Loss: -8.949539184570312, Learning Rate: 0.01\n",
      "Epoch [9485/20000], Loss: -9.281387329101562, Learning Rate: 0.01\n",
      "Epoch [9486/20000], Loss: -10.075958251953125, Learning Rate: 0.01\n",
      "Epoch [9487/20000], Loss: -11.163787841796875, Learning Rate: 0.01\n",
      "Epoch [9488/20000], Loss: -12.233291625976562, Learning Rate: 0.01\n",
      "Epoch [9489/20000], Loss: -12.973556518554688, Learning Rate: 0.01\n",
      "Epoch [9490/20000], Loss: -13.2232666015625, Learning Rate: 0.01\n",
      "Epoch [9491/20000], Loss: -13.026870727539062, Learning Rate: 0.01\n",
      "Epoch [9492/20000], Loss: -12.585052490234375, Learning Rate: 0.01\n",
      "Epoch [9493/20000], Loss: -12.150863647460938, Learning Rate: 0.01\n",
      "Epoch [9494/20000], Loss: -11.92999267578125, Learning Rate: 0.01\n",
      "Epoch [9495/20000], Loss: -12.010284423828125, Learning Rate: 0.01\n",
      "Epoch [9496/20000], Loss: -12.3404541015625, Learning Rate: 0.01\n",
      "Epoch [9497/20000], Loss: -12.765213012695312, Learning Rate: 0.01\n",
      "Epoch [9498/20000], Loss: -13.107711791992188, Learning Rate: 0.01\n",
      "Epoch [9499/20000], Loss: -13.254852294921875, Learning Rate: 0.01\n",
      "Epoch [9500/20000], Loss: -13.199005126953125, Learning Rate: 0.01\n",
      "Epoch [9501/20000], Loss: -13.022933959960938, Learning Rate: 0.01\n",
      "Epoch [9502/20000], Loss: -12.844451904296875, Learning Rate: 0.01\n",
      "Epoch [9503/20000], Loss: -12.759262084960938, Learning Rate: 0.01\n",
      "Epoch [9504/20000], Loss: -12.802993774414062, Learning Rate: 0.01\n",
      "Epoch [9505/20000], Loss: -12.945999145507812, Learning Rate: 0.01\n",
      "Epoch [9506/20000], Loss: -13.117431640625, Learning Rate: 0.01\n",
      "Epoch [9507/20000], Loss: -13.245574951171875, Learning Rate: 0.01\n",
      "Epoch [9508/20000], Loss: -13.29052734375, Learning Rate: 0.01\n",
      "Epoch [9509/20000], Loss: -13.257186889648438, Learning Rate: 0.01\n",
      "Epoch [9510/20000], Loss: -13.1837158203125, Learning Rate: 0.01\n",
      "Epoch [9511/20000], Loss: -13.118362426757812, Learning Rate: 0.01\n",
      "Epoch [9512/20000], Loss: -13.095687866210938, Learning Rate: 0.01\n",
      "Epoch [9513/20000], Loss: -13.124313354492188, Learning Rate: 0.01\n",
      "Epoch [9514/20000], Loss: -13.187820434570312, Learning Rate: 0.01\n",
      "Epoch [9515/20000], Loss: -13.257125854492188, Learning Rate: 0.01\n",
      "Epoch [9516/20000], Loss: -13.30609130859375, Learning Rate: 0.01\n",
      "Epoch [9517/20000], Loss: -13.321640014648438, Learning Rate: 0.01\n",
      "Epoch [9518/20000], Loss: -13.307998657226562, Learning Rate: 0.01\n",
      "Epoch [9519/20000], Loss: -13.280685424804688, Learning Rate: 0.01\n",
      "Epoch [9520/20000], Loss: -13.257736206054688, Learning Rate: 0.01\n",
      "Epoch [9521/20000], Loss: -13.251739501953125, Learning Rate: 0.01\n",
      "Epoch [9522/20000], Loss: -13.265045166015625, Learning Rate: 0.01\n",
      "Epoch [9523/20000], Loss: -13.291641235351562, Learning Rate: 0.01\n",
      "Epoch [9524/20000], Loss: -13.32086181640625, Learning Rate: 0.01\n",
      "Epoch [9525/20000], Loss: -13.342926025390625, Learning Rate: 0.01\n",
      "Epoch [9526/20000], Loss: -13.352752685546875, Learning Rate: 0.01\n",
      "Epoch [9527/20000], Loss: -13.35113525390625, Learning Rate: 0.01\n",
      "Epoch [9528/20000], Loss: -13.343429565429688, Learning Rate: 0.01\n",
      "Epoch [9529/20000], Loss: -13.335983276367188, Learning Rate: 0.01\n",
      "Epoch [9530/20000], Loss: -13.333816528320312, Learning Rate: 0.01\n",
      "Epoch [9531/20000], Loss: -13.338775634765625, Learning Rate: 0.01\n",
      "Epoch [9532/20000], Loss: -13.349624633789062, Learning Rate: 0.01\n",
      "Epoch [9533/20000], Loss: -13.362686157226562, Learning Rate: 0.01\n",
      "Epoch [9534/20000], Loss: -13.374588012695312, Learning Rate: 0.01\n",
      "Epoch [9535/20000], Loss: -13.382720947265625, Learning Rate: 0.01\n",
      "Epoch [9536/20000], Loss: -13.386383056640625, Learning Rate: 0.01\n",
      "Epoch [9537/20000], Loss: -13.386917114257812, Learning Rate: 0.01\n",
      "Epoch [9538/20000], Loss: -13.386032104492188, Learning Rate: 0.01\n",
      "Epoch [9539/20000], Loss: -13.38592529296875, Learning Rate: 0.01\n",
      "Epoch [9540/20000], Loss: -13.38812255859375, Learning Rate: 0.01\n",
      "Epoch [9541/20000], Loss: -13.392562866210938, Learning Rate: 0.01\n",
      "Epoch [9542/20000], Loss: -13.3988037109375, Learning Rate: 0.01\n",
      "Epoch [9543/20000], Loss: -13.40576171875, Learning Rate: 0.01\n",
      "Epoch [9544/20000], Loss: -13.412078857421875, Learning Rate: 0.01\n",
      "Epoch [9545/20000], Loss: -13.41717529296875, Learning Rate: 0.01\n",
      "Epoch [9546/20000], Loss: -13.420761108398438, Learning Rate: 0.01\n",
      "Epoch [9547/20000], Loss: -13.423202514648438, Learning Rate: 0.01\n",
      "Epoch [9548/20000], Loss: -13.42523193359375, Learning Rate: 0.01\n",
      "Epoch [9549/20000], Loss: -13.4273681640625, Learning Rate: 0.01\n",
      "Epoch [9550/20000], Loss: -13.430068969726562, Learning Rate: 0.01\n",
      "Epoch [9551/20000], Loss: -13.43359375, Learning Rate: 0.01\n",
      "Epoch [9552/20000], Loss: -13.437774658203125, Learning Rate: 0.01\n",
      "Epoch [9553/20000], Loss: -13.4422607421875, Learning Rate: 0.01\n",
      "Epoch [9554/20000], Loss: -13.44677734375, Learning Rate: 0.01\n",
      "Epoch [9555/20000], Loss: -13.45098876953125, Learning Rate: 0.01\n",
      "Epoch [9556/20000], Loss: -13.45477294921875, Learning Rate: 0.01\n",
      "Epoch [9557/20000], Loss: -13.458251953125, Learning Rate: 0.01\n",
      "Epoch [9558/20000], Loss: -13.461273193359375, Learning Rate: 0.01\n",
      "Epoch [9559/20000], Loss: -13.464263916015625, Learning Rate: 0.01\n",
      "Epoch [9560/20000], Loss: -13.467239379882812, Learning Rate: 0.01\n",
      "Epoch [9561/20000], Loss: -13.470428466796875, Learning Rate: 0.01\n",
      "Epoch [9562/20000], Loss: -13.473861694335938, Learning Rate: 0.01\n",
      "Epoch [9563/20000], Loss: -13.477386474609375, Learning Rate: 0.01\n",
      "Epoch [9564/20000], Loss: -13.48114013671875, Learning Rate: 0.01\n",
      "Epoch [9565/20000], Loss: -13.484817504882812, Learning Rate: 0.01\n",
      "Epoch [9566/20000], Loss: -13.488571166992188, Learning Rate: 0.01\n",
      "Epoch [9567/20000], Loss: -13.492156982421875, Learning Rate: 0.01\n",
      "Epoch [9568/20000], Loss: -13.495697021484375, Learning Rate: 0.01\n",
      "Epoch [9569/20000], Loss: -13.498992919921875, Learning Rate: 0.01\n",
      "Epoch [9570/20000], Loss: -13.502273559570312, Learning Rate: 0.01\n",
      "Epoch [9571/20000], Loss: -13.505630493164062, Learning Rate: 0.01\n",
      "Epoch [9572/20000], Loss: -13.5089111328125, Learning Rate: 0.01\n",
      "Epoch [9573/20000], Loss: -13.512161254882812, Learning Rate: 0.01\n",
      "Epoch [9574/20000], Loss: -13.515609741210938, Learning Rate: 0.01\n",
      "Epoch [9575/20000], Loss: -13.519027709960938, Learning Rate: 0.01\n",
      "Epoch [9576/20000], Loss: -13.522552490234375, Learning Rate: 0.01\n",
      "Epoch [9577/20000], Loss: -13.526031494140625, Learning Rate: 0.01\n",
      "Epoch [9578/20000], Loss: -13.529571533203125, Learning Rate: 0.01\n",
      "Epoch [9579/20000], Loss: -13.5330810546875, Learning Rate: 0.01\n",
      "Epoch [9580/20000], Loss: -13.53643798828125, Learning Rate: 0.01\n",
      "Epoch [9581/20000], Loss: -13.539871215820312, Learning Rate: 0.01\n",
      "Epoch [9582/20000], Loss: -13.543304443359375, Learning Rate: 0.01\n",
      "Epoch [9583/20000], Loss: -13.54669189453125, Learning Rate: 0.01\n",
      "Epoch [9584/20000], Loss: -13.550079345703125, Learning Rate: 0.01\n",
      "Epoch [9585/20000], Loss: -13.553375244140625, Learning Rate: 0.01\n",
      "Epoch [9586/20000], Loss: -13.556777954101562, Learning Rate: 0.01\n",
      "Epoch [9587/20000], Loss: -13.56024169921875, Learning Rate: 0.01\n",
      "Epoch [9588/20000], Loss: -13.5635986328125, Learning Rate: 0.01\n",
      "Epoch [9589/20000], Loss: -13.567108154296875, Learning Rate: 0.01\n",
      "Epoch [9590/20000], Loss: -13.570465087890625, Learning Rate: 0.01\n",
      "Epoch [9591/20000], Loss: -13.573898315429688, Learning Rate: 0.01\n",
      "Epoch [9592/20000], Loss: -13.577255249023438, Learning Rate: 0.01\n",
      "Epoch [9593/20000], Loss: -13.580734252929688, Learning Rate: 0.01\n",
      "Epoch [9594/20000], Loss: -13.584152221679688, Learning Rate: 0.01\n",
      "Epoch [9595/20000], Loss: -13.58758544921875, Learning Rate: 0.01\n",
      "Epoch [9596/20000], Loss: -13.5909423828125, Learning Rate: 0.01\n",
      "Epoch [9597/20000], Loss: -13.594375610351562, Learning Rate: 0.01\n",
      "Epoch [9598/20000], Loss: -13.597793579101562, Learning Rate: 0.01\n",
      "Epoch [9599/20000], Loss: -13.601242065429688, Learning Rate: 0.01\n",
      "Epoch [9600/20000], Loss: -13.604583740234375, Learning Rate: 0.01\n",
      "Epoch [9601/20000], Loss: -13.608001708984375, Learning Rate: 0.01\n",
      "Epoch [9602/20000], Loss: -13.611373901367188, Learning Rate: 0.01\n",
      "Epoch [9603/20000], Loss: -13.614837646484375, Learning Rate: 0.01\n",
      "Epoch [9604/20000], Loss: -13.618270874023438, Learning Rate: 0.01\n",
      "Epoch [9605/20000], Loss: -13.621551513671875, Learning Rate: 0.01\n",
      "Epoch [9606/20000], Loss: -13.625015258789062, Learning Rate: 0.01\n",
      "Epoch [9607/20000], Loss: -13.62847900390625, Learning Rate: 0.01\n",
      "Epoch [9608/20000], Loss: -13.631973266601562, Learning Rate: 0.01\n",
      "Epoch [9609/20000], Loss: -13.63531494140625, Learning Rate: 0.01\n",
      "Epoch [9610/20000], Loss: -13.638671875, Learning Rate: 0.01\n",
      "Epoch [9611/20000], Loss: -13.642074584960938, Learning Rate: 0.01\n",
      "Epoch [9612/20000], Loss: -13.6455078125, Learning Rate: 0.01\n",
      "Epoch [9613/20000], Loss: -13.64898681640625, Learning Rate: 0.01\n",
      "Epoch [9614/20000], Loss: -13.652420043945312, Learning Rate: 0.01\n",
      "Epoch [9615/20000], Loss: -13.65582275390625, Learning Rate: 0.01\n",
      "Epoch [9616/20000], Loss: -13.659164428710938, Learning Rate: 0.01\n",
      "Epoch [9617/20000], Loss: -13.662582397460938, Learning Rate: 0.01\n",
      "Epoch [9618/20000], Loss: -13.666000366210938, Learning Rate: 0.01\n",
      "Epoch [9619/20000], Loss: -13.669479370117188, Learning Rate: 0.01\n",
      "Epoch [9620/20000], Loss: -13.672744750976562, Learning Rate: 0.01\n",
      "Epoch [9621/20000], Loss: -13.676193237304688, Learning Rate: 0.01\n",
      "Epoch [9622/20000], Loss: -13.67962646484375, Learning Rate: 0.01\n",
      "Epoch [9623/20000], Loss: -13.683090209960938, Learning Rate: 0.01\n",
      "Epoch [9624/20000], Loss: -13.686431884765625, Learning Rate: 0.01\n",
      "Epoch [9625/20000], Loss: -13.689926147460938, Learning Rate: 0.01\n",
      "Epoch [9626/20000], Loss: -13.693313598632812, Learning Rate: 0.01\n",
      "Epoch [9627/20000], Loss: -13.696685791015625, Learning Rate: 0.01\n",
      "Epoch [9628/20000], Loss: -13.700103759765625, Learning Rate: 0.01\n",
      "Epoch [9629/20000], Loss: -13.703598022460938, Learning Rate: 0.01\n",
      "Epoch [9630/20000], Loss: -13.706954956054688, Learning Rate: 0.01\n",
      "Epoch [9631/20000], Loss: -13.71038818359375, Learning Rate: 0.01\n",
      "Epoch [9632/20000], Loss: -13.713821411132812, Learning Rate: 0.01\n",
      "Epoch [9633/20000], Loss: -13.71728515625, Learning Rate: 0.01\n",
      "Epoch [9634/20000], Loss: -13.720626831054688, Learning Rate: 0.01\n",
      "Epoch [9635/20000], Loss: -13.724014282226562, Learning Rate: 0.01\n",
      "Epoch [9636/20000], Loss: -13.727386474609375, Learning Rate: 0.01\n",
      "Epoch [9637/20000], Loss: -13.730911254882812, Learning Rate: 0.01\n",
      "Epoch [9638/20000], Loss: -13.73431396484375, Learning Rate: 0.01\n",
      "Epoch [9639/20000], Loss: -13.7376708984375, Learning Rate: 0.01\n",
      "Epoch [9640/20000], Loss: -13.741043090820312, Learning Rate: 0.01\n",
      "Epoch [9641/20000], Loss: -13.744522094726562, Learning Rate: 0.01\n",
      "Epoch [9642/20000], Loss: -13.74798583984375, Learning Rate: 0.01\n",
      "Epoch [9643/20000], Loss: -13.751358032226562, Learning Rate: 0.01\n",
      "Epoch [9644/20000], Loss: -13.754684448242188, Learning Rate: 0.01\n",
      "Epoch [9645/20000], Loss: -13.758224487304688, Learning Rate: 0.01\n",
      "Epoch [9646/20000], Loss: -13.761672973632812, Learning Rate: 0.01\n",
      "Epoch [9647/20000], Loss: -13.765045166015625, Learning Rate: 0.01\n",
      "Epoch [9648/20000], Loss: -13.768386840820312, Learning Rate: 0.01\n",
      "Epoch [9649/20000], Loss: -13.771820068359375, Learning Rate: 0.01\n",
      "Epoch [9650/20000], Loss: -13.7752685546875, Learning Rate: 0.01\n",
      "Epoch [9651/20000], Loss: -13.778610229492188, Learning Rate: 0.01\n",
      "Epoch [9652/20000], Loss: -13.782150268554688, Learning Rate: 0.01\n",
      "Epoch [9653/20000], Loss: -13.785491943359375, Learning Rate: 0.01\n",
      "Epoch [9654/20000], Loss: -13.788925170898438, Learning Rate: 0.01\n",
      "Epoch [9655/20000], Loss: -13.792388916015625, Learning Rate: 0.01\n",
      "Epoch [9656/20000], Loss: -13.795700073242188, Learning Rate: 0.01\n",
      "Epoch [9657/20000], Loss: -13.799087524414062, Learning Rate: 0.01\n",
      "Epoch [9658/20000], Loss: -13.802413940429688, Learning Rate: 0.01\n",
      "Epoch [9659/20000], Loss: -13.805740356445312, Learning Rate: 0.01\n",
      "Epoch [9660/20000], Loss: -13.809051513671875, Learning Rate: 0.01\n",
      "Epoch [9661/20000], Loss: -13.812408447265625, Learning Rate: 0.01\n",
      "Epoch [9662/20000], Loss: -13.815597534179688, Learning Rate: 0.01\n",
      "Epoch [9663/20000], Loss: -13.8187255859375, Learning Rate: 0.01\n",
      "Epoch [9664/20000], Loss: -13.821792602539062, Learning Rate: 0.01\n",
      "Epoch [9665/20000], Loss: -13.824615478515625, Learning Rate: 0.01\n",
      "Epoch [9666/20000], Loss: -13.827346801757812, Learning Rate: 0.01\n",
      "Epoch [9667/20000], Loss: -13.8297119140625, Learning Rate: 0.01\n",
      "Epoch [9668/20000], Loss: -13.83160400390625, Learning Rate: 0.01\n",
      "Epoch [9669/20000], Loss: -13.832931518554688, Learning Rate: 0.01\n",
      "Epoch [9670/20000], Loss: -13.83331298828125, Learning Rate: 0.01\n",
      "Epoch [9671/20000], Loss: -13.832275390625, Learning Rate: 0.01\n",
      "Epoch [9672/20000], Loss: -13.829360961914062, Learning Rate: 0.01\n",
      "Epoch [9673/20000], Loss: -13.823471069335938, Learning Rate: 0.01\n",
      "Epoch [9674/20000], Loss: -13.81353759765625, Learning Rate: 0.01\n",
      "Epoch [9675/20000], Loss: -13.797073364257812, Learning Rate: 0.01\n",
      "Epoch [9676/20000], Loss: -13.771575927734375, Learning Rate: 0.01\n",
      "Epoch [9677/20000], Loss: -13.732269287109375, Learning Rate: 0.01\n",
      "Epoch [9678/20000], Loss: -13.672698974609375, Learning Rate: 0.01\n",
      "Epoch [9679/20000], Loss: -13.5831298828125, Learning Rate: 0.01\n",
      "Epoch [9680/20000], Loss: -13.449630737304688, Learning Rate: 0.01\n",
      "Epoch [9681/20000], Loss: -13.252227783203125, Learning Rate: 0.01\n",
      "Epoch [9682/20000], Loss: -12.963638305664062, Learning Rate: 0.01\n",
      "Epoch [9683/20000], Loss: -12.549423217773438, Learning Rate: 0.01\n",
      "Epoch [9684/20000], Loss: -11.969436645507812, Learning Rate: 0.01\n",
      "Epoch [9685/20000], Loss: -11.192062377929688, Learning Rate: 0.01\n",
      "Epoch [9686/20000], Loss: -10.215652465820312, Learning Rate: 0.01\n",
      "Epoch [9687/20000], Loss: -9.11981201171875, Learning Rate: 0.01\n",
      "Epoch [9688/20000], Loss: -8.110870361328125, Learning Rate: 0.01\n",
      "Epoch [9689/20000], Loss: -7.5484161376953125, Learning Rate: 0.01\n",
      "Epoch [9690/20000], Loss: -7.8196258544921875, Learning Rate: 0.01\n",
      "Epoch [9691/20000], Loss: -9.089523315429688, Learning Rate: 0.01\n",
      "Epoch [9692/20000], Loss: -11.020645141601562, Learning Rate: 0.01\n",
      "Epoch [9693/20000], Loss: -12.851394653320312, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9694/20000], Loss: -13.84381103515625, Learning Rate: 0.01\n",
      "Epoch [9695/20000], Loss: -13.757659912109375, Learning Rate: 0.01\n",
      "Epoch [9696/20000], Loss: -12.93804931640625, Learning Rate: 0.01\n",
      "Epoch [9697/20000], Loss: -12.04205322265625, Learning Rate: 0.01\n",
      "Epoch [9698/20000], Loss: -11.659454345703125, Learning Rate: 0.01\n",
      "Epoch [9699/20000], Loss: -12.007522583007812, Learning Rate: 0.01\n",
      "Epoch [9700/20000], Loss: -12.839996337890625, Learning Rate: 0.01\n",
      "Epoch [9701/20000], Loss: -13.63250732421875, Learning Rate: 0.01\n",
      "Epoch [9702/20000], Loss: -13.958251953125, Learning Rate: 0.01\n",
      "Epoch [9703/20000], Loss: -13.753799438476562, Learning Rate: 0.01\n",
      "Epoch [9704/20000], Loss: -13.298477172851562, Learning Rate: 0.01\n",
      "Epoch [9705/20000], Loss: -12.975173950195312, Learning Rate: 0.01\n",
      "Epoch [9706/20000], Loss: -13.010482788085938, Learning Rate: 0.01\n",
      "Epoch [9707/20000], Loss: -13.355819702148438, Learning Rate: 0.01\n",
      "Epoch [9708/20000], Loss: -13.760696411132812, Learning Rate: 0.01\n",
      "Epoch [9709/20000], Loss: -13.97528076171875, Learning Rate: 0.01\n",
      "Epoch [9710/20000], Loss: -13.920333862304688, Learning Rate: 0.01\n",
      "Epoch [9711/20000], Loss: -13.709365844726562, Learning Rate: 0.01\n",
      "Epoch [9712/20000], Loss: -13.536361694335938, Learning Rate: 0.01\n",
      "Epoch [9713/20000], Loss: -13.531570434570312, Learning Rate: 0.01\n",
      "Epoch [9714/20000], Loss: -13.686569213867188, Learning Rate: 0.01\n",
      "Epoch [9715/20000], Loss: -13.884933471679688, Learning Rate: 0.01\n",
      "Epoch [9716/20000], Loss: -14.000732421875, Learning Rate: 0.01\n",
      "Epoch [9717/20000], Loss: -13.986343383789062, Learning Rate: 0.01\n",
      "Epoch [9718/20000], Loss: -13.889785766601562, Learning Rate: 0.01\n",
      "Epoch [9719/20000], Loss: -13.803817749023438, Learning Rate: 0.01\n",
      "Epoch [9720/20000], Loss: -13.79534912109375, Learning Rate: 0.01\n",
      "Epoch [9721/20000], Loss: -13.865692138671875, Learning Rate: 0.01\n",
      "Epoch [9722/20000], Loss: -13.962905883789062, Learning Rate: 0.01\n",
      "Epoch [9723/20000], Loss: -14.025711059570312, Learning Rate: 0.01\n",
      "Epoch [9724/20000], Loss: -14.027191162109375, Learning Rate: 0.01\n",
      "Epoch [9725/20000], Loss: -13.985671997070312, Learning Rate: 0.01\n",
      "Epoch [9726/20000], Loss: -13.943603515625, Learning Rate: 0.01\n",
      "Epoch [9727/20000], Loss: -13.935195922851562, Learning Rate: 0.01\n",
      "Epoch [9728/20000], Loss: -13.965621948242188, Learning Rate: 0.01\n",
      "Epoch [9729/20000], Loss: -14.013275146484375, Learning Rate: 0.01\n",
      "Epoch [9730/20000], Loss: -14.049224853515625, Learning Rate: 0.01\n",
      "Epoch [9731/20000], Loss: -14.057479858398438, Learning Rate: 0.01\n",
      "Epoch [9732/20000], Loss: -14.042617797851562, Learning Rate: 0.01\n",
      "Epoch [9733/20000], Loss: -14.023025512695312, Learning Rate: 0.01\n",
      "Epoch [9734/20000], Loss: -14.015975952148438, Learning Rate: 0.01\n",
      "Epoch [9735/20000], Loss: -14.02777099609375, Learning Rate: 0.01\n",
      "Epoch [9736/20000], Loss: -14.050933837890625, Learning Rate: 0.01\n",
      "Epoch [9737/20000], Loss: -14.072357177734375, Learning Rate: 0.01\n",
      "Epoch [9738/20000], Loss: -14.082290649414062, Learning Rate: 0.01\n",
      "Epoch [9739/20000], Loss: -14.080154418945312, Learning Rate: 0.01\n",
      "Epoch [9740/20000], Loss: -14.072525024414062, Learning Rate: 0.01\n",
      "Epoch [9741/20000], Loss: -14.0682373046875, Learning Rate: 0.01\n",
      "Epoch [9742/20000], Loss: -14.072006225585938, Learning Rate: 0.01\n",
      "Epoch [9743/20000], Loss: -14.08294677734375, Learning Rate: 0.01\n",
      "Epoch [9744/20000], Loss: -14.09564208984375, Learning Rate: 0.01\n",
      "Epoch [9745/20000], Loss: -14.104782104492188, Learning Rate: 0.01\n",
      "Epoch [9746/20000], Loss: -14.108139038085938, Learning Rate: 0.01\n",
      "Epoch [9747/20000], Loss: -14.10711669921875, Learning Rate: 0.01\n",
      "Epoch [9748/20000], Loss: -14.105728149414062, Learning Rate: 0.01\n",
      "Epoch [9749/20000], Loss: -14.107177734375, Learning Rate: 0.01\n",
      "Epoch [9750/20000], Loss: -14.112213134765625, Learning Rate: 0.01\n",
      "Epoch [9751/20000], Loss: -14.119613647460938, Learning Rate: 0.01\n",
      "Epoch [9752/20000], Loss: -14.126724243164062, Learning Rate: 0.01\n",
      "Epoch [9753/20000], Loss: -14.131805419921875, Learning Rate: 0.01\n",
      "Epoch [9754/20000], Loss: -14.134429931640625, Learning Rate: 0.01\n",
      "Epoch [9755/20000], Loss: -14.1356201171875, Learning Rate: 0.01\n",
      "Epoch [9756/20000], Loss: -14.137069702148438, Learning Rate: 0.01\n",
      "Epoch [9757/20000], Loss: -14.139923095703125, Learning Rate: 0.01\n",
      "Epoch [9758/20000], Loss: -14.144287109375, Learning Rate: 0.01\n",
      "Epoch [9759/20000], Loss: -14.149520874023438, Learning Rate: 0.01\n",
      "Epoch [9760/20000], Loss: -14.154312133789062, Learning Rate: 0.01\n",
      "Epoch [9761/20000], Loss: -14.158309936523438, Learning Rate: 0.01\n",
      "Epoch [9762/20000], Loss: -14.161209106445312, Learning Rate: 0.01\n",
      "Epoch [9763/20000], Loss: -14.163528442382812, Learning Rate: 0.01\n",
      "Epoch [9764/20000], Loss: -14.166046142578125, Learning Rate: 0.01\n",
      "Epoch [9765/20000], Loss: -14.16925048828125, Learning Rate: 0.01\n",
      "Epoch [9766/20000], Loss: -14.172943115234375, Learning Rate: 0.01\n",
      "Epoch [9767/20000], Loss: -14.177032470703125, Learning Rate: 0.01\n",
      "Epoch [9768/20000], Loss: -14.180999755859375, Learning Rate: 0.01\n",
      "Epoch [9769/20000], Loss: -14.184600830078125, Learning Rate: 0.01\n",
      "Epoch [9770/20000], Loss: -14.187789916992188, Learning Rate: 0.01\n",
      "Epoch [9771/20000], Loss: -14.190750122070312, Learning Rate: 0.01\n",
      "Epoch [9772/20000], Loss: -14.193695068359375, Learning Rate: 0.01\n",
      "Epoch [9773/20000], Loss: -14.196792602539062, Learning Rate: 0.01\n",
      "Epoch [9774/20000], Loss: -14.20025634765625, Learning Rate: 0.01\n",
      "Epoch [9775/20000], Loss: -14.203842163085938, Learning Rate: 0.01\n",
      "Epoch [9776/20000], Loss: -14.207504272460938, Learning Rate: 0.01\n",
      "Epoch [9777/20000], Loss: -14.210906982421875, Learning Rate: 0.01\n",
      "Epoch [9778/20000], Loss: -14.214248657226562, Learning Rate: 0.01\n",
      "Epoch [9779/20000], Loss: -14.217498779296875, Learning Rate: 0.01\n",
      "Epoch [9780/20000], Loss: -14.2205810546875, Learning Rate: 0.01\n",
      "Epoch [9781/20000], Loss: -14.223785400390625, Learning Rate: 0.01\n",
      "Epoch [9782/20000], Loss: -14.227081298828125, Learning Rate: 0.01\n",
      "Epoch [9783/20000], Loss: -14.230499267578125, Learning Rate: 0.01\n",
      "Epoch [9784/20000], Loss: -14.233932495117188, Learning Rate: 0.01\n",
      "Epoch [9785/20000], Loss: -14.237335205078125, Learning Rate: 0.01\n",
      "Epoch [9786/20000], Loss: -14.240737915039062, Learning Rate: 0.01\n",
      "Epoch [9787/20000], Loss: -14.244094848632812, Learning Rate: 0.01\n",
      "Epoch [9788/20000], Loss: -14.247207641601562, Learning Rate: 0.01\n",
      "Epoch [9789/20000], Loss: -14.250473022460938, Learning Rate: 0.01\n",
      "Epoch [9790/20000], Loss: -14.253814697265625, Learning Rate: 0.01\n",
      "Epoch [9791/20000], Loss: -14.25701904296875, Learning Rate: 0.01\n",
      "Epoch [9792/20000], Loss: -14.260330200195312, Learning Rate: 0.01\n",
      "Epoch [9793/20000], Loss: -14.263748168945312, Learning Rate: 0.01\n",
      "Epoch [9794/20000], Loss: -14.267120361328125, Learning Rate: 0.01\n",
      "Epoch [9795/20000], Loss: -14.270431518554688, Learning Rate: 0.01\n",
      "Epoch [9796/20000], Loss: -14.27374267578125, Learning Rate: 0.01\n",
      "Epoch [9797/20000], Loss: -14.2769775390625, Learning Rate: 0.01\n",
      "Epoch [9798/20000], Loss: -14.28021240234375, Learning Rate: 0.01\n",
      "Epoch [9799/20000], Loss: -14.283538818359375, Learning Rate: 0.01\n",
      "Epoch [9800/20000], Loss: -14.28680419921875, Learning Rate: 0.01\n",
      "Epoch [9801/20000], Loss: -14.290145874023438, Learning Rate: 0.01\n",
      "Epoch [9802/20000], Loss: -14.29339599609375, Learning Rate: 0.01\n",
      "Epoch [9803/20000], Loss: -14.296630859375, Learning Rate: 0.01\n",
      "Epoch [9804/20000], Loss: -14.2999267578125, Learning Rate: 0.01\n",
      "Epoch [9805/20000], Loss: -14.303115844726562, Learning Rate: 0.01\n",
      "Epoch [9806/20000], Loss: -14.306228637695312, Learning Rate: 0.01\n",
      "Epoch [9807/20000], Loss: -14.309371948242188, Learning Rate: 0.01\n",
      "Epoch [9808/20000], Loss: -14.31231689453125, Learning Rate: 0.01\n",
      "Epoch [9809/20000], Loss: -14.315109252929688, Learning Rate: 0.01\n",
      "Epoch [9810/20000], Loss: -14.317657470703125, Learning Rate: 0.01\n",
      "Epoch [9811/20000], Loss: -14.319625854492188, Learning Rate: 0.01\n",
      "Epoch [9812/20000], Loss: -14.321029663085938, Learning Rate: 0.01\n",
      "Epoch [9813/20000], Loss: -14.321365356445312, Learning Rate: 0.01\n",
      "Epoch [9814/20000], Loss: -14.31982421875, Learning Rate: 0.01\n",
      "Epoch [9815/20000], Loss: -14.315460205078125, Learning Rate: 0.01\n",
      "Epoch [9816/20000], Loss: -14.30670166015625, Learning Rate: 0.01\n",
      "Epoch [9817/20000], Loss: -14.290664672851562, Learning Rate: 0.01\n",
      "Epoch [9818/20000], Loss: -14.263336181640625, Learning Rate: 0.01\n",
      "Epoch [9819/20000], Loss: -14.217529296875, Learning Rate: 0.01\n",
      "Epoch [9820/20000], Loss: -14.142379760742188, Learning Rate: 0.01\n",
      "Epoch [9821/20000], Loss: -14.0203857421875, Learning Rate: 0.01\n",
      "Epoch [9822/20000], Loss: -13.8248291015625, Learning Rate: 0.01\n",
      "Epoch [9823/20000], Loss: -13.51507568359375, Learning Rate: 0.01\n",
      "Epoch [9824/20000], Loss: -13.036788940429688, Learning Rate: 0.01\n",
      "Epoch [9825/20000], Loss: -12.32135009765625, Learning Rate: 0.01\n",
      "Epoch [9826/20000], Loss: -11.317642211914062, Learning Rate: 0.01\n",
      "Epoch [9827/20000], Loss: -10.039413452148438, Learning Rate: 0.01\n",
      "Epoch [9828/20000], Loss: -8.70294189453125, Learning Rate: 0.01\n",
      "Epoch [9829/20000], Loss: -7.7819671630859375, Learning Rate: 0.01\n",
      "Epoch [9830/20000], Loss: -7.964599609375, Learning Rate: 0.01\n",
      "Epoch [9831/20000], Loss: -9.554351806640625, Learning Rate: 0.01\n",
      "Epoch [9832/20000], Loss: -11.98211669921875, Learning Rate: 0.01\n",
      "Epoch [9833/20000], Loss: -13.911911010742188, Learning Rate: 0.01\n",
      "Epoch [9834/20000], Loss: -14.355072021484375, Learning Rate: 0.01\n",
      "Epoch [9835/20000], Loss: -13.44232177734375, Learning Rate: 0.01\n",
      "Epoch [9836/20000], Loss: -12.199798583984375, Learning Rate: 0.01\n",
      "Epoch [9837/20000], Loss: -11.725738525390625, Learning Rate: 0.01\n",
      "Epoch [9838/20000], Loss: -12.390472412109375, Learning Rate: 0.01\n",
      "Epoch [9839/20000], Loss: -13.604843139648438, Learning Rate: 0.01\n",
      "Epoch [9840/20000], Loss: -14.371231079101562, Learning Rate: 0.01\n",
      "Epoch [9841/20000], Loss: -14.22015380859375, Learning Rate: 0.01\n",
      "Epoch [9842/20000], Loss: -13.540313720703125, Learning Rate: 0.01\n",
      "Epoch [9843/20000], Loss: -13.096466064453125, Learning Rate: 0.01\n",
      "Epoch [9844/20000], Loss: -13.313079833984375, Learning Rate: 0.01\n",
      "Epoch [9845/20000], Loss: -13.944976806640625, Learning Rate: 0.01\n",
      "Epoch [9846/20000], Loss: -14.402938842773438, Learning Rate: 0.01\n",
      "Epoch [9847/20000], Loss: -14.353073120117188, Learning Rate: 0.01\n",
      "Epoch [9848/20000], Loss: -13.986312866210938, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9849/20000], Loss: -13.746109008789062, Learning Rate: 0.01\n",
      "Epoch [9850/20000], Loss: -13.873306274414062, Learning Rate: 0.01\n",
      "Epoch [9851/20000], Loss: -14.2193603515625, Learning Rate: 0.01\n",
      "Epoch [9852/20000], Loss: -14.451034545898438, Learning Rate: 0.01\n",
      "Epoch [9853/20000], Loss: -14.40191650390625, Learning Rate: 0.01\n",
      "Epoch [9854/20000], Loss: -14.199859619140625, Learning Rate: 0.01\n",
      "Epoch [9855/20000], Loss: -14.090255737304688, Learning Rate: 0.01\n",
      "Epoch [9856/20000], Loss: -14.183090209960938, Learning Rate: 0.01\n",
      "Epoch [9857/20000], Loss: -14.373336791992188, Learning Rate: 0.01\n",
      "Epoch [9858/20000], Loss: -14.481033325195312, Learning Rate: 0.01\n",
      "Epoch [9859/20000], Loss: -14.437179565429688, Learning Rate: 0.01\n",
      "Epoch [9860/20000], Loss: -14.328689575195312, Learning Rate: 0.01\n",
      "Epoch [9861/20000], Loss: -14.285888671875, Learning Rate: 0.01\n",
      "Epoch [9862/20000], Loss: -14.350662231445312, Learning Rate: 0.01\n",
      "Epoch [9863/20000], Loss: -14.454238891601562, Learning Rate: 0.01\n",
      "Epoch [9864/20000], Loss: -14.502639770507812, Learning Rate: 0.01\n",
      "Epoch [9865/20000], Loss: -14.471282958984375, Learning Rate: 0.01\n",
      "Epoch [9866/20000], Loss: -14.414932250976562, Learning Rate: 0.01\n",
      "Epoch [9867/20000], Loss: -14.400619506835938, Learning Rate: 0.01\n",
      "Epoch [9868/20000], Loss: -14.442367553710938, Learning Rate: 0.01\n",
      "Epoch [9869/20000], Loss: -14.498764038085938, Learning Rate: 0.01\n",
      "Epoch [9870/20000], Loss: -14.521728515625, Learning Rate: 0.01\n",
      "Epoch [9871/20000], Loss: -14.503219604492188, Learning Rate: 0.01\n",
      "Epoch [9872/20000], Loss: -14.475112915039062, Learning Rate: 0.01\n",
      "Epoch [9873/20000], Loss: -14.471466064453125, Learning Rate: 0.01\n",
      "Epoch [9874/20000], Loss: -14.496917724609375, Learning Rate: 0.01\n",
      "Epoch [9875/20000], Loss: -14.527969360351562, Learning Rate: 0.01\n",
      "Epoch [9876/20000], Loss: -14.54034423828125, Learning Rate: 0.01\n",
      "Epoch [9877/20000], Loss: -14.531341552734375, Learning Rate: 0.01\n",
      "Epoch [9878/20000], Loss: -14.517959594726562, Learning Rate: 0.01\n",
      "Epoch [9879/20000], Loss: -14.518020629882812, Learning Rate: 0.01\n",
      "Epoch [9880/20000], Loss: -14.533157348632812, Learning Rate: 0.01\n",
      "Epoch [9881/20000], Loss: -14.55108642578125, Learning Rate: 0.01\n",
      "Epoch [9882/20000], Loss: -14.559341430664062, Learning Rate: 0.01\n",
      "Epoch [9883/20000], Loss: -14.556076049804688, Learning Rate: 0.01\n",
      "Epoch [9884/20000], Loss: -14.550369262695312, Learning Rate: 0.01\n",
      "Epoch [9885/20000], Loss: -14.551467895507812, Learning Rate: 0.01\n",
      "Epoch [9886/20000], Loss: -14.560699462890625, Learning Rate: 0.01\n",
      "Epoch [9887/20000], Loss: -14.571884155273438, Learning Rate: 0.01\n",
      "Epoch [9888/20000], Loss: -14.578125, Learning Rate: 0.01\n",
      "Epoch [9889/20000], Loss: -14.5782470703125, Learning Rate: 0.01\n",
      "Epoch [9890/20000], Loss: -14.576675415039062, Learning Rate: 0.01\n",
      "Epoch [9891/20000], Loss: -14.578201293945312, Learning Rate: 0.01\n",
      "Epoch [9892/20000], Loss: -14.584030151367188, Learning Rate: 0.01\n",
      "Epoch [9893/20000], Loss: -14.591537475585938, Learning Rate: 0.01\n",
      "Epoch [9894/20000], Loss: -14.596832275390625, Learning Rate: 0.01\n",
      "Epoch [9895/20000], Loss: -14.598770141601562, Learning Rate: 0.01\n",
      "Epoch [9896/20000], Loss: -14.599395751953125, Learning Rate: 0.01\n",
      "Epoch [9897/20000], Loss: -14.601226806640625, Learning Rate: 0.01\n",
      "Epoch [9898/20000], Loss: -14.605316162109375, Learning Rate: 0.01\n",
      "Epoch [9899/20000], Loss: -14.610748291015625, Learning Rate: 0.01\n",
      "Epoch [9900/20000], Loss: -14.615402221679688, Learning Rate: 0.01\n",
      "Epoch [9901/20000], Loss: -14.6182861328125, Learning Rate: 0.01\n",
      "Epoch [9902/20000], Loss: -14.62017822265625, Learning Rate: 0.01\n",
      "Epoch [9903/20000], Loss: -14.622329711914062, Learning Rate: 0.01\n",
      "Epoch [9904/20000], Loss: -14.625717163085938, Learning Rate: 0.01\n",
      "Epoch [9905/20000], Loss: -14.629837036132812, Learning Rate: 0.01\n",
      "Epoch [9906/20000], Loss: -14.633941650390625, Learning Rate: 0.01\n",
      "Epoch [9907/20000], Loss: -14.637252807617188, Learning Rate: 0.01\n",
      "Epoch [9908/20000], Loss: -14.639846801757812, Learning Rate: 0.01\n",
      "Epoch [9909/20000], Loss: -14.642333984375, Learning Rate: 0.01\n",
      "Epoch [9910/20000], Loss: -14.645309448242188, Learning Rate: 0.01\n",
      "Epoch [9911/20000], Loss: -14.64886474609375, Learning Rate: 0.01\n",
      "Epoch [9912/20000], Loss: -14.652618408203125, Learning Rate: 0.01\n",
      "Epoch [9913/20000], Loss: -14.656005859375, Learning Rate: 0.01\n",
      "Epoch [9914/20000], Loss: -14.658889770507812, Learning Rate: 0.01\n",
      "Epoch [9915/20000], Loss: -14.66168212890625, Learning Rate: 0.01\n",
      "Epoch [9916/20000], Loss: -14.664657592773438, Learning Rate: 0.01\n",
      "Epoch [9917/20000], Loss: -14.6678466796875, Learning Rate: 0.01\n",
      "Epoch [9918/20000], Loss: -14.671249389648438, Learning Rate: 0.01\n",
      "Epoch [9919/20000], Loss: -14.67462158203125, Learning Rate: 0.01\n",
      "Epoch [9920/20000], Loss: -14.677780151367188, Learning Rate: 0.01\n",
      "Epoch [9921/20000], Loss: -14.680755615234375, Learning Rate: 0.01\n",
      "Epoch [9922/20000], Loss: -14.683685302734375, Learning Rate: 0.01\n",
      "Epoch [9923/20000], Loss: -14.686737060546875, Learning Rate: 0.01\n",
      "Epoch [9924/20000], Loss: -14.689971923828125, Learning Rate: 0.01\n",
      "Epoch [9925/20000], Loss: -14.69317626953125, Learning Rate: 0.01\n",
      "Epoch [9926/20000], Loss: -14.696380615234375, Learning Rate: 0.01\n",
      "Epoch [9927/20000], Loss: -14.699539184570312, Learning Rate: 0.01\n",
      "Epoch [9928/20000], Loss: -14.7025146484375, Learning Rate: 0.01\n",
      "Epoch [9929/20000], Loss: -14.70562744140625, Learning Rate: 0.01\n",
      "Epoch [9930/20000], Loss: -14.708709716796875, Learning Rate: 0.01\n",
      "Epoch [9931/20000], Loss: -14.711929321289062, Learning Rate: 0.01\n",
      "Epoch [9932/20000], Loss: -14.715011596679688, Learning Rate: 0.01\n",
      "Epoch [9933/20000], Loss: -14.718292236328125, Learning Rate: 0.01\n",
      "Epoch [9934/20000], Loss: -14.72137451171875, Learning Rate: 0.01\n",
      "Epoch [9935/20000], Loss: -14.724456787109375, Learning Rate: 0.01\n",
      "Epoch [9936/20000], Loss: -14.727493286132812, Learning Rate: 0.01\n",
      "Epoch [9937/20000], Loss: -14.73065185546875, Learning Rate: 0.01\n",
      "Epoch [9938/20000], Loss: -14.73370361328125, Learning Rate: 0.01\n",
      "Epoch [9939/20000], Loss: -14.736968994140625, Learning Rate: 0.01\n",
      "Epoch [9940/20000], Loss: -14.740081787109375, Learning Rate: 0.01\n",
      "Epoch [9941/20000], Loss: -14.743118286132812, Learning Rate: 0.01\n",
      "Epoch [9942/20000], Loss: -14.746292114257812, Learning Rate: 0.01\n",
      "Epoch [9943/20000], Loss: -14.7493896484375, Learning Rate: 0.01\n",
      "Epoch [9944/20000], Loss: -14.75250244140625, Learning Rate: 0.01\n",
      "Epoch [9945/20000], Loss: -14.75555419921875, Learning Rate: 0.01\n",
      "Epoch [9946/20000], Loss: -14.758712768554688, Learning Rate: 0.01\n",
      "Epoch [9947/20000], Loss: -14.761810302734375, Learning Rate: 0.01\n",
      "Epoch [9948/20000], Loss: -14.76495361328125, Learning Rate: 0.01\n",
      "Epoch [9949/20000], Loss: -14.768020629882812, Learning Rate: 0.01\n",
      "Epoch [9950/20000], Loss: -14.771224975585938, Learning Rate: 0.01\n",
      "Epoch [9951/20000], Loss: -14.774307250976562, Learning Rate: 0.01\n",
      "Epoch [9952/20000], Loss: -14.777420043945312, Learning Rate: 0.01\n",
      "Epoch [9953/20000], Loss: -14.780593872070312, Learning Rate: 0.01\n",
      "Epoch [9954/20000], Loss: -14.783660888671875, Learning Rate: 0.01\n",
      "Epoch [9955/20000], Loss: -14.786788940429688, Learning Rate: 0.01\n",
      "Epoch [9956/20000], Loss: -14.789932250976562, Learning Rate: 0.01\n",
      "Epoch [9957/20000], Loss: -14.79302978515625, Learning Rate: 0.01\n",
      "Epoch [9958/20000], Loss: -14.796157836914062, Learning Rate: 0.01\n",
      "Epoch [9959/20000], Loss: -14.799240112304688, Learning Rate: 0.01\n",
      "Epoch [9960/20000], Loss: -14.802413940429688, Learning Rate: 0.01\n",
      "Epoch [9961/20000], Loss: -14.805511474609375, Learning Rate: 0.01\n",
      "Epoch [9962/20000], Loss: -14.808624267578125, Learning Rate: 0.01\n",
      "Epoch [9963/20000], Loss: -14.811721801757812, Learning Rate: 0.01\n",
      "Epoch [9964/20000], Loss: -14.814865112304688, Learning Rate: 0.01\n",
      "Epoch [9965/20000], Loss: -14.817962646484375, Learning Rate: 0.01\n",
      "Epoch [9966/20000], Loss: -14.821151733398438, Learning Rate: 0.01\n",
      "Epoch [9967/20000], Loss: -14.824172973632812, Learning Rate: 0.01\n",
      "Epoch [9968/20000], Loss: -14.827301025390625, Learning Rate: 0.01\n",
      "Epoch [9969/20000], Loss: -14.830398559570312, Learning Rate: 0.01\n",
      "Epoch [9970/20000], Loss: -14.833541870117188, Learning Rate: 0.01\n",
      "Epoch [9971/20000], Loss: -14.836715698242188, Learning Rate: 0.01\n",
      "Epoch [9972/20000], Loss: -14.83978271484375, Learning Rate: 0.01\n",
      "Epoch [9973/20000], Loss: -14.842910766601562, Learning Rate: 0.01\n",
      "Epoch [9974/20000], Loss: -14.846023559570312, Learning Rate: 0.01\n",
      "Epoch [9975/20000], Loss: -14.84906005859375, Learning Rate: 0.01\n",
      "Epoch [9976/20000], Loss: -14.852218627929688, Learning Rate: 0.01\n",
      "Epoch [9977/20000], Loss: -14.8553466796875, Learning Rate: 0.01\n",
      "Epoch [9978/20000], Loss: -14.858474731445312, Learning Rate: 0.01\n",
      "Epoch [9979/20000], Loss: -14.861557006835938, Learning Rate: 0.01\n",
      "Epoch [9980/20000], Loss: -14.86468505859375, Learning Rate: 0.01\n",
      "Epoch [9981/20000], Loss: -14.867889404296875, Learning Rate: 0.01\n",
      "Epoch [9982/20000], Loss: -14.870925903320312, Learning Rate: 0.01\n",
      "Epoch [9983/20000], Loss: -14.87408447265625, Learning Rate: 0.01\n",
      "Epoch [9984/20000], Loss: -14.877243041992188, Learning Rate: 0.01\n",
      "Epoch [9985/20000], Loss: -14.880401611328125, Learning Rate: 0.01\n",
      "Epoch [9986/20000], Loss: -14.883468627929688, Learning Rate: 0.01\n",
      "Epoch [9987/20000], Loss: -14.886642456054688, Learning Rate: 0.01\n",
      "Epoch [9988/20000], Loss: -14.889678955078125, Learning Rate: 0.01\n",
      "Epoch [9989/20000], Loss: -14.892837524414062, Learning Rate: 0.01\n",
      "Epoch [9990/20000], Loss: -14.89599609375, Learning Rate: 0.01\n",
      "Epoch [9991/20000], Loss: -14.8990478515625, Learning Rate: 0.01\n",
      "Epoch [9992/20000], Loss: -14.9022216796875, Learning Rate: 0.01\n",
      "Epoch [9993/20000], Loss: -14.905303955078125, Learning Rate: 0.01\n",
      "Epoch [9994/20000], Loss: -14.908462524414062, Learning Rate: 0.01\n",
      "Epoch [9995/20000], Loss: -14.911514282226562, Learning Rate: 0.01\n",
      "Epoch [9996/20000], Loss: -14.914642333984375, Learning Rate: 0.01\n",
      "Epoch [9997/20000], Loss: -14.917800903320312, Learning Rate: 0.01\n",
      "Epoch [9998/20000], Loss: -14.9208984375, Learning Rate: 0.01\n",
      "Epoch [9999/20000], Loss: -14.923980712890625, Learning Rate: 0.01\n",
      "Epoch [10000/20000], Loss: -14.92718505859375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10001/20000], Loss: -14.930252075195312, Learning Rate: 0.01\n",
      "Epoch [10002/20000], Loss: -14.933441162109375, Learning Rate: 0.01\n",
      "Epoch [10003/20000], Loss: -14.936538696289062, Learning Rate: 0.01\n",
      "Epoch [10004/20000], Loss: -14.93963623046875, Learning Rate: 0.01\n",
      "Epoch [10005/20000], Loss: -14.942825317382812, Learning Rate: 0.01\n",
      "Epoch [10006/20000], Loss: -14.94586181640625, Learning Rate: 0.01\n",
      "Epoch [10007/20000], Loss: -14.949005126953125, Learning Rate: 0.01\n",
      "Epoch [10008/20000], Loss: -14.952102661132812, Learning Rate: 0.01\n",
      "Epoch [10009/20000], Loss: -14.9552001953125, Learning Rate: 0.01\n",
      "Epoch [10010/20000], Loss: -14.958282470703125, Learning Rate: 0.01\n",
      "Epoch [10011/20000], Loss: -14.961441040039062, Learning Rate: 0.01\n",
      "Epoch [10012/20000], Loss: -14.964462280273438, Learning Rate: 0.01\n",
      "Epoch [10013/20000], Loss: -14.967529296875, Learning Rate: 0.01\n",
      "Epoch [10014/20000], Loss: -14.97064208984375, Learning Rate: 0.01\n",
      "Epoch [10015/20000], Loss: -14.97369384765625, Learning Rate: 0.01\n",
      "Epoch [10016/20000], Loss: -14.976593017578125, Learning Rate: 0.01\n",
      "Epoch [10017/20000], Loss: -14.979446411132812, Learning Rate: 0.01\n",
      "Epoch [10018/20000], Loss: -14.982391357421875, Learning Rate: 0.01\n",
      "Epoch [10019/20000], Loss: -14.985092163085938, Learning Rate: 0.01\n",
      "Epoch [10020/20000], Loss: -14.987747192382812, Learning Rate: 0.01\n",
      "Epoch [10021/20000], Loss: -14.989944458007812, Learning Rate: 0.01\n",
      "Epoch [10022/20000], Loss: -14.991943359375, Learning Rate: 0.01\n",
      "Epoch [10023/20000], Loss: -14.993545532226562, Learning Rate: 0.01\n",
      "Epoch [10024/20000], Loss: -14.994400024414062, Learning Rate: 0.01\n",
      "Epoch [10025/20000], Loss: -14.99420166015625, Learning Rate: 0.01\n",
      "Epoch [10026/20000], Loss: -14.992431640625, Learning Rate: 0.01\n",
      "Epoch [10027/20000], Loss: -14.988525390625, Learning Rate: 0.01\n",
      "Epoch [10028/20000], Loss: -14.981414794921875, Learning Rate: 0.01\n",
      "Epoch [10029/20000], Loss: -14.969467163085938, Learning Rate: 0.01\n",
      "Epoch [10030/20000], Loss: -14.950546264648438, Learning Rate: 0.01\n",
      "Epoch [10031/20000], Loss: -14.920928955078125, Learning Rate: 0.01\n",
      "Epoch [10032/20000], Loss: -14.87567138671875, Learning Rate: 0.01\n",
      "Epoch [10033/20000], Loss: -14.80712890625, Learning Rate: 0.01\n",
      "Epoch [10034/20000], Loss: -14.7042236328125, Learning Rate: 0.01\n",
      "Epoch [10035/20000], Loss: -14.5511474609375, Learning Rate: 0.01\n",
      "Epoch [10036/20000], Loss: -14.325607299804688, Learning Rate: 0.01\n",
      "Epoch [10037/20000], Loss: -13.998275756835938, Learning Rate: 0.01\n",
      "Epoch [10038/20000], Loss: -13.53375244140625, Learning Rate: 0.01\n",
      "Epoch [10039/20000], Loss: -12.898147583007812, Learning Rate: 0.01\n",
      "Epoch [10040/20000], Loss: -12.075241088867188, Learning Rate: 0.01\n",
      "Epoch [10041/20000], Loss: -11.105010986328125, Learning Rate: 0.01\n",
      "Epoch [10042/20000], Loss: -10.127426147460938, Learning Rate: 0.01\n",
      "Epoch [10043/20000], Loss: -9.428054809570312, Learning Rate: 0.01\n",
      "Epoch [10044/20000], Loss: -9.370590209960938, Learning Rate: 0.01\n",
      "Epoch [10045/20000], Loss: -10.217254638671875, Learning Rate: 0.01\n",
      "Epoch [10046/20000], Loss: -11.82867431640625, Learning Rate: 0.01\n",
      "Epoch [10047/20000], Loss: -13.610427856445312, Learning Rate: 0.01\n",
      "Epoch [10048/20000], Loss: -14.81768798828125, Learning Rate: 0.01\n",
      "Epoch [10049/20000], Loss: -15.047637939453125, Learning Rate: 0.01\n",
      "Epoch [10050/20000], Loss: -14.456512451171875, Learning Rate: 0.01\n",
      "Epoch [10051/20000], Loss: -13.59429931640625, Learning Rate: 0.01\n",
      "Epoch [10052/20000], Loss: -13.059906005859375, Learning Rate: 0.01\n",
      "Epoch [10053/20000], Loss: -13.1802978515625, Learning Rate: 0.01\n",
      "Epoch [10054/20000], Loss: -13.853240966796875, Learning Rate: 0.01\n",
      "Epoch [10055/20000], Loss: -14.637451171875, Learning Rate: 0.01\n",
      "Epoch [10056/20000], Loss: -15.077865600585938, Learning Rate: 0.01\n",
      "Epoch [10057/20000], Loss: -15.009506225585938, Learning Rate: 0.01\n",
      "Epoch [10058/20000], Loss: -14.619400024414062, Learning Rate: 0.01\n",
      "Epoch [10059/20000], Loss: -14.262313842773438, Learning Rate: 0.01\n",
      "Epoch [10060/20000], Loss: -14.204742431640625, Learning Rate: 0.01\n",
      "Epoch [10061/20000], Loss: -14.4676513671875, Learning Rate: 0.01\n",
      "Epoch [10062/20000], Loss: -14.849853515625, Learning Rate: 0.01\n",
      "Epoch [10063/20000], Loss: -15.100677490234375, Learning Rate: 0.01\n",
      "Epoch [10064/20000], Loss: -15.1016845703125, Learning Rate: 0.01\n",
      "Epoch [10065/20000], Loss: -14.92388916015625, Learning Rate: 0.01\n",
      "Epoch [10066/20000], Loss: -14.743988037109375, Learning Rate: 0.01\n",
      "Epoch [10067/20000], Loss: -14.704925537109375, Learning Rate: 0.01\n",
      "Epoch [10068/20000], Loss: -14.8260498046875, Learning Rate: 0.01\n",
      "Epoch [10069/20000], Loss: -15.011199951171875, Learning Rate: 0.01\n",
      "Epoch [10070/20000], Loss: -15.136749267578125, Learning Rate: 0.01\n",
      "Epoch [10071/20000], Loss: -15.141921997070312, Learning Rate: 0.01\n",
      "Epoch [10072/20000], Loss: -15.058731079101562, Learning Rate: 0.01\n",
      "Epoch [10073/20000], Loss: -14.972549438476562, Learning Rate: 0.01\n",
      "Epoch [10074/20000], Loss: -14.9534912109375, Learning Rate: 0.01\n",
      "Epoch [10075/20000], Loss: -15.01177978515625, Learning Rate: 0.01\n",
      "Epoch [10076/20000], Loss: -15.102203369140625, Learning Rate: 0.01\n",
      "Epoch [10077/20000], Loss: -15.1656494140625, Learning Rate: 0.01\n",
      "Epoch [10078/20000], Loss: -15.171966552734375, Learning Rate: 0.01\n",
      "Epoch [10079/20000], Loss: -15.134841918945312, Learning Rate: 0.01\n",
      "Epoch [10080/20000], Loss: -15.094284057617188, Learning Rate: 0.01\n",
      "Epoch [10081/20000], Loss: -15.0841064453125, Learning Rate: 0.01\n",
      "Epoch [10082/20000], Loss: -15.111160278320312, Learning Rate: 0.01\n",
      "Epoch [10083/20000], Loss: -15.155670166015625, Learning Rate: 0.01\n",
      "Epoch [10084/20000], Loss: -15.18975830078125, Learning Rate: 0.01\n",
      "Epoch [10085/20000], Loss: -15.197509765625, Learning Rate: 0.01\n",
      "Epoch [10086/20000], Loss: -15.183242797851562, Learning Rate: 0.01\n",
      "Epoch [10087/20000], Loss: -15.164718627929688, Learning Rate: 0.01\n",
      "Epoch [10088/20000], Loss: -15.158599853515625, Learning Rate: 0.01\n",
      "Epoch [10089/20000], Loss: -15.1702880859375, Learning Rate: 0.01\n",
      "Epoch [10090/20000], Loss: -15.192276000976562, Learning Rate: 0.01\n",
      "Epoch [10091/20000], Loss: -15.211929321289062, Learning Rate: 0.01\n",
      "Epoch [10092/20000], Loss: -15.220123291015625, Learning Rate: 0.01\n",
      "Epoch [10093/20000], Loss: -15.216934204101562, Learning Rate: 0.01\n",
      "Epoch [10094/20000], Loss: -15.209457397460938, Learning Rate: 0.01\n",
      "Epoch [10095/20000], Loss: -15.20611572265625, Learning Rate: 0.01\n",
      "Epoch [10096/20000], Loss: -15.2108154296875, Learning Rate: 0.01\n",
      "Epoch [10097/20000], Loss: -15.221755981445312, Learning Rate: 0.01\n",
      "Epoch [10098/20000], Loss: -15.233444213867188, Learning Rate: 0.01\n",
      "Epoch [10099/20000], Loss: -15.240997314453125, Learning Rate: 0.01\n",
      "Epoch [10100/20000], Loss: -15.24285888671875, Learning Rate: 0.01\n",
      "Epoch [10101/20000], Loss: -15.241455078125, Learning Rate: 0.01\n",
      "Epoch [10102/20000], Loss: -15.240341186523438, Learning Rate: 0.01\n",
      "Epoch [10103/20000], Loss: -15.242431640625, Learning Rate: 0.01\n",
      "Epoch [10104/20000], Loss: -15.248001098632812, Learning Rate: 0.01\n",
      "Epoch [10105/20000], Loss: -15.255157470703125, Learning Rate: 0.01\n",
      "Epoch [10106/20000], Loss: -15.261398315429688, Learning Rate: 0.01\n",
      "Epoch [10107/20000], Loss: -15.265243530273438, Learning Rate: 0.01\n",
      "Epoch [10108/20000], Loss: -15.266860961914062, Learning Rate: 0.01\n",
      "Epoch [10109/20000], Loss: -15.2677001953125, Learning Rate: 0.01\n",
      "Epoch [10110/20000], Loss: -15.269287109375, Learning Rate: 0.01\n",
      "Epoch [10111/20000], Loss: -15.272537231445312, Learning Rate: 0.01\n",
      "Epoch [10112/20000], Loss: -15.277084350585938, Learning Rate: 0.01\n",
      "Epoch [10113/20000], Loss: -15.281936645507812, Learning Rate: 0.01\n",
      "Epoch [10114/20000], Loss: -15.285995483398438, Learning Rate: 0.01\n",
      "Epoch [10115/20000], Loss: -15.289215087890625, Learning Rate: 0.01\n",
      "Epoch [10116/20000], Loss: -15.291366577148438, Learning Rate: 0.01\n",
      "Epoch [10117/20000], Loss: -15.29351806640625, Learning Rate: 0.01\n",
      "Epoch [10118/20000], Loss: -15.296035766601562, Learning Rate: 0.01\n",
      "Epoch [10119/20000], Loss: -15.2991943359375, Learning Rate: 0.01\n",
      "Epoch [10120/20000], Loss: -15.302932739257812, Learning Rate: 0.01\n",
      "Epoch [10121/20000], Loss: -15.306686401367188, Learning Rate: 0.01\n",
      "Epoch [10122/20000], Loss: -15.310195922851562, Learning Rate: 0.01\n",
      "Epoch [10123/20000], Loss: -15.313247680664062, Learning Rate: 0.01\n",
      "Epoch [10124/20000], Loss: -15.315780639648438, Learning Rate: 0.01\n",
      "Epoch [10125/20000], Loss: -15.3184814453125, Learning Rate: 0.01\n",
      "Epoch [10126/20000], Loss: -15.321243286132812, Learning Rate: 0.01\n",
      "Epoch [10127/20000], Loss: -15.32427978515625, Learning Rate: 0.01\n",
      "Epoch [10128/20000], Loss: -15.327606201171875, Learning Rate: 0.01\n",
      "Epoch [10129/20000], Loss: -15.331069946289062, Learning Rate: 0.01\n",
      "Epoch [10130/20000], Loss: -15.334182739257812, Learning Rate: 0.01\n",
      "Epoch [10131/20000], Loss: -15.337203979492188, Learning Rate: 0.01\n",
      "Epoch [10132/20000], Loss: -15.340042114257812, Learning Rate: 0.01\n",
      "Epoch [10133/20000], Loss: -15.34295654296875, Learning Rate: 0.01\n",
      "Epoch [10134/20000], Loss: -15.3458251953125, Learning Rate: 0.01\n",
      "Epoch [10135/20000], Loss: -15.348800659179688, Learning Rate: 0.01\n",
      "Epoch [10136/20000], Loss: -15.351898193359375, Learning Rate: 0.01\n",
      "Epoch [10137/20000], Loss: -15.35504150390625, Learning Rate: 0.01\n",
      "Epoch [10138/20000], Loss: -15.358154296875, Learning Rate: 0.01\n",
      "Epoch [10139/20000], Loss: -15.361251831054688, Learning Rate: 0.01\n",
      "Epoch [10140/20000], Loss: -15.364181518554688, Learning Rate: 0.01\n",
      "Epoch [10141/20000], Loss: -15.3670654296875, Learning Rate: 0.01\n",
      "Epoch [10142/20000], Loss: -15.370010375976562, Learning Rate: 0.01\n",
      "Epoch [10143/20000], Loss: -15.373077392578125, Learning Rate: 0.01\n",
      "Epoch [10144/20000], Loss: -15.376083374023438, Learning Rate: 0.01\n",
      "Epoch [10145/20000], Loss: -15.3790283203125, Learning Rate: 0.01\n",
      "Epoch [10146/20000], Loss: -15.382110595703125, Learning Rate: 0.01\n",
      "Epoch [10147/20000], Loss: -15.3851318359375, Learning Rate: 0.01\n",
      "Epoch [10148/20000], Loss: -15.388214111328125, Learning Rate: 0.01\n",
      "Epoch [10149/20000], Loss: -15.391082763671875, Learning Rate: 0.01\n",
      "Epoch [10150/20000], Loss: -15.39410400390625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10151/20000], Loss: -15.397125244140625, Learning Rate: 0.01\n",
      "Epoch [10152/20000], Loss: -15.400115966796875, Learning Rate: 0.01\n",
      "Epoch [10153/20000], Loss: -15.403106689453125, Learning Rate: 0.01\n",
      "Epoch [10154/20000], Loss: -15.406173706054688, Learning Rate: 0.01\n",
      "Epoch [10155/20000], Loss: -15.4091796875, Learning Rate: 0.01\n",
      "Epoch [10156/20000], Loss: -15.412246704101562, Learning Rate: 0.01\n",
      "Epoch [10157/20000], Loss: -15.41510009765625, Learning Rate: 0.01\n",
      "Epoch [10158/20000], Loss: -15.4180908203125, Learning Rate: 0.01\n",
      "Epoch [10159/20000], Loss: -15.421173095703125, Learning Rate: 0.01\n",
      "Epoch [10160/20000], Loss: -15.424087524414062, Learning Rate: 0.01\n",
      "Epoch [10161/20000], Loss: -15.42718505859375, Learning Rate: 0.01\n",
      "Epoch [10162/20000], Loss: -15.43017578125, Learning Rate: 0.01\n",
      "Epoch [10163/20000], Loss: -15.433135986328125, Learning Rate: 0.01\n",
      "Epoch [10164/20000], Loss: -15.4361572265625, Learning Rate: 0.01\n",
      "Epoch [10165/20000], Loss: -15.439132690429688, Learning Rate: 0.01\n",
      "Epoch [10166/20000], Loss: -15.442123413085938, Learning Rate: 0.01\n",
      "Epoch [10167/20000], Loss: -15.445098876953125, Learning Rate: 0.01\n",
      "Epoch [10168/20000], Loss: -15.448257446289062, Learning Rate: 0.01\n",
      "Epoch [10169/20000], Loss: -15.451141357421875, Learning Rate: 0.01\n",
      "Epoch [10170/20000], Loss: -15.454177856445312, Learning Rate: 0.01\n",
      "Epoch [10171/20000], Loss: -15.457122802734375, Learning Rate: 0.01\n",
      "Epoch [10172/20000], Loss: -15.46014404296875, Learning Rate: 0.01\n",
      "Epoch [10173/20000], Loss: -15.463134765625, Learning Rate: 0.01\n",
      "Epoch [10174/20000], Loss: -15.466079711914062, Learning Rate: 0.01\n",
      "Epoch [10175/20000], Loss: -15.469161987304688, Learning Rate: 0.01\n",
      "Epoch [10176/20000], Loss: -15.472091674804688, Learning Rate: 0.01\n",
      "Epoch [10177/20000], Loss: -15.475112915039062, Learning Rate: 0.01\n",
      "Epoch [10178/20000], Loss: -15.478164672851562, Learning Rate: 0.01\n",
      "Epoch [10179/20000], Loss: -15.48114013671875, Learning Rate: 0.01\n",
      "Epoch [10180/20000], Loss: -15.484146118164062, Learning Rate: 0.01\n",
      "Epoch [10181/20000], Loss: -15.487106323242188, Learning Rate: 0.01\n",
      "Epoch [10182/20000], Loss: -15.490158081054688, Learning Rate: 0.01\n",
      "Epoch [10183/20000], Loss: -15.493087768554688, Learning Rate: 0.01\n",
      "Epoch [10184/20000], Loss: -15.496109008789062, Learning Rate: 0.01\n",
      "Epoch [10185/20000], Loss: -15.499053955078125, Learning Rate: 0.01\n",
      "Epoch [10186/20000], Loss: -15.5020751953125, Learning Rate: 0.01\n",
      "Epoch [10187/20000], Loss: -15.505157470703125, Learning Rate: 0.01\n",
      "Epoch [10188/20000], Loss: -15.508026123046875, Learning Rate: 0.01\n",
      "Epoch [10189/20000], Loss: -15.51104736328125, Learning Rate: 0.01\n",
      "Epoch [10190/20000], Loss: -15.514144897460938, Learning Rate: 0.01\n",
      "Epoch [10191/20000], Loss: -15.517074584960938, Learning Rate: 0.01\n",
      "Epoch [10192/20000], Loss: -15.520050048828125, Learning Rate: 0.01\n",
      "Epoch [10193/20000], Loss: -15.523101806640625, Learning Rate: 0.01\n",
      "Epoch [10194/20000], Loss: -15.526031494140625, Learning Rate: 0.01\n",
      "Epoch [10195/20000], Loss: -15.52911376953125, Learning Rate: 0.01\n",
      "Epoch [10196/20000], Loss: -15.532089233398438, Learning Rate: 0.01\n",
      "Epoch [10197/20000], Loss: -15.535110473632812, Learning Rate: 0.01\n",
      "Epoch [10198/20000], Loss: -15.538116455078125, Learning Rate: 0.01\n",
      "Epoch [10199/20000], Loss: -15.541091918945312, Learning Rate: 0.01\n",
      "Epoch [10200/20000], Loss: -15.544113159179688, Learning Rate: 0.01\n",
      "Epoch [10201/20000], Loss: -15.547103881835938, Learning Rate: 0.01\n",
      "Epoch [10202/20000], Loss: -15.55010986328125, Learning Rate: 0.01\n",
      "Epoch [10203/20000], Loss: -15.553115844726562, Learning Rate: 0.01\n",
      "Epoch [10204/20000], Loss: -15.55609130859375, Learning Rate: 0.01\n",
      "Epoch [10205/20000], Loss: -15.55908203125, Learning Rate: 0.01\n",
      "Epoch [10206/20000], Loss: -15.562149047851562, Learning Rate: 0.01\n",
      "Epoch [10207/20000], Loss: -15.565093994140625, Learning Rate: 0.01\n",
      "Epoch [10208/20000], Loss: -15.568084716796875, Learning Rate: 0.01\n",
      "Epoch [10209/20000], Loss: -15.571075439453125, Learning Rate: 0.01\n",
      "Epoch [10210/20000], Loss: -15.574050903320312, Learning Rate: 0.01\n",
      "Epoch [10211/20000], Loss: -15.57708740234375, Learning Rate: 0.01\n",
      "Epoch [10212/20000], Loss: -15.58001708984375, Learning Rate: 0.01\n",
      "Epoch [10213/20000], Loss: -15.583084106445312, Learning Rate: 0.01\n",
      "Epoch [10214/20000], Loss: -15.586044311523438, Learning Rate: 0.01\n",
      "Epoch [10215/20000], Loss: -15.58905029296875, Learning Rate: 0.01\n",
      "Epoch [10216/20000], Loss: -15.592117309570312, Learning Rate: 0.01\n",
      "Epoch [10217/20000], Loss: -15.595108032226562, Learning Rate: 0.01\n",
      "Epoch [10218/20000], Loss: -15.598037719726562, Learning Rate: 0.01\n",
      "Epoch [10219/20000], Loss: -15.601028442382812, Learning Rate: 0.01\n",
      "Epoch [10220/20000], Loss: -15.604049682617188, Learning Rate: 0.01\n",
      "Epoch [10221/20000], Loss: -15.60699462890625, Learning Rate: 0.01\n",
      "Epoch [10222/20000], Loss: -15.61004638671875, Learning Rate: 0.01\n",
      "Epoch [10223/20000], Loss: -15.613037109375, Learning Rate: 0.01\n",
      "Epoch [10224/20000], Loss: -15.616043090820312, Learning Rate: 0.01\n",
      "Epoch [10225/20000], Loss: -15.6190185546875, Learning Rate: 0.01\n",
      "Epoch [10226/20000], Loss: -15.622039794921875, Learning Rate: 0.01\n",
      "Epoch [10227/20000], Loss: -15.624969482421875, Learning Rate: 0.01\n",
      "Epoch [10228/20000], Loss: -15.628021240234375, Learning Rate: 0.01\n",
      "Epoch [10229/20000], Loss: -15.630996704101562, Learning Rate: 0.01\n",
      "Epoch [10230/20000], Loss: -15.634017944335938, Learning Rate: 0.01\n",
      "Epoch [10231/20000], Loss: -15.637039184570312, Learning Rate: 0.01\n",
      "Epoch [10232/20000], Loss: -15.640029907226562, Learning Rate: 0.01\n",
      "Epoch [10233/20000], Loss: -15.64300537109375, Learning Rate: 0.01\n",
      "Epoch [10234/20000], Loss: -15.645965576171875, Learning Rate: 0.01\n",
      "Epoch [10235/20000], Loss: -15.649002075195312, Learning Rate: 0.01\n",
      "Epoch [10236/20000], Loss: -15.6519775390625, Learning Rate: 0.01\n",
      "Epoch [10237/20000], Loss: -15.654953002929688, Learning Rate: 0.01\n",
      "Epoch [10238/20000], Loss: -15.657958984375, Learning Rate: 0.01\n",
      "Epoch [10239/20000], Loss: -15.66094970703125, Learning Rate: 0.01\n",
      "Epoch [10240/20000], Loss: -15.663925170898438, Learning Rate: 0.01\n",
      "Epoch [10241/20000], Loss: -15.666976928710938, Learning Rate: 0.01\n",
      "Epoch [10242/20000], Loss: -15.66986083984375, Learning Rate: 0.01\n",
      "Epoch [10243/20000], Loss: -15.672866821289062, Learning Rate: 0.01\n",
      "Epoch [10244/20000], Loss: -15.675796508789062, Learning Rate: 0.01\n",
      "Epoch [10245/20000], Loss: -15.678634643554688, Learning Rate: 0.01\n",
      "Epoch [10246/20000], Loss: -15.681472778320312, Learning Rate: 0.01\n",
      "Epoch [10247/20000], Loss: -15.684341430664062, Learning Rate: 0.01\n",
      "Epoch [10248/20000], Loss: -15.687057495117188, Learning Rate: 0.01\n",
      "Epoch [10249/20000], Loss: -15.689697265625, Learning Rate: 0.01\n",
      "Epoch [10250/20000], Loss: -15.6922607421875, Learning Rate: 0.01\n",
      "Epoch [10251/20000], Loss: -15.694534301757812, Learning Rate: 0.01\n",
      "Epoch [10252/20000], Loss: -15.696365356445312, Learning Rate: 0.01\n",
      "Epoch [10253/20000], Loss: -15.697952270507812, Learning Rate: 0.01\n",
      "Epoch [10254/20000], Loss: -15.698699951171875, Learning Rate: 0.01\n",
      "Epoch [10255/20000], Loss: -15.698562622070312, Learning Rate: 0.01\n",
      "Epoch [10256/20000], Loss: -15.696868896484375, Learning Rate: 0.01\n",
      "Epoch [10257/20000], Loss: -15.69305419921875, Learning Rate: 0.01\n",
      "Epoch [10258/20000], Loss: -15.686004638671875, Learning Rate: 0.01\n",
      "Epoch [10259/20000], Loss: -15.673995971679688, Learning Rate: 0.01\n",
      "Epoch [10260/20000], Loss: -15.654861450195312, Learning Rate: 0.01\n",
      "Epoch [10261/20000], Loss: -15.625106811523438, Learning Rate: 0.01\n",
      "Epoch [10262/20000], Loss: -15.579254150390625, Learning Rate: 0.01\n",
      "Epoch [10263/20000], Loss: -15.509841918945312, Learning Rate: 0.01\n",
      "Epoch [10264/20000], Loss: -15.405548095703125, Learning Rate: 0.01\n",
      "Epoch [10265/20000], Loss: -15.251113891601562, Learning Rate: 0.01\n",
      "Epoch [10266/20000], Loss: -15.025619506835938, Learning Rate: 0.01\n",
      "Epoch [10267/20000], Loss: -14.705368041992188, Learning Rate: 0.01\n",
      "Epoch [10268/20000], Loss: -14.265243530273438, Learning Rate: 0.01\n",
      "Epoch [10269/20000], Loss: -13.699203491210938, Learning Rate: 0.01\n",
      "Epoch [10270/20000], Loss: -13.032562255859375, Learning Rate: 0.01\n",
      "Epoch [10271/20000], Loss: -12.379684448242188, Learning Rate: 0.01\n",
      "Epoch [10272/20000], Loss: -11.930740356445312, Learning Rate: 0.01\n",
      "Epoch [10273/20000], Loss: -11.955917358398438, Learning Rate: 0.01\n",
      "Epoch [10274/20000], Loss: -12.593521118164062, Learning Rate: 0.01\n",
      "Epoch [10275/20000], Loss: -13.728805541992188, Learning Rate: 0.01\n",
      "Epoch [10276/20000], Loss: -14.91192626953125, Learning Rate: 0.01\n",
      "Epoch [10277/20000], Loss: -15.651077270507812, Learning Rate: 0.01\n",
      "Epoch [10278/20000], Loss: -15.716766357421875, Learning Rate: 0.01\n",
      "Epoch [10279/20000], Loss: -15.263534545898438, Learning Rate: 0.01\n",
      "Epoch [10280/20000], Loss: -14.688613891601562, Learning Rate: 0.01\n",
      "Epoch [10281/20000], Loss: -14.384857177734375, Learning Rate: 0.01\n",
      "Epoch [10282/20000], Loss: -14.539566040039062, Learning Rate: 0.01\n",
      "Epoch [10283/20000], Loss: -15.034423828125, Learning Rate: 0.01\n",
      "Epoch [10284/20000], Loss: -15.55133056640625, Learning Rate: 0.01\n",
      "Epoch [10285/20000], Loss: -15.79693603515625, Learning Rate: 0.01\n",
      "Epoch [10286/20000], Loss: -15.700180053710938, Learning Rate: 0.01\n",
      "Epoch [10287/20000], Loss: -15.420394897460938, Learning Rate: 0.01\n",
      "Epoch [10288/20000], Loss: -15.202667236328125, Learning Rate: 0.01\n",
      "Epoch [10289/20000], Loss: -15.206649780273438, Learning Rate: 0.01\n",
      "Epoch [10290/20000], Loss: -15.414108276367188, Learning Rate: 0.01\n",
      "Epoch [10291/20000], Loss: -15.671463012695312, Learning Rate: 0.01\n",
      "Epoch [10292/20000], Loss: -15.814804077148438, Learning Rate: 0.01\n",
      "Epoch [10293/20000], Loss: -15.786270141601562, Learning Rate: 0.01\n",
      "Epoch [10294/20000], Loss: -15.654937744140625, Learning Rate: 0.01\n",
      "Epoch [10295/20000], Loss: -15.5452880859375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10296/20000], Loss: -15.5426025390625, Learning Rate: 0.01\n",
      "Epoch [10297/20000], Loss: -15.641769409179688, Learning Rate: 0.01\n",
      "Epoch [10298/20000], Loss: -15.767974853515625, Learning Rate: 0.01\n",
      "Epoch [10299/20000], Loss: -15.839981079101562, Learning Rate: 0.01\n",
      "Epoch [10300/20000], Loss: -15.828826904296875, Learning Rate: 0.01\n",
      "Epoch [10301/20000], Loss: -15.7672119140625, Learning Rate: 0.01\n",
      "Epoch [10302/20000], Loss: -15.715423583984375, Learning Rate: 0.01\n",
      "Epoch [10303/20000], Loss: -15.715011596679688, Learning Rate: 0.01\n",
      "Epoch [10304/20000], Loss: -15.763778686523438, Learning Rate: 0.01\n",
      "Epoch [10305/20000], Loss: -15.825927734375, Learning Rate: 0.01\n",
      "Epoch [10306/20000], Loss: -15.862686157226562, Learning Rate: 0.01\n",
      "Epoch [10307/20000], Loss: -15.859771728515625, Learning Rate: 0.01\n",
      "Epoch [10308/20000], Loss: -15.831939697265625, Learning Rate: 0.01\n",
      "Epoch [10309/20000], Loss: -15.807876586914062, Learning Rate: 0.01\n",
      "Epoch [10310/20000], Loss: -15.807464599609375, Learning Rate: 0.01\n",
      "Epoch [10311/20000], Loss: -15.831222534179688, Learning Rate: 0.01\n",
      "Epoch [10312/20000], Loss: -15.862503051757812, Learning Rate: 0.01\n",
      "Epoch [10313/20000], Loss: -15.883209228515625, Learning Rate: 0.01\n",
      "Epoch [10314/20000], Loss: -15.885223388671875, Learning Rate: 0.01\n",
      "Epoch [10315/20000], Loss: -15.8743896484375, Learning Rate: 0.01\n",
      "Epoch [10316/20000], Loss: -15.863479614257812, Learning Rate: 0.01\n",
      "Epoch [10317/20000], Loss: -15.86297607421875, Learning Rate: 0.01\n",
      "Epoch [10318/20000], Loss: -15.874176025390625, Learning Rate: 0.01\n",
      "Epoch [10319/20000], Loss: -15.890625, Learning Rate: 0.01\n",
      "Epoch [10320/20000], Loss: -15.903366088867188, Learning Rate: 0.01\n",
      "Epoch [10321/20000], Loss: -15.907730102539062, Learning Rate: 0.01\n",
      "Epoch [10322/20000], Loss: -15.9051513671875, Learning Rate: 0.01\n",
      "Epoch [10323/20000], Loss: -15.901046752929688, Learning Rate: 0.01\n",
      "Epoch [10324/20000], Loss: -15.900802612304688, Learning Rate: 0.01\n",
      "Epoch [10325/20000], Loss: -15.906158447265625, Learning Rate: 0.01\n",
      "Epoch [10326/20000], Loss: -15.9150390625, Learning Rate: 0.01\n",
      "Epoch [10327/20000], Loss: -15.92340087890625, Learning Rate: 0.01\n",
      "Epoch [10328/20000], Loss: -15.928253173828125, Learning Rate: 0.01\n",
      "Epoch [10329/20000], Loss: -15.929641723632812, Learning Rate: 0.01\n",
      "Epoch [10330/20000], Loss: -15.929244995117188, Learning Rate: 0.01\n",
      "Epoch [10331/20000], Loss: -15.92974853515625, Learning Rate: 0.01\n",
      "Epoch [10332/20000], Loss: -15.932632446289062, Learning Rate: 0.01\n",
      "Epoch [10333/20000], Loss: -15.937744140625, Learning Rate: 0.01\n",
      "Epoch [10334/20000], Loss: -15.943572998046875, Learning Rate: 0.01\n",
      "Epoch [10335/20000], Loss: -15.948211669921875, Learning Rate: 0.01\n",
      "Epoch [10336/20000], Loss: -15.951217651367188, Learning Rate: 0.01\n",
      "Epoch [10337/20000], Loss: -15.952865600585938, Learning Rate: 0.01\n",
      "Epoch [10338/20000], Loss: -15.954299926757812, Learning Rate: 0.01\n",
      "Epoch [10339/20000], Loss: -15.95648193359375, Learning Rate: 0.01\n",
      "Epoch [10340/20000], Loss: -15.959808349609375, Learning Rate: 0.01\n",
      "Epoch [10341/20000], Loss: -15.963882446289062, Learning Rate: 0.01\n",
      "Epoch [10342/20000], Loss: -15.967926025390625, Learning Rate: 0.01\n",
      "Epoch [10343/20000], Loss: -15.971405029296875, Learning Rate: 0.01\n",
      "Epoch [10344/20000], Loss: -15.974075317382812, Learning Rate: 0.01\n",
      "Epoch [10345/20000], Loss: -15.9764404296875, Learning Rate: 0.01\n",
      "Epoch [10346/20000], Loss: -15.978683471679688, Learning Rate: 0.01\n",
      "Epoch [10347/20000], Loss: -15.981338500976562, Learning Rate: 0.01\n",
      "Epoch [10348/20000], Loss: -15.984466552734375, Learning Rate: 0.01\n",
      "Epoch [10349/20000], Loss: -15.987823486328125, Learning Rate: 0.01\n",
      "Epoch [10350/20000], Loss: -15.991287231445312, Learning Rate: 0.01\n",
      "Epoch [10351/20000], Loss: -15.994277954101562, Learning Rate: 0.01\n",
      "Epoch [10352/20000], Loss: -15.997085571289062, Learning Rate: 0.01\n",
      "Epoch [10353/20000], Loss: -15.99969482421875, Learning Rate: 0.01\n",
      "Epoch [10354/20000], Loss: -16.002243041992188, Learning Rate: 0.01\n",
      "Epoch [10355/20000], Loss: -16.004959106445312, Learning Rate: 0.01\n",
      "Epoch [10356/20000], Loss: -16.008026123046875, Learning Rate: 0.01\n",
      "Epoch [10357/20000], Loss: -16.0111083984375, Learning Rate: 0.01\n",
      "Epoch [10358/20000], Loss: -16.0140380859375, Learning Rate: 0.01\n",
      "Epoch [10359/20000], Loss: -16.017074584960938, Learning Rate: 0.01\n",
      "Epoch [10360/20000], Loss: -16.0198974609375, Learning Rate: 0.01\n",
      "Epoch [10361/20000], Loss: -16.022506713867188, Learning Rate: 0.01\n",
      "Epoch [10362/20000], Loss: -16.025238037109375, Learning Rate: 0.01\n",
      "Epoch [10363/20000], Loss: -16.027847290039062, Learning Rate: 0.01\n",
      "Epoch [10364/20000], Loss: -16.030593872070312, Learning Rate: 0.01\n",
      "Epoch [10365/20000], Loss: -16.033309936523438, Learning Rate: 0.01\n",
      "Epoch [10366/20000], Loss: -16.036026000976562, Learning Rate: 0.01\n",
      "Epoch [10367/20000], Loss: -16.038558959960938, Learning Rate: 0.01\n",
      "Epoch [10368/20000], Loss: -16.040985107421875, Learning Rate: 0.01\n",
      "Epoch [10369/20000], Loss: -16.043182373046875, Learning Rate: 0.01\n",
      "Epoch [10370/20000], Loss: -16.044967651367188, Learning Rate: 0.01\n",
      "Epoch [10371/20000], Loss: -16.04644775390625, Learning Rate: 0.01\n",
      "Epoch [10372/20000], Loss: -16.047500610351562, Learning Rate: 0.01\n",
      "Epoch [10373/20000], Loss: -16.047683715820312, Learning Rate: 0.01\n",
      "Epoch [10374/20000], Loss: -16.0469970703125, Learning Rate: 0.01\n",
      "Epoch [10375/20000], Loss: -16.044830322265625, Learning Rate: 0.01\n",
      "Epoch [10376/20000], Loss: -16.040451049804688, Learning Rate: 0.01\n",
      "Epoch [10377/20000], Loss: -16.032974243164062, Learning Rate: 0.01\n",
      "Epoch [10378/20000], Loss: -16.021163940429688, Learning Rate: 0.01\n",
      "Epoch [10379/20000], Loss: -16.002975463867188, Learning Rate: 0.01\n",
      "Epoch [10380/20000], Loss: -15.975799560546875, Learning Rate: 0.01\n",
      "Epoch [10381/20000], Loss: -15.935501098632812, Learning Rate: 0.01\n",
      "Epoch [10382/20000], Loss: -15.876129150390625, Learning Rate: 0.01\n",
      "Epoch [10383/20000], Loss: -15.789947509765625, Learning Rate: 0.01\n",
      "Epoch [10384/20000], Loss: -15.665298461914062, Learning Rate: 0.01\n",
      "Epoch [10385/20000], Loss: -15.487625122070312, Learning Rate: 0.01\n",
      "Epoch [10386/20000], Loss: -15.237701416015625, Learning Rate: 0.01\n",
      "Epoch [10387/20000], Loss: -14.894500732421875, Learning Rate: 0.01\n",
      "Epoch [10388/20000], Loss: -14.43841552734375, Learning Rate: 0.01\n",
      "Epoch [10389/20000], Loss: -13.865188598632812, Learning Rate: 0.01\n",
      "Epoch [10390/20000], Loss: -13.201156616210938, Learning Rate: 0.01\n",
      "Epoch [10391/20000], Loss: -12.536544799804688, Learning Rate: 0.01\n",
      "Epoch [10392/20000], Loss: -12.033554077148438, Learning Rate: 0.01\n",
      "Epoch [10393/20000], Loss: -11.916229248046875, Learning Rate: 0.01\n",
      "Epoch [10394/20000], Loss: -12.354156494140625, Learning Rate: 0.01\n",
      "Epoch [10395/20000], Loss: -13.335342407226562, Learning Rate: 0.01\n",
      "Epoch [10396/20000], Loss: -14.568466186523438, Learning Rate: 0.01\n",
      "Epoch [10397/20000], Loss: -15.609771728515625, Learning Rate: 0.01\n",
      "Epoch [10398/20000], Loss: -16.107284545898438, Learning Rate: 0.01\n",
      "Epoch [10399/20000], Loss: -15.996719360351562, Learning Rate: 0.01\n",
      "Epoch [10400/20000], Loss: -15.498886108398438, Learning Rate: 0.01\n",
      "Epoch [10401/20000], Loss: -14.969085693359375, Learning Rate: 0.01\n",
      "Epoch [10402/20000], Loss: -14.71807861328125, Learning Rate: 0.01\n",
      "Epoch [10403/20000], Loss: -14.871978759765625, Learning Rate: 0.01\n",
      "Epoch [10404/20000], Loss: -15.328506469726562, Learning Rate: 0.01\n",
      "Epoch [10405/20000], Loss: -15.828536987304688, Learning Rate: 0.01\n",
      "Epoch [10406/20000], Loss: -16.1240234375, Learning Rate: 0.01\n",
      "Epoch [10407/20000], Loss: -16.11981201171875, Learning Rate: 0.01\n",
      "Epoch [10408/20000], Loss: -15.902847290039062, Learning Rate: 0.01\n",
      "Epoch [10409/20000], Loss: -15.660049438476562, Learning Rate: 0.01\n",
      "Epoch [10410/20000], Loss: -15.556442260742188, Learning Rate: 0.01\n",
      "Epoch [10411/20000], Loss: -15.647659301757812, Learning Rate: 0.01\n",
      "Epoch [10412/20000], Loss: -15.865386962890625, Learning Rate: 0.01\n",
      "Epoch [10413/20000], Loss: -16.077728271484375, Learning Rate: 0.01\n",
      "Epoch [10414/20000], Loss: -16.177871704101562, Learning Rate: 0.01\n",
      "Epoch [10415/20000], Loss: -16.142990112304688, Learning Rate: 0.01\n",
      "Epoch [10416/20000], Loss: -16.032470703125, Learning Rate: 0.01\n",
      "Epoch [10417/20000], Loss: -15.937332153320312, Learning Rate: 0.01\n",
      "Epoch [10418/20000], Loss: -15.921356201171875, Learning Rate: 0.01\n",
      "Epoch [10419/20000], Loss: -15.98919677734375, Learning Rate: 0.01\n",
      "Epoch [10420/20000], Loss: -16.094955444335938, Learning Rate: 0.01\n",
      "Epoch [10421/20000], Loss: -16.178085327148438, Learning Rate: 0.01\n",
      "Epoch [10422/20000], Loss: -16.202178955078125, Learning Rate: 0.01\n",
      "Epoch [10423/20000], Loss: -16.171554565429688, Learning Rate: 0.01\n",
      "Epoch [10424/20000], Loss: -16.121200561523438, Learning Rate: 0.01\n",
      "Epoch [10425/20000], Loss: -16.0899658203125, Learning Rate: 0.01\n",
      "Epoch [10426/20000], Loss: -16.098068237304688, Learning Rate: 0.01\n",
      "Epoch [10427/20000], Loss: -16.138626098632812, Learning Rate: 0.01\n",
      "Epoch [10428/20000], Loss: -16.187042236328125, Learning Rate: 0.01\n",
      "Epoch [10429/20000], Loss: -16.218612670898438, Learning Rate: 0.01\n",
      "Epoch [10430/20000], Loss: -16.222900390625, Learning Rate: 0.01\n",
      "Epoch [10431/20000], Loss: -16.206573486328125, Learning Rate: 0.01\n",
      "Epoch [10432/20000], Loss: -16.186386108398438, Learning Rate: 0.01\n",
      "Epoch [10433/20000], Loss: -16.178070068359375, Learning Rate: 0.01\n",
      "Epoch [10434/20000], Loss: -16.18719482421875, Learning Rate: 0.01\n",
      "Epoch [10435/20000], Loss: -16.20843505859375, Learning Rate: 0.01\n",
      "Epoch [10436/20000], Loss: -16.230697631835938, Learning Rate: 0.01\n",
      "Epoch [10437/20000], Loss: -16.244171142578125, Learning Rate: 0.01\n",
      "Epoch [10438/20000], Loss: -16.245803833007812, Learning Rate: 0.01\n",
      "Epoch [10439/20000], Loss: -16.239547729492188, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10440/20000], Loss: -16.232666015625, Learning Rate: 0.01\n",
      "Epoch [10441/20000], Loss: -16.231155395507812, Learning Rate: 0.01\n",
      "Epoch [10442/20000], Loss: -16.23724365234375, Learning Rate: 0.01\n",
      "Epoch [10443/20000], Loss: -16.248138427734375, Learning Rate: 0.01\n",
      "Epoch [10444/20000], Loss: -16.259307861328125, Learning Rate: 0.01\n",
      "Epoch [10445/20000], Loss: -16.266738891601562, Learning Rate: 0.01\n",
      "Epoch [10446/20000], Loss: -16.269195556640625, Learning Rate: 0.01\n",
      "Epoch [10447/20000], Loss: -16.268096923828125, Learning Rate: 0.01\n",
      "Epoch [10448/20000], Loss: -16.266677856445312, Learning Rate: 0.01\n",
      "Epoch [10449/20000], Loss: -16.267425537109375, Learning Rate: 0.01\n",
      "Epoch [10450/20000], Loss: -16.271102905273438, Learning Rate: 0.01\n",
      "Epoch [10451/20000], Loss: -16.277175903320312, Learning Rate: 0.01\n",
      "Epoch [10452/20000], Loss: -16.283676147460938, Learning Rate: 0.01\n",
      "Epoch [10453/20000], Loss: -16.288803100585938, Learning Rate: 0.01\n",
      "Epoch [10454/20000], Loss: -16.29193115234375, Learning Rate: 0.01\n",
      "Epoch [10455/20000], Loss: -16.293258666992188, Learning Rate: 0.01\n",
      "Epoch [10456/20000], Loss: -16.294235229492188, Learning Rate: 0.01\n",
      "Epoch [10457/20000], Loss: -16.295730590820312, Learning Rate: 0.01\n",
      "Epoch [10458/20000], Loss: -16.2984619140625, Learning Rate: 0.01\n",
      "Epoch [10459/20000], Loss: -16.302322387695312, Learning Rate: 0.01\n",
      "Epoch [10460/20000], Loss: -16.306671142578125, Learning Rate: 0.01\n",
      "Epoch [10461/20000], Loss: -16.31072998046875, Learning Rate: 0.01\n",
      "Epoch [10462/20000], Loss: -16.314041137695312, Learning Rate: 0.01\n",
      "Epoch [10463/20000], Loss: -16.316558837890625, Learning Rate: 0.01\n",
      "Epoch [10464/20000], Loss: -16.318588256835938, Learning Rate: 0.01\n",
      "Epoch [10465/20000], Loss: -16.320632934570312, Learning Rate: 0.01\n",
      "Epoch [10466/20000], Loss: -16.323028564453125, Learning Rate: 0.01\n",
      "Epoch [10467/20000], Loss: -16.325958251953125, Learning Rate: 0.01\n",
      "Epoch [10468/20000], Loss: -16.329208374023438, Learning Rate: 0.01\n",
      "Epoch [10469/20000], Loss: -16.332717895507812, Learning Rate: 0.01\n",
      "Epoch [10470/20000], Loss: -16.335861206054688, Learning Rate: 0.01\n",
      "Epoch [10471/20000], Loss: -16.338760375976562, Learning Rate: 0.01\n",
      "Epoch [10472/20000], Loss: -16.341354370117188, Learning Rate: 0.01\n",
      "Epoch [10473/20000], Loss: -16.343841552734375, Learning Rate: 0.01\n",
      "Epoch [10474/20000], Loss: -16.3463134765625, Learning Rate: 0.01\n",
      "Epoch [10475/20000], Loss: -16.348922729492188, Learning Rate: 0.01\n",
      "Epoch [10476/20000], Loss: -16.35174560546875, Learning Rate: 0.01\n",
      "Epoch [10477/20000], Loss: -16.35479736328125, Learning Rate: 0.01\n",
      "Epoch [10478/20000], Loss: -16.35772705078125, Learning Rate: 0.01\n",
      "Epoch [10479/20000], Loss: -16.360671997070312, Learning Rate: 0.01\n",
      "Epoch [10480/20000], Loss: -16.363494873046875, Learning Rate: 0.01\n",
      "Epoch [10481/20000], Loss: -16.366180419921875, Learning Rate: 0.01\n",
      "Epoch [10482/20000], Loss: -16.36883544921875, Learning Rate: 0.01\n",
      "Epoch [10483/20000], Loss: -16.371475219726562, Learning Rate: 0.01\n",
      "Epoch [10484/20000], Loss: -16.374221801757812, Learning Rate: 0.01\n",
      "Epoch [10485/20000], Loss: -16.376876831054688, Learning Rate: 0.01\n",
      "Epoch [10486/20000], Loss: -16.379745483398438, Learning Rate: 0.01\n",
      "Epoch [10487/20000], Loss: -16.382568359375, Learning Rate: 0.01\n",
      "Epoch [10488/20000], Loss: -16.3853759765625, Learning Rate: 0.01\n",
      "Epoch [10489/20000], Loss: -16.38812255859375, Learning Rate: 0.01\n",
      "Epoch [10490/20000], Loss: -16.390945434570312, Learning Rate: 0.01\n",
      "Epoch [10491/20000], Loss: -16.39361572265625, Learning Rate: 0.01\n",
      "Epoch [10492/20000], Loss: -16.3963623046875, Learning Rate: 0.01\n",
      "Epoch [10493/20000], Loss: -16.399032592773438, Learning Rate: 0.01\n",
      "Epoch [10494/20000], Loss: -16.401840209960938, Learning Rate: 0.01\n",
      "Epoch [10495/20000], Loss: -16.404586791992188, Learning Rate: 0.01\n",
      "Epoch [10496/20000], Loss: -16.407318115234375, Learning Rate: 0.01\n",
      "Epoch [10497/20000], Loss: -16.4100341796875, Learning Rate: 0.01\n",
      "Epoch [10498/20000], Loss: -16.412841796875, Learning Rate: 0.01\n",
      "Epoch [10499/20000], Loss: -16.415634155273438, Learning Rate: 0.01\n",
      "Epoch [10500/20000], Loss: -16.418319702148438, Learning Rate: 0.01\n",
      "Epoch [10501/20000], Loss: -16.421234130859375, Learning Rate: 0.01\n",
      "Epoch [10502/20000], Loss: -16.423828125, Learning Rate: 0.01\n",
      "Epoch [10503/20000], Loss: -16.426559448242188, Learning Rate: 0.01\n",
      "Epoch [10504/20000], Loss: -16.42926025390625, Learning Rate: 0.01\n",
      "Epoch [10505/20000], Loss: -16.432052612304688, Learning Rate: 0.01\n",
      "Epoch [10506/20000], Loss: -16.434707641601562, Learning Rate: 0.01\n",
      "Epoch [10507/20000], Loss: -16.437576293945312, Learning Rate: 0.01\n",
      "Epoch [10508/20000], Loss: -16.440292358398438, Learning Rate: 0.01\n",
      "Epoch [10509/20000], Loss: -16.443069458007812, Learning Rate: 0.01\n",
      "Epoch [10510/20000], Loss: -16.445816040039062, Learning Rate: 0.01\n",
      "Epoch [10511/20000], Loss: -16.448440551757812, Learning Rate: 0.01\n",
      "Epoch [10512/20000], Loss: -16.451217651367188, Learning Rate: 0.01\n",
      "Epoch [10513/20000], Loss: -16.453964233398438, Learning Rate: 0.01\n",
      "Epoch [10514/20000], Loss: -16.456680297851562, Learning Rate: 0.01\n",
      "Epoch [10515/20000], Loss: -16.459442138671875, Learning Rate: 0.01\n",
      "Epoch [10516/20000], Loss: -16.462234497070312, Learning Rate: 0.01\n",
      "Epoch [10517/20000], Loss: -16.465042114257812, Learning Rate: 0.01\n",
      "Epoch [10518/20000], Loss: -16.467681884765625, Learning Rate: 0.01\n",
      "Epoch [10519/20000], Loss: -16.470535278320312, Learning Rate: 0.01\n",
      "Epoch [10520/20000], Loss: -16.473190307617188, Learning Rate: 0.01\n",
      "Epoch [10521/20000], Loss: -16.475936889648438, Learning Rate: 0.01\n",
      "Epoch [10522/20000], Loss: -16.478683471679688, Learning Rate: 0.01\n",
      "Epoch [10523/20000], Loss: -16.481430053710938, Learning Rate: 0.01\n",
      "Epoch [10524/20000], Loss: -16.484161376953125, Learning Rate: 0.01\n",
      "Epoch [10525/20000], Loss: -16.486892700195312, Learning Rate: 0.01\n",
      "Epoch [10526/20000], Loss: -16.489593505859375, Learning Rate: 0.01\n",
      "Epoch [10527/20000], Loss: -16.492340087890625, Learning Rate: 0.01\n",
      "Epoch [10528/20000], Loss: -16.495101928710938, Learning Rate: 0.01\n",
      "Epoch [10529/20000], Loss: -16.497894287109375, Learning Rate: 0.01\n",
      "Epoch [10530/20000], Loss: -16.500640869140625, Learning Rate: 0.01\n",
      "Epoch [10531/20000], Loss: -16.50335693359375, Learning Rate: 0.01\n",
      "Epoch [10532/20000], Loss: -16.50604248046875, Learning Rate: 0.01\n",
      "Epoch [10533/20000], Loss: -16.508865356445312, Learning Rate: 0.01\n",
      "Epoch [10534/20000], Loss: -16.51153564453125, Learning Rate: 0.01\n",
      "Epoch [10535/20000], Loss: -16.514297485351562, Learning Rate: 0.01\n",
      "Epoch [10536/20000], Loss: -16.517059326171875, Learning Rate: 0.01\n",
      "Epoch [10537/20000], Loss: -16.519775390625, Learning Rate: 0.01\n",
      "Epoch [10538/20000], Loss: -16.522430419921875, Learning Rate: 0.01\n",
      "Epoch [10539/20000], Loss: -16.525299072265625, Learning Rate: 0.01\n",
      "Epoch [10540/20000], Loss: -16.527938842773438, Learning Rate: 0.01\n",
      "Epoch [10541/20000], Loss: -16.530654907226562, Learning Rate: 0.01\n",
      "Epoch [10542/20000], Loss: -16.533462524414062, Learning Rate: 0.01\n",
      "Epoch [10543/20000], Loss: -16.536178588867188, Learning Rate: 0.01\n",
      "Epoch [10544/20000], Loss: -16.538986206054688, Learning Rate: 0.01\n",
      "Epoch [10545/20000], Loss: -16.541671752929688, Learning Rate: 0.01\n",
      "Epoch [10546/20000], Loss: -16.544418334960938, Learning Rate: 0.01\n",
      "Epoch [10547/20000], Loss: -16.547134399414062, Learning Rate: 0.01\n",
      "Epoch [10548/20000], Loss: -16.549942016601562, Learning Rate: 0.01\n",
      "Epoch [10549/20000], Loss: -16.552566528320312, Learning Rate: 0.01\n",
      "Epoch [10550/20000], Loss: -16.55535888671875, Learning Rate: 0.01\n",
      "Epoch [10551/20000], Loss: -16.558120727539062, Learning Rate: 0.01\n",
      "Epoch [10552/20000], Loss: -16.560821533203125, Learning Rate: 0.01\n",
      "Epoch [10553/20000], Loss: -16.563583374023438, Learning Rate: 0.01\n",
      "Epoch [10554/20000], Loss: -16.566299438476562, Learning Rate: 0.01\n",
      "Epoch [10555/20000], Loss: -16.568984985351562, Learning Rate: 0.01\n",
      "Epoch [10556/20000], Loss: -16.571640014648438, Learning Rate: 0.01\n",
      "Epoch [10557/20000], Loss: -16.574508666992188, Learning Rate: 0.01\n",
      "Epoch [10558/20000], Loss: -16.577178955078125, Learning Rate: 0.01\n",
      "Epoch [10559/20000], Loss: -16.5799560546875, Learning Rate: 0.01\n",
      "Epoch [10560/20000], Loss: -16.582656860351562, Learning Rate: 0.01\n",
      "Epoch [10561/20000], Loss: -16.585433959960938, Learning Rate: 0.01\n",
      "Epoch [10562/20000], Loss: -16.588119506835938, Learning Rate: 0.01\n",
      "Epoch [10563/20000], Loss: -16.590927124023438, Learning Rate: 0.01\n",
      "Epoch [10564/20000], Loss: -16.593643188476562, Learning Rate: 0.01\n",
      "Epoch [10565/20000], Loss: -16.596343994140625, Learning Rate: 0.01\n",
      "Epoch [10566/20000], Loss: -16.599105834960938, Learning Rate: 0.01\n",
      "Epoch [10567/20000], Loss: -16.601852416992188, Learning Rate: 0.01\n",
      "Epoch [10568/20000], Loss: -16.60455322265625, Learning Rate: 0.01\n",
      "Epoch [10569/20000], Loss: -16.607376098632812, Learning Rate: 0.01\n",
      "Epoch [10570/20000], Loss: -16.61004638671875, Learning Rate: 0.01\n",
      "Epoch [10571/20000], Loss: -16.61285400390625, Learning Rate: 0.01\n",
      "Epoch [10572/20000], Loss: -16.61553955078125, Learning Rate: 0.01\n",
      "Epoch [10573/20000], Loss: -16.61822509765625, Learning Rate: 0.01\n",
      "Epoch [10574/20000], Loss: -16.621002197265625, Learning Rate: 0.01\n",
      "Epoch [10575/20000], Loss: -16.62371826171875, Learning Rate: 0.01\n",
      "Epoch [10576/20000], Loss: -16.626373291015625, Learning Rate: 0.01\n",
      "Epoch [10577/20000], Loss: -16.629165649414062, Learning Rate: 0.01\n",
      "Epoch [10578/20000], Loss: -16.63189697265625, Learning Rate: 0.01\n",
      "Epoch [10579/20000], Loss: -16.634658813476562, Learning Rate: 0.01\n",
      "Epoch [10580/20000], Loss: -16.637344360351562, Learning Rate: 0.01\n",
      "Epoch [10581/20000], Loss: -16.64007568359375, Learning Rate: 0.01\n",
      "Epoch [10582/20000], Loss: -16.642807006835938, Learning Rate: 0.01\n",
      "Epoch [10583/20000], Loss: -16.645477294921875, Learning Rate: 0.01\n",
      "Epoch [10584/20000], Loss: -16.648239135742188, Learning Rate: 0.01\n",
      "Epoch [10585/20000], Loss: -16.6510009765625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10586/20000], Loss: -16.65374755859375, Learning Rate: 0.01\n",
      "Epoch [10587/20000], Loss: -16.656387329101562, Learning Rate: 0.01\n",
      "Epoch [10588/20000], Loss: -16.659133911132812, Learning Rate: 0.01\n",
      "Epoch [10589/20000], Loss: -16.661941528320312, Learning Rate: 0.01\n",
      "Epoch [10590/20000], Loss: -16.664581298828125, Learning Rate: 0.01\n",
      "Epoch [10591/20000], Loss: -16.667327880859375, Learning Rate: 0.01\n",
      "Epoch [10592/20000], Loss: -16.670074462890625, Learning Rate: 0.01\n",
      "Epoch [10593/20000], Loss: -16.672775268554688, Learning Rate: 0.01\n",
      "Epoch [10594/20000], Loss: -16.675521850585938, Learning Rate: 0.01\n",
      "Epoch [10595/20000], Loss: -16.6783447265625, Learning Rate: 0.01\n",
      "Epoch [10596/20000], Loss: -16.680999755859375, Learning Rate: 0.01\n",
      "Epoch [10597/20000], Loss: -16.683792114257812, Learning Rate: 0.01\n",
      "Epoch [10598/20000], Loss: -16.686355590820312, Learning Rate: 0.01\n",
      "Epoch [10599/20000], Loss: -16.689193725585938, Learning Rate: 0.01\n",
      "Epoch [10600/20000], Loss: -16.691879272460938, Learning Rate: 0.01\n",
      "Epoch [10601/20000], Loss: -16.694625854492188, Learning Rate: 0.01\n",
      "Epoch [10602/20000], Loss: -16.697280883789062, Learning Rate: 0.01\n",
      "Epoch [10603/20000], Loss: -16.700057983398438, Learning Rate: 0.01\n",
      "Epoch [10604/20000], Loss: -16.70269775390625, Learning Rate: 0.01\n",
      "Epoch [10605/20000], Loss: -16.705429077148438, Learning Rate: 0.01\n",
      "Epoch [10606/20000], Loss: -16.70819091796875, Learning Rate: 0.01\n",
      "Epoch [10607/20000], Loss: -16.710845947265625, Learning Rate: 0.01\n",
      "Epoch [10608/20000], Loss: -16.71337890625, Learning Rate: 0.01\n",
      "Epoch [10609/20000], Loss: -16.716079711914062, Learning Rate: 0.01\n",
      "Epoch [10610/20000], Loss: -16.71868896484375, Learning Rate: 0.01\n",
      "Epoch [10611/20000], Loss: -16.721206665039062, Learning Rate: 0.01\n",
      "Epoch [10612/20000], Loss: -16.723587036132812, Learning Rate: 0.01\n",
      "Epoch [10613/20000], Loss: -16.725906372070312, Learning Rate: 0.01\n",
      "Epoch [10614/20000], Loss: -16.727920532226562, Learning Rate: 0.01\n",
      "Epoch [10615/20000], Loss: -16.729812622070312, Learning Rate: 0.01\n",
      "Epoch [10616/20000], Loss: -16.73126220703125, Learning Rate: 0.01\n",
      "Epoch [10617/20000], Loss: -16.732147216796875, Learning Rate: 0.01\n",
      "Epoch [10618/20000], Loss: -16.732101440429688, Learning Rate: 0.01\n",
      "Epoch [10619/20000], Loss: -16.730804443359375, Learning Rate: 0.01\n",
      "Epoch [10620/20000], Loss: -16.7276611328125, Learning Rate: 0.01\n",
      "Epoch [10621/20000], Loss: -16.721588134765625, Learning Rate: 0.01\n",
      "Epoch [10622/20000], Loss: -16.71142578125, Learning Rate: 0.01\n",
      "Epoch [10623/20000], Loss: -16.69488525390625, Learning Rate: 0.01\n",
      "Epoch [10624/20000], Loss: -16.6689453125, Learning Rate: 0.01\n",
      "Epoch [10625/20000], Loss: -16.629119873046875, Learning Rate: 0.01\n",
      "Epoch [10626/20000], Loss: -16.568130493164062, Learning Rate: 0.01\n",
      "Epoch [10627/20000], Loss: -16.476760864257812, Learning Rate: 0.01\n",
      "Epoch [10628/20000], Loss: -16.340484619140625, Learning Rate: 0.01\n",
      "Epoch [10629/20000], Loss: -16.141769409179688, Learning Rate: 0.01\n",
      "Epoch [10630/20000], Loss: -15.856735229492188, Learning Rate: 0.01\n",
      "Epoch [10631/20000], Loss: -15.466033935546875, Learning Rate: 0.01\n",
      "Epoch [10632/20000], Loss: -14.956344604492188, Learning Rate: 0.01\n",
      "Epoch [10633/20000], Loss: -14.358688354492188, Learning Rate: 0.01\n",
      "Epoch [10634/20000], Loss: -13.756195068359375, Learning Rate: 0.01\n",
      "Epoch [10635/20000], Loss: -13.3426513671875, Learning Rate: 0.01\n",
      "Epoch [10636/20000], Loss: -13.333358764648438, Learning Rate: 0.01\n",
      "Epoch [10637/20000], Loss: -13.897125244140625, Learning Rate: 0.01\n",
      "Epoch [10638/20000], Loss: -14.909408569335938, Learning Rate: 0.01\n",
      "Epoch [10639/20000], Loss: -15.988845825195312, Learning Rate: 0.01\n",
      "Epoch [10640/20000], Loss: -16.671981811523438, Learning Rate: 0.01\n",
      "Epoch [10641/20000], Loss: -16.742279052734375, Learning Rate: 0.01\n",
      "Epoch [10642/20000], Loss: -16.333343505859375, Learning Rate: 0.01\n",
      "Epoch [10643/20000], Loss: -15.805145263671875, Learning Rate: 0.01\n",
      "Epoch [10644/20000], Loss: -15.523635864257812, Learning Rate: 0.01\n",
      "Epoch [10645/20000], Loss: -15.658462524414062, Learning Rate: 0.01\n",
      "Epoch [10646/20000], Loss: -16.109466552734375, Learning Rate: 0.01\n",
      "Epoch [10647/20000], Loss: -16.579391479492188, Learning Rate: 0.01\n",
      "Epoch [10648/20000], Loss: -16.80010986328125, Learning Rate: 0.01\n",
      "Epoch [10649/20000], Loss: -16.707077026367188, Learning Rate: 0.01\n",
      "Epoch [10650/20000], Loss: -16.449508666992188, Learning Rate: 0.01\n",
      "Epoch [10651/20000], Loss: -16.255386352539062, Learning Rate: 0.01\n",
      "Epoch [10652/20000], Loss: -16.266830444335938, Learning Rate: 0.01\n",
      "Epoch [10653/20000], Loss: -16.464508056640625, Learning Rate: 0.01\n",
      "Epoch [10654/20000], Loss: -16.700241088867188, Learning Rate: 0.01\n",
      "Epoch [10655/20000], Loss: -16.824600219726562, Learning Rate: 0.01\n",
      "Epoch [10656/20000], Loss: -16.790618896484375, Learning Rate: 0.01\n",
      "Epoch [10657/20000], Loss: -16.667953491210938, Learning Rate: 0.01\n",
      "Epoch [10658/20000], Loss: -16.573379516601562, Learning Rate: 0.01\n",
      "Epoch [10659/20000], Loss: -16.580947875976562, Learning Rate: 0.01\n",
      "Epoch [10660/20000], Loss: -16.679611206054688, Learning Rate: 0.01\n",
      "Epoch [10661/20000], Loss: -16.794998168945312, Learning Rate: 0.01\n",
      "Epoch [10662/20000], Loss: -16.853927612304688, Learning Rate: 0.01\n",
      "Epoch [10663/20000], Loss: -16.835693359375, Learning Rate: 0.01\n",
      "Epoch [10664/20000], Loss: -16.7767333984375, Learning Rate: 0.01\n",
      "Epoch [10665/20000], Loss: -16.734146118164062, Learning Rate: 0.01\n",
      "Epoch [10666/20000], Loss: -16.742034912109375, Learning Rate: 0.01\n",
      "Epoch [10667/20000], Loss: -16.792510986328125, Learning Rate: 0.01\n",
      "Epoch [10668/20000], Loss: -16.848922729492188, Learning Rate: 0.01\n",
      "Epoch [10669/20000], Loss: -16.877059936523438, Learning Rate: 0.01\n",
      "Epoch [10670/20000], Loss: -16.86834716796875, Learning Rate: 0.01\n",
      "Epoch [10671/20000], Loss: -16.840911865234375, Learning Rate: 0.01\n",
      "Epoch [10672/20000], Loss: -16.822006225585938, Learning Rate: 0.01\n",
      "Epoch [10673/20000], Loss: -16.827239990234375, Learning Rate: 0.01\n",
      "Epoch [10674/20000], Loss: -16.852676391601562, Learning Rate: 0.01\n",
      "Epoch [10675/20000], Loss: -16.880950927734375, Learning Rate: 0.01\n",
      "Epoch [10676/20000], Loss: -16.895919799804688, Learning Rate: 0.01\n",
      "Epoch [10677/20000], Loss: -16.893280029296875, Learning Rate: 0.01\n",
      "Epoch [10678/20000], Loss: -16.881195068359375, Learning Rate: 0.01\n",
      "Epoch [10679/20000], Loss: -16.872543334960938, Learning Rate: 0.01\n",
      "Epoch [10680/20000], Loss: -16.874862670898438, Learning Rate: 0.01\n",
      "Epoch [10681/20000], Loss: -16.887130737304688, Learning Rate: 0.01\n",
      "Epoch [10682/20000], Loss: -16.901397705078125, Learning Rate: 0.01\n",
      "Epoch [10683/20000], Loss: -16.909942626953125, Learning Rate: 0.01\n",
      "Epoch [10684/20000], Loss: -16.909896850585938, Learning Rate: 0.01\n",
      "Epoch [10685/20000], Loss: -16.904403686523438, Learning Rate: 0.01\n",
      "Epoch [10686/20000], Loss: -16.899368286132812, Learning Rate: 0.01\n",
      "Epoch [10687/20000], Loss: -16.89862060546875, Learning Rate: 0.01\n",
      "Epoch [10688/20000], Loss: -16.902435302734375, Learning Rate: 0.01\n",
      "Epoch [10689/20000], Loss: -16.9073486328125, Learning Rate: 0.01\n",
      "Epoch [10690/20000], Loss: -16.909591674804688, Learning Rate: 0.01\n",
      "Epoch [10691/20000], Loss: -16.90679931640625, Learning Rate: 0.01\n",
      "Epoch [10692/20000], Loss: -16.899627685546875, Learning Rate: 0.01\n",
      "Epoch [10693/20000], Loss: -16.890274047851562, Learning Rate: 0.01\n",
      "Epoch [10694/20000], Loss: -16.880447387695312, Learning Rate: 0.01\n",
      "Epoch [10695/20000], Loss: -16.870315551757812, Learning Rate: 0.01\n",
      "Epoch [10696/20000], Loss: -16.857757568359375, Learning Rate: 0.01\n",
      "Epoch [10697/20000], Loss: -16.840362548828125, Learning Rate: 0.01\n",
      "Epoch [10698/20000], Loss: -16.815399169921875, Learning Rate: 0.01\n",
      "Epoch [10699/20000], Loss: -16.781478881835938, Learning Rate: 0.01\n",
      "Epoch [10700/20000], Loss: -16.737686157226562, Learning Rate: 0.01\n",
      "Epoch [10701/20000], Loss: -16.683090209960938, Learning Rate: 0.01\n",
      "Epoch [10702/20000], Loss: -16.616043090820312, Learning Rate: 0.01\n",
      "Epoch [10703/20000], Loss: -16.534378051757812, Learning Rate: 0.01\n",
      "Epoch [10704/20000], Loss: -16.435256958007812, Learning Rate: 0.01\n",
      "Epoch [10705/20000], Loss: -16.317581176757812, Learning Rate: 0.01\n",
      "Epoch [10706/20000], Loss: -16.181869506835938, Learning Rate: 0.01\n",
      "Epoch [10707/20000], Loss: -16.033782958984375, Learning Rate: 0.01\n",
      "Epoch [10708/20000], Loss: -15.882568359375, Learning Rate: 0.01\n",
      "Epoch [10709/20000], Loss: -15.745330810546875, Learning Rate: 0.01\n",
      "Epoch [10710/20000], Loss: -15.642532348632812, Learning Rate: 0.01\n",
      "Epoch [10711/20000], Loss: -15.599639892578125, Learning Rate: 0.01\n",
      "Epoch [10712/20000], Loss: -15.636367797851562, Learning Rate: 0.01\n",
      "Epoch [10713/20000], Loss: -15.764801025390625, Learning Rate: 0.01\n",
      "Epoch [10714/20000], Loss: -15.97589111328125, Learning Rate: 0.01\n",
      "Epoch [10715/20000], Loss: -16.242401123046875, Learning Rate: 0.01\n",
      "Epoch [10716/20000], Loss: -16.519699096679688, Learning Rate: 0.01\n",
      "Epoch [10717/20000], Loss: -16.760650634765625, Learning Rate: 0.01\n",
      "Epoch [10718/20000], Loss: -16.928802490234375, Learning Rate: 0.01\n",
      "Epoch [10719/20000], Loss: -17.008331298828125, Learning Rate: 0.01\n",
      "Epoch [10720/20000], Loss: -17.005828857421875, Learning Rate: 0.01\n",
      "Epoch [10721/20000], Loss: -16.944747924804688, Learning Rate: 0.01\n",
      "Epoch [10722/20000], Loss: -16.857025146484375, Learning Rate: 0.01\n",
      "Epoch [10723/20000], Loss: -16.773834228515625, Learning Rate: 0.01\n",
      "Epoch [10724/20000], Loss: -16.7191162109375, Learning Rate: 0.01\n",
      "Epoch [10725/20000], Loss: -16.705795288085938, Learning Rate: 0.01\n",
      "Epoch [10726/20000], Loss: -16.734832763671875, Learning Rate: 0.01\n",
      "Epoch [10727/20000], Loss: -16.796127319335938, Learning Rate: 0.01\n",
      "Epoch [10728/20000], Loss: -16.873245239257812, Learning Rate: 0.01\n",
      "Epoch [10729/20000], Loss: -16.948333740234375, Learning Rate: 0.01\n",
      "Epoch [10730/20000], Loss: -17.0067138671875, Learning Rate: 0.01\n",
      "Epoch [10731/20000], Loss: -17.040359497070312, Learning Rate: 0.01\n",
      "Epoch [10732/20000], Loss: -17.048660278320312, Learning Rate: 0.01\n",
      "Epoch [10733/20000], Loss: -17.037124633789062, Learning Rate: 0.01\n",
      "Epoch [10734/20000], Loss: -17.014846801757812, Learning Rate: 0.01\n",
      "Epoch [10735/20000], Loss: -16.99127197265625, Learning Rate: 0.01\n",
      "Epoch [10736/20000], Loss: -16.974349975585938, Learning Rate: 0.01\n",
      "Epoch [10737/20000], Loss: -16.968612670898438, Learning Rate: 0.01\n",
      "Epoch [10738/20000], Loss: -16.9749755859375, Learning Rate: 0.01\n",
      "Epoch [10739/20000], Loss: -16.99163818359375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10740/20000], Loss: -17.014083862304688, Learning Rate: 0.01\n",
      "Epoch [10741/20000], Loss: -17.037643432617188, Learning Rate: 0.01\n",
      "Epoch [10742/20000], Loss: -17.057952880859375, Learning Rate: 0.01\n",
      "Epoch [10743/20000], Loss: -17.072525024414062, Learning Rate: 0.01\n",
      "Epoch [10744/20000], Loss: -17.080215454101562, Learning Rate: 0.01\n",
      "Epoch [10745/20000], Loss: -17.08184814453125, Learning Rate: 0.01\n",
      "Epoch [10746/20000], Loss: -17.079254150390625, Learning Rate: 0.01\n",
      "Epoch [10747/20000], Loss: -17.074874877929688, Learning Rate: 0.01\n",
      "Epoch [10748/20000], Loss: -17.07086181640625, Learning Rate: 0.01\n",
      "Epoch [10749/20000], Loss: -17.068817138671875, Learning Rate: 0.01\n",
      "Epoch [10750/20000], Loss: -17.069564819335938, Learning Rate: 0.01\n",
      "Epoch [10751/20000], Loss: -17.073165893554688, Learning Rate: 0.01\n",
      "Epoch [10752/20000], Loss: -17.0792236328125, Learning Rate: 0.01\n",
      "Epoch [10753/20000], Loss: -17.086746215820312, Learning Rate: 0.01\n",
      "Epoch [10754/20000], Loss: -17.094680786132812, Learning Rate: 0.01\n",
      "Epoch [10755/20000], Loss: -17.102020263671875, Learning Rate: 0.01\n",
      "Epoch [10756/20000], Loss: -17.108245849609375, Learning Rate: 0.01\n",
      "Epoch [10757/20000], Loss: -17.112884521484375, Learning Rate: 0.01\n",
      "Epoch [10758/20000], Loss: -17.116195678710938, Learning Rate: 0.01\n",
      "Epoch [10759/20000], Loss: -17.118270874023438, Learning Rate: 0.01\n",
      "Epoch [10760/20000], Loss: -17.119552612304688, Learning Rate: 0.01\n",
      "Epoch [10761/20000], Loss: -17.120452880859375, Learning Rate: 0.01\n",
      "Epoch [10762/20000], Loss: -17.121536254882812, Learning Rate: 0.01\n",
      "Epoch [10763/20000], Loss: -17.122940063476562, Learning Rate: 0.01\n",
      "Epoch [10764/20000], Loss: -17.124862670898438, Learning Rate: 0.01\n",
      "Epoch [10765/20000], Loss: -17.12738037109375, Learning Rate: 0.01\n",
      "Epoch [10766/20000], Loss: -17.130477905273438, Learning Rate: 0.01\n",
      "Epoch [10767/20000], Loss: -17.13385009765625, Learning Rate: 0.01\n",
      "Epoch [10768/20000], Loss: -17.137451171875, Learning Rate: 0.01\n",
      "Epoch [10769/20000], Loss: -17.141189575195312, Learning Rate: 0.01\n",
      "Epoch [10770/20000], Loss: -17.144744873046875, Learning Rate: 0.01\n",
      "Epoch [10771/20000], Loss: -17.148178100585938, Learning Rate: 0.01\n",
      "Epoch [10772/20000], Loss: -17.151260375976562, Learning Rate: 0.01\n",
      "Epoch [10773/20000], Loss: -17.154083251953125, Learning Rate: 0.01\n",
      "Epoch [10774/20000], Loss: -17.15673828125, Learning Rate: 0.01\n",
      "Epoch [10775/20000], Loss: -17.159149169921875, Learning Rate: 0.01\n",
      "Epoch [10776/20000], Loss: -17.1614990234375, Learning Rate: 0.01\n",
      "Epoch [10777/20000], Loss: -17.163726806640625, Learning Rate: 0.01\n",
      "Epoch [10778/20000], Loss: -17.166046142578125, Learning Rate: 0.01\n",
      "Epoch [10779/20000], Loss: -17.168319702148438, Learning Rate: 0.01\n",
      "Epoch [10780/20000], Loss: -17.170608520507812, Learning Rate: 0.01\n",
      "Epoch [10781/20000], Loss: -17.173095703125, Learning Rate: 0.01\n",
      "Epoch [10782/20000], Loss: -17.175582885742188, Learning Rate: 0.01\n",
      "Epoch [10783/20000], Loss: -17.1781005859375, Learning Rate: 0.01\n",
      "Epoch [10784/20000], Loss: -17.180740356445312, Learning Rate: 0.01\n",
      "Epoch [10785/20000], Loss: -17.183425903320312, Learning Rate: 0.01\n",
      "Epoch [10786/20000], Loss: -17.186080932617188, Learning Rate: 0.01\n",
      "Epoch [10787/20000], Loss: -17.188812255859375, Learning Rate: 0.01\n",
      "Epoch [10788/20000], Loss: -17.191482543945312, Learning Rate: 0.01\n",
      "Epoch [10789/20000], Loss: -17.194168090820312, Learning Rate: 0.01\n",
      "Epoch [10790/20000], Loss: -17.196853637695312, Learning Rate: 0.01\n",
      "Epoch [10791/20000], Loss: -17.199493408203125, Learning Rate: 0.01\n",
      "Epoch [10792/20000], Loss: -17.202072143554688, Learning Rate: 0.01\n",
      "Epoch [10793/20000], Loss: -17.204666137695312, Learning Rate: 0.01\n",
      "Epoch [10794/20000], Loss: -17.207199096679688, Learning Rate: 0.01\n",
      "Epoch [10795/20000], Loss: -17.209762573242188, Learning Rate: 0.01\n",
      "Epoch [10796/20000], Loss: -17.212310791015625, Learning Rate: 0.01\n",
      "Epoch [10797/20000], Loss: -17.214767456054688, Learning Rate: 0.01\n",
      "Epoch [10798/20000], Loss: -17.217361450195312, Learning Rate: 0.01\n",
      "Epoch [10799/20000], Loss: -17.219894409179688, Learning Rate: 0.01\n",
      "Epoch [10800/20000], Loss: -17.222366333007812, Learning Rate: 0.01\n",
      "Epoch [10801/20000], Loss: -17.224822998046875, Learning Rate: 0.01\n",
      "Epoch [10802/20000], Loss: -17.22735595703125, Learning Rate: 0.01\n",
      "Epoch [10803/20000], Loss: -17.229873657226562, Learning Rate: 0.01\n",
      "Epoch [10804/20000], Loss: -17.232391357421875, Learning Rate: 0.01\n",
      "Epoch [10805/20000], Loss: -17.234848022460938, Learning Rate: 0.01\n",
      "Epoch [10806/20000], Loss: -17.23736572265625, Learning Rate: 0.01\n",
      "Epoch [10807/20000], Loss: -17.239837646484375, Learning Rate: 0.01\n",
      "Epoch [10808/20000], Loss: -17.242263793945312, Learning Rate: 0.01\n",
      "Epoch [10809/20000], Loss: -17.244903564453125, Learning Rate: 0.01\n",
      "Epoch [10810/20000], Loss: -17.247360229492188, Learning Rate: 0.01\n",
      "Epoch [10811/20000], Loss: -17.249893188476562, Learning Rate: 0.01\n",
      "Epoch [10812/20000], Loss: -17.25238037109375, Learning Rate: 0.01\n",
      "Epoch [10813/20000], Loss: -17.25494384765625, Learning Rate: 0.01\n",
      "Epoch [10814/20000], Loss: -17.257431030273438, Learning Rate: 0.01\n",
      "Epoch [10815/20000], Loss: -17.259918212890625, Learning Rate: 0.01\n",
      "Epoch [10816/20000], Loss: -17.262405395507812, Learning Rate: 0.01\n",
      "Epoch [10817/20000], Loss: -17.264938354492188, Learning Rate: 0.01\n",
      "Epoch [10818/20000], Loss: -17.267410278320312, Learning Rate: 0.01\n",
      "Epoch [10819/20000], Loss: -17.269821166992188, Learning Rate: 0.01\n",
      "Epoch [10820/20000], Loss: -17.2723388671875, Learning Rate: 0.01\n",
      "Epoch [10821/20000], Loss: -17.274810791015625, Learning Rate: 0.01\n",
      "Epoch [10822/20000], Loss: -17.277297973632812, Learning Rate: 0.01\n",
      "Epoch [10823/20000], Loss: -17.279739379882812, Learning Rate: 0.01\n",
      "Epoch [10824/20000], Loss: -17.28216552734375, Learning Rate: 0.01\n",
      "Epoch [10825/20000], Loss: -17.284683227539062, Learning Rate: 0.01\n",
      "Epoch [10826/20000], Loss: -17.287078857421875, Learning Rate: 0.01\n",
      "Epoch [10827/20000], Loss: -17.289505004882812, Learning Rate: 0.01\n",
      "Epoch [10828/20000], Loss: -17.291839599609375, Learning Rate: 0.01\n",
      "Epoch [10829/20000], Loss: -17.294158935546875, Learning Rate: 0.01\n",
      "Epoch [10830/20000], Loss: -17.29638671875, Learning Rate: 0.01\n",
      "Epoch [10831/20000], Loss: -17.29864501953125, Learning Rate: 0.01\n",
      "Epoch [10832/20000], Loss: -17.30078125, Learning Rate: 0.01\n",
      "Epoch [10833/20000], Loss: -17.30279541015625, Learning Rate: 0.01\n",
      "Epoch [10834/20000], Loss: -17.304794311523438, Learning Rate: 0.01\n",
      "Epoch [10835/20000], Loss: -17.306533813476562, Learning Rate: 0.01\n",
      "Epoch [10836/20000], Loss: -17.30810546875, Learning Rate: 0.01\n",
      "Epoch [10837/20000], Loss: -17.309371948242188, Learning Rate: 0.01\n",
      "Epoch [10838/20000], Loss: -17.310302734375, Learning Rate: 0.01\n",
      "Epoch [10839/20000], Loss: -17.310714721679688, Learning Rate: 0.01\n",
      "Epoch [10840/20000], Loss: -17.31060791015625, Learning Rate: 0.01\n",
      "Epoch [10841/20000], Loss: -17.309600830078125, Learning Rate: 0.01\n",
      "Epoch [10842/20000], Loss: -17.3074951171875, Learning Rate: 0.01\n",
      "Epoch [10843/20000], Loss: -17.303985595703125, Learning Rate: 0.01\n",
      "Epoch [10844/20000], Loss: -17.29840087890625, Learning Rate: 0.01\n",
      "Epoch [10845/20000], Loss: -17.290328979492188, Learning Rate: 0.01\n",
      "Epoch [10846/20000], Loss: -17.278732299804688, Learning Rate: 0.01\n",
      "Epoch [10847/20000], Loss: -17.262466430664062, Learning Rate: 0.01\n",
      "Epoch [10848/20000], Loss: -17.23980712890625, Learning Rate: 0.01\n",
      "Epoch [10849/20000], Loss: -17.20867919921875, Learning Rate: 0.01\n",
      "Epoch [10850/20000], Loss: -17.166336059570312, Learning Rate: 0.01\n",
      "Epoch [10851/20000], Loss: -17.109054565429688, Learning Rate: 0.01\n",
      "Epoch [10852/20000], Loss: -17.032363891601562, Learning Rate: 0.01\n",
      "Epoch [10853/20000], Loss: -16.930862426757812, Learning Rate: 0.01\n",
      "Epoch [10854/20000], Loss: -16.798568725585938, Learning Rate: 0.01\n",
      "Epoch [10855/20000], Loss: -16.62994384765625, Learning Rate: 0.01\n",
      "Epoch [10856/20000], Loss: -16.42120361328125, Learning Rate: 0.01\n",
      "Epoch [10857/20000], Loss: -16.174636840820312, Learning Rate: 0.01\n",
      "Epoch [10858/20000], Loss: -15.900863647460938, Learning Rate: 0.01\n",
      "Epoch [10859/20000], Loss: -15.627731323242188, Learning Rate: 0.01\n",
      "Epoch [10860/20000], Loss: -15.398727416992188, Learning Rate: 0.01\n",
      "Epoch [10861/20000], Loss: -15.276077270507812, Learning Rate: 0.01\n",
      "Epoch [10862/20000], Loss: -15.31781005859375, Learning Rate: 0.01\n",
      "Epoch [10863/20000], Loss: -15.559188842773438, Learning Rate: 0.01\n",
      "Epoch [10864/20000], Loss: -15.975173950195312, Learning Rate: 0.01\n",
      "Epoch [10865/20000], Loss: -16.4810791015625, Learning Rate: 0.01\n",
      "Epoch [10866/20000], Loss: -16.950897216796875, Learning Rate: 0.01\n",
      "Epoch [10867/20000], Loss: -17.273635864257812, Learning Rate: 0.01\n",
      "Epoch [10868/20000], Loss: -17.39453125, Learning Rate: 0.01\n",
      "Epoch [10869/20000], Loss: -17.331756591796875, Learning Rate: 0.01\n",
      "Epoch [10870/20000], Loss: -17.157455444335938, Learning Rate: 0.01\n",
      "Epoch [10871/20000], Loss: -16.964599609375, Learning Rate: 0.01\n",
      "Epoch [10872/20000], Loss: -16.835464477539062, Learning Rate: 0.01\n",
      "Epoch [10873/20000], Loss: -16.81634521484375, Learning Rate: 0.01\n",
      "Epoch [10874/20000], Loss: -16.908615112304688, Learning Rate: 0.01\n",
      "Epoch [10875/20000], Loss: -17.07177734375, Learning Rate: 0.01\n",
      "Epoch [10876/20000], Loss: -17.244644165039062, Learning Rate: 0.01\n",
      "Epoch [10877/20000], Loss: -17.371109008789062, Learning Rate: 0.01\n",
      "Epoch [10878/20000], Loss: -17.421112060546875, Learning Rate: 0.01\n",
      "Epoch [10879/20000], Loss: -17.398635864257812, Learning Rate: 0.01\n",
      "Epoch [10880/20000], Loss: -17.333084106445312, Learning Rate: 0.01\n",
      "Epoch [10881/20000], Loss: -17.263442993164062, Learning Rate: 0.01\n",
      "Epoch [10882/20000], Loss: -17.222412109375, Learning Rate: 0.01\n",
      "Epoch [10883/20000], Loss: -17.225692749023438, Learning Rate: 0.01\n",
      "Epoch [10884/20000], Loss: -17.2689208984375, Learning Rate: 0.01\n",
      "Epoch [10885/20000], Loss: -17.333236694335938, Learning Rate: 0.01\n",
      "Epoch [10886/20000], Loss: -17.394790649414062, Learning Rate: 0.01\n",
      "Epoch [10887/20000], Loss: -17.434982299804688, Learning Rate: 0.01\n",
      "Epoch [10888/20000], Loss: -17.446487426757812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10889/20000], Loss: -17.433761596679688, Learning Rate: 0.01\n",
      "Epoch [10890/20000], Loss: -17.40948486328125, Learning Rate: 0.01\n",
      "Epoch [10891/20000], Loss: -17.38751220703125, Learning Rate: 0.01\n",
      "Epoch [10892/20000], Loss: -17.378204345703125, Learning Rate: 0.01\n",
      "Epoch [10893/20000], Loss: -17.3848876953125, Learning Rate: 0.01\n",
      "Epoch [10894/20000], Loss: -17.404617309570312, Learning Rate: 0.01\n",
      "Epoch [10895/20000], Loss: -17.42987060546875, Learning Rate: 0.01\n",
      "Epoch [10896/20000], Loss: -17.452423095703125, Learning Rate: 0.01\n",
      "Epoch [10897/20000], Loss: -17.466796875, Learning Rate: 0.01\n",
      "Epoch [10898/20000], Loss: -17.471298217773438, Learning Rate: 0.01\n",
      "Epoch [10899/20000], Loss: -17.4676513671875, Learning Rate: 0.01\n",
      "Epoch [10900/20000], Loss: -17.460540771484375, Learning Rate: 0.01\n",
      "Epoch [10901/20000], Loss: -17.454544067382812, Learning Rate: 0.01\n",
      "Epoch [10902/20000], Loss: -17.4527587890625, Learning Rate: 0.01\n",
      "Epoch [10903/20000], Loss: -17.456649780273438, Learning Rate: 0.01\n",
      "Epoch [10904/20000], Loss: -17.464920043945312, Learning Rate: 0.01\n",
      "Epoch [10905/20000], Loss: -17.475204467773438, Learning Rate: 0.01\n",
      "Epoch [10906/20000], Loss: -17.484939575195312, Learning Rate: 0.01\n",
      "Epoch [10907/20000], Loss: -17.4921875, Learning Rate: 0.01\n",
      "Epoch [10908/20000], Loss: -17.496124267578125, Learning Rate: 0.01\n",
      "Epoch [10909/20000], Loss: -17.49725341796875, Learning Rate: 0.01\n",
      "Epoch [10910/20000], Loss: -17.496627807617188, Learning Rate: 0.01\n",
      "Epoch [10911/20000], Loss: -17.49591064453125, Learning Rate: 0.01\n",
      "Epoch [10912/20000], Loss: -17.496231079101562, Learning Rate: 0.01\n",
      "Epoch [10913/20000], Loss: -17.498321533203125, Learning Rate: 0.01\n",
      "Epoch [10914/20000], Loss: -17.501998901367188, Learning Rate: 0.01\n",
      "Epoch [10915/20000], Loss: -17.50677490234375, Learning Rate: 0.01\n",
      "Epoch [10916/20000], Loss: -17.51171875, Learning Rate: 0.01\n",
      "Epoch [10917/20000], Loss: -17.516342163085938, Learning Rate: 0.01\n",
      "Epoch [10918/20000], Loss: -17.520156860351562, Learning Rate: 0.01\n",
      "Epoch [10919/20000], Loss: -17.5228271484375, Learning Rate: 0.01\n",
      "Epoch [10920/20000], Loss: -17.524810791015625, Learning Rate: 0.01\n",
      "Epoch [10921/20000], Loss: -17.526229858398438, Learning Rate: 0.01\n",
      "Epoch [10922/20000], Loss: -17.52764892578125, Learning Rate: 0.01\n",
      "Epoch [10923/20000], Loss: -17.5294189453125, Learning Rate: 0.01\n",
      "Epoch [10924/20000], Loss: -17.531692504882812, Learning Rate: 0.01\n",
      "Epoch [10925/20000], Loss: -17.534530639648438, Learning Rate: 0.01\n",
      "Epoch [10926/20000], Loss: -17.537506103515625, Learning Rate: 0.01\n",
      "Epoch [10927/20000], Loss: -17.540802001953125, Learning Rate: 0.01\n",
      "Epoch [10928/20000], Loss: -17.543914794921875, Learning Rate: 0.01\n",
      "Epoch [10929/20000], Loss: -17.546920776367188, Learning Rate: 0.01\n",
      "Epoch [10930/20000], Loss: -17.549591064453125, Learning Rate: 0.01\n",
      "Epoch [10931/20000], Loss: -17.5518798828125, Learning Rate: 0.01\n",
      "Epoch [10932/20000], Loss: -17.55413818359375, Learning Rate: 0.01\n",
      "Epoch [10933/20000], Loss: -17.556350708007812, Learning Rate: 0.01\n",
      "Epoch [10934/20000], Loss: -17.558502197265625, Learning Rate: 0.01\n",
      "Epoch [10935/20000], Loss: -17.560699462890625, Learning Rate: 0.01\n",
      "Epoch [10936/20000], Loss: -17.563034057617188, Learning Rate: 0.01\n",
      "Epoch [10937/20000], Loss: -17.5655517578125, Learning Rate: 0.01\n",
      "Epoch [10938/20000], Loss: -17.568145751953125, Learning Rate: 0.01\n",
      "Epoch [10939/20000], Loss: -17.57080078125, Learning Rate: 0.01\n",
      "Epoch [10940/20000], Loss: -17.573455810546875, Learning Rate: 0.01\n",
      "Epoch [10941/20000], Loss: -17.576034545898438, Learning Rate: 0.01\n",
      "Epoch [10942/20000], Loss: -17.57867431640625, Learning Rate: 0.01\n",
      "Epoch [10943/20000], Loss: -17.581024169921875, Learning Rate: 0.01\n",
      "Epoch [10944/20000], Loss: -17.583465576171875, Learning Rate: 0.01\n",
      "Epoch [10945/20000], Loss: -17.585784912109375, Learning Rate: 0.01\n",
      "Epoch [10946/20000], Loss: -17.588043212890625, Learning Rate: 0.01\n",
      "Epoch [10947/20000], Loss: -17.590377807617188, Learning Rate: 0.01\n",
      "Epoch [10948/20000], Loss: -17.592849731445312, Learning Rate: 0.01\n",
      "Epoch [10949/20000], Loss: -17.595199584960938, Learning Rate: 0.01\n",
      "Epoch [10950/20000], Loss: -17.597625732421875, Learning Rate: 0.01\n",
      "Epoch [10951/20000], Loss: -17.600143432617188, Learning Rate: 0.01\n",
      "Epoch [10952/20000], Loss: -17.602630615234375, Learning Rate: 0.01\n",
      "Epoch [10953/20000], Loss: -17.605056762695312, Learning Rate: 0.01\n",
      "Epoch [10954/20000], Loss: -17.607513427734375, Learning Rate: 0.01\n",
      "Epoch [10955/20000], Loss: -17.610031127929688, Learning Rate: 0.01\n",
      "Epoch [10956/20000], Loss: -17.6124267578125, Learning Rate: 0.01\n",
      "Epoch [10957/20000], Loss: -17.614837646484375, Learning Rate: 0.01\n",
      "Epoch [10958/20000], Loss: -17.617218017578125, Learning Rate: 0.01\n",
      "Epoch [10959/20000], Loss: -17.619674682617188, Learning Rate: 0.01\n",
      "Epoch [10960/20000], Loss: -17.62200927734375, Learning Rate: 0.01\n",
      "Epoch [10961/20000], Loss: -17.624359130859375, Learning Rate: 0.01\n",
      "Epoch [10962/20000], Loss: -17.6268310546875, Learning Rate: 0.01\n",
      "Epoch [10963/20000], Loss: -17.62921142578125, Learning Rate: 0.01\n",
      "Epoch [10964/20000], Loss: -17.631576538085938, Learning Rate: 0.01\n",
      "Epoch [10965/20000], Loss: -17.634048461914062, Learning Rate: 0.01\n",
      "Epoch [10966/20000], Loss: -17.636505126953125, Learning Rate: 0.01\n",
      "Epoch [10967/20000], Loss: -17.638809204101562, Learning Rate: 0.01\n",
      "Epoch [10968/20000], Loss: -17.641265869140625, Learning Rate: 0.01\n",
      "Epoch [10969/20000], Loss: -17.643661499023438, Learning Rate: 0.01\n",
      "Epoch [10970/20000], Loss: -17.646072387695312, Learning Rate: 0.01\n",
      "Epoch [10971/20000], Loss: -17.648468017578125, Learning Rate: 0.01\n",
      "Epoch [10972/20000], Loss: -17.650909423828125, Learning Rate: 0.01\n",
      "Epoch [10973/20000], Loss: -17.653289794921875, Learning Rate: 0.01\n",
      "Epoch [10974/20000], Loss: -17.655746459960938, Learning Rate: 0.01\n",
      "Epoch [10975/20000], Loss: -17.658172607421875, Learning Rate: 0.01\n",
      "Epoch [10976/20000], Loss: -17.660598754882812, Learning Rate: 0.01\n",
      "Epoch [10977/20000], Loss: -17.6629638671875, Learning Rate: 0.01\n",
      "Epoch [10978/20000], Loss: -17.665328979492188, Learning Rate: 0.01\n",
      "Epoch [10979/20000], Loss: -17.667770385742188, Learning Rate: 0.01\n",
      "Epoch [10980/20000], Loss: -17.670150756835938, Learning Rate: 0.01\n",
      "Epoch [10981/20000], Loss: -17.672470092773438, Learning Rate: 0.01\n",
      "Epoch [10982/20000], Loss: -17.674911499023438, Learning Rate: 0.01\n",
      "Epoch [10983/20000], Loss: -17.677291870117188, Learning Rate: 0.01\n",
      "Epoch [10984/20000], Loss: -17.679718017578125, Learning Rate: 0.01\n",
      "Epoch [10985/20000], Loss: -17.682052612304688, Learning Rate: 0.01\n",
      "Epoch [10986/20000], Loss: -17.684417724609375, Learning Rate: 0.01\n",
      "Epoch [10987/20000], Loss: -17.686859130859375, Learning Rate: 0.01\n",
      "Epoch [10988/20000], Loss: -17.689193725585938, Learning Rate: 0.01\n",
      "Epoch [10989/20000], Loss: -17.6915283203125, Learning Rate: 0.01\n",
      "Epoch [10990/20000], Loss: -17.69390869140625, Learning Rate: 0.01\n",
      "Epoch [10991/20000], Loss: -17.696258544921875, Learning Rate: 0.01\n",
      "Epoch [10992/20000], Loss: -17.698455810546875, Learning Rate: 0.01\n",
      "Epoch [10993/20000], Loss: -17.700653076171875, Learning Rate: 0.01\n",
      "Epoch [10994/20000], Loss: -17.702804565429688, Learning Rate: 0.01\n",
      "Epoch [10995/20000], Loss: -17.704879760742188, Learning Rate: 0.01\n",
      "Epoch [10996/20000], Loss: -17.706893920898438, Learning Rate: 0.01\n",
      "Epoch [10997/20000], Loss: -17.70867919921875, Learning Rate: 0.01\n",
      "Epoch [10998/20000], Loss: -17.710205078125, Learning Rate: 0.01\n",
      "Epoch [10999/20000], Loss: -17.71136474609375, Learning Rate: 0.01\n",
      "Epoch [11000/20000], Loss: -17.712066650390625, Learning Rate: 0.01\n",
      "Epoch [11001/20000], Loss: -17.712142944335938, Learning Rate: 0.01\n",
      "Epoch [11002/20000], Loss: -17.711151123046875, Learning Rate: 0.01\n",
      "Epoch [11003/20000], Loss: -17.7086181640625, Learning Rate: 0.01\n",
      "Epoch [11004/20000], Loss: -17.704055786132812, Learning Rate: 0.01\n",
      "Epoch [11005/20000], Loss: -17.696456909179688, Learning Rate: 0.01\n",
      "Epoch [11006/20000], Loss: -17.684478759765625, Learning Rate: 0.01\n",
      "Epoch [11007/20000], Loss: -17.666046142578125, Learning Rate: 0.01\n",
      "Epoch [11008/20000], Loss: -17.638427734375, Learning Rate: 0.01\n",
      "Epoch [11009/20000], Loss: -17.597274780273438, Learning Rate: 0.01\n",
      "Epoch [11010/20000], Loss: -17.537155151367188, Learning Rate: 0.01\n",
      "Epoch [11011/20000], Loss: -17.449844360351562, Learning Rate: 0.01\n",
      "Epoch [11012/20000], Loss: -17.32525634765625, Learning Rate: 0.01\n",
      "Epoch [11013/20000], Loss: -17.149856567382812, Learning Rate: 0.01\n",
      "Epoch [11014/20000], Loss: -16.911636352539062, Learning Rate: 0.01\n",
      "Epoch [11015/20000], Loss: -16.599227905273438, Learning Rate: 0.01\n",
      "Epoch [11016/20000], Loss: -16.2205810546875, Learning Rate: 0.01\n",
      "Epoch [11017/20000], Loss: -15.803512573242188, Learning Rate: 0.01\n",
      "Epoch [11018/20000], Loss: -15.43505859375, Learning Rate: 0.01\n",
      "Epoch [11019/20000], Loss: -15.228759765625, Learning Rate: 0.01\n",
      "Epoch [11020/20000], Loss: -15.331954956054688, Learning Rate: 0.01\n",
      "Epoch [11021/20000], Loss: -15.787017822265625, Learning Rate: 0.01\n",
      "Epoch [11022/20000], Loss: -16.503646850585938, Learning Rate: 0.01\n",
      "Epoch [11023/20000], Loss: -17.223114013671875, Learning Rate: 0.01\n",
      "Epoch [11024/20000], Loss: -17.68450927734375, Learning Rate: 0.01\n",
      "Epoch [11025/20000], Loss: -17.762954711914062, Learning Rate: 0.01\n",
      "Epoch [11026/20000], Loss: -17.529617309570312, Learning Rate: 0.01\n",
      "Epoch [11027/20000], Loss: -17.185699462890625, Learning Rate: 0.01\n",
      "Epoch [11028/20000], Loss: -16.947418212890625, Learning Rate: 0.01\n",
      "Epoch [11029/20000], Loss: -16.948257446289062, Learning Rate: 0.01\n",
      "Epoch [11030/20000], Loss: -17.174789428710938, Learning Rate: 0.01\n",
      "Epoch [11031/20000], Loss: -17.494415283203125, Learning Rate: 0.01\n",
      "Epoch [11032/20000], Loss: -17.734909057617188, Learning Rate: 0.01\n",
      "Epoch [11033/20000], Loss: -17.796218872070312, Learning Rate: 0.01\n",
      "Epoch [11034/20000], Loss: -17.695938110351562, Learning Rate: 0.01\n",
      "Epoch [11035/20000], Loss: -17.537948608398438, Learning Rate: 0.01\n",
      "Epoch [11036/20000], Loss: -17.438552856445312, Learning Rate: 0.01\n",
      "Epoch [11037/20000], Loss: -17.458251953125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11038/20000], Loss: -17.57763671875, Learning Rate: 0.01\n",
      "Epoch [11039/20000], Loss: -17.719558715820312, Learning Rate: 0.01\n",
      "Epoch [11040/20000], Loss: -17.806915283203125, Learning Rate: 0.01\n",
      "Epoch [11041/20000], Loss: -17.807937622070312, Learning Rate: 0.01\n",
      "Epoch [11042/20000], Loss: -17.747116088867188, Learning Rate: 0.01\n",
      "Epoch [11043/20000], Loss: -17.680694580078125, Learning Rate: 0.01\n",
      "Epoch [11044/20000], Loss: -17.656158447265625, Learning Rate: 0.01\n",
      "Epoch [11045/20000], Loss: -17.6873779296875, Learning Rate: 0.01\n",
      "Epoch [11046/20000], Loss: -17.751632690429688, Learning Rate: 0.01\n",
      "Epoch [11047/20000], Loss: -17.810562133789062, Learning Rate: 0.01\n",
      "Epoch [11048/20000], Loss: -17.835617065429688, Learning Rate: 0.01\n",
      "Epoch [11049/20000], Loss: -17.823287963867188, Learning Rate: 0.01\n",
      "Epoch [11050/20000], Loss: -17.792709350585938, Learning Rate: 0.01\n",
      "Epoch [11051/20000], Loss: -17.769561767578125, Learning Rate: 0.01\n",
      "Epoch [11052/20000], Loss: -17.769912719726562, Learning Rate: 0.01\n",
      "Epoch [11053/20000], Loss: -17.792861938476562, Learning Rate: 0.01\n",
      "Epoch [11054/20000], Loss: -17.824234008789062, Learning Rate: 0.01\n",
      "Epoch [11055/20000], Loss: -17.84735107421875, Learning Rate: 0.01\n",
      "Epoch [11056/20000], Loss: -17.8536376953125, Learning Rate: 0.01\n",
      "Epoch [11057/20000], Loss: -17.845321655273438, Learning Rate: 0.01\n",
      "Epoch [11058/20000], Loss: -17.832794189453125, Learning Rate: 0.01\n",
      "Epoch [11059/20000], Loss: -17.826507568359375, Learning Rate: 0.01\n",
      "Epoch [11060/20000], Loss: -17.831298828125, Learning Rate: 0.01\n",
      "Epoch [11061/20000], Loss: -17.844558715820312, Learning Rate: 0.01\n",
      "Epoch [11062/20000], Loss: -17.85955810546875, Learning Rate: 0.01\n",
      "Epoch [11063/20000], Loss: -17.869522094726562, Learning Rate: 0.01\n",
      "Epoch [11064/20000], Loss: -17.871932983398438, Learning Rate: 0.01\n",
      "Epoch [11065/20000], Loss: -17.868865966796875, Learning Rate: 0.01\n",
      "Epoch [11066/20000], Loss: -17.86480712890625, Learning Rate: 0.01\n",
      "Epoch [11067/20000], Loss: -17.864044189453125, Learning Rate: 0.01\n",
      "Epoch [11068/20000], Loss: -17.86798095703125, Learning Rate: 0.01\n",
      "Epoch [11069/20000], Loss: -17.875289916992188, Learning Rate: 0.01\n",
      "Epoch [11070/20000], Loss: -17.883071899414062, Learning Rate: 0.01\n",
      "Epoch [11071/20000], Loss: -17.88861083984375, Learning Rate: 0.01\n",
      "Epoch [11072/20000], Loss: -17.890762329101562, Learning Rate: 0.01\n",
      "Epoch [11073/20000], Loss: -17.890716552734375, Learning Rate: 0.01\n",
      "Epoch [11074/20000], Loss: -17.890304565429688, Learning Rate: 0.01\n",
      "Epoch [11075/20000], Loss: -17.891265869140625, Learning Rate: 0.01\n",
      "Epoch [11076/20000], Loss: -17.89410400390625, Learning Rate: 0.01\n",
      "Epoch [11077/20000], Loss: -17.898452758789062, Learning Rate: 0.01\n",
      "Epoch [11078/20000], Loss: -17.903121948242188, Learning Rate: 0.01\n",
      "Epoch [11079/20000], Loss: -17.906951904296875, Learning Rate: 0.01\n",
      "Epoch [11080/20000], Loss: -17.909408569335938, Learning Rate: 0.01\n",
      "Epoch [11081/20000], Loss: -17.91082763671875, Learning Rate: 0.01\n",
      "Epoch [11082/20000], Loss: -17.911956787109375, Learning Rate: 0.01\n",
      "Epoch [11083/20000], Loss: -17.913436889648438, Learning Rate: 0.01\n",
      "Epoch [11084/20000], Loss: -17.915802001953125, Learning Rate: 0.01\n",
      "Epoch [11085/20000], Loss: -17.918746948242188, Learning Rate: 0.01\n",
      "Epoch [11086/20000], Loss: -17.92205810546875, Learning Rate: 0.01\n",
      "Epoch [11087/20000], Loss: -17.925201416015625, Learning Rate: 0.01\n",
      "Epoch [11088/20000], Loss: -17.927780151367188, Learning Rate: 0.01\n",
      "Epoch [11089/20000], Loss: -17.929901123046875, Learning Rate: 0.01\n",
      "Epoch [11090/20000], Loss: -17.93170166015625, Learning Rate: 0.01\n",
      "Epoch [11091/20000], Loss: -17.933425903320312, Learning Rate: 0.01\n",
      "Epoch [11092/20000], Loss: -17.935592651367188, Learning Rate: 0.01\n",
      "Epoch [11093/20000], Loss: -17.938003540039062, Learning Rate: 0.01\n",
      "Epoch [11094/20000], Loss: -17.940719604492188, Learning Rate: 0.01\n",
      "Epoch [11095/20000], Loss: -17.943374633789062, Learning Rate: 0.01\n",
      "Epoch [11096/20000], Loss: -17.94598388671875, Learning Rate: 0.01\n",
      "Epoch [11097/20000], Loss: -17.948318481445312, Learning Rate: 0.01\n",
      "Epoch [11098/20000], Loss: -17.950408935546875, Learning Rate: 0.01\n",
      "Epoch [11099/20000], Loss: -17.9525146484375, Learning Rate: 0.01\n",
      "Epoch [11100/20000], Loss: -17.95465087890625, Learning Rate: 0.01\n",
      "Epoch [11101/20000], Loss: -17.956863403320312, Learning Rate: 0.01\n",
      "Epoch [11102/20000], Loss: -17.959243774414062, Learning Rate: 0.01\n",
      "Epoch [11103/20000], Loss: -17.961654663085938, Learning Rate: 0.01\n",
      "Epoch [11104/20000], Loss: -17.964080810546875, Learning Rate: 0.01\n",
      "Epoch [11105/20000], Loss: -17.966400146484375, Learning Rate: 0.01\n",
      "Epoch [11106/20000], Loss: -17.968795776367188, Learning Rate: 0.01\n",
      "Epoch [11107/20000], Loss: -17.970962524414062, Learning Rate: 0.01\n",
      "Epoch [11108/20000], Loss: -17.973220825195312, Learning Rate: 0.01\n",
      "Epoch [11109/20000], Loss: -17.975418090820312, Learning Rate: 0.01\n",
      "Epoch [11110/20000], Loss: -17.977691650390625, Learning Rate: 0.01\n",
      "Epoch [11111/20000], Loss: -17.979934692382812, Learning Rate: 0.01\n",
      "Epoch [11112/20000], Loss: -17.982192993164062, Learning Rate: 0.01\n",
      "Epoch [11113/20000], Loss: -17.984542846679688, Learning Rate: 0.01\n",
      "Epoch [11114/20000], Loss: -17.986907958984375, Learning Rate: 0.01\n",
      "Epoch [11115/20000], Loss: -17.9891357421875, Learning Rate: 0.01\n",
      "Epoch [11116/20000], Loss: -17.99139404296875, Learning Rate: 0.01\n",
      "Epoch [11117/20000], Loss: -17.993621826171875, Learning Rate: 0.01\n",
      "Epoch [11118/20000], Loss: -17.995834350585938, Learning Rate: 0.01\n",
      "Epoch [11119/20000], Loss: -17.998123168945312, Learning Rate: 0.01\n",
      "Epoch [11120/20000], Loss: -18.000289916992188, Learning Rate: 0.01\n",
      "Epoch [11121/20000], Loss: -18.002456665039062, Learning Rate: 0.01\n",
      "Epoch [11122/20000], Loss: -18.004669189453125, Learning Rate: 0.01\n",
      "Epoch [11123/20000], Loss: -18.006820678710938, Learning Rate: 0.01\n",
      "Epoch [11124/20000], Loss: -18.008895874023438, Learning Rate: 0.01\n",
      "Epoch [11125/20000], Loss: -18.01092529296875, Learning Rate: 0.01\n",
      "Epoch [11126/20000], Loss: -18.012786865234375, Learning Rate: 0.01\n",
      "Epoch [11127/20000], Loss: -18.014511108398438, Learning Rate: 0.01\n",
      "Epoch [11128/20000], Loss: -18.015960693359375, Learning Rate: 0.01\n",
      "Epoch [11129/20000], Loss: -18.017120361328125, Learning Rate: 0.01\n",
      "Epoch [11130/20000], Loss: -18.017715454101562, Learning Rate: 0.01\n",
      "Epoch [11131/20000], Loss: -18.017578125, Learning Rate: 0.01\n",
      "Epoch [11132/20000], Loss: -18.016494750976562, Learning Rate: 0.01\n",
      "Epoch [11133/20000], Loss: -18.013809204101562, Learning Rate: 0.01\n",
      "Epoch [11134/20000], Loss: -18.00872802734375, Learning Rate: 0.01\n",
      "Epoch [11135/20000], Loss: -18.000289916992188, Learning Rate: 0.01\n",
      "Epoch [11136/20000], Loss: -17.9866943359375, Learning Rate: 0.01\n",
      "Epoch [11137/20000], Loss: -17.965805053710938, Learning Rate: 0.01\n",
      "Epoch [11138/20000], Loss: -17.933822631835938, Learning Rate: 0.01\n",
      "Epoch [11139/20000], Loss: -17.885589599609375, Learning Rate: 0.01\n",
      "Epoch [11140/20000], Loss: -17.813491821289062, Learning Rate: 0.01\n",
      "Epoch [11141/20000], Loss: -17.707000732421875, Learning Rate: 0.01\n",
      "Epoch [11142/20000], Loss: -17.551116943359375, Learning Rate: 0.01\n",
      "Epoch [11143/20000], Loss: -17.327178955078125, Learning Rate: 0.01\n",
      "Epoch [11144/20000], Loss: -17.012863159179688, Learning Rate: 0.01\n",
      "Epoch [11145/20000], Loss: -16.589645385742188, Learning Rate: 0.01\n",
      "Epoch [11146/20000], Loss: -16.051666259765625, Learning Rate: 0.01\n",
      "Epoch [11147/20000], Loss: -15.43499755859375, Learning Rate: 0.01\n",
      "Epoch [11148/20000], Loss: -14.8372802734375, Learning Rate: 0.01\n",
      "Epoch [11149/20000], Loss: -14.448043823242188, Learning Rate: 0.01\n",
      "Epoch [11150/20000], Loss: -14.484024047851562, Learning Rate: 0.01\n",
      "Epoch [11151/20000], Loss: -15.084564208984375, Learning Rate: 0.01\n",
      "Epoch [11152/20000], Loss: -16.1251220703125, Learning Rate: 0.01\n",
      "Epoch [11153/20000], Loss: -17.225662231445312, Learning Rate: 0.01\n",
      "Epoch [11154/20000], Loss: -17.94097900390625, Learning Rate: 0.01\n",
      "Epoch [11155/20000], Loss: -18.052291870117188, Learning Rate: 0.01\n",
      "Epoch [11156/20000], Loss: -17.672988891601562, Learning Rate: 0.01\n",
      "Epoch [11157/20000], Loss: -17.1417236328125, Learning Rate: 0.01\n",
      "Epoch [11158/20000], Loss: -16.817367553710938, Learning Rate: 0.01\n",
      "Epoch [11159/20000], Loss: -16.89208984375, Learning Rate: 0.01\n",
      "Epoch [11160/20000], Loss: -17.30438232421875, Learning Rate: 0.01\n",
      "Epoch [11161/20000], Loss: -17.790313720703125, Learning Rate: 0.01\n",
      "Epoch [11162/20000], Loss: -18.075180053710938, Learning Rate: 0.01\n",
      "Epoch [11163/20000], Loss: -18.051071166992188, Learning Rate: 0.01\n",
      "Epoch [11164/20000], Loss: -17.8204345703125, Learning Rate: 0.01\n",
      "Epoch [11165/20000], Loss: -17.59326171875, Learning Rate: 0.01\n",
      "Epoch [11166/20000], Loss: -17.537460327148438, Learning Rate: 0.01\n",
      "Epoch [11167/20000], Loss: -17.680862426757812, Learning Rate: 0.01\n",
      "Epoch [11168/20000], Loss: -17.913955688476562, Learning Rate: 0.01\n",
      "Epoch [11169/20000], Loss: -18.085189819335938, Learning Rate: 0.01\n",
      "Epoch [11170/20000], Loss: -18.109283447265625, Learning Rate: 0.01\n",
      "Epoch [11171/20000], Loss: -18.0133056640625, Learning Rate: 0.01\n",
      "Epoch [11172/20000], Loss: -17.8973388671875, Learning Rate: 0.01\n",
      "Epoch [11173/20000], Loss: -17.854690551757812, Learning Rate: 0.01\n",
      "Epoch [11174/20000], Loss: -17.911911010742188, Learning Rate: 0.01\n",
      "Epoch [11175/20000], Loss: -18.0224609375, Learning Rate: 0.01\n",
      "Epoch [11176/20000], Loss: -18.111892700195312, Learning Rate: 0.01\n",
      "Epoch [11177/20000], Loss: -18.132644653320312, Learning Rate: 0.01\n",
      "Epoch [11178/20000], Loss: -18.09210205078125, Learning Rate: 0.01\n",
      "Epoch [11179/20000], Loss: -18.036346435546875, Learning Rate: 0.01\n",
      "Epoch [11180/20000], Loss: -18.012298583984375, Learning Rate: 0.01\n",
      "Epoch [11181/20000], Loss: -18.036407470703125, Learning Rate: 0.01\n",
      "Epoch [11182/20000], Loss: -18.089141845703125, Learning Rate: 0.01\n",
      "Epoch [11183/20000], Loss: -18.135284423828125, Learning Rate: 0.01\n",
      "Epoch [11184/20000], Loss: -18.15020751953125, Learning Rate: 0.01\n",
      "Epoch [11185/20000], Loss: -18.134628295898438, Learning Rate: 0.01\n",
      "Epoch [11186/20000], Loss: -18.108673095703125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11187/20000], Loss: -18.095748901367188, Learning Rate: 0.01\n",
      "Epoch [11188/20000], Loss: -18.105270385742188, Learning Rate: 0.01\n",
      "Epoch [11189/20000], Loss: -18.130462646484375, Learning Rate: 0.01\n",
      "Epoch [11190/20000], Loss: -18.154876708984375, Learning Rate: 0.01\n",
      "Epoch [11191/20000], Loss: -18.165878295898438, Learning Rate: 0.01\n",
      "Epoch [11192/20000], Loss: -18.16180419921875, Learning Rate: 0.01\n",
      "Epoch [11193/20000], Loss: -18.150772094726562, Learning Rate: 0.01\n",
      "Epoch [11194/20000], Loss: -18.1439208984375, Learning Rate: 0.01\n",
      "Epoch [11195/20000], Loss: -18.147232055664062, Learning Rate: 0.01\n",
      "Epoch [11196/20000], Loss: -18.159194946289062, Learning Rate: 0.01\n",
      "Epoch [11197/20000], Loss: -18.172515869140625, Learning Rate: 0.01\n",
      "Epoch [11198/20000], Loss: -18.1807861328125, Learning Rate: 0.01\n",
      "Epoch [11199/20000], Loss: -18.18170166015625, Learning Rate: 0.01\n",
      "Epoch [11200/20000], Loss: -18.1781005859375, Learning Rate: 0.01\n",
      "Epoch [11201/20000], Loss: -18.175018310546875, Learning Rate: 0.01\n",
      "Epoch [11202/20000], Loss: -18.176193237304688, Learning Rate: 0.01\n",
      "Epoch [11203/20000], Loss: -18.181808471679688, Learning Rate: 0.01\n",
      "Epoch [11204/20000], Loss: -18.189315795898438, Learning Rate: 0.01\n",
      "Epoch [11205/20000], Loss: -18.195404052734375, Learning Rate: 0.01\n",
      "Epoch [11206/20000], Loss: -18.198211669921875, Learning Rate: 0.01\n",
      "Epoch [11207/20000], Loss: -18.198287963867188, Learning Rate: 0.01\n",
      "Epoch [11208/20000], Loss: -18.197616577148438, Learning Rate: 0.01\n",
      "Epoch [11209/20000], Loss: -18.19842529296875, Learning Rate: 0.01\n",
      "Epoch [11210/20000], Loss: -18.201278686523438, Learning Rate: 0.01\n",
      "Epoch [11211/20000], Loss: -18.205718994140625, Learning Rate: 0.01\n",
      "Epoch [11212/20000], Loss: -18.2100830078125, Learning Rate: 0.01\n",
      "Epoch [11213/20000], Loss: -18.213348388671875, Learning Rate: 0.01\n",
      "Epoch [11214/20000], Loss: -18.215164184570312, Learning Rate: 0.01\n",
      "Epoch [11215/20000], Loss: -18.216140747070312, Learning Rate: 0.01\n",
      "Epoch [11216/20000], Loss: -18.217208862304688, Learning Rate: 0.01\n",
      "Epoch [11217/20000], Loss: -18.219070434570312, Learning Rate: 0.01\n",
      "Epoch [11218/20000], Loss: -18.221847534179688, Learning Rate: 0.01\n",
      "Epoch [11219/20000], Loss: -18.225143432617188, Learning Rate: 0.01\n",
      "Epoch [11220/20000], Loss: -18.228134155273438, Learning Rate: 0.01\n",
      "Epoch [11221/20000], Loss: -18.230560302734375, Learning Rate: 0.01\n",
      "Epoch [11222/20000], Loss: -18.232391357421875, Learning Rate: 0.01\n",
      "Epoch [11223/20000], Loss: -18.233993530273438, Learning Rate: 0.01\n",
      "Epoch [11224/20000], Loss: -18.235671997070312, Learning Rate: 0.01\n",
      "Epoch [11225/20000], Loss: -18.237808227539062, Learning Rate: 0.01\n",
      "Epoch [11226/20000], Loss: -18.240325927734375, Learning Rate: 0.01\n",
      "Epoch [11227/20000], Loss: -18.242919921875, Learning Rate: 0.01\n",
      "Epoch [11228/20000], Loss: -18.245407104492188, Learning Rate: 0.01\n",
      "Epoch [11229/20000], Loss: -18.247634887695312, Learning Rate: 0.01\n",
      "Epoch [11230/20000], Loss: -18.24957275390625, Learning Rate: 0.01\n",
      "Epoch [11231/20000], Loss: -18.25146484375, Learning Rate: 0.01\n",
      "Epoch [11232/20000], Loss: -18.253448486328125, Learning Rate: 0.01\n",
      "Epoch [11233/20000], Loss: -18.255615234375, Learning Rate: 0.01\n",
      "Epoch [11234/20000], Loss: -18.257904052734375, Learning Rate: 0.01\n",
      "Epoch [11235/20000], Loss: -18.260284423828125, Learning Rate: 0.01\n",
      "Epoch [11236/20000], Loss: -18.262557983398438, Learning Rate: 0.01\n",
      "Epoch [11237/20000], Loss: -18.264694213867188, Learning Rate: 0.01\n",
      "Epoch [11238/20000], Loss: -18.266845703125, Learning Rate: 0.01\n",
      "Epoch [11239/20000], Loss: -18.268798828125, Learning Rate: 0.01\n",
      "Epoch [11240/20000], Loss: -18.270889282226562, Learning Rate: 0.01\n",
      "Epoch [11241/20000], Loss: -18.273040771484375, Learning Rate: 0.01\n",
      "Epoch [11242/20000], Loss: -18.27520751953125, Learning Rate: 0.01\n",
      "Epoch [11243/20000], Loss: -18.277420043945312, Learning Rate: 0.01\n",
      "Epoch [11244/20000], Loss: -18.279510498046875, Learning Rate: 0.01\n",
      "Epoch [11245/20000], Loss: -18.281753540039062, Learning Rate: 0.01\n",
      "Epoch [11246/20000], Loss: -18.283859252929688, Learning Rate: 0.01\n",
      "Epoch [11247/20000], Loss: -18.285873413085938, Learning Rate: 0.01\n",
      "Epoch [11248/20000], Loss: -18.288040161132812, Learning Rate: 0.01\n",
      "Epoch [11249/20000], Loss: -18.290084838867188, Learning Rate: 0.01\n",
      "Epoch [11250/20000], Loss: -18.292282104492188, Learning Rate: 0.01\n",
      "Epoch [11251/20000], Loss: -18.294448852539062, Learning Rate: 0.01\n",
      "Epoch [11252/20000], Loss: -18.296630859375, Learning Rate: 0.01\n",
      "Epoch [11253/20000], Loss: -18.298721313476562, Learning Rate: 0.01\n",
      "Epoch [11254/20000], Loss: -18.300872802734375, Learning Rate: 0.01\n",
      "Epoch [11255/20000], Loss: -18.302963256835938, Learning Rate: 0.01\n",
      "Epoch [11256/20000], Loss: -18.305084228515625, Learning Rate: 0.01\n",
      "Epoch [11257/20000], Loss: -18.307144165039062, Learning Rate: 0.01\n",
      "Epoch [11258/20000], Loss: -18.309295654296875, Learning Rate: 0.01\n",
      "Epoch [11259/20000], Loss: -18.31146240234375, Learning Rate: 0.01\n",
      "Epoch [11260/20000], Loss: -18.3135986328125, Learning Rate: 0.01\n",
      "Epoch [11261/20000], Loss: -18.31573486328125, Learning Rate: 0.01\n",
      "Epoch [11262/20000], Loss: -18.31787109375, Learning Rate: 0.01\n",
      "Epoch [11263/20000], Loss: -18.319931030273438, Learning Rate: 0.01\n",
      "Epoch [11264/20000], Loss: -18.322021484375, Learning Rate: 0.01\n",
      "Epoch [11265/20000], Loss: -18.324127197265625, Learning Rate: 0.01\n",
      "Epoch [11266/20000], Loss: -18.3262939453125, Learning Rate: 0.01\n",
      "Epoch [11267/20000], Loss: -18.328384399414062, Learning Rate: 0.01\n",
      "Epoch [11268/20000], Loss: -18.330520629882812, Learning Rate: 0.01\n",
      "Epoch [11269/20000], Loss: -18.332672119140625, Learning Rate: 0.01\n",
      "Epoch [11270/20000], Loss: -18.334793090820312, Learning Rate: 0.01\n",
      "Epoch [11271/20000], Loss: -18.336868286132812, Learning Rate: 0.01\n",
      "Epoch [11272/20000], Loss: -18.3389892578125, Learning Rate: 0.01\n",
      "Epoch [11273/20000], Loss: -18.341018676757812, Learning Rate: 0.01\n",
      "Epoch [11274/20000], Loss: -18.34320068359375, Learning Rate: 0.01\n",
      "Epoch [11275/20000], Loss: -18.345352172851562, Learning Rate: 0.01\n",
      "Epoch [11276/20000], Loss: -18.347488403320312, Learning Rate: 0.01\n",
      "Epoch [11277/20000], Loss: -18.349624633789062, Learning Rate: 0.01\n",
      "Epoch [11278/20000], Loss: -18.351638793945312, Learning Rate: 0.01\n",
      "Epoch [11279/20000], Loss: -18.353851318359375, Learning Rate: 0.01\n",
      "Epoch [11280/20000], Loss: -18.35589599609375, Learning Rate: 0.01\n",
      "Epoch [11281/20000], Loss: -18.35797119140625, Learning Rate: 0.01\n",
      "Epoch [11282/20000], Loss: -18.360061645507812, Learning Rate: 0.01\n",
      "Epoch [11283/20000], Loss: -18.362197875976562, Learning Rate: 0.01\n",
      "Epoch [11284/20000], Loss: -18.364273071289062, Learning Rate: 0.01\n",
      "Epoch [11285/20000], Loss: -18.36639404296875, Learning Rate: 0.01\n",
      "Epoch [11286/20000], Loss: -18.3685302734375, Learning Rate: 0.01\n",
      "Epoch [11287/20000], Loss: -18.370697021484375, Learning Rate: 0.01\n",
      "Epoch [11288/20000], Loss: -18.372726440429688, Learning Rate: 0.01\n",
      "Epoch [11289/20000], Loss: -18.374832153320312, Learning Rate: 0.01\n",
      "Epoch [11290/20000], Loss: -18.37689208984375, Learning Rate: 0.01\n",
      "Epoch [11291/20000], Loss: -18.379074096679688, Learning Rate: 0.01\n",
      "Epoch [11292/20000], Loss: -18.381179809570312, Learning Rate: 0.01\n",
      "Epoch [11293/20000], Loss: -18.383224487304688, Learning Rate: 0.01\n",
      "Epoch [11294/20000], Loss: -18.385391235351562, Learning Rate: 0.01\n",
      "Epoch [11295/20000], Loss: -18.387527465820312, Learning Rate: 0.01\n",
      "Epoch [11296/20000], Loss: -18.38958740234375, Learning Rate: 0.01\n",
      "Epoch [11297/20000], Loss: -18.391708374023438, Learning Rate: 0.01\n",
      "Epoch [11298/20000], Loss: -18.393753051757812, Learning Rate: 0.01\n",
      "Epoch [11299/20000], Loss: -18.3958740234375, Learning Rate: 0.01\n",
      "Epoch [11300/20000], Loss: -18.397964477539062, Learning Rate: 0.01\n",
      "Epoch [11301/20000], Loss: -18.400115966796875, Learning Rate: 0.01\n",
      "Epoch [11302/20000], Loss: -18.402252197265625, Learning Rate: 0.01\n",
      "Epoch [11303/20000], Loss: -18.404327392578125, Learning Rate: 0.01\n",
      "Epoch [11304/20000], Loss: -18.4063720703125, Learning Rate: 0.01\n",
      "Epoch [11305/20000], Loss: -18.408477783203125, Learning Rate: 0.01\n",
      "Epoch [11306/20000], Loss: -18.410552978515625, Learning Rate: 0.01\n",
      "Epoch [11307/20000], Loss: -18.412689208984375, Learning Rate: 0.01\n",
      "Epoch [11308/20000], Loss: -18.414764404296875, Learning Rate: 0.01\n",
      "Epoch [11309/20000], Loss: -18.416854858398438, Learning Rate: 0.01\n",
      "Epoch [11310/20000], Loss: -18.418914794921875, Learning Rate: 0.01\n",
      "Epoch [11311/20000], Loss: -18.42108154296875, Learning Rate: 0.01\n",
      "Epoch [11312/20000], Loss: -18.423187255859375, Learning Rate: 0.01\n",
      "Epoch [11313/20000], Loss: -18.425262451171875, Learning Rate: 0.01\n",
      "Epoch [11314/20000], Loss: -18.427383422851562, Learning Rate: 0.01\n",
      "Epoch [11315/20000], Loss: -18.429367065429688, Learning Rate: 0.01\n",
      "Epoch [11316/20000], Loss: -18.431488037109375, Learning Rate: 0.01\n",
      "Epoch [11317/20000], Loss: -18.433624267578125, Learning Rate: 0.01\n",
      "Epoch [11318/20000], Loss: -18.4356689453125, Learning Rate: 0.01\n",
      "Epoch [11319/20000], Loss: -18.43780517578125, Learning Rate: 0.01\n",
      "Epoch [11320/20000], Loss: -18.439834594726562, Learning Rate: 0.01\n",
      "Epoch [11321/20000], Loss: -18.441909790039062, Learning Rate: 0.01\n",
      "Epoch [11322/20000], Loss: -18.4439697265625, Learning Rate: 0.01\n",
      "Epoch [11323/20000], Loss: -18.446090698242188, Learning Rate: 0.01\n",
      "Epoch [11324/20000], Loss: -18.448089599609375, Learning Rate: 0.01\n",
      "Epoch [11325/20000], Loss: -18.450180053710938, Learning Rate: 0.01\n",
      "Epoch [11326/20000], Loss: -18.4522705078125, Learning Rate: 0.01\n",
      "Epoch [11327/20000], Loss: -18.454360961914062, Learning Rate: 0.01\n",
      "Epoch [11328/20000], Loss: -18.4564208984375, Learning Rate: 0.01\n",
      "Epoch [11329/20000], Loss: -18.458343505859375, Learning Rate: 0.01\n",
      "Epoch [11330/20000], Loss: -18.460433959960938, Learning Rate: 0.01\n",
      "Epoch [11331/20000], Loss: -18.462432861328125, Learning Rate: 0.01\n",
      "Epoch [11332/20000], Loss: -18.464431762695312, Learning Rate: 0.01\n",
      "Epoch [11333/20000], Loss: -18.466354370117188, Learning Rate: 0.01\n",
      "Epoch [11334/20000], Loss: -18.468231201171875, Learning Rate: 0.01\n",
      "Epoch [11335/20000], Loss: -18.47003173828125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11336/20000], Loss: -18.471771240234375, Learning Rate: 0.01\n",
      "Epoch [11337/20000], Loss: -18.473297119140625, Learning Rate: 0.01\n",
      "Epoch [11338/20000], Loss: -18.474807739257812, Learning Rate: 0.01\n",
      "Epoch [11339/20000], Loss: -18.475967407226562, Learning Rate: 0.01\n",
      "Epoch [11340/20000], Loss: -18.476837158203125, Learning Rate: 0.01\n",
      "Epoch [11341/20000], Loss: -18.477279663085938, Learning Rate: 0.01\n",
      "Epoch [11342/20000], Loss: -18.477020263671875, Learning Rate: 0.01\n",
      "Epoch [11343/20000], Loss: -18.47589111328125, Learning Rate: 0.01\n",
      "Epoch [11344/20000], Loss: -18.47357177734375, Learning Rate: 0.01\n",
      "Epoch [11345/20000], Loss: -18.469512939453125, Learning Rate: 0.01\n",
      "Epoch [11346/20000], Loss: -18.462814331054688, Learning Rate: 0.01\n",
      "Epoch [11347/20000], Loss: -18.452682495117188, Learning Rate: 0.01\n",
      "Epoch [11348/20000], Loss: -18.437484741210938, Learning Rate: 0.01\n",
      "Epoch [11349/20000], Loss: -18.415237426757812, Learning Rate: 0.01\n",
      "Epoch [11350/20000], Loss: -18.383041381835938, Learning Rate: 0.01\n",
      "Epoch [11351/20000], Loss: -18.336807250976562, Learning Rate: 0.01\n",
      "Epoch [11352/20000], Loss: -18.270980834960938, Learning Rate: 0.01\n",
      "Epoch [11353/20000], Loss: -18.17926025390625, Learning Rate: 0.01\n",
      "Epoch [11354/20000], Loss: -18.05242919921875, Learning Rate: 0.01\n",
      "Epoch [11355/20000], Loss: -17.883148193359375, Learning Rate: 0.01\n",
      "Epoch [11356/20000], Loss: -17.663436889648438, Learning Rate: 0.01\n",
      "Epoch [11357/20000], Loss: -17.39764404296875, Learning Rate: 0.01\n",
      "Epoch [11358/20000], Loss: -17.099105834960938, Learning Rate: 0.01\n",
      "Epoch [11359/20000], Loss: -16.818115234375, Learning Rate: 0.01\n",
      "Epoch [11360/20000], Loss: -16.6199951171875, Learning Rate: 0.01\n",
      "Epoch [11361/20000], Loss: -16.60516357421875, Learning Rate: 0.01\n",
      "Epoch [11362/20000], Loss: -16.82476806640625, Learning Rate: 0.01\n",
      "Epoch [11363/20000], Loss: -17.271591186523438, Learning Rate: 0.01\n",
      "Epoch [11364/20000], Loss: -17.813812255859375, Learning Rate: 0.01\n",
      "Epoch [11365/20000], Loss: -18.276626586914062, Learning Rate: 0.01\n",
      "Epoch [11366/20000], Loss: -18.515472412109375, Learning Rate: 0.01\n",
      "Epoch [11367/20000], Loss: -18.498580932617188, Learning Rate: 0.01\n",
      "Epoch [11368/20000], Loss: -18.30462646484375, Learning Rate: 0.01\n",
      "Epoch [11369/20000], Loss: -18.070022583007812, Learning Rate: 0.01\n",
      "Epoch [11370/20000], Loss: -17.925750732421875, Learning Rate: 0.01\n",
      "Epoch [11371/20000], Loss: -17.939712524414062, Learning Rate: 0.01\n",
      "Epoch [11372/20000], Loss: -18.099075317382812, Learning Rate: 0.01\n",
      "Epoch [11373/20000], Loss: -18.31744384765625, Learning Rate: 0.01\n",
      "Epoch [11374/20000], Loss: -18.491744995117188, Learning Rate: 0.01\n",
      "Epoch [11375/20000], Loss: -18.555618286132812, Learning Rate: 0.01\n",
      "Epoch [11376/20000], Loss: -18.509674072265625, Learning Rate: 0.01\n",
      "Epoch [11377/20000], Loss: -18.40887451171875, Learning Rate: 0.01\n",
      "Epoch [11378/20000], Loss: -18.323959350585938, Learning Rate: 0.01\n",
      "Epoch [11379/20000], Loss: -18.304931640625, Learning Rate: 0.01\n",
      "Epoch [11380/20000], Loss: -18.357498168945312, Learning Rate: 0.01\n",
      "Epoch [11381/20000], Loss: -18.449630737304688, Learning Rate: 0.01\n",
      "Epoch [11382/20000], Loss: -18.532852172851562, Learning Rate: 0.01\n",
      "Epoch [11383/20000], Loss: -18.571807861328125, Learning Rate: 0.01\n",
      "Epoch [11384/20000], Loss: -18.560562133789062, Learning Rate: 0.01\n",
      "Epoch [11385/20000], Loss: -18.519729614257812, Learning Rate: 0.01\n",
      "Epoch [11386/20000], Loss: -18.48126220703125, Learning Rate: 0.01\n",
      "Epoch [11387/20000], Loss: -18.46881103515625, Learning Rate: 0.01\n",
      "Epoch [11388/20000], Loss: -18.488433837890625, Learning Rate: 0.01\n",
      "Epoch [11389/20000], Loss: -18.5277099609375, Learning Rate: 0.01\n",
      "Epoch [11390/20000], Loss: -18.566360473632812, Learning Rate: 0.01\n",
      "Epoch [11391/20000], Loss: -18.587844848632812, Learning Rate: 0.01\n",
      "Epoch [11392/20000], Loss: -18.5875244140625, Learning Rate: 0.01\n",
      "Epoch [11393/20000], Loss: -18.572540283203125, Learning Rate: 0.01\n",
      "Epoch [11394/20000], Loss: -18.556076049804688, Learning Rate: 0.01\n",
      "Epoch [11395/20000], Loss: -18.549179077148438, Learning Rate: 0.01\n",
      "Epoch [11396/20000], Loss: -18.555816650390625, Learning Rate: 0.01\n",
      "Epoch [11397/20000], Loss: -18.572341918945312, Learning Rate: 0.01\n",
      "Epoch [11398/20000], Loss: -18.59075927734375, Learning Rate: 0.01\n",
      "Epoch [11399/20000], Loss: -18.60333251953125, Learning Rate: 0.01\n",
      "Epoch [11400/20000], Loss: -18.606979370117188, Learning Rate: 0.01\n",
      "Epoch [11401/20000], Loss: -18.603347778320312, Learning Rate: 0.01\n",
      "Epoch [11402/20000], Loss: -18.597259521484375, Learning Rate: 0.01\n",
      "Epoch [11403/20000], Loss: -18.593765258789062, Learning Rate: 0.01\n",
      "Epoch [11404/20000], Loss: -18.595565795898438, Learning Rate: 0.01\n",
      "Epoch [11405/20000], Loss: -18.602203369140625, Learning Rate: 0.01\n",
      "Epoch [11406/20000], Loss: -18.611129760742188, Learning Rate: 0.01\n",
      "Epoch [11407/20000], Loss: -18.618896484375, Learning Rate: 0.01\n",
      "Epoch [11408/20000], Loss: -18.623397827148438, Learning Rate: 0.01\n",
      "Epoch [11409/20000], Loss: -18.62451171875, Learning Rate: 0.01\n",
      "Epoch [11410/20000], Loss: -18.623489379882812, Learning Rate: 0.01\n",
      "Epoch [11411/20000], Loss: -18.622406005859375, Learning Rate: 0.01\n",
      "Epoch [11412/20000], Loss: -18.622970581054688, Learning Rate: 0.01\n",
      "Epoch [11413/20000], Loss: -18.6256103515625, Learning Rate: 0.01\n",
      "Epoch [11414/20000], Loss: -18.629974365234375, Learning Rate: 0.01\n",
      "Epoch [11415/20000], Loss: -18.6346435546875, Learning Rate: 0.01\n",
      "Epoch [11416/20000], Loss: -18.638687133789062, Learning Rate: 0.01\n",
      "Epoch [11417/20000], Loss: -18.641403198242188, Learning Rate: 0.01\n",
      "Epoch [11418/20000], Loss: -18.642852783203125, Learning Rate: 0.01\n",
      "Epoch [11419/20000], Loss: -18.64349365234375, Learning Rate: 0.01\n",
      "Epoch [11420/20000], Loss: -18.644363403320312, Learning Rate: 0.01\n",
      "Epoch [11421/20000], Loss: -18.645919799804688, Learning Rate: 0.01\n",
      "Epoch [11422/20000], Loss: -18.648117065429688, Learning Rate: 0.01\n",
      "Epoch [11423/20000], Loss: -18.65106201171875, Learning Rate: 0.01\n",
      "Epoch [11424/20000], Loss: -18.654037475585938, Learning Rate: 0.01\n",
      "Epoch [11425/20000], Loss: -18.656890869140625, Learning Rate: 0.01\n",
      "Epoch [11426/20000], Loss: -18.659225463867188, Learning Rate: 0.01\n",
      "Epoch [11427/20000], Loss: -18.661026000976562, Learning Rate: 0.01\n",
      "Epoch [11428/20000], Loss: -18.662567138671875, Learning Rate: 0.01\n",
      "Epoch [11429/20000], Loss: -18.66400146484375, Learning Rate: 0.01\n",
      "Epoch [11430/20000], Loss: -18.665802001953125, Learning Rate: 0.01\n",
      "Epoch [11431/20000], Loss: -18.667709350585938, Learning Rate: 0.01\n",
      "Epoch [11432/20000], Loss: -18.6700439453125, Learning Rate: 0.01\n",
      "Epoch [11433/20000], Loss: -18.672393798828125, Learning Rate: 0.01\n",
      "Epoch [11434/20000], Loss: -18.674774169921875, Learning Rate: 0.01\n",
      "Epoch [11435/20000], Loss: -18.67694091796875, Learning Rate: 0.01\n",
      "Epoch [11436/20000], Loss: -18.679000854492188, Learning Rate: 0.01\n",
      "Epoch [11437/20000], Loss: -18.680801391601562, Learning Rate: 0.01\n",
      "Epoch [11438/20000], Loss: -18.682540893554688, Learning Rate: 0.01\n",
      "Epoch [11439/20000], Loss: -18.684356689453125, Learning Rate: 0.01\n",
      "Epoch [11440/20000], Loss: -18.686294555664062, Learning Rate: 0.01\n",
      "Epoch [11441/20000], Loss: -18.688308715820312, Learning Rate: 0.01\n",
      "Epoch [11442/20000], Loss: -18.69036865234375, Learning Rate: 0.01\n",
      "Epoch [11443/20000], Loss: -18.692535400390625, Learning Rate: 0.01\n",
      "Epoch [11444/20000], Loss: -18.694610595703125, Learning Rate: 0.01\n",
      "Epoch [11445/20000], Loss: -18.696640014648438, Learning Rate: 0.01\n",
      "Epoch [11446/20000], Loss: -18.698593139648438, Learning Rate: 0.01\n",
      "Epoch [11447/20000], Loss: -18.700546264648438, Learning Rate: 0.01\n",
      "Epoch [11448/20000], Loss: -18.702392578125, Learning Rate: 0.01\n",
      "Epoch [11449/20000], Loss: -18.704391479492188, Learning Rate: 0.01\n",
      "Epoch [11450/20000], Loss: -18.706283569335938, Learning Rate: 0.01\n",
      "Epoch [11451/20000], Loss: -18.708251953125, Learning Rate: 0.01\n",
      "Epoch [11452/20000], Loss: -18.710220336914062, Learning Rate: 0.01\n",
      "Epoch [11453/20000], Loss: -18.712188720703125, Learning Rate: 0.01\n",
      "Epoch [11454/20000], Loss: -18.71429443359375, Learning Rate: 0.01\n",
      "Epoch [11455/20000], Loss: -18.716278076171875, Learning Rate: 0.01\n",
      "Epoch [11456/20000], Loss: -18.71826171875, Learning Rate: 0.01\n",
      "Epoch [11457/20000], Loss: -18.720184326171875, Learning Rate: 0.01\n",
      "Epoch [11458/20000], Loss: -18.722152709960938, Learning Rate: 0.01\n",
      "Epoch [11459/20000], Loss: -18.72406005859375, Learning Rate: 0.01\n",
      "Epoch [11460/20000], Loss: -18.725982666015625, Learning Rate: 0.01\n",
      "Epoch [11461/20000], Loss: -18.727935791015625, Learning Rate: 0.01\n",
      "Epoch [11462/20000], Loss: -18.729888916015625, Learning Rate: 0.01\n",
      "Epoch [11463/20000], Loss: -18.731857299804688, Learning Rate: 0.01\n",
      "Epoch [11464/20000], Loss: -18.7337646484375, Learning Rate: 0.01\n",
      "Epoch [11465/20000], Loss: -18.735748291015625, Learning Rate: 0.01\n",
      "Epoch [11466/20000], Loss: -18.737823486328125, Learning Rate: 0.01\n",
      "Epoch [11467/20000], Loss: -18.739700317382812, Learning Rate: 0.01\n",
      "Epoch [11468/20000], Loss: -18.741653442382812, Learning Rate: 0.01\n",
      "Epoch [11469/20000], Loss: -18.74371337890625, Learning Rate: 0.01\n",
      "Epoch [11470/20000], Loss: -18.74560546875, Learning Rate: 0.01\n",
      "Epoch [11471/20000], Loss: -18.74749755859375, Learning Rate: 0.01\n",
      "Epoch [11472/20000], Loss: -18.749404907226562, Learning Rate: 0.01\n",
      "Epoch [11473/20000], Loss: -18.75140380859375, Learning Rate: 0.01\n",
      "Epoch [11474/20000], Loss: -18.753387451171875, Learning Rate: 0.01\n",
      "Epoch [11475/20000], Loss: -18.755401611328125, Learning Rate: 0.01\n",
      "Epoch [11476/20000], Loss: -18.757217407226562, Learning Rate: 0.01\n",
      "Epoch [11477/20000], Loss: -18.759231567382812, Learning Rate: 0.01\n",
      "Epoch [11478/20000], Loss: -18.761199951171875, Learning Rate: 0.01\n",
      "Epoch [11479/20000], Loss: -18.76312255859375, Learning Rate: 0.01\n",
      "Epoch [11480/20000], Loss: -18.765106201171875, Learning Rate: 0.01\n",
      "Epoch [11481/20000], Loss: -18.76702880859375, Learning Rate: 0.01\n",
      "Epoch [11482/20000], Loss: -18.76898193359375, Learning Rate: 0.01\n",
      "Epoch [11483/20000], Loss: -18.770919799804688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11484/20000], Loss: -18.772842407226562, Learning Rate: 0.01\n",
      "Epoch [11485/20000], Loss: -18.774734497070312, Learning Rate: 0.01\n",
      "Epoch [11486/20000], Loss: -18.7767333984375, Learning Rate: 0.01\n",
      "Epoch [11487/20000], Loss: -18.7786865234375, Learning Rate: 0.01\n",
      "Epoch [11488/20000], Loss: -18.780624389648438, Learning Rate: 0.01\n",
      "Epoch [11489/20000], Loss: -18.782501220703125, Learning Rate: 0.01\n",
      "Epoch [11490/20000], Loss: -18.784500122070312, Learning Rate: 0.01\n",
      "Epoch [11491/20000], Loss: -18.786422729492188, Learning Rate: 0.01\n",
      "Epoch [11492/20000], Loss: -18.788360595703125, Learning Rate: 0.01\n",
      "Epoch [11493/20000], Loss: -18.790267944335938, Learning Rate: 0.01\n",
      "Epoch [11494/20000], Loss: -18.79217529296875, Learning Rate: 0.01\n",
      "Epoch [11495/20000], Loss: -18.794189453125, Learning Rate: 0.01\n",
      "Epoch [11496/20000], Loss: -18.796142578125, Learning Rate: 0.01\n",
      "Epoch [11497/20000], Loss: -18.798004150390625, Learning Rate: 0.01\n",
      "Epoch [11498/20000], Loss: -18.799972534179688, Learning Rate: 0.01\n",
      "Epoch [11499/20000], Loss: -18.801895141601562, Learning Rate: 0.01\n",
      "Epoch [11500/20000], Loss: -18.803848266601562, Learning Rate: 0.01\n",
      "Epoch [11501/20000], Loss: -18.805770874023438, Learning Rate: 0.01\n",
      "Epoch [11502/20000], Loss: -18.80767822265625, Learning Rate: 0.01\n",
      "Epoch [11503/20000], Loss: -18.809600830078125, Learning Rate: 0.01\n",
      "Epoch [11504/20000], Loss: -18.811569213867188, Learning Rate: 0.01\n",
      "Epoch [11505/20000], Loss: -18.8134765625, Learning Rate: 0.01\n",
      "Epoch [11506/20000], Loss: -18.815399169921875, Learning Rate: 0.01\n",
      "Epoch [11507/20000], Loss: -18.817352294921875, Learning Rate: 0.01\n",
      "Epoch [11508/20000], Loss: -18.8192138671875, Learning Rate: 0.01\n",
      "Epoch [11509/20000], Loss: -18.8211669921875, Learning Rate: 0.01\n",
      "Epoch [11510/20000], Loss: -18.823150634765625, Learning Rate: 0.01\n",
      "Epoch [11511/20000], Loss: -18.825057983398438, Learning Rate: 0.01\n",
      "Epoch [11512/20000], Loss: -18.826950073242188, Learning Rate: 0.01\n",
      "Epoch [11513/20000], Loss: -18.828872680664062, Learning Rate: 0.01\n",
      "Epoch [11514/20000], Loss: -18.830657958984375, Learning Rate: 0.01\n",
      "Epoch [11515/20000], Loss: -18.832565307617188, Learning Rate: 0.01\n",
      "Epoch [11516/20000], Loss: -18.834503173828125, Learning Rate: 0.01\n",
      "Epoch [11517/20000], Loss: -18.836349487304688, Learning Rate: 0.01\n",
      "Epoch [11518/20000], Loss: -18.838043212890625, Learning Rate: 0.01\n",
      "Epoch [11519/20000], Loss: -18.839935302734375, Learning Rate: 0.01\n",
      "Epoch [11520/20000], Loss: -18.841629028320312, Learning Rate: 0.01\n",
      "Epoch [11521/20000], Loss: -18.843292236328125, Learning Rate: 0.01\n",
      "Epoch [11522/20000], Loss: -18.844940185546875, Learning Rate: 0.01\n",
      "Epoch [11523/20000], Loss: -18.846389770507812, Learning Rate: 0.01\n",
      "Epoch [11524/20000], Loss: -18.847564697265625, Learning Rate: 0.01\n",
      "Epoch [11525/20000], Loss: -18.848556518554688, Learning Rate: 0.01\n",
      "Epoch [11526/20000], Loss: -18.849136352539062, Learning Rate: 0.01\n",
      "Epoch [11527/20000], Loss: -18.849166870117188, Learning Rate: 0.01\n",
      "Epoch [11528/20000], Loss: -18.848464965820312, Learning Rate: 0.01\n",
      "Epoch [11529/20000], Loss: -18.846649169921875, Learning Rate: 0.01\n",
      "Epoch [11530/20000], Loss: -18.843170166015625, Learning Rate: 0.01\n",
      "Epoch [11531/20000], Loss: -18.8375244140625, Learning Rate: 0.01\n",
      "Epoch [11532/20000], Loss: -18.828598022460938, Learning Rate: 0.01\n",
      "Epoch [11533/20000], Loss: -18.814956665039062, Learning Rate: 0.01\n",
      "Epoch [11534/20000], Loss: -18.794509887695312, Learning Rate: 0.01\n",
      "Epoch [11535/20000], Loss: -18.764434814453125, Learning Rate: 0.01\n",
      "Epoch [11536/20000], Loss: -18.720428466796875, Learning Rate: 0.01\n",
      "Epoch [11537/20000], Loss: -18.656585693359375, Learning Rate: 0.01\n",
      "Epoch [11538/20000], Loss: -18.5653076171875, Learning Rate: 0.01\n",
      "Epoch [11539/20000], Loss: -18.436004638671875, Learning Rate: 0.01\n",
      "Epoch [11540/20000], Loss: -18.256866455078125, Learning Rate: 0.01\n",
      "Epoch [11541/20000], Loss: -18.014633178710938, Learning Rate: 0.01\n",
      "Epoch [11542/20000], Loss: -17.701889038085938, Learning Rate: 0.01\n",
      "Epoch [11543/20000], Loss: -17.321884155273438, Learning Rate: 0.01\n",
      "Epoch [11544/20000], Loss: -16.908279418945312, Learning Rate: 0.01\n",
      "Epoch [11545/20000], Loss: -16.531570434570312, Learning Rate: 0.01\n",
      "Epoch [11546/20000], Loss: -16.313278198242188, Learning Rate: 0.01\n",
      "Epoch [11547/20000], Loss: -16.377426147460938, Learning Rate: 0.01\n",
      "Epoch [11548/20000], Loss: -16.794296264648438, Learning Rate: 0.01\n",
      "Epoch [11549/20000], Loss: -17.482208251953125, Learning Rate: 0.01\n",
      "Epoch [11550/20000], Loss: -18.219192504882812, Learning Rate: 0.01\n",
      "Epoch [11551/20000], Loss: -18.741912841796875, Learning Rate: 0.01\n",
      "Epoch [11552/20000], Loss: -18.900375366210938, Learning Rate: 0.01\n",
      "Epoch [11553/20000], Loss: -18.726959228515625, Learning Rate: 0.01\n",
      "Epoch [11554/20000], Loss: -18.395355224609375, Learning Rate: 0.01\n",
      "Epoch [11555/20000], Loss: -18.119705200195312, Learning Rate: 0.01\n",
      "Epoch [11556/20000], Loss: -18.052001953125, Learning Rate: 0.01\n",
      "Epoch [11557/20000], Loss: -18.220947265625, Learning Rate: 0.01\n",
      "Epoch [11558/20000], Loss: -18.524002075195312, Learning Rate: 0.01\n",
      "Epoch [11559/20000], Loss: -18.7987060546875, Learning Rate: 0.01\n",
      "Epoch [11560/20000], Loss: -18.920303344726562, Learning Rate: 0.01\n",
      "Epoch [11561/20000], Loss: -18.8687744140625, Learning Rate: 0.01\n",
      "Epoch [11562/20000], Loss: -18.721755981445312, Learning Rate: 0.01\n",
      "Epoch [11563/20000], Loss: -18.593215942382812, Learning Rate: 0.01\n",
      "Epoch [11564/20000], Loss: -18.565826416015625, Learning Rate: 0.01\n",
      "Epoch [11565/20000], Loss: -18.649154663085938, Learning Rate: 0.01\n",
      "Epoch [11566/20000], Loss: -18.78717041015625, Learning Rate: 0.01\n",
      "Epoch [11567/20000], Loss: -18.900497436523438, Learning Rate: 0.01\n",
      "Epoch [11568/20000], Loss: -18.937728881835938, Learning Rate: 0.01\n",
      "Epoch [11569/20000], Loss: -18.9002685546875, Learning Rate: 0.01\n",
      "Epoch [11570/20000], Loss: -18.83203125, Learning Rate: 0.01\n",
      "Epoch [11571/20000], Loss: -18.785598754882812, Learning Rate: 0.01\n",
      "Epoch [11572/20000], Loss: -18.790481567382812, Learning Rate: 0.01\n",
      "Epoch [11573/20000], Loss: -18.8404541015625, Learning Rate: 0.01\n",
      "Epoch [11574/20000], Loss: -18.903457641601562, Learning Rate: 0.01\n",
      "Epoch [11575/20000], Loss: -18.945449829101562, Learning Rate: 0.01\n",
      "Epoch [11576/20000], Loss: -18.950637817382812, Learning Rate: 0.01\n",
      "Epoch [11577/20000], Loss: -18.927322387695312, Learning Rate: 0.01\n",
      "Epoch [11578/20000], Loss: -18.89849853515625, Learning Rate: 0.01\n",
      "Epoch [11579/20000], Loss: -18.885543823242188, Learning Rate: 0.01\n",
      "Epoch [11580/20000], Loss: -18.89630126953125, Learning Rate: 0.01\n",
      "Epoch [11581/20000], Loss: -18.92333984375, Learning Rate: 0.01\n",
      "Epoch [11582/20000], Loss: -18.95098876953125, Learning Rate: 0.01\n",
      "Epoch [11583/20000], Loss: -18.96600341796875, Learning Rate: 0.01\n",
      "Epoch [11584/20000], Loss: -18.964736938476562, Learning Rate: 0.01\n",
      "Epoch [11585/20000], Loss: -18.953582763671875, Learning Rate: 0.01\n",
      "Epoch [11586/20000], Loss: -18.942977905273438, Learning Rate: 0.01\n",
      "Epoch [11587/20000], Loss: -18.940933227539062, Learning Rate: 0.01\n",
      "Epoch [11588/20000], Loss: -18.949188232421875, Learning Rate: 0.01\n",
      "Epoch [11589/20000], Loss: -18.96295166015625, Learning Rate: 0.01\n",
      "Epoch [11590/20000], Loss: -18.975296020507812, Learning Rate: 0.01\n",
      "Epoch [11591/20000], Loss: -18.981536865234375, Learning Rate: 0.01\n",
      "Epoch [11592/20000], Loss: -18.98077392578125, Learning Rate: 0.01\n",
      "Epoch [11593/20000], Loss: -18.976669311523438, Learning Rate: 0.01\n",
      "Epoch [11594/20000], Loss: -18.973403930664062, Learning Rate: 0.01\n",
      "Epoch [11595/20000], Loss: -18.97412109375, Learning Rate: 0.01\n",
      "Epoch [11596/20000], Loss: -18.979248046875, Learning Rate: 0.01\n",
      "Epoch [11597/20000], Loss: -18.98626708984375, Learning Rate: 0.01\n",
      "Epoch [11598/20000], Loss: -18.992599487304688, Learning Rate: 0.01\n",
      "Epoch [11599/20000], Loss: -18.996063232421875, Learning Rate: 0.01\n",
      "Epoch [11600/20000], Loss: -18.996719360351562, Learning Rate: 0.01\n",
      "Epoch [11601/20000], Loss: -18.99591064453125, Learning Rate: 0.01\n",
      "Epoch [11602/20000], Loss: -18.995559692382812, Learning Rate: 0.01\n",
      "Epoch [11603/20000], Loss: -18.996826171875, Learning Rate: 0.01\n",
      "Epoch [11604/20000], Loss: -19.0, Learning Rate: 0.01\n",
      "Epoch [11605/20000], Loss: -19.004058837890625, Learning Rate: 0.01\n",
      "Epoch [11606/20000], Loss: -19.00787353515625, Learning Rate: 0.01\n",
      "Epoch [11607/20000], Loss: -19.010543823242188, Learning Rate: 0.01\n",
      "Epoch [11608/20000], Loss: -19.012069702148438, Learning Rate: 0.01\n",
      "Epoch [11609/20000], Loss: -19.012771606445312, Learning Rate: 0.01\n",
      "Epoch [11610/20000], Loss: -19.013595581054688, Learning Rate: 0.01\n",
      "Epoch [11611/20000], Loss: -19.015029907226562, Learning Rate: 0.01\n",
      "Epoch [11612/20000], Loss: -19.017227172851562, Learning Rate: 0.01\n",
      "Epoch [11613/20000], Loss: -19.019912719726562, Learning Rate: 0.01\n",
      "Epoch [11614/20000], Loss: -19.022613525390625, Learning Rate: 0.01\n",
      "Epoch [11615/20000], Loss: -19.025039672851562, Learning Rate: 0.01\n",
      "Epoch [11616/20000], Loss: -19.02679443359375, Learning Rate: 0.01\n",
      "Epoch [11617/20000], Loss: -19.028213500976562, Learning Rate: 0.01\n",
      "Epoch [11618/20000], Loss: -19.029510498046875, Learning Rate: 0.01\n",
      "Epoch [11619/20000], Loss: -19.031097412109375, Learning Rate: 0.01\n",
      "Epoch [11620/20000], Loss: -19.032882690429688, Learning Rate: 0.01\n",
      "Epoch [11621/20000], Loss: -19.034988403320312, Learning Rate: 0.01\n",
      "Epoch [11622/20000], Loss: -19.037185668945312, Learning Rate: 0.01\n",
      "Epoch [11623/20000], Loss: -19.039352416992188, Learning Rate: 0.01\n",
      "Epoch [11624/20000], Loss: -19.04119873046875, Learning Rate: 0.01\n",
      "Epoch [11625/20000], Loss: -19.04290771484375, Learning Rate: 0.01\n",
      "Epoch [11626/20000], Loss: -19.044448852539062, Learning Rate: 0.01\n",
      "Epoch [11627/20000], Loss: -19.046157836914062, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11628/20000], Loss: -19.0478515625, Learning Rate: 0.01\n",
      "Epoch [11629/20000], Loss: -19.049697875976562, Learning Rate: 0.01\n",
      "Epoch [11630/20000], Loss: -19.0516357421875, Learning Rate: 0.01\n",
      "Epoch [11631/20000], Loss: -19.053558349609375, Learning Rate: 0.01\n",
      "Epoch [11632/20000], Loss: -19.05548095703125, Learning Rate: 0.01\n",
      "Epoch [11633/20000], Loss: -19.057159423828125, Learning Rate: 0.01\n",
      "Epoch [11634/20000], Loss: -19.058868408203125, Learning Rate: 0.01\n",
      "Epoch [11635/20000], Loss: -19.060531616210938, Learning Rate: 0.01\n",
      "Epoch [11636/20000], Loss: -19.062164306640625, Learning Rate: 0.01\n",
      "Epoch [11637/20000], Loss: -19.06390380859375, Learning Rate: 0.01\n",
      "Epoch [11638/20000], Loss: -19.065597534179688, Learning Rate: 0.01\n",
      "Epoch [11639/20000], Loss: -19.0672607421875, Learning Rate: 0.01\n",
      "Epoch [11640/20000], Loss: -19.068893432617188, Learning Rate: 0.01\n",
      "Epoch [11641/20000], Loss: -19.070419311523438, Learning Rate: 0.01\n",
      "Epoch [11642/20000], Loss: -19.071929931640625, Learning Rate: 0.01\n",
      "Epoch [11643/20000], Loss: -19.07318115234375, Learning Rate: 0.01\n",
      "Epoch [11644/20000], Loss: -19.074295043945312, Learning Rate: 0.01\n",
      "Epoch [11645/20000], Loss: -19.075088500976562, Learning Rate: 0.01\n",
      "Epoch [11646/20000], Loss: -19.07574462890625, Learning Rate: 0.01\n",
      "Epoch [11647/20000], Loss: -19.075927734375, Learning Rate: 0.01\n",
      "Epoch [11648/20000], Loss: -19.075592041015625, Learning Rate: 0.01\n",
      "Epoch [11649/20000], Loss: -19.074371337890625, Learning Rate: 0.01\n",
      "Epoch [11650/20000], Loss: -19.072067260742188, Learning Rate: 0.01\n",
      "Epoch [11651/20000], Loss: -19.068206787109375, Learning Rate: 0.01\n",
      "Epoch [11652/20000], Loss: -19.062042236328125, Learning Rate: 0.01\n",
      "Epoch [11653/20000], Loss: -19.052749633789062, Learning Rate: 0.01\n",
      "Epoch [11654/20000], Loss: -19.039154052734375, Learning Rate: 0.01\n",
      "Epoch [11655/20000], Loss: -19.01934814453125, Learning Rate: 0.01\n",
      "Epoch [11656/20000], Loss: -18.991073608398438, Learning Rate: 0.01\n",
      "Epoch [11657/20000], Loss: -18.95074462890625, Learning Rate: 0.01\n",
      "Epoch [11658/20000], Loss: -18.894241333007812, Learning Rate: 0.01\n",
      "Epoch [11659/20000], Loss: -18.815505981445312, Learning Rate: 0.01\n",
      "Epoch [11660/20000], Loss: -18.70867919921875, Learning Rate: 0.01\n",
      "Epoch [11661/20000], Loss: -18.565567016601562, Learning Rate: 0.01\n",
      "Epoch [11662/20000], Loss: -18.383132934570312, Learning Rate: 0.01\n",
      "Epoch [11663/20000], Loss: -18.15948486328125, Learning Rate: 0.01\n",
      "Epoch [11664/20000], Loss: -17.913116455078125, Learning Rate: 0.01\n",
      "Epoch [11665/20000], Loss: -17.671279907226562, Learning Rate: 0.01\n",
      "Epoch [11666/20000], Loss: -17.50128173828125, Learning Rate: 0.01\n",
      "Epoch [11667/20000], Loss: -17.463653564453125, Learning Rate: 0.01\n",
      "Epoch [11668/20000], Loss: -17.626556396484375, Learning Rate: 0.01\n",
      "Epoch [11669/20000], Loss: -17.97332763671875, Learning Rate: 0.01\n",
      "Epoch [11670/20000], Loss: -18.42645263671875, Learning Rate: 0.01\n",
      "Epoch [11671/20000], Loss: -18.837631225585938, Learning Rate: 0.01\n",
      "Epoch [11672/20000], Loss: -19.083770751953125, Learning Rate: 0.01\n",
      "Epoch [11673/20000], Loss: -19.117996215820312, Learning Rate: 0.01\n",
      "Epoch [11674/20000], Loss: -18.985458374023438, Learning Rate: 0.01\n",
      "Epoch [11675/20000], Loss: -18.78955078125, Learning Rate: 0.01\n",
      "Epoch [11676/20000], Loss: -18.638809204101562, Learning Rate: 0.01\n",
      "Epoch [11677/20000], Loss: -18.607315063476562, Learning Rate: 0.01\n",
      "Epoch [11678/20000], Loss: -18.70233154296875, Learning Rate: 0.01\n",
      "Epoch [11679/20000], Loss: -18.87457275390625, Learning Rate: 0.01\n",
      "Epoch [11680/20000], Loss: -19.040878295898438, Learning Rate: 0.01\n",
      "Epoch [11681/20000], Loss: -19.134429931640625, Learning Rate: 0.01\n",
      "Epoch [11682/20000], Loss: -19.133697509765625, Learning Rate: 0.01\n",
      "Epoch [11683/20000], Loss: -19.065841674804688, Learning Rate: 0.01\n",
      "Epoch [11684/20000], Loss: -18.98455810546875, Learning Rate: 0.01\n",
      "Epoch [11685/20000], Loss: -18.938751220703125, Learning Rate: 0.01\n",
      "Epoch [11686/20000], Loss: -18.951416015625, Learning Rate: 0.01\n",
      "Epoch [11687/20000], Loss: -19.011810302734375, Learning Rate: 0.01\n",
      "Epoch [11688/20000], Loss: -19.087417602539062, Learning Rate: 0.01\n",
      "Epoch [11689/20000], Loss: -19.14306640625, Learning Rate: 0.01\n",
      "Epoch [11690/20000], Loss: -19.1600341796875, Learning Rate: 0.01\n",
      "Epoch [11691/20000], Loss: -19.141555786132812, Learning Rate: 0.01\n",
      "Epoch [11692/20000], Loss: -19.107635498046875, Learning Rate: 0.01\n",
      "Epoch [11693/20000], Loss: -19.08160400390625, Learning Rate: 0.01\n",
      "Epoch [11694/20000], Loss: -19.077957153320312, Learning Rate: 0.01\n",
      "Epoch [11695/20000], Loss: -19.097625732421875, Learning Rate: 0.01\n",
      "Epoch [11696/20000], Loss: -19.129470825195312, Learning Rate: 0.01\n",
      "Epoch [11697/20000], Loss: -19.158432006835938, Learning Rate: 0.01\n",
      "Epoch [11698/20000], Loss: -19.173538208007812, Learning Rate: 0.01\n",
      "Epoch [11699/20000], Loss: -19.172592163085938, Learning Rate: 0.01\n",
      "Epoch [11700/20000], Loss: -19.16119384765625, Learning Rate: 0.01\n",
      "Epoch [11701/20000], Loss: -19.14886474609375, Learning Rate: 0.01\n",
      "Epoch [11702/20000], Loss: -19.1435546875, Learning Rate: 0.01\n",
      "Epoch [11703/20000], Loss: -19.148162841796875, Learning Rate: 0.01\n",
      "Epoch [11704/20000], Loss: -19.160385131835938, Learning Rate: 0.01\n",
      "Epoch [11705/20000], Loss: -19.174591064453125, Learning Rate: 0.01\n",
      "Epoch [11706/20000], Loss: -19.185394287109375, Learning Rate: 0.01\n",
      "Epoch [11707/20000], Loss: -19.189788818359375, Learning Rate: 0.01\n",
      "Epoch [11708/20000], Loss: -19.188385009765625, Learning Rate: 0.01\n",
      "Epoch [11709/20000], Loss: -19.184280395507812, Learning Rate: 0.01\n",
      "Epoch [11710/20000], Loss: -19.1810302734375, Learning Rate: 0.01\n",
      "Epoch [11711/20000], Loss: -19.181015014648438, Learning Rate: 0.01\n",
      "Epoch [11712/20000], Loss: -19.18487548828125, Learning Rate: 0.01\n",
      "Epoch [11713/20000], Loss: -19.191192626953125, Learning Rate: 0.01\n",
      "Epoch [11714/20000], Loss: -19.19775390625, Learning Rate: 0.01\n",
      "Epoch [11715/20000], Loss: -19.202667236328125, Learning Rate: 0.01\n",
      "Epoch [11716/20000], Loss: -19.205169677734375, Learning Rate: 0.01\n",
      "Epoch [11717/20000], Loss: -19.205490112304688, Learning Rate: 0.01\n",
      "Epoch [11718/20000], Loss: -19.20477294921875, Learning Rate: 0.01\n",
      "Epoch [11719/20000], Loss: -19.20465087890625, Learning Rate: 0.01\n",
      "Epoch [11720/20000], Loss: -19.205657958984375, Learning Rate: 0.01\n",
      "Epoch [11721/20000], Loss: -19.20806884765625, Learning Rate: 0.01\n",
      "Epoch [11722/20000], Loss: -19.211517333984375, Learning Rate: 0.01\n",
      "Epoch [11723/20000], Loss: -19.215118408203125, Learning Rate: 0.01\n",
      "Epoch [11724/20000], Loss: -19.218215942382812, Learning Rate: 0.01\n",
      "Epoch [11725/20000], Loss: -19.220443725585938, Learning Rate: 0.01\n",
      "Epoch [11726/20000], Loss: -19.221817016601562, Learning Rate: 0.01\n",
      "Epoch [11727/20000], Loss: -19.222671508789062, Learning Rate: 0.01\n",
      "Epoch [11728/20000], Loss: -19.223541259765625, Learning Rate: 0.01\n",
      "Epoch [11729/20000], Loss: -19.224746704101562, Learning Rate: 0.01\n",
      "Epoch [11730/20000], Loss: -19.226531982421875, Learning Rate: 0.01\n",
      "Epoch [11731/20000], Loss: -19.228683471679688, Learning Rate: 0.01\n",
      "Epoch [11732/20000], Loss: -19.231109619140625, Learning Rate: 0.01\n",
      "Epoch [11733/20000], Loss: -19.233383178710938, Learning Rate: 0.01\n",
      "Epoch [11734/20000], Loss: -19.235519409179688, Learning Rate: 0.01\n",
      "Epoch [11735/20000], Loss: -19.237274169921875, Learning Rate: 0.01\n",
      "Epoch [11736/20000], Loss: -19.238784790039062, Learning Rate: 0.01\n",
      "Epoch [11737/20000], Loss: -19.240158081054688, Learning Rate: 0.01\n",
      "Epoch [11738/20000], Loss: -19.24151611328125, Learning Rate: 0.01\n",
      "Epoch [11739/20000], Loss: -19.243026733398438, Learning Rate: 0.01\n",
      "Epoch [11740/20000], Loss: -19.244796752929688, Learning Rate: 0.01\n",
      "Epoch [11741/20000], Loss: -19.246688842773438, Learning Rate: 0.01\n",
      "Epoch [11742/20000], Loss: -19.248489379882812, Learning Rate: 0.01\n",
      "Epoch [11743/20000], Loss: -19.250442504882812, Learning Rate: 0.01\n",
      "Epoch [11744/20000], Loss: -19.252273559570312, Learning Rate: 0.01\n",
      "Epoch [11745/20000], Loss: -19.253982543945312, Learning Rate: 0.01\n",
      "Epoch [11746/20000], Loss: -19.255645751953125, Learning Rate: 0.01\n",
      "Epoch [11747/20000], Loss: -19.257156372070312, Learning Rate: 0.01\n",
      "Epoch [11748/20000], Loss: -19.258697509765625, Learning Rate: 0.01\n",
      "Epoch [11749/20000], Loss: -19.2603759765625, Learning Rate: 0.01\n",
      "Epoch [11750/20000], Loss: -19.262008666992188, Learning Rate: 0.01\n",
      "Epoch [11751/20000], Loss: -19.263717651367188, Learning Rate: 0.01\n",
      "Epoch [11752/20000], Loss: -19.26544189453125, Learning Rate: 0.01\n",
      "Epoch [11753/20000], Loss: -19.267227172851562, Learning Rate: 0.01\n",
      "Epoch [11754/20000], Loss: -19.269012451171875, Learning Rate: 0.01\n",
      "Epoch [11755/20000], Loss: -19.27069091796875, Learning Rate: 0.01\n",
      "Epoch [11756/20000], Loss: -19.272384643554688, Learning Rate: 0.01\n",
      "Epoch [11757/20000], Loss: -19.273956298828125, Learning Rate: 0.01\n",
      "Epoch [11758/20000], Loss: -19.275619506835938, Learning Rate: 0.01\n",
      "Epoch [11759/20000], Loss: -19.277297973632812, Learning Rate: 0.01\n",
      "Epoch [11760/20000], Loss: -19.278915405273438, Learning Rate: 0.01\n",
      "Epoch [11761/20000], Loss: -19.280517578125, Learning Rate: 0.01\n",
      "Epoch [11762/20000], Loss: -19.282241821289062, Learning Rate: 0.01\n",
      "Epoch [11763/20000], Loss: -19.283950805664062, Learning Rate: 0.01\n",
      "Epoch [11764/20000], Loss: -19.28564453125, Learning Rate: 0.01\n",
      "Epoch [11765/20000], Loss: -19.287246704101562, Learning Rate: 0.01\n",
      "Epoch [11766/20000], Loss: -19.289016723632812, Learning Rate: 0.01\n",
      "Epoch [11767/20000], Loss: -19.290695190429688, Learning Rate: 0.01\n",
      "Epoch [11768/20000], Loss: -19.29229736328125, Learning Rate: 0.01\n",
      "Epoch [11769/20000], Loss: -19.29400634765625, Learning Rate: 0.01\n",
      "Epoch [11770/20000], Loss: -19.29559326171875, Learning Rate: 0.01\n",
      "Epoch [11771/20000], Loss: -19.29730224609375, Learning Rate: 0.01\n",
      "Epoch [11772/20000], Loss: -19.298873901367188, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11773/20000], Loss: -19.300521850585938, Learning Rate: 0.01\n",
      "Epoch [11774/20000], Loss: -19.302169799804688, Learning Rate: 0.01\n",
      "Epoch [11775/20000], Loss: -19.303863525390625, Learning Rate: 0.01\n",
      "Epoch [11776/20000], Loss: -19.305511474609375, Learning Rate: 0.01\n",
      "Epoch [11777/20000], Loss: -19.307174682617188, Learning Rate: 0.01\n",
      "Epoch [11778/20000], Loss: -19.308914184570312, Learning Rate: 0.01\n",
      "Epoch [11779/20000], Loss: -19.31048583984375, Learning Rate: 0.01\n",
      "Epoch [11780/20000], Loss: -19.31207275390625, Learning Rate: 0.01\n",
      "Epoch [11781/20000], Loss: -19.31378173828125, Learning Rate: 0.01\n",
      "Epoch [11782/20000], Loss: -19.3154296875, Learning Rate: 0.01\n",
      "Epoch [11783/20000], Loss: -19.3170166015625, Learning Rate: 0.01\n",
      "Epoch [11784/20000], Loss: -19.318740844726562, Learning Rate: 0.01\n",
      "Epoch [11785/20000], Loss: -19.32037353515625, Learning Rate: 0.01\n",
      "Epoch [11786/20000], Loss: -19.321990966796875, Learning Rate: 0.01\n",
      "Epoch [11787/20000], Loss: -19.323654174804688, Learning Rate: 0.01\n",
      "Epoch [11788/20000], Loss: -19.3253173828125, Learning Rate: 0.01\n",
      "Epoch [11789/20000], Loss: -19.326889038085938, Learning Rate: 0.01\n",
      "Epoch [11790/20000], Loss: -19.328628540039062, Learning Rate: 0.01\n",
      "Epoch [11791/20000], Loss: -19.3302001953125, Learning Rate: 0.01\n",
      "Epoch [11792/20000], Loss: -19.331878662109375, Learning Rate: 0.01\n",
      "Epoch [11793/20000], Loss: -19.33349609375, Learning Rate: 0.01\n",
      "Epoch [11794/20000], Loss: -19.335159301757812, Learning Rate: 0.01\n",
      "Epoch [11795/20000], Loss: -19.336715698242188, Learning Rate: 0.01\n",
      "Epoch [11796/20000], Loss: -19.338348388671875, Learning Rate: 0.01\n",
      "Epoch [11797/20000], Loss: -19.339996337890625, Learning Rate: 0.01\n",
      "Epoch [11798/20000], Loss: -19.341659545898438, Learning Rate: 0.01\n",
      "Epoch [11799/20000], Loss: -19.343307495117188, Learning Rate: 0.01\n",
      "Epoch [11800/20000], Loss: -19.344894409179688, Learning Rate: 0.01\n",
      "Epoch [11801/20000], Loss: -19.346527099609375, Learning Rate: 0.01\n",
      "Epoch [11802/20000], Loss: -19.34820556640625, Learning Rate: 0.01\n",
      "Epoch [11803/20000], Loss: -19.349807739257812, Learning Rate: 0.01\n",
      "Epoch [11804/20000], Loss: -19.351470947265625, Learning Rate: 0.01\n",
      "Epoch [11805/20000], Loss: -19.35308837890625, Learning Rate: 0.01\n",
      "Epoch [11806/20000], Loss: -19.354736328125, Learning Rate: 0.01\n",
      "Epoch [11807/20000], Loss: -19.356353759765625, Learning Rate: 0.01\n",
      "Epoch [11808/20000], Loss: -19.35797119140625, Learning Rate: 0.01\n",
      "Epoch [11809/20000], Loss: -19.359649658203125, Learning Rate: 0.01\n",
      "Epoch [11810/20000], Loss: -19.361251831054688, Learning Rate: 0.01\n",
      "Epoch [11811/20000], Loss: -19.3629150390625, Learning Rate: 0.01\n",
      "Epoch [11812/20000], Loss: -19.364517211914062, Learning Rate: 0.01\n",
      "Epoch [11813/20000], Loss: -19.366134643554688, Learning Rate: 0.01\n",
      "Epoch [11814/20000], Loss: -19.367767333984375, Learning Rate: 0.01\n",
      "Epoch [11815/20000], Loss: -19.369354248046875, Learning Rate: 0.01\n",
      "Epoch [11816/20000], Loss: -19.371002197265625, Learning Rate: 0.01\n",
      "Epoch [11817/20000], Loss: -19.372573852539062, Learning Rate: 0.01\n",
      "Epoch [11818/20000], Loss: -19.37420654296875, Learning Rate: 0.01\n",
      "Epoch [11819/20000], Loss: -19.375839233398438, Learning Rate: 0.01\n",
      "Epoch [11820/20000], Loss: -19.377548217773438, Learning Rate: 0.01\n",
      "Epoch [11821/20000], Loss: -19.379180908203125, Learning Rate: 0.01\n",
      "Epoch [11822/20000], Loss: -19.3807373046875, Learning Rate: 0.01\n",
      "Epoch [11823/20000], Loss: -19.38232421875, Learning Rate: 0.01\n",
      "Epoch [11824/20000], Loss: -19.383987426757812, Learning Rate: 0.01\n",
      "Epoch [11825/20000], Loss: -19.385604858398438, Learning Rate: 0.01\n",
      "Epoch [11826/20000], Loss: -19.387252807617188, Learning Rate: 0.01\n",
      "Epoch [11827/20000], Loss: -19.388870239257812, Learning Rate: 0.01\n",
      "Epoch [11828/20000], Loss: -19.390411376953125, Learning Rate: 0.01\n",
      "Epoch [11829/20000], Loss: -19.39208984375, Learning Rate: 0.01\n",
      "Epoch [11830/20000], Loss: -19.39361572265625, Learning Rate: 0.01\n",
      "Epoch [11831/20000], Loss: -19.395309448242188, Learning Rate: 0.01\n",
      "Epoch [11832/20000], Loss: -19.396804809570312, Learning Rate: 0.01\n",
      "Epoch [11833/20000], Loss: -19.398483276367188, Learning Rate: 0.01\n",
      "Epoch [11834/20000], Loss: -19.400115966796875, Learning Rate: 0.01\n",
      "Epoch [11835/20000], Loss: -19.401763916015625, Learning Rate: 0.01\n",
      "Epoch [11836/20000], Loss: -19.403335571289062, Learning Rate: 0.01\n",
      "Epoch [11837/20000], Loss: -19.404953002929688, Learning Rate: 0.01\n",
      "Epoch [11838/20000], Loss: -19.406539916992188, Learning Rate: 0.01\n",
      "Epoch [11839/20000], Loss: -19.408096313476562, Learning Rate: 0.01\n",
      "Epoch [11840/20000], Loss: -19.409713745117188, Learning Rate: 0.01\n",
      "Epoch [11841/20000], Loss: -19.411392211914062, Learning Rate: 0.01\n",
      "Epoch [11842/20000], Loss: -19.412948608398438, Learning Rate: 0.01\n",
      "Epoch [11843/20000], Loss: -19.414566040039062, Learning Rate: 0.01\n",
      "Epoch [11844/20000], Loss: -19.416168212890625, Learning Rate: 0.01\n",
      "Epoch [11845/20000], Loss: -19.417755126953125, Learning Rate: 0.01\n",
      "Epoch [11846/20000], Loss: -19.4193115234375, Learning Rate: 0.01\n",
      "Epoch [11847/20000], Loss: -19.42095947265625, Learning Rate: 0.01\n",
      "Epoch [11848/20000], Loss: -19.422607421875, Learning Rate: 0.01\n",
      "Epoch [11849/20000], Loss: -19.424179077148438, Learning Rate: 0.01\n",
      "Epoch [11850/20000], Loss: -19.42572021484375, Learning Rate: 0.01\n",
      "Epoch [11851/20000], Loss: -19.427398681640625, Learning Rate: 0.01\n",
      "Epoch [11852/20000], Loss: -19.428955078125, Learning Rate: 0.01\n",
      "Epoch [11853/20000], Loss: -19.430526733398438, Learning Rate: 0.01\n",
      "Epoch [11854/20000], Loss: -19.432113647460938, Learning Rate: 0.01\n",
      "Epoch [11855/20000], Loss: -19.433731079101562, Learning Rate: 0.01\n",
      "Epoch [11856/20000], Loss: -19.435394287109375, Learning Rate: 0.01\n",
      "Epoch [11857/20000], Loss: -19.436859130859375, Learning Rate: 0.01\n",
      "Epoch [11858/20000], Loss: -19.438522338867188, Learning Rate: 0.01\n",
      "Epoch [11859/20000], Loss: -19.440109252929688, Learning Rate: 0.01\n",
      "Epoch [11860/20000], Loss: -19.441680908203125, Learning Rate: 0.01\n",
      "Epoch [11861/20000], Loss: -19.443267822265625, Learning Rate: 0.01\n",
      "Epoch [11862/20000], Loss: -19.44476318359375, Learning Rate: 0.01\n",
      "Epoch [11863/20000], Loss: -19.44635009765625, Learning Rate: 0.01\n",
      "Epoch [11864/20000], Loss: -19.4478759765625, Learning Rate: 0.01\n",
      "Epoch [11865/20000], Loss: -19.449371337890625, Learning Rate: 0.01\n",
      "Epoch [11866/20000], Loss: -19.450897216796875, Learning Rate: 0.01\n",
      "Epoch [11867/20000], Loss: -19.452239990234375, Learning Rate: 0.01\n",
      "Epoch [11868/20000], Loss: -19.453521728515625, Learning Rate: 0.01\n",
      "Epoch [11869/20000], Loss: -19.454742431640625, Learning Rate: 0.01\n",
      "Epoch [11870/20000], Loss: -19.455795288085938, Learning Rate: 0.01\n",
      "Epoch [11871/20000], Loss: -19.456497192382812, Learning Rate: 0.01\n",
      "Epoch [11872/20000], Loss: -19.456893920898438, Learning Rate: 0.01\n",
      "Epoch [11873/20000], Loss: -19.45672607421875, Learning Rate: 0.01\n",
      "Epoch [11874/20000], Loss: -19.455657958984375, Learning Rate: 0.01\n",
      "Epoch [11875/20000], Loss: -19.453353881835938, Learning Rate: 0.01\n",
      "Epoch [11876/20000], Loss: -19.449172973632812, Learning Rate: 0.01\n",
      "Epoch [11877/20000], Loss: -19.442169189453125, Learning Rate: 0.01\n",
      "Epoch [11878/20000], Loss: -19.430816650390625, Learning Rate: 0.01\n",
      "Epoch [11879/20000], Loss: -19.413131713867188, Learning Rate: 0.01\n",
      "Epoch [11880/20000], Loss: -19.385635375976562, Learning Rate: 0.01\n",
      "Epoch [11881/20000], Loss: -19.343399047851562, Learning Rate: 0.01\n",
      "Epoch [11882/20000], Loss: -19.27978515625, Learning Rate: 0.01\n",
      "Epoch [11883/20000], Loss: -19.18402099609375, Learning Rate: 0.01\n",
      "Epoch [11884/20000], Loss: -19.043106079101562, Learning Rate: 0.01\n",
      "Epoch [11885/20000], Loss: -18.838912963867188, Learning Rate: 0.01\n",
      "Epoch [11886/20000], Loss: -18.556594848632812, Learning Rate: 0.01\n",
      "Epoch [11887/20000], Loss: -18.183303833007812, Learning Rate: 0.01\n",
      "Epoch [11888/20000], Loss: -17.743209838867188, Learning Rate: 0.01\n",
      "Epoch [11889/20000], Loss: -17.29400634765625, Learning Rate: 0.01\n",
      "Epoch [11890/20000], Loss: -16.989425659179688, Learning Rate: 0.01\n",
      "Epoch [11891/20000], Loss: -16.987030029296875, Learning Rate: 0.01\n",
      "Epoch [11892/20000], Loss: -17.420883178710938, Learning Rate: 0.01\n",
      "Epoch [11893/20000], Loss: -18.177154541015625, Learning Rate: 0.01\n",
      "Epoch [11894/20000], Loss: -18.957870483398438, Learning Rate: 0.01\n",
      "Epoch [11895/20000], Loss: -19.414321899414062, Learning Rate: 0.01\n",
      "Epoch [11896/20000], Loss: -19.408645629882812, Learning Rate: 0.01\n",
      "Epoch [11897/20000], Loss: -19.073699951171875, Learning Rate: 0.01\n",
      "Epoch [11898/20000], Loss: -18.697525024414062, Learning Rate: 0.01\n",
      "Epoch [11899/20000], Loss: -18.550186157226562, Learning Rate: 0.01\n",
      "Epoch [11900/20000], Loss: -18.720504760742188, Learning Rate: 0.01\n",
      "Epoch [11901/20000], Loss: -19.09027099609375, Learning Rate: 0.01\n",
      "Epoch [11902/20000], Loss: -19.411956787109375, Learning Rate: 0.01\n",
      "Epoch [11903/20000], Loss: -19.50787353515625, Learning Rate: 0.01\n",
      "Epoch [11904/20000], Loss: -19.380630493164062, Learning Rate: 0.01\n",
      "Epoch [11905/20000], Loss: -19.181472778320312, Learning Rate: 0.01\n",
      "Epoch [11906/20000], Loss: -19.08172607421875, Learning Rate: 0.01\n",
      "Epoch [11907/20000], Loss: -19.150604248046875, Learning Rate: 0.01\n",
      "Epoch [11908/20000], Loss: -19.327346801757812, Learning Rate: 0.01\n",
      "Epoch [11909/20000], Loss: -19.4801025390625, Learning Rate: 0.01\n",
      "Epoch [11910/20000], Loss: -19.516998291015625, Learning Rate: 0.01\n",
      "Epoch [11911/20000], Loss: -19.445816040039062, Learning Rate: 0.01\n",
      "Epoch [11912/20000], Loss: -19.349319458007812, Learning Rate: 0.01\n",
      "Epoch [11913/20000], Loss: -19.312759399414062, Learning Rate: 0.01\n",
      "Epoch [11914/20000], Loss: -19.360946655273438, Learning Rate: 0.01\n",
      "Epoch [11915/20000], Loss: -19.452377319335938, Learning Rate: 0.01\n",
      "Epoch [11916/20000], Loss: -19.520431518554688, Learning Rate: 0.01\n",
      "Epoch [11917/20000], Loss: -19.527328491210938, Learning Rate: 0.01\n",
      "Epoch [11918/20000], Loss: -19.4869384765625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11919/20000], Loss: -19.443771362304688, Learning Rate: 0.01\n",
      "Epoch [11920/20000], Loss: -19.435791015625, Learning Rate: 0.01\n",
      "Epoch [11921/20000], Loss: -19.467269897460938, Learning Rate: 0.01\n",
      "Epoch [11922/20000], Loss: -19.512985229492188, Learning Rate: 0.01\n",
      "Epoch [11923/20000], Loss: -19.541305541992188, Learning Rate: 0.01\n",
      "Epoch [11924/20000], Loss: -19.539230346679688, Learning Rate: 0.01\n",
      "Epoch [11925/20000], Loss: -19.5177001953125, Learning Rate: 0.01\n",
      "Epoch [11926/20000], Loss: -19.499114990234375, Learning Rate: 0.01\n",
      "Epoch [11927/20000], Loss: -19.499542236328125, Learning Rate: 0.01\n",
      "Epoch [11928/20000], Loss: -19.517929077148438, Learning Rate: 0.01\n",
      "Epoch [11929/20000], Loss: -19.540451049804688, Learning Rate: 0.01\n",
      "Epoch [11930/20000], Loss: -19.553131103515625, Learning Rate: 0.01\n",
      "Epoch [11931/20000], Loss: -19.551254272460938, Learning Rate: 0.01\n",
      "Epoch [11932/20000], Loss: -19.541275024414062, Learning Rate: 0.01\n",
      "Epoch [11933/20000], Loss: -19.53387451171875, Learning Rate: 0.01\n",
      "Epoch [11934/20000], Loss: -19.535934448242188, Learning Rate: 0.01\n",
      "Epoch [11935/20000], Loss: -19.546112060546875, Learning Rate: 0.01\n",
      "Epoch [11936/20000], Loss: -19.557815551757812, Learning Rate: 0.01\n",
      "Epoch [11937/20000], Loss: -19.564422607421875, Learning Rate: 0.01\n",
      "Epoch [11938/20000], Loss: -19.564132690429688, Learning Rate: 0.01\n",
      "Epoch [11939/20000], Loss: -19.5601806640625, Learning Rate: 0.01\n",
      "Epoch [11940/20000], Loss: -19.557235717773438, Learning Rate: 0.01\n",
      "Epoch [11941/20000], Loss: -19.55889892578125, Learning Rate: 0.01\n",
      "Epoch [11942/20000], Loss: -19.564437866210938, Learning Rate: 0.01\n",
      "Epoch [11943/20000], Loss: -19.570724487304688, Learning Rate: 0.01\n",
      "Epoch [11944/20000], Loss: -19.574951171875, Learning Rate: 0.01\n",
      "Epoch [11945/20000], Loss: -19.57586669921875, Learning Rate: 0.01\n",
      "Epoch [11946/20000], Loss: -19.574737548828125, Learning Rate: 0.01\n",
      "Epoch [11947/20000], Loss: -19.573974609375, Learning Rate: 0.01\n",
      "Epoch [11948/20000], Loss: -19.5751953125, Learning Rate: 0.01\n",
      "Epoch [11949/20000], Loss: -19.578323364257812, Learning Rate: 0.01\n",
      "Epoch [11950/20000], Loss: -19.582107543945312, Learning Rate: 0.01\n",
      "Epoch [11951/20000], Loss: -19.585205078125, Learning Rate: 0.01\n",
      "Epoch [11952/20000], Loss: -19.586761474609375, Learning Rate: 0.01\n",
      "Epoch [11953/20000], Loss: -19.587142944335938, Learning Rate: 0.01\n",
      "Epoch [11954/20000], Loss: -19.587478637695312, Learning Rate: 0.01\n",
      "Epoch [11955/20000], Loss: -19.58843994140625, Learning Rate: 0.01\n",
      "Epoch [11956/20000], Loss: -19.59051513671875, Learning Rate: 0.01\n",
      "Epoch [11957/20000], Loss: -19.592987060546875, Learning Rate: 0.01\n",
      "Epoch [11958/20000], Loss: -19.595428466796875, Learning Rate: 0.01\n",
      "Epoch [11959/20000], Loss: -19.59722900390625, Learning Rate: 0.01\n",
      "Epoch [11960/20000], Loss: -19.598419189453125, Learning Rate: 0.01\n",
      "Epoch [11961/20000], Loss: -19.599258422851562, Learning Rate: 0.01\n",
      "Epoch [11962/20000], Loss: -19.600341796875, Learning Rate: 0.01\n",
      "Epoch [11963/20000], Loss: -19.601852416992188, Learning Rate: 0.01\n",
      "Epoch [11964/20000], Loss: -19.603744506835938, Learning Rate: 0.01\n",
      "Epoch [11965/20000], Loss: -19.605682373046875, Learning Rate: 0.01\n",
      "Epoch [11966/20000], Loss: -19.607498168945312, Learning Rate: 0.01\n",
      "Epoch [11967/20000], Loss: -19.608978271484375, Learning Rate: 0.01\n",
      "Epoch [11968/20000], Loss: -19.610198974609375, Learning Rate: 0.01\n",
      "Epoch [11969/20000], Loss: -19.611480712890625, Learning Rate: 0.01\n",
      "Epoch [11970/20000], Loss: -19.612777709960938, Learning Rate: 0.01\n",
      "Epoch [11971/20000], Loss: -19.614334106445312, Learning Rate: 0.01\n",
      "Epoch [11972/20000], Loss: -19.615921020507812, Learning Rate: 0.01\n",
      "Epoch [11973/20000], Loss: -19.617599487304688, Learning Rate: 0.01\n",
      "Epoch [11974/20000], Loss: -19.6192626953125, Learning Rate: 0.01\n",
      "Epoch [11975/20000], Loss: -19.620681762695312, Learning Rate: 0.01\n",
      "Epoch [11976/20000], Loss: -19.622024536132812, Learning Rate: 0.01\n",
      "Epoch [11977/20000], Loss: -19.623367309570312, Learning Rate: 0.01\n",
      "Epoch [11978/20000], Loss: -19.624740600585938, Learning Rate: 0.01\n",
      "Epoch [11979/20000], Loss: -19.626235961914062, Learning Rate: 0.01\n",
      "Epoch [11980/20000], Loss: -19.627761840820312, Learning Rate: 0.01\n",
      "Epoch [11981/20000], Loss: -19.629348754882812, Learning Rate: 0.01\n",
      "Epoch [11982/20000], Loss: -19.630859375, Learning Rate: 0.01\n",
      "Epoch [11983/20000], Loss: -19.632293701171875, Learning Rate: 0.01\n",
      "Epoch [11984/20000], Loss: -19.63372802734375, Learning Rate: 0.01\n",
      "Epoch [11985/20000], Loss: -19.635147094726562, Learning Rate: 0.01\n",
      "Epoch [11986/20000], Loss: -19.636627197265625, Learning Rate: 0.01\n",
      "Epoch [11987/20000], Loss: -19.637969970703125, Learning Rate: 0.01\n",
      "Epoch [11988/20000], Loss: -19.639495849609375, Learning Rate: 0.01\n",
      "Epoch [11989/20000], Loss: -19.641021728515625, Learning Rate: 0.01\n",
      "Epoch [11990/20000], Loss: -19.642486572265625, Learning Rate: 0.01\n",
      "Epoch [11991/20000], Loss: -19.6439208984375, Learning Rate: 0.01\n",
      "Epoch [11992/20000], Loss: -19.64532470703125, Learning Rate: 0.01\n",
      "Epoch [11993/20000], Loss: -19.646759033203125, Learning Rate: 0.01\n",
      "Epoch [11994/20000], Loss: -19.648162841796875, Learning Rate: 0.01\n",
      "Epoch [11995/20000], Loss: -19.649703979492188, Learning Rate: 0.01\n",
      "Epoch [11996/20000], Loss: -19.651046752929688, Learning Rate: 0.01\n",
      "Epoch [11997/20000], Loss: -19.65252685546875, Learning Rate: 0.01\n",
      "Epoch [11998/20000], Loss: -19.653884887695312, Learning Rate: 0.01\n",
      "Epoch [11999/20000], Loss: -19.65533447265625, Learning Rate: 0.01\n",
      "Epoch [12000/20000], Loss: -19.65679931640625, Learning Rate: 0.01\n",
      "Epoch [12001/20000], Loss: -19.65814208984375, Learning Rate: 0.01\n",
      "Epoch [12002/20000], Loss: -19.659469604492188, Learning Rate: 0.01\n",
      "Epoch [12003/20000], Loss: -19.660934448242188, Learning Rate: 0.01\n",
      "Epoch [12004/20000], Loss: -19.662384033203125, Learning Rate: 0.01\n",
      "Epoch [12005/20000], Loss: -19.663650512695312, Learning Rate: 0.01\n",
      "Epoch [12006/20000], Loss: -19.665008544921875, Learning Rate: 0.01\n",
      "Epoch [12007/20000], Loss: -19.666259765625, Learning Rate: 0.01\n",
      "Epoch [12008/20000], Loss: -19.667526245117188, Learning Rate: 0.01\n",
      "Epoch [12009/20000], Loss: -19.668731689453125, Learning Rate: 0.01\n",
      "Epoch [12010/20000], Loss: -19.6697998046875, Learning Rate: 0.01\n",
      "Epoch [12011/20000], Loss: -19.6707763671875, Learning Rate: 0.01\n",
      "Epoch [12012/20000], Loss: -19.6715087890625, Learning Rate: 0.01\n",
      "Epoch [12013/20000], Loss: -19.672119140625, Learning Rate: 0.01\n",
      "Epoch [12014/20000], Loss: -19.672317504882812, Learning Rate: 0.01\n",
      "Epoch [12015/20000], Loss: -19.672164916992188, Learning Rate: 0.01\n",
      "Epoch [12016/20000], Loss: -19.6715087890625, Learning Rate: 0.01\n",
      "Epoch [12017/20000], Loss: -19.66998291015625, Learning Rate: 0.01\n",
      "Epoch [12018/20000], Loss: -19.667312622070312, Learning Rate: 0.01\n",
      "Epoch [12019/20000], Loss: -19.663101196289062, Learning Rate: 0.01\n",
      "Epoch [12020/20000], Loss: -19.65673828125, Learning Rate: 0.01\n",
      "Epoch [12021/20000], Loss: -19.647293090820312, Learning Rate: 0.01\n",
      "Epoch [12022/20000], Loss: -19.633758544921875, Learning Rate: 0.01\n",
      "Epoch [12023/20000], Loss: -19.614288330078125, Learning Rate: 0.01\n",
      "Epoch [12024/20000], Loss: -19.586822509765625, Learning Rate: 0.01\n",
      "Epoch [12025/20000], Loss: -19.548065185546875, Learning Rate: 0.01\n",
      "Epoch [12026/20000], Loss: -19.494430541992188, Learning Rate: 0.01\n",
      "Epoch [12027/20000], Loss: -19.420562744140625, Learning Rate: 0.01\n",
      "Epoch [12028/20000], Loss: -19.3209228515625, Learning Rate: 0.01\n",
      "Epoch [12029/20000], Loss: -19.1888427734375, Learning Rate: 0.01\n",
      "Epoch [12030/20000], Loss: -19.020904541015625, Learning Rate: 0.01\n",
      "Epoch [12031/20000], Loss: -18.815826416015625, Learning Rate: 0.01\n",
      "Epoch [12032/20000], Loss: -18.586273193359375, Learning Rate: 0.01\n",
      "Epoch [12033/20000], Loss: -18.3558349609375, Learning Rate: 0.01\n",
      "Epoch [12034/20000], Loss: -18.176116943359375, Learning Rate: 0.01\n",
      "Epoch [12035/20000], Loss: -18.104156494140625, Learning Rate: 0.01\n",
      "Epoch [12036/20000], Loss: -18.2020263671875, Learning Rate: 0.01\n",
      "Epoch [12037/20000], Loss: -18.477645874023438, Learning Rate: 0.01\n",
      "Epoch [12038/20000], Loss: -18.881790161132812, Learning Rate: 0.01\n",
      "Epoch [12039/20000], Loss: -19.296157836914062, Learning Rate: 0.01\n",
      "Epoch [12040/20000], Loss: -19.598617553710938, Learning Rate: 0.01\n",
      "Epoch [12041/20000], Loss: -19.715988159179688, Learning Rate: 0.01\n",
      "Epoch [12042/20000], Loss: -19.6558837890625, Learning Rate: 0.01\n",
      "Epoch [12043/20000], Loss: -19.491851806640625, Learning Rate: 0.01\n",
      "Epoch [12044/20000], Loss: -19.322402954101562, Learning Rate: 0.01\n",
      "Epoch [12045/20000], Loss: -19.232025146484375, Learning Rate: 0.01\n",
      "Epoch [12046/20000], Loss: -19.258102416992188, Learning Rate: 0.01\n",
      "Epoch [12047/20000], Loss: -19.383377075195312, Learning Rate: 0.01\n",
      "Epoch [12048/20000], Loss: -19.546401977539062, Learning Rate: 0.01\n",
      "Epoch [12049/20000], Loss: -19.677032470703125, Learning Rate: 0.01\n",
      "Epoch [12050/20000], Loss: -19.730224609375, Learning Rate: 0.01\n",
      "Epoch [12051/20000], Loss: -19.704620361328125, Learning Rate: 0.01\n",
      "Epoch [12052/20000], Loss: -19.634506225585938, Learning Rate: 0.01\n",
      "Epoch [12053/20000], Loss: -19.567657470703125, Learning Rate: 0.01\n",
      "Epoch [12054/20000], Loss: -19.541336059570312, Learning Rate: 0.01\n",
      "Epoch [12055/20000], Loss: -19.566055297851562, Learning Rate: 0.01\n",
      "Epoch [12056/20000], Loss: -19.626358032226562, Learning Rate: 0.01\n",
      "Epoch [12057/20000], Loss: -19.691741943359375, Learning Rate: 0.01\n",
      "Epoch [12058/20000], Loss: -19.734329223632812, Learning Rate: 0.01\n",
      "Epoch [12059/20000], Loss: -19.742111206054688, Learning Rate: 0.01\n",
      "Epoch [12060/20000], Loss: -19.721664428710938, Learning Rate: 0.01\n",
      "Epoch [12061/20000], Loss: -19.691574096679688, Learning Rate: 0.01\n",
      "Epoch [12062/20000], Loss: -19.67108154296875, Learning Rate: 0.01\n",
      "Epoch [12063/20000], Loss: -19.671127319335938, Learning Rate: 0.01\n",
      "Epoch [12064/20000], Loss: -19.690505981445312, Learning Rate: 0.01\n",
      "Epoch [12065/20000], Loss: -19.718658447265625, Learning Rate: 0.01\n",
      "Epoch [12066/20000], Loss: -19.742965698242188, Learning Rate: 0.01\n",
      "Epoch [12067/20000], Loss: -19.754669189453125, Learning Rate: 0.01\n",
      "Epoch [12068/20000], Loss: -19.752716064453125, Learning Rate: 0.01\n",
      "Epoch [12069/20000], Loss: -19.742218017578125, Learning Rate: 0.01\n",
      "Epoch [12070/20000], Loss: -19.73162841796875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12071/20000], Loss: -19.727401733398438, Learning Rate: 0.01\n",
      "Epoch [12072/20000], Loss: -19.731903076171875, Learning Rate: 0.01\n",
      "Epoch [12073/20000], Loss: -19.742660522460938, Learning Rate: 0.01\n",
      "Epoch [12074/20000], Loss: -19.754837036132812, Learning Rate: 0.01\n",
      "Epoch [12075/20000], Loss: -19.7637939453125, Learning Rate: 0.01\n",
      "Epoch [12076/20000], Loss: -19.767333984375, Learning Rate: 0.01\n",
      "Epoch [12077/20000], Loss: -19.765777587890625, Learning Rate: 0.01\n",
      "Epoch [12078/20000], Loss: -19.76214599609375, Learning Rate: 0.01\n",
      "Epoch [12079/20000], Loss: -19.75921630859375, Learning Rate: 0.01\n",
      "Epoch [12080/20000], Loss: -19.759323120117188, Learning Rate: 0.01\n",
      "Epoch [12081/20000], Loss: -19.762680053710938, Learning Rate: 0.01\n",
      "Epoch [12082/20000], Loss: -19.76806640625, Learning Rate: 0.01\n",
      "Epoch [12083/20000], Loss: -19.773574829101562, Learning Rate: 0.01\n",
      "Epoch [12084/20000], Loss: -19.777679443359375, Learning Rate: 0.01\n",
      "Epoch [12085/20000], Loss: -19.779510498046875, Learning Rate: 0.01\n",
      "Epoch [12086/20000], Loss: -19.779525756835938, Learning Rate: 0.01\n",
      "Epoch [12087/20000], Loss: -19.778884887695312, Learning Rate: 0.01\n",
      "Epoch [12088/20000], Loss: -19.778594970703125, Learning Rate: 0.01\n",
      "Epoch [12089/20000], Loss: -19.779571533203125, Learning Rate: 0.01\n",
      "Epoch [12090/20000], Loss: -19.781723022460938, Learning Rate: 0.01\n",
      "Epoch [12091/20000], Loss: -19.78460693359375, Learning Rate: 0.01\n",
      "Epoch [12092/20000], Loss: -19.787490844726562, Learning Rate: 0.01\n",
      "Epoch [12093/20000], Loss: -19.790008544921875, Learning Rate: 0.01\n",
      "Epoch [12094/20000], Loss: -19.7916259765625, Learning Rate: 0.01\n",
      "Epoch [12095/20000], Loss: -19.79254150390625, Learning Rate: 0.01\n",
      "Epoch [12096/20000], Loss: -19.793075561523438, Learning Rate: 0.01\n",
      "Epoch [12097/20000], Loss: -19.793777465820312, Learning Rate: 0.01\n",
      "Epoch [12098/20000], Loss: -19.794845581054688, Learning Rate: 0.01\n",
      "Epoch [12099/20000], Loss: -19.796310424804688, Learning Rate: 0.01\n",
      "Epoch [12100/20000], Loss: -19.798263549804688, Learning Rate: 0.01\n",
      "Epoch [12101/20000], Loss: -19.8001708984375, Learning Rate: 0.01\n",
      "Epoch [12102/20000], Loss: -19.802032470703125, Learning Rate: 0.01\n",
      "Epoch [12103/20000], Loss: -19.803634643554688, Learning Rate: 0.01\n",
      "Epoch [12104/20000], Loss: -19.804824829101562, Learning Rate: 0.01\n",
      "Epoch [12105/20000], Loss: -19.805877685546875, Learning Rate: 0.01\n",
      "Epoch [12106/20000], Loss: -19.806869506835938, Learning Rate: 0.01\n",
      "Epoch [12107/20000], Loss: -19.808181762695312, Learning Rate: 0.01\n",
      "Epoch [12108/20000], Loss: -19.809417724609375, Learning Rate: 0.01\n",
      "Epoch [12109/20000], Loss: -19.81085205078125, Learning Rate: 0.01\n",
      "Epoch [12110/20000], Loss: -19.812454223632812, Learning Rate: 0.01\n",
      "Epoch [12111/20000], Loss: -19.814010620117188, Learning Rate: 0.01\n",
      "Epoch [12112/20000], Loss: -19.815475463867188, Learning Rate: 0.01\n",
      "Epoch [12113/20000], Loss: -19.816818237304688, Learning Rate: 0.01\n",
      "Epoch [12114/20000], Loss: -19.818099975585938, Learning Rate: 0.01\n",
      "Epoch [12115/20000], Loss: -19.819305419921875, Learning Rate: 0.01\n",
      "Epoch [12116/20000], Loss: -19.820526123046875, Learning Rate: 0.01\n",
      "Epoch [12117/20000], Loss: -19.82177734375, Learning Rate: 0.01\n",
      "Epoch [12118/20000], Loss: -19.8231201171875, Learning Rate: 0.01\n",
      "Epoch [12119/20000], Loss: -19.824493408203125, Learning Rate: 0.01\n",
      "Epoch [12120/20000], Loss: -19.82586669921875, Learning Rate: 0.01\n",
      "Epoch [12121/20000], Loss: -19.827224731445312, Learning Rate: 0.01\n",
      "Epoch [12122/20000], Loss: -19.82861328125, Learning Rate: 0.01\n",
      "Epoch [12123/20000], Loss: -19.829925537109375, Learning Rate: 0.01\n",
      "Epoch [12124/20000], Loss: -19.831268310546875, Learning Rate: 0.01\n",
      "Epoch [12125/20000], Loss: -19.83258056640625, Learning Rate: 0.01\n",
      "Epoch [12126/20000], Loss: -19.8338623046875, Learning Rate: 0.01\n",
      "Epoch [12127/20000], Loss: -19.835128784179688, Learning Rate: 0.01\n",
      "Epoch [12128/20000], Loss: -19.836410522460938, Learning Rate: 0.01\n",
      "Epoch [12129/20000], Loss: -19.837814331054688, Learning Rate: 0.01\n",
      "Epoch [12130/20000], Loss: -19.839065551757812, Learning Rate: 0.01\n",
      "Epoch [12131/20000], Loss: -19.840408325195312, Learning Rate: 0.01\n",
      "Epoch [12132/20000], Loss: -19.841720581054688, Learning Rate: 0.01\n",
      "Epoch [12133/20000], Loss: -19.843032836914062, Learning Rate: 0.01\n",
      "Epoch [12134/20000], Loss: -19.844375610351562, Learning Rate: 0.01\n",
      "Epoch [12135/20000], Loss: -19.84564208984375, Learning Rate: 0.01\n",
      "Epoch [12136/20000], Loss: -19.846969604492188, Learning Rate: 0.01\n",
      "Epoch [12137/20000], Loss: -19.84820556640625, Learning Rate: 0.01\n",
      "Epoch [12138/20000], Loss: -19.849502563476562, Learning Rate: 0.01\n",
      "Epoch [12139/20000], Loss: -19.850830078125, Learning Rate: 0.01\n",
      "Epoch [12140/20000], Loss: -19.852218627929688, Learning Rate: 0.01\n",
      "Epoch [12141/20000], Loss: -19.853515625, Learning Rate: 0.01\n",
      "Epoch [12142/20000], Loss: -19.854766845703125, Learning Rate: 0.01\n",
      "Epoch [12143/20000], Loss: -19.8560791015625, Learning Rate: 0.01\n",
      "Epoch [12144/20000], Loss: -19.857437133789062, Learning Rate: 0.01\n",
      "Epoch [12145/20000], Loss: -19.858673095703125, Learning Rate: 0.01\n",
      "Epoch [12146/20000], Loss: -19.86004638671875, Learning Rate: 0.01\n",
      "Epoch [12147/20000], Loss: -19.86126708984375, Learning Rate: 0.01\n",
      "Epoch [12148/20000], Loss: -19.862533569335938, Learning Rate: 0.01\n",
      "Epoch [12149/20000], Loss: -19.863815307617188, Learning Rate: 0.01\n",
      "Epoch [12150/20000], Loss: -19.865097045898438, Learning Rate: 0.01\n",
      "Epoch [12151/20000], Loss: -19.866348266601562, Learning Rate: 0.01\n",
      "Epoch [12152/20000], Loss: -19.867660522460938, Learning Rate: 0.01\n",
      "Epoch [12153/20000], Loss: -19.868972778320312, Learning Rate: 0.01\n",
      "Epoch [12154/20000], Loss: -19.870315551757812, Learning Rate: 0.01\n",
      "Epoch [12155/20000], Loss: -19.871551513671875, Learning Rate: 0.01\n",
      "Epoch [12156/20000], Loss: -19.872802734375, Learning Rate: 0.01\n",
      "Epoch [12157/20000], Loss: -19.874114990234375, Learning Rate: 0.01\n",
      "Epoch [12158/20000], Loss: -19.87548828125, Learning Rate: 0.01\n",
      "Epoch [12159/20000], Loss: -19.876724243164062, Learning Rate: 0.01\n",
      "Epoch [12160/20000], Loss: -19.877960205078125, Learning Rate: 0.01\n",
      "Epoch [12161/20000], Loss: -19.879302978515625, Learning Rate: 0.01\n",
      "Epoch [12162/20000], Loss: -19.880630493164062, Learning Rate: 0.01\n",
      "Epoch [12163/20000], Loss: -19.8818359375, Learning Rate: 0.01\n",
      "Epoch [12164/20000], Loss: -19.88311767578125, Learning Rate: 0.01\n",
      "Epoch [12165/20000], Loss: -19.88446044921875, Learning Rate: 0.01\n",
      "Epoch [12166/20000], Loss: -19.885726928710938, Learning Rate: 0.01\n",
      "Epoch [12167/20000], Loss: -19.887069702148438, Learning Rate: 0.01\n",
      "Epoch [12168/20000], Loss: -19.888214111328125, Learning Rate: 0.01\n",
      "Epoch [12169/20000], Loss: -19.889541625976562, Learning Rate: 0.01\n",
      "Epoch [12170/20000], Loss: -19.890777587890625, Learning Rate: 0.01\n",
      "Epoch [12171/20000], Loss: -19.89208984375, Learning Rate: 0.01\n",
      "Epoch [12172/20000], Loss: -19.893356323242188, Learning Rate: 0.01\n",
      "Epoch [12173/20000], Loss: -19.894699096679688, Learning Rate: 0.01\n",
      "Epoch [12174/20000], Loss: -19.89599609375, Learning Rate: 0.01\n",
      "Epoch [12175/20000], Loss: -19.897201538085938, Learning Rate: 0.01\n",
      "Epoch [12176/20000], Loss: -19.8984375, Learning Rate: 0.01\n",
      "Epoch [12177/20000], Loss: -19.899749755859375, Learning Rate: 0.01\n",
      "Epoch [12178/20000], Loss: -19.9010009765625, Learning Rate: 0.01\n",
      "Epoch [12179/20000], Loss: -19.902328491210938, Learning Rate: 0.01\n",
      "Epoch [12180/20000], Loss: -19.903549194335938, Learning Rate: 0.01\n",
      "Epoch [12181/20000], Loss: -19.904815673828125, Learning Rate: 0.01\n",
      "Epoch [12182/20000], Loss: -19.90606689453125, Learning Rate: 0.01\n",
      "Epoch [12183/20000], Loss: -19.907302856445312, Learning Rate: 0.01\n",
      "Epoch [12184/20000], Loss: -19.908584594726562, Learning Rate: 0.01\n",
      "Epoch [12185/20000], Loss: -19.90985107421875, Learning Rate: 0.01\n",
      "Epoch [12186/20000], Loss: -19.9111328125, Learning Rate: 0.01\n",
      "Epoch [12187/20000], Loss: -19.912429809570312, Learning Rate: 0.01\n",
      "Epoch [12188/20000], Loss: -19.913681030273438, Learning Rate: 0.01\n",
      "Epoch [12189/20000], Loss: -19.914932250976562, Learning Rate: 0.01\n",
      "Epoch [12190/20000], Loss: -19.916229248046875, Learning Rate: 0.01\n",
      "Epoch [12191/20000], Loss: -19.91741943359375, Learning Rate: 0.01\n",
      "Epoch [12192/20000], Loss: -19.918670654296875, Learning Rate: 0.01\n",
      "Epoch [12193/20000], Loss: -19.919891357421875, Learning Rate: 0.01\n",
      "Epoch [12194/20000], Loss: -19.921173095703125, Learning Rate: 0.01\n",
      "Epoch [12195/20000], Loss: -19.922332763671875, Learning Rate: 0.01\n",
      "Epoch [12196/20000], Loss: -19.923568725585938, Learning Rate: 0.01\n",
      "Epoch [12197/20000], Loss: -19.92474365234375, Learning Rate: 0.01\n",
      "Epoch [12198/20000], Loss: -19.925857543945312, Learning Rate: 0.01\n",
      "Epoch [12199/20000], Loss: -19.926925659179688, Learning Rate: 0.01\n",
      "Epoch [12200/20000], Loss: -19.927932739257812, Learning Rate: 0.01\n",
      "Epoch [12201/20000], Loss: -19.9288330078125, Learning Rate: 0.01\n",
      "Epoch [12202/20000], Loss: -19.929580688476562, Learning Rate: 0.01\n",
      "Epoch [12203/20000], Loss: -19.930130004882812, Learning Rate: 0.01\n",
      "Epoch [12204/20000], Loss: -19.930252075195312, Learning Rate: 0.01\n",
      "Epoch [12205/20000], Loss: -19.92999267578125, Learning Rate: 0.01\n",
      "Epoch [12206/20000], Loss: -19.929000854492188, Learning Rate: 0.01\n",
      "Epoch [12207/20000], Loss: -19.926895141601562, Learning Rate: 0.01\n",
      "Epoch [12208/20000], Loss: -19.9232177734375, Learning Rate: 0.01\n",
      "Epoch [12209/20000], Loss: -19.917205810546875, Learning Rate: 0.01\n",
      "Epoch [12210/20000], Loss: -19.907730102539062, Learning Rate: 0.01\n",
      "Epoch [12211/20000], Loss: -19.892715454101562, Learning Rate: 0.01\n",
      "Epoch [12212/20000], Loss: -19.870025634765625, Learning Rate: 0.01\n",
      "Epoch [12213/20000], Loss: -19.835174560546875, Learning Rate: 0.01\n",
      "Epoch [12214/20000], Loss: -19.782760620117188, Learning Rate: 0.01\n",
      "Epoch [12215/20000], Loss: -19.704208374023438, Learning Rate: 0.01\n",
      "Epoch [12216/20000], Loss: -19.588836669921875, Learning Rate: 0.01\n",
      "Epoch [12217/20000], Loss: -19.421035766601562, Learning Rate: 0.01\n",
      "Epoch [12218/20000], Loss: -19.188018798828125, Learning Rate: 0.01\n",
      "Epoch [12219/20000], Loss: -18.875473022460938, Learning Rate: 0.01\n",
      "Epoch [12220/20000], Loss: -18.498367309570312, Learning Rate: 0.01\n",
      "Epoch [12221/20000], Loss: -18.09246826171875, Learning Rate: 0.01\n",
      "Epoch [12222/20000], Loss: -17.780990600585938, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12223/20000], Loss: -17.694259643554688, Learning Rate: 0.01\n",
      "Epoch [12224/20000], Loss: -17.983795166015625, Learning Rate: 0.01\n",
      "Epoch [12225/20000], Loss: -18.59765625, Learning Rate: 0.01\n",
      "Epoch [12226/20000], Loss: -19.32373046875, Learning Rate: 0.01\n",
      "Epoch [12227/20000], Loss: -19.836334228515625, Learning Rate: 0.01\n",
      "Epoch [12228/20000], Loss: -19.951080322265625, Learning Rate: 0.01\n",
      "Epoch [12229/20000], Loss: -19.719146728515625, Learning Rate: 0.01\n",
      "Epoch [12230/20000], Loss: -19.366928100585938, Learning Rate: 0.01\n",
      "Epoch [12231/20000], Loss: -19.150558471679688, Learning Rate: 0.01\n",
      "Epoch [12232/20000], Loss: -19.203903198242188, Learning Rate: 0.01\n",
      "Epoch [12233/20000], Loss: -19.486190795898438, Learning Rate: 0.01\n",
      "Epoch [12234/20000], Loss: -19.802734375, Learning Rate: 0.01\n",
      "Epoch [12235/20000], Loss: -19.966384887695312, Learning Rate: 0.01\n",
      "Epoch [12236/20000], Loss: -19.91851806640625, Learning Rate: 0.01\n",
      "Epoch [12237/20000], Loss: -19.750091552734375, Learning Rate: 0.01\n",
      "Epoch [12238/20000], Loss: -19.614456176757812, Learning Rate: 0.01\n",
      "Epoch [12239/20000], Loss: -19.612747192382812, Learning Rate: 0.01\n",
      "Epoch [12240/20000], Loss: -19.738906860351562, Learning Rate: 0.01\n",
      "Epoch [12241/20000], Loss: -19.895095825195312, Learning Rate: 0.01\n",
      "Epoch [12242/20000], Loss: -19.9794921875, Learning Rate: 0.01\n",
      "Epoch [12243/20000], Loss: -19.957504272460938, Learning Rate: 0.01\n",
      "Epoch [12244/20000], Loss: -19.874893188476562, Learning Rate: 0.01\n",
      "Epoch [12245/20000], Loss: -19.810165405273438, Learning Rate: 0.01\n",
      "Epoch [12246/20000], Loss: -19.81353759765625, Learning Rate: 0.01\n",
      "Epoch [12247/20000], Loss: -19.87811279296875, Learning Rate: 0.01\n",
      "Epoch [12248/20000], Loss: -19.953338623046875, Learning Rate: 0.01\n",
      "Epoch [12249/20000], Loss: -19.9906005859375, Learning Rate: 0.01\n",
      "Epoch [12250/20000], Loss: -19.976547241210938, Learning Rate: 0.01\n",
      "Epoch [12251/20000], Loss: -19.936111450195312, Learning Rate: 0.01\n",
      "Epoch [12252/20000], Loss: -19.907699584960938, Learning Rate: 0.01\n",
      "Epoch [12253/20000], Loss: -19.913238525390625, Learning Rate: 0.01\n",
      "Epoch [12254/20000], Loss: -19.946807861328125, Learning Rate: 0.01\n",
      "Epoch [12255/20000], Loss: -19.983230590820312, Learning Rate: 0.01\n",
      "Epoch [12256/20000], Loss: -20.000076293945312, Learning Rate: 0.01\n",
      "Epoch [12257/20000], Loss: -19.992507934570312, Learning Rate: 0.01\n",
      "Epoch [12258/20000], Loss: -19.973403930664062, Learning Rate: 0.01\n",
      "Epoch [12259/20000], Loss: -19.960784912109375, Learning Rate: 0.01\n",
      "Epoch [12260/20000], Loss: -19.964431762695312, Learning Rate: 0.01\n",
      "Epoch [12261/20000], Loss: -19.981369018554688, Learning Rate: 0.01\n",
      "Epoch [12262/20000], Loss: -19.999420166015625, Learning Rate: 0.01\n",
      "Epoch [12263/20000], Loss: -20.008377075195312, Learning Rate: 0.01\n",
      "Epoch [12264/20000], Loss: -20.005752563476562, Learning Rate: 0.01\n",
      "Epoch [12265/20000], Loss: -19.997344970703125, Learning Rate: 0.01\n",
      "Epoch [12266/20000], Loss: -19.991500854492188, Learning Rate: 0.01\n",
      "Epoch [12267/20000], Loss: -19.9931640625, Learning Rate: 0.01\n",
      "Epoch [12268/20000], Loss: -20.00140380859375, Learning Rate: 0.01\n",
      "Epoch [12269/20000], Loss: -20.010986328125, Learning Rate: 0.01\n",
      "Epoch [12270/20000], Loss: -20.016555786132812, Learning Rate: 0.01\n",
      "Epoch [12271/20000], Loss: -20.016647338867188, Learning Rate: 0.01\n",
      "Epoch [12272/20000], Loss: -20.013519287109375, Learning Rate: 0.01\n",
      "Epoch [12273/20000], Loss: -20.010910034179688, Learning Rate: 0.01\n",
      "Epoch [12274/20000], Loss: -20.011474609375, Learning Rate: 0.01\n",
      "Epoch [12275/20000], Loss: -20.015411376953125, Learning Rate: 0.01\n",
      "Epoch [12276/20000], Loss: -20.020599365234375, Learning Rate: 0.01\n",
      "Epoch [12277/20000], Loss: -20.024429321289062, Learning Rate: 0.01\n",
      "Epoch [12278/20000], Loss: -20.025970458984375, Learning Rate: 0.01\n",
      "Epoch [12279/20000], Loss: -20.025405883789062, Learning Rate: 0.01\n",
      "Epoch [12280/20000], Loss: -20.024520874023438, Learning Rate: 0.01\n",
      "Epoch [12281/20000], Loss: -20.024795532226562, Learning Rate: 0.01\n",
      "Epoch [12282/20000], Loss: -20.026702880859375, Learning Rate: 0.01\n",
      "Epoch [12283/20000], Loss: -20.029556274414062, Learning Rate: 0.01\n",
      "Epoch [12284/20000], Loss: -20.032318115234375, Learning Rate: 0.01\n",
      "Epoch [12285/20000], Loss: -20.034225463867188, Learning Rate: 0.01\n",
      "Epoch [12286/20000], Loss: -20.035003662109375, Learning Rate: 0.01\n",
      "Epoch [12287/20000], Loss: -20.035263061523438, Learning Rate: 0.01\n",
      "Epoch [12288/20000], Loss: -20.0355224609375, Learning Rate: 0.01\n",
      "Epoch [12289/20000], Loss: -20.036636352539062, Learning Rate: 0.01\n",
      "Epoch [12290/20000], Loss: -20.038330078125, Learning Rate: 0.01\n",
      "Epoch [12291/20000], Loss: -20.040374755859375, Learning Rate: 0.01\n",
      "Epoch [12292/20000], Loss: -20.042068481445312, Learning Rate: 0.01\n",
      "Epoch [12293/20000], Loss: -20.043441772460938, Learning Rate: 0.01\n",
      "Epoch [12294/20000], Loss: -20.044326782226562, Learning Rate: 0.01\n",
      "Epoch [12295/20000], Loss: -20.0450439453125, Learning Rate: 0.01\n",
      "Epoch [12296/20000], Loss: -20.045989990234375, Learning Rate: 0.01\n",
      "Epoch [12297/20000], Loss: -20.047027587890625, Learning Rate: 0.01\n",
      "Epoch [12298/20000], Loss: -20.048507690429688, Learning Rate: 0.01\n",
      "Epoch [12299/20000], Loss: -20.050033569335938, Learning Rate: 0.01\n",
      "Epoch [12300/20000], Loss: -20.051406860351562, Learning Rate: 0.01\n",
      "Epoch [12301/20000], Loss: -20.052581787109375, Learning Rate: 0.01\n",
      "Epoch [12302/20000], Loss: -20.05364990234375, Learning Rate: 0.01\n",
      "Epoch [12303/20000], Loss: -20.054641723632812, Learning Rate: 0.01\n",
      "Epoch [12304/20000], Loss: -20.055633544921875, Learning Rate: 0.01\n",
      "Epoch [12305/20000], Loss: -20.0567626953125, Learning Rate: 0.01\n",
      "Epoch [12306/20000], Loss: -20.058029174804688, Learning Rate: 0.01\n",
      "Epoch [12307/20000], Loss: -20.05926513671875, Learning Rate: 0.01\n",
      "Epoch [12308/20000], Loss: -20.060592651367188, Learning Rate: 0.01\n",
      "Epoch [12309/20000], Loss: -20.061782836914062, Learning Rate: 0.01\n",
      "Epoch [12310/20000], Loss: -20.062835693359375, Learning Rate: 0.01\n",
      "Epoch [12311/20000], Loss: -20.063949584960938, Learning Rate: 0.01\n",
      "Epoch [12312/20000], Loss: -20.0650634765625, Learning Rate: 0.01\n",
      "Epoch [12313/20000], Loss: -20.066085815429688, Learning Rate: 0.01\n",
      "Epoch [12314/20000], Loss: -20.067245483398438, Learning Rate: 0.01\n",
      "Epoch [12315/20000], Loss: -20.068435668945312, Learning Rate: 0.01\n",
      "Epoch [12316/20000], Loss: -20.06964111328125, Learning Rate: 0.01\n",
      "Epoch [12317/20000], Loss: -20.07086181640625, Learning Rate: 0.01\n",
      "Epoch [12318/20000], Loss: -20.071990966796875, Learning Rate: 0.01\n",
      "Epoch [12319/20000], Loss: -20.073135375976562, Learning Rate: 0.01\n",
      "Epoch [12320/20000], Loss: -20.07415771484375, Learning Rate: 0.01\n",
      "Epoch [12321/20000], Loss: -20.0753173828125, Learning Rate: 0.01\n",
      "Epoch [12322/20000], Loss: -20.076400756835938, Learning Rate: 0.01\n",
      "Epoch [12323/20000], Loss: -20.077545166015625, Learning Rate: 0.01\n",
      "Epoch [12324/20000], Loss: -20.078659057617188, Learning Rate: 0.01\n",
      "Epoch [12325/20000], Loss: -20.079818725585938, Learning Rate: 0.01\n",
      "Epoch [12326/20000], Loss: -20.081008911132812, Learning Rate: 0.01\n",
      "Epoch [12327/20000], Loss: -20.08209228515625, Learning Rate: 0.01\n",
      "Epoch [12328/20000], Loss: -20.083282470703125, Learning Rate: 0.01\n",
      "Epoch [12329/20000], Loss: -20.084365844726562, Learning Rate: 0.01\n",
      "Epoch [12330/20000], Loss: -20.08544921875, Learning Rate: 0.01\n",
      "Epoch [12331/20000], Loss: -20.086532592773438, Learning Rate: 0.01\n",
      "Epoch [12332/20000], Loss: -20.087738037109375, Learning Rate: 0.01\n",
      "Epoch [12333/20000], Loss: -20.088821411132812, Learning Rate: 0.01\n",
      "Epoch [12334/20000], Loss: -20.0899658203125, Learning Rate: 0.01\n",
      "Epoch [12335/20000], Loss: -20.09112548828125, Learning Rate: 0.01\n",
      "Epoch [12336/20000], Loss: -20.092269897460938, Learning Rate: 0.01\n",
      "Epoch [12337/20000], Loss: -20.09332275390625, Learning Rate: 0.01\n",
      "Epoch [12338/20000], Loss: -20.094451904296875, Learning Rate: 0.01\n",
      "Epoch [12339/20000], Loss: -20.095550537109375, Learning Rate: 0.01\n",
      "Epoch [12340/20000], Loss: -20.096710205078125, Learning Rate: 0.01\n",
      "Epoch [12341/20000], Loss: -20.0977783203125, Learning Rate: 0.01\n",
      "Epoch [12342/20000], Loss: -20.098892211914062, Learning Rate: 0.01\n",
      "Epoch [12343/20000], Loss: -20.100021362304688, Learning Rate: 0.01\n",
      "Epoch [12344/20000], Loss: -20.10113525390625, Learning Rate: 0.01\n",
      "Epoch [12345/20000], Loss: -20.102249145507812, Learning Rate: 0.01\n",
      "Epoch [12346/20000], Loss: -20.103302001953125, Learning Rate: 0.01\n",
      "Epoch [12347/20000], Loss: -20.104446411132812, Learning Rate: 0.01\n",
      "Epoch [12348/20000], Loss: -20.10552978515625, Learning Rate: 0.01\n",
      "Epoch [12349/20000], Loss: -20.106689453125, Learning Rate: 0.01\n",
      "Epoch [12350/20000], Loss: -20.107711791992188, Learning Rate: 0.01\n",
      "Epoch [12351/20000], Loss: -20.108871459960938, Learning Rate: 0.01\n",
      "Epoch [12352/20000], Loss: -20.109939575195312, Learning Rate: 0.01\n",
      "Epoch [12353/20000], Loss: -20.11102294921875, Learning Rate: 0.01\n",
      "Epoch [12354/20000], Loss: -20.11212158203125, Learning Rate: 0.01\n",
      "Epoch [12355/20000], Loss: -20.113311767578125, Learning Rate: 0.01\n",
      "Epoch [12356/20000], Loss: -20.1143798828125, Learning Rate: 0.01\n",
      "Epoch [12357/20000], Loss: -20.11553955078125, Learning Rate: 0.01\n",
      "Epoch [12358/20000], Loss: -20.116592407226562, Learning Rate: 0.01\n",
      "Epoch [12359/20000], Loss: -20.11767578125, Learning Rate: 0.01\n",
      "Epoch [12360/20000], Loss: -20.1187744140625, Learning Rate: 0.01\n",
      "Epoch [12361/20000], Loss: -20.119842529296875, Learning Rate: 0.01\n",
      "Epoch [12362/20000], Loss: -20.121002197265625, Learning Rate: 0.01\n",
      "Epoch [12363/20000], Loss: -20.12213134765625, Learning Rate: 0.01\n",
      "Epoch [12364/20000], Loss: -20.123275756835938, Learning Rate: 0.01\n",
      "Epoch [12365/20000], Loss: -20.124313354492188, Learning Rate: 0.01\n",
      "Epoch [12366/20000], Loss: -20.125396728515625, Learning Rate: 0.01\n",
      "Epoch [12367/20000], Loss: -20.126541137695312, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12368/20000], Loss: -20.127593994140625, Learning Rate: 0.01\n",
      "Epoch [12369/20000], Loss: -20.128707885742188, Learning Rate: 0.01\n",
      "Epoch [12370/20000], Loss: -20.129791259765625, Learning Rate: 0.01\n",
      "Epoch [12371/20000], Loss: -20.130859375, Learning Rate: 0.01\n",
      "Epoch [12372/20000], Loss: -20.13189697265625, Learning Rate: 0.01\n",
      "Epoch [12373/20000], Loss: -20.132965087890625, Learning Rate: 0.01\n",
      "Epoch [12374/20000], Loss: -20.134063720703125, Learning Rate: 0.01\n",
      "Epoch [12375/20000], Loss: -20.135147094726562, Learning Rate: 0.01\n",
      "Epoch [12376/20000], Loss: -20.136276245117188, Learning Rate: 0.01\n",
      "Epoch [12377/20000], Loss: -20.137344360351562, Learning Rate: 0.01\n",
      "Epoch [12378/20000], Loss: -20.138397216796875, Learning Rate: 0.01\n",
      "Epoch [12379/20000], Loss: -20.139450073242188, Learning Rate: 0.01\n",
      "Epoch [12380/20000], Loss: -20.140518188476562, Learning Rate: 0.01\n",
      "Epoch [12381/20000], Loss: -20.141571044921875, Learning Rate: 0.01\n",
      "Epoch [12382/20000], Loss: -20.1427001953125, Learning Rate: 0.01\n",
      "Epoch [12383/20000], Loss: -20.143630981445312, Learning Rate: 0.01\n",
      "Epoch [12384/20000], Loss: -20.144622802734375, Learning Rate: 0.01\n",
      "Epoch [12385/20000], Loss: -20.145675659179688, Learning Rate: 0.01\n",
      "Epoch [12386/20000], Loss: -20.146591186523438, Learning Rate: 0.01\n",
      "Epoch [12387/20000], Loss: -20.1474609375, Learning Rate: 0.01\n",
      "Epoch [12388/20000], Loss: -20.14825439453125, Learning Rate: 0.01\n",
      "Epoch [12389/20000], Loss: -20.149002075195312, Learning Rate: 0.01\n",
      "Epoch [12390/20000], Loss: -20.1495361328125, Learning Rate: 0.01\n",
      "Epoch [12391/20000], Loss: -20.149993896484375, Learning Rate: 0.01\n",
      "Epoch [12392/20000], Loss: -20.150131225585938, Learning Rate: 0.01\n",
      "Epoch [12393/20000], Loss: -20.14984130859375, Learning Rate: 0.01\n",
      "Epoch [12394/20000], Loss: -20.149078369140625, Learning Rate: 0.01\n",
      "Epoch [12395/20000], Loss: -20.1475830078125, Learning Rate: 0.01\n",
      "Epoch [12396/20000], Loss: -20.145095825195312, Learning Rate: 0.01\n",
      "Epoch [12397/20000], Loss: -20.141143798828125, Learning Rate: 0.01\n",
      "Epoch [12398/20000], Loss: -20.135177612304688, Learning Rate: 0.01\n",
      "Epoch [12399/20000], Loss: -20.126220703125, Learning Rate: 0.01\n",
      "Epoch [12400/20000], Loss: -20.113174438476562, Learning Rate: 0.01\n",
      "Epoch [12401/20000], Loss: -20.094345092773438, Learning Rate: 0.01\n",
      "Epoch [12402/20000], Loss: -20.067291259765625, Learning Rate: 0.01\n",
      "Epoch [12403/20000], Loss: -20.028656005859375, Learning Rate: 0.01\n",
      "Epoch [12404/20000], Loss: -19.974395751953125, Learning Rate: 0.01\n",
      "Epoch [12405/20000], Loss: -19.898605346679688, Learning Rate: 0.01\n",
      "Epoch [12406/20000], Loss: -19.795318603515625, Learning Rate: 0.01\n",
      "Epoch [12407/20000], Loss: -19.657135009765625, Learning Rate: 0.01\n",
      "Epoch [12408/20000], Loss: -19.481109619140625, Learning Rate: 0.01\n",
      "Epoch [12409/20000], Loss: -19.267852783203125, Learning Rate: 0.01\n",
      "Epoch [12410/20000], Loss: -19.035675048828125, Learning Rate: 0.01\n",
      "Epoch [12411/20000], Loss: -18.817657470703125, Learning Rate: 0.01\n",
      "Epoch [12412/20000], Loss: -18.677780151367188, Learning Rate: 0.01\n",
      "Epoch [12413/20000], Loss: -18.678939819335938, Learning Rate: 0.01\n",
      "Epoch [12414/20000], Loss: -18.871475219726562, Learning Rate: 0.01\n",
      "Epoch [12415/20000], Loss: -19.227920532226562, Learning Rate: 0.01\n",
      "Epoch [12416/20000], Loss: -19.65142822265625, Learning Rate: 0.01\n",
      "Epoch [12417/20000], Loss: -20.000137329101562, Learning Rate: 0.01\n",
      "Epoch [12418/20000], Loss: -20.170257568359375, Learning Rate: 0.01\n",
      "Epoch [12419/20000], Loss: -20.143722534179688, Learning Rate: 0.01\n",
      "Epoch [12420/20000], Loss: -19.9866943359375, Learning Rate: 0.01\n",
      "Epoch [12421/20000], Loss: -19.807586669921875, Learning Rate: 0.01\n",
      "Epoch [12422/20000], Loss: -19.703948974609375, Learning Rate: 0.01\n",
      "Epoch [12423/20000], Loss: -19.72650146484375, Learning Rate: 0.01\n",
      "Epoch [12424/20000], Loss: -19.856597900390625, Learning Rate: 0.01\n",
      "Epoch [12425/20000], Loss: -20.02569580078125, Learning Rate: 0.01\n",
      "Epoch [12426/20000], Loss: -20.1531982421875, Learning Rate: 0.01\n",
      "Epoch [12427/20000], Loss: -20.192459106445312, Learning Rate: 0.01\n",
      "Epoch [12428/20000], Loss: -20.148849487304688, Learning Rate: 0.01\n",
      "Epoch [12429/20000], Loss: -20.068618774414062, Learning Rate: 0.01\n",
      "Epoch [12430/20000], Loss: -20.007186889648438, Learning Rate: 0.01\n",
      "Epoch [12431/20000], Loss: -19.999526977539062, Learning Rate: 0.01\n",
      "Epoch [12432/20000], Loss: -20.046554565429688, Learning Rate: 0.01\n",
      "Epoch [12433/20000], Loss: -20.11895751953125, Learning Rate: 0.01\n",
      "Epoch [12434/20000], Loss: -20.179168701171875, Learning Rate: 0.01\n",
      "Epoch [12435/20000], Loss: -20.202041625976562, Learning Rate: 0.01\n",
      "Epoch [12436/20000], Loss: -20.1868896484375, Learning Rate: 0.01\n",
      "Epoch [12437/20000], Loss: -20.1529541015625, Learning Rate: 0.01\n",
      "Epoch [12438/20000], Loss: -20.125320434570312, Learning Rate: 0.01\n",
      "Epoch [12439/20000], Loss: -20.120803833007812, Learning Rate: 0.01\n",
      "Epoch [12440/20000], Loss: -20.140518188476562, Learning Rate: 0.01\n",
      "Epoch [12441/20000], Loss: -20.172042846679688, Learning Rate: 0.01\n",
      "Epoch [12442/20000], Loss: -20.199111938476562, Learning Rate: 0.01\n",
      "Epoch [12443/20000], Loss: -20.210433959960938, Learning Rate: 0.01\n",
      "Epoch [12444/20000], Loss: -20.205215454101562, Learning Rate: 0.01\n",
      "Epoch [12445/20000], Loss: -20.191238403320312, Learning Rate: 0.01\n",
      "Epoch [12446/20000], Loss: -20.179458618164062, Learning Rate: 0.01\n",
      "Epoch [12447/20000], Loss: -20.177291870117188, Learning Rate: 0.01\n",
      "Epoch [12448/20000], Loss: -20.185638427734375, Learning Rate: 0.01\n",
      "Epoch [12449/20000], Loss: -20.199600219726562, Learning Rate: 0.01\n",
      "Epoch [12450/20000], Loss: -20.21234130859375, Learning Rate: 0.01\n",
      "Epoch [12451/20000], Loss: -20.21868896484375, Learning Rate: 0.01\n",
      "Epoch [12452/20000], Loss: -20.217819213867188, Learning Rate: 0.01\n",
      "Epoch [12453/20000], Loss: -20.212799072265625, Learning Rate: 0.01\n",
      "Epoch [12454/20000], Loss: -20.2078857421875, Learning Rate: 0.01\n",
      "Epoch [12455/20000], Loss: -20.206710815429688, Learning Rate: 0.01\n",
      "Epoch [12456/20000], Loss: -20.210006713867188, Learning Rate: 0.01\n",
      "Epoch [12457/20000], Loss: -20.216232299804688, Learning Rate: 0.01\n",
      "Epoch [12458/20000], Loss: -20.222579956054688, Learning Rate: 0.01\n",
      "Epoch [12459/20000], Loss: -20.22662353515625, Learning Rate: 0.01\n",
      "Epoch [12460/20000], Loss: -20.227569580078125, Learning Rate: 0.01\n",
      "Epoch [12461/20000], Loss: -20.226425170898438, Learning Rate: 0.01\n",
      "Epoch [12462/20000], Loss: -20.224655151367188, Learning Rate: 0.01\n",
      "Epoch [12463/20000], Loss: -20.224136352539062, Learning Rate: 0.01\n",
      "Epoch [12464/20000], Loss: -20.225433349609375, Learning Rate: 0.01\n",
      "Epoch [12465/20000], Loss: -20.228317260742188, Learning Rate: 0.01\n",
      "Epoch [12466/20000], Loss: -20.231658935546875, Learning Rate: 0.01\n",
      "Epoch [12467/20000], Loss: -20.234405517578125, Learning Rate: 0.01\n",
      "Epoch [12468/20000], Loss: -20.235916137695312, Learning Rate: 0.01\n",
      "Epoch [12469/20000], Loss: -20.236343383789062, Learning Rate: 0.01\n",
      "Epoch [12470/20000], Loss: -20.236312866210938, Learning Rate: 0.01\n",
      "Epoch [12471/20000], Loss: -20.236328125, Learning Rate: 0.01\n",
      "Epoch [12472/20000], Loss: -20.237014770507812, Learning Rate: 0.01\n",
      "Epoch [12473/20000], Loss: -20.238433837890625, Learning Rate: 0.01\n",
      "Epoch [12474/20000], Loss: -20.240341186523438, Learning Rate: 0.01\n",
      "Epoch [12475/20000], Loss: -20.242218017578125, Learning Rate: 0.01\n",
      "Epoch [12476/20000], Loss: -20.243682861328125, Learning Rate: 0.01\n",
      "Epoch [12477/20000], Loss: -20.24481201171875, Learning Rate: 0.01\n",
      "Epoch [12478/20000], Loss: -20.245468139648438, Learning Rate: 0.01\n",
      "Epoch [12479/20000], Loss: -20.246017456054688, Learning Rate: 0.01\n",
      "Epoch [12480/20000], Loss: -20.246673583984375, Learning Rate: 0.01\n",
      "Epoch [12481/20000], Loss: -20.247604370117188, Learning Rate: 0.01\n",
      "Epoch [12482/20000], Loss: -20.248779296875, Learning Rate: 0.01\n",
      "Epoch [12483/20000], Loss: -20.250076293945312, Learning Rate: 0.01\n",
      "Epoch [12484/20000], Loss: -20.25140380859375, Learning Rate: 0.01\n",
      "Epoch [12485/20000], Loss: -20.252578735351562, Learning Rate: 0.01\n",
      "Epoch [12486/20000], Loss: -20.2535400390625, Learning Rate: 0.01\n",
      "Epoch [12487/20000], Loss: -20.254425048828125, Learning Rate: 0.01\n",
      "Epoch [12488/20000], Loss: -20.255218505859375, Learning Rate: 0.01\n",
      "Epoch [12489/20000], Loss: -20.255966186523438, Learning Rate: 0.01\n",
      "Epoch [12490/20000], Loss: -20.25689697265625, Learning Rate: 0.01\n",
      "Epoch [12491/20000], Loss: -20.257858276367188, Learning Rate: 0.01\n",
      "Epoch [12492/20000], Loss: -20.258941650390625, Learning Rate: 0.01\n",
      "Epoch [12493/20000], Loss: -20.259963989257812, Learning Rate: 0.01\n",
      "Epoch [12494/20000], Loss: -20.260879516601562, Learning Rate: 0.01\n",
      "Epoch [12495/20000], Loss: -20.261688232421875, Learning Rate: 0.01\n",
      "Epoch [12496/20000], Loss: -20.26239013671875, Learning Rate: 0.01\n",
      "Epoch [12497/20000], Loss: -20.263076782226562, Learning Rate: 0.01\n",
      "Epoch [12498/20000], Loss: -20.263565063476562, Learning Rate: 0.01\n",
      "Epoch [12499/20000], Loss: -20.263931274414062, Learning Rate: 0.01\n",
      "Epoch [12500/20000], Loss: -20.264175415039062, Learning Rate: 0.01\n",
      "Epoch [12501/20000], Loss: -20.264114379882812, Learning Rate: 0.01\n",
      "Epoch [12502/20000], Loss: -20.263687133789062, Learning Rate: 0.01\n",
      "Epoch [12503/20000], Loss: -20.262710571289062, Learning Rate: 0.01\n",
      "Epoch [12504/20000], Loss: -20.260910034179688, Learning Rate: 0.01\n",
      "Epoch [12505/20000], Loss: -20.257919311523438, Learning Rate: 0.01\n",
      "Epoch [12506/20000], Loss: -20.253311157226562, Learning Rate: 0.01\n",
      "Epoch [12507/20000], Loss: -20.246307373046875, Learning Rate: 0.01\n",
      "Epoch [12508/20000], Loss: -20.236038208007812, Learning Rate: 0.01\n",
      "Epoch [12509/20000], Loss: -20.220870971679688, Learning Rate: 0.01\n",
      "Epoch [12510/20000], Loss: -20.1988525390625, Learning Rate: 0.01\n",
      "Epoch [12511/20000], Loss: -20.16680908203125, Learning Rate: 0.01\n",
      "Epoch [12512/20000], Loss: -20.120925903320312, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12513/20000], Loss: -20.054962158203125, Learning Rate: 0.01\n",
      "Epoch [12514/20000], Loss: -19.962738037109375, Learning Rate: 0.01\n",
      "Epoch [12515/20000], Loss: -19.834152221679688, Learning Rate: 0.01\n",
      "Epoch [12516/20000], Loss: -19.664047241210938, Learning Rate: 0.01\n",
      "Epoch [12517/20000], Loss: -19.444442749023438, Learning Rate: 0.01\n",
      "Epoch [12518/20000], Loss: -19.189956665039062, Learning Rate: 0.01\n",
      "Epoch [12519/20000], Loss: -18.920516967773438, Learning Rate: 0.01\n",
      "Epoch [12520/20000], Loss: -18.710540771484375, Learning Rate: 0.01\n",
      "Epoch [12521/20000], Loss: -18.627044677734375, Learning Rate: 0.01\n",
      "Epoch [12522/20000], Loss: -18.767333984375, Learning Rate: 0.01\n",
      "Epoch [12523/20000], Loss: -19.119766235351562, Learning Rate: 0.01\n",
      "Epoch [12524/20000], Loss: -19.603378295898438, Learning Rate: 0.01\n",
      "Epoch [12525/20000], Loss: -20.035430908203125, Learning Rate: 0.01\n",
      "Epoch [12526/20000], Loss: -20.26934814453125, Learning Rate: 0.01\n",
      "Epoch [12527/20000], Loss: -20.259735107421875, Learning Rate: 0.01\n",
      "Epoch [12528/20000], Loss: -20.078445434570312, Learning Rate: 0.01\n",
      "Epoch [12529/20000], Loss: -19.862060546875, Learning Rate: 0.01\n",
      "Epoch [12530/20000], Loss: -19.736419677734375, Learning Rate: 0.01\n",
      "Epoch [12531/20000], Loss: -19.769577026367188, Learning Rate: 0.01\n",
      "Epoch [12532/20000], Loss: -19.93231201171875, Learning Rate: 0.01\n",
      "Epoch [12533/20000], Loss: -20.134811401367188, Learning Rate: 0.01\n",
      "Epoch [12534/20000], Loss: -20.272979736328125, Learning Rate: 0.01\n",
      "Epoch [12535/20000], Loss: -20.2960205078125, Learning Rate: 0.01\n",
      "Epoch [12536/20000], Loss: -20.22406005859375, Learning Rate: 0.01\n",
      "Epoch [12537/20000], Loss: -20.124176025390625, Learning Rate: 0.01\n",
      "Epoch [12538/20000], Loss: -20.06597900390625, Learning Rate: 0.01\n",
      "Epoch [12539/20000], Loss: -20.081588745117188, Learning Rate: 0.01\n",
      "Epoch [12540/20000], Loss: -20.157302856445312, Learning Rate: 0.01\n",
      "Epoch [12541/20000], Loss: -20.245834350585938, Learning Rate: 0.01\n",
      "Epoch [12542/20000], Loss: -20.3011474609375, Learning Rate: 0.01\n",
      "Epoch [12543/20000], Loss: -20.3037109375, Learning Rate: 0.01\n",
      "Epoch [12544/20000], Loss: -20.267410278320312, Learning Rate: 0.01\n",
      "Epoch [12545/20000], Loss: -20.224639892578125, Learning Rate: 0.01\n",
      "Epoch [12546/20000], Loss: -20.204757690429688, Learning Rate: 0.01\n",
      "Epoch [12547/20000], Loss: -20.218490600585938, Learning Rate: 0.01\n",
      "Epoch [12548/20000], Loss: -20.255340576171875, Learning Rate: 0.01\n",
      "Epoch [12549/20000], Loss: -20.293716430664062, Learning Rate: 0.01\n",
      "Epoch [12550/20000], Loss: -20.314666748046875, Learning Rate: 0.01\n",
      "Epoch [12551/20000], Loss: -20.312637329101562, Learning Rate: 0.01\n",
      "Epoch [12552/20000], Loss: -20.295608520507812, Learning Rate: 0.01\n",
      "Epoch [12553/20000], Loss: -20.27813720703125, Learning Rate: 0.01\n",
      "Epoch [12554/20000], Loss: -20.271926879882812, Learning Rate: 0.01\n",
      "Epoch [12555/20000], Loss: -20.279876708984375, Learning Rate: 0.01\n",
      "Epoch [12556/20000], Loss: -20.2969970703125, Learning Rate: 0.01\n",
      "Epoch [12557/20000], Loss: -20.313873291015625, Learning Rate: 0.01\n",
      "Epoch [12558/20000], Loss: -20.323028564453125, Learning Rate: 0.01\n",
      "Epoch [12559/20000], Loss: -20.322586059570312, Learning Rate: 0.01\n",
      "Epoch [12560/20000], Loss: -20.3157958984375, Learning Rate: 0.01\n",
      "Epoch [12561/20000], Loss: -20.308731079101562, Learning Rate: 0.01\n",
      "Epoch [12562/20000], Loss: -20.30621337890625, Learning Rate: 0.01\n",
      "Epoch [12563/20000], Loss: -20.309707641601562, Learning Rate: 0.01\n",
      "Epoch [12564/20000], Loss: -20.317276000976562, Learning Rate: 0.01\n",
      "Epoch [12565/20000], Loss: -20.325302124023438, Learning Rate: 0.01\n",
      "Epoch [12566/20000], Loss: -20.33038330078125, Learning Rate: 0.01\n",
      "Epoch [12567/20000], Loss: -20.331512451171875, Learning Rate: 0.01\n",
      "Epoch [12568/20000], Loss: -20.329635620117188, Learning Rate: 0.01\n",
      "Epoch [12569/20000], Loss: -20.326950073242188, Learning Rate: 0.01\n",
      "Epoch [12570/20000], Loss: -20.325912475585938, Learning Rate: 0.01\n",
      "Epoch [12571/20000], Loss: -20.326950073242188, Learning Rate: 0.01\n",
      "Epoch [12572/20000], Loss: -20.330093383789062, Learning Rate: 0.01\n",
      "Epoch [12573/20000], Loss: -20.334136962890625, Learning Rate: 0.01\n",
      "Epoch [12574/20000], Loss: -20.337326049804688, Learning Rate: 0.01\n",
      "Epoch [12575/20000], Loss: -20.339218139648438, Learning Rate: 0.01\n",
      "Epoch [12576/20000], Loss: -20.339523315429688, Learning Rate: 0.01\n",
      "Epoch [12577/20000], Loss: -20.339035034179688, Learning Rate: 0.01\n",
      "Epoch [12578/20000], Loss: -20.338638305664062, Learning Rate: 0.01\n",
      "Epoch [12579/20000], Loss: -20.338973999023438, Learning Rate: 0.01\n",
      "Epoch [12580/20000], Loss: -20.34027099609375, Learning Rate: 0.01\n",
      "Epoch [12581/20000], Loss: -20.3421630859375, Learning Rate: 0.01\n",
      "Epoch [12582/20000], Loss: -20.344345092773438, Learning Rate: 0.01\n",
      "Epoch [12583/20000], Loss: -20.345993041992188, Learning Rate: 0.01\n",
      "Epoch [12584/20000], Loss: -20.34722900390625, Learning Rate: 0.01\n",
      "Epoch [12585/20000], Loss: -20.347946166992188, Learning Rate: 0.01\n",
      "Epoch [12586/20000], Loss: -20.348236083984375, Learning Rate: 0.01\n",
      "Epoch [12587/20000], Loss: -20.348663330078125, Learning Rate: 0.01\n",
      "Epoch [12588/20000], Loss: -20.349273681640625, Learning Rate: 0.01\n",
      "Epoch [12589/20000], Loss: -20.350265502929688, Learning Rate: 0.01\n",
      "Epoch [12590/20000], Loss: -20.35150146484375, Learning Rate: 0.01\n",
      "Epoch [12591/20000], Loss: -20.35296630859375, Learning Rate: 0.01\n",
      "Epoch [12592/20000], Loss: -20.354141235351562, Learning Rate: 0.01\n",
      "Epoch [12593/20000], Loss: -20.355209350585938, Learning Rate: 0.01\n",
      "Epoch [12594/20000], Loss: -20.3560791015625, Learning Rate: 0.01\n",
      "Epoch [12595/20000], Loss: -20.35687255859375, Learning Rate: 0.01\n",
      "Epoch [12596/20000], Loss: -20.357513427734375, Learning Rate: 0.01\n",
      "Epoch [12597/20000], Loss: -20.358245849609375, Learning Rate: 0.01\n",
      "Epoch [12598/20000], Loss: -20.359054565429688, Learning Rate: 0.01\n",
      "Epoch [12599/20000], Loss: -20.36004638671875, Learning Rate: 0.01\n",
      "Epoch [12600/20000], Loss: -20.361053466796875, Learning Rate: 0.01\n",
      "Epoch [12601/20000], Loss: -20.362091064453125, Learning Rate: 0.01\n",
      "Epoch [12602/20000], Loss: -20.362991333007812, Learning Rate: 0.01\n",
      "Epoch [12603/20000], Loss: -20.364059448242188, Learning Rate: 0.01\n",
      "Epoch [12604/20000], Loss: -20.364883422851562, Learning Rate: 0.01\n",
      "Epoch [12605/20000], Loss: -20.365707397460938, Learning Rate: 0.01\n",
      "Epoch [12606/20000], Loss: -20.36651611328125, Learning Rate: 0.01\n",
      "Epoch [12607/20000], Loss: -20.367340087890625, Learning Rate: 0.01\n",
      "Epoch [12608/20000], Loss: -20.368194580078125, Learning Rate: 0.01\n",
      "Epoch [12609/20000], Loss: -20.369033813476562, Learning Rate: 0.01\n",
      "Epoch [12610/20000], Loss: -20.3699951171875, Learning Rate: 0.01\n",
      "Epoch [12611/20000], Loss: -20.370880126953125, Learning Rate: 0.01\n",
      "Epoch [12612/20000], Loss: -20.371810913085938, Learning Rate: 0.01\n",
      "Epoch [12613/20000], Loss: -20.372756958007812, Learning Rate: 0.01\n",
      "Epoch [12614/20000], Loss: -20.373626708984375, Learning Rate: 0.01\n",
      "Epoch [12615/20000], Loss: -20.37451171875, Learning Rate: 0.01\n",
      "Epoch [12616/20000], Loss: -20.375381469726562, Learning Rate: 0.01\n",
      "Epoch [12617/20000], Loss: -20.376235961914062, Learning Rate: 0.01\n",
      "Epoch [12618/20000], Loss: -20.377029418945312, Learning Rate: 0.01\n",
      "Epoch [12619/20000], Loss: -20.377883911132812, Learning Rate: 0.01\n",
      "Epoch [12620/20000], Loss: -20.3787841796875, Learning Rate: 0.01\n",
      "Epoch [12621/20000], Loss: -20.379669189453125, Learning Rate: 0.01\n",
      "Epoch [12622/20000], Loss: -20.38055419921875, Learning Rate: 0.01\n",
      "Epoch [12623/20000], Loss: -20.381439208984375, Learning Rate: 0.01\n",
      "Epoch [12624/20000], Loss: -20.3822021484375, Learning Rate: 0.01\n",
      "Epoch [12625/20000], Loss: -20.38311767578125, Learning Rate: 0.01\n",
      "Epoch [12626/20000], Loss: -20.384048461914062, Learning Rate: 0.01\n",
      "Epoch [12627/20000], Loss: -20.384857177734375, Learning Rate: 0.01\n",
      "Epoch [12628/20000], Loss: -20.385757446289062, Learning Rate: 0.01\n",
      "Epoch [12629/20000], Loss: -20.3865966796875, Learning Rate: 0.01\n",
      "Epoch [12630/20000], Loss: -20.38739013671875, Learning Rate: 0.01\n",
      "Epoch [12631/20000], Loss: -20.388336181640625, Learning Rate: 0.01\n",
      "Epoch [12632/20000], Loss: -20.389114379882812, Learning Rate: 0.01\n",
      "Epoch [12633/20000], Loss: -20.390060424804688, Learning Rate: 0.01\n",
      "Epoch [12634/20000], Loss: -20.390838623046875, Learning Rate: 0.01\n",
      "Epoch [12635/20000], Loss: -20.391738891601562, Learning Rate: 0.01\n",
      "Epoch [12636/20000], Loss: -20.392578125, Learning Rate: 0.01\n",
      "Epoch [12637/20000], Loss: -20.3934326171875, Learning Rate: 0.01\n",
      "Epoch [12638/20000], Loss: -20.394302368164062, Learning Rate: 0.01\n",
      "Epoch [12639/20000], Loss: -20.395156860351562, Learning Rate: 0.01\n",
      "Epoch [12640/20000], Loss: -20.396026611328125, Learning Rate: 0.01\n",
      "Epoch [12641/20000], Loss: -20.396881103515625, Learning Rate: 0.01\n",
      "Epoch [12642/20000], Loss: -20.397750854492188, Learning Rate: 0.01\n",
      "Epoch [12643/20000], Loss: -20.398605346679688, Learning Rate: 0.01\n",
      "Epoch [12644/20000], Loss: -20.399398803710938, Learning Rate: 0.01\n",
      "Epoch [12645/20000], Loss: -20.4002685546875, Learning Rate: 0.01\n",
      "Epoch [12646/20000], Loss: -20.401123046875, Learning Rate: 0.01\n",
      "Epoch [12647/20000], Loss: -20.401931762695312, Learning Rate: 0.01\n",
      "Epoch [12648/20000], Loss: -20.402725219726562, Learning Rate: 0.01\n",
      "Epoch [12649/20000], Loss: -20.403610229492188, Learning Rate: 0.01\n",
      "Epoch [12650/20000], Loss: -20.404495239257812, Learning Rate: 0.01\n",
      "Epoch [12651/20000], Loss: -20.40533447265625, Learning Rate: 0.01\n",
      "Epoch [12652/20000], Loss: -20.406112670898438, Learning Rate: 0.01\n",
      "Epoch [12653/20000], Loss: -20.406951904296875, Learning Rate: 0.01\n",
      "Epoch [12654/20000], Loss: -20.407867431640625, Learning Rate: 0.01\n",
      "Epoch [12655/20000], Loss: -20.408721923828125, Learning Rate: 0.01\n",
      "Epoch [12656/20000], Loss: -20.409454345703125, Learning Rate: 0.01\n",
      "Epoch [12657/20000], Loss: -20.41033935546875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12658/20000], Loss: -20.4111328125, Learning Rate: 0.01\n",
      "Epoch [12659/20000], Loss: -20.412017822265625, Learning Rate: 0.01\n",
      "Epoch [12660/20000], Loss: -20.412933349609375, Learning Rate: 0.01\n",
      "Epoch [12661/20000], Loss: -20.41375732421875, Learning Rate: 0.01\n",
      "Epoch [12662/20000], Loss: -20.414596557617188, Learning Rate: 0.01\n",
      "Epoch [12663/20000], Loss: -20.415390014648438, Learning Rate: 0.01\n",
      "Epoch [12664/20000], Loss: -20.416213989257812, Learning Rate: 0.01\n",
      "Epoch [12665/20000], Loss: -20.417129516601562, Learning Rate: 0.01\n",
      "Epoch [12666/20000], Loss: -20.41790771484375, Learning Rate: 0.01\n",
      "Epoch [12667/20000], Loss: -20.418716430664062, Learning Rate: 0.01\n",
      "Epoch [12668/20000], Loss: -20.419570922851562, Learning Rate: 0.01\n",
      "Epoch [12669/20000], Loss: -20.42041015625, Learning Rate: 0.01\n",
      "Epoch [12670/20000], Loss: -20.421234130859375, Learning Rate: 0.01\n",
      "Epoch [12671/20000], Loss: -20.422027587890625, Learning Rate: 0.01\n",
      "Epoch [12672/20000], Loss: -20.422866821289062, Learning Rate: 0.01\n",
      "Epoch [12673/20000], Loss: -20.423721313476562, Learning Rate: 0.01\n",
      "Epoch [12674/20000], Loss: -20.424591064453125, Learning Rate: 0.01\n",
      "Epoch [12675/20000], Loss: -20.425399780273438, Learning Rate: 0.01\n",
      "Epoch [12676/20000], Loss: -20.42626953125, Learning Rate: 0.01\n",
      "Epoch [12677/20000], Loss: -20.427093505859375, Learning Rate: 0.01\n",
      "Epoch [12678/20000], Loss: -20.427871704101562, Learning Rate: 0.01\n",
      "Epoch [12679/20000], Loss: -20.428726196289062, Learning Rate: 0.01\n",
      "Epoch [12680/20000], Loss: -20.429534912109375, Learning Rate: 0.01\n",
      "Epoch [12681/20000], Loss: -20.4302978515625, Learning Rate: 0.01\n",
      "Epoch [12682/20000], Loss: -20.431167602539062, Learning Rate: 0.01\n",
      "Epoch [12683/20000], Loss: -20.432022094726562, Learning Rate: 0.01\n",
      "Epoch [12684/20000], Loss: -20.432846069335938, Learning Rate: 0.01\n",
      "Epoch [12685/20000], Loss: -20.433563232421875, Learning Rate: 0.01\n",
      "Epoch [12686/20000], Loss: -20.434432983398438, Learning Rate: 0.01\n",
      "Epoch [12687/20000], Loss: -20.435195922851562, Learning Rate: 0.01\n",
      "Epoch [12688/20000], Loss: -20.43609619140625, Learning Rate: 0.01\n",
      "Epoch [12689/20000], Loss: -20.43695068359375, Learning Rate: 0.01\n",
      "Epoch [12690/20000], Loss: -20.437713623046875, Learning Rate: 0.01\n",
      "Epoch [12691/20000], Loss: -20.438552856445312, Learning Rate: 0.01\n",
      "Epoch [12692/20000], Loss: -20.439407348632812, Learning Rate: 0.01\n",
      "Epoch [12693/20000], Loss: -20.440170288085938, Learning Rate: 0.01\n",
      "Epoch [12694/20000], Loss: -20.441024780273438, Learning Rate: 0.01\n",
      "Epoch [12695/20000], Loss: -20.4417724609375, Learning Rate: 0.01\n",
      "Epoch [12696/20000], Loss: -20.442611694335938, Learning Rate: 0.01\n",
      "Epoch [12697/20000], Loss: -20.443450927734375, Learning Rate: 0.01\n",
      "Epoch [12698/20000], Loss: -20.44427490234375, Learning Rate: 0.01\n",
      "Epoch [12699/20000], Loss: -20.445098876953125, Learning Rate: 0.01\n",
      "Epoch [12700/20000], Loss: -20.445907592773438, Learning Rate: 0.01\n",
      "Epoch [12701/20000], Loss: -20.44671630859375, Learning Rate: 0.01\n",
      "Epoch [12702/20000], Loss: -20.447555541992188, Learning Rate: 0.01\n",
      "Epoch [12703/20000], Loss: -20.448287963867188, Learning Rate: 0.01\n",
      "Epoch [12704/20000], Loss: -20.449050903320312, Learning Rate: 0.01\n",
      "Epoch [12705/20000], Loss: -20.449859619140625, Learning Rate: 0.01\n",
      "Epoch [12706/20000], Loss: -20.450546264648438, Learning Rate: 0.01\n",
      "Epoch [12707/20000], Loss: -20.4512939453125, Learning Rate: 0.01\n",
      "Epoch [12708/20000], Loss: -20.451980590820312, Learning Rate: 0.01\n",
      "Epoch [12709/20000], Loss: -20.452621459960938, Learning Rate: 0.01\n",
      "Epoch [12710/20000], Loss: -20.453018188476562, Learning Rate: 0.01\n",
      "Epoch [12711/20000], Loss: -20.45343017578125, Learning Rate: 0.01\n",
      "Epoch [12712/20000], Loss: -20.453567504882812, Learning Rate: 0.01\n",
      "Epoch [12713/20000], Loss: -20.453384399414062, Learning Rate: 0.01\n",
      "Epoch [12714/20000], Loss: -20.452545166015625, Learning Rate: 0.01\n",
      "Epoch [12715/20000], Loss: -20.4508056640625, Learning Rate: 0.01\n",
      "Epoch [12716/20000], Loss: -20.44775390625, Learning Rate: 0.01\n",
      "Epoch [12717/20000], Loss: -20.442489624023438, Learning Rate: 0.01\n",
      "Epoch [12718/20000], Loss: -20.433792114257812, Learning Rate: 0.01\n",
      "Epoch [12719/20000], Loss: -20.41961669921875, Learning Rate: 0.01\n",
      "Epoch [12720/20000], Loss: -20.396835327148438, Learning Rate: 0.01\n",
      "Epoch [12721/20000], Loss: -20.360321044921875, Learning Rate: 0.01\n",
      "Epoch [12722/20000], Loss: -20.302078247070312, Learning Rate: 0.01\n",
      "Epoch [12723/20000], Loss: -20.210494995117188, Learning Rate: 0.01\n",
      "Epoch [12724/20000], Loss: -20.06671142578125, Learning Rate: 0.01\n",
      "Epoch [12725/20000], Loss: -19.848587036132812, Learning Rate: 0.01\n",
      "Epoch [12726/20000], Loss: -19.523605346679688, Learning Rate: 0.01\n",
      "Epoch [12727/20000], Loss: -19.078323364257812, Learning Rate: 0.01\n",
      "Epoch [12728/20000], Loss: -18.511810302734375, Learning Rate: 0.01\n",
      "Epoch [12729/20000], Loss: -17.944747924804688, Learning Rate: 0.01\n",
      "Epoch [12730/20000], Loss: -17.556396484375, Learning Rate: 0.01\n",
      "Epoch [12731/20000], Loss: -17.6793212890625, Learning Rate: 0.01\n",
      "Epoch [12732/20000], Loss: -18.383468627929688, Learning Rate: 0.01\n",
      "Epoch [12733/20000], Loss: -19.444366455078125, Learning Rate: 0.01\n",
      "Epoch [12734/20000], Loss: -20.269454956054688, Learning Rate: 0.01\n",
      "Epoch [12735/20000], Loss: -20.453750610351562, Learning Rate: 0.01\n",
      "Epoch [12736/20000], Loss: -20.058685302734375, Learning Rate: 0.01\n",
      "Epoch [12737/20000], Loss: -19.517684936523438, Learning Rate: 0.01\n",
      "Epoch [12738/20000], Loss: -19.302764892578125, Learning Rate: 0.01\n",
      "Epoch [12739/20000], Loss: -19.57000732421875, Learning Rate: 0.01\n",
      "Epoch [12740/20000], Loss: -20.09442138671875, Learning Rate: 0.01\n",
      "Epoch [12741/20000], Loss: -20.44805908203125, Learning Rate: 0.01\n",
      "Epoch [12742/20000], Loss: -20.412704467773438, Learning Rate: 0.01\n",
      "Epoch [12743/20000], Loss: -20.127243041992188, Learning Rate: 0.01\n",
      "Epoch [12744/20000], Loss: -19.909347534179688, Learning Rate: 0.01\n",
      "Epoch [12745/20000], Loss: -19.966552734375, Learning Rate: 0.01\n",
      "Epoch [12746/20000], Loss: -20.225128173828125, Learning Rate: 0.01\n",
      "Epoch [12747/20000], Loss: -20.448196411132812, Learning Rate: 0.01\n",
      "Epoch [12748/20000], Loss: -20.465545654296875, Learning Rate: 0.01\n",
      "Epoch [12749/20000], Loss: -20.321182250976562, Learning Rate: 0.01\n",
      "Epoch [12750/20000], Loss: -20.193466186523438, Learning Rate: 0.01\n",
      "Epoch [12751/20000], Loss: -20.20989990234375, Learning Rate: 0.01\n",
      "Epoch [12752/20000], Loss: -20.345672607421875, Learning Rate: 0.01\n",
      "Epoch [12753/20000], Loss: -20.468124389648438, Learning Rate: 0.01\n",
      "Epoch [12754/20000], Loss: -20.481048583984375, Learning Rate: 0.01\n",
      "Epoch [12755/20000], Loss: -20.404556274414062, Learning Rate: 0.01\n",
      "Epoch [12756/20000], Loss: -20.335174560546875, Learning Rate: 0.01\n",
      "Epoch [12757/20000], Loss: -20.34442138671875, Learning Rate: 0.01\n",
      "Epoch [12758/20000], Loss: -20.417022705078125, Learning Rate: 0.01\n",
      "Epoch [12759/20000], Loss: -20.482955932617188, Learning Rate: 0.01\n",
      "Epoch [12760/20000], Loss: -20.490280151367188, Learning Rate: 0.01\n",
      "Epoch [12761/20000], Loss: -20.449752807617188, Learning Rate: 0.01\n",
      "Epoch [12762/20000], Loss: -20.41302490234375, Learning Rate: 0.01\n",
      "Epoch [12763/20000], Loss: -20.417633056640625, Learning Rate: 0.01\n",
      "Epoch [12764/20000], Loss: -20.456558227539062, Learning Rate: 0.01\n",
      "Epoch [12765/20000], Loss: -20.492477416992188, Learning Rate: 0.01\n",
      "Epoch [12766/20000], Loss: -20.497711181640625, Learning Rate: 0.01\n",
      "Epoch [12767/20000], Loss: -20.47705078125, Learning Rate: 0.01\n",
      "Epoch [12768/20000], Loss: -20.457122802734375, Learning Rate: 0.01\n",
      "Epoch [12769/20000], Loss: -20.45855712890625, Learning Rate: 0.01\n",
      "Epoch [12770/20000], Loss: -20.478805541992188, Learning Rate: 0.01\n",
      "Epoch [12771/20000], Loss: -20.49908447265625, Learning Rate: 0.01\n",
      "Epoch [12772/20000], Loss: -20.50384521484375, Learning Rate: 0.01\n",
      "Epoch [12773/20000], Loss: -20.494140625, Learning Rate: 0.01\n",
      "Epoch [12774/20000], Loss: -20.483261108398438, Learning Rate: 0.01\n",
      "Epoch [12775/20000], Loss: -20.482681274414062, Learning Rate: 0.01\n",
      "Epoch [12776/20000], Loss: -20.492828369140625, Learning Rate: 0.01\n",
      "Epoch [12777/20000], Loss: -20.504409790039062, Learning Rate: 0.01\n",
      "Epoch [12778/20000], Loss: -20.508956909179688, Learning Rate: 0.01\n",
      "Epoch [12779/20000], Loss: -20.505279541015625, Learning Rate: 0.01\n",
      "Epoch [12780/20000], Loss: -20.499420166015625, Learning Rate: 0.01\n",
      "Epoch [12781/20000], Loss: -20.498031616210938, Learning Rate: 0.01\n",
      "Epoch [12782/20000], Loss: -20.502639770507812, Learning Rate: 0.01\n",
      "Epoch [12783/20000], Loss: -20.509384155273438, Learning Rate: 0.01\n",
      "Epoch [12784/20000], Loss: -20.513381958007812, Learning Rate: 0.01\n",
      "Epoch [12785/20000], Loss: -20.512786865234375, Learning Rate: 0.01\n",
      "Epoch [12786/20000], Loss: -20.510055541992188, Learning Rate: 0.01\n",
      "Epoch [12787/20000], Loss: -20.508682250976562, Learning Rate: 0.01\n",
      "Epoch [12788/20000], Loss: -20.510589599609375, Learning Rate: 0.01\n",
      "Epoch [12789/20000], Loss: -20.514266967773438, Learning Rate: 0.01\n",
      "Epoch [12790/20000], Loss: -20.517333984375, Learning Rate: 0.01\n",
      "Epoch [12791/20000], Loss: -20.518402099609375, Learning Rate: 0.01\n",
      "Epoch [12792/20000], Loss: -20.517501831054688, Learning Rate: 0.01\n",
      "Epoch [12793/20000], Loss: -20.5167236328125, Learning Rate: 0.01\n",
      "Epoch [12794/20000], Loss: -20.517288208007812, Learning Rate: 0.01\n",
      "Epoch [12795/20000], Loss: -20.5191650390625, Learning Rate: 0.01\n",
      "Epoch [12796/20000], Loss: -20.521514892578125, Learning Rate: 0.01\n",
      "Epoch [12797/20000], Loss: -20.52288818359375, Learning Rate: 0.01\n",
      "Epoch [12798/20000], Loss: -20.523223876953125, Learning Rate: 0.01\n",
      "Epoch [12799/20000], Loss: -20.523040771484375, Learning Rate: 0.01\n",
      "Epoch [12800/20000], Loss: -20.523300170898438, Learning Rate: 0.01\n",
      "Epoch [12801/20000], Loss: -20.524200439453125, Learning Rate: 0.01\n",
      "Epoch [12802/20000], Loss: -20.525680541992188, Learning Rate: 0.01\n",
      "Epoch [12803/20000], Loss: -20.527053833007812, Learning Rate: 0.01\n",
      "Epoch [12804/20000], Loss: -20.527938842773438, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12805/20000], Loss: -20.528289794921875, Learning Rate: 0.01\n",
      "Epoch [12806/20000], Loss: -20.528656005859375, Learning Rate: 0.01\n",
      "Epoch [12807/20000], Loss: -20.529159545898438, Learning Rate: 0.01\n",
      "Epoch [12808/20000], Loss: -20.530059814453125, Learning Rate: 0.01\n",
      "Epoch [12809/20000], Loss: -20.531234741210938, Learning Rate: 0.01\n",
      "Epoch [12810/20000], Loss: -20.532196044921875, Learning Rate: 0.01\n",
      "Epoch [12811/20000], Loss: -20.532974243164062, Learning Rate: 0.01\n",
      "Epoch [12812/20000], Loss: -20.53350830078125, Learning Rate: 0.01\n",
      "Epoch [12813/20000], Loss: -20.533981323242188, Learning Rate: 0.01\n",
      "Epoch [12814/20000], Loss: -20.534713745117188, Learning Rate: 0.01\n",
      "Epoch [12815/20000], Loss: -20.5355224609375, Learning Rate: 0.01\n",
      "Epoch [12816/20000], Loss: -20.536346435546875, Learning Rate: 0.01\n",
      "Epoch [12817/20000], Loss: -20.537185668945312, Learning Rate: 0.01\n",
      "Epoch [12818/20000], Loss: -20.53790283203125, Learning Rate: 0.01\n",
      "Epoch [12819/20000], Loss: -20.53851318359375, Learning Rate: 0.01\n",
      "Epoch [12820/20000], Loss: -20.53912353515625, Learning Rate: 0.01\n",
      "Epoch [12821/20000], Loss: -20.53985595703125, Learning Rate: 0.01\n",
      "Epoch [12822/20000], Loss: -20.54058837890625, Learning Rate: 0.01\n",
      "Epoch [12823/20000], Loss: -20.541412353515625, Learning Rate: 0.01\n",
      "Epoch [12824/20000], Loss: -20.542160034179688, Learning Rate: 0.01\n",
      "Epoch [12825/20000], Loss: -20.54290771484375, Learning Rate: 0.01\n",
      "Epoch [12826/20000], Loss: -20.543563842773438, Learning Rate: 0.01\n",
      "Epoch [12827/20000], Loss: -20.544204711914062, Learning Rate: 0.01\n",
      "Epoch [12828/20000], Loss: -20.544921875, Learning Rate: 0.01\n",
      "Epoch [12829/20000], Loss: -20.545669555664062, Learning Rate: 0.01\n",
      "Epoch [12830/20000], Loss: -20.546401977539062, Learning Rate: 0.01\n",
      "Epoch [12831/20000], Loss: -20.547073364257812, Learning Rate: 0.01\n",
      "Epoch [12832/20000], Loss: -20.547821044921875, Learning Rate: 0.01\n",
      "Epoch [12833/20000], Loss: -20.548538208007812, Learning Rate: 0.01\n",
      "Epoch [12834/20000], Loss: -20.549163818359375, Learning Rate: 0.01\n",
      "Epoch [12835/20000], Loss: -20.549896240234375, Learning Rate: 0.01\n",
      "Epoch [12836/20000], Loss: -20.550537109375, Learning Rate: 0.01\n",
      "Epoch [12837/20000], Loss: -20.551300048828125, Learning Rate: 0.01\n",
      "Epoch [12838/20000], Loss: -20.552047729492188, Learning Rate: 0.01\n",
      "Epoch [12839/20000], Loss: -20.55279541015625, Learning Rate: 0.01\n",
      "Epoch [12840/20000], Loss: -20.553421020507812, Learning Rate: 0.01\n",
      "Epoch [12841/20000], Loss: -20.55413818359375, Learning Rate: 0.01\n",
      "Epoch [12842/20000], Loss: -20.554824829101562, Learning Rate: 0.01\n",
      "Epoch [12843/20000], Loss: -20.555572509765625, Learning Rate: 0.01\n",
      "Epoch [12844/20000], Loss: -20.55615234375, Learning Rate: 0.01\n",
      "Epoch [12845/20000], Loss: -20.556884765625, Learning Rate: 0.01\n",
      "Epoch [12846/20000], Loss: -20.557586669921875, Learning Rate: 0.01\n",
      "Epoch [12847/20000], Loss: -20.55828857421875, Learning Rate: 0.01\n",
      "Epoch [12848/20000], Loss: -20.558975219726562, Learning Rate: 0.01\n",
      "Epoch [12849/20000], Loss: -20.559600830078125, Learning Rate: 0.01\n",
      "Epoch [12850/20000], Loss: -20.560379028320312, Learning Rate: 0.01\n",
      "Epoch [12851/20000], Loss: -20.561050415039062, Learning Rate: 0.01\n",
      "Epoch [12852/20000], Loss: -20.561767578125, Learning Rate: 0.01\n",
      "Epoch [12853/20000], Loss: -20.562362670898438, Learning Rate: 0.01\n",
      "Epoch [12854/20000], Loss: -20.563140869140625, Learning Rate: 0.01\n",
      "Epoch [12855/20000], Loss: -20.56378173828125, Learning Rate: 0.01\n",
      "Epoch [12856/20000], Loss: -20.56451416015625, Learning Rate: 0.01\n",
      "Epoch [12857/20000], Loss: -20.565185546875, Learning Rate: 0.01\n",
      "Epoch [12858/20000], Loss: -20.56585693359375, Learning Rate: 0.01\n",
      "Epoch [12859/20000], Loss: -20.5665283203125, Learning Rate: 0.01\n",
      "Epoch [12860/20000], Loss: -20.56719970703125, Learning Rate: 0.01\n",
      "Epoch [12861/20000], Loss: -20.56787109375, Learning Rate: 0.01\n",
      "Epoch [12862/20000], Loss: -20.568572998046875, Learning Rate: 0.01\n",
      "Epoch [12863/20000], Loss: -20.569259643554688, Learning Rate: 0.01\n",
      "Epoch [12864/20000], Loss: -20.5699462890625, Learning Rate: 0.01\n",
      "Epoch [12865/20000], Loss: -20.570602416992188, Learning Rate: 0.01\n",
      "Epoch [12866/20000], Loss: -20.571319580078125, Learning Rate: 0.01\n",
      "Epoch [12867/20000], Loss: -20.572052001953125, Learning Rate: 0.01\n",
      "Epoch [12868/20000], Loss: -20.572708129882812, Learning Rate: 0.01\n",
      "Epoch [12869/20000], Loss: -20.573394775390625, Learning Rate: 0.01\n",
      "Epoch [12870/20000], Loss: -20.57403564453125, Learning Rate: 0.01\n",
      "Epoch [12871/20000], Loss: -20.574798583984375, Learning Rate: 0.01\n",
      "Epoch [12872/20000], Loss: -20.575454711914062, Learning Rate: 0.01\n",
      "Epoch [12873/20000], Loss: -20.5760498046875, Learning Rate: 0.01\n",
      "Epoch [12874/20000], Loss: -20.57672119140625, Learning Rate: 0.01\n",
      "Epoch [12875/20000], Loss: -20.577392578125, Learning Rate: 0.01\n",
      "Epoch [12876/20000], Loss: -20.57806396484375, Learning Rate: 0.01\n",
      "Epoch [12877/20000], Loss: -20.578704833984375, Learning Rate: 0.01\n",
      "Epoch [12878/20000], Loss: -20.579376220703125, Learning Rate: 0.01\n",
      "Epoch [12879/20000], Loss: -20.580047607421875, Learning Rate: 0.01\n",
      "Epoch [12880/20000], Loss: -20.580612182617188, Learning Rate: 0.01\n",
      "Epoch [12881/20000], Loss: -20.581268310546875, Learning Rate: 0.01\n",
      "Epoch [12882/20000], Loss: -20.581802368164062, Learning Rate: 0.01\n",
      "Epoch [12883/20000], Loss: -20.582351684570312, Learning Rate: 0.01\n",
      "Epoch [12884/20000], Loss: -20.582916259765625, Learning Rate: 0.01\n",
      "Epoch [12885/20000], Loss: -20.583328247070312, Learning Rate: 0.01\n",
      "Epoch [12886/20000], Loss: -20.5836181640625, Learning Rate: 0.01\n",
      "Epoch [12887/20000], Loss: -20.583908081054688, Learning Rate: 0.01\n",
      "Epoch [12888/20000], Loss: -20.5838623046875, Learning Rate: 0.01\n",
      "Epoch [12889/20000], Loss: -20.583587646484375, Learning Rate: 0.01\n",
      "Epoch [12890/20000], Loss: -20.582778930664062, Learning Rate: 0.01\n",
      "Epoch [12891/20000], Loss: -20.581527709960938, Learning Rate: 0.01\n",
      "Epoch [12892/20000], Loss: -20.579177856445312, Learning Rate: 0.01\n",
      "Epoch [12893/20000], Loss: -20.575592041015625, Learning Rate: 0.01\n",
      "Epoch [12894/20000], Loss: -20.5699462890625, Learning Rate: 0.01\n",
      "Epoch [12895/20000], Loss: -20.561447143554688, Learning Rate: 0.01\n",
      "Epoch [12896/20000], Loss: -20.548583984375, Learning Rate: 0.01\n",
      "Epoch [12897/20000], Loss: -20.529342651367188, Learning Rate: 0.01\n",
      "Epoch [12898/20000], Loss: -20.500762939453125, Learning Rate: 0.01\n",
      "Epoch [12899/20000], Loss: -20.458633422851562, Learning Rate: 0.01\n",
      "Epoch [12900/20000], Loss: -20.397415161132812, Learning Rate: 0.01\n",
      "Epoch [12901/20000], Loss: -20.308975219726562, Learning Rate: 0.01\n",
      "Epoch [12902/20000], Loss: -20.185073852539062, Learning Rate: 0.01\n",
      "Epoch [12903/20000], Loss: -20.015975952148438, Learning Rate: 0.01\n",
      "Epoch [12904/20000], Loss: -19.799835205078125, Learning Rate: 0.01\n",
      "Epoch [12905/20000], Loss: -19.542922973632812, Learning Rate: 0.01\n",
      "Epoch [12906/20000], Loss: -19.285797119140625, Learning Rate: 0.01\n",
      "Epoch [12907/20000], Loss: -19.090988159179688, Learning Rate: 0.01\n",
      "Epoch [12908/20000], Loss: -19.060943603515625, Learning Rate: 0.01\n",
      "Epoch [12909/20000], Loss: -19.255340576171875, Learning Rate: 0.01\n",
      "Epoch [12910/20000], Loss: -19.662628173828125, Learning Rate: 0.01\n",
      "Epoch [12911/20000], Loss: -20.136856079101562, Learning Rate: 0.01\n",
      "Epoch [12912/20000], Loss: -20.49151611328125, Learning Rate: 0.01\n",
      "Epoch [12913/20000], Loss: -20.603302001953125, Learning Rate: 0.01\n",
      "Epoch [12914/20000], Loss: -20.48577880859375, Learning Rate: 0.01\n",
      "Epoch [12915/20000], Loss: -20.263778686523438, Learning Rate: 0.01\n",
      "Epoch [12916/20000], Loss: -20.091278076171875, Learning Rate: 0.01\n",
      "Epoch [12917/20000], Loss: -20.074356079101562, Learning Rate: 0.01\n",
      "Epoch [12918/20000], Loss: -20.214599609375, Learning Rate: 0.01\n",
      "Epoch [12919/20000], Loss: -20.422637939453125, Learning Rate: 0.01\n",
      "Epoch [12920/20000], Loss: -20.57586669921875, Learning Rate: 0.01\n",
      "Epoch [12921/20000], Loss: -20.604705810546875, Learning Rate: 0.01\n",
      "Epoch [12922/20000], Loss: -20.526870727539062, Learning Rate: 0.01\n",
      "Epoch [12923/20000], Loss: -20.421539306640625, Learning Rate: 0.01\n",
      "Epoch [12924/20000], Loss: -20.369735717773438, Learning Rate: 0.01\n",
      "Epoch [12925/20000], Loss: -20.403305053710938, Learning Rate: 0.01\n",
      "Epoch [12926/20000], Loss: -20.494430541992188, Learning Rate: 0.01\n",
      "Epoch [12927/20000], Loss: -20.5810546875, Learning Rate: 0.01\n",
      "Epoch [12928/20000], Loss: -20.614791870117188, Learning Rate: 0.01\n",
      "Epoch [12929/20000], Loss: -20.589950561523438, Learning Rate: 0.01\n",
      "Epoch [12930/20000], Loss: -20.539352416992188, Learning Rate: 0.01\n",
      "Epoch [12931/20000], Loss: -20.506118774414062, Learning Rate: 0.01\n",
      "Epoch [12932/20000], Loss: -20.513107299804688, Learning Rate: 0.01\n",
      "Epoch [12933/20000], Loss: -20.552764892578125, Learning Rate: 0.01\n",
      "Epoch [12934/20000], Loss: -20.5966796875, Learning Rate: 0.01\n",
      "Epoch [12935/20000], Loss: -20.618820190429688, Learning Rate: 0.01\n",
      "Epoch [12936/20000], Loss: -20.612014770507812, Learning Rate: 0.01\n",
      "Epoch [12937/20000], Loss: -20.589080810546875, Learning Rate: 0.01\n",
      "Epoch [12938/20000], Loss: -20.570785522460938, Learning Rate: 0.01\n",
      "Epoch [12939/20000], Loss: -20.570449829101562, Learning Rate: 0.01\n",
      "Epoch [12940/20000], Loss: -20.587448120117188, Learning Rate: 0.01\n",
      "Epoch [12941/20000], Loss: -20.609283447265625, Learning Rate: 0.01\n",
      "Epoch [12942/20000], Loss: -20.622711181640625, Learning Rate: 0.01\n",
      "Epoch [12943/20000], Loss: -20.622482299804688, Learning Rate: 0.01\n",
      "Epoch [12944/20000], Loss: -20.612960815429688, Learning Rate: 0.01\n",
      "Epoch [12945/20000], Loss: -20.603378295898438, Learning Rate: 0.01\n",
      "Epoch [12946/20000], Loss: -20.601318359375, Learning Rate: 0.01\n",
      "Epoch [12947/20000], Loss: -20.607894897460938, Learning Rate: 0.01\n",
      "Epoch [12948/20000], Loss: -20.61846923828125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12949/20000], Loss: -20.626678466796875, Learning Rate: 0.01\n",
      "Epoch [12950/20000], Loss: -20.628829956054688, Learning Rate: 0.01\n",
      "Epoch [12951/20000], Loss: -20.625656127929688, Learning Rate: 0.01\n",
      "Epoch [12952/20000], Loss: -20.621047973632812, Learning Rate: 0.01\n",
      "Epoch [12953/20000], Loss: -20.619003295898438, Learning Rate: 0.01\n",
      "Epoch [12954/20000], Loss: -20.621063232421875, Learning Rate: 0.01\n",
      "Epoch [12955/20000], Loss: -20.625900268554688, Learning Rate: 0.01\n",
      "Epoch [12956/20000], Loss: -20.630828857421875, Learning Rate: 0.01\n",
      "Epoch [12957/20000], Loss: -20.633346557617188, Learning Rate: 0.01\n",
      "Epoch [12958/20000], Loss: -20.633132934570312, Learning Rate: 0.01\n",
      "Epoch [12959/20000], Loss: -20.631317138671875, Learning Rate: 0.01\n",
      "Epoch [12960/20000], Loss: -20.630020141601562, Learning Rate: 0.01\n",
      "Epoch [12961/20000], Loss: -20.63043212890625, Learning Rate: 0.01\n",
      "Epoch [12962/20000], Loss: -20.632400512695312, Learning Rate: 0.01\n",
      "Epoch [12963/20000], Loss: -20.635147094726562, Learning Rate: 0.01\n",
      "Epoch [12964/20000], Loss: -20.63726806640625, Learning Rate: 0.01\n",
      "Epoch [12965/20000], Loss: -20.638198852539062, Learning Rate: 0.01\n",
      "Epoch [12966/20000], Loss: -20.638076782226562, Learning Rate: 0.01\n",
      "Epoch [12967/20000], Loss: -20.6375732421875, Learning Rate: 0.01\n",
      "Epoch [12968/20000], Loss: -20.637603759765625, Learning Rate: 0.01\n",
      "Epoch [12969/20000], Loss: -20.638320922851562, Learning Rate: 0.01\n",
      "Epoch [12970/20000], Loss: -20.639785766601562, Learning Rate: 0.01\n",
      "Epoch [12971/20000], Loss: -20.641220092773438, Learning Rate: 0.01\n",
      "Epoch [12972/20000], Loss: -20.6424560546875, Learning Rate: 0.01\n",
      "Epoch [12973/20000], Loss: -20.642990112304688, Learning Rate: 0.01\n",
      "Epoch [12974/20000], Loss: -20.643295288085938, Learning Rate: 0.01\n",
      "Epoch [12975/20000], Loss: -20.643417358398438, Learning Rate: 0.01\n",
      "Epoch [12976/20000], Loss: -20.643722534179688, Learning Rate: 0.01\n",
      "Epoch [12977/20000], Loss: -20.644454956054688, Learning Rate: 0.01\n",
      "Epoch [12978/20000], Loss: -20.645416259765625, Learning Rate: 0.01\n",
      "Epoch [12979/20000], Loss: -20.64642333984375, Learning Rate: 0.01\n",
      "Epoch [12980/20000], Loss: -20.647232055664062, Learning Rate: 0.01\n",
      "Epoch [12981/20000], Loss: -20.647842407226562, Learning Rate: 0.01\n",
      "Epoch [12982/20000], Loss: -20.648330688476562, Learning Rate: 0.01\n",
      "Epoch [12983/20000], Loss: -20.648712158203125, Learning Rate: 0.01\n",
      "Epoch [12984/20000], Loss: -20.649169921875, Learning Rate: 0.01\n",
      "Epoch [12985/20000], Loss: -20.649810791015625, Learning Rate: 0.01\n",
      "Epoch [12986/20000], Loss: -20.6505126953125, Learning Rate: 0.01\n",
      "Epoch [12987/20000], Loss: -20.651275634765625, Learning Rate: 0.01\n",
      "Epoch [12988/20000], Loss: -20.652008056640625, Learning Rate: 0.01\n",
      "Epoch [12989/20000], Loss: -20.652618408203125, Learning Rate: 0.01\n",
      "Epoch [12990/20000], Loss: -20.6531982421875, Learning Rate: 0.01\n",
      "Epoch [12991/20000], Loss: -20.653701782226562, Learning Rate: 0.01\n",
      "Epoch [12992/20000], Loss: -20.654144287109375, Learning Rate: 0.01\n",
      "Epoch [12993/20000], Loss: -20.65484619140625, Learning Rate: 0.01\n",
      "Epoch [12994/20000], Loss: -20.655502319335938, Learning Rate: 0.01\n",
      "Epoch [12995/20000], Loss: -20.656112670898438, Learning Rate: 0.01\n",
      "Epoch [12996/20000], Loss: -20.656707763671875, Learning Rate: 0.01\n",
      "Epoch [12997/20000], Loss: -20.657333374023438, Learning Rate: 0.01\n",
      "Epoch [12998/20000], Loss: -20.657928466796875, Learning Rate: 0.01\n",
      "Epoch [12999/20000], Loss: -20.658477783203125, Learning Rate: 0.01\n",
      "Epoch [13000/20000], Loss: -20.659072875976562, Learning Rate: 0.01\n",
      "Epoch [13001/20000], Loss: -20.659622192382812, Learning Rate: 0.01\n",
      "Epoch [13002/20000], Loss: -20.660186767578125, Learning Rate: 0.01\n",
      "Epoch [13003/20000], Loss: -20.660873413085938, Learning Rate: 0.01\n",
      "Epoch [13004/20000], Loss: -20.66143798828125, Learning Rate: 0.01\n",
      "Epoch [13005/20000], Loss: -20.662033081054688, Learning Rate: 0.01\n",
      "Epoch [13006/20000], Loss: -20.66265869140625, Learning Rate: 0.01\n",
      "Epoch [13007/20000], Loss: -20.663223266601562, Learning Rate: 0.01\n",
      "Epoch [13008/20000], Loss: -20.663818359375, Learning Rate: 0.01\n",
      "Epoch [13009/20000], Loss: -20.664306640625, Learning Rate: 0.01\n",
      "Epoch [13010/20000], Loss: -20.664932250976562, Learning Rate: 0.01\n",
      "Epoch [13011/20000], Loss: -20.665512084960938, Learning Rate: 0.01\n",
      "Epoch [13012/20000], Loss: -20.666107177734375, Learning Rate: 0.01\n",
      "Epoch [13013/20000], Loss: -20.666656494140625, Learning Rate: 0.01\n",
      "Epoch [13014/20000], Loss: -20.667251586914062, Learning Rate: 0.01\n",
      "Epoch [13015/20000], Loss: -20.667877197265625, Learning Rate: 0.01\n",
      "Epoch [13016/20000], Loss: -20.66845703125, Learning Rate: 0.01\n",
      "Epoch [13017/20000], Loss: -20.66900634765625, Learning Rate: 0.01\n",
      "Epoch [13018/20000], Loss: -20.669570922851562, Learning Rate: 0.01\n",
      "Epoch [13019/20000], Loss: -20.670120239257812, Learning Rate: 0.01\n",
      "Epoch [13020/20000], Loss: -20.67071533203125, Learning Rate: 0.01\n",
      "Epoch [13021/20000], Loss: -20.671356201171875, Learning Rate: 0.01\n",
      "Epoch [13022/20000], Loss: -20.671920776367188, Learning Rate: 0.01\n",
      "Epoch [13023/20000], Loss: -20.6724853515625, Learning Rate: 0.01\n",
      "Epoch [13024/20000], Loss: -20.673049926757812, Learning Rate: 0.01\n",
      "Epoch [13025/20000], Loss: -20.673583984375, Learning Rate: 0.01\n",
      "Epoch [13026/20000], Loss: -20.6741943359375, Learning Rate: 0.01\n",
      "Epoch [13027/20000], Loss: -20.674713134765625, Learning Rate: 0.01\n",
      "Epoch [13028/20000], Loss: -20.675216674804688, Learning Rate: 0.01\n",
      "Epoch [13029/20000], Loss: -20.675811767578125, Learning Rate: 0.01\n",
      "Epoch [13030/20000], Loss: -20.67633056640625, Learning Rate: 0.01\n",
      "Epoch [13031/20000], Loss: -20.676834106445312, Learning Rate: 0.01\n",
      "Epoch [13032/20000], Loss: -20.677291870117188, Learning Rate: 0.01\n",
      "Epoch [13033/20000], Loss: -20.67767333984375, Learning Rate: 0.01\n",
      "Epoch [13034/20000], Loss: -20.678024291992188, Learning Rate: 0.01\n",
      "Epoch [13035/20000], Loss: -20.678237915039062, Learning Rate: 0.01\n",
      "Epoch [13036/20000], Loss: -20.678314208984375, Learning Rate: 0.01\n",
      "Epoch [13037/20000], Loss: -20.67822265625, Learning Rate: 0.01\n",
      "Epoch [13038/20000], Loss: -20.677642822265625, Learning Rate: 0.01\n",
      "Epoch [13039/20000], Loss: -20.676513671875, Learning Rate: 0.01\n",
      "Epoch [13040/20000], Loss: -20.67449951171875, Learning Rate: 0.01\n",
      "Epoch [13041/20000], Loss: -20.670989990234375, Learning Rate: 0.01\n",
      "Epoch [13042/20000], Loss: -20.665328979492188, Learning Rate: 0.01\n",
      "Epoch [13043/20000], Loss: -20.656051635742188, Learning Rate: 0.01\n",
      "Epoch [13044/20000], Loss: -20.641387939453125, Learning Rate: 0.01\n",
      "Epoch [13045/20000], Loss: -20.617874145507812, Learning Rate: 0.01\n",
      "Epoch [13046/20000], Loss: -20.580718994140625, Learning Rate: 0.01\n",
      "Epoch [13047/20000], Loss: -20.521804809570312, Learning Rate: 0.01\n",
      "Epoch [13048/20000], Loss: -20.430023193359375, Learning Rate: 0.01\n",
      "Epoch [13049/20000], Loss: -20.286773681640625, Learning Rate: 0.01\n",
      "Epoch [13050/20000], Loss: -20.0714111328125, Learning Rate: 0.01\n",
      "Epoch [13051/20000], Loss: -19.752761840820312, Learning Rate: 0.01\n",
      "Epoch [13052/20000], Loss: -19.3221435546875, Learning Rate: 0.01\n",
      "Epoch [13053/20000], Loss: -18.78070068359375, Learning Rate: 0.01\n",
      "Epoch [13054/20000], Loss: -18.255752563476562, Learning Rate: 0.01\n",
      "Epoch [13055/20000], Loss: -17.9149169921875, Learning Rate: 0.01\n",
      "Epoch [13056/20000], Loss: -18.075302124023438, Learning Rate: 0.01\n",
      "Epoch [13057/20000], Loss: -18.771194458007812, Learning Rate: 0.01\n",
      "Epoch [13058/20000], Loss: -19.774322509765625, Learning Rate: 0.01\n",
      "Epoch [13059/20000], Loss: -20.525436401367188, Learning Rate: 0.01\n",
      "Epoch [13060/20000], Loss: -20.668624877929688, Learning Rate: 0.01\n",
      "Epoch [13061/20000], Loss: -20.282867431640625, Learning Rate: 0.01\n",
      "Epoch [13062/20000], Loss: -19.777511596679688, Learning Rate: 0.01\n",
      "Epoch [13063/20000], Loss: -19.586654663085938, Learning Rate: 0.01\n",
      "Epoch [13064/20000], Loss: -19.843109130859375, Learning Rate: 0.01\n",
      "Epoch [13065/20000], Loss: -20.335235595703125, Learning Rate: 0.01\n",
      "Epoch [13066/20000], Loss: -20.666595458984375, Learning Rate: 0.01\n",
      "Epoch [13067/20000], Loss: -20.636550903320312, Learning Rate: 0.01\n",
      "Epoch [13068/20000], Loss: -20.371261596679688, Learning Rate: 0.01\n",
      "Epoch [13069/20000], Loss: -20.162506103515625, Learning Rate: 0.01\n",
      "Epoch [13070/20000], Loss: -20.206802368164062, Learning Rate: 0.01\n",
      "Epoch [13071/20000], Loss: -20.443862915039062, Learning Rate: 0.01\n",
      "Epoch [13072/20000], Loss: -20.658905029296875, Learning Rate: 0.01\n",
      "Epoch [13073/20000], Loss: -20.68804931640625, Learning Rate: 0.01\n",
      "Epoch [13074/20000], Loss: -20.55963134765625, Learning Rate: 0.01\n",
      "Epoch [13075/20000], Loss: -20.433349609375, Learning Rate: 0.01\n",
      "Epoch [13076/20000], Loss: -20.434295654296875, Learning Rate: 0.01\n",
      "Epoch [13077/20000], Loss: -20.553604125976562, Learning Rate: 0.01\n",
      "Epoch [13078/20000], Loss: -20.674713134765625, Learning Rate: 0.01\n",
      "Epoch [13079/20000], Loss: -20.7010498046875, Learning Rate: 0.01\n",
      "Epoch [13080/20000], Loss: -20.63720703125, Learning Rate: 0.01\n",
      "Epoch [13081/20000], Loss: -20.566177368164062, Learning Rate: 0.01\n",
      "Epoch [13082/20000], Loss: -20.561325073242188, Learning Rate: 0.01\n",
      "Epoch [13083/20000], Loss: -20.621566772460938, Learning Rate: 0.01\n",
      "Epoch [13084/20000], Loss: -20.688552856445312, Learning Rate: 0.01\n",
      "Epoch [13085/20000], Loss: -20.7073974609375, Learning Rate: 0.01\n",
      "Epoch [13086/20000], Loss: -20.676223754882812, Learning Rate: 0.01\n",
      "Epoch [13087/20000], Loss: -20.637313842773438, Learning Rate: 0.01\n",
      "Epoch [13088/20000], Loss: -20.630844116210938, Learning Rate: 0.01\n",
      "Epoch [13089/20000], Loss: -20.660797119140625, Learning Rate: 0.01\n",
      "Epoch [13090/20000], Loss: -20.697784423828125, Learning Rate: 0.01\n",
      "Epoch [13091/20000], Loss: -20.711669921875, Learning Rate: 0.01\n",
      "Epoch [13092/20000], Loss: -20.697967529296875, Learning Rate: 0.01\n",
      "Epoch [13093/20000], Loss: -20.67669677734375, Learning Rate: 0.01\n",
      "Epoch [13094/20000], Loss: -20.670333862304688, Learning Rate: 0.01\n",
      "Epoch [13095/20000], Loss: -20.683929443359375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13096/20000], Loss: -20.704254150390625, Learning Rate: 0.01\n",
      "Epoch [13097/20000], Loss: -20.714797973632812, Learning Rate: 0.01\n",
      "Epoch [13098/20000], Loss: -20.710342407226562, Learning Rate: 0.01\n",
      "Epoch [13099/20000], Loss: -20.699264526367188, Learning Rate: 0.01\n",
      "Epoch [13100/20000], Loss: -20.693634033203125, Learning Rate: 0.01\n",
      "Epoch [13101/20000], Loss: -20.698715209960938, Learning Rate: 0.01\n",
      "Epoch [13102/20000], Loss: -20.709457397460938, Learning Rate: 0.01\n",
      "Epoch [13103/20000], Loss: -20.717300415039062, Learning Rate: 0.01\n",
      "Epoch [13104/20000], Loss: -20.717391967773438, Learning Rate: 0.01\n",
      "Epoch [13105/20000], Loss: -20.7122802734375, Learning Rate: 0.01\n",
      "Epoch [13106/20000], Loss: -20.708114624023438, Learning Rate: 0.01\n",
      "Epoch [13107/20000], Loss: -20.709136962890625, Learning Rate: 0.01\n",
      "Epoch [13108/20000], Loss: -20.714324951171875, Learning Rate: 0.01\n",
      "Epoch [13109/20000], Loss: -20.7196044921875, Learning Rate: 0.01\n",
      "Epoch [13110/20000], Loss: -20.721588134765625, Learning Rate: 0.01\n",
      "Epoch [13111/20000], Loss: -20.720062255859375, Learning Rate: 0.01\n",
      "Epoch [13112/20000], Loss: -20.717681884765625, Learning Rate: 0.01\n",
      "Epoch [13113/20000], Loss: -20.717010498046875, Learning Rate: 0.01\n",
      "Epoch [13114/20000], Loss: -20.71905517578125, Learning Rate: 0.01\n",
      "Epoch [13115/20000], Loss: -20.72222900390625, Learning Rate: 0.01\n",
      "Epoch [13116/20000], Loss: -20.724533081054688, Learning Rate: 0.01\n",
      "Epoch [13117/20000], Loss: -20.72491455078125, Learning Rate: 0.01\n",
      "Epoch [13118/20000], Loss: -20.72406005859375, Learning Rate: 0.01\n",
      "Epoch [13119/20000], Loss: -20.723251342773438, Learning Rate: 0.01\n",
      "Epoch [13120/20000], Loss: -20.723739624023438, Learning Rate: 0.01\n",
      "Epoch [13121/20000], Loss: -20.725296020507812, Learning Rate: 0.01\n",
      "Epoch [13122/20000], Loss: -20.727142333984375, Learning Rate: 0.01\n",
      "Epoch [13123/20000], Loss: -20.728286743164062, Learning Rate: 0.01\n",
      "Epoch [13124/20000], Loss: -20.728469848632812, Learning Rate: 0.01\n",
      "Epoch [13125/20000], Loss: -20.728256225585938, Learning Rate: 0.01\n",
      "Epoch [13126/20000], Loss: -20.728286743164062, Learning Rate: 0.01\n",
      "Epoch [13127/20000], Loss: -20.728912353515625, Learning Rate: 0.01\n",
      "Epoch [13128/20000], Loss: -20.729949951171875, Learning Rate: 0.01\n",
      "Epoch [13129/20000], Loss: -20.731094360351562, Learning Rate: 0.01\n",
      "Epoch [13130/20000], Loss: -20.731857299804688, Learning Rate: 0.01\n",
      "Epoch [13131/20000], Loss: -20.732162475585938, Learning Rate: 0.01\n",
      "Epoch [13132/20000], Loss: -20.732315063476562, Learning Rate: 0.01\n",
      "Epoch [13133/20000], Loss: -20.732620239257812, Learning Rate: 0.01\n",
      "Epoch [13134/20000], Loss: -20.733108520507812, Learning Rate: 0.01\n",
      "Epoch [13135/20000], Loss: -20.733917236328125, Learning Rate: 0.01\n",
      "Epoch [13136/20000], Loss: -20.73468017578125, Learning Rate: 0.01\n",
      "Epoch [13137/20000], Loss: -20.735336303710938, Learning Rate: 0.01\n",
      "Epoch [13138/20000], Loss: -20.7357177734375, Learning Rate: 0.01\n",
      "Epoch [13139/20000], Loss: -20.736160278320312, Learning Rate: 0.01\n",
      "Epoch [13140/20000], Loss: -20.736495971679688, Learning Rate: 0.01\n",
      "Epoch [13141/20000], Loss: -20.736968994140625, Learning Rate: 0.01\n",
      "Epoch [13142/20000], Loss: -20.737579345703125, Learning Rate: 0.01\n",
      "Epoch [13143/20000], Loss: -20.738204956054688, Learning Rate: 0.01\n",
      "Epoch [13144/20000], Loss: -20.73883056640625, Learning Rate: 0.01\n",
      "Epoch [13145/20000], Loss: -20.739303588867188, Learning Rate: 0.01\n",
      "Epoch [13146/20000], Loss: -20.739791870117188, Learning Rate: 0.01\n",
      "Epoch [13147/20000], Loss: -20.740158081054688, Learning Rate: 0.01\n",
      "Epoch [13148/20000], Loss: -20.740615844726562, Learning Rate: 0.01\n",
      "Epoch [13149/20000], Loss: -20.741195678710938, Learning Rate: 0.01\n",
      "Epoch [13150/20000], Loss: -20.741790771484375, Learning Rate: 0.01\n",
      "Epoch [13151/20000], Loss: -20.742294311523438, Learning Rate: 0.01\n",
      "Epoch [13152/20000], Loss: -20.742782592773438, Learning Rate: 0.01\n",
      "Epoch [13153/20000], Loss: -20.743301391601562, Learning Rate: 0.01\n",
      "Epoch [13154/20000], Loss: -20.74371337890625, Learning Rate: 0.01\n",
      "Epoch [13155/20000], Loss: -20.744216918945312, Learning Rate: 0.01\n",
      "Epoch [13156/20000], Loss: -20.744705200195312, Learning Rate: 0.01\n",
      "Epoch [13157/20000], Loss: -20.745162963867188, Learning Rate: 0.01\n",
      "Epoch [13158/20000], Loss: -20.745681762695312, Learning Rate: 0.01\n",
      "Epoch [13159/20000], Loss: -20.746261596679688, Learning Rate: 0.01\n",
      "Epoch [13160/20000], Loss: -20.746780395507812, Learning Rate: 0.01\n",
      "Epoch [13161/20000], Loss: -20.747299194335938, Learning Rate: 0.01\n",
      "Epoch [13162/20000], Loss: -20.7476806640625, Learning Rate: 0.01\n",
      "Epoch [13163/20000], Loss: -20.748138427734375, Learning Rate: 0.01\n",
      "Epoch [13164/20000], Loss: -20.748687744140625, Learning Rate: 0.01\n",
      "Epoch [13165/20000], Loss: -20.749176025390625, Learning Rate: 0.01\n",
      "Epoch [13166/20000], Loss: -20.749664306640625, Learning Rate: 0.01\n",
      "Epoch [13167/20000], Loss: -20.75018310546875, Learning Rate: 0.01\n",
      "Epoch [13168/20000], Loss: -20.7506103515625, Learning Rate: 0.01\n",
      "Epoch [13169/20000], Loss: -20.751129150390625, Learning Rate: 0.01\n",
      "Epoch [13170/20000], Loss: -20.75164794921875, Learning Rate: 0.01\n",
      "Epoch [13171/20000], Loss: -20.75213623046875, Learning Rate: 0.01\n",
      "Epoch [13172/20000], Loss: -20.7525634765625, Learning Rate: 0.01\n",
      "Epoch [13173/20000], Loss: -20.753097534179688, Learning Rate: 0.01\n",
      "Epoch [13174/20000], Loss: -20.753570556640625, Learning Rate: 0.01\n",
      "Epoch [13175/20000], Loss: -20.7540283203125, Learning Rate: 0.01\n",
      "Epoch [13176/20000], Loss: -20.754592895507812, Learning Rate: 0.01\n",
      "Epoch [13177/20000], Loss: -20.7550048828125, Learning Rate: 0.01\n",
      "Epoch [13178/20000], Loss: -20.755569458007812, Learning Rate: 0.01\n",
      "Epoch [13179/20000], Loss: -20.755996704101562, Learning Rate: 0.01\n",
      "Epoch [13180/20000], Loss: -20.7564697265625, Learning Rate: 0.01\n",
      "Epoch [13181/20000], Loss: -20.756973266601562, Learning Rate: 0.01\n",
      "Epoch [13182/20000], Loss: -20.757431030273438, Learning Rate: 0.01\n",
      "Epoch [13183/20000], Loss: -20.757919311523438, Learning Rate: 0.01\n",
      "Epoch [13184/20000], Loss: -20.75836181640625, Learning Rate: 0.01\n",
      "Epoch [13185/20000], Loss: -20.7589111328125, Learning Rate: 0.01\n",
      "Epoch [13186/20000], Loss: -20.759323120117188, Learning Rate: 0.01\n",
      "Epoch [13187/20000], Loss: -20.759872436523438, Learning Rate: 0.01\n",
      "Epoch [13188/20000], Loss: -20.760284423828125, Learning Rate: 0.01\n",
      "Epoch [13189/20000], Loss: -20.760772705078125, Learning Rate: 0.01\n",
      "Epoch [13190/20000], Loss: -20.761260986328125, Learning Rate: 0.01\n",
      "Epoch [13191/20000], Loss: -20.76171875, Learning Rate: 0.01\n",
      "Epoch [13192/20000], Loss: -20.762191772460938, Learning Rate: 0.01\n",
      "Epoch [13193/20000], Loss: -20.762741088867188, Learning Rate: 0.01\n",
      "Epoch [13194/20000], Loss: -20.763214111328125, Learning Rate: 0.01\n",
      "Epoch [13195/20000], Loss: -20.763687133789062, Learning Rate: 0.01\n",
      "Epoch [13196/20000], Loss: -20.76409912109375, Learning Rate: 0.01\n",
      "Epoch [13197/20000], Loss: -20.764617919921875, Learning Rate: 0.01\n",
      "Epoch [13198/20000], Loss: -20.765060424804688, Learning Rate: 0.01\n",
      "Epoch [13199/20000], Loss: -20.765625, Learning Rate: 0.01\n",
      "Epoch [13200/20000], Loss: -20.765975952148438, Learning Rate: 0.01\n",
      "Epoch [13201/20000], Loss: -20.766555786132812, Learning Rate: 0.01\n",
      "Epoch [13202/20000], Loss: -20.766952514648438, Learning Rate: 0.01\n",
      "Epoch [13203/20000], Loss: -20.7674560546875, Learning Rate: 0.01\n",
      "Epoch [13204/20000], Loss: -20.767929077148438, Learning Rate: 0.01\n",
      "Epoch [13205/20000], Loss: -20.7684326171875, Learning Rate: 0.01\n",
      "Epoch [13206/20000], Loss: -20.768844604492188, Learning Rate: 0.01\n",
      "Epoch [13207/20000], Loss: -20.769256591796875, Learning Rate: 0.01\n",
      "Epoch [13208/20000], Loss: -20.769805908203125, Learning Rate: 0.01\n",
      "Epoch [13209/20000], Loss: -20.770248413085938, Learning Rate: 0.01\n",
      "Epoch [13210/20000], Loss: -20.770767211914062, Learning Rate: 0.01\n",
      "Epoch [13211/20000], Loss: -20.771148681640625, Learning Rate: 0.01\n",
      "Epoch [13212/20000], Loss: -20.77166748046875, Learning Rate: 0.01\n",
      "Epoch [13213/20000], Loss: -20.772125244140625, Learning Rate: 0.01\n",
      "Epoch [13214/20000], Loss: -20.772598266601562, Learning Rate: 0.01\n",
      "Epoch [13215/20000], Loss: -20.7730712890625, Learning Rate: 0.01\n",
      "Epoch [13216/20000], Loss: -20.773574829101562, Learning Rate: 0.01\n",
      "Epoch [13217/20000], Loss: -20.77398681640625, Learning Rate: 0.01\n",
      "Epoch [13218/20000], Loss: -20.774459838867188, Learning Rate: 0.01\n",
      "Epoch [13219/20000], Loss: -20.77496337890625, Learning Rate: 0.01\n",
      "Epoch [13220/20000], Loss: -20.775360107421875, Learning Rate: 0.01\n",
      "Epoch [13221/20000], Loss: -20.775863647460938, Learning Rate: 0.01\n",
      "Epoch [13222/20000], Loss: -20.776351928710938, Learning Rate: 0.01\n",
      "Epoch [13223/20000], Loss: -20.776809692382812, Learning Rate: 0.01\n",
      "Epoch [13224/20000], Loss: -20.777236938476562, Learning Rate: 0.01\n",
      "Epoch [13225/20000], Loss: -20.777725219726562, Learning Rate: 0.01\n",
      "Epoch [13226/20000], Loss: -20.7781982421875, Learning Rate: 0.01\n",
      "Epoch [13227/20000], Loss: -20.77862548828125, Learning Rate: 0.01\n",
      "Epoch [13228/20000], Loss: -20.779129028320312, Learning Rate: 0.01\n",
      "Epoch [13229/20000], Loss: -20.77960205078125, Learning Rate: 0.01\n",
      "Epoch [13230/20000], Loss: -20.780044555664062, Learning Rate: 0.01\n",
      "Epoch [13231/20000], Loss: -20.780502319335938, Learning Rate: 0.01\n",
      "Epoch [13232/20000], Loss: -20.780960083007812, Learning Rate: 0.01\n",
      "Epoch [13233/20000], Loss: -20.781402587890625, Learning Rate: 0.01\n",
      "Epoch [13234/20000], Loss: -20.7818603515625, Learning Rate: 0.01\n",
      "Epoch [13235/20000], Loss: -20.78228759765625, Learning Rate: 0.01\n",
      "Epoch [13236/20000], Loss: -20.782806396484375, Learning Rate: 0.01\n",
      "Epoch [13237/20000], Loss: -20.783248901367188, Learning Rate: 0.01\n",
      "Epoch [13238/20000], Loss: -20.78369140625, Learning Rate: 0.01\n",
      "Epoch [13239/20000], Loss: -20.784210205078125, Learning Rate: 0.01\n",
      "Epoch [13240/20000], Loss: -20.784652709960938, Learning Rate: 0.01\n",
      "Epoch [13241/20000], Loss: -20.78509521484375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13242/20000], Loss: -20.785552978515625, Learning Rate: 0.01\n",
      "Epoch [13243/20000], Loss: -20.785980224609375, Learning Rate: 0.01\n",
      "Epoch [13244/20000], Loss: -20.786514282226562, Learning Rate: 0.01\n",
      "Epoch [13245/20000], Loss: -20.786956787109375, Learning Rate: 0.01\n",
      "Epoch [13246/20000], Loss: -20.787384033203125, Learning Rate: 0.01\n",
      "Epoch [13247/20000], Loss: -20.787857055664062, Learning Rate: 0.01\n",
      "Epoch [13248/20000], Loss: -20.788253784179688, Learning Rate: 0.01\n",
      "Epoch [13249/20000], Loss: -20.788742065429688, Learning Rate: 0.01\n",
      "Epoch [13250/20000], Loss: -20.789230346679688, Learning Rate: 0.01\n",
      "Epoch [13251/20000], Loss: -20.78961181640625, Learning Rate: 0.01\n",
      "Epoch [13252/20000], Loss: -20.790069580078125, Learning Rate: 0.01\n",
      "Epoch [13253/20000], Loss: -20.79052734375, Learning Rate: 0.01\n",
      "Epoch [13254/20000], Loss: -20.790939331054688, Learning Rate: 0.01\n",
      "Epoch [13255/20000], Loss: -20.791427612304688, Learning Rate: 0.01\n",
      "Epoch [13256/20000], Loss: -20.791900634765625, Learning Rate: 0.01\n",
      "Epoch [13257/20000], Loss: -20.792388916015625, Learning Rate: 0.01\n",
      "Epoch [13258/20000], Loss: -20.792831420898438, Learning Rate: 0.01\n",
      "Epoch [13259/20000], Loss: -20.793304443359375, Learning Rate: 0.01\n",
      "Epoch [13260/20000], Loss: -20.793701171875, Learning Rate: 0.01\n",
      "Epoch [13261/20000], Loss: -20.794097900390625, Learning Rate: 0.01\n",
      "Epoch [13262/20000], Loss: -20.794540405273438, Learning Rate: 0.01\n",
      "Epoch [13263/20000], Loss: -20.7950439453125, Learning Rate: 0.01\n",
      "Epoch [13264/20000], Loss: -20.795455932617188, Learning Rate: 0.01\n",
      "Epoch [13265/20000], Loss: -20.7958984375, Learning Rate: 0.01\n",
      "Epoch [13266/20000], Loss: -20.79638671875, Learning Rate: 0.01\n",
      "Epoch [13267/20000], Loss: -20.796783447265625, Learning Rate: 0.01\n",
      "Epoch [13268/20000], Loss: -20.797256469726562, Learning Rate: 0.01\n",
      "Epoch [13269/20000], Loss: -20.797683715820312, Learning Rate: 0.01\n",
      "Epoch [13270/20000], Loss: -20.79803466796875, Learning Rate: 0.01\n",
      "Epoch [13271/20000], Loss: -20.798477172851562, Learning Rate: 0.01\n",
      "Epoch [13272/20000], Loss: -20.798843383789062, Learning Rate: 0.01\n",
      "Epoch [13273/20000], Loss: -20.799224853515625, Learning Rate: 0.01\n",
      "Epoch [13274/20000], Loss: -20.799560546875, Learning Rate: 0.01\n",
      "Epoch [13275/20000], Loss: -20.79986572265625, Learning Rate: 0.01\n",
      "Epoch [13276/20000], Loss: -20.80010986328125, Learning Rate: 0.01\n",
      "Epoch [13277/20000], Loss: -20.8001708984375, Learning Rate: 0.01\n",
      "Epoch [13278/20000], Loss: -20.800094604492188, Learning Rate: 0.01\n",
      "Epoch [13279/20000], Loss: -20.799911499023438, Learning Rate: 0.01\n",
      "Epoch [13280/20000], Loss: -20.79931640625, Learning Rate: 0.01\n",
      "Epoch [13281/20000], Loss: -20.798233032226562, Learning Rate: 0.01\n",
      "Epoch [13282/20000], Loss: -20.796585083007812, Learning Rate: 0.01\n",
      "Epoch [13283/20000], Loss: -20.793701171875, Learning Rate: 0.01\n",
      "Epoch [13284/20000], Loss: -20.789352416992188, Learning Rate: 0.01\n",
      "Epoch [13285/20000], Loss: -20.782562255859375, Learning Rate: 0.01\n",
      "Epoch [13286/20000], Loss: -20.772232055664062, Learning Rate: 0.01\n",
      "Epoch [13287/20000], Loss: -20.756622314453125, Learning Rate: 0.01\n",
      "Epoch [13288/20000], Loss: -20.732940673828125, Learning Rate: 0.01\n",
      "Epoch [13289/20000], Loss: -20.697128295898438, Learning Rate: 0.01\n",
      "Epoch [13290/20000], Loss: -20.643966674804688, Learning Rate: 0.01\n",
      "Epoch [13291/20000], Loss: -20.564865112304688, Learning Rate: 0.01\n",
      "Epoch [13292/20000], Loss: -20.450790405273438, Learning Rate: 0.01\n",
      "Epoch [13293/20000], Loss: -20.28863525390625, Learning Rate: 0.01\n",
      "Epoch [13294/20000], Loss: -20.073333740234375, Learning Rate: 0.01\n",
      "Epoch [13295/20000], Loss: -19.801651000976562, Learning Rate: 0.01\n",
      "Epoch [13296/20000], Loss: -19.512405395507812, Learning Rate: 0.01\n",
      "Epoch [13297/20000], Loss: -19.2596435546875, Learning Rate: 0.01\n",
      "Epoch [13298/20000], Loss: -19.170440673828125, Learning Rate: 0.01\n",
      "Epoch [13299/20000], Loss: -19.312255859375, Learning Rate: 0.01\n",
      "Epoch [13300/20000], Loss: -19.707275390625, Learning Rate: 0.01\n",
      "Epoch [13301/20000], Loss: -20.183349609375, Learning Rate: 0.01\n",
      "Epoch [13302/20000], Loss: -20.529769897460938, Learning Rate: 0.01\n",
      "Epoch [13303/20000], Loss: -20.5921630859375, Learning Rate: 0.01\n",
      "Epoch [13304/20000], Loss: -20.3936767578125, Learning Rate: 0.01\n",
      "Epoch [13305/20000], Loss: -20.095046997070312, Learning Rate: 0.01\n",
      "Epoch [13306/20000], Loss: -19.89862060546875, Learning Rate: 0.01\n",
      "Epoch [13307/20000], Loss: -19.928314208984375, Learning Rate: 0.01\n",
      "Epoch [13308/20000], Loss: -20.156692504882812, Learning Rate: 0.01\n",
      "Epoch [13309/20000], Loss: -20.419677734375, Learning Rate: 0.01\n",
      "Epoch [13310/20000], Loss: -20.552810668945312, Learning Rate: 0.01\n",
      "Epoch [13311/20000], Loss: -20.512985229492188, Learning Rate: 0.01\n",
      "Epoch [13312/20000], Loss: -20.402847290039062, Learning Rate: 0.01\n",
      "Epoch [13313/20000], Loss: -20.36041259765625, Learning Rate: 0.01\n",
      "Epoch [13314/20000], Loss: -20.457595825195312, Learning Rate: 0.01\n",
      "Epoch [13315/20000], Loss: -20.635421752929688, Learning Rate: 0.01\n",
      "Epoch [13316/20000], Loss: -20.778549194335938, Learning Rate: 0.01\n",
      "Epoch [13317/20000], Loss: -20.8082275390625, Learning Rate: 0.01\n",
      "Epoch [13318/20000], Loss: -20.743408203125, Learning Rate: 0.01\n",
      "Epoch [13319/20000], Loss: -20.666900634765625, Learning Rate: 0.01\n",
      "Epoch [13320/20000], Loss: -20.648391723632812, Learning Rate: 0.01\n",
      "Epoch [13321/20000], Loss: -20.694229125976562, Learning Rate: 0.01\n",
      "Epoch [13322/20000], Loss: -20.753555297851562, Learning Rate: 0.01\n",
      "Epoch [13323/20000], Loss: -20.774398803710938, Learning Rate: 0.01\n",
      "Epoch [13324/20000], Loss: -20.746810913085938, Learning Rate: 0.01\n",
      "Epoch [13325/20000], Loss: -20.704925537109375, Learning Rate: 0.01\n",
      "Epoch [13326/20000], Loss: -20.690719604492188, Learning Rate: 0.01\n",
      "Epoch [13327/20000], Loss: -20.718276977539062, Learning Rate: 0.01\n",
      "Epoch [13328/20000], Loss: -20.76617431640625, Learning Rate: 0.01\n",
      "Epoch [13329/20000], Loss: -20.800979614257812, Learning Rate: 0.01\n",
      "Epoch [13330/20000], Loss: -20.806015014648438, Learning Rate: 0.01\n",
      "Epoch [13331/20000], Loss: -20.7916259765625, Learning Rate: 0.01\n",
      "Epoch [13332/20000], Loss: -20.780517578125, Learning Rate: 0.01\n",
      "Epoch [13333/20000], Loss: -20.787063598632812, Learning Rate: 0.01\n",
      "Epoch [13334/20000], Loss: -20.8065185546875, Learning Rate: 0.01\n",
      "Epoch [13335/20000], Loss: -20.823562622070312, Learning Rate: 0.01\n",
      "Epoch [13336/20000], Loss: -20.826126098632812, Learning Rate: 0.01\n",
      "Epoch [13337/20000], Loss: -20.815658569335938, Learning Rate: 0.01\n",
      "Epoch [13338/20000], Loss: -20.80316162109375, Learning Rate: 0.01\n",
      "Epoch [13339/20000], Loss: -20.798980712890625, Learning Rate: 0.01\n",
      "Epoch [13340/20000], Loss: -20.80487060546875, Learning Rate: 0.01\n",
      "Epoch [13341/20000], Loss: -20.81427001953125, Learning Rate: 0.01\n",
      "Epoch [13342/20000], Loss: -20.819473266601562, Learning Rate: 0.01\n",
      "Epoch [13343/20000], Loss: -20.8177490234375, Learning Rate: 0.01\n",
      "Epoch [13344/20000], Loss: -20.812759399414062, Learning Rate: 0.01\n",
      "Epoch [13345/20000], Loss: -20.810333251953125, Learning Rate: 0.01\n",
      "Epoch [13346/20000], Loss: -20.8131103515625, Learning Rate: 0.01\n",
      "Epoch [13347/20000], Loss: -20.819412231445312, Learning Rate: 0.01\n",
      "Epoch [13348/20000], Loss: -20.82525634765625, Learning Rate: 0.01\n",
      "Epoch [13349/20000], Loss: -20.827606201171875, Learning Rate: 0.01\n",
      "Epoch [13350/20000], Loss: -20.826705932617188, Learning Rate: 0.01\n",
      "Epoch [13351/20000], Loss: -20.825302124023438, Learning Rate: 0.01\n",
      "Epoch [13352/20000], Loss: -20.825714111328125, Learning Rate: 0.01\n",
      "Epoch [13353/20000], Loss: -20.828460693359375, Learning Rate: 0.01\n",
      "Epoch [13354/20000], Loss: -20.83203125, Learning Rate: 0.01\n",
      "Epoch [13355/20000], Loss: -20.834457397460938, Learning Rate: 0.01\n",
      "Epoch [13356/20000], Loss: -20.835037231445312, Learning Rate: 0.01\n",
      "Epoch [13357/20000], Loss: -20.834426879882812, Learning Rate: 0.01\n",
      "Epoch [13358/20000], Loss: -20.83380126953125, Learning Rate: 0.01\n",
      "Epoch [13359/20000], Loss: -20.834365844726562, Learning Rate: 0.01\n",
      "Epoch [13360/20000], Loss: -20.836044311523438, Learning Rate: 0.01\n",
      "Epoch [13361/20000], Loss: -20.837722778320312, Learning Rate: 0.01\n",
      "Epoch [13362/20000], Loss: -20.838836669921875, Learning Rate: 0.01\n",
      "Epoch [13363/20000], Loss: -20.838973999023438, Learning Rate: 0.01\n",
      "Epoch [13364/20000], Loss: -20.83868408203125, Learning Rate: 0.01\n",
      "Epoch [13365/20000], Loss: -20.838607788085938, Learning Rate: 0.01\n",
      "Epoch [13366/20000], Loss: -20.839111328125, Learning Rate: 0.01\n",
      "Epoch [13367/20000], Loss: -20.840103149414062, Learning Rate: 0.01\n",
      "Epoch [13368/20000], Loss: -20.841079711914062, Learning Rate: 0.01\n",
      "Epoch [13369/20000], Loss: -20.841567993164062, Learning Rate: 0.01\n",
      "Epoch [13370/20000], Loss: -20.84173583984375, Learning Rate: 0.01\n",
      "Epoch [13371/20000], Loss: -20.841659545898438, Learning Rate: 0.01\n",
      "Epoch [13372/20000], Loss: -20.84173583984375, Learning Rate: 0.01\n",
      "Epoch [13373/20000], Loss: -20.842086791992188, Learning Rate: 0.01\n",
      "Epoch [13374/20000], Loss: -20.842483520507812, Learning Rate: 0.01\n",
      "Epoch [13375/20000], Loss: -20.842910766601562, Learning Rate: 0.01\n",
      "Epoch [13376/20000], Loss: -20.842987060546875, Learning Rate: 0.01\n",
      "Epoch [13377/20000], Loss: -20.842849731445312, Learning Rate: 0.01\n",
      "Epoch [13378/20000], Loss: -20.842361450195312, Learning Rate: 0.01\n",
      "Epoch [13379/20000], Loss: -20.84161376953125, Learning Rate: 0.01\n",
      "Epoch [13380/20000], Loss: -20.840621948242188, Learning Rate: 0.01\n",
      "Epoch [13381/20000], Loss: -20.839279174804688, Learning Rate: 0.01\n",
      "Epoch [13382/20000], Loss: -20.837234497070312, Learning Rate: 0.01\n",
      "Epoch [13383/20000], Loss: -20.834091186523438, Learning Rate: 0.01\n",
      "Epoch [13384/20000], Loss: -20.829254150390625, Learning Rate: 0.01\n",
      "Epoch [13385/20000], Loss: -20.82220458984375, Learning Rate: 0.01\n",
      "Epoch [13386/20000], Loss: -20.811935424804688, Learning Rate: 0.01\n",
      "Epoch [13387/20000], Loss: -20.797149658203125, Learning Rate: 0.01\n",
      "Epoch [13388/20000], Loss: -20.7757568359375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13389/20000], Loss: -20.744659423828125, Learning Rate: 0.01\n",
      "Epoch [13390/20000], Loss: -20.6998291015625, Learning Rate: 0.01\n",
      "Epoch [13391/20000], Loss: -20.634933471679688, Learning Rate: 0.01\n",
      "Epoch [13392/20000], Loss: -20.54339599609375, Learning Rate: 0.01\n",
      "Epoch [13393/20000], Loss: -20.413970947265625, Learning Rate: 0.01\n",
      "Epoch [13394/20000], Loss: -20.2412109375, Learning Rate: 0.01\n",
      "Epoch [13395/20000], Loss: -20.014312744140625, Learning Rate: 0.01\n",
      "Epoch [13396/20000], Loss: -19.749191284179688, Learning Rate: 0.01\n",
      "Epoch [13397/20000], Loss: -19.46337890625, Learning Rate: 0.01\n",
      "Epoch [13398/20000], Loss: -19.242599487304688, Learning Rate: 0.01\n",
      "Epoch [13399/20000], Loss: -19.155319213867188, Learning Rate: 0.01\n",
      "Epoch [13400/20000], Loss: -19.316574096679688, Learning Rate: 0.01\n",
      "Epoch [13401/20000], Loss: -19.701034545898438, Learning Rate: 0.01\n",
      "Epoch [13402/20000], Loss: -20.214508056640625, Learning Rate: 0.01\n",
      "Epoch [13403/20000], Loss: -20.647369384765625, Learning Rate: 0.01\n",
      "Epoch [13404/20000], Loss: -20.848312377929688, Learning Rate: 0.01\n",
      "Epoch [13405/20000], Loss: -20.78924560546875, Learning Rate: 0.01\n",
      "Epoch [13406/20000], Loss: -20.5703125, Learning Rate: 0.01\n",
      "Epoch [13407/20000], Loss: -20.350631713867188, Learning Rate: 0.01\n",
      "Epoch [13408/20000], Loss: -20.258285522460938, Learning Rate: 0.01\n",
      "Epoch [13409/20000], Loss: -20.345474243164062, Learning Rate: 0.01\n",
      "Epoch [13410/20000], Loss: -20.548675537109375, Learning Rate: 0.01\n",
      "Epoch [13411/20000], Loss: -20.753341674804688, Learning Rate: 0.01\n",
      "Epoch [13412/20000], Loss: -20.854766845703125, Learning Rate: 0.01\n",
      "Epoch [13413/20000], Loss: -20.826950073242188, Learning Rate: 0.01\n",
      "Epoch [13414/20000], Loss: -20.721893310546875, Learning Rate: 0.01\n",
      "Epoch [13415/20000], Loss: -20.624465942382812, Learning Rate: 0.01\n",
      "Epoch [13416/20000], Loss: -20.6002197265625, Learning Rate: 0.01\n",
      "Epoch [13417/20000], Loss: -20.657516479492188, Learning Rate: 0.01\n",
      "Epoch [13418/20000], Loss: -20.756759643554688, Learning Rate: 0.01\n",
      "Epoch [13419/20000], Loss: -20.837417602539062, Learning Rate: 0.01\n",
      "Epoch [13420/20000], Loss: -20.861572265625, Learning Rate: 0.01\n",
      "Epoch [13421/20000], Loss: -20.8311767578125, Learning Rate: 0.01\n",
      "Epoch [13422/20000], Loss: -20.779678344726562, Learning Rate: 0.01\n",
      "Epoch [13423/20000], Loss: -20.746246337890625, Learning Rate: 0.01\n",
      "Epoch [13424/20000], Loss: -20.7508544921875, Learning Rate: 0.01\n",
      "Epoch [13425/20000], Loss: -20.788055419921875, Learning Rate: 0.01\n",
      "Epoch [13426/20000], Loss: -20.832870483398438, Learning Rate: 0.01\n",
      "Epoch [13427/20000], Loss: -20.861083984375, Learning Rate: 0.01\n",
      "Epoch [13428/20000], Loss: -20.861968994140625, Learning Rate: 0.01\n",
      "Epoch [13429/20000], Loss: -20.8428955078125, Learning Rate: 0.01\n",
      "Epoch [13430/20000], Loss: -20.820907592773438, Learning Rate: 0.01\n",
      "Epoch [13431/20000], Loss: -20.811111450195312, Learning Rate: 0.01\n",
      "Epoch [13432/20000], Loss: -20.818679809570312, Learning Rate: 0.01\n",
      "Epoch [13433/20000], Loss: -20.837646484375, Learning Rate: 0.01\n",
      "Epoch [13434/20000], Loss: -20.856842041015625, Learning Rate: 0.01\n",
      "Epoch [13435/20000], Loss: -20.866958618164062, Learning Rate: 0.01\n",
      "Epoch [13436/20000], Loss: -20.865432739257812, Learning Rate: 0.01\n",
      "Epoch [13437/20000], Loss: -20.856491088867188, Learning Rate: 0.01\n",
      "Epoch [13438/20000], Loss: -20.847503662109375, Learning Rate: 0.01\n",
      "Epoch [13439/20000], Loss: -20.84423828125, Learning Rate: 0.01\n",
      "Epoch [13440/20000], Loss: -20.84814453125, Learning Rate: 0.01\n",
      "Epoch [13441/20000], Loss: -20.856704711914062, Learning Rate: 0.01\n",
      "Epoch [13442/20000], Loss: -20.865280151367188, Learning Rate: 0.01\n",
      "Epoch [13443/20000], Loss: -20.870025634765625, Learning Rate: 0.01\n",
      "Epoch [13444/20000], Loss: -20.870025634765625, Learning Rate: 0.01\n",
      "Epoch [13445/20000], Loss: -20.866653442382812, Learning Rate: 0.01\n",
      "Epoch [13446/20000], Loss: -20.86285400390625, Learning Rate: 0.01\n",
      "Epoch [13447/20000], Loss: -20.86102294921875, Learning Rate: 0.01\n",
      "Epoch [13448/20000], Loss: -20.862197875976562, Learning Rate: 0.01\n",
      "Epoch [13449/20000], Loss: -20.865631103515625, Learning Rate: 0.01\n",
      "Epoch [13450/20000], Loss: -20.869613647460938, Learning Rate: 0.01\n",
      "Epoch [13451/20000], Loss: -20.87255859375, Learning Rate: 0.01\n",
      "Epoch [13452/20000], Loss: -20.873626708984375, Learning Rate: 0.01\n",
      "Epoch [13453/20000], Loss: -20.872970581054688, Learning Rate: 0.01\n",
      "Epoch [13454/20000], Loss: -20.871566772460938, Learning Rate: 0.01\n",
      "Epoch [13455/20000], Loss: -20.870468139648438, Learning Rate: 0.01\n",
      "Epoch [13456/20000], Loss: -20.870437622070312, Learning Rate: 0.01\n",
      "Epoch [13457/20000], Loss: -20.8714599609375, Learning Rate: 0.01\n",
      "Epoch [13458/20000], Loss: -20.8731689453125, Learning Rate: 0.01\n",
      "Epoch [13459/20000], Loss: -20.875, Learning Rate: 0.01\n",
      "Epoch [13460/20000], Loss: -20.876266479492188, Learning Rate: 0.01\n",
      "Epoch [13461/20000], Loss: -20.876785278320312, Learning Rate: 0.01\n",
      "Epoch [13462/20000], Loss: -20.876724243164062, Learning Rate: 0.01\n",
      "Epoch [13463/20000], Loss: -20.87640380859375, Learning Rate: 0.01\n",
      "Epoch [13464/20000], Loss: -20.876144409179688, Learning Rate: 0.01\n",
      "Epoch [13465/20000], Loss: -20.876251220703125, Learning Rate: 0.01\n",
      "Epoch [13466/20000], Loss: -20.876739501953125, Learning Rate: 0.01\n",
      "Epoch [13467/20000], Loss: -20.877639770507812, Learning Rate: 0.01\n",
      "Epoch [13468/20000], Loss: -20.878494262695312, Learning Rate: 0.01\n",
      "Epoch [13469/20000], Loss: -20.879364013671875, Learning Rate: 0.01\n",
      "Epoch [13470/20000], Loss: -20.8798828125, Learning Rate: 0.01\n",
      "Epoch [13471/20000], Loss: -20.880218505859375, Learning Rate: 0.01\n",
      "Epoch [13472/20000], Loss: -20.880386352539062, Learning Rate: 0.01\n",
      "Epoch [13473/20000], Loss: -20.880447387695312, Learning Rate: 0.01\n",
      "Epoch [13474/20000], Loss: -20.880645751953125, Learning Rate: 0.01\n",
      "Epoch [13475/20000], Loss: -20.880935668945312, Learning Rate: 0.01\n",
      "Epoch [13476/20000], Loss: -20.881317138671875, Learning Rate: 0.01\n",
      "Epoch [13477/20000], Loss: -20.8818359375, Learning Rate: 0.01\n",
      "Epoch [13478/20000], Loss: -20.882400512695312, Learning Rate: 0.01\n",
      "Epoch [13479/20000], Loss: -20.882904052734375, Learning Rate: 0.01\n",
      "Epoch [13480/20000], Loss: -20.883316040039062, Learning Rate: 0.01\n",
      "Epoch [13481/20000], Loss: -20.88372802734375, Learning Rate: 0.01\n",
      "Epoch [13482/20000], Loss: -20.884063720703125, Learning Rate: 0.01\n",
      "Epoch [13483/20000], Loss: -20.884262084960938, Learning Rate: 0.01\n",
      "Epoch [13484/20000], Loss: -20.884552001953125, Learning Rate: 0.01\n",
      "Epoch [13485/20000], Loss: -20.884872436523438, Learning Rate: 0.01\n",
      "Epoch [13486/20000], Loss: -20.8851318359375, Learning Rate: 0.01\n",
      "Epoch [13487/20000], Loss: -20.885452270507812, Learning Rate: 0.01\n",
      "Epoch [13488/20000], Loss: -20.885818481445312, Learning Rate: 0.01\n",
      "Epoch [13489/20000], Loss: -20.886322021484375, Learning Rate: 0.01\n",
      "Epoch [13490/20000], Loss: -20.886672973632812, Learning Rate: 0.01\n",
      "Epoch [13491/20000], Loss: -20.887115478515625, Learning Rate: 0.01\n",
      "Epoch [13492/20000], Loss: -20.88739013671875, Learning Rate: 0.01\n",
      "Epoch [13493/20000], Loss: -20.8878173828125, Learning Rate: 0.01\n",
      "Epoch [13494/20000], Loss: -20.888107299804688, Learning Rate: 0.01\n",
      "Epoch [13495/20000], Loss: -20.888397216796875, Learning Rate: 0.01\n",
      "Epoch [13496/20000], Loss: -20.888702392578125, Learning Rate: 0.01\n",
      "Epoch [13497/20000], Loss: -20.889114379882812, Learning Rate: 0.01\n",
      "Epoch [13498/20000], Loss: -20.889404296875, Learning Rate: 0.01\n",
      "Epoch [13499/20000], Loss: -20.889739990234375, Learning Rate: 0.01\n",
      "Epoch [13500/20000], Loss: -20.890029907226562, Learning Rate: 0.01\n",
      "Epoch [13501/20000], Loss: -20.890380859375, Learning Rate: 0.01\n",
      "Epoch [13502/20000], Loss: -20.890823364257812, Learning Rate: 0.01\n",
      "Epoch [13503/20000], Loss: -20.89117431640625, Learning Rate: 0.01\n",
      "Epoch [13504/20000], Loss: -20.891510009765625, Learning Rate: 0.01\n",
      "Epoch [13505/20000], Loss: -20.891799926757812, Learning Rate: 0.01\n",
      "Epoch [13506/20000], Loss: -20.892181396484375, Learning Rate: 0.01\n",
      "Epoch [13507/20000], Loss: -20.892562866210938, Learning Rate: 0.01\n",
      "Epoch [13508/20000], Loss: -20.892898559570312, Learning Rate: 0.01\n",
      "Epoch [13509/20000], Loss: -20.893218994140625, Learning Rate: 0.01\n",
      "Epoch [13510/20000], Loss: -20.893508911132812, Learning Rate: 0.01\n",
      "Epoch [13511/20000], Loss: -20.893768310546875, Learning Rate: 0.01\n",
      "Epoch [13512/20000], Loss: -20.894149780273438, Learning Rate: 0.01\n",
      "Epoch [13513/20000], Loss: -20.894546508789062, Learning Rate: 0.01\n",
      "Epoch [13514/20000], Loss: -20.894821166992188, Learning Rate: 0.01\n",
      "Epoch [13515/20000], Loss: -20.8951416015625, Learning Rate: 0.01\n",
      "Epoch [13516/20000], Loss: -20.89544677734375, Learning Rate: 0.01\n",
      "Epoch [13517/20000], Loss: -20.895782470703125, Learning Rate: 0.01\n",
      "Epoch [13518/20000], Loss: -20.8961181640625, Learning Rate: 0.01\n",
      "Epoch [13519/20000], Loss: -20.896499633789062, Learning Rate: 0.01\n",
      "Epoch [13520/20000], Loss: -20.896804809570312, Learning Rate: 0.01\n",
      "Epoch [13521/20000], Loss: -20.897171020507812, Learning Rate: 0.01\n",
      "Epoch [13522/20000], Loss: -20.897476196289062, Learning Rate: 0.01\n",
      "Epoch [13523/20000], Loss: -20.8978271484375, Learning Rate: 0.01\n",
      "Epoch [13524/20000], Loss: -20.898162841796875, Learning Rate: 0.01\n",
      "Epoch [13525/20000], Loss: -20.898513793945312, Learning Rate: 0.01\n",
      "Epoch [13526/20000], Loss: -20.898849487304688, Learning Rate: 0.01\n",
      "Epoch [13527/20000], Loss: -20.899185180664062, Learning Rate: 0.01\n",
      "Epoch [13528/20000], Loss: -20.899490356445312, Learning Rate: 0.01\n",
      "Epoch [13529/20000], Loss: -20.899856567382812, Learning Rate: 0.01\n",
      "Epoch [13530/20000], Loss: -20.900192260742188, Learning Rate: 0.01\n",
      "Epoch [13531/20000], Loss: -20.9005126953125, Learning Rate: 0.01\n",
      "Epoch [13532/20000], Loss: -20.900848388671875, Learning Rate: 0.01\n",
      "Epoch [13533/20000], Loss: -20.901138305664062, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13534/20000], Loss: -20.901535034179688, Learning Rate: 0.01\n",
      "Epoch [13535/20000], Loss: -20.901779174804688, Learning Rate: 0.01\n",
      "Epoch [13536/20000], Loss: -20.902099609375, Learning Rate: 0.01\n",
      "Epoch [13537/20000], Loss: -20.902481079101562, Learning Rate: 0.01\n",
      "Epoch [13538/20000], Loss: -20.902801513671875, Learning Rate: 0.01\n",
      "Epoch [13539/20000], Loss: -20.903106689453125, Learning Rate: 0.01\n",
      "Epoch [13540/20000], Loss: -20.903427124023438, Learning Rate: 0.01\n",
      "Epoch [13541/20000], Loss: -20.903732299804688, Learning Rate: 0.01\n",
      "Epoch [13542/20000], Loss: -20.904037475585938, Learning Rate: 0.01\n",
      "Epoch [13543/20000], Loss: -20.904495239257812, Learning Rate: 0.01\n",
      "Epoch [13544/20000], Loss: -20.904693603515625, Learning Rate: 0.01\n",
      "Epoch [13545/20000], Loss: -20.905044555664062, Learning Rate: 0.01\n",
      "Epoch [13546/20000], Loss: -20.905410766601562, Learning Rate: 0.01\n",
      "Epoch [13547/20000], Loss: -20.90570068359375, Learning Rate: 0.01\n",
      "Epoch [13548/20000], Loss: -20.906051635742188, Learning Rate: 0.01\n",
      "Epoch [13549/20000], Loss: -20.906326293945312, Learning Rate: 0.01\n",
      "Epoch [13550/20000], Loss: -20.906692504882812, Learning Rate: 0.01\n",
      "Epoch [13551/20000], Loss: -20.907028198242188, Learning Rate: 0.01\n",
      "Epoch [13552/20000], Loss: -20.907318115234375, Learning Rate: 0.01\n",
      "Epoch [13553/20000], Loss: -20.9075927734375, Learning Rate: 0.01\n",
      "Epoch [13554/20000], Loss: -20.907958984375, Learning Rate: 0.01\n",
      "Epoch [13555/20000], Loss: -20.9083251953125, Learning Rate: 0.01\n",
      "Epoch [13556/20000], Loss: -20.908615112304688, Learning Rate: 0.01\n",
      "Epoch [13557/20000], Loss: -20.908981323242188, Learning Rate: 0.01\n",
      "Epoch [13558/20000], Loss: -20.909286499023438, Learning Rate: 0.01\n",
      "Epoch [13559/20000], Loss: -20.909591674804688, Learning Rate: 0.01\n",
      "Epoch [13560/20000], Loss: -20.909881591796875, Learning Rate: 0.01\n",
      "Epoch [13561/20000], Loss: -20.91021728515625, Learning Rate: 0.01\n",
      "Epoch [13562/20000], Loss: -20.910598754882812, Learning Rate: 0.01\n",
      "Epoch [13563/20000], Loss: -20.910858154296875, Learning Rate: 0.01\n",
      "Epoch [13564/20000], Loss: -20.911163330078125, Learning Rate: 0.01\n",
      "Epoch [13565/20000], Loss: -20.911407470703125, Learning Rate: 0.01\n",
      "Epoch [13566/20000], Loss: -20.911712646484375, Learning Rate: 0.01\n",
      "Epoch [13567/20000], Loss: -20.912033081054688, Learning Rate: 0.01\n",
      "Epoch [13568/20000], Loss: -20.912246704101562, Learning Rate: 0.01\n",
      "Epoch [13569/20000], Loss: -20.91241455078125, Learning Rate: 0.01\n",
      "Epoch [13570/20000], Loss: -20.912490844726562, Learning Rate: 0.01\n",
      "Epoch [13571/20000], Loss: -20.912521362304688, Learning Rate: 0.01\n",
      "Epoch [13572/20000], Loss: -20.912200927734375, Learning Rate: 0.01\n",
      "Epoch [13573/20000], Loss: -20.91162109375, Learning Rate: 0.01\n",
      "Epoch [13574/20000], Loss: -20.910491943359375, Learning Rate: 0.01\n",
      "Epoch [13575/20000], Loss: -20.908477783203125, Learning Rate: 0.01\n",
      "Epoch [13576/20000], Loss: -20.904937744140625, Learning Rate: 0.01\n",
      "Epoch [13577/20000], Loss: -20.898941040039062, Learning Rate: 0.01\n",
      "Epoch [13578/20000], Loss: -20.888870239257812, Learning Rate: 0.01\n",
      "Epoch [13579/20000], Loss: -20.8720703125, Learning Rate: 0.01\n",
      "Epoch [13580/20000], Loss: -20.8441162109375, Learning Rate: 0.01\n",
      "Epoch [13581/20000], Loss: -20.797500610351562, Learning Rate: 0.01\n",
      "Epoch [13582/20000], Loss: -20.719680786132812, Learning Rate: 0.01\n",
      "Epoch [13583/20000], Loss: -20.591842651367188, Learning Rate: 0.01\n",
      "Epoch [13584/20000], Loss: -20.382049560546875, Learning Rate: 0.01\n",
      "Epoch [13585/20000], Loss: -20.0533447265625, Learning Rate: 0.01\n",
      "Epoch [13586/20000], Loss: -19.549179077148438, Learning Rate: 0.01\n",
      "Epoch [13587/20000], Loss: -18.866867065429688, Learning Rate: 0.01\n",
      "Epoch [13588/20000], Loss: -18.041091918945312, Learning Rate: 0.01\n",
      "Epoch [13589/20000], Loss: -17.398605346679688, Learning Rate: 0.01\n",
      "Epoch [13590/20000], Loss: -17.295181274414062, Learning Rate: 0.01\n",
      "Epoch [13591/20000], Loss: -18.160812377929688, Learning Rate: 0.01\n",
      "Epoch [13592/20000], Loss: -19.582427978515625, Learning Rate: 0.01\n",
      "Epoch [13593/20000], Loss: -20.710601806640625, Learning Rate: 0.01\n",
      "Epoch [13594/20000], Loss: -20.846710205078125, Learning Rate: 0.01\n",
      "Epoch [13595/20000], Loss: -20.169479370117188, Learning Rate: 0.01\n",
      "Epoch [13596/20000], Loss: -19.475662231445312, Learning Rate: 0.01\n",
      "Epoch [13597/20000], Loss: -19.436782836914062, Learning Rate: 0.01\n",
      "Epoch [13598/20000], Loss: -20.094345092773438, Learning Rate: 0.01\n",
      "Epoch [13599/20000], Loss: -20.774002075195312, Learning Rate: 0.01\n",
      "Epoch [13600/20000], Loss: -20.884002685546875, Learning Rate: 0.01\n",
      "Epoch [13601/20000], Loss: -20.482376098632812, Learning Rate: 0.01\n",
      "Epoch [13602/20000], Loss: -20.11865234375, Learning Rate: 0.01\n",
      "Epoch [13603/20000], Loss: -20.215286254882812, Learning Rate: 0.01\n",
      "Epoch [13604/20000], Loss: -20.630615234375, Learning Rate: 0.01\n",
      "Epoch [13605/20000], Loss: -20.913650512695312, Learning Rate: 0.01\n",
      "Epoch [13606/20000], Loss: -20.822219848632812, Learning Rate: 0.01\n",
      "Epoch [13607/20000], Loss: -20.55853271484375, Learning Rate: 0.01\n",
      "Epoch [13608/20000], Loss: -20.47216796875, Learning Rate: 0.01\n",
      "Epoch [13609/20000], Loss: -20.649948120117188, Learning Rate: 0.01\n",
      "Epoch [13610/20000], Loss: -20.875732421875, Learning Rate: 0.01\n",
      "Epoch [13611/20000], Loss: -20.912948608398438, Learning Rate: 0.01\n",
      "Epoch [13612/20000], Loss: -20.776687622070312, Learning Rate: 0.01\n",
      "Epoch [13613/20000], Loss: -20.669357299804688, Learning Rate: 0.01\n",
      "Epoch [13614/20000], Loss: -20.72003173828125, Learning Rate: 0.01\n",
      "Epoch [13615/20000], Loss: -20.859237670898438, Learning Rate: 0.01\n",
      "Epoch [13616/20000], Loss: -20.92816162109375, Learning Rate: 0.01\n",
      "Epoch [13617/20000], Loss: -20.87591552734375, Learning Rate: 0.01\n",
      "Epoch [13618/20000], Loss: -20.794677734375, Learning Rate: 0.01\n",
      "Epoch [13619/20000], Loss: -20.7896728515625, Learning Rate: 0.01\n",
      "Epoch [13620/20000], Loss: -20.861587524414062, Learning Rate: 0.01\n",
      "Epoch [13621/20000], Loss: -20.92401123046875, Learning Rate: 0.01\n",
      "Epoch [13622/20000], Loss: -20.917129516601562, Learning Rate: 0.01\n",
      "Epoch [13623/20000], Loss: -20.8682861328125, Learning Rate: 0.01\n",
      "Epoch [13624/20000], Loss: -20.845169067382812, Learning Rate: 0.01\n",
      "Epoch [13625/20000], Loss: -20.873748779296875, Learning Rate: 0.01\n",
      "Epoch [13626/20000], Loss: -20.917709350585938, Learning Rate: 0.01\n",
      "Epoch [13627/20000], Loss: -20.930618286132812, Learning Rate: 0.01\n",
      "Epoch [13628/20000], Loss: -20.908355712890625, Learning Rate: 0.01\n",
      "Epoch [13629/20000], Loss: -20.885208129882812, Learning Rate: 0.01\n",
      "Epoch [13630/20000], Loss: -20.889938354492188, Learning Rate: 0.01\n",
      "Epoch [13631/20000], Loss: -20.914901733398438, Learning Rate: 0.01\n",
      "Epoch [13632/20000], Loss: -20.932342529296875, Learning Rate: 0.01\n",
      "Epoch [13633/20000], Loss: -20.927398681640625, Learning Rate: 0.01\n",
      "Epoch [13634/20000], Loss: -20.911773681640625, Learning Rate: 0.01\n",
      "Epoch [13635/20000], Loss: -20.906341552734375, Learning Rate: 0.01\n",
      "Epoch [13636/20000], Loss: -20.916580200195312, Learning Rate: 0.01\n",
      "Epoch [13637/20000], Loss: -20.930526733398438, Learning Rate: 0.01\n",
      "Epoch [13638/20000], Loss: -20.934417724609375, Learning Rate: 0.01\n",
      "Epoch [13639/20000], Loss: -20.927474975585938, Learning Rate: 0.01\n",
      "Epoch [13640/20000], Loss: -20.920303344726562, Learning Rate: 0.01\n",
      "Epoch [13641/20000], Loss: -20.921539306640625, Learning Rate: 0.01\n",
      "Epoch [13642/20000], Loss: -20.929534912109375, Learning Rate: 0.01\n",
      "Epoch [13643/20000], Loss: -20.935653686523438, Learning Rate: 0.01\n",
      "Epoch [13644/20000], Loss: -20.93511962890625, Learning Rate: 0.01\n",
      "Epoch [13645/20000], Loss: -20.9305419921875, Learning Rate: 0.01\n",
      "Epoch [13646/20000], Loss: -20.928207397460938, Learning Rate: 0.01\n",
      "Epoch [13647/20000], Loss: -20.930709838867188, Learning Rate: 0.01\n",
      "Epoch [13648/20000], Loss: -20.935379028320312, Learning Rate: 0.01\n",
      "Epoch [13649/20000], Loss: -20.9378662109375, Learning Rate: 0.01\n",
      "Epoch [13650/20000], Loss: -20.936691284179688, Learning Rate: 0.01\n",
      "Epoch [13651/20000], Loss: -20.934371948242188, Learning Rate: 0.01\n",
      "Epoch [13652/20000], Loss: -20.93389892578125, Learning Rate: 0.01\n",
      "Epoch [13653/20000], Loss: -20.935928344726562, Learning Rate: 0.01\n",
      "Epoch [13654/20000], Loss: -20.938613891601562, Learning Rate: 0.01\n",
      "Epoch [13655/20000], Loss: -20.939544677734375, Learning Rate: 0.01\n",
      "Epoch [13656/20000], Loss: -20.938751220703125, Learning Rate: 0.01\n",
      "Epoch [13657/20000], Loss: -20.937698364257812, Learning Rate: 0.01\n",
      "Epoch [13658/20000], Loss: -20.9378662109375, Learning Rate: 0.01\n",
      "Epoch [13659/20000], Loss: -20.939178466796875, Learning Rate: 0.01\n",
      "Epoch [13660/20000], Loss: -20.940673828125, Learning Rate: 0.01\n",
      "Epoch [13661/20000], Loss: -20.941207885742188, Learning Rate: 0.01\n",
      "Epoch [13662/20000], Loss: -20.940811157226562, Learning Rate: 0.01\n",
      "Epoch [13663/20000], Loss: -20.9404296875, Learning Rate: 0.01\n",
      "Epoch [13664/20000], Loss: -20.940719604492188, Learning Rate: 0.01\n",
      "Epoch [13665/20000], Loss: -20.941574096679688, Learning Rate: 0.01\n",
      "Epoch [13666/20000], Loss: -20.942489624023438, Learning Rate: 0.01\n",
      "Epoch [13667/20000], Loss: -20.942916870117188, Learning Rate: 0.01\n",
      "Epoch [13668/20000], Loss: -20.942840576171875, Learning Rate: 0.01\n",
      "Epoch [13669/20000], Loss: -20.942764282226562, Learning Rate: 0.01\n",
      "Epoch [13670/20000], Loss: -20.943069458007812, Learning Rate: 0.01\n",
      "Epoch [13671/20000], Loss: -20.943618774414062, Learning Rate: 0.01\n",
      "Epoch [13672/20000], Loss: -20.944183349609375, Learning Rate: 0.01\n",
      "Epoch [13673/20000], Loss: -20.944610595703125, Learning Rate: 0.01\n",
      "Epoch [13674/20000], Loss: -20.9447021484375, Learning Rate: 0.01\n",
      "Epoch [13675/20000], Loss: -20.94482421875, Learning Rate: 0.01\n",
      "Epoch [13676/20000], Loss: -20.945022583007812, Learning Rate: 0.01\n",
      "Epoch [13677/20000], Loss: -20.945404052734375, Learning Rate: 0.01\n",
      "Epoch [13678/20000], Loss: -20.945877075195312, Learning Rate: 0.01\n",
      "Epoch [13679/20000], Loss: -20.946212768554688, Learning Rate: 0.01\n",
      "Epoch [13680/20000], Loss: -20.946487426757812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13681/20000], Loss: -20.946685791015625, Learning Rate: 0.01\n",
      "Epoch [13682/20000], Loss: -20.946868896484375, Learning Rate: 0.01\n",
      "Epoch [13683/20000], Loss: -20.947219848632812, Learning Rate: 0.01\n",
      "Epoch [13684/20000], Loss: -20.947647094726562, Learning Rate: 0.01\n",
      "Epoch [13685/20000], Loss: -20.947967529296875, Learning Rate: 0.01\n",
      "Epoch [13686/20000], Loss: -20.948211669921875, Learning Rate: 0.01\n",
      "Epoch [13687/20000], Loss: -20.948440551757812, Learning Rate: 0.01\n",
      "Epoch [13688/20000], Loss: -20.948684692382812, Learning Rate: 0.01\n",
      "Epoch [13689/20000], Loss: -20.948928833007812, Learning Rate: 0.01\n",
      "Epoch [13690/20000], Loss: -20.949203491210938, Learning Rate: 0.01\n",
      "Epoch [13691/20000], Loss: -20.949539184570312, Learning Rate: 0.01\n",
      "Epoch [13692/20000], Loss: -20.949813842773438, Learning Rate: 0.01\n",
      "Epoch [13693/20000], Loss: -20.950103759765625, Learning Rate: 0.01\n",
      "Epoch [13694/20000], Loss: -20.950332641601562, Learning Rate: 0.01\n",
      "Epoch [13695/20000], Loss: -20.950607299804688, Learning Rate: 0.01\n",
      "Epoch [13696/20000], Loss: -20.950881958007812, Learning Rate: 0.01\n",
      "Epoch [13697/20000], Loss: -20.951202392578125, Learning Rate: 0.01\n",
      "Epoch [13698/20000], Loss: -20.951431274414062, Learning Rate: 0.01\n",
      "Epoch [13699/20000], Loss: -20.951797485351562, Learning Rate: 0.01\n",
      "Epoch [13700/20000], Loss: -20.952041625976562, Learning Rate: 0.01\n",
      "Epoch [13701/20000], Loss: -20.952285766601562, Learning Rate: 0.01\n",
      "Epoch [13702/20000], Loss: -20.952529907226562, Learning Rate: 0.01\n",
      "Epoch [13703/20000], Loss: -20.952835083007812, Learning Rate: 0.01\n",
      "Epoch [13704/20000], Loss: -20.953079223632812, Learning Rate: 0.01\n",
      "Epoch [13705/20000], Loss: -20.953338623046875, Learning Rate: 0.01\n",
      "Epoch [13706/20000], Loss: -20.953643798828125, Learning Rate: 0.01\n",
      "Epoch [13707/20000], Loss: -20.953887939453125, Learning Rate: 0.01\n",
      "Epoch [13708/20000], Loss: -20.95416259765625, Learning Rate: 0.01\n",
      "Epoch [13709/20000], Loss: -20.954437255859375, Learning Rate: 0.01\n",
      "Epoch [13710/20000], Loss: -20.954727172851562, Learning Rate: 0.01\n",
      "Epoch [13711/20000], Loss: -20.9549560546875, Learning Rate: 0.01\n",
      "Epoch [13712/20000], Loss: -20.955245971679688, Learning Rate: 0.01\n",
      "Epoch [13713/20000], Loss: -20.955520629882812, Learning Rate: 0.01\n",
      "Epoch [13714/20000], Loss: -20.955795288085938, Learning Rate: 0.01\n",
      "Epoch [13715/20000], Loss: -20.956100463867188, Learning Rate: 0.01\n",
      "Epoch [13716/20000], Loss: -20.95635986328125, Learning Rate: 0.01\n",
      "Epoch [13717/20000], Loss: -20.956634521484375, Learning Rate: 0.01\n",
      "Epoch [13718/20000], Loss: -20.956878662109375, Learning Rate: 0.01\n",
      "Epoch [13719/20000], Loss: -20.957138061523438, Learning Rate: 0.01\n",
      "Epoch [13720/20000], Loss: -20.95733642578125, Learning Rate: 0.01\n",
      "Epoch [13721/20000], Loss: -20.957672119140625, Learning Rate: 0.01\n",
      "Epoch [13722/20000], Loss: -20.957855224609375, Learning Rate: 0.01\n",
      "Epoch [13723/20000], Loss: -20.958175659179688, Learning Rate: 0.01\n",
      "Epoch [13724/20000], Loss: -20.958419799804688, Learning Rate: 0.01\n",
      "Epoch [13725/20000], Loss: -20.958694458007812, Learning Rate: 0.01\n",
      "Epoch [13726/20000], Loss: -20.95904541015625, Learning Rate: 0.01\n",
      "Epoch [13727/20000], Loss: -20.959274291992188, Learning Rate: 0.01\n",
      "Epoch [13728/20000], Loss: -20.959518432617188, Learning Rate: 0.01\n",
      "Epoch [13729/20000], Loss: -20.95977783203125, Learning Rate: 0.01\n",
      "Epoch [13730/20000], Loss: -20.960052490234375, Learning Rate: 0.01\n",
      "Epoch [13731/20000], Loss: -20.960311889648438, Learning Rate: 0.01\n",
      "Epoch [13732/20000], Loss: -20.960662841796875, Learning Rate: 0.01\n",
      "Epoch [13733/20000], Loss: -20.960845947265625, Learning Rate: 0.01\n",
      "Epoch [13734/20000], Loss: -20.961074829101562, Learning Rate: 0.01\n",
      "Epoch [13735/20000], Loss: -20.961288452148438, Learning Rate: 0.01\n",
      "Epoch [13736/20000], Loss: -20.961578369140625, Learning Rate: 0.01\n",
      "Epoch [13737/20000], Loss: -20.961898803710938, Learning Rate: 0.01\n",
      "Epoch [13738/20000], Loss: -20.962188720703125, Learning Rate: 0.01\n",
      "Epoch [13739/20000], Loss: -20.962417602539062, Learning Rate: 0.01\n",
      "Epoch [13740/20000], Loss: -20.962661743164062, Learning Rate: 0.01\n",
      "Epoch [13741/20000], Loss: -20.962982177734375, Learning Rate: 0.01\n",
      "Epoch [13742/20000], Loss: -20.96319580078125, Learning Rate: 0.01\n",
      "Epoch [13743/20000], Loss: -20.963531494140625, Learning Rate: 0.01\n",
      "Epoch [13744/20000], Loss: -20.963775634765625, Learning Rate: 0.01\n",
      "Epoch [13745/20000], Loss: -20.9639892578125, Learning Rate: 0.01\n",
      "Epoch [13746/20000], Loss: -20.964263916015625, Learning Rate: 0.01\n",
      "Epoch [13747/20000], Loss: -20.964492797851562, Learning Rate: 0.01\n",
      "Epoch [13748/20000], Loss: -20.964813232421875, Learning Rate: 0.01\n",
      "Epoch [13749/20000], Loss: -20.964996337890625, Learning Rate: 0.01\n",
      "Epoch [13750/20000], Loss: -20.965301513671875, Learning Rate: 0.01\n",
      "Epoch [13751/20000], Loss: -20.965530395507812, Learning Rate: 0.01\n",
      "Epoch [13752/20000], Loss: -20.965850830078125, Learning Rate: 0.01\n",
      "Epoch [13753/20000], Loss: -20.966079711914062, Learning Rate: 0.01\n",
      "Epoch [13754/20000], Loss: -20.966384887695312, Learning Rate: 0.01\n",
      "Epoch [13755/20000], Loss: -20.966598510742188, Learning Rate: 0.01\n",
      "Epoch [13756/20000], Loss: -20.966812133789062, Learning Rate: 0.01\n",
      "Epoch [13757/20000], Loss: -20.967086791992188, Learning Rate: 0.01\n",
      "Epoch [13758/20000], Loss: -20.96734619140625, Learning Rate: 0.01\n",
      "Epoch [13759/20000], Loss: -20.96759033203125, Learning Rate: 0.01\n",
      "Epoch [13760/20000], Loss: -20.967849731445312, Learning Rate: 0.01\n",
      "Epoch [13761/20000], Loss: -20.968048095703125, Learning Rate: 0.01\n",
      "Epoch [13762/20000], Loss: -20.968307495117188, Learning Rate: 0.01\n",
      "Epoch [13763/20000], Loss: -20.968475341796875, Learning Rate: 0.01\n",
      "Epoch [13764/20000], Loss: -20.968597412109375, Learning Rate: 0.01\n",
      "Epoch [13765/20000], Loss: -20.968704223632812, Learning Rate: 0.01\n",
      "Epoch [13766/20000], Loss: -20.968826293945312, Learning Rate: 0.01\n",
      "Epoch [13767/20000], Loss: -20.968734741210938, Learning Rate: 0.01\n",
      "Epoch [13768/20000], Loss: -20.968521118164062, Learning Rate: 0.01\n",
      "Epoch [13769/20000], Loss: -20.968017578125, Learning Rate: 0.01\n",
      "Epoch [13770/20000], Loss: -20.9671630859375, Learning Rate: 0.01\n",
      "Epoch [13771/20000], Loss: -20.965728759765625, Learning Rate: 0.01\n",
      "Epoch [13772/20000], Loss: -20.963394165039062, Learning Rate: 0.01\n",
      "Epoch [13773/20000], Loss: -20.959381103515625, Learning Rate: 0.01\n",
      "Epoch [13774/20000], Loss: -20.953262329101562, Learning Rate: 0.01\n",
      "Epoch [13775/20000], Loss: -20.943344116210938, Learning Rate: 0.01\n",
      "Epoch [13776/20000], Loss: -20.927627563476562, Learning Rate: 0.01\n",
      "Epoch [13777/20000], Loss: -20.902862548828125, Learning Rate: 0.01\n",
      "Epoch [13778/20000], Loss: -20.863861083984375, Learning Rate: 0.01\n",
      "Epoch [13779/20000], Loss: -20.802780151367188, Learning Rate: 0.01\n",
      "Epoch [13780/20000], Loss: -20.708892822265625, Learning Rate: 0.01\n",
      "Epoch [13781/20000], Loss: -20.566925048828125, Learning Rate: 0.01\n",
      "Epoch [13782/20000], Loss: -20.362442016601562, Learning Rate: 0.01\n",
      "Epoch [13783/20000], Loss: -20.084259033203125, Learning Rate: 0.01\n",
      "Epoch [13784/20000], Loss: -19.75421142578125, Learning Rate: 0.01\n",
      "Epoch [13785/20000], Loss: -19.434173583984375, Learning Rate: 0.01\n",
      "Epoch [13786/20000], Loss: -19.273147583007812, Learning Rate: 0.01\n",
      "Epoch [13787/20000], Loss: -19.406784057617188, Learning Rate: 0.01\n",
      "Epoch [13788/20000], Loss: -19.881378173828125, Learning Rate: 0.01\n",
      "Epoch [13789/20000], Loss: -20.485000610351562, Learning Rate: 0.01\n",
      "Epoch [13790/20000], Loss: -20.902801513671875, Learning Rate: 0.01\n",
      "Epoch [13791/20000], Loss: -20.943191528320312, Learning Rate: 0.01\n",
      "Epoch [13792/20000], Loss: -20.687149047851562, Learning Rate: 0.01\n",
      "Epoch [13793/20000], Loss: -20.3953857421875, Learning Rate: 0.01\n",
      "Epoch [13794/20000], Loss: -20.309478759765625, Learning Rate: 0.01\n",
      "Epoch [13795/20000], Loss: -20.493911743164062, Learning Rate: 0.01\n",
      "Epoch [13796/20000], Loss: -20.788604736328125, Learning Rate: 0.01\n",
      "Epoch [13797/20000], Loss: -20.96551513671875, Learning Rate: 0.01\n",
      "Epoch [13798/20000], Loss: -20.924514770507812, Learning Rate: 0.01\n",
      "Epoch [13799/20000], Loss: -20.760086059570312, Learning Rate: 0.01\n",
      "Epoch [13800/20000], Loss: -20.649856567382812, Learning Rate: 0.01\n",
      "Epoch [13801/20000], Loss: -20.69366455078125, Learning Rate: 0.01\n",
      "Epoch [13802/20000], Loss: -20.842971801757812, Learning Rate: 0.01\n",
      "Epoch [13803/20000], Loss: -20.962753295898438, Learning Rate: 0.01\n",
      "Epoch [13804/20000], Loss: -20.966293334960938, Learning Rate: 0.01\n",
      "Epoch [13805/20000], Loss: -20.8834228515625, Learning Rate: 0.01\n",
      "Epoch [13806/20000], Loss: -20.813064575195312, Learning Rate: 0.01\n",
      "Epoch [13807/20000], Loss: -20.824203491210938, Learning Rate: 0.01\n",
      "Epoch [13808/20000], Loss: -20.90008544921875, Learning Rate: 0.01\n",
      "Epoch [13809/20000], Loss: -20.968460083007812, Learning Rate: 0.01\n",
      "Epoch [13810/20000], Loss: -20.976333618164062, Learning Rate: 0.01\n",
      "Epoch [13811/20000], Loss: -20.9344482421875, Learning Rate: 0.01\n",
      "Epoch [13812/20000], Loss: -20.895294189453125, Learning Rate: 0.01\n",
      "Epoch [13813/20000], Loss: -20.8983154296875, Learning Rate: 0.01\n",
      "Epoch [13814/20000], Loss: -20.937591552734375, Learning Rate: 0.01\n",
      "Epoch [13815/20000], Loss: -20.975021362304688, Learning Rate: 0.01\n",
      "Epoch [13816/20000], Loss: -20.981048583984375, Learning Rate: 0.01\n",
      "Epoch [13817/20000], Loss: -20.95953369140625, Learning Rate: 0.01\n",
      "Epoch [13818/20000], Loss: -20.937881469726562, Learning Rate: 0.01\n",
      "Epoch [13819/20000], Loss: -20.938125610351562, Learning Rate: 0.01\n",
      "Epoch [13820/20000], Loss: -20.958343505859375, Learning Rate: 0.01\n",
      "Epoch [13821/20000], Loss: -20.979217529296875, Learning Rate: 0.01\n",
      "Epoch [13822/20000], Loss: -20.984222412109375, Learning Rate: 0.01\n",
      "Epoch [13823/20000], Loss: -20.973953247070312, Learning Rate: 0.01\n",
      "Epoch [13824/20000], Loss: -20.9619140625, Learning Rate: 0.01\n",
      "Epoch [13825/20000], Loss: -20.960540771484375, Learning Rate: 0.01\n",
      "Epoch [13826/20000], Loss: -20.9703369140625, Learning Rate: 0.01\n",
      "Epoch [13827/20000], Loss: -20.981964111328125, Learning Rate: 0.01\n",
      "Epoch [13828/20000], Loss: -20.98614501953125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13829/20000], Loss: -20.981689453125, Learning Rate: 0.01\n",
      "Epoch [13830/20000], Loss: -20.97509765625, Learning Rate: 0.01\n",
      "Epoch [13831/20000], Loss: -20.9732666015625, Learning Rate: 0.01\n",
      "Epoch [13832/20000], Loss: -20.9776611328125, Learning Rate: 0.01\n",
      "Epoch [13833/20000], Loss: -20.984283447265625, Learning Rate: 0.01\n",
      "Epoch [13834/20000], Loss: -20.9876708984375, Learning Rate: 0.01\n",
      "Epoch [13835/20000], Loss: -20.986419677734375, Learning Rate: 0.01\n",
      "Epoch [13836/20000], Loss: -20.982925415039062, Learning Rate: 0.01\n",
      "Epoch [13837/20000], Loss: -20.98114013671875, Learning Rate: 0.01\n",
      "Epoch [13838/20000], Loss: -20.982742309570312, Learning Rate: 0.01\n",
      "Epoch [13839/20000], Loss: -20.9862060546875, Learning Rate: 0.01\n",
      "Epoch [13840/20000], Loss: -20.988845825195312, Learning Rate: 0.01\n",
      "Epoch [13841/20000], Loss: -20.989044189453125, Learning Rate: 0.01\n",
      "Epoch [13842/20000], Loss: -20.987533569335938, Learning Rate: 0.01\n",
      "Epoch [13843/20000], Loss: -20.986312866210938, Learning Rate: 0.01\n",
      "Epoch [13844/20000], Loss: -20.986526489257812, Learning Rate: 0.01\n",
      "Epoch [13845/20000], Loss: -20.988128662109375, Learning Rate: 0.01\n",
      "Epoch [13846/20000], Loss: -20.989990234375, Learning Rate: 0.01\n",
      "Epoch [13847/20000], Loss: -20.990737915039062, Learning Rate: 0.01\n",
      "Epoch [13848/20000], Loss: -20.990463256835938, Learning Rate: 0.01\n",
      "Epoch [13849/20000], Loss: -20.989776611328125, Learning Rate: 0.01\n",
      "Epoch [13850/20000], Loss: -20.989532470703125, Learning Rate: 0.01\n",
      "Epoch [13851/20000], Loss: -20.990188598632812, Learning Rate: 0.01\n",
      "Epoch [13852/20000], Loss: -20.991241455078125, Learning Rate: 0.01\n",
      "Epoch [13853/20000], Loss: -20.992050170898438, Learning Rate: 0.01\n",
      "Epoch [13854/20000], Loss: -20.992416381835938, Learning Rate: 0.01\n",
      "Epoch [13855/20000], Loss: -20.992172241210938, Learning Rate: 0.01\n",
      "Epoch [13856/20000], Loss: -20.992019653320312, Learning Rate: 0.01\n",
      "Epoch [13857/20000], Loss: -20.992156982421875, Learning Rate: 0.01\n",
      "Epoch [13858/20000], Loss: -20.99267578125, Learning Rate: 0.01\n",
      "Epoch [13859/20000], Loss: -20.993331909179688, Learning Rate: 0.01\n",
      "Epoch [13860/20000], Loss: -20.993759155273438, Learning Rate: 0.01\n",
      "Epoch [13861/20000], Loss: -20.99407958984375, Learning Rate: 0.01\n",
      "Epoch [13862/20000], Loss: -20.993988037109375, Learning Rate: 0.01\n",
      "Epoch [13863/20000], Loss: -20.994049072265625, Learning Rate: 0.01\n",
      "Epoch [13864/20000], Loss: -20.994277954101562, Learning Rate: 0.01\n",
      "Epoch [13865/20000], Loss: -20.994552612304688, Learning Rate: 0.01\n",
      "Epoch [13866/20000], Loss: -20.99505615234375, Learning Rate: 0.01\n",
      "Epoch [13867/20000], Loss: -20.995407104492188, Learning Rate: 0.01\n",
      "Epoch [13868/20000], Loss: -20.99560546875, Learning Rate: 0.01\n",
      "Epoch [13869/20000], Loss: -20.995742797851562, Learning Rate: 0.01\n",
      "Epoch [13870/20000], Loss: -20.995895385742188, Learning Rate: 0.01\n",
      "Epoch [13871/20000], Loss: -20.99609375, Learning Rate: 0.01\n",
      "Epoch [13872/20000], Loss: -20.996414184570312, Learning Rate: 0.01\n",
      "Epoch [13873/20000], Loss: -20.996688842773438, Learning Rate: 0.01\n",
      "Epoch [13874/20000], Loss: -20.99700927734375, Learning Rate: 0.01\n",
      "Epoch [13875/20000], Loss: -20.997238159179688, Learning Rate: 0.01\n",
      "Epoch [13876/20000], Loss: -20.997421264648438, Learning Rate: 0.01\n",
      "Epoch [13877/20000], Loss: -20.99755859375, Learning Rate: 0.01\n",
      "Epoch [13878/20000], Loss: -20.997756958007812, Learning Rate: 0.01\n",
      "Epoch [13879/20000], Loss: -20.998031616210938, Learning Rate: 0.01\n",
      "Epoch [13880/20000], Loss: -20.998336791992188, Learning Rate: 0.01\n",
      "Epoch [13881/20000], Loss: -20.998611450195312, Learning Rate: 0.01\n",
      "Epoch [13882/20000], Loss: -20.998809814453125, Learning Rate: 0.01\n",
      "Epoch [13883/20000], Loss: -20.998992919921875, Learning Rate: 0.01\n",
      "Epoch [13884/20000], Loss: -20.999191284179688, Learning Rate: 0.01\n",
      "Epoch [13885/20000], Loss: -20.999359130859375, Learning Rate: 0.01\n",
      "Epoch [13886/20000], Loss: -20.999649047851562, Learning Rate: 0.01\n",
      "Epoch [13887/20000], Loss: -20.99981689453125, Learning Rate: 0.01\n",
      "Epoch [13888/20000], Loss: -21.000015258789062, Learning Rate: 0.01\n",
      "Epoch [13889/20000], Loss: -21.00018310546875, Learning Rate: 0.01\n",
      "Epoch [13890/20000], Loss: -21.000335693359375, Learning Rate: 0.01\n",
      "Epoch [13891/20000], Loss: -21.00048828125, Learning Rate: 0.01\n",
      "Epoch [13892/20000], Loss: -21.000564575195312, Learning Rate: 0.01\n",
      "Epoch [13893/20000], Loss: -21.000564575195312, Learning Rate: 0.01\n",
      "Epoch [13894/20000], Loss: -21.00067138671875, Learning Rate: 0.01\n",
      "Epoch [13895/20000], Loss: -21.000503540039062, Learning Rate: 0.01\n",
      "Epoch [13896/20000], Loss: -21.000274658203125, Learning Rate: 0.01\n",
      "Epoch [13897/20000], Loss: -20.99957275390625, Learning Rate: 0.01\n",
      "Epoch [13898/20000], Loss: -20.998748779296875, Learning Rate: 0.01\n",
      "Epoch [13899/20000], Loss: -20.997299194335938, Learning Rate: 0.01\n",
      "Epoch [13900/20000], Loss: -20.994873046875, Learning Rate: 0.01\n",
      "Epoch [13901/20000], Loss: -20.9912109375, Learning Rate: 0.01\n",
      "Epoch [13902/20000], Loss: -20.985519409179688, Learning Rate: 0.01\n",
      "Epoch [13903/20000], Loss: -20.976669311523438, Learning Rate: 0.01\n",
      "Epoch [13904/20000], Loss: -20.963058471679688, Learning Rate: 0.01\n",
      "Epoch [13905/20000], Loss: -20.9417724609375, Learning Rate: 0.01\n",
      "Epoch [13906/20000], Loss: -20.908905029296875, Learning Rate: 0.01\n",
      "Epoch [13907/20000], Loss: -20.85772705078125, Learning Rate: 0.01\n",
      "Epoch [13908/20000], Loss: -20.779266357421875, Learning Rate: 0.01\n",
      "Epoch [13909/20000], Loss: -20.6580810546875, Learning Rate: 0.01\n",
      "Epoch [13910/20000], Loss: -20.4781494140625, Learning Rate: 0.01\n",
      "Epoch [13911/20000], Loss: -20.21148681640625, Learning Rate: 0.01\n",
      "Epoch [13912/20000], Loss: -19.850448608398438, Learning Rate: 0.01\n",
      "Epoch [13913/20000], Loss: -19.382110595703125, Learning Rate: 0.01\n",
      "Epoch [13914/20000], Loss: -18.900360107421875, Learning Rate: 0.01\n",
      "Epoch [13915/20000], Loss: -18.51226806640625, Learning Rate: 0.01\n",
      "Epoch [13916/20000], Loss: -18.506683349609375, Learning Rate: 0.01\n",
      "Epoch [13917/20000], Loss: -18.962173461914062, Learning Rate: 0.01\n",
      "Epoch [13918/20000], Loss: -19.823257446289062, Learning Rate: 0.01\n",
      "Epoch [13919/20000], Loss: -20.634735107421875, Learning Rate: 0.01\n",
      "Epoch [13920/20000], Loss: -21.001907348632812, Learning Rate: 0.01\n",
      "Epoch [13921/20000], Loss: -20.829147338867188, Learning Rate: 0.01\n",
      "Epoch [13922/20000], Loss: -20.37567138671875, Learning Rate: 0.01\n",
      "Epoch [13923/20000], Loss: -20.041091918945312, Learning Rate: 0.01\n",
      "Epoch [13924/20000], Loss: -20.071014404296875, Learning Rate: 0.01\n",
      "Epoch [13925/20000], Loss: -20.442413330078125, Learning Rate: 0.01\n",
      "Epoch [13926/20000], Loss: -20.848907470703125, Learning Rate: 0.01\n",
      "Epoch [13927/20000], Loss: -21.008865356445312, Learning Rate: 0.01\n",
      "Epoch [13928/20000], Loss: -20.872817993164062, Learning Rate: 0.01\n",
      "Epoch [13929/20000], Loss: -20.62945556640625, Learning Rate: 0.01\n",
      "Epoch [13930/20000], Loss: -20.5186767578125, Learning Rate: 0.01\n",
      "Epoch [13931/20000], Loss: -20.62518310546875, Learning Rate: 0.01\n",
      "Epoch [13932/20000], Loss: -20.847702026367188, Learning Rate: 0.01\n",
      "Epoch [13933/20000], Loss: -20.997283935546875, Learning Rate: 0.01\n",
      "Epoch [13934/20000], Loss: -20.9805908203125, Learning Rate: 0.01\n",
      "Epoch [13935/20000], Loss: -20.858001708984375, Learning Rate: 0.01\n",
      "Epoch [13936/20000], Loss: -20.7645263671875, Learning Rate: 0.01\n",
      "Epoch [13937/20000], Loss: -20.786376953125, Learning Rate: 0.01\n",
      "Epoch [13938/20000], Loss: -20.89373779296875, Learning Rate: 0.01\n",
      "Epoch [13939/20000], Loss: -20.990936279296875, Learning Rate: 0.01\n",
      "Epoch [13940/20000], Loss: -21.00689697265625, Learning Rate: 0.01\n",
      "Epoch [13941/20000], Loss: -20.9515380859375, Learning Rate: 0.01\n",
      "Epoch [13942/20000], Loss: -20.892379760742188, Learning Rate: 0.01\n",
      "Epoch [13943/20000], Loss: -20.8858642578125, Learning Rate: 0.01\n",
      "Epoch [13944/20000], Loss: -20.93402099609375, Learning Rate: 0.01\n",
      "Epoch [13945/20000], Loss: -20.99127197265625, Learning Rate: 0.01\n",
      "Epoch [13946/20000], Loss: -21.013198852539062, Learning Rate: 0.01\n",
      "Epoch [13947/20000], Loss: -20.992462158203125, Learning Rate: 0.01\n",
      "Epoch [13948/20000], Loss: -20.9583740234375, Learning Rate: 0.01\n",
      "Epoch [13949/20000], Loss: -20.945083618164062, Learning Rate: 0.01\n",
      "Epoch [13950/20000], Loss: -20.96258544921875, Learning Rate: 0.01\n",
      "Epoch [13951/20000], Loss: -20.993850708007812, Learning Rate: 0.01\n",
      "Epoch [13952/20000], Loss: -21.013275146484375, Learning Rate: 0.01\n",
      "Epoch [13953/20000], Loss: -21.00982666015625, Learning Rate: 0.01\n",
      "Epoch [13954/20000], Loss: -20.992752075195312, Learning Rate: 0.01\n",
      "Epoch [13955/20000], Loss: -20.980270385742188, Learning Rate: 0.01\n",
      "Epoch [13956/20000], Loss: -20.983184814453125, Learning Rate: 0.01\n",
      "Epoch [13957/20000], Loss: -20.997787475585938, Learning Rate: 0.01\n",
      "Epoch [13958/20000], Loss: -21.011886596679688, Learning Rate: 0.01\n",
      "Epoch [13959/20000], Loss: -21.015762329101562, Learning Rate: 0.01\n",
      "Epoch [13960/20000], Loss: -21.0096435546875, Learning Rate: 0.01\n",
      "Epoch [13961/20000], Loss: -21.001174926757812, Learning Rate: 0.01\n",
      "Epoch [13962/20000], Loss: -20.998245239257812, Learning Rate: 0.01\n",
      "Epoch [13963/20000], Loss: -21.003005981445312, Learning Rate: 0.01\n",
      "Epoch [13964/20000], Loss: -21.01116943359375, Learning Rate: 0.01\n",
      "Epoch [13965/20000], Loss: -21.016708374023438, Learning Rate: 0.01\n",
      "Epoch [13966/20000], Loss: -21.0166015625, Learning Rate: 0.01\n",
      "Epoch [13967/20000], Loss: -21.0126953125, Learning Rate: 0.01\n",
      "Epoch [13968/20000], Loss: -21.009231567382812, Learning Rate: 0.01\n",
      "Epoch [13969/20000], Loss: -21.00909423828125, Learning Rate: 0.01\n",
      "Epoch [13970/20000], Loss: -21.012283325195312, Learning Rate: 0.01\n",
      "Epoch [13971/20000], Loss: -21.01629638671875, Learning Rate: 0.01\n",
      "Epoch [13972/20000], Loss: -21.01861572265625, Learning Rate: 0.01\n",
      "Epoch [13973/20000], Loss: -21.018173217773438, Learning Rate: 0.01\n",
      "Epoch [13974/20000], Loss: -21.016326904296875, Learning Rate: 0.01\n",
      "Epoch [13975/20000], Loss: -21.014816284179688, Learning Rate: 0.01\n",
      "Epoch [13976/20000], Loss: -21.01507568359375, Learning Rate: 0.01\n",
      "Epoch [13977/20000], Loss: -21.016769409179688, Learning Rate: 0.01\n",
      "Epoch [13978/20000], Loss: -21.018829345703125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13979/20000], Loss: -21.020050048828125, Learning Rate: 0.01\n",
      "Epoch [13980/20000], Loss: -21.019973754882812, Learning Rate: 0.01\n",
      "Epoch [13981/20000], Loss: -21.01922607421875, Learning Rate: 0.01\n",
      "Epoch [13982/20000], Loss: -21.01861572265625, Learning Rate: 0.01\n",
      "Epoch [13983/20000], Loss: -21.0186767578125, Learning Rate: 0.01\n",
      "Epoch [13984/20000], Loss: -21.019515991210938, Learning Rate: 0.01\n",
      "Epoch [13985/20000], Loss: -21.0206298828125, Learning Rate: 0.01\n",
      "Epoch [13986/20000], Loss: -21.021377563476562, Learning Rate: 0.01\n",
      "Epoch [13987/20000], Loss: -21.021636962890625, Learning Rate: 0.01\n",
      "Epoch [13988/20000], Loss: -21.021514892578125, Learning Rate: 0.01\n",
      "Epoch [13989/20000], Loss: -21.021209716796875, Learning Rate: 0.01\n",
      "Epoch [13990/20000], Loss: -21.021224975585938, Learning Rate: 0.01\n",
      "Epoch [13991/20000], Loss: -21.021682739257812, Learning Rate: 0.01\n",
      "Epoch [13992/20000], Loss: -21.022247314453125, Learning Rate: 0.01\n",
      "Epoch [13993/20000], Loss: -21.02276611328125, Learning Rate: 0.01\n",
      "Epoch [13994/20000], Loss: -21.02313232421875, Learning Rate: 0.01\n",
      "Epoch [13995/20000], Loss: -21.023208618164062, Learning Rate: 0.01\n",
      "Epoch [13996/20000], Loss: -21.023208618164062, Learning Rate: 0.01\n",
      "Epoch [13997/20000], Loss: -21.023208618164062, Learning Rate: 0.01\n",
      "Epoch [13998/20000], Loss: -21.023391723632812, Learning Rate: 0.01\n",
      "Epoch [13999/20000], Loss: -21.023712158203125, Learning Rate: 0.01\n",
      "Epoch [14000/20000], Loss: -21.024124145507812, Learning Rate: 0.01\n",
      "Epoch [14001/20000], Loss: -21.024490356445312, Learning Rate: 0.01\n",
      "Epoch [14002/20000], Loss: -21.024703979492188, Learning Rate: 0.01\n",
      "Epoch [14003/20000], Loss: -21.024826049804688, Learning Rate: 0.01\n",
      "Epoch [14004/20000], Loss: -21.024932861328125, Learning Rate: 0.01\n",
      "Epoch [14005/20000], Loss: -21.025009155273438, Learning Rate: 0.01\n",
      "Epoch [14006/20000], Loss: -21.025238037109375, Learning Rate: 0.01\n",
      "Epoch [14007/20000], Loss: -21.0255126953125, Learning Rate: 0.01\n",
      "Epoch [14008/20000], Loss: -21.025833129882812, Learning Rate: 0.01\n",
      "Epoch [14009/20000], Loss: -21.026016235351562, Learning Rate: 0.01\n",
      "Epoch [14010/20000], Loss: -21.026229858398438, Learning Rate: 0.01\n",
      "Epoch [14011/20000], Loss: -21.02642822265625, Learning Rate: 0.01\n",
      "Epoch [14012/20000], Loss: -21.026611328125, Learning Rate: 0.01\n",
      "Epoch [14013/20000], Loss: -21.02679443359375, Learning Rate: 0.01\n",
      "Epoch [14014/20000], Loss: -21.026885986328125, Learning Rate: 0.01\n",
      "Epoch [14015/20000], Loss: -21.027175903320312, Learning Rate: 0.01\n",
      "Epoch [14016/20000], Loss: -21.027389526367188, Learning Rate: 0.01\n",
      "Epoch [14017/20000], Loss: -21.027572631835938, Learning Rate: 0.01\n",
      "Epoch [14018/20000], Loss: -21.027847290039062, Learning Rate: 0.01\n",
      "Epoch [14019/20000], Loss: -21.028076171875, Learning Rate: 0.01\n",
      "Epoch [14020/20000], Loss: -21.02825927734375, Learning Rate: 0.01\n",
      "Epoch [14021/20000], Loss: -21.0284423828125, Learning Rate: 0.01\n",
      "Epoch [14022/20000], Loss: -21.02862548828125, Learning Rate: 0.01\n",
      "Epoch [14023/20000], Loss: -21.028839111328125, Learning Rate: 0.01\n",
      "Epoch [14024/20000], Loss: -21.029006958007812, Learning Rate: 0.01\n",
      "Epoch [14025/20000], Loss: -21.029220581054688, Learning Rate: 0.01\n",
      "Epoch [14026/20000], Loss: -21.029464721679688, Learning Rate: 0.01\n",
      "Epoch [14027/20000], Loss: -21.029632568359375, Learning Rate: 0.01\n",
      "Epoch [14028/20000], Loss: -21.02984619140625, Learning Rate: 0.01\n",
      "Epoch [14029/20000], Loss: -21.029953002929688, Learning Rate: 0.01\n",
      "Epoch [14030/20000], Loss: -21.030227661132812, Learning Rate: 0.01\n",
      "Epoch [14031/20000], Loss: -21.030441284179688, Learning Rate: 0.01\n",
      "Epoch [14032/20000], Loss: -21.030532836914062, Learning Rate: 0.01\n",
      "Epoch [14033/20000], Loss: -21.030746459960938, Learning Rate: 0.01\n",
      "Epoch [14034/20000], Loss: -21.030990600585938, Learning Rate: 0.01\n",
      "Epoch [14035/20000], Loss: -21.031143188476562, Learning Rate: 0.01\n",
      "Epoch [14036/20000], Loss: -21.031402587890625, Learning Rate: 0.01\n",
      "Epoch [14037/20000], Loss: -21.0316162109375, Learning Rate: 0.01\n",
      "Epoch [14038/20000], Loss: -21.031814575195312, Learning Rate: 0.01\n",
      "Epoch [14039/20000], Loss: -21.031997680664062, Learning Rate: 0.01\n",
      "Epoch [14040/20000], Loss: -21.032150268554688, Learning Rate: 0.01\n",
      "Epoch [14041/20000], Loss: -21.0323486328125, Learning Rate: 0.01\n",
      "Epoch [14042/20000], Loss: -21.032516479492188, Learning Rate: 0.01\n",
      "Epoch [14043/20000], Loss: -21.032730102539062, Learning Rate: 0.01\n",
      "Epoch [14044/20000], Loss: -21.03289794921875, Learning Rate: 0.01\n",
      "Epoch [14045/20000], Loss: -21.033050537109375, Learning Rate: 0.01\n",
      "Epoch [14046/20000], Loss: -21.033248901367188, Learning Rate: 0.01\n",
      "Epoch [14047/20000], Loss: -21.033477783203125, Learning Rate: 0.01\n",
      "Epoch [14048/20000], Loss: -21.03363037109375, Learning Rate: 0.01\n",
      "Epoch [14049/20000], Loss: -21.03387451171875, Learning Rate: 0.01\n",
      "Epoch [14050/20000], Loss: -21.034042358398438, Learning Rate: 0.01\n",
      "Epoch [14051/20000], Loss: -21.034286499023438, Learning Rate: 0.01\n",
      "Epoch [14052/20000], Loss: -21.034500122070312, Learning Rate: 0.01\n",
      "Epoch [14053/20000], Loss: -21.03460693359375, Learning Rate: 0.01\n",
      "Epoch [14054/20000], Loss: -21.034835815429688, Learning Rate: 0.01\n",
      "Epoch [14055/20000], Loss: -21.035003662109375, Learning Rate: 0.01\n",
      "Epoch [14056/20000], Loss: -21.035202026367188, Learning Rate: 0.01\n",
      "Epoch [14057/20000], Loss: -21.035400390625, Learning Rate: 0.01\n",
      "Epoch [14058/20000], Loss: -21.035629272460938, Learning Rate: 0.01\n",
      "Epoch [14059/20000], Loss: -21.035888671875, Learning Rate: 0.01\n",
      "Epoch [14060/20000], Loss: -21.03594970703125, Learning Rate: 0.01\n",
      "Epoch [14061/20000], Loss: -21.03619384765625, Learning Rate: 0.01\n",
      "Epoch [14062/20000], Loss: -21.036361694335938, Learning Rate: 0.01\n",
      "Epoch [14063/20000], Loss: -21.036575317382812, Learning Rate: 0.01\n",
      "Epoch [14064/20000], Loss: -21.036727905273438, Learning Rate: 0.01\n",
      "Epoch [14065/20000], Loss: -21.03692626953125, Learning Rate: 0.01\n",
      "Epoch [14066/20000], Loss: -21.037139892578125, Learning Rate: 0.01\n",
      "Epoch [14067/20000], Loss: -21.037322998046875, Learning Rate: 0.01\n",
      "Epoch [14068/20000], Loss: -21.037490844726562, Learning Rate: 0.01\n",
      "Epoch [14069/20000], Loss: -21.037689208984375, Learning Rate: 0.01\n",
      "Epoch [14070/20000], Loss: -21.037887573242188, Learning Rate: 0.01\n",
      "Epoch [14071/20000], Loss: -21.038101196289062, Learning Rate: 0.01\n",
      "Epoch [14072/20000], Loss: -21.038284301757812, Learning Rate: 0.01\n",
      "Epoch [14073/20000], Loss: -21.038482666015625, Learning Rate: 0.01\n",
      "Epoch [14074/20000], Loss: -21.038650512695312, Learning Rate: 0.01\n",
      "Epoch [14075/20000], Loss: -21.03887939453125, Learning Rate: 0.01\n",
      "Epoch [14076/20000], Loss: -21.039016723632812, Learning Rate: 0.01\n",
      "Epoch [14077/20000], Loss: -21.039337158203125, Learning Rate: 0.01\n",
      "Epoch [14078/20000], Loss: -21.039443969726562, Learning Rate: 0.01\n",
      "Epoch [14079/20000], Loss: -21.039642333984375, Learning Rate: 0.01\n",
      "Epoch [14080/20000], Loss: -21.039794921875, Learning Rate: 0.01\n",
      "Epoch [14081/20000], Loss: -21.040054321289062, Learning Rate: 0.01\n",
      "Epoch [14082/20000], Loss: -21.04010009765625, Learning Rate: 0.01\n",
      "Epoch [14083/20000], Loss: -21.040374755859375, Learning Rate: 0.01\n",
      "Epoch [14084/20000], Loss: -21.040603637695312, Learning Rate: 0.01\n",
      "Epoch [14085/20000], Loss: -21.040786743164062, Learning Rate: 0.01\n",
      "Epoch [14086/20000], Loss: -21.040939331054688, Learning Rate: 0.01\n",
      "Epoch [14087/20000], Loss: -21.041168212890625, Learning Rate: 0.01\n",
      "Epoch [14088/20000], Loss: -21.041366577148438, Learning Rate: 0.01\n",
      "Epoch [14089/20000], Loss: -21.041488647460938, Learning Rate: 0.01\n",
      "Epoch [14090/20000], Loss: -21.041763305664062, Learning Rate: 0.01\n",
      "Epoch [14091/20000], Loss: -21.04193115234375, Learning Rate: 0.01\n",
      "Epoch [14092/20000], Loss: -21.04205322265625, Learning Rate: 0.01\n",
      "Epoch [14093/20000], Loss: -21.04229736328125, Learning Rate: 0.01\n",
      "Epoch [14094/20000], Loss: -21.042449951171875, Learning Rate: 0.01\n",
      "Epoch [14095/20000], Loss: -21.042617797851562, Learning Rate: 0.01\n",
      "Epoch [14096/20000], Loss: -21.042816162109375, Learning Rate: 0.01\n",
      "Epoch [14097/20000], Loss: -21.043014526367188, Learning Rate: 0.01\n",
      "Epoch [14098/20000], Loss: -21.043243408203125, Learning Rate: 0.01\n",
      "Epoch [14099/20000], Loss: -21.043380737304688, Learning Rate: 0.01\n",
      "Epoch [14100/20000], Loss: -21.04351806640625, Learning Rate: 0.01\n",
      "Epoch [14101/20000], Loss: -21.043701171875, Learning Rate: 0.01\n",
      "Epoch [14102/20000], Loss: -21.0439453125, Learning Rate: 0.01\n",
      "Epoch [14103/20000], Loss: -21.044158935546875, Learning Rate: 0.01\n",
      "Epoch [14104/20000], Loss: -21.044342041015625, Learning Rate: 0.01\n",
      "Epoch [14105/20000], Loss: -21.044525146484375, Learning Rate: 0.01\n",
      "Epoch [14106/20000], Loss: -21.044754028320312, Learning Rate: 0.01\n",
      "Epoch [14107/20000], Loss: -21.044876098632812, Learning Rate: 0.01\n",
      "Epoch [14108/20000], Loss: -21.045089721679688, Learning Rate: 0.01\n",
      "Epoch [14109/20000], Loss: -21.045303344726562, Learning Rate: 0.01\n",
      "Epoch [14110/20000], Loss: -21.045425415039062, Learning Rate: 0.01\n",
      "Epoch [14111/20000], Loss: -21.045623779296875, Learning Rate: 0.01\n",
      "Epoch [14112/20000], Loss: -21.045791625976562, Learning Rate: 0.01\n",
      "Epoch [14113/20000], Loss: -21.046035766601562, Learning Rate: 0.01\n",
      "Epoch [14114/20000], Loss: -21.046157836914062, Learning Rate: 0.01\n",
      "Epoch [14115/20000], Loss: -21.046401977539062, Learning Rate: 0.01\n",
      "Epoch [14116/20000], Loss: -21.046493530273438, Learning Rate: 0.01\n",
      "Epoch [14117/20000], Loss: -21.046661376953125, Learning Rate: 0.01\n",
      "Epoch [14118/20000], Loss: -21.04681396484375, Learning Rate: 0.01\n",
      "Epoch [14119/20000], Loss: -21.046951293945312, Learning Rate: 0.01\n",
      "Epoch [14120/20000], Loss: -21.046951293945312, Learning Rate: 0.01\n",
      "Epoch [14121/20000], Loss: -21.047042846679688, Learning Rate: 0.01\n",
      "Epoch [14122/20000], Loss: -21.04693603515625, Learning Rate: 0.01\n",
      "Epoch [14123/20000], Loss: -21.046737670898438, Learning Rate: 0.01\n",
      "Epoch [14124/20000], Loss: -21.046432495117188, Learning Rate: 0.01\n",
      "Epoch [14125/20000], Loss: -21.045745849609375, Learning Rate: 0.01\n",
      "Epoch [14126/20000], Loss: -21.0445556640625, Learning Rate: 0.01\n",
      "Epoch [14127/20000], Loss: -21.042816162109375, Learning Rate: 0.01\n",
      "Epoch [14128/20000], Loss: -21.039657592773438, Learning Rate: 0.01\n",
      "Epoch [14129/20000], Loss: -21.034637451171875, Learning Rate: 0.01\n",
      "Epoch [14130/20000], Loss: -21.026763916015625, Learning Rate: 0.01\n",
      "Epoch [14131/20000], Loss: -21.013870239257812, Learning Rate: 0.01\n",
      "Epoch [14132/20000], Loss: -20.9932861328125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14133/20000], Loss: -20.960037231445312, Learning Rate: 0.01\n",
      "Epoch [14134/20000], Loss: -20.907058715820312, Learning Rate: 0.01\n",
      "Epoch [14135/20000], Loss: -20.82183837890625, Learning Rate: 0.01\n",
      "Epoch [14136/20000], Loss: -20.68902587890625, Learning Rate: 0.01\n",
      "Epoch [14137/20000], Loss: -20.48150634765625, Learning Rate: 0.01\n",
      "Epoch [14138/20000], Loss: -20.17974853515625, Learning Rate: 0.01\n",
      "Epoch [14139/20000], Loss: -19.752349853515625, Learning Rate: 0.01\n",
      "Epoch [14140/20000], Loss: -19.24530029296875, Learning Rate: 0.01\n",
      "Epoch [14141/20000], Loss: -18.718826293945312, Learning Rate: 0.01\n",
      "Epoch [14142/20000], Loss: -18.446975708007812, Learning Rate: 0.01\n",
      "Epoch [14143/20000], Loss: -18.561187744140625, Learning Rate: 0.01\n",
      "Epoch [14144/20000], Loss: -19.171768188476562, Learning Rate: 0.01\n",
      "Epoch [14145/20000], Loss: -19.878524780273438, Learning Rate: 0.01\n",
      "Epoch [14146/20000], Loss: -20.273788452148438, Learning Rate: 0.01\n",
      "Epoch [14147/20000], Loss: -20.138992309570312, Learning Rate: 0.01\n",
      "Epoch [14148/20000], Loss: -19.729171752929688, Learning Rate: 0.01\n",
      "Epoch [14149/20000], Loss: -19.549407958984375, Learning Rate: 0.01\n",
      "Epoch [14150/20000], Loss: -19.92388916015625, Learning Rate: 0.01\n",
      "Epoch [14151/20000], Loss: -20.622528076171875, Learning Rate: 0.01\n",
      "Epoch [14152/20000], Loss: -21.040359497070312, Learning Rate: 0.01\n",
      "Epoch [14153/20000], Loss: -20.891769409179688, Learning Rate: 0.01\n",
      "Epoch [14154/20000], Loss: -20.481613159179688, Learning Rate: 0.01\n",
      "Epoch [14155/20000], Loss: -20.310760498046875, Learning Rate: 0.01\n",
      "Epoch [14156/20000], Loss: -20.543685913085938, Learning Rate: 0.01\n",
      "Epoch [14157/20000], Loss: -20.8719482421875, Learning Rate: 0.01\n",
      "Epoch [14158/20000], Loss: -20.938980102539062, Learning Rate: 0.01\n",
      "Epoch [14159/20000], Loss: -20.765960693359375, Learning Rate: 0.01\n",
      "Epoch [14160/20000], Loss: -20.653839111328125, Learning Rate: 0.01\n",
      "Epoch [14161/20000], Loss: -20.779647827148438, Learning Rate: 0.01\n",
      "Epoch [14162/20000], Loss: -20.988250732421875, Learning Rate: 0.01\n",
      "Epoch [14163/20000], Loss: -21.039337158203125, Learning Rate: 0.01\n",
      "Epoch [14164/20000], Loss: -20.912322998046875, Learning Rate: 0.01\n",
      "Epoch [14165/20000], Loss: -20.79913330078125, Learning Rate: 0.01\n",
      "Epoch [14166/20000], Loss: -20.842803955078125, Learning Rate: 0.01\n",
      "Epoch [14167/20000], Loss: -20.971817016601562, Learning Rate: 0.01\n",
      "Epoch [14168/20000], Loss: -21.032150268554688, Learning Rate: 0.01\n",
      "Epoch [14169/20000], Loss: -20.984848022460938, Learning Rate: 0.01\n",
      "Epoch [14170/20000], Loss: -20.930038452148438, Learning Rate: 0.01\n",
      "Epoch [14171/20000], Loss: -20.953506469726562, Learning Rate: 0.01\n",
      "Epoch [14172/20000], Loss: -21.021865844726562, Learning Rate: 0.01\n",
      "Epoch [14173/20000], Loss: -21.047348022460938, Learning Rate: 0.01\n",
      "Epoch [14174/20000], Loss: -21.008621215820312, Learning Rate: 0.01\n",
      "Epoch [14175/20000], Loss: -20.967727661132812, Learning Rate: 0.01\n",
      "Epoch [14176/20000], Loss: -20.979812622070312, Learning Rate: 0.01\n",
      "Epoch [14177/20000], Loss: -21.026931762695312, Learning Rate: 0.01\n",
      "Epoch [14178/20000], Loss: -21.053253173828125, Learning Rate: 0.01\n",
      "Epoch [14179/20000], Loss: -21.038986206054688, Learning Rate: 0.01\n",
      "Epoch [14180/20000], Loss: -21.016464233398438, Learning Rate: 0.01\n",
      "Epoch [14181/20000], Loss: -21.019790649414062, Learning Rate: 0.01\n",
      "Epoch [14182/20000], Loss: -21.042861938476562, Learning Rate: 0.01\n",
      "Epoch [14183/20000], Loss: -21.055068969726562, Learning Rate: 0.01\n",
      "Epoch [14184/20000], Loss: -21.04443359375, Learning Rate: 0.01\n",
      "Epoch [14185/20000], Loss: -21.029190063476562, Learning Rate: 0.01\n",
      "Epoch [14186/20000], Loss: -21.03057861328125, Learning Rate: 0.01\n",
      "Epoch [14187/20000], Loss: -21.046371459960938, Learning Rate: 0.01\n",
      "Epoch [14188/20000], Loss: -21.058120727539062, Learning Rate: 0.01\n",
      "Epoch [14189/20000], Loss: -21.055633544921875, Learning Rate: 0.01\n",
      "Epoch [14190/20000], Loss: -21.047332763671875, Learning Rate: 0.01\n",
      "Epoch [14191/20000], Loss: -21.046218872070312, Learning Rate: 0.01\n",
      "Epoch [14192/20000], Loss: -21.05340576171875, Learning Rate: 0.01\n",
      "Epoch [14193/20000], Loss: -21.05950927734375, Learning Rate: 0.01\n",
      "Epoch [14194/20000], Loss: -21.057952880859375, Learning Rate: 0.01\n",
      "Epoch [14195/20000], Loss: -21.052322387695312, Learning Rate: 0.01\n",
      "Epoch [14196/20000], Loss: -21.050613403320312, Learning Rate: 0.01\n",
      "Epoch [14197/20000], Loss: -21.054794311523438, Learning Rate: 0.01\n",
      "Epoch [14198/20000], Loss: -21.060043334960938, Learning Rate: 0.01\n",
      "Epoch [14199/20000], Loss: -21.061172485351562, Learning Rate: 0.01\n",
      "Epoch [14200/20000], Loss: -21.058792114257812, Learning Rate: 0.01\n",
      "Epoch [14201/20000], Loss: -21.057373046875, Learning Rate: 0.01\n",
      "Epoch [14202/20000], Loss: -21.059097290039062, Learning Rate: 0.01\n",
      "Epoch [14203/20000], Loss: -21.062026977539062, Learning Rate: 0.01\n",
      "Epoch [14204/20000], Loss: -21.063003540039062, Learning Rate: 0.01\n",
      "Epoch [14205/20000], Loss: -21.061508178710938, Learning Rate: 0.01\n",
      "Epoch [14206/20000], Loss: -21.059967041015625, Learning Rate: 0.01\n",
      "Epoch [14207/20000], Loss: -21.060302734375, Learning Rate: 0.01\n",
      "Epoch [14208/20000], Loss: -21.062057495117188, Learning Rate: 0.01\n",
      "Epoch [14209/20000], Loss: -21.063339233398438, Learning Rate: 0.01\n",
      "Epoch [14210/20000], Loss: -21.063217163085938, Learning Rate: 0.01\n",
      "Epoch [14211/20000], Loss: -21.06256103515625, Learning Rate: 0.01\n",
      "Epoch [14212/20000], Loss: -21.062591552734375, Learning Rate: 0.01\n",
      "Epoch [14213/20000], Loss: -21.063568115234375, Learning Rate: 0.01\n",
      "Epoch [14214/20000], Loss: -21.064712524414062, Learning Rate: 0.01\n",
      "Epoch [14215/20000], Loss: -21.064956665039062, Learning Rate: 0.01\n",
      "Epoch [14216/20000], Loss: -21.064605712890625, Learning Rate: 0.01\n",
      "Epoch [14217/20000], Loss: -21.064346313476562, Learning Rate: 0.01\n",
      "Epoch [14218/20000], Loss: -21.064605712890625, Learning Rate: 0.01\n",
      "Epoch [14219/20000], Loss: -21.06536865234375, Learning Rate: 0.01\n",
      "Epoch [14220/20000], Loss: -21.065658569335938, Learning Rate: 0.01\n",
      "Epoch [14221/20000], Loss: -21.065658569335938, Learning Rate: 0.01\n",
      "Epoch [14222/20000], Loss: -21.06549072265625, Learning Rate: 0.01\n",
      "Epoch [14223/20000], Loss: -21.065673828125, Learning Rate: 0.01\n",
      "Epoch [14224/20000], Loss: -21.066009521484375, Learning Rate: 0.01\n",
      "Epoch [14225/20000], Loss: -21.06640625, Learning Rate: 0.01\n",
      "Epoch [14226/20000], Loss: -21.066650390625, Learning Rate: 0.01\n",
      "Epoch [14227/20000], Loss: -21.066665649414062, Learning Rate: 0.01\n",
      "Epoch [14228/20000], Loss: -21.066726684570312, Learning Rate: 0.01\n",
      "Epoch [14229/20000], Loss: -21.067001342773438, Learning Rate: 0.01\n",
      "Epoch [14230/20000], Loss: -21.067245483398438, Learning Rate: 0.01\n",
      "Epoch [14231/20000], Loss: -21.06756591796875, Learning Rate: 0.01\n",
      "Epoch [14232/20000], Loss: -21.0677490234375, Learning Rate: 0.01\n",
      "Epoch [14233/20000], Loss: -21.067794799804688, Learning Rate: 0.01\n",
      "Epoch [14234/20000], Loss: -21.06793212890625, Learning Rate: 0.01\n",
      "Epoch [14235/20000], Loss: -21.068115234375, Learning Rate: 0.01\n",
      "Epoch [14236/20000], Loss: -21.068405151367188, Learning Rate: 0.01\n",
      "Epoch [14237/20000], Loss: -21.068572998046875, Learning Rate: 0.01\n",
      "Epoch [14238/20000], Loss: -21.06878662109375, Learning Rate: 0.01\n",
      "Epoch [14239/20000], Loss: -21.068801879882812, Learning Rate: 0.01\n",
      "Epoch [14240/20000], Loss: -21.068984985351562, Learning Rate: 0.01\n",
      "Epoch [14241/20000], Loss: -21.069183349609375, Learning Rate: 0.01\n",
      "Epoch [14242/20000], Loss: -21.069412231445312, Learning Rate: 0.01\n",
      "Epoch [14243/20000], Loss: -21.069534301757812, Learning Rate: 0.01\n",
      "Epoch [14244/20000], Loss: -21.069671630859375, Learning Rate: 0.01\n",
      "Epoch [14245/20000], Loss: -21.069793701171875, Learning Rate: 0.01\n",
      "Epoch [14246/20000], Loss: -21.0699462890625, Learning Rate: 0.01\n",
      "Epoch [14247/20000], Loss: -21.070098876953125, Learning Rate: 0.01\n",
      "Epoch [14248/20000], Loss: -21.0703125, Learning Rate: 0.01\n",
      "Epoch [14249/20000], Loss: -21.070465087890625, Learning Rate: 0.01\n",
      "Epoch [14250/20000], Loss: -21.070632934570312, Learning Rate: 0.01\n",
      "Epoch [14251/20000], Loss: -21.070755004882812, Learning Rate: 0.01\n",
      "Epoch [14252/20000], Loss: -21.0709228515625, Learning Rate: 0.01\n",
      "Epoch [14253/20000], Loss: -21.071060180664062, Learning Rate: 0.01\n",
      "Epoch [14254/20000], Loss: -21.071182250976562, Learning Rate: 0.01\n",
      "Epoch [14255/20000], Loss: -21.071395874023438, Learning Rate: 0.01\n",
      "Epoch [14256/20000], Loss: -21.071533203125, Learning Rate: 0.01\n",
      "Epoch [14257/20000], Loss: -21.071533203125, Learning Rate: 0.01\n",
      "Epoch [14258/20000], Loss: -21.071624755859375, Learning Rate: 0.01\n",
      "Epoch [14259/20000], Loss: -21.071792602539062, Learning Rate: 0.01\n",
      "Epoch [14260/20000], Loss: -21.071853637695312, Learning Rate: 0.01\n",
      "Epoch [14261/20000], Loss: -21.071792602539062, Learning Rate: 0.01\n",
      "Epoch [14262/20000], Loss: -21.07177734375, Learning Rate: 0.01\n",
      "Epoch [14263/20000], Loss: -21.071533203125, Learning Rate: 0.01\n",
      "Epoch [14264/20000], Loss: -21.071212768554688, Learning Rate: 0.01\n",
      "Epoch [14265/20000], Loss: -21.070785522460938, Learning Rate: 0.01\n",
      "Epoch [14266/20000], Loss: -21.069976806640625, Learning Rate: 0.01\n",
      "Epoch [14267/20000], Loss: -21.0687255859375, Learning Rate: 0.01\n",
      "Epoch [14268/20000], Loss: -21.066940307617188, Learning Rate: 0.01\n",
      "Epoch [14269/20000], Loss: -21.064193725585938, Learning Rate: 0.01\n",
      "Epoch [14270/20000], Loss: -21.059951782226562, Learning Rate: 0.01\n",
      "Epoch [14271/20000], Loss: -21.053558349609375, Learning Rate: 0.01\n",
      "Epoch [14272/20000], Loss: -21.043960571289062, Learning Rate: 0.01\n",
      "Epoch [14273/20000], Loss: -21.029281616210938, Learning Rate: 0.01\n",
      "Epoch [14274/20000], Loss: -21.007156372070312, Learning Rate: 0.01\n",
      "Epoch [14275/20000], Loss: -20.973236083984375, Learning Rate: 0.01\n",
      "Epoch [14276/20000], Loss: -20.922012329101562, Learning Rate: 0.01\n",
      "Epoch [14277/20000], Loss: -20.84393310546875, Learning Rate: 0.01\n",
      "Epoch [14278/20000], Loss: -20.728134155273438, Learning Rate: 0.01\n",
      "Epoch [14279/20000], Loss: -20.55548095703125, Learning Rate: 0.01\n",
      "Epoch [14280/20000], Loss: -20.313461303710938, Learning Rate: 0.01\n",
      "Epoch [14281/20000], Loss: -19.979827880859375, Learning Rate: 0.01\n",
      "Epoch [14282/20000], Loss: -19.580230712890625, Learning Rate: 0.01\n",
      "Epoch [14283/20000], Loss: -19.14630126953125, Learning Rate: 0.01\n",
      "Epoch [14284/20000], Loss: -18.84918212890625, Learning Rate: 0.01\n",
      "Epoch [14285/20000], Loss: -18.817245483398438, Learning Rate: 0.01\n",
      "Epoch [14286/20000], Loss: -19.2330322265625, Learning Rate: 0.01\n",
      "Epoch [14287/20000], Loss: -19.9493408203125, Learning Rate: 0.01\n",
      "Epoch [14288/20000], Loss: -20.67730712890625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14289/20000], Loss: -21.055770874023438, Learning Rate: 0.01\n",
      "Epoch [14290/20000], Loss: -20.97686767578125, Learning Rate: 0.01\n",
      "Epoch [14291/20000], Loss: -20.615020751953125, Learning Rate: 0.01\n",
      "Epoch [14292/20000], Loss: -20.276870727539062, Learning Rate: 0.01\n",
      "Epoch [14293/20000], Loss: -20.218124389648438, Learning Rate: 0.01\n",
      "Epoch [14294/20000], Loss: -20.459121704101562, Learning Rate: 0.01\n",
      "Epoch [14295/20000], Loss: -20.824951171875, Learning Rate: 0.01\n",
      "Epoch [14296/20000], Loss: -21.056365966796875, Learning Rate: 0.01\n",
      "Epoch [14297/20000], Loss: -21.037109375, Learning Rate: 0.01\n",
      "Epoch [14298/20000], Loss: -20.851119995117188, Learning Rate: 0.01\n",
      "Epoch [14299/20000], Loss: -20.684967041015625, Learning Rate: 0.01\n",
      "Epoch [14300/20000], Loss: -20.679718017578125, Learning Rate: 0.01\n",
      "Epoch [14301/20000], Loss: -20.824752807617188, Learning Rate: 0.01\n",
      "Epoch [14302/20000], Loss: -21.000473022460938, Learning Rate: 0.01\n",
      "Epoch [14303/20000], Loss: -21.078857421875, Learning Rate: 0.01\n",
      "Epoch [14304/20000], Loss: -21.030685424804688, Learning Rate: 0.01\n",
      "Epoch [14305/20000], Loss: -20.928359985351562, Learning Rate: 0.01\n",
      "Epoch [14306/20000], Loss: -20.87017822265625, Learning Rate: 0.01\n",
      "Epoch [14307/20000], Loss: -20.902969360351562, Learning Rate: 0.01\n",
      "Epoch [14308/20000], Loss: -20.991867065429688, Learning Rate: 0.01\n",
      "Epoch [14309/20000], Loss: -21.0655517578125, Learning Rate: 0.01\n",
      "Epoch [14310/20000], Loss: -21.076217651367188, Learning Rate: 0.01\n",
      "Epoch [14311/20000], Loss: -21.033721923828125, Learning Rate: 0.01\n",
      "Epoch [14312/20000], Loss: -20.986373901367188, Learning Rate: 0.01\n",
      "Epoch [14313/20000], Loss: -20.975997924804688, Learning Rate: 0.01\n",
      "Epoch [14314/20000], Loss: -21.00830078125, Learning Rate: 0.01\n",
      "Epoch [14315/20000], Loss: -21.0543212890625, Learning Rate: 0.01\n",
      "Epoch [14316/20000], Loss: -21.08013916015625, Learning Rate: 0.01\n",
      "Epoch [14317/20000], Loss: -21.0732421875, Learning Rate: 0.01\n",
      "Epoch [14318/20000], Loss: -21.047988891601562, Learning Rate: 0.01\n",
      "Epoch [14319/20000], Loss: -21.029449462890625, Learning Rate: 0.01\n",
      "Epoch [14320/20000], Loss: -21.032272338867188, Learning Rate: 0.01\n",
      "Epoch [14321/20000], Loss: -21.05267333984375, Learning Rate: 0.01\n",
      "Epoch [14322/20000], Loss: -21.07403564453125, Learning Rate: 0.01\n",
      "Epoch [14323/20000], Loss: -21.082244873046875, Learning Rate: 0.01\n",
      "Epoch [14324/20000], Loss: -21.075485229492188, Learning Rate: 0.01\n",
      "Epoch [14325/20000], Loss: -21.062774658203125, Learning Rate: 0.01\n",
      "Epoch [14326/20000], Loss: -21.0557861328125, Learning Rate: 0.01\n",
      "Epoch [14327/20000], Loss: -21.059417724609375, Learning Rate: 0.01\n",
      "Epoch [14328/20000], Loss: -21.070220947265625, Learning Rate: 0.01\n",
      "Epoch [14329/20000], Loss: -21.080093383789062, Learning Rate: 0.01\n",
      "Epoch [14330/20000], Loss: -21.083221435546875, Learning Rate: 0.01\n",
      "Epoch [14331/20000], Loss: -21.07952880859375, Learning Rate: 0.01\n",
      "Epoch [14332/20000], Loss: -21.073562622070312, Learning Rate: 0.01\n",
      "Epoch [14333/20000], Loss: -21.070419311523438, Learning Rate: 0.01\n",
      "Epoch [14334/20000], Loss: -21.072280883789062, Learning Rate: 0.01\n",
      "Epoch [14335/20000], Loss: -21.077529907226562, Learning Rate: 0.01\n",
      "Epoch [14336/20000], Loss: -21.082427978515625, Learning Rate: 0.01\n",
      "Epoch [14337/20000], Loss: -21.084335327148438, Learning Rate: 0.01\n",
      "Epoch [14338/20000], Loss: -21.082916259765625, Learning Rate: 0.01\n",
      "Epoch [14339/20000], Loss: -21.080184936523438, Learning Rate: 0.01\n",
      "Epoch [14340/20000], Loss: -21.078414916992188, Learning Rate: 0.01\n",
      "Epoch [14341/20000], Loss: -21.078903198242188, Learning Rate: 0.01\n",
      "Epoch [14342/20000], Loss: -21.0811767578125, Learning Rate: 0.01\n",
      "Epoch [14343/20000], Loss: -21.08380126953125, Learning Rate: 0.01\n",
      "Epoch [14344/20000], Loss: -21.085281372070312, Learning Rate: 0.01\n",
      "Epoch [14345/20000], Loss: -21.085174560546875, Learning Rate: 0.01\n",
      "Epoch [14346/20000], Loss: -21.084060668945312, Learning Rate: 0.01\n",
      "Epoch [14347/20000], Loss: -21.083099365234375, Learning Rate: 0.01\n",
      "Epoch [14348/20000], Loss: -21.082901000976562, Learning Rate: 0.01\n",
      "Epoch [14349/20000], Loss: -21.083694458007812, Learning Rate: 0.01\n",
      "Epoch [14350/20000], Loss: -21.08502197265625, Learning Rate: 0.01\n",
      "Epoch [14351/20000], Loss: -21.086044311523438, Learning Rate: 0.01\n",
      "Epoch [14352/20000], Loss: -21.086532592773438, Learning Rate: 0.01\n",
      "Epoch [14353/20000], Loss: -21.086318969726562, Learning Rate: 0.01\n",
      "Epoch [14354/20000], Loss: -21.085891723632812, Learning Rate: 0.01\n",
      "Epoch [14355/20000], Loss: -21.085525512695312, Learning Rate: 0.01\n",
      "Epoch [14356/20000], Loss: -21.085693359375, Learning Rate: 0.01\n",
      "Epoch [14357/20000], Loss: -21.086181640625, Learning Rate: 0.01\n",
      "Epoch [14358/20000], Loss: -21.086837768554688, Learning Rate: 0.01\n",
      "Epoch [14359/20000], Loss: -21.087326049804688, Learning Rate: 0.01\n",
      "Epoch [14360/20000], Loss: -21.087646484375, Learning Rate: 0.01\n",
      "Epoch [14361/20000], Loss: -21.087615966796875, Learning Rate: 0.01\n",
      "Epoch [14362/20000], Loss: -21.087615966796875, Learning Rate: 0.01\n",
      "Epoch [14363/20000], Loss: -21.087493896484375, Learning Rate: 0.01\n",
      "Epoch [14364/20000], Loss: -21.08758544921875, Learning Rate: 0.01\n",
      "Epoch [14365/20000], Loss: -21.087860107421875, Learning Rate: 0.01\n",
      "Epoch [14366/20000], Loss: -21.088241577148438, Learning Rate: 0.01\n",
      "Epoch [14367/20000], Loss: -21.088577270507812, Learning Rate: 0.01\n",
      "Epoch [14368/20000], Loss: -21.0887451171875, Learning Rate: 0.01\n",
      "Epoch [14369/20000], Loss: -21.088882446289062, Learning Rate: 0.01\n",
      "Epoch [14370/20000], Loss: -21.089080810546875, Learning Rate: 0.01\n",
      "Epoch [14371/20000], Loss: -21.089004516601562, Learning Rate: 0.01\n",
      "Epoch [14372/20000], Loss: -21.089080810546875, Learning Rate: 0.01\n",
      "Epoch [14373/20000], Loss: -21.08929443359375, Learning Rate: 0.01\n",
      "Epoch [14374/20000], Loss: -21.089447021484375, Learning Rate: 0.01\n",
      "Epoch [14375/20000], Loss: -21.089675903320312, Learning Rate: 0.01\n",
      "Epoch [14376/20000], Loss: -21.089859008789062, Learning Rate: 0.01\n",
      "Epoch [14377/20000], Loss: -21.090042114257812, Learning Rate: 0.01\n",
      "Epoch [14378/20000], Loss: -21.090240478515625, Learning Rate: 0.01\n",
      "Epoch [14379/20000], Loss: -21.090347290039062, Learning Rate: 0.01\n",
      "Epoch [14380/20000], Loss: -21.090484619140625, Learning Rate: 0.01\n",
      "Epoch [14381/20000], Loss: -21.090591430664062, Learning Rate: 0.01\n",
      "Epoch [14382/20000], Loss: -21.090713500976562, Learning Rate: 0.01\n",
      "Epoch [14383/20000], Loss: -21.090866088867188, Learning Rate: 0.01\n",
      "Epoch [14384/20000], Loss: -21.091079711914062, Learning Rate: 0.01\n",
      "Epoch [14385/20000], Loss: -21.09124755859375, Learning Rate: 0.01\n",
      "Epoch [14386/20000], Loss: -21.091415405273438, Learning Rate: 0.01\n",
      "Epoch [14387/20000], Loss: -21.091506958007812, Learning Rate: 0.01\n",
      "Epoch [14388/20000], Loss: -21.091598510742188, Learning Rate: 0.01\n",
      "Epoch [14389/20000], Loss: -21.091766357421875, Learning Rate: 0.01\n",
      "Epoch [14390/20000], Loss: -21.0919189453125, Learning Rate: 0.01\n",
      "Epoch [14391/20000], Loss: -21.092010498046875, Learning Rate: 0.01\n",
      "Epoch [14392/20000], Loss: -21.092254638671875, Learning Rate: 0.01\n",
      "Epoch [14393/20000], Loss: -21.09234619140625, Learning Rate: 0.01\n",
      "Epoch [14394/20000], Loss: -21.092483520507812, Learning Rate: 0.01\n",
      "Epoch [14395/20000], Loss: -21.092666625976562, Learning Rate: 0.01\n",
      "Epoch [14396/20000], Loss: -21.0927734375, Learning Rate: 0.01\n",
      "Epoch [14397/20000], Loss: -21.092926025390625, Learning Rate: 0.01\n",
      "Epoch [14398/20000], Loss: -21.093109130859375, Learning Rate: 0.01\n",
      "Epoch [14399/20000], Loss: -21.093185424804688, Learning Rate: 0.01\n",
      "Epoch [14400/20000], Loss: -21.09344482421875, Learning Rate: 0.01\n",
      "Epoch [14401/20000], Loss: -21.093475341796875, Learning Rate: 0.01\n",
      "Epoch [14402/20000], Loss: -21.093597412109375, Learning Rate: 0.01\n",
      "Epoch [14403/20000], Loss: -21.093765258789062, Learning Rate: 0.01\n",
      "Epoch [14404/20000], Loss: -21.093887329101562, Learning Rate: 0.01\n",
      "Epoch [14405/20000], Loss: -21.094009399414062, Learning Rate: 0.01\n",
      "Epoch [14406/20000], Loss: -21.094192504882812, Learning Rate: 0.01\n",
      "Epoch [14407/20000], Loss: -21.094375610351562, Learning Rate: 0.01\n",
      "Epoch [14408/20000], Loss: -21.094497680664062, Learning Rate: 0.01\n",
      "Epoch [14409/20000], Loss: -21.094650268554688, Learning Rate: 0.01\n",
      "Epoch [14410/20000], Loss: -21.094772338867188, Learning Rate: 0.01\n",
      "Epoch [14411/20000], Loss: -21.094924926757812, Learning Rate: 0.01\n",
      "Epoch [14412/20000], Loss: -21.095016479492188, Learning Rate: 0.01\n",
      "Epoch [14413/20000], Loss: -21.095230102539062, Learning Rate: 0.01\n",
      "Epoch [14414/20000], Loss: -21.095382690429688, Learning Rate: 0.01\n",
      "Epoch [14415/20000], Loss: -21.095504760742188, Learning Rate: 0.01\n",
      "Epoch [14416/20000], Loss: -21.095611572265625, Learning Rate: 0.01\n",
      "Epoch [14417/20000], Loss: -21.095748901367188, Learning Rate: 0.01\n",
      "Epoch [14418/20000], Loss: -21.095901489257812, Learning Rate: 0.01\n",
      "Epoch [14419/20000], Loss: -21.09600830078125, Learning Rate: 0.01\n",
      "Epoch [14420/20000], Loss: -21.09613037109375, Learning Rate: 0.01\n",
      "Epoch [14421/20000], Loss: -21.0963134765625, Learning Rate: 0.01\n",
      "Epoch [14422/20000], Loss: -21.096450805664062, Learning Rate: 0.01\n",
      "Epoch [14423/20000], Loss: -21.096527099609375, Learning Rate: 0.01\n",
      "Epoch [14424/20000], Loss: -21.096710205078125, Learning Rate: 0.01\n",
      "Epoch [14425/20000], Loss: -21.09686279296875, Learning Rate: 0.01\n",
      "Epoch [14426/20000], Loss: -21.09698486328125, Learning Rate: 0.01\n",
      "Epoch [14427/20000], Loss: -21.097259521484375, Learning Rate: 0.01\n",
      "Epoch [14428/20000], Loss: -21.097244262695312, Learning Rate: 0.01\n",
      "Epoch [14429/20000], Loss: -21.097427368164062, Learning Rate: 0.01\n",
      "Epoch [14430/20000], Loss: -21.097579956054688, Learning Rate: 0.01\n",
      "Epoch [14431/20000], Loss: -21.097686767578125, Learning Rate: 0.01\n",
      "Epoch [14432/20000], Loss: -21.097869873046875, Learning Rate: 0.01\n",
      "Epoch [14433/20000], Loss: -21.0980224609375, Learning Rate: 0.01\n",
      "Epoch [14434/20000], Loss: -21.098037719726562, Learning Rate: 0.01\n",
      "Epoch [14435/20000], Loss: -21.09820556640625, Learning Rate: 0.01\n",
      "Epoch [14436/20000], Loss: -21.098388671875, Learning Rate: 0.01\n",
      "Epoch [14437/20000], Loss: -21.098587036132812, Learning Rate: 0.01\n",
      "Epoch [14438/20000], Loss: -21.098663330078125, Learning Rate: 0.01\n",
      "Epoch [14439/20000], Loss: -21.098800659179688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14440/20000], Loss: -21.098983764648438, Learning Rate: 0.01\n",
      "Epoch [14441/20000], Loss: -21.099075317382812, Learning Rate: 0.01\n",
      "Epoch [14442/20000], Loss: -21.0992431640625, Learning Rate: 0.01\n",
      "Epoch [14443/20000], Loss: -21.099395751953125, Learning Rate: 0.01\n",
      "Epoch [14444/20000], Loss: -21.099533081054688, Learning Rate: 0.01\n",
      "Epoch [14445/20000], Loss: -21.099655151367188, Learning Rate: 0.01\n",
      "Epoch [14446/20000], Loss: -21.099746704101562, Learning Rate: 0.01\n",
      "Epoch [14447/20000], Loss: -21.09991455078125, Learning Rate: 0.01\n",
      "Epoch [14448/20000], Loss: -21.100051879882812, Learning Rate: 0.01\n",
      "Epoch [14449/20000], Loss: -21.10015869140625, Learning Rate: 0.01\n",
      "Epoch [14450/20000], Loss: -21.100341796875, Learning Rate: 0.01\n",
      "Epoch [14451/20000], Loss: -21.100479125976562, Learning Rate: 0.01\n",
      "Epoch [14452/20000], Loss: -21.10064697265625, Learning Rate: 0.01\n",
      "Epoch [14453/20000], Loss: -21.100753784179688, Learning Rate: 0.01\n",
      "Epoch [14454/20000], Loss: -21.100936889648438, Learning Rate: 0.01\n",
      "Epoch [14455/20000], Loss: -21.100982666015625, Learning Rate: 0.01\n",
      "Epoch [14456/20000], Loss: -21.1011962890625, Learning Rate: 0.01\n",
      "Epoch [14457/20000], Loss: -21.101303100585938, Learning Rate: 0.01\n",
      "Epoch [14458/20000], Loss: -21.101516723632812, Learning Rate: 0.01\n",
      "Epoch [14459/20000], Loss: -21.101608276367188, Learning Rate: 0.01\n",
      "Epoch [14460/20000], Loss: -21.10174560546875, Learning Rate: 0.01\n",
      "Epoch [14461/20000], Loss: -21.101898193359375, Learning Rate: 0.01\n",
      "Epoch [14462/20000], Loss: -21.101959228515625, Learning Rate: 0.01\n",
      "Epoch [14463/20000], Loss: -21.102096557617188, Learning Rate: 0.01\n",
      "Epoch [14464/20000], Loss: -21.102264404296875, Learning Rate: 0.01\n",
      "Epoch [14465/20000], Loss: -21.10235595703125, Learning Rate: 0.01\n",
      "Epoch [14466/20000], Loss: -21.102371215820312, Learning Rate: 0.01\n",
      "Epoch [14467/20000], Loss: -21.102493286132812, Learning Rate: 0.01\n",
      "Epoch [14468/20000], Loss: -21.10247802734375, Learning Rate: 0.01\n",
      "Epoch [14469/20000], Loss: -21.102508544921875, Learning Rate: 0.01\n",
      "Epoch [14470/20000], Loss: -21.102386474609375, Learning Rate: 0.01\n",
      "Epoch [14471/20000], Loss: -21.1021728515625, Learning Rate: 0.01\n",
      "Epoch [14472/20000], Loss: -21.101882934570312, Learning Rate: 0.01\n",
      "Epoch [14473/20000], Loss: -21.101226806640625, Learning Rate: 0.01\n",
      "Epoch [14474/20000], Loss: -21.1002197265625, Learning Rate: 0.01\n",
      "Epoch [14475/20000], Loss: -21.09869384765625, Learning Rate: 0.01\n",
      "Epoch [14476/20000], Loss: -21.096054077148438, Learning Rate: 0.01\n",
      "Epoch [14477/20000], Loss: -21.092086791992188, Learning Rate: 0.01\n",
      "Epoch [14478/20000], Loss: -21.08563232421875, Learning Rate: 0.01\n",
      "Epoch [14479/20000], Loss: -21.075469970703125, Learning Rate: 0.01\n",
      "Epoch [14480/20000], Loss: -21.059463500976562, Learning Rate: 0.01\n",
      "Epoch [14481/20000], Loss: -21.034042358398438, Learning Rate: 0.01\n",
      "Epoch [14482/20000], Loss: -20.99334716796875, Learning Rate: 0.01\n",
      "Epoch [14483/20000], Loss: -20.929168701171875, Learning Rate: 0.01\n",
      "Epoch [14484/20000], Loss: -20.826797485351562, Learning Rate: 0.01\n",
      "Epoch [14485/20000], Loss: -20.66839599609375, Learning Rate: 0.01\n",
      "Epoch [14486/20000], Loss: -20.422943115234375, Learning Rate: 0.01\n",
      "Epoch [14487/20000], Loss: -20.068206787109375, Learning Rate: 0.01\n",
      "Epoch [14488/20000], Loss: -19.570236206054688, Learning Rate: 0.01\n",
      "Epoch [14489/20000], Loss: -18.9862060546875, Learning Rate: 0.01\n",
      "Epoch [14490/20000], Loss: -18.399581909179688, Learning Rate: 0.01\n",
      "Epoch [14491/20000], Loss: -18.145751953125, Learning Rate: 0.01\n",
      "Epoch [14492/20000], Loss: -18.4180908203125, Learning Rate: 0.01\n",
      "Epoch [14493/20000], Loss: -19.336318969726562, Learning Rate: 0.01\n",
      "Epoch [14494/20000], Loss: -20.406463623046875, Learning Rate: 0.01\n",
      "Epoch [14495/20000], Loss: -21.048934936523438, Learning Rate: 0.01\n",
      "Epoch [14496/20000], Loss: -20.974609375, Learning Rate: 0.01\n",
      "Epoch [14497/20000], Loss: -20.435546875, Learning Rate: 0.01\n",
      "Epoch [14498/20000], Loss: -19.971878051757812, Learning Rate: 0.01\n",
      "Epoch [14499/20000], Loss: -19.968582153320312, Learning Rate: 0.01\n",
      "Epoch [14500/20000], Loss: -20.431747436523438, Learning Rate: 0.01\n",
      "Epoch [14501/20000], Loss: -20.938873291015625, Learning Rate: 0.01\n",
      "Epoch [14502/20000], Loss: -21.100311279296875, Learning Rate: 0.01\n",
      "Epoch [14503/20000], Loss: -20.880340576171875, Learning Rate: 0.01\n",
      "Epoch [14504/20000], Loss: -20.577194213867188, Learning Rate: 0.01\n",
      "Epoch [14505/20000], Loss: -20.512100219726562, Learning Rate: 0.01\n",
      "Epoch [14506/20000], Loss: -20.7274169921875, Learning Rate: 0.01\n",
      "Epoch [14507/20000], Loss: -21.008987426757812, Learning Rate: 0.01\n",
      "Epoch [14508/20000], Loss: -21.106658935546875, Learning Rate: 0.01\n",
      "Epoch [14509/20000], Loss: -20.986831665039062, Learning Rate: 0.01\n",
      "Epoch [14510/20000], Loss: -20.822860717773438, Learning Rate: 0.01\n",
      "Epoch [14511/20000], Loss: -20.792800903320312, Learning Rate: 0.01\n",
      "Epoch [14512/20000], Loss: -20.919204711914062, Learning Rate: 0.01\n",
      "Epoch [14513/20000], Loss: -21.066650390625, Learning Rate: 0.01\n",
      "Epoch [14514/20000], Loss: -21.105560302734375, Learning Rate: 0.01\n",
      "Epoch [14515/20000], Loss: -21.031692504882812, Learning Rate: 0.01\n",
      "Epoch [14516/20000], Loss: -20.947021484375, Learning Rate: 0.01\n",
      "Epoch [14517/20000], Loss: -20.944442749023438, Learning Rate: 0.01\n",
      "Epoch [14518/20000], Loss: -21.01898193359375, Learning Rate: 0.01\n",
      "Epoch [14519/20000], Loss: -21.094131469726562, Learning Rate: 0.01\n",
      "Epoch [14520/20000], Loss: -21.105178833007812, Learning Rate: 0.01\n",
      "Epoch [14521/20000], Loss: -21.0604248046875, Learning Rate: 0.01\n",
      "Epoch [14522/20000], Loss: -21.018524169921875, Learning Rate: 0.01\n",
      "Epoch [14523/20000], Loss: -21.023193359375, Learning Rate: 0.01\n",
      "Epoch [14524/20000], Loss: -21.066482543945312, Learning Rate: 0.01\n",
      "Epoch [14525/20000], Loss: -21.104629516601562, Learning Rate: 0.01\n",
      "Epoch [14526/20000], Loss: -21.107070922851562, Learning Rate: 0.01\n",
      "Epoch [14527/20000], Loss: -21.081787109375, Learning Rate: 0.01\n",
      "Epoch [14528/20000], Loss: -21.0604248046875, Learning Rate: 0.01\n",
      "Epoch [14529/20000], Loss: -21.064727783203125, Learning Rate: 0.01\n",
      "Epoch [14530/20000], Loss: -21.0882568359375, Learning Rate: 0.01\n",
      "Epoch [14531/20000], Loss: -21.108367919921875, Learning Rate: 0.01\n",
      "Epoch [14532/20000], Loss: -21.10943603515625, Learning Rate: 0.01\n",
      "Epoch [14533/20000], Loss: -21.096038818359375, Learning Rate: 0.01\n",
      "Epoch [14534/20000], Loss: -21.084640502929688, Learning Rate: 0.01\n",
      "Epoch [14535/20000], Loss: -21.086502075195312, Learning Rate: 0.01\n",
      "Epoch [14536/20000], Loss: -21.098953247070312, Learning Rate: 0.01\n",
      "Epoch [14537/20000], Loss: -21.110076904296875, Learning Rate: 0.01\n",
      "Epoch [14538/20000], Loss: -21.111587524414062, Learning Rate: 0.01\n",
      "Epoch [14539/20000], Loss: -21.104995727539062, Learning Rate: 0.01\n",
      "Epoch [14540/20000], Loss: -21.098480224609375, Learning Rate: 0.01\n",
      "Epoch [14541/20000], Loss: -21.09844970703125, Learning Rate: 0.01\n",
      "Epoch [14542/20000], Loss: -21.104598999023438, Learning Rate: 0.01\n",
      "Epoch [14543/20000], Loss: -21.111053466796875, Learning Rate: 0.01\n",
      "Epoch [14544/20000], Loss: -21.11297607421875, Learning Rate: 0.01\n",
      "Epoch [14545/20000], Loss: -21.110260009765625, Learning Rate: 0.01\n",
      "Epoch [14546/20000], Loss: -21.106552124023438, Learning Rate: 0.01\n",
      "Epoch [14547/20000], Loss: -21.1055908203125, Learning Rate: 0.01\n",
      "Epoch [14548/20000], Loss: -21.108139038085938, Learning Rate: 0.01\n",
      "Epoch [14549/20000], Loss: -21.111785888671875, Learning Rate: 0.01\n",
      "Epoch [14550/20000], Loss: -21.11376953125, Learning Rate: 0.01\n",
      "Epoch [14551/20000], Loss: -21.113143920898438, Learning Rate: 0.01\n",
      "Epoch [14552/20000], Loss: -21.11126708984375, Learning Rate: 0.01\n",
      "Epoch [14553/20000], Loss: -21.110137939453125, Learning Rate: 0.01\n",
      "Epoch [14554/20000], Loss: -21.1107177734375, Learning Rate: 0.01\n",
      "Epoch [14555/20000], Loss: -21.112655639648438, Learning Rate: 0.01\n",
      "Epoch [14556/20000], Loss: -21.114227294921875, Learning Rate: 0.01\n",
      "Epoch [14557/20000], Loss: -21.114654541015625, Learning Rate: 0.01\n",
      "Epoch [14558/20000], Loss: -21.113937377929688, Learning Rate: 0.01\n",
      "Epoch [14559/20000], Loss: -21.113143920898438, Learning Rate: 0.01\n",
      "Epoch [14560/20000], Loss: -21.112945556640625, Learning Rate: 0.01\n",
      "Epoch [14561/20000], Loss: -21.1136474609375, Learning Rate: 0.01\n",
      "Epoch [14562/20000], Loss: -21.11468505859375, Learning Rate: 0.01\n",
      "Epoch [14563/20000], Loss: -21.115386962890625, Learning Rate: 0.01\n",
      "Epoch [14564/20000], Loss: -21.115463256835938, Learning Rate: 0.01\n",
      "Epoch [14565/20000], Loss: -21.115081787109375, Learning Rate: 0.01\n",
      "Epoch [14566/20000], Loss: -21.114791870117188, Learning Rate: 0.01\n",
      "Epoch [14567/20000], Loss: -21.114883422851562, Learning Rate: 0.01\n",
      "Epoch [14568/20000], Loss: -21.115371704101562, Learning Rate: 0.01\n",
      "Epoch [14569/20000], Loss: -21.11590576171875, Learning Rate: 0.01\n",
      "Epoch [14570/20000], Loss: -21.116241455078125, Learning Rate: 0.01\n",
      "Epoch [14571/20000], Loss: -21.11639404296875, Learning Rate: 0.01\n",
      "Epoch [14572/20000], Loss: -21.116241455078125, Learning Rate: 0.01\n",
      "Epoch [14573/20000], Loss: -21.116119384765625, Learning Rate: 0.01\n",
      "Epoch [14574/20000], Loss: -21.116256713867188, Learning Rate: 0.01\n",
      "Epoch [14575/20000], Loss: -21.11651611328125, Learning Rate: 0.01\n",
      "Epoch [14576/20000], Loss: -21.11688232421875, Learning Rate: 0.01\n",
      "Epoch [14577/20000], Loss: -21.117141723632812, Learning Rate: 0.01\n",
      "Epoch [14578/20000], Loss: -21.117279052734375, Learning Rate: 0.01\n",
      "Epoch [14579/20000], Loss: -21.117279052734375, Learning Rate: 0.01\n",
      "Epoch [14580/20000], Loss: -21.1173095703125, Learning Rate: 0.01\n",
      "Epoch [14581/20000], Loss: -21.117401123046875, Learning Rate: 0.01\n",
      "Epoch [14582/20000], Loss: -21.117599487304688, Learning Rate: 0.01\n",
      "Epoch [14583/20000], Loss: -21.117828369140625, Learning Rate: 0.01\n",
      "Epoch [14584/20000], Loss: -21.117996215820312, Learning Rate: 0.01\n",
      "Epoch [14585/20000], Loss: -21.118118286132812, Learning Rate: 0.01\n",
      "Epoch [14586/20000], Loss: -21.118209838867188, Learning Rate: 0.01\n",
      "Epoch [14587/20000], Loss: -21.118316650390625, Learning Rate: 0.01\n",
      "Epoch [14588/20000], Loss: -21.118377685546875, Learning Rate: 0.01\n",
      "Epoch [14589/20000], Loss: -21.118499755859375, Learning Rate: 0.01\n",
      "Epoch [14590/20000], Loss: -21.11865234375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14591/20000], Loss: -21.118850708007812, Learning Rate: 0.01\n",
      "Epoch [14592/20000], Loss: -21.119064331054688, Learning Rate: 0.01\n",
      "Epoch [14593/20000], Loss: -21.119110107421875, Learning Rate: 0.01\n",
      "Epoch [14594/20000], Loss: -21.119155883789062, Learning Rate: 0.01\n",
      "Epoch [14595/20000], Loss: -21.119308471679688, Learning Rate: 0.01\n",
      "Epoch [14596/20000], Loss: -21.11944580078125, Learning Rate: 0.01\n",
      "Epoch [14597/20000], Loss: -21.119552612304688, Learning Rate: 0.01\n",
      "Epoch [14598/20000], Loss: -21.11968994140625, Learning Rate: 0.01\n",
      "Epoch [14599/20000], Loss: -21.119796752929688, Learning Rate: 0.01\n",
      "Epoch [14600/20000], Loss: -21.119949340820312, Learning Rate: 0.01\n",
      "Epoch [14601/20000], Loss: -21.1201171875, Learning Rate: 0.01\n",
      "Epoch [14602/20000], Loss: -21.12017822265625, Learning Rate: 0.01\n",
      "Epoch [14603/20000], Loss: -21.12030029296875, Learning Rate: 0.01\n",
      "Epoch [14604/20000], Loss: -21.12042236328125, Learning Rate: 0.01\n",
      "Epoch [14605/20000], Loss: -21.120529174804688, Learning Rate: 0.01\n",
      "Epoch [14606/20000], Loss: -21.120697021484375, Learning Rate: 0.01\n",
      "Epoch [14607/20000], Loss: -21.120849609375, Learning Rate: 0.01\n",
      "Epoch [14608/20000], Loss: -21.120925903320312, Learning Rate: 0.01\n",
      "Epoch [14609/20000], Loss: -21.121124267578125, Learning Rate: 0.01\n",
      "Epoch [14610/20000], Loss: -21.12115478515625, Learning Rate: 0.01\n",
      "Epoch [14611/20000], Loss: -21.121292114257812, Learning Rate: 0.01\n",
      "Epoch [14612/20000], Loss: -21.121353149414062, Learning Rate: 0.01\n",
      "Epoch [14613/20000], Loss: -21.12152099609375, Learning Rate: 0.01\n",
      "Epoch [14614/20000], Loss: -21.121597290039062, Learning Rate: 0.01\n",
      "Epoch [14615/20000], Loss: -21.121734619140625, Learning Rate: 0.01\n",
      "Epoch [14616/20000], Loss: -21.121795654296875, Learning Rate: 0.01\n",
      "Epoch [14617/20000], Loss: -21.121932983398438, Learning Rate: 0.01\n",
      "Epoch [14618/20000], Loss: -21.121978759765625, Learning Rate: 0.01\n",
      "Epoch [14619/20000], Loss: -21.121978759765625, Learning Rate: 0.01\n",
      "Epoch [14620/20000], Loss: -21.1219482421875, Learning Rate: 0.01\n",
      "Epoch [14621/20000], Loss: -21.121932983398438, Learning Rate: 0.01\n",
      "Epoch [14622/20000], Loss: -21.1217041015625, Learning Rate: 0.01\n",
      "Epoch [14623/20000], Loss: -21.121292114257812, Learning Rate: 0.01\n",
      "Epoch [14624/20000], Loss: -21.120651245117188, Learning Rate: 0.01\n",
      "Epoch [14625/20000], Loss: -21.119537353515625, Learning Rate: 0.01\n",
      "Epoch [14626/20000], Loss: -21.117721557617188, Learning Rate: 0.01\n",
      "Epoch [14627/20000], Loss: -21.114852905273438, Learning Rate: 0.01\n",
      "Epoch [14628/20000], Loss: -21.11016845703125, Learning Rate: 0.01\n",
      "Epoch [14629/20000], Loss: -21.102737426757812, Learning Rate: 0.01\n",
      "Epoch [14630/20000], Loss: -21.09075927734375, Learning Rate: 0.01\n",
      "Epoch [14631/20000], Loss: -21.071380615234375, Learning Rate: 0.01\n",
      "Epoch [14632/20000], Loss: -21.040313720703125, Learning Rate: 0.01\n",
      "Epoch [14633/20000], Loss: -20.990432739257812, Learning Rate: 0.01\n",
      "Epoch [14634/20000], Loss: -20.911666870117188, Learning Rate: 0.01\n",
      "Epoch [14635/20000], Loss: -20.788619995117188, Learning Rate: 0.01\n",
      "Epoch [14636/20000], Loss: -20.60504150390625, Learning Rate: 0.01\n",
      "Epoch [14637/20000], Loss: -20.343185424804688, Learning Rate: 0.01\n",
      "Epoch [14638/20000], Loss: -20.015609741210938, Learning Rate: 0.01\n",
      "Epoch [14639/20000], Loss: -19.669677734375, Learning Rate: 0.01\n",
      "Epoch [14640/20000], Loss: -19.459121704101562, Learning Rate: 0.01\n",
      "Epoch [14641/20000], Loss: -19.53436279296875, Learning Rate: 0.01\n",
      "Epoch [14642/20000], Loss: -19.985397338867188, Learning Rate: 0.01\n",
      "Epoch [14643/20000], Loss: -20.595352172851562, Learning Rate: 0.01\n",
      "Epoch [14644/20000], Loss: -21.024276733398438, Learning Rate: 0.01\n",
      "Epoch [14645/20000], Loss: -21.056381225585938, Learning Rate: 0.01\n",
      "Epoch [14646/20000], Loss: -20.79107666015625, Learning Rate: 0.01\n",
      "Epoch [14647/20000], Loss: -20.521240234375, Learning Rate: 0.01\n",
      "Epoch [14648/20000], Loss: -20.492507934570312, Learning Rate: 0.01\n",
      "Epoch [14649/20000], Loss: -20.724472045898438, Learning Rate: 0.01\n",
      "Epoch [14650/20000], Loss: -20.998992919921875, Learning Rate: 0.01\n",
      "Epoch [14651/20000], Loss: -21.091323852539062, Learning Rate: 0.01\n",
      "Epoch [14652/20000], Loss: -20.973983764648438, Learning Rate: 0.01\n",
      "Epoch [14653/20000], Loss: -20.814682006835938, Learning Rate: 0.01\n",
      "Epoch [14654/20000], Loss: -20.790283203125, Learning Rate: 0.01\n",
      "Epoch [14655/20000], Loss: -20.923187255859375, Learning Rate: 0.01\n",
      "Epoch [14656/20000], Loss: -21.08001708984375, Learning Rate: 0.01\n",
      "Epoch [14657/20000], Loss: -21.122787475585938, Learning Rate: 0.01\n",
      "Epoch [14658/20000], Loss: -21.043991088867188, Learning Rate: 0.01\n",
      "Epoch [14659/20000], Loss: -20.9517822265625, Learning Rate: 0.01\n",
      "Epoch [14660/20000], Loss: -20.946121215820312, Learning Rate: 0.01\n",
      "Epoch [14661/20000], Loss: -21.025100708007812, Learning Rate: 0.01\n",
      "Epoch [14662/20000], Loss: -21.104949951171875, Learning Rate: 0.01\n",
      "Epoch [14663/20000], Loss: -21.11663818359375, Learning Rate: 0.01\n",
      "Epoch [14664/20000], Loss: -21.070846557617188, Learning Rate: 0.01\n",
      "Epoch [14665/20000], Loss: -21.031707763671875, Learning Rate: 0.01\n",
      "Epoch [14666/20000], Loss: -21.043121337890625, Learning Rate: 0.01\n",
      "Epoch [14667/20000], Loss: -21.089675903320312, Learning Rate: 0.01\n",
      "Epoch [14668/20000], Loss: -21.122817993164062, Learning Rate: 0.01\n",
      "Epoch [14669/20000], Loss: -21.115814208984375, Learning Rate: 0.01\n",
      "Epoch [14670/20000], Loss: -21.086074829101562, Learning Rate: 0.01\n",
      "Epoch [14671/20000], Loss: -21.070281982421875, Learning Rate: 0.01\n",
      "Epoch [14672/20000], Loss: -21.084564208984375, Learning Rate: 0.01\n",
      "Epoch [14673/20000], Loss: -21.112884521484375, Learning Rate: 0.01\n",
      "Epoch [14674/20000], Loss: -21.128448486328125, Learning Rate: 0.01\n",
      "Epoch [14675/20000], Loss: -21.12152099609375, Learning Rate: 0.01\n",
      "Epoch [14676/20000], Loss: -21.10552978515625, Learning Rate: 0.01\n",
      "Epoch [14677/20000], Loss: -21.099319458007812, Learning Rate: 0.01\n",
      "Epoch [14678/20000], Loss: -21.1085205078125, Learning Rate: 0.01\n",
      "Epoch [14679/20000], Loss: -21.122604370117188, Learning Rate: 0.01\n",
      "Epoch [14680/20000], Loss: -21.12823486328125, Learning Rate: 0.01\n",
      "Epoch [14681/20000], Loss: -21.122802734375, Learning Rate: 0.01\n",
      "Epoch [14682/20000], Loss: -21.114791870117188, Learning Rate: 0.01\n",
      "Epoch [14683/20000], Loss: -21.113235473632812, Learning Rate: 0.01\n",
      "Epoch [14684/20000], Loss: -21.11968994140625, Learning Rate: 0.01\n",
      "Epoch [14685/20000], Loss: -21.127731323242188, Learning Rate: 0.01\n",
      "Epoch [14686/20000], Loss: -21.1304931640625, Learning Rate: 0.01\n",
      "Epoch [14687/20000], Loss: -21.12744140625, Learning Rate: 0.01\n",
      "Epoch [14688/20000], Loss: -21.12298583984375, Learning Rate: 0.01\n",
      "Epoch [14689/20000], Loss: -21.12200927734375, Learning Rate: 0.01\n",
      "Epoch [14690/20000], Loss: -21.125198364257812, Learning Rate: 0.01\n",
      "Epoch [14691/20000], Loss: -21.12921142578125, Learning Rate: 0.01\n",
      "Epoch [14692/20000], Loss: -21.130752563476562, Learning Rate: 0.01\n",
      "Epoch [14693/20000], Loss: -21.129241943359375, Learning Rate: 0.01\n",
      "Epoch [14694/20000], Loss: -21.126968383789062, Learning Rate: 0.01\n",
      "Epoch [14695/20000], Loss: -21.1265869140625, Learning Rate: 0.01\n",
      "Epoch [14696/20000], Loss: -21.12835693359375, Learning Rate: 0.01\n",
      "Epoch [14697/20000], Loss: -21.130767822265625, Learning Rate: 0.01\n",
      "Epoch [14698/20000], Loss: -21.132003784179688, Learning Rate: 0.01\n",
      "Epoch [14699/20000], Loss: -21.1314697265625, Learning Rate: 0.01\n",
      "Epoch [14700/20000], Loss: -21.130294799804688, Learning Rate: 0.01\n",
      "Epoch [14701/20000], Loss: -21.129806518554688, Learning Rate: 0.01\n",
      "Epoch [14702/20000], Loss: -21.130477905273438, Learning Rate: 0.01\n",
      "Epoch [14703/20000], Loss: -21.131729125976562, Learning Rate: 0.01\n",
      "Epoch [14704/20000], Loss: -21.13250732421875, Learning Rate: 0.01\n",
      "Epoch [14705/20000], Loss: -21.132476806640625, Learning Rate: 0.01\n",
      "Epoch [14706/20000], Loss: -21.131942749023438, Learning Rate: 0.01\n",
      "Epoch [14707/20000], Loss: -21.131561279296875, Learning Rate: 0.01\n",
      "Epoch [14708/20000], Loss: -21.131790161132812, Learning Rate: 0.01\n",
      "Epoch [14709/20000], Loss: -21.132492065429688, Learning Rate: 0.01\n",
      "Epoch [14710/20000], Loss: -21.13311767578125, Learning Rate: 0.01\n",
      "Epoch [14711/20000], Loss: -21.133316040039062, Learning Rate: 0.01\n",
      "Epoch [14712/20000], Loss: -21.133224487304688, Learning Rate: 0.01\n",
      "Epoch [14713/20000], Loss: -21.133087158203125, Learning Rate: 0.01\n",
      "Epoch [14714/20000], Loss: -21.133102416992188, Learning Rate: 0.01\n",
      "Epoch [14715/20000], Loss: -21.133453369140625, Learning Rate: 0.01\n",
      "Epoch [14716/20000], Loss: -21.133865356445312, Learning Rate: 0.01\n",
      "Epoch [14717/20000], Loss: -21.13409423828125, Learning Rate: 0.01\n",
      "Epoch [14718/20000], Loss: -21.134170532226562, Learning Rate: 0.01\n",
      "Epoch [14719/20000], Loss: -21.134170532226562, Learning Rate: 0.01\n",
      "Epoch [14720/20000], Loss: -21.134140014648438, Learning Rate: 0.01\n",
      "Epoch [14721/20000], Loss: -21.134231567382812, Learning Rate: 0.01\n",
      "Epoch [14722/20000], Loss: -21.134521484375, Learning Rate: 0.01\n",
      "Epoch [14723/20000], Loss: -21.134719848632812, Learning Rate: 0.01\n",
      "Epoch [14724/20000], Loss: -21.13494873046875, Learning Rate: 0.01\n",
      "Epoch [14725/20000], Loss: -21.134963989257812, Learning Rate: 0.01\n",
      "Epoch [14726/20000], Loss: -21.134963989257812, Learning Rate: 0.01\n",
      "Epoch [14727/20000], Loss: -21.135101318359375, Learning Rate: 0.01\n",
      "Epoch [14728/20000], Loss: -21.135238647460938, Learning Rate: 0.01\n",
      "Epoch [14729/20000], Loss: -21.135345458984375, Learning Rate: 0.01\n",
      "Epoch [14730/20000], Loss: -21.13555908203125, Learning Rate: 0.01\n",
      "Epoch [14731/20000], Loss: -21.135650634765625, Learning Rate: 0.01\n",
      "Epoch [14732/20000], Loss: -21.135726928710938, Learning Rate: 0.01\n",
      "Epoch [14733/20000], Loss: -21.135848999023438, Learning Rate: 0.01\n",
      "Epoch [14734/20000], Loss: -21.135910034179688, Learning Rate: 0.01\n",
      "Epoch [14735/20000], Loss: -21.135955810546875, Learning Rate: 0.01\n",
      "Epoch [14736/20000], Loss: -21.136077880859375, Learning Rate: 0.01\n",
      "Epoch [14737/20000], Loss: -21.136199951171875, Learning Rate: 0.01\n",
      "Epoch [14738/20000], Loss: -21.136398315429688, Learning Rate: 0.01\n",
      "Epoch [14739/20000], Loss: -21.136428833007812, Learning Rate: 0.01\n",
      "Epoch [14740/20000], Loss: -21.136474609375, Learning Rate: 0.01\n",
      "Epoch [14741/20000], Loss: -21.13653564453125, Learning Rate: 0.01\n",
      "Epoch [14742/20000], Loss: -21.136489868164062, Learning Rate: 0.01\n",
      "Epoch [14743/20000], Loss: -21.13653564453125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14744/20000], Loss: -21.136505126953125, Learning Rate: 0.01\n",
      "Epoch [14745/20000], Loss: -21.136398315429688, Learning Rate: 0.01\n",
      "Epoch [14746/20000], Loss: -21.13623046875, Learning Rate: 0.01\n",
      "Epoch [14747/20000], Loss: -21.135894775390625, Learning Rate: 0.01\n",
      "Epoch [14748/20000], Loss: -21.135482788085938, Learning Rate: 0.01\n",
      "Epoch [14749/20000], Loss: -21.134658813476562, Learning Rate: 0.01\n",
      "Epoch [14750/20000], Loss: -21.133697509765625, Learning Rate: 0.01\n",
      "Epoch [14751/20000], Loss: -21.132049560546875, Learning Rate: 0.01\n",
      "Epoch [14752/20000], Loss: -21.129867553710938, Learning Rate: 0.01\n",
      "Epoch [14753/20000], Loss: -21.126449584960938, Learning Rate: 0.01\n",
      "Epoch [14754/20000], Loss: -21.1214599609375, Learning Rate: 0.01\n",
      "Epoch [14755/20000], Loss: -21.11407470703125, Learning Rate: 0.01\n",
      "Epoch [14756/20000], Loss: -21.10302734375, Learning Rate: 0.01\n",
      "Epoch [14757/20000], Loss: -21.086578369140625, Learning Rate: 0.01\n",
      "Epoch [14758/20000], Loss: -21.061782836914062, Learning Rate: 0.01\n",
      "Epoch [14759/20000], Loss: -21.0247802734375, Learning Rate: 0.01\n",
      "Epoch [14760/20000], Loss: -20.969100952148438, Learning Rate: 0.01\n",
      "Epoch [14761/20000], Loss: -20.886947631835938, Learning Rate: 0.01\n",
      "Epoch [14762/20000], Loss: -20.76446533203125, Learning Rate: 0.01\n",
      "Epoch [14763/20000], Loss: -20.591033935546875, Learning Rate: 0.01\n",
      "Epoch [14764/20000], Loss: -20.344833374023438, Learning Rate: 0.01\n",
      "Epoch [14765/20000], Loss: -20.031753540039062, Learning Rate: 0.01\n",
      "Epoch [14766/20000], Loss: -19.648300170898438, Learning Rate: 0.01\n",
      "Epoch [14767/20000], Loss: -19.29296875, Learning Rate: 0.01\n",
      "Epoch [14768/20000], Loss: -19.043487548828125, Learning Rate: 0.01\n",
      "Epoch [14769/20000], Loss: -19.117233276367188, Learning Rate: 0.01\n",
      "Epoch [14770/20000], Loss: -19.525039672851562, Learning Rate: 0.01\n",
      "Epoch [14771/20000], Loss: -20.206253051757812, Learning Rate: 0.01\n",
      "Epoch [14772/20000], Loss: -20.829483032226562, Learning Rate: 0.01\n",
      "Epoch [14773/20000], Loss: -21.129791259765625, Learning Rate: 0.01\n",
      "Epoch [14774/20000], Loss: -21.035614013671875, Learning Rate: 0.01\n",
      "Epoch [14775/20000], Loss: -20.709152221679688, Learning Rate: 0.01\n",
      "Epoch [14776/20000], Loss: -20.41937255859375, Learning Rate: 0.01\n",
      "Epoch [14777/20000], Loss: -20.359405517578125, Learning Rate: 0.01\n",
      "Epoch [14778/20000], Loss: -20.572463989257812, Learning Rate: 0.01\n",
      "Epoch [14779/20000], Loss: -20.8917236328125, Learning Rate: 0.01\n",
      "Epoch [14780/20000], Loss: -21.1109619140625, Learning Rate: 0.01\n",
      "Epoch [14781/20000], Loss: -21.117416381835938, Learning Rate: 0.01\n",
      "Epoch [14782/20000], Loss: -20.966262817382812, Learning Rate: 0.01\n",
      "Epoch [14783/20000], Loss: -20.810653686523438, Learning Rate: 0.01\n",
      "Epoch [14784/20000], Loss: -20.774490356445312, Learning Rate: 0.01\n",
      "Epoch [14785/20000], Loss: -20.880401611328125, Learning Rate: 0.01\n",
      "Epoch [14786/20000], Loss: -21.036636352539062, Learning Rate: 0.01\n",
      "Epoch [14787/20000], Loss: -21.133712768554688, Learning Rate: 0.01\n",
      "Epoch [14788/20000], Loss: -21.122085571289062, Learning Rate: 0.01\n",
      "Epoch [14789/20000], Loss: -21.040390014648438, Learning Rate: 0.01\n",
      "Epoch [14790/20000], Loss: -20.970443725585938, Learning Rate: 0.01\n",
      "Epoch [14791/20000], Loss: -20.967880249023438, Learning Rate: 0.01\n",
      "Epoch [14792/20000], Loss: -21.030181884765625, Learning Rate: 0.01\n",
      "Epoch [14793/20000], Loss: -21.105148315429688, Learning Rate: 0.01\n",
      "Epoch [14794/20000], Loss: -21.141708374023438, Learning Rate: 0.01\n",
      "Epoch [14795/20000], Loss: -21.125686645507812, Learning Rate: 0.01\n",
      "Epoch [14796/20000], Loss: -21.083221435546875, Learning Rate: 0.01\n",
      "Epoch [14797/20000], Loss: -21.054306030273438, Learning Rate: 0.01\n",
      "Epoch [14798/20000], Loss: -21.060653686523438, Learning Rate: 0.01\n",
      "Epoch [14799/20000], Loss: -21.094680786132812, Learning Rate: 0.01\n",
      "Epoch [14800/20000], Loss: -21.129714965820312, Learning Rate: 0.01\n",
      "Epoch [14801/20000], Loss: -21.143264770507812, Learning Rate: 0.01\n",
      "Epoch [14802/20000], Loss: -21.132369995117188, Learning Rate: 0.01\n",
      "Epoch [14803/20000], Loss: -21.111602783203125, Learning Rate: 0.01\n",
      "Epoch [14804/20000], Loss: -21.099334716796875, Learning Rate: 0.01\n",
      "Epoch [14805/20000], Loss: -21.104202270507812, Learning Rate: 0.01\n",
      "Epoch [14806/20000], Loss: -21.12115478515625, Learning Rate: 0.01\n",
      "Epoch [14807/20000], Loss: -21.137832641601562, Learning Rate: 0.01\n",
      "Epoch [14808/20000], Loss: -21.144058227539062, Learning Rate: 0.01\n",
      "Epoch [14809/20000], Loss: -21.138931274414062, Learning Rate: 0.01\n",
      "Epoch [14810/20000], Loss: -21.129058837890625, Learning Rate: 0.01\n",
      "Epoch [14811/20000], Loss: -21.122940063476562, Learning Rate: 0.01\n",
      "Epoch [14812/20000], Loss: -21.124679565429688, Learning Rate: 0.01\n",
      "Epoch [14813/20000], Loss: -21.132522583007812, Learning Rate: 0.01\n",
      "Epoch [14814/20000], Loss: -21.140792846679688, Learning Rate: 0.01\n",
      "Epoch [14815/20000], Loss: -21.1448974609375, Learning Rate: 0.01\n",
      "Epoch [14816/20000], Loss: -21.143325805664062, Learning Rate: 0.01\n",
      "Epoch [14817/20000], Loss: -21.139007568359375, Learning Rate: 0.01\n",
      "Epoch [14818/20000], Loss: -21.135391235351562, Learning Rate: 0.01\n",
      "Epoch [14819/20000], Loss: -21.135177612304688, Learning Rate: 0.01\n",
      "Epoch [14820/20000], Loss: -21.13818359375, Learning Rate: 0.01\n",
      "Epoch [14821/20000], Loss: -21.142303466796875, Learning Rate: 0.01\n",
      "Epoch [14822/20000], Loss: -21.145156860351562, Learning Rate: 0.01\n",
      "Epoch [14823/20000], Loss: -21.145584106445312, Learning Rate: 0.01\n",
      "Epoch [14824/20000], Loss: -21.144073486328125, Learning Rate: 0.01\n",
      "Epoch [14825/20000], Loss: -21.142135620117188, Learning Rate: 0.01\n",
      "Epoch [14826/20000], Loss: -21.14111328125, Learning Rate: 0.01\n",
      "Epoch [14827/20000], Loss: -21.141845703125, Learning Rate: 0.01\n",
      "Epoch [14828/20000], Loss: -21.143600463867188, Learning Rate: 0.01\n",
      "Epoch [14829/20000], Loss: -21.145401000976562, Learning Rate: 0.01\n",
      "Epoch [14830/20000], Loss: -21.146469116210938, Learning Rate: 0.01\n",
      "Epoch [14831/20000], Loss: -21.146499633789062, Learning Rate: 0.01\n",
      "Epoch [14832/20000], Loss: -21.145706176757812, Learning Rate: 0.01\n",
      "Epoch [14833/20000], Loss: -21.144973754882812, Learning Rate: 0.01\n",
      "Epoch [14834/20000], Loss: -21.14471435546875, Learning Rate: 0.01\n",
      "Epoch [14835/20000], Loss: -21.1451416015625, Learning Rate: 0.01\n",
      "Epoch [14836/20000], Loss: -21.145950317382812, Learning Rate: 0.01\n",
      "Epoch [14837/20000], Loss: -21.146804809570312, Learning Rate: 0.01\n",
      "Epoch [14838/20000], Loss: -21.147384643554688, Learning Rate: 0.01\n",
      "Epoch [14839/20000], Loss: -21.1474609375, Learning Rate: 0.01\n",
      "Epoch [14840/20000], Loss: -21.147201538085938, Learning Rate: 0.01\n",
      "Epoch [14841/20000], Loss: -21.146926879882812, Learning Rate: 0.01\n",
      "Epoch [14842/20000], Loss: -21.1468505859375, Learning Rate: 0.01\n",
      "Epoch [14843/20000], Loss: -21.14697265625, Learning Rate: 0.01\n",
      "Epoch [14844/20000], Loss: -21.1473388671875, Learning Rate: 0.01\n",
      "Epoch [14845/20000], Loss: -21.147842407226562, Learning Rate: 0.01\n",
      "Epoch [14846/20000], Loss: -21.148086547851562, Learning Rate: 0.01\n",
      "Epoch [14847/20000], Loss: -21.1483154296875, Learning Rate: 0.01\n",
      "Epoch [14848/20000], Loss: -21.148391723632812, Learning Rate: 0.01\n",
      "Epoch [14849/20000], Loss: -21.148345947265625, Learning Rate: 0.01\n",
      "Epoch [14850/20000], Loss: -21.148269653320312, Learning Rate: 0.01\n",
      "Epoch [14851/20000], Loss: -21.148330688476562, Learning Rate: 0.01\n",
      "Epoch [14852/20000], Loss: -21.148483276367188, Learning Rate: 0.01\n",
      "Epoch [14853/20000], Loss: -21.148681640625, Learning Rate: 0.01\n",
      "Epoch [14854/20000], Loss: -21.148956298828125, Learning Rate: 0.01\n",
      "Epoch [14855/20000], Loss: -21.149139404296875, Learning Rate: 0.01\n",
      "Epoch [14856/20000], Loss: -21.149246215820312, Learning Rate: 0.01\n",
      "Epoch [14857/20000], Loss: -21.149383544921875, Learning Rate: 0.01\n",
      "Epoch [14858/20000], Loss: -21.149398803710938, Learning Rate: 0.01\n",
      "Epoch [14859/20000], Loss: -21.149444580078125, Learning Rate: 0.01\n",
      "Epoch [14860/20000], Loss: -21.149520874023438, Learning Rate: 0.01\n",
      "Epoch [14861/20000], Loss: -21.149627685546875, Learning Rate: 0.01\n",
      "Epoch [14862/20000], Loss: -21.14971923828125, Learning Rate: 0.01\n",
      "Epoch [14863/20000], Loss: -21.149948120117188, Learning Rate: 0.01\n",
      "Epoch [14864/20000], Loss: -21.150054931640625, Learning Rate: 0.01\n",
      "Epoch [14865/20000], Loss: -21.150161743164062, Learning Rate: 0.01\n",
      "Epoch [14866/20000], Loss: -21.150253295898438, Learning Rate: 0.01\n",
      "Epoch [14867/20000], Loss: -21.15032958984375, Learning Rate: 0.01\n",
      "Epoch [14868/20000], Loss: -21.150558471679688, Learning Rate: 0.01\n",
      "Epoch [14869/20000], Loss: -21.15057373046875, Learning Rate: 0.01\n",
      "Epoch [14870/20000], Loss: -21.150650024414062, Learning Rate: 0.01\n",
      "Epoch [14871/20000], Loss: -21.150741577148438, Learning Rate: 0.01\n",
      "Epoch [14872/20000], Loss: -21.150833129882812, Learning Rate: 0.01\n",
      "Epoch [14873/20000], Loss: -21.150909423828125, Learning Rate: 0.01\n",
      "Epoch [14874/20000], Loss: -21.15106201171875, Learning Rate: 0.01\n",
      "Epoch [14875/20000], Loss: -21.151229858398438, Learning Rate: 0.01\n",
      "Epoch [14876/20000], Loss: -21.151290893554688, Learning Rate: 0.01\n",
      "Epoch [14877/20000], Loss: -21.1513671875, Learning Rate: 0.01\n",
      "Epoch [14878/20000], Loss: -21.151535034179688, Learning Rate: 0.01\n",
      "Epoch [14879/20000], Loss: -21.151641845703125, Learning Rate: 0.01\n",
      "Epoch [14880/20000], Loss: -21.151687622070312, Learning Rate: 0.01\n",
      "Epoch [14881/20000], Loss: -21.15179443359375, Learning Rate: 0.01\n",
      "Epoch [14882/20000], Loss: -21.151824951171875, Learning Rate: 0.01\n",
      "Epoch [14883/20000], Loss: -21.15203857421875, Learning Rate: 0.01\n",
      "Epoch [14884/20000], Loss: -21.152053833007812, Learning Rate: 0.01\n",
      "Epoch [14885/20000], Loss: -21.152191162109375, Learning Rate: 0.01\n",
      "Epoch [14886/20000], Loss: -21.152252197265625, Learning Rate: 0.01\n",
      "Epoch [14887/20000], Loss: -21.152389526367188, Learning Rate: 0.01\n",
      "Epoch [14888/20000], Loss: -21.152481079101562, Learning Rate: 0.01\n",
      "Epoch [14889/20000], Loss: -21.152618408203125, Learning Rate: 0.01\n",
      "Epoch [14890/20000], Loss: -21.152725219726562, Learning Rate: 0.01\n",
      "Epoch [14891/20000], Loss: -21.152816772460938, Learning Rate: 0.01\n",
      "Epoch [14892/20000], Loss: -21.152923583984375, Learning Rate: 0.01\n",
      "Epoch [14893/20000], Loss: -21.152999877929688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14894/20000], Loss: -21.153091430664062, Learning Rate: 0.01\n",
      "Epoch [14895/20000], Loss: -21.15325927734375, Learning Rate: 0.01\n",
      "Epoch [14896/20000], Loss: -21.153350830078125, Learning Rate: 0.01\n",
      "Epoch [14897/20000], Loss: -21.153396606445312, Learning Rate: 0.01\n",
      "Epoch [14898/20000], Loss: -21.153533935546875, Learning Rate: 0.01\n",
      "Epoch [14899/20000], Loss: -21.153640747070312, Learning Rate: 0.01\n",
      "Epoch [14900/20000], Loss: -21.15374755859375, Learning Rate: 0.01\n",
      "Epoch [14901/20000], Loss: -21.15380859375, Learning Rate: 0.01\n",
      "Epoch [14902/20000], Loss: -21.15386962890625, Learning Rate: 0.01\n",
      "Epoch [14903/20000], Loss: -21.154052734375, Learning Rate: 0.01\n",
      "Epoch [14904/20000], Loss: -21.1541748046875, Learning Rate: 0.01\n",
      "Epoch [14905/20000], Loss: -21.154251098632812, Learning Rate: 0.01\n",
      "Epoch [14906/20000], Loss: -21.15435791015625, Learning Rate: 0.01\n",
      "Epoch [14907/20000], Loss: -21.154434204101562, Learning Rate: 0.01\n",
      "Epoch [14908/20000], Loss: -21.154541015625, Learning Rate: 0.01\n",
      "Epoch [14909/20000], Loss: -21.154617309570312, Learning Rate: 0.01\n",
      "Epoch [14910/20000], Loss: -21.154769897460938, Learning Rate: 0.01\n",
      "Epoch [14911/20000], Loss: -21.1549072265625, Learning Rate: 0.01\n",
      "Epoch [14912/20000], Loss: -21.15484619140625, Learning Rate: 0.01\n",
      "Epoch [14913/20000], Loss: -21.155044555664062, Learning Rate: 0.01\n",
      "Epoch [14914/20000], Loss: -21.1551513671875, Learning Rate: 0.01\n",
      "Epoch [14915/20000], Loss: -21.1552734375, Learning Rate: 0.01\n",
      "Epoch [14916/20000], Loss: -21.1552734375, Learning Rate: 0.01\n",
      "Epoch [14917/20000], Loss: -21.1553955078125, Learning Rate: 0.01\n",
      "Epoch [14918/20000], Loss: -21.155517578125, Learning Rate: 0.01\n",
      "Epoch [14919/20000], Loss: -21.155609130859375, Learning Rate: 0.01\n",
      "Epoch [14920/20000], Loss: -21.155685424804688, Learning Rate: 0.01\n",
      "Epoch [14921/20000], Loss: -21.15576171875, Learning Rate: 0.01\n",
      "Epoch [14922/20000], Loss: -21.155899047851562, Learning Rate: 0.01\n",
      "Epoch [14923/20000], Loss: -21.156005859375, Learning Rate: 0.01\n",
      "Epoch [14924/20000], Loss: -21.156036376953125, Learning Rate: 0.01\n",
      "Epoch [14925/20000], Loss: -21.1561279296875, Learning Rate: 0.01\n",
      "Epoch [14926/20000], Loss: -21.156158447265625, Learning Rate: 0.01\n",
      "Epoch [14927/20000], Loss: -21.156204223632812, Learning Rate: 0.01\n",
      "Epoch [14928/20000], Loss: -21.156219482421875, Learning Rate: 0.01\n",
      "Epoch [14929/20000], Loss: -21.15618896484375, Learning Rate: 0.01\n",
      "Epoch [14930/20000], Loss: -21.156143188476562, Learning Rate: 0.01\n",
      "Epoch [14931/20000], Loss: -21.1558837890625, Learning Rate: 0.01\n",
      "Epoch [14932/20000], Loss: -21.155548095703125, Learning Rate: 0.01\n",
      "Epoch [14933/20000], Loss: -21.154891967773438, Learning Rate: 0.01\n",
      "Epoch [14934/20000], Loss: -21.153915405273438, Learning Rate: 0.01\n",
      "Epoch [14935/20000], Loss: -21.152420043945312, Learning Rate: 0.01\n",
      "Epoch [14936/20000], Loss: -21.14996337890625, Learning Rate: 0.01\n",
      "Epoch [14937/20000], Loss: -21.146102905273438, Learning Rate: 0.01\n",
      "Epoch [14938/20000], Loss: -21.140121459960938, Learning Rate: 0.01\n",
      "Epoch [14939/20000], Loss: -21.130538940429688, Learning Rate: 0.01\n",
      "Epoch [14940/20000], Loss: -21.115386962890625, Learning Rate: 0.01\n",
      "Epoch [14941/20000], Loss: -21.091522216796875, Learning Rate: 0.01\n",
      "Epoch [14942/20000], Loss: -21.053802490234375, Learning Rate: 0.01\n",
      "Epoch [14943/20000], Loss: -20.994491577148438, Learning Rate: 0.01\n",
      "Epoch [14944/20000], Loss: -20.90228271484375, Learning Rate: 0.01\n",
      "Epoch [14945/20000], Loss: -20.762542724609375, Learning Rate: 0.01\n",
      "Epoch [14946/20000], Loss: -20.55816650390625, Learning Rate: 0.01\n",
      "Epoch [14947/20000], Loss: -20.281234741210938, Learning Rate: 0.01\n",
      "Epoch [14948/20000], Loss: -19.946502685546875, Learning Rate: 0.01\n",
      "Epoch [14949/20000], Loss: -19.630767822265625, Learning Rate: 0.01\n",
      "Epoch [14950/20000], Loss: -19.464202880859375, Learning Rate: 0.01\n",
      "Epoch [14951/20000], Loss: -19.60015869140625, Learning Rate: 0.01\n",
      "Epoch [14952/20000], Loss: -20.023941040039062, Learning Rate: 0.01\n",
      "Epoch [14953/20000], Loss: -20.5238037109375, Learning Rate: 0.01\n",
      "Epoch [14954/20000], Loss: -20.77716064453125, Learning Rate: 0.01\n",
      "Epoch [14955/20000], Loss: -20.657333374023438, Learning Rate: 0.01\n",
      "Epoch [14956/20000], Loss: -20.298797607421875, Learning Rate: 0.01\n",
      "Epoch [14957/20000], Loss: -20.020767211914062, Learning Rate: 0.01\n",
      "Epoch [14958/20000], Loss: -20.039138793945312, Learning Rate: 0.01\n",
      "Epoch [14959/20000], Loss: -20.370376586914062, Learning Rate: 0.01\n",
      "Epoch [14960/20000], Loss: -20.723434448242188, Learning Rate: 0.01\n",
      "Epoch [14961/20000], Loss: -20.859848022460938, Learning Rate: 0.01\n",
      "Epoch [14962/20000], Loss: -20.77618408203125, Learning Rate: 0.01\n",
      "Epoch [14963/20000], Loss: -20.6904296875, Learning Rate: 0.01\n",
      "Epoch [14964/20000], Loss: -20.774368286132812, Learning Rate: 0.01\n",
      "Epoch [14965/20000], Loss: -20.990142822265625, Learning Rate: 0.01\n",
      "Epoch [14966/20000], Loss: -21.1448974609375, Learning Rate: 0.01\n",
      "Epoch [14967/20000], Loss: -21.117340087890625, Learning Rate: 0.01\n",
      "Epoch [14968/20000], Loss: -20.975173950195312, Learning Rate: 0.01\n",
      "Epoch [14969/20000], Loss: -20.879318237304688, Learning Rate: 0.01\n",
      "Epoch [14970/20000], Loss: -20.912445068359375, Learning Rate: 0.01\n",
      "Epoch [14971/20000], Loss: -21.010177612304688, Learning Rate: 0.01\n",
      "Epoch [14972/20000], Loss: -21.062362670898438, Learning Rate: 0.01\n",
      "Epoch [14973/20000], Loss: -21.035263061523438, Learning Rate: 0.01\n",
      "Epoch [14974/20000], Loss: -20.995162963867188, Learning Rate: 0.01\n",
      "Epoch [14975/20000], Loss: -21.013076782226562, Learning Rate: 0.01\n",
      "Epoch [14976/20000], Loss: -21.087112426757812, Learning Rate: 0.01\n",
      "Epoch [14977/20000], Loss: -21.150436401367188, Learning Rate: 0.01\n",
      "Epoch [14978/20000], Loss: -21.1541748046875, Learning Rate: 0.01\n",
      "Epoch [14979/20000], Loss: -21.115081787109375, Learning Rate: 0.01\n",
      "Epoch [14980/20000], Loss: -21.0860595703125, Learning Rate: 0.01\n",
      "Epoch [14981/20000], Loss: -21.095245361328125, Learning Rate: 0.01\n",
      "Epoch [14982/20000], Loss: -21.122634887695312, Learning Rate: 0.01\n",
      "Epoch [14983/20000], Loss: -21.133529663085938, Learning Rate: 0.01\n",
      "Epoch [14984/20000], Loss: -21.11859130859375, Learning Rate: 0.01\n",
      "Epoch [14985/20000], Loss: -21.100341796875, Learning Rate: 0.01\n",
      "Epoch [14986/20000], Loss: -21.102859497070312, Learning Rate: 0.01\n",
      "Epoch [14987/20000], Loss: -21.125930786132812, Learning Rate: 0.01\n",
      "Epoch [14988/20000], Loss: -21.148468017578125, Learning Rate: 0.01\n",
      "Epoch [14989/20000], Loss: -21.154205322265625, Learning Rate: 0.01\n",
      "Epoch [14990/20000], Loss: -21.146591186523438, Learning Rate: 0.01\n",
      "Epoch [14991/20000], Loss: -21.14105224609375, Learning Rate: 0.01\n",
      "Epoch [14992/20000], Loss: -21.1461181640625, Learning Rate: 0.01\n",
      "Epoch [14993/20000], Loss: -21.156448364257812, Learning Rate: 0.01\n",
      "Epoch [14994/20000], Loss: -21.16119384765625, Learning Rate: 0.01\n",
      "Epoch [14995/20000], Loss: -21.1561279296875, Learning Rate: 0.01\n",
      "Epoch [14996/20000], Loss: -21.14794921875, Learning Rate: 0.01\n",
      "Epoch [14997/20000], Loss: -21.144622802734375, Learning Rate: 0.01\n",
      "Epoch [14998/20000], Loss: -21.148330688476562, Learning Rate: 0.01\n",
      "Epoch [14999/20000], Loss: -21.153823852539062, Learning Rate: 0.01\n",
      "Epoch [15000/20000], Loss: -21.155746459960938, Learning Rate: 0.01\n",
      "Epoch [15001/20000], Loss: -21.153610229492188, Learning Rate: 0.01\n",
      "Epoch [15002/20000], Loss: -21.151397705078125, Learning Rate: 0.01\n",
      "Epoch [15003/20000], Loss: -21.152496337890625, Learning Rate: 0.01\n",
      "Epoch [15004/20000], Loss: -21.156723022460938, Learning Rate: 0.01\n",
      "Epoch [15005/20000], Loss: -21.160675048828125, Learning Rate: 0.01\n",
      "Epoch [15006/20000], Loss: -21.161895751953125, Learning Rate: 0.01\n",
      "Epoch [15007/20000], Loss: -21.160873413085938, Learning Rate: 0.01\n",
      "Epoch [15008/20000], Loss: -21.16009521484375, Learning Rate: 0.01\n",
      "Epoch [15009/20000], Loss: -21.160995483398438, Learning Rate: 0.01\n",
      "Epoch [15010/20000], Loss: -21.163131713867188, Learning Rate: 0.01\n",
      "Epoch [15011/20000], Loss: -21.164718627929688, Learning Rate: 0.01\n",
      "Epoch [15012/20000], Loss: -21.16473388671875, Learning Rate: 0.01\n",
      "Epoch [15013/20000], Loss: -21.163803100585938, Learning Rate: 0.01\n",
      "Epoch [15014/20000], Loss: -21.163040161132812, Learning Rate: 0.01\n",
      "Epoch [15015/20000], Loss: -21.163314819335938, Learning Rate: 0.01\n",
      "Epoch [15016/20000], Loss: -21.164169311523438, Learning Rate: 0.01\n",
      "Epoch [15017/20000], Loss: -21.164886474609375, Learning Rate: 0.01\n",
      "Epoch [15018/20000], Loss: -21.16473388671875, Learning Rate: 0.01\n",
      "Epoch [15019/20000], Loss: -21.16412353515625, Learning Rate: 0.01\n",
      "Epoch [15020/20000], Loss: -21.163589477539062, Learning Rate: 0.01\n",
      "Epoch [15021/20000], Loss: -21.163650512695312, Learning Rate: 0.01\n",
      "Epoch [15022/20000], Loss: -21.163970947265625, Learning Rate: 0.01\n",
      "Epoch [15023/20000], Loss: -21.164321899414062, Learning Rate: 0.01\n",
      "Epoch [15024/20000], Loss: -21.164138793945312, Learning Rate: 0.01\n",
      "Epoch [15025/20000], Loss: -21.163619995117188, Learning Rate: 0.01\n",
      "Epoch [15026/20000], Loss: -21.163055419921875, Learning Rate: 0.01\n",
      "Epoch [15027/20000], Loss: -21.162750244140625, Learning Rate: 0.01\n",
      "Epoch [15028/20000], Loss: -21.162384033203125, Learning Rate: 0.01\n",
      "Epoch [15029/20000], Loss: -21.162002563476562, Learning Rate: 0.01\n",
      "Epoch [15030/20000], Loss: -21.161117553710938, Learning Rate: 0.01\n",
      "Epoch [15031/20000], Loss: -21.159744262695312, Learning Rate: 0.01\n",
      "Epoch [15032/20000], Loss: -21.158035278320312, Learning Rate: 0.01\n",
      "Epoch [15033/20000], Loss: -21.155868530273438, Learning Rate: 0.01\n",
      "Epoch [15034/20000], Loss: -21.153182983398438, Learning Rate: 0.01\n",
      "Epoch [15035/20000], Loss: -21.14947509765625, Learning Rate: 0.01\n",
      "Epoch [15036/20000], Loss: -21.144378662109375, Learning Rate: 0.01\n",
      "Epoch [15037/20000], Loss: -21.13726806640625, Learning Rate: 0.01\n",
      "Epoch [15038/20000], Loss: -21.12744140625, Learning Rate: 0.01\n",
      "Epoch [15039/20000], Loss: -21.114181518554688, Learning Rate: 0.01\n",
      "Epoch [15040/20000], Loss: -21.095870971679688, Learning Rate: 0.01\n",
      "Epoch [15041/20000], Loss: -21.070892333984375, Learning Rate: 0.01\n",
      "Epoch [15042/20000], Loss: -21.036087036132812, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15043/20000], Loss: -20.988845825195312, Learning Rate: 0.01\n",
      "Epoch [15044/20000], Loss: -20.923599243164062, Learning Rate: 0.01\n",
      "Epoch [15045/20000], Loss: -20.837661743164062, Learning Rate: 0.01\n",
      "Epoch [15046/20000], Loss: -20.722702026367188, Learning Rate: 0.01\n",
      "Epoch [15047/20000], Loss: -20.58148193359375, Learning Rate: 0.01\n",
      "Epoch [15048/20000], Loss: -20.40771484375, Learning Rate: 0.01\n",
      "Epoch [15049/20000], Loss: -20.227203369140625, Learning Rate: 0.01\n",
      "Epoch [15050/20000], Loss: -20.049835205078125, Learning Rate: 0.01\n",
      "Epoch [15051/20000], Loss: -19.946319580078125, Learning Rate: 0.01\n",
      "Epoch [15052/20000], Loss: -19.93902587890625, Learning Rate: 0.01\n",
      "Epoch [15053/20000], Loss: -20.095077514648438, Learning Rate: 0.01\n",
      "Epoch [15054/20000], Loss: -20.369735717773438, Learning Rate: 0.01\n",
      "Epoch [15055/20000], Loss: -20.710418701171875, Learning Rate: 0.01\n",
      "Epoch [15056/20000], Loss: -20.997314453125, Learning Rate: 0.01\n",
      "Epoch [15057/20000], Loss: -21.15167236328125, Learning Rate: 0.01\n",
      "Epoch [15058/20000], Loss: -21.15087890625, Learning Rate: 0.01\n",
      "Epoch [15059/20000], Loss: -21.037979125976562, Learning Rate: 0.01\n",
      "Epoch [15060/20000], Loss: -20.891693115234375, Learning Rate: 0.01\n",
      "Epoch [15061/20000], Loss: -20.786544799804688, Learning Rate: 0.01\n",
      "Epoch [15062/20000], Loss: -20.773834228515625, Learning Rate: 0.01\n",
      "Epoch [15063/20000], Loss: -20.849334716796875, Learning Rate: 0.01\n",
      "Epoch [15064/20000], Loss: -20.977981567382812, Learning Rate: 0.01\n",
      "Epoch [15065/20000], Loss: -21.097625732421875, Learning Rate: 0.01\n",
      "Epoch [15066/20000], Loss: -21.163299560546875, Learning Rate: 0.01\n",
      "Epoch [15067/20000], Loss: -21.160888671875, Learning Rate: 0.01\n",
      "Epoch [15068/20000], Loss: -21.110382080078125, Learning Rate: 0.01\n",
      "Epoch [15069/20000], Loss: -21.0491943359375, Learning Rate: 0.01\n",
      "Epoch [15070/20000], Loss: -21.01123046875, Learning Rate: 0.01\n",
      "Epoch [15071/20000], Loss: -21.014999389648438, Learning Rate: 0.01\n",
      "Epoch [15072/20000], Loss: -21.053695678710938, Learning Rate: 0.01\n",
      "Epoch [15073/20000], Loss: -21.107681274414062, Learning Rate: 0.01\n",
      "Epoch [15074/20000], Loss: -21.151763916015625, Learning Rate: 0.01\n",
      "Epoch [15075/20000], Loss: -21.17059326171875, Learning Rate: 0.01\n",
      "Epoch [15076/20000], Loss: -21.16290283203125, Learning Rate: 0.01\n",
      "Epoch [15077/20000], Loss: -21.139556884765625, Learning Rate: 0.01\n",
      "Epoch [15078/20000], Loss: -21.116012573242188, Learning Rate: 0.01\n",
      "Epoch [15079/20000], Loss: -21.104278564453125, Learning Rate: 0.01\n",
      "Epoch [15080/20000], Loss: -21.109375, Learning Rate: 0.01\n",
      "Epoch [15081/20000], Loss: -21.126739501953125, Learning Rate: 0.01\n",
      "Epoch [15082/20000], Loss: -21.148361206054688, Learning Rate: 0.01\n",
      "Epoch [15083/20000], Loss: -21.165008544921875, Learning Rate: 0.01\n",
      "Epoch [15084/20000], Loss: -21.171661376953125, Learning Rate: 0.01\n",
      "Epoch [15085/20000], Loss: -21.168472290039062, Learning Rate: 0.01\n",
      "Epoch [15086/20000], Loss: -21.159439086914062, Learning Rate: 0.01\n",
      "Epoch [15087/20000], Loss: -21.15020751953125, Learning Rate: 0.01\n",
      "Epoch [15088/20000], Loss: -21.14508056640625, Learning Rate: 0.01\n",
      "Epoch [15089/20000], Loss: -21.146072387695312, Learning Rate: 0.01\n",
      "Epoch [15090/20000], Loss: -21.152069091796875, Learning Rate: 0.01\n",
      "Epoch [15091/20000], Loss: -21.160308837890625, Learning Rate: 0.01\n",
      "Epoch [15092/20000], Loss: -21.167633056640625, Learning Rate: 0.01\n",
      "Epoch [15093/20000], Loss: -21.171905517578125, Learning Rate: 0.01\n",
      "Epoch [15094/20000], Loss: -21.172439575195312, Learning Rate: 0.01\n",
      "Epoch [15095/20000], Loss: -21.170120239257812, Learning Rate: 0.01\n",
      "Epoch [15096/20000], Loss: -21.166580200195312, Learning Rate: 0.01\n",
      "Epoch [15097/20000], Loss: -21.163711547851562, Learning Rate: 0.01\n",
      "Epoch [15098/20000], Loss: -21.162353515625, Learning Rate: 0.01\n",
      "Epoch [15099/20000], Loss: -21.163177490234375, Learning Rate: 0.01\n",
      "Epoch [15100/20000], Loss: -21.165451049804688, Learning Rate: 0.01\n",
      "Epoch [15101/20000], Loss: -21.168411254882812, Learning Rate: 0.01\n",
      "Epoch [15102/20000], Loss: -21.171157836914062, Learning Rate: 0.01\n",
      "Epoch [15103/20000], Loss: -21.1729736328125, Learning Rate: 0.01\n",
      "Epoch [15104/20000], Loss: -21.173568725585938, Learning Rate: 0.01\n",
      "Epoch [15105/20000], Loss: -21.17315673828125, Learning Rate: 0.01\n",
      "Epoch [15106/20000], Loss: -21.172164916992188, Learning Rate: 0.01\n",
      "Epoch [15107/20000], Loss: -21.171157836914062, Learning Rate: 0.01\n",
      "Epoch [15108/20000], Loss: -21.170425415039062, Learning Rate: 0.01\n",
      "Epoch [15109/20000], Loss: -21.170196533203125, Learning Rate: 0.01\n",
      "Epoch [15110/20000], Loss: -21.170501708984375, Learning Rate: 0.01\n",
      "Epoch [15111/20000], Loss: -21.171401977539062, Learning Rate: 0.01\n",
      "Epoch [15112/20000], Loss: -21.172286987304688, Learning Rate: 0.01\n",
      "Epoch [15113/20000], Loss: -21.17333984375, Learning Rate: 0.01\n",
      "Epoch [15114/20000], Loss: -21.17401123046875, Learning Rate: 0.01\n",
      "Epoch [15115/20000], Loss: -21.1744384765625, Learning Rate: 0.01\n",
      "Epoch [15116/20000], Loss: -21.174636840820312, Learning Rate: 0.01\n",
      "Epoch [15117/20000], Loss: -21.17449951171875, Learning Rate: 0.01\n",
      "Epoch [15118/20000], Loss: -21.174362182617188, Learning Rate: 0.01\n",
      "Epoch [15119/20000], Loss: -21.174057006835938, Learning Rate: 0.01\n",
      "Epoch [15120/20000], Loss: -21.173934936523438, Learning Rate: 0.01\n",
      "Epoch [15121/20000], Loss: -21.173843383789062, Learning Rate: 0.01\n",
      "Epoch [15122/20000], Loss: -21.1739501953125, Learning Rate: 0.01\n",
      "Epoch [15123/20000], Loss: -21.174163818359375, Learning Rate: 0.01\n",
      "Epoch [15124/20000], Loss: -21.1744384765625, Learning Rate: 0.01\n",
      "Epoch [15125/20000], Loss: -21.174697875976562, Learning Rate: 0.01\n",
      "Epoch [15126/20000], Loss: -21.175048828125, Learning Rate: 0.01\n",
      "Epoch [15127/20000], Loss: -21.17529296875, Learning Rate: 0.01\n",
      "Epoch [15128/20000], Loss: -21.175567626953125, Learning Rate: 0.01\n",
      "Epoch [15129/20000], Loss: -21.175765991210938, Learning Rate: 0.01\n",
      "Epoch [15130/20000], Loss: -21.175933837890625, Learning Rate: 0.01\n",
      "Epoch [15131/20000], Loss: -21.17596435546875, Learning Rate: 0.01\n",
      "Epoch [15132/20000], Loss: -21.175949096679688, Learning Rate: 0.01\n",
      "Epoch [15133/20000], Loss: -21.176010131835938, Learning Rate: 0.01\n",
      "Epoch [15134/20000], Loss: -21.176040649414062, Learning Rate: 0.01\n",
      "Epoch [15135/20000], Loss: -21.176055908203125, Learning Rate: 0.01\n",
      "Epoch [15136/20000], Loss: -21.1761474609375, Learning Rate: 0.01\n",
      "Epoch [15137/20000], Loss: -21.17620849609375, Learning Rate: 0.01\n",
      "Epoch [15138/20000], Loss: -21.17626953125, Learning Rate: 0.01\n",
      "Epoch [15139/20000], Loss: -21.176315307617188, Learning Rate: 0.01\n",
      "Epoch [15140/20000], Loss: -21.176422119140625, Learning Rate: 0.01\n",
      "Epoch [15141/20000], Loss: -21.176528930664062, Learning Rate: 0.01\n",
      "Epoch [15142/20000], Loss: -21.176681518554688, Learning Rate: 0.01\n",
      "Epoch [15143/20000], Loss: -21.176712036132812, Learning Rate: 0.01\n",
      "Epoch [15144/20000], Loss: -21.1768798828125, Learning Rate: 0.01\n",
      "Epoch [15145/20000], Loss: -21.176986694335938, Learning Rate: 0.01\n",
      "Epoch [15146/20000], Loss: -21.17706298828125, Learning Rate: 0.01\n",
      "Epoch [15147/20000], Loss: -21.17718505859375, Learning Rate: 0.01\n",
      "Epoch [15148/20000], Loss: -21.177322387695312, Learning Rate: 0.01\n",
      "Epoch [15149/20000], Loss: -21.177474975585938, Learning Rate: 0.01\n",
      "Epoch [15150/20000], Loss: -21.177490234375, Learning Rate: 0.01\n",
      "Epoch [15151/20000], Loss: -21.177688598632812, Learning Rate: 0.01\n",
      "Epoch [15152/20000], Loss: -21.177734375, Learning Rate: 0.01\n",
      "Epoch [15153/20000], Loss: -21.177825927734375, Learning Rate: 0.01\n",
      "Epoch [15154/20000], Loss: -21.177902221679688, Learning Rate: 0.01\n",
      "Epoch [15155/20000], Loss: -21.178054809570312, Learning Rate: 0.01\n",
      "Epoch [15156/20000], Loss: -21.178146362304688, Learning Rate: 0.01\n",
      "Epoch [15157/20000], Loss: -21.178176879882812, Learning Rate: 0.01\n",
      "Epoch [15158/20000], Loss: -21.178268432617188, Learning Rate: 0.01\n",
      "Epoch [15159/20000], Loss: -21.17828369140625, Learning Rate: 0.01\n",
      "Epoch [15160/20000], Loss: -21.178436279296875, Learning Rate: 0.01\n",
      "Epoch [15161/20000], Loss: -21.178482055664062, Learning Rate: 0.01\n",
      "Epoch [15162/20000], Loss: -21.178512573242188, Learning Rate: 0.01\n",
      "Epoch [15163/20000], Loss: -21.178543090820312, Learning Rate: 0.01\n",
      "Epoch [15164/20000], Loss: -21.178604125976562, Learning Rate: 0.01\n",
      "Epoch [15165/20000], Loss: -21.178665161132812, Learning Rate: 0.01\n",
      "Epoch [15166/20000], Loss: -21.178665161132812, Learning Rate: 0.01\n",
      "Epoch [15167/20000], Loss: -21.178619384765625, Learning Rate: 0.01\n",
      "Epoch [15168/20000], Loss: -21.178543090820312, Learning Rate: 0.01\n",
      "Epoch [15169/20000], Loss: -21.17840576171875, Learning Rate: 0.01\n",
      "Epoch [15170/20000], Loss: -21.178253173828125, Learning Rate: 0.01\n",
      "Epoch [15171/20000], Loss: -21.177947998046875, Learning Rate: 0.01\n",
      "Epoch [15172/20000], Loss: -21.1773681640625, Learning Rate: 0.01\n",
      "Epoch [15173/20000], Loss: -21.176712036132812, Learning Rate: 0.01\n",
      "Epoch [15174/20000], Loss: -21.175613403320312, Learning Rate: 0.01\n",
      "Epoch [15175/20000], Loss: -21.174026489257812, Learning Rate: 0.01\n",
      "Epoch [15176/20000], Loss: -21.171615600585938, Learning Rate: 0.01\n",
      "Epoch [15177/20000], Loss: -21.168228149414062, Learning Rate: 0.01\n",
      "Epoch [15178/20000], Loss: -21.163070678710938, Learning Rate: 0.01\n",
      "Epoch [15179/20000], Loss: -21.155548095703125, Learning Rate: 0.01\n",
      "Epoch [15180/20000], Loss: -21.144210815429688, Learning Rate: 0.01\n",
      "Epoch [15181/20000], Loss: -21.12744140625, Learning Rate: 0.01\n",
      "Epoch [15182/20000], Loss: -21.102035522460938, Learning Rate: 0.01\n",
      "Epoch [15183/20000], Loss: -21.064208984375, Learning Rate: 0.01\n",
      "Epoch [15184/20000], Loss: -21.007003784179688, Learning Rate: 0.01\n",
      "Epoch [15185/20000], Loss: -20.922744750976562, Learning Rate: 0.01\n",
      "Epoch [15186/20000], Loss: -20.796783447265625, Learning Rate: 0.01\n",
      "Epoch [15187/20000], Loss: -20.61871337890625, Learning Rate: 0.01\n",
      "Epoch [15188/20000], Loss: -20.365737915039062, Learning Rate: 0.01\n",
      "Epoch [15189/20000], Loss: -20.046173095703125, Learning Rate: 0.01\n",
      "Epoch [15190/20000], Loss: -19.65704345703125, Learning Rate: 0.01\n",
      "Epoch [15191/20000], Loss: -19.30548095703125, Learning Rate: 0.01\n",
      "Epoch [15192/20000], Loss: -19.071609497070312, Learning Rate: 0.01\n",
      "Epoch [15193/20000], Loss: -19.177978515625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15194/20000], Loss: -19.619842529296875, Learning Rate: 0.01\n",
      "Epoch [15195/20000], Loss: -20.315353393554688, Learning Rate: 0.01\n",
      "Epoch [15196/20000], Loss: -20.919769287109375, Learning Rate: 0.01\n",
      "Epoch [15197/20000], Loss: -21.177352905273438, Learning Rate: 0.01\n",
      "Epoch [15198/20000], Loss: -21.043731689453125, Learning Rate: 0.01\n",
      "Epoch [15199/20000], Loss: -20.703872680664062, Learning Rate: 0.01\n",
      "Epoch [15200/20000], Loss: -20.433975219726562, Learning Rate: 0.01\n",
      "Epoch [15201/20000], Loss: -20.4122314453125, Learning Rate: 0.01\n",
      "Epoch [15202/20000], Loss: -20.656036376953125, Learning Rate: 0.01\n",
      "Epoch [15203/20000], Loss: -20.976165771484375, Learning Rate: 0.01\n",
      "Epoch [15204/20000], Loss: -21.167266845703125, Learning Rate: 0.01\n",
      "Epoch [15205/20000], Loss: -21.139694213867188, Learning Rate: 0.01\n",
      "Epoch [15206/20000], Loss: -20.973281860351562, Learning Rate: 0.01\n",
      "Epoch [15207/20000], Loss: -20.830429077148438, Learning Rate: 0.01\n",
      "Epoch [15208/20000], Loss: -20.822280883789062, Learning Rate: 0.01\n",
      "Epoch [15209/20000], Loss: -20.94940185546875, Learning Rate: 0.01\n",
      "Epoch [15210/20000], Loss: -21.10479736328125, Learning Rate: 0.01\n",
      "Epoch [15211/20000], Loss: -21.181503295898438, Learning Rate: 0.01\n",
      "Epoch [15212/20000], Loss: -21.148117065429688, Learning Rate: 0.01\n",
      "Epoch [15213/20000], Loss: -21.059600830078125, Learning Rate: 0.01\n",
      "Epoch [15214/20000], Loss: -21.000885009765625, Learning Rate: 0.01\n",
      "Epoch [15215/20000], Loss: -21.0166015625, Learning Rate: 0.01\n",
      "Epoch [15216/20000], Loss: -21.089584350585938, Learning Rate: 0.01\n",
      "Epoch [15217/20000], Loss: -21.160476684570312, Learning Rate: 0.01\n",
      "Epoch [15218/20000], Loss: -21.183013916015625, Learning Rate: 0.01\n",
      "Epoch [15219/20000], Loss: -21.15496826171875, Learning Rate: 0.01\n",
      "Epoch [15220/20000], Loss: -21.110916137695312, Learning Rate: 0.01\n",
      "Epoch [15221/20000], Loss: -21.090866088867188, Learning Rate: 0.01\n",
      "Epoch [15222/20000], Loss: -21.108154296875, Learning Rate: 0.01\n",
      "Epoch [15223/20000], Loss: -21.147048950195312, Learning Rate: 0.01\n",
      "Epoch [15224/20000], Loss: -21.177810668945312, Learning Rate: 0.01\n",
      "Epoch [15225/20000], Loss: -21.182693481445312, Learning Rate: 0.01\n",
      "Epoch [15226/20000], Loss: -21.165496826171875, Learning Rate: 0.01\n",
      "Epoch [15227/20000], Loss: -21.144866943359375, Learning Rate: 0.01\n",
      "Epoch [15228/20000], Loss: -21.138107299804688, Learning Rate: 0.01\n",
      "Epoch [15229/20000], Loss: -21.149032592773438, Learning Rate: 0.01\n",
      "Epoch [15230/20000], Loss: -21.168426513671875, Learning Rate: 0.01\n",
      "Epoch [15231/20000], Loss: -21.182388305664062, Learning Rate: 0.01\n",
      "Epoch [15232/20000], Loss: -21.183731079101562, Learning Rate: 0.01\n",
      "Epoch [15233/20000], Loss: -21.175079345703125, Learning Rate: 0.01\n",
      "Epoch [15234/20000], Loss: -21.165252685546875, Learning Rate: 0.01\n",
      "Epoch [15235/20000], Loss: -21.162155151367188, Learning Rate: 0.01\n",
      "Epoch [15236/20000], Loss: -21.167312622070312, Learning Rate: 0.01\n",
      "Epoch [15237/20000], Loss: -21.176620483398438, Learning Rate: 0.01\n",
      "Epoch [15238/20000], Loss: -21.183761596679688, Learning Rate: 0.01\n",
      "Epoch [15239/20000], Loss: -21.18505859375, Learning Rate: 0.01\n",
      "Epoch [15240/20000], Loss: -21.181549072265625, Learning Rate: 0.01\n",
      "Epoch [15241/20000], Loss: -21.176727294921875, Learning Rate: 0.01\n",
      "Epoch [15242/20000], Loss: -21.174484252929688, Learning Rate: 0.01\n",
      "Epoch [15243/20000], Loss: -21.1761474609375, Learning Rate: 0.01\n",
      "Epoch [15244/20000], Loss: -21.180328369140625, Learning Rate: 0.01\n",
      "Epoch [15245/20000], Loss: -21.184280395507812, Learning Rate: 0.01\n",
      "Epoch [15246/20000], Loss: -21.185897827148438, Learning Rate: 0.01\n",
      "Epoch [15247/20000], Loss: -21.184967041015625, Learning Rate: 0.01\n",
      "Epoch [15248/20000], Loss: -21.182830810546875, Learning Rate: 0.01\n",
      "Epoch [15249/20000], Loss: -21.181259155273438, Learning Rate: 0.01\n",
      "Epoch [15250/20000], Loss: -21.181137084960938, Learning Rate: 0.01\n",
      "Epoch [15251/20000], Loss: -21.182571411132812, Learning Rate: 0.01\n",
      "Epoch [15252/20000], Loss: -21.184677124023438, Learning Rate: 0.01\n",
      "Epoch [15253/20000], Loss: -21.186203002929688, Learning Rate: 0.01\n",
      "Epoch [15254/20000], Loss: -21.186538696289062, Learning Rate: 0.01\n",
      "Epoch [15255/20000], Loss: -21.185943603515625, Learning Rate: 0.01\n",
      "Epoch [15256/20000], Loss: -21.184967041015625, Learning Rate: 0.01\n",
      "Epoch [15257/20000], Loss: -21.1844482421875, Learning Rate: 0.01\n",
      "Epoch [15258/20000], Loss: -21.1845703125, Learning Rate: 0.01\n",
      "Epoch [15259/20000], Loss: -21.185333251953125, Learning Rate: 0.01\n",
      "Epoch [15260/20000], Loss: -21.186294555664062, Learning Rate: 0.01\n",
      "Epoch [15261/20000], Loss: -21.186965942382812, Learning Rate: 0.01\n",
      "Epoch [15262/20000], Loss: -21.187210083007812, Learning Rate: 0.01\n",
      "Epoch [15263/20000], Loss: -21.187057495117188, Learning Rate: 0.01\n",
      "Epoch [15264/20000], Loss: -21.1866455078125, Learning Rate: 0.01\n",
      "Epoch [15265/20000], Loss: -21.186416625976562, Learning Rate: 0.01\n",
      "Epoch [15266/20000], Loss: -21.186492919921875, Learning Rate: 0.01\n",
      "Epoch [15267/20000], Loss: -21.186813354492188, Learning Rate: 0.01\n",
      "Epoch [15268/20000], Loss: -21.187286376953125, Learning Rate: 0.01\n",
      "Epoch [15269/20000], Loss: -21.1876220703125, Learning Rate: 0.01\n",
      "Epoch [15270/20000], Loss: -21.187881469726562, Learning Rate: 0.01\n",
      "Epoch [15271/20000], Loss: -21.187881469726562, Learning Rate: 0.01\n",
      "Epoch [15272/20000], Loss: -21.18780517578125, Learning Rate: 0.01\n",
      "Epoch [15273/20000], Loss: -21.187713623046875, Learning Rate: 0.01\n",
      "Epoch [15274/20000], Loss: -21.187713623046875, Learning Rate: 0.01\n",
      "Epoch [15275/20000], Loss: -21.187820434570312, Learning Rate: 0.01\n",
      "Epoch [15276/20000], Loss: -21.188064575195312, Learning Rate: 0.01\n",
      "Epoch [15277/20000], Loss: -21.188262939453125, Learning Rate: 0.01\n",
      "Epoch [15278/20000], Loss: -21.18841552734375, Learning Rate: 0.01\n",
      "Epoch [15279/20000], Loss: -21.188522338867188, Learning Rate: 0.01\n",
      "Epoch [15280/20000], Loss: -21.18853759765625, Learning Rate: 0.01\n",
      "Epoch [15281/20000], Loss: -21.188522338867188, Learning Rate: 0.01\n",
      "Epoch [15282/20000], Loss: -21.18853759765625, Learning Rate: 0.01\n",
      "Epoch [15283/20000], Loss: -21.188491821289062, Learning Rate: 0.01\n",
      "Epoch [15284/20000], Loss: -21.188446044921875, Learning Rate: 0.01\n",
      "Epoch [15285/20000], Loss: -21.188446044921875, Learning Rate: 0.01\n",
      "Epoch [15286/20000], Loss: -21.188339233398438, Learning Rate: 0.01\n",
      "Epoch [15287/20000], Loss: -21.188156127929688, Learning Rate: 0.01\n",
      "Epoch [15288/20000], Loss: -21.187881469726562, Learning Rate: 0.01\n",
      "Epoch [15289/20000], Loss: -21.187362670898438, Learning Rate: 0.01\n",
      "Epoch [15290/20000], Loss: -21.186431884765625, Learning Rate: 0.01\n",
      "Epoch [15291/20000], Loss: -21.18511962890625, Learning Rate: 0.01\n",
      "Epoch [15292/20000], Loss: -21.1829833984375, Learning Rate: 0.01\n",
      "Epoch [15293/20000], Loss: -21.179641723632812, Learning Rate: 0.01\n",
      "Epoch [15294/20000], Loss: -21.17462158203125, Learning Rate: 0.01\n",
      "Epoch [15295/20000], Loss: -21.166778564453125, Learning Rate: 0.01\n",
      "Epoch [15296/20000], Loss: -21.154739379882812, Learning Rate: 0.01\n",
      "Epoch [15297/20000], Loss: -21.13592529296875, Learning Rate: 0.01\n",
      "Epoch [15298/20000], Loss: -21.106735229492188, Learning Rate: 0.01\n",
      "Epoch [15299/20000], Loss: -21.06158447265625, Learning Rate: 0.01\n",
      "Epoch [15300/20000], Loss: -20.9923095703125, Learning Rate: 0.01\n",
      "Epoch [15301/20000], Loss: -20.88897705078125, Learning Rate: 0.01\n",
      "Epoch [15302/20000], Loss: -20.738998413085938, Learning Rate: 0.01\n",
      "Epoch [15303/20000], Loss: -20.536056518554688, Learning Rate: 0.01\n",
      "Epoch [15304/20000], Loss: -20.286697387695312, Learning Rate: 0.01\n",
      "Epoch [15305/20000], Loss: -20.039077758789062, Learning Rate: 0.01\n",
      "Epoch [15306/20000], Loss: -19.882171630859375, Learning Rate: 0.01\n",
      "Epoch [15307/20000], Loss: -19.939727783203125, Learning Rate: 0.01\n",
      "Epoch [15308/20000], Loss: -20.25323486328125, Learning Rate: 0.01\n",
      "Epoch [15309/20000], Loss: -20.71343994140625, Learning Rate: 0.01\n",
      "Epoch [15310/20000], Loss: -21.081893920898438, Learning Rate: 0.01\n",
      "Epoch [15311/20000], Loss: -21.181793212890625, Learning Rate: 0.01\n",
      "Epoch [15312/20000], Loss: -21.028228759765625, Learning Rate: 0.01\n",
      "Epoch [15313/20000], Loss: -20.793777465820312, Learning Rate: 0.01\n",
      "Epoch [15314/20000], Loss: -20.677505493164062, Learning Rate: 0.01\n",
      "Epoch [15315/20000], Loss: -20.764846801757812, Learning Rate: 0.01\n",
      "Epoch [15316/20000], Loss: -20.98175048828125, Learning Rate: 0.01\n",
      "Epoch [15317/20000], Loss: -21.156341552734375, Learning Rate: 0.01\n",
      "Epoch [15318/20000], Loss: -21.175491333007812, Learning Rate: 0.01\n",
      "Epoch [15319/20000], Loss: -21.068023681640625, Learning Rate: 0.01\n",
      "Epoch [15320/20000], Loss: -20.95721435546875, Learning Rate: 0.01\n",
      "Epoch [15321/20000], Loss: -20.94744873046875, Learning Rate: 0.01\n",
      "Epoch [15322/20000], Loss: -21.0401611328125, Learning Rate: 0.01\n",
      "Epoch [15323/20000], Loss: -21.150833129882812, Learning Rate: 0.01\n",
      "Epoch [15324/20000], Loss: -21.192062377929688, Learning Rate: 0.01\n",
      "Epoch [15325/20000], Loss: -21.150497436523438, Learning Rate: 0.01\n",
      "Epoch [15326/20000], Loss: -21.084564208984375, Learning Rate: 0.01\n",
      "Epoch [15327/20000], Loss: -21.0614013671875, Learning Rate: 0.01\n",
      "Epoch [15328/20000], Loss: -21.0992431640625, Learning Rate: 0.01\n",
      "Epoch [15329/20000], Loss: -21.1593017578125, Learning Rate: 0.01\n",
      "Epoch [15330/20000], Loss: -21.190994262695312, Learning Rate: 0.01\n",
      "Epoch [15331/20000], Loss: -21.1767578125, Learning Rate: 0.01\n",
      "Epoch [15332/20000], Loss: -21.141830444335938, Learning Rate: 0.01\n",
      "Epoch [15333/20000], Loss: -21.123809814453125, Learning Rate: 0.01\n",
      "Epoch [15334/20000], Loss: -21.138381958007812, Learning Rate: 0.01\n",
      "Epoch [15335/20000], Loss: -21.170211791992188, Learning Rate: 0.01\n",
      "Epoch [15336/20000], Loss: -21.191497802734375, Learning Rate: 0.01\n",
      "Epoch [15337/20000], Loss: -21.188629150390625, Learning Rate: 0.01\n",
      "Epoch [15338/20000], Loss: -21.170623779296875, Learning Rate: 0.01\n",
      "Epoch [15339/20000], Loss: -21.157546997070312, Learning Rate: 0.01\n",
      "Epoch [15340/20000], Loss: -21.161117553710938, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15341/20000], Loss: -21.176727294921875, Learning Rate: 0.01\n",
      "Epoch [15342/20000], Loss: -21.190460205078125, Learning Rate: 0.01\n",
      "Epoch [15343/20000], Loss: -21.192642211914062, Learning Rate: 0.01\n",
      "Epoch [15344/20000], Loss: -21.184860229492188, Learning Rate: 0.01\n",
      "Epoch [15345/20000], Loss: -21.176376342773438, Learning Rate: 0.01\n",
      "Epoch [15346/20000], Loss: -21.175491333007812, Learning Rate: 0.01\n",
      "Epoch [15347/20000], Loss: -21.182144165039062, Learning Rate: 0.01\n",
      "Epoch [15348/20000], Loss: -21.1905517578125, Learning Rate: 0.01\n",
      "Epoch [15349/20000], Loss: -21.194259643554688, Learning Rate: 0.01\n",
      "Epoch [15350/20000], Loss: -21.19183349609375, Learning Rate: 0.01\n",
      "Epoch [15351/20000], Loss: -21.187042236328125, Learning Rate: 0.01\n",
      "Epoch [15352/20000], Loss: -21.184539794921875, Learning Rate: 0.01\n",
      "Epoch [15353/20000], Loss: -21.186294555664062, Learning Rate: 0.01\n",
      "Epoch [15354/20000], Loss: -21.190643310546875, Learning Rate: 0.01\n",
      "Epoch [15355/20000], Loss: -21.194000244140625, Learning Rate: 0.01\n",
      "Epoch [15356/20000], Loss: -21.19439697265625, Learning Rate: 0.01\n",
      "Epoch [15357/20000], Loss: -21.192413330078125, Learning Rate: 0.01\n",
      "Epoch [15358/20000], Loss: -21.190322875976562, Learning Rate: 0.01\n",
      "Epoch [15359/20000], Loss: -21.189956665039062, Learning Rate: 0.01\n",
      "Epoch [15360/20000], Loss: -21.191558837890625, Learning Rate: 0.01\n",
      "Epoch [15361/20000], Loss: -21.19378662109375, Learning Rate: 0.01\n",
      "Epoch [15362/20000], Loss: -21.195159912109375, Learning Rate: 0.01\n",
      "Epoch [15363/20000], Loss: -21.195022583007812, Learning Rate: 0.01\n",
      "Epoch [15364/20000], Loss: -21.193893432617188, Learning Rate: 0.01\n",
      "Epoch [15365/20000], Loss: -21.193099975585938, Learning Rate: 0.01\n",
      "Epoch [15366/20000], Loss: -21.19317626953125, Learning Rate: 0.01\n",
      "Epoch [15367/20000], Loss: -21.194061279296875, Learning Rate: 0.01\n",
      "Epoch [15368/20000], Loss: -21.195175170898438, Learning Rate: 0.01\n",
      "Epoch [15369/20000], Loss: -21.19580078125, Learning Rate: 0.01\n",
      "Epoch [15370/20000], Loss: -21.1956787109375, Learning Rate: 0.01\n",
      "Epoch [15371/20000], Loss: -21.195266723632812, Learning Rate: 0.01\n",
      "Epoch [15372/20000], Loss: -21.194869995117188, Learning Rate: 0.01\n",
      "Epoch [15373/20000], Loss: -21.194931030273438, Learning Rate: 0.01\n",
      "Epoch [15374/20000], Loss: -21.195388793945312, Learning Rate: 0.01\n",
      "Epoch [15375/20000], Loss: -21.195968627929688, Learning Rate: 0.01\n",
      "Epoch [15376/20000], Loss: -21.1962890625, Learning Rate: 0.01\n",
      "Epoch [15377/20000], Loss: -21.1964111328125, Learning Rate: 0.01\n",
      "Epoch [15378/20000], Loss: -21.196212768554688, Learning Rate: 0.01\n",
      "Epoch [15379/20000], Loss: -21.196075439453125, Learning Rate: 0.01\n",
      "Epoch [15380/20000], Loss: -21.196044921875, Learning Rate: 0.01\n",
      "Epoch [15381/20000], Loss: -21.1962890625, Learning Rate: 0.01\n",
      "Epoch [15382/20000], Loss: -21.196624755859375, Learning Rate: 0.01\n",
      "Epoch [15383/20000], Loss: -21.196823120117188, Learning Rate: 0.01\n",
      "Epoch [15384/20000], Loss: -21.196929931640625, Learning Rate: 0.01\n",
      "Epoch [15385/20000], Loss: -21.196975708007812, Learning Rate: 0.01\n",
      "Epoch [15386/20000], Loss: -21.196884155273438, Learning Rate: 0.01\n",
      "Epoch [15387/20000], Loss: -21.196945190429688, Learning Rate: 0.01\n",
      "Epoch [15388/20000], Loss: -21.1968994140625, Learning Rate: 0.01\n",
      "Epoch [15389/20000], Loss: -21.197036743164062, Learning Rate: 0.01\n",
      "Epoch [15390/20000], Loss: -21.19720458984375, Learning Rate: 0.01\n",
      "Epoch [15391/20000], Loss: -21.197296142578125, Learning Rate: 0.01\n",
      "Epoch [15392/20000], Loss: -21.197372436523438, Learning Rate: 0.01\n",
      "Epoch [15393/20000], Loss: -21.197280883789062, Learning Rate: 0.01\n",
      "Epoch [15394/20000], Loss: -21.197158813476562, Learning Rate: 0.01\n",
      "Epoch [15395/20000], Loss: -21.197052001953125, Learning Rate: 0.01\n",
      "Epoch [15396/20000], Loss: -21.196914672851562, Learning Rate: 0.01\n",
      "Epoch [15397/20000], Loss: -21.196609497070312, Learning Rate: 0.01\n",
      "Epoch [15398/20000], Loss: -21.196334838867188, Learning Rate: 0.01\n",
      "Epoch [15399/20000], Loss: -21.195770263671875, Learning Rate: 0.01\n",
      "Epoch [15400/20000], Loss: -21.19482421875, Learning Rate: 0.01\n",
      "Epoch [15401/20000], Loss: -21.193466186523438, Learning Rate: 0.01\n",
      "Epoch [15402/20000], Loss: -21.191375732421875, Learning Rate: 0.01\n",
      "Epoch [15403/20000], Loss: -21.1883544921875, Learning Rate: 0.01\n",
      "Epoch [15404/20000], Loss: -21.18377685546875, Learning Rate: 0.01\n",
      "Epoch [15405/20000], Loss: -21.177108764648438, Learning Rate: 0.01\n",
      "Epoch [15406/20000], Loss: -21.166732788085938, Learning Rate: 0.01\n",
      "Epoch [15407/20000], Loss: -21.151229858398438, Learning Rate: 0.01\n",
      "Epoch [15408/20000], Loss: -21.127548217773438, Learning Rate: 0.01\n",
      "Epoch [15409/20000], Loss: -21.091873168945312, Learning Rate: 0.01\n",
      "Epoch [15410/20000], Loss: -21.036911010742188, Learning Rate: 0.01\n",
      "Epoch [15411/20000], Loss: -20.954849243164062, Learning Rate: 0.01\n",
      "Epoch [15412/20000], Loss: -20.8304443359375, Learning Rate: 0.01\n",
      "Epoch [15413/20000], Loss: -20.651092529296875, Learning Rate: 0.01\n",
      "Epoch [15414/20000], Loss: -20.3916015625, Learning Rate: 0.01\n",
      "Epoch [15415/20000], Loss: -20.055526733398438, Learning Rate: 0.01\n",
      "Epoch [15416/20000], Loss: -19.63543701171875, Learning Rate: 0.01\n",
      "Epoch [15417/20000], Loss: -19.240463256835938, Learning Rate: 0.01\n",
      "Epoch [15418/20000], Loss: -18.9588623046875, Learning Rate: 0.01\n",
      "Epoch [15419/20000], Loss: -19.045761108398438, Learning Rate: 0.01\n",
      "Epoch [15420/20000], Loss: -19.510589599609375, Learning Rate: 0.01\n",
      "Epoch [15421/20000], Loss: -20.270904541015625, Learning Rate: 0.01\n",
      "Epoch [15422/20000], Loss: -20.931777954101562, Learning Rate: 0.01\n",
      "Epoch [15423/20000], Loss: -21.1981201171875, Learning Rate: 0.01\n",
      "Epoch [15424/20000], Loss: -21.027023315429688, Learning Rate: 0.01\n",
      "Epoch [15425/20000], Loss: -20.643096923828125, Learning Rate: 0.01\n",
      "Epoch [15426/20000], Loss: -20.368118286132812, Learning Rate: 0.01\n",
      "Epoch [15427/20000], Loss: -20.389724731445312, Learning Rate: 0.01\n",
      "Epoch [15428/20000], Loss: -20.694122314453125, Learning Rate: 0.01\n",
      "Epoch [15429/20000], Loss: -21.038848876953125, Learning Rate: 0.01\n",
      "Epoch [15430/20000], Loss: -21.196853637695312, Learning Rate: 0.01\n",
      "Epoch [15431/20000], Loss: -21.10986328125, Learning Rate: 0.01\n",
      "Epoch [15432/20000], Loss: -20.91046142578125, Learning Rate: 0.01\n",
      "Epoch [15433/20000], Loss: -20.7913818359375, Learning Rate: 0.01\n",
      "Epoch [15434/20000], Loss: -20.843307495117188, Learning Rate: 0.01\n",
      "Epoch [15435/20000], Loss: -21.016189575195312, Learning Rate: 0.01\n",
      "Epoch [15436/20000], Loss: -21.16522216796875, Learning Rate: 0.01\n",
      "Epoch [15437/20000], Loss: -21.19232177734375, Learning Rate: 0.01\n",
      "Epoch [15438/20000], Loss: -21.111465454101562, Learning Rate: 0.01\n",
      "Epoch [15439/20000], Loss: -21.017364501953125, Learning Rate: 0.01\n",
      "Epoch [15440/20000], Loss: -20.99761962890625, Learning Rate: 0.01\n",
      "Epoch [15441/20000], Loss: -21.061553955078125, Learning Rate: 0.01\n",
      "Epoch [15442/20000], Loss: -21.15203857421875, Learning Rate: 0.01\n",
      "Epoch [15443/20000], Loss: -21.1986083984375, Learning Rate: 0.01\n",
      "Epoch [15444/20000], Loss: -21.179473876953125, Learning Rate: 0.01\n",
      "Epoch [15445/20000], Loss: -21.128067016601562, Learning Rate: 0.01\n",
      "Epoch [15446/20000], Loss: -21.09600830078125, Learning Rate: 0.01\n",
      "Epoch [15447/20000], Loss: -21.110275268554688, Learning Rate: 0.01\n",
      "Epoch [15448/20000], Loss: -21.154693603515625, Learning Rate: 0.01\n",
      "Epoch [15449/20000], Loss: -21.19287109375, Learning Rate: 0.01\n",
      "Epoch [15450/20000], Loss: -21.199661254882812, Learning Rate: 0.01\n",
      "Epoch [15451/20000], Loss: -21.178726196289062, Learning Rate: 0.01\n",
      "Epoch [15452/20000], Loss: -21.154266357421875, Learning Rate: 0.01\n",
      "Epoch [15453/20000], Loss: -21.148117065429688, Learning Rate: 0.01\n",
      "Epoch [15454/20000], Loss: -21.1640625, Learning Rate: 0.01\n",
      "Epoch [15455/20000], Loss: -21.187667846679688, Learning Rate: 0.01\n",
      "Epoch [15456/20000], Loss: -21.201583862304688, Learning Rate: 0.01\n",
      "Epoch [15457/20000], Loss: -21.19879150390625, Learning Rate: 0.01\n",
      "Epoch [15458/20000], Loss: -21.186004638671875, Learning Rate: 0.01\n",
      "Epoch [15459/20000], Loss: -21.176010131835938, Learning Rate: 0.01\n",
      "Epoch [15460/20000], Loss: -21.17657470703125, Learning Rate: 0.01\n",
      "Epoch [15461/20000], Loss: -21.186492919921875, Learning Rate: 0.01\n",
      "Epoch [15462/20000], Loss: -21.197845458984375, Learning Rate: 0.01\n",
      "Epoch [15463/20000], Loss: -21.20306396484375, Learning Rate: 0.01\n",
      "Epoch [15464/20000], Loss: -21.200408935546875, Learning Rate: 0.01\n",
      "Epoch [15465/20000], Loss: -21.1939697265625, Learning Rate: 0.01\n",
      "Epoch [15466/20000], Loss: -21.189651489257812, Learning Rate: 0.01\n",
      "Epoch [15467/20000], Loss: -21.190582275390625, Learning Rate: 0.01\n",
      "Epoch [15468/20000], Loss: -21.195724487304688, Learning Rate: 0.01\n",
      "Epoch [15469/20000], Loss: -21.201141357421875, Learning Rate: 0.01\n",
      "Epoch [15470/20000], Loss: -21.203521728515625, Learning Rate: 0.01\n",
      "Epoch [15471/20000], Loss: -21.202255249023438, Learning Rate: 0.01\n",
      "Epoch [15472/20000], Loss: -21.199325561523438, Learning Rate: 0.01\n",
      "Epoch [15473/20000], Loss: -21.197158813476562, Learning Rate: 0.01\n",
      "Epoch [15474/20000], Loss: -21.197525024414062, Learning Rate: 0.01\n",
      "Epoch [15475/20000], Loss: -21.1998291015625, Learning Rate: 0.01\n",
      "Epoch [15476/20000], Loss: -21.202499389648438, Learning Rate: 0.01\n",
      "Epoch [15477/20000], Loss: -21.203933715820312, Learning Rate: 0.01\n",
      "Epoch [15478/20000], Loss: -21.203842163085938, Learning Rate: 0.01\n",
      "Epoch [15479/20000], Loss: -21.202560424804688, Learning Rate: 0.01\n",
      "Epoch [15480/20000], Loss: -21.201324462890625, Learning Rate: 0.01\n",
      "Epoch [15481/20000], Loss: -21.201126098632812, Learning Rate: 0.01\n",
      "Epoch [15482/20000], Loss: -21.201950073242188, Learning Rate: 0.01\n",
      "Epoch [15483/20000], Loss: -21.20330810546875, Learning Rate: 0.01\n",
      "Epoch [15484/20000], Loss: -21.204345703125, Learning Rate: 0.01\n",
      "Epoch [15485/20000], Loss: -21.204666137695312, Learning Rate: 0.01\n",
      "Epoch [15486/20000], Loss: -21.204345703125, Learning Rate: 0.01\n",
      "Epoch [15487/20000], Loss: -21.203704833984375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15488/20000], Loss: -21.203399658203125, Learning Rate: 0.01\n",
      "Epoch [15489/20000], Loss: -21.203475952148438, Learning Rate: 0.01\n",
      "Epoch [15490/20000], Loss: -21.204071044921875, Learning Rate: 0.01\n",
      "Epoch [15491/20000], Loss: -21.204681396484375, Learning Rate: 0.01\n",
      "Epoch [15492/20000], Loss: -21.2052001953125, Learning Rate: 0.01\n",
      "Epoch [15493/20000], Loss: -21.20526123046875, Learning Rate: 0.01\n",
      "Epoch [15494/20000], Loss: -21.205123901367188, Learning Rate: 0.01\n",
      "Epoch [15495/20000], Loss: -21.20489501953125, Learning Rate: 0.01\n",
      "Epoch [15496/20000], Loss: -21.204849243164062, Learning Rate: 0.01\n",
      "Epoch [15497/20000], Loss: -21.204910278320312, Learning Rate: 0.01\n",
      "Epoch [15498/20000], Loss: -21.205184936523438, Learning Rate: 0.01\n",
      "Epoch [15499/20000], Loss: -21.205474853515625, Learning Rate: 0.01\n",
      "Epoch [15500/20000], Loss: -21.205703735351562, Learning Rate: 0.01\n",
      "Epoch [15501/20000], Loss: -21.20587158203125, Learning Rate: 0.01\n",
      "Epoch [15502/20000], Loss: -21.205810546875, Learning Rate: 0.01\n",
      "Epoch [15503/20000], Loss: -21.205841064453125, Learning Rate: 0.01\n",
      "Epoch [15504/20000], Loss: -21.205764770507812, Learning Rate: 0.01\n",
      "Epoch [15505/20000], Loss: -21.205856323242188, Learning Rate: 0.01\n",
      "Epoch [15506/20000], Loss: -21.205947875976562, Learning Rate: 0.01\n",
      "Epoch [15507/20000], Loss: -21.206130981445312, Learning Rate: 0.01\n",
      "Epoch [15508/20000], Loss: -21.206314086914062, Learning Rate: 0.01\n",
      "Epoch [15509/20000], Loss: -21.206451416015625, Learning Rate: 0.01\n",
      "Epoch [15510/20000], Loss: -21.206497192382812, Learning Rate: 0.01\n",
      "Epoch [15511/20000], Loss: -21.206588745117188, Learning Rate: 0.01\n",
      "Epoch [15512/20000], Loss: -21.206588745117188, Learning Rate: 0.01\n",
      "Epoch [15513/20000], Loss: -21.206588745117188, Learning Rate: 0.01\n",
      "Epoch [15514/20000], Loss: -21.2066650390625, Learning Rate: 0.01\n",
      "Epoch [15515/20000], Loss: -21.206787109375, Learning Rate: 0.01\n",
      "Epoch [15516/20000], Loss: -21.206924438476562, Learning Rate: 0.01\n",
      "Epoch [15517/20000], Loss: -21.206985473632812, Learning Rate: 0.01\n",
      "Epoch [15518/20000], Loss: -21.207061767578125, Learning Rate: 0.01\n",
      "Epoch [15519/20000], Loss: -21.207168579101562, Learning Rate: 0.01\n",
      "Epoch [15520/20000], Loss: -21.207199096679688, Learning Rate: 0.01\n",
      "Epoch [15521/20000], Loss: -21.207305908203125, Learning Rate: 0.01\n",
      "Epoch [15522/20000], Loss: -21.207382202148438, Learning Rate: 0.01\n",
      "Epoch [15523/20000], Loss: -21.20745849609375, Learning Rate: 0.01\n",
      "Epoch [15524/20000], Loss: -21.207427978515625, Learning Rate: 0.01\n",
      "Epoch [15525/20000], Loss: -21.207595825195312, Learning Rate: 0.01\n",
      "Epoch [15526/20000], Loss: -21.207595825195312, Learning Rate: 0.01\n",
      "Epoch [15527/20000], Loss: -21.207748413085938, Learning Rate: 0.01\n",
      "Epoch [15528/20000], Loss: -21.207809448242188, Learning Rate: 0.01\n",
      "Epoch [15529/20000], Loss: -21.207855224609375, Learning Rate: 0.01\n",
      "Epoch [15530/20000], Loss: -21.207962036132812, Learning Rate: 0.01\n",
      "Epoch [15531/20000], Loss: -21.2080078125, Learning Rate: 0.01\n",
      "Epoch [15532/20000], Loss: -21.20806884765625, Learning Rate: 0.01\n",
      "Epoch [15533/20000], Loss: -21.20819091796875, Learning Rate: 0.01\n",
      "Epoch [15534/20000], Loss: -21.20819091796875, Learning Rate: 0.01\n",
      "Epoch [15535/20000], Loss: -21.208236694335938, Learning Rate: 0.01\n",
      "Epoch [15536/20000], Loss: -21.208343505859375, Learning Rate: 0.01\n",
      "Epoch [15537/20000], Loss: -21.208450317382812, Learning Rate: 0.01\n",
      "Epoch [15538/20000], Loss: -21.208480834960938, Learning Rate: 0.01\n",
      "Epoch [15539/20000], Loss: -21.2086181640625, Learning Rate: 0.01\n",
      "Epoch [15540/20000], Loss: -21.208740234375, Learning Rate: 0.01\n",
      "Epoch [15541/20000], Loss: -21.208755493164062, Learning Rate: 0.01\n",
      "Epoch [15542/20000], Loss: -21.208786010742188, Learning Rate: 0.01\n",
      "Epoch [15543/20000], Loss: -21.20892333984375, Learning Rate: 0.01\n",
      "Epoch [15544/20000], Loss: -21.208953857421875, Learning Rate: 0.01\n",
      "Epoch [15545/20000], Loss: -21.209030151367188, Learning Rate: 0.01\n",
      "Epoch [15546/20000], Loss: -21.209091186523438, Learning Rate: 0.01\n",
      "Epoch [15547/20000], Loss: -21.209213256835938, Learning Rate: 0.01\n",
      "Epoch [15548/20000], Loss: -21.209274291992188, Learning Rate: 0.01\n",
      "Epoch [15549/20000], Loss: -21.2093505859375, Learning Rate: 0.01\n",
      "Epoch [15550/20000], Loss: -21.20941162109375, Learning Rate: 0.01\n",
      "Epoch [15551/20000], Loss: -21.209457397460938, Learning Rate: 0.01\n",
      "Epoch [15552/20000], Loss: -21.209609985351562, Learning Rate: 0.01\n",
      "Epoch [15553/20000], Loss: -21.209625244140625, Learning Rate: 0.01\n",
      "Epoch [15554/20000], Loss: -21.209625244140625, Learning Rate: 0.01\n",
      "Epoch [15555/20000], Loss: -21.20977783203125, Learning Rate: 0.01\n",
      "Epoch [15556/20000], Loss: -21.209808349609375, Learning Rate: 0.01\n",
      "Epoch [15557/20000], Loss: -21.209854125976562, Learning Rate: 0.01\n",
      "Epoch [15558/20000], Loss: -21.2099609375, Learning Rate: 0.01\n",
      "Epoch [15559/20000], Loss: -21.210037231445312, Learning Rate: 0.01\n",
      "Epoch [15560/20000], Loss: -21.2100830078125, Learning Rate: 0.01\n",
      "Epoch [15561/20000], Loss: -21.210220336914062, Learning Rate: 0.01\n",
      "Epoch [15562/20000], Loss: -21.210189819335938, Learning Rate: 0.01\n",
      "Epoch [15563/20000], Loss: -21.210250854492188, Learning Rate: 0.01\n",
      "Epoch [15564/20000], Loss: -21.2103271484375, Learning Rate: 0.01\n",
      "Epoch [15565/20000], Loss: -21.21038818359375, Learning Rate: 0.01\n",
      "Epoch [15566/20000], Loss: -21.210372924804688, Learning Rate: 0.01\n",
      "Epoch [15567/20000], Loss: -21.210479736328125, Learning Rate: 0.01\n",
      "Epoch [15568/20000], Loss: -21.2103271484375, Learning Rate: 0.01\n",
      "Epoch [15569/20000], Loss: -21.210250854492188, Learning Rate: 0.01\n",
      "Epoch [15570/20000], Loss: -21.2100830078125, Learning Rate: 0.01\n",
      "Epoch [15571/20000], Loss: -21.209732055664062, Learning Rate: 0.01\n",
      "Epoch [15572/20000], Loss: -21.209091186523438, Learning Rate: 0.01\n",
      "Epoch [15573/20000], Loss: -21.208023071289062, Learning Rate: 0.01\n",
      "Epoch [15574/20000], Loss: -21.206222534179688, Learning Rate: 0.01\n",
      "Epoch [15575/20000], Loss: -21.203353881835938, Learning Rate: 0.01\n",
      "Epoch [15576/20000], Loss: -21.19854736328125, Learning Rate: 0.01\n",
      "Epoch [15577/20000], Loss: -21.19061279296875, Learning Rate: 0.01\n",
      "Epoch [15578/20000], Loss: -21.177337646484375, Learning Rate: 0.01\n",
      "Epoch [15579/20000], Loss: -21.155319213867188, Learning Rate: 0.01\n",
      "Epoch [15580/20000], Loss: -21.118682861328125, Learning Rate: 0.01\n",
      "Epoch [15581/20000], Loss: -21.058364868164062, Learning Rate: 0.01\n",
      "Epoch [15582/20000], Loss: -20.959579467773438, Learning Rate: 0.01\n",
      "Epoch [15583/20000], Loss: -20.80206298828125, Learning Rate: 0.01\n",
      "Epoch [15584/20000], Loss: -20.561050415039062, Learning Rate: 0.01\n",
      "Epoch [15585/20000], Loss: -20.222259521484375, Learning Rate: 0.01\n",
      "Epoch [15586/20000], Loss: -19.809326171875, Learning Rate: 0.01\n",
      "Epoch [15587/20000], Loss: -19.442352294921875, Learning Rate: 0.01\n",
      "Epoch [15588/20000], Loss: -19.33734130859375, Learning Rate: 0.01\n",
      "Epoch [15589/20000], Loss: -19.686553955078125, Learning Rate: 0.01\n",
      "Epoch [15590/20000], Loss: -20.383148193359375, Learning Rate: 0.01\n",
      "Epoch [15591/20000], Loss: -20.986984252929688, Learning Rate: 0.01\n",
      "Epoch [15592/20000], Loss: -21.11334228515625, Learning Rate: 0.01\n",
      "Epoch [15593/20000], Loss: -20.794784545898438, Learning Rate: 0.01\n",
      "Epoch [15594/20000], Loss: -20.423477172851562, Learning Rate: 0.01\n",
      "Epoch [15595/20000], Loss: -20.372268676757812, Learning Rate: 0.01\n",
      "Epoch [15596/20000], Loss: -20.7010498046875, Learning Rate: 0.01\n",
      "Epoch [15597/20000], Loss: -21.072540283203125, Learning Rate: 0.01\n",
      "Epoch [15598/20000], Loss: -21.157958984375, Learning Rate: 0.01\n",
      "Epoch [15599/20000], Loss: -20.964950561523438, Learning Rate: 0.01\n",
      "Epoch [15600/20000], Loss: -20.783187866210938, Learning Rate: 0.01\n",
      "Epoch [15601/20000], Loss: -20.837921142578125, Learning Rate: 0.01\n",
      "Epoch [15602/20000], Loss: -21.061553955078125, Learning Rate: 0.01\n",
      "Epoch [15603/20000], Loss: -21.2076416015625, Learning Rate: 0.01\n",
      "Epoch [15604/20000], Loss: -21.153121948242188, Learning Rate: 0.01\n",
      "Epoch [15605/20000], Loss: -21.015579223632812, Learning Rate: 0.01\n",
      "Epoch [15606/20000], Loss: -20.980148315429688, Learning Rate: 0.01\n",
      "Epoch [15607/20000], Loss: -21.081085205078125, Learning Rate: 0.01\n",
      "Epoch [15608/20000], Loss: -21.190414428710938, Learning Rate: 0.01\n",
      "Epoch [15609/20000], Loss: -21.191299438476562, Learning Rate: 0.01\n",
      "Epoch [15610/20000], Loss: -21.111831665039062, Learning Rate: 0.01\n",
      "Epoch [15611/20000], Loss: -21.066177368164062, Learning Rate: 0.01\n",
      "Epoch [15612/20000], Loss: -21.107376098632812, Learning Rate: 0.01\n",
      "Epoch [15613/20000], Loss: -21.179779052734375, Learning Rate: 0.01\n",
      "Epoch [15614/20000], Loss: -21.200927734375, Learning Rate: 0.01\n",
      "Epoch [15615/20000], Loss: -21.163192749023438, Learning Rate: 0.01\n",
      "Epoch [15616/20000], Loss: -21.127716064453125, Learning Rate: 0.01\n",
      "Epoch [15617/20000], Loss: -21.141326904296875, Learning Rate: 0.01\n",
      "Epoch [15618/20000], Loss: -21.18511962890625, Learning Rate: 0.01\n",
      "Epoch [15619/20000], Loss: -21.208633422851562, Learning Rate: 0.01\n",
      "Epoch [15620/20000], Loss: -21.193954467773438, Learning Rate: 0.01\n",
      "Epoch [15621/20000], Loss: -21.169692993164062, Learning Rate: 0.01\n",
      "Epoch [15622/20000], Loss: -21.169448852539062, Learning Rate: 0.01\n",
      "Epoch [15623/20000], Loss: -21.192794799804688, Learning Rate: 0.01\n",
      "Epoch [15624/20000], Loss: -21.211883544921875, Learning Rate: 0.01\n",
      "Epoch [15625/20000], Loss: -21.208877563476562, Learning Rate: 0.01\n",
      "Epoch [15626/20000], Loss: -21.19378662109375, Learning Rate: 0.01\n",
      "Epoch [15627/20000], Loss: -21.188186645507812, Learning Rate: 0.01\n",
      "Epoch [15628/20000], Loss: -21.19842529296875, Learning Rate: 0.01\n",
      "Epoch [15629/20000], Loss: -21.211776733398438, Learning Rate: 0.01\n",
      "Epoch [15630/20000], Loss: -21.214248657226562, Learning Rate: 0.01\n",
      "Epoch [15631/20000], Loss: -21.206512451171875, Learning Rate: 0.01\n",
      "Epoch [15632/20000], Loss: -21.200103759765625, Learning Rate: 0.01\n",
      "Epoch [15633/20000], Loss: -21.202987670898438, Learning Rate: 0.01\n",
      "Epoch [15634/20000], Loss: -21.211013793945312, Learning Rate: 0.01\n",
      "Epoch [15635/20000], Loss: -21.215316772460938, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15636/20000], Loss: -21.212615966796875, Learning Rate: 0.01\n",
      "Epoch [15637/20000], Loss: -21.207855224609375, Learning Rate: 0.01\n",
      "Epoch [15638/20000], Loss: -21.207015991210938, Learning Rate: 0.01\n",
      "Epoch [15639/20000], Loss: -21.2108154296875, Learning Rate: 0.01\n",
      "Epoch [15640/20000], Loss: -21.214752197265625, Learning Rate: 0.01\n",
      "Epoch [15641/20000], Loss: -21.215057373046875, Learning Rate: 0.01\n",
      "Epoch [15642/20000], Loss: -21.212356567382812, Learning Rate: 0.01\n",
      "Epoch [15643/20000], Loss: -21.210403442382812, Learning Rate: 0.01\n",
      "Epoch [15644/20000], Loss: -21.211212158203125, Learning Rate: 0.01\n",
      "Epoch [15645/20000], Loss: -21.213546752929688, Learning Rate: 0.01\n",
      "Epoch [15646/20000], Loss: -21.214691162109375, Learning Rate: 0.01\n",
      "Epoch [15647/20000], Loss: -21.213653564453125, Learning Rate: 0.01\n",
      "Epoch [15648/20000], Loss: -21.2115478515625, Learning Rate: 0.01\n",
      "Epoch [15649/20000], Loss: -21.210250854492188, Learning Rate: 0.01\n",
      "Epoch [15650/20000], Loss: -21.210174560546875, Learning Rate: 0.01\n",
      "Epoch [15651/20000], Loss: -21.209915161132812, Learning Rate: 0.01\n",
      "Epoch [15652/20000], Loss: -21.20806884765625, Learning Rate: 0.01\n",
      "Epoch [15653/20000], Loss: -21.204086303710938, Learning Rate: 0.01\n",
      "Epoch [15654/20000], Loss: -21.19873046875, Learning Rate: 0.01\n",
      "Epoch [15655/20000], Loss: -21.191802978515625, Learning Rate: 0.01\n",
      "Epoch [15656/20000], Loss: -21.182632446289062, Learning Rate: 0.01\n",
      "Epoch [15657/20000], Loss: -21.168716430664062, Learning Rate: 0.01\n",
      "Epoch [15658/20000], Loss: -21.147506713867188, Learning Rate: 0.01\n",
      "Epoch [15659/20000], Loss: -21.115570068359375, Learning Rate: 0.01\n",
      "Epoch [15660/20000], Loss: -21.069412231445312, Learning Rate: 0.01\n",
      "Epoch [15661/20000], Loss: -21.001663208007812, Learning Rate: 0.01\n",
      "Epoch [15662/20000], Loss: -20.905380249023438, Learning Rate: 0.01\n",
      "Epoch [15663/20000], Loss: -20.765472412109375, Learning Rate: 0.01\n",
      "Epoch [15664/20000], Loss: -20.576446533203125, Learning Rate: 0.01\n",
      "Epoch [15665/20000], Loss: -20.320953369140625, Learning Rate: 0.01\n",
      "Epoch [15666/20000], Loss: -20.0245361328125, Learning Rate: 0.01\n",
      "Epoch [15667/20000], Loss: -19.69921875, Learning Rate: 0.01\n",
      "Epoch [15668/20000], Loss: -19.469024658203125, Learning Rate: 0.01\n",
      "Epoch [15669/20000], Loss: -19.399673461914062, Learning Rate: 0.01\n",
      "Epoch [15670/20000], Loss: -19.646209716796875, Learning Rate: 0.01\n",
      "Epoch [15671/20000], Loss: -20.128311157226562, Learning Rate: 0.01\n",
      "Epoch [15672/20000], Loss: -20.706771850585938, Learning Rate: 0.01\n",
      "Epoch [15673/20000], Loss: -21.114715576171875, Learning Rate: 0.01\n",
      "Epoch [15674/20000], Loss: -21.211639404296875, Learning Rate: 0.01\n",
      "Epoch [15675/20000], Loss: -21.033660888671875, Learning Rate: 0.01\n",
      "Epoch [15676/20000], Loss: -20.752410888671875, Learning Rate: 0.01\n",
      "Epoch [15677/20000], Loss: -20.571243286132812, Learning Rate: 0.01\n",
      "Epoch [15678/20000], Loss: -20.59344482421875, Learning Rate: 0.01\n",
      "Epoch [15679/20000], Loss: -20.80615234375, Learning Rate: 0.01\n",
      "Epoch [15680/20000], Loss: -21.059158325195312, Learning Rate: 0.01\n",
      "Epoch [15681/20000], Loss: -21.20660400390625, Learning Rate: 0.01\n",
      "Epoch [15682/20000], Loss: -21.18878173828125, Learning Rate: 0.01\n",
      "Epoch [15683/20000], Loss: -21.062850952148438, Learning Rate: 0.01\n",
      "Epoch [15684/20000], Loss: -20.944381713867188, Learning Rate: 0.01\n",
      "Epoch [15685/20000], Loss: -20.9189453125, Learning Rate: 0.01\n",
      "Epoch [15686/20000], Loss: -21.000457763671875, Learning Rate: 0.01\n",
      "Epoch [15687/20000], Loss: -21.122879028320312, Learning Rate: 0.01\n",
      "Epoch [15688/20000], Loss: -21.206802368164062, Learning Rate: 0.01\n",
      "Epoch [15689/20000], Loss: -21.210723876953125, Learning Rate: 0.01\n",
      "Epoch [15690/20000], Loss: -21.1544189453125, Learning Rate: 0.01\n",
      "Epoch [15691/20000], Loss: -21.093399047851562, Learning Rate: 0.01\n",
      "Epoch [15692/20000], Loss: -21.074447631835938, Learning Rate: 0.01\n",
      "Epoch [15693/20000], Loss: -21.108627319335938, Learning Rate: 0.01\n",
      "Epoch [15694/20000], Loss: -21.167007446289062, Learning Rate: 0.01\n",
      "Epoch [15695/20000], Loss: -21.210845947265625, Learning Rate: 0.01\n",
      "Epoch [15696/20000], Loss: -21.217437744140625, Learning Rate: 0.01\n",
      "Epoch [15697/20000], Loss: -21.192916870117188, Learning Rate: 0.01\n",
      "Epoch [15698/20000], Loss: -21.16241455078125, Learning Rate: 0.01\n",
      "Epoch [15699/20000], Loss: -21.149566650390625, Learning Rate: 0.01\n",
      "Epoch [15700/20000], Loss: -21.1622314453125, Learning Rate: 0.01\n",
      "Epoch [15701/20000], Loss: -21.189224243164062, Learning Rate: 0.01\n",
      "Epoch [15702/20000], Loss: -21.212661743164062, Learning Rate: 0.01\n",
      "Epoch [15703/20000], Loss: -21.22003173828125, Learning Rate: 0.01\n",
      "Epoch [15704/20000], Loss: -21.211273193359375, Learning Rate: 0.01\n",
      "Epoch [15705/20000], Loss: -21.196578979492188, Learning Rate: 0.01\n",
      "Epoch [15706/20000], Loss: -21.187423706054688, Learning Rate: 0.01\n",
      "Epoch [15707/20000], Loss: -21.189666748046875, Learning Rate: 0.01\n",
      "Epoch [15708/20000], Loss: -21.200851440429688, Learning Rate: 0.01\n",
      "Epoch [15709/20000], Loss: -21.213241577148438, Learning Rate: 0.01\n",
      "Epoch [15710/20000], Loss: -21.2200927734375, Learning Rate: 0.01\n",
      "Epoch [15711/20000], Loss: -21.219100952148438, Learning Rate: 0.01\n",
      "Epoch [15712/20000], Loss: -21.213043212890625, Learning Rate: 0.01\n",
      "Epoch [15713/20000], Loss: -21.20703125, Learning Rate: 0.01\n",
      "Epoch [15714/20000], Loss: -21.205078125, Learning Rate: 0.01\n",
      "Epoch [15715/20000], Loss: -21.2081298828125, Learning Rate: 0.01\n",
      "Epoch [15716/20000], Loss: -21.213882446289062, Learning Rate: 0.01\n",
      "Epoch [15717/20000], Loss: -21.219009399414062, Learning Rate: 0.01\n",
      "Epoch [15718/20000], Loss: -21.221084594726562, Learning Rate: 0.01\n",
      "Epoch [15719/20000], Loss: -21.219970703125, Learning Rate: 0.01\n",
      "Epoch [15720/20000], Loss: -21.21710205078125, Learning Rate: 0.01\n",
      "Epoch [15721/20000], Loss: -21.21478271484375, Learning Rate: 0.01\n",
      "Epoch [15722/20000], Loss: -21.214263916015625, Learning Rate: 0.01\n",
      "Epoch [15723/20000], Loss: -21.215728759765625, Learning Rate: 0.01\n",
      "Epoch [15724/20000], Loss: -21.218307495117188, Learning Rate: 0.01\n",
      "Epoch [15725/20000], Loss: -21.220535278320312, Learning Rate: 0.01\n",
      "Epoch [15726/20000], Loss: -21.221588134765625, Learning Rate: 0.01\n",
      "Epoch [15727/20000], Loss: -21.2213134765625, Learning Rate: 0.01\n",
      "Epoch [15728/20000], Loss: -21.220138549804688, Learning Rate: 0.01\n",
      "Epoch [15729/20000], Loss: -21.219100952148438, Learning Rate: 0.01\n",
      "Epoch [15730/20000], Loss: -21.218719482421875, Learning Rate: 0.01\n",
      "Epoch [15731/20000], Loss: -21.219223022460938, Learning Rate: 0.01\n",
      "Epoch [15732/20000], Loss: -21.220169067382812, Learning Rate: 0.01\n",
      "Epoch [15733/20000], Loss: -21.221298217773438, Learning Rate: 0.01\n",
      "Epoch [15734/20000], Loss: -21.22198486328125, Learning Rate: 0.01\n",
      "Epoch [15735/20000], Loss: -21.222183227539062, Learning Rate: 0.01\n",
      "Epoch [15736/20000], Loss: -21.221908569335938, Learning Rate: 0.01\n",
      "Epoch [15737/20000], Loss: -21.22149658203125, Learning Rate: 0.01\n",
      "Epoch [15738/20000], Loss: -21.221176147460938, Learning Rate: 0.01\n",
      "Epoch [15739/20000], Loss: -21.221160888671875, Learning Rate: 0.01\n",
      "Epoch [15740/20000], Loss: -21.221389770507812, Learning Rate: 0.01\n",
      "Epoch [15741/20000], Loss: -21.221832275390625, Learning Rate: 0.01\n",
      "Epoch [15742/20000], Loss: -21.222320556640625, Learning Rate: 0.01\n",
      "Epoch [15743/20000], Loss: -21.222686767578125, Learning Rate: 0.01\n",
      "Epoch [15744/20000], Loss: -21.2227783203125, Learning Rate: 0.01\n",
      "Epoch [15745/20000], Loss: -21.222808837890625, Learning Rate: 0.01\n",
      "Epoch [15746/20000], Loss: -21.222686767578125, Learning Rate: 0.01\n",
      "Epoch [15747/20000], Loss: -21.222564697265625, Learning Rate: 0.01\n",
      "Epoch [15748/20000], Loss: -21.222549438476562, Learning Rate: 0.01\n",
      "Epoch [15749/20000], Loss: -21.222610473632812, Learning Rate: 0.01\n",
      "Epoch [15750/20000], Loss: -21.222808837890625, Learning Rate: 0.01\n",
      "Epoch [15751/20000], Loss: -21.2230224609375, Learning Rate: 0.01\n",
      "Epoch [15752/20000], Loss: -21.223236083984375, Learning Rate: 0.01\n",
      "Epoch [15753/20000], Loss: -21.223388671875, Learning Rate: 0.01\n",
      "Epoch [15754/20000], Loss: -21.223434448242188, Learning Rate: 0.01\n",
      "Epoch [15755/20000], Loss: -21.223419189453125, Learning Rate: 0.01\n",
      "Epoch [15756/20000], Loss: -21.2235107421875, Learning Rate: 0.01\n",
      "Epoch [15757/20000], Loss: -21.223556518554688, Learning Rate: 0.01\n",
      "Epoch [15758/20000], Loss: -21.223541259765625, Learning Rate: 0.01\n",
      "Epoch [15759/20000], Loss: -21.22357177734375, Learning Rate: 0.01\n",
      "Epoch [15760/20000], Loss: -21.223648071289062, Learning Rate: 0.01\n",
      "Epoch [15761/20000], Loss: -21.223770141601562, Learning Rate: 0.01\n",
      "Epoch [15762/20000], Loss: -21.223892211914062, Learning Rate: 0.01\n",
      "Epoch [15763/20000], Loss: -21.2239990234375, Learning Rate: 0.01\n",
      "Epoch [15764/20000], Loss: -21.224090576171875, Learning Rate: 0.01\n",
      "Epoch [15765/20000], Loss: -21.224166870117188, Learning Rate: 0.01\n",
      "Epoch [15766/20000], Loss: -21.224273681640625, Learning Rate: 0.01\n",
      "Epoch [15767/20000], Loss: -21.224288940429688, Learning Rate: 0.01\n",
      "Epoch [15768/20000], Loss: -21.224319458007812, Learning Rate: 0.01\n",
      "Epoch [15769/20000], Loss: -21.22442626953125, Learning Rate: 0.01\n",
      "Epoch [15770/20000], Loss: -21.224456787109375, Learning Rate: 0.01\n",
      "Epoch [15771/20000], Loss: -21.224517822265625, Learning Rate: 0.01\n",
      "Epoch [15772/20000], Loss: -21.224609375, Learning Rate: 0.01\n",
      "Epoch [15773/20000], Loss: -21.224685668945312, Learning Rate: 0.01\n",
      "Epoch [15774/20000], Loss: -21.224777221679688, Learning Rate: 0.01\n",
      "Epoch [15775/20000], Loss: -21.224853515625, Learning Rate: 0.01\n",
      "Epoch [15776/20000], Loss: -21.22491455078125, Learning Rate: 0.01\n",
      "Epoch [15777/20000], Loss: -21.2249755859375, Learning Rate: 0.01\n",
      "Epoch [15778/20000], Loss: -21.225128173828125, Learning Rate: 0.01\n",
      "Epoch [15779/20000], Loss: -21.22515869140625, Learning Rate: 0.01\n",
      "Epoch [15780/20000], Loss: -21.2252197265625, Learning Rate: 0.01\n",
      "Epoch [15781/20000], Loss: -21.2252197265625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15782/20000], Loss: -21.225296020507812, Learning Rate: 0.01\n",
      "Epoch [15783/20000], Loss: -21.225372314453125, Learning Rate: 0.01\n",
      "Epoch [15784/20000], Loss: -21.225433349609375, Learning Rate: 0.01\n",
      "Epoch [15785/20000], Loss: -21.225479125976562, Learning Rate: 0.01\n",
      "Epoch [15786/20000], Loss: -21.22552490234375, Learning Rate: 0.01\n",
      "Epoch [15787/20000], Loss: -21.225631713867188, Learning Rate: 0.01\n",
      "Epoch [15788/20000], Loss: -21.225723266601562, Learning Rate: 0.01\n",
      "Epoch [15789/20000], Loss: -21.225692749023438, Learning Rate: 0.01\n",
      "Epoch [15790/20000], Loss: -21.225784301757812, Learning Rate: 0.01\n",
      "Epoch [15791/20000], Loss: -21.225906372070312, Learning Rate: 0.01\n",
      "Epoch [15792/20000], Loss: -21.225967407226562, Learning Rate: 0.01\n",
      "Epoch [15793/20000], Loss: -21.22607421875, Learning Rate: 0.01\n",
      "Epoch [15794/20000], Loss: -21.226104736328125, Learning Rate: 0.01\n",
      "Epoch [15795/20000], Loss: -21.226181030273438, Learning Rate: 0.01\n",
      "Epoch [15796/20000], Loss: -21.22625732421875, Learning Rate: 0.01\n",
      "Epoch [15797/20000], Loss: -21.226318359375, Learning Rate: 0.01\n",
      "Epoch [15798/20000], Loss: -21.226318359375, Learning Rate: 0.01\n",
      "Epoch [15799/20000], Loss: -21.226425170898438, Learning Rate: 0.01\n",
      "Epoch [15800/20000], Loss: -21.22650146484375, Learning Rate: 0.01\n",
      "Epoch [15801/20000], Loss: -21.226593017578125, Learning Rate: 0.01\n",
      "Epoch [15802/20000], Loss: -21.226638793945312, Learning Rate: 0.01\n",
      "Epoch [15803/20000], Loss: -21.226760864257812, Learning Rate: 0.01\n",
      "Epoch [15804/20000], Loss: -21.226760864257812, Learning Rate: 0.01\n",
      "Epoch [15805/20000], Loss: -21.226806640625, Learning Rate: 0.01\n",
      "Epoch [15806/20000], Loss: -21.226943969726562, Learning Rate: 0.01\n",
      "Epoch [15807/20000], Loss: -21.226974487304688, Learning Rate: 0.01\n",
      "Epoch [15808/20000], Loss: -21.227035522460938, Learning Rate: 0.01\n",
      "Epoch [15809/20000], Loss: -21.227127075195312, Learning Rate: 0.01\n",
      "Epoch [15810/20000], Loss: -21.227142333984375, Learning Rate: 0.01\n",
      "Epoch [15811/20000], Loss: -21.227279663085938, Learning Rate: 0.01\n",
      "Epoch [15812/20000], Loss: -21.227310180664062, Learning Rate: 0.01\n",
      "Epoch [15813/20000], Loss: -21.22735595703125, Learning Rate: 0.01\n",
      "Epoch [15814/20000], Loss: -21.227493286132812, Learning Rate: 0.01\n",
      "Epoch [15815/20000], Loss: -21.227447509765625, Learning Rate: 0.01\n",
      "Epoch [15816/20000], Loss: -21.2275390625, Learning Rate: 0.01\n",
      "Epoch [15817/20000], Loss: -21.227569580078125, Learning Rate: 0.01\n",
      "Epoch [15818/20000], Loss: -21.2276611328125, Learning Rate: 0.01\n",
      "Epoch [15819/20000], Loss: -21.227630615234375, Learning Rate: 0.01\n",
      "Epoch [15820/20000], Loss: -21.227767944335938, Learning Rate: 0.01\n",
      "Epoch [15821/20000], Loss: -21.227798461914062, Learning Rate: 0.01\n",
      "Epoch [15822/20000], Loss: -21.227920532226562, Learning Rate: 0.01\n",
      "Epoch [15823/20000], Loss: -21.227981567382812, Learning Rate: 0.01\n",
      "Epoch [15824/20000], Loss: -21.22796630859375, Learning Rate: 0.01\n",
      "Epoch [15825/20000], Loss: -21.228042602539062, Learning Rate: 0.01\n",
      "Epoch [15826/20000], Loss: -21.228073120117188, Learning Rate: 0.01\n",
      "Epoch [15827/20000], Loss: -21.22808837890625, Learning Rate: 0.01\n",
      "Epoch [15828/20000], Loss: -21.228042602539062, Learning Rate: 0.01\n",
      "Epoch [15829/20000], Loss: -21.22796630859375, Learning Rate: 0.01\n",
      "Epoch [15830/20000], Loss: -21.227890014648438, Learning Rate: 0.01\n",
      "Epoch [15831/20000], Loss: -21.227645874023438, Learning Rate: 0.01\n",
      "Epoch [15832/20000], Loss: -21.22723388671875, Learning Rate: 0.01\n",
      "Epoch [15833/20000], Loss: -21.226654052734375, Learning Rate: 0.01\n",
      "Epoch [15834/20000], Loss: -21.22576904296875, Learning Rate: 0.01\n",
      "Epoch [15835/20000], Loss: -21.224319458007812, Learning Rate: 0.01\n",
      "Epoch [15836/20000], Loss: -21.2220458984375, Learning Rate: 0.01\n",
      "Epoch [15837/20000], Loss: -21.218429565429688, Learning Rate: 0.01\n",
      "Epoch [15838/20000], Loss: -21.212814331054688, Learning Rate: 0.01\n",
      "Epoch [15839/20000], Loss: -21.203857421875, Learning Rate: 0.01\n",
      "Epoch [15840/20000], Loss: -21.189682006835938, Learning Rate: 0.01\n",
      "Epoch [15841/20000], Loss: -21.166900634765625, Learning Rate: 0.01\n",
      "Epoch [15842/20000], Loss: -21.13079833984375, Learning Rate: 0.01\n",
      "Epoch [15843/20000], Loss: -21.072494506835938, Learning Rate: 0.01\n",
      "Epoch [15844/20000], Loss: -20.980712890625, Learning Rate: 0.01\n",
      "Epoch [15845/20000], Loss: -20.834060668945312, Learning Rate: 0.01\n",
      "Epoch [15846/20000], Loss: -20.611099243164062, Learning Rate: 0.01\n",
      "Epoch [15847/20000], Loss: -20.271804809570312, Learning Rate: 0.01\n",
      "Epoch [15848/20000], Loss: -19.811386108398438, Learning Rate: 0.01\n",
      "Epoch [15849/20000], Loss: -19.215530395507812, Learning Rate: 0.01\n",
      "Epoch [15850/20000], Loss: -18.657241821289062, Learning Rate: 0.01\n",
      "Epoch [15851/20000], Loss: -18.302276611328125, Learning Rate: 0.01\n",
      "Epoch [15852/20000], Loss: -18.564407348632812, Learning Rate: 0.01\n",
      "Epoch [15853/20000], Loss: -19.392791748046875, Learning Rate: 0.01\n",
      "Epoch [15854/20000], Loss: -20.479217529296875, Learning Rate: 0.01\n",
      "Epoch [15855/20000], Loss: -21.149337768554688, Learning Rate: 0.01\n",
      "Epoch [15856/20000], Loss: -21.100189208984375, Learning Rate: 0.01\n",
      "Epoch [15857/20000], Loss: -20.57220458984375, Learning Rate: 0.01\n",
      "Epoch [15858/20000], Loss: -20.100753784179688, Learning Rate: 0.01\n",
      "Epoch [15859/20000], Loss: -20.126144409179688, Learning Rate: 0.01\n",
      "Epoch [15860/20000], Loss: -20.588943481445312, Learning Rate: 0.01\n",
      "Epoch [15861/20000], Loss: -21.086837768554688, Learning Rate: 0.01\n",
      "Epoch [15862/20000], Loss: -21.211990356445312, Learning Rate: 0.01\n",
      "Epoch [15863/20000], Loss: -20.963226318359375, Learning Rate: 0.01\n",
      "Epoch [15864/20000], Loss: -20.674468994140625, Learning Rate: 0.01\n",
      "Epoch [15865/20000], Loss: -20.64764404296875, Learning Rate: 0.01\n",
      "Epoch [15866/20000], Loss: -20.902145385742188, Learning Rate: 0.01\n",
      "Epoch [15867/20000], Loss: -21.168991088867188, Learning Rate: 0.01\n",
      "Epoch [15868/20000], Loss: -21.21795654296875, Learning Rate: 0.01\n",
      "Epoch [15869/20000], Loss: -21.065658569335938, Learning Rate: 0.01\n",
      "Epoch [15870/20000], Loss: -20.91705322265625, Learning Rate: 0.01\n",
      "Epoch [15871/20000], Loss: -20.936965942382812, Learning Rate: 0.01\n",
      "Epoch [15872/20000], Loss: -21.088638305664062, Learning Rate: 0.01\n",
      "Epoch [15873/20000], Loss: -21.214874267578125, Learning Rate: 0.01\n",
      "Epoch [15874/20000], Loss: -21.208160400390625, Learning Rate: 0.01\n",
      "Epoch [15875/20000], Loss: -21.112960815429688, Learning Rate: 0.01\n",
      "Epoch [15876/20000], Loss: -21.05206298828125, Learning Rate: 0.01\n",
      "Epoch [15877/20000], Loss: -21.088699340820312, Learning Rate: 0.01\n",
      "Epoch [15878/20000], Loss: -21.178665161132812, Learning Rate: 0.01\n",
      "Epoch [15879/20000], Loss: -21.230316162109375, Learning Rate: 0.01\n",
      "Epoch [15880/20000], Loss: -21.207443237304688, Learning Rate: 0.01\n",
      "Epoch [15881/20000], Loss: -21.1529541015625, Learning Rate: 0.01\n",
      "Epoch [15882/20000], Loss: -21.13201904296875, Learning Rate: 0.01\n",
      "Epoch [15883/20000], Loss: -21.164459228515625, Learning Rate: 0.01\n",
      "Epoch [15884/20000], Loss: -21.212677001953125, Learning Rate: 0.01\n",
      "Epoch [15885/20000], Loss: -21.231170654296875, Learning Rate: 0.01\n",
      "Epoch [15886/20000], Loss: -21.211410522460938, Learning Rate: 0.01\n",
      "Epoch [15887/20000], Loss: -21.18353271484375, Learning Rate: 0.01\n",
      "Epoch [15888/20000], Loss: -21.179290771484375, Learning Rate: 0.01\n",
      "Epoch [15889/20000], Loss: -21.200973510742188, Learning Rate: 0.01\n",
      "Epoch [15890/20000], Loss: -21.225570678710938, Learning Rate: 0.01\n",
      "Epoch [15891/20000], Loss: -21.231399536132812, Learning Rate: 0.01\n",
      "Epoch [15892/20000], Loss: -21.218338012695312, Learning Rate: 0.01\n",
      "Epoch [15893/20000], Loss: -21.204147338867188, Learning Rate: 0.01\n",
      "Epoch [15894/20000], Loss: -21.204010009765625, Learning Rate: 0.01\n",
      "Epoch [15895/20000], Loss: -21.216873168945312, Learning Rate: 0.01\n",
      "Epoch [15896/20000], Loss: -21.22967529296875, Learning Rate: 0.01\n",
      "Epoch [15897/20000], Loss: -21.231781005859375, Learning Rate: 0.01\n",
      "Epoch [15898/20000], Loss: -21.224411010742188, Learning Rate: 0.01\n",
      "Epoch [15899/20000], Loss: -21.217269897460938, Learning Rate: 0.01\n",
      "Epoch [15900/20000], Loss: -21.21759033203125, Learning Rate: 0.01\n",
      "Epoch [15901/20000], Loss: -21.224578857421875, Learning Rate: 0.01\n",
      "Epoch [15902/20000], Loss: -21.231307983398438, Learning Rate: 0.01\n",
      "Epoch [15903/20000], Loss: -21.232574462890625, Learning Rate: 0.01\n",
      "Epoch [15904/20000], Loss: -21.22882080078125, Learning Rate: 0.01\n",
      "Epoch [15905/20000], Loss: -21.224853515625, Learning Rate: 0.01\n",
      "Epoch [15906/20000], Loss: -21.22467041015625, Learning Rate: 0.01\n",
      "Epoch [15907/20000], Loss: -21.228164672851562, Learning Rate: 0.01\n",
      "Epoch [15908/20000], Loss: -21.23193359375, Learning Rate: 0.01\n",
      "Epoch [15909/20000], Loss: -21.233261108398438, Learning Rate: 0.01\n",
      "Epoch [15910/20000], Loss: -21.231582641601562, Learning Rate: 0.01\n",
      "Epoch [15911/20000], Loss: -21.229324340820312, Learning Rate: 0.01\n",
      "Epoch [15912/20000], Loss: -21.22882080078125, Learning Rate: 0.01\n",
      "Epoch [15913/20000], Loss: -21.230361938476562, Learning Rate: 0.01\n",
      "Epoch [15914/20000], Loss: -21.232513427734375, Learning Rate: 0.01\n",
      "Epoch [15915/20000], Loss: -21.233642578125, Learning Rate: 0.01\n",
      "Epoch [15916/20000], Loss: -21.233184814453125, Learning Rate: 0.01\n",
      "Epoch [15917/20000], Loss: -21.232086181640625, Learning Rate: 0.01\n",
      "Epoch [15918/20000], Loss: -21.231399536132812, Learning Rate: 0.01\n",
      "Epoch [15919/20000], Loss: -21.231842041015625, Learning Rate: 0.01\n",
      "Epoch [15920/20000], Loss: -21.232940673828125, Learning Rate: 0.01\n",
      "Epoch [15921/20000], Loss: -21.23382568359375, Learning Rate: 0.01\n",
      "Epoch [15922/20000], Loss: -21.23406982421875, Learning Rate: 0.01\n",
      "Epoch [15923/20000], Loss: -21.233535766601562, Learning Rate: 0.01\n",
      "Epoch [15924/20000], Loss: -21.233001708984375, Learning Rate: 0.01\n",
      "Epoch [15925/20000], Loss: -21.233001708984375, Learning Rate: 0.01\n",
      "Epoch [15926/20000], Loss: -21.233444213867188, Learning Rate: 0.01\n",
      "Epoch [15927/20000], Loss: -21.234115600585938, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15928/20000], Loss: -21.23443603515625, Learning Rate: 0.01\n",
      "Epoch [15929/20000], Loss: -21.234390258789062, Learning Rate: 0.01\n",
      "Epoch [15930/20000], Loss: -21.234146118164062, Learning Rate: 0.01\n",
      "Epoch [15931/20000], Loss: -21.23394775390625, Learning Rate: 0.01\n",
      "Epoch [15932/20000], Loss: -21.234024047851562, Learning Rate: 0.01\n",
      "Epoch [15933/20000], Loss: -21.234298706054688, Learning Rate: 0.01\n",
      "Epoch [15934/20000], Loss: -21.234619140625, Learning Rate: 0.01\n",
      "Epoch [15935/20000], Loss: -21.234817504882812, Learning Rate: 0.01\n",
      "Epoch [15936/20000], Loss: -21.234771728515625, Learning Rate: 0.01\n",
      "Epoch [15937/20000], Loss: -21.23468017578125, Learning Rate: 0.01\n",
      "Epoch [15938/20000], Loss: -21.234649658203125, Learning Rate: 0.01\n",
      "Epoch [15939/20000], Loss: -21.23480224609375, Learning Rate: 0.01\n",
      "Epoch [15940/20000], Loss: -21.234939575195312, Learning Rate: 0.01\n",
      "Epoch [15941/20000], Loss: -21.235122680664062, Learning Rate: 0.01\n",
      "Epoch [15942/20000], Loss: -21.235183715820312, Learning Rate: 0.01\n",
      "Epoch [15943/20000], Loss: -21.235275268554688, Learning Rate: 0.01\n",
      "Epoch [15944/20000], Loss: -21.235214233398438, Learning Rate: 0.01\n",
      "Epoch [15945/20000], Loss: -21.235183715820312, Learning Rate: 0.01\n",
      "Epoch [15946/20000], Loss: -21.235260009765625, Learning Rate: 0.01\n",
      "Epoch [15947/20000], Loss: -21.235305786132812, Learning Rate: 0.01\n",
      "Epoch [15948/20000], Loss: -21.23541259765625, Learning Rate: 0.01\n",
      "Epoch [15949/20000], Loss: -21.2354736328125, Learning Rate: 0.01\n",
      "Epoch [15950/20000], Loss: -21.235321044921875, Learning Rate: 0.01\n",
      "Epoch [15951/20000], Loss: -21.2352294921875, Learning Rate: 0.01\n",
      "Epoch [15952/20000], Loss: -21.235015869140625, Learning Rate: 0.01\n",
      "Epoch [15953/20000], Loss: -21.234832763671875, Learning Rate: 0.01\n",
      "Epoch [15954/20000], Loss: -21.234451293945312, Learning Rate: 0.01\n",
      "Epoch [15955/20000], Loss: -21.234024047851562, Learning Rate: 0.01\n",
      "Epoch [15956/20000], Loss: -21.233245849609375, Learning Rate: 0.01\n",
      "Epoch [15957/20000], Loss: -21.232086181640625, Learning Rate: 0.01\n",
      "Epoch [15958/20000], Loss: -21.230392456054688, Learning Rate: 0.01\n",
      "Epoch [15959/20000], Loss: -21.227813720703125, Learning Rate: 0.01\n",
      "Epoch [15960/20000], Loss: -21.22393798828125, Learning Rate: 0.01\n",
      "Epoch [15961/20000], Loss: -21.218276977539062, Learning Rate: 0.01\n",
      "Epoch [15962/20000], Loss: -21.209686279296875, Learning Rate: 0.01\n",
      "Epoch [15963/20000], Loss: -21.196548461914062, Learning Rate: 0.01\n",
      "Epoch [15964/20000], Loss: -21.1768798828125, Learning Rate: 0.01\n",
      "Epoch [15965/20000], Loss: -21.147293090820312, Learning Rate: 0.01\n",
      "Epoch [15966/20000], Loss: -21.102737426757812, Learning Rate: 0.01\n",
      "Epoch [15967/20000], Loss: -21.0369873046875, Learning Rate: 0.01\n",
      "Epoch [15968/20000], Loss: -20.941558837890625, Learning Rate: 0.01\n",
      "Epoch [15969/20000], Loss: -20.809494018554688, Learning Rate: 0.01\n",
      "Epoch [15970/20000], Loss: -20.636886596679688, Learning Rate: 0.01\n",
      "Epoch [15971/20000], Loss: -20.438644409179688, Learning Rate: 0.01\n",
      "Epoch [15972/20000], Loss: -20.251312255859375, Learning Rate: 0.01\n",
      "Epoch [15973/20000], Loss: -20.154266357421875, Learning Rate: 0.01\n",
      "Epoch [15974/20000], Loss: -20.221282958984375, Learning Rate: 0.01\n",
      "Epoch [15975/20000], Loss: -20.483505249023438, Learning Rate: 0.01\n",
      "Epoch [15976/20000], Loss: -20.844818115234375, Learning Rate: 0.01\n",
      "Epoch [15977/20000], Loss: -21.139312744140625, Learning Rate: 0.01\n",
      "Epoch [15978/20000], Loss: -21.23687744140625, Learning Rate: 0.01\n",
      "Epoch [15979/20000], Loss: -21.138031005859375, Learning Rate: 0.01\n",
      "Epoch [15980/20000], Loss: -20.956756591796875, Learning Rate: 0.01\n",
      "Epoch [15981/20000], Loss: -20.83477783203125, Learning Rate: 0.01\n",
      "Epoch [15982/20000], Loss: -20.85858154296875, Learning Rate: 0.01\n",
      "Epoch [15983/20000], Loss: -21.003768920898438, Learning Rate: 0.01\n",
      "Epoch [15984/20000], Loss: -21.164749145507812, Learning Rate: 0.01\n",
      "Epoch [15985/20000], Loss: -21.236831665039062, Learning Rate: 0.01\n",
      "Epoch [15986/20000], Loss: -21.196136474609375, Learning Rate: 0.01\n",
      "Epoch [15987/20000], Loss: -21.10357666015625, Learning Rate: 0.01\n",
      "Epoch [15988/20000], Loss: -21.044876098632812, Learning Rate: 0.01\n",
      "Epoch [15989/20000], Loss: -21.066192626953125, Learning Rate: 0.01\n",
      "Epoch [15990/20000], Loss: -21.144515991210938, Learning Rate: 0.01\n",
      "Epoch [15991/20000], Loss: -21.217391967773438, Learning Rate: 0.01\n",
      "Epoch [15992/20000], Loss: -21.236572265625, Learning Rate: 0.01\n",
      "Epoch [15993/20000], Loss: -21.203170776367188, Learning Rate: 0.01\n",
      "Epoch [15994/20000], Loss: -21.157546997070312, Learning Rate: 0.01\n",
      "Epoch [15995/20000], Loss: -21.14117431640625, Learning Rate: 0.01\n",
      "Epoch [15996/20000], Loss: -21.165252685546875, Learning Rate: 0.01\n",
      "Epoch [15997/20000], Loss: -21.20758056640625, Learning Rate: 0.01\n",
      "Epoch [15998/20000], Loss: -21.23553466796875, Learning Rate: 0.01\n",
      "Epoch [15999/20000], Loss: -21.233627319335938, Learning Rate: 0.01\n",
      "Epoch [16000/20000], Loss: -21.211395263671875, Learning Rate: 0.01\n",
      "Epoch [16001/20000], Loss: -21.192169189453125, Learning Rate: 0.01\n",
      "Epoch [16002/20000], Loss: -21.191925048828125, Learning Rate: 0.01\n",
      "Epoch [16003/20000], Loss: -21.209304809570312, Learning Rate: 0.01\n",
      "Epoch [16004/20000], Loss: -21.229721069335938, Learning Rate: 0.01\n",
      "Epoch [16005/20000], Loss: -21.238876342773438, Learning Rate: 0.01\n",
      "Epoch [16006/20000], Loss: -21.233505249023438, Learning Rate: 0.01\n",
      "Epoch [16007/20000], Loss: -21.221664428710938, Learning Rate: 0.01\n",
      "Epoch [16008/20000], Loss: -21.214370727539062, Learning Rate: 0.01\n",
      "Epoch [16009/20000], Loss: -21.21710205078125, Learning Rate: 0.01\n",
      "Epoch [16010/20000], Loss: -21.227020263671875, Learning Rate: 0.01\n",
      "Epoch [16011/20000], Loss: -21.236282348632812, Learning Rate: 0.01\n",
      "Epoch [16012/20000], Loss: -21.239181518554688, Learning Rate: 0.01\n",
      "Epoch [16013/20000], Loss: -21.235641479492188, Learning Rate: 0.01\n",
      "Epoch [16014/20000], Loss: -21.229904174804688, Learning Rate: 0.01\n",
      "Epoch [16015/20000], Loss: -21.227035522460938, Learning Rate: 0.01\n",
      "Epoch [16016/20000], Loss: -21.228912353515625, Learning Rate: 0.01\n",
      "Epoch [16017/20000], Loss: -21.233901977539062, Learning Rate: 0.01\n",
      "Epoch [16018/20000], Loss: -21.238327026367188, Learning Rate: 0.01\n",
      "Epoch [16019/20000], Loss: -21.239700317382812, Learning Rate: 0.01\n",
      "Epoch [16020/20000], Loss: -21.237945556640625, Learning Rate: 0.01\n",
      "Epoch [16021/20000], Loss: -21.235244750976562, Learning Rate: 0.01\n",
      "Epoch [16022/20000], Loss: -21.233810424804688, Learning Rate: 0.01\n",
      "Epoch [16023/20000], Loss: -21.234588623046875, Learning Rate: 0.01\n",
      "Epoch [16024/20000], Loss: -21.23681640625, Learning Rate: 0.01\n",
      "Epoch [16025/20000], Loss: -21.239105224609375, Learning Rate: 0.01\n",
      "Epoch [16026/20000], Loss: -21.240158081054688, Learning Rate: 0.01\n",
      "Epoch [16027/20000], Loss: -21.239578247070312, Learning Rate: 0.01\n",
      "Epoch [16028/20000], Loss: -21.238388061523438, Learning Rate: 0.01\n",
      "Epoch [16029/20000], Loss: -21.237518310546875, Learning Rate: 0.01\n",
      "Epoch [16030/20000], Loss: -21.237564086914062, Learning Rate: 0.01\n",
      "Epoch [16031/20000], Loss: -21.238494873046875, Learning Rate: 0.01\n",
      "Epoch [16032/20000], Loss: -21.239608764648438, Learning Rate: 0.01\n",
      "Epoch [16033/20000], Loss: -21.24041748046875, Learning Rate: 0.01\n",
      "Epoch [16034/20000], Loss: -21.240554809570312, Learning Rate: 0.01\n",
      "Epoch [16035/20000], Loss: -21.240188598632812, Learning Rate: 0.01\n",
      "Epoch [16036/20000], Loss: -21.23968505859375, Learning Rate: 0.01\n",
      "Epoch [16037/20000], Loss: -21.239486694335938, Learning Rate: 0.01\n",
      "Epoch [16038/20000], Loss: -21.23968505859375, Learning Rate: 0.01\n",
      "Epoch [16039/20000], Loss: -21.2401123046875, Learning Rate: 0.01\n",
      "Epoch [16040/20000], Loss: -21.240692138671875, Learning Rate: 0.01\n",
      "Epoch [16041/20000], Loss: -21.240966796875, Learning Rate: 0.01\n",
      "Epoch [16042/20000], Loss: -21.241043090820312, Learning Rate: 0.01\n",
      "Epoch [16043/20000], Loss: -21.240936279296875, Learning Rate: 0.01\n",
      "Epoch [16044/20000], Loss: -21.24072265625, Learning Rate: 0.01\n",
      "Epoch [16045/20000], Loss: -21.240646362304688, Learning Rate: 0.01\n",
      "Epoch [16046/20000], Loss: -21.240753173828125, Learning Rate: 0.01\n",
      "Epoch [16047/20000], Loss: -21.240982055664062, Learning Rate: 0.01\n",
      "Epoch [16048/20000], Loss: -21.2413330078125, Learning Rate: 0.01\n",
      "Epoch [16049/20000], Loss: -21.241455078125, Learning Rate: 0.01\n",
      "Epoch [16050/20000], Loss: -21.2415771484375, Learning Rate: 0.01\n",
      "Epoch [16051/20000], Loss: -21.241546630859375, Learning Rate: 0.01\n",
      "Epoch [16052/20000], Loss: -21.241500854492188, Learning Rate: 0.01\n",
      "Epoch [16053/20000], Loss: -21.241455078125, Learning Rate: 0.01\n",
      "Epoch [16054/20000], Loss: -21.241485595703125, Learning Rate: 0.01\n",
      "Epoch [16055/20000], Loss: -21.241622924804688, Learning Rate: 0.01\n",
      "Epoch [16056/20000], Loss: -21.241775512695312, Learning Rate: 0.01\n",
      "Epoch [16057/20000], Loss: -21.241958618164062, Learning Rate: 0.01\n",
      "Epoch [16058/20000], Loss: -21.2420654296875, Learning Rate: 0.01\n",
      "Epoch [16059/20000], Loss: -21.242080688476562, Learning Rate: 0.01\n",
      "Epoch [16060/20000], Loss: -21.242156982421875, Learning Rate: 0.01\n",
      "Epoch [16061/20000], Loss: -21.242111206054688, Learning Rate: 0.01\n",
      "Epoch [16062/20000], Loss: -21.24224853515625, Learning Rate: 0.01\n",
      "Epoch [16063/20000], Loss: -21.24224853515625, Learning Rate: 0.01\n",
      "Epoch [16064/20000], Loss: -21.2423095703125, Learning Rate: 0.01\n",
      "Epoch [16065/20000], Loss: -21.242324829101562, Learning Rate: 0.01\n",
      "Epoch [16066/20000], Loss: -21.242401123046875, Learning Rate: 0.01\n",
      "Epoch [16067/20000], Loss: -21.242523193359375, Learning Rate: 0.01\n",
      "Epoch [16068/20000], Loss: -21.24267578125, Learning Rate: 0.01\n",
      "Epoch [16069/20000], Loss: -21.242706298828125, Learning Rate: 0.01\n",
      "Epoch [16070/20000], Loss: -21.242721557617188, Learning Rate: 0.01\n",
      "Epoch [16071/20000], Loss: -21.24273681640625, Learning Rate: 0.01\n",
      "Epoch [16072/20000], Loss: -21.242843627929688, Learning Rate: 0.01\n",
      "Epoch [16073/20000], Loss: -21.242843627929688, Learning Rate: 0.01\n",
      "Epoch [16074/20000], Loss: -21.242965698242188, Learning Rate: 0.01\n",
      "Epoch [16075/20000], Loss: -21.2430419921875, Learning Rate: 0.01\n",
      "Epoch [16076/20000], Loss: -21.2430419921875, Learning Rate: 0.01\n",
      "Epoch [16077/20000], Loss: -21.24310302734375, Learning Rate: 0.01\n",
      "Epoch [16078/20000], Loss: -21.243240356445312, Learning Rate: 0.01\n",
      "Epoch [16079/20000], Loss: -21.243316650390625, Learning Rate: 0.01\n",
      "Epoch [16080/20000], Loss: -21.243331909179688, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16081/20000], Loss: -21.243377685546875, Learning Rate: 0.01\n",
      "Epoch [16082/20000], Loss: -21.24346923828125, Learning Rate: 0.01\n",
      "Epoch [16083/20000], Loss: -21.243515014648438, Learning Rate: 0.01\n",
      "Epoch [16084/20000], Loss: -21.243560791015625, Learning Rate: 0.01\n",
      "Epoch [16085/20000], Loss: -21.243606567382812, Learning Rate: 0.01\n",
      "Epoch [16086/20000], Loss: -21.243637084960938, Learning Rate: 0.01\n",
      "Epoch [16087/20000], Loss: -21.243698120117188, Learning Rate: 0.01\n",
      "Epoch [16088/20000], Loss: -21.243743896484375, Learning Rate: 0.01\n",
      "Epoch [16089/20000], Loss: -21.243896484375, Learning Rate: 0.01\n",
      "Epoch [16090/20000], Loss: -21.24395751953125, Learning Rate: 0.01\n",
      "Epoch [16091/20000], Loss: -21.24395751953125, Learning Rate: 0.01\n",
      "Epoch [16092/20000], Loss: -21.244033813476562, Learning Rate: 0.01\n",
      "Epoch [16093/20000], Loss: -21.244125366210938, Learning Rate: 0.01\n",
      "Epoch [16094/20000], Loss: -21.244125366210938, Learning Rate: 0.01\n",
      "Epoch [16095/20000], Loss: -21.244155883789062, Learning Rate: 0.01\n",
      "Epoch [16096/20000], Loss: -21.244186401367188, Learning Rate: 0.01\n",
      "Epoch [16097/20000], Loss: -21.244293212890625, Learning Rate: 0.01\n",
      "Epoch [16098/20000], Loss: -21.244308471679688, Learning Rate: 0.01\n",
      "Epoch [16099/20000], Loss: -21.2442626953125, Learning Rate: 0.01\n",
      "Epoch [16100/20000], Loss: -21.244232177734375, Learning Rate: 0.01\n",
      "Epoch [16101/20000], Loss: -21.244140625, Learning Rate: 0.01\n",
      "Epoch [16102/20000], Loss: -21.2440185546875, Learning Rate: 0.01\n",
      "Epoch [16103/20000], Loss: -21.243743896484375, Learning Rate: 0.01\n",
      "Epoch [16104/20000], Loss: -21.243316650390625, Learning Rate: 0.01\n",
      "Epoch [16105/20000], Loss: -21.242599487304688, Learning Rate: 0.01\n",
      "Epoch [16106/20000], Loss: -21.241409301757812, Learning Rate: 0.01\n",
      "Epoch [16107/20000], Loss: -21.239578247070312, Learning Rate: 0.01\n",
      "Epoch [16108/20000], Loss: -21.236602783203125, Learning Rate: 0.01\n",
      "Epoch [16109/20000], Loss: -21.231948852539062, Learning Rate: 0.01\n",
      "Epoch [16110/20000], Loss: -21.224258422851562, Learning Rate: 0.01\n",
      "Epoch [16111/20000], Loss: -21.2119140625, Learning Rate: 0.01\n",
      "Epoch [16112/20000], Loss: -21.191864013671875, Learning Rate: 0.01\n",
      "Epoch [16113/20000], Loss: -21.159271240234375, Learning Rate: 0.01\n",
      "Epoch [16114/20000], Loss: -21.105865478515625, Learning Rate: 0.01\n",
      "Epoch [16115/20000], Loss: -21.019851684570312, Learning Rate: 0.01\n",
      "Epoch [16116/20000], Loss: -20.879623413085938, Learning Rate: 0.01\n",
      "Epoch [16117/20000], Loss: -20.660964965820312, Learning Rate: 0.01\n",
      "Epoch [16118/20000], Loss: -20.319061279296875, Learning Rate: 0.01\n",
      "Epoch [16119/20000], Loss: -19.83868408203125, Learning Rate: 0.01\n",
      "Epoch [16120/20000], Loss: -19.191558837890625, Learning Rate: 0.01\n",
      "Epoch [16121/20000], Loss: -18.54669189453125, Learning Rate: 0.01\n",
      "Epoch [16122/20000], Loss: -18.084014892578125, Learning Rate: 0.01\n",
      "Epoch [16123/20000], Loss: -18.2987060546875, Learning Rate: 0.01\n",
      "Epoch [16124/20000], Loss: -19.180221557617188, Learning Rate: 0.01\n",
      "Epoch [16125/20000], Loss: -20.403289794921875, Learning Rate: 0.01\n",
      "Epoch [16126/20000], Loss: -21.15966796875, Learning Rate: 0.01\n",
      "Epoch [16127/20000], Loss: -21.0789794921875, Learning Rate: 0.01\n",
      "Epoch [16128/20000], Loss: -20.462203979492188, Learning Rate: 0.01\n",
      "Epoch [16129/20000], Loss: -19.960479736328125, Learning Rate: 0.01\n",
      "Epoch [16130/20000], Loss: -20.073226928710938, Learning Rate: 0.01\n",
      "Epoch [16131/20000], Loss: -20.652069091796875, Learning Rate: 0.01\n",
      "Epoch [16132/20000], Loss: -21.16473388671875, Learning Rate: 0.01\n",
      "Epoch [16133/20000], Loss: -21.185317993164062, Learning Rate: 0.01\n",
      "Epoch [16134/20000], Loss: -20.83349609375, Learning Rate: 0.01\n",
      "Epoch [16135/20000], Loss: -20.566314697265625, Learning Rate: 0.01\n",
      "Epoch [16136/20000], Loss: -20.665756225585938, Learning Rate: 0.01\n",
      "Epoch [16137/20000], Loss: -21.013442993164062, Learning Rate: 0.01\n",
      "Epoch [16138/20000], Loss: -21.23681640625, Learning Rate: 0.01\n",
      "Epoch [16139/20000], Loss: -21.161239624023438, Learning Rate: 0.01\n",
      "Epoch [16140/20000], Loss: -20.949569702148438, Learning Rate: 0.01\n",
      "Epoch [16141/20000], Loss: -20.870513916015625, Learning Rate: 0.01\n",
      "Epoch [16142/20000], Loss: -21.008331298828125, Learning Rate: 0.01\n",
      "Epoch [16143/20000], Loss: -21.193603515625, Learning Rate: 0.01\n",
      "Epoch [16144/20000], Loss: -21.239242553710938, Learning Rate: 0.01\n",
      "Epoch [16145/20000], Loss: -21.138320922851562, Learning Rate: 0.01\n",
      "Epoch [16146/20000], Loss: -21.039871215820312, Learning Rate: 0.01\n",
      "Epoch [16147/20000], Loss: -21.062850952148438, Learning Rate: 0.01\n",
      "Epoch [16148/20000], Loss: -21.170928955078125, Learning Rate: 0.01\n",
      "Epoch [16149/20000], Loss: -21.244140625, Learning Rate: 0.01\n",
      "Epoch [16150/20000], Loss: -21.220230102539062, Learning Rate: 0.01\n",
      "Epoch [16151/20000], Loss: -21.15191650390625, Learning Rate: 0.01\n",
      "Epoch [16152/20000], Loss: -21.128494262695312, Learning Rate: 0.01\n",
      "Epoch [16153/20000], Loss: -21.172805786132812, Learning Rate: 0.01\n",
      "Epoch [16154/20000], Loss: -21.23138427734375, Learning Rate: 0.01\n",
      "Epoch [16155/20000], Loss: -21.244583129882812, Learning Rate: 0.01\n",
      "Epoch [16156/20000], Loss: -21.211883544921875, Learning Rate: 0.01\n",
      "Epoch [16157/20000], Loss: -21.181243896484375, Learning Rate: 0.01\n",
      "Epoch [16158/20000], Loss: -21.188308715820312, Learning Rate: 0.01\n",
      "Epoch [16159/20000], Loss: -21.222763061523438, Learning Rate: 0.01\n",
      "Epoch [16160/20000], Loss: -21.24652099609375, Learning Rate: 0.01\n",
      "Epoch [16161/20000], Loss: -21.240142822265625, Learning Rate: 0.01\n",
      "Epoch [16162/20000], Loss: -21.218704223632812, Learning Rate: 0.01\n",
      "Epoch [16163/20000], Loss: -21.209564208984375, Learning Rate: 0.01\n",
      "Epoch [16164/20000], Loss: -21.221969604492188, Learning Rate: 0.01\n",
      "Epoch [16165/20000], Loss: -21.241058349609375, Learning Rate: 0.01\n",
      "Epoch [16166/20000], Loss: -21.24774169921875, Learning Rate: 0.01\n",
      "Epoch [16167/20000], Loss: -21.239212036132812, Learning Rate: 0.01\n",
      "Epoch [16168/20000], Loss: -21.228347778320312, Learning Rate: 0.01\n",
      "Epoch [16169/20000], Loss: -21.227828979492188, Learning Rate: 0.01\n",
      "Epoch [16170/20000], Loss: -21.237319946289062, Learning Rate: 0.01\n",
      "Epoch [16171/20000], Loss: -21.2467041015625, Learning Rate: 0.01\n",
      "Epoch [16172/20000], Loss: -21.247573852539062, Learning Rate: 0.01\n",
      "Epoch [16173/20000], Loss: -21.241683959960938, Learning Rate: 0.01\n",
      "Epoch [16174/20000], Loss: -21.236831665039062, Learning Rate: 0.01\n",
      "Epoch [16175/20000], Loss: -21.238327026367188, Learning Rate: 0.01\n",
      "Epoch [16176/20000], Loss: -21.244110107421875, Learning Rate: 0.01\n",
      "Epoch [16177/20000], Loss: -21.248321533203125, Learning Rate: 0.01\n",
      "Epoch [16178/20000], Loss: -21.247817993164062, Learning Rate: 0.01\n",
      "Epoch [16179/20000], Loss: -21.244384765625, Learning Rate: 0.01\n",
      "Epoch [16180/20000], Loss: -21.242233276367188, Learning Rate: 0.01\n",
      "Epoch [16181/20000], Loss: -21.243545532226562, Learning Rate: 0.01\n",
      "Epoch [16182/20000], Loss: -21.24676513671875, Learning Rate: 0.01\n",
      "Epoch [16183/20000], Loss: -21.248855590820312, Learning Rate: 0.01\n",
      "Epoch [16184/20000], Loss: -21.248367309570312, Learning Rate: 0.01\n",
      "Epoch [16185/20000], Loss: -21.24652099609375, Learning Rate: 0.01\n",
      "Epoch [16186/20000], Loss: -21.24554443359375, Learning Rate: 0.01\n",
      "Epoch [16187/20000], Loss: -21.246307373046875, Learning Rate: 0.01\n",
      "Epoch [16188/20000], Loss: -21.248123168945312, Learning Rate: 0.01\n",
      "Epoch [16189/20000], Loss: -21.249252319335938, Learning Rate: 0.01\n",
      "Epoch [16190/20000], Loss: -21.249069213867188, Learning Rate: 0.01\n",
      "Epoch [16191/20000], Loss: -21.248199462890625, Learning Rate: 0.01\n",
      "Epoch [16192/20000], Loss: -21.2476806640625, Learning Rate: 0.01\n",
      "Epoch [16193/20000], Loss: -21.247955322265625, Learning Rate: 0.01\n",
      "Epoch [16194/20000], Loss: -21.24884033203125, Learning Rate: 0.01\n",
      "Epoch [16195/20000], Loss: -21.249603271484375, Learning Rate: 0.01\n",
      "Epoch [16196/20000], Loss: -21.249664306640625, Learning Rate: 0.01\n",
      "Epoch [16197/20000], Loss: -21.249237060546875, Learning Rate: 0.01\n",
      "Epoch [16198/20000], Loss: -21.248794555664062, Learning Rate: 0.01\n",
      "Epoch [16199/20000], Loss: -21.2489013671875, Learning Rate: 0.01\n",
      "Epoch [16200/20000], Loss: -21.249404907226562, Learning Rate: 0.01\n",
      "Epoch [16201/20000], Loss: -21.249893188476562, Learning Rate: 0.01\n",
      "Epoch [16202/20000], Loss: -21.25, Learning Rate: 0.01\n",
      "Epoch [16203/20000], Loss: -21.249908447265625, Learning Rate: 0.01\n",
      "Epoch [16204/20000], Loss: -21.249664306640625, Learning Rate: 0.01\n",
      "Epoch [16205/20000], Loss: -21.249618530273438, Learning Rate: 0.01\n",
      "Epoch [16206/20000], Loss: -21.249832153320312, Learning Rate: 0.01\n",
      "Epoch [16207/20000], Loss: -21.2501220703125, Learning Rate: 0.01\n",
      "Epoch [16208/20000], Loss: -21.25030517578125, Learning Rate: 0.01\n",
      "Epoch [16209/20000], Loss: -21.250350952148438, Learning Rate: 0.01\n",
      "Epoch [16210/20000], Loss: -21.250274658203125, Learning Rate: 0.01\n",
      "Epoch [16211/20000], Loss: -21.250198364257812, Learning Rate: 0.01\n",
      "Epoch [16212/20000], Loss: -21.250274658203125, Learning Rate: 0.01\n",
      "Epoch [16213/20000], Loss: -21.250457763671875, Learning Rate: 0.01\n",
      "Epoch [16214/20000], Loss: -21.250625610351562, Learning Rate: 0.01\n",
      "Epoch [16215/20000], Loss: -21.250686645507812, Learning Rate: 0.01\n",
      "Epoch [16216/20000], Loss: -21.25067138671875, Learning Rate: 0.01\n",
      "Epoch [16217/20000], Loss: -21.250625610351562, Learning Rate: 0.01\n",
      "Epoch [16218/20000], Loss: -21.250625610351562, Learning Rate: 0.01\n",
      "Epoch [16219/20000], Loss: -21.2506103515625, Learning Rate: 0.01\n",
      "Epoch [16220/20000], Loss: -21.250564575195312, Learning Rate: 0.01\n",
      "Epoch [16221/20000], Loss: -21.250579833984375, Learning Rate: 0.01\n",
      "Epoch [16222/20000], Loss: -21.250442504882812, Learning Rate: 0.01\n",
      "Epoch [16223/20000], Loss: -21.250244140625, Learning Rate: 0.01\n",
      "Epoch [16224/20000], Loss: -21.24993896484375, Learning Rate: 0.01\n",
      "Epoch [16225/20000], Loss: -21.2493896484375, Learning Rate: 0.01\n",
      "Epoch [16226/20000], Loss: -21.248641967773438, Learning Rate: 0.01\n",
      "Epoch [16227/20000], Loss: -21.247512817382812, Learning Rate: 0.01\n",
      "Epoch [16228/20000], Loss: -21.245773315429688, Learning Rate: 0.01\n",
      "Epoch [16229/20000], Loss: -21.243194580078125, Learning Rate: 0.01\n",
      "Epoch [16230/20000], Loss: -21.239181518554688, Learning Rate: 0.01\n",
      "Epoch [16231/20000], Loss: -21.233062744140625, Learning Rate: 0.01\n",
      "Epoch [16232/20000], Loss: -21.223648071289062, Learning Rate: 0.01\n",
      "Epoch [16233/20000], Loss: -21.209228515625, Learning Rate: 0.01\n",
      "Epoch [16234/20000], Loss: -21.186965942382812, Learning Rate: 0.01\n",
      "Epoch [16235/20000], Loss: -21.152816772460938, Learning Rate: 0.01\n",
      "Epoch [16236/20000], Loss: -21.100723266601562, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16237/20000], Loss: -21.022354125976562, Learning Rate: 0.01\n",
      "Epoch [16238/20000], Loss: -20.908126831054688, Learning Rate: 0.01\n",
      "Epoch [16239/20000], Loss: -20.74810791015625, Learning Rate: 0.01\n",
      "Epoch [16240/20000], Loss: -20.543563842773438, Learning Rate: 0.01\n",
      "Epoch [16241/20000], Loss: -20.313613891601562, Learning Rate: 0.01\n",
      "Epoch [16242/20000], Loss: -20.124847412109375, Learning Rate: 0.01\n",
      "Epoch [16243/20000], Loss: -20.06878662109375, Learning Rate: 0.01\n",
      "Epoch [16244/20000], Loss: -20.237945556640625, Learning Rate: 0.01\n",
      "Epoch [16245/20000], Loss: -20.602371215820312, Learning Rate: 0.01\n",
      "Epoch [16246/20000], Loss: -21.002304077148438, Learning Rate: 0.01\n",
      "Epoch [16247/20000], Loss: -21.232177734375, Learning Rate: 0.01\n",
      "Epoch [16248/20000], Loss: -21.2086181640625, Learning Rate: 0.01\n",
      "Epoch [16249/20000], Loss: -21.017333984375, Learning Rate: 0.01\n",
      "Epoch [16250/20000], Loss: -20.833267211914062, Learning Rate: 0.01\n",
      "Epoch [16251/20000], Loss: -20.80108642578125, Learning Rate: 0.01\n",
      "Epoch [16252/20000], Loss: -20.93817138671875, Learning Rate: 0.01\n",
      "Epoch [16253/20000], Loss: -21.13458251953125, Learning Rate: 0.01\n",
      "Epoch [16254/20000], Loss: -21.245986938476562, Learning Rate: 0.01\n",
      "Epoch [16255/20000], Loss: -21.216567993164062, Learning Rate: 0.01\n",
      "Epoch [16256/20000], Loss: -21.108795166015625, Learning Rate: 0.01\n",
      "Epoch [16257/20000], Loss: -21.033065795898438, Learning Rate: 0.01\n",
      "Epoch [16258/20000], Loss: -21.055328369140625, Learning Rate: 0.01\n",
      "Epoch [16259/20000], Loss: -21.14886474609375, Learning Rate: 0.01\n",
      "Epoch [16260/20000], Loss: -21.23297119140625, Learning Rate: 0.01\n",
      "Epoch [16261/20000], Loss: -21.247512817382812, Learning Rate: 0.01\n",
      "Epoch [16262/20000], Loss: -21.200820922851562, Learning Rate: 0.01\n",
      "Epoch [16263/20000], Loss: -21.14959716796875, Learning Rate: 0.01\n",
      "Epoch [16264/20000], Loss: -21.142898559570312, Learning Rate: 0.01\n",
      "Epoch [16265/20000], Loss: -21.183349609375, Learning Rate: 0.01\n",
      "Epoch [16266/20000], Loss: -21.232818603515625, Learning Rate: 0.01\n",
      "Epoch [16267/20000], Loss: -21.252197265625, Learning Rate: 0.01\n",
      "Epoch [16268/20000], Loss: -21.234710693359375, Learning Rate: 0.01\n",
      "Epoch [16269/20000], Loss: -21.205337524414062, Learning Rate: 0.01\n",
      "Epoch [16270/20000], Loss: -21.193923950195312, Learning Rate: 0.01\n",
      "Epoch [16271/20000], Loss: -21.20928955078125, Learning Rate: 0.01\n",
      "Epoch [16272/20000], Loss: -21.236221313476562, Learning Rate: 0.01\n",
      "Epoch [16273/20000], Loss: -21.25238037109375, Learning Rate: 0.01\n",
      "Epoch [16274/20000], Loss: -21.24847412109375, Learning Rate: 0.01\n",
      "Epoch [16275/20000], Loss: -21.233245849609375, Learning Rate: 0.01\n",
      "Epoch [16276/20000], Loss: -21.22283935546875, Learning Rate: 0.01\n",
      "Epoch [16277/20000], Loss: -21.226348876953125, Learning Rate: 0.01\n",
      "Epoch [16278/20000], Loss: -21.239639282226562, Learning Rate: 0.01\n",
      "Epoch [16279/20000], Loss: -21.251358032226562, Learning Rate: 0.01\n",
      "Epoch [16280/20000], Loss: -21.25347900390625, Learning Rate: 0.01\n",
      "Epoch [16281/20000], Loss: -21.247100830078125, Learning Rate: 0.01\n",
      "Epoch [16282/20000], Loss: -21.23968505859375, Learning Rate: 0.01\n",
      "Epoch [16283/20000], Loss: -21.238037109375, Learning Rate: 0.01\n",
      "Epoch [16284/20000], Loss: -21.243072509765625, Learning Rate: 0.01\n",
      "Epoch [16285/20000], Loss: -21.250213623046875, Learning Rate: 0.01\n",
      "Epoch [16286/20000], Loss: -21.254165649414062, Learning Rate: 0.01\n",
      "Epoch [16287/20000], Loss: -21.253097534179688, Learning Rate: 0.01\n",
      "Epoch [16288/20000], Loss: -21.249160766601562, Learning Rate: 0.01\n",
      "Epoch [16289/20000], Loss: -21.246383666992188, Learning Rate: 0.01\n",
      "Epoch [16290/20000], Loss: -21.246963500976562, Learning Rate: 0.01\n",
      "Epoch [16291/20000], Loss: -21.250167846679688, Learning Rate: 0.01\n",
      "Epoch [16292/20000], Loss: -21.253372192382812, Learning Rate: 0.01\n",
      "Epoch [16293/20000], Loss: -21.254669189453125, Learning Rate: 0.01\n",
      "Epoch [16294/20000], Loss: -21.253616333007812, Learning Rate: 0.01\n",
      "Epoch [16295/20000], Loss: -21.251693725585938, Learning Rate: 0.01\n",
      "Epoch [16296/20000], Loss: -21.250686645507812, Learning Rate: 0.01\n",
      "Epoch [16297/20000], Loss: -21.251312255859375, Learning Rate: 0.01\n",
      "Epoch [16298/20000], Loss: -21.253005981445312, Learning Rate: 0.01\n",
      "Epoch [16299/20000], Loss: -21.254531860351562, Learning Rate: 0.01\n",
      "Epoch [16300/20000], Loss: -21.255035400390625, Learning Rate: 0.01\n",
      "Epoch [16301/20000], Loss: -21.254501342773438, Learning Rate: 0.01\n",
      "Epoch [16302/20000], Loss: -21.253692626953125, Learning Rate: 0.01\n",
      "Epoch [16303/20000], Loss: -21.253204345703125, Learning Rate: 0.01\n",
      "Epoch [16304/20000], Loss: -21.253433227539062, Learning Rate: 0.01\n",
      "Epoch [16305/20000], Loss: -21.25433349609375, Learning Rate: 0.01\n",
      "Epoch [16306/20000], Loss: -21.255081176757812, Learning Rate: 0.01\n",
      "Epoch [16307/20000], Loss: -21.255416870117188, Learning Rate: 0.01\n",
      "Epoch [16308/20000], Loss: -21.25531005859375, Learning Rate: 0.01\n",
      "Epoch [16309/20000], Loss: -21.254928588867188, Learning Rate: 0.01\n",
      "Epoch [16310/20000], Loss: -21.254669189453125, Learning Rate: 0.01\n",
      "Epoch [16311/20000], Loss: -21.254714965820312, Learning Rate: 0.01\n",
      "Epoch [16312/20000], Loss: -21.25506591796875, Learning Rate: 0.01\n",
      "Epoch [16313/20000], Loss: -21.255523681640625, Learning Rate: 0.01\n",
      "Epoch [16314/20000], Loss: -21.255844116210938, Learning Rate: 0.01\n",
      "Epoch [16315/20000], Loss: -21.255859375, Learning Rate: 0.01\n",
      "Epoch [16316/20000], Loss: -21.255767822265625, Learning Rate: 0.01\n",
      "Epoch [16317/20000], Loss: -21.255645751953125, Learning Rate: 0.01\n",
      "Epoch [16318/20000], Loss: -21.255615234375, Learning Rate: 0.01\n",
      "Epoch [16319/20000], Loss: -21.255706787109375, Learning Rate: 0.01\n",
      "Epoch [16320/20000], Loss: -21.255935668945312, Learning Rate: 0.01\n",
      "Epoch [16321/20000], Loss: -21.256149291992188, Learning Rate: 0.01\n",
      "Epoch [16322/20000], Loss: -21.256256103515625, Learning Rate: 0.01\n",
      "Epoch [16323/20000], Loss: -21.256301879882812, Learning Rate: 0.01\n",
      "Epoch [16324/20000], Loss: -21.256240844726562, Learning Rate: 0.01\n",
      "Epoch [16325/20000], Loss: -21.256301879882812, Learning Rate: 0.01\n",
      "Epoch [16326/20000], Loss: -21.256271362304688, Learning Rate: 0.01\n",
      "Epoch [16327/20000], Loss: -21.256332397460938, Learning Rate: 0.01\n",
      "Epoch [16328/20000], Loss: -21.256423950195312, Learning Rate: 0.01\n",
      "Epoch [16329/20000], Loss: -21.256576538085938, Learning Rate: 0.01\n",
      "Epoch [16330/20000], Loss: -21.256729125976562, Learning Rate: 0.01\n",
      "Epoch [16331/20000], Loss: -21.25677490234375, Learning Rate: 0.01\n",
      "Epoch [16332/20000], Loss: -21.2568359375, Learning Rate: 0.01\n",
      "Epoch [16333/20000], Loss: -21.25677490234375, Learning Rate: 0.01\n",
      "Epoch [16334/20000], Loss: -21.256881713867188, Learning Rate: 0.01\n",
      "Epoch [16335/20000], Loss: -21.25689697265625, Learning Rate: 0.01\n",
      "Epoch [16336/20000], Loss: -21.2569580078125, Learning Rate: 0.01\n",
      "Epoch [16337/20000], Loss: -21.257034301757812, Learning Rate: 0.01\n",
      "Epoch [16338/20000], Loss: -21.25714111328125, Learning Rate: 0.01\n",
      "Epoch [16339/20000], Loss: -21.257171630859375, Learning Rate: 0.01\n",
      "Epoch [16340/20000], Loss: -21.257217407226562, Learning Rate: 0.01\n",
      "Epoch [16341/20000], Loss: -21.257278442382812, Learning Rate: 0.01\n",
      "Epoch [16342/20000], Loss: -21.257339477539062, Learning Rate: 0.01\n",
      "Epoch [16343/20000], Loss: -21.25732421875, Learning Rate: 0.01\n",
      "Epoch [16344/20000], Loss: -21.25738525390625, Learning Rate: 0.01\n",
      "Epoch [16345/20000], Loss: -21.257461547851562, Learning Rate: 0.01\n",
      "Epoch [16346/20000], Loss: -21.257553100585938, Learning Rate: 0.01\n",
      "Epoch [16347/20000], Loss: -21.257568359375, Learning Rate: 0.01\n",
      "Epoch [16348/20000], Loss: -21.2576904296875, Learning Rate: 0.01\n",
      "Epoch [16349/20000], Loss: -21.257736206054688, Learning Rate: 0.01\n",
      "Epoch [16350/20000], Loss: -21.257705688476562, Learning Rate: 0.01\n",
      "Epoch [16351/20000], Loss: -21.257781982421875, Learning Rate: 0.01\n",
      "Epoch [16352/20000], Loss: -21.257904052734375, Learning Rate: 0.01\n",
      "Epoch [16353/20000], Loss: -21.2579345703125, Learning Rate: 0.01\n",
      "Epoch [16354/20000], Loss: -21.257904052734375, Learning Rate: 0.01\n",
      "Epoch [16355/20000], Loss: -21.258041381835938, Learning Rate: 0.01\n",
      "Epoch [16356/20000], Loss: -21.258041381835938, Learning Rate: 0.01\n",
      "Epoch [16357/20000], Loss: -21.258193969726562, Learning Rate: 0.01\n",
      "Epoch [16358/20000], Loss: -21.258209228515625, Learning Rate: 0.01\n",
      "Epoch [16359/20000], Loss: -21.25823974609375, Learning Rate: 0.01\n",
      "Epoch [16360/20000], Loss: -21.258255004882812, Learning Rate: 0.01\n",
      "Epoch [16361/20000], Loss: -21.258255004882812, Learning Rate: 0.01\n",
      "Epoch [16362/20000], Loss: -21.258377075195312, Learning Rate: 0.01\n",
      "Epoch [16363/20000], Loss: -21.258392333984375, Learning Rate: 0.01\n",
      "Epoch [16364/20000], Loss: -21.258514404296875, Learning Rate: 0.01\n",
      "Epoch [16365/20000], Loss: -21.258529663085938, Learning Rate: 0.01\n",
      "Epoch [16366/20000], Loss: -21.258636474609375, Learning Rate: 0.01\n",
      "Epoch [16367/20000], Loss: -21.2586669921875, Learning Rate: 0.01\n",
      "Epoch [16368/20000], Loss: -21.258743286132812, Learning Rate: 0.01\n",
      "Epoch [16369/20000], Loss: -21.258743286132812, Learning Rate: 0.01\n",
      "Epoch [16370/20000], Loss: -21.258819580078125, Learning Rate: 0.01\n",
      "Epoch [16371/20000], Loss: -21.25885009765625, Learning Rate: 0.01\n",
      "Epoch [16372/20000], Loss: -21.258926391601562, Learning Rate: 0.01\n",
      "Epoch [16373/20000], Loss: -21.259002685546875, Learning Rate: 0.01\n",
      "Epoch [16374/20000], Loss: -21.259002685546875, Learning Rate: 0.01\n",
      "Epoch [16375/20000], Loss: -21.25909423828125, Learning Rate: 0.01\n",
      "Epoch [16376/20000], Loss: -21.2591552734375, Learning Rate: 0.01\n",
      "Epoch [16377/20000], Loss: -21.259185791015625, Learning Rate: 0.01\n",
      "Epoch [16378/20000], Loss: -21.259262084960938, Learning Rate: 0.01\n",
      "Epoch [16379/20000], Loss: -21.25933837890625, Learning Rate: 0.01\n",
      "Epoch [16380/20000], Loss: -21.259292602539062, Learning Rate: 0.01\n",
      "Epoch [16381/20000], Loss: -21.259368896484375, Learning Rate: 0.01\n",
      "Epoch [16382/20000], Loss: -21.259384155273438, Learning Rate: 0.01\n",
      "Epoch [16383/20000], Loss: -21.259521484375, Learning Rate: 0.01\n",
      "Epoch [16384/20000], Loss: -21.25958251953125, Learning Rate: 0.01\n",
      "Epoch [16385/20000], Loss: -21.259628295898438, Learning Rate: 0.01\n",
      "Epoch [16386/20000], Loss: -21.259658813476562, Learning Rate: 0.01\n",
      "Epoch [16387/20000], Loss: -21.259658813476562, Learning Rate: 0.01\n",
      "Epoch [16388/20000], Loss: -21.259658813476562, Learning Rate: 0.01\n",
      "Epoch [16389/20000], Loss: -21.259674072265625, Learning Rate: 0.01\n",
      "Epoch [16390/20000], Loss: -21.259613037109375, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16391/20000], Loss: -21.259521484375, Learning Rate: 0.01\n",
      "Epoch [16392/20000], Loss: -21.259353637695312, Learning Rate: 0.01\n",
      "Epoch [16393/20000], Loss: -21.259078979492188, Learning Rate: 0.01\n",
      "Epoch [16394/20000], Loss: -21.258544921875, Learning Rate: 0.01\n",
      "Epoch [16395/20000], Loss: -21.257766723632812, Learning Rate: 0.01\n",
      "Epoch [16396/20000], Loss: -21.256393432617188, Learning Rate: 0.01\n",
      "Epoch [16397/20000], Loss: -21.254226684570312, Learning Rate: 0.01\n",
      "Epoch [16398/20000], Loss: -21.250640869140625, Learning Rate: 0.01\n",
      "Epoch [16399/20000], Loss: -21.244598388671875, Learning Rate: 0.01\n",
      "Epoch [16400/20000], Loss: -21.234695434570312, Learning Rate: 0.01\n",
      "Epoch [16401/20000], Loss: -21.218353271484375, Learning Rate: 0.01\n",
      "Epoch [16402/20000], Loss: -21.191253662109375, Learning Rate: 0.01\n",
      "Epoch [16403/20000], Loss: -21.145767211914062, Learning Rate: 0.01\n",
      "Epoch [16404/20000], Loss: -21.070907592773438, Learning Rate: 0.01\n",
      "Epoch [16405/20000], Loss: -20.945556640625, Learning Rate: 0.01\n",
      "Epoch [16406/20000], Loss: -20.74383544921875, Learning Rate: 0.01\n",
      "Epoch [16407/20000], Loss: -20.417526245117188, Learning Rate: 0.01\n",
      "Epoch [16408/20000], Loss: -19.93621826171875, Learning Rate: 0.01\n",
      "Epoch [16409/20000], Loss: -19.249740600585938, Learning Rate: 0.01\n",
      "Epoch [16410/20000], Loss: -18.49261474609375, Learning Rate: 0.01\n",
      "Epoch [16411/20000], Loss: -17.837921142578125, Learning Rate: 0.01\n",
      "Epoch [16412/20000], Loss: -17.871917724609375, Learning Rate: 0.01\n",
      "Epoch [16413/20000], Loss: -18.709381103515625, Learning Rate: 0.01\n",
      "Epoch [16414/20000], Loss: -20.111740112304688, Learning Rate: 0.01\n",
      "Epoch [16415/20000], Loss: -21.093109130859375, Learning Rate: 0.01\n",
      "Epoch [16416/20000], Loss: -21.093170166015625, Learning Rate: 0.01\n",
      "Epoch [16417/20000], Loss: -20.39630126953125, Learning Rate: 0.01\n",
      "Epoch [16418/20000], Loss: -19.8016357421875, Learning Rate: 0.01\n",
      "Epoch [16419/20000], Loss: -19.949264526367188, Learning Rate: 0.01\n",
      "Epoch [16420/20000], Loss: -20.645492553710938, Learning Rate: 0.01\n",
      "Epoch [16421/20000], Loss: -21.205184936523438, Learning Rate: 0.01\n",
      "Epoch [16422/20000], Loss: -21.141708374023438, Learning Rate: 0.01\n",
      "Epoch [16423/20000], Loss: -20.692947387695312, Learning Rate: 0.01\n",
      "Epoch [16424/20000], Loss: -20.45770263671875, Learning Rate: 0.01\n",
      "Epoch [16425/20000], Loss: -20.689102172851562, Learning Rate: 0.01\n",
      "Epoch [16426/20000], Loss: -21.107772827148438, Learning Rate: 0.01\n",
      "Epoch [16427/20000], Loss: -21.251876831054688, Learning Rate: 0.01\n",
      "Epoch [16428/20000], Loss: -21.051849365234375, Learning Rate: 0.01\n",
      "Epoch [16429/20000], Loss: -20.83380126953125, Learning Rate: 0.01\n",
      "Epoch [16430/20000], Loss: -20.875762939453125, Learning Rate: 0.01\n",
      "Epoch [16431/20000], Loss: -21.113540649414062, Learning Rate: 0.01\n",
      "Epoch [16432/20000], Loss: -21.257766723632812, Learning Rate: 0.01\n",
      "Epoch [16433/20000], Loss: -21.181259155273438, Learning Rate: 0.01\n",
      "Epoch [16434/20000], Loss: -21.033477783203125, Learning Rate: 0.01\n",
      "Epoch [16435/20000], Loss: -21.01519775390625, Learning Rate: 0.01\n",
      "Epoch [16436/20000], Loss: -21.14154052734375, Learning Rate: 0.01\n",
      "Epoch [16437/20000], Loss: -21.251861572265625, Learning Rate: 0.01\n",
      "Epoch [16438/20000], Loss: -21.234695434570312, Learning Rate: 0.01\n",
      "Epoch [16439/20000], Loss: -21.14666748046875, Learning Rate: 0.01\n",
      "Epoch [16440/20000], Loss: -21.114334106445312, Learning Rate: 0.01\n",
      "Epoch [16441/20000], Loss: -21.175033569335938, Learning Rate: 0.01\n",
      "Epoch [16442/20000], Loss: -21.248199462890625, Learning Rate: 0.01\n",
      "Epoch [16443/20000], Loss: -21.253936767578125, Learning Rate: 0.01\n",
      "Epoch [16444/20000], Loss: -21.204910278320312, Learning Rate: 0.01\n",
      "Epoch [16445/20000], Loss: -21.174041748046875, Learning Rate: 0.01\n",
      "Epoch [16446/20000], Loss: -21.199615478515625, Learning Rate: 0.01\n",
      "Epoch [16447/20000], Loss: -21.246261596679688, Learning Rate: 0.01\n",
      "Epoch [16448/20000], Loss: -21.261444091796875, Learning Rate: 0.01\n",
      "Epoch [16449/20000], Loss: -21.238021850585938, Learning Rate: 0.01\n",
      "Epoch [16450/20000], Loss: -21.213775634765625, Learning Rate: 0.01\n",
      "Epoch [16451/20000], Loss: -21.220046997070312, Learning Rate: 0.01\n",
      "Epoch [16452/20000], Loss: -21.246627807617188, Learning Rate: 0.01\n",
      "Epoch [16453/20000], Loss: -21.262420654296875, Learning Rate: 0.01\n",
      "Epoch [16454/20000], Loss: -21.25384521484375, Learning Rate: 0.01\n",
      "Epoch [16455/20000], Loss: -21.237274169921875, Learning Rate: 0.01\n",
      "Epoch [16456/20000], Loss: -21.234527587890625, Learning Rate: 0.01\n",
      "Epoch [16457/20000], Loss: -21.247955322265625, Learning Rate: 0.01\n",
      "Epoch [16458/20000], Loss: -21.26104736328125, Learning Rate: 0.01\n",
      "Epoch [16459/20000], Loss: -21.261001586914062, Learning Rate: 0.01\n",
      "Epoch [16460/20000], Loss: -21.251724243164062, Learning Rate: 0.01\n",
      "Epoch [16461/20000], Loss: -21.246337890625, Learning Rate: 0.01\n",
      "Epoch [16462/20000], Loss: -21.251052856445312, Learning Rate: 0.01\n",
      "Epoch [16463/20000], Loss: -21.25982666015625, Learning Rate: 0.01\n",
      "Epoch [16464/20000], Loss: -21.263336181640625, Learning Rate: 0.01\n",
      "Epoch [16465/20000], Loss: -21.259506225585938, Learning Rate: 0.01\n",
      "Epoch [16466/20000], Loss: -21.25457763671875, Learning Rate: 0.01\n",
      "Epoch [16467/20000], Loss: -21.254531860351562, Learning Rate: 0.01\n",
      "Epoch [16468/20000], Loss: -21.259063720703125, Learning Rate: 0.01\n",
      "Epoch [16469/20000], Loss: -21.263031005859375, Learning Rate: 0.01\n",
      "Epoch [16470/20000], Loss: -21.262802124023438, Learning Rate: 0.01\n",
      "Epoch [16471/20000], Loss: -21.25994873046875, Learning Rate: 0.01\n",
      "Epoch [16472/20000], Loss: -21.258209228515625, Learning Rate: 0.01\n",
      "Epoch [16473/20000], Loss: -21.259597778320312, Learning Rate: 0.01\n",
      "Epoch [16474/20000], Loss: -21.262527465820312, Learning Rate: 0.01\n",
      "Epoch [16475/20000], Loss: -21.263839721679688, Learning Rate: 0.01\n",
      "Epoch [16476/20000], Loss: -21.262954711914062, Learning Rate: 0.01\n",
      "Epoch [16477/20000], Loss: -21.261428833007812, Learning Rate: 0.01\n",
      "Epoch [16478/20000], Loss: -21.261093139648438, Learning Rate: 0.01\n",
      "Epoch [16479/20000], Loss: -21.262283325195312, Learning Rate: 0.01\n",
      "Epoch [16480/20000], Loss: -21.263717651367188, Learning Rate: 0.01\n",
      "Epoch [16481/20000], Loss: -21.264190673828125, Learning Rate: 0.01\n",
      "Epoch [16482/20000], Loss: -21.263473510742188, Learning Rate: 0.01\n",
      "Epoch [16483/20000], Loss: -21.262710571289062, Learning Rate: 0.01\n",
      "Epoch [16484/20000], Loss: -21.262832641601562, Learning Rate: 0.01\n",
      "Epoch [16485/20000], Loss: -21.26361083984375, Learning Rate: 0.01\n",
      "Epoch [16486/20000], Loss: -21.264328002929688, Learning Rate: 0.01\n",
      "Epoch [16487/20000], Loss: -21.264419555664062, Learning Rate: 0.01\n",
      "Epoch [16488/20000], Loss: -21.263992309570312, Learning Rate: 0.01\n",
      "Epoch [16489/20000], Loss: -21.263641357421875, Learning Rate: 0.01\n",
      "Epoch [16490/20000], Loss: -21.2637939453125, Learning Rate: 0.01\n",
      "Epoch [16491/20000], Loss: -21.264175415039062, Learning Rate: 0.01\n",
      "Epoch [16492/20000], Loss: -21.264617919921875, Learning Rate: 0.01\n",
      "Epoch [16493/20000], Loss: -21.264678955078125, Learning Rate: 0.01\n",
      "Epoch [16494/20000], Loss: -21.26446533203125, Learning Rate: 0.01\n",
      "Epoch [16495/20000], Loss: -21.2642822265625, Learning Rate: 0.01\n",
      "Epoch [16496/20000], Loss: -21.264419555664062, Learning Rate: 0.01\n",
      "Epoch [16497/20000], Loss: -21.264694213867188, Learning Rate: 0.01\n",
      "Epoch [16498/20000], Loss: -21.264984130859375, Learning Rate: 0.01\n",
      "Epoch [16499/20000], Loss: -21.264984130859375, Learning Rate: 0.01\n",
      "Epoch [16500/20000], Loss: -21.264877319335938, Learning Rate: 0.01\n",
      "Epoch [16501/20000], Loss: -21.264801025390625, Learning Rate: 0.01\n",
      "Epoch [16502/20000], Loss: -21.264892578125, Learning Rate: 0.01\n",
      "Epoch [16503/20000], Loss: -21.264984130859375, Learning Rate: 0.01\n",
      "Epoch [16504/20000], Loss: -21.265151977539062, Learning Rate: 0.01\n",
      "Epoch [16505/20000], Loss: -21.265167236328125, Learning Rate: 0.01\n",
      "Epoch [16506/20000], Loss: -21.265151977539062, Learning Rate: 0.01\n",
      "Epoch [16507/20000], Loss: -21.265060424804688, Learning Rate: 0.01\n",
      "Epoch [16508/20000], Loss: -21.264984130859375, Learning Rate: 0.01\n",
      "Epoch [16509/20000], Loss: -21.264999389648438, Learning Rate: 0.01\n",
      "Epoch [16510/20000], Loss: -21.26495361328125, Learning Rate: 0.01\n",
      "Epoch [16511/20000], Loss: -21.264816284179688, Learning Rate: 0.01\n",
      "Epoch [16512/20000], Loss: -21.264541625976562, Learning Rate: 0.01\n",
      "Epoch [16513/20000], Loss: -21.264175415039062, Learning Rate: 0.01\n",
      "Epoch [16514/20000], Loss: -21.263595581054688, Learning Rate: 0.01\n",
      "Epoch [16515/20000], Loss: -21.262832641601562, Learning Rate: 0.01\n",
      "Epoch [16516/20000], Loss: -21.26165771484375, Learning Rate: 0.01\n",
      "Epoch [16517/20000], Loss: -21.259963989257812, Learning Rate: 0.01\n",
      "Epoch [16518/20000], Loss: -21.257492065429688, Learning Rate: 0.01\n",
      "Epoch [16519/20000], Loss: -21.253753662109375, Learning Rate: 0.01\n",
      "Epoch [16520/20000], Loss: -21.2481689453125, Learning Rate: 0.01\n",
      "Epoch [16521/20000], Loss: -21.239883422851562, Learning Rate: 0.01\n",
      "Epoch [16522/20000], Loss: -21.227462768554688, Learning Rate: 0.01\n",
      "Epoch [16523/20000], Loss: -21.20892333984375, Learning Rate: 0.01\n",
      "Epoch [16524/20000], Loss: -21.181015014648438, Learning Rate: 0.01\n",
      "Epoch [16525/20000], Loss: -21.13970947265625, Learning Rate: 0.01\n",
      "Epoch [16526/20000], Loss: -21.078811645507812, Learning Rate: 0.01\n",
      "Epoch [16527/20000], Loss: -20.991653442382812, Learning Rate: 0.01\n",
      "Epoch [16528/20000], Loss: -20.870529174804688, Learning Rate: 0.01\n",
      "Epoch [16529/20000], Loss: -20.714630126953125, Learning Rate: 0.01\n",
      "Epoch [16530/20000], Loss: -20.5321044921875, Learning Rate: 0.01\n",
      "Epoch [16531/20000], Loss: -20.362045288085938, Learning Rate: 0.01\n",
      "Epoch [16532/20000], Loss: -20.262832641601562, Learning Rate: 0.01\n",
      "Epoch [16533/20000], Loss: -20.314849853515625, Learning Rate: 0.01\n",
      "Epoch [16534/20000], Loss: -20.537628173828125, Learning Rate: 0.01\n",
      "Epoch [16535/20000], Loss: -20.866790771484375, Learning Rate: 0.01\n",
      "Epoch [16536/20000], Loss: -21.15057373046875, Learning Rate: 0.01\n",
      "Epoch [16537/20000], Loss: -21.26617431640625, Learning Rate: 0.01\n",
      "Epoch [16538/20000], Loss: -21.197784423828125, Learning Rate: 0.01\n",
      "Epoch [16539/20000], Loss: -21.035873413085938, Learning Rate: 0.01\n",
      "Epoch [16540/20000], Loss: -20.9091796875, Learning Rate: 0.01\n",
      "Epoch [16541/20000], Loss: -20.904052734375, Learning Rate: 0.01\n",
      "Epoch [16542/20000], Loss: -21.020248413085938, Learning Rate: 0.01\n",
      "Epoch [16543/20000], Loss: -21.172164916992188, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16544/20000], Loss: -21.26092529296875, Learning Rate: 0.01\n",
      "Epoch [16545/20000], Loss: -21.246475219726562, Learning Rate: 0.01\n",
      "Epoch [16546/20000], Loss: -21.167556762695312, Learning Rate: 0.01\n",
      "Epoch [16547/20000], Loss: -21.100234985351562, Learning Rate: 0.01\n",
      "Epoch [16548/20000], Loss: -21.09765625, Learning Rate: 0.01\n",
      "Epoch [16549/20000], Loss: -21.157608032226562, Learning Rate: 0.01\n",
      "Epoch [16550/20000], Loss: -21.230865478515625, Learning Rate: 0.01\n",
      "Epoch [16551/20000], Loss: -21.266693115234375, Learning Rate: 0.01\n",
      "Epoch [16552/20000], Loss: -21.25079345703125, Learning Rate: 0.01\n",
      "Epoch [16553/20000], Loss: -21.209182739257812, Learning Rate: 0.01\n",
      "Epoch [16554/20000], Loss: -21.181396484375, Learning Rate: 0.01\n",
      "Epoch [16555/20000], Loss: -21.18896484375, Learning Rate: 0.01\n",
      "Epoch [16556/20000], Loss: -21.223159790039062, Learning Rate: 0.01\n",
      "Epoch [16557/20000], Loss: -21.256668090820312, Learning Rate: 0.01\n",
      "Epoch [16558/20000], Loss: -21.267486572265625, Learning Rate: 0.01\n",
      "Epoch [16559/20000], Loss: -21.25445556640625, Learning Rate: 0.01\n",
      "Epoch [16560/20000], Loss: -21.233779907226562, Learning Rate: 0.01\n",
      "Epoch [16561/20000], Loss: -21.224014282226562, Learning Rate: 0.01\n",
      "Epoch [16562/20000], Loss: -21.2318115234375, Learning Rate: 0.01\n",
      "Epoch [16563/20000], Loss: -21.24993896484375, Learning Rate: 0.01\n",
      "Epoch [16564/20000], Loss: -21.264739990234375, Learning Rate: 0.01\n",
      "Epoch [16565/20000], Loss: -21.26739501953125, Learning Rate: 0.01\n",
      "Epoch [16566/20000], Loss: -21.259658813476562, Learning Rate: 0.01\n",
      "Epoch [16567/20000], Loss: -21.249923706054688, Learning Rate: 0.01\n",
      "Epoch [16568/20000], Loss: -21.246490478515625, Learning Rate: 0.01\n",
      "Epoch [16569/20000], Loss: -21.251129150390625, Learning Rate: 0.01\n",
      "Epoch [16570/20000], Loss: -21.2601318359375, Learning Rate: 0.01\n",
      "Epoch [16571/20000], Loss: -21.2669677734375, Learning Rate: 0.01\n",
      "Epoch [16572/20000], Loss: -21.267974853515625, Learning Rate: 0.01\n",
      "Epoch [16573/20000], Loss: -21.26416015625, Learning Rate: 0.01\n",
      "Epoch [16574/20000], Loss: -21.259490966796875, Learning Rate: 0.01\n",
      "Epoch [16575/20000], Loss: -21.257736206054688, Learning Rate: 0.01\n",
      "Epoch [16576/20000], Loss: -21.259872436523438, Learning Rate: 0.01\n",
      "Epoch [16577/20000], Loss: -21.264175415039062, Learning Rate: 0.01\n",
      "Epoch [16578/20000], Loss: -21.267669677734375, Learning Rate: 0.01\n",
      "Epoch [16579/20000], Loss: -21.268630981445312, Learning Rate: 0.01\n",
      "Epoch [16580/20000], Loss: -21.26715087890625, Learning Rate: 0.01\n",
      "Epoch [16581/20000], Loss: -21.264923095703125, Learning Rate: 0.01\n",
      "Epoch [16582/20000], Loss: -21.263717651367188, Learning Rate: 0.01\n",
      "Epoch [16583/20000], Loss: -21.264236450195312, Learning Rate: 0.01\n",
      "Epoch [16584/20000], Loss: -21.266021728515625, Learning Rate: 0.01\n",
      "Epoch [16585/20000], Loss: -21.268051147460938, Learning Rate: 0.01\n",
      "Epoch [16586/20000], Loss: -21.268951416015625, Learning Rate: 0.01\n",
      "Epoch [16587/20000], Loss: -21.2686767578125, Learning Rate: 0.01\n",
      "Epoch [16588/20000], Loss: -21.267868041992188, Learning Rate: 0.01\n",
      "Epoch [16589/20000], Loss: -21.266983032226562, Learning Rate: 0.01\n",
      "Epoch [16590/20000], Loss: -21.266830444335938, Learning Rate: 0.01\n",
      "Epoch [16591/20000], Loss: -21.267364501953125, Learning Rate: 0.01\n",
      "Epoch [16592/20000], Loss: -21.268280029296875, Learning Rate: 0.01\n",
      "Epoch [16593/20000], Loss: -21.269073486328125, Learning Rate: 0.01\n",
      "Epoch [16594/20000], Loss: -21.269378662109375, Learning Rate: 0.01\n",
      "Epoch [16595/20000], Loss: -21.269195556640625, Learning Rate: 0.01\n",
      "Epoch [16596/20000], Loss: -21.26873779296875, Learning Rate: 0.01\n",
      "Epoch [16597/20000], Loss: -21.268478393554688, Learning Rate: 0.01\n",
      "Epoch [16598/20000], Loss: -21.26849365234375, Learning Rate: 0.01\n",
      "Epoch [16599/20000], Loss: -21.268798828125, Learning Rate: 0.01\n",
      "Epoch [16600/20000], Loss: -21.269180297851562, Learning Rate: 0.01\n",
      "Epoch [16601/20000], Loss: -21.269577026367188, Learning Rate: 0.01\n",
      "Epoch [16602/20000], Loss: -21.269729614257812, Learning Rate: 0.01\n",
      "Epoch [16603/20000], Loss: -21.269790649414062, Learning Rate: 0.01\n",
      "Epoch [16604/20000], Loss: -21.269638061523438, Learning Rate: 0.01\n",
      "Epoch [16605/20000], Loss: -21.269561767578125, Learning Rate: 0.01\n",
      "Epoch [16606/20000], Loss: -21.26947021484375, Learning Rate: 0.01\n",
      "Epoch [16607/20000], Loss: -21.269622802734375, Learning Rate: 0.01\n",
      "Epoch [16608/20000], Loss: -21.269760131835938, Learning Rate: 0.01\n",
      "Epoch [16609/20000], Loss: -21.27001953125, Learning Rate: 0.01\n",
      "Epoch [16610/20000], Loss: -21.2701416015625, Learning Rate: 0.01\n",
      "Epoch [16611/20000], Loss: -21.270217895507812, Learning Rate: 0.01\n",
      "Epoch [16612/20000], Loss: -21.270217895507812, Learning Rate: 0.01\n",
      "Epoch [16613/20000], Loss: -21.27020263671875, Learning Rate: 0.01\n",
      "Epoch [16614/20000], Loss: -21.270172119140625, Learning Rate: 0.01\n",
      "Epoch [16615/20000], Loss: -21.27020263671875, Learning Rate: 0.01\n",
      "Epoch [16616/20000], Loss: -21.27032470703125, Learning Rate: 0.01\n",
      "Epoch [16617/20000], Loss: -21.270339965820312, Learning Rate: 0.01\n",
      "Epoch [16618/20000], Loss: -21.270431518554688, Learning Rate: 0.01\n",
      "Epoch [16619/20000], Loss: -21.2706298828125, Learning Rate: 0.01\n",
      "Epoch [16620/20000], Loss: -21.270675659179688, Learning Rate: 0.01\n",
      "Epoch [16621/20000], Loss: -21.270706176757812, Learning Rate: 0.01\n",
      "Epoch [16622/20000], Loss: -21.27069091796875, Learning Rate: 0.01\n",
      "Epoch [16623/20000], Loss: -21.270706176757812, Learning Rate: 0.01\n",
      "Epoch [16624/20000], Loss: -21.270736694335938, Learning Rate: 0.01\n",
      "Epoch [16625/20000], Loss: -21.270797729492188, Learning Rate: 0.01\n",
      "Epoch [16626/20000], Loss: -21.270919799804688, Learning Rate: 0.01\n",
      "Epoch [16627/20000], Loss: -21.270919799804688, Learning Rate: 0.01\n",
      "Epoch [16628/20000], Loss: -21.270965576171875, Learning Rate: 0.01\n",
      "Epoch [16629/20000], Loss: -21.271011352539062, Learning Rate: 0.01\n",
      "Epoch [16630/20000], Loss: -21.271087646484375, Learning Rate: 0.01\n",
      "Epoch [16631/20000], Loss: -21.271148681640625, Learning Rate: 0.01\n",
      "Epoch [16632/20000], Loss: -21.271209716796875, Learning Rate: 0.01\n",
      "Epoch [16633/20000], Loss: -21.271255493164062, Learning Rate: 0.01\n",
      "Epoch [16634/20000], Loss: -21.271240234375, Learning Rate: 0.01\n",
      "Epoch [16635/20000], Loss: -21.271316528320312, Learning Rate: 0.01\n",
      "Epoch [16636/20000], Loss: -21.271392822265625, Learning Rate: 0.01\n",
      "Epoch [16637/20000], Loss: -21.271453857421875, Learning Rate: 0.01\n",
      "Epoch [16638/20000], Loss: -21.271438598632812, Learning Rate: 0.01\n",
      "Epoch [16639/20000], Loss: -21.271530151367188, Learning Rate: 0.01\n",
      "Epoch [16640/20000], Loss: -21.271575927734375, Learning Rate: 0.01\n",
      "Epoch [16641/20000], Loss: -21.271652221679688, Learning Rate: 0.01\n",
      "Epoch [16642/20000], Loss: -21.271713256835938, Learning Rate: 0.01\n",
      "Epoch [16643/20000], Loss: -21.271774291992188, Learning Rate: 0.01\n",
      "Epoch [16644/20000], Loss: -21.27178955078125, Learning Rate: 0.01\n",
      "Epoch [16645/20000], Loss: -21.27191162109375, Learning Rate: 0.01\n",
      "Epoch [16646/20000], Loss: -21.271835327148438, Learning Rate: 0.01\n",
      "Epoch [16647/20000], Loss: -21.271942138671875, Learning Rate: 0.01\n",
      "Epoch [16648/20000], Loss: -21.272018432617188, Learning Rate: 0.01\n",
      "Epoch [16649/20000], Loss: -21.27203369140625, Learning Rate: 0.01\n",
      "Epoch [16650/20000], Loss: -21.272125244140625, Learning Rate: 0.01\n",
      "Epoch [16651/20000], Loss: -21.272079467773438, Learning Rate: 0.01\n",
      "Epoch [16652/20000], Loss: -21.272171020507812, Learning Rate: 0.01\n",
      "Epoch [16653/20000], Loss: -21.272232055664062, Learning Rate: 0.01\n",
      "Epoch [16654/20000], Loss: -21.272262573242188, Learning Rate: 0.01\n",
      "Epoch [16655/20000], Loss: -21.2723388671875, Learning Rate: 0.01\n",
      "Epoch [16656/20000], Loss: -21.2723388671875, Learning Rate: 0.01\n",
      "Epoch [16657/20000], Loss: -21.272369384765625, Learning Rate: 0.01\n",
      "Epoch [16658/20000], Loss: -21.2724609375, Learning Rate: 0.01\n",
      "Epoch [16659/20000], Loss: -21.272506713867188, Learning Rate: 0.01\n",
      "Epoch [16660/20000], Loss: -21.272598266601562, Learning Rate: 0.01\n",
      "Epoch [16661/20000], Loss: -21.27252197265625, Learning Rate: 0.01\n",
      "Epoch [16662/20000], Loss: -21.27264404296875, Learning Rate: 0.01\n",
      "Epoch [16663/20000], Loss: -21.272705078125, Learning Rate: 0.01\n",
      "Epoch [16664/20000], Loss: -21.272720336914062, Learning Rate: 0.01\n",
      "Epoch [16665/20000], Loss: -21.272796630859375, Learning Rate: 0.01\n",
      "Epoch [16666/20000], Loss: -21.272872924804688, Learning Rate: 0.01\n",
      "Epoch [16667/20000], Loss: -21.272964477539062, Learning Rate: 0.01\n",
      "Epoch [16668/20000], Loss: -21.27294921875, Learning Rate: 0.01\n",
      "Epoch [16669/20000], Loss: -21.273025512695312, Learning Rate: 0.01\n",
      "Epoch [16670/20000], Loss: -21.27294921875, Learning Rate: 0.01\n",
      "Epoch [16671/20000], Loss: -21.273117065429688, Learning Rate: 0.01\n",
      "Epoch [16672/20000], Loss: -21.273117065429688, Learning Rate: 0.01\n",
      "Epoch [16673/20000], Loss: -21.273239135742188, Learning Rate: 0.01\n",
      "Epoch [16674/20000], Loss: -21.273223876953125, Learning Rate: 0.01\n",
      "Epoch [16675/20000], Loss: -21.27325439453125, Learning Rate: 0.01\n",
      "Epoch [16676/20000], Loss: -21.273269653320312, Learning Rate: 0.01\n",
      "Epoch [16677/20000], Loss: -21.273361206054688, Learning Rate: 0.01\n",
      "Epoch [16678/20000], Loss: -21.273361206054688, Learning Rate: 0.01\n",
      "Epoch [16679/20000], Loss: -21.273422241210938, Learning Rate: 0.01\n",
      "Epoch [16680/20000], Loss: -21.273529052734375, Learning Rate: 0.01\n",
      "Epoch [16681/20000], Loss: -21.273529052734375, Learning Rate: 0.01\n",
      "Epoch [16682/20000], Loss: -21.273605346679688, Learning Rate: 0.01\n",
      "Epoch [16683/20000], Loss: -21.273651123046875, Learning Rate: 0.01\n",
      "Epoch [16684/20000], Loss: -21.273696899414062, Learning Rate: 0.01\n",
      "Epoch [16685/20000], Loss: -21.273681640625, Learning Rate: 0.01\n",
      "Epoch [16686/20000], Loss: -21.273773193359375, Learning Rate: 0.01\n",
      "Epoch [16687/20000], Loss: -21.273834228515625, Learning Rate: 0.01\n",
      "Epoch [16688/20000], Loss: -21.273757934570312, Learning Rate: 0.01\n",
      "Epoch [16689/20000], Loss: -21.273895263671875, Learning Rate: 0.01\n",
      "Epoch [16690/20000], Loss: -21.273773193359375, Learning Rate: 0.01\n",
      "Epoch [16691/20000], Loss: -21.273834228515625, Learning Rate: 0.01\n",
      "Epoch [16692/20000], Loss: -21.27374267578125, Learning Rate: 0.01\n",
      "Epoch [16693/20000], Loss: -21.273666381835938, Learning Rate: 0.01\n",
      "Epoch [16694/20000], Loss: -21.273361206054688, Learning Rate: 0.01\n",
      "Epoch [16695/20000], Loss: -21.2730712890625, Learning Rate: 0.01\n",
      "Epoch [16696/20000], Loss: -21.27239990234375, Learning Rate: 0.01\n",
      "Epoch [16697/20000], Loss: -21.271392822265625, Learning Rate: 0.01\n",
      "Epoch [16698/20000], Loss: -21.269683837890625, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16699/20000], Loss: -21.26702880859375, Learning Rate: 0.01\n",
      "Epoch [16700/20000], Loss: -21.262741088867188, Learning Rate: 0.01\n",
      "Epoch [16701/20000], Loss: -21.25543212890625, Learning Rate: 0.01\n",
      "Epoch [16702/20000], Loss: -21.243682861328125, Learning Rate: 0.01\n",
      "Epoch [16703/20000], Loss: -21.22393798828125, Learning Rate: 0.01\n",
      "Epoch [16704/20000], Loss: -21.191665649414062, Learning Rate: 0.01\n",
      "Epoch [16705/20000], Loss: -21.1375732421875, Learning Rate: 0.01\n",
      "Epoch [16706/20000], Loss: -21.049041748046875, Learning Rate: 0.01\n",
      "Epoch [16707/20000], Loss: -20.901611328125, Learning Rate: 0.01\n",
      "Epoch [16708/20000], Loss: -20.667388916015625, Learning Rate: 0.01\n",
      "Epoch [16709/20000], Loss: -20.293716430664062, Learning Rate: 0.01\n",
      "Epoch [16710/20000], Loss: -19.758544921875, Learning Rate: 0.01\n",
      "Epoch [16711/20000], Loss: -19.02593994140625, Learning Rate: 0.01\n",
      "Epoch [16712/20000], Loss: -18.29046630859375, Learning Rate: 0.01\n",
      "Epoch [16713/20000], Loss: -17.770889282226562, Learning Rate: 0.01\n",
      "Epoch [16714/20000], Loss: -18.050308227539062, Learning Rate: 0.01\n",
      "Epoch [16715/20000], Loss: -19.081619262695312, Learning Rate: 0.01\n",
      "Epoch [16716/20000], Loss: -20.41204833984375, Learning Rate: 0.01\n",
      "Epoch [16717/20000], Loss: -21.100311279296875, Learning Rate: 0.01\n",
      "Epoch [16718/20000], Loss: -20.826309204101562, Learning Rate: 0.01\n",
      "Epoch [16719/20000], Loss: -20.088607788085938, Learning Rate: 0.01\n",
      "Epoch [16720/20000], Loss: -19.683151245117188, Learning Rate: 0.01\n",
      "Epoch [16721/20000], Loss: -20.078964233398438, Learning Rate: 0.01\n",
      "Epoch [16722/20000], Loss: -20.848968505859375, Learning Rate: 0.01\n",
      "Epoch [16723/20000], Loss: -21.266738891601562, Learning Rate: 0.01\n",
      "Epoch [16724/20000], Loss: -21.03997802734375, Learning Rate: 0.01\n",
      "Epoch [16725/20000], Loss: -20.586868286132812, Learning Rate: 0.01\n",
      "Epoch [16726/20000], Loss: -20.489730834960938, Learning Rate: 0.01\n",
      "Epoch [16727/20000], Loss: -20.825164794921875, Learning Rate: 0.01\n",
      "Epoch [16728/20000], Loss: -21.191604614257812, Learning Rate: 0.01\n",
      "Epoch [16729/20000], Loss: -21.202865600585938, Learning Rate: 0.01\n",
      "Epoch [16730/20000], Loss: -20.948348999023438, Learning Rate: 0.01\n",
      "Epoch [16731/20000], Loss: -20.805404663085938, Learning Rate: 0.01\n",
      "Epoch [16732/20000], Loss: -20.949234008789062, Learning Rate: 0.01\n",
      "Epoch [16733/20000], Loss: -21.197677612304688, Learning Rate: 0.01\n",
      "Epoch [16734/20000], Loss: -21.26885986328125, Learning Rate: 0.01\n",
      "Epoch [16735/20000], Loss: -21.137893676757812, Learning Rate: 0.01\n",
      "Epoch [16736/20000], Loss: -21.016586303710938, Learning Rate: 0.01\n",
      "Epoch [16737/20000], Loss: -21.05975341796875, Learning Rate: 0.01\n",
      "Epoch [16738/20000], Loss: -21.200912475585938, Learning Rate: 0.01\n",
      "Epoch [16739/20000], Loss: -21.266342163085938, Learning Rate: 0.01\n",
      "Epoch [16740/20000], Loss: -21.205459594726562, Learning Rate: 0.01\n",
      "Epoch [16741/20000], Loss: -21.126693725585938, Learning Rate: 0.01\n",
      "Epoch [16742/20000], Loss: -21.137664794921875, Learning Rate: 0.01\n",
      "Epoch [16743/20000], Loss: -21.221115112304688, Learning Rate: 0.01\n",
      "Epoch [16744/20000], Loss: -21.274826049804688, Learning Rate: 0.01\n",
      "Epoch [16745/20000], Loss: -21.250320434570312, Learning Rate: 0.01\n",
      "Epoch [16746/20000], Loss: -21.1982421875, Learning Rate: 0.01\n",
      "Epoch [16747/20000], Loss: -21.190261840820312, Learning Rate: 0.01\n",
      "Epoch [16748/20000], Loss: -21.23248291015625, Learning Rate: 0.01\n",
      "Epoch [16749/20000], Loss: -21.270050048828125, Learning Rate: 0.01\n",
      "Epoch [16750/20000], Loss: -21.264358520507812, Learning Rate: 0.01\n",
      "Epoch [16751/20000], Loss: -21.234085083007812, Learning Rate: 0.01\n",
      "Epoch [16752/20000], Loss: -21.222686767578125, Learning Rate: 0.01\n",
      "Epoch [16753/20000], Loss: -21.24371337890625, Learning Rate: 0.01\n",
      "Epoch [16754/20000], Loss: -21.270355224609375, Learning Rate: 0.01\n",
      "Epoch [16755/20000], Loss: -21.274505615234375, Learning Rate: 0.01\n",
      "Epoch [16756/20000], Loss: -21.2584228515625, Learning Rate: 0.01\n",
      "Epoch [16757/20000], Loss: -21.24652099609375, Learning Rate: 0.01\n",
      "Epoch [16758/20000], Loss: -21.253402709960938, Learning Rate: 0.01\n",
      "Epoch [16759/20000], Loss: -21.269149780273438, Learning Rate: 0.01\n",
      "Epoch [16760/20000], Loss: -21.275848388671875, Learning Rate: 0.01\n",
      "Epoch [16761/20000], Loss: -21.268966674804688, Learning Rate: 0.01\n",
      "Epoch [16762/20000], Loss: -21.259719848632812, Learning Rate: 0.01\n",
      "Epoch [16763/20000], Loss: -21.260055541992188, Learning Rate: 0.01\n",
      "Epoch [16764/20000], Loss: -21.268783569335938, Learning Rate: 0.01\n",
      "Epoch [16765/20000], Loss: -21.275924682617188, Learning Rate: 0.01\n",
      "Epoch [16766/20000], Loss: -21.274978637695312, Learning Rate: 0.01\n",
      "Epoch [16767/20000], Loss: -21.26947021484375, Learning Rate: 0.01\n",
      "Epoch [16768/20000], Loss: -21.267105102539062, Learning Rate: 0.01\n",
      "Epoch [16769/20000], Loss: -21.270492553710938, Learning Rate: 0.01\n",
      "Epoch [16770/20000], Loss: -21.275558471679688, Learning Rate: 0.01\n",
      "Epoch [16771/20000], Loss: -21.277145385742188, Learning Rate: 0.01\n",
      "Epoch [16772/20000], Loss: -21.274658203125, Learning Rate: 0.01\n",
      "Epoch [16773/20000], Loss: -21.271865844726562, Learning Rate: 0.01\n",
      "Epoch [16774/20000], Loss: -21.272125244140625, Learning Rate: 0.01\n",
      "Epoch [16775/20000], Loss: -21.2747802734375, Learning Rate: 0.01\n",
      "Epoch [16776/20000], Loss: -21.276947021484375, Learning Rate: 0.01\n",
      "Epoch [16777/20000], Loss: -21.2767333984375, Learning Rate: 0.01\n",
      "Epoch [16778/20000], Loss: -21.27508544921875, Learning Rate: 0.01\n",
      "Epoch [16779/20000], Loss: -21.274139404296875, Learning Rate: 0.01\n",
      "Epoch [16780/20000], Loss: -21.275039672851562, Learning Rate: 0.01\n",
      "Epoch [16781/20000], Loss: -21.276687622070312, Learning Rate: 0.01\n",
      "Epoch [16782/20000], Loss: -21.277572631835938, Learning Rate: 0.01\n",
      "Epoch [16783/20000], Loss: -21.277114868164062, Learning Rate: 0.01\n",
      "Epoch [16784/20000], Loss: -21.276199340820312, Learning Rate: 0.01\n",
      "Epoch [16785/20000], Loss: -21.276031494140625, Learning Rate: 0.01\n",
      "Epoch [16786/20000], Loss: -21.27679443359375, Learning Rate: 0.01\n",
      "Epoch [16787/20000], Loss: -21.277664184570312, Learning Rate: 0.01\n",
      "Epoch [16788/20000], Loss: -21.277969360351562, Learning Rate: 0.01\n",
      "Epoch [16789/20000], Loss: -21.277603149414062, Learning Rate: 0.01\n",
      "Epoch [16790/20000], Loss: -21.277145385742188, Learning Rate: 0.01\n",
      "Epoch [16791/20000], Loss: -21.277175903320312, Learning Rate: 0.01\n",
      "Epoch [16792/20000], Loss: -21.277679443359375, Learning Rate: 0.01\n",
      "Epoch [16793/20000], Loss: -21.278106689453125, Learning Rate: 0.01\n",
      "Epoch [16794/20000], Loss: -21.278213500976562, Learning Rate: 0.01\n",
      "Epoch [16795/20000], Loss: -21.27801513671875, Learning Rate: 0.01\n",
      "Epoch [16796/20000], Loss: -21.277801513671875, Learning Rate: 0.01\n",
      "Epoch [16797/20000], Loss: -21.277923583984375, Learning Rate: 0.01\n",
      "Epoch [16798/20000], Loss: -21.278152465820312, Learning Rate: 0.01\n",
      "Epoch [16799/20000], Loss: -21.27838134765625, Learning Rate: 0.01\n",
      "Epoch [16800/20000], Loss: -21.278549194335938, Learning Rate: 0.01\n",
      "Epoch [16801/20000], Loss: -21.278396606445312, Learning Rate: 0.01\n",
      "Epoch [16802/20000], Loss: -21.278305053710938, Learning Rate: 0.01\n",
      "Epoch [16803/20000], Loss: -21.2783203125, Learning Rate: 0.01\n",
      "Epoch [16804/20000], Loss: -21.278594970703125, Learning Rate: 0.01\n",
      "Epoch [16805/20000], Loss: -21.278640747070312, Learning Rate: 0.01\n",
      "Epoch [16806/20000], Loss: -21.278732299804688, Learning Rate: 0.01\n",
      "Epoch [16807/20000], Loss: -21.2786865234375, Learning Rate: 0.01\n",
      "Epoch [16808/20000], Loss: -21.27862548828125, Learning Rate: 0.01\n",
      "Epoch [16809/20000], Loss: -21.278610229492188, Learning Rate: 0.01\n",
      "Epoch [16810/20000], Loss: -21.278701782226562, Learning Rate: 0.01\n",
      "Epoch [16811/20000], Loss: -21.278701782226562, Learning Rate: 0.01\n",
      "Epoch [16812/20000], Loss: -21.278793334960938, Learning Rate: 0.01\n",
      "Epoch [16813/20000], Loss: -21.27862548828125, Learning Rate: 0.01\n",
      "Epoch [16814/20000], Loss: -21.278488159179688, Learning Rate: 0.01\n",
      "Epoch [16815/20000], Loss: -21.278335571289062, Learning Rate: 0.01\n",
      "Epoch [16816/20000], Loss: -21.278106689453125, Learning Rate: 0.01\n",
      "Epoch [16817/20000], Loss: -21.277679443359375, Learning Rate: 0.01\n",
      "Epoch [16818/20000], Loss: -21.277084350585938, Learning Rate: 0.01\n",
      "Epoch [16819/20000], Loss: -21.276123046875, Learning Rate: 0.01\n",
      "Epoch [16820/20000], Loss: -21.274688720703125, Learning Rate: 0.01\n",
      "Epoch [16821/20000], Loss: -21.272476196289062, Learning Rate: 0.01\n",
      "Epoch [16822/20000], Loss: -21.2691650390625, Learning Rate: 0.01\n",
      "Epoch [16823/20000], Loss: -21.264205932617188, Learning Rate: 0.01\n",
      "Epoch [16824/20000], Loss: -21.256439208984375, Learning Rate: 0.01\n",
      "Epoch [16825/20000], Loss: -21.244552612304688, Learning Rate: 0.01\n",
      "Epoch [16826/20000], Loss: -21.226318359375, Learning Rate: 0.01\n",
      "Epoch [16827/20000], Loss: -21.198150634765625, Learning Rate: 0.01\n",
      "Epoch [16828/20000], Loss: -21.154983520507812, Learning Rate: 0.01\n",
      "Epoch [16829/20000], Loss: -21.089874267578125, Learning Rate: 0.01\n",
      "Epoch [16830/20000], Loss: -20.992904663085938, Learning Rate: 0.01\n",
      "Epoch [16831/20000], Loss: -20.855575561523438, Learning Rate: 0.01\n",
      "Epoch [16832/20000], Loss: -20.670211791992188, Learning Rate: 0.01\n",
      "Epoch [16833/20000], Loss: -20.45184326171875, Learning Rate: 0.01\n",
      "Epoch [16834/20000], Loss: -20.237380981445312, Learning Rate: 0.01\n",
      "Epoch [16835/20000], Loss: -20.122268676757812, Learning Rate: 0.01\n",
      "Epoch [16836/20000], Loss: -20.1915283203125, Learning Rate: 0.01\n",
      "Epoch [16837/20000], Loss: -20.489028930664062, Learning Rate: 0.01\n",
      "Epoch [16838/20000], Loss: -20.891372680664062, Learning Rate: 0.01\n",
      "Epoch [16839/20000], Loss: -21.200851440429688, Learning Rate: 0.01\n",
      "Epoch [16840/20000], Loss: -21.272781372070312, Learning Rate: 0.01\n",
      "Epoch [16841/20000], Loss: -21.130905151367188, Learning Rate: 0.01\n",
      "Epoch [16842/20000], Loss: -20.92803955078125, Learning Rate: 0.01\n",
      "Epoch [16843/20000], Loss: -20.828750610351562, Learning Rate: 0.01\n",
      "Epoch [16844/20000], Loss: -20.907562255859375, Learning Rate: 0.01\n",
      "Epoch [16845/20000], Loss: -21.09405517578125, Learning Rate: 0.01\n",
      "Epoch [16846/20000], Loss: -21.246688842773438, Learning Rate: 0.01\n",
      "Epoch [16847/20000], Loss: -21.268310546875, Learning Rate: 0.01\n",
      "Epoch [16848/20000], Loss: -21.180526733398438, Learning Rate: 0.01\n",
      "Epoch [16849/20000], Loss: -21.085235595703125, Learning Rate: 0.01\n",
      "Epoch [16850/20000], Loss: -21.069564819335938, Learning Rate: 0.01\n",
      "Epoch [16851/20000], Loss: -21.141876220703125, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16852/20000], Loss: -21.234603881835938, Learning Rate: 0.01\n",
      "Epoch [16853/20000], Loss: -21.275680541992188, Learning Rate: 0.01\n",
      "Epoch [16854/20000], Loss: -21.248336791992188, Learning Rate: 0.01\n",
      "Epoch [16855/20000], Loss: -21.19549560546875, Learning Rate: 0.01\n",
      "Epoch [16856/20000], Loss: -21.171920776367188, Learning Rate: 0.01\n",
      "Epoch [16857/20000], Loss: -21.196578979492188, Learning Rate: 0.01\n",
      "Epoch [16858/20000], Loss: -21.24420166015625, Learning Rate: 0.01\n",
      "Epoch [16859/20000], Loss: -21.274215698242188, Learning Rate: 0.01\n",
      "Epoch [16860/20000], Loss: -21.268753051757812, Learning Rate: 0.01\n",
      "Epoch [16861/20000], Loss: -21.243087768554688, Learning Rate: 0.01\n",
      "Epoch [16862/20000], Loss: -21.225982666015625, Learning Rate: 0.01\n",
      "Epoch [16863/20000], Loss: -21.232925415039062, Learning Rate: 0.01\n",
      "Epoch [16864/20000], Loss: -21.2557373046875, Learning Rate: 0.01\n",
      "Epoch [16865/20000], Loss: -21.274490356445312, Learning Rate: 0.01\n",
      "Epoch [16866/20000], Loss: -21.27618408203125, Learning Rate: 0.01\n",
      "Epoch [16867/20000], Loss: -21.264663696289062, Learning Rate: 0.01\n",
      "Epoch [16868/20000], Loss: -21.253829956054688, Learning Rate: 0.01\n",
      "Epoch [16869/20000], Loss: -21.254074096679688, Learning Rate: 0.01\n",
      "Epoch [16870/20000], Loss: -21.264495849609375, Learning Rate: 0.01\n",
      "Epoch [16871/20000], Loss: -21.275741577148438, Learning Rate: 0.01\n",
      "Epoch [16872/20000], Loss: -21.279434204101562, Learning Rate: 0.01\n",
      "Epoch [16873/20000], Loss: -21.275070190429688, Learning Rate: 0.01\n",
      "Epoch [16874/20000], Loss: -21.268508911132812, Learning Rate: 0.01\n",
      "Epoch [16875/20000], Loss: -21.266326904296875, Learning Rate: 0.01\n",
      "Epoch [16876/20000], Loss: -21.270065307617188, Learning Rate: 0.01\n",
      "Epoch [16877/20000], Loss: -21.27655029296875, Learning Rate: 0.01\n",
      "Epoch [16878/20000], Loss: -21.280593872070312, Learning Rate: 0.01\n",
      "Epoch [16879/20000], Loss: -21.28009033203125, Learning Rate: 0.01\n",
      "Epoch [16880/20000], Loss: -21.276611328125, Learning Rate: 0.01\n",
      "Epoch [16881/20000], Loss: -21.273880004882812, Learning Rate: 0.01\n",
      "Epoch [16882/20000], Loss: -21.274246215820312, Learning Rate: 0.01\n",
      "Epoch [16883/20000], Loss: -21.277099609375, Learning Rate: 0.01\n",
      "Epoch [16884/20000], Loss: -21.280441284179688, Learning Rate: 0.01\n",
      "Epoch [16885/20000], Loss: -21.28179931640625, Learning Rate: 0.01\n",
      "Epoch [16886/20000], Loss: -21.2808837890625, Learning Rate: 0.01\n",
      "Epoch [16887/20000], Loss: -21.279006958007812, Learning Rate: 0.01\n",
      "Epoch [16888/20000], Loss: -21.277862548828125, Learning Rate: 0.01\n",
      "Epoch [16889/20000], Loss: -21.278350830078125, Learning Rate: 0.01\n",
      "Epoch [16890/20000], Loss: -21.280044555664062, Learning Rate: 0.01\n",
      "Epoch [16891/20000], Loss: -21.281585693359375, Learning Rate: 0.01\n",
      "Epoch [16892/20000], Loss: -21.28228759765625, Learning Rate: 0.01\n",
      "Epoch [16893/20000], Loss: -21.28173828125, Learning Rate: 0.01\n",
      "Epoch [16894/20000], Loss: -21.28082275390625, Learning Rate: 0.01\n",
      "Epoch [16895/20000], Loss: -21.280242919921875, Learning Rate: 0.01\n",
      "Epoch [16896/20000], Loss: -21.280441284179688, Learning Rate: 0.01\n",
      "Epoch [16897/20000], Loss: -21.28118896484375, Learning Rate: 0.01\n",
      "Epoch [16898/20000], Loss: -21.282012939453125, Learning Rate: 0.01\n",
      "Epoch [16899/20000], Loss: -21.282501220703125, Learning Rate: 0.01\n",
      "Epoch [16900/20000], Loss: -21.28240966796875, Learning Rate: 0.01\n",
      "Epoch [16901/20000], Loss: -21.282089233398438, Learning Rate: 0.01\n",
      "Epoch [16902/20000], Loss: -21.281784057617188, Learning Rate: 0.01\n",
      "Epoch [16903/20000], Loss: -21.281707763671875, Learning Rate: 0.01\n",
      "Epoch [16904/20000], Loss: -21.282012939453125, Learning Rate: 0.01\n",
      "Epoch [16905/20000], Loss: -21.282440185546875, Learning Rate: 0.01\n",
      "Epoch [16906/20000], Loss: -21.282745361328125, Learning Rate: 0.01\n",
      "Epoch [16907/20000], Loss: -21.2828369140625, Learning Rate: 0.01\n",
      "Epoch [16908/20000], Loss: -21.28277587890625, Learning Rate: 0.01\n",
      "Epoch [16909/20000], Loss: -21.2825927734375, Learning Rate: 0.01\n",
      "Epoch [16910/20000], Loss: -21.282608032226562, Learning Rate: 0.01\n",
      "Epoch [16911/20000], Loss: -21.28265380859375, Learning Rate: 0.01\n",
      "Epoch [16912/20000], Loss: -21.282821655273438, Learning Rate: 0.01\n",
      "Epoch [16913/20000], Loss: -21.28302001953125, Learning Rate: 0.01\n",
      "Epoch [16914/20000], Loss: -21.283111572265625, Learning Rate: 0.01\n",
      "Epoch [16915/20000], Loss: -21.283203125, Learning Rate: 0.01\n",
      "Epoch [16916/20000], Loss: -21.28326416015625, Learning Rate: 0.01\n",
      "Epoch [16917/20000], Loss: -21.283126831054688, Learning Rate: 0.01\n",
      "Epoch [16918/20000], Loss: -21.283218383789062, Learning Rate: 0.01\n",
      "Epoch [16919/20000], Loss: -21.283187866210938, Learning Rate: 0.01\n",
      "Epoch [16920/20000], Loss: -21.283294677734375, Learning Rate: 0.01\n",
      "Epoch [16921/20000], Loss: -21.283432006835938, Learning Rate: 0.01\n",
      "Epoch [16922/20000], Loss: -21.283493041992188, Learning Rate: 0.01\n",
      "Epoch [16923/20000], Loss: -21.283538818359375, Learning Rate: 0.01\n",
      "Epoch [16924/20000], Loss: -21.283615112304688, Learning Rate: 0.01\n",
      "Epoch [16925/20000], Loss: -21.283660888671875, Learning Rate: 0.01\n",
      "Epoch [16926/20000], Loss: -21.28363037109375, Learning Rate: 0.01\n",
      "Epoch [16927/20000], Loss: -21.283599853515625, Learning Rate: 0.01\n",
      "Epoch [16928/20000], Loss: -21.283660888671875, Learning Rate: 0.01\n",
      "Epoch [16929/20000], Loss: -21.283782958984375, Learning Rate: 0.01\n",
      "Epoch [16930/20000], Loss: -21.283889770507812, Learning Rate: 0.01\n",
      "Epoch [16931/20000], Loss: -21.283920288085938, Learning Rate: 0.01\n",
      "Epoch [16932/20000], Loss: -21.28399658203125, Learning Rate: 0.01\n",
      "Epoch [16933/20000], Loss: -21.284027099609375, Learning Rate: 0.01\n",
      "Epoch [16934/20000], Loss: -21.284072875976562, Learning Rate: 0.01\n",
      "Epoch [16935/20000], Loss: -21.284072875976562, Learning Rate: 0.01\n",
      "Epoch [16936/20000], Loss: -21.284103393554688, Learning Rate: 0.01\n",
      "Epoch [16937/20000], Loss: -21.284149169921875, Learning Rate: 0.01\n",
      "Epoch [16938/20000], Loss: -21.284194946289062, Learning Rate: 0.01\n",
      "Epoch [16939/20000], Loss: -21.28424072265625, Learning Rate: 0.01\n",
      "Epoch [16940/20000], Loss: -21.284332275390625, Learning Rate: 0.01\n",
      "Epoch [16941/20000], Loss: -21.284408569335938, Learning Rate: 0.01\n",
      "Epoch [16942/20000], Loss: -21.284423828125, Learning Rate: 0.01\n",
      "Epoch [16943/20000], Loss: -21.284469604492188, Learning Rate: 0.01\n",
      "Epoch [16944/20000], Loss: -21.28448486328125, Learning Rate: 0.01\n",
      "Epoch [16945/20000], Loss: -21.284500122070312, Learning Rate: 0.01\n",
      "Epoch [16946/20000], Loss: -21.284591674804688, Learning Rate: 0.01\n",
      "Epoch [16947/20000], Loss: -21.2845458984375, Learning Rate: 0.01\n",
      "Epoch [16948/20000], Loss: -21.284561157226562, Learning Rate: 0.01\n",
      "Epoch [16949/20000], Loss: -21.284698486328125, Learning Rate: 0.01\n",
      "Epoch [16950/20000], Loss: -21.284652709960938, Learning Rate: 0.01\n",
      "Epoch [16951/20000], Loss: -21.284774780273438, Learning Rate: 0.01\n",
      "Epoch [16952/20000], Loss: -21.284881591796875, Learning Rate: 0.01\n",
      "Epoch [16953/20000], Loss: -21.28485107421875, Learning Rate: 0.01\n",
      "Epoch [16954/20000], Loss: -21.284896850585938, Learning Rate: 0.01\n",
      "Epoch [16955/20000], Loss: -21.2850341796875, Learning Rate: 0.01\n",
      "Epoch [16956/20000], Loss: -21.285003662109375, Learning Rate: 0.01\n",
      "Epoch [16957/20000], Loss: -21.285064697265625, Learning Rate: 0.01\n",
      "Epoch [16958/20000], Loss: -21.285049438476562, Learning Rate: 0.01\n",
      "Epoch [16959/20000], Loss: -21.285064697265625, Learning Rate: 0.01\n",
      "Epoch [16960/20000], Loss: -21.285140991210938, Learning Rate: 0.01\n",
      "Epoch [16961/20000], Loss: -21.285232543945312, Learning Rate: 0.01\n",
      "Epoch [16962/20000], Loss: -21.285263061523438, Learning Rate: 0.01\n",
      "Epoch [16963/20000], Loss: -21.285308837890625, Learning Rate: 0.01\n",
      "Epoch [16964/20000], Loss: -21.285369873046875, Learning Rate: 0.01\n",
      "Epoch [16965/20000], Loss: -21.285354614257812, Learning Rate: 0.01\n",
      "Epoch [16966/20000], Loss: -21.285369873046875, Learning Rate: 0.01\n",
      "Epoch [16967/20000], Loss: -21.285476684570312, Learning Rate: 0.01\n",
      "Epoch [16968/20000], Loss: -21.285507202148438, Learning Rate: 0.01\n",
      "Epoch [16969/20000], Loss: -21.285568237304688, Learning Rate: 0.01\n",
      "Epoch [16970/20000], Loss: -21.28558349609375, Learning Rate: 0.01\n",
      "Epoch [16971/20000], Loss: -21.285629272460938, Learning Rate: 0.01\n",
      "Epoch [16972/20000], Loss: -21.285690307617188, Learning Rate: 0.01\n",
      "Epoch [16973/20000], Loss: -21.28570556640625, Learning Rate: 0.01\n",
      "Epoch [16974/20000], Loss: -21.2857666015625, Learning Rate: 0.01\n",
      "Epoch [16975/20000], Loss: -21.285842895507812, Learning Rate: 0.01\n",
      "Epoch [16976/20000], Loss: -21.285858154296875, Learning Rate: 0.01\n",
      "Epoch [16977/20000], Loss: -21.285858154296875, Learning Rate: 0.01\n",
      "Epoch [16978/20000], Loss: -21.285842895507812, Learning Rate: 0.01\n",
      "Epoch [16979/20000], Loss: -21.28594970703125, Learning Rate: 0.01\n",
      "Epoch [16980/20000], Loss: -21.285980224609375, Learning Rate: 0.01\n",
      "Epoch [16981/20000], Loss: -21.286056518554688, Learning Rate: 0.01\n",
      "Epoch [16982/20000], Loss: -21.2861328125, Learning Rate: 0.01\n",
      "Epoch [16983/20000], Loss: -21.286178588867188, Learning Rate: 0.01\n",
      "Epoch [16984/20000], Loss: -21.28619384765625, Learning Rate: 0.01\n",
      "Epoch [16985/20000], Loss: -21.286270141601562, Learning Rate: 0.01\n",
      "Epoch [16986/20000], Loss: -21.286300659179688, Learning Rate: 0.01\n",
      "Epoch [16987/20000], Loss: -21.286361694335938, Learning Rate: 0.01\n",
      "Epoch [16988/20000], Loss: -21.286376953125, Learning Rate: 0.01\n",
      "Epoch [16989/20000], Loss: -21.286407470703125, Learning Rate: 0.01\n",
      "Epoch [16990/20000], Loss: -21.28643798828125, Learning Rate: 0.01\n",
      "Epoch [16991/20000], Loss: -21.286453247070312, Learning Rate: 0.01\n",
      "Epoch [16992/20000], Loss: -21.286529541015625, Learning Rate: 0.01\n",
      "Epoch [16993/20000], Loss: -21.286529541015625, Learning Rate: 0.01\n",
      "Epoch [16994/20000], Loss: -21.286605834960938, Learning Rate: 0.01\n",
      "Epoch [16995/20000], Loss: -21.286666870117188, Learning Rate: 0.01\n",
      "Epoch [16996/20000], Loss: -21.286666870117188, Learning Rate: 0.01\n",
      "Epoch [16997/20000], Loss: -21.286727905273438, Learning Rate: 0.01\n",
      "Epoch [16998/20000], Loss: -21.286727905273438, Learning Rate: 0.01\n",
      "Epoch [16999/20000], Loss: -21.286849975585938, Learning Rate: 0.01\n",
      "Epoch [17000/20000], Loss: -21.286849975585938, Learning Rate: 0.01\n",
      "Epoch [17001/20000], Loss: -21.286880493164062, Learning Rate: 0.01\n",
      "Epoch [17002/20000], Loss: -21.286834716796875, Learning Rate: 0.01\n",
      "Epoch [17003/20000], Loss: -21.286849975585938, Learning Rate: 0.01\n",
      "Epoch [17004/20000], Loss: -21.28680419921875, Learning Rate: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17005/20000], Loss: -21.286697387695312, Learning Rate: 0.01\n",
      "Epoch [17006/20000], Loss: -21.286529541015625, Learning Rate: 0.01\n",
      "Epoch [17007/20000], Loss: -21.286285400390625, Learning Rate: 0.01\n",
      "Epoch [17008/20000], Loss: -21.285720825195312, Learning Rate: 0.01\n",
      "Epoch [17009/20000], Loss: -21.284774780273438, Learning Rate: 0.01\n",
      "Epoch [17010/20000], Loss: -21.283248901367188, Learning Rate: 0.01\n",
      "Epoch [17011/20000], Loss: -21.280838012695312, Learning Rate: 0.01\n",
      "Epoch [17012/20000], Loss: -21.276626586914062, Learning Rate: 0.01\n",
      "Epoch [17013/20000], Loss: -21.26953125, Learning Rate: 0.01\n",
      "Epoch [17014/20000], Loss: -21.257537841796875, Learning Rate: 0.01\n",
      "Epoch [17015/20000], Loss: -21.2373046875, Learning Rate: 0.01\n",
      "Epoch [17016/20000], Loss: -21.202896118164062, Learning Rate: 0.01\n",
      "Epoch [17017/20000], Loss: -21.14398193359375, Learning Rate: 0.01\n",
      "Epoch [17018/20000], Loss: -21.04498291015625, Learning Rate: 0.01\n",
      "Epoch [17019/20000], Loss: -20.876541137695312, Learning Rate: 0.01\n",
      "Epoch [17020/20000], Loss: -20.603408813476562, Learning Rate: 0.01\n",
      "Epoch [17021/20000], Loss: -20.160598754882812, Learning Rate: 0.01\n",
      "Epoch [17022/20000], Loss: -19.52728271484375, Learning Rate: 0.01\n",
      "Epoch [17023/20000], Loss: -18.675994873046875, Learning Rate: 0.01\n",
      "Epoch [17024/20000], Loss: -17.9080810546875, Learning Rate: 0.01\n",
      "Epoch [17025/20000], Loss: -17.54248046875, Learning Rate: 0.01\n",
      "Epoch [17026/20000], Loss: -18.239410400390625, Learning Rate: 0.01\n",
      "Epoch [17027/20000], Loss: -19.669204711914062, Learning Rate: 0.01\n",
      "Epoch [17028/20000], Loss: -20.975494384765625, Learning Rate: 0.01\n",
      "Epoch [17029/20000], Loss: -21.21868896484375, Learning Rate: 0.01\n",
      "Epoch [17030/20000], Loss: -20.503341674804688, Learning Rate: 0.01\n",
      "Epoch [17031/20000], Loss: -19.760162353515625, Learning Rate: 0.01\n",
      "Epoch [17032/20000], Loss: -19.772674560546875, Learning Rate: 0.01\n",
      "Epoch [17033/20000], Loss: -20.548904418945312, Learning Rate: 0.01\n",
      "Epoch [17034/20000], Loss: -21.212234497070312, Learning Rate: 0.01\n",
      "Epoch [17035/20000], Loss: -21.156753540039062, Learning Rate: 0.01\n",
      "Epoch [17036/20000], Loss: -20.650421142578125, Learning Rate: 0.01\n",
      "Epoch [17037/20000], Loss: -20.396286010742188, Learning Rate: 0.01\n",
      "Epoch [17038/20000], Loss: -20.67919921875, Learning Rate: 0.01\n",
      "Epoch [17039/20000], Loss: -21.110015869140625, Learning Rate: 0.01\n",
      "Epoch [17040/20000], Loss: -21.287094116210938, Learning Rate: 0.01\n",
      "Epoch [17041/20000], Loss: -21.110580444335938, Learning Rate: 0.01\n",
      "Epoch [17042/20000], Loss: -20.849868774414062, Learning Rate: 0.01\n",
      "Epoch [17043/20000], Loss: -20.800537109375, Learning Rate: 0.01\n",
      "Epoch [17044/20000], Loss: -21.002395629882812, Learning Rate: 0.01\n",
      "Epoch [17045/20000], Loss: -21.243087768554688, Learning Rate: 0.01\n",
      "Epoch [17046/20000], Loss: -21.252761840820312, Learning Rate: 0.01\n",
      "Epoch [17047/20000], Loss: -21.08282470703125, Learning Rate: 0.01\n",
      "Epoch [17048/20000], Loss: -20.981796264648438, Learning Rate: 0.01\n",
      "Epoch [17049/20000], Loss: -21.067886352539062, Learning Rate: 0.01\n",
      "Epoch [17050/20000], Loss: -21.214950561523438, Learning Rate: 0.01\n",
      "Epoch [17051/20000], Loss: -21.251190185546875, Learning Rate: 0.01\n",
      "Epoch [17052/20000], Loss: -21.161285400390625, Learning Rate: 0.01\n",
      "Epoch [17053/20000], Loss: -21.075897216796875, Learning Rate: 0.01\n",
      "Epoch [17054/20000], Loss: -21.094818115234375, Learning Rate: 0.01\n",
      "Epoch [17055/20000], Loss: -21.187713623046875, Learning Rate: 0.01\n",
      "Epoch [17056/20000], Loss: -21.2313232421875, Learning Rate: 0.01\n",
      "Epoch [17057/20000], Loss: -21.18768310546875, Learning Rate: 0.01\n",
      "Epoch [17058/20000], Loss: -21.132522583007812, Learning Rate: 0.01\n",
      "Epoch [17059/20000], Loss: -21.141464233398438, Learning Rate: 0.01\n",
      "Epoch [17060/20000], Loss: -21.203048706054688, Learning Rate: 0.01\n",
      "Epoch [17061/20000], Loss: -21.241165161132812, Learning Rate: 0.01\n",
      "Epoch [17062/20000], Loss: -21.225601196289062, Learning Rate: 0.01\n",
      "Epoch [17063/20000], Loss: -21.199752807617188, Learning Rate: 0.01\n",
      "Epoch [17064/20000], Loss: -21.211593627929688, Learning Rate: 0.01\n",
      "Epoch [17065/20000], Loss: -21.254653930664062, Learning Rate: 0.01\n",
      "Epoch [17066/20000], Loss: -21.283233642578125, Learning Rate: 0.01\n",
      "Epoch [17067/20000], Loss: -21.275955200195312, Learning Rate: 0.01\n",
      "Epoch [17068/20000], Loss: -21.255081176757812, Learning Rate: 0.01\n",
      "Epoch [17069/20000], Loss: -21.252166748046875, Learning Rate: 0.01\n",
      "Epoch [17070/20000], Loss: -21.2701416015625, Learning Rate: 0.01\n",
      "Epoch [17071/20000], Loss: -21.285354614257812, Learning Rate: 0.01\n",
      "Epoch [17072/20000], Loss: -21.28076171875, Learning Rate: 0.01\n",
      "Epoch [17073/20000], Loss: -21.264556884765625, Learning Rate: 0.01\n",
      "Epoch [17074/20000], Loss: -21.255172729492188, Learning Rate: 0.01\n",
      "Epoch [17075/20000], Loss: -21.259475708007812, Learning Rate: 0.01\n",
      "Epoch [17076/20000], Loss: -21.265228271484375, Learning Rate: 0.01\n",
      "Epoch [17077/20000], Loss: -21.260025024414062, Learning Rate: 0.01\n",
      "Epoch [17078/20000], Loss: -21.244293212890625, Learning Rate: 0.01\n",
      "Epoch [17079/20000], Loss: -21.228622436523438, Learning Rate: 0.01\n",
      "Epoch [17080/20000], Loss: -21.221298217773438, Learning Rate: 0.01\n",
      "Epoch [17081/20000], Loss: -21.219146728515625, Learning Rate: 0.01\n",
      "Epoch [17082/20000], Loss: -21.216583251953125, Learning Rate: 0.01\n",
      "Epoch [17083/20000], Loss: -21.212127685546875, Learning Rate: 0.01\n",
      "Epoch [17084/20000], Loss: -21.211898803710938, Learning Rate: 0.01\n",
      "Epoch [17085/20000], Loss: -21.220382690429688, Learning Rate: 0.01\n",
      "Epoch [17086/20000], Loss: -21.235198974609375, Learning Rate: 0.01\n",
      "Epoch [17087/20000], Loss: -21.250076293945312, Learning Rate: 0.01\n",
      "Epoch [17088/20000], Loss: -21.260879516601562, Learning Rate: 0.01\n",
      "Epoch [17089/20000], Loss: -21.268890380859375, Learning Rate: 0.01\n",
      "Epoch [17090/20000], Loss: -21.276443481445312, Learning Rate: 0.01\n",
      "Epoch [17091/20000], Loss: -21.28387451171875, Learning Rate: 0.01\n",
      "Epoch [17092/20000], Loss: -21.288925170898438, Learning Rate: 0.01\n",
      "Epoch [17093/20000], Loss: -21.289962768554688, Learning Rate: 0.01\n",
      "Epoch [17094/20000], Loss: -21.288009643554688, Learning Rate: 0.01\n",
      "Epoch [17095/20000], Loss: -21.285491943359375, Learning Rate: 0.01\n",
      "Epoch [17096/20000], Loss: -21.283966064453125, Learning Rate: 0.01\n",
      "Epoch [17097/20000], Loss: -21.282501220703125, Learning Rate: 0.01\n",
      "Epoch [17098/20000], Loss: -21.280044555664062, Learning Rate: 0.01\n",
      "Epoch [17099/20000], Loss: -21.276504516601562, Learning Rate: 0.01\n",
      "Epoch [17100/20000], Loss: -21.273025512695312, Learning Rate: 0.01\n",
      "Epoch [17101/20000], Loss: -21.270278930664062, Learning Rate: 0.01\n",
      "Epoch [17102/20000], Loss: -21.268447875976562, Learning Rate: 0.01\n",
      "Epoch [17103/20000], Loss: -21.266738891601562, Learning Rate: 0.01\n",
      "Epoch [17104/20000], Loss: -21.264739990234375, Learning Rate: 0.01\n",
      "Epoch [17105/20000], Loss: -21.262664794921875, Learning Rate: 0.01\n",
      "Epoch [17106/20000], Loss: -21.261154174804688, Learning Rate: 0.01\n",
      "Epoch [17107/20000], Loss: -21.260665893554688, Learning Rate: 0.01\n",
      "Epoch [17108/20000], Loss: -21.260696411132812, Learning Rate: 0.01\n",
      "Epoch [17109/20000], Loss: -21.261123657226562, Learning Rate: 0.01\n",
      "Epoch [17110/20000], Loss: -21.261627197265625, Learning Rate: 0.01\n",
      "Epoch [17111/20000], Loss: -21.262451171875, Learning Rate: 0.01\n",
      "Epoch [17112/20000], Loss: -21.263809204101562, Learning Rate: 0.01\n",
      "Epoch [17113/20000], Loss: -21.26556396484375, Learning Rate: 0.01\n",
      "Epoch [17114/20000], Loss: -21.267486572265625, Learning Rate: 0.01\n",
      "Epoch [17115/20000], Loss: -21.269363403320312, Learning Rate: 0.01\n",
      "Epoch [17116/20000], Loss: -21.271087646484375, Learning Rate: 0.01\n",
      "Epoch [17117/20000], Loss: -21.272674560546875, Learning Rate: 0.01\n",
      "Epoch [17118/20000], Loss: -21.274246215820312, Learning Rate: 0.01\n",
      "Epoch [17119/20000], Loss: -21.275802612304688, Learning Rate: 0.01\n",
      "Epoch [17120/20000], Loss: -21.277145385742188, Learning Rate: 0.01\n",
      "Epoch [17121/20000], Loss: -21.2781982421875, Learning Rate: 0.01\n",
      "Epoch [17122/20000], Loss: -21.278915405273438, Learning Rate: 0.01\n",
      "Epoch [17123/20000], Loss: -21.279510498046875, Learning Rate: 0.01\n",
      "Epoch [17124/20000], Loss: -21.280044555664062, Learning Rate: 0.01\n",
      "Epoch [17125/20000], Loss: -21.280349731445312, Learning Rate: 0.01\n",
      "Epoch [17126/20000], Loss: -21.280426025390625, Learning Rate: 0.01\n",
      "Epoch [17127/20000], Loss: -21.280288696289062, Learning Rate: 0.01\n",
      "Epoch [17128/20000], Loss: -21.27978515625, Learning Rate: 0.01\n",
      "Epoch [17129/20000], Loss: -21.279159545898438, Learning Rate: 0.01\n",
      "Epoch [17130/20000], Loss: -21.27813720703125, Learning Rate: 0.01\n",
      "Epoch [17131/20000], Loss: -21.276763916015625, Learning Rate: 0.01\n",
      "Epoch [17132/20000], Loss: -21.27496337890625, Learning Rate: 0.01\n",
      "Epoch [17133/20000], Loss: -21.272567749023438, Learning Rate: 0.01\n",
      "Epoch [17134/20000], Loss: -21.269485473632812, Learning Rate: 0.01\n",
      "Epoch [17135/20000], Loss: -21.265396118164062, Learning Rate: 0.01\n",
      "Epoch [17136/20000], Loss: -21.260406494140625, Learning Rate: 0.01\n",
      "Epoch [17137/20000], Loss: -21.253875732421875, Learning Rate: 0.01\n",
      "Epoch [17138/20000], Loss: -21.245529174804688, Learning Rate: 0.01\n",
      "Epoch [17139/20000], Loss: -21.234909057617188, Learning Rate: 0.01\n",
      "Epoch [17140/20000], Loss: -21.221527099609375, Learning Rate: 0.01\n",
      "Epoch [17141/20000], Loss: -21.204742431640625, Learning Rate: 0.01\n",
      "Epoch [17142/20000], Loss: -21.183975219726562, Learning Rate: 0.01\n",
      "Epoch [17143/20000], Loss: -21.158615112304688, Learning Rate: 0.01\n",
      "Epoch [17144/20000], Loss: -21.128860473632812, Learning Rate: 0.01\n",
      "Epoch [17145/20000], Loss: -21.094955444335938, Learning Rate: 0.01\n",
      "Epoch [17146/20000], Loss: -21.291854858398438, Learning Rate: 0.005\n",
      "Epoch [17147/20000], Loss: -21.13226318359375, Learning Rate: 0.005\n",
      "Epoch [17148/20000], Loss: -21.29193115234375, Learning Rate: 0.005\n",
      "Epoch [17149/20000], Loss: -21.162551879882812, Learning Rate: 0.005\n",
      "Epoch [17150/20000], Loss: -21.292037963867188, Learning Rate: 0.005\n",
      "Epoch [17151/20000], Loss: -21.187210083007812, Learning Rate: 0.005\n",
      "Epoch [17152/20000], Loss: -21.292037963867188, Learning Rate: 0.005\n",
      "Epoch [17153/20000], Loss: -21.207183837890625, Learning Rate: 0.005\n",
      "Epoch [17154/20000], Loss: -21.292160034179688, Learning Rate: 0.005\n",
      "Epoch [17155/20000], Loss: -21.223358154296875, Learning Rate: 0.005\n",
      "Epoch [17156/20000], Loss: -21.292221069335938, Learning Rate: 0.005\n",
      "Epoch [17157/20000], Loss: -21.236434936523438, Learning Rate: 0.005\n",
      "Epoch [17158/20000], Loss: -21.292236328125, Learning Rate: 0.005\n",
      "Epoch [17159/20000], Loss: -21.247085571289062, Learning Rate: 0.005\n",
      "Epoch [17160/20000], Loss: -21.292343139648438, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17161/20000], Loss: -21.255752563476562, Learning Rate: 0.005\n",
      "Epoch [17162/20000], Loss: -21.2923583984375, Learning Rate: 0.005\n",
      "Epoch [17163/20000], Loss: -21.262710571289062, Learning Rate: 0.005\n",
      "Epoch [17164/20000], Loss: -21.292404174804688, Learning Rate: 0.005\n",
      "Epoch [17165/20000], Loss: -21.268417358398438, Learning Rate: 0.005\n",
      "Epoch [17166/20000], Loss: -21.292465209960938, Learning Rate: 0.005\n",
      "Epoch [17167/20000], Loss: -21.2730712890625, Learning Rate: 0.005\n",
      "Epoch [17168/20000], Loss: -21.292465209960938, Learning Rate: 0.005\n",
      "Epoch [17169/20000], Loss: -21.276840209960938, Learning Rate: 0.005\n",
      "Epoch [17170/20000], Loss: -21.29254150390625, Learning Rate: 0.005\n",
      "Epoch [17171/20000], Loss: -21.279922485351562, Learning Rate: 0.005\n",
      "Epoch [17172/20000], Loss: -21.2926025390625, Learning Rate: 0.005\n",
      "Epoch [17173/20000], Loss: -21.2823486328125, Learning Rate: 0.005\n",
      "Epoch [17174/20000], Loss: -21.292617797851562, Learning Rate: 0.005\n",
      "Epoch [17175/20000], Loss: -21.284378051757812, Learning Rate: 0.005\n",
      "Epoch [17176/20000], Loss: -21.29266357421875, Learning Rate: 0.005\n",
      "Epoch [17177/20000], Loss: -21.286056518554688, Learning Rate: 0.005\n",
      "Epoch [17178/20000], Loss: -21.292648315429688, Learning Rate: 0.005\n",
      "Epoch [17179/20000], Loss: -21.287368774414062, Learning Rate: 0.005\n",
      "Epoch [17180/20000], Loss: -21.292755126953125, Learning Rate: 0.005\n",
      "Epoch [17181/20000], Loss: -21.288482666015625, Learning Rate: 0.005\n",
      "Epoch [17182/20000], Loss: -21.292770385742188, Learning Rate: 0.005\n",
      "Epoch [17183/20000], Loss: -21.289413452148438, Learning Rate: 0.005\n",
      "Epoch [17184/20000], Loss: -21.292816162109375, Learning Rate: 0.005\n",
      "Epoch [17185/20000], Loss: -21.290115356445312, Learning Rate: 0.005\n",
      "Epoch [17186/20000], Loss: -21.2928466796875, Learning Rate: 0.005\n",
      "Epoch [17187/20000], Loss: -21.290740966796875, Learning Rate: 0.005\n",
      "Epoch [17188/20000], Loss: -21.292831420898438, Learning Rate: 0.005\n",
      "Epoch [17189/20000], Loss: -21.29119873046875, Learning Rate: 0.005\n",
      "Epoch [17190/20000], Loss: -21.29290771484375, Learning Rate: 0.005\n",
      "Epoch [17191/20000], Loss: -21.291580200195312, Learning Rate: 0.005\n",
      "Epoch [17192/20000], Loss: -21.292938232421875, Learning Rate: 0.005\n",
      "Epoch [17193/20000], Loss: -21.291961669921875, Learning Rate: 0.005\n",
      "Epoch [17194/20000], Loss: -21.292999267578125, Learning Rate: 0.005\n",
      "Epoch [17195/20000], Loss: -21.292205810546875, Learning Rate: 0.005\n",
      "Epoch [17196/20000], Loss: -21.293045043945312, Learning Rate: 0.005\n",
      "Epoch [17197/20000], Loss: -21.292449951171875, Learning Rate: 0.005\n",
      "Epoch [17198/20000], Loss: -21.2930908203125, Learning Rate: 0.005\n",
      "Epoch [17199/20000], Loss: -21.292694091796875, Learning Rate: 0.005\n",
      "Epoch [17200/20000], Loss: -21.293121337890625, Learning Rate: 0.005\n",
      "Epoch [17201/20000], Loss: -21.292877197265625, Learning Rate: 0.005\n",
      "Epoch [17202/20000], Loss: -21.293136596679688, Learning Rate: 0.005\n",
      "Epoch [17203/20000], Loss: -21.29296875, Learning Rate: 0.005\n",
      "Epoch [17204/20000], Loss: -21.293182373046875, Learning Rate: 0.005\n",
      "Epoch [17205/20000], Loss: -21.293121337890625, Learning Rate: 0.005\n",
      "Epoch [17206/20000], Loss: -21.29327392578125, Learning Rate: 0.005\n",
      "Epoch [17207/20000], Loss: -21.29315185546875, Learning Rate: 0.005\n",
      "Epoch [17208/20000], Loss: -21.29327392578125, Learning Rate: 0.005\n",
      "Epoch [17209/20000], Loss: -21.293319702148438, Learning Rate: 0.005\n",
      "Epoch [17210/20000], Loss: -21.293365478515625, Learning Rate: 0.005\n",
      "Epoch [17211/20000], Loss: -21.293304443359375, Learning Rate: 0.005\n",
      "Epoch [17212/20000], Loss: -21.293441772460938, Learning Rate: 0.005\n",
      "Epoch [17213/20000], Loss: -21.293365478515625, Learning Rate: 0.005\n",
      "Epoch [17214/20000], Loss: -21.293411254882812, Learning Rate: 0.005\n",
      "Epoch [17215/20000], Loss: -21.293411254882812, Learning Rate: 0.005\n",
      "Epoch [17216/20000], Loss: -21.293472290039062, Learning Rate: 0.005\n",
      "Epoch [17217/20000], Loss: -21.293594360351562, Learning Rate: 0.005\n",
      "Epoch [17218/20000], Loss: -21.293548583984375, Learning Rate: 0.005\n",
      "Epoch [17219/20000], Loss: -21.293609619140625, Learning Rate: 0.005\n",
      "Epoch [17220/20000], Loss: -21.293563842773438, Learning Rate: 0.005\n",
      "Epoch [17221/20000], Loss: -21.29364013671875, Learning Rate: 0.005\n",
      "Epoch [17222/20000], Loss: -21.293624877929688, Learning Rate: 0.005\n",
      "Epoch [17223/20000], Loss: -21.293716430664062, Learning Rate: 0.005\n",
      "Epoch [17224/20000], Loss: -21.293685913085938, Learning Rate: 0.005\n",
      "Epoch [17225/20000], Loss: -21.29376220703125, Learning Rate: 0.005\n",
      "Epoch [17226/20000], Loss: -21.293731689453125, Learning Rate: 0.005\n",
      "Epoch [17227/20000], Loss: -21.293746948242188, Learning Rate: 0.005\n",
      "Epoch [17228/20000], Loss: -21.29376220703125, Learning Rate: 0.005\n",
      "Epoch [17229/20000], Loss: -21.293838500976562, Learning Rate: 0.005\n",
      "Epoch [17230/20000], Loss: -21.2938232421875, Learning Rate: 0.005\n",
      "Epoch [17231/20000], Loss: -21.293869018554688, Learning Rate: 0.005\n",
      "Epoch [17232/20000], Loss: -21.2938232421875, Learning Rate: 0.005\n",
      "Epoch [17233/20000], Loss: -21.293899536132812, Learning Rate: 0.005\n",
      "Epoch [17234/20000], Loss: -21.2939453125, Learning Rate: 0.005\n",
      "Epoch [17235/20000], Loss: -21.293930053710938, Learning Rate: 0.005\n",
      "Epoch [17236/20000], Loss: -21.29388427734375, Learning Rate: 0.005\n",
      "Epoch [17237/20000], Loss: -21.293975830078125, Learning Rate: 0.005\n",
      "Epoch [17238/20000], Loss: -21.29400634765625, Learning Rate: 0.005\n",
      "Epoch [17239/20000], Loss: -21.294052124023438, Learning Rate: 0.005\n",
      "Epoch [17240/20000], Loss: -21.294052124023438, Learning Rate: 0.005\n",
      "Epoch [17241/20000], Loss: -21.294052124023438, Learning Rate: 0.005\n",
      "Epoch [17242/20000], Loss: -21.294113159179688, Learning Rate: 0.005\n",
      "Epoch [17243/20000], Loss: -21.294143676757812, Learning Rate: 0.005\n",
      "Epoch [17244/20000], Loss: -21.29412841796875, Learning Rate: 0.005\n",
      "Epoch [17245/20000], Loss: -21.29412841796875, Learning Rate: 0.005\n",
      "Epoch [17246/20000], Loss: -21.294082641601562, Learning Rate: 0.005\n",
      "Epoch [17247/20000], Loss: -21.294174194335938, Learning Rate: 0.005\n",
      "Epoch [17248/20000], Loss: -21.294189453125, Learning Rate: 0.005\n",
      "Epoch [17249/20000], Loss: -21.294204711914062, Learning Rate: 0.005\n",
      "Epoch [17250/20000], Loss: -21.294158935546875, Learning Rate: 0.005\n",
      "Epoch [17251/20000], Loss: -21.294158935546875, Learning Rate: 0.005\n",
      "Epoch [17252/20000], Loss: -21.29425048828125, Learning Rate: 0.005\n",
      "Epoch [17253/20000], Loss: -21.29425048828125, Learning Rate: 0.005\n",
      "Epoch [17254/20000], Loss: -21.294296264648438, Learning Rate: 0.005\n",
      "Epoch [17255/20000], Loss: -21.294326782226562, Learning Rate: 0.005\n",
      "Epoch [17256/20000], Loss: -21.294357299804688, Learning Rate: 0.005\n",
      "Epoch [17257/20000], Loss: -21.294357299804688, Learning Rate: 0.005\n",
      "Epoch [17258/20000], Loss: -21.294326782226562, Learning Rate: 0.005\n",
      "Epoch [17259/20000], Loss: -21.294387817382812, Learning Rate: 0.005\n",
      "Epoch [17260/20000], Loss: -21.294418334960938, Learning Rate: 0.005\n",
      "Epoch [17261/20000], Loss: -21.294418334960938, Learning Rate: 0.005\n",
      "Epoch [17262/20000], Loss: -21.29443359375, Learning Rate: 0.005\n",
      "Epoch [17263/20000], Loss: -21.294479370117188, Learning Rate: 0.005\n",
      "Epoch [17264/20000], Loss: -21.294464111328125, Learning Rate: 0.005\n",
      "Epoch [17265/20000], Loss: -21.294479370117188, Learning Rate: 0.005\n",
      "Epoch [17266/20000], Loss: -21.2945556640625, Learning Rate: 0.005\n",
      "Epoch [17267/20000], Loss: -21.294540405273438, Learning Rate: 0.005\n",
      "Epoch [17268/20000], Loss: -21.294540405273438, Learning Rate: 0.005\n",
      "Epoch [17269/20000], Loss: -21.294540405273438, Learning Rate: 0.005\n",
      "Epoch [17270/20000], Loss: -21.294601440429688, Learning Rate: 0.005\n",
      "Epoch [17271/20000], Loss: -21.2945556640625, Learning Rate: 0.005\n",
      "Epoch [17272/20000], Loss: -21.294662475585938, Learning Rate: 0.005\n",
      "Epoch [17273/20000], Loss: -21.294692993164062, Learning Rate: 0.005\n",
      "Epoch [17274/20000], Loss: -21.294647216796875, Learning Rate: 0.005\n",
      "Epoch [17275/20000], Loss: -21.294677734375, Learning Rate: 0.005\n",
      "Epoch [17276/20000], Loss: -21.294677734375, Learning Rate: 0.005\n",
      "Epoch [17277/20000], Loss: -21.294708251953125, Learning Rate: 0.005\n",
      "Epoch [17278/20000], Loss: -21.294784545898438, Learning Rate: 0.005\n",
      "Epoch [17279/20000], Loss: -21.294784545898438, Learning Rate: 0.005\n",
      "Epoch [17280/20000], Loss: -21.2947998046875, Learning Rate: 0.005\n",
      "Epoch [17281/20000], Loss: -21.294754028320312, Learning Rate: 0.005\n",
      "Epoch [17282/20000], Loss: -21.294845581054688, Learning Rate: 0.005\n",
      "Epoch [17283/20000], Loss: -21.29486083984375, Learning Rate: 0.005\n",
      "Epoch [17284/20000], Loss: -21.294906616210938, Learning Rate: 0.005\n",
      "Epoch [17285/20000], Loss: -21.294769287109375, Learning Rate: 0.005\n",
      "Epoch [17286/20000], Loss: -21.29486083984375, Learning Rate: 0.005\n",
      "Epoch [17287/20000], Loss: -21.294952392578125, Learning Rate: 0.005\n",
      "Epoch [17288/20000], Loss: -21.294952392578125, Learning Rate: 0.005\n",
      "Epoch [17289/20000], Loss: -21.294937133789062, Learning Rate: 0.005\n",
      "Epoch [17290/20000], Loss: -21.294998168945312, Learning Rate: 0.005\n",
      "Epoch [17291/20000], Loss: -21.294967651367188, Learning Rate: 0.005\n",
      "Epoch [17292/20000], Loss: -21.2950439453125, Learning Rate: 0.005\n",
      "Epoch [17293/20000], Loss: -21.2950439453125, Learning Rate: 0.005\n",
      "Epoch [17294/20000], Loss: -21.295074462890625, Learning Rate: 0.005\n",
      "Epoch [17295/20000], Loss: -21.29510498046875, Learning Rate: 0.005\n",
      "Epoch [17296/20000], Loss: -21.29510498046875, Learning Rate: 0.005\n",
      "Epoch [17297/20000], Loss: -21.295120239257812, Learning Rate: 0.005\n",
      "Epoch [17298/20000], Loss: -21.295181274414062, Learning Rate: 0.005\n",
      "Epoch [17299/20000], Loss: -21.295135498046875, Learning Rate: 0.005\n",
      "Epoch [17300/20000], Loss: -21.295211791992188, Learning Rate: 0.005\n",
      "Epoch [17301/20000], Loss: -21.29522705078125, Learning Rate: 0.005\n",
      "Epoch [17302/20000], Loss: -21.29522705078125, Learning Rate: 0.005\n",
      "Epoch [17303/20000], Loss: -21.295272827148438, Learning Rate: 0.005\n",
      "Epoch [17304/20000], Loss: -21.29522705078125, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17305/20000], Loss: -21.295257568359375, Learning Rate: 0.005\n",
      "Epoch [17306/20000], Loss: -21.295257568359375, Learning Rate: 0.005\n",
      "Epoch [17307/20000], Loss: -21.295272827148438, Learning Rate: 0.005\n",
      "Epoch [17308/20000], Loss: -21.29534912109375, Learning Rate: 0.005\n",
      "Epoch [17309/20000], Loss: -21.295364379882812, Learning Rate: 0.005\n",
      "Epoch [17310/20000], Loss: -21.295379638671875, Learning Rate: 0.005\n",
      "Epoch [17311/20000], Loss: -21.295394897460938, Learning Rate: 0.005\n",
      "Epoch [17312/20000], Loss: -21.29541015625, Learning Rate: 0.005\n",
      "Epoch [17313/20000], Loss: -21.295425415039062, Learning Rate: 0.005\n",
      "Epoch [17314/20000], Loss: -21.29547119140625, Learning Rate: 0.005\n",
      "Epoch [17315/20000], Loss: -21.295486450195312, Learning Rate: 0.005\n",
      "Epoch [17316/20000], Loss: -21.295486450195312, Learning Rate: 0.005\n",
      "Epoch [17317/20000], Loss: -21.295486450195312, Learning Rate: 0.005\n",
      "Epoch [17318/20000], Loss: -21.295578002929688, Learning Rate: 0.005\n",
      "Epoch [17319/20000], Loss: -21.2955322265625, Learning Rate: 0.005\n",
      "Epoch [17320/20000], Loss: -21.2955322265625, Learning Rate: 0.005\n",
      "Epoch [17321/20000], Loss: -21.295578002929688, Learning Rate: 0.005\n",
      "Epoch [17322/20000], Loss: -21.29559326171875, Learning Rate: 0.005\n",
      "Epoch [17323/20000], Loss: -21.295623779296875, Learning Rate: 0.005\n",
      "Epoch [17324/20000], Loss: -21.295623779296875, Learning Rate: 0.005\n",
      "Epoch [17325/20000], Loss: -21.295684814453125, Learning Rate: 0.005\n",
      "Epoch [17326/20000], Loss: -21.295700073242188, Learning Rate: 0.005\n",
      "Epoch [17327/20000], Loss: -21.295684814453125, Learning Rate: 0.005\n",
      "Epoch [17328/20000], Loss: -21.295700073242188, Learning Rate: 0.005\n",
      "Epoch [17329/20000], Loss: -21.295730590820312, Learning Rate: 0.005\n",
      "Epoch [17330/20000], Loss: -21.29571533203125, Learning Rate: 0.005\n",
      "Epoch [17331/20000], Loss: -21.295745849609375, Learning Rate: 0.005\n",
      "Epoch [17332/20000], Loss: -21.295822143554688, Learning Rate: 0.005\n",
      "Epoch [17333/20000], Loss: -21.295852661132812, Learning Rate: 0.005\n",
      "Epoch [17334/20000], Loss: -21.295867919921875, Learning Rate: 0.005\n",
      "Epoch [17335/20000], Loss: -21.295867919921875, Learning Rate: 0.005\n",
      "Epoch [17336/20000], Loss: -21.2958984375, Learning Rate: 0.005\n",
      "Epoch [17337/20000], Loss: -21.295928955078125, Learning Rate: 0.005\n",
      "Epoch [17338/20000], Loss: -21.295852661132812, Learning Rate: 0.005\n",
      "Epoch [17339/20000], Loss: -21.295928955078125, Learning Rate: 0.005\n",
      "Epoch [17340/20000], Loss: -21.295913696289062, Learning Rate: 0.005\n",
      "Epoch [17341/20000], Loss: -21.295928955078125, Learning Rate: 0.005\n",
      "Epoch [17342/20000], Loss: -21.2960205078125, Learning Rate: 0.005\n",
      "Epoch [17343/20000], Loss: -21.29608154296875, Learning Rate: 0.005\n",
      "Epoch [17344/20000], Loss: -21.296112060546875, Learning Rate: 0.005\n",
      "Epoch [17345/20000], Loss: -21.296066284179688, Learning Rate: 0.005\n",
      "Epoch [17346/20000], Loss: -21.29608154296875, Learning Rate: 0.005\n",
      "Epoch [17347/20000], Loss: -21.296096801757812, Learning Rate: 0.005\n",
      "Epoch [17348/20000], Loss: -21.296127319335938, Learning Rate: 0.005\n",
      "Epoch [17349/20000], Loss: -21.296173095703125, Learning Rate: 0.005\n",
      "Epoch [17350/20000], Loss: -21.296157836914062, Learning Rate: 0.005\n",
      "Epoch [17351/20000], Loss: -21.296218872070312, Learning Rate: 0.005\n",
      "Epoch [17352/20000], Loss: -21.296218872070312, Learning Rate: 0.005\n",
      "Epoch [17353/20000], Loss: -21.296218872070312, Learning Rate: 0.005\n",
      "Epoch [17354/20000], Loss: -21.296310424804688, Learning Rate: 0.005\n",
      "Epoch [17355/20000], Loss: -21.296279907226562, Learning Rate: 0.005\n",
      "Epoch [17356/20000], Loss: -21.296295166015625, Learning Rate: 0.005\n",
      "Epoch [17357/20000], Loss: -21.296310424804688, Learning Rate: 0.005\n",
      "Epoch [17358/20000], Loss: -21.296340942382812, Learning Rate: 0.005\n",
      "Epoch [17359/20000], Loss: -21.296371459960938, Learning Rate: 0.005\n",
      "Epoch [17360/20000], Loss: -21.296356201171875, Learning Rate: 0.005\n",
      "Epoch [17361/20000], Loss: -21.296356201171875, Learning Rate: 0.005\n",
      "Epoch [17362/20000], Loss: -21.296371459960938, Learning Rate: 0.005\n",
      "Epoch [17363/20000], Loss: -21.29644775390625, Learning Rate: 0.005\n",
      "Epoch [17364/20000], Loss: -21.296401977539062, Learning Rate: 0.005\n",
      "Epoch [17365/20000], Loss: -21.296432495117188, Learning Rate: 0.005\n",
      "Epoch [17366/20000], Loss: -21.296524047851562, Learning Rate: 0.005\n",
      "Epoch [17367/20000], Loss: -21.296539306640625, Learning Rate: 0.005\n",
      "Epoch [17368/20000], Loss: -21.296493530273438, Learning Rate: 0.005\n",
      "Epoch [17369/20000], Loss: -21.296539306640625, Learning Rate: 0.005\n",
      "Epoch [17370/20000], Loss: -21.296539306640625, Learning Rate: 0.005\n",
      "Epoch [17371/20000], Loss: -21.29656982421875, Learning Rate: 0.005\n",
      "Epoch [17372/20000], Loss: -21.296615600585938, Learning Rate: 0.005\n",
      "Epoch [17373/20000], Loss: -21.296615600585938, Learning Rate: 0.005\n",
      "Epoch [17374/20000], Loss: -21.296646118164062, Learning Rate: 0.005\n",
      "Epoch [17375/20000], Loss: -21.29669189453125, Learning Rate: 0.005\n",
      "Epoch [17376/20000], Loss: -21.296676635742188, Learning Rate: 0.005\n",
      "Epoch [17377/20000], Loss: -21.296722412109375, Learning Rate: 0.005\n",
      "Epoch [17378/20000], Loss: -21.2967529296875, Learning Rate: 0.005\n",
      "Epoch [17379/20000], Loss: -21.29669189453125, Learning Rate: 0.005\n",
      "Epoch [17380/20000], Loss: -21.296783447265625, Learning Rate: 0.005\n",
      "Epoch [17381/20000], Loss: -21.2967529296875, Learning Rate: 0.005\n",
      "Epoch [17382/20000], Loss: -21.296798706054688, Learning Rate: 0.005\n",
      "Epoch [17383/20000], Loss: -21.296798706054688, Learning Rate: 0.005\n",
      "Epoch [17384/20000], Loss: -21.296875, Learning Rate: 0.005\n",
      "Epoch [17385/20000], Loss: -21.296844482421875, Learning Rate: 0.005\n",
      "Epoch [17386/20000], Loss: -21.29693603515625, Learning Rate: 0.005\n",
      "Epoch [17387/20000], Loss: -21.296905517578125, Learning Rate: 0.005\n",
      "Epoch [17388/20000], Loss: -21.296920776367188, Learning Rate: 0.005\n",
      "Epoch [17389/20000], Loss: -21.296981811523438, Learning Rate: 0.005\n",
      "Epoch [17390/20000], Loss: -21.296905517578125, Learning Rate: 0.005\n",
      "Epoch [17391/20000], Loss: -21.297027587890625, Learning Rate: 0.005\n",
      "Epoch [17392/20000], Loss: -21.296966552734375, Learning Rate: 0.005\n",
      "Epoch [17393/20000], Loss: -21.297027587890625, Learning Rate: 0.005\n",
      "Epoch [17394/20000], Loss: -21.297012329101562, Learning Rate: 0.005\n",
      "Epoch [17395/20000], Loss: -21.297119140625, Learning Rate: 0.005\n",
      "Epoch [17396/20000], Loss: -21.297088623046875, Learning Rate: 0.005\n",
      "Epoch [17397/20000], Loss: -21.297103881835938, Learning Rate: 0.005\n",
      "Epoch [17398/20000], Loss: -21.297149658203125, Learning Rate: 0.005\n",
      "Epoch [17399/20000], Loss: -21.297119140625, Learning Rate: 0.005\n",
      "Epoch [17400/20000], Loss: -21.297149658203125, Learning Rate: 0.005\n",
      "Epoch [17401/20000], Loss: -21.297149658203125, Learning Rate: 0.005\n",
      "Epoch [17402/20000], Loss: -21.297210693359375, Learning Rate: 0.005\n",
      "Epoch [17403/20000], Loss: -21.297225952148438, Learning Rate: 0.005\n",
      "Epoch [17404/20000], Loss: -21.297256469726562, Learning Rate: 0.005\n",
      "Epoch [17405/20000], Loss: -21.297271728515625, Learning Rate: 0.005\n",
      "Epoch [17406/20000], Loss: -21.297332763671875, Learning Rate: 0.005\n",
      "Epoch [17407/20000], Loss: -21.297317504882812, Learning Rate: 0.005\n",
      "Epoch [17408/20000], Loss: -21.297348022460938, Learning Rate: 0.005\n",
      "Epoch [17409/20000], Loss: -21.297332763671875, Learning Rate: 0.005\n",
      "Epoch [17410/20000], Loss: -21.29736328125, Learning Rate: 0.005\n",
      "Epoch [17411/20000], Loss: -21.29742431640625, Learning Rate: 0.005\n",
      "Epoch [17412/20000], Loss: -21.297409057617188, Learning Rate: 0.005\n",
      "Epoch [17413/20000], Loss: -21.297439575195312, Learning Rate: 0.005\n",
      "Epoch [17414/20000], Loss: -21.297393798828125, Learning Rate: 0.005\n",
      "Epoch [17415/20000], Loss: -21.29754638671875, Learning Rate: 0.005\n",
      "Epoch [17416/20000], Loss: -21.2974853515625, Learning Rate: 0.005\n",
      "Epoch [17417/20000], Loss: -21.297500610351562, Learning Rate: 0.005\n",
      "Epoch [17418/20000], Loss: -21.297500610351562, Learning Rate: 0.005\n",
      "Epoch [17419/20000], Loss: -21.297531127929688, Learning Rate: 0.005\n",
      "Epoch [17420/20000], Loss: -21.29754638671875, Learning Rate: 0.005\n",
      "Epoch [17421/20000], Loss: -21.297592163085938, Learning Rate: 0.005\n",
      "Epoch [17422/20000], Loss: -21.297576904296875, Learning Rate: 0.005\n",
      "Epoch [17423/20000], Loss: -21.297607421875, Learning Rate: 0.005\n",
      "Epoch [17424/20000], Loss: -21.297592163085938, Learning Rate: 0.005\n",
      "Epoch [17425/20000], Loss: -21.297653198242188, Learning Rate: 0.005\n",
      "Epoch [17426/20000], Loss: -21.29766845703125, Learning Rate: 0.005\n",
      "Epoch [17427/20000], Loss: -21.297698974609375, Learning Rate: 0.005\n",
      "Epoch [17428/20000], Loss: -21.297714233398438, Learning Rate: 0.005\n",
      "Epoch [17429/20000], Loss: -21.297775268554688, Learning Rate: 0.005\n",
      "Epoch [17430/20000], Loss: -21.297775268554688, Learning Rate: 0.005\n",
      "Epoch [17431/20000], Loss: -21.297714233398438, Learning Rate: 0.005\n",
      "Epoch [17432/20000], Loss: -21.297866821289062, Learning Rate: 0.005\n",
      "Epoch [17433/20000], Loss: -21.297836303710938, Learning Rate: 0.005\n",
      "Epoch [17434/20000], Loss: -21.297897338867188, Learning Rate: 0.005\n",
      "Epoch [17435/20000], Loss: -21.297866821289062, Learning Rate: 0.005\n",
      "Epoch [17436/20000], Loss: -21.297821044921875, Learning Rate: 0.005\n",
      "Epoch [17437/20000], Loss: -21.297927856445312, Learning Rate: 0.005\n",
      "Epoch [17438/20000], Loss: -21.297897338867188, Learning Rate: 0.005\n",
      "Epoch [17439/20000], Loss: -21.2979736328125, Learning Rate: 0.005\n",
      "Epoch [17440/20000], Loss: -21.297927856445312, Learning Rate: 0.005\n",
      "Epoch [17441/20000], Loss: -21.2979736328125, Learning Rate: 0.005\n",
      "Epoch [17442/20000], Loss: -21.2979736328125, Learning Rate: 0.005\n",
      "Epoch [17443/20000], Loss: -21.29803466796875, Learning Rate: 0.005\n",
      "Epoch [17444/20000], Loss: -21.298019409179688, Learning Rate: 0.005\n",
      "Epoch [17445/20000], Loss: -21.298049926757812, Learning Rate: 0.005\n",
      "Epoch [17446/20000], Loss: -21.298049926757812, Learning Rate: 0.005\n",
      "Epoch [17447/20000], Loss: -21.298080444335938, Learning Rate: 0.005\n",
      "Epoch [17448/20000], Loss: -21.298202514648438, Learning Rate: 0.005\n",
      "Epoch [17449/20000], Loss: -21.298187255859375, Learning Rate: 0.005\n",
      "Epoch [17450/20000], Loss: -21.298141479492188, Learning Rate: 0.005\n",
      "Epoch [17451/20000], Loss: -21.298187255859375, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17452/20000], Loss: -21.298187255859375, Learning Rate: 0.005\n",
      "Epoch [17453/20000], Loss: -21.298202514648438, Learning Rate: 0.005\n",
      "Epoch [17454/20000], Loss: -21.298263549804688, Learning Rate: 0.005\n",
      "Epoch [17455/20000], Loss: -21.298294067382812, Learning Rate: 0.005\n",
      "Epoch [17456/20000], Loss: -21.298294067382812, Learning Rate: 0.005\n",
      "Epoch [17457/20000], Loss: -21.298309326171875, Learning Rate: 0.005\n",
      "Epoch [17458/20000], Loss: -21.298385620117188, Learning Rate: 0.005\n",
      "Epoch [17459/20000], Loss: -21.298355102539062, Learning Rate: 0.005\n",
      "Epoch [17460/20000], Loss: -21.29840087890625, Learning Rate: 0.005\n",
      "Epoch [17461/20000], Loss: -21.298370361328125, Learning Rate: 0.005\n",
      "Epoch [17462/20000], Loss: -21.298416137695312, Learning Rate: 0.005\n",
      "Epoch [17463/20000], Loss: -21.29840087890625, Learning Rate: 0.005\n",
      "Epoch [17464/20000], Loss: -21.298477172851562, Learning Rate: 0.005\n",
      "Epoch [17465/20000], Loss: -21.298477172851562, Learning Rate: 0.005\n",
      "Epoch [17466/20000], Loss: -21.298416137695312, Learning Rate: 0.005\n",
      "Epoch [17467/20000], Loss: -21.298492431640625, Learning Rate: 0.005\n",
      "Epoch [17468/20000], Loss: -21.298599243164062, Learning Rate: 0.005\n",
      "Epoch [17469/20000], Loss: -21.298599243164062, Learning Rate: 0.005\n",
      "Epoch [17470/20000], Loss: -21.298614501953125, Learning Rate: 0.005\n",
      "Epoch [17471/20000], Loss: -21.298599243164062, Learning Rate: 0.005\n",
      "Epoch [17472/20000], Loss: -21.29864501953125, Learning Rate: 0.005\n",
      "Epoch [17473/20000], Loss: -21.298629760742188, Learning Rate: 0.005\n",
      "Epoch [17474/20000], Loss: -21.298690795898438, Learning Rate: 0.005\n",
      "Epoch [17475/20000], Loss: -21.29864501953125, Learning Rate: 0.005\n",
      "Epoch [17476/20000], Loss: -21.298690795898438, Learning Rate: 0.005\n",
      "Epoch [17477/20000], Loss: -21.29876708984375, Learning Rate: 0.005\n",
      "Epoch [17478/20000], Loss: -21.298736572265625, Learning Rate: 0.005\n",
      "Epoch [17479/20000], Loss: -21.298812866210938, Learning Rate: 0.005\n",
      "Epoch [17480/20000], Loss: -21.298782348632812, Learning Rate: 0.005\n",
      "Epoch [17481/20000], Loss: -21.298828125, Learning Rate: 0.005\n",
      "Epoch [17482/20000], Loss: -21.298858642578125, Learning Rate: 0.005\n",
      "Epoch [17483/20000], Loss: -21.298843383789062, Learning Rate: 0.005\n",
      "Epoch [17484/20000], Loss: -21.298828125, Learning Rate: 0.005\n",
      "Epoch [17485/20000], Loss: -21.298843383789062, Learning Rate: 0.005\n",
      "Epoch [17486/20000], Loss: -21.298812866210938, Learning Rate: 0.005\n",
      "Epoch [17487/20000], Loss: -21.29888916015625, Learning Rate: 0.005\n",
      "Epoch [17488/20000], Loss: -21.298980712890625, Learning Rate: 0.005\n",
      "Epoch [17489/20000], Loss: -21.2989501953125, Learning Rate: 0.005\n",
      "Epoch [17490/20000], Loss: -21.298965454101562, Learning Rate: 0.005\n",
      "Epoch [17491/20000], Loss: -21.29901123046875, Learning Rate: 0.005\n",
      "Epoch [17492/20000], Loss: -21.299026489257812, Learning Rate: 0.005\n",
      "Epoch [17493/20000], Loss: -21.299072265625, Learning Rate: 0.005\n",
      "Epoch [17494/20000], Loss: -21.299057006835938, Learning Rate: 0.005\n",
      "Epoch [17495/20000], Loss: -21.29901123046875, Learning Rate: 0.005\n",
      "Epoch [17496/20000], Loss: -21.299057006835938, Learning Rate: 0.005\n",
      "Epoch [17497/20000], Loss: -21.299163818359375, Learning Rate: 0.005\n",
      "Epoch [17498/20000], Loss: -21.299118041992188, Learning Rate: 0.005\n",
      "Epoch [17499/20000], Loss: -21.299163818359375, Learning Rate: 0.005\n",
      "Epoch [17500/20000], Loss: -21.299224853515625, Learning Rate: 0.005\n",
      "Epoch [17501/20000], Loss: -21.299163818359375, Learning Rate: 0.005\n",
      "Epoch [17502/20000], Loss: -21.2991943359375, Learning Rate: 0.005\n",
      "Epoch [17503/20000], Loss: -21.29925537109375, Learning Rate: 0.005\n",
      "Epoch [17504/20000], Loss: -21.299270629882812, Learning Rate: 0.005\n",
      "Epoch [17505/20000], Loss: -21.299346923828125, Learning Rate: 0.005\n",
      "Epoch [17506/20000], Loss: -21.299331665039062, Learning Rate: 0.005\n",
      "Epoch [17507/20000], Loss: -21.299346923828125, Learning Rate: 0.005\n",
      "Epoch [17508/20000], Loss: -21.29937744140625, Learning Rate: 0.005\n",
      "Epoch [17509/20000], Loss: -21.299392700195312, Learning Rate: 0.005\n",
      "Epoch [17510/20000], Loss: -21.299407958984375, Learning Rate: 0.005\n",
      "Epoch [17511/20000], Loss: -21.299407958984375, Learning Rate: 0.005\n",
      "Epoch [17512/20000], Loss: -21.299423217773438, Learning Rate: 0.005\n",
      "Epoch [17513/20000], Loss: -21.2994384765625, Learning Rate: 0.005\n",
      "Epoch [17514/20000], Loss: -21.299468994140625, Learning Rate: 0.005\n",
      "Epoch [17515/20000], Loss: -21.299545288085938, Learning Rate: 0.005\n",
      "Epoch [17516/20000], Loss: -21.299530029296875, Learning Rate: 0.005\n",
      "Epoch [17517/20000], Loss: -21.299530029296875, Learning Rate: 0.005\n",
      "Epoch [17518/20000], Loss: -21.299606323242188, Learning Rate: 0.005\n",
      "Epoch [17519/20000], Loss: -21.299606323242188, Learning Rate: 0.005\n",
      "Epoch [17520/20000], Loss: -21.299606323242188, Learning Rate: 0.005\n",
      "Epoch [17521/20000], Loss: -21.299636840820312, Learning Rate: 0.005\n",
      "Epoch [17522/20000], Loss: -21.299652099609375, Learning Rate: 0.005\n",
      "Epoch [17523/20000], Loss: -21.299606323242188, Learning Rate: 0.005\n",
      "Epoch [17524/20000], Loss: -21.299667358398438, Learning Rate: 0.005\n",
      "Epoch [17525/20000], Loss: -21.299667358398438, Learning Rate: 0.005\n",
      "Epoch [17526/20000], Loss: -21.299728393554688, Learning Rate: 0.005\n",
      "Epoch [17527/20000], Loss: -21.299728393554688, Learning Rate: 0.005\n",
      "Epoch [17528/20000], Loss: -21.299758911132812, Learning Rate: 0.005\n",
      "Epoch [17529/20000], Loss: -21.299789428710938, Learning Rate: 0.005\n",
      "Epoch [17530/20000], Loss: -21.299774169921875, Learning Rate: 0.005\n",
      "Epoch [17531/20000], Loss: -21.2998046875, Learning Rate: 0.005\n",
      "Epoch [17532/20000], Loss: -21.299835205078125, Learning Rate: 0.005\n",
      "Epoch [17533/20000], Loss: -21.299911499023438, Learning Rate: 0.005\n",
      "Epoch [17534/20000], Loss: -21.299880981445312, Learning Rate: 0.005\n",
      "Epoch [17535/20000], Loss: -21.299835205078125, Learning Rate: 0.005\n",
      "Epoch [17536/20000], Loss: -21.299942016601562, Learning Rate: 0.005\n",
      "Epoch [17537/20000], Loss: -21.299957275390625, Learning Rate: 0.005\n",
      "Epoch [17538/20000], Loss: -21.2999267578125, Learning Rate: 0.005\n",
      "Epoch [17539/20000], Loss: -21.300003051757812, Learning Rate: 0.005\n",
      "Epoch [17540/20000], Loss: -21.299972534179688, Learning Rate: 0.005\n",
      "Epoch [17541/20000], Loss: -21.300033569335938, Learning Rate: 0.005\n",
      "Epoch [17542/20000], Loss: -21.300018310546875, Learning Rate: 0.005\n",
      "Epoch [17543/20000], Loss: -21.300094604492188, Learning Rate: 0.005\n",
      "Epoch [17544/20000], Loss: -21.3001708984375, Learning Rate: 0.005\n",
      "Epoch [17545/20000], Loss: -21.300079345703125, Learning Rate: 0.005\n",
      "Epoch [17546/20000], Loss: -21.300155639648438, Learning Rate: 0.005\n",
      "Epoch [17547/20000], Loss: -21.30010986328125, Learning Rate: 0.005\n",
      "Epoch [17548/20000], Loss: -21.30010986328125, Learning Rate: 0.005\n",
      "Epoch [17549/20000], Loss: -21.300216674804688, Learning Rate: 0.005\n",
      "Epoch [17550/20000], Loss: -21.300216674804688, Learning Rate: 0.005\n",
      "Epoch [17551/20000], Loss: -21.30023193359375, Learning Rate: 0.005\n",
      "Epoch [17552/20000], Loss: -21.30023193359375, Learning Rate: 0.005\n",
      "Epoch [17553/20000], Loss: -21.300247192382812, Learning Rate: 0.005\n",
      "Epoch [17554/20000], Loss: -21.300338745117188, Learning Rate: 0.005\n",
      "Epoch [17555/20000], Loss: -21.300277709960938, Learning Rate: 0.005\n",
      "Epoch [17556/20000], Loss: -21.300338745117188, Learning Rate: 0.005\n",
      "Epoch [17557/20000], Loss: -21.300399780273438, Learning Rate: 0.005\n",
      "Epoch [17558/20000], Loss: -21.300430297851562, Learning Rate: 0.005\n",
      "Epoch [17559/20000], Loss: -21.300384521484375, Learning Rate: 0.005\n",
      "Epoch [17560/20000], Loss: -21.3004150390625, Learning Rate: 0.005\n",
      "Epoch [17561/20000], Loss: -21.30047607421875, Learning Rate: 0.005\n",
      "Epoch [17562/20000], Loss: -21.300445556640625, Learning Rate: 0.005\n",
      "Epoch [17563/20000], Loss: -21.30047607421875, Learning Rate: 0.005\n",
      "Epoch [17564/20000], Loss: -21.300506591796875, Learning Rate: 0.005\n",
      "Epoch [17565/20000], Loss: -21.300582885742188, Learning Rate: 0.005\n",
      "Epoch [17566/20000], Loss: -21.300537109375, Learning Rate: 0.005\n",
      "Epoch [17567/20000], Loss: -21.300567626953125, Learning Rate: 0.005\n",
      "Epoch [17568/20000], Loss: -21.300613403320312, Learning Rate: 0.005\n",
      "Epoch [17569/20000], Loss: -21.300582885742188, Learning Rate: 0.005\n",
      "Epoch [17570/20000], Loss: -21.300689697265625, Learning Rate: 0.005\n",
      "Epoch [17571/20000], Loss: -21.3006591796875, Learning Rate: 0.005\n",
      "Epoch [17572/20000], Loss: -21.30072021484375, Learning Rate: 0.005\n",
      "Epoch [17573/20000], Loss: -21.300704956054688, Learning Rate: 0.005\n",
      "Epoch [17574/20000], Loss: -21.300750732421875, Learning Rate: 0.005\n",
      "Epoch [17575/20000], Loss: -21.300796508789062, Learning Rate: 0.005\n",
      "Epoch [17576/20000], Loss: -21.300796508789062, Learning Rate: 0.005\n",
      "Epoch [17577/20000], Loss: -21.300796508789062, Learning Rate: 0.005\n",
      "Epoch [17578/20000], Loss: -21.300827026367188, Learning Rate: 0.005\n",
      "Epoch [17579/20000], Loss: -21.300796508789062, Learning Rate: 0.005\n",
      "Epoch [17580/20000], Loss: -21.30084228515625, Learning Rate: 0.005\n",
      "Epoch [17581/20000], Loss: -21.300888061523438, Learning Rate: 0.005\n",
      "Epoch [17582/20000], Loss: -21.300918579101562, Learning Rate: 0.005\n",
      "Epoch [17583/20000], Loss: -21.300888061523438, Learning Rate: 0.005\n",
      "Epoch [17584/20000], Loss: -21.300918579101562, Learning Rate: 0.005\n",
      "Epoch [17585/20000], Loss: -21.300918579101562, Learning Rate: 0.005\n",
      "Epoch [17586/20000], Loss: -21.300994873046875, Learning Rate: 0.005\n",
      "Epoch [17587/20000], Loss: -21.301040649414062, Learning Rate: 0.005\n",
      "Epoch [17588/20000], Loss: -21.301025390625, Learning Rate: 0.005\n",
      "Epoch [17589/20000], Loss: -21.301025390625, Learning Rate: 0.005\n",
      "Epoch [17590/20000], Loss: -21.30108642578125, Learning Rate: 0.005\n",
      "Epoch [17591/20000], Loss: -21.30108642578125, Learning Rate: 0.005\n",
      "Epoch [17592/20000], Loss: -21.301055908203125, Learning Rate: 0.005\n",
      "Epoch [17593/20000], Loss: -21.301177978515625, Learning Rate: 0.005\n",
      "Epoch [17594/20000], Loss: -21.301193237304688, Learning Rate: 0.005\n",
      "Epoch [17595/20000], Loss: -21.301177978515625, Learning Rate: 0.005\n",
      "Epoch [17596/20000], Loss: -21.301177978515625, Learning Rate: 0.005\n",
      "Epoch [17597/20000], Loss: -21.301300048828125, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17598/20000], Loss: -21.30126953125, Learning Rate: 0.005\n",
      "Epoch [17599/20000], Loss: -21.301300048828125, Learning Rate: 0.005\n",
      "Epoch [17600/20000], Loss: -21.301239013671875, Learning Rate: 0.005\n",
      "Epoch [17601/20000], Loss: -21.301315307617188, Learning Rate: 0.005\n",
      "Epoch [17602/20000], Loss: -21.301300048828125, Learning Rate: 0.005\n",
      "Epoch [17603/20000], Loss: -21.301376342773438, Learning Rate: 0.005\n",
      "Epoch [17604/20000], Loss: -21.301345825195312, Learning Rate: 0.005\n",
      "Epoch [17605/20000], Loss: -21.301376342773438, Learning Rate: 0.005\n",
      "Epoch [17606/20000], Loss: -21.301406860351562, Learning Rate: 0.005\n",
      "Epoch [17607/20000], Loss: -21.3013916015625, Learning Rate: 0.005\n",
      "Epoch [17608/20000], Loss: -21.301467895507812, Learning Rate: 0.005\n",
      "Epoch [17609/20000], Loss: -21.301513671875, Learning Rate: 0.005\n",
      "Epoch [17610/20000], Loss: -21.301406860351562, Learning Rate: 0.005\n",
      "Epoch [17611/20000], Loss: -21.301513671875, Learning Rate: 0.005\n",
      "Epoch [17612/20000], Loss: -21.301544189453125, Learning Rate: 0.005\n",
      "Epoch [17613/20000], Loss: -21.301544189453125, Learning Rate: 0.005\n",
      "Epoch [17614/20000], Loss: -21.301605224609375, Learning Rate: 0.005\n",
      "Epoch [17615/20000], Loss: -21.301589965820312, Learning Rate: 0.005\n",
      "Epoch [17616/20000], Loss: -21.301712036132812, Learning Rate: 0.005\n",
      "Epoch [17617/20000], Loss: -21.301651000976562, Learning Rate: 0.005\n",
      "Epoch [17618/20000], Loss: -21.301651000976562, Learning Rate: 0.005\n",
      "Epoch [17619/20000], Loss: -21.3016357421875, Learning Rate: 0.005\n",
      "Epoch [17620/20000], Loss: -21.301666259765625, Learning Rate: 0.005\n",
      "Epoch [17621/20000], Loss: -21.301712036132812, Learning Rate: 0.005\n",
      "Epoch [17622/20000], Loss: -21.301788330078125, Learning Rate: 0.005\n",
      "Epoch [17623/20000], Loss: -21.3017578125, Learning Rate: 0.005\n",
      "Epoch [17624/20000], Loss: -21.30181884765625, Learning Rate: 0.005\n",
      "Epoch [17625/20000], Loss: -21.3017578125, Learning Rate: 0.005\n",
      "Epoch [17626/20000], Loss: -21.3017578125, Learning Rate: 0.005\n",
      "Epoch [17627/20000], Loss: -21.301788330078125, Learning Rate: 0.005\n",
      "Epoch [17628/20000], Loss: -21.301834106445312, Learning Rate: 0.005\n",
      "Epoch [17629/20000], Loss: -21.301834106445312, Learning Rate: 0.005\n",
      "Epoch [17630/20000], Loss: -21.301864624023438, Learning Rate: 0.005\n",
      "Epoch [17631/20000], Loss: -21.301895141601562, Learning Rate: 0.005\n",
      "Epoch [17632/20000], Loss: -21.301925659179688, Learning Rate: 0.005\n",
      "Epoch [17633/20000], Loss: -21.301971435546875, Learning Rate: 0.005\n",
      "Epoch [17634/20000], Loss: -21.302001953125, Learning Rate: 0.005\n",
      "Epoch [17635/20000], Loss: -21.302001953125, Learning Rate: 0.005\n",
      "Epoch [17636/20000], Loss: -21.302078247070312, Learning Rate: 0.005\n",
      "Epoch [17637/20000], Loss: -21.30206298828125, Learning Rate: 0.005\n",
      "Epoch [17638/20000], Loss: -21.302078247070312, Learning Rate: 0.005\n",
      "Epoch [17639/20000], Loss: -21.30206298828125, Learning Rate: 0.005\n",
      "Epoch [17640/20000], Loss: -21.302139282226562, Learning Rate: 0.005\n",
      "Epoch [17641/20000], Loss: -21.3021240234375, Learning Rate: 0.005\n",
      "Epoch [17642/20000], Loss: -21.302169799804688, Learning Rate: 0.005\n",
      "Epoch [17643/20000], Loss: -21.302230834960938, Learning Rate: 0.005\n",
      "Epoch [17644/20000], Loss: -21.302200317382812, Learning Rate: 0.005\n",
      "Epoch [17645/20000], Loss: -21.30224609375, Learning Rate: 0.005\n",
      "Epoch [17646/20000], Loss: -21.30224609375, Learning Rate: 0.005\n",
      "Epoch [17647/20000], Loss: -21.302261352539062, Learning Rate: 0.005\n",
      "Epoch [17648/20000], Loss: -21.302276611328125, Learning Rate: 0.005\n",
      "Epoch [17649/20000], Loss: -21.302261352539062, Learning Rate: 0.005\n",
      "Epoch [17650/20000], Loss: -21.302322387695312, Learning Rate: 0.005\n",
      "Epoch [17651/20000], Loss: -21.302383422851562, Learning Rate: 0.005\n",
      "Epoch [17652/20000], Loss: -21.302398681640625, Learning Rate: 0.005\n",
      "Epoch [17653/20000], Loss: -21.3023681640625, Learning Rate: 0.005\n",
      "Epoch [17654/20000], Loss: -21.302444458007812, Learning Rate: 0.005\n",
      "Epoch [17655/20000], Loss: -21.302444458007812, Learning Rate: 0.005\n",
      "Epoch [17656/20000], Loss: -21.302474975585938, Learning Rate: 0.005\n",
      "Epoch [17657/20000], Loss: -21.302474975585938, Learning Rate: 0.005\n",
      "Epoch [17658/20000], Loss: -21.302490234375, Learning Rate: 0.005\n",
      "Epoch [17659/20000], Loss: -21.302505493164062, Learning Rate: 0.005\n",
      "Epoch [17660/20000], Loss: -21.302520751953125, Learning Rate: 0.005\n",
      "Epoch [17661/20000], Loss: -21.30255126953125, Learning Rate: 0.005\n",
      "Epoch [17662/20000], Loss: -21.30255126953125, Learning Rate: 0.005\n",
      "Epoch [17663/20000], Loss: -21.302597045898438, Learning Rate: 0.005\n",
      "Epoch [17664/20000], Loss: -21.302658081054688, Learning Rate: 0.005\n",
      "Epoch [17665/20000], Loss: -21.302719116210938, Learning Rate: 0.005\n",
      "Epoch [17666/20000], Loss: -21.302642822265625, Learning Rate: 0.005\n",
      "Epoch [17667/20000], Loss: -21.302688598632812, Learning Rate: 0.005\n",
      "Epoch [17668/20000], Loss: -21.302703857421875, Learning Rate: 0.005\n",
      "Epoch [17669/20000], Loss: -21.302734375, Learning Rate: 0.005\n",
      "Epoch [17670/20000], Loss: -21.302734375, Learning Rate: 0.005\n",
      "Epoch [17671/20000], Loss: -21.302749633789062, Learning Rate: 0.005\n",
      "Epoch [17672/20000], Loss: -21.302780151367188, Learning Rate: 0.005\n",
      "Epoch [17673/20000], Loss: -21.302841186523438, Learning Rate: 0.005\n",
      "Epoch [17674/20000], Loss: -21.302825927734375, Learning Rate: 0.005\n",
      "Epoch [17675/20000], Loss: -21.302871704101562, Learning Rate: 0.005\n",
      "Epoch [17676/20000], Loss: -21.302841186523438, Learning Rate: 0.005\n",
      "Epoch [17677/20000], Loss: -21.302871704101562, Learning Rate: 0.005\n",
      "Epoch [17678/20000], Loss: -21.302963256835938, Learning Rate: 0.005\n",
      "Epoch [17679/20000], Loss: -21.302932739257812, Learning Rate: 0.005\n",
      "Epoch [17680/20000], Loss: -21.302947998046875, Learning Rate: 0.005\n",
      "Epoch [17681/20000], Loss: -21.302993774414062, Learning Rate: 0.005\n",
      "Epoch [17682/20000], Loss: -21.30303955078125, Learning Rate: 0.005\n",
      "Epoch [17683/20000], Loss: -21.303054809570312, Learning Rate: 0.005\n",
      "Epoch [17684/20000], Loss: -21.303085327148438, Learning Rate: 0.005\n",
      "Epoch [17685/20000], Loss: -21.303070068359375, Learning Rate: 0.005\n",
      "Epoch [17686/20000], Loss: -21.303131103515625, Learning Rate: 0.005\n",
      "Epoch [17687/20000], Loss: -21.3031005859375, Learning Rate: 0.005\n",
      "Epoch [17688/20000], Loss: -21.303176879882812, Learning Rate: 0.005\n",
      "Epoch [17689/20000], Loss: -21.30322265625, Learning Rate: 0.005\n",
      "Epoch [17690/20000], Loss: -21.30322265625, Learning Rate: 0.005\n",
      "Epoch [17691/20000], Loss: -21.303192138671875, Learning Rate: 0.005\n",
      "Epoch [17692/20000], Loss: -21.30316162109375, Learning Rate: 0.005\n",
      "Epoch [17693/20000], Loss: -21.303207397460938, Learning Rate: 0.005\n",
      "Epoch [17694/20000], Loss: -21.303268432617188, Learning Rate: 0.005\n",
      "Epoch [17695/20000], Loss: -21.3033447265625, Learning Rate: 0.005\n",
      "Epoch [17696/20000], Loss: -21.3033447265625, Learning Rate: 0.005\n",
      "Epoch [17697/20000], Loss: -21.303329467773438, Learning Rate: 0.005\n",
      "Epoch [17698/20000], Loss: -21.303329467773438, Learning Rate: 0.005\n",
      "Epoch [17699/20000], Loss: -21.303329467773438, Learning Rate: 0.005\n",
      "Epoch [17700/20000], Loss: -21.303451538085938, Learning Rate: 0.005\n",
      "Epoch [17701/20000], Loss: -21.303436279296875, Learning Rate: 0.005\n",
      "Epoch [17702/20000], Loss: -21.303436279296875, Learning Rate: 0.005\n",
      "Epoch [17703/20000], Loss: -21.303482055664062, Learning Rate: 0.005\n",
      "Epoch [17704/20000], Loss: -21.303543090820312, Learning Rate: 0.005\n",
      "Epoch [17705/20000], Loss: -21.303543090820312, Learning Rate: 0.005\n",
      "Epoch [17706/20000], Loss: -21.303512573242188, Learning Rate: 0.005\n",
      "Epoch [17707/20000], Loss: -21.303573608398438, Learning Rate: 0.005\n",
      "Epoch [17708/20000], Loss: -21.3035888671875, Learning Rate: 0.005\n",
      "Epoch [17709/20000], Loss: -21.303573608398438, Learning Rate: 0.005\n",
      "Epoch [17710/20000], Loss: -21.303634643554688, Learning Rate: 0.005\n",
      "Epoch [17711/20000], Loss: -21.30364990234375, Learning Rate: 0.005\n",
      "Epoch [17712/20000], Loss: -21.303695678710938, Learning Rate: 0.005\n",
      "Epoch [17713/20000], Loss: -21.303665161132812, Learning Rate: 0.005\n",
      "Epoch [17714/20000], Loss: -21.303726196289062, Learning Rate: 0.005\n",
      "Epoch [17715/20000], Loss: -21.3037109375, Learning Rate: 0.005\n",
      "Epoch [17716/20000], Loss: -21.30377197265625, Learning Rate: 0.005\n",
      "Epoch [17717/20000], Loss: -21.30377197265625, Learning Rate: 0.005\n",
      "Epoch [17718/20000], Loss: -21.303802490234375, Learning Rate: 0.005\n",
      "Epoch [17719/20000], Loss: -21.303848266601562, Learning Rate: 0.005\n",
      "Epoch [17720/20000], Loss: -21.303817749023438, Learning Rate: 0.005\n",
      "Epoch [17721/20000], Loss: -21.303878784179688, Learning Rate: 0.005\n",
      "Epoch [17722/20000], Loss: -21.303909301757812, Learning Rate: 0.005\n",
      "Epoch [17723/20000], Loss: -21.303939819335938, Learning Rate: 0.005\n",
      "Epoch [17724/20000], Loss: -21.303924560546875, Learning Rate: 0.005\n",
      "Epoch [17725/20000], Loss: -21.303970336914062, Learning Rate: 0.005\n",
      "Epoch [17726/20000], Loss: -21.303955078125, Learning Rate: 0.005\n",
      "Epoch [17727/20000], Loss: -21.304000854492188, Learning Rate: 0.005\n",
      "Epoch [17728/20000], Loss: -21.304031372070312, Learning Rate: 0.005\n",
      "Epoch [17729/20000], Loss: -21.304031372070312, Learning Rate: 0.005\n",
      "Epoch [17730/20000], Loss: -21.304031372070312, Learning Rate: 0.005\n",
      "Epoch [17731/20000], Loss: -21.3040771484375, Learning Rate: 0.005\n",
      "Epoch [17732/20000], Loss: -21.304061889648438, Learning Rate: 0.005\n",
      "Epoch [17733/20000], Loss: -21.304092407226562, Learning Rate: 0.005\n",
      "Epoch [17734/20000], Loss: -21.304168701171875, Learning Rate: 0.005\n",
      "Epoch [17735/20000], Loss: -21.30413818359375, Learning Rate: 0.005\n",
      "Epoch [17736/20000], Loss: -21.304183959960938, Learning Rate: 0.005\n",
      "Epoch [17737/20000], Loss: -21.30419921875, Learning Rate: 0.005\n",
      "Epoch [17738/20000], Loss: -21.304214477539062, Learning Rate: 0.005\n",
      "Epoch [17739/20000], Loss: -21.304244995117188, Learning Rate: 0.005\n",
      "Epoch [17740/20000], Loss: -21.30419921875, Learning Rate: 0.005\n",
      "Epoch [17741/20000], Loss: -21.3043212890625, Learning Rate: 0.005\n",
      "Epoch [17742/20000], Loss: -21.304351806640625, Learning Rate: 0.005\n",
      "Epoch [17743/20000], Loss: -21.304306030273438, Learning Rate: 0.005\n",
      "Epoch [17744/20000], Loss: -21.304351806640625, Learning Rate: 0.005\n",
      "Epoch [17745/20000], Loss: -21.304367065429688, Learning Rate: 0.005\n",
      "Epoch [17746/20000], Loss: -21.304412841796875, Learning Rate: 0.005\n",
      "Epoch [17747/20000], Loss: -21.304397583007812, Learning Rate: 0.005\n",
      "Epoch [17748/20000], Loss: -21.304428100585938, Learning Rate: 0.005\n",
      "Epoch [17749/20000], Loss: -21.30450439453125, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17750/20000], Loss: -21.30450439453125, Learning Rate: 0.005\n",
      "Epoch [17751/20000], Loss: -21.30450439453125, Learning Rate: 0.005\n",
      "Epoch [17752/20000], Loss: -21.3045654296875, Learning Rate: 0.005\n",
      "Epoch [17753/20000], Loss: -21.304534912109375, Learning Rate: 0.005\n",
      "Epoch [17754/20000], Loss: -21.304595947265625, Learning Rate: 0.005\n",
      "Epoch [17755/20000], Loss: -21.304580688476562, Learning Rate: 0.005\n",
      "Epoch [17756/20000], Loss: -21.304641723632812, Learning Rate: 0.005\n",
      "Epoch [17757/20000], Loss: -21.304595947265625, Learning Rate: 0.005\n",
      "Epoch [17758/20000], Loss: -21.30462646484375, Learning Rate: 0.005\n",
      "Epoch [17759/20000], Loss: -21.304656982421875, Learning Rate: 0.005\n",
      "Epoch [17760/20000], Loss: -21.304656982421875, Learning Rate: 0.005\n",
      "Epoch [17761/20000], Loss: -21.304672241210938, Learning Rate: 0.005\n",
      "Epoch [17762/20000], Loss: -21.30474853515625, Learning Rate: 0.005\n",
      "Epoch [17763/20000], Loss: -21.304824829101562, Learning Rate: 0.005\n",
      "Epoch [17764/20000], Loss: -21.304779052734375, Learning Rate: 0.005\n",
      "Epoch [17765/20000], Loss: -21.304824829101562, Learning Rate: 0.005\n",
      "Epoch [17766/20000], Loss: -21.30487060546875, Learning Rate: 0.005\n",
      "Epoch [17767/20000], Loss: -21.30487060546875, Learning Rate: 0.005\n",
      "Epoch [17768/20000], Loss: -21.304824829101562, Learning Rate: 0.005\n",
      "Epoch [17769/20000], Loss: -21.304840087890625, Learning Rate: 0.005\n",
      "Epoch [17770/20000], Loss: -21.304946899414062, Learning Rate: 0.005\n",
      "Epoch [17771/20000], Loss: -21.304946899414062, Learning Rate: 0.005\n",
      "Epoch [17772/20000], Loss: -21.30499267578125, Learning Rate: 0.005\n",
      "Epoch [17773/20000], Loss: -21.305007934570312, Learning Rate: 0.005\n",
      "Epoch [17774/20000], Loss: -21.305038452148438, Learning Rate: 0.005\n",
      "Epoch [17775/20000], Loss: -21.305084228515625, Learning Rate: 0.005\n",
      "Epoch [17776/20000], Loss: -21.30511474609375, Learning Rate: 0.005\n",
      "Epoch [17777/20000], Loss: -21.305068969726562, Learning Rate: 0.005\n",
      "Epoch [17778/20000], Loss: -21.30511474609375, Learning Rate: 0.005\n",
      "Epoch [17779/20000], Loss: -21.305160522460938, Learning Rate: 0.005\n",
      "Epoch [17780/20000], Loss: -21.30517578125, Learning Rate: 0.005\n",
      "Epoch [17781/20000], Loss: -21.305160522460938, Learning Rate: 0.005\n",
      "Epoch [17782/20000], Loss: -21.305206298828125, Learning Rate: 0.005\n",
      "Epoch [17783/20000], Loss: -21.30523681640625, Learning Rate: 0.005\n",
      "Epoch [17784/20000], Loss: -21.305267333984375, Learning Rate: 0.005\n",
      "Epoch [17785/20000], Loss: -21.30523681640625, Learning Rate: 0.005\n",
      "Epoch [17786/20000], Loss: -21.305267333984375, Learning Rate: 0.005\n",
      "Epoch [17787/20000], Loss: -21.305374145507812, Learning Rate: 0.005\n",
      "Epoch [17788/20000], Loss: -21.305374145507812, Learning Rate: 0.005\n",
      "Epoch [17789/20000], Loss: -21.305328369140625, Learning Rate: 0.005\n",
      "Epoch [17790/20000], Loss: -21.305374145507812, Learning Rate: 0.005\n",
      "Epoch [17791/20000], Loss: -21.305435180664062, Learning Rate: 0.005\n",
      "Epoch [17792/20000], Loss: -21.305450439453125, Learning Rate: 0.005\n",
      "Epoch [17793/20000], Loss: -21.305435180664062, Learning Rate: 0.005\n",
      "Epoch [17794/20000], Loss: -21.305435180664062, Learning Rate: 0.005\n",
      "Epoch [17795/20000], Loss: -21.3055419921875, Learning Rate: 0.005\n",
      "Epoch [17796/20000], Loss: -21.305572509765625, Learning Rate: 0.005\n",
      "Epoch [17797/20000], Loss: -21.305572509765625, Learning Rate: 0.005\n",
      "Epoch [17798/20000], Loss: -21.305572509765625, Learning Rate: 0.005\n",
      "Epoch [17799/20000], Loss: -21.305618286132812, Learning Rate: 0.005\n",
      "Epoch [17800/20000], Loss: -21.305587768554688, Learning Rate: 0.005\n",
      "Epoch [17801/20000], Loss: -21.305633544921875, Learning Rate: 0.005\n",
      "Epoch [17802/20000], Loss: -21.305648803710938, Learning Rate: 0.005\n",
      "Epoch [17803/20000], Loss: -21.305648803710938, Learning Rate: 0.005\n",
      "Epoch [17804/20000], Loss: -21.3056640625, Learning Rate: 0.005\n",
      "Epoch [17805/20000], Loss: -21.305709838867188, Learning Rate: 0.005\n",
      "Epoch [17806/20000], Loss: -21.305740356445312, Learning Rate: 0.005\n",
      "Epoch [17807/20000], Loss: -21.305694580078125, Learning Rate: 0.005\n",
      "Epoch [17808/20000], Loss: -21.305755615234375, Learning Rate: 0.005\n",
      "Epoch [17809/20000], Loss: -21.305770874023438, Learning Rate: 0.005\n",
      "Epoch [17810/20000], Loss: -21.305801391601562, Learning Rate: 0.005\n",
      "Epoch [17811/20000], Loss: -21.305816650390625, Learning Rate: 0.005\n",
      "Epoch [17812/20000], Loss: -21.305816650390625, Learning Rate: 0.005\n",
      "Epoch [17813/20000], Loss: -21.30584716796875, Learning Rate: 0.005\n",
      "Epoch [17814/20000], Loss: -21.305908203125, Learning Rate: 0.005\n",
      "Epoch [17815/20000], Loss: -21.305923461914062, Learning Rate: 0.005\n",
      "Epoch [17816/20000], Loss: -21.305908203125, Learning Rate: 0.005\n",
      "Epoch [17817/20000], Loss: -21.30596923828125, Learning Rate: 0.005\n",
      "Epoch [17818/20000], Loss: -21.305953979492188, Learning Rate: 0.005\n",
      "Epoch [17819/20000], Loss: -21.305984497070312, Learning Rate: 0.005\n",
      "Epoch [17820/20000], Loss: -21.306060791015625, Learning Rate: 0.005\n",
      "Epoch [17821/20000], Loss: -21.3060302734375, Learning Rate: 0.005\n",
      "Epoch [17822/20000], Loss: -21.3060302734375, Learning Rate: 0.005\n",
      "Epoch [17823/20000], Loss: -21.306060791015625, Learning Rate: 0.005\n",
      "Epoch [17824/20000], Loss: -21.306106567382812, Learning Rate: 0.005\n",
      "Epoch [17825/20000], Loss: -21.306106567382812, Learning Rate: 0.005\n",
      "Epoch [17826/20000], Loss: -21.30615234375, Learning Rate: 0.005\n",
      "Epoch [17827/20000], Loss: -21.3062744140625, Learning Rate: 0.005\n",
      "Epoch [17828/20000], Loss: -21.306243896484375, Learning Rate: 0.005\n",
      "Epoch [17829/20000], Loss: -21.306198120117188, Learning Rate: 0.005\n",
      "Epoch [17830/20000], Loss: -21.3062744140625, Learning Rate: 0.005\n",
      "Epoch [17831/20000], Loss: -21.306304931640625, Learning Rate: 0.005\n",
      "Epoch [17832/20000], Loss: -21.30633544921875, Learning Rate: 0.005\n",
      "Epoch [17833/20000], Loss: -21.306289672851562, Learning Rate: 0.005\n",
      "Epoch [17834/20000], Loss: -21.306365966796875, Learning Rate: 0.005\n",
      "Epoch [17835/20000], Loss: -21.306396484375, Learning Rate: 0.005\n",
      "Epoch [17836/20000], Loss: -21.306442260742188, Learning Rate: 0.005\n",
      "Epoch [17837/20000], Loss: -21.306365966796875, Learning Rate: 0.005\n",
      "Epoch [17838/20000], Loss: -21.306503295898438, Learning Rate: 0.005\n",
      "Epoch [17839/20000], Loss: -21.306472778320312, Learning Rate: 0.005\n",
      "Epoch [17840/20000], Loss: -21.306472778320312, Learning Rate: 0.005\n",
      "Epoch [17841/20000], Loss: -21.306549072265625, Learning Rate: 0.005\n",
      "Epoch [17842/20000], Loss: -21.306472778320312, Learning Rate: 0.005\n",
      "Epoch [17843/20000], Loss: -21.306533813476562, Learning Rate: 0.005\n",
      "Epoch [17844/20000], Loss: -21.306533813476562, Learning Rate: 0.005\n",
      "Epoch [17845/20000], Loss: -21.306564331054688, Learning Rate: 0.005\n",
      "Epoch [17846/20000], Loss: -21.306640625, Learning Rate: 0.005\n",
      "Epoch [17847/20000], Loss: -21.306655883789062, Learning Rate: 0.005\n",
      "Epoch [17848/20000], Loss: -21.306686401367188, Learning Rate: 0.005\n",
      "Epoch [17849/20000], Loss: -21.306686401367188, Learning Rate: 0.005\n",
      "Epoch [17850/20000], Loss: -21.306671142578125, Learning Rate: 0.005\n",
      "Epoch [17851/20000], Loss: -21.306747436523438, Learning Rate: 0.005\n",
      "Epoch [17852/20000], Loss: -21.30682373046875, Learning Rate: 0.005\n",
      "Epoch [17853/20000], Loss: -21.306777954101562, Learning Rate: 0.005\n",
      "Epoch [17854/20000], Loss: -21.306793212890625, Learning Rate: 0.005\n",
      "Epoch [17855/20000], Loss: -21.306808471679688, Learning Rate: 0.005\n",
      "Epoch [17856/20000], Loss: -21.306854248046875, Learning Rate: 0.005\n",
      "Epoch [17857/20000], Loss: -21.306884765625, Learning Rate: 0.005\n",
      "Epoch [17858/20000], Loss: -21.306869506835938, Learning Rate: 0.005\n",
      "Epoch [17859/20000], Loss: -21.306915283203125, Learning Rate: 0.005\n",
      "Epoch [17860/20000], Loss: -21.30694580078125, Learning Rate: 0.005\n",
      "Epoch [17861/20000], Loss: -21.30694580078125, Learning Rate: 0.005\n",
      "Epoch [17862/20000], Loss: -21.30694580078125, Learning Rate: 0.005\n",
      "Epoch [17863/20000], Loss: -21.30694580078125, Learning Rate: 0.005\n",
      "Epoch [17864/20000], Loss: -21.3070068359375, Learning Rate: 0.005\n",
      "Epoch [17865/20000], Loss: -21.307037353515625, Learning Rate: 0.005\n",
      "Epoch [17866/20000], Loss: -21.30706787109375, Learning Rate: 0.005\n",
      "Epoch [17867/20000], Loss: -21.307113647460938, Learning Rate: 0.005\n",
      "Epoch [17868/20000], Loss: -21.307083129882812, Learning Rate: 0.005\n",
      "Epoch [17869/20000], Loss: -21.307159423828125, Learning Rate: 0.005\n",
      "Epoch [17870/20000], Loss: -21.30712890625, Learning Rate: 0.005\n",
      "Epoch [17871/20000], Loss: -21.307159423828125, Learning Rate: 0.005\n",
      "Epoch [17872/20000], Loss: -21.307220458984375, Learning Rate: 0.005\n",
      "Epoch [17873/20000], Loss: -21.307235717773438, Learning Rate: 0.005\n",
      "Epoch [17874/20000], Loss: -21.307266235351562, Learning Rate: 0.005\n",
      "Epoch [17875/20000], Loss: -21.307296752929688, Learning Rate: 0.005\n",
      "Epoch [17876/20000], Loss: -21.30731201171875, Learning Rate: 0.005\n",
      "Epoch [17877/20000], Loss: -21.307342529296875, Learning Rate: 0.005\n",
      "Epoch [17878/20000], Loss: -21.307388305664062, Learning Rate: 0.005\n",
      "Epoch [17879/20000], Loss: -21.307327270507812, Learning Rate: 0.005\n",
      "Epoch [17880/20000], Loss: -21.307357788085938, Learning Rate: 0.005\n",
      "Epoch [17881/20000], Loss: -21.307388305664062, Learning Rate: 0.005\n",
      "Epoch [17882/20000], Loss: -21.307418823242188, Learning Rate: 0.005\n",
      "Epoch [17883/20000], Loss: -21.3074951171875, Learning Rate: 0.005\n",
      "Epoch [17884/20000], Loss: -21.3074951171875, Learning Rate: 0.005\n",
      "Epoch [17885/20000], Loss: -21.30755615234375, Learning Rate: 0.005\n",
      "Epoch [17886/20000], Loss: -21.307525634765625, Learning Rate: 0.005\n",
      "Epoch [17887/20000], Loss: -21.307525634765625, Learning Rate: 0.005\n",
      "Epoch [17888/20000], Loss: -21.307647705078125, Learning Rate: 0.005\n",
      "Epoch [17889/20000], Loss: -21.3076171875, Learning Rate: 0.005\n",
      "Epoch [17890/20000], Loss: -21.307632446289062, Learning Rate: 0.005\n",
      "Epoch [17891/20000], Loss: -21.307662963867188, Learning Rate: 0.005\n",
      "Epoch [17892/20000], Loss: -21.307662963867188, Learning Rate: 0.005\n",
      "Epoch [17893/20000], Loss: -21.30767822265625, Learning Rate: 0.005\n",
      "Epoch [17894/20000], Loss: -21.30767822265625, Learning Rate: 0.005\n",
      "Epoch [17895/20000], Loss: -21.307723999023438, Learning Rate: 0.005\n",
      "Epoch [17896/20000], Loss: -21.3077392578125, Learning Rate: 0.005\n",
      "Epoch [17897/20000], Loss: -21.307785034179688, Learning Rate: 0.005\n",
      "Epoch [17898/20000], Loss: -21.307785034179688, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17899/20000], Loss: -21.30780029296875, Learning Rate: 0.005\n",
      "Epoch [17900/20000], Loss: -21.307846069335938, Learning Rate: 0.005\n",
      "Epoch [17901/20000], Loss: -21.307846069335938, Learning Rate: 0.005\n",
      "Epoch [17902/20000], Loss: -21.307830810546875, Learning Rate: 0.005\n",
      "Epoch [17903/20000], Loss: -21.307937622070312, Learning Rate: 0.005\n",
      "Epoch [17904/20000], Loss: -21.307907104492188, Learning Rate: 0.005\n",
      "Epoch [17905/20000], Loss: -21.307952880859375, Learning Rate: 0.005\n",
      "Epoch [17906/20000], Loss: -21.308013916015625, Learning Rate: 0.005\n",
      "Epoch [17907/20000], Loss: -21.3079833984375, Learning Rate: 0.005\n",
      "Epoch [17908/20000], Loss: -21.30804443359375, Learning Rate: 0.005\n",
      "Epoch [17909/20000], Loss: -21.30804443359375, Learning Rate: 0.005\n",
      "Epoch [17910/20000], Loss: -21.307998657226562, Learning Rate: 0.005\n",
      "Epoch [17911/20000], Loss: -21.308090209960938, Learning Rate: 0.005\n",
      "Epoch [17912/20000], Loss: -21.308135986328125, Learning Rate: 0.005\n",
      "Epoch [17913/20000], Loss: -21.308135986328125, Learning Rate: 0.005\n",
      "Epoch [17914/20000], Loss: -21.308090209960938, Learning Rate: 0.005\n",
      "Epoch [17915/20000], Loss: -21.308151245117188, Learning Rate: 0.005\n",
      "Epoch [17916/20000], Loss: -21.308197021484375, Learning Rate: 0.005\n",
      "Epoch [17917/20000], Loss: -21.3082275390625, Learning Rate: 0.005\n",
      "Epoch [17918/20000], Loss: -21.308212280273438, Learning Rate: 0.005\n",
      "Epoch [17919/20000], Loss: -21.308258056640625, Learning Rate: 0.005\n",
      "Epoch [17920/20000], Loss: -21.308273315429688, Learning Rate: 0.005\n",
      "Epoch [17921/20000], Loss: -21.308273315429688, Learning Rate: 0.005\n",
      "Epoch [17922/20000], Loss: -21.308319091796875, Learning Rate: 0.005\n",
      "Epoch [17923/20000], Loss: -21.308334350585938, Learning Rate: 0.005\n",
      "Epoch [17924/20000], Loss: -21.308319091796875, Learning Rate: 0.005\n",
      "Epoch [17925/20000], Loss: -21.30841064453125, Learning Rate: 0.005\n",
      "Epoch [17926/20000], Loss: -21.308395385742188, Learning Rate: 0.005\n",
      "Epoch [17927/20000], Loss: -21.308456420898438, Learning Rate: 0.005\n",
      "Epoch [17928/20000], Loss: -21.308456420898438, Learning Rate: 0.005\n",
      "Epoch [17929/20000], Loss: -21.308486938476562, Learning Rate: 0.005\n",
      "Epoch [17930/20000], Loss: -21.308486938476562, Learning Rate: 0.005\n",
      "Epoch [17931/20000], Loss: -21.308563232421875, Learning Rate: 0.005\n",
      "Epoch [17932/20000], Loss: -21.30859375, Learning Rate: 0.005\n",
      "Epoch [17933/20000], Loss: -21.30859375, Learning Rate: 0.005\n",
      "Epoch [17934/20000], Loss: -21.308578491210938, Learning Rate: 0.005\n",
      "Epoch [17935/20000], Loss: -21.308685302734375, Learning Rate: 0.005\n",
      "Epoch [17936/20000], Loss: -21.308609008789062, Learning Rate: 0.005\n",
      "Epoch [17937/20000], Loss: -21.308685302734375, Learning Rate: 0.005\n",
      "Epoch [17938/20000], Loss: -21.308685302734375, Learning Rate: 0.005\n",
      "Epoch [17939/20000], Loss: -21.308685302734375, Learning Rate: 0.005\n",
      "Epoch [17940/20000], Loss: -21.308731079101562, Learning Rate: 0.005\n",
      "Epoch [17941/20000], Loss: -21.308792114257812, Learning Rate: 0.005\n",
      "Epoch [17942/20000], Loss: -21.308822631835938, Learning Rate: 0.005\n",
      "Epoch [17943/20000], Loss: -21.308792114257812, Learning Rate: 0.005\n",
      "Epoch [17944/20000], Loss: -21.308883666992188, Learning Rate: 0.005\n",
      "Epoch [17945/20000], Loss: -21.308853149414062, Learning Rate: 0.005\n",
      "Epoch [17946/20000], Loss: -21.308837890625, Learning Rate: 0.005\n",
      "Epoch [17947/20000], Loss: -21.308914184570312, Learning Rate: 0.005\n",
      "Epoch [17948/20000], Loss: -21.308914184570312, Learning Rate: 0.005\n",
      "Epoch [17949/20000], Loss: -21.308944702148438, Learning Rate: 0.005\n",
      "Epoch [17950/20000], Loss: -21.308990478515625, Learning Rate: 0.005\n",
      "Epoch [17951/20000], Loss: -21.308990478515625, Learning Rate: 0.005\n",
      "Epoch [17952/20000], Loss: -21.308975219726562, Learning Rate: 0.005\n",
      "Epoch [17953/20000], Loss: -21.309036254882812, Learning Rate: 0.005\n",
      "Epoch [17954/20000], Loss: -21.309066772460938, Learning Rate: 0.005\n",
      "Epoch [17955/20000], Loss: -21.309066772460938, Learning Rate: 0.005\n",
      "Epoch [17956/20000], Loss: -21.309097290039062, Learning Rate: 0.005\n",
      "Epoch [17957/20000], Loss: -21.309112548828125, Learning Rate: 0.005\n",
      "Epoch [17958/20000], Loss: -21.309173583984375, Learning Rate: 0.005\n",
      "Epoch [17959/20000], Loss: -21.309158325195312, Learning Rate: 0.005\n",
      "Epoch [17960/20000], Loss: -21.309188842773438, Learning Rate: 0.005\n",
      "Epoch [17961/20000], Loss: -21.309280395507812, Learning Rate: 0.005\n",
      "Epoch [17962/20000], Loss: -21.309295654296875, Learning Rate: 0.005\n",
      "Epoch [17963/20000], Loss: -21.30926513671875, Learning Rate: 0.005\n",
      "Epoch [17964/20000], Loss: -21.309326171875, Learning Rate: 0.005\n",
      "Epoch [17965/20000], Loss: -21.309371948242188, Learning Rate: 0.005\n",
      "Epoch [17966/20000], Loss: -21.309341430664062, Learning Rate: 0.005\n",
      "Epoch [17967/20000], Loss: -21.30938720703125, Learning Rate: 0.005\n",
      "Epoch [17968/20000], Loss: -21.309356689453125, Learning Rate: 0.005\n",
      "Epoch [17969/20000], Loss: -21.309402465820312, Learning Rate: 0.005\n",
      "Epoch [17970/20000], Loss: -21.309478759765625, Learning Rate: 0.005\n",
      "Epoch [17971/20000], Loss: -21.30950927734375, Learning Rate: 0.005\n",
      "Epoch [17972/20000], Loss: -21.309432983398438, Learning Rate: 0.005\n",
      "Epoch [17973/20000], Loss: -21.309478759765625, Learning Rate: 0.005\n",
      "Epoch [17974/20000], Loss: -21.30950927734375, Learning Rate: 0.005\n",
      "Epoch [17975/20000], Loss: -21.3095703125, Learning Rate: 0.005\n",
      "Epoch [17976/20000], Loss: -21.309539794921875, Learning Rate: 0.005\n",
      "Epoch [17977/20000], Loss: -21.309539794921875, Learning Rate: 0.005\n",
      "Epoch [17978/20000], Loss: -21.309585571289062, Learning Rate: 0.005\n",
      "Epoch [17979/20000], Loss: -21.309616088867188, Learning Rate: 0.005\n",
      "Epoch [17980/20000], Loss: -21.309661865234375, Learning Rate: 0.005\n",
      "Epoch [17981/20000], Loss: -21.3096923828125, Learning Rate: 0.005\n",
      "Epoch [17982/20000], Loss: -21.309738159179688, Learning Rate: 0.005\n",
      "Epoch [17983/20000], Loss: -21.309738159179688, Learning Rate: 0.005\n",
      "Epoch [17984/20000], Loss: -21.309783935546875, Learning Rate: 0.005\n",
      "Epoch [17985/20000], Loss: -21.309722900390625, Learning Rate: 0.005\n",
      "Epoch [17986/20000], Loss: -21.309814453125, Learning Rate: 0.005\n",
      "Epoch [17987/20000], Loss: -21.309799194335938, Learning Rate: 0.005\n",
      "Epoch [17988/20000], Loss: -21.309844970703125, Learning Rate: 0.005\n",
      "Epoch [17989/20000], Loss: -21.309860229492188, Learning Rate: 0.005\n",
      "Epoch [17990/20000], Loss: -21.309844970703125, Learning Rate: 0.005\n",
      "Epoch [17991/20000], Loss: -21.309921264648438, Learning Rate: 0.005\n",
      "Epoch [17992/20000], Loss: -21.309906005859375, Learning Rate: 0.005\n",
      "Epoch [17993/20000], Loss: -21.309982299804688, Learning Rate: 0.005\n",
      "Epoch [17994/20000], Loss: -21.310012817382812, Learning Rate: 0.005\n",
      "Epoch [17995/20000], Loss: -21.30999755859375, Learning Rate: 0.005\n",
      "Epoch [17996/20000], Loss: -21.310043334960938, Learning Rate: 0.005\n",
      "Epoch [17997/20000], Loss: -21.310073852539062, Learning Rate: 0.005\n",
      "Epoch [17998/20000], Loss: -21.31005859375, Learning Rate: 0.005\n",
      "Epoch [17999/20000], Loss: -21.310104370117188, Learning Rate: 0.005\n",
      "Epoch [18000/20000], Loss: -21.310073852539062, Learning Rate: 0.005\n",
      "Epoch [18001/20000], Loss: -21.3101806640625, Learning Rate: 0.005\n",
      "Epoch [18002/20000], Loss: -21.310165405273438, Learning Rate: 0.005\n",
      "Epoch [18003/20000], Loss: -21.310195922851562, Learning Rate: 0.005\n",
      "Epoch [18004/20000], Loss: -21.310165405273438, Learning Rate: 0.005\n",
      "Epoch [18005/20000], Loss: -21.310226440429688, Learning Rate: 0.005\n",
      "Epoch [18006/20000], Loss: -21.310287475585938, Learning Rate: 0.005\n",
      "Epoch [18007/20000], Loss: -21.310272216796875, Learning Rate: 0.005\n",
      "Epoch [18008/20000], Loss: -21.310287475585938, Learning Rate: 0.005\n",
      "Epoch [18009/20000], Loss: -21.310333251953125, Learning Rate: 0.005\n",
      "Epoch [18010/20000], Loss: -21.310348510742188, Learning Rate: 0.005\n",
      "Epoch [18011/20000], Loss: -21.310348510742188, Learning Rate: 0.005\n",
      "Epoch [18012/20000], Loss: -21.31036376953125, Learning Rate: 0.005\n",
      "Epoch [18013/20000], Loss: -21.310409545898438, Learning Rate: 0.005\n",
      "Epoch [18014/20000], Loss: -21.310440063476562, Learning Rate: 0.005\n",
      "Epoch [18015/20000], Loss: -21.310455322265625, Learning Rate: 0.005\n",
      "Epoch [18016/20000], Loss: -21.310440063476562, Learning Rate: 0.005\n",
      "Epoch [18017/20000], Loss: -21.31048583984375, Learning Rate: 0.005\n",
      "Epoch [18018/20000], Loss: -21.310531616210938, Learning Rate: 0.005\n",
      "Epoch [18019/20000], Loss: -21.310577392578125, Learning Rate: 0.005\n",
      "Epoch [18020/20000], Loss: -21.310623168945312, Learning Rate: 0.005\n",
      "Epoch [18021/20000], Loss: -21.310623168945312, Learning Rate: 0.005\n",
      "Epoch [18022/20000], Loss: -21.310577392578125, Learning Rate: 0.005\n",
      "Epoch [18023/20000], Loss: -21.310653686523438, Learning Rate: 0.005\n",
      "Epoch [18024/20000], Loss: -21.310699462890625, Learning Rate: 0.005\n",
      "Epoch [18025/20000], Loss: -21.3106689453125, Learning Rate: 0.005\n",
      "Epoch [18026/20000], Loss: -21.310714721679688, Learning Rate: 0.005\n",
      "Epoch [18027/20000], Loss: -21.310745239257812, Learning Rate: 0.005\n",
      "Epoch [18028/20000], Loss: -21.310760498046875, Learning Rate: 0.005\n",
      "Epoch [18029/20000], Loss: -21.310791015625, Learning Rate: 0.005\n",
      "Epoch [18030/20000], Loss: -21.310806274414062, Learning Rate: 0.005\n",
      "Epoch [18031/20000], Loss: -21.310791015625, Learning Rate: 0.005\n",
      "Epoch [18032/20000], Loss: -21.310867309570312, Learning Rate: 0.005\n",
      "Epoch [18033/20000], Loss: -21.310867309570312, Learning Rate: 0.005\n",
      "Epoch [18034/20000], Loss: -21.310867309570312, Learning Rate: 0.005\n",
      "Epoch [18035/20000], Loss: -21.310958862304688, Learning Rate: 0.005\n",
      "Epoch [18036/20000], Loss: -21.311004638671875, Learning Rate: 0.005\n",
      "Epoch [18037/20000], Loss: -21.311004638671875, Learning Rate: 0.005\n",
      "Epoch [18038/20000], Loss: -21.311065673828125, Learning Rate: 0.005\n",
      "Epoch [18039/20000], Loss: -21.311050415039062, Learning Rate: 0.005\n",
      "Epoch [18040/20000], Loss: -21.31109619140625, Learning Rate: 0.005\n",
      "Epoch [18041/20000], Loss: -21.311172485351562, Learning Rate: 0.005\n",
      "Epoch [18042/20000], Loss: -21.31121826171875, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18043/20000], Loss: -21.311126708984375, Learning Rate: 0.005\n",
      "Epoch [18044/20000], Loss: -21.31121826171875, Learning Rate: 0.005\n",
      "Epoch [18045/20000], Loss: -21.311141967773438, Learning Rate: 0.005\n",
      "Epoch [18046/20000], Loss: -21.311172485351562, Learning Rate: 0.005\n",
      "Epoch [18047/20000], Loss: -21.31121826171875, Learning Rate: 0.005\n",
      "Epoch [18048/20000], Loss: -21.311279296875, Learning Rate: 0.005\n",
      "Epoch [18049/20000], Loss: -21.311264038085938, Learning Rate: 0.005\n",
      "Epoch [18050/20000], Loss: -21.311294555664062, Learning Rate: 0.005\n",
      "Epoch [18051/20000], Loss: -21.311325073242188, Learning Rate: 0.005\n",
      "Epoch [18052/20000], Loss: -21.311370849609375, Learning Rate: 0.005\n",
      "Epoch [18053/20000], Loss: -21.311386108398438, Learning Rate: 0.005\n",
      "Epoch [18054/20000], Loss: -21.311386108398438, Learning Rate: 0.005\n",
      "Epoch [18055/20000], Loss: -21.311416625976562, Learning Rate: 0.005\n",
      "Epoch [18056/20000], Loss: -21.311431884765625, Learning Rate: 0.005\n",
      "Epoch [18057/20000], Loss: -21.311447143554688, Learning Rate: 0.005\n",
      "Epoch [18058/20000], Loss: -21.311492919921875, Learning Rate: 0.005\n",
      "Epoch [18059/20000], Loss: -21.311477661132812, Learning Rate: 0.005\n",
      "Epoch [18060/20000], Loss: -21.311599731445312, Learning Rate: 0.005\n",
      "Epoch [18061/20000], Loss: -21.31158447265625, Learning Rate: 0.005\n",
      "Epoch [18062/20000], Loss: -21.31158447265625, Learning Rate: 0.005\n",
      "Epoch [18063/20000], Loss: -21.311569213867188, Learning Rate: 0.005\n",
      "Epoch [18064/20000], Loss: -21.311676025390625, Learning Rate: 0.005\n",
      "Epoch [18065/20000], Loss: -21.311660766601562, Learning Rate: 0.005\n",
      "Epoch [18066/20000], Loss: -21.311676025390625, Learning Rate: 0.005\n",
      "Epoch [18067/20000], Loss: -21.311676025390625, Learning Rate: 0.005\n",
      "Epoch [18068/20000], Loss: -21.311737060546875, Learning Rate: 0.005\n",
      "Epoch [18069/20000], Loss: -21.311691284179688, Learning Rate: 0.005\n",
      "Epoch [18070/20000], Loss: -21.311798095703125, Learning Rate: 0.005\n",
      "Epoch [18071/20000], Loss: -21.311798095703125, Learning Rate: 0.005\n",
      "Epoch [18072/20000], Loss: -21.311752319335938, Learning Rate: 0.005\n",
      "Epoch [18073/20000], Loss: -21.311843872070312, Learning Rate: 0.005\n",
      "Epoch [18074/20000], Loss: -21.311859130859375, Learning Rate: 0.005\n",
      "Epoch [18075/20000], Loss: -21.311843872070312, Learning Rate: 0.005\n",
      "Epoch [18076/20000], Loss: -21.3118896484375, Learning Rate: 0.005\n",
      "Epoch [18077/20000], Loss: -21.311859130859375, Learning Rate: 0.005\n",
      "Epoch [18078/20000], Loss: -21.311965942382812, Learning Rate: 0.005\n",
      "Epoch [18079/20000], Loss: -21.311904907226562, Learning Rate: 0.005\n",
      "Epoch [18080/20000], Loss: -21.311981201171875, Learning Rate: 0.005\n",
      "Epoch [18081/20000], Loss: -21.311965942382812, Learning Rate: 0.005\n",
      "Epoch [18082/20000], Loss: -21.311981201171875, Learning Rate: 0.005\n",
      "Epoch [18083/20000], Loss: -21.31207275390625, Learning Rate: 0.005\n",
      "Epoch [18084/20000], Loss: -21.312042236328125, Learning Rate: 0.005\n",
      "Epoch [18085/20000], Loss: -21.312118530273438, Learning Rate: 0.005\n",
      "Epoch [18086/20000], Loss: -21.312149047851562, Learning Rate: 0.005\n",
      "Epoch [18087/20000], Loss: -21.312149047851562, Learning Rate: 0.005\n",
      "Epoch [18088/20000], Loss: -21.31219482421875, Learning Rate: 0.005\n",
      "Epoch [18089/20000], Loss: -21.312210083007812, Learning Rate: 0.005\n",
      "Epoch [18090/20000], Loss: -21.312225341796875, Learning Rate: 0.005\n",
      "Epoch [18091/20000], Loss: -21.312240600585938, Learning Rate: 0.005\n",
      "Epoch [18092/20000], Loss: -21.312225341796875, Learning Rate: 0.005\n",
      "Epoch [18093/20000], Loss: -21.312240600585938, Learning Rate: 0.005\n",
      "Epoch [18094/20000], Loss: -21.31231689453125, Learning Rate: 0.005\n",
      "Epoch [18095/20000], Loss: -21.3123779296875, Learning Rate: 0.005\n",
      "Epoch [18096/20000], Loss: -21.3123779296875, Learning Rate: 0.005\n",
      "Epoch [18097/20000], Loss: -21.31243896484375, Learning Rate: 0.005\n",
      "Epoch [18098/20000], Loss: -21.312423706054688, Learning Rate: 0.005\n",
      "Epoch [18099/20000], Loss: -21.312408447265625, Learning Rate: 0.005\n",
      "Epoch [18100/20000], Loss: -21.312454223632812, Learning Rate: 0.005\n",
      "Epoch [18101/20000], Loss: -21.312484741210938, Learning Rate: 0.005\n",
      "Epoch [18102/20000], Loss: -21.3125, Learning Rate: 0.005\n",
      "Epoch [18103/20000], Loss: -21.312576293945312, Learning Rate: 0.005\n",
      "Epoch [18104/20000], Loss: -21.312530517578125, Learning Rate: 0.005\n",
      "Epoch [18105/20000], Loss: -21.31256103515625, Learning Rate: 0.005\n",
      "Epoch [18106/20000], Loss: -21.312591552734375, Learning Rate: 0.005\n",
      "Epoch [18107/20000], Loss: -21.3126220703125, Learning Rate: 0.005\n",
      "Epoch [18108/20000], Loss: -21.312652587890625, Learning Rate: 0.005\n",
      "Epoch [18109/20000], Loss: -21.31268310546875, Learning Rate: 0.005\n",
      "Epoch [18110/20000], Loss: -21.312713623046875, Learning Rate: 0.005\n",
      "Epoch [18111/20000], Loss: -21.312698364257812, Learning Rate: 0.005\n",
      "Epoch [18112/20000], Loss: -21.312728881835938, Learning Rate: 0.005\n",
      "Epoch [18113/20000], Loss: -21.312759399414062, Learning Rate: 0.005\n",
      "Epoch [18114/20000], Loss: -21.31280517578125, Learning Rate: 0.005\n",
      "Epoch [18115/20000], Loss: -21.312850952148438, Learning Rate: 0.005\n",
      "Epoch [18116/20000], Loss: -21.312820434570312, Learning Rate: 0.005\n",
      "Epoch [18117/20000], Loss: -21.312820434570312, Learning Rate: 0.005\n",
      "Epoch [18118/20000], Loss: -21.31292724609375, Learning Rate: 0.005\n",
      "Epoch [18119/20000], Loss: -21.312911987304688, Learning Rate: 0.005\n",
      "Epoch [18120/20000], Loss: -21.312973022460938, Learning Rate: 0.005\n",
      "Epoch [18121/20000], Loss: -21.313003540039062, Learning Rate: 0.005\n",
      "Epoch [18122/20000], Loss: -21.31298828125, Learning Rate: 0.005\n",
      "Epoch [18123/20000], Loss: -21.313079833984375, Learning Rate: 0.005\n",
      "Epoch [18124/20000], Loss: -21.31304931640625, Learning Rate: 0.005\n",
      "Epoch [18125/20000], Loss: -21.313064575195312, Learning Rate: 0.005\n",
      "Epoch [18126/20000], Loss: -21.313079833984375, Learning Rate: 0.005\n",
      "Epoch [18127/20000], Loss: -21.313064575195312, Learning Rate: 0.005\n",
      "Epoch [18128/20000], Loss: -21.313156127929688, Learning Rate: 0.005\n",
      "Epoch [18129/20000], Loss: -21.31317138671875, Learning Rate: 0.005\n",
      "Epoch [18130/20000], Loss: -21.313217163085938, Learning Rate: 0.005\n",
      "Epoch [18131/20000], Loss: -21.313217163085938, Learning Rate: 0.005\n",
      "Epoch [18132/20000], Loss: -21.313232421875, Learning Rate: 0.005\n",
      "Epoch [18133/20000], Loss: -21.313323974609375, Learning Rate: 0.005\n",
      "Epoch [18134/20000], Loss: -21.313308715820312, Learning Rate: 0.005\n",
      "Epoch [18135/20000], Loss: -21.31329345703125, Learning Rate: 0.005\n",
      "Epoch [18136/20000], Loss: -21.313323974609375, Learning Rate: 0.005\n",
      "Epoch [18137/20000], Loss: -21.313323974609375, Learning Rate: 0.005\n",
      "Epoch [18138/20000], Loss: -21.313369750976562, Learning Rate: 0.005\n",
      "Epoch [18139/20000], Loss: -21.313385009765625, Learning Rate: 0.005\n",
      "Epoch [18140/20000], Loss: -21.313385009765625, Learning Rate: 0.005\n",
      "Epoch [18141/20000], Loss: -21.313446044921875, Learning Rate: 0.005\n",
      "Epoch [18142/20000], Loss: -21.313461303710938, Learning Rate: 0.005\n",
      "Epoch [18143/20000], Loss: -21.313507080078125, Learning Rate: 0.005\n",
      "Epoch [18144/20000], Loss: -21.313522338867188, Learning Rate: 0.005\n",
      "Epoch [18145/20000], Loss: -21.3134765625, Learning Rate: 0.005\n",
      "Epoch [18146/20000], Loss: -21.313552856445312, Learning Rate: 0.005\n",
      "Epoch [18147/20000], Loss: -21.313568115234375, Learning Rate: 0.005\n",
      "Epoch [18148/20000], Loss: -21.313568115234375, Learning Rate: 0.005\n",
      "Epoch [18149/20000], Loss: -21.313629150390625, Learning Rate: 0.005\n",
      "Epoch [18150/20000], Loss: -21.313705444335938, Learning Rate: 0.005\n",
      "Epoch [18151/20000], Loss: -21.313705444335938, Learning Rate: 0.005\n",
      "Epoch [18152/20000], Loss: -21.313690185546875, Learning Rate: 0.005\n",
      "Epoch [18153/20000], Loss: -21.313735961914062, Learning Rate: 0.005\n",
      "Epoch [18154/20000], Loss: -21.313690185546875, Learning Rate: 0.005\n",
      "Epoch [18155/20000], Loss: -21.313735961914062, Learning Rate: 0.005\n",
      "Epoch [18156/20000], Loss: -21.3138427734375, Learning Rate: 0.005\n",
      "Epoch [18157/20000], Loss: -21.3138427734375, Learning Rate: 0.005\n",
      "Epoch [18158/20000], Loss: -21.313827514648438, Learning Rate: 0.005\n",
      "Epoch [18159/20000], Loss: -21.31390380859375, Learning Rate: 0.005\n",
      "Epoch [18160/20000], Loss: -21.313949584960938, Learning Rate: 0.005\n",
      "Epoch [18161/20000], Loss: -21.313949584960938, Learning Rate: 0.005\n",
      "Epoch [18162/20000], Loss: -21.313980102539062, Learning Rate: 0.005\n",
      "Epoch [18163/20000], Loss: -21.31396484375, Learning Rate: 0.005\n",
      "Epoch [18164/20000], Loss: -21.314010620117188, Learning Rate: 0.005\n",
      "Epoch [18165/20000], Loss: -21.314010620117188, Learning Rate: 0.005\n",
      "Epoch [18166/20000], Loss: -21.31402587890625, Learning Rate: 0.005\n",
      "Epoch [18167/20000], Loss: -21.314041137695312, Learning Rate: 0.005\n",
      "Epoch [18168/20000], Loss: -21.314071655273438, Learning Rate: 0.005\n",
      "Epoch [18169/20000], Loss: -21.314117431640625, Learning Rate: 0.005\n",
      "Epoch [18170/20000], Loss: -21.31414794921875, Learning Rate: 0.005\n",
      "Epoch [18171/20000], Loss: -21.314163208007812, Learning Rate: 0.005\n",
      "Epoch [18172/20000], Loss: -21.314163208007812, Learning Rate: 0.005\n",
      "Epoch [18173/20000], Loss: -21.314254760742188, Learning Rate: 0.005\n",
      "Epoch [18174/20000], Loss: -21.314285278320312, Learning Rate: 0.005\n",
      "Epoch [18175/20000], Loss: -21.314254760742188, Learning Rate: 0.005\n",
      "Epoch [18176/20000], Loss: -21.314346313476562, Learning Rate: 0.005\n",
      "Epoch [18177/20000], Loss: -21.314300537109375, Learning Rate: 0.005\n",
      "Epoch [18178/20000], Loss: -21.314346313476562, Learning Rate: 0.005\n",
      "Epoch [18179/20000], Loss: -21.314407348632812, Learning Rate: 0.005\n",
      "Epoch [18180/20000], Loss: -21.31439208984375, Learning Rate: 0.005\n",
      "Epoch [18181/20000], Loss: -21.314407348632812, Learning Rate: 0.005\n",
      "Epoch [18182/20000], Loss: -21.314407348632812, Learning Rate: 0.005\n",
      "Epoch [18183/20000], Loss: -21.314453125, Learning Rate: 0.005\n",
      "Epoch [18184/20000], Loss: -21.314453125, Learning Rate: 0.005\n",
      "Epoch [18185/20000], Loss: -21.314498901367188, Learning Rate: 0.005\n",
      "Epoch [18186/20000], Loss: -21.314544677734375, Learning Rate: 0.005\n",
      "Epoch [18187/20000], Loss: -21.314605712890625, Learning Rate: 0.005\n",
      "Epoch [18188/20000], Loss: -21.3145751953125, Learning Rate: 0.005\n",
      "Epoch [18189/20000], Loss: -21.314559936523438, Learning Rate: 0.005\n",
      "Epoch [18190/20000], Loss: -21.314605712890625, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18191/20000], Loss: -21.31463623046875, Learning Rate: 0.005\n",
      "Epoch [18192/20000], Loss: -21.314727783203125, Learning Rate: 0.005\n",
      "Epoch [18193/20000], Loss: -21.314651489257812, Learning Rate: 0.005\n",
      "Epoch [18194/20000], Loss: -21.314773559570312, Learning Rate: 0.005\n",
      "Epoch [18195/20000], Loss: -21.314743041992188, Learning Rate: 0.005\n",
      "Epoch [18196/20000], Loss: -21.31475830078125, Learning Rate: 0.005\n",
      "Epoch [18197/20000], Loss: -21.31488037109375, Learning Rate: 0.005\n",
      "Epoch [18198/20000], Loss: -21.314788818359375, Learning Rate: 0.005\n",
      "Epoch [18199/20000], Loss: -21.314849853515625, Learning Rate: 0.005\n",
      "Epoch [18200/20000], Loss: -21.31488037109375, Learning Rate: 0.005\n",
      "Epoch [18201/20000], Loss: -21.31488037109375, Learning Rate: 0.005\n",
      "Epoch [18202/20000], Loss: -21.31494140625, Learning Rate: 0.005\n",
      "Epoch [18203/20000], Loss: -21.314987182617188, Learning Rate: 0.005\n",
      "Epoch [18204/20000], Loss: -21.314971923828125, Learning Rate: 0.005\n",
      "Epoch [18205/20000], Loss: -21.315017700195312, Learning Rate: 0.005\n",
      "Epoch [18206/20000], Loss: -21.31500244140625, Learning Rate: 0.005\n",
      "Epoch [18207/20000], Loss: -21.314987182617188, Learning Rate: 0.005\n",
      "Epoch [18208/20000], Loss: -21.315032958984375, Learning Rate: 0.005\n",
      "Epoch [18209/20000], Loss: -21.315093994140625, Learning Rate: 0.005\n",
      "Epoch [18210/20000], Loss: -21.315139770507812, Learning Rate: 0.005\n",
      "Epoch [18211/20000], Loss: -21.315109252929688, Learning Rate: 0.005\n",
      "Epoch [18212/20000], Loss: -21.315185546875, Learning Rate: 0.005\n",
      "Epoch [18213/20000], Loss: -21.315200805664062, Learning Rate: 0.005\n",
      "Epoch [18214/20000], Loss: -21.315155029296875, Learning Rate: 0.005\n",
      "Epoch [18215/20000], Loss: -21.315216064453125, Learning Rate: 0.005\n",
      "Epoch [18216/20000], Loss: -21.315200805664062, Learning Rate: 0.005\n",
      "Epoch [18217/20000], Loss: -21.315277099609375, Learning Rate: 0.005\n",
      "Epoch [18218/20000], Loss: -21.315322875976562, Learning Rate: 0.005\n",
      "Epoch [18219/20000], Loss: -21.315414428710938, Learning Rate: 0.005\n",
      "Epoch [18220/20000], Loss: -21.31536865234375, Learning Rate: 0.005\n",
      "Epoch [18221/20000], Loss: -21.315353393554688, Learning Rate: 0.005\n",
      "Epoch [18222/20000], Loss: -21.315414428710938, Learning Rate: 0.005\n",
      "Epoch [18223/20000], Loss: -21.315460205078125, Learning Rate: 0.005\n",
      "Epoch [18224/20000], Loss: -21.315505981445312, Learning Rate: 0.005\n",
      "Epoch [18225/20000], Loss: -21.315475463867188, Learning Rate: 0.005\n",
      "Epoch [18226/20000], Loss: -21.315505981445312, Learning Rate: 0.005\n",
      "Epoch [18227/20000], Loss: -21.315521240234375, Learning Rate: 0.005\n",
      "Epoch [18228/20000], Loss: -21.315567016601562, Learning Rate: 0.005\n",
      "Epoch [18229/20000], Loss: -21.315536499023438, Learning Rate: 0.005\n",
      "Epoch [18230/20000], Loss: -21.315597534179688, Learning Rate: 0.005\n",
      "Epoch [18231/20000], Loss: -21.315597534179688, Learning Rate: 0.005\n",
      "Epoch [18232/20000], Loss: -21.315658569335938, Learning Rate: 0.005\n",
      "Epoch [18233/20000], Loss: -21.315704345703125, Learning Rate: 0.005\n",
      "Epoch [18234/20000], Loss: -21.315643310546875, Learning Rate: 0.005\n",
      "Epoch [18235/20000], Loss: -21.315719604492188, Learning Rate: 0.005\n",
      "Epoch [18236/20000], Loss: -21.315765380859375, Learning Rate: 0.005\n",
      "Epoch [18237/20000], Loss: -21.315780639648438, Learning Rate: 0.005\n",
      "Epoch [18238/20000], Loss: -21.31573486328125, Learning Rate: 0.005\n",
      "Epoch [18239/20000], Loss: -21.315811157226562, Learning Rate: 0.005\n",
      "Epoch [18240/20000], Loss: -21.315872192382812, Learning Rate: 0.005\n",
      "Epoch [18241/20000], Loss: -21.315811157226562, Learning Rate: 0.005\n",
      "Epoch [18242/20000], Loss: -21.31585693359375, Learning Rate: 0.005\n",
      "Epoch [18243/20000], Loss: -21.315872192382812, Learning Rate: 0.005\n",
      "Epoch [18244/20000], Loss: -21.315902709960938, Learning Rate: 0.005\n",
      "Epoch [18245/20000], Loss: -21.315887451171875, Learning Rate: 0.005\n",
      "Epoch [18246/20000], Loss: -21.315933227539062, Learning Rate: 0.005\n",
      "Epoch [18247/20000], Loss: -21.316009521484375, Learning Rate: 0.005\n",
      "Epoch [18248/20000], Loss: -21.3160400390625, Learning Rate: 0.005\n",
      "Epoch [18249/20000], Loss: -21.315994262695312, Learning Rate: 0.005\n",
      "Epoch [18250/20000], Loss: -21.316055297851562, Learning Rate: 0.005\n",
      "Epoch [18251/20000], Loss: -21.31610107421875, Learning Rate: 0.005\n",
      "Epoch [18252/20000], Loss: -21.316116333007812, Learning Rate: 0.005\n",
      "Epoch [18253/20000], Loss: -21.31610107421875, Learning Rate: 0.005\n",
      "Epoch [18254/20000], Loss: -21.316162109375, Learning Rate: 0.005\n",
      "Epoch [18255/20000], Loss: -21.316146850585938, Learning Rate: 0.005\n",
      "Epoch [18256/20000], Loss: -21.31622314453125, Learning Rate: 0.005\n",
      "Epoch [18257/20000], Loss: -21.3162841796875, Learning Rate: 0.005\n",
      "Epoch [18258/20000], Loss: -21.3162841796875, Learning Rate: 0.005\n",
      "Epoch [18259/20000], Loss: -21.31634521484375, Learning Rate: 0.005\n",
      "Epoch [18260/20000], Loss: -21.316375732421875, Learning Rate: 0.005\n",
      "Epoch [18261/20000], Loss: -21.31634521484375, Learning Rate: 0.005\n",
      "Epoch [18262/20000], Loss: -21.316360473632812, Learning Rate: 0.005\n",
      "Epoch [18263/20000], Loss: -21.31640625, Learning Rate: 0.005\n",
      "Epoch [18264/20000], Loss: -21.31646728515625, Learning Rate: 0.005\n",
      "Epoch [18265/20000], Loss: -21.316452026367188, Learning Rate: 0.005\n",
      "Epoch [18266/20000], Loss: -21.31646728515625, Learning Rate: 0.005\n",
      "Epoch [18267/20000], Loss: -21.316482543945312, Learning Rate: 0.005\n",
      "Epoch [18268/20000], Loss: -21.316543579101562, Learning Rate: 0.005\n",
      "Epoch [18269/20000], Loss: -21.3165283203125, Learning Rate: 0.005\n",
      "Epoch [18270/20000], Loss: -21.316558837890625, Learning Rate: 0.005\n",
      "Epoch [18271/20000], Loss: -21.316558837890625, Learning Rate: 0.005\n",
      "Epoch [18272/20000], Loss: -21.316680908203125, Learning Rate: 0.005\n",
      "Epoch [18273/20000], Loss: -21.316696166992188, Learning Rate: 0.005\n",
      "Epoch [18274/20000], Loss: -21.316696166992188, Learning Rate: 0.005\n",
      "Epoch [18275/20000], Loss: -21.316696166992188, Learning Rate: 0.005\n",
      "Epoch [18276/20000], Loss: -21.31671142578125, Learning Rate: 0.005\n",
      "Epoch [18277/20000], Loss: -21.316757202148438, Learning Rate: 0.005\n",
      "Epoch [18278/20000], Loss: -21.31671142578125, Learning Rate: 0.005\n",
      "Epoch [18279/20000], Loss: -21.316818237304688, Learning Rate: 0.005\n",
      "Epoch [18280/20000], Loss: -21.316787719726562, Learning Rate: 0.005\n",
      "Epoch [18281/20000], Loss: -21.316864013671875, Learning Rate: 0.005\n",
      "Epoch [18282/20000], Loss: -21.316879272460938, Learning Rate: 0.005\n",
      "Epoch [18283/20000], Loss: -21.316909790039062, Learning Rate: 0.005\n",
      "Epoch [18284/20000], Loss: -21.316879272460938, Learning Rate: 0.005\n",
      "Epoch [18285/20000], Loss: -21.316925048828125, Learning Rate: 0.005\n",
      "Epoch [18286/20000], Loss: -21.316940307617188, Learning Rate: 0.005\n",
      "Epoch [18287/20000], Loss: -21.316925048828125, Learning Rate: 0.005\n",
      "Epoch [18288/20000], Loss: -21.3170166015625, Learning Rate: 0.005\n",
      "Epoch [18289/20000], Loss: -21.31707763671875, Learning Rate: 0.005\n",
      "Epoch [18290/20000], Loss: -21.317031860351562, Learning Rate: 0.005\n",
      "Epoch [18291/20000], Loss: -21.317123413085938, Learning Rate: 0.005\n",
      "Epoch [18292/20000], Loss: -21.317062377929688, Learning Rate: 0.005\n",
      "Epoch [18293/20000], Loss: -21.317108154296875, Learning Rate: 0.005\n",
      "Epoch [18294/20000], Loss: -21.31719970703125, Learning Rate: 0.005\n",
      "Epoch [18295/20000], Loss: -21.31719970703125, Learning Rate: 0.005\n",
      "Epoch [18296/20000], Loss: -21.317230224609375, Learning Rate: 0.005\n",
      "Epoch [18297/20000], Loss: -21.31719970703125, Learning Rate: 0.005\n",
      "Epoch [18298/20000], Loss: -21.317230224609375, Learning Rate: 0.005\n",
      "Epoch [18299/20000], Loss: -21.317306518554688, Learning Rate: 0.005\n",
      "Epoch [18300/20000], Loss: -21.317306518554688, Learning Rate: 0.005\n",
      "Epoch [18301/20000], Loss: -21.31732177734375, Learning Rate: 0.005\n",
      "Epoch [18302/20000], Loss: -21.317306518554688, Learning Rate: 0.005\n",
      "Epoch [18303/20000], Loss: -21.31732177734375, Learning Rate: 0.005\n",
      "Epoch [18304/20000], Loss: -21.317428588867188, Learning Rate: 0.005\n",
      "Epoch [18305/20000], Loss: -21.317428588867188, Learning Rate: 0.005\n",
      "Epoch [18306/20000], Loss: -21.31744384765625, Learning Rate: 0.005\n",
      "Epoch [18307/20000], Loss: -21.317474365234375, Learning Rate: 0.005\n",
      "Epoch [18308/20000], Loss: -21.3175048828125, Learning Rate: 0.005\n",
      "Epoch [18309/20000], Loss: -21.317520141601562, Learning Rate: 0.005\n",
      "Epoch [18310/20000], Loss: -21.317581176757812, Learning Rate: 0.005\n",
      "Epoch [18311/20000], Loss: -21.317596435546875, Learning Rate: 0.005\n",
      "Epoch [18312/20000], Loss: -21.317581176757812, Learning Rate: 0.005\n",
      "Epoch [18313/20000], Loss: -21.317581176757812, Learning Rate: 0.005\n",
      "Epoch [18314/20000], Loss: -21.31768798828125, Learning Rate: 0.005\n",
      "Epoch [18315/20000], Loss: -21.317672729492188, Learning Rate: 0.005\n",
      "Epoch [18316/20000], Loss: -21.31768798828125, Learning Rate: 0.005\n",
      "Epoch [18317/20000], Loss: -21.317794799804688, Learning Rate: 0.005\n",
      "Epoch [18318/20000], Loss: -21.317764282226562, Learning Rate: 0.005\n",
      "Epoch [18319/20000], Loss: -21.317825317382812, Learning Rate: 0.005\n",
      "Epoch [18320/20000], Loss: -21.317840576171875, Learning Rate: 0.005\n",
      "Epoch [18321/20000], Loss: -21.317794799804688, Learning Rate: 0.005\n",
      "Epoch [18322/20000], Loss: -21.31787109375, Learning Rate: 0.005\n",
      "Epoch [18323/20000], Loss: -21.317901611328125, Learning Rate: 0.005\n",
      "Epoch [18324/20000], Loss: -21.317916870117188, Learning Rate: 0.005\n",
      "Epoch [18325/20000], Loss: -21.317916870117188, Learning Rate: 0.005\n",
      "Epoch [18326/20000], Loss: -21.317962646484375, Learning Rate: 0.005\n",
      "Epoch [18327/20000], Loss: -21.317916870117188, Learning Rate: 0.005\n",
      "Epoch [18328/20000], Loss: -21.317947387695312, Learning Rate: 0.005\n",
      "Epoch [18329/20000], Loss: -21.31805419921875, Learning Rate: 0.005\n",
      "Epoch [18330/20000], Loss: -21.318038940429688, Learning Rate: 0.005\n",
      "Epoch [18331/20000], Loss: -21.318069458007812, Learning Rate: 0.005\n",
      "Epoch [18332/20000], Loss: -21.318115234375, Learning Rate: 0.005\n",
      "Epoch [18333/20000], Loss: -21.318099975585938, Learning Rate: 0.005\n",
      "Epoch [18334/20000], Loss: -21.318161010742188, Learning Rate: 0.005\n",
      "Epoch [18335/20000], Loss: -21.318130493164062, Learning Rate: 0.005\n",
      "Epoch [18336/20000], Loss: -21.31817626953125, Learning Rate: 0.005\n",
      "Epoch [18337/20000], Loss: -21.31817626953125, Learning Rate: 0.005\n",
      "Epoch [18338/20000], Loss: -21.318252563476562, Learning Rate: 0.005\n",
      "Epoch [18339/20000], Loss: -21.318206787109375, Learning Rate: 0.005\n",
      "Epoch [18340/20000], Loss: -21.318267822265625, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18341/20000], Loss: -21.31829833984375, Learning Rate: 0.005\n",
      "Epoch [18342/20000], Loss: -21.318344116210938, Learning Rate: 0.005\n",
      "Epoch [18343/20000], Loss: -21.318374633789062, Learning Rate: 0.005\n",
      "Epoch [18344/20000], Loss: -21.318344116210938, Learning Rate: 0.005\n",
      "Epoch [18345/20000], Loss: -21.31842041015625, Learning Rate: 0.005\n",
      "Epoch [18346/20000], Loss: -21.318435668945312, Learning Rate: 0.005\n",
      "Epoch [18347/20000], Loss: -21.318496704101562, Learning Rate: 0.005\n",
      "Epoch [18348/20000], Loss: -21.31854248046875, Learning Rate: 0.005\n",
      "Epoch [18349/20000], Loss: -21.318527221679688, Learning Rate: 0.005\n",
      "Epoch [18350/20000], Loss: -21.318527221679688, Learning Rate: 0.005\n",
      "Epoch [18351/20000], Loss: -21.31854248046875, Learning Rate: 0.005\n",
      "Epoch [18352/20000], Loss: -21.318588256835938, Learning Rate: 0.005\n",
      "Epoch [18353/20000], Loss: -21.318679809570312, Learning Rate: 0.005\n",
      "Epoch [18354/20000], Loss: -21.318588256835938, Learning Rate: 0.005\n",
      "Epoch [18355/20000], Loss: -21.318618774414062, Learning Rate: 0.005\n",
      "Epoch [18356/20000], Loss: -21.318740844726562, Learning Rate: 0.005\n",
      "Epoch [18357/20000], Loss: -21.3187255859375, Learning Rate: 0.005\n",
      "Epoch [18358/20000], Loss: -21.318771362304688, Learning Rate: 0.005\n",
      "Epoch [18359/20000], Loss: -21.318771362304688, Learning Rate: 0.005\n",
      "Epoch [18360/20000], Loss: -21.31878662109375, Learning Rate: 0.005\n",
      "Epoch [18361/20000], Loss: -21.318801879882812, Learning Rate: 0.005\n",
      "Epoch [18362/20000], Loss: -21.31884765625, Learning Rate: 0.005\n",
      "Epoch [18363/20000], Loss: -21.318832397460938, Learning Rate: 0.005\n",
      "Epoch [18364/20000], Loss: -21.318862915039062, Learning Rate: 0.005\n",
      "Epoch [18365/20000], Loss: -21.31890869140625, Learning Rate: 0.005\n",
      "Epoch [18366/20000], Loss: -21.318954467773438, Learning Rate: 0.005\n",
      "Epoch [18367/20000], Loss: -21.3189697265625, Learning Rate: 0.005\n",
      "Epoch [18368/20000], Loss: -21.3189697265625, Learning Rate: 0.005\n",
      "Epoch [18369/20000], Loss: -21.31903076171875, Learning Rate: 0.005\n",
      "Epoch [18370/20000], Loss: -21.319046020507812, Learning Rate: 0.005\n",
      "Epoch [18371/20000], Loss: -21.319046020507812, Learning Rate: 0.005\n",
      "Epoch [18372/20000], Loss: -21.319076538085938, Learning Rate: 0.005\n",
      "Epoch [18373/20000], Loss: -21.319061279296875, Learning Rate: 0.005\n",
      "Epoch [18374/20000], Loss: -21.319168090820312, Learning Rate: 0.005\n",
      "Epoch [18375/20000], Loss: -21.319183349609375, Learning Rate: 0.005\n",
      "Epoch [18376/20000], Loss: -21.319183349609375, Learning Rate: 0.005\n",
      "Epoch [18377/20000], Loss: -21.319183349609375, Learning Rate: 0.005\n",
      "Epoch [18378/20000], Loss: -21.31927490234375, Learning Rate: 0.005\n",
      "Epoch [18379/20000], Loss: -21.31927490234375, Learning Rate: 0.005\n",
      "Epoch [18380/20000], Loss: -21.319290161132812, Learning Rate: 0.005\n",
      "Epoch [18381/20000], Loss: -21.319290161132812, Learning Rate: 0.005\n",
      "Epoch [18382/20000], Loss: -21.319381713867188, Learning Rate: 0.005\n",
      "Epoch [18383/20000], Loss: -21.319366455078125, Learning Rate: 0.005\n",
      "Epoch [18384/20000], Loss: -21.319351196289062, Learning Rate: 0.005\n",
      "Epoch [18385/20000], Loss: -21.319412231445312, Learning Rate: 0.005\n",
      "Epoch [18386/20000], Loss: -21.319488525390625, Learning Rate: 0.005\n",
      "Epoch [18387/20000], Loss: -21.3194580078125, Learning Rate: 0.005\n",
      "Epoch [18388/20000], Loss: -21.319488525390625, Learning Rate: 0.005\n",
      "Epoch [18389/20000], Loss: -21.319534301757812, Learning Rate: 0.005\n",
      "Epoch [18390/20000], Loss: -21.319580078125, Learning Rate: 0.005\n",
      "Epoch [18391/20000], Loss: -21.319595336914062, Learning Rate: 0.005\n",
      "Epoch [18392/20000], Loss: -21.319564819335938, Learning Rate: 0.005\n",
      "Epoch [18393/20000], Loss: -21.319656372070312, Learning Rate: 0.005\n",
      "Epoch [18394/20000], Loss: -21.31964111328125, Learning Rate: 0.005\n",
      "Epoch [18395/20000], Loss: -21.319656372070312, Learning Rate: 0.005\n",
      "Epoch [18396/20000], Loss: -21.319671630859375, Learning Rate: 0.005\n",
      "Epoch [18397/20000], Loss: -21.319717407226562, Learning Rate: 0.005\n",
      "Epoch [18398/20000], Loss: -21.319686889648438, Learning Rate: 0.005\n",
      "Epoch [18399/20000], Loss: -21.319717407226562, Learning Rate: 0.005\n",
      "Epoch [18400/20000], Loss: -21.319808959960938, Learning Rate: 0.005\n",
      "Epoch [18401/20000], Loss: -21.319793701171875, Learning Rate: 0.005\n",
      "Epoch [18402/20000], Loss: -21.319808959960938, Learning Rate: 0.005\n",
      "Epoch [18403/20000], Loss: -21.31988525390625, Learning Rate: 0.005\n",
      "Epoch [18404/20000], Loss: -21.319854736328125, Learning Rate: 0.005\n",
      "Epoch [18405/20000], Loss: -21.319931030273438, Learning Rate: 0.005\n",
      "Epoch [18406/20000], Loss: -21.3199462890625, Learning Rate: 0.005\n",
      "Epoch [18407/20000], Loss: -21.32000732421875, Learning Rate: 0.005\n",
      "Epoch [18408/20000], Loss: -21.320022583007812, Learning Rate: 0.005\n",
      "Epoch [18409/20000], Loss: -21.319976806640625, Learning Rate: 0.005\n",
      "Epoch [18410/20000], Loss: -21.320053100585938, Learning Rate: 0.005\n",
      "Epoch [18411/20000], Loss: -21.320037841796875, Learning Rate: 0.005\n",
      "Epoch [18412/20000], Loss: -21.320053100585938, Learning Rate: 0.005\n",
      "Epoch [18413/20000], Loss: -21.320114135742188, Learning Rate: 0.005\n",
      "Epoch [18414/20000], Loss: -21.320098876953125, Learning Rate: 0.005\n",
      "Epoch [18415/20000], Loss: -21.320144653320312, Learning Rate: 0.005\n",
      "Epoch [18416/20000], Loss: -21.320175170898438, Learning Rate: 0.005\n",
      "Epoch [18417/20000], Loss: -21.320220947265625, Learning Rate: 0.005\n",
      "Epoch [18418/20000], Loss: -21.320266723632812, Learning Rate: 0.005\n",
      "Epoch [18419/20000], Loss: -21.320236206054688, Learning Rate: 0.005\n",
      "Epoch [18420/20000], Loss: -21.320236206054688, Learning Rate: 0.005\n",
      "Epoch [18421/20000], Loss: -21.320343017578125, Learning Rate: 0.005\n",
      "Epoch [18422/20000], Loss: -21.320343017578125, Learning Rate: 0.005\n",
      "Epoch [18423/20000], Loss: -21.3203125, Learning Rate: 0.005\n",
      "Epoch [18424/20000], Loss: -21.320388793945312, Learning Rate: 0.005\n",
      "Epoch [18425/20000], Loss: -21.320419311523438, Learning Rate: 0.005\n",
      "Epoch [18426/20000], Loss: -21.320404052734375, Learning Rate: 0.005\n",
      "Epoch [18427/20000], Loss: -21.320526123046875, Learning Rate: 0.005\n",
      "Epoch [18428/20000], Loss: -21.320510864257812, Learning Rate: 0.005\n",
      "Epoch [18429/20000], Loss: -21.320510864257812, Learning Rate: 0.005\n",
      "Epoch [18430/20000], Loss: -21.320602416992188, Learning Rate: 0.005\n",
      "Epoch [18431/20000], Loss: -21.320571899414062, Learning Rate: 0.005\n",
      "Epoch [18432/20000], Loss: -21.32061767578125, Learning Rate: 0.005\n",
      "Epoch [18433/20000], Loss: -21.320663452148438, Learning Rate: 0.005\n",
      "Epoch [18434/20000], Loss: -21.32061767578125, Learning Rate: 0.005\n",
      "Epoch [18435/20000], Loss: -21.3206787109375, Learning Rate: 0.005\n",
      "Epoch [18436/20000], Loss: -21.320632934570312, Learning Rate: 0.005\n",
      "Epoch [18437/20000], Loss: -21.320693969726562, Learning Rate: 0.005\n",
      "Epoch [18438/20000], Loss: -21.3206787109375, Learning Rate: 0.005\n",
      "Epoch [18439/20000], Loss: -21.320755004882812, Learning Rate: 0.005\n",
      "Epoch [18440/20000], Loss: -21.320755004882812, Learning Rate: 0.005\n",
      "Epoch [18441/20000], Loss: -21.320770263671875, Learning Rate: 0.005\n",
      "Epoch [18442/20000], Loss: -21.32080078125, Learning Rate: 0.005\n",
      "Epoch [18443/20000], Loss: -21.320877075195312, Learning Rate: 0.005\n",
      "Epoch [18444/20000], Loss: -21.320846557617188, Learning Rate: 0.005\n",
      "Epoch [18445/20000], Loss: -21.320877075195312, Learning Rate: 0.005\n",
      "Epoch [18446/20000], Loss: -21.320877075195312, Learning Rate: 0.005\n",
      "Epoch [18447/20000], Loss: -21.320953369140625, Learning Rate: 0.005\n",
      "Epoch [18448/20000], Loss: -21.32098388671875, Learning Rate: 0.005\n",
      "Epoch [18449/20000], Loss: -21.32098388671875, Learning Rate: 0.005\n",
      "Epoch [18450/20000], Loss: -21.321014404296875, Learning Rate: 0.005\n",
      "Epoch [18451/20000], Loss: -21.321060180664062, Learning Rate: 0.005\n",
      "Epoch [18452/20000], Loss: -21.321075439453125, Learning Rate: 0.005\n",
      "Epoch [18453/20000], Loss: -21.321090698242188, Learning Rate: 0.005\n",
      "Epoch [18454/20000], Loss: -21.321182250976562, Learning Rate: 0.005\n",
      "Epoch [18455/20000], Loss: -21.321151733398438, Learning Rate: 0.005\n",
      "Epoch [18456/20000], Loss: -21.32122802734375, Learning Rate: 0.005\n",
      "Epoch [18457/20000], Loss: -21.321243286132812, Learning Rate: 0.005\n",
      "Epoch [18458/20000], Loss: -21.321304321289062, Learning Rate: 0.005\n",
      "Epoch [18459/20000], Loss: -21.321258544921875, Learning Rate: 0.005\n",
      "Epoch [18460/20000], Loss: -21.321243286132812, Learning Rate: 0.005\n",
      "Epoch [18461/20000], Loss: -21.3212890625, Learning Rate: 0.005\n",
      "Epoch [18462/20000], Loss: -21.321319580078125, Learning Rate: 0.005\n",
      "Epoch [18463/20000], Loss: -21.321304321289062, Learning Rate: 0.005\n",
      "Epoch [18464/20000], Loss: -21.32135009765625, Learning Rate: 0.005\n",
      "Epoch [18465/20000], Loss: -21.32147216796875, Learning Rate: 0.005\n",
      "Epoch [18466/20000], Loss: -21.321441650390625, Learning Rate: 0.005\n",
      "Epoch [18467/20000], Loss: -21.32147216796875, Learning Rate: 0.005\n",
      "Epoch [18468/20000], Loss: -21.321456909179688, Learning Rate: 0.005\n",
      "Epoch [18469/20000], Loss: -21.321563720703125, Learning Rate: 0.005\n",
      "Epoch [18470/20000], Loss: -21.321517944335938, Learning Rate: 0.005\n",
      "Epoch [18471/20000], Loss: -21.321609497070312, Learning Rate: 0.005\n",
      "Epoch [18472/20000], Loss: -21.321533203125, Learning Rate: 0.005\n",
      "Epoch [18473/20000], Loss: -21.32159423828125, Learning Rate: 0.005\n",
      "Epoch [18474/20000], Loss: -21.32159423828125, Learning Rate: 0.005\n",
      "Epoch [18475/20000], Loss: -21.321670532226562, Learning Rate: 0.005\n",
      "Epoch [18476/20000], Loss: -21.321746826171875, Learning Rate: 0.005\n",
      "Epoch [18477/20000], Loss: -21.321731567382812, Learning Rate: 0.005\n",
      "Epoch [18478/20000], Loss: -21.321792602539062, Learning Rate: 0.005\n",
      "Epoch [18479/20000], Loss: -21.321746826171875, Learning Rate: 0.005\n",
      "Epoch [18480/20000], Loss: -21.321807861328125, Learning Rate: 0.005\n",
      "Epoch [18481/20000], Loss: -21.321823120117188, Learning Rate: 0.005\n",
      "Epoch [18482/20000], Loss: -21.32183837890625, Learning Rate: 0.005\n",
      "Epoch [18483/20000], Loss: -21.321914672851562, Learning Rate: 0.005\n",
      "Epoch [18484/20000], Loss: -21.321929931640625, Learning Rate: 0.005\n",
      "Epoch [18485/20000], Loss: -21.32196044921875, Learning Rate: 0.005\n",
      "Epoch [18486/20000], Loss: -21.32196044921875, Learning Rate: 0.005\n",
      "Epoch [18487/20000], Loss: -21.321990966796875, Learning Rate: 0.005\n",
      "Epoch [18488/20000], Loss: -21.322006225585938, Learning Rate: 0.005\n",
      "Epoch [18489/20000], Loss: -21.322006225585938, Learning Rate: 0.005\n",
      "Epoch [18490/20000], Loss: -21.322036743164062, Learning Rate: 0.005\n",
      "Epoch [18491/20000], Loss: -21.322097778320312, Learning Rate: 0.005\n",
      "Epoch [18492/20000], Loss: -21.322113037109375, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18493/20000], Loss: -21.322097778320312, Learning Rate: 0.005\n",
      "Epoch [18494/20000], Loss: -21.32220458984375, Learning Rate: 0.005\n",
      "Epoch [18495/20000], Loss: -21.3221435546875, Learning Rate: 0.005\n",
      "Epoch [18496/20000], Loss: -21.322235107421875, Learning Rate: 0.005\n",
      "Epoch [18497/20000], Loss: -21.322250366210938, Learning Rate: 0.005\n",
      "Epoch [18498/20000], Loss: -21.322219848632812, Learning Rate: 0.005\n",
      "Epoch [18499/20000], Loss: -21.322265625, Learning Rate: 0.005\n",
      "Epoch [18500/20000], Loss: -21.322250366210938, Learning Rate: 0.005\n",
      "Epoch [18501/20000], Loss: -21.32232666015625, Learning Rate: 0.005\n",
      "Epoch [18502/20000], Loss: -21.322341918945312, Learning Rate: 0.005\n",
      "Epoch [18503/20000], Loss: -21.322418212890625, Learning Rate: 0.005\n",
      "Epoch [18504/20000], Loss: -21.32244873046875, Learning Rate: 0.005\n",
      "Epoch [18505/20000], Loss: -21.322433471679688, Learning Rate: 0.005\n",
      "Epoch [18506/20000], Loss: -21.322479248046875, Learning Rate: 0.005\n",
      "Epoch [18507/20000], Loss: -21.322509765625, Learning Rate: 0.005\n",
      "Epoch [18508/20000], Loss: -21.322494506835938, Learning Rate: 0.005\n",
      "Epoch [18509/20000], Loss: -21.322601318359375, Learning Rate: 0.005\n",
      "Epoch [18510/20000], Loss: -21.322601318359375, Learning Rate: 0.005\n",
      "Epoch [18511/20000], Loss: -21.322616577148438, Learning Rate: 0.005\n",
      "Epoch [18512/20000], Loss: -21.322616577148438, Learning Rate: 0.005\n",
      "Epoch [18513/20000], Loss: -21.322708129882812, Learning Rate: 0.005\n",
      "Epoch [18514/20000], Loss: -21.322708129882812, Learning Rate: 0.005\n",
      "Epoch [18515/20000], Loss: -21.322647094726562, Learning Rate: 0.005\n",
      "Epoch [18516/20000], Loss: -21.322738647460938, Learning Rate: 0.005\n",
      "Epoch [18517/20000], Loss: -21.322769165039062, Learning Rate: 0.005\n",
      "Epoch [18518/20000], Loss: -21.32275390625, Learning Rate: 0.005\n",
      "Epoch [18519/20000], Loss: -21.322769165039062, Learning Rate: 0.005\n",
      "Epoch [18520/20000], Loss: -21.322845458984375, Learning Rate: 0.005\n",
      "Epoch [18521/20000], Loss: -21.322860717773438, Learning Rate: 0.005\n",
      "Epoch [18522/20000], Loss: -21.3228759765625, Learning Rate: 0.005\n",
      "Epoch [18523/20000], Loss: -21.322906494140625, Learning Rate: 0.005\n",
      "Epoch [18524/20000], Loss: -21.322891235351562, Learning Rate: 0.005\n",
      "Epoch [18525/20000], Loss: -21.322982788085938, Learning Rate: 0.005\n",
      "Epoch [18526/20000], Loss: -21.322952270507812, Learning Rate: 0.005\n",
      "Epoch [18527/20000], Loss: -21.322998046875, Learning Rate: 0.005\n",
      "Epoch [18528/20000], Loss: -21.322998046875, Learning Rate: 0.005\n",
      "Epoch [18529/20000], Loss: -21.322998046875, Learning Rate: 0.005\n",
      "Epoch [18530/20000], Loss: -21.322998046875, Learning Rate: 0.005\n",
      "Epoch [18531/20000], Loss: -21.3228759765625, Learning Rate: 0.005\n",
      "Epoch [18532/20000], Loss: -21.322784423828125, Learning Rate: 0.005\n",
      "Epoch [18533/20000], Loss: -21.322479248046875, Learning Rate: 0.005\n",
      "Epoch [18534/20000], Loss: -21.322128295898438, Learning Rate: 0.005\n",
      "Epoch [18535/20000], Loss: -21.321212768554688, Learning Rate: 0.005\n",
      "Epoch [18536/20000], Loss: -21.319778442382812, Learning Rate: 0.005\n",
      "Epoch [18537/20000], Loss: -21.317047119140625, Learning Rate: 0.005\n",
      "Epoch [18538/20000], Loss: -21.312179565429688, Learning Rate: 0.005\n",
      "Epoch [18539/20000], Loss: -21.3037109375, Learning Rate: 0.005\n",
      "Epoch [18540/20000], Loss: -21.289169311523438, Learning Rate: 0.005\n",
      "Epoch [18541/20000], Loss: -21.265243530273438, Learning Rate: 0.005\n",
      "Epoch [18542/20000], Loss: -21.228363037109375, Learning Rate: 0.005\n",
      "Epoch [18543/20000], Loss: -21.176193237304688, Learning Rate: 0.005\n",
      "Epoch [18544/20000], Loss: -21.1119384765625, Learning Rate: 0.005\n",
      "Epoch [18545/20000], Loss: -21.0494384765625, Learning Rate: 0.005\n",
      "Epoch [18546/20000], Loss: -21.015701293945312, Learning Rate: 0.005\n",
      "Epoch [18547/20000], Loss: -21.039016723632812, Learning Rate: 0.005\n",
      "Epoch [18548/20000], Loss: -21.124038696289062, Learning Rate: 0.005\n",
      "Epoch [18549/20000], Loss: -21.232009887695312, Learning Rate: 0.005\n",
      "Epoch [18550/20000], Loss: -21.305831909179688, Learning Rate: 0.005\n",
      "Epoch [18551/20000], Loss: -21.314285278320312, Learning Rate: 0.005\n",
      "Epoch [18552/20000], Loss: -21.2723388671875, Learning Rate: 0.005\n",
      "Epoch [18553/20000], Loss: -21.222000122070312, Learning Rate: 0.005\n",
      "Epoch [18554/20000], Loss: -21.20196533203125, Learning Rate: 0.005\n",
      "Epoch [18555/20000], Loss: -21.224044799804688, Learning Rate: 0.005\n",
      "Epoch [18556/20000], Loss: -21.268798828125, Learning Rate: 0.005\n",
      "Epoch [18557/20000], Loss: -21.304275512695312, Learning Rate: 0.005\n",
      "Epoch [18558/20000], Loss: -21.3116455078125, Learning Rate: 0.005\n",
      "Epoch [18559/20000], Loss: -21.296905517578125, Learning Rate: 0.005\n",
      "Epoch [18560/20000], Loss: -21.279525756835938, Learning Rate: 0.005\n",
      "Epoch [18561/20000], Loss: -21.275314331054688, Learning Rate: 0.005\n",
      "Epoch [18562/20000], Loss: -21.285736083984375, Learning Rate: 0.005\n",
      "Epoch [18563/20000], Loss: -21.30047607421875, Learning Rate: 0.005\n",
      "Epoch [18564/20000], Loss: -21.3094482421875, Learning Rate: 0.005\n",
      "Epoch [18565/20000], Loss: -21.3101806640625, Learning Rate: 0.005\n",
      "Epoch [18566/20000], Loss: -21.3072509765625, Learning Rate: 0.005\n",
      "Epoch [18567/20000], Loss: -21.305694580078125, Learning Rate: 0.005\n",
      "Epoch [18568/20000], Loss: -21.307037353515625, Learning Rate: 0.005\n",
      "Epoch [18569/20000], Loss: -21.309417724609375, Learning Rate: 0.005\n",
      "Epoch [18570/20000], Loss: -21.31097412109375, Learning Rate: 0.005\n",
      "Epoch [18571/20000], Loss: -21.311965942382812, Learning Rate: 0.005\n",
      "Epoch [18572/20000], Loss: -21.3138427734375, Learning Rate: 0.005\n",
      "Epoch [18573/20000], Loss: -21.316604614257812, Learning Rate: 0.005\n",
      "Epoch [18574/20000], Loss: -21.3187255859375, Learning Rate: 0.005\n",
      "Epoch [18575/20000], Loss: -21.31884765625, Learning Rate: 0.005\n",
      "Epoch [18576/20000], Loss: -21.317062377929688, Learning Rate: 0.005\n",
      "Epoch [18577/20000], Loss: -21.3154296875, Learning Rate: 0.005\n",
      "Epoch [18578/20000], Loss: -21.315902709960938, Learning Rate: 0.005\n",
      "Epoch [18579/20000], Loss: -21.318588256835938, Learning Rate: 0.005\n",
      "Epoch [18580/20000], Loss: -21.321807861328125, Learning Rate: 0.005\n",
      "Epoch [18581/20000], Loss: -21.323379516601562, Learning Rate: 0.005\n",
      "Epoch [18582/20000], Loss: -21.322540283203125, Learning Rate: 0.005\n",
      "Epoch [18583/20000], Loss: -21.320419311523438, Learning Rate: 0.005\n",
      "Epoch [18584/20000], Loss: -21.319046020507812, Learning Rate: 0.005\n",
      "Epoch [18585/20000], Loss: -21.319625854492188, Learning Rate: 0.005\n",
      "Epoch [18586/20000], Loss: -21.3216552734375, Learning Rate: 0.005\n",
      "Epoch [18587/20000], Loss: -21.32366943359375, Learning Rate: 0.005\n",
      "Epoch [18588/20000], Loss: -21.324478149414062, Learning Rate: 0.005\n",
      "Epoch [18589/20000], Loss: -21.323867797851562, Learning Rate: 0.005\n",
      "Epoch [18590/20000], Loss: -21.322662353515625, Learning Rate: 0.005\n",
      "Epoch [18591/20000], Loss: -21.321868896484375, Learning Rate: 0.005\n",
      "Epoch [18592/20000], Loss: -21.322036743164062, Learning Rate: 0.005\n",
      "Epoch [18593/20000], Loss: -21.323043823242188, Learning Rate: 0.005\n",
      "Epoch [18594/20000], Loss: -21.324111938476562, Learning Rate: 0.005\n",
      "Epoch [18595/20000], Loss: -21.324600219726562, Learning Rate: 0.005\n",
      "Epoch [18596/20000], Loss: -21.324447631835938, Learning Rate: 0.005\n",
      "Epoch [18597/20000], Loss: -21.32403564453125, Learning Rate: 0.005\n",
      "Epoch [18598/20000], Loss: -21.323562622070312, Learning Rate: 0.005\n",
      "Epoch [18599/20000], Loss: -21.32366943359375, Learning Rate: 0.005\n",
      "Epoch [18600/20000], Loss: -21.323959350585938, Learning Rate: 0.005\n",
      "Epoch [18601/20000], Loss: -21.324371337890625, Learning Rate: 0.005\n",
      "Epoch [18602/20000], Loss: -21.3245849609375, Learning Rate: 0.005\n",
      "Epoch [18603/20000], Loss: -21.324676513671875, Learning Rate: 0.005\n",
      "Epoch [18604/20000], Loss: -21.324661254882812, Learning Rate: 0.005\n",
      "Epoch [18605/20000], Loss: -21.324615478515625, Learning Rate: 0.005\n",
      "Epoch [18606/20000], Loss: -21.3245849609375, Learning Rate: 0.005\n",
      "Epoch [18607/20000], Loss: -21.324722290039062, Learning Rate: 0.005\n",
      "Epoch [18608/20000], Loss: -21.32470703125, Learning Rate: 0.005\n",
      "Epoch [18609/20000], Loss: -21.3248291015625, Learning Rate: 0.005\n",
      "Epoch [18610/20000], Loss: -21.3248291015625, Learning Rate: 0.005\n",
      "Epoch [18611/20000], Loss: -21.3248291015625, Learning Rate: 0.005\n",
      "Epoch [18612/20000], Loss: -21.324874877929688, Learning Rate: 0.005\n",
      "Epoch [18613/20000], Loss: -21.324996948242188, Learning Rate: 0.005\n",
      "Epoch [18614/20000], Loss: -21.325088500976562, Learning Rate: 0.005\n",
      "Epoch [18615/20000], Loss: -21.325119018554688, Learning Rate: 0.005\n",
      "Epoch [18616/20000], Loss: -21.325103759765625, Learning Rate: 0.005\n",
      "Epoch [18617/20000], Loss: -21.325088500976562, Learning Rate: 0.005\n",
      "Epoch [18618/20000], Loss: -21.325103759765625, Learning Rate: 0.005\n",
      "Epoch [18619/20000], Loss: -21.325103759765625, Learning Rate: 0.005\n",
      "Epoch [18620/20000], Loss: -21.3251953125, Learning Rate: 0.005\n",
      "Epoch [18621/20000], Loss: -21.3253173828125, Learning Rate: 0.005\n",
      "Epoch [18622/20000], Loss: -21.325347900390625, Learning Rate: 0.005\n",
      "Epoch [18623/20000], Loss: -21.325393676757812, Learning Rate: 0.005\n",
      "Epoch [18624/20000], Loss: -21.325439453125, Learning Rate: 0.005\n",
      "Epoch [18625/20000], Loss: -21.32537841796875, Learning Rate: 0.005\n",
      "Epoch [18626/20000], Loss: -21.325424194335938, Learning Rate: 0.005\n",
      "Epoch [18627/20000], Loss: -21.32550048828125, Learning Rate: 0.005\n",
      "Epoch [18628/20000], Loss: -21.325439453125, Learning Rate: 0.005\n",
      "Epoch [18629/20000], Loss: -21.325531005859375, Learning Rate: 0.005\n",
      "Epoch [18630/20000], Loss: -21.3255615234375, Learning Rate: 0.005\n",
      "Epoch [18631/20000], Loss: -21.32562255859375, Learning Rate: 0.005\n",
      "Epoch [18632/20000], Loss: -21.32568359375, Learning Rate: 0.005\n",
      "Epoch [18633/20000], Loss: -21.325653076171875, Learning Rate: 0.005\n",
      "Epoch [18634/20000], Loss: -21.32568359375, Learning Rate: 0.005\n",
      "Epoch [18635/20000], Loss: -21.325653076171875, Learning Rate: 0.005\n",
      "Epoch [18636/20000], Loss: -21.325668334960938, Learning Rate: 0.005\n",
      "Epoch [18637/20000], Loss: -21.325759887695312, Learning Rate: 0.005\n",
      "Epoch [18638/20000], Loss: -21.325775146484375, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18639/20000], Loss: -21.3258056640625, Learning Rate: 0.005\n",
      "Epoch [18640/20000], Loss: -21.325836181640625, Learning Rate: 0.005\n",
      "Epoch [18641/20000], Loss: -21.325836181640625, Learning Rate: 0.005\n",
      "Epoch [18642/20000], Loss: -21.32586669921875, Learning Rate: 0.005\n",
      "Epoch [18643/20000], Loss: -21.325927734375, Learning Rate: 0.005\n",
      "Epoch [18644/20000], Loss: -21.325958251953125, Learning Rate: 0.005\n",
      "Epoch [18645/20000], Loss: -21.325927734375, Learning Rate: 0.005\n",
      "Epoch [18646/20000], Loss: -21.32598876953125, Learning Rate: 0.005\n",
      "Epoch [18647/20000], Loss: -21.3260498046875, Learning Rate: 0.005\n",
      "Epoch [18648/20000], Loss: -21.326095581054688, Learning Rate: 0.005\n",
      "Epoch [18649/20000], Loss: -21.326126098632812, Learning Rate: 0.005\n",
      "Epoch [18650/20000], Loss: -21.326034545898438, Learning Rate: 0.005\n",
      "Epoch [18651/20000], Loss: -21.326095581054688, Learning Rate: 0.005\n",
      "Epoch [18652/20000], Loss: -21.326171875, Learning Rate: 0.005\n",
      "Epoch [18653/20000], Loss: -21.326171875, Learning Rate: 0.005\n",
      "Epoch [18654/20000], Loss: -21.32623291015625, Learning Rate: 0.005\n",
      "Epoch [18655/20000], Loss: -21.326263427734375, Learning Rate: 0.005\n",
      "Epoch [18656/20000], Loss: -21.326202392578125, Learning Rate: 0.005\n",
      "Epoch [18657/20000], Loss: -21.3262939453125, Learning Rate: 0.005\n",
      "Epoch [18658/20000], Loss: -21.326339721679688, Learning Rate: 0.005\n",
      "Epoch [18659/20000], Loss: -21.326278686523438, Learning Rate: 0.005\n",
      "Epoch [18660/20000], Loss: -21.326370239257812, Learning Rate: 0.005\n",
      "Epoch [18661/20000], Loss: -21.326339721679688, Learning Rate: 0.005\n",
      "Epoch [18662/20000], Loss: -21.326370239257812, Learning Rate: 0.005\n",
      "Epoch [18663/20000], Loss: -21.326431274414062, Learning Rate: 0.005\n",
      "Epoch [18664/20000], Loss: -21.326431274414062, Learning Rate: 0.005\n",
      "Epoch [18665/20000], Loss: -21.326492309570312, Learning Rate: 0.005\n",
      "Epoch [18666/20000], Loss: -21.326507568359375, Learning Rate: 0.005\n",
      "Epoch [18667/20000], Loss: -21.326522827148438, Learning Rate: 0.005\n",
      "Epoch [18668/20000], Loss: -21.326507568359375, Learning Rate: 0.005\n",
      "Epoch [18669/20000], Loss: -21.326614379882812, Learning Rate: 0.005\n",
      "Epoch [18670/20000], Loss: -21.326629638671875, Learning Rate: 0.005\n",
      "Epoch [18671/20000], Loss: -21.326583862304688, Learning Rate: 0.005\n",
      "Epoch [18672/20000], Loss: -21.326690673828125, Learning Rate: 0.005\n",
      "Epoch [18673/20000], Loss: -21.326751708984375, Learning Rate: 0.005\n",
      "Epoch [18674/20000], Loss: -21.32672119140625, Learning Rate: 0.005\n",
      "Epoch [18675/20000], Loss: -21.326705932617188, Learning Rate: 0.005\n",
      "Epoch [18676/20000], Loss: -21.326705932617188, Learning Rate: 0.005\n",
      "Epoch [18677/20000], Loss: -21.326705932617188, Learning Rate: 0.005\n",
      "Epoch [18678/20000], Loss: -21.326828002929688, Learning Rate: 0.005\n",
      "Epoch [18679/20000], Loss: -21.326828002929688, Learning Rate: 0.005\n",
      "Epoch [18680/20000], Loss: -21.32684326171875, Learning Rate: 0.005\n",
      "Epoch [18681/20000], Loss: -21.326904296875, Learning Rate: 0.005\n",
      "Epoch [18682/20000], Loss: -21.326934814453125, Learning Rate: 0.005\n",
      "Epoch [18683/20000], Loss: -21.326889038085938, Learning Rate: 0.005\n",
      "Epoch [18684/20000], Loss: -21.326950073242188, Learning Rate: 0.005\n",
      "Epoch [18685/20000], Loss: -21.3270263671875, Learning Rate: 0.005\n",
      "Epoch [18686/20000], Loss: -21.327056884765625, Learning Rate: 0.005\n",
      "Epoch [18687/20000], Loss: -21.32708740234375, Learning Rate: 0.005\n",
      "Epoch [18688/20000], Loss: -21.327072143554688, Learning Rate: 0.005\n",
      "Epoch [18689/20000], Loss: -21.327133178710938, Learning Rate: 0.005\n",
      "Epoch [18690/20000], Loss: -21.327117919921875, Learning Rate: 0.005\n",
      "Epoch [18691/20000], Loss: -21.327102661132812, Learning Rate: 0.005\n",
      "Epoch [18692/20000], Loss: -21.3271484375, Learning Rate: 0.005\n",
      "Epoch [18693/20000], Loss: -21.3271484375, Learning Rate: 0.005\n",
      "Epoch [18694/20000], Loss: -21.327102661132812, Learning Rate: 0.005\n",
      "Epoch [18695/20000], Loss: -21.327163696289062, Learning Rate: 0.005\n",
      "Epoch [18696/20000], Loss: -21.327178955078125, Learning Rate: 0.005\n",
      "Epoch [18697/20000], Loss: -21.327133178710938, Learning Rate: 0.005\n",
      "Epoch [18698/20000], Loss: -21.327102661132812, Learning Rate: 0.005\n",
      "Epoch [18699/20000], Loss: -21.32696533203125, Learning Rate: 0.005\n",
      "Epoch [18700/20000], Loss: -21.32696533203125, Learning Rate: 0.005\n",
      "Epoch [18701/20000], Loss: -21.326690673828125, Learning Rate: 0.005\n",
      "Epoch [18702/20000], Loss: -21.326339721679688, Learning Rate: 0.005\n",
      "Epoch [18703/20000], Loss: -21.325897216796875, Learning Rate: 0.005\n",
      "Epoch [18704/20000], Loss: -21.325042724609375, Learning Rate: 0.005\n",
      "Epoch [18705/20000], Loss: -21.323806762695312, Learning Rate: 0.005\n",
      "Epoch [18706/20000], Loss: -21.32177734375, Learning Rate: 0.005\n",
      "Epoch [18707/20000], Loss: -21.318634033203125, Learning Rate: 0.005\n",
      "Epoch [18708/20000], Loss: -21.313613891601562, Learning Rate: 0.005\n",
      "Epoch [18709/20000], Loss: -21.305648803710938, Learning Rate: 0.005\n",
      "Epoch [18710/20000], Loss: -21.2928466796875, Learning Rate: 0.005\n",
      "Epoch [18711/20000], Loss: -21.272552490234375, Learning Rate: 0.005\n",
      "Epoch [18712/20000], Loss: -21.240234375, Learning Rate: 0.005\n",
      "Epoch [18713/20000], Loss: -21.190399169921875, Learning Rate: 0.005\n",
      "Epoch [18714/20000], Loss: -21.114517211914062, Learning Rate: 0.005\n",
      "Epoch [18715/20000], Loss: -21.007553100585938, Learning Rate: 0.005\n",
      "Epoch [18716/20000], Loss: -20.86578369140625, Learning Rate: 0.005\n",
      "Epoch [18717/20000], Loss: -20.715850830078125, Learning Rate: 0.005\n",
      "Epoch [18718/20000], Loss: -20.598770141601562, Learning Rate: 0.005\n",
      "Epoch [18719/20000], Loss: -20.606842041015625, Learning Rate: 0.005\n",
      "Epoch [18720/20000], Loss: -20.768341064453125, Learning Rate: 0.005\n",
      "Epoch [18721/20000], Loss: -21.040908813476562, Learning Rate: 0.005\n",
      "Epoch [18722/20000], Loss: -21.265518188476562, Learning Rate: 0.005\n",
      "Epoch [18723/20000], Loss: -21.324417114257812, Learning Rate: 0.005\n",
      "Epoch [18724/20000], Loss: -21.224594116210938, Learning Rate: 0.005\n",
      "Epoch [18725/20000], Loss: -21.081146240234375, Learning Rate: 0.005\n",
      "Epoch [18726/20000], Loss: -21.024307250976562, Learning Rate: 0.005\n",
      "Epoch [18727/20000], Loss: -21.096420288085938, Learning Rate: 0.005\n",
      "Epoch [18728/20000], Loss: -21.23492431640625, Learning Rate: 0.005\n",
      "Epoch [18729/20000], Loss: -21.322128295898438, Learning Rate: 0.005\n",
      "Epoch [18730/20000], Loss: -21.30389404296875, Learning Rate: 0.005\n",
      "Epoch [18731/20000], Loss: -21.225570678710938, Learning Rate: 0.005\n",
      "Epoch [18732/20000], Loss: -21.17486572265625, Learning Rate: 0.005\n",
      "Epoch [18733/20000], Loss: -21.201004028320312, Learning Rate: 0.005\n",
      "Epoch [18734/20000], Loss: -21.273101806640625, Learning Rate: 0.005\n",
      "Epoch [18735/20000], Loss: -21.3238525390625, Learning Rate: 0.005\n",
      "Epoch [18736/20000], Loss: -21.315872192382812, Learning Rate: 0.005\n",
      "Epoch [18737/20000], Loss: -21.273086547851562, Learning Rate: 0.005\n",
      "Epoch [18738/20000], Loss: -21.246566772460938, Learning Rate: 0.005\n",
      "Epoch [18739/20000], Loss: -21.261932373046875, Learning Rate: 0.005\n",
      "Epoch [18740/20000], Loss: -21.301513671875, Learning Rate: 0.005\n",
      "Epoch [18741/20000], Loss: -21.326828002929688, Learning Rate: 0.005\n",
      "Epoch [18742/20000], Loss: -21.320022583007812, Learning Rate: 0.005\n",
      "Epoch [18743/20000], Loss: -21.296401977539062, Learning Rate: 0.005\n",
      "Epoch [18744/20000], Loss: -21.283721923828125, Learning Rate: 0.005\n",
      "Epoch [18745/20000], Loss: -21.2943115234375, Learning Rate: 0.005\n",
      "Epoch [18746/20000], Loss: -21.315841674804688, Learning Rate: 0.005\n",
      "Epoch [18747/20000], Loss: -21.328109741210938, Learning Rate: 0.005\n",
      "Epoch [18748/20000], Loss: -21.32293701171875, Learning Rate: 0.005\n",
      "Epoch [18749/20000], Loss: -21.310104370117188, Learning Rate: 0.005\n",
      "Epoch [18750/20000], Loss: -21.304275512695312, Learning Rate: 0.005\n",
      "Epoch [18751/20000], Loss: -21.310699462890625, Learning Rate: 0.005\n",
      "Epoch [18752/20000], Loss: -21.322235107421875, Learning Rate: 0.005\n",
      "Epoch [18753/20000], Loss: -21.328460693359375, Learning Rate: 0.005\n",
      "Epoch [18754/20000], Loss: -21.32550048828125, Learning Rate: 0.005\n",
      "Epoch [18755/20000], Loss: -21.318634033203125, Learning Rate: 0.005\n",
      "Epoch [18756/20000], Loss: -21.31549072265625, Learning Rate: 0.005\n",
      "Epoch [18757/20000], Loss: -21.318893432617188, Learning Rate: 0.005\n",
      "Epoch [18758/20000], Loss: -21.325103759765625, Learning Rate: 0.005\n",
      "Epoch [18759/20000], Loss: -21.328582763671875, Learning Rate: 0.005\n",
      "Epoch [18760/20000], Loss: -21.327239990234375, Learning Rate: 0.005\n",
      "Epoch [18761/20000], Loss: -21.323699951171875, Learning Rate: 0.005\n",
      "Epoch [18762/20000], Loss: -21.321640014648438, Learning Rate: 0.005\n",
      "Epoch [18763/20000], Loss: -21.323196411132812, Learning Rate: 0.005\n",
      "Epoch [18764/20000], Loss: -21.326385498046875, Learning Rate: 0.005\n",
      "Epoch [18765/20000], Loss: -21.328643798828125, Learning Rate: 0.005\n",
      "Epoch [18766/20000], Loss: -21.328399658203125, Learning Rate: 0.005\n",
      "Epoch [18767/20000], Loss: -21.326553344726562, Learning Rate: 0.005\n",
      "Epoch [18768/20000], Loss: -21.325286865234375, Learning Rate: 0.005\n",
      "Epoch [18769/20000], Loss: -21.325607299804688, Learning Rate: 0.005\n",
      "Epoch [18770/20000], Loss: -21.327224731445312, Learning Rate: 0.005\n",
      "Epoch [18771/20000], Loss: -21.328659057617188, Learning Rate: 0.005\n",
      "Epoch [18772/20000], Loss: -21.328903198242188, Learning Rate: 0.005\n",
      "Epoch [18773/20000], Loss: -21.328155517578125, Learning Rate: 0.005\n",
      "Epoch [18774/20000], Loss: -21.32733154296875, Learning Rate: 0.005\n",
      "Epoch [18775/20000], Loss: -21.3271484375, Learning Rate: 0.005\n",
      "Epoch [18776/20000], Loss: -21.327850341796875, Learning Rate: 0.005\n",
      "Epoch [18777/20000], Loss: -21.328659057617188, Learning Rate: 0.005\n",
      "Epoch [18778/20000], Loss: -21.329132080078125, Learning Rate: 0.005\n",
      "Epoch [18779/20000], Loss: -21.3289794921875, Learning Rate: 0.005\n",
      "Epoch [18780/20000], Loss: -21.328475952148438, Learning Rate: 0.005\n",
      "Epoch [18781/20000], Loss: -21.32818603515625, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18782/20000], Loss: -21.328292846679688, Learning Rate: 0.005\n",
      "Epoch [18783/20000], Loss: -21.328811645507812, Learning Rate: 0.005\n",
      "Epoch [18784/20000], Loss: -21.329193115234375, Learning Rate: 0.005\n",
      "Epoch [18785/20000], Loss: -21.32928466796875, Learning Rate: 0.005\n",
      "Epoch [18786/20000], Loss: -21.32916259765625, Learning Rate: 0.005\n",
      "Epoch [18787/20000], Loss: -21.328903198242188, Learning Rate: 0.005\n",
      "Epoch [18788/20000], Loss: -21.328903198242188, Learning Rate: 0.005\n",
      "Epoch [18789/20000], Loss: -21.329010009765625, Learning Rate: 0.005\n",
      "Epoch [18790/20000], Loss: -21.329208374023438, Learning Rate: 0.005\n",
      "Epoch [18791/20000], Loss: -21.329345703125, Learning Rate: 0.005\n",
      "Epoch [18792/20000], Loss: -21.329437255859375, Learning Rate: 0.005\n",
      "Epoch [18793/20000], Loss: -21.329376220703125, Learning Rate: 0.005\n",
      "Epoch [18794/20000], Loss: -21.329345703125, Learning Rate: 0.005\n",
      "Epoch [18795/20000], Loss: -21.329315185546875, Learning Rate: 0.005\n",
      "Epoch [18796/20000], Loss: -21.329330444335938, Learning Rate: 0.005\n",
      "Epoch [18797/20000], Loss: -21.329483032226562, Learning Rate: 0.005\n",
      "Epoch [18798/20000], Loss: -21.329605102539062, Learning Rate: 0.005\n",
      "Epoch [18799/20000], Loss: -21.32958984375, Learning Rate: 0.005\n",
      "Epoch [18800/20000], Loss: -21.329666137695312, Learning Rate: 0.005\n",
      "Epoch [18801/20000], Loss: -21.329544067382812, Learning Rate: 0.005\n",
      "Epoch [18802/20000], Loss: -21.329620361328125, Learning Rate: 0.005\n",
      "Epoch [18803/20000], Loss: -21.329635620117188, Learning Rate: 0.005\n",
      "Epoch [18804/20000], Loss: -21.329681396484375, Learning Rate: 0.005\n",
      "Epoch [18805/20000], Loss: -21.329696655273438, Learning Rate: 0.005\n",
      "Epoch [18806/20000], Loss: -21.329803466796875, Learning Rate: 0.005\n",
      "Epoch [18807/20000], Loss: -21.329833984375, Learning Rate: 0.005\n",
      "Epoch [18808/20000], Loss: -21.329818725585938, Learning Rate: 0.005\n",
      "Epoch [18809/20000], Loss: -21.329803466796875, Learning Rate: 0.005\n",
      "Epoch [18810/20000], Loss: -21.329788208007812, Learning Rate: 0.005\n",
      "Epoch [18811/20000], Loss: -21.329879760742188, Learning Rate: 0.005\n",
      "Epoch [18812/20000], Loss: -21.329910278320312, Learning Rate: 0.005\n",
      "Epoch [18813/20000], Loss: -21.3299560546875, Learning Rate: 0.005\n",
      "Epoch [18814/20000], Loss: -21.329971313476562, Learning Rate: 0.005\n",
      "Epoch [18815/20000], Loss: -21.33001708984375, Learning Rate: 0.005\n",
      "Epoch [18816/20000], Loss: -21.330032348632812, Learning Rate: 0.005\n",
      "Epoch [18817/20000], Loss: -21.330078125, Learning Rate: 0.005\n",
      "Epoch [18818/20000], Loss: -21.329986572265625, Learning Rate: 0.005\n",
      "Epoch [18819/20000], Loss: -21.330078125, Learning Rate: 0.005\n",
      "Epoch [18820/20000], Loss: -21.33013916015625, Learning Rate: 0.005\n",
      "Epoch [18821/20000], Loss: -21.3302001953125, Learning Rate: 0.005\n",
      "Epoch [18822/20000], Loss: -21.33013916015625, Learning Rate: 0.005\n",
      "Epoch [18823/20000], Loss: -21.330184936523438, Learning Rate: 0.005\n",
      "Epoch [18824/20000], Loss: -21.330230712890625, Learning Rate: 0.005\n",
      "Epoch [18825/20000], Loss: -21.330276489257812, Learning Rate: 0.005\n",
      "Epoch [18826/20000], Loss: -21.330215454101562, Learning Rate: 0.005\n",
      "Epoch [18827/20000], Loss: -21.33026123046875, Learning Rate: 0.005\n",
      "Epoch [18828/20000], Loss: -21.330276489257812, Learning Rate: 0.005\n",
      "Epoch [18829/20000], Loss: -21.33038330078125, Learning Rate: 0.005\n",
      "Epoch [18830/20000], Loss: -21.33038330078125, Learning Rate: 0.005\n",
      "Epoch [18831/20000], Loss: -21.330429077148438, Learning Rate: 0.005\n",
      "Epoch [18832/20000], Loss: -21.330352783203125, Learning Rate: 0.005\n",
      "Epoch [18833/20000], Loss: -21.330429077148438, Learning Rate: 0.005\n",
      "Epoch [18834/20000], Loss: -21.3304443359375, Learning Rate: 0.005\n",
      "Epoch [18835/20000], Loss: -21.33056640625, Learning Rate: 0.005\n",
      "Epoch [18836/20000], Loss: -21.33050537109375, Learning Rate: 0.005\n",
      "Epoch [18837/20000], Loss: -21.33050537109375, Learning Rate: 0.005\n",
      "Epoch [18838/20000], Loss: -21.330581665039062, Learning Rate: 0.005\n",
      "Epoch [18839/20000], Loss: -21.330642700195312, Learning Rate: 0.005\n",
      "Epoch [18840/20000], Loss: -21.330596923828125, Learning Rate: 0.005\n",
      "Epoch [18841/20000], Loss: -21.330657958984375, Learning Rate: 0.005\n",
      "Epoch [18842/20000], Loss: -21.330642700195312, Learning Rate: 0.005\n",
      "Epoch [18843/20000], Loss: -21.330734252929688, Learning Rate: 0.005\n",
      "Epoch [18844/20000], Loss: -21.330673217773438, Learning Rate: 0.005\n",
      "Epoch [18845/20000], Loss: -21.3306884765625, Learning Rate: 0.005\n",
      "Epoch [18846/20000], Loss: -21.330734252929688, Learning Rate: 0.005\n",
      "Epoch [18847/20000], Loss: -21.330841064453125, Learning Rate: 0.005\n",
      "Epoch [18848/20000], Loss: -21.330810546875, Learning Rate: 0.005\n",
      "Epoch [18849/20000], Loss: -21.330886840820312, Learning Rate: 0.005\n",
      "Epoch [18850/20000], Loss: -21.33087158203125, Learning Rate: 0.005\n",
      "Epoch [18851/20000], Loss: -21.330841064453125, Learning Rate: 0.005\n",
      "Epoch [18852/20000], Loss: -21.330917358398438, Learning Rate: 0.005\n",
      "Epoch [18853/20000], Loss: -21.330886840820312, Learning Rate: 0.005\n",
      "Epoch [18854/20000], Loss: -21.330963134765625, Learning Rate: 0.005\n",
      "Epoch [18855/20000], Loss: -21.3309326171875, Learning Rate: 0.005\n",
      "Epoch [18856/20000], Loss: -21.331008911132812, Learning Rate: 0.005\n",
      "Epoch [18857/20000], Loss: -21.330963134765625, Learning Rate: 0.005\n",
      "Epoch [18858/20000], Loss: -21.331008911132812, Learning Rate: 0.005\n",
      "Epoch [18859/20000], Loss: -21.331039428710938, Learning Rate: 0.005\n",
      "Epoch [18860/20000], Loss: -21.331130981445312, Learning Rate: 0.005\n",
      "Epoch [18861/20000], Loss: -21.33111572265625, Learning Rate: 0.005\n",
      "Epoch [18862/20000], Loss: -21.331085205078125, Learning Rate: 0.005\n",
      "Epoch [18863/20000], Loss: -21.331192016601562, Learning Rate: 0.005\n",
      "Epoch [18864/20000], Loss: -21.33111572265625, Learning Rate: 0.005\n",
      "Epoch [18865/20000], Loss: -21.331161499023438, Learning Rate: 0.005\n",
      "Epoch [18866/20000], Loss: -21.331222534179688, Learning Rate: 0.005\n",
      "Epoch [18867/20000], Loss: -21.3311767578125, Learning Rate: 0.005\n",
      "Epoch [18868/20000], Loss: -21.331100463867188, Learning Rate: 0.005\n",
      "Epoch [18869/20000], Loss: -21.3311767578125, Learning Rate: 0.005\n",
      "Epoch [18870/20000], Loss: -21.331069946289062, Learning Rate: 0.005\n",
      "Epoch [18871/20000], Loss: -21.33099365234375, Learning Rate: 0.005\n",
      "Epoch [18872/20000], Loss: -21.330780029296875, Learning Rate: 0.005\n",
      "Epoch [18873/20000], Loss: -21.330490112304688, Learning Rate: 0.005\n",
      "Epoch [18874/20000], Loss: -21.329986572265625, Learning Rate: 0.005\n",
      "Epoch [18875/20000], Loss: -21.3291015625, Learning Rate: 0.005\n",
      "Epoch [18876/20000], Loss: -21.327865600585938, Learning Rate: 0.005\n",
      "Epoch [18877/20000], Loss: -21.32568359375, Learning Rate: 0.005\n",
      "Epoch [18878/20000], Loss: -21.322128295898438, Learning Rate: 0.005\n",
      "Epoch [18879/20000], Loss: -21.316299438476562, Learning Rate: 0.005\n",
      "Epoch [18880/20000], Loss: -21.30682373046875, Learning Rate: 0.005\n",
      "Epoch [18881/20000], Loss: -21.291366577148438, Learning Rate: 0.005\n",
      "Epoch [18882/20000], Loss: -21.266494750976562, Learning Rate: 0.005\n",
      "Epoch [18883/20000], Loss: -21.226699829101562, Learning Rate: 0.005\n",
      "Epoch [18884/20000], Loss: -21.1661376953125, Learning Rate: 0.005\n",
      "Epoch [18885/20000], Loss: -21.077560424804688, Learning Rate: 0.005\n",
      "Epoch [18886/20000], Loss: -20.965194702148438, Learning Rate: 0.005\n",
      "Epoch [18887/20000], Loss: -20.84222412109375, Learning Rate: 0.005\n",
      "Epoch [18888/20000], Loss: -20.765457153320312, Learning Rate: 0.005\n",
      "Epoch [18889/20000], Loss: -20.7794189453125, Learning Rate: 0.005\n",
      "Epoch [18890/20000], Loss: -20.91729736328125, Learning Rate: 0.005\n",
      "Epoch [18891/20000], Loss: -21.098663330078125, Learning Rate: 0.005\n",
      "Epoch [18892/20000], Loss: -21.22442626953125, Learning Rate: 0.005\n",
      "Epoch [18893/20000], Loss: -21.240036010742188, Learning Rate: 0.005\n",
      "Epoch [18894/20000], Loss: -21.188491821289062, Learning Rate: 0.005\n",
      "Epoch [18895/20000], Loss: -21.153335571289062, Learning Rate: 0.005\n",
      "Epoch [18896/20000], Loss: -21.179916381835938, Learning Rate: 0.005\n",
      "Epoch [18897/20000], Loss: -21.239059448242188, Learning Rate: 0.005\n",
      "Epoch [18898/20000], Loss: -21.26470947265625, Learning Rate: 0.005\n",
      "Epoch [18899/20000], Loss: -21.238433837890625, Learning Rate: 0.005\n",
      "Epoch [18900/20000], Loss: -21.203414916992188, Learning Rate: 0.005\n",
      "Epoch [18901/20000], Loss: -21.2138671875, Learning Rate: 0.005\n",
      "Epoch [18902/20000], Loss: -21.270721435546875, Learning Rate: 0.005\n",
      "Epoch [18903/20000], Loss: -21.321884155273438, Learning Rate: 0.005\n",
      "Epoch [18904/20000], Loss: -21.322097778320312, Learning Rate: 0.005\n",
      "Epoch [18905/20000], Loss: -21.282135009765625, Learning Rate: 0.005\n",
      "Epoch [18906/20000], Loss: -21.251358032226562, Learning Rate: 0.005\n",
      "Epoch [18907/20000], Loss: -21.26141357421875, Learning Rate: 0.005\n",
      "Epoch [18908/20000], Loss: -21.297683715820312, Learning Rate: 0.005\n",
      "Epoch [18909/20000], Loss: -21.322372436523438, Learning Rate: 0.005\n",
      "Epoch [18910/20000], Loss: -21.318832397460938, Learning Rate: 0.005\n",
      "Epoch [18911/20000], Loss: -21.303497314453125, Learning Rate: 0.005\n",
      "Epoch [18912/20000], Loss: -21.299880981445312, Learning Rate: 0.005\n",
      "Epoch [18913/20000], Loss: -21.310714721679688, Learning Rate: 0.005\n",
      "Epoch [18914/20000], Loss: -21.320465087890625, Learning Rate: 0.005\n",
      "Epoch [18915/20000], Loss: -21.317947387695312, Learning Rate: 0.005\n",
      "Epoch [18916/20000], Loss: -21.309585571289062, Learning Rate: 0.005\n",
      "Epoch [18917/20000], Loss: -21.308807373046875, Learning Rate: 0.005\n",
      "Epoch [18918/20000], Loss: -21.318801879882812, Learning Rate: 0.005\n",
      "Epoch [18919/20000], Loss: -21.329879760742188, Learning Rate: 0.005\n",
      "Epoch [18920/20000], Loss: -21.331527709960938, Learning Rate: 0.005\n",
      "Epoch [18921/20000], Loss: -21.324371337890625, Learning Rate: 0.005\n",
      "Epoch [18922/20000], Loss: -21.317962646484375, Learning Rate: 0.005\n",
      "Epoch [18923/20000], Loss: -21.318756103515625, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18924/20000], Loss: -21.324905395507812, Learning Rate: 0.005\n",
      "Epoch [18925/20000], Loss: -21.329421997070312, Learning Rate: 0.005\n",
      "Epoch [18926/20000], Loss: -21.329193115234375, Learning Rate: 0.005\n",
      "Epoch [18927/20000], Loss: -21.326950073242188, Learning Rate: 0.005\n",
      "Epoch [18928/20000], Loss: -21.326690673828125, Learning Rate: 0.005\n",
      "Epoch [18929/20000], Loss: -21.329025268554688, Learning Rate: 0.005\n",
      "Epoch [18930/20000], Loss: -21.331100463867188, Learning Rate: 0.005\n",
      "Epoch [18931/20000], Loss: -21.330551147460938, Learning Rate: 0.005\n",
      "Epoch [18932/20000], Loss: -21.32843017578125, Learning Rate: 0.005\n",
      "Epoch [18933/20000], Loss: -21.327392578125, Learning Rate: 0.005\n",
      "Epoch [18934/20000], Loss: -21.328720092773438, Learning Rate: 0.005\n",
      "Epoch [18935/20000], Loss: -21.331161499023438, Learning Rate: 0.005\n",
      "Epoch [18936/20000], Loss: -21.332534790039062, Learning Rate: 0.005\n",
      "Epoch [18937/20000], Loss: -21.332138061523438, Learning Rate: 0.005\n",
      "Epoch [18938/20000], Loss: -21.33099365234375, Learning Rate: 0.005\n",
      "Epoch [18939/20000], Loss: -21.33056640625, Learning Rate: 0.005\n",
      "Epoch [18940/20000], Loss: -21.331130981445312, Learning Rate: 0.005\n",
      "Epoch [18941/20000], Loss: -21.331832885742188, Learning Rate: 0.005\n",
      "Epoch [18942/20000], Loss: -21.332046508789062, Learning Rate: 0.005\n",
      "Epoch [18943/20000], Loss: -21.331619262695312, Learning Rate: 0.005\n",
      "Epoch [18944/20000], Loss: -21.331390380859375, Learning Rate: 0.005\n",
      "Epoch [18945/20000], Loss: -21.331741333007812, Learning Rate: 0.005\n",
      "Epoch [18946/20000], Loss: -21.3323974609375, Learning Rate: 0.005\n",
      "Epoch [18947/20000], Loss: -21.332977294921875, Learning Rate: 0.005\n",
      "Epoch [18948/20000], Loss: -21.332901000976562, Learning Rate: 0.005\n",
      "Epoch [18949/20000], Loss: -21.332565307617188, Learning Rate: 0.005\n",
      "Epoch [18950/20000], Loss: -21.332260131835938, Learning Rate: 0.005\n",
      "Epoch [18951/20000], Loss: -21.3323974609375, Learning Rate: 0.005\n",
      "Epoch [18952/20000], Loss: -21.332672119140625, Learning Rate: 0.005\n",
      "Epoch [18953/20000], Loss: -21.332839965820312, Learning Rate: 0.005\n",
      "Epoch [18954/20000], Loss: -21.3328857421875, Learning Rate: 0.005\n",
      "Epoch [18955/20000], Loss: -21.332809448242188, Learning Rate: 0.005\n",
      "Epoch [18956/20000], Loss: -21.332778930664062, Learning Rate: 0.005\n",
      "Epoch [18957/20000], Loss: -21.332931518554688, Learning Rate: 0.005\n",
      "Epoch [18958/20000], Loss: -21.333206176757812, Learning Rate: 0.005\n",
      "Epoch [18959/20000], Loss: -21.333297729492188, Learning Rate: 0.005\n",
      "Epoch [18960/20000], Loss: -21.333328247070312, Learning Rate: 0.005\n",
      "Epoch [18961/20000], Loss: -21.33319091796875, Learning Rate: 0.005\n",
      "Epoch [18962/20000], Loss: -21.3331298828125, Learning Rate: 0.005\n",
      "Epoch [18963/20000], Loss: -21.333145141601562, Learning Rate: 0.005\n",
      "Epoch [18964/20000], Loss: -21.33331298828125, Learning Rate: 0.005\n",
      "Epoch [18965/20000], Loss: -21.333389282226562, Learning Rate: 0.005\n",
      "Epoch [18966/20000], Loss: -21.333358764648438, Learning Rate: 0.005\n",
      "Epoch [18967/20000], Loss: -21.333358764648438, Learning Rate: 0.005\n",
      "Epoch [18968/20000], Loss: -21.3333740234375, Learning Rate: 0.005\n",
      "Epoch [18969/20000], Loss: -21.333419799804688, Learning Rate: 0.005\n",
      "Epoch [18970/20000], Loss: -21.33349609375, Learning Rate: 0.005\n",
      "Epoch [18971/20000], Loss: -21.333587646484375, Learning Rate: 0.005\n",
      "Epoch [18972/20000], Loss: -21.3336181640625, Learning Rate: 0.005\n",
      "Epoch [18973/20000], Loss: -21.3336181640625, Learning Rate: 0.005\n",
      "Epoch [18974/20000], Loss: -21.333648681640625, Learning Rate: 0.005\n",
      "Epoch [18975/20000], Loss: -21.333648681640625, Learning Rate: 0.005\n",
      "Epoch [18976/20000], Loss: -21.333663940429688, Learning Rate: 0.005\n",
      "Epoch [18977/20000], Loss: -21.333724975585938, Learning Rate: 0.005\n",
      "Epoch [18978/20000], Loss: -21.333709716796875, Learning Rate: 0.005\n",
      "Epoch [18979/20000], Loss: -21.333740234375, Learning Rate: 0.005\n",
      "Epoch [18980/20000], Loss: -21.333709716796875, Learning Rate: 0.005\n",
      "Epoch [18981/20000], Loss: -21.333740234375, Learning Rate: 0.005\n",
      "Epoch [18982/20000], Loss: -21.33380126953125, Learning Rate: 0.005\n",
      "Epoch [18983/20000], Loss: -21.333786010742188, Learning Rate: 0.005\n",
      "Epoch [18984/20000], Loss: -21.333816528320312, Learning Rate: 0.005\n",
      "Epoch [18985/20000], Loss: -21.33380126953125, Learning Rate: 0.005\n",
      "Epoch [18986/20000], Loss: -21.333877563476562, Learning Rate: 0.005\n",
      "Epoch [18987/20000], Loss: -21.33392333984375, Learning Rate: 0.005\n",
      "Epoch [18988/20000], Loss: -21.333984375, Learning Rate: 0.005\n",
      "Epoch [18989/20000], Loss: -21.334030151367188, Learning Rate: 0.005\n",
      "Epoch [18990/20000], Loss: -21.333969116210938, Learning Rate: 0.005\n",
      "Epoch [18991/20000], Loss: -21.33404541015625, Learning Rate: 0.005\n",
      "Epoch [18992/20000], Loss: -21.334014892578125, Learning Rate: 0.005\n",
      "Epoch [18993/20000], Loss: -21.3341064453125, Learning Rate: 0.005\n",
      "Epoch [18994/20000], Loss: -21.334121704101562, Learning Rate: 0.005\n",
      "Epoch [18995/20000], Loss: -21.33416748046875, Learning Rate: 0.005\n",
      "Epoch [18996/20000], Loss: -21.334152221679688, Learning Rate: 0.005\n",
      "Epoch [18997/20000], Loss: -21.334136962890625, Learning Rate: 0.005\n",
      "Epoch [18998/20000], Loss: -21.334152221679688, Learning Rate: 0.005\n",
      "Epoch [18999/20000], Loss: -21.334213256835938, Learning Rate: 0.005\n",
      "Epoch [19000/20000], Loss: -21.334213256835938, Learning Rate: 0.005\n",
      "Epoch [19001/20000], Loss: -21.334274291992188, Learning Rate: 0.005\n",
      "Epoch [19002/20000], Loss: -21.334259033203125, Learning Rate: 0.005\n",
      "Epoch [19003/20000], Loss: -21.33428955078125, Learning Rate: 0.005\n",
      "Epoch [19004/20000], Loss: -21.334320068359375, Learning Rate: 0.005\n",
      "Epoch [19005/20000], Loss: -21.334304809570312, Learning Rate: 0.005\n",
      "Epoch [19006/20000], Loss: -21.334365844726562, Learning Rate: 0.005\n",
      "Epoch [19007/20000], Loss: -21.334365844726562, Learning Rate: 0.005\n",
      "Epoch [19008/20000], Loss: -21.3343505859375, Learning Rate: 0.005\n",
      "Epoch [19009/20000], Loss: -21.334396362304688, Learning Rate: 0.005\n",
      "Epoch [19010/20000], Loss: -21.334381103515625, Learning Rate: 0.005\n",
      "Epoch [19011/20000], Loss: -21.33441162109375, Learning Rate: 0.005\n",
      "Epoch [19012/20000], Loss: -21.334335327148438, Learning Rate: 0.005\n",
      "Epoch [19013/20000], Loss: -21.334274291992188, Learning Rate: 0.005\n",
      "Epoch [19014/20000], Loss: -21.334213256835938, Learning Rate: 0.005\n",
      "Epoch [19015/20000], Loss: -21.334091186523438, Learning Rate: 0.005\n",
      "Epoch [19016/20000], Loss: -21.33392333984375, Learning Rate: 0.005\n",
      "Epoch [19017/20000], Loss: -21.333648681640625, Learning Rate: 0.005\n",
      "Epoch [19018/20000], Loss: -21.333206176757812, Learning Rate: 0.005\n",
      "Epoch [19019/20000], Loss: -21.332489013671875, Learning Rate: 0.005\n",
      "Epoch [19020/20000], Loss: -21.331512451171875, Learning Rate: 0.005\n",
      "Epoch [19021/20000], Loss: -21.33001708984375, Learning Rate: 0.005\n",
      "Epoch [19022/20000], Loss: -21.3277587890625, Learning Rate: 0.005\n",
      "Epoch [19023/20000], Loss: -21.324310302734375, Learning Rate: 0.005\n",
      "Epoch [19024/20000], Loss: -21.319122314453125, Learning Rate: 0.005\n",
      "Epoch [19025/20000], Loss: -21.311126708984375, Learning Rate: 0.005\n",
      "Epoch [19026/20000], Loss: -21.298858642578125, Learning Rate: 0.005\n",
      "Epoch [19027/20000], Loss: -21.280258178710938, Learning Rate: 0.005\n",
      "Epoch [19028/20000], Loss: -21.25201416015625, Learning Rate: 0.005\n",
      "Epoch [19029/20000], Loss: -21.210479736328125, Learning Rate: 0.005\n",
      "Epoch [19030/20000], Loss: -21.150039672851562, Learning Rate: 0.005\n",
      "Epoch [19031/20000], Loss: -21.06939697265625, Learning Rate: 0.005\n",
      "Epoch [19032/20000], Loss: -20.967086791992188, Learning Rate: 0.005\n",
      "Epoch [19033/20000], Loss: -20.864273071289062, Learning Rate: 0.005\n",
      "Epoch [19034/20000], Loss: -20.7861328125, Learning Rate: 0.005\n",
      "Epoch [19035/20000], Loss: -20.791030883789062, Learning Rate: 0.005\n",
      "Epoch [19036/20000], Loss: -20.894790649414062, Learning Rate: 0.005\n",
      "Epoch [19037/20000], Loss: -21.07965087890625, Learning Rate: 0.005\n",
      "Epoch [19038/20000], Loss: -21.251907348632812, Learning Rate: 0.005\n",
      "Epoch [19039/20000], Loss: -21.330978393554688, Learning Rate: 0.005\n",
      "Epoch [19040/20000], Loss: -21.298324584960938, Learning Rate: 0.005\n",
      "Epoch [19041/20000], Loss: -21.20538330078125, Learning Rate: 0.005\n",
      "Epoch [19042/20000], Loss: -21.132003784179688, Learning Rate: 0.005\n",
      "Epoch [19043/20000], Loss: -21.130111694335938, Learning Rate: 0.005\n",
      "Epoch [19044/20000], Loss: -21.199752807617188, Learning Rate: 0.005\n",
      "Epoch [19045/20000], Loss: -21.28497314453125, Learning Rate: 0.005\n",
      "Epoch [19046/20000], Loss: -21.328582763671875, Learning Rate: 0.005\n",
      "Epoch [19047/20000], Loss: -21.312713623046875, Learning Rate: 0.005\n",
      "Epoch [19048/20000], Loss: -21.266998291015625, Learning Rate: 0.005\n",
      "Epoch [19049/20000], Loss: -21.237060546875, Learning Rate: 0.005\n",
      "Epoch [19050/20000], Loss: -21.24652099609375, Learning Rate: 0.005\n",
      "Epoch [19051/20000], Loss: -21.2850341796875, Learning Rate: 0.005\n",
      "Epoch [19052/20000], Loss: -21.319900512695312, Learning Rate: 0.005\n",
      "Epoch [19053/20000], Loss: -21.328201293945312, Learning Rate: 0.005\n",
      "Epoch [19054/20000], Loss: -21.312149047851562, Learning Rate: 0.005\n",
      "Epoch [19055/20000], Loss: -21.29241943359375, Learning Rate: 0.005\n",
      "Epoch [19056/20000], Loss: -21.288131713867188, Learning Rate: 0.005\n",
      "Epoch [19057/20000], Loss: -21.301651000976562, Learning Rate: 0.005\n",
      "Epoch [19058/20000], Loss: -21.320556640625, Learning Rate: 0.005\n",
      "Epoch [19059/20000], Loss: -21.330184936523438, Learning Rate: 0.005\n",
      "Epoch [19060/20000], Loss: -21.326339721679688, Learning Rate: 0.005\n",
      "Epoch [19061/20000], Loss: -21.316635131835938, Learning Rate: 0.005\n",
      "Epoch [19062/20000], Loss: -21.31134033203125, Learning Rate: 0.005\n",
      "Epoch [19063/20000], Loss: -21.315261840820312, Learning Rate: 0.005\n",
      "Epoch [19064/20000], Loss: -21.324630737304688, Learning Rate: 0.005\n",
      "Epoch [19065/20000], Loss: -21.331695556640625, Learning Rate: 0.005\n",
      "Epoch [19066/20000], Loss: -21.332122802734375, Learning Rate: 0.005\n",
      "Epoch [19067/20000], Loss: -21.327651977539062, Learning Rate: 0.005\n",
      "Epoch [19068/20000], Loss: -21.323410034179688, Learning Rate: 0.005\n",
      "Epoch [19069/20000], Loss: -21.323471069335938, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19070/20000], Loss: -21.327728271484375, Learning Rate: 0.005\n",
      "Epoch [19071/20000], Loss: -21.332565307617188, Learning Rate: 0.005\n",
      "Epoch [19072/20000], Loss: -21.334671020507812, Learning Rate: 0.005\n",
      "Epoch [19073/20000], Loss: -21.333358764648438, Learning Rate: 0.005\n",
      "Epoch [19074/20000], Loss: -21.330413818359375, Learning Rate: 0.005\n",
      "Epoch [19075/20000], Loss: -21.328826904296875, Learning Rate: 0.005\n",
      "Epoch [19076/20000], Loss: -21.329757690429688, Learning Rate: 0.005\n",
      "Epoch [19077/20000], Loss: -21.332473754882812, Learning Rate: 0.005\n",
      "Epoch [19078/20000], Loss: -21.334930419921875, Learning Rate: 0.005\n",
      "Epoch [19079/20000], Loss: -21.335769653320312, Learning Rate: 0.005\n",
      "Epoch [19080/20000], Loss: -21.334686279296875, Learning Rate: 0.005\n",
      "Epoch [19081/20000], Loss: -21.3331298828125, Learning Rate: 0.005\n",
      "Epoch [19082/20000], Loss: -21.332244873046875, Learning Rate: 0.005\n",
      "Epoch [19083/20000], Loss: -21.332809448242188, Learning Rate: 0.005\n",
      "Epoch [19084/20000], Loss: -21.334274291992188, Learning Rate: 0.005\n",
      "Epoch [19085/20000], Loss: -21.335586547851562, Learning Rate: 0.005\n",
      "Epoch [19086/20000], Loss: -21.3360595703125, Learning Rate: 0.005\n",
      "Epoch [19087/20000], Loss: -21.335647583007812, Learning Rate: 0.005\n",
      "Epoch [19088/20000], Loss: -21.334823608398438, Learning Rate: 0.005\n",
      "Epoch [19089/20000], Loss: -21.334365844726562, Learning Rate: 0.005\n",
      "Epoch [19090/20000], Loss: -21.334442138671875, Learning Rate: 0.005\n",
      "Epoch [19091/20000], Loss: -21.335037231445312, Learning Rate: 0.005\n",
      "Epoch [19092/20000], Loss: -21.335769653320312, Learning Rate: 0.005\n",
      "Epoch [19093/20000], Loss: -21.336135864257812, Learning Rate: 0.005\n",
      "Epoch [19094/20000], Loss: -21.3360595703125, Learning Rate: 0.005\n",
      "Epoch [19095/20000], Loss: -21.335830688476562, Learning Rate: 0.005\n",
      "Epoch [19096/20000], Loss: -21.335617065429688, Learning Rate: 0.005\n",
      "Epoch [19097/20000], Loss: -21.335479736328125, Learning Rate: 0.005\n",
      "Epoch [19098/20000], Loss: -21.335678100585938, Learning Rate: 0.005\n",
      "Epoch [19099/20000], Loss: -21.336013793945312, Learning Rate: 0.005\n",
      "Epoch [19100/20000], Loss: -21.336181640625, Learning Rate: 0.005\n",
      "Epoch [19101/20000], Loss: -21.336273193359375, Learning Rate: 0.005\n",
      "Epoch [19102/20000], Loss: -21.336257934570312, Learning Rate: 0.005\n",
      "Epoch [19103/20000], Loss: -21.336151123046875, Learning Rate: 0.005\n",
      "Epoch [19104/20000], Loss: -21.3360595703125, Learning Rate: 0.005\n",
      "Epoch [19105/20000], Loss: -21.336135864257812, Learning Rate: 0.005\n",
      "Epoch [19106/20000], Loss: -21.336257934570312, Learning Rate: 0.005\n",
      "Epoch [19107/20000], Loss: -21.336380004882812, Learning Rate: 0.005\n",
      "Epoch [19108/20000], Loss: -21.336456298828125, Learning Rate: 0.005\n",
      "Epoch [19109/20000], Loss: -21.336456298828125, Learning Rate: 0.005\n",
      "Epoch [19110/20000], Loss: -21.33648681640625, Learning Rate: 0.005\n",
      "Epoch [19111/20000], Loss: -21.336456298828125, Learning Rate: 0.005\n",
      "Epoch [19112/20000], Loss: -21.336410522460938, Learning Rate: 0.005\n",
      "Epoch [19113/20000], Loss: -21.33642578125, Learning Rate: 0.005\n",
      "Epoch [19114/20000], Loss: -21.336456298828125, Learning Rate: 0.005\n",
      "Epoch [19115/20000], Loss: -21.336593627929688, Learning Rate: 0.005\n",
      "Epoch [19116/20000], Loss: -21.336700439453125, Learning Rate: 0.005\n",
      "Epoch [19117/20000], Loss: -21.336700439453125, Learning Rate: 0.005\n",
      "Epoch [19118/20000], Loss: -21.336700439453125, Learning Rate: 0.005\n",
      "Epoch [19119/20000], Loss: -21.336715698242188, Learning Rate: 0.005\n",
      "Epoch [19120/20000], Loss: -21.336700439453125, Learning Rate: 0.005\n",
      "Epoch [19121/20000], Loss: -21.33673095703125, Learning Rate: 0.005\n",
      "Epoch [19122/20000], Loss: -21.336746215820312, Learning Rate: 0.005\n",
      "Epoch [19123/20000], Loss: -21.336746215820312, Learning Rate: 0.005\n",
      "Epoch [19124/20000], Loss: -21.33685302734375, Learning Rate: 0.005\n",
      "Epoch [19125/20000], Loss: -21.336868286132812, Learning Rate: 0.005\n",
      "Epoch [19126/20000], Loss: -21.336944580078125, Learning Rate: 0.005\n",
      "Epoch [19127/20000], Loss: -21.336959838867188, Learning Rate: 0.005\n",
      "Epoch [19128/20000], Loss: -21.336959838867188, Learning Rate: 0.005\n",
      "Epoch [19129/20000], Loss: -21.336959838867188, Learning Rate: 0.005\n",
      "Epoch [19130/20000], Loss: -21.336959838867188, Learning Rate: 0.005\n",
      "Epoch [19131/20000], Loss: -21.3370361328125, Learning Rate: 0.005\n",
      "Epoch [19132/20000], Loss: -21.337051391601562, Learning Rate: 0.005\n",
      "Epoch [19133/20000], Loss: -21.33697509765625, Learning Rate: 0.005\n",
      "Epoch [19134/20000], Loss: -21.337112426757812, Learning Rate: 0.005\n",
      "Epoch [19135/20000], Loss: -21.337112426757812, Learning Rate: 0.005\n",
      "Epoch [19136/20000], Loss: -21.337188720703125, Learning Rate: 0.005\n",
      "Epoch [19137/20000], Loss: -21.337158203125, Learning Rate: 0.005\n",
      "Epoch [19138/20000], Loss: -21.337203979492188, Learning Rate: 0.005\n",
      "Epoch [19139/20000], Loss: -21.337173461914062, Learning Rate: 0.005\n",
      "Epoch [19140/20000], Loss: -21.337173461914062, Learning Rate: 0.005\n",
      "Epoch [19141/20000], Loss: -21.337203979492188, Learning Rate: 0.005\n",
      "Epoch [19142/20000], Loss: -21.33721923828125, Learning Rate: 0.005\n",
      "Epoch [19143/20000], Loss: -21.337203979492188, Learning Rate: 0.005\n",
      "Epoch [19144/20000], Loss: -21.3372802734375, Learning Rate: 0.005\n",
      "Epoch [19145/20000], Loss: -21.337265014648438, Learning Rate: 0.005\n",
      "Epoch [19146/20000], Loss: -21.337265014648438, Learning Rate: 0.005\n",
      "Epoch [19147/20000], Loss: -21.337356567382812, Learning Rate: 0.005\n",
      "Epoch [19148/20000], Loss: -21.337356567382812, Learning Rate: 0.005\n",
      "Epoch [19149/20000], Loss: -21.33740234375, Learning Rate: 0.005\n",
      "Epoch [19150/20000], Loss: -21.337417602539062, Learning Rate: 0.005\n",
      "Epoch [19151/20000], Loss: -21.337387084960938, Learning Rate: 0.005\n",
      "Epoch [19152/20000], Loss: -21.33746337890625, Learning Rate: 0.005\n",
      "Epoch [19153/20000], Loss: -21.337509155273438, Learning Rate: 0.005\n",
      "Epoch [19154/20000], Loss: -21.337493896484375, Learning Rate: 0.005\n",
      "Epoch [19155/20000], Loss: -21.337554931640625, Learning Rate: 0.005\n",
      "Epoch [19156/20000], Loss: -21.3375244140625, Learning Rate: 0.005\n",
      "Epoch [19157/20000], Loss: -21.337600708007812, Learning Rate: 0.005\n",
      "Epoch [19158/20000], Loss: -21.337677001953125, Learning Rate: 0.005\n",
      "Epoch [19159/20000], Loss: -21.337615966796875, Learning Rate: 0.005\n",
      "Epoch [19160/20000], Loss: -21.33758544921875, Learning Rate: 0.005\n",
      "Epoch [19161/20000], Loss: -21.337661743164062, Learning Rate: 0.005\n",
      "Epoch [19162/20000], Loss: -21.337677001953125, Learning Rate: 0.005\n",
      "Epoch [19163/20000], Loss: -21.337738037109375, Learning Rate: 0.005\n",
      "Epoch [19164/20000], Loss: -21.337738037109375, Learning Rate: 0.005\n",
      "Epoch [19165/20000], Loss: -21.337753295898438, Learning Rate: 0.005\n",
      "Epoch [19166/20000], Loss: -21.337738037109375, Learning Rate: 0.005\n",
      "Epoch [19167/20000], Loss: -21.337722778320312, Learning Rate: 0.005\n",
      "Epoch [19168/20000], Loss: -21.337753295898438, Learning Rate: 0.005\n",
      "Epoch [19169/20000], Loss: -21.337753295898438, Learning Rate: 0.005\n",
      "Epoch [19170/20000], Loss: -21.337814331054688, Learning Rate: 0.005\n",
      "Epoch [19171/20000], Loss: -21.33782958984375, Learning Rate: 0.005\n",
      "Epoch [19172/20000], Loss: -21.337814331054688, Learning Rate: 0.005\n",
      "Epoch [19173/20000], Loss: -21.337814331054688, Learning Rate: 0.005\n",
      "Epoch [19174/20000], Loss: -21.337783813476562, Learning Rate: 0.005\n",
      "Epoch [19175/20000], Loss: -21.337661743164062, Learning Rate: 0.005\n",
      "Epoch [19176/20000], Loss: -21.337661743164062, Learning Rate: 0.005\n",
      "Epoch [19177/20000], Loss: -21.337509155273438, Learning Rate: 0.005\n",
      "Epoch [19178/20000], Loss: -21.337310791015625, Learning Rate: 0.005\n",
      "Epoch [19179/20000], Loss: -21.337051391601562, Learning Rate: 0.005\n",
      "Epoch [19180/20000], Loss: -21.336563110351562, Learning Rate: 0.005\n",
      "Epoch [19181/20000], Loss: -21.33587646484375, Learning Rate: 0.005\n",
      "Epoch [19182/20000], Loss: -21.334869384765625, Learning Rate: 0.005\n",
      "Epoch [19183/20000], Loss: -21.3333740234375, Learning Rate: 0.005\n",
      "Epoch [19184/20000], Loss: -21.3310546875, Learning Rate: 0.005\n",
      "Epoch [19185/20000], Loss: -21.3275146484375, Learning Rate: 0.005\n",
      "Epoch [19186/20000], Loss: -21.3221435546875, Learning Rate: 0.005\n",
      "Epoch [19187/20000], Loss: -21.313995361328125, Learning Rate: 0.005\n",
      "Epoch [19188/20000], Loss: -21.301605224609375, Learning Rate: 0.005\n",
      "Epoch [19189/20000], Loss: -21.283157348632812, Learning Rate: 0.005\n",
      "Epoch [19190/20000], Loss: -21.256149291992188, Learning Rate: 0.005\n",
      "Epoch [19191/20000], Loss: -21.218307495117188, Learning Rate: 0.005\n",
      "Epoch [19192/20000], Loss: -21.16876220703125, Learning Rate: 0.005\n",
      "Epoch [19193/20000], Loss: -21.1109619140625, Learning Rate: 0.005\n",
      "Epoch [19194/20000], Loss: -21.056060791015625, Learning Rate: 0.005\n",
      "Epoch [19195/20000], Loss: -21.025634765625, Learning Rate: 0.005\n",
      "Epoch [19196/20000], Loss: -21.040054321289062, Learning Rate: 0.005\n",
      "Epoch [19197/20000], Loss: -21.104888916015625, Learning Rate: 0.005\n",
      "Epoch [19198/20000], Loss: -21.190338134765625, Learning Rate: 0.005\n",
      "Epoch [19199/20000], Loss: -21.25250244140625, Learning Rate: 0.005\n",
      "Epoch [19200/20000], Loss: -21.261062622070312, Learning Rate: 0.005\n",
      "Epoch [19201/20000], Loss: -21.2252197265625, Learning Rate: 0.005\n",
      "Epoch [19202/20000], Loss: -21.180419921875, Learning Rate: 0.005\n",
      "Epoch [19203/20000], Loss: -21.168136596679688, Learning Rate: 0.005\n",
      "Epoch [19204/20000], Loss: -21.201629638671875, Learning Rate: 0.005\n",
      "Epoch [19205/20000], Loss: -21.263702392578125, Learning Rate: 0.005\n",
      "Epoch [19206/20000], Loss: -21.313888549804688, Learning Rate: 0.005\n",
      "Epoch [19207/20000], Loss: -21.326858520507812, Learning Rate: 0.005\n",
      "Epoch [19208/20000], Loss: -21.308486938476562, Learning Rate: 0.005\n",
      "Epoch [19209/20000], Loss: -21.28564453125, Learning Rate: 0.005\n",
      "Epoch [19210/20000], Loss: -21.281936645507812, Learning Rate: 0.005\n",
      "Epoch [19211/20000], Loss: -21.299774169921875, Learning Rate: 0.005\n",
      "Epoch [19212/20000], Loss: -21.322067260742188, Learning Rate: 0.005\n",
      "Epoch [19213/20000], Loss: -21.329925537109375, Learning Rate: 0.005\n",
      "Epoch [19214/20000], Loss: -21.319625854492188, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19215/20000], Loss: -21.302734375, Learning Rate: 0.005\n",
      "Epoch [19216/20000], Loss: -21.294967651367188, Learning Rate: 0.005\n",
      "Epoch [19217/20000], Loss: -21.302108764648438, Learning Rate: 0.005\n",
      "Epoch [19218/20000], Loss: -21.31732177734375, Learning Rate: 0.005\n",
      "Epoch [19219/20000], Loss: -21.328445434570312, Learning Rate: 0.005\n",
      "Epoch [19220/20000], Loss: -21.329788208007812, Learning Rate: 0.005\n",
      "Epoch [19221/20000], Loss: -21.324417114257812, Learning Rate: 0.005\n",
      "Epoch [19222/20000], Loss: -21.320648193359375, Learning Rate: 0.005\n",
      "Epoch [19223/20000], Loss: -21.32330322265625, Learning Rate: 0.005\n",
      "Epoch [19224/20000], Loss: -21.330856323242188, Learning Rate: 0.005\n",
      "Epoch [19225/20000], Loss: -21.337356567382812, Learning Rate: 0.005\n",
      "Epoch [19226/20000], Loss: -21.338638305664062, Learning Rate: 0.005\n",
      "Epoch [19227/20000], Loss: -21.335296630859375, Learning Rate: 0.005\n",
      "Epoch [19228/20000], Loss: -21.3310546875, Learning Rate: 0.005\n",
      "Epoch [19229/20000], Loss: -21.329681396484375, Learning Rate: 0.005\n",
      "Epoch [19230/20000], Loss: -21.331710815429688, Learning Rate: 0.005\n",
      "Epoch [19231/20000], Loss: -21.33489990234375, Learning Rate: 0.005\n",
      "Epoch [19232/20000], Loss: -21.336273193359375, Learning Rate: 0.005\n",
      "Epoch [19233/20000], Loss: -21.335311889648438, Learning Rate: 0.005\n",
      "Epoch [19234/20000], Loss: -21.333099365234375, Learning Rate: 0.005\n",
      "Epoch [19235/20000], Loss: -21.331695556640625, Learning Rate: 0.005\n",
      "Epoch [19236/20000], Loss: -21.33221435546875, Learning Rate: 0.005\n",
      "Epoch [19237/20000], Loss: -21.3341064453125, Learning Rate: 0.005\n",
      "Epoch [19238/20000], Loss: -21.335784912109375, Learning Rate: 0.005\n",
      "Epoch [19239/20000], Loss: -21.33636474609375, Learning Rate: 0.005\n",
      "Epoch [19240/20000], Loss: -21.335800170898438, Learning Rate: 0.005\n",
      "Epoch [19241/20000], Loss: -21.334915161132812, Learning Rate: 0.005\n",
      "Epoch [19242/20000], Loss: -21.334686279296875, Learning Rate: 0.005\n",
      "Epoch [19243/20000], Loss: -21.335433959960938, Learning Rate: 0.005\n",
      "Epoch [19244/20000], Loss: -21.336502075195312, Learning Rate: 0.005\n",
      "Epoch [19245/20000], Loss: -21.337234497070312, Learning Rate: 0.005\n",
      "Epoch [19246/20000], Loss: -21.3372802734375, Learning Rate: 0.005\n",
      "Epoch [19247/20000], Loss: -21.336929321289062, Learning Rate: 0.005\n",
      "Epoch [19248/20000], Loss: -21.33648681640625, Learning Rate: 0.005\n",
      "Epoch [19249/20000], Loss: -21.3363037109375, Learning Rate: 0.005\n",
      "Epoch [19250/20000], Loss: -21.336517333984375, Learning Rate: 0.005\n",
      "Epoch [19251/20000], Loss: -21.336837768554688, Learning Rate: 0.005\n",
      "Epoch [19252/20000], Loss: -21.336929321289062, Learning Rate: 0.005\n",
      "Epoch [19253/20000], Loss: -21.336685180664062, Learning Rate: 0.005\n",
      "Epoch [19254/20000], Loss: -21.336013793945312, Learning Rate: 0.005\n",
      "Epoch [19255/20000], Loss: -21.335235595703125, Learning Rate: 0.005\n",
      "Epoch [19256/20000], Loss: -21.334503173828125, Learning Rate: 0.005\n",
      "Epoch [19257/20000], Loss: -21.333770751953125, Learning Rate: 0.005\n",
      "Epoch [19258/20000], Loss: -21.332717895507812, Learning Rate: 0.005\n",
      "Epoch [19259/20000], Loss: -21.331375122070312, Learning Rate: 0.005\n",
      "Epoch [19260/20000], Loss: -21.329376220703125, Learning Rate: 0.005\n",
      "Epoch [19261/20000], Loss: -21.326461791992188, Learning Rate: 0.005\n",
      "Epoch [19262/20000], Loss: -21.322616577148438, Learning Rate: 0.005\n",
      "Epoch [19263/20000], Loss: -21.3177490234375, Learning Rate: 0.005\n",
      "Epoch [19264/20000], Loss: -21.311111450195312, Learning Rate: 0.005\n",
      "Epoch [19265/20000], Loss: -21.302642822265625, Learning Rate: 0.005\n",
      "Epoch [19266/20000], Loss: -21.291259765625, Learning Rate: 0.005\n",
      "Epoch [19267/20000], Loss: -21.276580810546875, Learning Rate: 0.005\n",
      "Epoch [19268/20000], Loss: -21.25750732421875, Learning Rate: 0.005\n",
      "Epoch [19269/20000], Loss: -21.23419189453125, Learning Rate: 0.005\n",
      "Epoch [19270/20000], Loss: -21.206085205078125, Learning Rate: 0.005\n",
      "Epoch [19271/20000], Loss: -21.175979614257812, Learning Rate: 0.005\n",
      "Epoch [19272/20000], Loss: -21.145248413085938, Learning Rate: 0.005\n",
      "Epoch [19273/20000], Loss: -21.121841430664062, Learning Rate: 0.005\n",
      "Epoch [19274/20000], Loss: -21.109970092773438, Learning Rate: 0.005\n",
      "Epoch [19275/20000], Loss: -21.119781494140625, Learning Rate: 0.005\n",
      "Epoch [19276/20000], Loss: -21.150604248046875, Learning Rate: 0.005\n",
      "Epoch [19277/20000], Loss: -21.201004028320312, Learning Rate: 0.005\n",
      "Epoch [19278/20000], Loss: -21.2569580078125, Learning Rate: 0.005\n",
      "Epoch [19279/20000], Loss: -21.305145263671875, Learning Rate: 0.005\n",
      "Epoch [19280/20000], Loss: -21.33367919921875, Learning Rate: 0.005\n",
      "Epoch [19281/20000], Loss: -21.33917236328125, Learning Rate: 0.005\n",
      "Epoch [19282/20000], Loss: -21.326217651367188, Learning Rate: 0.005\n",
      "Epoch [19283/20000], Loss: -21.304107666015625, Learning Rate: 0.005\n",
      "Epoch [19284/20000], Loss: -21.283538818359375, Learning Rate: 0.005\n",
      "Epoch [19285/20000], Loss: -21.272018432617188, Learning Rate: 0.005\n",
      "Epoch [19286/20000], Loss: -21.273651123046875, Learning Rate: 0.005\n",
      "Epoch [19287/20000], Loss: -21.286605834960938, Learning Rate: 0.005\n",
      "Epoch [19288/20000], Loss: -21.305984497070312, Learning Rate: 0.005\n",
      "Epoch [19289/20000], Loss: -21.324447631835938, Learning Rate: 0.005\n",
      "Epoch [19290/20000], Loss: -21.336624145507812, Learning Rate: 0.005\n",
      "Epoch [19291/20000], Loss: -21.3402099609375, Learning Rate: 0.005\n",
      "Epoch [19292/20000], Loss: -21.336181640625, Learning Rate: 0.005\n",
      "Epoch [19293/20000], Loss: -21.328262329101562, Learning Rate: 0.005\n",
      "Epoch [19294/20000], Loss: -21.320281982421875, Learning Rate: 0.005\n",
      "Epoch [19295/20000], Loss: -21.315673828125, Learning Rate: 0.005\n",
      "Epoch [19296/20000], Loss: -21.315673828125, Learning Rate: 0.005\n",
      "Epoch [19297/20000], Loss: -21.320083618164062, Learning Rate: 0.005\n",
      "Epoch [19298/20000], Loss: -21.326766967773438, Learning Rate: 0.005\n",
      "Epoch [19299/20000], Loss: -21.333587646484375, Learning Rate: 0.005\n",
      "Epoch [19300/20000], Loss: -21.338485717773438, Learning Rate: 0.005\n",
      "Epoch [19301/20000], Loss: -21.340469360351562, Learning Rate: 0.005\n",
      "Epoch [19302/20000], Loss: -21.339752197265625, Learning Rate: 0.005\n",
      "Epoch [19303/20000], Loss: -21.337295532226562, Learning Rate: 0.005\n",
      "Epoch [19304/20000], Loss: -21.334442138671875, Learning Rate: 0.005\n",
      "Epoch [19305/20000], Loss: -21.332260131835938, Learning Rate: 0.005\n",
      "Epoch [19306/20000], Loss: -21.331405639648438, Learning Rate: 0.005\n",
      "Epoch [19307/20000], Loss: -21.332000732421875, Learning Rate: 0.005\n",
      "Epoch [19308/20000], Loss: -21.333831787109375, Learning Rate: 0.005\n",
      "Epoch [19309/20000], Loss: -21.336074829101562, Learning Rate: 0.005\n",
      "Epoch [19310/20000], Loss: -21.338287353515625, Learning Rate: 0.005\n",
      "Epoch [19311/20000], Loss: -21.33990478515625, Learning Rate: 0.005\n",
      "Epoch [19312/20000], Loss: -21.340667724609375, Learning Rate: 0.005\n",
      "Epoch [19313/20000], Loss: -21.340560913085938, Learning Rate: 0.005\n",
      "Epoch [19314/20000], Loss: -21.340011596679688, Learning Rate: 0.005\n",
      "Epoch [19315/20000], Loss: -21.339218139648438, Learning Rate: 0.005\n",
      "Epoch [19316/20000], Loss: -21.338394165039062, Learning Rate: 0.005\n",
      "Epoch [19317/20000], Loss: -21.337890625, Learning Rate: 0.005\n",
      "Epoch [19318/20000], Loss: -21.337722778320312, Learning Rate: 0.005\n",
      "Epoch [19319/20000], Loss: -21.33795166015625, Learning Rate: 0.005\n",
      "Epoch [19320/20000], Loss: -21.3385009765625, Learning Rate: 0.005\n",
      "Epoch [19321/20000], Loss: -21.339111328125, Learning Rate: 0.005\n",
      "Epoch [19322/20000], Loss: -21.339752197265625, Learning Rate: 0.005\n",
      "Epoch [19323/20000], Loss: -21.340301513671875, Learning Rate: 0.005\n",
      "Epoch [19324/20000], Loss: -21.340744018554688, Learning Rate: 0.005\n",
      "Epoch [19325/20000], Loss: -21.3409423828125, Learning Rate: 0.005\n",
      "Epoch [19326/20000], Loss: -21.341018676757812, Learning Rate: 0.005\n",
      "Epoch [19327/20000], Loss: -21.340957641601562, Learning Rate: 0.005\n",
      "Epoch [19328/20000], Loss: -21.340728759765625, Learning Rate: 0.005\n",
      "Epoch [19329/20000], Loss: -21.340652465820312, Learning Rate: 0.005\n",
      "Epoch [19330/20000], Loss: -21.3404541015625, Learning Rate: 0.005\n",
      "Epoch [19331/20000], Loss: -21.34033203125, Learning Rate: 0.005\n",
      "Epoch [19332/20000], Loss: -21.340194702148438, Learning Rate: 0.005\n",
      "Epoch [19333/20000], Loss: -21.340255737304688, Learning Rate: 0.005\n",
      "Epoch [19334/20000], Loss: -21.34027099609375, Learning Rate: 0.005\n",
      "Epoch [19335/20000], Loss: -21.34039306640625, Learning Rate: 0.005\n",
      "Epoch [19336/20000], Loss: -21.340484619140625, Learning Rate: 0.005\n",
      "Epoch [19337/20000], Loss: -21.340606689453125, Learning Rate: 0.005\n",
      "Epoch [19338/20000], Loss: -21.340774536132812, Learning Rate: 0.005\n",
      "Epoch [19339/20000], Loss: -21.340850830078125, Learning Rate: 0.005\n",
      "Epoch [19340/20000], Loss: -21.341018676757812, Learning Rate: 0.005\n",
      "Epoch [19341/20000], Loss: -21.341110229492188, Learning Rate: 0.005\n",
      "Epoch [19342/20000], Loss: -21.341217041015625, Learning Rate: 0.005\n",
      "Epoch [19343/20000], Loss: -21.34124755859375, Learning Rate: 0.005\n",
      "Epoch [19344/20000], Loss: -21.341323852539062, Learning Rate: 0.005\n",
      "Epoch [19345/20000], Loss: -21.341384887695312, Learning Rate: 0.005\n",
      "Epoch [19346/20000], Loss: -21.341400146484375, Learning Rate: 0.005\n",
      "Epoch [19347/20000], Loss: -21.341354370117188, Learning Rate: 0.005\n",
      "Epoch [19348/20000], Loss: -21.341415405273438, Learning Rate: 0.005\n",
      "Epoch [19349/20000], Loss: -21.3414306640625, Learning Rate: 0.005\n",
      "Epoch [19350/20000], Loss: -21.341461181640625, Learning Rate: 0.005\n",
      "Epoch [19351/20000], Loss: -21.341445922851562, Learning Rate: 0.005\n",
      "Epoch [19352/20000], Loss: -21.341445922851562, Learning Rate: 0.005\n",
      "Epoch [19353/20000], Loss: -21.341445922851562, Learning Rate: 0.005\n",
      "Epoch [19354/20000], Loss: -21.341445922851562, Learning Rate: 0.005\n",
      "Epoch [19355/20000], Loss: -21.341461181640625, Learning Rate: 0.005\n",
      "Epoch [19356/20000], Loss: -21.341415405273438, Learning Rate: 0.005\n",
      "Epoch [19357/20000], Loss: -21.341415405273438, Learning Rate: 0.005\n",
      "Epoch [19358/20000], Loss: -21.341461181640625, Learning Rate: 0.005\n",
      "Epoch [19359/20000], Loss: -21.34136962890625, Learning Rate: 0.005\n",
      "Epoch [19360/20000], Loss: -21.34136962890625, Learning Rate: 0.005\n",
      "Epoch [19361/20000], Loss: -21.341323852539062, Learning Rate: 0.005\n",
      "Epoch [19362/20000], Loss: -21.341217041015625, Learning Rate: 0.005\n",
      "Epoch [19363/20000], Loss: -21.341140747070312, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19364/20000], Loss: -21.341049194335938, Learning Rate: 0.005\n",
      "Epoch [19365/20000], Loss: -21.340805053710938, Learning Rate: 0.005\n",
      "Epoch [19366/20000], Loss: -21.340560913085938, Learning Rate: 0.005\n",
      "Epoch [19367/20000], Loss: -21.34027099609375, Learning Rate: 0.005\n",
      "Epoch [19368/20000], Loss: -21.3397216796875, Learning Rate: 0.005\n",
      "Epoch [19369/20000], Loss: -21.33905029296875, Learning Rate: 0.005\n",
      "Epoch [19370/20000], Loss: -21.338058471679688, Learning Rate: 0.005\n",
      "Epoch [19371/20000], Loss: -21.336669921875, Learning Rate: 0.005\n",
      "Epoch [19372/20000], Loss: -21.33477783203125, Learning Rate: 0.005\n",
      "Epoch [19373/20000], Loss: -21.332015991210938, Learning Rate: 0.005\n",
      "Epoch [19374/20000], Loss: -21.32806396484375, Learning Rate: 0.005\n",
      "Epoch [19375/20000], Loss: -21.322357177734375, Learning Rate: 0.005\n",
      "Epoch [19376/20000], Loss: -21.314163208007812, Learning Rate: 0.005\n",
      "Epoch [19377/20000], Loss: -21.302276611328125, Learning Rate: 0.005\n",
      "Epoch [19378/20000], Loss: -21.285293579101562, Learning Rate: 0.005\n",
      "Epoch [19379/20000], Loss: -21.26141357421875, Learning Rate: 0.005\n",
      "Epoch [19380/20000], Loss: -21.227890014648438, Learning Rate: 0.005\n",
      "Epoch [19381/20000], Loss: -21.183517456054688, Learning Rate: 0.005\n",
      "Epoch [19382/20000], Loss: -21.126007080078125, Learning Rate: 0.005\n",
      "Epoch [19383/20000], Loss: -21.060562133789062, Learning Rate: 0.005\n",
      "Epoch [19384/20000], Loss: -20.992767333984375, Learning Rate: 0.005\n",
      "Epoch [19385/20000], Loss: -20.94610595703125, Learning Rate: 0.005\n",
      "Epoch [19386/20000], Loss: -20.936996459960938, Learning Rate: 0.005\n",
      "Epoch [19387/20000], Loss: -20.990203857421875, Learning Rate: 0.005\n",
      "Epoch [19388/20000], Loss: -21.091751098632812, Learning Rate: 0.005\n",
      "Epoch [19389/20000], Loss: -21.211837768554688, Learning Rate: 0.005\n",
      "Epoch [19390/20000], Loss: -21.300369262695312, Learning Rate: 0.005\n",
      "Epoch [19391/20000], Loss: -21.329132080078125, Learning Rate: 0.005\n",
      "Epoch [19392/20000], Loss: -21.301605224609375, Learning Rate: 0.005\n",
      "Epoch [19393/20000], Loss: -21.248062133789062, Learning Rate: 0.005\n",
      "Epoch [19394/20000], Loss: -21.206680297851562, Learning Rate: 0.005\n",
      "Epoch [19395/20000], Loss: -21.201705932617188, Learning Rate: 0.005\n",
      "Epoch [19396/20000], Loss: -21.236724853515625, Learning Rate: 0.005\n",
      "Epoch [19397/20000], Loss: -21.289031982421875, Learning Rate: 0.005\n",
      "Epoch [19398/20000], Loss: -21.329742431640625, Learning Rate: 0.005\n",
      "Epoch [19399/20000], Loss: -21.340103149414062, Learning Rate: 0.005\n",
      "Epoch [19400/20000], Loss: -21.322311401367188, Learning Rate: 0.005\n",
      "Epoch [19401/20000], Loss: -21.294418334960938, Learning Rate: 0.005\n",
      "Epoch [19402/20000], Loss: -21.276641845703125, Learning Rate: 0.005\n",
      "Epoch [19403/20000], Loss: -21.280029296875, Learning Rate: 0.005\n",
      "Epoch [19404/20000], Loss: -21.300567626953125, Learning Rate: 0.005\n",
      "Epoch [19405/20000], Loss: -21.324905395507812, Learning Rate: 0.005\n",
      "Epoch [19406/20000], Loss: -21.339630126953125, Learning Rate: 0.005\n",
      "Epoch [19407/20000], Loss: -21.339202880859375, Learning Rate: 0.005\n",
      "Epoch [19408/20000], Loss: -21.328369140625, Learning Rate: 0.005\n",
      "Epoch [19409/20000], Loss: -21.316970825195312, Learning Rate: 0.005\n",
      "Epoch [19410/20000], Loss: -21.313079833984375, Learning Rate: 0.005\n",
      "Epoch [19411/20000], Loss: -21.318527221679688, Learning Rate: 0.005\n",
      "Epoch [19412/20000], Loss: -21.32928466796875, Learning Rate: 0.005\n",
      "Epoch [19413/20000], Loss: -21.338790893554688, Learning Rate: 0.005\n",
      "Epoch [19414/20000], Loss: -21.342315673828125, Learning Rate: 0.005\n",
      "Epoch [19415/20000], Loss: -21.339385986328125, Learning Rate: 0.005\n",
      "Epoch [19416/20000], Loss: -21.333572387695312, Learning Rate: 0.005\n",
      "Epoch [19417/20000], Loss: -21.3291015625, Learning Rate: 0.005\n",
      "Epoch [19418/20000], Loss: -21.32861328125, Learning Rate: 0.005\n",
      "Epoch [19419/20000], Loss: -21.332122802734375, Learning Rate: 0.005\n",
      "Epoch [19420/20000], Loss: -21.337173461914062, Learning Rate: 0.005\n",
      "Epoch [19421/20000], Loss: -21.341094970703125, Learning Rate: 0.005\n",
      "Epoch [19422/20000], Loss: -21.342193603515625, Learning Rate: 0.005\n",
      "Epoch [19423/20000], Loss: -21.340713500976562, Learning Rate: 0.005\n",
      "Epoch [19424/20000], Loss: -21.338287353515625, Learning Rate: 0.005\n",
      "Epoch [19425/20000], Loss: -21.336578369140625, Learning Rate: 0.005\n",
      "Epoch [19426/20000], Loss: -21.336654663085938, Learning Rate: 0.005\n",
      "Epoch [19427/20000], Loss: -21.3382568359375, Learning Rate: 0.005\n",
      "Epoch [19428/20000], Loss: -21.340469360351562, Learning Rate: 0.005\n",
      "Epoch [19429/20000], Loss: -21.342208862304688, Learning Rate: 0.005\n",
      "Epoch [19430/20000], Loss: -21.342880249023438, Learning Rate: 0.005\n",
      "Epoch [19431/20000], Loss: -21.342391967773438, Learning Rate: 0.005\n",
      "Epoch [19432/20000], Loss: -21.3414306640625, Learning Rate: 0.005\n",
      "Epoch [19433/20000], Loss: -21.340545654296875, Learning Rate: 0.005\n",
      "Epoch [19434/20000], Loss: -21.340286254882812, Learning Rate: 0.005\n",
      "Epoch [19435/20000], Loss: -21.340774536132812, Learning Rate: 0.005\n",
      "Epoch [19436/20000], Loss: -21.341629028320312, Learning Rate: 0.005\n",
      "Epoch [19437/20000], Loss: -21.342437744140625, Learning Rate: 0.005\n",
      "Epoch [19438/20000], Loss: -21.343017578125, Learning Rate: 0.005\n",
      "Epoch [19439/20000], Loss: -21.343048095703125, Learning Rate: 0.005\n",
      "Epoch [19440/20000], Loss: -21.342742919921875, Learning Rate: 0.005\n",
      "Epoch [19441/20000], Loss: -21.34234619140625, Learning Rate: 0.005\n",
      "Epoch [19442/20000], Loss: -21.342041015625, Learning Rate: 0.005\n",
      "Epoch [19443/20000], Loss: -21.341949462890625, Learning Rate: 0.005\n",
      "Epoch [19444/20000], Loss: -21.3421630859375, Learning Rate: 0.005\n",
      "Epoch [19445/20000], Loss: -21.342529296875, Learning Rate: 0.005\n",
      "Epoch [19446/20000], Loss: -21.342849731445312, Learning Rate: 0.005\n",
      "Epoch [19447/20000], Loss: -21.34307861328125, Learning Rate: 0.005\n",
      "Epoch [19448/20000], Loss: -21.343154907226562, Learning Rate: 0.005\n",
      "Epoch [19449/20000], Loss: -21.343002319335938, Learning Rate: 0.005\n",
      "Epoch [19450/20000], Loss: -21.34283447265625, Learning Rate: 0.005\n",
      "Epoch [19451/20000], Loss: -21.342575073242188, Learning Rate: 0.005\n",
      "Epoch [19452/20000], Loss: -21.34246826171875, Learning Rate: 0.005\n",
      "Epoch [19453/20000], Loss: -21.342376708984375, Learning Rate: 0.005\n",
      "Epoch [19454/20000], Loss: -21.342254638671875, Learning Rate: 0.005\n",
      "Epoch [19455/20000], Loss: -21.3421630859375, Learning Rate: 0.005\n",
      "Epoch [19456/20000], Loss: -21.341888427734375, Learning Rate: 0.005\n",
      "Epoch [19457/20000], Loss: -21.341461181640625, Learning Rate: 0.005\n",
      "Epoch [19458/20000], Loss: -21.340606689453125, Learning Rate: 0.005\n",
      "Epoch [19459/20000], Loss: -21.339462280273438, Learning Rate: 0.005\n",
      "Epoch [19460/20000], Loss: -21.337722778320312, Learning Rate: 0.005\n",
      "Epoch [19461/20000], Loss: -21.335205078125, Learning Rate: 0.005\n",
      "Epoch [19462/20000], Loss: -21.331588745117188, Learning Rate: 0.005\n",
      "Epoch [19463/20000], Loss: -21.326248168945312, Learning Rate: 0.005\n",
      "Epoch [19464/20000], Loss: -21.318435668945312, Learning Rate: 0.005\n",
      "Epoch [19465/20000], Loss: -21.307235717773438, Learning Rate: 0.005\n",
      "Epoch [19466/20000], Loss: -21.291152954101562, Learning Rate: 0.005\n",
      "Epoch [19467/20000], Loss: -21.2689208984375, Learning Rate: 0.005\n",
      "Epoch [19468/20000], Loss: -21.239349365234375, Learning Rate: 0.005\n",
      "Epoch [19469/20000], Loss: -21.2032470703125, Learning Rate: 0.005\n",
      "Epoch [19470/20000], Loss: -21.164108276367188, Learning Rate: 0.005\n",
      "Epoch [19471/20000], Loss: -21.131591796875, Learning Rate: 0.005\n",
      "Epoch [19472/20000], Loss: -21.119171142578125, Learning Rate: 0.005\n",
      "Epoch [19473/20000], Loss: -21.140380859375, Learning Rate: 0.005\n",
      "Epoch [19474/20000], Loss: -21.195297241210938, Learning Rate: 0.005\n",
      "Epoch [19475/20000], Loss: -21.266098022460938, Learning Rate: 0.005\n",
      "Epoch [19476/20000], Loss: -21.32275390625, Learning Rate: 0.005\n",
      "Epoch [19477/20000], Loss: -21.34356689453125, Learning Rate: 0.005\n",
      "Epoch [19478/20000], Loss: -21.328323364257812, Learning Rate: 0.005\n",
      "Epoch [19479/20000], Loss: -21.294952392578125, Learning Rate: 0.005\n",
      "Epoch [19480/20000], Loss: -21.26763916015625, Learning Rate: 0.005\n",
      "Epoch [19481/20000], Loss: -21.263214111328125, Learning Rate: 0.005\n",
      "Epoch [19482/20000], Loss: -21.283584594726562, Learning Rate: 0.005\n",
      "Epoch [19483/20000], Loss: -21.314971923828125, Learning Rate: 0.005\n",
      "Epoch [19484/20000], Loss: -21.338455200195312, Learning Rate: 0.005\n",
      "Epoch [19485/20000], Loss: -21.342971801757812, Learning Rate: 0.005\n",
      "Epoch [19486/20000], Loss: -21.330978393554688, Learning Rate: 0.005\n",
      "Epoch [19487/20000], Loss: -21.314712524414062, Learning Rate: 0.005\n",
      "Epoch [19488/20000], Loss: -21.306655883789062, Learning Rate: 0.005\n",
      "Epoch [19489/20000], Loss: -21.311721801757812, Learning Rate: 0.005\n",
      "Epoch [19490/20000], Loss: -21.325637817382812, Learning Rate: 0.005\n",
      "Epoch [19491/20000], Loss: -21.3388671875, Learning Rate: 0.005\n",
      "Epoch [19492/20000], Loss: -21.344024658203125, Learning Rate: 0.005\n",
      "Epoch [19493/20000], Loss: -21.340301513671875, Learning Rate: 0.005\n",
      "Epoch [19494/20000], Loss: -21.332366943359375, Learning Rate: 0.005\n",
      "Epoch [19495/20000], Loss: -21.326904296875, Learning Rate: 0.005\n",
      "Epoch [19496/20000], Loss: -21.327545166015625, Learning Rate: 0.005\n",
      "Epoch [19497/20000], Loss: -21.333389282226562, Learning Rate: 0.005\n",
      "Epoch [19498/20000], Loss: -21.340103149414062, Learning Rate: 0.005\n",
      "Epoch [19499/20000], Loss: -21.343917846679688, Learning Rate: 0.005\n",
      "Epoch [19500/20000], Loss: -21.343292236328125, Learning Rate: 0.005\n",
      "Epoch [19501/20000], Loss: -21.339874267578125, Learning Rate: 0.005\n",
      "Epoch [19502/20000], Loss: -21.336685180664062, Learning Rate: 0.005\n",
      "Epoch [19503/20000], Loss: -21.3359375, Learning Rate: 0.005\n",
      "Epoch [19504/20000], Loss: -21.337936401367188, Learning Rate: 0.005\n",
      "Epoch [19505/20000], Loss: -21.341079711914062, Learning Rate: 0.005\n",
      "Epoch [19506/20000], Loss: -21.343673706054688, Learning Rate: 0.005\n",
      "Epoch [19507/20000], Loss: -21.34429931640625, Learning Rate: 0.005\n",
      "Epoch [19508/20000], Loss: -21.343154907226562, Learning Rate: 0.005\n",
      "Epoch [19509/20000], Loss: -21.341552734375, Learning Rate: 0.005\n",
      "Epoch [19510/20000], Loss: -21.340545654296875, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19511/20000], Loss: -21.340744018554688, Learning Rate: 0.005\n",
      "Epoch [19512/20000], Loss: -21.341964721679688, Learning Rate: 0.005\n",
      "Epoch [19513/20000], Loss: -21.343475341796875, Learning Rate: 0.005\n",
      "Epoch [19514/20000], Loss: -21.3443603515625, Learning Rate: 0.005\n",
      "Epoch [19515/20000], Loss: -21.344406127929688, Learning Rate: 0.005\n",
      "Epoch [19516/20000], Loss: -21.343856811523438, Learning Rate: 0.005\n",
      "Epoch [19517/20000], Loss: -21.343109130859375, Learning Rate: 0.005\n",
      "Epoch [19518/20000], Loss: -21.342727661132812, Learning Rate: 0.005\n",
      "Epoch [19519/20000], Loss: -21.342910766601562, Learning Rate: 0.005\n",
      "Epoch [19520/20000], Loss: -21.343429565429688, Learning Rate: 0.005\n",
      "Epoch [19521/20000], Loss: -21.344085693359375, Learning Rate: 0.005\n",
      "Epoch [19522/20000], Loss: -21.34454345703125, Learning Rate: 0.005\n",
      "Epoch [19523/20000], Loss: -21.344635009765625, Learning Rate: 0.005\n",
      "Epoch [19524/20000], Loss: -21.344467163085938, Learning Rate: 0.005\n",
      "Epoch [19525/20000], Loss: -21.344161987304688, Learning Rate: 0.005\n",
      "Epoch [19526/20000], Loss: -21.344009399414062, Learning Rate: 0.005\n",
      "Epoch [19527/20000], Loss: -21.34393310546875, Learning Rate: 0.005\n",
      "Epoch [19528/20000], Loss: -21.344085693359375, Learning Rate: 0.005\n",
      "Epoch [19529/20000], Loss: -21.344436645507812, Learning Rate: 0.005\n",
      "Epoch [19530/20000], Loss: -21.344650268554688, Learning Rate: 0.005\n",
      "Epoch [19531/20000], Loss: -21.344741821289062, Learning Rate: 0.005\n",
      "Epoch [19532/20000], Loss: -21.344757080078125, Learning Rate: 0.005\n",
      "Epoch [19533/20000], Loss: -21.34466552734375, Learning Rate: 0.005\n",
      "Epoch [19534/20000], Loss: -21.344573974609375, Learning Rate: 0.005\n",
      "Epoch [19535/20000], Loss: -21.34454345703125, Learning Rate: 0.005\n",
      "Epoch [19536/20000], Loss: -21.344558715820312, Learning Rate: 0.005\n",
      "Epoch [19537/20000], Loss: -21.344619750976562, Learning Rate: 0.005\n",
      "Epoch [19538/20000], Loss: -21.344741821289062, Learning Rate: 0.005\n",
      "Epoch [19539/20000], Loss: -21.3448486328125, Learning Rate: 0.005\n",
      "Epoch [19540/20000], Loss: -21.344940185546875, Learning Rate: 0.005\n",
      "Epoch [19541/20000], Loss: -21.344833374023438, Learning Rate: 0.005\n",
      "Epoch [19542/20000], Loss: -21.34478759765625, Learning Rate: 0.005\n",
      "Epoch [19543/20000], Loss: -21.344741821289062, Learning Rate: 0.005\n",
      "Epoch [19544/20000], Loss: -21.344650268554688, Learning Rate: 0.005\n",
      "Epoch [19545/20000], Loss: -21.344482421875, Learning Rate: 0.005\n",
      "Epoch [19546/20000], Loss: -21.344436645507812, Learning Rate: 0.005\n",
      "Epoch [19547/20000], Loss: -21.344253540039062, Learning Rate: 0.005\n",
      "Epoch [19548/20000], Loss: -21.343978881835938, Learning Rate: 0.005\n",
      "Epoch [19549/20000], Loss: -21.343612670898438, Learning Rate: 0.005\n",
      "Epoch [19550/20000], Loss: -21.343002319335938, Learning Rate: 0.005\n",
      "Epoch [19551/20000], Loss: -21.342025756835938, Learning Rate: 0.005\n",
      "Epoch [19552/20000], Loss: -21.340591430664062, Learning Rate: 0.005\n",
      "Epoch [19553/20000], Loss: -21.338409423828125, Learning Rate: 0.005\n",
      "Epoch [19554/20000], Loss: -21.335113525390625, Learning Rate: 0.005\n",
      "Epoch [19555/20000], Loss: -21.330169677734375, Learning Rate: 0.005\n",
      "Epoch [19556/20000], Loss: -21.322738647460938, Learning Rate: 0.005\n",
      "Epoch [19557/20000], Loss: -21.311416625976562, Learning Rate: 0.005\n",
      "Epoch [19558/20000], Loss: -21.294174194335938, Learning Rate: 0.005\n",
      "Epoch [19559/20000], Loss: -21.268447875976562, Learning Rate: 0.005\n",
      "Epoch [19560/20000], Loss: -21.230117797851562, Learning Rate: 0.005\n",
      "Epoch [19561/20000], Loss: -21.175949096679688, Learning Rate: 0.005\n",
      "Epoch [19562/20000], Loss: -21.100860595703125, Learning Rate: 0.005\n",
      "Epoch [19563/20000], Loss: -21.009262084960938, Learning Rate: 0.005\n",
      "Epoch [19564/20000], Loss: -20.90771484375, Learning Rate: 0.005\n",
      "Epoch [19565/20000], Loss: -20.832977294921875, Learning Rate: 0.005\n",
      "Epoch [19566/20000], Loss: -20.816513061523438, Learning Rate: 0.005\n",
      "Epoch [19567/20000], Loss: -20.902145385742188, Learning Rate: 0.005\n",
      "Epoch [19568/20000], Loss: -21.063201904296875, Learning Rate: 0.005\n",
      "Epoch [19569/20000], Loss: -21.2354736328125, Learning Rate: 0.005\n",
      "Epoch [19570/20000], Loss: -21.332748413085938, Learning Rate: 0.005\n",
      "Epoch [19571/20000], Loss: -21.324142456054688, Learning Rate: 0.005\n",
      "Epoch [19572/20000], Loss: -21.245269775390625, Learning Rate: 0.005\n",
      "Epoch [19573/20000], Loss: -21.165756225585938, Learning Rate: 0.005\n",
      "Epoch [19574/20000], Loss: -21.146499633789062, Learning Rate: 0.005\n",
      "Epoch [19575/20000], Loss: -21.196441650390625, Learning Rate: 0.005\n",
      "Epoch [19576/20000], Loss: -21.2784423828125, Learning Rate: 0.005\n",
      "Epoch [19577/20000], Loss: -21.333404541015625, Learning Rate: 0.005\n",
      "Epoch [19578/20000], Loss: -21.332611083984375, Learning Rate: 0.005\n",
      "Epoch [19579/20000], Loss: -21.292724609375, Learning Rate: 0.005\n",
      "Epoch [19580/20000], Loss: -21.255523681640625, Learning Rate: 0.005\n",
      "Epoch [19581/20000], Loss: -21.253555297851562, Learning Rate: 0.005\n",
      "Epoch [19582/20000], Loss: -21.2857666015625, Learning Rate: 0.005\n",
      "Epoch [19583/20000], Loss: -21.325485229492188, Learning Rate: 0.005\n",
      "Epoch [19584/20000], Loss: -21.343673706054688, Learning Rate: 0.005\n",
      "Epoch [19585/20000], Loss: -21.332992553710938, Learning Rate: 0.005\n",
      "Epoch [19586/20000], Loss: -21.309524536132812, Learning Rate: 0.005\n",
      "Epoch [19587/20000], Loss: -21.2958984375, Learning Rate: 0.005\n",
      "Epoch [19588/20000], Loss: -21.303268432617188, Learning Rate: 0.005\n",
      "Epoch [19589/20000], Loss: -21.324050903320312, Learning Rate: 0.005\n",
      "Epoch [19590/20000], Loss: -21.341873168945312, Learning Rate: 0.005\n",
      "Epoch [19591/20000], Loss: -21.345199584960938, Learning Rate: 0.005\n",
      "Epoch [19592/20000], Loss: -21.3355712890625, Learning Rate: 0.005\n",
      "Epoch [19593/20000], Loss: -21.324172973632812, Learning Rate: 0.005\n",
      "Epoch [19594/20000], Loss: -21.32073974609375, Learning Rate: 0.005\n",
      "Epoch [19595/20000], Loss: -21.327606201171875, Learning Rate: 0.005\n",
      "Epoch [19596/20000], Loss: -21.338211059570312, Learning Rate: 0.005\n",
      "Epoch [19597/20000], Loss: -21.344924926757812, Learning Rate: 0.005\n",
      "Epoch [19598/20000], Loss: -21.344192504882812, Learning Rate: 0.005\n",
      "Epoch [19599/20000], Loss: -21.338623046875, Learning Rate: 0.005\n",
      "Epoch [19600/20000], Loss: -21.334182739257812, Learning Rate: 0.005\n",
      "Epoch [19601/20000], Loss: -21.334213256835938, Learning Rate: 0.005\n",
      "Epoch [19602/20000], Loss: -21.338516235351562, Learning Rate: 0.005\n",
      "Epoch [19603/20000], Loss: -21.343521118164062, Learning Rate: 0.005\n",
      "Epoch [19604/20000], Loss: -21.345809936523438, Learning Rate: 0.005\n",
      "Epoch [19605/20000], Loss: -21.3446044921875, Learning Rate: 0.005\n",
      "Epoch [19606/20000], Loss: -21.341705322265625, Learning Rate: 0.005\n",
      "Epoch [19607/20000], Loss: -21.33978271484375, Learning Rate: 0.005\n",
      "Epoch [19608/20000], Loss: -21.340225219726562, Learning Rate: 0.005\n",
      "Epoch [19609/20000], Loss: -21.342620849609375, Learning Rate: 0.005\n",
      "Epoch [19610/20000], Loss: -21.345077514648438, Learning Rate: 0.005\n",
      "Epoch [19611/20000], Loss: -21.346115112304688, Learning Rate: 0.005\n",
      "Epoch [19612/20000], Loss: -21.3455810546875, Learning Rate: 0.005\n",
      "Epoch [19613/20000], Loss: -21.344192504882812, Learning Rate: 0.005\n",
      "Epoch [19614/20000], Loss: -21.3431396484375, Learning Rate: 0.005\n",
      "Epoch [19615/20000], Loss: -21.343292236328125, Learning Rate: 0.005\n",
      "Epoch [19616/20000], Loss: -21.344345092773438, Learning Rate: 0.005\n",
      "Epoch [19617/20000], Loss: -21.345474243164062, Learning Rate: 0.005\n",
      "Epoch [19618/20000], Loss: -21.346176147460938, Learning Rate: 0.005\n",
      "Epoch [19619/20000], Loss: -21.3460693359375, Learning Rate: 0.005\n",
      "Epoch [19620/20000], Loss: -21.345550537109375, Learning Rate: 0.005\n",
      "Epoch [19621/20000], Loss: -21.344955444335938, Learning Rate: 0.005\n",
      "Epoch [19622/20000], Loss: -21.344863891601562, Learning Rate: 0.005\n",
      "Epoch [19623/20000], Loss: -21.345245361328125, Learning Rate: 0.005\n",
      "Epoch [19624/20000], Loss: -21.3458251953125, Learning Rate: 0.005\n",
      "Epoch [19625/20000], Loss: -21.3463134765625, Learning Rate: 0.005\n",
      "Epoch [19626/20000], Loss: -21.346511840820312, Learning Rate: 0.005\n",
      "Epoch [19627/20000], Loss: -21.346267700195312, Learning Rate: 0.005\n",
      "Epoch [19628/20000], Loss: -21.345947265625, Learning Rate: 0.005\n",
      "Epoch [19629/20000], Loss: -21.345794677734375, Learning Rate: 0.005\n",
      "Epoch [19630/20000], Loss: -21.345840454101562, Learning Rate: 0.005\n",
      "Epoch [19631/20000], Loss: -21.346038818359375, Learning Rate: 0.005\n",
      "Epoch [19632/20000], Loss: -21.346343994140625, Learning Rate: 0.005\n",
      "Epoch [19633/20000], Loss: -21.3465576171875, Learning Rate: 0.005\n",
      "Epoch [19634/20000], Loss: -21.346511840820312, Learning Rate: 0.005\n",
      "Epoch [19635/20000], Loss: -21.346435546875, Learning Rate: 0.005\n",
      "Epoch [19636/20000], Loss: -21.3463134765625, Learning Rate: 0.005\n",
      "Epoch [19637/20000], Loss: -21.34625244140625, Learning Rate: 0.005\n",
      "Epoch [19638/20000], Loss: -21.346389770507812, Learning Rate: 0.005\n",
      "Epoch [19639/20000], Loss: -21.346466064453125, Learning Rate: 0.005\n",
      "Epoch [19640/20000], Loss: -21.34649658203125, Learning Rate: 0.005\n",
      "Epoch [19641/20000], Loss: -21.346664428710938, Learning Rate: 0.005\n",
      "Epoch [19642/20000], Loss: -21.346725463867188, Learning Rate: 0.005\n",
      "Epoch [19643/20000], Loss: -21.346710205078125, Learning Rate: 0.005\n",
      "Epoch [19644/20000], Loss: -21.3466796875, Learning Rate: 0.005\n",
      "Epoch [19645/20000], Loss: -21.346649169921875, Learning Rate: 0.005\n",
      "Epoch [19646/20000], Loss: -21.346725463867188, Learning Rate: 0.005\n",
      "Epoch [19647/20000], Loss: -21.346694946289062, Learning Rate: 0.005\n",
      "Epoch [19648/20000], Loss: -21.34674072265625, Learning Rate: 0.005\n",
      "Epoch [19649/20000], Loss: -21.346817016601562, Learning Rate: 0.005\n",
      "Epoch [19650/20000], Loss: -21.346832275390625, Learning Rate: 0.005\n",
      "Epoch [19651/20000], Loss: -21.346878051757812, Learning Rate: 0.005\n",
      "Epoch [19652/20000], Loss: -21.346893310546875, Learning Rate: 0.005\n",
      "Epoch [19653/20000], Loss: -21.346923828125, Learning Rate: 0.005\n",
      "Epoch [19654/20000], Loss: -21.346878051757812, Learning Rate: 0.005\n",
      "Epoch [19655/20000], Loss: -21.346878051757812, Learning Rate: 0.005\n",
      "Epoch [19656/20000], Loss: -21.346878051757812, Learning Rate: 0.005\n",
      "Epoch [19657/20000], Loss: -21.346939086914062, Learning Rate: 0.005\n",
      "Epoch [19658/20000], Loss: -21.34698486328125, Learning Rate: 0.005\n",
      "Epoch [19659/20000], Loss: -21.34698486328125, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19660/20000], Loss: -21.347000122070312, Learning Rate: 0.005\n",
      "Epoch [19661/20000], Loss: -21.347030639648438, Learning Rate: 0.005\n",
      "Epoch [19662/20000], Loss: -21.347015380859375, Learning Rate: 0.005\n",
      "Epoch [19663/20000], Loss: -21.347076416015625, Learning Rate: 0.005\n",
      "Epoch [19664/20000], Loss: -21.347091674804688, Learning Rate: 0.005\n",
      "Epoch [19665/20000], Loss: -21.3470458984375, Learning Rate: 0.005\n",
      "Epoch [19666/20000], Loss: -21.34710693359375, Learning Rate: 0.005\n",
      "Epoch [19667/20000], Loss: -21.34710693359375, Learning Rate: 0.005\n",
      "Epoch [19668/20000], Loss: -21.347061157226562, Learning Rate: 0.005\n",
      "Epoch [19669/20000], Loss: -21.34710693359375, Learning Rate: 0.005\n",
      "Epoch [19670/20000], Loss: -21.3470458984375, Learning Rate: 0.005\n",
      "Epoch [19671/20000], Loss: -21.347015380859375, Learning Rate: 0.005\n",
      "Epoch [19672/20000], Loss: -21.346908569335938, Learning Rate: 0.005\n",
      "Epoch [19673/20000], Loss: -21.346786499023438, Learning Rate: 0.005\n",
      "Epoch [19674/20000], Loss: -21.346603393554688, Learning Rate: 0.005\n",
      "Epoch [19675/20000], Loss: -21.346282958984375, Learning Rate: 0.005\n",
      "Epoch [19676/20000], Loss: -21.34576416015625, Learning Rate: 0.005\n",
      "Epoch [19677/20000], Loss: -21.3450927734375, Learning Rate: 0.005\n",
      "Epoch [19678/20000], Loss: -21.343963623046875, Learning Rate: 0.005\n",
      "Epoch [19679/20000], Loss: -21.342300415039062, Learning Rate: 0.005\n",
      "Epoch [19680/20000], Loss: -21.339752197265625, Learning Rate: 0.005\n",
      "Epoch [19681/20000], Loss: -21.335769653320312, Learning Rate: 0.005\n",
      "Epoch [19682/20000], Loss: -21.329666137695312, Learning Rate: 0.005\n",
      "Epoch [19683/20000], Loss: -21.320297241210938, Learning Rate: 0.005\n",
      "Epoch [19684/20000], Loss: -21.305953979492188, Learning Rate: 0.005\n",
      "Epoch [19685/20000], Loss: -21.28448486328125, Learning Rate: 0.005\n",
      "Epoch [19686/20000], Loss: -21.2532958984375, Learning Rate: 0.005\n",
      "Epoch [19687/20000], Loss: -21.210159301757812, Learning Rate: 0.005\n",
      "Epoch [19688/20000], Loss: -21.155715942382812, Learning Rate: 0.005\n",
      "Epoch [19689/20000], Loss: -21.097122192382812, Learning Rate: 0.005\n",
      "Epoch [19690/20000], Loss: -21.052017211914062, Learning Rate: 0.005\n",
      "Epoch [19691/20000], Loss: -21.046188354492188, Learning Rate: 0.005\n",
      "Epoch [19692/20000], Loss: -21.097808837890625, Learning Rate: 0.005\n",
      "Epoch [19693/20000], Loss: -21.193405151367188, Learning Rate: 0.005\n",
      "Epoch [19694/20000], Loss: -21.28759765625, Learning Rate: 0.005\n",
      "Epoch [19695/20000], Loss: -21.33306884765625, Learning Rate: 0.005\n",
      "Epoch [19696/20000], Loss: -21.318222045898438, Learning Rate: 0.005\n",
      "Epoch [19697/20000], Loss: -21.271377563476562, Learning Rate: 0.005\n",
      "Epoch [19698/20000], Loss: -21.236099243164062, Learning Rate: 0.005\n",
      "Epoch [19699/20000], Loss: -21.240814208984375, Learning Rate: 0.005\n",
      "Epoch [19700/20000], Loss: -21.282073974609375, Learning Rate: 0.005\n",
      "Epoch [19701/20000], Loss: -21.3272705078125, Learning Rate: 0.005\n",
      "Epoch [19702/20000], Loss: -21.344985961914062, Learning Rate: 0.005\n",
      "Epoch [19703/20000], Loss: -21.329055786132812, Learning Rate: 0.005\n",
      "Epoch [19704/20000], Loss: -21.300521850585938, Learning Rate: 0.005\n",
      "Epoch [19705/20000], Loss: -21.286239624023438, Learning Rate: 0.005\n",
      "Epoch [19706/20000], Loss: -21.29705810546875, Learning Rate: 0.005\n",
      "Epoch [19707/20000], Loss: -21.32269287109375, Learning Rate: 0.005\n",
      "Epoch [19708/20000], Loss: -21.342208862304688, Learning Rate: 0.005\n",
      "Epoch [19709/20000], Loss: -21.343490600585938, Learning Rate: 0.005\n",
      "Epoch [19710/20000], Loss: -21.331405639648438, Learning Rate: 0.005\n",
      "Epoch [19711/20000], Loss: -21.320358276367188, Learning Rate: 0.005\n",
      "Epoch [19712/20000], Loss: -21.321044921875, Learning Rate: 0.005\n",
      "Epoch [19713/20000], Loss: -21.332122802734375, Learning Rate: 0.005\n",
      "Epoch [19714/20000], Loss: -21.343719482421875, Learning Rate: 0.005\n",
      "Epoch [19715/20000], Loss: -21.34716796875, Learning Rate: 0.005\n",
      "Epoch [19716/20000], Loss: -21.341781616210938, Learning Rate: 0.005\n",
      "Epoch [19717/20000], Loss: -21.334228515625, Learning Rate: 0.005\n",
      "Epoch [19718/20000], Loss: -21.331573486328125, Learning Rate: 0.005\n",
      "Epoch [19719/20000], Loss: -21.3355712890625, Learning Rate: 0.005\n",
      "Epoch [19720/20000], Loss: -21.34234619140625, Learning Rate: 0.005\n",
      "Epoch [19721/20000], Loss: -21.346572875976562, Learning Rate: 0.005\n",
      "Epoch [19722/20000], Loss: -21.345840454101562, Learning Rate: 0.005\n",
      "Epoch [19723/20000], Loss: -21.342361450195312, Learning Rate: 0.005\n",
      "Epoch [19724/20000], Loss: -21.339950561523438, Learning Rate: 0.005\n",
      "Epoch [19725/20000], Loss: -21.3408203125, Learning Rate: 0.005\n",
      "Epoch [19726/20000], Loss: -21.34417724609375, Learning Rate: 0.005\n",
      "Epoch [19727/20000], Loss: -21.347259521484375, Learning Rate: 0.005\n",
      "Epoch [19728/20000], Loss: -21.348068237304688, Learning Rate: 0.005\n",
      "Epoch [19729/20000], Loss: -21.3468017578125, Learning Rate: 0.005\n",
      "Epoch [19730/20000], Loss: -21.344894409179688, Learning Rate: 0.005\n",
      "Epoch [19731/20000], Loss: -21.34417724609375, Learning Rate: 0.005\n",
      "Epoch [19732/20000], Loss: -21.3450927734375, Learning Rate: 0.005\n",
      "Epoch [19733/20000], Loss: -21.346710205078125, Learning Rate: 0.005\n",
      "Epoch [19734/20000], Loss: -21.34783935546875, Learning Rate: 0.005\n",
      "Epoch [19735/20000], Loss: -21.34783935546875, Learning Rate: 0.005\n",
      "Epoch [19736/20000], Loss: -21.346939086914062, Learning Rate: 0.005\n",
      "Epoch [19737/20000], Loss: -21.346115112304688, Learning Rate: 0.005\n",
      "Epoch [19738/20000], Loss: -21.346023559570312, Learning Rate: 0.005\n",
      "Epoch [19739/20000], Loss: -21.346588134765625, Learning Rate: 0.005\n",
      "Epoch [19740/20000], Loss: -21.347366333007812, Learning Rate: 0.005\n",
      "Epoch [19741/20000], Loss: -21.347900390625, Learning Rate: 0.005\n",
      "Epoch [19742/20000], Loss: -21.347808837890625, Learning Rate: 0.005\n",
      "Epoch [19743/20000], Loss: -21.34735107421875, Learning Rate: 0.005\n",
      "Epoch [19744/20000], Loss: -21.346908569335938, Learning Rate: 0.005\n",
      "Epoch [19745/20000], Loss: -21.346847534179688, Learning Rate: 0.005\n",
      "Epoch [19746/20000], Loss: -21.347137451171875, Learning Rate: 0.005\n",
      "Epoch [19747/20000], Loss: -21.347503662109375, Learning Rate: 0.005\n",
      "Epoch [19748/20000], Loss: -21.347702026367188, Learning Rate: 0.005\n",
      "Epoch [19749/20000], Loss: -21.347686767578125, Learning Rate: 0.005\n",
      "Epoch [19750/20000], Loss: -21.347320556640625, Learning Rate: 0.005\n",
      "Epoch [19751/20000], Loss: -21.347030639648438, Learning Rate: 0.005\n",
      "Epoch [19752/20000], Loss: -21.346771240234375, Learning Rate: 0.005\n",
      "Epoch [19753/20000], Loss: -21.3466796875, Learning Rate: 0.005\n",
      "Epoch [19754/20000], Loss: -21.346588134765625, Learning Rate: 0.005\n",
      "Epoch [19755/20000], Loss: -21.346359252929688, Learning Rate: 0.005\n",
      "Epoch [19756/20000], Loss: -21.345809936523438, Learning Rate: 0.005\n",
      "Epoch [19757/20000], Loss: -21.345016479492188, Learning Rate: 0.005\n",
      "Epoch [19758/20000], Loss: -21.344009399414062, Learning Rate: 0.005\n",
      "Epoch [19759/20000], Loss: -21.342681884765625, Learning Rate: 0.005\n",
      "Epoch [19760/20000], Loss: -21.340972900390625, Learning Rate: 0.005\n",
      "Epoch [19761/20000], Loss: -21.338607788085938, Learning Rate: 0.005\n",
      "Epoch [19762/20000], Loss: -21.335464477539062, Learning Rate: 0.005\n",
      "Epoch [19763/20000], Loss: -21.33099365234375, Learning Rate: 0.005\n",
      "Epoch [19764/20000], Loss: -21.324783325195312, Learning Rate: 0.005\n",
      "Epoch [19765/20000], Loss: -21.316207885742188, Learning Rate: 0.005\n",
      "Epoch [19766/20000], Loss: -21.30438232421875, Learning Rate: 0.005\n",
      "Epoch [19767/20000], Loss: -21.288589477539062, Learning Rate: 0.005\n",
      "Epoch [19768/20000], Loss: -21.267166137695312, Learning Rate: 0.005\n",
      "Epoch [19769/20000], Loss: -21.239669799804688, Learning Rate: 0.005\n",
      "Epoch [19770/20000], Loss: -21.204437255859375, Learning Rate: 0.005\n",
      "Epoch [19771/20000], Loss: -21.163787841796875, Learning Rate: 0.005\n",
      "Epoch [19772/20000], Loss: -21.118942260742188, Learning Rate: 0.005\n",
      "Epoch [19773/20000], Loss: -21.080490112304688, Learning Rate: 0.005\n",
      "Epoch [19774/20000], Loss: -21.05548095703125, Learning Rate: 0.005\n",
      "Epoch [19775/20000], Loss: -21.061386108398438, Learning Rate: 0.005\n",
      "Epoch [19776/20000], Loss: -21.099990844726562, Learning Rate: 0.005\n",
      "Epoch [19777/20000], Loss: -21.170394897460938, Learning Rate: 0.005\n",
      "Epoch [19778/20000], Loss: -21.249679565429688, Learning Rate: 0.005\n",
      "Epoch [19779/20000], Loss: -21.314498901367188, Learning Rate: 0.005\n",
      "Epoch [19780/20000], Loss: -21.346084594726562, Learning Rate: 0.005\n",
      "Epoch [19781/20000], Loss: -21.341934204101562, Learning Rate: 0.005\n",
      "Epoch [19782/20000], Loss: -21.313644409179688, Learning Rate: 0.005\n",
      "Epoch [19783/20000], Loss: -21.279541015625, Learning Rate: 0.005\n",
      "Epoch [19784/20000], Loss: -21.25726318359375, Learning Rate: 0.005\n",
      "Epoch [19785/20000], Loss: -21.255615234375, Learning Rate: 0.005\n",
      "Epoch [19786/20000], Loss: -21.274856567382812, Learning Rate: 0.005\n",
      "Epoch [19787/20000], Loss: -21.304473876953125, Learning Rate: 0.005\n",
      "Epoch [19788/20000], Loss: -21.331771850585938, Learning Rate: 0.005\n",
      "Epoch [19789/20000], Loss: -21.34649658203125, Learning Rate: 0.005\n",
      "Epoch [19790/20000], Loss: -21.346145629882812, Learning Rate: 0.005\n",
      "Epoch [19791/20000], Loss: -21.335052490234375, Learning Rate: 0.005\n",
      "Epoch [19792/20000], Loss: -21.321441650390625, Learning Rate: 0.005\n",
      "Epoch [19793/20000], Loss: -21.312652587890625, Learning Rate: 0.005\n",
      "Epoch [19794/20000], Loss: -21.312606811523438, Learning Rate: 0.005\n",
      "Epoch [19795/20000], Loss: -21.32049560546875, Learning Rate: 0.005\n",
      "Epoch [19796/20000], Loss: -21.332122802734375, Learning Rate: 0.005\n",
      "Epoch [19797/20000], Loss: -21.342483520507812, Learning Rate: 0.005\n",
      "Epoch [19798/20000], Loss: -21.347930908203125, Learning Rate: 0.005\n",
      "Epoch [19799/20000], Loss: -21.347686767578125, Learning Rate: 0.005\n",
      "Epoch [19800/20000], Loss: -21.34356689453125, Learning Rate: 0.005\n",
      "Epoch [19801/20000], Loss: -21.338272094726562, Learning Rate: 0.005\n",
      "Epoch [19802/20000], Loss: -21.33489990234375, Learning Rate: 0.005\n",
      "Epoch [19803/20000], Loss: -21.33447265625, Learning Rate: 0.005\n",
      "Epoch [19804/20000], Loss: -21.337158203125, Learning Rate: 0.005\n",
      "Epoch [19805/20000], Loss: -21.341461181640625, Learning Rate: 0.005\n",
      "Epoch [19806/20000], Loss: -21.345565795898438, Learning Rate: 0.005\n",
      "Epoch [19807/20000], Loss: -21.348358154296875, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19808/20000], Loss: -21.349044799804688, Learning Rate: 0.005\n",
      "Epoch [19809/20000], Loss: -21.348052978515625, Learning Rate: 0.005\n",
      "Epoch [19810/20000], Loss: -21.346237182617188, Learning Rate: 0.005\n",
      "Epoch [19811/20000], Loss: -21.344497680664062, Learning Rate: 0.005\n",
      "Epoch [19812/20000], Loss: -21.343658447265625, Learning Rate: 0.005\n",
      "Epoch [19813/20000], Loss: -21.343917846679688, Learning Rate: 0.005\n",
      "Epoch [19814/20000], Loss: -21.3450927734375, Learning Rate: 0.005\n",
      "Epoch [19815/20000], Loss: -21.346710205078125, Learning Rate: 0.005\n",
      "Epoch [19816/20000], Loss: -21.348236083984375, Learning Rate: 0.005\n",
      "Epoch [19817/20000], Loss: -21.349197387695312, Learning Rate: 0.005\n",
      "Epoch [19818/20000], Loss: -21.349517822265625, Learning Rate: 0.005\n",
      "Epoch [19819/20000], Loss: -21.349258422851562, Learning Rate: 0.005\n",
      "Epoch [19820/20000], Loss: -21.348709106445312, Learning Rate: 0.005\n",
      "Epoch [19821/20000], Loss: -21.348129272460938, Learning Rate: 0.005\n",
      "Epoch [19822/20000], Loss: -21.34765625, Learning Rate: 0.005\n",
      "Epoch [19823/20000], Loss: -21.347503662109375, Learning Rate: 0.005\n",
      "Epoch [19824/20000], Loss: -21.3477783203125, Learning Rate: 0.005\n",
      "Epoch [19825/20000], Loss: -21.348175048828125, Learning Rate: 0.005\n",
      "Epoch [19826/20000], Loss: -21.348739624023438, Learning Rate: 0.005\n",
      "Epoch [19827/20000], Loss: -21.3492431640625, Learning Rate: 0.005\n",
      "Epoch [19828/20000], Loss: -21.349609375, Learning Rate: 0.005\n",
      "Epoch [19829/20000], Loss: -21.34979248046875, Learning Rate: 0.005\n",
      "Epoch [19830/20000], Loss: -21.34979248046875, Learning Rate: 0.005\n",
      "Epoch [19831/20000], Loss: -21.349685668945312, Learning Rate: 0.005\n",
      "Epoch [19832/20000], Loss: -21.349533081054688, Learning Rate: 0.005\n",
      "Epoch [19833/20000], Loss: -21.349349975585938, Learning Rate: 0.005\n",
      "Epoch [19834/20000], Loss: -21.349212646484375, Learning Rate: 0.005\n",
      "Epoch [19835/20000], Loss: -21.349227905273438, Learning Rate: 0.005\n",
      "Epoch [19836/20000], Loss: -21.349288940429688, Learning Rate: 0.005\n",
      "Epoch [19837/20000], Loss: -21.349334716796875, Learning Rate: 0.005\n",
      "Epoch [19838/20000], Loss: -21.349472045898438, Learning Rate: 0.005\n",
      "Epoch [19839/20000], Loss: -21.349700927734375, Learning Rate: 0.005\n",
      "Epoch [19840/20000], Loss: -21.349868774414062, Learning Rate: 0.005\n",
      "Epoch [19841/20000], Loss: -21.350006103515625, Learning Rate: 0.005\n",
      "Epoch [19842/20000], Loss: -21.350051879882812, Learning Rate: 0.005\n",
      "Epoch [19843/20000], Loss: -21.350067138671875, Learning Rate: 0.005\n",
      "Epoch [19844/20000], Loss: -21.35009765625, Learning Rate: 0.005\n",
      "Epoch [19845/20000], Loss: -21.350006103515625, Learning Rate: 0.005\n",
      "Epoch [19846/20000], Loss: -21.350082397460938, Learning Rate: 0.005\n",
      "Epoch [19847/20000], Loss: -21.35003662109375, Learning Rate: 0.005\n",
      "Epoch [19848/20000], Loss: -21.349929809570312, Learning Rate: 0.005\n",
      "Epoch [19849/20000], Loss: -21.349945068359375, Learning Rate: 0.005\n",
      "Epoch [19850/20000], Loss: -21.349929809570312, Learning Rate: 0.005\n",
      "Epoch [19851/20000], Loss: -21.350006103515625, Learning Rate: 0.005\n",
      "Epoch [19852/20000], Loss: -21.350067138671875, Learning Rate: 0.005\n",
      "Epoch [19853/20000], Loss: -21.349990844726562, Learning Rate: 0.005\n",
      "Epoch [19854/20000], Loss: -21.35009765625, Learning Rate: 0.005\n",
      "Epoch [19855/20000], Loss: -21.350112915039062, Learning Rate: 0.005\n",
      "Epoch [19856/20000], Loss: -21.350067138671875, Learning Rate: 0.005\n",
      "Epoch [19857/20000], Loss: -21.350204467773438, Learning Rate: 0.005\n",
      "Epoch [19858/20000], Loss: -21.35015869140625, Learning Rate: 0.005\n",
      "Epoch [19859/20000], Loss: -21.350234985351562, Learning Rate: 0.005\n",
      "Epoch [19860/20000], Loss: -21.350250244140625, Learning Rate: 0.005\n",
      "Epoch [19861/20000], Loss: -21.35028076171875, Learning Rate: 0.005\n",
      "Epoch [19862/20000], Loss: -21.350326538085938, Learning Rate: 0.005\n",
      "Epoch [19863/20000], Loss: -21.350387573242188, Learning Rate: 0.005\n",
      "Epoch [19864/20000], Loss: -21.350387573242188, Learning Rate: 0.005\n",
      "Epoch [19865/20000], Loss: -21.350418090820312, Learning Rate: 0.005\n",
      "Epoch [19866/20000], Loss: -21.35040283203125, Learning Rate: 0.005\n",
      "Epoch [19867/20000], Loss: -21.350372314453125, Learning Rate: 0.005\n",
      "Epoch [19868/20000], Loss: -21.350540161132812, Learning Rate: 0.005\n",
      "Epoch [19869/20000], Loss: -21.350494384765625, Learning Rate: 0.005\n",
      "Epoch [19870/20000], Loss: -21.350509643554688, Learning Rate: 0.005\n",
      "Epoch [19871/20000], Loss: -21.350555419921875, Learning Rate: 0.005\n",
      "Epoch [19872/20000], Loss: -21.350555419921875, Learning Rate: 0.005\n",
      "Epoch [19873/20000], Loss: -21.350555419921875, Learning Rate: 0.005\n",
      "Epoch [19874/20000], Loss: -21.35064697265625, Learning Rate: 0.005\n",
      "Epoch [19875/20000], Loss: -21.35052490234375, Learning Rate: 0.005\n",
      "Epoch [19876/20000], Loss: -21.35064697265625, Learning Rate: 0.005\n",
      "Epoch [19877/20000], Loss: -21.350631713867188, Learning Rate: 0.005\n",
      "Epoch [19878/20000], Loss: -21.350601196289062, Learning Rate: 0.005\n",
      "Epoch [19879/20000], Loss: -21.35064697265625, Learning Rate: 0.005\n",
      "Epoch [19880/20000], Loss: -21.350570678710938, Learning Rate: 0.005\n",
      "Epoch [19881/20000], Loss: -21.350616455078125, Learning Rate: 0.005\n",
      "Epoch [19882/20000], Loss: -21.350601196289062, Learning Rate: 0.005\n",
      "Epoch [19883/20000], Loss: -21.350555419921875, Learning Rate: 0.005\n",
      "Epoch [19884/20000], Loss: -21.350540161132812, Learning Rate: 0.005\n",
      "Epoch [19885/20000], Loss: -21.350433349609375, Learning Rate: 0.005\n",
      "Epoch [19886/20000], Loss: -21.350311279296875, Learning Rate: 0.005\n",
      "Epoch [19887/20000], Loss: -21.350067138671875, Learning Rate: 0.005\n",
      "Epoch [19888/20000], Loss: -21.349884033203125, Learning Rate: 0.005\n",
      "Epoch [19889/20000], Loss: -21.349472045898438, Learning Rate: 0.005\n",
      "Epoch [19890/20000], Loss: -21.348892211914062, Learning Rate: 0.005\n",
      "Epoch [19891/20000], Loss: -21.348037719726562, Learning Rate: 0.005\n",
      "Epoch [19892/20000], Loss: -21.346710205078125, Learning Rate: 0.005\n",
      "Epoch [19893/20000], Loss: -21.344680786132812, Learning Rate: 0.005\n",
      "Epoch [19894/20000], Loss: -21.341659545898438, Learning Rate: 0.005\n",
      "Epoch [19895/20000], Loss: -21.336929321289062, Learning Rate: 0.005\n",
      "Epoch [19896/20000], Loss: -21.329803466796875, Learning Rate: 0.005\n",
      "Epoch [19897/20000], Loss: -21.318695068359375, Learning Rate: 0.005\n",
      "Epoch [19898/20000], Loss: -21.301849365234375, Learning Rate: 0.005\n",
      "Epoch [19899/20000], Loss: -21.275924682617188, Learning Rate: 0.005\n",
      "Epoch [19900/20000], Loss: -21.237350463867188, Learning Rate: 0.005\n",
      "Epoch [19901/20000], Loss: -21.179824829101562, Learning Rate: 0.005\n",
      "Epoch [19902/20000], Loss: -21.10009765625, Learning Rate: 0.005\n",
      "Epoch [19903/20000], Loss: -20.99359130859375, Learning Rate: 0.005\n",
      "Epoch [19904/20000], Loss: -20.87451171875, Learning Rate: 0.005\n",
      "Epoch [19905/20000], Loss: -20.763015747070312, Learning Rate: 0.005\n",
      "Epoch [19906/20000], Loss: -20.721176147460938, Learning Rate: 0.005\n",
      "Epoch [19907/20000], Loss: -20.783615112304688, Learning Rate: 0.005\n",
      "Epoch [19908/20000], Loss: -20.966217041015625, Learning Rate: 0.005\n",
      "Epoch [19909/20000], Loss: -21.1820068359375, Learning Rate: 0.005\n",
      "Epoch [19910/20000], Loss: -21.326080322265625, Learning Rate: 0.005\n",
      "Epoch [19911/20000], Loss: -21.336685180664062, Learning Rate: 0.005\n",
      "Epoch [19912/20000], Loss: -21.24505615234375, Learning Rate: 0.005\n",
      "Epoch [19913/20000], Loss: -21.140548706054688, Learning Rate: 0.005\n",
      "Epoch [19914/20000], Loss: -21.104080200195312, Learning Rate: 0.005\n",
      "Epoch [19915/20000], Loss: -21.163864135742188, Learning Rate: 0.005\n",
      "Epoch [19916/20000], Loss: -21.268295288085938, Learning Rate: 0.005\n",
      "Epoch [19917/20000], Loss: -21.341445922851562, Learning Rate: 0.005\n",
      "Epoch [19918/20000], Loss: -21.339996337890625, Learning Rate: 0.005\n",
      "Epoch [19919/20000], Loss: -21.285964965820312, Learning Rate: 0.005\n",
      "Epoch [19920/20000], Loss: -21.237335205078125, Learning Rate: 0.005\n",
      "Epoch [19921/20000], Loss: -21.236358642578125, Learning Rate: 0.005\n",
      "Epoch [19922/20000], Loss: -21.281631469726562, Learning Rate: 0.005\n",
      "Epoch [19923/20000], Loss: -21.332504272460938, Learning Rate: 0.005\n",
      "Epoch [19924/20000], Loss: -21.351119995117188, Learning Rate: 0.005\n",
      "Epoch [19925/20000], Loss: -21.332183837890625, Learning Rate: 0.005\n",
      "Epoch [19926/20000], Loss: -21.30145263671875, Learning Rate: 0.005\n",
      "Epoch [19927/20000], Loss: -21.289260864257812, Learning Rate: 0.005\n",
      "Epoch [19928/20000], Loss: -21.304733276367188, Learning Rate: 0.005\n",
      "Epoch [19929/20000], Loss: -21.332794189453125, Learning Rate: 0.005\n",
      "Epoch [19930/20000], Loss: -21.3502197265625, Learning Rate: 0.005\n",
      "Epoch [19931/20000], Loss: -21.346755981445312, Learning Rate: 0.005\n",
      "Epoch [19932/20000], Loss: -21.331069946289062, Learning Rate: 0.005\n",
      "Epoch [19933/20000], Loss: -21.319854736328125, Learning Rate: 0.005\n",
      "Epoch [19934/20000], Loss: -21.322952270507812, Learning Rate: 0.005\n",
      "Epoch [19935/20000], Loss: -21.3363037109375, Learning Rate: 0.005\n",
      "Epoch [19936/20000], Loss: -21.348556518554688, Learning Rate: 0.005\n",
      "Epoch [19937/20000], Loss: -21.3509521484375, Learning Rate: 0.005\n",
      "Epoch [19938/20000], Loss: -21.344497680664062, Learning Rate: 0.005\n",
      "Epoch [19939/20000], Loss: -21.336822509765625, Learning Rate: 0.005\n",
      "Epoch [19940/20000], Loss: -21.33502197265625, Learning Rate: 0.005\n",
      "Epoch [19941/20000], Loss: -21.340103149414062, Learning Rate: 0.005\n",
      "Epoch [19942/20000], Loss: -21.347442626953125, Learning Rate: 0.005\n",
      "Epoch [19943/20000], Loss: -21.351425170898438, Learning Rate: 0.005\n",
      "Epoch [19944/20000], Loss: -21.350128173828125, Learning Rate: 0.005\n",
      "Epoch [19945/20000], Loss: -21.346038818359375, Learning Rate: 0.005\n",
      "Epoch [19946/20000], Loss: -21.343185424804688, Learning Rate: 0.005\n",
      "Epoch [19947/20000], Loss: -21.343887329101562, Learning Rate: 0.005\n",
      "Epoch [19948/20000], Loss: -21.3472900390625, Learning Rate: 0.005\n",
      "Epoch [19949/20000], Loss: -21.350540161132812, Learning Rate: 0.005\n",
      "Epoch [19950/20000], Loss: -21.351593017578125, Learning Rate: 0.005\n",
      "Epoch [19951/20000], Loss: -21.350357055664062, Learning Rate: 0.005\n",
      "Epoch [19952/20000], Loss: -21.348342895507812, Learning Rate: 0.005\n",
      "Epoch [19953/20000], Loss: -21.347396850585938, Learning Rate: 0.005\n",
      "Epoch [19954/20000], Loss: -21.34814453125, Learning Rate: 0.005\n",
      "Epoch [19955/20000], Loss: -21.349899291992188, Learning Rate: 0.005\n",
      "Epoch [19956/20000], Loss: -21.3514404296875, Learning Rate: 0.005\n",
      "Epoch [19957/20000], Loss: -21.351699829101562, Learning Rate: 0.005\n",
      "Epoch [19958/20000], Loss: -21.351043701171875, Learning Rate: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19959/20000], Loss: -21.350006103515625, Learning Rate: 0.005\n",
      "Epoch [19960/20000], Loss: -21.349639892578125, Learning Rate: 0.005\n",
      "Epoch [19961/20000], Loss: -21.350082397460938, Learning Rate: 0.005\n",
      "Epoch [19962/20000], Loss: -21.350967407226562, Learning Rate: 0.005\n",
      "Epoch [19963/20000], Loss: -21.351654052734375, Learning Rate: 0.005\n",
      "Epoch [19964/20000], Loss: -21.351852416992188, Learning Rate: 0.005\n",
      "Epoch [19965/20000], Loss: -21.351577758789062, Learning Rate: 0.005\n",
      "Epoch [19966/20000], Loss: -21.351150512695312, Learning Rate: 0.005\n",
      "Epoch [19967/20000], Loss: -21.350845336914062, Learning Rate: 0.005\n",
      "Epoch [19968/20000], Loss: -21.350997924804688, Learning Rate: 0.005\n",
      "Epoch [19969/20000], Loss: -21.351348876953125, Learning Rate: 0.005\n",
      "Epoch [19970/20000], Loss: -21.351730346679688, Learning Rate: 0.005\n",
      "Epoch [19971/20000], Loss: -21.35198974609375, Learning Rate: 0.005\n",
      "Epoch [19972/20000], Loss: -21.351974487304688, Learning Rate: 0.005\n",
      "Epoch [19973/20000], Loss: -21.35174560546875, Learning Rate: 0.005\n",
      "Epoch [19974/20000], Loss: -21.351608276367188, Learning Rate: 0.005\n",
      "Epoch [19975/20000], Loss: -21.351577758789062, Learning Rate: 0.005\n",
      "Epoch [19976/20000], Loss: -21.351730346679688, Learning Rate: 0.005\n",
      "Epoch [19977/20000], Loss: -21.351898193359375, Learning Rate: 0.005\n",
      "Epoch [19978/20000], Loss: -21.35205078125, Learning Rate: 0.005\n",
      "Epoch [19979/20000], Loss: -21.352081298828125, Learning Rate: 0.005\n",
      "Epoch [19980/20000], Loss: -21.352066040039062, Learning Rate: 0.005\n",
      "Epoch [19981/20000], Loss: -21.351974487304688, Learning Rate: 0.005\n",
      "Epoch [19982/20000], Loss: -21.351913452148438, Learning Rate: 0.005\n",
      "Epoch [19983/20000], Loss: -21.352020263671875, Learning Rate: 0.005\n",
      "Epoch [19984/20000], Loss: -21.352035522460938, Learning Rate: 0.005\n",
      "Epoch [19985/20000], Loss: -21.352081298828125, Learning Rate: 0.005\n",
      "Epoch [19986/20000], Loss: -21.352188110351562, Learning Rate: 0.005\n",
      "Epoch [19987/20000], Loss: -21.35223388671875, Learning Rate: 0.005\n",
      "Epoch [19988/20000], Loss: -21.352279663085938, Learning Rate: 0.005\n",
      "Epoch [19989/20000], Loss: -21.352218627929688, Learning Rate: 0.005\n",
      "Epoch [19990/20000], Loss: -21.3521728515625, Learning Rate: 0.005\n",
      "Epoch [19991/20000], Loss: -21.3521728515625, Learning Rate: 0.005\n",
      "Epoch [19992/20000], Loss: -21.3521728515625, Learning Rate: 0.005\n",
      "Epoch [19993/20000], Loss: -21.352325439453125, Learning Rate: 0.005\n",
      "Epoch [19994/20000], Loss: -21.352264404296875, Learning Rate: 0.005\n",
      "Epoch [19995/20000], Loss: -21.35235595703125, Learning Rate: 0.005\n",
      "Epoch [19996/20000], Loss: -21.3524169921875, Learning Rate: 0.005\n",
      "Epoch [19997/20000], Loss: -21.352371215820312, Learning Rate: 0.005\n",
      "Epoch [19998/20000], Loss: -21.35235595703125, Learning Rate: 0.005\n",
      "Epoch [19999/20000], Loss: -21.352386474609375, Learning Rate: 0.005\n",
      "Epoch [20000/20000], Loss: -21.352340698242188, Learning Rate: 0.005\n"
     ]
    }
   ],
   "source": [
    "from Inference.PointEstimate import AdamGradientDescent\n",
    "def _MAP(nbiter, std_init,logposterior, dim, device='cpu'):\n",
    "        optimizer = AdamGradientDescent(logposterior, nbiter, .01, .00000001, 50, .5, device, True)\n",
    "\n",
    "        theta0 = torch.empty((1, dim), device=device).normal_(0., std=std_init)\n",
    "        best_theta, best_score, score = optimizer.run(theta0)\n",
    "\n",
    "        return best_theta.detach().clone()\n",
    "\n",
    "theta=_MAP(20000,1., logposterior, param_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mylogpdf(x):\n",
    "    return logposterior(x)\n",
    "def potential(x):\n",
    "    theta=torch.Tensor(x).requires_grad_(True).float()\n",
    "    #print(x)\n",
    "    lp=mylogpdf(theta.unsqueeze(0))\n",
    "    lp.backward()\n",
    "    return -lp.detach().numpy(), -theta.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76949e853f8e4208a4b9525c8770110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "samples, rates = hamiltonian_monte_carlo(100,0,1, potential,\n",
    "                                  initial_position=theta.squeeze().numpy(), \n",
    "                                  initial_step_size=0.002,\n",
    "                                  path_len=100\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2.55600542e-01, -1.36940241e-01,  1.20946191e-01,  5.80011830e-02,\n",
       "         2.49584228e-01,  1.11164279e-01,  3.81458290e-02,  3.91270012e-01,\n",
       "        -4.95542943e-01,  1.97284758e-01,  4.73438025e-01,  1.81126818e-01,\n",
       "         5.27404547e-02, -5.05873799e-01,  8.13803915e-03,  4.09969032e-01,\n",
       "        -2.24281251e-01, -2.66965300e-01,  2.27345200e-03,  1.02012560e-01,\n",
       "         2.72324133e+00,  2.40828827e-01,  1.05487019e-01,  2.08751541e-02,\n",
       "         2.13388458e-01, -7.56162219e-03, -1.90708920e-01,  2.82526398e+00,\n",
       "         9.55456719e-02, -3.51008743e-01, -3.55735630e-01, -1.06725477e-01,\n",
       "        -3.06687206e-01, -2.20429063e-01, -2.68177927e-01,  2.13220525e+00,\n",
       "         1.15140751e-01, -4.51260209e-01,  4.86685455e-01,  2.77369946e-01,\n",
       "        -3.27750564e-01, -4.39222485e-01,  6.51904866e-02, -1.25734866e-01,\n",
       "        -4.40919608e-01, -2.48175740e-01,  1.54078081e-01, -1.76617429e-01,\n",
       "         5.62349483e-02,  3.92896056e-01, -1.60654798e-01, -1.05387203e-01,\n",
       "         3.86104882e-01, -2.68130489e-02,  2.41253860e-02, -6.66940868e-01,\n",
       "        -1.05792128e-01, -2.10080579e-01, -1.57231633e-02, -2.53213704e-01,\n",
       "         3.30226049e-02,  5.76043487e-01,  2.70037763e-02,  1.16371118e-01,\n",
       "         3.45832825e-01,  7.76709318e-02, -1.93793356e-01,  3.85523438e-01,\n",
       "         9.63720679e-02, -1.90133631e-01, -2.42662454e+00,  3.68224829e-02,\n",
       "         1.73544034e-01, -1.33805498e-01,  4.03601378e-01, -1.03318490e-01,\n",
       "        -1.44672841e-01,  1.57809961e+00, -2.37553164e-01,  2.88435191e-01,\n",
       "        -2.86661256e-02, -3.95348184e-02,  6.15547933e-02,  2.86151111e-01,\n",
       "        -1.44678563e-01, -3.71919185e-01, -1.84557080e-01, -1.05350025e-01,\n",
       "         8.76781195e-02, -3.29492718e-01,  1.92035846e-02, -4.37912308e-02,\n",
       "         2.58981347e-01,  2.56818142e-02,  2.26864234e-01, -1.26491278e-01,\n",
       "         1.16232798e-01,  8.12653005e-02,  1.20780319e-01,  4.69895750e-02,\n",
       "        -3.27210009e-01, -1.57044277e-01,  3.81877124e-02,  3.32997650e-01,\n",
       "         1.98405817e-01,  3.58122960e-02,  2.08939984e-02,  2.33797967e-01,\n",
       "         7.07425654e-01,  3.86387467e-01, -3.27488780e-01, -6.74340129e-03,\n",
       "        -1.41404375e-01, -2.97174454e-01, -1.27712369e-01, -2.62450010e-01,\n",
       "         1.80703133e-01,  2.27001593e-01, -6.40030503e-02, -9.72875431e-02,\n",
       "         1.82590675e+00,  3.85402665e-02, -1.50576904e-01, -4.29628715e-02,\n",
       "        -1.06847472e-01, -2.32107610e-01,  1.61540374e-01,  1.99389422e+00,\n",
       "         5.43903671e-02,  2.60634720e-01, -4.93324623e-02,  3.13530773e-01,\n",
       "        -4.20817256e-01,  9.77156833e-02,  8.03402543e-01, -1.68531561e+00,\n",
       "         1.83524191e-02,  1.56323183e-02,  5.53692691e-03, -3.64731431e-01,\n",
       "         4.44248915e-02,  1.56017154e-01, -1.19666636e-01, -3.44647050e-01,\n",
       "        -3.35897893e-01,  3.24150681e-01, -1.69534296e-01, -1.44074345e-02,\n",
       "         1.85418800e-01, -3.17439944e-01,  2.76954442e-01], dtype=float32),\n",
       " array([ 2.8273875e-01,  1.0452934e-01,  9.7281277e-02,  1.2617792e-01,\n",
       "         9.6429437e-02,  4.6270642e-01, -4.6283711e-02,  4.3066275e-01,\n",
       "        -3.8555059e-01,  2.9554129e-01,  6.7915851e-01,  3.8947400e-01,\n",
       "         1.6484790e-01, -4.2674017e-01, -2.4913150e-01,  9.7488165e-01,\n",
       "        -2.9102802e-01, -6.1525449e-02,  2.5110143e-01,  2.1386181e-01,\n",
       "         2.8071096e+00,  2.3176505e-01,  2.8741416e-01, -1.0181861e-02,\n",
       "         1.0652684e-01,  8.4068701e-02, -3.0399835e-01,  3.0030634e+00,\n",
       "         1.0737255e-01, -1.9618103e-01, -8.5927546e-01, -2.3446266e-01,\n",
       "         8.3815403e-02, -9.3865715e-02, -1.7162128e-01,  1.9483411e+00,\n",
       "        -1.4849080e-03, -4.8816743e-01,  5.5516803e-01,  4.2579395e-01,\n",
       "        -3.3516753e-01, -3.3361143e-01,  3.1612569e-01,  2.8371228e-02,\n",
       "        -4.4578442e-01, -2.1704383e-01, -6.3286982e-02, -4.6004602e-01,\n",
       "         7.9805866e-02,  4.9948278e-01, -3.3396262e-01, -7.4756421e-02,\n",
       "         1.2993178e-01, -2.1125322e-02, -7.3434591e-02, -9.5122808e-01,\n",
       "        -2.8677806e-01, -1.1825247e-01,  1.7642288e-01, -2.2784451e-01,\n",
       "        -2.6535472e-01,  3.9948189e-01, -1.8790324e-01,  2.7517337e-01,\n",
       "         8.2541978e-01, -9.9361025e-02, -2.7956834e-01,  2.5696436e-01,\n",
       "        -1.8002836e-02, -3.1910971e-01, -2.4168944e+00,  4.4639308e-02,\n",
       "         3.9667448e-01, -3.7075803e-01,  9.2072582e-01, -3.9178708e-01,\n",
       "         1.3752548e-01,  1.6294852e+00, -3.8570136e-01,  1.7535010e-01,\n",
       "        -3.0267379e-01, -1.3934119e-01,  3.9190236e-02,  5.5441177e-01,\n",
       "        -1.1522134e-01, -3.4215471e-01, -1.8822843e-01, -2.7543134e-03,\n",
       "         5.1417840e-01, -1.7033063e-01, -2.0916378e-01, -4.1603480e-02,\n",
       "         5.5729490e-02,  6.1211754e-03,  8.8283584e-02, -1.7662470e-01,\n",
       "         7.7176481e-02,  2.9833844e-01,  2.3460662e-01, -2.9006263e-02,\n",
       "        -9.0610169e-02, -2.8457483e-02,  1.6811882e-01,  8.5839584e-02,\n",
       "         1.4298828e-01,  1.4199771e-01,  4.8112977e-02,  1.8425287e-01,\n",
       "         6.3752294e-01,  4.1982415e-01, -6.4550853e-01, -2.2436875e-01,\n",
       "        -3.4371421e-01, -2.1952426e-01, -2.0401411e-01, -8.8584447e-01,\n",
       "        -4.8272315e-02,  2.4407326e-01,  4.7241396e-01,  1.5711372e-01,\n",
       "         2.0958107e+00, -4.3769889e-03, -2.5619760e-01, -3.3097136e-01,\n",
       "        -2.9907808e-01,  6.4936198e-02,  5.5853263e-02,  2.2502041e+00,\n",
       "        -2.0022756e-01,  3.5703823e-01, -1.2032438e-01,  1.6529220e-01,\n",
       "        -5.4745388e-01,  2.2941114e-02,  4.6620068e-01, -1.6357523e+00,\n",
       "         2.7021265e-01,  1.8561925e-01,  1.3486682e-01, -3.7446740e-01,\n",
       "         8.7727226e-02, -8.3068743e-02, -4.7376588e-01, -2.8046945e-01,\n",
       "        -4.6857038e-01,  3.3519155e-01,  1.4482188e-01,  2.2759345e-01,\n",
       "         4.4289123e-02,  5.8373496e-02,  2.7944911e-01], dtype=float32),\n",
       " array([ 0.47537372,  0.6993943 ,  0.11996581, -0.03856545,  0.18956071,\n",
       "         0.3365088 , -0.01492669,  0.47764206, -0.37079233,  0.139426  ,\n",
       "         0.8383963 ,  0.2665395 , -0.05735285, -0.3378346 , -0.26709616,\n",
       "         1.3693844 , -0.10633413,  0.12212525, -0.10798396,  0.04995992,\n",
       "         2.9157677 ,  0.32440346,  0.35033202,  0.35367942, -0.09920565,\n",
       "         0.08622568, -0.06420744,  3.0352385 , -0.24272083, -0.27337825,\n",
       "        -0.41031155, -0.09146694,  0.04182044, -0.17328183, -0.12513712,\n",
       "         1.9043479 , -0.0439679 , -0.81598306,  0.5163659 ,  0.6140137 ,\n",
       "        -0.27197772, -0.15563391,  0.5270265 , -0.047671  , -0.8101099 ,\n",
       "        -0.52729326,  0.10280176, -0.5931144 , -0.01930791,  0.1694663 ,\n",
       "        -0.02916048,  0.08920899, -0.13662249, -0.05354768, -0.3009744 ,\n",
       "        -1.0909731 ,  0.08871069, -0.11644059,  0.29485902, -0.06237299,\n",
       "        -0.3906547 ,  0.6065128 , -0.23772247,  0.62857777,  1.1489409 ,\n",
       "        -0.21510243, -0.23215127, -0.0133172 ,  0.18656272, -0.38261583,\n",
       "        -2.6502924 ,  0.6837859 ,  0.46772158, -0.5931511 ,  0.70672286,\n",
       "        -0.7674773 ,  0.02496953,  1.5980203 , -0.16367295, -0.07880543,\n",
       "        -0.3498196 , -0.15384872, -0.14094673,  0.6706144 ,  0.18291157,\n",
       "        -0.00557437,  0.10358714, -0.12567991,  0.55622125, -0.05025366,\n",
       "        -0.34350014,  0.2831408 ,  0.18448015,  0.60218   ,  0.34525064,\n",
       "         0.09229451, -0.00310223, -0.00677381, -0.05254293, -0.01297533,\n",
       "         0.07332887,  0.1488053 ,  0.07453543, -0.02036894,  0.14395729,\n",
       "        -0.09494489, -0.16565777,  0.15293232,  0.850904  ,  0.18416485,\n",
       "        -0.6010766 , -0.14774472, -0.6646226 , -0.49655277, -0.4069064 ,\n",
       "        -0.92888904, -0.03215928,  0.644089  ,  0.32777596,  0.09816633,\n",
       "         1.8833703 ,  0.08484587, -0.13318262, -0.24395318, -0.4767984 ,\n",
       "        -0.00546161, -0.21097973,  2.4421084 , -0.24299684, -0.21569139,\n",
       "        -0.28329864,  0.2938063 , -0.79192704, -0.09775813,  0.635346  ,\n",
       "        -1.4820541 ,  0.14208856, -0.01672261,  0.0389349 , -0.40169308,\n",
       "        -0.19334128, -0.38182592, -0.5908259 , -0.19441245, -0.27260825,\n",
       "         0.42978317, -0.04549494,  0.26310375,  0.53513926,  0.08529071,\n",
       "         0.32386076], dtype=float32),\n",
       " array([ 4.21838731e-01,  8.52667570e-01,  4.53013957e-01,  1.69145186e-02,\n",
       "        -8.30245018e-02,  3.11641514e-01, -1.53444065e-02,  7.21300006e-01,\n",
       "        -1.85029849e-01,  4.03883606e-02,  5.07137060e-01, -2.80173659e-01,\n",
       "        -1.19090870e-01, -4.19882417e-01, -5.56670725e-01,  1.47657645e+00,\n",
       "         1.22599520e-01, -3.65723521e-02,  8.67881402e-02, -1.46890841e-02,\n",
       "         3.09571099e+00,  2.53641158e-01,  1.32403001e-01,  7.01418996e-01,\n",
       "        -1.52363583e-01,  2.11616337e-01,  1.53554454e-01,  2.74739528e+00,\n",
       "        -3.96307945e-01, -3.00787568e-01, -4.65536803e-01, -2.15003546e-02,\n",
       "        -2.03409186e-03, -3.78533930e-01,  1.21961132e-01,  1.68646312e+00,\n",
       "         3.94837290e-01, -6.80468857e-01,  9.74603176e-01,  4.30213273e-01,\n",
       "        -1.23031594e-01, -6.15954921e-02,  6.68588102e-01, -7.22803408e-03,\n",
       "        -7.52744317e-01, -6.14601254e-01,  3.66947949e-01, -4.36928809e-01,\n",
       "         5.45337014e-02,  6.47927001e-02,  1.66767582e-01, -5.56563586e-02,\n",
       "        -8.30554813e-02, -1.61301568e-01, -4.22571838e-01, -1.09591293e+00,\n",
       "         4.77595488e-03, -5.11297107e-01,  6.09125972e-01,  1.38286397e-01,\n",
       "        -3.92137319e-01,  4.65482086e-01,  6.84980443e-03,  6.87690914e-01,\n",
       "         1.11502457e+00,  2.72889286e-01, -4.35842872e-01, -8.13714787e-02,\n",
       "         1.36617512e-01, -1.00497156e-01, -2.71821761e+00,  2.43393600e-01,\n",
       "         1.36775762e-01, -5.15861928e-01,  6.81102037e-01, -8.80380392e-01,\n",
       "         6.40987083e-02,  1.49117517e+00, -1.40568659e-01,  1.83301821e-01,\n",
       "        -1.29260048e-01, -1.31974041e-01,  2.15988100e-01,  5.88858187e-01,\n",
       "         2.09037781e-01, -2.84158617e-01,  1.80476338e-01, -2.82201860e-02,\n",
       "         2.50663757e-01,  3.46857250e-01, -2.02793613e-01,  5.54317236e-01,\n",
       "        -7.43624801e-03,  5.47314346e-01,  2.92794794e-01, -1.15676634e-01,\n",
       "        -1.06051140e-01, -2.23577410e-01, -1.30797446e-01,  1.43275395e-01,\n",
       "        -1.70283347e-01,  7.28708804e-02,  1.48278385e-01, -2.10792288e-01,\n",
       "         1.77920803e-01, -7.08525479e-02,  1.55806169e-02, -2.08907306e-01,\n",
       "         9.45196271e-01,  1.51039049e-01, -7.70189106e-01,  7.09140971e-02,\n",
       "        -3.45549226e-01, -6.55777574e-01, -3.82310033e-01, -9.31860864e-01,\n",
       "        -2.45793819e-01,  4.46502924e-01,  2.14856088e-01,  1.38702273e-01,\n",
       "         1.88123882e+00, -1.13244280e-01, -3.97857130e-01, -3.31195623e-01,\n",
       "        -2.45866299e-01,  2.24716216e-02,  4.37474474e-02,  2.50374269e+00,\n",
       "        -1.37471497e-01,  8.96718800e-02, -5.66277862e-01,  1.76491350e-01,\n",
       "        -8.29634905e-01, -1.31833209e-02,  2.70184457e-01, -1.36954677e+00,\n",
       "         2.50962019e-01, -2.65426356e-02,  1.93493322e-01, -4.33546603e-01,\n",
       "         1.28467575e-01,  5.15897498e-02, -6.73566282e-01, -5.50126195e-01,\n",
       "        -1.09547772e-01,  6.17597461e-01, -1.28327623e-01,  3.87291610e-01,\n",
       "         6.16105676e-01,  2.22069710e-01,  3.65002930e-01], dtype=float32),\n",
       " array([ 4.21838731e-01,  8.52667570e-01,  4.53013957e-01,  1.69145186e-02,\n",
       "        -8.30245018e-02,  3.11641514e-01, -1.53444065e-02,  7.21300006e-01,\n",
       "        -1.85029849e-01,  4.03883606e-02,  5.07137060e-01, -2.80173659e-01,\n",
       "        -1.19090870e-01, -4.19882417e-01, -5.56670725e-01,  1.47657645e+00,\n",
       "         1.22599520e-01, -3.65723521e-02,  8.67881402e-02, -1.46890841e-02,\n",
       "         3.09571099e+00,  2.53641158e-01,  1.32403001e-01,  7.01418996e-01,\n",
       "        -1.52363583e-01,  2.11616337e-01,  1.53554454e-01,  2.74739528e+00,\n",
       "        -3.96307945e-01, -3.00787568e-01, -4.65536803e-01, -2.15003546e-02,\n",
       "        -2.03409186e-03, -3.78533930e-01,  1.21961132e-01,  1.68646312e+00,\n",
       "         3.94837290e-01, -6.80468857e-01,  9.74603176e-01,  4.30213273e-01,\n",
       "        -1.23031594e-01, -6.15954921e-02,  6.68588102e-01, -7.22803408e-03,\n",
       "        -7.52744317e-01, -6.14601254e-01,  3.66947949e-01, -4.36928809e-01,\n",
       "         5.45337014e-02,  6.47927001e-02,  1.66767582e-01, -5.56563586e-02,\n",
       "        -8.30554813e-02, -1.61301568e-01, -4.22571838e-01, -1.09591293e+00,\n",
       "         4.77595488e-03, -5.11297107e-01,  6.09125972e-01,  1.38286397e-01,\n",
       "        -3.92137319e-01,  4.65482086e-01,  6.84980443e-03,  6.87690914e-01,\n",
       "         1.11502457e+00,  2.72889286e-01, -4.35842872e-01, -8.13714787e-02,\n",
       "         1.36617512e-01, -1.00497156e-01, -2.71821761e+00,  2.43393600e-01,\n",
       "         1.36775762e-01, -5.15861928e-01,  6.81102037e-01, -8.80380392e-01,\n",
       "         6.40987083e-02,  1.49117517e+00, -1.40568659e-01,  1.83301821e-01,\n",
       "        -1.29260048e-01, -1.31974041e-01,  2.15988100e-01,  5.88858187e-01,\n",
       "         2.09037781e-01, -2.84158617e-01,  1.80476338e-01, -2.82201860e-02,\n",
       "         2.50663757e-01,  3.46857250e-01, -2.02793613e-01,  5.54317236e-01,\n",
       "        -7.43624801e-03,  5.47314346e-01,  2.92794794e-01, -1.15676634e-01,\n",
       "        -1.06051140e-01, -2.23577410e-01, -1.30797446e-01,  1.43275395e-01,\n",
       "        -1.70283347e-01,  7.28708804e-02,  1.48278385e-01, -2.10792288e-01,\n",
       "         1.77920803e-01, -7.08525479e-02,  1.55806169e-02, -2.08907306e-01,\n",
       "         9.45196271e-01,  1.51039049e-01, -7.70189106e-01,  7.09140971e-02,\n",
       "        -3.45549226e-01, -6.55777574e-01, -3.82310033e-01, -9.31860864e-01,\n",
       "        -2.45793819e-01,  4.46502924e-01,  2.14856088e-01,  1.38702273e-01,\n",
       "         1.88123882e+00, -1.13244280e-01, -3.97857130e-01, -3.31195623e-01,\n",
       "        -2.45866299e-01,  2.24716216e-02,  4.37474474e-02,  2.50374269e+00,\n",
       "        -1.37471497e-01,  8.96718800e-02, -5.66277862e-01,  1.76491350e-01,\n",
       "        -8.29634905e-01, -1.31833209e-02,  2.70184457e-01, -1.36954677e+00,\n",
       "         2.50962019e-01, -2.65426356e-02,  1.93493322e-01, -4.33546603e-01,\n",
       "         1.28467575e-01,  5.15897498e-02, -6.73566282e-01, -5.50126195e-01,\n",
       "        -1.09547772e-01,  6.17597461e-01, -1.28327623e-01,  3.87291610e-01,\n",
       "         6.16105676e-01,  2.22069710e-01,  3.65002930e-01], dtype=float32),\n",
       " array([ 0.36016393,  0.87730086,  0.81892616, -0.00800208, -0.08011911,\n",
       "        -0.05967585, -0.28162277,  0.6226414 , -0.07082875, -0.39714244,\n",
       "         0.62156355, -0.30231187, -0.38508025, -0.45984417, -0.52075464,\n",
       "         1.5048752 ,  0.11161829,  0.23308574,  0.26478043,  0.06599427,\n",
       "         3.0258276 ,  0.3854985 ,  0.02116132,  0.5017952 , -0.32543486,\n",
       "         0.34699   ,  0.23311889,  2.8636959 , -0.65872127, -0.31769413,\n",
       "        -0.9134715 , -0.23284277,  0.04093317, -0.47233927,  0.2010555 ,\n",
       "         1.52443   ,  0.4152191 ,  0.04935079,  1.082379  ,  0.5441543 ,\n",
       "        -0.408621  , -0.05899893,  0.77464485,  0.20083177, -0.84723437,\n",
       "        -0.7014318 ,  0.2936261 , -0.54838413,  0.00617018,  0.47351322,\n",
       "         0.46335942, -0.02874785, -0.08285674, -0.22651279, -0.22411443,\n",
       "        -0.9989038 , -0.15844263, -0.7268161 ,  0.69052446,  0.29656813,\n",
       "        -0.06672503,  0.3001842 ,  0.0716738 ,  0.58139163,  1.1860735 ,\n",
       "         0.18440737, -0.10244545,  0.32529593, -0.07642934,  0.1713044 ,\n",
       "        -2.7599416 ,  0.5335377 ,  0.37487903, -0.47103035,  1.0944695 ,\n",
       "        -0.868173  ,  0.4143772 ,  1.6591974 ,  0.16936125,  0.07980303,\n",
       "         0.10724026, -0.18178742,  0.06568041,  0.6180114 ,  0.79624534,\n",
       "        -0.38255796,  0.33541703,  0.30350018,  0.17164063,  0.1399056 ,\n",
       "        -0.29688475,  0.34256023,  0.12976775,  0.18751943,  0.35892865,\n",
       "        -0.3042817 ,  0.04607021, -0.05454214, -0.11856141,  0.32773367,\n",
       "         0.1518858 , -0.01413094, -0.08709557, -0.34440053,  0.24148448,\n",
       "        -0.13035157, -0.26007736, -0.21659076,  0.74283385,  0.33346003,\n",
       "        -0.9806237 ,  0.4492116 , -0.325893  , -0.6395467 , -0.74990284,\n",
       "        -0.7297487 , -0.31704268,  0.07153527,  0.06065215,  0.11950967,\n",
       "         1.7766181 , -0.58091104, -0.00775742, -0.45692298, -0.3225726 ,\n",
       "         0.19228444, -0.02038995,  2.0838907 , -0.34041175,  0.10633134,\n",
       "        -0.28160423, -0.03407947, -0.8111718 ,  0.11927719,  0.08227834,\n",
       "        -1.4264753 ,  0.47723404, -0.25586188,  0.25296843, -0.36482412,\n",
       "         0.01474105,  0.02510593, -0.27713856, -0.48102465, -0.5084459 ,\n",
       "         0.40450618, -0.01194878,  0.52029765,  0.64617616,  0.18536901,\n",
       "         0.46360302], dtype=float32),\n",
       " array([ 0.22895978,  0.5688106 ,  0.33874947, -0.05972938,  0.33655292,\n",
       "        -0.09616905, -0.3208196 ,  0.8487822 ,  0.14221346, -0.5093687 ,\n",
       "         0.5276591 ,  0.03375896, -0.34109384,  0.02541183, -0.6098164 ,\n",
       "         1.2536436 , -0.2272764 ,  0.19746985,  0.04078661,  0.14765547,\n",
       "         3.0128074 ,  0.23384656,  0.2891947 ,  0.19871643, -0.43358126,\n",
       "         0.27139783,  0.43629587,  2.6659327 , -0.6446139 , -0.3648841 ,\n",
       "        -0.73010564,  0.02334115,  0.06735195, -0.574852  , -0.01459276,\n",
       "         1.7592299 ,  0.1952789 , -0.1674628 ,  1.3294994 ,  0.2700091 ,\n",
       "        -0.36127016,  0.30026308,  0.68326545, -0.3393223 , -0.718819  ,\n",
       "        -0.6436289 ,  0.37947398, -0.7929755 , -0.09425818,  0.651558  ,\n",
       "         0.38516545,  0.03386508, -0.17254786, -0.82779205, -0.20340562,\n",
       "        -0.9063273 , -0.2974001 , -0.71781975,  0.6850272 ,  0.27174243,\n",
       "         0.5492235 ,  0.55930346, -0.2016482 ,  0.24892785,  1.3479121 ,\n",
       "         0.08849074, -0.0953818 ,  0.26118618, -0.25936902,  0.27940664,\n",
       "        -2.6963665 ,  0.59899676,  0.25148037, -0.17012157,  1.1240193 ,\n",
       "        -1.199849  ,  0.6226894 ,  1.5722475 ,  0.05630898, -0.04313665,\n",
       "         0.03512978, -0.25172415, -0.32925066,  0.5076227 ,  0.751504  ,\n",
       "        -0.2489525 ,  0.4582527 ,  0.5646296 , -0.07886422,  0.12870046,\n",
       "        -0.38121969,  0.38857713,  0.08142187,  0.28896865,  0.13161029,\n",
       "        -0.76426923,  0.31593013,  0.03888501,  0.09537569,  0.45052335,\n",
       "         0.16565403, -0.37308022, -0.03704477, -0.3016969 ,  0.19789693,\n",
       "        -0.22509189, -0.37542394, -0.31082135,  0.71416754,  0.32334578,\n",
       "        -0.8898082 ,  0.47553667, -0.40492576, -0.7578731 , -0.74693716,\n",
       "        -0.8765657 , -0.36371186, -0.26107314,  0.07203612, -0.19276446,\n",
       "         1.6969182 , -0.78325224, -0.23931432, -0.46993914, -0.05385386,\n",
       "         0.3613784 ,  0.09253146,  2.1719625 , -0.16569936,  0.05438685,\n",
       "         0.01703975, -0.11688276, -0.714456  , -0.01705894,  0.17165959,\n",
       "        -1.2445576 ,  0.64165395,  0.3858782 ,  0.15675816, -0.32861072,\n",
       "         0.02906286, -0.11237279, -0.2638582 , -0.35634017, -0.5210033 ,\n",
       "         0.62027776, -0.2410301 ,  0.3111975 ,  0.37995464,  0.29645842,\n",
       "         0.05818817], dtype=float32),\n",
       " array([ 0.22895978,  0.5688106 ,  0.33874947, -0.05972938,  0.33655292,\n",
       "        -0.09616905, -0.3208196 ,  0.8487822 ,  0.14221346, -0.5093687 ,\n",
       "         0.5276591 ,  0.03375896, -0.34109384,  0.02541183, -0.6098164 ,\n",
       "         1.2536436 , -0.2272764 ,  0.19746985,  0.04078661,  0.14765547,\n",
       "         3.0128074 ,  0.23384656,  0.2891947 ,  0.19871643, -0.43358126,\n",
       "         0.27139783,  0.43629587,  2.6659327 , -0.6446139 , -0.3648841 ,\n",
       "        -0.73010564,  0.02334115,  0.06735195, -0.574852  , -0.01459276,\n",
       "         1.7592299 ,  0.1952789 , -0.1674628 ,  1.3294994 ,  0.2700091 ,\n",
       "        -0.36127016,  0.30026308,  0.68326545, -0.3393223 , -0.718819  ,\n",
       "        -0.6436289 ,  0.37947398, -0.7929755 , -0.09425818,  0.651558  ,\n",
       "         0.38516545,  0.03386508, -0.17254786, -0.82779205, -0.20340562,\n",
       "        -0.9063273 , -0.2974001 , -0.71781975,  0.6850272 ,  0.27174243,\n",
       "         0.5492235 ,  0.55930346, -0.2016482 ,  0.24892785,  1.3479121 ,\n",
       "         0.08849074, -0.0953818 ,  0.26118618, -0.25936902,  0.27940664,\n",
       "        -2.6963665 ,  0.59899676,  0.25148037, -0.17012157,  1.1240193 ,\n",
       "        -1.199849  ,  0.6226894 ,  1.5722475 ,  0.05630898, -0.04313665,\n",
       "         0.03512978, -0.25172415, -0.32925066,  0.5076227 ,  0.751504  ,\n",
       "        -0.2489525 ,  0.4582527 ,  0.5646296 , -0.07886422,  0.12870046,\n",
       "        -0.38121969,  0.38857713,  0.08142187,  0.28896865,  0.13161029,\n",
       "        -0.76426923,  0.31593013,  0.03888501,  0.09537569,  0.45052335,\n",
       "         0.16565403, -0.37308022, -0.03704477, -0.3016969 ,  0.19789693,\n",
       "        -0.22509189, -0.37542394, -0.31082135,  0.71416754,  0.32334578,\n",
       "        -0.8898082 ,  0.47553667, -0.40492576, -0.7578731 , -0.74693716,\n",
       "        -0.8765657 , -0.36371186, -0.26107314,  0.07203612, -0.19276446,\n",
       "         1.6969182 , -0.78325224, -0.23931432, -0.46993914, -0.05385386,\n",
       "         0.3613784 ,  0.09253146,  2.1719625 , -0.16569936,  0.05438685,\n",
       "         0.01703975, -0.11688276, -0.714456  , -0.01705894,  0.17165959,\n",
       "        -1.2445576 ,  0.64165395,  0.3858782 ,  0.15675816, -0.32861072,\n",
       "         0.02906286, -0.11237279, -0.2638582 , -0.35634017, -0.5210033 ,\n",
       "         0.62027776, -0.2410301 ,  0.3111975 ,  0.37995464,  0.29645842,\n",
       "         0.05818817], dtype=float32),\n",
       " array([ 0.4551443 ,  0.6980501 ,  0.30825207, -0.0896963 ,  0.5248736 ,\n",
       "        -0.51871973, -0.15816969,  1.2196047 ,  0.01395698, -0.6283363 ,\n",
       "         0.5853665 ,  0.16737026, -0.48702952,  0.43654305, -0.6216897 ,\n",
       "         1.347452  , -0.21908964, -0.13565005,  0.5277492 ,  0.5669692 ,\n",
       "         2.9365745 ,  0.5574928 ,  0.33842993,  0.13148177, -0.14416504,\n",
       "         0.528548  ,  0.3455234 ,  2.733406  , -0.38893846, -0.5376962 ,\n",
       "        -0.91338557,  0.30950457,  0.11427594, -0.69128567, -0.02790133,\n",
       "         1.7515036 ,  0.21516609, -0.3058159 ,  1.5842127 ,  0.6403249 ,\n",
       "        -0.1266962 ,  0.3127106 ,  0.92652935, -0.162828  , -0.6258785 ,\n",
       "        -0.8518948 ,  0.40133354, -0.7532792 , -0.3986143 ,  0.26796922,\n",
       "         0.15636656,  0.26728868,  0.2888806 , -0.95864606, -0.27774912,\n",
       "        -1.0541629 , -0.0917697 , -0.7506699 ,  0.49160546,  0.3913813 ,\n",
       "         0.5599922 ,  0.45845658, -0.4199561 ,  0.16871059,  1.3222753 ,\n",
       "         0.09003025,  0.12811024,  0.295332  ,  0.10745946,  0.5009089 ,\n",
       "        -2.6057765 ,  0.47989422, -0.21174048, -0.41831917,  1.159768  ,\n",
       "        -1.2409941 ,  0.42589226,  1.6855673 , -0.13975279, -0.15644763,\n",
       "         0.15461823, -0.14672726,  0.07048202,  0.48527157,  1.0298157 ,\n",
       "        -0.4284608 ,  0.54635775,  0.68430144,  0.01234885,  0.13552274,\n",
       "        -0.26966637,  0.7645683 ,  0.01182878,  0.43863654, -0.28882244,\n",
       "        -0.812975  ,  0.25434682, -0.31308758, -0.12382557,  0.49620414,\n",
       "         0.0642994 , -0.4425634 , -0.3286179 , -0.25002387, -0.31554398,\n",
       "         0.03644148, -0.31148228, -0.16320452,  0.61051357,  0.4186306 ,\n",
       "        -0.71645874,  0.4339881 , -0.4687974 , -0.52360946, -0.7860639 ,\n",
       "        -0.11847255, -0.29797244, -0.56603724, -0.3364576 , -0.22589864,\n",
       "         1.9236703 , -0.9221565 , -0.10596461, -0.42645454, -0.1640427 ,\n",
       "         0.18717742, -0.04994354,  2.249515  , -0.67105293,  0.03361716,\n",
       "         0.11329167,  0.09036109, -0.21509305,  0.44990817,  0.29206255,\n",
       "        -1.0660231 ,  0.5695679 ,  0.7114207 ,  0.38581768, -0.35089096,\n",
       "        -0.03997042, -0.30963078, -0.20712651, -0.49232766, -0.5236854 ,\n",
       "         0.4204309 , -0.21413916,  0.42730537, -0.1742535 ,  0.5520001 ,\n",
       "        -0.03540129], dtype=float32),\n",
       " array([ 0.19740431,  0.8620923 ,  0.16810381,  0.2915614 ,  0.3952177 ,\n",
       "        -0.69961464, -0.04526702,  1.3125153 ,  0.5357395 , -1.0476985 ,\n",
       "         0.4926844 ,  0.23119354,  0.18938175,  0.23313718, -0.57487184,\n",
       "         1.4389327 , -0.4135628 ,  0.15365222,  0.81399673,  0.346726  ,\n",
       "         3.206721  ,  0.32116473,  0.2973069 ,  0.24034119,  0.325516  ,\n",
       "         0.46160117,  0.5219066 ,  2.8082848 , -0.27525672, -0.28138766,\n",
       "        -0.61107206,  0.20765968, -0.09316671, -0.7673896 , -0.11375773,\n",
       "         1.6823871 , -0.24179177, -0.1495474 ,  1.7359533 ,  0.445997  ,\n",
       "        -0.03030677,  0.21057376,  1.0589097 , -0.14740132, -1.0630877 ,\n",
       "        -0.37774876,  0.5185166 , -0.37345135, -0.4768168 ,  0.32912648,\n",
       "         0.0068943 ,  0.3270933 ,  0.09580131, -1.1998206 , -0.4110957 ,\n",
       "        -0.9557674 ,  0.17042638, -0.63900965,  0.59594184,  0.2483902 ,\n",
       "         0.45817345,  0.41563654, -0.56868076,  0.14985791,  1.3321359 ,\n",
       "         0.6168569 ,  0.14767407,  0.4759962 , -0.1281112 ,  0.6176544 ,\n",
       "        -2.7492342 ,  0.3671986 , -0.36114383, -0.59368056,  0.9827197 ,\n",
       "        -1.3439089 ,  0.11337832,  1.618648  , -0.13554277, -0.01735645,\n",
       "         0.22135404, -0.4627132 , -0.06379958,  0.42966446,  0.95183533,\n",
       "        -0.30318734,  0.53786767,  1.1036284 ,  0.25234815,  0.23836757,\n",
       "        -0.15515801,  0.37805334,  0.09340176,  0.27439466, -0.17390172,\n",
       "        -0.74994695,  0.59521294,  0.10936292, -0.02683666,  0.32181627,\n",
       "         0.01770444, -0.68886805, -0.3448491 , -0.0886789 , -0.10314751,\n",
       "         0.1032339 , -0.29721576, -0.42183286,  0.38851163,  0.5195268 ,\n",
       "        -0.5527372 ,  0.43079257, -0.66143245, -0.6968606 , -0.8262813 ,\n",
       "        -0.33348218, -0.52516395, -0.7684813 , -0.37023637, -0.44841176,\n",
       "         1.9311167 , -0.8087982 ,  0.2265403 , -0.45970228, -0.19038236,\n",
       "         0.2655705 ,  0.32973894,  2.2754374 , -0.80484253,  0.27765685,\n",
       "         0.20147043,  0.0897429 , -0.09220112,  0.6180114 ,  0.10796474,\n",
       "        -1.3060012 ,  0.50119555,  1.0733917 ,  0.18371893, -0.16539863,\n",
       "         0.24828425, -0.2553197 , -0.09539755, -0.42666015, -0.6154321 ,\n",
       "         0.33584195, -0.05354907,  0.06648859, -0.6045625 ,  0.39497203,\n",
       "         0.13258038], dtype=float32),\n",
       " array([ 0.02896877,  0.9272614 ,  0.29542056,  0.29414043,  0.25738004,\n",
       "        -0.47297713,  0.07808721,  1.4381232 ,  0.15220504, -0.9777297 ,\n",
       "         0.7030998 ,  0.30634663,  0.34734088,  0.18881683, -0.5800877 ,\n",
       "         1.5122014 , -0.14769839,  0.03562052,  0.83024484,  0.28104934,\n",
       "         3.1396763 ,  0.4913954 ,  0.18962252,  0.25226665,  0.46358183,\n",
       "         0.16277827,  0.77145463,  2.786809  , -0.10649816, -0.12432873,\n",
       "        -0.24786338,  0.3144441 , -0.07983386, -1.0275253 ,  0.19157591,\n",
       "         1.63929   , -0.4541171 ,  0.12210631,  1.383406  ,  0.08950958,\n",
       "        -0.1252273 ,  0.38964614,  1.2919338 ,  0.1974433 , -0.956606  ,\n",
       "        -0.61051446,  0.7278881 ,  0.01953366, -0.34158435,  0.48174456,\n",
       "         0.21565361,  0.1519957 , -0.00843754, -1.1359181 , -0.7540738 ,\n",
       "        -0.97225034,  0.16536914, -0.4957489 ,  1.007955  ,  0.07547779,\n",
       "         0.5231894 ,  0.23337789, -0.34589097,  0.40016168,  1.4649129 ,\n",
       "         0.6380582 ,  0.12005068,  0.26295695,  0.20301235,  0.68031263,\n",
       "        -2.7422547 ,  0.3537885 , -0.31438753, -0.7497355 ,  1.1790968 ,\n",
       "        -1.3588358 ,  0.06311163,  1.6711365 , -0.15411864, -0.2774227 ,\n",
       "         0.27819708, -0.20977943, -0.10356648,  0.30663583,  0.7353439 ,\n",
       "        -0.3323307 , -0.04768254,  0.9891438 ,  0.49240944,  0.10157432,\n",
       "        -0.20892397,  0.52379364, -0.30800024,  0.48485854,  0.11178779,\n",
       "        -0.826245  ,  0.5546706 ,  0.05528064, -0.43473166,  0.24987423,\n",
       "         0.07317332, -0.43361455, -0.46842214,  0.44174457, -0.40762016,\n",
       "         0.20625812, -0.13982837, -0.3739824 ,  0.65176445,  0.56364626,\n",
       "        -0.5021487 ,  0.39118487, -0.11055999, -0.6780845 , -0.8840816 ,\n",
       "        -0.08652335, -0.5913864 , -0.4985462 , -0.50161654, -0.76494014,\n",
       "         1.8575943 , -0.9398104 ,  0.38179606, -0.5281185 ,  0.07870018,\n",
       "         0.19961344,  0.36577386,  2.5509303 , -0.99555373,  0.56868374,\n",
       "         0.49629462,  0.09023874, -0.13387214,  0.60512084,  0.03702904,\n",
       "        -0.83788437,  0.65210533,  1.4663239 , -0.00725851, -0.30467883,\n",
       "        -0.03259772, -0.1782467 ,  0.24283098, -0.7072548 , -0.4218634 ,\n",
       "         0.4210757 , -0.04302605,  0.08044762, -0.3560631 ,  0.45966738,\n",
       "         0.5733487 ], dtype=float32),\n",
       " array([ 0.06906234,  0.97791445,  0.16019756,  0.38304973,  0.32039657,\n",
       "        -0.20109178, -0.06157766,  1.1021236 , -0.06716068, -0.9557181 ,\n",
       "         0.7360144 ,  0.1526564 ,  0.5603649 ,  0.23136047, -0.75926864,\n",
       "         1.324975  , -0.32511154, -0.3372587 ,  1.0811776 , -0.09082293,\n",
       "         3.1878319 ,  0.5068508 ,  0.46363378,  0.6016334 ,  0.1420972 ,\n",
       "         0.16636743,  0.7045946 ,  2.9011295 , -0.39141908, -0.16024636,\n",
       "        -0.27907723,  0.48826954, -0.34513786, -1.3325512 ,  0.41861448,\n",
       "         1.4553163 , -0.64802444, -0.07453024,  1.235458  , -0.20693305,\n",
       "        -0.66322654,  0.00956699,  1.0782468 ,  0.1108281 , -0.9065637 ,\n",
       "        -0.5087432 ,  0.88989985, -0.12431749, -0.38101593,  0.22873871,\n",
       "         0.53455967,  0.0118513 ,  0.02830759, -0.7883051 , -0.72201484,\n",
       "        -0.5639333 , -0.04614631, -0.71008784,  0.7450131 ,  0.01098936,\n",
       "         0.6076756 , -0.00523614, -0.54622674,  0.7032248 ,  1.2171171 ,\n",
       "         0.8667949 , -0.02435837,  0.502589  ,  0.37861398,  0.35316795,\n",
       "        -2.7534585 ,  0.03553671, -0.03141335, -0.75868547,  1.1890713 ,\n",
       "        -1.4616473 ,  0.120226  ,  1.6998701 , -0.11704376, -0.20828167,\n",
       "         0.05537064, -0.05670356, -0.02270384,  0.35048026,  0.44061214,\n",
       "        -0.47305602,  0.0696627 ,  0.99547803,  0.45940012,  0.1759341 ,\n",
       "        -0.22100051,  0.8142473 , -0.50009614,  0.3937422 , -0.0772025 ,\n",
       "        -0.9958056 ,  0.33433226,  0.32489973, -0.41609085,  0.11207823,\n",
       "         0.31335366, -0.697877  , -0.4942544 ,  0.51620805, -0.05065878,\n",
       "         0.41255096, -0.21522611, -0.09310883,  0.28223896,  0.5911923 ,\n",
       "        -0.43672016,  0.5575725 , -0.5321355 , -0.7584848 , -0.96262556,\n",
       "        -0.07172818, -0.74834996, -0.41735297, -0.35143363, -0.71735734,\n",
       "         1.7809485 , -1.3215628 ,  0.58234745, -0.7068845 ,  0.24038664,\n",
       "         0.18826541,  0.4935459 ,  2.2071476 , -0.74408877,  0.5740617 ,\n",
       "         0.51077306,  0.4834047 , -0.03273267,  0.85966694, -0.5411954 ,\n",
       "        -0.83575064,  0.5145704 ,  1.4475204 ,  0.00762458, -0.11127379,\n",
       "         0.2920054 , -0.17346922,  0.14931442, -0.60418874, -0.28484428,\n",
       "         0.6464039 ,  0.08818768,  0.19837731, -0.9404107 ,  0.60254127,\n",
       "         0.51529676], dtype=float32),\n",
       " array([-1.99982241e-01,  1.30374706e+00,  2.37996522e-02,  2.04453707e-01,\n",
       "         5.75904608e-01, -2.85036094e-03, -2.12901860e-01,  5.84794879e-01,\n",
       "         1.47077972e-02, -9.83808994e-01,  4.34006721e-01,  2.13405848e-01,\n",
       "         8.45889151e-01, -7.24718813e-03, -4.47212756e-01,  1.53624105e+00,\n",
       "        -2.41054550e-01, -2.24689856e-01,  1.21402121e+00, -2.89578438e-01,\n",
       "         2.92883801e+00,  6.85155571e-01,  6.48791373e-01,  7.59558558e-01,\n",
       "         3.13105643e-01,  4.99511480e-01,  1.14802086e+00,  3.13800192e+00,\n",
       "        -5.59378982e-01, -4.22155112e-01, -5.97291052e-01,  5.74810147e-01,\n",
       "        -9.68219060e-03, -8.96449149e-01,  5.10196239e-02,  1.30505037e+00,\n",
       "        -6.07974529e-01,  9.52136815e-02,  1.08428836e+00,  1.09664261e-01,\n",
       "        -6.86577380e-01,  2.66753376e-01,  1.24237931e+00, -1.11411817e-01,\n",
       "        -1.08816540e+00, -1.67481959e-01,  9.30894732e-01, -3.06394070e-01,\n",
       "        -1.43820152e-01,  5.27893543e-01,  7.18706787e-01, -5.27068339e-02,\n",
       "        -1.20989025e-01, -8.98966253e-01, -1.06632769e+00, -6.06612563e-01,\n",
       "         2.31876880e-01, -6.22971952e-01,  7.92015135e-01,  1.83245197e-01,\n",
       "         4.49268132e-01, -1.71405286e-01, -7.51310110e-01,  9.38298285e-01,\n",
       "         1.18602800e+00,  4.24717695e-01, -2.22723797e-01,  3.21344078e-01,\n",
       "         4.15502161e-01,  1.00991480e-01, -2.58201504e+00, -1.62823215e-01,\n",
       "        -3.00362974e-01, -2.71495640e-01,  9.32180464e-01, -1.17238939e+00,\n",
       "         1.50304288e-01,  1.87835515e+00,  1.44183353e-01,  1.28809914e-01,\n",
       "         4.18537930e-02, -1.57274693e-01, -4.66707684e-02,  1.46429524e-01,\n",
       "         3.41621995e-01, -4.09249753e-01,  2.02926606e-01,  1.06964576e+00,\n",
       "         6.66949451e-01,  1.36873841e-01, -7.98804983e-02,  1.20375788e+00,\n",
       "        -5.42797327e-01,  6.18202984e-01,  7.10388944e-02, -3.96174222e-01,\n",
       "         5.46281159e-01,  1.43866226e-01, -2.56255418e-01,  1.54735381e-02,\n",
       "         3.00968319e-01, -6.24739110e-01, -2.41039261e-01,  4.82221514e-01,\n",
       "         1.91439494e-01,  2.98159391e-01,  7.61416405e-02,  2.66151112e-02,\n",
       "         6.17727041e-01,  2.10878149e-01, -5.92648864e-01,  2.71876268e-02,\n",
       "        -4.76997554e-01, -7.33180881e-01, -7.65753806e-01,  2.79521644e-01,\n",
       "        -6.42467558e-01, -5.14784753e-01, -2.01203600e-01, -5.92256486e-01,\n",
       "         2.04794955e+00, -1.32016122e+00,  6.49911642e-01, -5.61927497e-01,\n",
       "        -8.45244974e-02,  1.68793853e-02,  4.43309367e-01,  1.79000854e+00,\n",
       "        -4.31563407e-01,  3.00322682e-01,  3.19989204e-01,  5.69279313e-01,\n",
       "        -1.73215494e-01,  1.08831823e+00, -2.10784629e-01, -1.10103214e+00,\n",
       "         5.18496633e-01,  9.11460638e-01, -2.15567455e-01,  3.28614702e-03,\n",
       "         3.06258947e-01,  1.42873287e-01, -1.31270602e-01, -4.39468622e-01,\n",
       "        -3.49745214e-01,  7.44637430e-01, -4.39157411e-02,  3.20035547e-01,\n",
       "        -9.70881820e-01,  7.28403628e-01,  5.50630867e-01], dtype=float32),\n",
       " array([-0.6259967 ,  1.2796978 ,  0.3541918 ,  0.36366615,  0.5886071 ,\n",
       "         0.57135946, -0.1460459 ,  0.6636507 ,  0.06915142, -0.64152473,\n",
       "         0.34041327,  0.02581298,  0.9211571 , -0.0852892 , -0.43879265,\n",
       "         1.5139855 ,  0.01853595, -0.31388968,  1.0669197 , -0.41156718,\n",
       "         2.9860077 ,  0.31521472,  0.525855  ,  0.6512703 ,  0.65027034,\n",
       "         0.21877383,  0.68398213,  3.0765753 , -0.78755933, -0.35562614,\n",
       "        -0.7319596 ,  0.5680281 , -0.10466111, -1.0955412 ,  0.03204603,\n",
       "         1.2987393 , -0.74335235,  0.05959257,  1.202837  ,  0.05621586,\n",
       "        -1.026793  ,  0.5139542 ,  1.0254762 ,  0.03058508, -1.3223617 ,\n",
       "         0.11779159,  0.81985855, -0.1756355 , -0.08251867,  0.29094112,\n",
       "         0.76230747, -0.23104958, -0.02898041, -1.0960506 , -0.8950301 ,\n",
       "        -0.15772529,  0.15093343, -0.58256716,  0.96114826,  0.38896826,\n",
       "         0.5384417 , -0.11843516, -1.2532706 ,  1.421203  ,  1.1695373 ,\n",
       "         0.33892825, -0.37733254, -0.01465263,  0.8962707 ,  0.03288679,\n",
       "        -2.588052  , -0.2686765 , -0.12537402, -0.12023897,  1.1011944 ,\n",
       "        -0.88210166,  0.21155076,  1.8713744 ,  0.1774989 ,  0.62317723,\n",
       "         0.43869257, -0.31822142,  0.27885836,  0.1379826 ,  0.3156066 ,\n",
       "        -0.73101485,  0.39675042,  1.0575991 ,  0.35957265, -0.24579145,\n",
       "         0.03819488,  1.5891688 , -0.27426744,  0.53010947,  0.19335115,\n",
       "        -0.7503802 ,  0.595647  ,  0.5611516 ,  0.04831298,  0.13298805,\n",
       "         0.12979013, -0.56271374,  0.03433362,  0.45384857,  0.15708841,\n",
       "         0.45294294,  0.05381597,  0.30441466,  0.8915667 ,  0.3331715 ,\n",
       "        -0.34165698,  0.38769093, -0.3753552 , -0.74902666, -0.73982847,\n",
       "         0.21842425, -0.6933038 , -0.31150842, -0.14412317, -0.6781987 ,\n",
       "         2.194668  , -1.4568398 ,  0.3701586 , -0.5438471 , -0.36171353,\n",
       "         0.07063586,  0.10208533,  1.6052853 , -0.38149533,  0.1782775 ,\n",
       "         0.38607377,  0.6273681 , -0.49289516,  0.9180728 , -0.05526134,\n",
       "        -1.2726986 ,  0.73725295,  0.7007227 ,  0.04811675,  0.14496462,\n",
       "         0.39282924,  0.03428109, -0.5047727 , -0.38153735, -0.16780348,\n",
       "         1.2660383 , -0.2188864 ,  0.33362162, -1.0970125 ,  0.66349137,\n",
       "         0.62456274], dtype=float32),\n",
       " array([-0.5959292 ,  1.2601347 , -0.00448088,  0.22648324,  0.0507905 ,\n",
       "         0.930222  , -0.6363699 ,  0.7398203 , -0.06567626, -0.63621294,\n",
       "         0.7061665 ,  0.14235686,  1.0549282 ,  0.04877364, -0.6362025 ,\n",
       "         1.54618   , -0.32540268, -0.14904207,  0.8910957 , -0.46250695,\n",
       "         2.886608  ,  0.49349025,  0.03452227,  0.81550086,  0.6852183 ,\n",
       "         0.3379853 ,  0.8990877 ,  3.574546  , -0.8718611 , -0.21623741,\n",
       "        -0.5121363 ,  0.7814277 ,  0.00497952, -0.88462716, -0.00424984,\n",
       "         1.4959565 , -0.90481895, -0.25389412,  1.4543877 ,  0.42109197,\n",
       "        -1.452014  ,  0.46181938,  1.0858663 ,  0.1840686 , -1.5440178 ,\n",
       "         0.18736257,  0.44358748, -0.48768872, -0.07785605,  0.55422384,\n",
       "         0.60456777, -0.32826874,  0.00531596, -1.058997  , -1.0205269 ,\n",
       "         0.03614891,  0.2301949 , -0.60529846,  1.3715572 ,  0.6017422 ,\n",
       "         0.31847057, -0.00862123, -1.3391365 ,  1.5486275 ,  0.99681324,\n",
       "         0.09307253, -0.07600694,  0.11157334,  0.73496217,  0.056964  ,\n",
       "        -2.4959471 , -0.5385438 , -0.08167114, -0.00718832,  1.3409928 ,\n",
       "        -0.79833275,  0.17965174,  2.093024  , -0.03087897,  0.67904973,\n",
       "         0.6309793 , -0.01990621, -0.07879727,  0.20279105,  0.44683197,\n",
       "        -0.35025936,  0.68212277,  0.864898  ,  0.43799734,  0.12668109,\n",
       "         0.374783  ,  1.8634433 , -0.46632916,  0.20819877,  0.34767345,\n",
       "        -0.671757  ,  0.30463994,  0.5406376 ,  0.08981461,  0.04438517,\n",
       "         0.0091034 , -0.6668679 , -0.20744178,  0.7022906 ,  0.36741242,\n",
       "         0.77310055,  0.22740296,  0.45091838,  0.57582253,  0.35827678,\n",
       "        -0.3268525 ,  0.23493338, -0.31187454, -0.632327  , -1.1083206 ,\n",
       "         0.12972395, -0.40948078, -0.22649653, -0.33620268, -0.7134645 ,\n",
       "         2.2104373 , -1.4426951 ,  0.43610224, -0.36892903, -0.16001467,\n",
       "        -0.08098491, -0.25512516,  1.5384563 , -0.29189694,  0.29846668,\n",
       "         0.26373127,  0.98704076, -0.2026478 ,  1.2530779 , -0.03588546,\n",
       "        -1.3851428 ,  0.74261   ,  0.7364045 ,  0.22507332,  0.14976647,\n",
       "         0.236257  ,  0.29438728, -0.07993591, -0.17281526,  0.11756171,\n",
       "         1.2336456 , -0.49178898,  0.47246718, -1.2928643 ,  0.43870798,\n",
       "         0.35984784], dtype=float32),\n",
       " array([-0.5959292 ,  1.2601347 , -0.00448088,  0.22648324,  0.0507905 ,\n",
       "         0.930222  , -0.6363699 ,  0.7398203 , -0.06567626, -0.63621294,\n",
       "         0.7061665 ,  0.14235686,  1.0549282 ,  0.04877364, -0.6362025 ,\n",
       "         1.54618   , -0.32540268, -0.14904207,  0.8910957 , -0.46250695,\n",
       "         2.886608  ,  0.49349025,  0.03452227,  0.81550086,  0.6852183 ,\n",
       "         0.3379853 ,  0.8990877 ,  3.574546  , -0.8718611 , -0.21623741,\n",
       "        -0.5121363 ,  0.7814277 ,  0.00497952, -0.88462716, -0.00424984,\n",
       "         1.4959565 , -0.90481895, -0.25389412,  1.4543877 ,  0.42109197,\n",
       "        -1.452014  ,  0.46181938,  1.0858663 ,  0.1840686 , -1.5440178 ,\n",
       "         0.18736257,  0.44358748, -0.48768872, -0.07785605,  0.55422384,\n",
       "         0.60456777, -0.32826874,  0.00531596, -1.058997  , -1.0205269 ,\n",
       "         0.03614891,  0.2301949 , -0.60529846,  1.3715572 ,  0.6017422 ,\n",
       "         0.31847057, -0.00862123, -1.3391365 ,  1.5486275 ,  0.99681324,\n",
       "         0.09307253, -0.07600694,  0.11157334,  0.73496217,  0.056964  ,\n",
       "        -2.4959471 , -0.5385438 , -0.08167114, -0.00718832,  1.3409928 ,\n",
       "        -0.79833275,  0.17965174,  2.093024  , -0.03087897,  0.67904973,\n",
       "         0.6309793 , -0.01990621, -0.07879727,  0.20279105,  0.44683197,\n",
       "        -0.35025936,  0.68212277,  0.864898  ,  0.43799734,  0.12668109,\n",
       "         0.374783  ,  1.8634433 , -0.46632916,  0.20819877,  0.34767345,\n",
       "        -0.671757  ,  0.30463994,  0.5406376 ,  0.08981461,  0.04438517,\n",
       "         0.0091034 , -0.6668679 , -0.20744178,  0.7022906 ,  0.36741242,\n",
       "         0.77310055,  0.22740296,  0.45091838,  0.57582253,  0.35827678,\n",
       "        -0.3268525 ,  0.23493338, -0.31187454, -0.632327  , -1.1083206 ,\n",
       "         0.12972395, -0.40948078, -0.22649653, -0.33620268, -0.7134645 ,\n",
       "         2.2104373 , -1.4426951 ,  0.43610224, -0.36892903, -0.16001467,\n",
       "        -0.08098491, -0.25512516,  1.5384563 , -0.29189694,  0.29846668,\n",
       "         0.26373127,  0.98704076, -0.2026478 ,  1.2530779 , -0.03588546,\n",
       "        -1.3851428 ,  0.74261   ,  0.7364045 ,  0.22507332,  0.14976647,\n",
       "         0.236257  ,  0.29438728, -0.07993591, -0.17281526,  0.11756171,\n",
       "         1.2336456 , -0.49178898,  0.47246718, -1.2928643 ,  0.43870798,\n",
       "         0.35984784], dtype=float32),\n",
       " array([-4.45142031e-01,  1.29677033e+00,  2.78908480e-02,  3.15579653e-01,\n",
       "         4.33085486e-02,  1.07839000e+00, -5.48899114e-01,  6.82293892e-01,\n",
       "         3.77566963e-01, -8.76247168e-01,  9.20817971e-01,  6.53975755e-02,\n",
       "         7.24660814e-01, -2.35141441e-01, -5.63205242e-01,  1.68421650e+00,\n",
       "         1.92354336e-01, -2.10080668e-01,  6.33763731e-01, -4.83576387e-01,\n",
       "         2.59022021e+00, -1.11117847e-02,  2.89823085e-01,  8.23267996e-01,\n",
       "         6.99821115e-01,  3.77454251e-01,  6.38108015e-01,  3.66211390e+00,\n",
       "        -9.73350286e-01, -1.79474011e-01, -7.71602809e-01,  5.78975916e-01,\n",
       "         1.44204751e-01, -1.05375481e+00,  7.48871565e-02,  1.40854836e+00,\n",
       "        -7.64025271e-01, -1.64355814e-01,  1.78233659e+00,  1.39398023e-01,\n",
       "        -1.29233265e+00,  4.87300366e-01,  1.72003508e+00,  1.56453356e-01,\n",
       "        -1.37975967e+00,  1.88137949e-01,  3.63117635e-01, -3.85475069e-01,\n",
       "        -4.56608366e-03,  4.82416004e-01,  6.15486801e-01,  1.03633456e-01,\n",
       "        -2.47079104e-01, -1.20133030e+00, -1.34359741e+00,  1.95081070e-01,\n",
       "         3.00764531e-01, -5.76237619e-01,  1.32021773e+00,  3.80400628e-01,\n",
       "         2.57498503e-01, -3.04275960e-01, -1.71471262e+00,  1.65553188e+00,\n",
       "         1.45033276e+00, -1.43890753e-01, -1.57234773e-01, -3.98995519e-01,\n",
       "         7.96110451e-01,  9.83922035e-02, -2.29854345e+00, -5.86041272e-01,\n",
       "         6.89136796e-03,  8.72864425e-02,  1.31896675e+00, -8.06713343e-01,\n",
       "         2.28740767e-01,  2.24308324e+00,  1.37465214e-02,  9.81561005e-01,\n",
       "         4.23279047e-01,  1.13606416e-01,  3.02551568e-01,  2.84320056e-01,\n",
       "         3.95586938e-01, -2.19733581e-01,  4.14116889e-01,  6.22348666e-01,\n",
       "         2.13550091e-01,  5.91083355e-02,  4.48774666e-01,  1.76501739e+00,\n",
       "        -4.48863178e-01, -2.16953546e-01,  3.79022121e-01, -7.05491602e-01,\n",
       "         1.99125171e-01,  8.34678292e-01,  4.16299701e-01, -2.48723000e-01,\n",
       "         9.09479484e-02, -6.09560847e-01, -4.83694106e-01,  7.28822768e-01,\n",
       "         1.88951105e-01,  4.07744616e-01,  9.35079530e-02,  3.92333180e-01,\n",
       "         8.57363105e-01,  8.37861598e-01, -3.12284917e-01,  7.01947987e-01,\n",
       "        -1.73881605e-01, -6.20959640e-01, -9.41947043e-01,  4.63912427e-01,\n",
       "        -2.18922302e-01, -5.85007370e-01, -4.62490380e-01, -7.51774073e-01,\n",
       "         2.20154166e+00, -1.42583752e+00,  3.42953712e-01, -4.00219917e-01,\n",
       "        -1.38984114e-01, -6.37582561e-04, -2.54291713e-01,  1.63114369e+00,\n",
       "        -1.10470513e-02,  6.03147805e-01,  1.91562578e-01,  9.68720675e-01,\n",
       "        -2.19441161e-01,  1.50255620e+00,  3.36016598e-03, -1.21716082e+00,\n",
       "         6.81774318e-01,  8.83501351e-01,  1.19247600e-01,  3.34386200e-01,\n",
       "         2.91717589e-01,  2.08420426e-01,  3.61918956e-02, -6.03824109e-02,\n",
       "        -3.14279795e-01,  9.95084345e-01, -7.59729803e-01,  4.82082158e-01,\n",
       "        -1.55520082e+00,  5.01859486e-01,  4.80214119e-01], dtype=float32),\n",
       " array([-4.45142031e-01,  1.29677033e+00,  2.78908480e-02,  3.15579653e-01,\n",
       "         4.33085486e-02,  1.07839000e+00, -5.48899114e-01,  6.82293892e-01,\n",
       "         3.77566963e-01, -8.76247168e-01,  9.20817971e-01,  6.53975755e-02,\n",
       "         7.24660814e-01, -2.35141441e-01, -5.63205242e-01,  1.68421650e+00,\n",
       "         1.92354336e-01, -2.10080668e-01,  6.33763731e-01, -4.83576387e-01,\n",
       "         2.59022021e+00, -1.11117847e-02,  2.89823085e-01,  8.23267996e-01,\n",
       "         6.99821115e-01,  3.77454251e-01,  6.38108015e-01,  3.66211390e+00,\n",
       "        -9.73350286e-01, -1.79474011e-01, -7.71602809e-01,  5.78975916e-01,\n",
       "         1.44204751e-01, -1.05375481e+00,  7.48871565e-02,  1.40854836e+00,\n",
       "        -7.64025271e-01, -1.64355814e-01,  1.78233659e+00,  1.39398023e-01,\n",
       "        -1.29233265e+00,  4.87300366e-01,  1.72003508e+00,  1.56453356e-01,\n",
       "        -1.37975967e+00,  1.88137949e-01,  3.63117635e-01, -3.85475069e-01,\n",
       "        -4.56608366e-03,  4.82416004e-01,  6.15486801e-01,  1.03633456e-01,\n",
       "        -2.47079104e-01, -1.20133030e+00, -1.34359741e+00,  1.95081070e-01,\n",
       "         3.00764531e-01, -5.76237619e-01,  1.32021773e+00,  3.80400628e-01,\n",
       "         2.57498503e-01, -3.04275960e-01, -1.71471262e+00,  1.65553188e+00,\n",
       "         1.45033276e+00, -1.43890753e-01, -1.57234773e-01, -3.98995519e-01,\n",
       "         7.96110451e-01,  9.83922035e-02, -2.29854345e+00, -5.86041272e-01,\n",
       "         6.89136796e-03,  8.72864425e-02,  1.31896675e+00, -8.06713343e-01,\n",
       "         2.28740767e-01,  2.24308324e+00,  1.37465214e-02,  9.81561005e-01,\n",
       "         4.23279047e-01,  1.13606416e-01,  3.02551568e-01,  2.84320056e-01,\n",
       "         3.95586938e-01, -2.19733581e-01,  4.14116889e-01,  6.22348666e-01,\n",
       "         2.13550091e-01,  5.91083355e-02,  4.48774666e-01,  1.76501739e+00,\n",
       "        -4.48863178e-01, -2.16953546e-01,  3.79022121e-01, -7.05491602e-01,\n",
       "         1.99125171e-01,  8.34678292e-01,  4.16299701e-01, -2.48723000e-01,\n",
       "         9.09479484e-02, -6.09560847e-01, -4.83694106e-01,  7.28822768e-01,\n",
       "         1.88951105e-01,  4.07744616e-01,  9.35079530e-02,  3.92333180e-01,\n",
       "         8.57363105e-01,  8.37861598e-01, -3.12284917e-01,  7.01947987e-01,\n",
       "        -1.73881605e-01, -6.20959640e-01, -9.41947043e-01,  4.63912427e-01,\n",
       "        -2.18922302e-01, -5.85007370e-01, -4.62490380e-01, -7.51774073e-01,\n",
       "         2.20154166e+00, -1.42583752e+00,  3.42953712e-01, -4.00219917e-01,\n",
       "        -1.38984114e-01, -6.37582561e-04, -2.54291713e-01,  1.63114369e+00,\n",
       "        -1.10470513e-02,  6.03147805e-01,  1.91562578e-01,  9.68720675e-01,\n",
       "        -2.19441161e-01,  1.50255620e+00,  3.36016598e-03, -1.21716082e+00,\n",
       "         6.81774318e-01,  8.83501351e-01,  1.19247600e-01,  3.34386200e-01,\n",
       "         2.91717589e-01,  2.08420426e-01,  3.61918956e-02, -6.03824109e-02,\n",
       "        -3.14279795e-01,  9.95084345e-01, -7.59729803e-01,  4.82082158e-01,\n",
       "        -1.55520082e+00,  5.01859486e-01,  4.80214119e-01], dtype=float32),\n",
       " array([-5.1105177e-01,  1.6636853e+00,  1.3671587e-01,  1.4255275e-01,\n",
       "         1.6261771e-01,  1.2269368e+00, -1.3442996e-01,  4.3810931e-01,\n",
       "         4.8109788e-01, -7.6394540e-01,  8.0961335e-01,  2.7038172e-01,\n",
       "         4.6360537e-01, -2.5068268e-01, -7.0912415e-01,  1.6072816e+00,\n",
       "         7.9650074e-02, -6.3704662e-02,  6.3492298e-01, -3.4937766e-01,\n",
       "         2.6465850e+00, -1.0385510e-01,  3.8750738e-01,  9.1944551e-01,\n",
       "         7.6655543e-01,  6.8007505e-01,  6.1358017e-01,  3.3734493e+00,\n",
       "        -8.7133712e-01, -2.1607755e-01, -1.0035948e+00,  3.6176524e-01,\n",
       "         3.9274305e-02, -7.8732830e-01, -1.6981174e-01,  1.1938932e+00,\n",
       "        -4.7888517e-01, -1.5932539e-01,  1.7286844e+00,  1.6998392e-02,\n",
       "        -1.1886550e+00,  2.6313218e-01,  1.7054141e+00, -2.8793660e-01,\n",
       "        -1.4087305e+00,  2.5021827e-01,  2.8758770e-01, -3.1672999e-01,\n",
       "         3.2362753e-01,  3.9655700e-01,  2.7396515e-01,  2.1321903e-01,\n",
       "        -4.4223964e-01, -1.2333871e+00, -9.1603547e-01, -1.0569705e-02,\n",
       "         2.4146910e-01, -8.3772492e-01,  1.1005923e+00,  7.8880316e-01,\n",
       "         3.5958382e-01, -5.4089713e-01, -1.7575670e+00,  1.8014966e+00,\n",
       "         1.4948380e+00, -5.2744710e-01, -2.0917863e-01, -8.3675437e-02,\n",
       "         2.0924890e-01,  2.6423177e-01, -2.4062343e+00, -7.0492709e-01,\n",
       "        -3.0918309e-01, -9.7720966e-02,  1.0097678e+00, -7.5896305e-01,\n",
       "         1.4000194e-01,  1.9940741e+00,  2.2975868e-01,  7.3101670e-01,\n",
       "         3.6145994e-01,  1.0059218e-01, -1.6261013e-02,  5.2308017e-01,\n",
       "         2.6446074e-01, -4.2681119e-01,  4.2141545e-01,  1.1685362e+00,\n",
       "        -8.7572761e-02, -3.6340287e-01,  8.1677008e-01,  1.4061025e+00,\n",
       "        -4.9287596e-01, -2.9218137e-01,  5.6048095e-01, -8.1381893e-01,\n",
       "         4.2770423e-02,  8.5916746e-01,  5.7044792e-01,  6.1072841e-02,\n",
       "         2.3689528e-03, -8.1694180e-01, -3.0249795e-01,  3.9299282e-01,\n",
       "         4.7079077e-01,  5.1214212e-01, -2.7672132e-03,  3.6495352e-01,\n",
       "         8.0915248e-01,  3.6500522e-01, -4.6418616e-01,  7.6804680e-01,\n",
       "        -7.9963341e-02, -5.8741784e-01, -1.1900272e+00,  3.5016215e-01,\n",
       "        -2.7605614e-01, -7.2784793e-01, -7.7105975e-01, -7.4707901e-01,\n",
       "         2.2412045e+00, -1.4001921e+00,  3.2415444e-01, -2.1545488e-02,\n",
       "        -5.6399085e-02,  2.3935644e-01, -1.8729597e-01,  1.8381931e+00,\n",
       "        -2.4635654e-02,  7.3767126e-01,  6.4194733e-01,  1.1546621e+00,\n",
       "        -3.3297497e-01,  1.6342680e+00, -1.4147450e-01, -1.6461672e+00,\n",
       "         4.8362637e-01,  1.1007277e+00, -2.5847245e-03,  6.8587619e-01,\n",
       "         2.2541393e-01,  3.5553157e-01, -3.1568736e-01, -6.0436213e-01,\n",
       "        -6.7779005e-01,  8.4167123e-01, -4.8186561e-01,  7.0331997e-01,\n",
       "        -1.6604583e+00,  3.0968276e-01,  5.9674498e-02], dtype=float32),\n",
       " array([-3.86900127e-01,  1.59516907e+00,  7.76567264e-03,  8.12427253e-02,\n",
       "         2.02642411e-01,  8.97318065e-01, -2.87236601e-01,  4.50579375e-01,\n",
       "         4.63632733e-01, -7.69271314e-01,  8.17201555e-01,  3.14643160e-02,\n",
       "        -1.24895409e-01, -1.53600097e-01, -7.76204228e-01,  1.09991288e+00,\n",
       "         1.09541297e-01, -9.23652276e-02,  3.36145818e-01, -3.72921899e-02,\n",
       "         2.65505290e+00, -8.84277672e-02,  1.78541631e-01,  8.21009278e-01,\n",
       "         8.12696338e-01,  6.09140337e-01,  4.55977857e-01,  3.34944439e+00,\n",
       "        -8.10240507e-01, -1.86032981e-01, -8.86384249e-01,  6.14546478e-01,\n",
       "        -1.23395361e-01, -6.64446056e-01, -2.32468411e-01,  1.25857830e+00,\n",
       "        -5.23226798e-01, -5.92262328e-01,  2.04288006e+00, -1.00651801e-01,\n",
       "        -1.24201441e+00, -1.90096125e-02,  1.84079397e+00, -1.27499059e-01,\n",
       "        -1.11401188e+00,  6.72842860e-01,  4.21038419e-01, -4.12103772e-01,\n",
       "         2.73138076e-01,  7.02701449e-01,  2.66651243e-01,  3.57086957e-01,\n",
       "        -5.11311948e-01, -1.10666883e+00, -7.07866549e-01,  2.31151879e-01,\n",
       "         4.63450581e-01, -7.96974003e-01,  1.15164375e+00,  4.96645749e-01,\n",
       "         3.95685285e-01, -5.39589822e-01, -2.00405312e+00,  1.75612569e+00,\n",
       "         1.27768242e+00, -3.70949924e-01, -3.84101212e-01, -9.82181504e-02,\n",
       "        -4.57209349e-02,  2.89089859e-01, -2.50280929e+00, -4.96029675e-01,\n",
       "        -2.29678527e-01, -4.19371873e-01,  1.11585069e+00, -5.55855095e-01,\n",
       "         2.02513933e-01,  2.05532241e+00,  2.26467520e-01,  9.93301392e-01,\n",
       "         3.31210554e-01,  9.03397873e-02, -1.68166235e-01,  6.42487049e-01,\n",
       "         4.01303142e-01, -9.02755372e-03,  4.78357196e-01,  1.43027699e+00,\n",
       "        -2.51474548e-02, -7.73118258e-01,  6.34392202e-01,  7.90183783e-01,\n",
       "        -3.17265242e-01, -3.44621748e-01,  4.58653986e-01, -1.08470023e+00,\n",
       "         2.42805183e-01,  8.58473182e-01,  4.86909539e-01, -2.19982848e-01,\n",
       "        -8.63081291e-02, -1.08714449e+00, -5.47765374e-01,  6.54982924e-01,\n",
       "         8.34312737e-01,  6.58347189e-01,  4.80378494e-02,  1.20118156e-01,\n",
       "         1.19529259e+00,  7.35560358e-01, -2.92327166e-01,  8.49945664e-01,\n",
       "        -2.97420800e-01, -1.22625172e+00, -1.31530333e+00, -3.56438644e-02,\n",
       "        -4.71331716e-01, -5.53253591e-01, -3.11474472e-01, -5.65257251e-01,\n",
       "         1.80572224e+00, -1.68417454e+00,  7.43211806e-02,  2.54113615e-01,\n",
       "        -9.26700945e-04, -9.34300050e-02, -4.05736655e-01,  1.79095995e+00,\n",
       "        -3.57482493e-01,  9.43415999e-01,  3.61095965e-01,  8.31591845e-01,\n",
       "        -2.80640870e-01,  1.55789685e+00,  1.28414094e-01, -1.76256287e+00,\n",
       "         3.48477542e-01,  1.19324565e+00,  3.29442292e-01,  6.66586101e-01,\n",
       "         1.00637525e-02,  4.04737473e-01, -2.36534789e-01, -7.31313407e-01,\n",
       "        -5.99364698e-01,  8.30663145e-01, -5.76960504e-01,  6.06728196e-01,\n",
       "        -1.45805812e+00,  4.31231916e-01, -2.58481115e-01], dtype=float32),\n",
       " array([-2.14084998e-01,  1.45918536e+00, -2.65065074e-01, -4.04288590e-01,\n",
       "         4.01990175e-01,  1.00036299e+00,  1.21782795e-01,  4.54194546e-01,\n",
       "         6.76391482e-01, -1.00312531e+00,  5.17207682e-01, -2.27240443e-01,\n",
       "        -2.33118059e-04, -3.25634778e-01, -1.01784861e+00,  9.62993145e-01,\n",
       "        -8.61563310e-02,  2.03752384e-01,  4.03115422e-01,  4.91813868e-02,\n",
       "         2.85749578e+00,  1.67700842e-01,  2.05965221e-01,  1.08771884e+00,\n",
       "         1.14149666e+00,  3.44067246e-01,  5.97508967e-01,  3.22597361e+00,\n",
       "        -4.94952977e-01, -5.94499946e-01, -1.11757553e+00,  5.72908223e-01,\n",
       "        -1.53322011e-01, -7.38761961e-01, -2.88044125e-01,  1.16805661e+00,\n",
       "        -3.62806380e-01, -4.80377167e-01,  2.39367151e+00, -3.08552925e-02,\n",
       "        -1.47405815e+00, -2.81202555e-01,  1.63808775e+00, -3.21354091e-01,\n",
       "        -5.90039909e-01,  6.62264109e-01,  7.27054656e-01, -5.85932434e-01,\n",
       "         4.23778966e-02,  5.47156036e-01,  3.97431910e-01,  4.26812693e-02,\n",
       "        -5.25564551e-01, -1.18228638e+00, -2.20882103e-01,  4.66957927e-01,\n",
       "         4.41720605e-01, -9.78955865e-01,  1.40614080e+00,  7.12707877e-01,\n",
       "         8.50326642e-02, -2.18897760e-01, -2.08166385e+00,  1.81591928e+00,\n",
       "         8.67136776e-01, -5.31702638e-01, -4.30212498e-01,  3.43150049e-01,\n",
       "         4.71898094e-02,  3.24816018e-01, -2.61385417e+00, -3.11141729e-01,\n",
       "        -3.98919463e-01, -5.61256826e-01,  1.05904937e+00, -7.13804305e-01,\n",
       "         1.30415708e-01,  1.77790010e+00,  4.79913317e-02,  6.40406370e-01,\n",
       "        -5.77682145e-02, -5.91120757e-02, -1.66194558e-01,  9.74262416e-01,\n",
       "         3.57790828e-01,  8.83134231e-02,  3.58041406e-01,  1.55175066e+00,\n",
       "         3.10258269e-01, -6.03972733e-01,  2.17062965e-01,  9.61077750e-01,\n",
       "        -3.98804873e-01, -2.96102583e-01,  6.49914563e-01, -1.13086343e+00,\n",
       "        -1.43697694e-01,  5.44365346e-01,  7.35346973e-01,  8.94593596e-02,\n",
       "         3.96772325e-02, -8.88273418e-01, -3.16283911e-01,  4.47937280e-01,\n",
       "         7.09482074e-01,  6.91392362e-01,  1.23952970e-01,  1.83519557e-01,\n",
       "         9.88019288e-01,  4.01028752e-01, -5.33432737e-02,  7.55699813e-01,\n",
       "        -2.00573057e-01, -1.21690822e+00, -1.34285879e+00, -1.26195937e-01,\n",
       "        -3.60922843e-01, -6.69847786e-01, -4.17772412e-01, -6.42311990e-01,\n",
       "         1.97042644e+00, -1.84732473e+00,  1.23304524e-01,  9.25292671e-02,\n",
       "         3.38874781e-03, -2.12584198e-01, -4.49758440e-01,  2.20276046e+00,\n",
       "        -2.14313060e-01,  8.16423953e-01,  5.74835241e-01,  1.17594874e+00,\n",
       "        -4.16490257e-01,  1.25073314e+00,  2.37840757e-01, -1.28877926e+00,\n",
       "         7.21116126e-01,  1.12392199e+00, -2.72801965e-01,  9.54211891e-01,\n",
       "        -1.64772734e-01, -7.04498738e-02, -2.85486519e-01, -2.62349993e-01,\n",
       "        -2.45181397e-01,  6.21951699e-01, -8.00156832e-01,  6.75397754e-01,\n",
       "        -1.14965665e+00,  7.20837057e-01, -1.26871601e-01], dtype=float32),\n",
       " array([ 0.1793371 ,  1.4825629 , -0.45593777, -0.40437868,  0.20599762,\n",
       "         1.386682  ,  0.20958397,  0.46062586,  0.6642276 , -0.5413012 ,\n",
       "         0.20734033, -0.08210721,  0.07302219, -0.4409844 , -0.92558056,\n",
       "         0.8975311 , -0.08072109,  0.3647182 ,  0.14043668,  0.30914512,\n",
       "         2.7736104 ,  0.20039165, -0.13352583,  0.8148433 ,  1.1353852 ,\n",
       "         0.2912532 ,  0.79909563,  3.0565898 , -0.79690266, -0.21628547,\n",
       "        -0.9326879 ,  0.0631566 , -0.32062155, -0.5471172 , -0.20213526,\n",
       "         1.333007  , -0.2324951 , -0.52669924,  2.3876696 , -0.29666564,\n",
       "        -1.5624014 , -0.09833305,  1.3159945 , -0.57118493, -0.6226868 ,\n",
       "         0.6853304 ,  0.5956764 , -0.6511679 ,  0.12203502,  0.6511837 ,\n",
       "        -0.18265927, -0.00802126, -0.34947702, -0.93165267, -0.38949528,\n",
       "         0.31879774,  0.3168252 , -0.97796047,  1.5324167 ,  0.8056203 ,\n",
       "        -0.04666135,  0.1362414 , -1.8933756 ,  2.0855265 ,  0.84061134,\n",
       "        -0.64110297, -0.57928216,  0.15011935, -0.18728654,  0.68914396,\n",
       "        -2.5126123 ,  0.0975074 , -0.33099416, -0.77091527,  0.8792176 ,\n",
       "        -0.74609524, -0.12754157,  1.7380866 ,  0.26306543,  0.09517104,\n",
       "         0.10285909, -0.24797666, -0.14280327,  1.1067122 ,  0.32689133,\n",
       "        -0.1095937 ,  0.37681893,  1.7033687 ,  0.2337362 , -0.617845  ,\n",
       "         0.30533072,  0.9604145 , -0.06902366, -0.2603886 ,  0.387424  ,\n",
       "        -1.2587364 ,  0.19156693,  0.48165572,  0.6502217 ,  0.10820001,\n",
       "         0.42661014, -0.69391704, -0.35090536,  0.31666028,  0.37438196,\n",
       "         0.6918274 ,  0.44797474, -0.21190815,  0.94174   , -0.03569917,\n",
       "         0.19479182,  1.2179859 , -0.3588343 , -1.0767493 , -1.0574638 ,\n",
       "        -0.30167806, -0.34396762, -0.75517726, -0.7298306 , -0.75423956,\n",
       "         2.0999699 , -1.6255946 , -0.02150194,  0.391     ,  0.27018228,\n",
       "        -0.17721304, -0.5297923 ,  2.2145073 , -0.14545971,  0.7631521 ,\n",
       "         0.49931496,  1.0971156 , -0.5663571 ,  1.4572197 ,  0.86663586,\n",
       "        -1.6082107 ,  0.93132067,  1.082949  , -0.13921899,  0.9341358 ,\n",
       "        -0.32097182, -0.19615354, -0.43047956, -0.13934712, -0.2804315 ,\n",
       "         0.66329515, -0.753026  ,  0.66598576, -0.9647516 ,  0.8384376 ,\n",
       "        -0.4183479 ], dtype=float32),\n",
       " array([ 0.08152729,  1.3675984 , -0.7491681 ,  0.00376774, -0.3240602 ,\n",
       "         1.414851  , -0.14685981,  0.23661885,  0.46771303, -0.40746614,\n",
       "         0.22163923, -0.1487459 ,  0.02926353, -0.2768965 , -0.59720635,\n",
       "         0.8489483 ,  0.18801889,  0.33451983,  0.0316789 , -0.1463326 ,\n",
       "         2.9209754 ,  0.08506086, -0.24989803,  1.1809117 ,  1.2164372 ,\n",
       "         0.37025335,  0.822786  ,  2.7966437 , -0.9711655 , -0.3367983 ,\n",
       "        -0.8433094 ,  0.04643699, -0.24827704, -0.4242826 ,  0.545738  ,\n",
       "         1.4248873 , -0.19980435, -0.6552711 ,  2.5772767 , -0.2070533 ,\n",
       "        -1.5159749 , -0.18680137,  1.4927367 , -0.4930575 , -0.7726245 ,\n",
       "         1.0878297 ,  0.6421958 , -0.6713859 ,  0.37591404,  0.66157097,\n",
       "         0.09616498, -0.2653511 , -0.21499862, -1.6210933 , -0.05093969,\n",
       "         0.2964665 ,  0.49835378, -0.85901946,  1.4627371 ,  0.6182061 ,\n",
       "        -0.37395442,  0.48503172, -1.9157593 ,  1.7649733 ,  0.77428603,\n",
       "        -0.4081552 , -0.7350778 , -0.0318425 , -0.06751214,  0.35113105,\n",
       "        -2.6265697 ,  0.20354766, -0.17692882, -0.9057263 ,  0.9007407 ,\n",
       "        -0.8003298 ,  0.09003057,  1.7337884 ,  0.37235403, -0.11723632,\n",
       "        -0.08765482, -0.03523362, -0.2766898 ,  1.4262774 ,  0.14491004,\n",
       "        -0.3757773 ,  0.5715863 ,  1.7713628 ,  0.06127993, -0.779172  ,\n",
       "         0.6622353 ,  0.9459857 , -0.41701445, -0.4049852 ,  0.82766575,\n",
       "        -1.1087745 , -0.08708464,  0.7357042 ,  0.66319436,  0.3442433 ,\n",
       "         0.4105523 , -0.79842997, -0.12702179,  0.4531659 ,  0.7864298 ,\n",
       "         0.42076117,  0.4732383 , -0.01615394,  0.75025266, -0.05271446,\n",
       "         0.3192435 ,  1.3727336 ,  0.00963705, -0.9061942 , -0.78336173,\n",
       "        -0.5659896 , -0.40347093, -0.9577341 , -0.49093   , -1.0594784 ,\n",
       "         1.8569357 , -1.5116432 ,  0.19180465,  0.32611632, -0.03143786,\n",
       "        -0.06285662, -0.45977238,  1.7826126 , -0.18982151,  0.35407025,\n",
       "         0.33481196,  1.1057488 , -0.4265474 ,  1.3680009 ,  0.95589167,\n",
       "        -1.4726006 ,  0.56548697,  0.9715906 ,  0.1471876 ,  0.86173415,\n",
       "        -0.45538518, -0.18606977, -0.42663285, -0.28785878, -0.7912555 ,\n",
       "         0.4018913 , -0.92071575,  0.4159501 , -0.9672865 ,  0.5548159 ,\n",
       "        -0.59544265], dtype=float32),\n",
       " array([ 0.08152729,  1.3675984 , -0.7491681 ,  0.00376774, -0.3240602 ,\n",
       "         1.414851  , -0.14685981,  0.23661885,  0.46771303, -0.40746614,\n",
       "         0.22163923, -0.1487459 ,  0.02926353, -0.2768965 , -0.59720635,\n",
       "         0.8489483 ,  0.18801889,  0.33451983,  0.0316789 , -0.1463326 ,\n",
       "         2.9209754 ,  0.08506086, -0.24989803,  1.1809117 ,  1.2164372 ,\n",
       "         0.37025335,  0.822786  ,  2.7966437 , -0.9711655 , -0.3367983 ,\n",
       "        -0.8433094 ,  0.04643699, -0.24827704, -0.4242826 ,  0.545738  ,\n",
       "         1.4248873 , -0.19980435, -0.6552711 ,  2.5772767 , -0.2070533 ,\n",
       "        -1.5159749 , -0.18680137,  1.4927367 , -0.4930575 , -0.7726245 ,\n",
       "         1.0878297 ,  0.6421958 , -0.6713859 ,  0.37591404,  0.66157097,\n",
       "         0.09616498, -0.2653511 , -0.21499862, -1.6210933 , -0.05093969,\n",
       "         0.2964665 ,  0.49835378, -0.85901946,  1.4627371 ,  0.6182061 ,\n",
       "        -0.37395442,  0.48503172, -1.9157593 ,  1.7649733 ,  0.77428603,\n",
       "        -0.4081552 , -0.7350778 , -0.0318425 , -0.06751214,  0.35113105,\n",
       "        -2.6265697 ,  0.20354766, -0.17692882, -0.9057263 ,  0.9007407 ,\n",
       "        -0.8003298 ,  0.09003057,  1.7337884 ,  0.37235403, -0.11723632,\n",
       "        -0.08765482, -0.03523362, -0.2766898 ,  1.4262774 ,  0.14491004,\n",
       "        -0.3757773 ,  0.5715863 ,  1.7713628 ,  0.06127993, -0.779172  ,\n",
       "         0.6622353 ,  0.9459857 , -0.41701445, -0.4049852 ,  0.82766575,\n",
       "        -1.1087745 , -0.08708464,  0.7357042 ,  0.66319436,  0.3442433 ,\n",
       "         0.4105523 , -0.79842997, -0.12702179,  0.4531659 ,  0.7864298 ,\n",
       "         0.42076117,  0.4732383 , -0.01615394,  0.75025266, -0.05271446,\n",
       "         0.3192435 ,  1.3727336 ,  0.00963705, -0.9061942 , -0.78336173,\n",
       "        -0.5659896 , -0.40347093, -0.9577341 , -0.49093   , -1.0594784 ,\n",
       "         1.8569357 , -1.5116432 ,  0.19180465,  0.32611632, -0.03143786,\n",
       "        -0.06285662, -0.45977238,  1.7826126 , -0.18982151,  0.35407025,\n",
       "         0.33481196,  1.1057488 , -0.4265474 ,  1.3680009 ,  0.95589167,\n",
       "        -1.4726006 ,  0.56548697,  0.9715906 ,  0.1471876 ,  0.86173415,\n",
       "        -0.45538518, -0.18606977, -0.42663285, -0.28785878, -0.7912555 ,\n",
       "         0.4018913 , -0.92071575,  0.4159501 , -0.9672865 ,  0.5548159 ,\n",
       "        -0.59544265], dtype=float32),\n",
       " array([-1.69157833e-02,  1.10472131e+00, -5.83666205e-01,  7.27213472e-02,\n",
       "        -4.59590435e-01,  1.34330726e+00, -1.93291649e-01,  2.10957080e-01,\n",
       "         5.23521423e-01, -3.20564121e-01,  1.17296204e-02, -6.77412748e-01,\n",
       "         1.17893726e-01, -2.47108385e-01, -5.62732220e-01,  1.29546762e+00,\n",
       "         3.23436186e-02,  1.55702174e-01,  1.10019162e-01, -3.70766163e-01,\n",
       "         2.84466410e+00,  3.88797909e-01, -4.06049579e-01,  1.24554670e+00,\n",
       "         1.01088631e+00,  1.72779113e-01,  1.02900505e+00,  2.93876123e+00,\n",
       "        -9.27401423e-01, -3.59195918e-01, -6.13710999e-01, -4.16225605e-02,\n",
       "         1.06265452e-02, -2.23370180e-01,  8.35842669e-01,  1.07815647e+00,\n",
       "        -4.20266747e-01, -4.99485701e-01,  2.49019647e+00, -2.00948445e-03,\n",
       "        -1.24425864e+00, -4.31240469e-01,  1.46136153e+00, -6.57141924e-01,\n",
       "        -7.47090995e-01,  1.19874120e+00,  5.65619528e-01, -6.61109090e-01,\n",
       "         6.03255451e-01,  7.72216201e-01,  4.31163698e-01, -2.06499279e-01,\n",
       "        -4.12497908e-01, -1.52466178e+00,  1.58817098e-01,  6.38098896e-01,\n",
       "         4.47716236e-01, -6.19224727e-01,  1.18928635e+00,  5.20784199e-01,\n",
       "        -5.32964289e-01,  1.10799348e+00, -1.90046430e+00,  1.73946917e+00,\n",
       "         7.40225792e-01, -2.99064636e-01, -9.84789014e-01,  1.23187780e-01,\n",
       "         5.35685336e-03,  1.81186393e-01, -2.57403994e+00,  3.66771787e-01,\n",
       "        -3.69634479e-01, -1.27081442e+00,  8.18117857e-01, -6.37262106e-01,\n",
       "        -1.11976817e-01,  1.90760934e+00,  3.55469584e-01, -2.95929432e-01,\n",
       "         3.21123719e-01, -1.55121848e-01,  4.39527594e-02,  1.19479585e+00,\n",
       "        -2.44633183e-02, -1.71081387e-02,  7.95928121e-01,  1.85263884e+00,\n",
       "         1.87014997e-01, -1.08137417e+00,  9.25706565e-01,  4.95533705e-01,\n",
       "        -1.69882014e-01, -2.48571187e-01,  8.76296401e-01, -9.64561462e-01,\n",
       "         9.24370531e-03,  7.66468108e-01,  7.10308254e-01,  2.76351333e-01,\n",
       "         1.71291471e-01, -5.89929998e-01, -1.92350239e-01,  6.85759708e-02,\n",
       "         6.76660597e-01,  4.12097186e-01,  1.85939163e-01, -4.00946021e-01,\n",
       "         3.06401223e-01,  2.10086592e-02, -5.64496070e-02,  1.60139358e+00,\n",
       "         2.21936464e-01, -9.56135809e-01, -5.86285591e-01, -6.34197593e-01,\n",
       "        -5.76679826e-01, -5.59518158e-01, -4.21574622e-01, -1.30619752e+00,\n",
       "         2.10694003e+00, -1.21641695e+00,  6.59729913e-02,  1.97666973e-01,\n",
       "        -2.87729770e-01,  3.71761583e-02, -8.91974568e-01,  2.17272902e+00,\n",
       "        -4.19427484e-01,  1.78259656e-01,  3.38584691e-01,  1.50316393e+00,\n",
       "        -3.38862866e-01,  1.59073377e+00,  1.06299722e+00, -1.51340795e+00,\n",
       "         3.62955034e-01,  1.11730933e+00,  6.56034112e-01,  9.57421362e-01,\n",
       "        -1.55418545e-01,  2.37207547e-01, -3.68563265e-01, -4.01517786e-02,\n",
       "        -8.91600907e-01,  3.95217985e-01, -1.05975914e+00,  1.89340144e-01,\n",
       "        -1.39122450e+00,  3.41717243e-01, -5.00939012e-01], dtype=float32),\n",
       " array([ 1.3414256e-01,  1.7289363e+00, -4.1908491e-01,  2.1329305e-01,\n",
       "        -5.6470376e-01,  1.2918299e+00, -1.6282080e-01,  6.1370470e-02,\n",
       "         4.9471599e-01, -9.2886800e-01,  1.7273024e-01, -7.1350002e-01,\n",
       "         1.9477654e-02,  6.6347979e-02, -5.1914006e-01,  1.2232468e+00,\n",
       "        -2.7060419e-01,  2.4284181e-01,  4.7471426e-02, -6.3362187e-01,\n",
       "         2.7083876e+00,  4.3282837e-01, -7.2554868e-01,  9.4880050e-01,\n",
       "         7.8438234e-01,  6.9798395e-02,  9.5480531e-01,  3.1677170e+00,\n",
       "        -8.9393926e-01, -5.1314956e-01, -5.4541785e-01,  1.9830959e-01,\n",
       "         2.0022103e-01, -3.7413248e-01,  7.2687685e-01,  1.2258893e+00,\n",
       "        -6.0291660e-01, -2.0333703e-01,  2.5384603e+00, -1.6347572e-02,\n",
       "        -1.1526606e+00, -7.7188331e-01,  1.5029799e+00, -3.8099408e-01,\n",
       "        -1.0448840e+00,  9.3029088e-01,  5.6848633e-01, -2.1348144e-01,\n",
       "         2.7085119e-01,  3.5571709e-01,  4.8338461e-01,  3.7783869e-02,\n",
       "        -2.0486040e-01, -1.2645396e+00,  1.6505274e-01,  6.0443366e-01,\n",
       "         4.5941737e-01, -6.4278740e-01,  8.6969203e-01,  7.0099115e-01,\n",
       "        -3.9196259e-01,  8.8812667e-01, -1.7580007e+00,  1.4502525e+00,\n",
       "         1.1493968e+00, -3.1348196e-01, -1.0600684e+00,  3.0492720e-01,\n",
       "         3.5418767e-01,  1.4316095e-01, -2.4270525e+00,  2.2658315e-01,\n",
       "        -3.5333961e-01, -1.2750503e+00,  4.4369945e-01, -8.5551441e-01,\n",
       "         2.4010380e-01,  1.9325044e+00,  5.4361856e-01, -8.1587452e-01,\n",
       "         2.3110174e-01,  6.8126373e-02, -2.0324491e-01,  1.0935996e+00,\n",
       "        -1.2600322e-01, -1.8270321e-01,  8.1031746e-01,  2.0447576e+00,\n",
       "         1.9466342e-01, -1.0190004e+00,  7.7279049e-01, -8.3187848e-02,\n",
       "        -6.8517178e-01, -3.7315407e-01,  1.0048876e+00, -7.1494138e-01,\n",
       "        -1.0928362e-01,  1.0429243e+00,  2.3170772e-01,  9.6087985e-02,\n",
       "         3.9261669e-01, -7.6371920e-01, -6.0188785e-02,  3.7597632e-01,\n",
       "         6.7424297e-01,  3.2212645e-02,  3.7223709e-01, -3.8447246e-01,\n",
       "         1.2957162e-02, -1.5566467e-01,  5.0972477e-02,  1.9058782e+00,\n",
       "         3.0700353e-01, -1.2533193e+00, -6.3423979e-01, -5.1556307e-01,\n",
       "        -4.0543082e-01, -5.5711627e-01, -4.4932184e-01, -1.6355377e+00,\n",
       "         2.2878189e+00, -1.2991010e+00, -4.0324298e-03,  2.9592019e-01,\n",
       "        -9.1430105e-02,  6.3792899e-02, -6.5095204e-01,  2.0936065e+00,\n",
       "        -3.6680496e-01,  2.1623890e-01,  3.4313071e-01,  1.4844438e+00,\n",
       "        -6.2182617e-01,  1.8815162e+00,  1.1039523e+00, -1.4856213e+00,\n",
       "         2.3640688e-01,  1.1060361e+00,  6.5406024e-01,  1.0453486e+00,\n",
       "        -6.4910820e-04,  3.5817213e-02, -4.4013393e-01, -6.3914126e-01,\n",
       "        -8.0745155e-01, -2.4806349e-02, -1.0725517e+00,  3.8966125e-01,\n",
       "        -1.4634762e+00,  2.9250097e-01, -3.7894708e-01], dtype=float32),\n",
       " array([-2.20665336e-01,  1.38381684e+00, -1.05372652e-01,  2.29245543e-01,\n",
       "        -7.00305998e-01,  1.24905717e+00, -7.13626146e-01,  1.80034757e-01,\n",
       "         3.16802979e-01, -1.19460857e+00,  9.65407491e-02, -5.35622120e-01,\n",
       "         1.95117965e-02,  2.92652667e-01, -8.80493402e-01,  1.20591223e+00,\n",
       "        -5.67927480e-01,  6.57133088e-02,  1.42798975e-01, -2.13781536e-01,\n",
       "         2.77421689e+00,  5.43880641e-01, -8.18820477e-01,  8.61552894e-01,\n",
       "         9.12328780e-01,  1.82899103e-01,  1.13482332e+00,  3.03453374e+00,\n",
       "        -4.64330941e-01, -8.11495841e-01, -2.01174483e-01,  4.62487876e-01,\n",
       "         1.89508691e-01, -2.63038933e-01,  4.90903765e-01,  1.17981982e+00,\n",
       "        -1.50662556e-01, -4.51723069e-01,  2.52680659e+00,  2.94911035e-04,\n",
       "        -8.41841459e-01, -1.20378542e+00,  1.77402031e+00, -9.74052548e-01,\n",
       "        -7.31727481e-01,  1.10261929e+00,  4.23795640e-01, -1.71141624e-01,\n",
       "         2.04505146e-01,  3.08738708e-01,  3.68452281e-01,  6.14982955e-02,\n",
       "        -9.58616883e-02, -1.07418013e+00,  2.14987129e-01,  7.30288029e-01,\n",
       "         5.74711919e-01, -4.58668023e-01,  6.46123350e-01,  9.12398338e-01,\n",
       "        -3.96984875e-01,  1.10195577e+00, -1.92265737e+00,  1.83476365e+00,\n",
       "         1.58317828e+00, -3.02981317e-01, -8.35888565e-01,  4.75707322e-01,\n",
       "         1.80081315e-02,  3.45831871e-01, -2.45383596e+00,  3.45992327e-01,\n",
       "        -5.47814548e-01, -1.29706419e+00,  4.69306409e-01, -1.14737189e+00,\n",
       "        -1.49140060e-01,  1.82419455e+00,  5.53471327e-01, -4.53309625e-01,\n",
       "         1.74662307e-01,  1.18566029e-01, -3.85291994e-01,  1.15966630e+00,\n",
       "        -1.58889964e-01, -1.79448664e-01,  1.18626952e+00,  2.10369110e+00,\n",
       "         2.32502352e-02, -1.28114247e+00,  6.88990057e-01,  2.87143495e-02,\n",
       "        -8.43612492e-01, -2.66661376e-01,  9.87567484e-01, -9.74052772e-02,\n",
       "        -2.65064865e-01,  1.37673604e+00,  3.93070839e-02, -1.86768137e-02,\n",
       "         4.43159342e-01, -5.93634129e-01,  4.05503213e-02,  4.90084380e-01,\n",
       "         9.48550284e-01,  2.83840001e-01,  6.15928531e-01, -3.42557251e-01,\n",
       "        -1.27520084e-01, -1.87688380e-01,  5.45755565e-01,  1.81793499e+00,\n",
       "         3.84678870e-01, -1.24311972e+00, -7.76716650e-01, -4.85920221e-01,\n",
       "        -1.56636730e-01, -9.44393277e-01, -3.77610862e-01, -1.31586003e+00,\n",
       "         2.47832012e+00, -1.35515845e+00,  2.97648370e-01, -2.77673155e-01,\n",
       "         3.01657557e-01,  2.79566348e-02, -6.49447203e-01,  2.12110877e+00,\n",
       "        -2.90809184e-01,  2.51911670e-01,  3.23711246e-01,  1.43847752e+00,\n",
       "        -4.38581228e-01,  1.91948473e+00,  1.28043258e+00, -1.57952106e+00,\n",
       "         2.76980668e-01,  1.24512613e+00,  6.53764784e-01,  9.42471623e-01,\n",
       "        -2.97826290e-01,  1.31333336e-01, -2.41844133e-01, -6.00213408e-01,\n",
       "        -7.41682827e-01, -2.57545531e-01, -9.07276094e-01,  8.51186514e-02,\n",
       "        -1.38034689e+00, -1.02608405e-01, -8.02316219e-02], dtype=float32),\n",
       " array([ 0.04190155,  1.2238274 , -0.051803  ,  0.3006272 , -0.71642095,\n",
       "         1.1697316 , -0.71508133,  0.12207246,  0.48767763, -1.0146371 ,\n",
       "         0.07535759, -0.5768854 , -0.17422092,  0.3052731 , -0.5335735 ,\n",
       "         1.4770799 , -0.459882  ,  0.1815771 ,  0.0656737 ,  0.06009452,\n",
       "         2.938513  ,  0.96404564, -0.78156555,  0.652213  ,  0.6879228 ,\n",
       "         0.0895839 ,  0.75608766,  2.9635358 , -0.43091166, -0.8851846 ,\n",
       "        -0.08421738,  0.42738914,  0.04831641, -0.5135968 ,  0.5629067 ,\n",
       "         0.96753234, -0.25080517, -0.3880664 ,  2.3569    , -0.04414451,\n",
       "        -0.70555896, -1.2535269 ,  1.464297  , -0.9590453 , -0.62717134,\n",
       "         0.7394918 ,  0.06464314, -0.19142026,  0.33590272, -0.24334614,\n",
       "         0.2540978 ,  0.17019346, -0.3041696 , -0.8097307 ,  0.43601504,\n",
       "         0.4706775 ,  0.6079418 , -0.46673313,  0.71764594,  0.63044137,\n",
       "        -0.34633666,  1.6526664 , -2.1096938 ,  1.6705987 ,  1.9227265 ,\n",
       "        -0.21070656, -0.7082362 ,  0.5825744 , -0.03523392,  0.32882038,\n",
       "        -2.621958  ,  0.2578687 , -0.34534088, -1.1795952 ,  0.06108779,\n",
       "        -1.2480572 , -0.09336908,  1.8706721 ,  0.39533553, -0.46588442,\n",
       "         0.11880747,  0.09107311, -0.02340258,  1.0763729 , -0.12897429,\n",
       "        -0.07808355,  1.1344533 ,  1.7401482 ,  0.14018822, -1.6209502 ,\n",
       "         0.5427411 ,  0.02482275, -0.8381765 , -0.6170321 ,  0.9692233 ,\n",
       "        -0.13330133, -0.2799286 ,  1.2103207 , -0.11829773, -0.25328463,\n",
       "         0.08235444, -0.416776  ,  0.03562107,  0.3516855 ,  0.8892522 ,\n",
       "        -0.02236443,  0.549044  , -0.13656971, -0.09360281, -0.22397825,\n",
       "         0.29106596,  1.7473848 ,  0.309015  , -1.6874912 , -0.7296786 ,\n",
       "        -0.43874255, -0.32622576, -1.214933  , -0.36818743, -1.2878007 ,\n",
       "         2.409163  , -1.3841164 ,  0.39867452, -0.27863374,  0.29367688,\n",
       "         0.12777774, -0.4575505 ,  1.9974049 , -0.04923743,  0.20444968,\n",
       "         0.45784017,  1.7541714 , -0.23526719,  2.0139205 ,  1.4179711 ,\n",
       "        -1.6046952 ,  0.18584895,  1.1438235 ,  0.9566833 ,  0.63988674,\n",
       "        -0.2365122 ,  0.23340514, -0.2493373 , -0.7411382 , -0.8596385 ,\n",
       "        -0.4941982 , -0.8829993 ,  0.02676834, -0.9449591 , -0.07563739,\n",
       "        -0.1462297 ], dtype=float32),\n",
       " array([-0.14991398,  1.3446225 ,  0.06787541,  0.42097178, -0.43420622,\n",
       "         1.3848609 , -0.47092646,  0.16476175,  0.37262097, -0.9426526 ,\n",
       "        -0.09352306, -0.6857486 , -0.2903389 ,  0.32771486, -0.658089  ,\n",
       "         1.5932466 , -0.51228404,  0.27446467, -0.086431  ,  0.12344929,\n",
       "         2.8012598 ,  1.0520544 , -1.0361835 ,  0.19061098,  0.75842226,\n",
       "        -0.05349727,  0.77485394,  3.4432976 , -0.61866766, -1.1901845 ,\n",
       "        -0.29126957,  0.48637614,  0.04508254, -0.5239088 ,  0.55207705,\n",
       "         0.8885549 , -0.3051017 , -0.40992555,  2.1441784 ,  0.2968687 ,\n",
       "        -0.5248324 , -1.419259  ,  1.6187389 , -0.94262785, -0.5507488 ,\n",
       "         0.97628146,  0.00656614,  0.02274584,  0.3732831 , -0.43475902,\n",
       "         0.37331617, -0.26371348, -0.6651303 , -0.86996454,  0.9370373 ,\n",
       "         0.1704673 ,  0.5716314 , -0.28281596,  0.886365  ,  0.5623673 ,\n",
       "         0.01142375,  1.7290366 , -2.3078182 ,  1.4809718 ,  2.3984892 ,\n",
       "        -0.33700752, -0.5571155 ,  0.57471895, -0.18819039,  0.24329889,\n",
       "        -2.5008788 ,  0.26108602, -0.61896366, -1.2643822 ,  0.10066003,\n",
       "        -1.2243441 ,  0.24778862,  2.0487225 ,  0.4328654 , -0.31909946,\n",
       "        -0.099688  ,  0.23400232, -0.10835097,  1.0559255 , -0.23972857,\n",
       "        -0.15855856,  1.5220646 ,  1.8551446 ,  0.00730769, -1.8376262 ,\n",
       "         0.637973  ,  0.04802306, -0.54091823, -0.8703792 ,  1.1464182 ,\n",
       "        -0.3847386 , -0.3407461 ,  1.0407145 , -0.05960883, -0.25211573,\n",
       "        -0.07609072, -0.77920717,  0.07070972,  0.5698738 ,  0.74685925,\n",
       "         0.2915369 ,  0.76777583,  0.03231655,  0.15966068, -0.4052011 ,\n",
       "         0.95263356,  2.0459042 ,  0.20890735, -1.915973  , -0.8561973 ,\n",
       "        -0.96704763, -0.15804282, -1.6075525 , -0.79416   , -1.2254078 ,\n",
       "         2.362077  , -1.2793685 ,  0.21939532, -0.20120046,  0.28498352,\n",
       "         0.28814092, -0.46690112,  1.8771478 ,  0.12427833, -0.07487654,\n",
       "         0.07295553,  1.7046341 , -0.33141568,  1.9259524 ,  1.5115952 ,\n",
       "        -1.3458773 ,  0.19250888,  1.2459975 ,  0.8623919 ,  0.4771374 ,\n",
       "        -0.820436  ,  0.49653125, -0.02611301, -0.5684281 , -1.1049234 ,\n",
       "        -0.48658526, -1.0620534 ,  0.12588246, -1.1381541 , -0.18954614,\n",
       "        -0.29990524], dtype=float32),\n",
       " array([-1.3509493e-01,  1.3337073e+00,  1.3478774e-01,  1.8080255e-01,\n",
       "        -2.1328215e-01,  1.2585756e+00, -3.2000870e-01, -2.2077983e-02,\n",
       "         6.8835622e-01, -6.8143135e-01,  2.2377197e-01, -8.1344807e-01,\n",
       "        -5.0212935e-02,  4.5579681e-01, -2.2670156e-01,  1.8196039e+00,\n",
       "        -3.7377617e-01,  8.3514087e-02, -3.2251731e-01,  2.2789055e-01,\n",
       "         2.6390581e+00,  6.0304129e-01, -9.9230641e-01, -2.5590206e-03,\n",
       "         7.3605222e-01, -6.1219212e-02,  9.5311338e-01,  3.4904678e+00,\n",
       "        -6.8924242e-01, -1.3451890e+00, -3.6518782e-01,  4.0906304e-01,\n",
       "         1.3613363e-01, -6.2989092e-01,  8.0701458e-01,  1.3298919e+00,\n",
       "        -2.8635105e-01, -4.7610068e-01,  2.3579414e+00,  1.2564847e-01,\n",
       "        -8.5919499e-01, -1.3931276e+00,  1.6356401e+00, -1.0591195e+00,\n",
       "        -7.3749107e-01,  8.9811099e-01, -2.7024892e-01, -2.3497494e-01,\n",
       "         3.5710070e-01, -3.0942959e-01,  4.4912997e-01, -5.4127938e-01,\n",
       "        -1.4791542e-02, -8.5800207e-01,  1.1294779e+00,  7.9061866e-02,\n",
       "         2.1325780e-01, -2.4227381e-01,  1.1007200e+00,  5.4892987e-01,\n",
       "         2.5247544e-01,  1.7429620e+00, -2.0520124e+00,  1.5486857e+00,\n",
       "         2.2897427e+00,  2.4453914e-01, -2.0466074e-01,  7.1835709e-01,\n",
       "        -2.2040163e-01,  3.5190618e-01, -2.5059433e+00,  2.3759402e-01,\n",
       "        -7.4976689e-01, -9.4427454e-01, -3.5058424e-01, -1.0946027e+00,\n",
       "         3.1635502e-01,  2.2192578e+00,  5.5113244e-01, -8.0410945e-01,\n",
       "         8.1261303e-03,  1.3350697e-01, -2.0254810e-01,  9.6724415e-01,\n",
       "        -1.3745759e-01, -2.2803363e-01,  1.7951602e+00,  1.6969252e+00,\n",
       "        -4.3882090e-01, -1.9033095e+00,  7.2382808e-01, -3.8127549e-02,\n",
       "        -6.1675555e-01, -7.5136638e-01,  8.0297393e-01, -1.6969076e-01,\n",
       "        -2.1164995e-01,  1.4782957e+00, -3.9908849e-03, -3.9231485e-01,\n",
       "         4.0823128e-02, -1.0327535e+00,  1.8546170e-01,  2.3085190e-01,\n",
       "         9.1449046e-01,  2.8579718e-01,  4.3958297e-01, -2.3658444e-01,\n",
       "         2.1436471e-01, -4.4376373e-01,  7.5860965e-01,  2.1635876e+00,\n",
       "         4.5503524e-01, -1.5681632e+00, -6.8512553e-01, -7.3386663e-01,\n",
       "        -3.2418124e-02, -1.3586117e+00, -8.7641138e-01, -1.7290603e+00,\n",
       "         2.1968055e+00, -1.5968666e+00,  4.9858207e-01, -2.6265147e-01,\n",
       "         4.0879175e-01,  2.4668314e-01, -7.2287226e-01,  1.7138891e+00,\n",
       "         9.0943865e-02, -1.0246814e-01, -1.7378379e-01,  1.8816900e+00,\n",
       "        -4.1376668e-01,  2.2166939e+00,  1.4171380e+00, -1.3154193e+00,\n",
       "        -1.6525553e-01,  1.3631271e+00,  9.6127158e-01,  4.1755742e-01,\n",
       "        -9.0780574e-01,  2.8553993e-01, -5.3787935e-01, -1.9135742e-01,\n",
       "        -1.0535481e+00, -3.6116269e-01, -1.2898989e+00,  4.5149654e-02,\n",
       "        -1.1175389e+00,  3.4015971e-01, -2.9379081e-02], dtype=float32),\n",
       " array([-0.39699337,  1.4928839 ,  0.2102295 ,  0.03489856, -0.48409244,\n",
       "         1.0941103 ,  0.01384325,  0.03576985,  0.85297924, -0.51475316,\n",
       "         0.10678614, -0.46948457, -0.09119023,  0.908168  , -0.20806631,\n",
       "         1.6575614 , -0.34190357,  0.2792448 , -0.6313235 , -0.04694022,\n",
       "         2.658314  ,  0.7916403 , -1.0983088 ,  0.11841409,  0.7228398 ,\n",
       "        -0.03238811,  0.91529304,  3.5443685 , -0.7362273 , -1.0888237 ,\n",
       "        -0.8330044 ,  0.56550264,  0.09434807, -0.6300488 ,  0.92191786,\n",
       "         1.4303483 , -0.58364886, -0.21342488,  2.4929764 ,  0.1562479 ,\n",
       "        -1.4296856 , -1.2876953 ,  1.5078285 , -1.014015  , -0.93562037,\n",
       "         0.6432032 ,  0.09985811,  0.01479562,  0.7380722 , -0.68481326,\n",
       "         0.42476103, -0.56859326, -0.08307344, -0.3811043 ,  0.35743913,\n",
       "        -0.15782644, -0.06068654, -0.48775864,  1.2557158 ,  0.26388857,\n",
       "         0.05775134,  1.6190325 , -2.3956227 ,  1.6205004 ,  2.4921591 ,\n",
       "         0.37966117, -0.53098345,  0.8288112 , -0.53168535,  0.5251285 ,\n",
       "        -2.389526  ,  0.17488915, -0.9922583 , -0.9271505 , -0.42554724,\n",
       "        -1.2133298 ,  0.3151524 ,  2.290591  ,  0.22985554, -0.919283  ,\n",
       "        -0.03555906,  0.21042767,  0.02989004,  0.8934645 ,  0.3158321 ,\n",
       "        -0.37870216,  1.3846376 ,  1.1816908 , -0.38435894, -1.6659077 ,\n",
       "         0.70073915,  0.19978961, -0.7605406 , -0.5751696 ,  1.1704985 ,\n",
       "        -0.24835002, -0.3760751 ,  1.2583836 ,  0.11499732, -0.31872576,\n",
       "         0.13787456, -0.8531638 ,  0.27315006, -0.26405084,  1.0317008 ,\n",
       "        -0.00372295,  0.6774868 , -0.6620241 , -0.0649559 , -0.51435745,\n",
       "         0.5045634 ,  2.0530567 ,  0.40670493, -1.4793252 , -0.7620061 ,\n",
       "        -0.6244541 ,  0.01913021, -0.95818347, -0.9637504 , -1.7723773 ,\n",
       "         2.4373364 , -1.5683216 ,  0.21251923, -0.48373273,  0.4912228 ,\n",
       "         0.19266853, -0.67610234,  1.7360716 ,  0.02921443, -0.18829586,\n",
       "        -0.16480026,  1.9978528 , -0.7760831 ,  1.6263399 ,  1.4741236 ,\n",
       "        -1.4661946 ,  0.17868781,  1.3311064 ,  1.1349214 ,  0.6582318 ,\n",
       "        -0.9655403 ,  0.5092869 , -0.7164168 , -0.37964615, -1.2298596 ,\n",
       "        -0.74197567, -1.0495888 , -0.08126142, -1.2327195 ,  0.46975467,\n",
       "        -0.23766977], dtype=float32),\n",
       " array([-5.8321965e-01,  1.5385158e+00,  1.7379288e-02,  3.5513476e-01,\n",
       "        -2.0383158e-01,  1.1483102e+00, -2.6201036e-02,  2.5140983e-01,\n",
       "         9.4767517e-01, -7.0793283e-01,  2.9352490e-02, -3.9751202e-01,\n",
       "         1.6166690e-01,  9.0741646e-01,  2.6590103e-01,  1.5304971e+00,\n",
       "        -1.3317654e-01,  8.6557811e-01, -8.5823584e-01, -3.1362340e-01,\n",
       "         2.6998560e+00,  5.0935876e-01, -9.9820107e-01,  1.4086099e-01,\n",
       "         8.6462480e-01,  2.4881031e-01,  1.0826178e+00,  3.6469951e+00,\n",
       "        -1.1488781e+00, -1.1821949e+00, -1.0230531e+00,  6.3472766e-01,\n",
       "        -3.1953983e-02, -2.7297294e-01,  9.8714900e-01,  1.5696347e+00,\n",
       "        -4.9239498e-01, -4.1693616e-01,  2.2190847e+00, -2.7104920e-02,\n",
       "        -1.2784506e+00, -1.4489946e+00,  1.4304202e+00, -1.3020091e+00,\n",
       "        -9.6543115e-01,  7.7848053e-01,  2.6373491e-01,  3.2971978e-01,\n",
       "         6.8974882e-01, -3.3079576e-01,  3.7691465e-01, -6.0058510e-01,\n",
       "        -1.7280188e-01, -2.7372885e-01,  4.1075906e-01, -1.1847235e-01,\n",
       "         1.6978482e-02, -4.4983569e-01,  1.0332311e+00, -1.5782654e-01,\n",
       "         2.0932268e-01,  1.3113562e+00, -2.3583121e+00,  1.5313370e+00,\n",
       "         2.2851493e+00,  6.0846645e-01, -7.0240480e-01,  8.8521713e-01,\n",
       "        -6.4210927e-01,  3.7898326e-01, -2.4578805e+00, -1.3315520e-01,\n",
       "        -1.1157136e+00, -8.5420346e-01, -5.5156659e-02, -1.1049773e+00,\n",
       "         2.1799952e-01,  2.1871748e+00,  2.5398934e-01, -9.3997931e-01,\n",
       "         6.2597245e-02,  2.6260722e-02, -2.5952083e-01,  9.2752904e-01,\n",
       "         7.8767234e-01, -3.1370166e-01,  1.3386887e+00,  9.3381798e-01,\n",
       "        -7.5362402e-01, -1.4382342e+00,  5.8588767e-01,  5.3397352e-01,\n",
       "        -9.0991998e-01, -7.0695651e-01,  1.2838287e+00, -7.0796460e-02,\n",
       "        -9.0680279e-02,  1.0505921e+00,  4.5053786e-04, -2.7344790e-01,\n",
       "        -1.8174144e-02, -8.8392359e-01,  1.6946103e-01, -5.6050473e-01,\n",
       "         1.3921455e+00, -3.3875577e-02,  6.5606344e-01, -7.7039111e-01,\n",
       "         4.0019271e-01, -6.6061282e-01,  2.0682719e-02,  1.8635869e+00,\n",
       "         3.9681670e-01, -1.1262758e+00, -8.0186236e-01, -8.5830438e-01,\n",
       "         3.2654068e-01, -1.0028749e+00, -8.5222667e-01, -1.8618888e+00,\n",
       "         2.3592517e+00, -1.5966349e+00,  3.0520666e-01, -3.7226251e-01,\n",
       "         7.0115650e-01,  3.5272452e-01, -4.0546143e-01,  1.7745767e+00,\n",
       "        -2.0918596e-01, -3.5240397e-01, -1.3428093e-01,  2.0528708e+00,\n",
       "        -5.8731967e-01,  1.6845452e+00,  8.9008868e-01, -1.5593152e+00,\n",
       "         1.2133504e-01,  1.1207848e+00,  1.2349330e+00,  9.8207211e-01,\n",
       "        -6.9747663e-01,  8.8686627e-01, -9.2248011e-01, -4.0254569e-01,\n",
       "        -1.3725151e+00, -7.5760412e-01, -7.3391819e-01, -1.2824657e-02,\n",
       "        -1.4740846e+00,  6.3698792e-01, -4.6753669e-01], dtype=float32),\n",
       " array([-4.75438029e-01,  1.34979200e+00, -1.86555848e-01,  4.43764120e-01,\n",
       "        -2.55668879e-01,  1.20552981e+00,  2.33653754e-01,  2.52401471e-01,\n",
       "         6.69124067e-01, -8.57133627e-01, -1.28172114e-01, -3.89385343e-01,\n",
       "         8.80583972e-02,  9.96048331e-01,  3.33892345e-01,  1.63092589e+00,\n",
       "         3.95073026e-01,  9.34766710e-01, -8.36048782e-01,  2.79386155e-02,\n",
       "         3.07979846e+00,  3.77791822e-01, -7.35851943e-01,  3.28526318e-01,\n",
       "         9.89150882e-01,  1.23859942e-01,  5.66297412e-01,  3.38315058e+00,\n",
       "        -1.18462825e+00, -1.03904939e+00, -1.06937099e+00,  5.43234289e-01,\n",
       "        -2.41193309e-01, -3.61262299e-02,  7.73209751e-01,  1.44432902e+00,\n",
       "        -3.78508925e-01, -5.53564548e-01,  1.80126846e+00,  9.06990767e-02,\n",
       "        -1.14711642e+00, -1.42134655e+00,  1.11882794e+00, -1.45198834e+00,\n",
       "        -9.16389942e-01,  9.86150980e-01,  2.88171113e-01,  2.30914846e-01,\n",
       "         6.37452126e-01, -5.31370759e-01,  1.70152217e-01, -6.28277838e-01,\n",
       "        -3.33944820e-02, -1.12221576e-01,  1.31861925e-01, -2.79240727e-01,\n",
       "         8.19330215e-02, -1.10429935e-01,  8.77803802e-01, -1.47585943e-01,\n",
       "         4.23754975e-02,  1.30573595e+00, -2.12674594e+00,  1.25722599e+00,\n",
       "         2.04787278e+00,  6.93682373e-01, -4.89658177e-01,  1.07441735e+00,\n",
       "        -9.76141691e-01, -1.45366877e-01, -2.72223139e+00, -1.52133077e-01,\n",
       "        -1.24602878e+00, -6.28295183e-01, -2.57954419e-01, -1.39539540e+00,\n",
       "         6.19662292e-02,  2.03311753e+00,  2.07130864e-01, -1.07093942e+00,\n",
       "         2.95165241e-01, -1.71932966e-01, -3.69441688e-01,  3.94058764e-01,\n",
       "         7.74339795e-01, -3.88443083e-01,  7.03280926e-01,  7.92734683e-01,\n",
       "        -5.05804479e-01, -1.65159440e+00,  7.70300388e-01,  8.09402883e-01,\n",
       "        -9.14818764e-01, -8.98662865e-01,  1.18796980e+00,  7.08848611e-02,\n",
       "        -8.78804550e-02,  8.02058578e-01, -2.40439370e-01, -5.32073200e-01,\n",
       "         2.05872551e-01, -7.90405989e-01,  5.38233042e-01, -4.77782786e-01,\n",
       "         1.29525185e+00, -7.96615779e-02,  6.15197897e-01, -7.24304318e-01,\n",
       "         4.03734803e-01, -1.11566281e+00, -2.94727504e-01,  1.57638597e+00,\n",
       "         7.11787343e-01, -9.65752065e-01, -5.50518095e-01, -9.27852392e-01,\n",
       "         3.14745754e-01, -1.13347757e+00, -1.05801380e+00, -1.70276749e+00,\n",
       "         2.28031898e+00, -2.00856185e+00,  2.99690425e-01, -3.89198303e-01,\n",
       "         4.32572305e-01,  4.20300752e-01, -4.63787287e-01,  1.90373778e+00,\n",
       "         1.40830442e-01, -5.38439751e-01, -1.15865268e-01,  2.05393028e+00,\n",
       "        -3.39472860e-01,  1.86288643e+00,  1.07485652e+00, -1.68569183e+00,\n",
       "         1.00980382e-02,  1.15055883e+00,  1.24215829e+00,  9.35872257e-01,\n",
       "        -4.98169959e-01,  9.23216105e-01, -6.78348720e-01, -5.92063189e-01,\n",
       "        -1.77169073e+00, -5.87235153e-01, -5.73337257e-01, -2.18518544e-03,\n",
       "        -1.34994400e+00,  4.85770911e-01, -4.32865441e-01], dtype=float32),\n",
       " array([-4.75438029e-01,  1.34979200e+00, -1.86555848e-01,  4.43764120e-01,\n",
       "        -2.55668879e-01,  1.20552981e+00,  2.33653754e-01,  2.52401471e-01,\n",
       "         6.69124067e-01, -8.57133627e-01, -1.28172114e-01, -3.89385343e-01,\n",
       "         8.80583972e-02,  9.96048331e-01,  3.33892345e-01,  1.63092589e+00,\n",
       "         3.95073026e-01,  9.34766710e-01, -8.36048782e-01,  2.79386155e-02,\n",
       "         3.07979846e+00,  3.77791822e-01, -7.35851943e-01,  3.28526318e-01,\n",
       "         9.89150882e-01,  1.23859942e-01,  5.66297412e-01,  3.38315058e+00,\n",
       "        -1.18462825e+00, -1.03904939e+00, -1.06937099e+00,  5.43234289e-01,\n",
       "        -2.41193309e-01, -3.61262299e-02,  7.73209751e-01,  1.44432902e+00,\n",
       "        -3.78508925e-01, -5.53564548e-01,  1.80126846e+00,  9.06990767e-02,\n",
       "        -1.14711642e+00, -1.42134655e+00,  1.11882794e+00, -1.45198834e+00,\n",
       "        -9.16389942e-01,  9.86150980e-01,  2.88171113e-01,  2.30914846e-01,\n",
       "         6.37452126e-01, -5.31370759e-01,  1.70152217e-01, -6.28277838e-01,\n",
       "        -3.33944820e-02, -1.12221576e-01,  1.31861925e-01, -2.79240727e-01,\n",
       "         8.19330215e-02, -1.10429935e-01,  8.77803802e-01, -1.47585943e-01,\n",
       "         4.23754975e-02,  1.30573595e+00, -2.12674594e+00,  1.25722599e+00,\n",
       "         2.04787278e+00,  6.93682373e-01, -4.89658177e-01,  1.07441735e+00,\n",
       "        -9.76141691e-01, -1.45366877e-01, -2.72223139e+00, -1.52133077e-01,\n",
       "        -1.24602878e+00, -6.28295183e-01, -2.57954419e-01, -1.39539540e+00,\n",
       "         6.19662292e-02,  2.03311753e+00,  2.07130864e-01, -1.07093942e+00,\n",
       "         2.95165241e-01, -1.71932966e-01, -3.69441688e-01,  3.94058764e-01,\n",
       "         7.74339795e-01, -3.88443083e-01,  7.03280926e-01,  7.92734683e-01,\n",
       "        -5.05804479e-01, -1.65159440e+00,  7.70300388e-01,  8.09402883e-01,\n",
       "        -9.14818764e-01, -8.98662865e-01,  1.18796980e+00,  7.08848611e-02,\n",
       "        -8.78804550e-02,  8.02058578e-01, -2.40439370e-01, -5.32073200e-01,\n",
       "         2.05872551e-01, -7.90405989e-01,  5.38233042e-01, -4.77782786e-01,\n",
       "         1.29525185e+00, -7.96615779e-02,  6.15197897e-01, -7.24304318e-01,\n",
       "         4.03734803e-01, -1.11566281e+00, -2.94727504e-01,  1.57638597e+00,\n",
       "         7.11787343e-01, -9.65752065e-01, -5.50518095e-01, -9.27852392e-01,\n",
       "         3.14745754e-01, -1.13347757e+00, -1.05801380e+00, -1.70276749e+00,\n",
       "         2.28031898e+00, -2.00856185e+00,  2.99690425e-01, -3.89198303e-01,\n",
       "         4.32572305e-01,  4.20300752e-01, -4.63787287e-01,  1.90373778e+00,\n",
       "         1.40830442e-01, -5.38439751e-01, -1.15865268e-01,  2.05393028e+00,\n",
       "        -3.39472860e-01,  1.86288643e+00,  1.07485652e+00, -1.68569183e+00,\n",
       "         1.00980382e-02,  1.15055883e+00,  1.24215829e+00,  9.35872257e-01,\n",
       "        -4.98169959e-01,  9.23216105e-01, -6.78348720e-01, -5.92063189e-01,\n",
       "        -1.77169073e+00, -5.87235153e-01, -5.73337257e-01, -2.18518544e-03,\n",
       "        -1.34994400e+00,  4.85770911e-01, -4.32865441e-01], dtype=float32),\n",
       " array([-4.75438029e-01,  1.34979200e+00, -1.86555848e-01,  4.43764120e-01,\n",
       "        -2.55668879e-01,  1.20552981e+00,  2.33653754e-01,  2.52401471e-01,\n",
       "         6.69124067e-01, -8.57133627e-01, -1.28172114e-01, -3.89385343e-01,\n",
       "         8.80583972e-02,  9.96048331e-01,  3.33892345e-01,  1.63092589e+00,\n",
       "         3.95073026e-01,  9.34766710e-01, -8.36048782e-01,  2.79386155e-02,\n",
       "         3.07979846e+00,  3.77791822e-01, -7.35851943e-01,  3.28526318e-01,\n",
       "         9.89150882e-01,  1.23859942e-01,  5.66297412e-01,  3.38315058e+00,\n",
       "        -1.18462825e+00, -1.03904939e+00, -1.06937099e+00,  5.43234289e-01,\n",
       "        -2.41193309e-01, -3.61262299e-02,  7.73209751e-01,  1.44432902e+00,\n",
       "        -3.78508925e-01, -5.53564548e-01,  1.80126846e+00,  9.06990767e-02,\n",
       "        -1.14711642e+00, -1.42134655e+00,  1.11882794e+00, -1.45198834e+00,\n",
       "        -9.16389942e-01,  9.86150980e-01,  2.88171113e-01,  2.30914846e-01,\n",
       "         6.37452126e-01, -5.31370759e-01,  1.70152217e-01, -6.28277838e-01,\n",
       "        -3.33944820e-02, -1.12221576e-01,  1.31861925e-01, -2.79240727e-01,\n",
       "         8.19330215e-02, -1.10429935e-01,  8.77803802e-01, -1.47585943e-01,\n",
       "         4.23754975e-02,  1.30573595e+00, -2.12674594e+00,  1.25722599e+00,\n",
       "         2.04787278e+00,  6.93682373e-01, -4.89658177e-01,  1.07441735e+00,\n",
       "        -9.76141691e-01, -1.45366877e-01, -2.72223139e+00, -1.52133077e-01,\n",
       "        -1.24602878e+00, -6.28295183e-01, -2.57954419e-01, -1.39539540e+00,\n",
       "         6.19662292e-02,  2.03311753e+00,  2.07130864e-01, -1.07093942e+00,\n",
       "         2.95165241e-01, -1.71932966e-01, -3.69441688e-01,  3.94058764e-01,\n",
       "         7.74339795e-01, -3.88443083e-01,  7.03280926e-01,  7.92734683e-01,\n",
       "        -5.05804479e-01, -1.65159440e+00,  7.70300388e-01,  8.09402883e-01,\n",
       "        -9.14818764e-01, -8.98662865e-01,  1.18796980e+00,  7.08848611e-02,\n",
       "        -8.78804550e-02,  8.02058578e-01, -2.40439370e-01, -5.32073200e-01,\n",
       "         2.05872551e-01, -7.90405989e-01,  5.38233042e-01, -4.77782786e-01,\n",
       "         1.29525185e+00, -7.96615779e-02,  6.15197897e-01, -7.24304318e-01,\n",
       "         4.03734803e-01, -1.11566281e+00, -2.94727504e-01,  1.57638597e+00,\n",
       "         7.11787343e-01, -9.65752065e-01, -5.50518095e-01, -9.27852392e-01,\n",
       "         3.14745754e-01, -1.13347757e+00, -1.05801380e+00, -1.70276749e+00,\n",
       "         2.28031898e+00, -2.00856185e+00,  2.99690425e-01, -3.89198303e-01,\n",
       "         4.32572305e-01,  4.20300752e-01, -4.63787287e-01,  1.90373778e+00,\n",
       "         1.40830442e-01, -5.38439751e-01, -1.15865268e-01,  2.05393028e+00,\n",
       "        -3.39472860e-01,  1.86288643e+00,  1.07485652e+00, -1.68569183e+00,\n",
       "         1.00980382e-02,  1.15055883e+00,  1.24215829e+00,  9.35872257e-01,\n",
       "        -4.98169959e-01,  9.23216105e-01, -6.78348720e-01, -5.92063189e-01,\n",
       "        -1.77169073e+00, -5.87235153e-01, -5.73337257e-01, -2.18518544e-03,\n",
       "        -1.34994400e+00,  4.85770911e-01, -4.32865441e-01], dtype=float32),\n",
       " array([-0.47457644,  1.0964061 , -0.15525806,  0.29625052,  0.0155032 ,\n",
       "         1.0800072 ,  0.03570959,  0.37403685,  0.7645445 , -0.8935256 ,\n",
       "        -0.11110251, -0.5272489 , -0.11068446,  1.1476595 ,  0.69532603,\n",
       "         1.6268018 ,  0.20388122,  0.46962875, -1.0149972 ,  0.33844215,\n",
       "         3.1148262 ,  0.5231553 , -0.33615312,  0.39965564,  1.0121311 ,\n",
       "         0.01325375,  0.32187283,  3.4272497 , -1.2841296 , -0.79178876,\n",
       "        -1.1821995 ,  0.81173646, -0.21724434,  0.29105955,  0.92894   ,\n",
       "         1.6087353 ,  0.09098762, -0.4234272 ,  1.5834953 ,  0.12549347,\n",
       "        -1.3127232 , -1.4897164 ,  1.0638273 , -1.5030841 , -0.88320243,\n",
       "         0.58525634,  0.13733838,  0.37493545,  0.9655132 , -0.58388126,\n",
       "        -0.43545833, -0.5039717 , -0.31743   , -0.3163712 ,  0.53143483,\n",
       "        -0.48161665,  0.1414501 , -0.29079363,  1.4221529 , -0.23915362,\n",
       "         0.19938248,  1.4541594 , -1.8713584 ,  1.2115949 ,  1.5943414 ,\n",
       "         0.41513506, -0.49437916,  1.3270257 , -1.1578256 , -0.12177841,\n",
       "        -2.7295468 , -0.3872835 , -1.4948386 , -0.8076468 , -0.29615006,\n",
       "        -1.3488891 ,  0.3201836 ,  1.9696302 ,  0.40144473, -1.1042492 ,\n",
       "         0.41085228, -0.3737172 , -0.38166207,  0.47637334,  0.7229567 ,\n",
       "        -0.2795789 ,  0.90605956,  1.0320307 , -0.49010015, -1.6943678 ,\n",
       "         0.69750524,  0.76641834, -0.9122849 , -0.67976195,  1.0600412 ,\n",
       "        -0.33601934,  0.2265848 ,  1.009409  , -0.21261913, -0.6427713 ,\n",
       "         0.15244587, -0.8064624 ,  0.925969  , -0.6685671 ,  1.5200233 ,\n",
       "        -0.37096488,  0.7609519 , -0.8507097 ,  0.26809043, -1.1883557 ,\n",
       "        -0.43902558,  1.5856881 ,  0.7605255 , -1.2018344 , -0.88629615,\n",
       "        -0.9184745 ,  0.7270668 , -1.102722  , -0.8219517 , -1.4885504 ,\n",
       "         2.2293167 , -1.3823197 ,  0.3102643 , -0.5536319 ,  0.35828543,\n",
       "         0.72295314, -0.5122053 ,  1.6953628 ,  0.23530106, -0.91816723,\n",
       "         0.13941547,  1.7795664 , -0.24205461,  1.991919  ,  0.9507209 ,\n",
       "        -1.25615   , -0.17805815,  1.1013432 ,  1.0639073 ,  0.6714909 ,\n",
       "        -0.49207953,  0.8329008 , -0.41875857, -0.90217346, -1.416432  ,\n",
       "        -1.2331514 , -0.71047777, -0.02390266, -1.4532269 ,  0.829067  ,\n",
       "        -0.44115436], dtype=float32),\n",
       " array([-0.47457644,  1.0964061 , -0.15525806,  0.29625052,  0.0155032 ,\n",
       "         1.0800072 ,  0.03570959,  0.37403685,  0.7645445 , -0.8935256 ,\n",
       "        -0.11110251, -0.5272489 , -0.11068446,  1.1476595 ,  0.69532603,\n",
       "         1.6268018 ,  0.20388122,  0.46962875, -1.0149972 ,  0.33844215,\n",
       "         3.1148262 ,  0.5231553 , -0.33615312,  0.39965564,  1.0121311 ,\n",
       "         0.01325375,  0.32187283,  3.4272497 , -1.2841296 , -0.79178876,\n",
       "        -1.1821995 ,  0.81173646, -0.21724434,  0.29105955,  0.92894   ,\n",
       "         1.6087353 ,  0.09098762, -0.4234272 ,  1.5834953 ,  0.12549347,\n",
       "        -1.3127232 , -1.4897164 ,  1.0638273 , -1.5030841 , -0.88320243,\n",
       "         0.58525634,  0.13733838,  0.37493545,  0.9655132 , -0.58388126,\n",
       "        -0.43545833, -0.5039717 , -0.31743   , -0.3163712 ,  0.53143483,\n",
       "        -0.48161665,  0.1414501 , -0.29079363,  1.4221529 , -0.23915362,\n",
       "         0.19938248,  1.4541594 , -1.8713584 ,  1.2115949 ,  1.5943414 ,\n",
       "         0.41513506, -0.49437916,  1.3270257 , -1.1578256 , -0.12177841,\n",
       "        -2.7295468 , -0.3872835 , -1.4948386 , -0.8076468 , -0.29615006,\n",
       "        -1.3488891 ,  0.3201836 ,  1.9696302 ,  0.40144473, -1.1042492 ,\n",
       "         0.41085228, -0.3737172 , -0.38166207,  0.47637334,  0.7229567 ,\n",
       "        -0.2795789 ,  0.90605956,  1.0320307 , -0.49010015, -1.6943678 ,\n",
       "         0.69750524,  0.76641834, -0.9122849 , -0.67976195,  1.0600412 ,\n",
       "        -0.33601934,  0.2265848 ,  1.009409  , -0.21261913, -0.6427713 ,\n",
       "         0.15244587, -0.8064624 ,  0.925969  , -0.6685671 ,  1.5200233 ,\n",
       "        -0.37096488,  0.7609519 , -0.8507097 ,  0.26809043, -1.1883557 ,\n",
       "        -0.43902558,  1.5856881 ,  0.7605255 , -1.2018344 , -0.88629615,\n",
       "        -0.9184745 ,  0.7270668 , -1.102722  , -0.8219517 , -1.4885504 ,\n",
       "         2.2293167 , -1.3823197 ,  0.3102643 , -0.5536319 ,  0.35828543,\n",
       "         0.72295314, -0.5122053 ,  1.6953628 ,  0.23530106, -0.91816723,\n",
       "         0.13941547,  1.7795664 , -0.24205461,  1.991919  ,  0.9507209 ,\n",
       "        -1.25615   , -0.17805815,  1.1013432 ,  1.0639073 ,  0.6714909 ,\n",
       "        -0.49207953,  0.8329008 , -0.41875857, -0.90217346, -1.416432  ,\n",
       "        -1.2331514 , -0.71047777, -0.02390266, -1.4532269 ,  0.829067  ,\n",
       "        -0.44115436], dtype=float32),\n",
       " array([-0.1849742 ,  0.33251023, -0.1162852 ,  0.20897505,  0.44212508,\n",
       "         0.93441457,  0.09919669,  0.49175143,  0.7666021 , -0.6401952 ,\n",
       "        -0.02639814, -0.45974344, -0.27201   ,  1.3909639 ,  0.75172424,\n",
       "         1.5512052 ,  0.13149014,  0.46699527, -1.2131566 ,  0.3325087 ,\n",
       "         2.9868472 ,  0.34779888, -0.41695166,  0.47055328,  0.92355216,\n",
       "         0.00872285,  0.41245678,  3.2888112 , -1.0927745 , -0.97684467,\n",
       "        -1.3981911 ,  0.820626  , -0.45903856,  0.40680313,  0.62999284,\n",
       "         1.5745864 ,  0.06452031, -0.36438984,  1.4311441 , -0.05592094,\n",
       "        -1.1054256 , -1.4137642 ,  0.92702127, -1.0886836 , -0.78382146,\n",
       "         0.62178534,  0.34676442,  0.46486375,  1.1875957 , -0.6079137 ,\n",
       "        -0.49358323, -0.5150713 , -0.46022418, -0.27451038,  0.6244401 ,\n",
       "        -0.31486535,  0.32455298, -0.25841162,  1.6643537 , -0.26350784,\n",
       "         0.55633014,  1.4179449 , -1.2295549 ,  1.29512   ,  1.4263608 ,\n",
       "         0.38668194, -0.07427992,  1.56163   , -1.0557388 , -0.38351086,\n",
       "        -2.6759794 , -0.4909153 , -1.8555768 , -0.9050287 , -0.09825762,\n",
       "        -1.1057773 ,  0.41219634,  1.8731722 ,  0.43426582, -0.64506954,\n",
       "         0.4183909 , -0.2403127 , -0.37493515,  0.21823175,  0.89352953,\n",
       "        -0.2912728 ,  0.97529715,  0.7716921 , -0.29247314, -1.8424171 ,\n",
       "         0.62085676,  0.5236892 , -0.9711831 , -0.7623055 ,  1.2390879 ,\n",
       "        -0.24365553,  0.43382317,  0.6903416 ,  0.08597242, -0.6180743 ,\n",
       "         0.07193919, -1.0251474 ,  0.7732738 , -0.49384812,  1.3714037 ,\n",
       "        -0.6033229 ,  0.66397005, -0.8203862 , -0.01531499, -1.0620226 ,\n",
       "        -0.5015337 ,  1.3213586 ,  0.9305428 , -1.4966856 , -0.8087691 ,\n",
       "        -1.082739  ,  0.67352545, -1.3285301 , -1.077341  , -1.6757258 ,\n",
       "         1.964571  , -1.4104141 ,  0.5735727 , -0.9155445 ,  0.17758174,\n",
       "         0.6060658 , -0.43310207,  1.7923476 ,  0.35522854, -1.2110329 ,\n",
       "         0.05811647,  1.4480035 ,  0.08974832,  2.2856734 ,  1.0594457 ,\n",
       "        -1.1991696 , -0.25851297,  0.96327424,  0.794725  ,  0.2128377 ,\n",
       "        -0.7715704 ,  0.56485707, -0.52313465, -0.7200032 , -1.6225884 ,\n",
       "        -1.1340541 , -0.61227924,  0.18569282, -1.3884094 ,  0.866208  ,\n",
       "        -0.23323171], dtype=float32),\n",
       " array([-3.3285320e-01,  3.9348048e-01, -2.6611492e-01,  2.8489428e-04,\n",
       "         8.3401024e-01,  9.2325193e-01, -2.6621285e-01,  1.7766953e-01,\n",
       "         8.3050293e-01, -5.3929991e-01,  7.8328457e-03, -2.3053342e-01,\n",
       "        -3.1381860e-01,  1.3650987e+00,  8.2428920e-01,  1.7845414e+00,\n",
       "         4.7939807e-02,  3.6830199e-01, -1.0938433e+00,  6.7661092e-02,\n",
       "         2.9624734e+00,  3.2417357e-01, -7.1604818e-01,  2.8198996e-01,\n",
       "         1.1615045e+00, -6.7696750e-01,  7.3676342e-01,  3.2059801e+00,\n",
       "        -7.1887642e-01, -1.0922647e+00, -1.1466020e+00,  5.1873952e-01,\n",
       "        -6.6847366e-01,  4.6263561e-01,  4.4917363e-01,  1.4910454e+00,\n",
       "         6.7209103e-03,  4.7478238e-03,  9.2453730e-01, -5.8169270e-01,\n",
       "        -1.0319781e+00, -1.5946982e+00,  1.3298120e+00, -1.2914243e+00,\n",
       "        -9.4515407e-01,  4.4786572e-01,  6.4660299e-01,  6.8441188e-01,\n",
       "         1.2923592e+00, -8.2551038e-01, -4.2017639e-01, -4.1338772e-01,\n",
       "        -8.2781029e-01, -4.4137612e-01,  8.9349604e-01, -2.3947194e-01,\n",
       "         7.6534349e-01, -1.8963544e-01,  1.5984060e+00, -2.6404691e-01,\n",
       "         7.1309942e-01,  1.6224233e+00, -1.6416299e+00,  1.3967735e+00,\n",
       "         1.3391770e+00,  5.4878837e-01, -2.0573837e-01,  1.5316541e+00,\n",
       "        -7.9534012e-01, -3.7944701e-01, -2.5610223e+00, -5.8077884e-01,\n",
       "        -2.0777977e+00, -8.9518052e-01,  4.9416929e-02, -1.4625319e+00,\n",
       "         6.2871605e-01,  1.8159748e+00,  1.8913378e-01, -9.1772354e-01,\n",
       "         5.9232569e-01, -3.5963096e-02, -6.9551289e-01, -8.7706439e-02,\n",
       "         5.1322317e-01, -4.1522101e-01,  1.1504937e+00,  1.0493449e+00,\n",
       "        -8.4780328e-02, -1.8394924e+00,  5.6036514e-01,  2.1519959e-01,\n",
       "        -9.5043802e-01, -6.1113423e-01,  1.3612436e+00, -5.5544579e-01,\n",
       "         4.3560237e-01,  2.7222523e-01, -1.2494211e-01, -9.7343975e-01,\n",
       "         4.0305755e-01, -1.1204686e+00,  1.2418793e+00, -7.9876220e-01,\n",
       "         1.5700866e+00, -7.3401713e-01,  8.1388760e-01, -8.1161684e-01,\n",
       "        -2.6187366e-01, -1.0027512e+00, -1.3521661e-01,  1.6025083e+00,\n",
       "         9.8387825e-01, -1.5059785e+00, -7.4056506e-01, -8.1438690e-01,\n",
       "         8.1457925e-01, -1.3184447e+00, -7.1925628e-01, -1.3208885e+00,\n",
       "         1.9084305e+00, -1.3156402e+00,  8.0017364e-01, -8.7308365e-01,\n",
       "         1.3464309e-01,  7.3217773e-01, -5.0840849e-01,  1.7293333e+00,\n",
       "         6.2346834e-01, -1.4367417e+00,  2.1566598e-01,  1.5481204e+00,\n",
       "        -3.6720279e-01,  2.2270947e+00,  5.5935615e-01, -1.0689771e+00,\n",
       "        -4.8232391e-01,  8.3392256e-01,  5.2967852e-01, -3.2088226e-01,\n",
       "        -6.2947476e-01,  6.4167875e-01, -6.3492119e-01, -5.4327726e-01,\n",
       "        -1.2895012e+00, -1.1940596e+00, -9.6895300e-02,  3.0381035e-02,\n",
       "        -1.3547955e+00,  7.5446373e-01, -3.5985082e-01], dtype=float32),\n",
       " array([-1.93282381e-01,  3.55229229e-01,  4.62190881e-02,  1.88930538e-02,\n",
       "         5.85585952e-01,  6.90598845e-01, -1.31161168e-01,  3.15534055e-01,\n",
       "         1.23393130e+00, -3.22177649e-01,  5.41343866e-03, -2.85628676e-01,\n",
       "        -4.60660875e-01,  1.16250420e+00,  9.80316460e-01,  1.60243702e+00,\n",
       "         6.49229288e-02,  1.30178571e-01, -8.51862967e-01, -1.53464489e-02,\n",
       "         2.97758508e+00,  4.40579027e-01, -1.08642983e+00, -3.79435718e-02,\n",
       "         1.19935548e+00, -5.43962121e-01,  1.08656347e+00,  3.12223434e+00,\n",
       "        -6.21419311e-01, -1.41902804e+00, -1.21091115e+00,  5.55598378e-01,\n",
       "        -6.62489653e-01,  2.50404745e-01,  3.91573429e-01,  1.56137574e+00,\n",
       "         5.55689633e-01,  1.28010318e-01,  1.09741604e+00, -6.65862501e-01,\n",
       "        -7.78347492e-01, -1.61258912e+00,  1.05316627e+00, -1.29747951e+00,\n",
       "        -9.94352043e-01,  3.99800688e-01,  2.73560375e-01,  8.40064704e-01,\n",
       "         1.57612443e+00, -8.04526865e-01, -2.74911344e-01, -5.51306605e-01,\n",
       "        -1.05948448e+00, -4.69708204e-01,  8.88271630e-01, -2.57420421e-01,\n",
       "         8.14074576e-01, -4.35861170e-01,  1.72686887e+00, -2.48875648e-01,\n",
       "         8.87335777e-01,  1.51874757e+00, -1.79755223e+00,  1.26813900e+00,\n",
       "         1.37683725e+00,  4.60456163e-01, -2.54686981e-01,  1.31011081e+00,\n",
       "        -1.44839895e+00, -2.40050107e-01, -2.55208135e+00, -4.49664921e-01,\n",
       "        -2.22306347e+00, -7.37034559e-01,  9.11036283e-02, -1.33653283e+00,\n",
       "         4.89727616e-01,  1.76957774e+00,  1.42041713e-01, -9.49230134e-01,\n",
       "         4.21540856e-01,  2.35315003e-02, -8.72753859e-01,  2.54007075e-02,\n",
       "         2.82390296e-01, -8.10402215e-01,  9.10441637e-01,  1.07203948e+00,\n",
       "        -5.15720904e-01, -1.43214905e+00,  5.68839788e-01,  2.18803927e-01,\n",
       "        -6.80507123e-01, -4.62520182e-01,  1.42816389e+00, -5.74817359e-01,\n",
       "         2.37317145e-01,  2.99645275e-01, -1.22202784e-01, -9.30502057e-01,\n",
       "         3.44288796e-01, -9.17575657e-01,  1.32183754e+00, -9.34110463e-01,\n",
       "         1.47941816e+00, -4.93807495e-01,  9.54403102e-01, -8.89362037e-01,\n",
       "        -3.35997105e-01, -7.33106256e-01, -1.70446187e-01,  1.68795025e+00,\n",
       "         1.06260741e+00, -1.24298120e+00, -1.18401611e+00, -6.12209439e-01,\n",
       "         8.27608526e-01, -1.24046457e+00, -4.83191580e-01, -1.41483796e+00,\n",
       "         1.95362353e+00, -1.82929695e+00,  9.80048835e-01, -9.16065574e-01,\n",
       "        -1.38722416e-02,  6.60615861e-01, -4.50731725e-01,  1.86757326e+00,\n",
       "         5.77825308e-01, -1.41637349e+00,  3.55472594e-01,  1.95937765e+00,\n",
       "        -4.98436481e-01,  2.28050447e+00,  3.25547576e-01, -1.12130654e+00,\n",
       "        -2.04808325e-01,  7.28138804e-01,  3.07458013e-01, -4.16335166e-01,\n",
       "        -6.90431058e-01,  2.93115556e-01, -4.78309244e-01, -6.12707138e-01,\n",
       "        -1.24747837e+00, -1.09881318e+00,  4.85371973e-04, -2.00026527e-01,\n",
       "        -1.32668412e+00,  6.18916690e-01, -7.30587780e-01], dtype=float32),\n",
       " array([-1.93282381e-01,  3.55229229e-01,  4.62190881e-02,  1.88930538e-02,\n",
       "         5.85585952e-01,  6.90598845e-01, -1.31161168e-01,  3.15534055e-01,\n",
       "         1.23393130e+00, -3.22177649e-01,  5.41343866e-03, -2.85628676e-01,\n",
       "        -4.60660875e-01,  1.16250420e+00,  9.80316460e-01,  1.60243702e+00,\n",
       "         6.49229288e-02,  1.30178571e-01, -8.51862967e-01, -1.53464489e-02,\n",
       "         2.97758508e+00,  4.40579027e-01, -1.08642983e+00, -3.79435718e-02,\n",
       "         1.19935548e+00, -5.43962121e-01,  1.08656347e+00,  3.12223434e+00,\n",
       "        -6.21419311e-01, -1.41902804e+00, -1.21091115e+00,  5.55598378e-01,\n",
       "        -6.62489653e-01,  2.50404745e-01,  3.91573429e-01,  1.56137574e+00,\n",
       "         5.55689633e-01,  1.28010318e-01,  1.09741604e+00, -6.65862501e-01,\n",
       "        -7.78347492e-01, -1.61258912e+00,  1.05316627e+00, -1.29747951e+00,\n",
       "        -9.94352043e-01,  3.99800688e-01,  2.73560375e-01,  8.40064704e-01,\n",
       "         1.57612443e+00, -8.04526865e-01, -2.74911344e-01, -5.51306605e-01,\n",
       "        -1.05948448e+00, -4.69708204e-01,  8.88271630e-01, -2.57420421e-01,\n",
       "         8.14074576e-01, -4.35861170e-01,  1.72686887e+00, -2.48875648e-01,\n",
       "         8.87335777e-01,  1.51874757e+00, -1.79755223e+00,  1.26813900e+00,\n",
       "         1.37683725e+00,  4.60456163e-01, -2.54686981e-01,  1.31011081e+00,\n",
       "        -1.44839895e+00, -2.40050107e-01, -2.55208135e+00, -4.49664921e-01,\n",
       "        -2.22306347e+00, -7.37034559e-01,  9.11036283e-02, -1.33653283e+00,\n",
       "         4.89727616e-01,  1.76957774e+00,  1.42041713e-01, -9.49230134e-01,\n",
       "         4.21540856e-01,  2.35315003e-02, -8.72753859e-01,  2.54007075e-02,\n",
       "         2.82390296e-01, -8.10402215e-01,  9.10441637e-01,  1.07203948e+00,\n",
       "        -5.15720904e-01, -1.43214905e+00,  5.68839788e-01,  2.18803927e-01,\n",
       "        -6.80507123e-01, -4.62520182e-01,  1.42816389e+00, -5.74817359e-01,\n",
       "         2.37317145e-01,  2.99645275e-01, -1.22202784e-01, -9.30502057e-01,\n",
       "         3.44288796e-01, -9.17575657e-01,  1.32183754e+00, -9.34110463e-01,\n",
       "         1.47941816e+00, -4.93807495e-01,  9.54403102e-01, -8.89362037e-01,\n",
       "        -3.35997105e-01, -7.33106256e-01, -1.70446187e-01,  1.68795025e+00,\n",
       "         1.06260741e+00, -1.24298120e+00, -1.18401611e+00, -6.12209439e-01,\n",
       "         8.27608526e-01, -1.24046457e+00, -4.83191580e-01, -1.41483796e+00,\n",
       "         1.95362353e+00, -1.82929695e+00,  9.80048835e-01, -9.16065574e-01,\n",
       "        -1.38722416e-02,  6.60615861e-01, -4.50731725e-01,  1.86757326e+00,\n",
       "         5.77825308e-01, -1.41637349e+00,  3.55472594e-01,  1.95937765e+00,\n",
       "        -4.98436481e-01,  2.28050447e+00,  3.25547576e-01, -1.12130654e+00,\n",
       "        -2.04808325e-01,  7.28138804e-01,  3.07458013e-01, -4.16335166e-01,\n",
       "        -6.90431058e-01,  2.93115556e-01, -4.78309244e-01, -6.12707138e-01,\n",
       "        -1.24747837e+00, -1.09881318e+00,  4.85371973e-04, -2.00026527e-01,\n",
       "        -1.32668412e+00,  6.18916690e-01, -7.30587780e-01], dtype=float32),\n",
       " array([-0.31065527,  0.19727656,  0.5247185 ,  0.0358418 ,  0.6309293 ,\n",
       "         0.2894621 , -0.28417188,  0.37799555,  1.5107399 , -0.03228763,\n",
       "        -0.0446    , -0.4009166 , -0.21226001,  0.9960508 ,  1.2194012 ,\n",
       "         1.5176774 , -0.25884095,  0.34405237, -1.0413156 , -0.2260511 ,\n",
       "         3.1740904 ,  0.66428804, -1.0102482 ,  0.15076342,  1.236508  ,\n",
       "        -0.4580442 ,  1.2462863 ,  2.97805   , -0.7327402 , -1.1801958 ,\n",
       "        -1.0152141 ,  0.5580836 , -0.81958616,  0.30303386,  0.09281284,\n",
       "         1.5495858 ,  0.7767287 , -0.04856606,  1.0281826 , -0.59599805,\n",
       "        -0.6849145 , -1.9198433 ,  0.9466871 , -1.9418797 , -1.076489  ,\n",
       "         0.47884837,  0.4347052 ,  1.2038658 ,  1.8972126 , -0.5985407 ,\n",
       "        -0.49052894, -0.16564742, -0.8191236 , -0.12635727,  0.93731755,\n",
       "        -0.65777373,  0.34527197, -0.5728163 ,  1.6985512 , -0.20382562,\n",
       "         0.7716632 ,  1.4820518 , -1.8030347 ,  1.1691837 ,  1.5157975 ,\n",
       "         0.5172517 , -0.63359857,  1.3315697 , -1.2147348 , -0.6003548 ,\n",
       "        -2.7523458 , -0.18884279, -2.4314332 , -0.5781373 ,  0.20323563,\n",
       "        -1.1525136 ,  0.49386075,  1.7756916 ,  0.10316867, -1.0369934 ,\n",
       "         0.40235654, -0.08587057, -0.9361563 ,  0.0712325 ,  0.13660295,\n",
       "        -1.1712025 ,  0.8699493 ,  0.84639776, -0.8500216 , -1.0713203 ,\n",
       "         0.4671757 ,  0.23482344, -0.5774639 , -0.68257827,  1.2822398 ,\n",
       "        -0.41240984,  0.7166613 ,  0.4832518 , -0.23939295, -1.2762904 ,\n",
       "         0.42182556, -0.5048479 ,  1.4025596 , -1.0806428 ,  1.4577134 ,\n",
       "        -0.75592196,  0.83922017, -0.7728994 , -0.44115523, -0.61156934,\n",
       "        -0.15418619,  1.8778282 ,  1.1866083 , -1.325181  , -0.9162294 ,\n",
       "        -0.33405113,  0.75036484, -1.061126  , -0.702841  , -1.3757395 ,\n",
       "         1.806296  , -1.780392  ,  0.46762672, -1.0730582 , -0.30489618,\n",
       "         0.63596463, -0.4132272 ,  1.653601  ,  0.6081598 , -1.3133847 ,\n",
       "         0.2511347 ,  1.854581  , -0.22151795,  2.456927  ,  0.63884795,\n",
       "        -1.3662399 , -0.18194951,  0.80677134,  0.59673715, -0.3283896 ,\n",
       "        -0.54650676,  0.05829377, -0.45880052, -1.0412728 , -1.4418049 ,\n",
       "        -1.055308  , -0.04651992, -0.21475823, -1.4261832 ,  0.7979493 ,\n",
       "        -0.35364434], dtype=float32),\n",
       " array([-0.31065527,  0.19727656,  0.5247185 ,  0.0358418 ,  0.6309293 ,\n",
       "         0.2894621 , -0.28417188,  0.37799555,  1.5107399 , -0.03228763,\n",
       "        -0.0446    , -0.4009166 , -0.21226001,  0.9960508 ,  1.2194012 ,\n",
       "         1.5176774 , -0.25884095,  0.34405237, -1.0413156 , -0.2260511 ,\n",
       "         3.1740904 ,  0.66428804, -1.0102482 ,  0.15076342,  1.236508  ,\n",
       "        -0.4580442 ,  1.2462863 ,  2.97805   , -0.7327402 , -1.1801958 ,\n",
       "        -1.0152141 ,  0.5580836 , -0.81958616,  0.30303386,  0.09281284,\n",
       "         1.5495858 ,  0.7767287 , -0.04856606,  1.0281826 , -0.59599805,\n",
       "        -0.6849145 , -1.9198433 ,  0.9466871 , -1.9418797 , -1.076489  ,\n",
       "         0.47884837,  0.4347052 ,  1.2038658 ,  1.8972126 , -0.5985407 ,\n",
       "        -0.49052894, -0.16564742, -0.8191236 , -0.12635727,  0.93731755,\n",
       "        -0.65777373,  0.34527197, -0.5728163 ,  1.6985512 , -0.20382562,\n",
       "         0.7716632 ,  1.4820518 , -1.8030347 ,  1.1691837 ,  1.5157975 ,\n",
       "         0.5172517 , -0.63359857,  1.3315697 , -1.2147348 , -0.6003548 ,\n",
       "        -2.7523458 , -0.18884279, -2.4314332 , -0.5781373 ,  0.20323563,\n",
       "        -1.1525136 ,  0.49386075,  1.7756916 ,  0.10316867, -1.0369934 ,\n",
       "         0.40235654, -0.08587057, -0.9361563 ,  0.0712325 ,  0.13660295,\n",
       "        -1.1712025 ,  0.8699493 ,  0.84639776, -0.8500216 , -1.0713203 ,\n",
       "         0.4671757 ,  0.23482344, -0.5774639 , -0.68257827,  1.2822398 ,\n",
       "        -0.41240984,  0.7166613 ,  0.4832518 , -0.23939295, -1.2762904 ,\n",
       "         0.42182556, -0.5048479 ,  1.4025596 , -1.0806428 ,  1.4577134 ,\n",
       "        -0.75592196,  0.83922017, -0.7728994 , -0.44115523, -0.61156934,\n",
       "        -0.15418619,  1.8778282 ,  1.1866083 , -1.325181  , -0.9162294 ,\n",
       "        -0.33405113,  0.75036484, -1.061126  , -0.702841  , -1.3757395 ,\n",
       "         1.806296  , -1.780392  ,  0.46762672, -1.0730582 , -0.30489618,\n",
       "         0.63596463, -0.4132272 ,  1.653601  ,  0.6081598 , -1.3133847 ,\n",
       "         0.2511347 ,  1.854581  , -0.22151795,  2.456927  ,  0.63884795,\n",
       "        -1.3662399 , -0.18194951,  0.80677134,  0.59673715, -0.3283896 ,\n",
       "        -0.54650676,  0.05829377, -0.45880052, -1.0412728 , -1.4418049 ,\n",
       "        -1.055308  , -0.04651992, -0.21475823, -1.4261832 ,  0.7979493 ,\n",
       "        -0.35364434], dtype=float32),\n",
       " array([-0.31065527,  0.19727656,  0.5247185 ,  0.0358418 ,  0.6309293 ,\n",
       "         0.2894621 , -0.28417188,  0.37799555,  1.5107399 , -0.03228763,\n",
       "        -0.0446    , -0.4009166 , -0.21226001,  0.9960508 ,  1.2194012 ,\n",
       "         1.5176774 , -0.25884095,  0.34405237, -1.0413156 , -0.2260511 ,\n",
       "         3.1740904 ,  0.66428804, -1.0102482 ,  0.15076342,  1.236508  ,\n",
       "        -0.4580442 ,  1.2462863 ,  2.97805   , -0.7327402 , -1.1801958 ,\n",
       "        -1.0152141 ,  0.5580836 , -0.81958616,  0.30303386,  0.09281284,\n",
       "         1.5495858 ,  0.7767287 , -0.04856606,  1.0281826 , -0.59599805,\n",
       "        -0.6849145 , -1.9198433 ,  0.9466871 , -1.9418797 , -1.076489  ,\n",
       "         0.47884837,  0.4347052 ,  1.2038658 ,  1.8972126 , -0.5985407 ,\n",
       "        -0.49052894, -0.16564742, -0.8191236 , -0.12635727,  0.93731755,\n",
       "        -0.65777373,  0.34527197, -0.5728163 ,  1.6985512 , -0.20382562,\n",
       "         0.7716632 ,  1.4820518 , -1.8030347 ,  1.1691837 ,  1.5157975 ,\n",
       "         0.5172517 , -0.63359857,  1.3315697 , -1.2147348 , -0.6003548 ,\n",
       "        -2.7523458 , -0.18884279, -2.4314332 , -0.5781373 ,  0.20323563,\n",
       "        -1.1525136 ,  0.49386075,  1.7756916 ,  0.10316867, -1.0369934 ,\n",
       "         0.40235654, -0.08587057, -0.9361563 ,  0.0712325 ,  0.13660295,\n",
       "        -1.1712025 ,  0.8699493 ,  0.84639776, -0.8500216 , -1.0713203 ,\n",
       "         0.4671757 ,  0.23482344, -0.5774639 , -0.68257827,  1.2822398 ,\n",
       "        -0.41240984,  0.7166613 ,  0.4832518 , -0.23939295, -1.2762904 ,\n",
       "         0.42182556, -0.5048479 ,  1.4025596 , -1.0806428 ,  1.4577134 ,\n",
       "        -0.75592196,  0.83922017, -0.7728994 , -0.44115523, -0.61156934,\n",
       "        -0.15418619,  1.8778282 ,  1.1866083 , -1.325181  , -0.9162294 ,\n",
       "        -0.33405113,  0.75036484, -1.061126  , -0.702841  , -1.3757395 ,\n",
       "         1.806296  , -1.780392  ,  0.46762672, -1.0730582 , -0.30489618,\n",
       "         0.63596463, -0.4132272 ,  1.653601  ,  0.6081598 , -1.3133847 ,\n",
       "         0.2511347 ,  1.854581  , -0.22151795,  2.456927  ,  0.63884795,\n",
       "        -1.3662399 , -0.18194951,  0.80677134,  0.59673715, -0.3283896 ,\n",
       "        -0.54650676,  0.05829377, -0.45880052, -1.0412728 , -1.4418049 ,\n",
       "        -1.055308  , -0.04651992, -0.21475823, -1.4261832 ,  0.7979493 ,\n",
       "        -0.35364434], dtype=float32),\n",
       " array([-0.31065527,  0.19727656,  0.5247185 ,  0.0358418 ,  0.6309293 ,\n",
       "         0.2894621 , -0.28417188,  0.37799555,  1.5107399 , -0.03228763,\n",
       "        -0.0446    , -0.4009166 , -0.21226001,  0.9960508 ,  1.2194012 ,\n",
       "         1.5176774 , -0.25884095,  0.34405237, -1.0413156 , -0.2260511 ,\n",
       "         3.1740904 ,  0.66428804, -1.0102482 ,  0.15076342,  1.236508  ,\n",
       "        -0.4580442 ,  1.2462863 ,  2.97805   , -0.7327402 , -1.1801958 ,\n",
       "        -1.0152141 ,  0.5580836 , -0.81958616,  0.30303386,  0.09281284,\n",
       "         1.5495858 ,  0.7767287 , -0.04856606,  1.0281826 , -0.59599805,\n",
       "        -0.6849145 , -1.9198433 ,  0.9466871 , -1.9418797 , -1.076489  ,\n",
       "         0.47884837,  0.4347052 ,  1.2038658 ,  1.8972126 , -0.5985407 ,\n",
       "        -0.49052894, -0.16564742, -0.8191236 , -0.12635727,  0.93731755,\n",
       "        -0.65777373,  0.34527197, -0.5728163 ,  1.6985512 , -0.20382562,\n",
       "         0.7716632 ,  1.4820518 , -1.8030347 ,  1.1691837 ,  1.5157975 ,\n",
       "         0.5172517 , -0.63359857,  1.3315697 , -1.2147348 , -0.6003548 ,\n",
       "        -2.7523458 , -0.18884279, -2.4314332 , -0.5781373 ,  0.20323563,\n",
       "        -1.1525136 ,  0.49386075,  1.7756916 ,  0.10316867, -1.0369934 ,\n",
       "         0.40235654, -0.08587057, -0.9361563 ,  0.0712325 ,  0.13660295,\n",
       "        -1.1712025 ,  0.8699493 ,  0.84639776, -0.8500216 , -1.0713203 ,\n",
       "         0.4671757 ,  0.23482344, -0.5774639 , -0.68257827,  1.2822398 ,\n",
       "        -0.41240984,  0.7166613 ,  0.4832518 , -0.23939295, -1.2762904 ,\n",
       "         0.42182556, -0.5048479 ,  1.4025596 , -1.0806428 ,  1.4577134 ,\n",
       "        -0.75592196,  0.83922017, -0.7728994 , -0.44115523, -0.61156934,\n",
       "        -0.15418619,  1.8778282 ,  1.1866083 , -1.325181  , -0.9162294 ,\n",
       "        -0.33405113,  0.75036484, -1.061126  , -0.702841  , -1.3757395 ,\n",
       "         1.806296  , -1.780392  ,  0.46762672, -1.0730582 , -0.30489618,\n",
       "         0.63596463, -0.4132272 ,  1.653601  ,  0.6081598 , -1.3133847 ,\n",
       "         0.2511347 ,  1.854581  , -0.22151795,  2.456927  ,  0.63884795,\n",
       "        -1.3662399 , -0.18194951,  0.80677134,  0.59673715, -0.3283896 ,\n",
       "        -0.54650676,  0.05829377, -0.45880052, -1.0412728 , -1.4418049 ,\n",
       "        -1.055308  , -0.04651992, -0.21475823, -1.4261832 ,  0.7979493 ,\n",
       "        -0.35364434], dtype=float32),\n",
       " array([-0.31065527,  0.19727656,  0.5247185 ,  0.0358418 ,  0.6309293 ,\n",
       "         0.2894621 , -0.28417188,  0.37799555,  1.5107399 , -0.03228763,\n",
       "        -0.0446    , -0.4009166 , -0.21226001,  0.9960508 ,  1.2194012 ,\n",
       "         1.5176774 , -0.25884095,  0.34405237, -1.0413156 , -0.2260511 ,\n",
       "         3.1740904 ,  0.66428804, -1.0102482 ,  0.15076342,  1.236508  ,\n",
       "        -0.4580442 ,  1.2462863 ,  2.97805   , -0.7327402 , -1.1801958 ,\n",
       "        -1.0152141 ,  0.5580836 , -0.81958616,  0.30303386,  0.09281284,\n",
       "         1.5495858 ,  0.7767287 , -0.04856606,  1.0281826 , -0.59599805,\n",
       "        -0.6849145 , -1.9198433 ,  0.9466871 , -1.9418797 , -1.076489  ,\n",
       "         0.47884837,  0.4347052 ,  1.2038658 ,  1.8972126 , -0.5985407 ,\n",
       "        -0.49052894, -0.16564742, -0.8191236 , -0.12635727,  0.93731755,\n",
       "        -0.65777373,  0.34527197, -0.5728163 ,  1.6985512 , -0.20382562,\n",
       "         0.7716632 ,  1.4820518 , -1.8030347 ,  1.1691837 ,  1.5157975 ,\n",
       "         0.5172517 , -0.63359857,  1.3315697 , -1.2147348 , -0.6003548 ,\n",
       "        -2.7523458 , -0.18884279, -2.4314332 , -0.5781373 ,  0.20323563,\n",
       "        -1.1525136 ,  0.49386075,  1.7756916 ,  0.10316867, -1.0369934 ,\n",
       "         0.40235654, -0.08587057, -0.9361563 ,  0.0712325 ,  0.13660295,\n",
       "        -1.1712025 ,  0.8699493 ,  0.84639776, -0.8500216 , -1.0713203 ,\n",
       "         0.4671757 ,  0.23482344, -0.5774639 , -0.68257827,  1.2822398 ,\n",
       "        -0.41240984,  0.7166613 ,  0.4832518 , -0.23939295, -1.2762904 ,\n",
       "         0.42182556, -0.5048479 ,  1.4025596 , -1.0806428 ,  1.4577134 ,\n",
       "        -0.75592196,  0.83922017, -0.7728994 , -0.44115523, -0.61156934,\n",
       "        -0.15418619,  1.8778282 ,  1.1866083 , -1.325181  , -0.9162294 ,\n",
       "        -0.33405113,  0.75036484, -1.061126  , -0.702841  , -1.3757395 ,\n",
       "         1.806296  , -1.780392  ,  0.46762672, -1.0730582 , -0.30489618,\n",
       "         0.63596463, -0.4132272 ,  1.653601  ,  0.6081598 , -1.3133847 ,\n",
       "         0.2511347 ,  1.854581  , -0.22151795,  2.456927  ,  0.63884795,\n",
       "        -1.3662399 , -0.18194951,  0.80677134,  0.59673715, -0.3283896 ,\n",
       "        -0.54650676,  0.05829377, -0.45880052, -1.0412728 , -1.4418049 ,\n",
       "        -1.055308  , -0.04651992, -0.21475823, -1.4261832 ,  0.7979493 ,\n",
       "        -0.35364434], dtype=float32),\n",
       " array([-3.76798838e-01,  3.02723467e-01,  6.58963501e-01,  1.00046605e-01,\n",
       "         5.55624664e-01,  2.52627730e-01, -3.87952834e-01,  1.84723094e-01,\n",
       "         1.48011899e+00,  2.58686304e-01,  1.98059991e-01, -4.51054089e-02,\n",
       "        -2.99554199e-01,  7.19453335e-01,  1.06356072e+00,  1.68547499e+00,\n",
       "        -8.44985917e-02,  3.74492288e-01, -6.29048884e-01,  1.47991080e-03,\n",
       "         2.86786270e+00,  7.30910659e-01, -1.11361754e+00, -1.83791310e-01,\n",
       "         1.16461277e+00, -2.35952660e-01,  1.23923302e+00,  2.90462804e+00,\n",
       "        -8.42181027e-01, -1.23316324e+00, -1.00589859e+00,  3.42840523e-01,\n",
       "        -8.74580264e-01,  3.22523385e-01,  5.18767871e-02,  1.29090345e+00,\n",
       "         1.00489473e+00,  1.13350637e-01,  5.09702384e-01, -9.23588634e-01,\n",
       "        -7.32870936e-01, -2.26637268e+00,  9.62356448e-01, -2.32511353e+00,\n",
       "        -8.69576156e-01,  1.35992184e-01,  2.87879109e-01,  1.15000117e+00,\n",
       "         2.10129499e+00, -6.58808947e-01, -6.15321279e-01, -5.06944120e-01,\n",
       "        -7.25560665e-01, -6.24575419e-03,  7.93879569e-01, -2.12996185e-01,\n",
       "         2.91229963e-01, -5.72355449e-01,  1.41060317e+00, -2.05029726e-01,\n",
       "         9.58243966e-01,  1.06084800e+00, -1.50390518e+00,  7.18623281e-01,\n",
       "         1.14553535e+00,  2.43101090e-01, -8.33963037e-01,  1.03129900e+00,\n",
       "        -1.44117188e+00, -2.54065245e-01, -2.49621940e+00, -5.67941181e-02,\n",
       "        -2.25682759e+00, -5.10405600e-01, -8.88745412e-02, -8.49300563e-01,\n",
       "         1.07215130e+00,  1.74295175e+00,  1.87148005e-01, -1.12395048e+00,\n",
       "         5.15290618e-01, -1.68480620e-01, -8.24812710e-01,  3.20112228e-01,\n",
       "         1.36419356e-01, -1.32976043e+00,  1.11174738e+00,  9.98969436e-01,\n",
       "        -5.62791705e-01, -1.10287476e+00,  6.04786217e-01, -2.07503319e-01,\n",
       "        -4.85931784e-01, -7.20567465e-01,  1.42478931e+00, -9.41203311e-02,\n",
       "         2.96548665e-01,  3.85454535e-01, -3.49901825e-01, -1.06162775e+00,\n",
       "         4.04606402e-01, -4.25062150e-01,  1.45773757e+00, -9.50008035e-01,\n",
       "         1.31618869e+00, -8.28265667e-01,  7.76216090e-01, -8.85143161e-01,\n",
       "        -4.29077357e-01, -2.26952806e-01, -3.10640156e-01,  1.94674909e+00,\n",
       "         7.73408234e-01, -1.23027122e+00, -8.02137792e-01, -2.99141139e-01,\n",
       "         1.16973555e+00, -8.78063798e-01, -3.33741277e-01, -1.14887965e+00,\n",
       "         2.24316621e+00, -2.02098489e+00,  2.56750345e-01, -1.33156884e+00,\n",
       "        -2.71181792e-01,  4.25730884e-01, -3.65153760e-01,  1.71955597e+00,\n",
       "         8.68441880e-01, -1.64280140e+00,  2.93062657e-01,  1.56223130e+00,\n",
       "        -2.77365863e-01,  2.27370906e+00,  8.74316871e-01, -1.44804406e+00,\n",
       "        -4.85328883e-02,  7.38166928e-01,  5.12346983e-01, -2.72584170e-01,\n",
       "        -6.63657248e-01,  1.87264934e-01, -4.50977445e-01, -1.32281137e+00,\n",
       "        -1.29504049e+00, -8.92624378e-01,  1.54635578e-01, -3.52525771e-01,\n",
       "        -1.24235189e+00,  9.90511179e-01, -4.78106976e-01], dtype=float32),\n",
       " array([-0.26096073,  0.57087946,  0.8992069 , -0.2339702 ,  0.551773  ,\n",
       "         0.01035582, -0.58777255,  0.31789958,  1.4296008 ,  0.08392739,\n",
       "         0.26144862, -0.1198171 , -0.43905896,  0.5817575 ,  1.0995386 ,\n",
       "         1.7914512 , -0.5497238 ,  0.40135378, -0.6390183 ,  0.13980499,\n",
       "         2.991774  ,  0.42572036, -0.9527472 , -0.14807358,  1.253173  ,\n",
       "        -0.06300036,  0.8545555 ,  2.868196  , -0.7262802 , -1.186656  ,\n",
       "        -1.132635  ,  0.48062587, -0.6217867 ,  0.22750708,  0.26877582,\n",
       "         1.2829232 ,  1.1313837 , -0.10365263,  0.5470558 , -1.1777952 ,\n",
       "        -0.93010193, -2.5029705 ,  1.1180083 , -2.5410748 , -0.20830144,\n",
       "         0.16848609,  0.675056  ,  0.7975079 ,  1.513367  , -0.53805804,\n",
       "        -0.46651837, -0.2614556 , -0.68503386,  0.10545465,  1.0657271 ,\n",
       "        -0.26634413,  0.16652194, -0.53224444,  1.3853606 ,  0.17256498,\n",
       "         0.77548283,  0.97517455, -1.6139197 ,  0.85572165,  1.2891148 ,\n",
       "         0.7705738 , -0.802914  ,  0.61371547, -1.677513  , -0.46998563,\n",
       "        -2.6851761 , -0.2547156 , -1.9305876 , -0.55747384, -0.6972971 ,\n",
       "        -0.67144465,  1.3959885 ,  1.7912985 ,  0.10107479, -1.0788428 ,\n",
       "         0.19219875, -0.3661278 , -1.0592762 ,  0.73493594, -0.03429377,\n",
       "        -1.0085067 ,  1.2296294 ,  0.8196774 , -0.26743907, -1.4404459 ,\n",
       "         0.8422249 , -0.19889094, -0.6341677 , -0.6028118 ,  1.4457465 ,\n",
       "        -0.11525768,  0.282149  ,  0.25674343, -0.06417327, -1.452422  ,\n",
       "         0.36224276, -0.08328709,  1.346224  , -0.99322325,  1.0505252 ,\n",
       "        -0.7218345 ,  0.6450654 , -0.80815405, -0.1723296 , -0.00872867,\n",
       "        -0.55776006,  1.6544979 ,  0.84245265, -0.86436814, -0.7667912 ,\n",
       "        -0.28563195,  1.1694349 , -0.5749854 , -0.835873  , -1.2111722 ,\n",
       "         2.1930146 , -1.7826668 ,  0.3700545 , -1.359185  , -0.36067647,\n",
       "         0.27471226, -0.5014984 ,  1.7884406 ,  0.52003384, -1.6933069 ,\n",
       "         0.36800316,  1.8404677 , -0.4012006 ,  1.9218509 ,  0.470719  ,\n",
       "        -1.0590192 , -0.09651141,  0.8000634 ,  0.24097218, -0.26408982,\n",
       "        -0.26359963,  0.7393737 , -0.70533985, -1.2540817 , -1.3981357 ,\n",
       "        -0.7452982 , -0.34335646, -0.4745281 , -0.8077745 ,  1.397454  ,\n",
       "        -0.5080552 ], dtype=float32),\n",
       " array([-0.51211625,  0.27766103,  0.43683794, -0.02071243,  0.59654576,\n",
       "         0.17951031, -0.99336064,  0.3721609 ,  1.3436316 ,  0.33003694,\n",
       "         0.32869208, -0.18885301, -0.5330684 ,  0.6667781 ,  1.059346  ,\n",
       "         1.408702  , -0.49033168,  0.7355776 , -0.80984926,  0.32096693,\n",
       "         3.0647242 ,  0.3736365 , -0.8900376 , -0.19406621,  0.8701571 ,\n",
       "        -0.01201289,  0.7094592 ,  3.0560586 , -0.03099855, -1.1547754 ,\n",
       "        -1.3950758 ,  0.17504132, -0.30933574,  0.37201312,  0.27498785,\n",
       "         1.1664238 ,  0.796695  ,  0.11585528,  0.32368505, -1.2464094 ,\n",
       "        -1.0297091 , -2.2435737 ,  1.1034534 , -2.3235016 , -0.3170979 ,\n",
       "         0.20733255,  0.7294718 ,  0.42959538,  1.3154422 , -1.0029639 ,\n",
       "        -0.28884545, -0.21269546, -0.59113085,  0.37260333,  0.82133895,\n",
       "        -0.19106588,  0.13158993, -0.59192187,  1.2542372 , -0.05845876,\n",
       "         0.7073392 ,  1.0501689 , -1.5938742 ,  0.73547465,  1.0141121 ,\n",
       "         0.69617796, -0.71611106,  0.31657946, -1.4763422 , -0.60416913,\n",
       "        -2.6341598 , -0.1452214 , -1.9598114 , -0.47423455, -0.48225647,\n",
       "        -0.62557936,  1.4754211 ,  1.8275603 , -0.02400599, -0.9494975 ,\n",
       "         0.3123277 , -0.48707095, -0.957952  ,  0.5799613 , -0.00697412,\n",
       "        -0.6735154 ,  1.5030516 ,  0.6647067 , -0.4273523 , -1.4334041 ,\n",
       "         0.96438575, -0.4173032 , -0.40681887, -0.52099544,  1.5964025 ,\n",
       "        -0.05508192,  0.15876988,  0.15149735,  0.26908362, -1.489629  ,\n",
       "         0.17603755,  0.11853047,  1.2876757 , -0.81790584,  1.1528758 ,\n",
       "        -0.9875695 ,  0.76142895, -1.0255505 , -0.37517333, -0.5193045 ,\n",
       "        -0.8223045 ,  1.5591382 ,  0.7002037 , -0.61075765, -0.37378612,\n",
       "        -0.42671192,  0.8180845 , -0.30094314, -0.89264816, -1.1065801 ,\n",
       "         2.2065928 , -1.351672  ,  0.5788167 , -1.2311387 , -0.08346418,\n",
       "         0.14977813, -0.4161755 ,  1.9560169 ,  0.7708296 , -1.7294807 ,\n",
       "         0.652592  ,  2.1829228 , -0.8186707 ,  2.065026  ,  0.34300572,\n",
       "        -1.0034789 , -0.03810649,  1.1381316 ,  0.17899083, -0.5458621 ,\n",
       "        -0.37632665,  0.71445376, -0.6862041 , -1.2079991 , -1.3643363 ,\n",
       "        -1.0130847 , -0.1710444 , -0.69186807, -0.4176583 ,  1.8038974 ,\n",
       "        -0.11852343], dtype=float32),\n",
       " array([-0.16761439,  0.44260463,  0.5542887 ,  0.16166528,  0.357889  ,\n",
       "         0.22614163, -0.5347201 ,  0.2512874 ,  1.0926951 ,  0.16976507,\n",
       "         0.05268525, -0.6859697 , -0.6081065 ,  0.78370535,  0.87464696,\n",
       "         1.3775272 , -0.58466786,  0.8670295 , -0.77961546,  0.31984958,\n",
       "         3.009111  ,  0.6713929 , -1.2796855 , -0.3183756 ,  0.8911645 ,\n",
       "        -0.03037054,  0.6726139 ,  2.8058505 , -0.16432296, -1.1190162 ,\n",
       "        -1.3078396 ,  0.08043232, -0.40050903,  0.4289971 ,  0.22254649,\n",
       "         1.2088062 ,  0.82873935, -0.22675262,  0.30556747, -1.202189  ,\n",
       "        -1.3350052 , -2.103221  ,  0.85588944, -2.479523  , -0.0704359 ,\n",
       "         0.14241645,  0.89156204,  0.5183256 ,  1.0483768 , -1.3452368 ,\n",
       "        -0.3882587 , -0.44356975, -0.17955072,  0.569772  ,  0.5256686 ,\n",
       "        -0.03974919,  0.07622415, -0.5120352 ,  1.6232381 ,  0.09349561,\n",
       "         0.7425236 ,  1.0210108 , -1.6734627 ,  0.42227998,  1.0534055 ,\n",
       "         0.79567623, -0.60662705,  0.7038703 , -1.2460339 , -0.49344814,\n",
       "        -2.718898  ,  0.12685564, -2.1259418 , -0.5676999 , -0.3561892 ,\n",
       "        -0.26899084,  1.456353  ,  1.9176607 ,  0.29375148, -0.75942063,\n",
       "         0.09450966, -0.82555234, -1.0252601 ,  0.37736413,  0.0584607 ,\n",
       "        -0.52453995,  1.3275889 ,  0.8169589 , -0.5001628 , -1.4996934 ,\n",
       "         1.0873696 , -0.24283051, -0.2774851 , -0.69543767,  1.4804363 ,\n",
       "        -0.1329905 ,  0.29249898,  0.15565073,  0.15004297, -1.5518429 ,\n",
       "        -0.30229503, -0.11051949,  1.6249629 , -0.75759876,  0.71496636,\n",
       "        -1.2111453 ,  1.183616  , -0.55637205, -0.54487413, -0.5626432 ,\n",
       "        -0.5560876 ,  1.489427  ,  0.9337342 , -0.77590674, -0.38311997,\n",
       "        -0.10325614,  0.80307734,  0.05976725, -1.1860042 , -0.98171353,\n",
       "         2.444077  , -1.0231577 ,  0.5115845 , -1.151958  , -0.12338304,\n",
       "         0.18913728, -0.09620794,  1.8016671 ,  1.0716596 , -1.648743  ,\n",
       "         0.46000692,  1.9978946 , -1.0817398 ,  2.2485273 ,  0.2870201 ,\n",
       "        -0.6443793 ,  0.09707741,  1.2138629 , -0.13945474, -0.85255337,\n",
       "         0.01363999,  0.9935173 , -0.458069  , -1.3336033 , -1.2189126 ,\n",
       "        -1.1098342 , -0.02444233, -0.58950245, -0.4378897 ,  2.0630329 ,\n",
       "        -0.13174866], dtype=float32),\n",
       " array([-0.16761439,  0.44260463,  0.5542887 ,  0.16166528,  0.357889  ,\n",
       "         0.22614163, -0.5347201 ,  0.2512874 ,  1.0926951 ,  0.16976507,\n",
       "         0.05268525, -0.6859697 , -0.6081065 ,  0.78370535,  0.87464696,\n",
       "         1.3775272 , -0.58466786,  0.8670295 , -0.77961546,  0.31984958,\n",
       "         3.009111  ,  0.6713929 , -1.2796855 , -0.3183756 ,  0.8911645 ,\n",
       "        -0.03037054,  0.6726139 ,  2.8058505 , -0.16432296, -1.1190162 ,\n",
       "        -1.3078396 ,  0.08043232, -0.40050903,  0.4289971 ,  0.22254649,\n",
       "         1.2088062 ,  0.82873935, -0.22675262,  0.30556747, -1.202189  ,\n",
       "        -1.3350052 , -2.103221  ,  0.85588944, -2.479523  , -0.0704359 ,\n",
       "         0.14241645,  0.89156204,  0.5183256 ,  1.0483768 , -1.3452368 ,\n",
       "        -0.3882587 , -0.44356975, -0.17955072,  0.569772  ,  0.5256686 ,\n",
       "        -0.03974919,  0.07622415, -0.5120352 ,  1.6232381 ,  0.09349561,\n",
       "         0.7425236 ,  1.0210108 , -1.6734627 ,  0.42227998,  1.0534055 ,\n",
       "         0.79567623, -0.60662705,  0.7038703 , -1.2460339 , -0.49344814,\n",
       "        -2.718898  ,  0.12685564, -2.1259418 , -0.5676999 , -0.3561892 ,\n",
       "        -0.26899084,  1.456353  ,  1.9176607 ,  0.29375148, -0.75942063,\n",
       "         0.09450966, -0.82555234, -1.0252601 ,  0.37736413,  0.0584607 ,\n",
       "        -0.52453995,  1.3275889 ,  0.8169589 , -0.5001628 , -1.4996934 ,\n",
       "         1.0873696 , -0.24283051, -0.2774851 , -0.69543767,  1.4804363 ,\n",
       "        -0.1329905 ,  0.29249898,  0.15565073,  0.15004297, -1.5518429 ,\n",
       "        -0.30229503, -0.11051949,  1.6249629 , -0.75759876,  0.71496636,\n",
       "        -1.2111453 ,  1.183616  , -0.55637205, -0.54487413, -0.5626432 ,\n",
       "        -0.5560876 ,  1.489427  ,  0.9337342 , -0.77590674, -0.38311997,\n",
       "        -0.10325614,  0.80307734,  0.05976725, -1.1860042 , -0.98171353,\n",
       "         2.444077  , -1.0231577 ,  0.5115845 , -1.151958  , -0.12338304,\n",
       "         0.18913728, -0.09620794,  1.8016671 ,  1.0716596 , -1.648743  ,\n",
       "         0.46000692,  1.9978946 , -1.0817398 ,  2.2485273 ,  0.2870201 ,\n",
       "        -0.6443793 ,  0.09707741,  1.2138629 , -0.13945474, -0.85255337,\n",
       "         0.01363999,  0.9935173 , -0.458069  , -1.3336033 , -1.2189126 ,\n",
       "        -1.1098342 , -0.02444233, -0.58950245, -0.4378897 ,  2.0630329 ,\n",
       "        -0.13174866], dtype=float32),\n",
       " array([-0.16761439,  0.44260463,  0.5542887 ,  0.16166528,  0.357889  ,\n",
       "         0.22614163, -0.5347201 ,  0.2512874 ,  1.0926951 ,  0.16976507,\n",
       "         0.05268525, -0.6859697 , -0.6081065 ,  0.78370535,  0.87464696,\n",
       "         1.3775272 , -0.58466786,  0.8670295 , -0.77961546,  0.31984958,\n",
       "         3.009111  ,  0.6713929 , -1.2796855 , -0.3183756 ,  0.8911645 ,\n",
       "        -0.03037054,  0.6726139 ,  2.8058505 , -0.16432296, -1.1190162 ,\n",
       "        -1.3078396 ,  0.08043232, -0.40050903,  0.4289971 ,  0.22254649,\n",
       "         1.2088062 ,  0.82873935, -0.22675262,  0.30556747, -1.202189  ,\n",
       "        -1.3350052 , -2.103221  ,  0.85588944, -2.479523  , -0.0704359 ,\n",
       "         0.14241645,  0.89156204,  0.5183256 ,  1.0483768 , -1.3452368 ,\n",
       "        -0.3882587 , -0.44356975, -0.17955072,  0.569772  ,  0.5256686 ,\n",
       "        -0.03974919,  0.07622415, -0.5120352 ,  1.6232381 ,  0.09349561,\n",
       "         0.7425236 ,  1.0210108 , -1.6734627 ,  0.42227998,  1.0534055 ,\n",
       "         0.79567623, -0.60662705,  0.7038703 , -1.2460339 , -0.49344814,\n",
       "        -2.718898  ,  0.12685564, -2.1259418 , -0.5676999 , -0.3561892 ,\n",
       "        -0.26899084,  1.456353  ,  1.9176607 ,  0.29375148, -0.75942063,\n",
       "         0.09450966, -0.82555234, -1.0252601 ,  0.37736413,  0.0584607 ,\n",
       "        -0.52453995,  1.3275889 ,  0.8169589 , -0.5001628 , -1.4996934 ,\n",
       "         1.0873696 , -0.24283051, -0.2774851 , -0.69543767,  1.4804363 ,\n",
       "        -0.1329905 ,  0.29249898,  0.15565073,  0.15004297, -1.5518429 ,\n",
       "        -0.30229503, -0.11051949,  1.6249629 , -0.75759876,  0.71496636,\n",
       "        -1.2111453 ,  1.183616  , -0.55637205, -0.54487413, -0.5626432 ,\n",
       "        -0.5560876 ,  1.489427  ,  0.9337342 , -0.77590674, -0.38311997,\n",
       "        -0.10325614,  0.80307734,  0.05976725, -1.1860042 , -0.98171353,\n",
       "         2.444077  , -1.0231577 ,  0.5115845 , -1.151958  , -0.12338304,\n",
       "         0.18913728, -0.09620794,  1.8016671 ,  1.0716596 , -1.648743  ,\n",
       "         0.46000692,  1.9978946 , -1.0817398 ,  2.2485273 ,  0.2870201 ,\n",
       "        -0.6443793 ,  0.09707741,  1.2138629 , -0.13945474, -0.85255337,\n",
       "         0.01363999,  0.9935173 , -0.458069  , -1.3336033 , -1.2189126 ,\n",
       "        -1.1098342 , -0.02444233, -0.58950245, -0.4378897 ,  2.0630329 ,\n",
       "        -0.13174866], dtype=float32),\n",
       " array([-1.99329898e-01,  6.26563311e-01,  6.86453462e-01,  1.30736232e-01,\n",
       "         3.31666291e-01,  4.41834658e-01, -6.60171270e-01,  4.84648585e-01,\n",
       "         1.39923787e+00,  1.14064224e-01, -4.52149898e-01, -6.49244428e-01,\n",
       "        -7.57070780e-01,  1.22712338e+00,  1.17120492e+00,  1.38754857e+00,\n",
       "        -4.67638284e-01,  1.00050509e+00, -1.24730992e+00,  2.34643757e-01,\n",
       "         3.08086276e+00,  5.70435107e-01, -1.31740892e+00, -2.45161161e-01,\n",
       "         1.19909632e+00,  7.17016384e-02,  5.87379456e-01,  2.71076012e+00,\n",
       "        -2.63457417e-01, -1.29364145e+00, -1.77363896e+00,  5.12309074e-01,\n",
       "        -3.63985717e-01,  3.47147465e-01,  2.26561382e-01,  1.26637244e+00,\n",
       "         4.70042348e-01, -4.56265509e-01,  5.18644415e-02, -1.62835872e+00,\n",
       "        -1.31397915e+00, -1.92287016e+00,  3.51342022e-01, -2.55456972e+00,\n",
       "         1.05408840e-01,  1.11910239e-01,  8.61482024e-01,  4.67624158e-01,\n",
       "         1.33301556e+00, -1.51224744e+00, -5.00020325e-01, -5.28870225e-01,\n",
       "        -2.42394164e-01,  6.83683574e-01,  1.49427772e-01,  1.55808389e-01,\n",
       "         3.43375921e-01, -5.78245282e-01,  1.96868229e+00, -2.74487376e-01,\n",
       "         9.65069890e-01,  1.06747437e+00, -1.86698699e+00,  2.08562583e-01,\n",
       "         1.34343350e+00,  7.89764106e-01, -4.15307224e-01,  6.35110199e-01,\n",
       "        -1.02085578e+00, -4.03380454e-01, -2.79113269e+00,  2.83989429e-01,\n",
       "        -1.98279393e+00, -3.81547600e-01, -3.39869000e-02,  6.39765244e-03,\n",
       "         1.50545096e+00,  1.70777333e+00,  3.27379107e-01, -9.52699840e-01,\n",
       "        -1.30532878e-02, -8.29974890e-01, -7.41708577e-01,  5.39538980e-01,\n",
       "         1.54821962e-01, -4.07762617e-01,  1.30689240e+00,  7.78992057e-01,\n",
       "        -3.86146337e-01, -1.49194109e+00,  4.81983185e-01, -1.19312569e-01,\n",
       "        -3.76215070e-01, -6.07538760e-01,  1.58032894e+00, -1.06782891e-01,\n",
       "         3.54675442e-01,  4.00345504e-01,  3.26013982e-01, -1.88116419e+00,\n",
       "        -6.60989106e-01, -2.66270638e-01,  1.58275151e+00, -9.94085252e-01,\n",
       "         3.95774305e-01, -1.10436380e+00,  1.02270639e+00, -3.46886963e-01,\n",
       "        -4.36665475e-01, -1.88032210e-01, -3.41430604e-01,  1.87838888e+00,\n",
       "         1.24849272e+00, -8.83805394e-01, -4.34050292e-01, -1.89726889e-01,\n",
       "         8.97842526e-01,  1.67370096e-01, -1.24485707e+00, -7.42123246e-01,\n",
       "         2.19662166e+00, -1.22603977e+00,  2.91148245e-01, -9.08872783e-01,\n",
       "         2.38485020e-02, -1.76967919e-01,  2.64430583e-01,  1.76502335e+00,\n",
       "         9.85582650e-01, -1.47346771e+00,  7.63176620e-01,  1.68758392e+00,\n",
       "        -1.23121643e+00,  2.51725793e+00,  2.53632963e-01, -4.93214577e-01,\n",
       "         3.74068588e-01,  1.12857425e+00, -1.35035858e-01, -6.44059896e-01,\n",
       "        -4.90840001e-04,  7.85326600e-01, -4.31148827e-01, -1.02528834e+00,\n",
       "        -1.20166290e+00, -1.21529269e+00, -3.90243046e-02, -1.14949882e+00,\n",
       "        -4.43480432e-01,  2.26205420e+00,  2.04761297e-01], dtype=float32),\n",
       " array([-1.99329898e-01,  6.26563311e-01,  6.86453462e-01,  1.30736232e-01,\n",
       "         3.31666291e-01,  4.41834658e-01, -6.60171270e-01,  4.84648585e-01,\n",
       "         1.39923787e+00,  1.14064224e-01, -4.52149898e-01, -6.49244428e-01,\n",
       "        -7.57070780e-01,  1.22712338e+00,  1.17120492e+00,  1.38754857e+00,\n",
       "        -4.67638284e-01,  1.00050509e+00, -1.24730992e+00,  2.34643757e-01,\n",
       "         3.08086276e+00,  5.70435107e-01, -1.31740892e+00, -2.45161161e-01,\n",
       "         1.19909632e+00,  7.17016384e-02,  5.87379456e-01,  2.71076012e+00,\n",
       "        -2.63457417e-01, -1.29364145e+00, -1.77363896e+00,  5.12309074e-01,\n",
       "        -3.63985717e-01,  3.47147465e-01,  2.26561382e-01,  1.26637244e+00,\n",
       "         4.70042348e-01, -4.56265509e-01,  5.18644415e-02, -1.62835872e+00,\n",
       "        -1.31397915e+00, -1.92287016e+00,  3.51342022e-01, -2.55456972e+00,\n",
       "         1.05408840e-01,  1.11910239e-01,  8.61482024e-01,  4.67624158e-01,\n",
       "         1.33301556e+00, -1.51224744e+00, -5.00020325e-01, -5.28870225e-01,\n",
       "        -2.42394164e-01,  6.83683574e-01,  1.49427772e-01,  1.55808389e-01,\n",
       "         3.43375921e-01, -5.78245282e-01,  1.96868229e+00, -2.74487376e-01,\n",
       "         9.65069890e-01,  1.06747437e+00, -1.86698699e+00,  2.08562583e-01,\n",
       "         1.34343350e+00,  7.89764106e-01, -4.15307224e-01,  6.35110199e-01,\n",
       "        -1.02085578e+00, -4.03380454e-01, -2.79113269e+00,  2.83989429e-01,\n",
       "        -1.98279393e+00, -3.81547600e-01, -3.39869000e-02,  6.39765244e-03,\n",
       "         1.50545096e+00,  1.70777333e+00,  3.27379107e-01, -9.52699840e-01,\n",
       "        -1.30532878e-02, -8.29974890e-01, -7.41708577e-01,  5.39538980e-01,\n",
       "         1.54821962e-01, -4.07762617e-01,  1.30689240e+00,  7.78992057e-01,\n",
       "        -3.86146337e-01, -1.49194109e+00,  4.81983185e-01, -1.19312569e-01,\n",
       "        -3.76215070e-01, -6.07538760e-01,  1.58032894e+00, -1.06782891e-01,\n",
       "         3.54675442e-01,  4.00345504e-01,  3.26013982e-01, -1.88116419e+00,\n",
       "        -6.60989106e-01, -2.66270638e-01,  1.58275151e+00, -9.94085252e-01,\n",
       "         3.95774305e-01, -1.10436380e+00,  1.02270639e+00, -3.46886963e-01,\n",
       "        -4.36665475e-01, -1.88032210e-01, -3.41430604e-01,  1.87838888e+00,\n",
       "         1.24849272e+00, -8.83805394e-01, -4.34050292e-01, -1.89726889e-01,\n",
       "         8.97842526e-01,  1.67370096e-01, -1.24485707e+00, -7.42123246e-01,\n",
       "         2.19662166e+00, -1.22603977e+00,  2.91148245e-01, -9.08872783e-01,\n",
       "         2.38485020e-02, -1.76967919e-01,  2.64430583e-01,  1.76502335e+00,\n",
       "         9.85582650e-01, -1.47346771e+00,  7.63176620e-01,  1.68758392e+00,\n",
       "        -1.23121643e+00,  2.51725793e+00,  2.53632963e-01, -4.93214577e-01,\n",
       "         3.74068588e-01,  1.12857425e+00, -1.35035858e-01, -6.44059896e-01,\n",
       "        -4.90840001e-04,  7.85326600e-01, -4.31148827e-01, -1.02528834e+00,\n",
       "        -1.20166290e+00, -1.21529269e+00, -3.90243046e-02, -1.14949882e+00,\n",
       "        -4.43480432e-01,  2.26205420e+00,  2.04761297e-01], dtype=float32),\n",
       " array([-0.11537889, -0.21335961,  0.8983907 , -0.02822549,  0.19822551,\n",
       "         0.42859873, -0.53641963,  0.15331507,  0.69835466,  0.13267238,\n",
       "        -0.3300883 , -0.483329  , -0.3443474 ,  1.2297381 ,  1.250096  ,\n",
       "         1.0022086 , -0.63740474,  0.91600156, -0.87257904,  0.5406743 ,\n",
       "         3.035209  ,  0.49788553, -1.0004697 , -0.34232923,  1.183115  ,\n",
       "         0.155238  ,  0.74590564,  2.8144555 , -0.5905301 , -1.2975268 ,\n",
       "        -1.9058682 ,  0.25019765, -0.47624877,  0.16690676,  0.2886258 ,\n",
       "         1.4791181 ,  0.6489704 , -0.32852066,  0.140241  , -1.3830196 ,\n",
       "        -1.1688377 , -1.8319774 ,  0.79116297, -2.4685183 ,  0.2728199 ,\n",
       "        -0.0594247 ,  0.85605866,  0.46524644,  1.524604  , -1.2221441 ,\n",
       "        -0.59864146, -0.64289045, -0.31290784,  0.62276727,  0.18022153,\n",
       "         0.03560544,  0.05898004, -0.8600229 ,  1.7000982 , -0.4994386 ,\n",
       "         1.2866924 ,  1.1659173 , -1.810126  ,  0.19697656,  1.3152183 ,\n",
       "         0.6690107 , -0.61050916,  0.5491287 , -0.92942333, -0.264902  ,\n",
       "        -2.7980034 ,  0.17089404, -1.9273287 , -0.7685786 ,  0.26065493,\n",
       "         0.2331421 ,  0.99281365,  1.653338  ,  0.2648951 , -1.0658278 ,\n",
       "         0.07304496, -0.8384113 , -0.58113354,  0.35659766, -0.27771688,\n",
       "        -0.22202852,  1.3604257 ,  0.9548496 , -0.6858343 , -1.4220294 ,\n",
       "         0.45697814, -0.09303974, -0.10819644, -0.24006101,  1.1946486 ,\n",
       "        -0.21734124, -0.31128854,  0.5323122 , -0.03701755, -2.042383  ,\n",
       "        -0.37069738, -0.5169932 ,  1.465759  , -1.14249   ,  0.326393  ,\n",
       "        -1.0257612 ,  1.2168125 , -0.4825203 , -0.51313823, -0.5355757 ,\n",
       "        -0.5418435 ,  1.8339012 ,  1.1445057 , -1.1108735 , -0.2631749 ,\n",
       "        -0.06525213,  0.7341021 ,  0.3502822 , -1.2317228 , -0.7634484 ,\n",
       "         2.1177547 , -1.2409981 ,  0.3404589 , -0.79036534, -0.14275934,\n",
       "         0.2825278 ,  0.6347672 ,  1.7468083 ,  1.2831461 , -1.1105446 ,\n",
       "         0.47553986,  1.855594  , -1.423321  ,  2.345016  ,  0.00433536,\n",
       "        -0.28255394, -0.0353638 ,  1.4413003 ,  0.11507493, -0.6027993 ,\n",
       "        -0.34377405,  0.9236227 , -0.6605079 , -1.1601468 , -0.86191994,\n",
       "        -1.2322674 , -0.1254401 , -0.8269661 , -0.5918425 ,  2.4629915 ,\n",
       "         0.4081496 ], dtype=float32),\n",
       " array([-0.11537889, -0.21335961,  0.8983907 , -0.02822549,  0.19822551,\n",
       "         0.42859873, -0.53641963,  0.15331507,  0.69835466,  0.13267238,\n",
       "        -0.3300883 , -0.483329  , -0.3443474 ,  1.2297381 ,  1.250096  ,\n",
       "         1.0022086 , -0.63740474,  0.91600156, -0.87257904,  0.5406743 ,\n",
       "         3.035209  ,  0.49788553, -1.0004697 , -0.34232923,  1.183115  ,\n",
       "         0.155238  ,  0.74590564,  2.8144555 , -0.5905301 , -1.2975268 ,\n",
       "        -1.9058682 ,  0.25019765, -0.47624877,  0.16690676,  0.2886258 ,\n",
       "         1.4791181 ,  0.6489704 , -0.32852066,  0.140241  , -1.3830196 ,\n",
       "        -1.1688377 , -1.8319774 ,  0.79116297, -2.4685183 ,  0.2728199 ,\n",
       "        -0.0594247 ,  0.85605866,  0.46524644,  1.524604  , -1.2221441 ,\n",
       "        -0.59864146, -0.64289045, -0.31290784,  0.62276727,  0.18022153,\n",
       "         0.03560544,  0.05898004, -0.8600229 ,  1.7000982 , -0.4994386 ,\n",
       "         1.2866924 ,  1.1659173 , -1.810126  ,  0.19697656,  1.3152183 ,\n",
       "         0.6690107 , -0.61050916,  0.5491287 , -0.92942333, -0.264902  ,\n",
       "        -2.7980034 ,  0.17089404, -1.9273287 , -0.7685786 ,  0.26065493,\n",
       "         0.2331421 ,  0.99281365,  1.653338  ,  0.2648951 , -1.0658278 ,\n",
       "         0.07304496, -0.8384113 , -0.58113354,  0.35659766, -0.27771688,\n",
       "        -0.22202852,  1.3604257 ,  0.9548496 , -0.6858343 , -1.4220294 ,\n",
       "         0.45697814, -0.09303974, -0.10819644, -0.24006101,  1.1946486 ,\n",
       "        -0.21734124, -0.31128854,  0.5323122 , -0.03701755, -2.042383  ,\n",
       "        -0.37069738, -0.5169932 ,  1.465759  , -1.14249   ,  0.326393  ,\n",
       "        -1.0257612 ,  1.2168125 , -0.4825203 , -0.51313823, -0.5355757 ,\n",
       "        -0.5418435 ,  1.8339012 ,  1.1445057 , -1.1108735 , -0.2631749 ,\n",
       "        -0.06525213,  0.7341021 ,  0.3502822 , -1.2317228 , -0.7634484 ,\n",
       "         2.1177547 , -1.2409981 ,  0.3404589 , -0.79036534, -0.14275934,\n",
       "         0.2825278 ,  0.6347672 ,  1.7468083 ,  1.2831461 , -1.1105446 ,\n",
       "         0.47553986,  1.855594  , -1.423321  ,  2.345016  ,  0.00433536,\n",
       "        -0.28255394, -0.0353638 ,  1.4413003 ,  0.11507493, -0.6027993 ,\n",
       "        -0.34377405,  0.9236227 , -0.6605079 , -1.1601468 , -0.86191994,\n",
       "        -1.2322674 , -0.1254401 , -0.8269661 , -0.5918425 ,  2.4629915 ,\n",
       "         0.4081496 ], dtype=float32),\n",
       " array([-0.19143017, -0.28153655,  0.6962821 ,  0.4298509 , -0.1256955 ,\n",
       "         0.2605577 , -0.41051775,  0.3118602 ,  0.9099011 ,  0.522711  ,\n",
       "        -0.31082553, -0.45591563, -0.14186665,  1.3207204 ,  1.3397484 ,\n",
       "         1.2386775 , -0.7899089 ,  0.7182856 , -0.9564281 ,  0.6572705 ,\n",
       "         2.9809322 ,  0.23807974, -1.1707076 , -0.1916371 ,  0.93363816,\n",
       "         0.11446871,  0.9371939 ,  3.3345325 , -0.39302728, -1.0590777 ,\n",
       "        -1.8228219 ,  0.3066155 , -0.6474708 ,  0.3078338 ,  0.36955735,\n",
       "         1.2987549 ,  0.69515115, -0.14366794,  0.32764634, -1.3321722 ,\n",
       "        -1.1329441 , -1.8317614 ,  0.85704815, -2.1324086 ,  0.40070558,\n",
       "        -0.22313003,  0.47384197,  0.86748177,  1.3752407 , -1.5511681 ,\n",
       "        -0.66239864, -0.26107296, -0.27685913,  0.9042874 ,  0.4281518 ,\n",
       "         0.54751146,  0.22676453, -0.9313821 ,  1.7609136 , -0.5576344 ,\n",
       "         1.0717094 ,  1.5425862 , -2.160664  ,  0.21833666,  1.3069783 ,\n",
       "         0.69594413, -0.7116271 ,  0.227545  , -0.7797288 , -0.20516987,\n",
       "        -2.745889  ,  0.0489267 , -2.029077  , -0.6870752 ,  0.42960772,\n",
       "         0.38285932,  1.2452853 ,  2.1198792 ,  0.42288202, -1.198916  ,\n",
       "        -0.33537126, -0.81453377, -0.5369513 ,  0.04950142, -0.34489515,\n",
       "        -0.34223256,  1.6091079 ,  0.9311845 , -0.565048  , -1.1593097 ,\n",
       "         0.40713343,  0.5095852 , -0.30883586,  0.09665632,  1.3907855 ,\n",
       "        -0.3573487 , -0.2537262 ,  0.5686545 , -0.05824457, -2.132966  ,\n",
       "        -0.30706537, -0.30878988,  1.5441163 , -1.0372065 ,  0.5913456 ,\n",
       "        -0.88965845,  1.2634428 , -0.6697228 , -0.8723964 , -0.31840503,\n",
       "        -0.1229022 ,  1.8450998 ,  0.8061924 , -1.0001463 , -0.21624486,\n",
       "        -0.03061307,  0.7381097 ,  0.33628407, -1.2274708 , -0.3821268 ,\n",
       "         1.7796485 , -1.0441501 ,  0.54397076, -0.798553  , -0.4191364 ,\n",
       "         0.2182507 ,  0.9031773 ,  1.3585582 ,  1.0885814 , -1.3946167 ,\n",
       "         0.39387217,  2.0368717 , -1.5433717 ,  2.360957  ,  0.16382198,\n",
       "        -0.411765  , -0.1404274 ,  1.5500481 ,  0.12908237, -0.6765962 ,\n",
       "        -0.5445312 ,  1.1129202 , -0.6188434 , -0.80383855, -0.9939259 ,\n",
       "        -1.5731063 , -0.27362815, -0.96339345, -1.0072572 ,  2.2496665 ,\n",
       "        -0.00860938], dtype=float32),\n",
       " array([-0.19143017, -0.28153655,  0.6962821 ,  0.4298509 , -0.1256955 ,\n",
       "         0.2605577 , -0.41051775,  0.3118602 ,  0.9099011 ,  0.522711  ,\n",
       "        -0.31082553, -0.45591563, -0.14186665,  1.3207204 ,  1.3397484 ,\n",
       "         1.2386775 , -0.7899089 ,  0.7182856 , -0.9564281 ,  0.6572705 ,\n",
       "         2.9809322 ,  0.23807974, -1.1707076 , -0.1916371 ,  0.93363816,\n",
       "         0.11446871,  0.9371939 ,  3.3345325 , -0.39302728, -1.0590777 ,\n",
       "        -1.8228219 ,  0.3066155 , -0.6474708 ,  0.3078338 ,  0.36955735,\n",
       "         1.2987549 ,  0.69515115, -0.14366794,  0.32764634, -1.3321722 ,\n",
       "        -1.1329441 , -1.8317614 ,  0.85704815, -2.1324086 ,  0.40070558,\n",
       "        -0.22313003,  0.47384197,  0.86748177,  1.3752407 , -1.5511681 ,\n",
       "        -0.66239864, -0.26107296, -0.27685913,  0.9042874 ,  0.4281518 ,\n",
       "         0.54751146,  0.22676453, -0.9313821 ,  1.7609136 , -0.5576344 ,\n",
       "         1.0717094 ,  1.5425862 , -2.160664  ,  0.21833666,  1.3069783 ,\n",
       "         0.69594413, -0.7116271 ,  0.227545  , -0.7797288 , -0.20516987,\n",
       "        -2.745889  ,  0.0489267 , -2.029077  , -0.6870752 ,  0.42960772,\n",
       "         0.38285932,  1.2452853 ,  2.1198792 ,  0.42288202, -1.198916  ,\n",
       "        -0.33537126, -0.81453377, -0.5369513 ,  0.04950142, -0.34489515,\n",
       "        -0.34223256,  1.6091079 ,  0.9311845 , -0.565048  , -1.1593097 ,\n",
       "         0.40713343,  0.5095852 , -0.30883586,  0.09665632,  1.3907855 ,\n",
       "        -0.3573487 , -0.2537262 ,  0.5686545 , -0.05824457, -2.132966  ,\n",
       "        -0.30706537, -0.30878988,  1.5441163 , -1.0372065 ,  0.5913456 ,\n",
       "        -0.88965845,  1.2634428 , -0.6697228 , -0.8723964 , -0.31840503,\n",
       "        -0.1229022 ,  1.8450998 ,  0.8061924 , -1.0001463 , -0.21624486,\n",
       "        -0.03061307,  0.7381097 ,  0.33628407, -1.2274708 , -0.3821268 ,\n",
       "         1.7796485 , -1.0441501 ,  0.54397076, -0.798553  , -0.4191364 ,\n",
       "         0.2182507 ,  0.9031773 ,  1.3585582 ,  1.0885814 , -1.3946167 ,\n",
       "         0.39387217,  2.0368717 , -1.5433717 ,  2.360957  ,  0.16382198,\n",
       "        -0.411765  , -0.1404274 ,  1.5500481 ,  0.12908237, -0.6765962 ,\n",
       "        -0.5445312 ,  1.1129202 , -0.6188434 , -0.80383855, -0.9939259 ,\n",
       "        -1.5731063 , -0.27362815, -0.96339345, -1.0072572 ,  2.2496665 ,\n",
       "        -0.00860938], dtype=float32),\n",
       " array([-7.4451286e-01, -6.2157327e-01,  9.7956115e-01,  2.8119102e-01,\n",
       "        -3.5828203e-01,  5.7886750e-01, -6.7390180e-01,  3.1889278e-01,\n",
       "         7.7458805e-01,  7.2124332e-01,  1.3162042e-03, -2.4562910e-01,\n",
       "         4.0819701e-02,  1.3243365e+00,  1.4907359e+00,  1.5564362e+00,\n",
       "        -7.3227274e-01,  5.2112556e-01, -1.2857690e+00,  9.7998315e-01,\n",
       "         3.1470706e+00, -2.0514669e-01, -1.1601740e+00,  1.3924536e-02,\n",
       "         7.0355707e-01,  1.6846345e-01,  7.7094918e-01,  3.4999559e+00,\n",
       "        -5.3639632e-01, -1.1040283e+00, -1.9103324e+00,  3.8100517e-01,\n",
       "        -4.3927675e-01,  2.2778368e-01,  1.9984595e-01,  1.2358501e+00,\n",
       "         1.0391065e+00, -3.4272122e-01,  1.4457387e-01, -1.4890635e+00,\n",
       "        -1.3850622e+00, -1.6513988e+00,  1.0347877e+00, -1.4203633e+00,\n",
       "         4.9159589e-01,  2.6070541e-02,  6.8116820e-01,  5.4860860e-01,\n",
       "         1.2944261e+00, -1.6505609e+00, -4.0230829e-01, -1.3485952e-01,\n",
       "        -2.0481646e-01,  8.5721177e-01,  4.9490997e-01,  6.9411266e-01,\n",
       "         4.6841919e-01, -7.6503819e-01,  1.6872329e+00, -4.7352642e-01,\n",
       "         1.0203140e+00,  1.0975485e+00, -2.2215581e+00,  2.7185589e-01,\n",
       "         1.4142973e+00,  8.7712878e-01, -7.6039976e-01,  6.3610345e-01,\n",
       "        -2.5468325e-02, -4.7928255e-02, -2.8748367e+00,  8.3260626e-01,\n",
       "        -2.0115049e+00, -6.0909235e-01,  4.2785612e-01,  2.1728937e-01,\n",
       "         1.2050165e+00,  2.1555631e+00,  2.8719297e-01, -1.0834059e+00,\n",
       "        -1.9833136e-01, -9.7331542e-01, -8.1411254e-01,  3.6370182e-01,\n",
       "        -4.9571723e-01, -1.2341821e-01,  1.2971712e+00,  8.8472211e-01,\n",
       "        -6.6283429e-01, -1.1533599e+00,  4.4550627e-01,  4.0651798e-01,\n",
       "        -5.3472751e-01, -8.9462236e-02,  1.8248782e+00, -4.3865666e-01,\n",
       "        -2.3400341e-01,  1.5558198e-01, -3.4558466e-01, -2.3509574e+00,\n",
       "        -2.0705970e-01, -3.1536481e-01,  1.3031676e+00, -7.5454849e-01,\n",
       "         1.1012121e+00, -8.3392614e-01,  1.1858653e+00, -5.8512330e-01,\n",
       "        -1.0425539e+00, -5.4637390e-01, -3.7515470e-01,  2.1882343e+00,\n",
       "         8.2003808e-01, -1.2116975e+00, -1.7926853e-02,  2.6532745e-01,\n",
       "         5.8444750e-01,  6.9555062e-01, -1.0451347e+00, -2.1432628e-01,\n",
       "         2.1213436e+00, -1.5088407e+00,  4.8444238e-01, -8.3212453e-01,\n",
       "        -3.5043868e-01,  2.7522084e-01,  9.0340352e-01,  1.4424403e+00,\n",
       "         9.1013765e-01, -1.1125250e+00,  5.1127297e-01,  2.1143568e+00,\n",
       "        -1.5452787e+00,  2.5466554e+00,  9.2476182e-02, -5.3147095e-01,\n",
       "         2.5566503e-01,  1.5157470e+00,  2.0691951e-01, -7.3872608e-01,\n",
       "        -6.5307647e-01,  9.9720383e-01, -2.0192035e-01, -8.0667973e-01,\n",
       "        -8.8080657e-01, -1.4578085e+00, -8.3718401e-01, -1.1294316e+00,\n",
       "        -9.8566866e-01,  2.5250752e+00,  2.8453028e-01], dtype=float32),\n",
       " array([-0.41936964, -0.7646872 ,  0.8898323 ,  0.2351558 , -0.44161168,\n",
       "         0.41654938, -0.69604003,  0.50704753,  0.70230156,  0.8294302 ,\n",
       "        -0.08131348, -0.27997673,  0.11349279,  1.1465452 ,  1.4418117 ,\n",
       "         1.2120291 , -0.9216032 ,  0.39758867, -1.3847729 ,  0.9986905 ,\n",
       "         3.0302432 , -0.12607235, -1.1459246 ,  0.15992932,  0.3874715 ,\n",
       "        -0.15903021,  0.46185464,  3.448637  , -0.22657363, -1.0022508 ,\n",
       "        -1.9092227 ,  0.3143014 , -0.30931237,  0.22971173,  0.26099342,\n",
       "         0.8776464 ,  0.912121  , -0.21073176, -0.4294304 , -1.4839311 ,\n",
       "        -1.7865846 , -1.890528  ,  0.8269335 , -1.2681938 ,  0.56725746,\n",
       "        -0.22333509,  0.90428174,  0.63513064,  0.967354  , -1.6948004 ,\n",
       "        -0.29775456,  0.3504443 , -0.08942164,  0.93637717,  0.7317748 ,\n",
       "         0.7356765 ,  0.32063618, -0.690673  ,  1.8550907 , -0.13046676,\n",
       "         1.1735193 ,  0.9482784 , -2.489365  ,  0.18517885,  1.4193907 ,\n",
       "         0.8425842 , -0.65333664,  0.75168467, -0.7142673 , -0.2041905 ,\n",
       "        -2.6298873 ,  0.51582146, -1.8555856 , -0.5475108 ,  0.1872246 ,\n",
       "         0.16400637,  1.0444546 ,  2.1363275 ,  0.12834093, -1.0366126 ,\n",
       "        -0.38609105, -1.1577687 , -1.3707918 ,  0.47351497, -0.73993295,\n",
       "        -0.25554517,  0.73266596,  0.76950693, -0.34944734, -1.354481  ,\n",
       "         0.40303192,  0.6338156 , -0.34461752,  0.03977942,  2.1854575 ,\n",
       "        -0.7785278 , -0.5001002 ,  0.57698435, -0.27094355, -2.3589947 ,\n",
       "        -0.4455925 , -0.3138418 ,  1.2210242 , -0.8524072 ,  0.8510073 ,\n",
       "        -0.9369747 ,  1.1696904 , -0.42768538, -1.0105747 , -0.29968157,\n",
       "        -0.39931622,  2.2949824 ,  0.91561997, -1.4197776 ,  0.23776558,\n",
       "         0.37137833,  0.4706479 ,  0.9022885 , -1.1484133 , -0.27567357,\n",
       "         2.2783813 , -1.3492752 ,  0.70939535, -0.95149577, -0.19578671,\n",
       "         0.10196394,  0.7811131 ,  1.2937338 ,  0.8371408 , -1.2411933 ,\n",
       "         0.5707442 ,  1.9960871 , -1.6489999 ,  2.6618512 ,  0.23437947,\n",
       "        -0.23823382,  0.43309963,  0.96067834,  0.40264696, -0.54910916,\n",
       "        -0.43281958,  1.2220521 ,  0.0273513 , -0.7467001 , -1.214326  ,\n",
       "        -1.4793977 , -0.8190208 , -0.75864595, -1.03234   ,  2.8119872 ,\n",
       "         0.23116636], dtype=float32),\n",
       " array([-0.24631792, -0.75600845,  0.806473  ,  0.03607655, -0.32253915,\n",
       "         0.51277405, -0.576055  ,  0.2971697 ,  0.7803224 ,  1.2204654 ,\n",
       "        -0.12093883, -0.34112522,  0.17005867,  0.66103345,  1.4596827 ,\n",
       "         1.719673  , -0.6750403 ,  0.0057159 , -1.4484817 ,  0.76311916,\n",
       "         3.14723   , -0.43201333, -1.2099817 ,  0.19403172,  0.8375879 ,\n",
       "        -0.26861635,  1.0081546 ,  3.0573812 , -0.7286919 , -1.0371891 ,\n",
       "        -1.6888975 ,  0.2980526 ,  0.3056975 ,  0.277457  , -0.1097308 ,\n",
       "         0.45022622,  0.98069644, -0.35759258, -0.48234928, -1.6001452 ,\n",
       "        -2.0134351 , -1.8071846 ,  0.88442206, -1.253964  ,  0.55283505,\n",
       "        -0.6194049 ,  1.2229139 ,  0.6583851 ,  0.8554833 , -1.8688961 ,\n",
       "        -0.04240057,  0.27859777, -0.15805861,  0.96076095,  0.86689   ,\n",
       "         0.5109062 , -0.00509975, -0.7473537 ,  1.4760698 , -0.80451787,\n",
       "         0.764754  ,  0.6795865 , -2.2179234 ,  0.22680868,  1.7241362 ,\n",
       "         0.7504747 , -0.30652955,  0.6070979 , -0.73558694, -0.2548633 ,\n",
       "        -2.7648802 ,  0.4952596 , -1.660469  , -0.4519587 , -0.04456861,\n",
       "        -0.09447768,  0.98836064,  2.0952368 ,  0.08925209, -1.0589671 ,\n",
       "        -0.4770396 , -1.2537552 , -1.2424716 ,  0.4847518 , -1.0321215 ,\n",
       "        -0.30367908,  0.6463758 ,  1.1105803 , -0.12502393, -1.4656211 ,\n",
       "         0.4152219 ,  0.53627264, -0.27209085,  0.1329297 ,  2.0938494 ,\n",
       "        -0.6417735 , -0.35250548,  0.5881706 , -0.37244934, -2.2090507 ,\n",
       "        -0.5748622 , -0.48022568,  1.5185397 , -1.1498337 ,  1.1439779 ,\n",
       "        -0.8807071 ,  1.2024993 , -0.5660636 , -0.8809209 ,  0.01116307,\n",
       "        -0.12086855,  2.2941563 ,  0.7176108 , -1.5165904 , -0.06025048,\n",
       "         0.32408765,  0.45142463,  0.6574022 , -1.40322   , -0.2293617 ,\n",
       "         2.2134428 , -1.6491741 ,  0.5606313 , -0.85070676,  0.18577234,\n",
       "         0.03356678,  0.8838125 ,  1.4393063 ,  0.5706523 , -1.2393088 ,\n",
       "         1.0277605 ,  1.8749268 , -1.6662823 ,  2.3200116 ,  0.53670514,\n",
       "        -0.24735527,  0.6233212 ,  0.9181687 ,  0.5894141 , -0.47144127,\n",
       "        -0.613569  ,  1.4817405 ,  0.01326876, -0.5609766 , -1.0304387 ,\n",
       "        -1.624335  , -1.222305  , -0.95198125, -1.1009574 ,  2.4844024 ,\n",
       "         0.38375178], dtype=float32),\n",
       " array([-0.24631792, -0.75600845,  0.806473  ,  0.03607655, -0.32253915,\n",
       "         0.51277405, -0.576055  ,  0.2971697 ,  0.7803224 ,  1.2204654 ,\n",
       "        -0.12093883, -0.34112522,  0.17005867,  0.66103345,  1.4596827 ,\n",
       "         1.719673  , -0.6750403 ,  0.0057159 , -1.4484817 ,  0.76311916,\n",
       "         3.14723   , -0.43201333, -1.2099817 ,  0.19403172,  0.8375879 ,\n",
       "        -0.26861635,  1.0081546 ,  3.0573812 , -0.7286919 , -1.0371891 ,\n",
       "        -1.6888975 ,  0.2980526 ,  0.3056975 ,  0.277457  , -0.1097308 ,\n",
       "         0.45022622,  0.98069644, -0.35759258, -0.48234928, -1.6001452 ,\n",
       "        -2.0134351 , -1.8071846 ,  0.88442206, -1.253964  ,  0.55283505,\n",
       "        -0.6194049 ,  1.2229139 ,  0.6583851 ,  0.8554833 , -1.8688961 ,\n",
       "        -0.04240057,  0.27859777, -0.15805861,  0.96076095,  0.86689   ,\n",
       "         0.5109062 , -0.00509975, -0.7473537 ,  1.4760698 , -0.80451787,\n",
       "         0.764754  ,  0.6795865 , -2.2179234 ,  0.22680868,  1.7241362 ,\n",
       "         0.7504747 , -0.30652955,  0.6070979 , -0.73558694, -0.2548633 ,\n",
       "        -2.7648802 ,  0.4952596 , -1.660469  , -0.4519587 , -0.04456861,\n",
       "        -0.09447768,  0.98836064,  2.0952368 ,  0.08925209, -1.0589671 ,\n",
       "        -0.4770396 , -1.2537552 , -1.2424716 ,  0.4847518 , -1.0321215 ,\n",
       "        -0.30367908,  0.6463758 ,  1.1105803 , -0.12502393, -1.4656211 ,\n",
       "         0.4152219 ,  0.53627264, -0.27209085,  0.1329297 ,  2.0938494 ,\n",
       "        -0.6417735 , -0.35250548,  0.5881706 , -0.37244934, -2.2090507 ,\n",
       "        -0.5748622 , -0.48022568,  1.5185397 , -1.1498337 ,  1.1439779 ,\n",
       "        -0.8807071 ,  1.2024993 , -0.5660636 , -0.8809209 ,  0.01116307,\n",
       "        -0.12086855,  2.2941563 ,  0.7176108 , -1.5165904 , -0.06025048,\n",
       "         0.32408765,  0.45142463,  0.6574022 , -1.40322   , -0.2293617 ,\n",
       "         2.2134428 , -1.6491741 ,  0.5606313 , -0.85070676,  0.18577234,\n",
       "         0.03356678,  0.8838125 ,  1.4393063 ,  0.5706523 , -1.2393088 ,\n",
       "         1.0277605 ,  1.8749268 , -1.6662823 ,  2.3200116 ,  0.53670514,\n",
       "        -0.24735527,  0.6233212 ,  0.9181687 ,  0.5894141 , -0.47144127,\n",
       "        -0.613569  ,  1.4817405 ,  0.01326876, -0.5609766 , -1.0304387 ,\n",
       "        -1.624335  , -1.222305  , -0.95198125, -1.1009574 ,  2.4844024 ,\n",
       "         0.38375178], dtype=float32),\n",
       " array([-0.24631792, -0.75600845,  0.806473  ,  0.03607655, -0.32253915,\n",
       "         0.51277405, -0.576055  ,  0.2971697 ,  0.7803224 ,  1.2204654 ,\n",
       "        -0.12093883, -0.34112522,  0.17005867,  0.66103345,  1.4596827 ,\n",
       "         1.719673  , -0.6750403 ,  0.0057159 , -1.4484817 ,  0.76311916,\n",
       "         3.14723   , -0.43201333, -1.2099817 ,  0.19403172,  0.8375879 ,\n",
       "        -0.26861635,  1.0081546 ,  3.0573812 , -0.7286919 , -1.0371891 ,\n",
       "        -1.6888975 ,  0.2980526 ,  0.3056975 ,  0.277457  , -0.1097308 ,\n",
       "         0.45022622,  0.98069644, -0.35759258, -0.48234928, -1.6001452 ,\n",
       "        -2.0134351 , -1.8071846 ,  0.88442206, -1.253964  ,  0.55283505,\n",
       "        -0.6194049 ,  1.2229139 ,  0.6583851 ,  0.8554833 , -1.8688961 ,\n",
       "        -0.04240057,  0.27859777, -0.15805861,  0.96076095,  0.86689   ,\n",
       "         0.5109062 , -0.00509975, -0.7473537 ,  1.4760698 , -0.80451787,\n",
       "         0.764754  ,  0.6795865 , -2.2179234 ,  0.22680868,  1.7241362 ,\n",
       "         0.7504747 , -0.30652955,  0.6070979 , -0.73558694, -0.2548633 ,\n",
       "        -2.7648802 ,  0.4952596 , -1.660469  , -0.4519587 , -0.04456861,\n",
       "        -0.09447768,  0.98836064,  2.0952368 ,  0.08925209, -1.0589671 ,\n",
       "        -0.4770396 , -1.2537552 , -1.2424716 ,  0.4847518 , -1.0321215 ,\n",
       "        -0.30367908,  0.6463758 ,  1.1105803 , -0.12502393, -1.4656211 ,\n",
       "         0.4152219 ,  0.53627264, -0.27209085,  0.1329297 ,  2.0938494 ,\n",
       "        -0.6417735 , -0.35250548,  0.5881706 , -0.37244934, -2.2090507 ,\n",
       "        -0.5748622 , -0.48022568,  1.5185397 , -1.1498337 ,  1.1439779 ,\n",
       "        -0.8807071 ,  1.2024993 , -0.5660636 , -0.8809209 ,  0.01116307,\n",
       "        -0.12086855,  2.2941563 ,  0.7176108 , -1.5165904 , -0.06025048,\n",
       "         0.32408765,  0.45142463,  0.6574022 , -1.40322   , -0.2293617 ,\n",
       "         2.2134428 , -1.6491741 ,  0.5606313 , -0.85070676,  0.18577234,\n",
       "         0.03356678,  0.8838125 ,  1.4393063 ,  0.5706523 , -1.2393088 ,\n",
       "         1.0277605 ,  1.8749268 , -1.6662823 ,  2.3200116 ,  0.53670514,\n",
       "        -0.24735527,  0.6233212 ,  0.9181687 ,  0.5894141 , -0.47144127,\n",
       "        -0.613569  ,  1.4817405 ,  0.01326876, -0.5609766 , -1.0304387 ,\n",
       "        -1.624335  , -1.222305  , -0.95198125, -1.1009574 ,  2.4844024 ,\n",
       "         0.38375178], dtype=float32),\n",
       " array([-0.24631792, -0.75600845,  0.806473  ,  0.03607655, -0.32253915,\n",
       "         0.51277405, -0.576055  ,  0.2971697 ,  0.7803224 ,  1.2204654 ,\n",
       "        -0.12093883, -0.34112522,  0.17005867,  0.66103345,  1.4596827 ,\n",
       "         1.719673  , -0.6750403 ,  0.0057159 , -1.4484817 ,  0.76311916,\n",
       "         3.14723   , -0.43201333, -1.2099817 ,  0.19403172,  0.8375879 ,\n",
       "        -0.26861635,  1.0081546 ,  3.0573812 , -0.7286919 , -1.0371891 ,\n",
       "        -1.6888975 ,  0.2980526 ,  0.3056975 ,  0.277457  , -0.1097308 ,\n",
       "         0.45022622,  0.98069644, -0.35759258, -0.48234928, -1.6001452 ,\n",
       "        -2.0134351 , -1.8071846 ,  0.88442206, -1.253964  ,  0.55283505,\n",
       "        -0.6194049 ,  1.2229139 ,  0.6583851 ,  0.8554833 , -1.8688961 ,\n",
       "        -0.04240057,  0.27859777, -0.15805861,  0.96076095,  0.86689   ,\n",
       "         0.5109062 , -0.00509975, -0.7473537 ,  1.4760698 , -0.80451787,\n",
       "         0.764754  ,  0.6795865 , -2.2179234 ,  0.22680868,  1.7241362 ,\n",
       "         0.7504747 , -0.30652955,  0.6070979 , -0.73558694, -0.2548633 ,\n",
       "        -2.7648802 ,  0.4952596 , -1.660469  , -0.4519587 , -0.04456861,\n",
       "        -0.09447768,  0.98836064,  2.0952368 ,  0.08925209, -1.0589671 ,\n",
       "        -0.4770396 , -1.2537552 , -1.2424716 ,  0.4847518 , -1.0321215 ,\n",
       "        -0.30367908,  0.6463758 ,  1.1105803 , -0.12502393, -1.4656211 ,\n",
       "         0.4152219 ,  0.53627264, -0.27209085,  0.1329297 ,  2.0938494 ,\n",
       "        -0.6417735 , -0.35250548,  0.5881706 , -0.37244934, -2.2090507 ,\n",
       "        -0.5748622 , -0.48022568,  1.5185397 , -1.1498337 ,  1.1439779 ,\n",
       "        -0.8807071 ,  1.2024993 , -0.5660636 , -0.8809209 ,  0.01116307,\n",
       "        -0.12086855,  2.2941563 ,  0.7176108 , -1.5165904 , -0.06025048,\n",
       "         0.32408765,  0.45142463,  0.6574022 , -1.40322   , -0.2293617 ,\n",
       "         2.2134428 , -1.6491741 ,  0.5606313 , -0.85070676,  0.18577234,\n",
       "         0.03356678,  0.8838125 ,  1.4393063 ,  0.5706523 , -1.2393088 ,\n",
       "         1.0277605 ,  1.8749268 , -1.6662823 ,  2.3200116 ,  0.53670514,\n",
       "        -0.24735527,  0.6233212 ,  0.9181687 ,  0.5894141 , -0.47144127,\n",
       "        -0.613569  ,  1.4817405 ,  0.01326876, -0.5609766 , -1.0304387 ,\n",
       "        -1.624335  , -1.222305  , -0.95198125, -1.1009574 ,  2.4844024 ,\n",
       "         0.38375178], dtype=float32),\n",
       " array([-0.24631792, -0.75600845,  0.806473  ,  0.03607655, -0.32253915,\n",
       "         0.51277405, -0.576055  ,  0.2971697 ,  0.7803224 ,  1.2204654 ,\n",
       "        -0.12093883, -0.34112522,  0.17005867,  0.66103345,  1.4596827 ,\n",
       "         1.719673  , -0.6750403 ,  0.0057159 , -1.4484817 ,  0.76311916,\n",
       "         3.14723   , -0.43201333, -1.2099817 ,  0.19403172,  0.8375879 ,\n",
       "        -0.26861635,  1.0081546 ,  3.0573812 , -0.7286919 , -1.0371891 ,\n",
       "        -1.6888975 ,  0.2980526 ,  0.3056975 ,  0.277457  , -0.1097308 ,\n",
       "         0.45022622,  0.98069644, -0.35759258, -0.48234928, -1.6001452 ,\n",
       "        -2.0134351 , -1.8071846 ,  0.88442206, -1.253964  ,  0.55283505,\n",
       "        -0.6194049 ,  1.2229139 ,  0.6583851 ,  0.8554833 , -1.8688961 ,\n",
       "        -0.04240057,  0.27859777, -0.15805861,  0.96076095,  0.86689   ,\n",
       "         0.5109062 , -0.00509975, -0.7473537 ,  1.4760698 , -0.80451787,\n",
       "         0.764754  ,  0.6795865 , -2.2179234 ,  0.22680868,  1.7241362 ,\n",
       "         0.7504747 , -0.30652955,  0.6070979 , -0.73558694, -0.2548633 ,\n",
       "        -2.7648802 ,  0.4952596 , -1.660469  , -0.4519587 , -0.04456861,\n",
       "        -0.09447768,  0.98836064,  2.0952368 ,  0.08925209, -1.0589671 ,\n",
       "        -0.4770396 , -1.2537552 , -1.2424716 ,  0.4847518 , -1.0321215 ,\n",
       "        -0.30367908,  0.6463758 ,  1.1105803 , -0.12502393, -1.4656211 ,\n",
       "         0.4152219 ,  0.53627264, -0.27209085,  0.1329297 ,  2.0938494 ,\n",
       "        -0.6417735 , -0.35250548,  0.5881706 , -0.37244934, -2.2090507 ,\n",
       "        -0.5748622 , -0.48022568,  1.5185397 , -1.1498337 ,  1.1439779 ,\n",
       "        -0.8807071 ,  1.2024993 , -0.5660636 , -0.8809209 ,  0.01116307,\n",
       "        -0.12086855,  2.2941563 ,  0.7176108 , -1.5165904 , -0.06025048,\n",
       "         0.32408765,  0.45142463,  0.6574022 , -1.40322   , -0.2293617 ,\n",
       "         2.2134428 , -1.6491741 ,  0.5606313 , -0.85070676,  0.18577234,\n",
       "         0.03356678,  0.8838125 ,  1.4393063 ,  0.5706523 , -1.2393088 ,\n",
       "         1.0277605 ,  1.8749268 , -1.6662823 ,  2.3200116 ,  0.53670514,\n",
       "        -0.24735527,  0.6233212 ,  0.9181687 ,  0.5894141 , -0.47144127,\n",
       "        -0.613569  ,  1.4817405 ,  0.01326876, -0.5609766 , -1.0304387 ,\n",
       "        -1.624335  , -1.222305  , -0.95198125, -1.1009574 ,  2.4844024 ,\n",
       "         0.38375178], dtype=float32),\n",
       " array([-0.24631792, -0.75600845,  0.806473  ,  0.03607655, -0.32253915,\n",
       "         0.51277405, -0.576055  ,  0.2971697 ,  0.7803224 ,  1.2204654 ,\n",
       "        -0.12093883, -0.34112522,  0.17005867,  0.66103345,  1.4596827 ,\n",
       "         1.719673  , -0.6750403 ,  0.0057159 , -1.4484817 ,  0.76311916,\n",
       "         3.14723   , -0.43201333, -1.2099817 ,  0.19403172,  0.8375879 ,\n",
       "        -0.26861635,  1.0081546 ,  3.0573812 , -0.7286919 , -1.0371891 ,\n",
       "        -1.6888975 ,  0.2980526 ,  0.3056975 ,  0.277457  , -0.1097308 ,\n",
       "         0.45022622,  0.98069644, -0.35759258, -0.48234928, -1.6001452 ,\n",
       "        -2.0134351 , -1.8071846 ,  0.88442206, -1.253964  ,  0.55283505,\n",
       "        -0.6194049 ,  1.2229139 ,  0.6583851 ,  0.8554833 , -1.8688961 ,\n",
       "        -0.04240057,  0.27859777, -0.15805861,  0.96076095,  0.86689   ,\n",
       "         0.5109062 , -0.00509975, -0.7473537 ,  1.4760698 , -0.80451787,\n",
       "         0.764754  ,  0.6795865 , -2.2179234 ,  0.22680868,  1.7241362 ,\n",
       "         0.7504747 , -0.30652955,  0.6070979 , -0.73558694, -0.2548633 ,\n",
       "        -2.7648802 ,  0.4952596 , -1.660469  , -0.4519587 , -0.04456861,\n",
       "        -0.09447768,  0.98836064,  2.0952368 ,  0.08925209, -1.0589671 ,\n",
       "        -0.4770396 , -1.2537552 , -1.2424716 ,  0.4847518 , -1.0321215 ,\n",
       "        -0.30367908,  0.6463758 ,  1.1105803 , -0.12502393, -1.4656211 ,\n",
       "         0.4152219 ,  0.53627264, -0.27209085,  0.1329297 ,  2.0938494 ,\n",
       "        -0.6417735 , -0.35250548,  0.5881706 , -0.37244934, -2.2090507 ,\n",
       "        -0.5748622 , -0.48022568,  1.5185397 , -1.1498337 ,  1.1439779 ,\n",
       "        -0.8807071 ,  1.2024993 , -0.5660636 , -0.8809209 ,  0.01116307,\n",
       "        -0.12086855,  2.2941563 ,  0.7176108 , -1.5165904 , -0.06025048,\n",
       "         0.32408765,  0.45142463,  0.6574022 , -1.40322   , -0.2293617 ,\n",
       "         2.2134428 , -1.6491741 ,  0.5606313 , -0.85070676,  0.18577234,\n",
       "         0.03356678,  0.8838125 ,  1.4393063 ,  0.5706523 , -1.2393088 ,\n",
       "         1.0277605 ,  1.8749268 , -1.6662823 ,  2.3200116 ,  0.53670514,\n",
       "        -0.24735527,  0.6233212 ,  0.9181687 ,  0.5894141 , -0.47144127,\n",
       "        -0.613569  ,  1.4817405 ,  0.01326876, -0.5609766 , -1.0304387 ,\n",
       "        -1.624335  , -1.222305  , -0.95198125, -1.1009574 ,  2.4844024 ,\n",
       "         0.38375178], dtype=float32),\n",
       " array([-0.24631792, -0.75600845,  0.806473  ,  0.03607655, -0.32253915,\n",
       "         0.51277405, -0.576055  ,  0.2971697 ,  0.7803224 ,  1.2204654 ,\n",
       "        -0.12093883, -0.34112522,  0.17005867,  0.66103345,  1.4596827 ,\n",
       "         1.719673  , -0.6750403 ,  0.0057159 , -1.4484817 ,  0.76311916,\n",
       "         3.14723   , -0.43201333, -1.2099817 ,  0.19403172,  0.8375879 ,\n",
       "        -0.26861635,  1.0081546 ,  3.0573812 , -0.7286919 , -1.0371891 ,\n",
       "        -1.6888975 ,  0.2980526 ,  0.3056975 ,  0.277457  , -0.1097308 ,\n",
       "         0.45022622,  0.98069644, -0.35759258, -0.48234928, -1.6001452 ,\n",
       "        -2.0134351 , -1.8071846 ,  0.88442206, -1.253964  ,  0.55283505,\n",
       "        -0.6194049 ,  1.2229139 ,  0.6583851 ,  0.8554833 , -1.8688961 ,\n",
       "        -0.04240057,  0.27859777, -0.15805861,  0.96076095,  0.86689   ,\n",
       "         0.5109062 , -0.00509975, -0.7473537 ,  1.4760698 , -0.80451787,\n",
       "         0.764754  ,  0.6795865 , -2.2179234 ,  0.22680868,  1.7241362 ,\n",
       "         0.7504747 , -0.30652955,  0.6070979 , -0.73558694, -0.2548633 ,\n",
       "        -2.7648802 ,  0.4952596 , -1.660469  , -0.4519587 , -0.04456861,\n",
       "        -0.09447768,  0.98836064,  2.0952368 ,  0.08925209, -1.0589671 ,\n",
       "        -0.4770396 , -1.2537552 , -1.2424716 ,  0.4847518 , -1.0321215 ,\n",
       "        -0.30367908,  0.6463758 ,  1.1105803 , -0.12502393, -1.4656211 ,\n",
       "         0.4152219 ,  0.53627264, -0.27209085,  0.1329297 ,  2.0938494 ,\n",
       "        -0.6417735 , -0.35250548,  0.5881706 , -0.37244934, -2.2090507 ,\n",
       "        -0.5748622 , -0.48022568,  1.5185397 , -1.1498337 ,  1.1439779 ,\n",
       "        -0.8807071 ,  1.2024993 , -0.5660636 , -0.8809209 ,  0.01116307,\n",
       "        -0.12086855,  2.2941563 ,  0.7176108 , -1.5165904 , -0.06025048,\n",
       "         0.32408765,  0.45142463,  0.6574022 , -1.40322   , -0.2293617 ,\n",
       "         2.2134428 , -1.6491741 ,  0.5606313 , -0.85070676,  0.18577234,\n",
       "         0.03356678,  0.8838125 ,  1.4393063 ,  0.5706523 , -1.2393088 ,\n",
       "         1.0277605 ,  1.8749268 , -1.6662823 ,  2.3200116 ,  0.53670514,\n",
       "        -0.24735527,  0.6233212 ,  0.9181687 ,  0.5894141 , -0.47144127,\n",
       "        -0.613569  ,  1.4817405 ,  0.01326876, -0.5609766 , -1.0304387 ,\n",
       "        -1.624335  , -1.222305  , -0.95198125, -1.1009574 ,  2.4844024 ,\n",
       "         0.38375178], dtype=float32),\n",
       " array([-0.19474445, -1.1573685 ,  0.6947281 , -0.2966647 , -0.21182519,\n",
       "         0.5650383 , -0.94021726, -0.2728392 ,  0.9826296 ,  1.0545014 ,\n",
       "         0.09404413, -0.29755616,  0.08006459,  0.5323308 ,  1.184938  ,\n",
       "         1.5951351 , -0.5994962 ,  0.11060374, -1.5153358 ,  0.818903  ,\n",
       "         3.0581026 , -0.34360644, -1.2428308 ,  0.06048344,  0.67982876,\n",
       "        -0.22356975,  0.7368262 ,  3.087764  , -0.4146202 , -0.8025534 ,\n",
       "        -1.5768597 ,  0.4405852 ,  0.56760776,  0.39232895, -0.38001692,\n",
       "         0.6991181 ,  1.0923147 , -0.33505777, -0.6397197 , -1.5905434 ,\n",
       "        -1.9272543 , -2.0661092 ,  1.0218258 , -0.9326438 ,  0.22894178,\n",
       "        -0.53633   ,  1.1646286 ,  0.59387684,  0.91533554, -2.1188664 ,\n",
       "        -0.2665706 , -0.08681484, -0.655494  ,  0.71248186,  1.2269564 ,\n",
       "         0.21104012, -0.04121546, -0.56805766,  1.3927935 , -0.64373934,\n",
       "         0.8085502 ,  1.0774225 , -2.109691  , -0.00706207,  1.037968  ,\n",
       "         0.7969694 , -0.5025524 ,  0.69926023, -0.85761106, -0.218295  ,\n",
       "        -2.6395693 ,  0.43722618, -1.796611  , -0.6614549 ,  0.10370097,\n",
       "        -0.1362106 ,  0.75263286,  1.8197881 , -0.1081613 , -0.9562635 ,\n",
       "        -0.57582307, -1.2824101 , -0.9973159 ,  0.32831144, -1.4737355 ,\n",
       "        -0.36550835,  0.7739287 ,  0.94614303, -0.14166778, -1.5379822 ,\n",
       "         0.38163945,  0.34711543, -0.3592558 , -0.18730146,  2.1258128 ,\n",
       "        -0.5773242 , -0.36718905,  0.7082634 , -0.42017758, -2.540512  ,\n",
       "        -0.4150853 , -0.31314465,  1.8041735 , -1.1456397 ,  0.9228498 ,\n",
       "        -1.3054229 ,  0.7974536 , -0.38607344, -0.9921395 , -0.14347789,\n",
       "        -0.05048219,  2.3551247 ,  0.7819399 , -1.4562874 ,  0.30192342,\n",
       "         0.44630593,  0.1972599 ,  0.6645756 , -1.0913332 , -0.2555356 ,\n",
       "         2.038401  , -1.3432472 ,  0.49695042, -0.8213966 ,  0.07587526,\n",
       "         0.36568117,  1.0038879 ,  1.7449601 ,  1.1225431 , -1.285377  ,\n",
       "         1.0876904 ,  1.5845033 , -1.590381  ,  2.3312552 ,  0.84126836,\n",
       "        -0.23038505,  0.807305  ,  0.8034502 ,  0.32053593, -0.42477146,\n",
       "        -0.5488312 ,  1.802815  , -0.23715548, -0.27253684, -0.91788447,\n",
       "        -1.7025955 , -1.1643902 , -0.47634894, -0.92484707,  2.6531274 ,\n",
       "         0.0922724 ], dtype=float32),\n",
       " array([-0.19474445, -1.1573685 ,  0.6947281 , -0.2966647 , -0.21182519,\n",
       "         0.5650383 , -0.94021726, -0.2728392 ,  0.9826296 ,  1.0545014 ,\n",
       "         0.09404413, -0.29755616,  0.08006459,  0.5323308 ,  1.184938  ,\n",
       "         1.5951351 , -0.5994962 ,  0.11060374, -1.5153358 ,  0.818903  ,\n",
       "         3.0581026 , -0.34360644, -1.2428308 ,  0.06048344,  0.67982876,\n",
       "        -0.22356975,  0.7368262 ,  3.087764  , -0.4146202 , -0.8025534 ,\n",
       "        -1.5768597 ,  0.4405852 ,  0.56760776,  0.39232895, -0.38001692,\n",
       "         0.6991181 ,  1.0923147 , -0.33505777, -0.6397197 , -1.5905434 ,\n",
       "        -1.9272543 , -2.0661092 ,  1.0218258 , -0.9326438 ,  0.22894178,\n",
       "        -0.53633   ,  1.1646286 ,  0.59387684,  0.91533554, -2.1188664 ,\n",
       "        -0.2665706 , -0.08681484, -0.655494  ,  0.71248186,  1.2269564 ,\n",
       "         0.21104012, -0.04121546, -0.56805766,  1.3927935 , -0.64373934,\n",
       "         0.8085502 ,  1.0774225 , -2.109691  , -0.00706207,  1.037968  ,\n",
       "         0.7969694 , -0.5025524 ,  0.69926023, -0.85761106, -0.218295  ,\n",
       "        -2.6395693 ,  0.43722618, -1.796611  , -0.6614549 ,  0.10370097,\n",
       "        -0.1362106 ,  0.75263286,  1.8197881 , -0.1081613 , -0.9562635 ,\n",
       "        -0.57582307, -1.2824101 , -0.9973159 ,  0.32831144, -1.4737355 ,\n",
       "        -0.36550835,  0.7739287 ,  0.94614303, -0.14166778, -1.5379822 ,\n",
       "         0.38163945,  0.34711543, -0.3592558 , -0.18730146,  2.1258128 ,\n",
       "        -0.5773242 , -0.36718905,  0.7082634 , -0.42017758, -2.540512  ,\n",
       "        -0.4150853 , -0.31314465,  1.8041735 , -1.1456397 ,  0.9228498 ,\n",
       "        -1.3054229 ,  0.7974536 , -0.38607344, -0.9921395 , -0.14347789,\n",
       "        -0.05048219,  2.3551247 ,  0.7819399 , -1.4562874 ,  0.30192342,\n",
       "         0.44630593,  0.1972599 ,  0.6645756 , -1.0913332 , -0.2555356 ,\n",
       "         2.038401  , -1.3432472 ,  0.49695042, -0.8213966 ,  0.07587526,\n",
       "         0.36568117,  1.0038879 ,  1.7449601 ,  1.1225431 , -1.285377  ,\n",
       "         1.0876904 ,  1.5845033 , -1.590381  ,  2.3312552 ,  0.84126836,\n",
       "        -0.23038505,  0.807305  ,  0.8034502 ,  0.32053593, -0.42477146,\n",
       "        -0.5488312 ,  1.802815  , -0.23715548, -0.27253684, -0.91788447,\n",
       "        -1.7025955 , -1.1643902 , -0.47634894, -0.92484707,  2.6531274 ,\n",
       "         0.0922724 ], dtype=float32),\n",
       " array([-0.19474445, -1.1573685 ,  0.6947281 , -0.2966647 , -0.21182519,\n",
       "         0.5650383 , -0.94021726, -0.2728392 ,  0.9826296 ,  1.0545014 ,\n",
       "         0.09404413, -0.29755616,  0.08006459,  0.5323308 ,  1.184938  ,\n",
       "         1.5951351 , -0.5994962 ,  0.11060374, -1.5153358 ,  0.818903  ,\n",
       "         3.0581026 , -0.34360644, -1.2428308 ,  0.06048344,  0.67982876,\n",
       "        -0.22356975,  0.7368262 ,  3.087764  , -0.4146202 , -0.8025534 ,\n",
       "        -1.5768597 ,  0.4405852 ,  0.56760776,  0.39232895, -0.38001692,\n",
       "         0.6991181 ,  1.0923147 , -0.33505777, -0.6397197 , -1.5905434 ,\n",
       "        -1.9272543 , -2.0661092 ,  1.0218258 , -0.9326438 ,  0.22894178,\n",
       "        -0.53633   ,  1.1646286 ,  0.59387684,  0.91533554, -2.1188664 ,\n",
       "        -0.2665706 , -0.08681484, -0.655494  ,  0.71248186,  1.2269564 ,\n",
       "         0.21104012, -0.04121546, -0.56805766,  1.3927935 , -0.64373934,\n",
       "         0.8085502 ,  1.0774225 , -2.109691  , -0.00706207,  1.037968  ,\n",
       "         0.7969694 , -0.5025524 ,  0.69926023, -0.85761106, -0.218295  ,\n",
       "        -2.6395693 ,  0.43722618, -1.796611  , -0.6614549 ,  0.10370097,\n",
       "        -0.1362106 ,  0.75263286,  1.8197881 , -0.1081613 , -0.9562635 ,\n",
       "        -0.57582307, -1.2824101 , -0.9973159 ,  0.32831144, -1.4737355 ,\n",
       "        -0.36550835,  0.7739287 ,  0.94614303, -0.14166778, -1.5379822 ,\n",
       "         0.38163945,  0.34711543, -0.3592558 , -0.18730146,  2.1258128 ,\n",
       "        -0.5773242 , -0.36718905,  0.7082634 , -0.42017758, -2.540512  ,\n",
       "        -0.4150853 , -0.31314465,  1.8041735 , -1.1456397 ,  0.9228498 ,\n",
       "        -1.3054229 ,  0.7974536 , -0.38607344, -0.9921395 , -0.14347789,\n",
       "        -0.05048219,  2.3551247 ,  0.7819399 , -1.4562874 ,  0.30192342,\n",
       "         0.44630593,  0.1972599 ,  0.6645756 , -1.0913332 , -0.2555356 ,\n",
       "         2.038401  , -1.3432472 ,  0.49695042, -0.8213966 ,  0.07587526,\n",
       "         0.36568117,  1.0038879 ,  1.7449601 ,  1.1225431 , -1.285377  ,\n",
       "         1.0876904 ,  1.5845033 , -1.590381  ,  2.3312552 ,  0.84126836,\n",
       "        -0.23038505,  0.807305  ,  0.8034502 ,  0.32053593, -0.42477146,\n",
       "        -0.5488312 ,  1.802815  , -0.23715548, -0.27253684, -0.91788447,\n",
       "        -1.7025955 , -1.1643902 , -0.47634894, -0.92484707,  2.6531274 ,\n",
       "         0.0922724 ], dtype=float32),\n",
       " array([-0.19474445, -1.1573685 ,  0.6947281 , -0.2966647 , -0.21182519,\n",
       "         0.5650383 , -0.94021726, -0.2728392 ,  0.9826296 ,  1.0545014 ,\n",
       "         0.09404413, -0.29755616,  0.08006459,  0.5323308 ,  1.184938  ,\n",
       "         1.5951351 , -0.5994962 ,  0.11060374, -1.5153358 ,  0.818903  ,\n",
       "         3.0581026 , -0.34360644, -1.2428308 ,  0.06048344,  0.67982876,\n",
       "        -0.22356975,  0.7368262 ,  3.087764  , -0.4146202 , -0.8025534 ,\n",
       "        -1.5768597 ,  0.4405852 ,  0.56760776,  0.39232895, -0.38001692,\n",
       "         0.6991181 ,  1.0923147 , -0.33505777, -0.6397197 , -1.5905434 ,\n",
       "        -1.9272543 , -2.0661092 ,  1.0218258 , -0.9326438 ,  0.22894178,\n",
       "        -0.53633   ,  1.1646286 ,  0.59387684,  0.91533554, -2.1188664 ,\n",
       "        -0.2665706 , -0.08681484, -0.655494  ,  0.71248186,  1.2269564 ,\n",
       "         0.21104012, -0.04121546, -0.56805766,  1.3927935 , -0.64373934,\n",
       "         0.8085502 ,  1.0774225 , -2.109691  , -0.00706207,  1.037968  ,\n",
       "         0.7969694 , -0.5025524 ,  0.69926023, -0.85761106, -0.218295  ,\n",
       "        -2.6395693 ,  0.43722618, -1.796611  , -0.6614549 ,  0.10370097,\n",
       "        -0.1362106 ,  0.75263286,  1.8197881 , -0.1081613 , -0.9562635 ,\n",
       "        -0.57582307, -1.2824101 , -0.9973159 ,  0.32831144, -1.4737355 ,\n",
       "        -0.36550835,  0.7739287 ,  0.94614303, -0.14166778, -1.5379822 ,\n",
       "         0.38163945,  0.34711543, -0.3592558 , -0.18730146,  2.1258128 ,\n",
       "        -0.5773242 , -0.36718905,  0.7082634 , -0.42017758, -2.540512  ,\n",
       "        -0.4150853 , -0.31314465,  1.8041735 , -1.1456397 ,  0.9228498 ,\n",
       "        -1.3054229 ,  0.7974536 , -0.38607344, -0.9921395 , -0.14347789,\n",
       "        -0.05048219,  2.3551247 ,  0.7819399 , -1.4562874 ,  0.30192342,\n",
       "         0.44630593,  0.1972599 ,  0.6645756 , -1.0913332 , -0.2555356 ,\n",
       "         2.038401  , -1.3432472 ,  0.49695042, -0.8213966 ,  0.07587526,\n",
       "         0.36568117,  1.0038879 ,  1.7449601 ,  1.1225431 , -1.285377  ,\n",
       "         1.0876904 ,  1.5845033 , -1.590381  ,  2.3312552 ,  0.84126836,\n",
       "        -0.23038505,  0.807305  ,  0.8034502 ,  0.32053593, -0.42477146,\n",
       "        -0.5488312 ,  1.802815  , -0.23715548, -0.27253684, -0.91788447,\n",
       "        -1.7025955 , -1.1643902 , -0.47634894, -0.92484707,  2.6531274 ,\n",
       "         0.0922724 ], dtype=float32),\n",
       " array([-0.19474445, -1.1573685 ,  0.6947281 , -0.2966647 , -0.21182519,\n",
       "         0.5650383 , -0.94021726, -0.2728392 ,  0.9826296 ,  1.0545014 ,\n",
       "         0.09404413, -0.29755616,  0.08006459,  0.5323308 ,  1.184938  ,\n",
       "         1.5951351 , -0.5994962 ,  0.11060374, -1.5153358 ,  0.818903  ,\n",
       "         3.0581026 , -0.34360644, -1.2428308 ,  0.06048344,  0.67982876,\n",
       "        -0.22356975,  0.7368262 ,  3.087764  , -0.4146202 , -0.8025534 ,\n",
       "        -1.5768597 ,  0.4405852 ,  0.56760776,  0.39232895, -0.38001692,\n",
       "         0.6991181 ,  1.0923147 , -0.33505777, -0.6397197 , -1.5905434 ,\n",
       "        -1.9272543 , -2.0661092 ,  1.0218258 , -0.9326438 ,  0.22894178,\n",
       "        -0.53633   ,  1.1646286 ,  0.59387684,  0.91533554, -2.1188664 ,\n",
       "        -0.2665706 , -0.08681484, -0.655494  ,  0.71248186,  1.2269564 ,\n",
       "         0.21104012, -0.04121546, -0.56805766,  1.3927935 , -0.64373934,\n",
       "         0.8085502 ,  1.0774225 , -2.109691  , -0.00706207,  1.037968  ,\n",
       "         0.7969694 , -0.5025524 ,  0.69926023, -0.85761106, -0.218295  ,\n",
       "        -2.6395693 ,  0.43722618, -1.796611  , -0.6614549 ,  0.10370097,\n",
       "        -0.1362106 ,  0.75263286,  1.8197881 , -0.1081613 , -0.9562635 ,\n",
       "        -0.57582307, -1.2824101 , -0.9973159 ,  0.32831144, -1.4737355 ,\n",
       "        -0.36550835,  0.7739287 ,  0.94614303, -0.14166778, -1.5379822 ,\n",
       "         0.38163945,  0.34711543, -0.3592558 , -0.18730146,  2.1258128 ,\n",
       "        -0.5773242 , -0.36718905,  0.7082634 , -0.42017758, -2.540512  ,\n",
       "        -0.4150853 , -0.31314465,  1.8041735 , -1.1456397 ,  0.9228498 ,\n",
       "        -1.3054229 ,  0.7974536 , -0.38607344, -0.9921395 , -0.14347789,\n",
       "        -0.05048219,  2.3551247 ,  0.7819399 , -1.4562874 ,  0.30192342,\n",
       "         0.44630593,  0.1972599 ,  0.6645756 , -1.0913332 , -0.2555356 ,\n",
       "         2.038401  , -1.3432472 ,  0.49695042, -0.8213966 ,  0.07587526,\n",
       "         0.36568117,  1.0038879 ,  1.7449601 ,  1.1225431 , -1.285377  ,\n",
       "         1.0876904 ,  1.5845033 , -1.590381  ,  2.3312552 ,  0.84126836,\n",
       "        -0.23038505,  0.807305  ,  0.8034502 ,  0.32053593, -0.42477146,\n",
       "        -0.5488312 ,  1.802815  , -0.23715548, -0.27253684, -0.91788447,\n",
       "        -1.7025955 , -1.1643902 , -0.47634894, -0.92484707,  2.6531274 ,\n",
       "         0.0922724 ], dtype=float32),\n",
       " array([-0.19474445, -1.1573685 ,  0.6947281 , -0.2966647 , -0.21182519,\n",
       "         0.5650383 , -0.94021726, -0.2728392 ,  0.9826296 ,  1.0545014 ,\n",
       "         0.09404413, -0.29755616,  0.08006459,  0.5323308 ,  1.184938  ,\n",
       "         1.5951351 , -0.5994962 ,  0.11060374, -1.5153358 ,  0.818903  ,\n",
       "         3.0581026 , -0.34360644, -1.2428308 ,  0.06048344,  0.67982876,\n",
       "        -0.22356975,  0.7368262 ,  3.087764  , -0.4146202 , -0.8025534 ,\n",
       "        -1.5768597 ,  0.4405852 ,  0.56760776,  0.39232895, -0.38001692,\n",
       "         0.6991181 ,  1.0923147 , -0.33505777, -0.6397197 , -1.5905434 ,\n",
       "        -1.9272543 , -2.0661092 ,  1.0218258 , -0.9326438 ,  0.22894178,\n",
       "        -0.53633   ,  1.1646286 ,  0.59387684,  0.91533554, -2.1188664 ,\n",
       "        -0.2665706 , -0.08681484, -0.655494  ,  0.71248186,  1.2269564 ,\n",
       "         0.21104012, -0.04121546, -0.56805766,  1.3927935 , -0.64373934,\n",
       "         0.8085502 ,  1.0774225 , -2.109691  , -0.00706207,  1.037968  ,\n",
       "         0.7969694 , -0.5025524 ,  0.69926023, -0.85761106, -0.218295  ,\n",
       "        -2.6395693 ,  0.43722618, -1.796611  , -0.6614549 ,  0.10370097,\n",
       "        -0.1362106 ,  0.75263286,  1.8197881 , -0.1081613 , -0.9562635 ,\n",
       "        -0.57582307, -1.2824101 , -0.9973159 ,  0.32831144, -1.4737355 ,\n",
       "        -0.36550835,  0.7739287 ,  0.94614303, -0.14166778, -1.5379822 ,\n",
       "         0.38163945,  0.34711543, -0.3592558 , -0.18730146,  2.1258128 ,\n",
       "        -0.5773242 , -0.36718905,  0.7082634 , -0.42017758, -2.540512  ,\n",
       "        -0.4150853 , -0.31314465,  1.8041735 , -1.1456397 ,  0.9228498 ,\n",
       "        -1.3054229 ,  0.7974536 , -0.38607344, -0.9921395 , -0.14347789,\n",
       "        -0.05048219,  2.3551247 ,  0.7819399 , -1.4562874 ,  0.30192342,\n",
       "         0.44630593,  0.1972599 ,  0.6645756 , -1.0913332 , -0.2555356 ,\n",
       "         2.038401  , -1.3432472 ,  0.49695042, -0.8213966 ,  0.07587526,\n",
       "         0.36568117,  1.0038879 ,  1.7449601 ,  1.1225431 , -1.285377  ,\n",
       "         1.0876904 ,  1.5845033 , -1.590381  ,  2.3312552 ,  0.84126836,\n",
       "        -0.23038505,  0.807305  ,  0.8034502 ,  0.32053593, -0.42477146,\n",
       "        -0.5488312 ,  1.802815  , -0.23715548, -0.27253684, -0.91788447,\n",
       "        -1.7025955 , -1.1643902 , -0.47634894, -0.92484707,  2.6531274 ,\n",
       "         0.0922724 ], dtype=float32),\n",
       " array([-0.05728455, -1.5606561 ,  0.5249312 , -0.21975195, -0.24640043,\n",
       "         0.4148811 , -1.2156897 , -0.30509433,  0.46916136,  0.9518177 ,\n",
       "        -0.01411714, -0.30405077,  0.25654963,  0.42351657,  1.2186629 ,\n",
       "         1.5166728 , -0.88464326, -0.18704371, -1.9720243 ,  1.0782114 ,\n",
       "         2.7938883 , -0.4668225 , -0.9781889 , -0.13503762,  0.79797554,\n",
       "        -0.09299251,  0.71371967,  2.4409595 , -0.08769722, -0.51437205,\n",
       "        -1.551923  ,  0.8991199 ,  0.40812108,  0.36435226, -0.11867908,\n",
       "         0.4455205 ,  1.3835831 , -0.8755779 , -0.94168454, -1.6566172 ,\n",
       "        -1.7169373 , -2.181803  ,  0.4130049 , -1.1627449 ,  0.6018971 ,\n",
       "        -0.83656   ,  0.90085685,  0.53309095,  0.7437705 , -2.069335  ,\n",
       "        -0.42939636, -0.10730795, -0.71010524,  0.7983749 ,  1.2772806 ,\n",
       "        -0.02973471, -0.23517613, -0.8749785 ,  0.7776714 , -0.568314  ,\n",
       "         0.6407998 ,  0.97896737, -1.8102438 , -0.20706227,  0.7145667 ,\n",
       "         0.723095  , -0.6081153 ,  0.62739575, -0.7642806 , -0.40986314,\n",
       "        -2.4949698 ,  0.52143866, -1.5864278 , -0.56103367,  0.0606302 ,\n",
       "         0.123228  ,  0.51767915,  1.9978588 , -0.4270691 , -0.71056676,\n",
       "        -0.5780666 , -1.5099074 , -1.2005143 ,  0.47151637, -1.2251998 ,\n",
       "        -0.33845437,  0.6249153 ,  0.7840842 , -0.6289021 , -1.5679623 ,\n",
       "         0.52451104,  0.5170244 ,  0.10480352, -0.05521162,  1.9203031 ,\n",
       "        -0.75601816, -0.60489386,  0.83752656, -0.63878864, -2.3068888 ,\n",
       "        -0.6828257 , -0.29041034,  1.4720153 , -1.190688  ,  0.84535515,\n",
       "        -1.4038748 ,  0.7210994 , -0.41080856, -0.6878351 ,  0.03839251,\n",
       "        -0.22751354,  2.2204993 ,  0.97093093, -1.3568387 ,  0.7089219 ,\n",
       "         0.1108488 ,  0.2249592 ,  0.6511723 , -1.0660143 , -0.15966813,\n",
       "         2.133955  , -1.3244107 ,  0.7593721 , -0.7661691 , -0.05128109,\n",
       "         0.11940587,  1.0333874 ,  1.7150383 ,  1.4603486 , -1.3446869 ,\n",
       "         0.93861514,  1.5167391 , -1.631882  ,  2.1766145 ,  0.9173667 ,\n",
       "        -0.38123652,  0.7453962 ,  0.69535166,  0.28800276, -0.10168004,\n",
       "        -0.3786518 ,  1.9233814 , -0.33000115,  0.02439296, -1.0411376 ,\n",
       "        -1.57692   , -1.1211562 , -0.48574126, -0.89749515,  2.599644  ,\n",
       "         0.25989744], dtype=float32),\n",
       " array([-0.54780483, -1.9508913 ,  0.5732916 , -0.11112574, -0.3547205 ,\n",
       "         0.2568662 , -1.2066141 , -0.47762266,  0.4621472 ,  0.91356635,\n",
       "        -0.42119637, -0.4238661 ,  0.37821257,  0.31878564,  0.87943697,\n",
       "         1.4196821 , -0.65244853, -0.41892558, -2.1860967 ,  1.0815747 ,\n",
       "         2.9146578 , -0.60494715, -1.1418495 , -0.32096478,  0.70545965,\n",
       "        -0.10332029,  0.8003869 ,  2.74296   , -0.17990102, -0.47006273,\n",
       "        -1.6146078 ,  0.82267964,  0.33272007,  0.01379536, -0.4778378 ,\n",
       "         0.44413072,  1.4528348 , -0.90253186, -1.1329018 , -1.2945025 ,\n",
       "        -1.4888933 , -2.3659823 ,  0.57785   , -1.4737743 ,  0.5685381 ,\n",
       "        -0.7172628 ,  0.7526701 ,  0.81742793,  0.39383546, -1.9130571 ,\n",
       "        -0.90066713, -0.25350904, -0.89669883,  0.7271979 ,  1.4517013 ,\n",
       "        -0.20985575, -0.0330003 , -1.1380253 ,  0.9002343 , -0.5083337 ,\n",
       "         0.76028943,  1.028957  , -1.4729066 , -0.5205353 ,  0.9719499 ,\n",
       "         0.23390593, -0.92649233,  0.87901264, -0.7444109 , -0.38944864,\n",
       "        -2.5920637 ,  0.54176706, -1.6653861 , -0.6661342 ,  0.18760028,\n",
       "         0.6160394 ,  0.31664148,  1.8326906 , -0.6620496 , -0.39296463,\n",
       "        -0.47716522, -1.9208597 , -1.188495  ,  0.59658426, -1.0495361 ,\n",
       "        -0.33627573,  0.6288745 ,  1.0174656 , -0.42871508, -1.6476644 ,\n",
       "         0.82180005,  0.3278419 ,  0.2621586 , -0.00706759,  2.013805  ,\n",
       "        -0.6568928 , -0.34293205,  0.63517416, -0.27974722, -2.1145732 ,\n",
       "        -0.74727005, -0.5160806 ,  1.7287164 , -1.3351475 ,  0.7651262 ,\n",
       "        -1.3923141 ,  1.1940513 , -0.10701478, -0.44978267, -0.4280356 ,\n",
       "        -0.66571   ,  2.20909   ,  1.3133513 , -1.1215135 ,  1.0398909 ,\n",
       "        -0.11838159,  0.0813987 ,  0.4291432 , -0.8974215 , -0.4245815 ,\n",
       "         1.9621131 , -1.3138072 ,  0.58062375, -0.8528163 ,  0.15398535,\n",
       "        -0.05722266,  1.1934559 ,  1.8580084 ,  1.3441486 , -1.1373112 ,\n",
       "         0.9818754 ,  1.7595494 , -1.8335783 ,  2.020719  ,  0.81625116,\n",
       "        -0.65544295,  1.1625516 ,  0.92349386,  0.14975828,  0.15156855,\n",
       "        -0.44975942,  2.090912  , -0.34717584, -0.13281165, -1.142164  ,\n",
       "        -1.8210759 , -1.1030703 , -0.3000814 , -1.0345836 ,  2.342156  ,\n",
       "         0.25432685], dtype=float32),\n",
       " array([-0.57110023, -1.9961206 ,  0.733881  , -0.14245181, -0.18998507,\n",
       "         0.22637199, -0.57979506, -0.7744332 ,  0.6417678 ,  1.0415361 ,\n",
       "         0.14355467, -0.33200127,  0.42570585,  0.4071857 ,  1.0570349 ,\n",
       "         1.3383884 , -0.8528101 , -0.6430007 , -2.1449366 ,  1.1816823 ,\n",
       "         3.0697517 , -0.730657  , -1.0490365 , -0.40462285,  0.51773113,\n",
       "        -0.14854345,  0.74419546,  2.532264  , -0.1385516 , -0.36638018,\n",
       "        -1.4559677 ,  0.56182325, -0.13563542,  0.04677008, -0.42567056,\n",
       "         0.47192565,  1.0308338 , -0.76305765, -1.0591285 , -1.192702  ,\n",
       "        -1.7044268 , -2.4231844 ,  0.59618837, -1.213818  ,  0.4030359 ,\n",
       "        -0.30310285,  0.4416101 ,  1.0616726 ,  0.33654878, -1.5335594 ,\n",
       "        -0.9483416 , -0.04172501, -0.9171283 ,  0.9543185 ,  1.3756711 ,\n",
       "        -0.12979756,  0.28061697, -1.1397263 ,  1.1583604 , -0.48418984,\n",
       "         0.66946936,  1.0952563 , -1.5354621 , -0.26670086,  0.9137501 ,\n",
       "         0.11175019, -0.6764473 ,  0.8527916 , -0.692309  , -0.5508921 ,\n",
       "        -2.843994  ,  0.46452746, -1.3119258 , -0.64639705,  0.24180333,\n",
       "         0.4923788 ,  0.39635313,  1.6620545 , -0.558128  , -0.42181557,\n",
       "        -0.17134911, -1.7651268 , -1.061696  ,  0.31182644, -0.5224209 ,\n",
       "        -0.2162023 ,  0.58694196,  0.8013674 , -0.13113905, -1.1778835 ,\n",
       "         0.64685196,  0.20228872,  0.33970714,  0.08323231,  1.8216491 ,\n",
       "        -0.7325175 , -0.20529431,  0.6243307 , -0.75423366, -1.6693738 ,\n",
       "        -0.53679866, -0.6409628 ,  1.8836867 , -1.6123806 ,  0.57984185,\n",
       "        -1.497183  ,  1.2301881 , -0.26960707, -0.85798484, -0.25273424,\n",
       "        -0.9786354 ,  1.9011071 ,  1.3821777 , -1.2202784 ,  1.1910691 ,\n",
       "        -0.2133792 , -0.08145624,  0.6643624 , -0.555054  , -0.75343657,\n",
       "         1.8032646 , -1.1027886 ,  0.4317558 , -0.763923  ,  0.01458073,\n",
       "         0.02252301,  0.87669355,  1.7340351 ,  1.4200146 , -0.9258918 ,\n",
       "         0.06751096,  1.624277  , -1.5282689 ,  1.8859854 ,  0.44973668,\n",
       "        -1.0136706 ,  1.0449423 ,  1.2001662 ,  0.04192727,  0.04971286,\n",
       "        -0.3225743 ,  2.2588894 , -0.28378484, -0.74786776, -1.3075627 ,\n",
       "        -1.9419982 , -1.20083   , -0.30825964, -1.1078343 ,  2.114838  ,\n",
       "         0.16374426], dtype=float32),\n",
       " array([-0.57110023, -1.9961206 ,  0.733881  , -0.14245181, -0.18998507,\n",
       "         0.22637199, -0.57979506, -0.7744332 ,  0.6417678 ,  1.0415361 ,\n",
       "         0.14355467, -0.33200127,  0.42570585,  0.4071857 ,  1.0570349 ,\n",
       "         1.3383884 , -0.8528101 , -0.6430007 , -2.1449366 ,  1.1816823 ,\n",
       "         3.0697517 , -0.730657  , -1.0490365 , -0.40462285,  0.51773113,\n",
       "        -0.14854345,  0.74419546,  2.532264  , -0.1385516 , -0.36638018,\n",
       "        -1.4559677 ,  0.56182325, -0.13563542,  0.04677008, -0.42567056,\n",
       "         0.47192565,  1.0308338 , -0.76305765, -1.0591285 , -1.192702  ,\n",
       "        -1.7044268 , -2.4231844 ,  0.59618837, -1.213818  ,  0.4030359 ,\n",
       "        -0.30310285,  0.4416101 ,  1.0616726 ,  0.33654878, -1.5335594 ,\n",
       "        -0.9483416 , -0.04172501, -0.9171283 ,  0.9543185 ,  1.3756711 ,\n",
       "        -0.12979756,  0.28061697, -1.1397263 ,  1.1583604 , -0.48418984,\n",
       "         0.66946936,  1.0952563 , -1.5354621 , -0.26670086,  0.9137501 ,\n",
       "         0.11175019, -0.6764473 ,  0.8527916 , -0.692309  , -0.5508921 ,\n",
       "        -2.843994  ,  0.46452746, -1.3119258 , -0.64639705,  0.24180333,\n",
       "         0.4923788 ,  0.39635313,  1.6620545 , -0.558128  , -0.42181557,\n",
       "        -0.17134911, -1.7651268 , -1.061696  ,  0.31182644, -0.5224209 ,\n",
       "        -0.2162023 ,  0.58694196,  0.8013674 , -0.13113905, -1.1778835 ,\n",
       "         0.64685196,  0.20228872,  0.33970714,  0.08323231,  1.8216491 ,\n",
       "        -0.7325175 , -0.20529431,  0.6243307 , -0.75423366, -1.6693738 ,\n",
       "        -0.53679866, -0.6409628 ,  1.8836867 , -1.6123806 ,  0.57984185,\n",
       "        -1.497183  ,  1.2301881 , -0.26960707, -0.85798484, -0.25273424,\n",
       "        -0.9786354 ,  1.9011071 ,  1.3821777 , -1.2202784 ,  1.1910691 ,\n",
       "        -0.2133792 , -0.08145624,  0.6643624 , -0.555054  , -0.75343657,\n",
       "         1.8032646 , -1.1027886 ,  0.4317558 , -0.763923  ,  0.01458073,\n",
       "         0.02252301,  0.87669355,  1.7340351 ,  1.4200146 , -0.9258918 ,\n",
       "         0.06751096,  1.624277  , -1.5282689 ,  1.8859854 ,  0.44973668,\n",
       "        -1.0136706 ,  1.0449423 ,  1.2001662 ,  0.04192727,  0.04971286,\n",
       "        -0.3225743 ,  2.2588894 , -0.28378484, -0.74786776, -1.3075627 ,\n",
       "        -1.9419982 , -1.20083   , -0.30825964, -1.1078343 ,  2.114838  ,\n",
       "         0.16374426], dtype=float32),\n",
       " array([-0.57110023, -1.9961206 ,  0.733881  , -0.14245181, -0.18998507,\n",
       "         0.22637199, -0.57979506, -0.7744332 ,  0.6417678 ,  1.0415361 ,\n",
       "         0.14355467, -0.33200127,  0.42570585,  0.4071857 ,  1.0570349 ,\n",
       "         1.3383884 , -0.8528101 , -0.6430007 , -2.1449366 ,  1.1816823 ,\n",
       "         3.0697517 , -0.730657  , -1.0490365 , -0.40462285,  0.51773113,\n",
       "        -0.14854345,  0.74419546,  2.532264  , -0.1385516 , -0.36638018,\n",
       "        -1.4559677 ,  0.56182325, -0.13563542,  0.04677008, -0.42567056,\n",
       "         0.47192565,  1.0308338 , -0.76305765, -1.0591285 , -1.192702  ,\n",
       "        -1.7044268 , -2.4231844 ,  0.59618837, -1.213818  ,  0.4030359 ,\n",
       "        -0.30310285,  0.4416101 ,  1.0616726 ,  0.33654878, -1.5335594 ,\n",
       "        -0.9483416 , -0.04172501, -0.9171283 ,  0.9543185 ,  1.3756711 ,\n",
       "        -0.12979756,  0.28061697, -1.1397263 ,  1.1583604 , -0.48418984,\n",
       "         0.66946936,  1.0952563 , -1.5354621 , -0.26670086,  0.9137501 ,\n",
       "         0.11175019, -0.6764473 ,  0.8527916 , -0.692309  , -0.5508921 ,\n",
       "        -2.843994  ,  0.46452746, -1.3119258 , -0.64639705,  0.24180333,\n",
       "         0.4923788 ,  0.39635313,  1.6620545 , -0.558128  , -0.42181557,\n",
       "        -0.17134911, -1.7651268 , -1.061696  ,  0.31182644, -0.5224209 ,\n",
       "        -0.2162023 ,  0.58694196,  0.8013674 , -0.13113905, -1.1778835 ,\n",
       "         0.64685196,  0.20228872,  0.33970714,  0.08323231,  1.8216491 ,\n",
       "        -0.7325175 , -0.20529431,  0.6243307 , -0.75423366, -1.6693738 ,\n",
       "        -0.53679866, -0.6409628 ,  1.8836867 , -1.6123806 ,  0.57984185,\n",
       "        -1.497183  ,  1.2301881 , -0.26960707, -0.85798484, -0.25273424,\n",
       "        -0.9786354 ,  1.9011071 ,  1.3821777 , -1.2202784 ,  1.1910691 ,\n",
       "        -0.2133792 , -0.08145624,  0.6643624 , -0.555054  , -0.75343657,\n",
       "         1.8032646 , -1.1027886 ,  0.4317558 , -0.763923  ,  0.01458073,\n",
       "         0.02252301,  0.87669355,  1.7340351 ,  1.4200146 , -0.9258918 ,\n",
       "         0.06751096,  1.624277  , -1.5282689 ,  1.8859854 ,  0.44973668,\n",
       "        -1.0136706 ,  1.0449423 ,  1.2001662 ,  0.04192727,  0.04971286,\n",
       "        -0.3225743 ,  2.2588894 , -0.28378484, -0.74786776, -1.3075627 ,\n",
       "        -1.9419982 , -1.20083   , -0.30825964, -1.1078343 ,  2.114838  ,\n",
       "         0.16374426], dtype=float32),\n",
       " array([-0.57110023, -1.9961206 ,  0.733881  , -0.14245181, -0.18998507,\n",
       "         0.22637199, -0.57979506, -0.7744332 ,  0.6417678 ,  1.0415361 ,\n",
       "         0.14355467, -0.33200127,  0.42570585,  0.4071857 ,  1.0570349 ,\n",
       "         1.3383884 , -0.8528101 , -0.6430007 , -2.1449366 ,  1.1816823 ,\n",
       "         3.0697517 , -0.730657  , -1.0490365 , -0.40462285,  0.51773113,\n",
       "        -0.14854345,  0.74419546,  2.532264  , -0.1385516 , -0.36638018,\n",
       "        -1.4559677 ,  0.56182325, -0.13563542,  0.04677008, -0.42567056,\n",
       "         0.47192565,  1.0308338 , -0.76305765, -1.0591285 , -1.192702  ,\n",
       "        -1.7044268 , -2.4231844 ,  0.59618837, -1.213818  ,  0.4030359 ,\n",
       "        -0.30310285,  0.4416101 ,  1.0616726 ,  0.33654878, -1.5335594 ,\n",
       "        -0.9483416 , -0.04172501, -0.9171283 ,  0.9543185 ,  1.3756711 ,\n",
       "        -0.12979756,  0.28061697, -1.1397263 ,  1.1583604 , -0.48418984,\n",
       "         0.66946936,  1.0952563 , -1.5354621 , -0.26670086,  0.9137501 ,\n",
       "         0.11175019, -0.6764473 ,  0.8527916 , -0.692309  , -0.5508921 ,\n",
       "        -2.843994  ,  0.46452746, -1.3119258 , -0.64639705,  0.24180333,\n",
       "         0.4923788 ,  0.39635313,  1.6620545 , -0.558128  , -0.42181557,\n",
       "        -0.17134911, -1.7651268 , -1.061696  ,  0.31182644, -0.5224209 ,\n",
       "        -0.2162023 ,  0.58694196,  0.8013674 , -0.13113905, -1.1778835 ,\n",
       "         0.64685196,  0.20228872,  0.33970714,  0.08323231,  1.8216491 ,\n",
       "        -0.7325175 , -0.20529431,  0.6243307 , -0.75423366, -1.6693738 ,\n",
       "        -0.53679866, -0.6409628 ,  1.8836867 , -1.6123806 ,  0.57984185,\n",
       "        -1.497183  ,  1.2301881 , -0.26960707, -0.85798484, -0.25273424,\n",
       "        -0.9786354 ,  1.9011071 ,  1.3821777 , -1.2202784 ,  1.1910691 ,\n",
       "        -0.2133792 , -0.08145624,  0.6643624 , -0.555054  , -0.75343657,\n",
       "         1.8032646 , -1.1027886 ,  0.4317558 , -0.763923  ,  0.01458073,\n",
       "         0.02252301,  0.87669355,  1.7340351 ,  1.4200146 , -0.9258918 ,\n",
       "         0.06751096,  1.624277  , -1.5282689 ,  1.8859854 ,  0.44973668,\n",
       "        -1.0136706 ,  1.0449423 ,  1.2001662 ,  0.04192727,  0.04971286,\n",
       "        -0.3225743 ,  2.2588894 , -0.28378484, -0.74786776, -1.3075627 ,\n",
       "        -1.9419982 , -1.20083   , -0.30825964, -1.1078343 ,  2.114838  ,\n",
       "         0.16374426], dtype=float32),\n",
       " array([-0.6298536 , -1.8367347 ,  0.71847504, -0.30375147, -0.25193402,\n",
       "         0.28571013, -0.97066027, -1.3103142 ,  1.0691004 ,  1.714622  ,\n",
       "         0.14919153, -0.24451683,  0.2962957 ,  0.42717344,  1.1683289 ,\n",
       "         1.4201417 , -1.0572189 , -0.38220927, -2.1618366 ,  1.0826797 ,\n",
       "         3.2081888 , -0.8737907 , -0.7386104 , -0.14708942,  1.0593    ,\n",
       "        -0.2804674 ,  0.7865387 ,  2.4324715 , -0.5700999 , -0.5064886 ,\n",
       "        -1.5061842 ,  0.6308376 , -0.04344665,  0.13620538, -0.2862435 ,\n",
       "         0.6307474 ,  1.3172709 , -0.98220223, -1.0635027 , -1.1677083 ,\n",
       "        -1.7361468 , -2.2474208 ,  0.751925  , -0.61349183,  0.5989425 ,\n",
       "        -0.62924767,  0.26543128,  0.6473448 ,  0.27113113, -1.537181  ,\n",
       "        -0.88925135, -0.22073743, -0.66455257,  0.760307  ,  1.5542114 ,\n",
       "        -0.02618075,  0.25483003, -1.2150273 ,  0.89408886, -0.6048542 ,\n",
       "         0.47180974,  1.3282124 , -1.656489  , -0.17273495,  0.82215846,\n",
       "         0.39935318, -0.5930527 ,  0.8755114 , -0.54368633, -0.40682343,\n",
       "        -2.72931   ,  0.7456883 , -1.5446391 , -0.70617366,  0.46626106,\n",
       "         0.33411115,  0.15642804,  1.6472325 , -0.5564732 , -0.16158177,\n",
       "        -0.26600558, -1.6028934 , -0.78525424,  0.6489696 , -0.5816102 ,\n",
       "        -0.08316445,  0.5419698 ,  0.7927042 ,  0.07648989, -1.153378  ,\n",
       "         0.24141961,  0.3695836 ,  0.10000896, -0.02940999,  1.9637774 ,\n",
       "        -0.6567252 , -0.35827065,  0.35546708, -0.6170594 , -1.8841515 ,\n",
       "        -0.47682345, -0.9326297 ,  1.653781  , -1.6176282 ,  1.0644524 ,\n",
       "        -1.2032607 ,  0.915821  ,  0.1737757 , -0.91298735, -0.31963176,\n",
       "        -1.0224653 ,  1.8323895 ,  1.5811158 , -1.3953122 ,  1.5327022 ,\n",
       "        -0.29739684, -0.4904071 ,  0.45288372, -0.52972144, -0.6142671 ,\n",
       "         1.7647986 , -0.95249575, -0.2333783 , -0.5302534 ,  0.20961444,\n",
       "         0.05254295,  0.6339196 ,  1.4962078 ,  1.2048645 , -0.6089216 ,\n",
       "        -0.21344396,  1.5506247 , -1.3563911 ,  2.2779787 ,  0.74939626,\n",
       "        -0.8008226 ,  0.45481643,  1.1409899 , -0.08675793,  0.17778566,\n",
       "         0.01546525,  2.5837235 , -0.3954399 , -0.62739074, -1.4637077 ,\n",
       "        -2.1230395 , -1.1078286 , -0.2030551 , -1.2591031 ,  2.4287236 ,\n",
       "        -0.11953412], dtype=float32),\n",
       " array([-0.6298536 , -1.8367347 ,  0.71847504, -0.30375147, -0.25193402,\n",
       "         0.28571013, -0.97066027, -1.3103142 ,  1.0691004 ,  1.714622  ,\n",
       "         0.14919153, -0.24451683,  0.2962957 ,  0.42717344,  1.1683289 ,\n",
       "         1.4201417 , -1.0572189 , -0.38220927, -2.1618366 ,  1.0826797 ,\n",
       "         3.2081888 , -0.8737907 , -0.7386104 , -0.14708942,  1.0593    ,\n",
       "        -0.2804674 ,  0.7865387 ,  2.4324715 , -0.5700999 , -0.5064886 ,\n",
       "        -1.5061842 ,  0.6308376 , -0.04344665,  0.13620538, -0.2862435 ,\n",
       "         0.6307474 ,  1.3172709 , -0.98220223, -1.0635027 , -1.1677083 ,\n",
       "        -1.7361468 , -2.2474208 ,  0.751925  , -0.61349183,  0.5989425 ,\n",
       "        -0.62924767,  0.26543128,  0.6473448 ,  0.27113113, -1.537181  ,\n",
       "        -0.88925135, -0.22073743, -0.66455257,  0.760307  ,  1.5542114 ,\n",
       "        -0.02618075,  0.25483003, -1.2150273 ,  0.89408886, -0.6048542 ,\n",
       "         0.47180974,  1.3282124 , -1.656489  , -0.17273495,  0.82215846,\n",
       "         0.39935318, -0.5930527 ,  0.8755114 , -0.54368633, -0.40682343,\n",
       "        -2.72931   ,  0.7456883 , -1.5446391 , -0.70617366,  0.46626106,\n",
       "         0.33411115,  0.15642804,  1.6472325 , -0.5564732 , -0.16158177,\n",
       "        -0.26600558, -1.6028934 , -0.78525424,  0.6489696 , -0.5816102 ,\n",
       "        -0.08316445,  0.5419698 ,  0.7927042 ,  0.07648989, -1.153378  ,\n",
       "         0.24141961,  0.3695836 ,  0.10000896, -0.02940999,  1.9637774 ,\n",
       "        -0.6567252 , -0.35827065,  0.35546708, -0.6170594 , -1.8841515 ,\n",
       "        -0.47682345, -0.9326297 ,  1.653781  , -1.6176282 ,  1.0644524 ,\n",
       "        -1.2032607 ,  0.915821  ,  0.1737757 , -0.91298735, -0.31963176,\n",
       "        -1.0224653 ,  1.8323895 ,  1.5811158 , -1.3953122 ,  1.5327022 ,\n",
       "        -0.29739684, -0.4904071 ,  0.45288372, -0.52972144, -0.6142671 ,\n",
       "         1.7647986 , -0.95249575, -0.2333783 , -0.5302534 ,  0.20961444,\n",
       "         0.05254295,  0.6339196 ,  1.4962078 ,  1.2048645 , -0.6089216 ,\n",
       "        -0.21344396,  1.5506247 , -1.3563911 ,  2.2779787 ,  0.74939626,\n",
       "        -0.8008226 ,  0.45481643,  1.1409899 , -0.08675793,  0.17778566,\n",
       "         0.01546525,  2.5837235 , -0.3954399 , -0.62739074, -1.4637077 ,\n",
       "        -2.1230395 , -1.1078286 , -0.2030551 , -1.2591031 ,  2.4287236 ,\n",
       "        -0.11953412], dtype=float32),\n",
       " array([-0.6298536 , -1.8367347 ,  0.71847504, -0.30375147, -0.25193402,\n",
       "         0.28571013, -0.97066027, -1.3103142 ,  1.0691004 ,  1.714622  ,\n",
       "         0.14919153, -0.24451683,  0.2962957 ,  0.42717344,  1.1683289 ,\n",
       "         1.4201417 , -1.0572189 , -0.38220927, -2.1618366 ,  1.0826797 ,\n",
       "         3.2081888 , -0.8737907 , -0.7386104 , -0.14708942,  1.0593    ,\n",
       "        -0.2804674 ,  0.7865387 ,  2.4324715 , -0.5700999 , -0.5064886 ,\n",
       "        -1.5061842 ,  0.6308376 , -0.04344665,  0.13620538, -0.2862435 ,\n",
       "         0.6307474 ,  1.3172709 , -0.98220223, -1.0635027 , -1.1677083 ,\n",
       "        -1.7361468 , -2.2474208 ,  0.751925  , -0.61349183,  0.5989425 ,\n",
       "        -0.62924767,  0.26543128,  0.6473448 ,  0.27113113, -1.537181  ,\n",
       "        -0.88925135, -0.22073743, -0.66455257,  0.760307  ,  1.5542114 ,\n",
       "        -0.02618075,  0.25483003, -1.2150273 ,  0.89408886, -0.6048542 ,\n",
       "         0.47180974,  1.3282124 , -1.656489  , -0.17273495,  0.82215846,\n",
       "         0.39935318, -0.5930527 ,  0.8755114 , -0.54368633, -0.40682343,\n",
       "        -2.72931   ,  0.7456883 , -1.5446391 , -0.70617366,  0.46626106,\n",
       "         0.33411115,  0.15642804,  1.6472325 , -0.5564732 , -0.16158177,\n",
       "        -0.26600558, -1.6028934 , -0.78525424,  0.6489696 , -0.5816102 ,\n",
       "        -0.08316445,  0.5419698 ,  0.7927042 ,  0.07648989, -1.153378  ,\n",
       "         0.24141961,  0.3695836 ,  0.10000896, -0.02940999,  1.9637774 ,\n",
       "        -0.6567252 , -0.35827065,  0.35546708, -0.6170594 , -1.8841515 ,\n",
       "        -0.47682345, -0.9326297 ,  1.653781  , -1.6176282 ,  1.0644524 ,\n",
       "        -1.2032607 ,  0.915821  ,  0.1737757 , -0.91298735, -0.31963176,\n",
       "        -1.0224653 ,  1.8323895 ,  1.5811158 , -1.3953122 ,  1.5327022 ,\n",
       "        -0.29739684, -0.4904071 ,  0.45288372, -0.52972144, -0.6142671 ,\n",
       "         1.7647986 , -0.95249575, -0.2333783 , -0.5302534 ,  0.20961444,\n",
       "         0.05254295,  0.6339196 ,  1.4962078 ,  1.2048645 , -0.6089216 ,\n",
       "        -0.21344396,  1.5506247 , -1.3563911 ,  2.2779787 ,  0.74939626,\n",
       "        -0.8008226 ,  0.45481643,  1.1409899 , -0.08675793,  0.17778566,\n",
       "         0.01546525,  2.5837235 , -0.3954399 , -0.62739074, -1.4637077 ,\n",
       "        -2.1230395 , -1.1078286 , -0.2030551 , -1.2591031 ,  2.4287236 ,\n",
       "        -0.11953412], dtype=float32),\n",
       " array([-0.4427338 , -1.5705959 ,  0.86439896, -0.06105169, -0.53902996,\n",
       "         0.2621941 , -0.6709832 , -1.5869957 ,  1.1702536 ,  1.5761849 ,\n",
       "         0.0634347 , -0.04731476,  0.2631008 ,  0.4107202 ,  1.0762628 ,\n",
       "         1.1681981 , -1.0025759 , -0.5779682 , -2.1070223 ,  0.872389  ,\n",
       "         2.979731  , -0.9046064 , -0.9842123 , -0.02924426,  1.3536005 ,\n",
       "        -0.119799  ,  1.0155556 ,  2.1345475 , -0.5262571 , -0.41821668,\n",
       "        -1.474431  ,  0.5335938 , -0.37470016, -0.03215579, -0.29318398,\n",
       "         0.65369207,  1.5123765 , -1.3013586 , -1.1251695 , -1.2212634 ,\n",
       "        -1.8555926 , -2.4931734 ,  0.8333873 , -0.6623654 ,  0.92916954,\n",
       "        -0.65931237,  0.03560984,  0.6772767 ,  0.14774151, -1.2762154 ,\n",
       "        -0.74631596, -0.1606141 , -0.54347885,  0.8690106 ,  1.4967211 ,\n",
       "        -0.21902038, -0.20337656, -0.96124715,  0.9832834 , -0.534759  ,\n",
       "         0.20412607,  0.980524  , -1.4409491 ,  0.13566975,  0.627794  ,\n",
       "         0.06558776, -0.5981075 ,  0.81359595, -0.9087179 , -0.46670803,\n",
       "        -2.7176309 ,  0.26657742, -1.6873444 , -1.0784469 ,  0.69631   ,\n",
       "         0.4022443 ,  0.5613305 ,  1.3424119 , -0.08241083,  0.06293451,\n",
       "        -0.4255571 , -1.7084767 , -0.9389576 ,  0.71828276, -0.9043818 ,\n",
       "        -0.09755401,  0.53194684,  0.41469708, -0.2337161 , -1.0367973 ,\n",
       "         0.3108367 ,  0.32623914,  0.32973275, -0.02752578,  2.328422  ,\n",
       "        -0.75920707, -0.22159778,  0.82376397, -0.72667366, -1.6971585 ,\n",
       "        -0.9595656 , -0.71866375,  1.6548625 , -1.7954898 ,  0.9360022 ,\n",
       "        -1.3136464 ,  0.71905017,  0.1276807 , -0.80335295, -0.20909427,\n",
       "        -1.2550929 ,  1.8621292 ,  1.8829525 , -1.076112  ,  1.8624028 ,\n",
       "        -0.28760722, -0.26389349,  0.7739122 , -0.43820587, -0.17489232,\n",
       "         1.4833127 , -1.1067265 , -0.42488986, -0.4426481 ,  0.04831211,\n",
       "         0.2801073 ,  0.50869906,  1.4242179 ,  1.5034608 , -0.5536365 ,\n",
       "         0.13135499,  1.4320074 , -1.5010194 ,  1.8935925 ,  0.8235972 ,\n",
       "        -0.36082923,  0.17978346,  1.0683664 , -0.4391358 , -0.03758732,\n",
       "         0.14144082,  2.2622275 , -0.64980805, -0.19496107, -1.3662171 ,\n",
       "        -1.8150954 , -0.8576745 , -0.27518758, -0.886284  ,  2.4539928 ,\n",
       "         0.31136635], dtype=float32),\n",
       " array([-0.4427338 , -1.5705959 ,  0.86439896, -0.06105169, -0.53902996,\n",
       "         0.2621941 , -0.6709832 , -1.5869957 ,  1.1702536 ,  1.5761849 ,\n",
       "         0.0634347 , -0.04731476,  0.2631008 ,  0.4107202 ,  1.0762628 ,\n",
       "         1.1681981 , -1.0025759 , -0.5779682 , -2.1070223 ,  0.872389  ,\n",
       "         2.979731  , -0.9046064 , -0.9842123 , -0.02924426,  1.3536005 ,\n",
       "        -0.119799  ,  1.0155556 ,  2.1345475 , -0.5262571 , -0.41821668,\n",
       "        -1.474431  ,  0.5335938 , -0.37470016, -0.03215579, -0.29318398,\n",
       "         0.65369207,  1.5123765 , -1.3013586 , -1.1251695 , -1.2212634 ,\n",
       "        -1.8555926 , -2.4931734 ,  0.8333873 , -0.6623654 ,  0.92916954,\n",
       "        -0.65931237,  0.03560984,  0.6772767 ,  0.14774151, -1.2762154 ,\n",
       "        -0.74631596, -0.1606141 , -0.54347885,  0.8690106 ,  1.4967211 ,\n",
       "        -0.21902038, -0.20337656, -0.96124715,  0.9832834 , -0.534759  ,\n",
       "         0.20412607,  0.980524  , -1.4409491 ,  0.13566975,  0.627794  ,\n",
       "         0.06558776, -0.5981075 ,  0.81359595, -0.9087179 , -0.46670803,\n",
       "        -2.7176309 ,  0.26657742, -1.6873444 , -1.0784469 ,  0.69631   ,\n",
       "         0.4022443 ,  0.5613305 ,  1.3424119 , -0.08241083,  0.06293451,\n",
       "        -0.4255571 , -1.7084767 , -0.9389576 ,  0.71828276, -0.9043818 ,\n",
       "        -0.09755401,  0.53194684,  0.41469708, -0.2337161 , -1.0367973 ,\n",
       "         0.3108367 ,  0.32623914,  0.32973275, -0.02752578,  2.328422  ,\n",
       "        -0.75920707, -0.22159778,  0.82376397, -0.72667366, -1.6971585 ,\n",
       "        -0.9595656 , -0.71866375,  1.6548625 , -1.7954898 ,  0.9360022 ,\n",
       "        -1.3136464 ,  0.71905017,  0.1276807 , -0.80335295, -0.20909427,\n",
       "        -1.2550929 ,  1.8621292 ,  1.8829525 , -1.076112  ,  1.8624028 ,\n",
       "        -0.28760722, -0.26389349,  0.7739122 , -0.43820587, -0.17489232,\n",
       "         1.4833127 , -1.1067265 , -0.42488986, -0.4426481 ,  0.04831211,\n",
       "         0.2801073 ,  0.50869906,  1.4242179 ,  1.5034608 , -0.5536365 ,\n",
       "         0.13135499,  1.4320074 , -1.5010194 ,  1.8935925 ,  0.8235972 ,\n",
       "        -0.36082923,  0.17978346,  1.0683664 , -0.4391358 , -0.03758732,\n",
       "         0.14144082,  2.2622275 , -0.64980805, -0.19496107, -1.3662171 ,\n",
       "        -1.8150954 , -0.8576745 , -0.27518758, -0.886284  ,  2.4539928 ,\n",
       "         0.31136635], dtype=float32),\n",
       " array([-0.3212093 , -1.4796956 ,  0.51187855, -0.14353785, -0.5627144 ,\n",
       "        -0.0042876 , -0.8478949 , -1.6627626 ,  0.9846215 ,  1.7245173 ,\n",
       "         0.11606742,  0.11435594,  0.32107604,  0.31676432,  1.209638  ,\n",
       "         0.8456904 , -1.1439086 , -0.6646139 , -2.273473  ,  1.330904  ,\n",
       "         2.9894826 , -0.7467485 , -0.919055  ,  0.3985172 ,  1.2032795 ,\n",
       "         0.19337037,  1.2428132 ,  2.451352  , -0.41982642, -0.6905949 ,\n",
       "        -1.2648892 ,  0.27212548,  0.20308745, -0.04586042, -0.20351091,\n",
       "         0.47342494,  1.1788543 , -1.380237  , -0.9168709 , -1.1722343 ,\n",
       "        -1.7458704 , -3.0454533 ,  0.7466826 , -0.7725156 ,  0.7047136 ,\n",
       "        -0.6536606 , -0.09870728,  0.6981027 ,  0.03936857, -1.3971865 ,\n",
       "        -0.611094  , -0.31645212, -0.59495753,  0.9785017 ,  1.0566343 ,\n",
       "        -0.8362551 , -0.14312887, -0.9368264 ,  0.4950702 , -0.6549153 ,\n",
       "         0.32694963,  0.9779997 , -1.4617584 ,  0.03640059,  0.5771352 ,\n",
       "         0.08082125, -0.47406122,  0.8937272 , -0.8712494 , -0.87102175,\n",
       "        -2.6227484 ,  0.10643512, -1.3703914 , -1.2550983 ,  0.9165224 ,\n",
       "         0.80726296,  0.86689806,  1.250002  , -0.10061301,  0.39934012,\n",
       "        -0.4428105 , -1.7627206 , -1.1739539 ,  0.5965238 , -0.77756584,\n",
       "        -0.18783045,  0.64294136,  0.19843124, -0.11312287, -1.3122593 ,\n",
       "         0.11455942,  0.5660769 ,  0.26542923, -0.10973447,  2.0155268 ,\n",
       "        -0.79487467,  0.19956437,  0.90124655, -0.860104  , -1.58763   ,\n",
       "        -1.2465941 , -0.67522794,  1.7422109 , -1.9092926 ,  0.96344346,\n",
       "        -1.2589271 ,  0.5151343 ,  0.03464429, -1.0074412 , -0.3114771 ,\n",
       "        -1.4632822 ,  1.8461938 ,  1.7045792 , -0.6339966 ,  1.2768251 ,\n",
       "        -0.3107582 ,  0.0479922 ,  0.81222284, -0.6584856 , -0.02329235,\n",
       "         1.4844799 , -0.969077  , -0.84947866, -0.18942295,  0.08958749,\n",
       "         0.31700426,  0.30305365,  1.4995613 ,  1.2484987 , -0.70706034,\n",
       "         0.13025066,  1.5020717 , -1.5657136 ,  2.3269005 ,  0.6521622 ,\n",
       "        -0.15982619,  0.28070828,  0.99763376, -0.3788567 , -0.05361941,\n",
       "        -0.03758245,  2.4017086 , -0.6103985 , -0.11821948, -1.0964836 ,\n",
       "        -1.8822649 , -0.81313056, -0.06928653, -0.5983884 ,  2.6512868 ,\n",
       "        -0.10913292], dtype=float32),\n",
       " array([-0.45861083, -1.5380362 ,  0.2920996 ,  0.07779393, -0.513221  ,\n",
       "        -0.14399935, -0.14198299, -1.615583  ,  1.1586412 ,  1.8798149 ,\n",
       "         0.02363875,  0.03878821,  0.27870524,  0.59584254,  1.4114397 ,\n",
       "         1.3289322 , -0.91107136, -0.68879277, -2.3338242 ,  0.8921043 ,\n",
       "         2.8528066 , -0.76206577, -0.7296214 ,  0.62907636,  0.979823  ,\n",
       "         0.19895054,  1.2540193 ,  2.505702  , -0.42017958, -0.45211497,\n",
       "        -1.2895669 ,  0.3385907 , -0.02010139, -0.21046562, -0.10728388,\n",
       "         0.30168602,  1.0087498 , -1.0782504 , -1.183479  , -1.2616067 ,\n",
       "        -1.8141898 , -2.5785227 ,  0.41494918, -0.5335437 ,  0.50162894,\n",
       "        -0.11497623,  0.07128526,  0.7165804 ,  0.15162745, -0.923975  ,\n",
       "        -0.5986898 , -0.0919103 , -0.6326958 ,  0.42597753,  0.8668486 ,\n",
       "        -0.73682773,  0.30443206, -0.7435051 ,  0.7490546 , -0.702472  ,\n",
       "        -0.264009  ,  0.8703738 , -1.2876496 ,  0.16754691,  0.9603621 ,\n",
       "         0.10681209, -0.40698245,  0.7191483 , -0.89178866, -0.9454522 ,\n",
       "        -2.5500104 ,  0.01082154, -1.0268724 , -1.2064731 ,  1.2904757 ,\n",
       "         0.7539695 ,  0.9194983 ,  1.0236931 , -0.60103077,  0.8927468 ,\n",
       "        -0.74734086, -1.8587378 , -1.3712418 ,  0.5637335 , -0.43644074,\n",
       "        -0.14626697, -0.15378849,  0.05877867, -0.01322901, -1.1061915 ,\n",
       "         0.43434966,  0.3102569 ,  0.55728406, -0.29271218,  1.8104749 ,\n",
       "        -0.7265469 ,  0.0591394 ,  0.85709786, -0.80568457, -1.956808  ,\n",
       "        -1.3954822 , -0.474369  ,  1.8089516 , -1.9647777 ,  0.9388241 ,\n",
       "        -1.593787  ,  0.4952233 ,  0.20265436, -1.0675915 , -0.3577891 ,\n",
       "        -1.8130584 ,  1.8936114 ,  1.641315  , -0.37704858,  1.4185143 ,\n",
       "        -0.3245238 ,  0.43326142,  0.8152296 , -1.1794018 , -0.15707844,\n",
       "         1.8225089 , -0.99353737, -0.8037861 , -0.077557  ,  0.21961641,\n",
       "        -0.18709615,  0.18995601,  1.1971006 ,  1.1119715 , -0.81907624,\n",
       "        -0.07575281,  1.2865114 , -1.7763685 ,  2.2405055 ,  0.7350155 ,\n",
       "        -0.5142337 ,  0.20598158,  0.9873551 , -0.76868737,  0.1726785 ,\n",
       "        -0.04069462,  2.5797105 , -0.45104322, -0.28548104, -1.1518636 ,\n",
       "        -1.8024503 , -1.2134781 ,  0.03690981, -0.6516126 ,  2.747655  ,\n",
       "        -0.01223142], dtype=float32),\n",
       " array([-0.45861083, -1.5380362 ,  0.2920996 ,  0.07779393, -0.513221  ,\n",
       "        -0.14399935, -0.14198299, -1.615583  ,  1.1586412 ,  1.8798149 ,\n",
       "         0.02363875,  0.03878821,  0.27870524,  0.59584254,  1.4114397 ,\n",
       "         1.3289322 , -0.91107136, -0.68879277, -2.3338242 ,  0.8921043 ,\n",
       "         2.8528066 , -0.76206577, -0.7296214 ,  0.62907636,  0.979823  ,\n",
       "         0.19895054,  1.2540193 ,  2.505702  , -0.42017958, -0.45211497,\n",
       "        -1.2895669 ,  0.3385907 , -0.02010139, -0.21046562, -0.10728388,\n",
       "         0.30168602,  1.0087498 , -1.0782504 , -1.183479  , -1.2616067 ,\n",
       "        -1.8141898 , -2.5785227 ,  0.41494918, -0.5335437 ,  0.50162894,\n",
       "        -0.11497623,  0.07128526,  0.7165804 ,  0.15162745, -0.923975  ,\n",
       "        -0.5986898 , -0.0919103 , -0.6326958 ,  0.42597753,  0.8668486 ,\n",
       "        -0.73682773,  0.30443206, -0.7435051 ,  0.7490546 , -0.702472  ,\n",
       "        -0.264009  ,  0.8703738 , -1.2876496 ,  0.16754691,  0.9603621 ,\n",
       "         0.10681209, -0.40698245,  0.7191483 , -0.89178866, -0.9454522 ,\n",
       "        -2.5500104 ,  0.01082154, -1.0268724 , -1.2064731 ,  1.2904757 ,\n",
       "         0.7539695 ,  0.9194983 ,  1.0236931 , -0.60103077,  0.8927468 ,\n",
       "        -0.74734086, -1.8587378 , -1.3712418 ,  0.5637335 , -0.43644074,\n",
       "        -0.14626697, -0.15378849,  0.05877867, -0.01322901, -1.1061915 ,\n",
       "         0.43434966,  0.3102569 ,  0.55728406, -0.29271218,  1.8104749 ,\n",
       "        -0.7265469 ,  0.0591394 ,  0.85709786, -0.80568457, -1.956808  ,\n",
       "        -1.3954822 , -0.474369  ,  1.8089516 , -1.9647777 ,  0.9388241 ,\n",
       "        -1.593787  ,  0.4952233 ,  0.20265436, -1.0675915 , -0.3577891 ,\n",
       "        -1.8130584 ,  1.8936114 ,  1.641315  , -0.37704858,  1.4185143 ,\n",
       "        -0.3245238 ,  0.43326142,  0.8152296 , -1.1794018 , -0.15707844,\n",
       "         1.8225089 , -0.99353737, -0.8037861 , -0.077557  ,  0.21961641,\n",
       "        -0.18709615,  0.18995601,  1.1971006 ,  1.1119715 , -0.81907624,\n",
       "        -0.07575281,  1.2865114 , -1.7763685 ,  2.2405055 ,  0.7350155 ,\n",
       "        -0.5142337 ,  0.20598158,  0.9873551 , -0.76868737,  0.1726785 ,\n",
       "        -0.04069462,  2.5797105 , -0.45104322, -0.28548104, -1.1518636 ,\n",
       "        -1.8024503 , -1.2134781 ,  0.03690981, -0.6516126 ,  2.747655  ,\n",
       "        -0.01223142], dtype=float32),\n",
       " array([-0.64671665, -1.5295526 ,  0.37807778,  0.42344385, -0.57102734,\n",
       "         0.04498833, -0.16931428, -1.4336706 ,  1.2305083 ,  1.8536109 ,\n",
       "         0.0360268 ,  0.1775925 ,  0.14910693,  0.96670973,  1.576749  ,\n",
       "         1.1030906 , -0.98787355, -0.85435975, -2.2448022 ,  0.9054998 ,\n",
       "         2.9684708 , -0.6823434 , -0.64224726,  0.7829826 ,  0.7280516 ,\n",
       "         0.45633122,  1.1197788 ,  2.7317634 , -0.38831612, -0.8741936 ,\n",
       "        -1.1781845 ,  0.2095603 ,  0.06146394, -0.2116755 , -0.04526963,\n",
       "        -0.31026193,  0.91199887, -0.73891944, -1.1858736 , -1.1724868 ,\n",
       "        -1.5305326 , -2.3086092 ,  0.4662064 , -0.16506971,  0.52126086,\n",
       "        -0.06370467, -0.21996215,  0.9633115 ,  0.15557274, -1.0580071 ,\n",
       "        -0.5339259 ,  0.04707032, -0.17796806,  0.2498383 ,  0.7224661 ,\n",
       "        -0.62018865, -0.15321863, -0.8054545 ,  0.7808723 , -0.6334954 ,\n",
       "        -0.37918168,  0.8498274 , -1.5918467 ,  0.18658587,  1.1394453 ,\n",
       "         0.2632999 , -0.4969166 ,  0.9013842 , -0.9645261 , -1.0849086 ,\n",
       "        -2.6328952 ,  0.01987434, -0.8720225 , -0.9432183 ,  1.1552515 ,\n",
       "         0.9577034 ,  0.91768485,  1.2357284 , -0.71193266,  0.6982526 ,\n",
       "        -0.735542  , -1.8385723 , -1.8841357 ,  0.25396785, -0.35722613,\n",
       "         0.18098685, -0.6271143 , -0.1207775 , -0.04991389, -0.98489064,\n",
       "         0.3890406 ,  0.253883  ,  0.8216281 ,  0.04490384,  1.6709883 ,\n",
       "        -0.6886902 ,  0.23967925,  0.8320264 , -0.8325631 , -1.8342823 ,\n",
       "        -1.0689983 , -0.62842655,  1.6554362 , -1.9668727 ,  0.7127214 ,\n",
       "        -1.858239  ,  0.37510175,  0.4643169 , -1.2410846 , -0.28505754,\n",
       "        -1.8396488 ,  2.0643048 ,  1.7792609 , -0.22278762,  1.637682  ,\n",
       "        -0.31562904,  0.34607273,  0.9993339 , -1.289946  , -0.1669353 ,\n",
       "         1.7468058 , -1.2557272 , -0.9391692 ,  0.07345666,  0.24780455,\n",
       "        -0.18373726, -0.05376661,  1.339953  ,  1.1665819 , -0.7407375 ,\n",
       "         0.09274991,  1.2389096 , -1.6705884 ,  2.4406168 ,  0.6898346 ,\n",
       "        -0.46072876, -0.16251606,  0.9935013 , -0.87907296, -0.43031633,\n",
       "         0.08431551,  2.9259474 , -0.4499872 , -0.3489117 , -1.1432557 ,\n",
       "        -1.992368  , -1.159096  , -0.07086789, -0.63641787,  2.8309863 ,\n",
       "        -0.14556581], dtype=float32),\n",
       " array([-0.64671665, -1.5295526 ,  0.37807778,  0.42344385, -0.57102734,\n",
       "         0.04498833, -0.16931428, -1.4336706 ,  1.2305083 ,  1.8536109 ,\n",
       "         0.0360268 ,  0.1775925 ,  0.14910693,  0.96670973,  1.576749  ,\n",
       "         1.1030906 , -0.98787355, -0.85435975, -2.2448022 ,  0.9054998 ,\n",
       "         2.9684708 , -0.6823434 , -0.64224726,  0.7829826 ,  0.7280516 ,\n",
       "         0.45633122,  1.1197788 ,  2.7317634 , -0.38831612, -0.8741936 ,\n",
       "        -1.1781845 ,  0.2095603 ,  0.06146394, -0.2116755 , -0.04526963,\n",
       "        -0.31026193,  0.91199887, -0.73891944, -1.1858736 , -1.1724868 ,\n",
       "        -1.5305326 , -2.3086092 ,  0.4662064 , -0.16506971,  0.52126086,\n",
       "        -0.06370467, -0.21996215,  0.9633115 ,  0.15557274, -1.0580071 ,\n",
       "        -0.5339259 ,  0.04707032, -0.17796806,  0.2498383 ,  0.7224661 ,\n",
       "        -0.62018865, -0.15321863, -0.8054545 ,  0.7808723 , -0.6334954 ,\n",
       "        -0.37918168,  0.8498274 , -1.5918467 ,  0.18658587,  1.1394453 ,\n",
       "         0.2632999 , -0.4969166 ,  0.9013842 , -0.9645261 , -1.0849086 ,\n",
       "        -2.6328952 ,  0.01987434, -0.8720225 , -0.9432183 ,  1.1552515 ,\n",
       "         0.9577034 ,  0.91768485,  1.2357284 , -0.71193266,  0.6982526 ,\n",
       "        -0.735542  , -1.8385723 , -1.8841357 ,  0.25396785, -0.35722613,\n",
       "         0.18098685, -0.6271143 , -0.1207775 , -0.04991389, -0.98489064,\n",
       "         0.3890406 ,  0.253883  ,  0.8216281 ,  0.04490384,  1.6709883 ,\n",
       "        -0.6886902 ,  0.23967925,  0.8320264 , -0.8325631 , -1.8342823 ,\n",
       "        -1.0689983 , -0.62842655,  1.6554362 , -1.9668727 ,  0.7127214 ,\n",
       "        -1.858239  ,  0.37510175,  0.4643169 , -1.2410846 , -0.28505754,\n",
       "        -1.8396488 ,  2.0643048 ,  1.7792609 , -0.22278762,  1.637682  ,\n",
       "        -0.31562904,  0.34607273,  0.9993339 , -1.289946  , -0.1669353 ,\n",
       "         1.7468058 , -1.2557272 , -0.9391692 ,  0.07345666,  0.24780455,\n",
       "        -0.18373726, -0.05376661,  1.339953  ,  1.1665819 , -0.7407375 ,\n",
       "         0.09274991,  1.2389096 , -1.6705884 ,  2.4406168 ,  0.6898346 ,\n",
       "        -0.46072876, -0.16251606,  0.9935013 , -0.87907296, -0.43031633,\n",
       "         0.08431551,  2.9259474 , -0.4499872 , -0.3489117 , -1.1432557 ,\n",
       "        -1.992368  , -1.159096  , -0.07086789, -0.63641787,  2.8309863 ,\n",
       "        -0.14556581], dtype=float32),\n",
       " array([-0.64671665, -1.5295526 ,  0.37807778,  0.42344385, -0.57102734,\n",
       "         0.04498833, -0.16931428, -1.4336706 ,  1.2305083 ,  1.8536109 ,\n",
       "         0.0360268 ,  0.1775925 ,  0.14910693,  0.96670973,  1.576749  ,\n",
       "         1.1030906 , -0.98787355, -0.85435975, -2.2448022 ,  0.9054998 ,\n",
       "         2.9684708 , -0.6823434 , -0.64224726,  0.7829826 ,  0.7280516 ,\n",
       "         0.45633122,  1.1197788 ,  2.7317634 , -0.38831612, -0.8741936 ,\n",
       "        -1.1781845 ,  0.2095603 ,  0.06146394, -0.2116755 , -0.04526963,\n",
       "        -0.31026193,  0.91199887, -0.73891944, -1.1858736 , -1.1724868 ,\n",
       "        -1.5305326 , -2.3086092 ,  0.4662064 , -0.16506971,  0.52126086,\n",
       "        -0.06370467, -0.21996215,  0.9633115 ,  0.15557274, -1.0580071 ,\n",
       "        -0.5339259 ,  0.04707032, -0.17796806,  0.2498383 ,  0.7224661 ,\n",
       "        -0.62018865, -0.15321863, -0.8054545 ,  0.7808723 , -0.6334954 ,\n",
       "        -0.37918168,  0.8498274 , -1.5918467 ,  0.18658587,  1.1394453 ,\n",
       "         0.2632999 , -0.4969166 ,  0.9013842 , -0.9645261 , -1.0849086 ,\n",
       "        -2.6328952 ,  0.01987434, -0.8720225 , -0.9432183 ,  1.1552515 ,\n",
       "         0.9577034 ,  0.91768485,  1.2357284 , -0.71193266,  0.6982526 ,\n",
       "        -0.735542  , -1.8385723 , -1.8841357 ,  0.25396785, -0.35722613,\n",
       "         0.18098685, -0.6271143 , -0.1207775 , -0.04991389, -0.98489064,\n",
       "         0.3890406 ,  0.253883  ,  0.8216281 ,  0.04490384,  1.6709883 ,\n",
       "        -0.6886902 ,  0.23967925,  0.8320264 , -0.8325631 , -1.8342823 ,\n",
       "        -1.0689983 , -0.62842655,  1.6554362 , -1.9668727 ,  0.7127214 ,\n",
       "        -1.858239  ,  0.37510175,  0.4643169 , -1.2410846 , -0.28505754,\n",
       "        -1.8396488 ,  2.0643048 ,  1.7792609 , -0.22278762,  1.637682  ,\n",
       "        -0.31562904,  0.34607273,  0.9993339 , -1.289946  , -0.1669353 ,\n",
       "         1.7468058 , -1.2557272 , -0.9391692 ,  0.07345666,  0.24780455,\n",
       "        -0.18373726, -0.05376661,  1.339953  ,  1.1665819 , -0.7407375 ,\n",
       "         0.09274991,  1.2389096 , -1.6705884 ,  2.4406168 ,  0.6898346 ,\n",
       "        -0.46072876, -0.16251606,  0.9935013 , -0.87907296, -0.43031633,\n",
       "         0.08431551,  2.9259474 , -0.4499872 , -0.3489117 , -1.1432557 ,\n",
       "        -1.992368  , -1.159096  , -0.07086789, -0.63641787,  2.8309863 ,\n",
       "        -0.14556581], dtype=float32),\n",
       " array([-0.64671665, -1.5295526 ,  0.37807778,  0.42344385, -0.57102734,\n",
       "         0.04498833, -0.16931428, -1.4336706 ,  1.2305083 ,  1.8536109 ,\n",
       "         0.0360268 ,  0.1775925 ,  0.14910693,  0.96670973,  1.576749  ,\n",
       "         1.1030906 , -0.98787355, -0.85435975, -2.2448022 ,  0.9054998 ,\n",
       "         2.9684708 , -0.6823434 , -0.64224726,  0.7829826 ,  0.7280516 ,\n",
       "         0.45633122,  1.1197788 ,  2.7317634 , -0.38831612, -0.8741936 ,\n",
       "        -1.1781845 ,  0.2095603 ,  0.06146394, -0.2116755 , -0.04526963,\n",
       "        -0.31026193,  0.91199887, -0.73891944, -1.1858736 , -1.1724868 ,\n",
       "        -1.5305326 , -2.3086092 ,  0.4662064 , -0.16506971,  0.52126086,\n",
       "        -0.06370467, -0.21996215,  0.9633115 ,  0.15557274, -1.0580071 ,\n",
       "        -0.5339259 ,  0.04707032, -0.17796806,  0.2498383 ,  0.7224661 ,\n",
       "        -0.62018865, -0.15321863, -0.8054545 ,  0.7808723 , -0.6334954 ,\n",
       "        -0.37918168,  0.8498274 , -1.5918467 ,  0.18658587,  1.1394453 ,\n",
       "         0.2632999 , -0.4969166 ,  0.9013842 , -0.9645261 , -1.0849086 ,\n",
       "        -2.6328952 ,  0.01987434, -0.8720225 , -0.9432183 ,  1.1552515 ,\n",
       "         0.9577034 ,  0.91768485,  1.2357284 , -0.71193266,  0.6982526 ,\n",
       "        -0.735542  , -1.8385723 , -1.8841357 ,  0.25396785, -0.35722613,\n",
       "         0.18098685, -0.6271143 , -0.1207775 , -0.04991389, -0.98489064,\n",
       "         0.3890406 ,  0.253883  ,  0.8216281 ,  0.04490384,  1.6709883 ,\n",
       "        -0.6886902 ,  0.23967925,  0.8320264 , -0.8325631 , -1.8342823 ,\n",
       "        -1.0689983 , -0.62842655,  1.6554362 , -1.9668727 ,  0.7127214 ,\n",
       "        -1.858239  ,  0.37510175,  0.4643169 , -1.2410846 , -0.28505754,\n",
       "        -1.8396488 ,  2.0643048 ,  1.7792609 , -0.22278762,  1.637682  ,\n",
       "        -0.31562904,  0.34607273,  0.9993339 , -1.289946  , -0.1669353 ,\n",
       "         1.7468058 , -1.2557272 , -0.9391692 ,  0.07345666,  0.24780455,\n",
       "        -0.18373726, -0.05376661,  1.339953  ,  1.1665819 , -0.7407375 ,\n",
       "         0.09274991,  1.2389096 , -1.6705884 ,  2.4406168 ,  0.6898346 ,\n",
       "        -0.46072876, -0.16251606,  0.9935013 , -0.87907296, -0.43031633,\n",
       "         0.08431551,  2.9259474 , -0.4499872 , -0.3489117 , -1.1432557 ,\n",
       "        -1.992368  , -1.159096  , -0.07086789, -0.63641787,  2.8309863 ,\n",
       "        -0.14556581], dtype=float32),\n",
       " array([-0.64671665, -1.5295526 ,  0.37807778,  0.42344385, -0.57102734,\n",
       "         0.04498833, -0.16931428, -1.4336706 ,  1.2305083 ,  1.8536109 ,\n",
       "         0.0360268 ,  0.1775925 ,  0.14910693,  0.96670973,  1.576749  ,\n",
       "         1.1030906 , -0.98787355, -0.85435975, -2.2448022 ,  0.9054998 ,\n",
       "         2.9684708 , -0.6823434 , -0.64224726,  0.7829826 ,  0.7280516 ,\n",
       "         0.45633122,  1.1197788 ,  2.7317634 , -0.38831612, -0.8741936 ,\n",
       "        -1.1781845 ,  0.2095603 ,  0.06146394, -0.2116755 , -0.04526963,\n",
       "        -0.31026193,  0.91199887, -0.73891944, -1.1858736 , -1.1724868 ,\n",
       "        -1.5305326 , -2.3086092 ,  0.4662064 , -0.16506971,  0.52126086,\n",
       "        -0.06370467, -0.21996215,  0.9633115 ,  0.15557274, -1.0580071 ,\n",
       "        -0.5339259 ,  0.04707032, -0.17796806,  0.2498383 ,  0.7224661 ,\n",
       "        -0.62018865, -0.15321863, -0.8054545 ,  0.7808723 , -0.6334954 ,\n",
       "        -0.37918168,  0.8498274 , -1.5918467 ,  0.18658587,  1.1394453 ,\n",
       "         0.2632999 , -0.4969166 ,  0.9013842 , -0.9645261 , -1.0849086 ,\n",
       "        -2.6328952 ,  0.01987434, -0.8720225 , -0.9432183 ,  1.1552515 ,\n",
       "         0.9577034 ,  0.91768485,  1.2357284 , -0.71193266,  0.6982526 ,\n",
       "        -0.735542  , -1.8385723 , -1.8841357 ,  0.25396785, -0.35722613,\n",
       "         0.18098685, -0.6271143 , -0.1207775 , -0.04991389, -0.98489064,\n",
       "         0.3890406 ,  0.253883  ,  0.8216281 ,  0.04490384,  1.6709883 ,\n",
       "        -0.6886902 ,  0.23967925,  0.8320264 , -0.8325631 , -1.8342823 ,\n",
       "        -1.0689983 , -0.62842655,  1.6554362 , -1.9668727 ,  0.7127214 ,\n",
       "        -1.858239  ,  0.37510175,  0.4643169 , -1.2410846 , -0.28505754,\n",
       "        -1.8396488 ,  2.0643048 ,  1.7792609 , -0.22278762,  1.637682  ,\n",
       "        -0.31562904,  0.34607273,  0.9993339 , -1.289946  , -0.1669353 ,\n",
       "         1.7468058 , -1.2557272 , -0.9391692 ,  0.07345666,  0.24780455,\n",
       "        -0.18373726, -0.05376661,  1.339953  ,  1.1665819 , -0.7407375 ,\n",
       "         0.09274991,  1.2389096 , -1.6705884 ,  2.4406168 ,  0.6898346 ,\n",
       "        -0.46072876, -0.16251606,  0.9935013 , -0.87907296, -0.43031633,\n",
       "         0.08431551,  2.9259474 , -0.4499872 , -0.3489117 , -1.1432557 ,\n",
       "        -1.992368  , -1.159096  , -0.07086789, -0.63641787,  2.8309863 ,\n",
       "        -0.14556581], dtype=float32),\n",
       " array([-0.64671665, -1.5295526 ,  0.37807778,  0.42344385, -0.57102734,\n",
       "         0.04498833, -0.16931428, -1.4336706 ,  1.2305083 ,  1.8536109 ,\n",
       "         0.0360268 ,  0.1775925 ,  0.14910693,  0.96670973,  1.576749  ,\n",
       "         1.1030906 , -0.98787355, -0.85435975, -2.2448022 ,  0.9054998 ,\n",
       "         2.9684708 , -0.6823434 , -0.64224726,  0.7829826 ,  0.7280516 ,\n",
       "         0.45633122,  1.1197788 ,  2.7317634 , -0.38831612, -0.8741936 ,\n",
       "        -1.1781845 ,  0.2095603 ,  0.06146394, -0.2116755 , -0.04526963,\n",
       "        -0.31026193,  0.91199887, -0.73891944, -1.1858736 , -1.1724868 ,\n",
       "        -1.5305326 , -2.3086092 ,  0.4662064 , -0.16506971,  0.52126086,\n",
       "        -0.06370467, -0.21996215,  0.9633115 ,  0.15557274, -1.0580071 ,\n",
       "        -0.5339259 ,  0.04707032, -0.17796806,  0.2498383 ,  0.7224661 ,\n",
       "        -0.62018865, -0.15321863, -0.8054545 ,  0.7808723 , -0.6334954 ,\n",
       "        -0.37918168,  0.8498274 , -1.5918467 ,  0.18658587,  1.1394453 ,\n",
       "         0.2632999 , -0.4969166 ,  0.9013842 , -0.9645261 , -1.0849086 ,\n",
       "        -2.6328952 ,  0.01987434, -0.8720225 , -0.9432183 ,  1.1552515 ,\n",
       "         0.9577034 ,  0.91768485,  1.2357284 , -0.71193266,  0.6982526 ,\n",
       "        -0.735542  , -1.8385723 , -1.8841357 ,  0.25396785, -0.35722613,\n",
       "         0.18098685, -0.6271143 , -0.1207775 , -0.04991389, -0.98489064,\n",
       "         0.3890406 ,  0.253883  ,  0.8216281 ,  0.04490384,  1.6709883 ,\n",
       "        -0.6886902 ,  0.23967925,  0.8320264 , -0.8325631 , -1.8342823 ,\n",
       "        -1.0689983 , -0.62842655,  1.6554362 , -1.9668727 ,  0.7127214 ,\n",
       "        -1.858239  ,  0.37510175,  0.4643169 , -1.2410846 , -0.28505754,\n",
       "        -1.8396488 ,  2.0643048 ,  1.7792609 , -0.22278762,  1.637682  ,\n",
       "        -0.31562904,  0.34607273,  0.9993339 , -1.289946  , -0.1669353 ,\n",
       "         1.7468058 , -1.2557272 , -0.9391692 ,  0.07345666,  0.24780455,\n",
       "        -0.18373726, -0.05376661,  1.339953  ,  1.1665819 , -0.7407375 ,\n",
       "         0.09274991,  1.2389096 , -1.6705884 ,  2.4406168 ,  0.6898346 ,\n",
       "        -0.46072876, -0.16251606,  0.9935013 , -0.87907296, -0.43031633,\n",
       "         0.08431551,  2.9259474 , -0.4499872 , -0.3489117 , -1.1432557 ,\n",
       "        -1.992368  , -1.159096  , -0.07086789, -0.63641787,  2.8309863 ,\n",
       "        -0.14556581], dtype=float32),\n",
       " array([-0.9272468 , -1.4887258 ,  0.53374004,  0.52762806, -0.37491778,\n",
       "         0.15274376, -0.00738964, -1.3158871 ,  0.9094201 ,  1.9646299 ,\n",
       "        -0.24402349,  0.24753356,  0.35077137,  0.82959604,  1.8268718 ,\n",
       "         1.2537745 , -1.1318355 , -1.1017758 , -2.145459  ,  1.2059901 ,\n",
       "         2.9465451 , -0.8431083 , -0.36373502,  0.62702763,  0.8035157 ,\n",
       "         0.5616513 ,  0.9432655 ,  2.4787288 , -0.51208425, -0.87797886,\n",
       "        -1.3607098 , -0.2286586 , -0.06451932, -0.03494547, -0.4760648 ,\n",
       "         0.14417954,  0.64903945, -0.6475056 , -0.94377744, -1.140927  ,\n",
       "        -1.8511072 , -2.4714713 ,  0.32437083, -0.10605491,  0.1069153 ,\n",
       "         0.00577128, -0.63979614,  0.53274655,  0.36848685, -0.602886  ,\n",
       "        -0.72972953,  0.49177897, -0.04285453,  0.67712444,  0.48519504,\n",
       "        -0.6885441 ,  0.28233504, -0.36455643,  0.770882  , -0.6835381 ,\n",
       "        -0.22816312,  1.1216552 , -1.5493397 ,  0.4118102 ,  0.86212015,\n",
       "         0.32312703, -0.3558142 ,  1.1409001 , -1.1965064 , -0.8393018 ,\n",
       "        -2.5504718 , -0.20803875, -0.91699886, -0.7862092 ,  0.9851561 ,\n",
       "         1.118508  ,  0.73842233,  0.999978  , -0.8141672 ,  0.9029099 ,\n",
       "        -0.76010907, -1.5067903 , -2.295574  ,  0.1576308 , -0.21250446,\n",
       "         0.20411086, -0.4962678 ,  0.17161345,  0.16780147, -0.7886365 ,\n",
       "         0.3792537 ,  0.5195115 ,  0.70923084, -0.06988398,  1.6818559 ,\n",
       "        -0.79762775,  0.19030356,  0.77167237, -0.81444806, -1.4417166 ,\n",
       "        -1.0398434 , -0.48183554,  1.318314  , -1.8881522 ,  0.8841797 ,\n",
       "        -1.9285285 ,  0.8679448 ,  0.4368244 , -1.2228363 , -0.13622364,\n",
       "        -1.4907296 ,  1.984669  ,  1.7493819 ,  0.03136516,  1.4975283 ,\n",
       "        -0.20891665,  0.36824372,  0.92857355, -1.2741528 , -0.23200426,\n",
       "         1.8575027 , -1.4257376 , -0.8631198 ,  0.06035431,  0.34507468,\n",
       "        -0.29857445, -0.08022595,  0.9110706 ,  1.3880613 , -0.65763265,\n",
       "         0.57899606,  1.3278706 , -1.3067808 ,  2.4267554 ,  0.68227553,\n",
       "        -0.25094882, -0.520125  ,  0.8903351 , -0.6625417 , -0.2315559 ,\n",
       "         0.19641559,  2.76082   , -0.5336011 , -0.4611275 , -0.9482618 ,\n",
       "        -2.3646336 , -0.9416424 , -0.0053831 , -0.7012283 ,  2.9346044 ,\n",
       "        -0.03558597], dtype=float32),\n",
       " array([-9.3047923e-01, -1.4418038e+00,  7.7860498e-01,  9.8851360e-02,\n",
       "        -3.0347046e-01,  2.3797164e-03, -3.9490212e-02, -8.6843282e-01,\n",
       "         8.2547796e-01,  2.1129222e+00, -1.6164295e-01,  2.6891354e-01,\n",
       "         2.0073248e-01,  2.4383375e-01,  1.6900051e+00,  1.2821929e+00,\n",
       "        -1.1656382e+00, -1.0808572e+00, -1.8554308e+00,  1.5435178e+00,\n",
       "         2.9369426e+00, -8.1674778e-01, -6.4027917e-01,  7.9177767e-01,\n",
       "         9.5975041e-01,  8.9186865e-01,  9.3742478e-01,  2.6698663e+00,\n",
       "        -9.5088822e-01, -7.7575034e-01, -1.2534454e+00, -1.5142174e-01,\n",
       "        -3.1100670e-01, -2.4967691e-01, -6.6598183e-01,  2.8211022e-02,\n",
       "         8.5010242e-01, -6.9577622e-01, -8.1276155e-01, -1.2466359e+00,\n",
       "        -1.6607244e+00, -2.2999403e+00,  4.7189513e-01, -3.4196603e-01,\n",
       "         1.0772652e-01,  1.8610510e-01, -4.6785581e-01,  6.1130363e-01,\n",
       "         1.6178250e-01, -8.3174205e-01, -5.7020921e-01,  6.8616962e-01,\n",
       "        -3.8001642e-01,  7.5485533e-01,  3.8010195e-01, -6.9686639e-01,\n",
       "         3.2339826e-01, -8.1178558e-01,  6.2492001e-01, -7.0071441e-01,\n",
       "        -2.8650574e-02,  1.2781339e+00, -1.3639497e+00,  4.5211431e-01,\n",
       "         8.5280794e-01,  6.1342996e-01, -2.9365394e-01,  1.5492035e+00,\n",
       "        -1.3517281e+00, -9.5574707e-01, -2.5940275e+00,  3.6321238e-02,\n",
       "        -1.2277596e+00, -4.3090391e-01,  9.5400161e-01,  1.4075034e+00,\n",
       "         7.2576123e-01,  1.0409425e+00, -4.7260886e-01,  8.0775619e-01,\n",
       "        -9.1138881e-01, -1.3567650e+00, -2.5006526e+00, -9.1416992e-02,\n",
       "        -1.7139147e-01, -1.8482848e-01, -9.3528873e-01,  2.5053594e-01,\n",
       "        -3.7479938e-03, -8.1862497e-01,  2.6224574e-01,  4.2049029e-01,\n",
       "         6.0767168e-01,  9.6135721e-02,  1.7377974e+00, -1.0208489e+00,\n",
       "         1.3415647e-01,  7.5162822e-01, -8.3267051e-01, -1.3954345e+00,\n",
       "        -1.1972576e+00, -4.9486142e-01,  1.4593259e+00, -1.7117608e+00,\n",
       "         7.7435589e-01, -1.5752444e+00,  1.4399118e+00,  5.9696430e-01,\n",
       "        -9.4138116e-01, -1.0922987e-01, -1.4907461e+00,  2.0982575e+00,\n",
       "         1.4486225e+00,  8.1601955e-02,  1.5064455e+00, -6.2149506e-02,\n",
       "         1.5748246e-01,  8.9000076e-01, -1.3173207e+00, -3.9760685e-01,\n",
       "         1.7442700e+00, -1.6047975e+00, -5.2673340e-01, -2.6220611e-01,\n",
       "         2.1850032e-01, -7.3547715e-01,  2.4049442e-01,  8.5711807e-01,\n",
       "         1.1499865e+00, -8.6938196e-01,  2.6860029e-01,  1.5190933e+00,\n",
       "        -1.2096630e+00,  2.3704965e+00,  7.7072710e-01, -4.2919368e-02,\n",
       "        -3.2107756e-01,  7.9203194e-01, -4.2613587e-01, -4.3788558e-01,\n",
       "         1.6741464e-01,  2.7100053e+00, -4.4996789e-01, -1.4675772e-01,\n",
       "        -9.5679218e-01, -2.2660661e+00, -5.8842981e-01, -1.4296398e-01,\n",
       "        -8.6373085e-01,  2.8205252e+00, -1.9697368e-02], dtype=float32),\n",
       " array([-0.61805725, -1.3339826 ,  0.8452792 ,  0.20792505, -0.33906576,\n",
       "         0.22621904,  0.06726405, -0.8466258 ,  0.9993821 ,  2.1448495 ,\n",
       "        -0.19603248,  0.12735091,  0.21483733,  0.69427615,  1.4232479 ,\n",
       "         1.3401678 , -0.94230884, -1.2574173 , -2.477179  ,  1.5543625 ,\n",
       "         2.891023  , -0.8256429 , -0.15751186,  0.54087216,  0.8278082 ,\n",
       "         0.97982866,  0.7193495 ,  2.840783  , -1.033298  , -0.6318094 ,\n",
       "        -1.0841138 ,  0.10730541, -0.27949572, -0.4282057 , -0.7153384 ,\n",
       "        -0.03485352,  1.0785528 , -0.4963314 , -0.7043723 , -1.1663204 ,\n",
       "        -1.8033745 , -2.317149  ,  0.74230695, -0.56299156,  0.00617327,\n",
       "         0.31250712, -0.70704705,  0.8723371 ,  0.02619481, -0.85177743,\n",
       "        -0.51846844,  1.0197763 , -0.26969236,  0.9852599 ,  0.3281094 ,\n",
       "        -0.4108334 ,  0.2900915 , -0.8357017 ,  0.8584646 , -0.6331306 ,\n",
       "        -0.19806379,  1.0099732 , -1.6673946 ,  0.07927189,  0.7656673 ,\n",
       "         0.5352531 , -0.48787487,  1.732339  , -1.4584993 , -0.8677639 ,\n",
       "        -2.6190512 , -0.25613278, -1.2573344 , -0.7371226 ,  0.64625967,\n",
       "         1.3096511 ,  0.08597916,  1.4354122 , -0.4046018 ,  0.78861696,\n",
       "        -0.7361128 , -1.2440364 , -2.9839325 , -0.21897005, -0.31645313,\n",
       "        -0.02494216, -1.6326286 ,  0.2788657 , -0.30083883, -0.6656354 ,\n",
       "         0.38438264,  0.37477973,  0.7143614 ,  0.30877805,  1.8441008 ,\n",
       "        -1.1817739 ,  0.2126873 ,  0.79768056, -0.8956973 , -1.2457737 ,\n",
       "        -1.3132347 , -0.4235548 ,  1.5918344 , -1.6447824 ,  0.7407782 ,\n",
       "        -1.4845198 ,  1.6520767 ,  0.54981583, -0.82465196,  0.06148814,\n",
       "        -1.8935602 ,  2.026692  ,  1.472837  ,  0.5320385 ,  1.5234634 ,\n",
       "        -0.22149682, -0.02108379,  1.0424253 , -1.4009576 , -0.4046478 ,\n",
       "         1.6730152 , -1.0076187 , -0.23126706, -0.02599909,  0.03936547,\n",
       "        -0.9956996 ,  0.3339319 ,  0.9903219 ,  0.81636167, -0.5621548 ,\n",
       "         0.05948443,  1.4719604 , -1.0501325 ,  2.178853  ,  0.8059608 ,\n",
       "        -0.11495354, -0.12779738,  0.7330186 , -0.2730497 , -0.27864656,\n",
       "         0.34645018,  2.6814456 , -0.14935254, -0.28957757, -1.0839691 ,\n",
       "        -2.393767  , -0.6932948 , -0.09270021, -0.7799035 ,  2.5268743 ,\n",
       "         0.30921894], dtype=float32),\n",
       " array([-0.61805725, -1.3339826 ,  0.8452792 ,  0.20792505, -0.33906576,\n",
       "         0.22621904,  0.06726405, -0.8466258 ,  0.9993821 ,  2.1448495 ,\n",
       "        -0.19603248,  0.12735091,  0.21483733,  0.69427615,  1.4232479 ,\n",
       "         1.3401678 , -0.94230884, -1.2574173 , -2.477179  ,  1.5543625 ,\n",
       "         2.891023  , -0.8256429 , -0.15751186,  0.54087216,  0.8278082 ,\n",
       "         0.97982866,  0.7193495 ,  2.840783  , -1.033298  , -0.6318094 ,\n",
       "        -1.0841138 ,  0.10730541, -0.27949572, -0.4282057 , -0.7153384 ,\n",
       "        -0.03485352,  1.0785528 , -0.4963314 , -0.7043723 , -1.1663204 ,\n",
       "        -1.8033745 , -2.317149  ,  0.74230695, -0.56299156,  0.00617327,\n",
       "         0.31250712, -0.70704705,  0.8723371 ,  0.02619481, -0.85177743,\n",
       "        -0.51846844,  1.0197763 , -0.26969236,  0.9852599 ,  0.3281094 ,\n",
       "        -0.4108334 ,  0.2900915 , -0.8357017 ,  0.8584646 , -0.6331306 ,\n",
       "        -0.19806379,  1.0099732 , -1.6673946 ,  0.07927189,  0.7656673 ,\n",
       "         0.5352531 , -0.48787487,  1.732339  , -1.4584993 , -0.8677639 ,\n",
       "        -2.6190512 , -0.25613278, -1.2573344 , -0.7371226 ,  0.64625967,\n",
       "         1.3096511 ,  0.08597916,  1.4354122 , -0.4046018 ,  0.78861696,\n",
       "        -0.7361128 , -1.2440364 , -2.9839325 , -0.21897005, -0.31645313,\n",
       "        -0.02494216, -1.6326286 ,  0.2788657 , -0.30083883, -0.6656354 ,\n",
       "         0.38438264,  0.37477973,  0.7143614 ,  0.30877805,  1.8441008 ,\n",
       "        -1.1817739 ,  0.2126873 ,  0.79768056, -0.8956973 , -1.2457737 ,\n",
       "        -1.3132347 , -0.4235548 ,  1.5918344 , -1.6447824 ,  0.7407782 ,\n",
       "        -1.4845198 ,  1.6520767 ,  0.54981583, -0.82465196,  0.06148814,\n",
       "        -1.8935602 ,  2.026692  ,  1.472837  ,  0.5320385 ,  1.5234634 ,\n",
       "        -0.22149682, -0.02108379,  1.0424253 , -1.4009576 , -0.4046478 ,\n",
       "         1.6730152 , -1.0076187 , -0.23126706, -0.02599909,  0.03936547,\n",
       "        -0.9956996 ,  0.3339319 ,  0.9903219 ,  0.81636167, -0.5621548 ,\n",
       "         0.05948443,  1.4719604 , -1.0501325 ,  2.178853  ,  0.8059608 ,\n",
       "        -0.11495354, -0.12779738,  0.7330186 , -0.2730497 , -0.27864656,\n",
       "         0.34645018,  2.6814456 , -0.14935254, -0.28957757, -1.0839691 ,\n",
       "        -2.393767  , -0.6932948 , -0.09270021, -0.7799035 ,  2.5268743 ,\n",
       "         0.30921894], dtype=float32),\n",
       " array([-7.46075690e-01, -1.70153058e+00,  6.94810808e-01, -3.40163291e-01,\n",
       "        -2.37161666e-01,  4.87793647e-02,  2.80349907e-02, -1.19461679e+00,\n",
       "         9.69667673e-01,  1.51421332e+00, -7.09818676e-02,  3.50315005e-01,\n",
       "         6.05426393e-02,  9.76855814e-01,  1.65877557e+00,  1.11823237e+00,\n",
       "        -5.70692062e-01, -1.26384985e+00, -2.60411024e+00,  1.30422938e+00,\n",
       "         3.10490990e+00, -6.78044796e-01, -2.09608868e-01,  5.84654987e-01,\n",
       "         5.40999651e-01,  8.33603799e-01,  6.08090937e-01,  2.88831568e+00,\n",
       "        -1.34938526e+00, -6.74828947e-01, -1.15148401e+00,  1.71844289e-01,\n",
       "         1.60523951e-01, -6.69233263e-01, -6.46746874e-01,  6.39325613e-03,\n",
       "         1.11656070e+00, -6.19762480e-01, -5.74034452e-01, -1.40361702e+00,\n",
       "        -1.79769182e+00, -2.17913628e+00,  8.53936136e-01, -2.69776672e-01,\n",
       "         6.45285621e-02,  6.62140667e-01, -6.06512487e-01,  9.71794009e-01,\n",
       "        -2.63053715e-01, -7.20562160e-01, -7.00577557e-01,  1.21564269e+00,\n",
       "        -1.37421116e-03,  9.79253531e-01,  3.20315361e-01, -5.32510936e-01,\n",
       "        -1.54302567e-01, -6.71824098e-01,  1.04191244e+00, -7.86243021e-01,\n",
       "        -4.49799627e-01,  6.80328965e-01, -1.24394286e+00, -4.74416852e-01,\n",
       "         5.23911178e-01,  6.01072252e-01, -6.86143279e-01,  1.80583191e+00,\n",
       "        -1.53970611e+00, -7.27290571e-01, -2.84453988e+00,  1.74235795e-02,\n",
       "        -1.31048286e+00, -8.55985820e-01,  5.00398636e-01,  1.19653904e+00,\n",
       "         1.58755943e-01,  1.12479222e+00, -5.70540726e-01,  1.04409671e+00,\n",
       "        -4.66275573e-01, -8.27550054e-01, -3.25975132e+00, -2.29024678e-01,\n",
       "        -2.43786126e-01,  1.22628562e-01, -1.87007928e+00,  1.13504596e-01,\n",
       "        -4.58350807e-01, -5.49368262e-01,  3.04956526e-01,  1.37398034e-01,\n",
       "         7.06635714e-01,  7.52444625e-01,  1.84991860e+00, -1.19520104e+00,\n",
       "         2.99889177e-01,  1.03604186e+00, -1.16911769e+00, -1.41911924e+00,\n",
       "        -1.47693765e+00, -2.32573956e-01,  1.42686009e+00, -1.52298141e+00,\n",
       "         6.80665076e-01, -1.50493765e+00,  1.68968821e+00,  4.58204776e-01,\n",
       "        -6.75359368e-01,  9.96724218e-02, -2.21260929e+00,  1.69869530e+00,\n",
       "         1.32738018e+00,  3.25953752e-01,  1.23822117e+00, -3.24073195e-01,\n",
       "         2.48539537e-01,  8.00050080e-01, -1.67824340e+00, -3.32642019e-01,\n",
       "         1.61566639e+00, -8.68613422e-01, -5.96419983e-02, -3.05309415e-01,\n",
       "         3.15761149e-01, -1.03795409e+00,  7.36467466e-02,  8.02848518e-01,\n",
       "         6.58615768e-01, -2.55008101e-01,  1.60510931e-02,  1.57823086e+00,\n",
       "        -9.31965768e-01,  2.07059360e+00,  9.23620880e-01,  1.92663610e-01,\n",
       "        -4.45322059e-02,  8.92909110e-01, -4.84080344e-01, -2.71020949e-01,\n",
       "         3.96779269e-01,  2.56181169e+00,  7.28846341e-02, -7.01240450e-02,\n",
       "        -5.44962406e-01, -2.44199157e+00, -1.00349760e+00, -6.87447563e-02,\n",
       "        -6.21605098e-01,  2.33771753e+00,  2.31323436e-01], dtype=float32),\n",
       " array([-7.46075690e-01, -1.70153058e+00,  6.94810808e-01, -3.40163291e-01,\n",
       "        -2.37161666e-01,  4.87793647e-02,  2.80349907e-02, -1.19461679e+00,\n",
       "         9.69667673e-01,  1.51421332e+00, -7.09818676e-02,  3.50315005e-01,\n",
       "         6.05426393e-02,  9.76855814e-01,  1.65877557e+00,  1.11823237e+00,\n",
       "        -5.70692062e-01, -1.26384985e+00, -2.60411024e+00,  1.30422938e+00,\n",
       "         3.10490990e+00, -6.78044796e-01, -2.09608868e-01,  5.84654987e-01,\n",
       "         5.40999651e-01,  8.33603799e-01,  6.08090937e-01,  2.88831568e+00,\n",
       "        -1.34938526e+00, -6.74828947e-01, -1.15148401e+00,  1.71844289e-01,\n",
       "         1.60523951e-01, -6.69233263e-01, -6.46746874e-01,  6.39325613e-03,\n",
       "         1.11656070e+00, -6.19762480e-01, -5.74034452e-01, -1.40361702e+00,\n",
       "        -1.79769182e+00, -2.17913628e+00,  8.53936136e-01, -2.69776672e-01,\n",
       "         6.45285621e-02,  6.62140667e-01, -6.06512487e-01,  9.71794009e-01,\n",
       "        -2.63053715e-01, -7.20562160e-01, -7.00577557e-01,  1.21564269e+00,\n",
       "        -1.37421116e-03,  9.79253531e-01,  3.20315361e-01, -5.32510936e-01,\n",
       "        -1.54302567e-01, -6.71824098e-01,  1.04191244e+00, -7.86243021e-01,\n",
       "        -4.49799627e-01,  6.80328965e-01, -1.24394286e+00, -4.74416852e-01,\n",
       "         5.23911178e-01,  6.01072252e-01, -6.86143279e-01,  1.80583191e+00,\n",
       "        -1.53970611e+00, -7.27290571e-01, -2.84453988e+00,  1.74235795e-02,\n",
       "        -1.31048286e+00, -8.55985820e-01,  5.00398636e-01,  1.19653904e+00,\n",
       "         1.58755943e-01,  1.12479222e+00, -5.70540726e-01,  1.04409671e+00,\n",
       "        -4.66275573e-01, -8.27550054e-01, -3.25975132e+00, -2.29024678e-01,\n",
       "        -2.43786126e-01,  1.22628562e-01, -1.87007928e+00,  1.13504596e-01,\n",
       "        -4.58350807e-01, -5.49368262e-01,  3.04956526e-01,  1.37398034e-01,\n",
       "         7.06635714e-01,  7.52444625e-01,  1.84991860e+00, -1.19520104e+00,\n",
       "         2.99889177e-01,  1.03604186e+00, -1.16911769e+00, -1.41911924e+00,\n",
       "        -1.47693765e+00, -2.32573956e-01,  1.42686009e+00, -1.52298141e+00,\n",
       "         6.80665076e-01, -1.50493765e+00,  1.68968821e+00,  4.58204776e-01,\n",
       "        -6.75359368e-01,  9.96724218e-02, -2.21260929e+00,  1.69869530e+00,\n",
       "         1.32738018e+00,  3.25953752e-01,  1.23822117e+00, -3.24073195e-01,\n",
       "         2.48539537e-01,  8.00050080e-01, -1.67824340e+00, -3.32642019e-01,\n",
       "         1.61566639e+00, -8.68613422e-01, -5.96419983e-02, -3.05309415e-01,\n",
       "         3.15761149e-01, -1.03795409e+00,  7.36467466e-02,  8.02848518e-01,\n",
       "         6.58615768e-01, -2.55008101e-01,  1.60510931e-02,  1.57823086e+00,\n",
       "        -9.31965768e-01,  2.07059360e+00,  9.23620880e-01,  1.92663610e-01,\n",
       "        -4.45322059e-02,  8.92909110e-01, -4.84080344e-01, -2.71020949e-01,\n",
       "         3.96779269e-01,  2.56181169e+00,  7.28846341e-02, -7.01240450e-02,\n",
       "        -5.44962406e-01, -2.44199157e+00, -1.00349760e+00, -6.87447563e-02,\n",
       "        -6.21605098e-01,  2.33771753e+00,  2.31323436e-01], dtype=float32),\n",
       " array([-5.60803890e-01, -1.22354639e+00,  7.35591412e-01, -1.09984584e-01,\n",
       "         1.31562158e-01,  1.35400161e-01,  1.59026936e-01, -7.53230333e-01,\n",
       "         1.10910678e+00,  1.58273566e+00,  3.70905221e-01,  4.21993941e-01,\n",
       "         1.18792757e-01,  8.16921711e-01,  1.40589762e+00,  6.76699400e-01,\n",
       "        -7.54443347e-01, -5.47688186e-01, -2.76030755e+00,  1.58644354e+00,\n",
       "         3.10433984e+00, -7.81639874e-01, -2.89467275e-01,  9.07307386e-01,\n",
       "         1.81057245e-01,  6.72325134e-01,  4.32717472e-01,  2.47901821e+00,\n",
       "        -1.67967725e+00, -3.03038955e-01, -1.14597535e+00, -2.02946454e-01,\n",
       "         3.17958236e-01, -4.75866020e-01, -8.81870031e-01,  2.79708505e-01,\n",
       "         1.33153665e+00, -3.69629592e-01, -2.64285952e-01, -1.03431261e+00,\n",
       "        -1.66408682e+00, -2.07301664e+00,  1.18822968e+00,  1.84873626e-01,\n",
       "         2.36793920e-01,  3.32206488e-01, -6.98760271e-01,  6.74909115e-01,\n",
       "        -7.02435434e-01, -6.64188981e-01, -4.52767581e-01,  8.64253402e-01,\n",
       "        -1.05360590e-01,  1.01712418e+00,  3.49633545e-01, -4.56838518e-01,\n",
       "        -1.04202162e-02, -5.25265694e-01,  9.00718033e-01, -4.11308914e-01,\n",
       "        -7.36060262e-01,  7.42789090e-01, -1.18723905e+00, -9.76680040e-01,\n",
       "         6.07051134e-01,  8.64487529e-01, -4.84988779e-01,  2.22911978e+00,\n",
       "        -1.61943305e+00, -9.00964975e-01, -2.81631422e+00,  2.58973360e-01,\n",
       "        -1.51440644e+00, -1.18972933e+00,  6.72246039e-01,  1.39956045e+00,\n",
       "         5.18152595e-01,  6.65620089e-01, -3.88194203e-01,  1.21346545e+00,\n",
       "        -1.95215836e-01, -9.49003100e-01, -3.07824302e+00, -1.53047293e-01,\n",
       "        -5.04277825e-01,  7.99389005e-01, -2.15711427e+00,  2.11005621e-02,\n",
       "        -2.01034769e-01, -4.74100769e-01,  3.89160782e-01,  9.28230956e-02,\n",
       "         7.48002768e-01,  4.67451215e-01,  1.80829263e+00, -1.43025911e+00,\n",
       "         4.28540617e-01,  9.36324179e-01, -6.77290916e-01, -1.29309869e+00,\n",
       "        -1.11684060e+00,  1.94524787e-02,  9.94851410e-01, -1.57979763e+00,\n",
       "         8.00603628e-01, -1.51820385e+00,  1.31650674e+00,  7.99273014e-01,\n",
       "        -6.93897009e-01,  5.92384279e-01, -2.12906981e+00,  1.52151847e+00,\n",
       "         1.40251708e+00,  4.24603552e-01,  1.47872365e+00, -1.95592895e-01,\n",
       "         2.12725341e-01,  1.07119763e+00, -1.94080400e+00, -2.81091303e-01,\n",
       "         1.45335388e+00, -4.34104711e-01, -1.87624663e-01, -2.40031164e-02,\n",
       "         5.20735502e-01, -8.75591695e-01,  2.24107191e-01,  7.74271011e-01,\n",
       "         8.19681287e-01, -3.35834622e-01,  1.56357124e-01,  1.36443293e+00,\n",
       "        -5.13127565e-01,  2.04456091e+00,  9.27471697e-01, -1.54067725e-01,\n",
       "        -5.09630479e-02,  7.24871814e-01, -9.07183111e-01, -2.92168349e-01,\n",
       "         4.56905365e-01,  2.52387857e+00, -1.91142864e-03,  8.82643834e-02,\n",
       "        -8.44388306e-01, -2.15271044e+00, -8.07547569e-01, -3.88463676e-01,\n",
       "        -9.05951977e-01,  2.00611448e+00,  3.02299708e-01], dtype=float32)]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2556, -0.1369,  0.1209,  ...,  0.1854, -0.3174,  0.2770],\n",
       "        [ 0.2827,  0.1045,  0.0973,  ...,  0.0443,  0.0584,  0.2794],\n",
       "        [ 0.4754,  0.6994,  0.1200,  ...,  0.5351,  0.0853,  0.3239],\n",
       "        ...,\n",
       "        [-0.7461, -1.7015,  0.6948,  ..., -0.6216,  2.3377,  0.2313],\n",
       "        [-0.7461, -1.7015,  0.6948,  ..., -0.6216,  2.3377,  0.2313],\n",
       "        [-0.5608, -1.2235,  0.7356,  ..., -0.9060,  2.0061,  0.3023]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S=torch.as_tensor(samples)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=setup.makePlot(S,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
